<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 32]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.CY](#cs.CY) [Total: 13]
- [econ.EM](#econ.EM) [Total: 3]
- [q-fin.MF](#q-fin.MF) [Total: 2]
- [cs.AI](#cs.AI) [Total: 38]
- [eess.SY](#eess.SY) [Total: 15]
- [stat.ML](#stat.ML) [Total: 8]
- [math.OC](#math.OC) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition](https://arxiv.org/abs/2512.13884)
*Jonas Golde,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: FiNERweb是一个多语言命名实体识别数据集创建流程，利用教师-学生范式在91种语言和25种文字上生成约225k个段落和235k个实体标签，数据量比基线少19倍但性能相当或更好。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示大语言模型能提供有效的合成监督，但这些数据集多为实验副产品而非系统化、可复用的资源。需要创建系统化的多语言NER数据集来促进更有效的教师-学生训练。

Method: 基于FineWeb-Edu，训练回归模型识别NER相关段落，然后用多语言LLM进行标注。流程覆盖91种语言和25种文字，包含英语标签和翻译的目标语言标签集。

Result: 回归模型F1分数超过84；FiNERweb训练模型在英语、泰语和斯瓦希里语的零样本迁移中性能相当或更好（数据量少19倍）。标注质量高：忠实度3.99/5，完整性4.05/5。

Conclusion: FiNERweb提供了高质量、可复用的多语言NER数据集，支持更有效的教师-学生训练。观察到当前SOTA模型使用目标语言标签时性能下降0.02-0.09 F1，因此同时发布英语和翻译标签集。

Abstract: Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.

</details>


### [2] [Olmo 3](https://arxiv.org/abs/2512.13961)
*Team Olmo,:,Allyson Ettinger,Amanda Bertsch,Bailey Kuehl,David Graham,David Heineman,Dirk Groeneveld,Faeze Brahman,Finbarr Timbers,Hamish Ivison,Jacob Morrison,Jake Poznanski,Kyle Lo,Luca Soldaini,Matt Jordan,Mayee Chen,Michael Noukhovitch,Nathan Lambert,Pete Walsh,Pradeep Dasigi,Robert Berry,Saumya Malik,Saurabh Shah,Scott Geng,Shane Arora,Shashank Gupta,Taira Anderson,Teng Xiao,Tyler Murray,Tyler Romero,Victoria Graf,Akari Asai,Akshita Bhagia,Alexander Wettig,Alisa Liu,Aman Rangapur,Chloe Anastasiades,Costa Huang,Dustin Schwenk,Harsh Trivedi,Ian Magnusson,Jaron Lochner,Jiacheng Liu,Lester James V. Miranda,Maarten Sap,Malia Morgan,Michael Schmitz,Michal Guerquin,Michael Wilson,Regan Huff,Ronan Le Bras,Rui Xin,Rulin Shao,Sam Skjonsberg,Shannon Zejiang Shen,Shuyue Stella Li,Tucker Wilde,Valentina Pyatkin,Will Merrill,Yapei Chang,Yuling Gu,Zhiyuan Zeng,Ashish Sabharwal,Luke Zettlemoyer,Pang Wei Koh,Ali Farhadi,Noah A. Smith,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: Olmo 3是一个包含7B和32B参数规模的全开源语言模型家族，专注于长上下文推理、函数调用、编码、指令跟随、通用聊天和知识回忆，其中32B模型是目前最强的全开源思考模型。


<details>
  <summary>Details</summary>
Motivation: 构建一个完全开放的语言模型家族，涵盖从数据到最终模型的全生命周期，提供当前最强的全开源思考模型，满足长上下文推理、函数调用、编码等多种应用需求。

Method: 开发了包含7B和32B参数规模的语言模型家族，专注于长上下文推理、函数调用、编码等能力构建，并完整公开了模型构建的整个流程，包括每个阶段、检查点、数据点和依赖项。

Result: 发布了Olmo 3模型家族，其中旗舰模型Olmo 3 Think 32B是目前最强的全开源思考模型，在长上下文推理、函数调用、编码、指令跟随等方面表现出色。

Conclusion: Olmo 3模型家族代表了全开源语言模型的重要进展，特别是32B模型在思考能力方面达到了当前全开源模型的最高水平，为研究社区提供了完整的模型构建流程和资源。

Abstract: We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.

</details>


### [3] [Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models](https://arxiv.org/abs/2512.13980)
*Zhimin Qiu,Di Wu,Feng Liu,Chenrui Hu,Yuxiao Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的结构感知解码方法，用于解决嵌套和重叠实体提取任务中语义完整性和结构一致性的难题，在ACE 2005数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统方法在嵌套和重叠实体提取任务中难以同时保持语义完整性和结构一致性，需要一种能够统一建模实体边界、层次关系和跨依赖的方法。

Method: 提出结构感知解码方法：1）使用预训练语言模型获取上下文语义表示；2）通过候选表示组合捕获多粒度实体跨度特征；3）引入层次结构约束确保语义与结构一致性；4）联合优化分类损失和结构一致性损失增强稳定性。

Result: 在ACE 2005数据集上的实验显示，该方法在准确率、精确率、召回率和F1分数上均有显著提升，特别是在嵌套和重叠实体识别中表现出更强的边界定位和结构建模能力。

Conclusion: 验证了结构感知解码在复杂语义提取任务中的有效性，为开发具有层次理解能力的语言模型提供了新视角，为高精度信息提取建立了方法论基础。

Abstract: This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.

</details>


### [4] [What Affects the Effective Depth of Large Language Models?](https://arxiv.org/abs/2512.14064)
*Yi Hu,Cai Zhou,Muhan Zhang*

Main category: cs.CL

TL;DR: 研究发现当前大语言模型在不同规模、训练范式和任务难度下都存在层利用率不足的问题，有效深度比例保持稳定而非随难度增加


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型深度不断增加，性能提升却逐渐减少。先前研究提出"有效深度"概念，认为更深层的模型未能充分利用其层进行有意义的计算。本研究旨在系统探究有效深度如何随模型规模、训练类型和任务难度变化

Method: 分析Qwen-2.5系列模型（1.5B-32B）的行为，比较基础模型与对应长思维链模型的有效深度，评估不同难度任务下的层使用情况

Result: 1. 有效层数随模型规模增长，但有效深度比例保持稳定；2. 长思维链模型相比基础模型未增加有效深度，推理能力提升源于更长上下文而非更深层计算；3. 模型不会为更难问题动态使用更多层

Conclusion: 当前LLMs在不同规模、训练范式和任务难度下都存在层利用率不足的问题，这为提升LLMs层利用率、模型剪枝和早期退出等研究方向提供了机会

Abstract: The scaling of large language models (LLMs) emphasizes increasing depth, yet performance gains diminish with added layers. Prior work introduces the concept of "effective depth", arguing that deeper models fail to fully utilize their layers for meaningful computation. Building on this, we systematically study how effective depth varies with model scale, training type, and task difficulty. First, we analyze the model behavior of Qwen-2.5 family (1.5B-32B) and find that while the number of effective layers grows with model size, the effective depth ratio remains stable. Besides, comparisons between base and corresponding long-CoT models show no increase in effective depth, suggesting that improved reasoning stems from longer context rather than deeper per-token computation. Furthermore, evaluations across tasks of varying difficulty indicate that models do not dynamically use more layers for harder problems. Our results suggest that current LLMs underuse available depth across scales, training paradigms and tasks of varying difficulties, pointing out research opportunities on increasing the layer utilization rate of LLMs, model pruning, and early exiting. Our code is released at https://github.com/AheadOFpotato/what_affects_effective_depth.

</details>


### [5] [Inflation Attitudes of Large Language Models](https://arxiv.org/abs/2512.14306)
*Nikoleta Anesti,Edward Hill,Andreas Joseph*

Main category: cs.CL

TL;DR: 研究GPT-3.5-turbo基于宏观经济价格信号形成通胀感知和预期的能力，并与英国家庭调查数据对比


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在社会科学研究中的适用性，特别是能否模拟人类对通胀的感知和预期形成过程

Method: 采用准实验设计，利用GPT训练截止时间（2021年9月）作为自然实验，模拟英国央行通胀态度调查的信息集和人口特征，使用Shapley值分解分析模型输出驱动因素

Result: GPT在短期能跟踪总体调查预测和官方统计数据，在收入、住房状况和社会阶层等维度能复制家庭通胀感知的关键经验规律，对食品通胀信息表现出类似人类的高度敏感性，但缺乏一致的消费者价格通胀模型

Conclusion: 该方法可用于评估大语言模型在社会科学中的行为、比较不同模型或辅助调查设计，但GPT对通胀的理解仍有限

Abstract: This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.

</details>


### [6] [Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed](https://arxiv.org/abs/2512.14067)
*Yonggan Fu,Lexington Whalen,Zhifan Ye,Xin Dong,Shizhe Diao,Jingyu Liu,Chengyue Wu,Hao Zhang,Enze Xie,Song Han,Maksim Khadkevich,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 该论文提出了一种将预训练自回归语言模型转换为高效扩散语言模型的方法，通过改进注意力模式和训练策略，在保持准确性的同时实现并行生成和更高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然支持并行生成，但训练效率低于自回归模型。为了结合两者的优势，研究者希望将预训练的自回归模型转换为扩散模型，在保持任务准确性的同时获得更高的生成速度。

Method: 1. 提出块级注意力模式，在块内保持双向建模，在块间保持因果性，以更好地保留预训练权重分布；2. 引入位置相关的掩码策略，在训练时给后续token更高的掩码概率，以缩小训练-测试差距；3. 系统研究扩散语言模型的注意力模式、训练动态等设计选择。

Result: 提出的Efficient-DLM系列模型在准确性和效率上都超越了现有最佳模型。例如，8B参数的Efficient-DLM相比Dream 7B和Qwen3 4B，准确率分别提升5.4%和2.7%，吞吐量分别提高4.5倍和2.7倍。

Conclusion: 通过改进的AR-to-dLM转换方法，可以构建既保持自回归模型准确性又具备扩散模型高效并行生成能力的语言模型，为大规模语言模型的高效部署提供了可行方案。

Abstract: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.

</details>


### [7] [A Unified Sparse Attention via Multi-Granularity Compression](https://arxiv.org/abs/2512.14082)
*Siran Liu,Zane Cao,Yongchao He*

Main category: cs.CL

TL;DR: UniSparse提出了一种统一的稀疏注意力机制，通过复合令牌和多粒度压缩实现高效的长上下文处理，在保持高精度的同时显著提升计算速度。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法存在局限性：基于训练的方法成本高且不能作为加速插件直接应用于其他模型，而推理时方法往往在效率或多模态通用性上有所妥协。自注意力机制的二次方复杂度成为长上下文处理的计算瓶颈。

Method: 引入复合令牌概念，通过多粒度压缩和块级选择动态构建稀疏注意力。复合令牌聚合多粒度上下文信息，实现硬件友好的GPU执行。

Result: 在多种模态和任务中，UniSparse在准确性和效率上均优于现有稀疏注意力方法（如MInference、XAttention、FlexPrefill），达到≥99%全注意力精度，注意力计算比FlashAttention快达2.61倍。

Conclusion: UniSparse提供了一种统一、高效的稀疏注意力机制，有效解决了长上下文处理中的计算瓶颈问题，在保持高精度的同时显著提升计算效率。

Abstract: Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\ge$ 99% of full-attention accuracy and up to 2.61$\times$ faster attention computation than FlashAttention.

</details>


### [8] [Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study](https://arxiv.org/abs/2512.14085)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Taiga Mori,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 提出多语言连续反馈预测模型，用于研究日语、英语和汉语的跨语言时序行为，模型在三种语言上均达到或超过单语基线，揭示了不同语言的反馈时序差异。


<details>
  <summary>Details</summary>
Motivation: 研究不同语言中反馈信号（如"嗯"、"啊哈"等）的时序差异，为设计更自然、文化感知的语音对话系统提供理论基础和实用模型。

Method: 基于Transformer的帧级多语言模型，在约300小时的双人对话数据上进行联合训练，包含辅助任务，支持日语、英语和汉语三种语言。

Result: 多语言模型在所有三种语言上均匹配或超越单语基线；零样本迁移有限，表明实质性跨语言差异；扰动分析显示日语依赖短期语言信息，英语和汉语对沉默时长和韵律变化更敏感；日语对短上下文相对鲁棒，汉语明显受益于长上下文。

Conclusion: 该研究提供了统一的模型和实证证据，揭示了不同语言中反馈时序的差异，有助于设计更自然、文化感知的语音对话系统，并展示了实时CPU推理的可行性。

Abstract: We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.

</details>


### [9] [CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models](https://arxiv.org/abs/2512.14118)
*Yiran Zhang,Jincheng Hu,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: CogMem是一个受认知启发的记忆增强LLM架构，通过结构化持久记忆支持持续迭代推理，包含三层记忆系统，有效缓解推理失败并控制上下文增长。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮推理中表现出色，但在多轮交互中会失去准确性和连贯性。现有方法通常附加完整对话历史，导致无限制的上下文增长、计算成本增加和推理效率下降。

Method: 引入CogMem架构，包含三层：长期记忆（LTM）整合跨会话推理策略；直接访问（DA）记忆维护会话级笔记并检索相关长期记忆；注意力焦点（FoA）机制动态重建简洁的任务相关上下文。

Result: 在TurnBench上的实验表明，这种分层设计缓解了推理失败，控制了上下文增长，并提高了扩展推理链的一致性。

Conclusion: CogMem通过结构化记忆系统使LLM能够进行更可靠、更类似人类的推理，朝着持续迭代推理的方向发展。

Abstract: Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.

</details>


### [10] [Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents](https://arxiv.org/abs/2512.14142)
*Hongqiu Ni,Jiabao Zhang,Guopeng Li,Zilong Wang,Ruiqi Wu,Chi Zhang,Haisheng Tan*

Main category: cs.CL

TL;DR: Astraea：一种面向LLM智能体工作流的服务引擎，通过全局调度优化减少端到端延迟


<details>
  <summary>Details</summary>
Motivation: 现有推理系统（如vLLM）专注于局部段优化，无法最小化LLM智能体多阶段工作流的端到端延迟，需要在本地计算和外部API调用之间交替时进行全局优化

Method: 1. 状态感知的分层调度算法，整合请求历史状态和未来预测；2. 动态分类请求的I/O和计算密集型特性；3. 增强的HRRN策略平衡效率和公平性；4. 自适应KV缓存管理器，根据系统内存压力智能处理I/O等待期间的智能体状态

Result: 相比基线方法，Astraea将平均作业完成时间（JCT）降低高达25.5%，在高负载下表现出强大的鲁棒性和稳定性，适用于各种模型规模

Conclusion: Astraea通过将优化重点从局部段转移到全局请求生命周期，有效解决了LLM智能体工作流的端到端延迟问题，为智能体服务系统提供了更高效的调度方案

Abstract: Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.

</details>


### [11] [A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs](https://arxiv.org/abs/2512.14179)
*K. M. Jubair Sami,Dipto Sumit,Ariyan Hossain,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文提出并比较了两种用于标准孟加拉语到方言翻译的RAG管道，发现基于标准化句对的管道优于基于转录本的管道，使较小模型能超越更大模型，为低资源方言翻译提供了无需微调的有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 标准语言到区域方言的翻译是NLP的重要挑战，特别是在孟加拉语中，由于数据稀缺和语言变异性，这一问题尤为突出。需要开发有效的解决方案来处理低资源方言翻译问题。

Method: 提出并比较两种RAG管道：1) 基于转录本的管道，使用音频转录中的大方言句子上下文；2) 标准化句对管道，使用结构化的local_dialect:standard_bengali句对。在六种孟加拉方言和多种LLM上使用BLEU、ChrF、WER和BERTScore进行评估。

Result: 句对管道始终优于转录本管道，将吉大港方言的词错误率从76%降至55%。关键发现是精心设计的检索策略比模型规模更重要，使较小模型(如Llama-3.1-8B)能超越更大模型(如GPT-OSS-120B)。

Conclusion: 这项工作为低资源方言翻译提供了有效且无需微调的解决方案，展示了精心设计的检索策略可以比模型规模更重要，为保护语言多样性提供了实用蓝图。

Abstract: Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\_dialect:standard\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\% to 55\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.

</details>


### [12] [Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets](https://arxiv.org/abs/2512.14237)
*Estelle Zheng,Nathan Cerisara,Sébastien Warichet,Emmanuel Helbert,Christophe Cerisara*

Main category: cs.CL

TL;DR: Ladder Side Tuning (LST) 是一种参数高效微调方法，通过添加轻量级侧网络，在保持与QLoRA相似性能的同时，将峰值内存使用减少50%，使7B模型能在12GB GPU上微调2k上下文长度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调受限于GPU内存，现有参数高效微调方法如QLoRA虽然减少了可训练参数，但反向传播仍导致高内存使用。需要更内存高效的微调方法。

Method: 重新探索Ladder Side Tuning (LST)，添加轻量级侧网络，减少内存使用。提出xLadder变体，通过交叉连接增加有效深度，缩短思维链推理。

Result: LST在自然语言理解、数学和LLM批评任务上性能与QLoRA相当，峰值内存减少50%，使7B模型能在12GB GPU上微调2k上下文。xLadder在相同参数下实现更深推理。

Conclusion: LST是一种内存高效的微调方法，特别适合内存受限场景；xLadder进一步扩展了架构灵活性，实现更深推理而无额外内存开销。

Abstract: Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

</details>


### [13] [Two CFG Nahuatl for automatic corpora expansion](https://arxiv.org/abs/2512.14239)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Graham Ranger Martha-Lorena Avendaño-Garrido*

Main category: cs.CL

TL;DR: 本文提出两种用于Nawatl语料库扩展的上下文无关文法，以解决这种墨西哥原住民语言数字资源匮乏的问题，通过生成大量语法正确的句子来扩展语料库，用于学习非上下文嵌入，实验结果显示扩展后的语料库在语义相似度任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: Nawatl是一种墨西哥原住民语言，属于π-语言类型（数字资源匮乏的语言），现有可用于大型语言模型学习的语料库几乎不存在，这构成了重大挑战。需要解决语料库稀缺问题以支持Nawatl语言处理任务。

Method: 提出了两种新的Nawatl上下文无关文法，并在生成模式下使用这些文法。通过这些文法可以显著扩展Nawatl语料库，生成大量语法正确的合成句子，然后使用扩展后的语料库学习嵌入表示。

Result: 结果显示，与仅使用原始语料库相比，使用人工扩展后的语料库在句子语义相似度任务上表现有所改善。同时发现经济型嵌入（economical embeddings）在某些情况下比一些大型语言模型表现更好。

Conclusion: 提出的上下文无关文法方法能够有效扩展Nawatl语料库，解决数字资源匮乏语言的语料库稀缺问题，为学习非上下文嵌入提供了可行方案，并且经济型嵌入在特定任务中可能优于大型语言模型。

Abstract: The aim of this article is to introduce two Context-Free Grammars (CFG) for Nawatl Corpora expansion. Nawatl is an Amerindian language (it is a National Language of Mexico) of the $π$-language type, i.e. a language with few digital resources. For this reason the corpora available for the learning of Large Language Models (LLMs) are virtually non-existent, posing a significant challenge. The goal is to produce a substantial number of syntactically valid artificial Nawatl sentences and thereby to expand the corpora for the purpose of learning non contextual embeddings. For this objective, we introduce two new Nawatl CFGs and use them in generative mode. Using these grammars, it is possible to expand Nawatl corpus significantly and subsequently to use it to learn embeddings and to evaluate their relevance in a sentences semantic similarity task. The results show an improvement compared to the results obtained using only the original corpus without artificial expansion, and also demonstrate that economic embeddings often perform better than some LLMs.

</details>


### [14] [From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition](https://arxiv.org/abs/2512.14244)
*Yiqing Zhou,Yu Lei,Shuzheng Si,Qingyan Sun,Wei Wang,Yifei Wu,Hao Wen,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于教育话语单元（EDU）的上下文压缩器，通过结构-选择两阶段方法解决长上下文处理中的计算成本和噪声问题，在保持全局结构和细粒度细节的同时显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理长上下文时面临计算成本高、引入噪声的问题。现有压缩技术要么通过离散标记删除破坏局部连贯性，要么依赖存在位置偏差且与闭源API不兼容的隐式潜在编码。

Method: 提出EDU-based Context Compressor框架，采用结构-选择两阶段方法：1) LingoEDU将线性文本转换为基于源索引锚定的教育话语单元（EDU）结构关系树；2) 轻量级排名模块选择查询相关的子树进行线性化。同时发布了StructBench评估数据集。

Result: 该方法在结构预测准确性方面达到最先进水平，显著优于前沿LLM同时降低成本。结构感知压缩在从长上下文任务到复杂深度搜索场景的各种下游任务中大幅提升性能。

Conclusion: EDU-based Context Compressor通过显式压缩框架有效解决了长上下文处理中的关键瓶颈，在保持文本结构完整性的同时实现了高效压缩，为长文档问答和自主代理等应用提供了实用解决方案。

Abstract: Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.

</details>


### [15] [Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring](https://arxiv.org/abs/2512.14332)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,John D. Kelleher*

Main category: cs.CL

TL;DR: 提出Step-Tagging框架，通过实时标注推理步骤类型实现语言推理模型的高效早期停止，减少20-50%的token生成


<details>
  <summary>Details</summary>
Motivation: 当前语言推理模型(LRMs)存在效率低下、过度生成验证和反思步骤的问题，需要更高效的控制机制

Method: 引入Step-Tagging框架和ReasonType分类法，通过轻量级句子分类器实时标注推理步骤类型，监控推理行为并建立可解释的早期停止标准

Result: 在MATH500、GSM8K、AIME等数学任务及GPQA、MMLU-Pro非数学任务上，实现20-50%的token减少，同时保持与标准生成相当的准确率

Conclusion: 该工作为语言推理模型提供了新的生成控制方法，并为研究LRMs行为提供了新工具

Abstract: The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.

</details>


### [16] [Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.14427)
*Gabriele Prato,Shagun Sodhani,Alessandro Sordoni,Sarath Chandar*

Main category: cs.CL

TL;DR: 研究发现文档打包训练策略能提升大语言模型的多跳推理能力，但会增加计算成本，通过消融实验揭示了其优势机制


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练中普遍采用文档打包策略以提高计算效率，但这种策略对模型能力的影响尚未得到充分研究，特别是对多跳推理能力的影响

Method: 研究不同文档打包策略对LLM多跳推理能力的影响，通过对比打包训练与单文档训练的效果，并进行消融实验分析关键影响因素

Result: 打包训练相比单文档训练能提升模型性能，但需要更多计算资源；消融实验识别出解释打包优势的关键因素

Conclusion: 研究深化了对LLM训练动态的理解，为优化模型开发提供了实用见解，文档打包策略在提升性能方面具有价值但需权衡计算成本

Abstract: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.

</details>


### [17] [SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models](https://arxiv.org/abs/2512.14481)
*Shizhuo Mao,Song Chen,Yi Kang*

Main category: cs.CL

TL;DR: SASQ是一个轻量级的量化感知训练框架，专门针对激活量化因子进行优化，通过仅调整量化因子而不改变预训练权重，实现高精度静态推理，在LLaMA2-7B上比现有SOTA量化方案和FP16模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型规模增长快于GPU内存发展，部署面临挑战。现有量化方案存在基本权衡：动态量化计算开销大且边缘设备部署困难，静态量化牺牲精度，量化感知训练权重训练成本高。

Method: 提出SASQ框架，专门针对激活量化因子进行轻量级量化感知训练。仅优化量化因子而不改变预训练权重，自适应截断部分异常值，降低量化难度同时保持激活分布特性。

Result: SASQ不仅超越现有SOTA量化方案，还优于对应的FP16模型。在LLaMA2-7B上，WikiText2数据集上比QuaRot困惑度降低5.2%，比FP16模型困惑度降低4.7%。

Conclusion: SASQ通过仅优化激活量化因子的轻量级方法，解决了现有量化方案的权衡问题，实现了高精度静态推理，同时保持部署效率，为LLM部署提供了有效解决方案。

Abstract: Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

</details>


### [18] [C-ing Clearly: Enhanced Binary Code Explanations using C code](https://arxiv.org/abs/2512.14500)
*Teodor Poncu,Ioana Pintilie,Marius Dragoi,Dragos Tantaru,Florin Brad*

Main category: cs.CL

TL;DR: 提出C-ing Clearly方法，利用C代码生成合成数据来增强LLM对汇编语言的理解，在二进制代码摘要和漏洞检测任务上取得性能提升


<details>
  <summary>Details</summary>
Motivation: LLM通常在高级编程语言任务上表现优异，但在低级语言（如汇编）上表现不佳，需要提升LLM对汇编代码的理解能力

Method: 提出C-ing Clearly合成数据生成方法，利用对应的C代码来增强LLM对汇编的理解，通过微调生成的合成数据来提升模型性能

Result: 在二进制代码摘要和漏洞检测任务上展示了性能提升，在不同LLM家族和模型规模上都获得了稳定的性能增益

Conclusion: 通过利用C代码生成合成数据的方法，可以有效增强LLM对汇编语言的理解能力，在低级编程语言任务上取得显著改进

Abstract: Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.

</details>


### [19] [Linguists should learn to love speech-based deep learning models](https://arxiv.org/abs/2512.14506)
*Marianne de Heer Kloots,Paul Boersma,Willem Zuidema*

Main category: cs.CL

TL;DR: 论文批评Futrell和Mahowald框架过于关注基于文本的LLMs，主张音频深度学习模型对语言学研究更重要


<details>
  <summary>Details</summary>
Motivation: Futrell和Mahowald提出的框架虽然连接了深度学习系统和语言学理论，但过于关注生成式文本LLMs，忽略了人类语言中许多无法通过书面文本捕捉的重要方面

Method: 通过批判性分析现有框架的局限性，提出音频深度学习模型应发挥关键作用，强调语音信号包含的丰富语言信息

Result: 指出基于文本的LLMs框架存在根本性限制，无法充分解决人类语言研究的许多有趣问题，特别是那些超出书面文本范围的语言现象

Conclusion: 音频深度学习模型在连接深度学习与语言学研究中应扮演关键角色，因为它们能捕捉人类语言中书面文本无法表达的重要维度

Abstract: Futrell and Mahowald present a useful framework bridging technology-oriented deep learning systems and explanation-oriented linguistic theories. Unfortunately, the target article's focus on generative text-based LLMs fundamentally limits fruitful interactions with linguistics, as many interesting questions on human language fall outside what is captured by written text. We argue that audio-based deep learning models can and should play a crucial role.

</details>


### [20] [VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse](https://arxiv.org/abs/2512.14531)
*Ying Nie,Kai Han,Hongguang Li,Hang Zhou,Tianyu Guo,Enhua Wu,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: VersatileFFN是一种新颖的前馈网络，通过宽度和深度两个维度灵活复用参数，在固定参数预算下提升模型能力，无需增加内存开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型扩展带来显著性能提升，但导致内存成本过高。现有的参数高效方法（如剪枝和量化）主要压缩预训练模型而不增强架构能力，因此受到基础模型表示能力的限制。

Method: 受认知双过程理论启发，VersatileFFN包含两个自适应路径：宽度灵活路径从单个共享FFN生成子专家混合，模拟稀疏专家路由而不增加参数；深度灵活路径递归应用相同FFN模拟对复杂token的更深处理。难度感知门控动态平衡两个路径，引导"简单"token通过高效的宽度路径，为"困难"token分配更深迭代细化。

Result: 在不同基准测试和模型规模上的实验证明了该方法的有效性。两个路径复用相同参数，所有额外能力来自计算而非内存。

Conclusion: VersatileFFN通过参数在宽度和深度维度的灵活复用，在固定参数预算下提升模型能力，为解决LLM内存成本问题提供了新思路。

Abstract: The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering "easy" tokens through the efficient width-wise route and allocating deeper iterative refinement to "hard" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.

</details>


### [21] [Dual Language Models: Balancing Training Efficiency and Overfitting Resilience](https://arxiv.org/abs/2512.14549)
*David Samuel,Lucas Georges Gabriel Charpentier*

Main category: cs.CL

TL;DR: 结合自回归和掩码扩散训练目标，无需架构修改，获得优于单一目标模型的灵活语言模型


<details>
  <summary>Details</summary>
Motivation: 自回归模型训练效率高但容易过拟合，掩码扩散模型训练效率低但抗过拟合能力强，需要结合两者优势

Method: 在50个语言模型上训练和评估不同数据重复程度下的双目标训练，寻找自回归和掩码扩散目标之间的最优比例

Result: 在所有评估设置下，结合两种目标都是最优的，且无论针对自回归还是掩码扩散下游任务，最优比例都相似

Conclusion: 双目标训练实现了两种方法的优势互补，无需架构修改即可获得更灵活、性能更好的语言模型

Abstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

</details>


### [22] [VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models](https://arxiv.org/abs/2512.14554)
*Nguyen Tien Dong,Minh-Anh Nguyen,Thanh Dat Hoang,Nguyen Tuan Ngoc,Dao Xuan Quang Minh,Phan Phi Hai,Nguyen Thi Ngoc Anh,Dang Van Tu,Binh Vu*

Main category: cs.CL

TL;DR: VLegal-Bench是首个针对越南法律的综合基准测试，包含10,450个样本，基于Bloom认知分类法设计，用于系统评估LLM在越南法律任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 越南法律的复杂性、层级结构和频繁修订给评估LLM理解和应用法律知识带来了挑战，需要专门的基准测试来系统评估LLM在越南法律领域的表现。

Method: 基于Bloom认知分类法设计多层次法律理解任务，通过严格的标注流程生成10,450个样本，由法律专家标注和交叉验证，确保每个样本基于权威法律文件，涵盖一般法律问答、检索增强生成、多步推理和基于场景的问题解决等实际法律助理工作流程。

Result: 创建了首个全面的越南法律基准测试VLegal-Bench，提供了标准化、透明且基于认知理论的评估框架，为评估LLM在越南法律环境中的表现奠定了坚实基础。

Conclusion: VLegal-Bench为评估LLM在越南法律环境中的性能提供了可靠基础，支持开发更可靠、可解释且符合伦理的AI辅助法律系统。

Abstract: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.

</details>


### [23] [Agreement Between Large Language Models and Human Raters in Essay Scoring: A Research Synthesis](https://arxiv.org/abs/2512.14561)
*Hongli Li,Che Han Chen,Kevin Fan,Chiho Young-Johnson,Soyoung Lim,Yali Feng*

Main category: cs.CL

TL;DR: LLMs在自动作文评分中与人类评分者的一致性为中等至良好水平（0.30-0.80），但存在显著变异性，需要标准化报告实践。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自动作文评分中显示出潜力，但其与人类评分者一致性的实证研究结果存在矛盾，需要系统性的综合评估。

Method: 遵循PRISMA 2020指南，对2022年1月至2025年8月间的65项已发表和未发表研究进行系统综述，分析LLMs与人类评分者在作文评分中的一致性。

Result: LLM与人类评分者的一致性总体上为中等至良好水平，一致性指数（如二次加权Kappa、Pearson相关系数、Spearman's rho）主要在0.30-0.80之间，但不同研究间存在显著变异性。

Conclusion: LLMs在作文评分中表现出与人类评分者相当的一致性，但研究结果的变异性凸显了标准化报告实践的重要性，并为未来研究提供了方向。

Abstract: Despite the growing promise of large language models (LLMs) in automatic essay scoring (AES), empirical findings regarding their reliability compared to human raters remain mixed. Following the PRISMA 2020 guidelines, we synthesized 65 published and unpublished studies from January 2022 to August 2025 that examined agreement between LLMs and human raters in AES. Across studies, reported LLM-human agreement was generally moderate to good, with agreement indices (e.g., Quadratic Weighted Kappa, Pearson correlation, and Spearman's rho) mostly ranging between 0.30 and 0.80. Substantial variability in agreement levels was observed across studies, reflecting differences in study-specific factors as well as the lack of standardized reporting practices. Implications and directions for future research are discussed.

</details>


### [24] [Polypersona: Persona-Grounded LLM for Synthetic Survey Responses](https://arxiv.org/abs/2512.14562)
*Tejaswani Dash,Dinesh Karri,Anudeep Vurity,Gautam Datla,Tazeem Ahmad,Saima Rafi,Rohith Tangudu*

Main category: cs.CL

TL;DR: PolyPersona是一个生成框架，使用参数高效的LoRA适配器和4位量化，通过指令微调紧凑聊天模型来合成跨多个领域的人物条件调查响应。


<details>
  <summary>Details</summary>
Motivation: 需要一种高效且可重复的方法来生成合成调查数据，以支持可扩展的评估，同时通过透明和开放的协议促进偏见分析。当前需要人物条件化的响应生成，但资源有限。

Method: 使用参数高效的LoRA适配器配合4位量化，在资源自适应训练设置下对紧凑聊天模型进行指令微调。采用基于对话的数据管道，明确保留人物线索，确保生成响应中的行为一致性。构建了包含3,568个合成调查响应的数据集，涵盖10个领域和433个不同人物。

Result: 紧凑模型如TinyLlama 1.1B和Phi-2实现了与较大的7B到8B基线相当的性能，最高BLEU得分为0.090，ROUGE-1为0.429。人物条件化微调使小型语言模型能够生成可靠且连贯的合成调查数据。

Conclusion: 该框架为调查数据生成提供了一种高效且可重复的方法，支持可扩展的评估，同时通过透明和开放的协议促进偏见分析。人物条件化微调使小型语言模型能够生成可靠的合成调查数据。

Abstract: This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.

</details>


### [25] [Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies](https://arxiv.org/abs/2512.14576)
*Ekaterina Artemova,Laurie Burchell,Daryna Dementieva,Shu Okabe,Mariya Shmatova,Pedro Ortiz Suarez*

Main category: cs.CL

TL;DR: 这是一个关于为多语言和低资源语言构建NLP管道的实践教程，涵盖从数据收集到下游应用的完整流程，重点关注公平性、可重复性和社区参与。


<details>
  <summary>Details</summary>
Motivation: 为NLP从业者、研究人员和开发者提供工具，帮助他们为代表性不足的语言创建更公平、更具社会影响力的语言技术，解决数据稀缺和文化差异的挑战。

Method: 提供端到端NLP管道构建的实践工具包，包括数据收集、网络爬取、平行句挖掘、机器翻译，以及文本分类和多模态推理等下游应用。采用公平、可重复、社区知情的发展方法。

Result: 教程展示了涵盖10多种来自不同语系和地缘政治背景的语言的多样化用例，包括数字资源丰富和严重代表性不足的语言。

Conclusion: 该教程为构建低资源语言NLP系统提供了全面的实践指导，强调公平性和社区参与，有助于推动语言技术的包容性发展。

Abstract: This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.

</details>


### [26] [Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer](https://arxiv.org/abs/2512.14585)
*Adarsha Shrestha,Basanta Pokharel,Binit Shrestha,Smriti Adhikari,Dinesh Gothe*

Main category: cs.CL

TL;DR: 本研究开发了一个基于GPT-2的尼泊尔语语言模型，采用GPT-3启发的训练策略和专门训练的BPE分词器，在尼泊尔语新闻文本生成上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔语作为低资源语言，由于复杂的语法、粘着性形态和高质量语料库有限，在自然语言处理领域面临挑战。现有编码器架构对尼泊尔语文本生成不足，需要专门的语言模型。

Method: 采用GPT-2架构，结合GPT-3启发的训练策略（优化学习率调度、批量缩放和架构改进）。训练专门的16k BPE分词器，使用10.75GB清洁尼泊尔语语料和网络爬取的新闻文章进行预训练，集成FlashAttention减少内存使用。

Result: 经过两个epoch训练后，模型训练损失为3.168177，验证损失为3.081982，最终困惑度为21.80，能够生成连贯的尼泊尔语新闻风格文本。

Conclusion: 该研究成功开发了专门针对尼泊尔语的GPT-2模型，通过定制分词器和优化训练策略，有效提升了尼泊尔语文本生成能力，为低资源语言NLP研究提供了参考。

Abstract: Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.

</details>


### [27] [JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction](https://arxiv.org/abs/2512.14620)
*Atsuyuki Miyai,Shota Onohara,Jeonghun Baek,Kiyoharu Aizawa*

Main category: cs.CL

TL;DR: JMMMU-Pro是一个基于图像的日语多学科多模态理解基准，通过将问题图像和文本组合成单一图像，创建需要视觉感知的集成视觉-文本理解评估工具。使用Vibe Benchmark Construction方法，利用图像生成模型（如Nano Banana Pro）生成候选视觉问题，人工验证和调整以确保质量。


<details>
  <summary>Details</summary>
Motivation: 从MMMU到MMMU-Pro的演进中，需要为日语环境创建更严格的评估基准。现有基准可能无法充分评估LMMs的日语多模态理解能力，特别是需要集成视觉感知和文本理解的任务。

Method: 提出Vibe Benchmark Construction方法：1）使用图像生成模型（Nano Banana Pro）生成候选视觉问题；2）人工验证输出质量；3）必要时调整提示重新生成以确保质量。利用Nano Banana Pro的高真实感图像生成和干净日语文本嵌入能力。

Result: 构建了高质量的JMMMU-Pro基准，涵盖广泛的背景和布局设计。实验结果显示所有开源LMMs在JMMMU-Pro上都表现不佳，突显了该基准的挑战性。

Conclusion: JMMMU-Pro为评估LMMs的日语能力提供了更严格的工具，Vibe Benchmark Construction方法为未来基于图像的VQA基准开发提供了高效指南。该基准对开源社区的未来发展具有重要指导意义。

Abstract: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.

</details>


### [28] [TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines](https://arxiv.org/abs/2512.14645)
*David Schulmeister,Valentin Hartmann,Lars Klein,Robert West*

Main category: cs.CL

TL;DR: 该论文提出了TiME（Tiny Monolingual Encoders）小型单语编码器，针对需要高效处理大量数据或实时响应的NLP应用，通过蒸馏等技术训练小型模型，在性能与效率之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 当前研究过度关注大型通用语言模型，但许多NLP应用只需要特定的小型能力集。大型模型在处理大量数据时速度不足，能耗过高，不适合实时响应或电池供电设备部署，存在可持续性问题。

Method: 使用现代训练技术如蒸馏（distillation），从多语言教师模型蒸馏出单语模型，从使用相对位置嵌入的教师模型蒸馏出使用绝对位置嵌入的模型，支持低资源语言。

Result: TiME模型在常见NLP任务上进行了全面评估，在基准性能与吞吐量、延迟和能耗之间取得了更好的权衡。证明了从多语言教师模型蒸馏单语模型以及从相对位置嵌入模型蒸馏绝对位置嵌入模型的可行性。

Conclusion: 针对效率关键型应用，训练小型专用模型比使用大型通用模型更合适。TiME模型展示了通过蒸馏等技术可以在保持性能的同时显著提升效率，为资源受限环境提供了实用解决方案。

Abstract: Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

</details>


### [29] [Fast and Accurate Causal Parallel Decoding using Jacobi Forcing](https://arxiv.org/abs/2512.14681)
*Lanxiang Hu,Siqi Kou,Yichao Fu,Samyam Rajbhandari,Tajana Rosing,Yuxiong He,Zhijie Deng,Hao Zhang*

Main category: cs.CL

TL;DR: Jacobi Forcing是一种渐进式蒸馏范式，通过训练模型在自身生成的并行解码轨迹上，将自回归模型平滑转换为高效并行解码器，同时保持其预训练的因果推理特性，实现3.8-4.0倍的实际加速。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散大语言模型（dLLMs）并行解码方法存在预训练与后训练不匹配问题：后训练中的掩码数据分布偏离预训练时的真实数据分布，且双向注意力机制与预训练的因果先验冲突，阻碍了精确KV缓存重用，导致加速效果有限。

Method: 提出Jacobi Forcing渐进式蒸馏范式，让模型在自身生成的并行解码轨迹上进行训练，平滑地将自回归模型转变为高效并行解码器。基于Jacobi Forcing模型的轨迹特性，引入多块解码与拒绝回收机制，提高每次迭代的令牌接受数量。

Result: Jacobi Forcing模型在编程和数学基准测试上实现了3.8倍的实际加速，性能损失最小。通过多块解码与拒绝回收机制，每次迭代的令牌接受数量提高了4.5倍，实现近4.0倍的实际加速。

Conclusion: Jacobi Forcing通过渐进式蒸馏解决了预训练-后训练不匹配问题，成功将自回归模型转换为高效并行解码器，在保持生成质量的同时显著加速推理，为大规模模型推理加速提供了有效方案。

Abstract: Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.

</details>


### [30] [Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization](https://arxiv.org/abs/2512.14687)
*Yen-Ju Lu,Kunxiao Gao,Mingrui Liang,Helin Wang,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: 论文提出了首个将原始对话音频与事实摘要、情感摘要及副语言线索对齐的数据集Spoken DialogSum，包含13,460个情感多样对话，并通过Audio-LLM基线验证了端到端语音建模的价值。


<details>
  <summary>Details</summary>
Motivation: 当前音频语言模型能处理长对话，但情感感知或口语对话摘要研究受限于缺乏连接语音、摘要和副语言线索的数据集。

Method: 分两阶段构建数据集：1) 用LLM重写DialogSum脚本，添加填充词和反馈词，并为每个话语标注情感、音高和语速；2) 用表达性TTS引擎从标注脚本合成语音，并与副语言标签对齐。

Result: 创建了包含13,460个情感多样对话的Spoken DialogSum数据集，每个对话配有事实摘要和情感摘要。基线实验显示Audio-LLM相比级联ASR-LLM系统将情感摘要ROUGE-L提高了28%。

Conclusion: Spoken DialogSum填补了语音-摘要-副语言对齐数据的空白，证明了端到端语音建模在情感感知对话摘要中的优势，为相关研究提供了宝贵资源。

Abstract: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.

</details>


### [31] [MMGR: Multi-Modal Generative Reasoning](https://arxiv.org/abs/2512.14691)
*Zefan Cai,Haoyi Qiu,Tianyi Ma,Haozhe Zhao,Gengze Zhou,Kung-Hsiang Huang,Parisa Kordjamshidi,Minjia Zhang,Xiao Wen,Jiuxiang Gu,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: MMGR是一个评估视频生成模型推理能力的多模态基准，包含物理、逻辑、空间等5种推理能力，在抽象推理、具身导航和物理常识三个领域测试，发现当前模型在抽象推理和长期空间规划上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型评估指标（如FVD）过于关注感知质量，忽略了模型在因果性、物理规律和全局一致性方面的推理失败。需要建立一个能系统评估生成模型推理能力的基准。

Method: 提出MMGR评估框架，基于5种推理能力（物理、逻辑、3D空间、2D空间、时序），在三个领域（抽象推理、具身导航、物理常识）进行测试，使用细粒度指标要求视频和图像生成的全面正确性。

Result: 测试了领先的视频模型（Veo-3, Sora-2, Wan-2.2）和图像模型，发现模型在物理常识任务上表现尚可，但在抽象推理上表现极差（ARC-AGI准确率低于10%），在具身导航的长期空间规划上也很困难。

Conclusion: 当前模型过度依赖感知数据，全局状态一致性弱，目标函数奖励视觉合理性而非因果正确性。MMGR提供了一个统一的诊断基准，为构建具有推理能力的生成世界模型指明了方向。

Abstract: Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.

</details>


### [32] [MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset](https://arxiv.org/abs/2511.19317)
*Md. Tanzim Ferdous,Naeem Ahsan Chowdhury,Prithwiraj Bhattacharjee*

Main category: cs.CL

TL;DR: 开发了一个包含54,000+条孟加拉语文章和摘要的多领域数据集，用于抽象摘要任务，并建立了深度学习基准模型


<details>
  <summary>Details</summary>
Motivation: 现有孟加拉语摘要研究主要集中于新闻文章，但现实中的孟加拉语内容来源多样（博客、报纸、社交媒体），需要能够适应不同写作风格的摘要系统来应对信息过载问题

Method: 从Cinegolpo博客、Samakal和The Business Standard报纸等多个来源收集了超过54,000篇孟加拉语文章和摘要，构建多领域数据集；使用LSTM、BanglaT5-small和MTS-small等深度学习和迁移学习模型进行训练和评估

Result: 成功创建了首个大规模多领域孟加拉语抽象摘要数据集，建立了强基线模型，展示了该数据集作为未来孟加拉语NLP研究基准的潜力

Conclusion: 该数据集为构建鲁棒的摘要系统提供了坚实基础，有助于扩展低资源语言的NLP资源，推动孟加拉语自然语言处理研究发展

Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset](https://arxiv.org/abs/2512.13696)
*Md Shahabub Alam,Md Asifuzzaman Jishan,Ayan Kumar Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种物理引导的深度神经网络方法，用于热泵应力分类，在When2Heat数据集上实现了78.1%的测试准确率，比基线方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 热泵系统是现代节能建筑的关键组件，但由于复杂的热力学相互作用和有限的真实世界数据，其运行应力检测仍然具有挑战性。

Method: 提出物理引导的深度神经网络方法，结合物理引导的特征选择和类别定义，采用5个隐藏层的深度神经网络架构，配备双重正则化策略。

Result: 模型在测试集上达到78.1%准确率，验证集78.5%准确率，比浅层网络提升5.0%，比有限特征集提升4.0%，比单一正则化策略提升2.0%。

Conclusion: 该方法为热泵应力检测提供了生产就绪的解决方案，通过全面的消融研究验证了物理引导特征选择、可变阈值和跨国能源模式分析的有效性。

Abstract: Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.

</details>


### [34] [Scaling and Transferability of Annealing Strategies in Large Language Model Training](https://arxiv.org/abs/2512.13705)
*Siqi Wang,Zhengyu Chen,Teng Xiao,Zheqi Lv,Jinluan Yang,Xunliang Cai,Jingang Wang,Xiaomeng Li*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型训练中退火策略的可迁移性，提出了改进的Warmup-Steady-Decay调度器预测框架，能够跨不同模型配置优化学习率调度策略。


<details>
  <summary>Details</summary>
Motivation: 学习率调度对训练大型语言模型至关重要，但不同模型配置下的最优退火策略难以确定，需要进行大量超参数搜索，这既耗时又资源密集。

Method: 改进Warmup-Steady-Decay调度器的预测框架，纳入训练步数、最大学习率和退火行为等参数，通过较小模型作为代理来优化较大模型的训练动态。

Result: 验证了最优退火比率在不同训练配置中遵循一致模式且可迁移，为密集模型和混合专家模型提供了实用的学习率调度指导，无需大量超参数搜索。

Conclusion: 较小模型可作为优化较大模型训练动态的可靠代理，提出的框架为选择最优退火策略提供了实用指导，显著减少了超参数搜索成本。

Abstract: Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.

</details>


### [35] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 研究微调大语言模型时出现的灾难性遗忘问题，提出混合训练策略完全消除遗忘，同时保持专业任务性能


<details>
  <summary>Details</summary>
Motivation: 当微调大语言模型进行数学推理等专业任务时，模型会出现灾难性遗忘，丧失先前学到的能力。研究者希望通过混合训练策略解决这一问题。

Method: 使用Flan-T5-Base模型（2.5亿参数），在DeepMind Mathematics数据集上进行微调，测量在MultiNLI上的遗忘情况。提出混合训练策略，在训练中交错使用数学和NLI示例，系统探索从1:1到15:1的混合比例。

Result: 纯数学训练将数学准确率从3.1%提升到12.0%，但导致NLI准确率从81.0%崩溃到16.5%（下降64.5个百分点）。混合训练完全消除灾难性遗忘，平衡的1:1比例实现12.0%数学准确率（与纯数学训练相当），同时保持86.2%的NLI准确率。即使最小NLI暴露（6.2%）也能提供有效正则化。

Conclusion: 专业化不一定需要遗忘通用能力，混合训练策略能完全消除灾难性遗忘，同时保持专业任务性能。这对扩展到更大模型具有重要意义，混合训练可能带来超越遗忘预防的额外好处。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [36] [Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States](https://arxiv.org/abs/2512.13708)
*Kaiming Luo*

Main category: cs.LG

TL;DR: VPIA方法通过变分物理信息表示，仅从稳态数据中推断复杂系统的非线性、异质和高阶耦合结构


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理非线性、异质和高阶耦合的系统重构，特别是在只能观测到稳态数据的情况下

Method: 提出变分物理信息表示(VPIA)，将稳态约束嵌入可微分变分表示中，通过最小化物理导出的稳态残差来重构耦合结构，无需时间轨迹、导数估计或监督

Result: VPIA能准确恢复有向、加权和多体结构，即使在强噪声下也能有效工作，为仅能获得快照观测的复杂网络推断提供了统一框架

Conclusion: VPIA为仅能从稳态数据推断复杂交互网络提供了鲁棒且可扩展的物理约束推断框架

Abstract: The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) that infers general interaction operators directly from heterogeneous steady-state data. VPIA embeds the steady-state constraints of the dynamics into a differentiable variational representation and reconstructs the underlying couplings by minimizing a physics-derived steady-state residual, without requiring temporal trajectories, derivative estimation, or supervision. Residual sampling combined with natural-gradient optimization enables scalable learning of large and higher-order networks. Across diverse nonlinear systems, VPIA accurately recovers directed, weighted, and multi-body structures under substantial noise, providing a unified and robust framework for physics-constrained inference of complex interaction networks in settings where only snapshot observations are available.

</details>


### [37] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 该研究结合SAR遥感数据与多源环境因子，采用机器学习方法对肯尼亚Nyando河流域进行洪水易发性建模，发现随机森林模型性能最佳，识别出维多利亚湖附近的Kano平原为最高风险区。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球最具破坏性的自然灾害之一，对生态系统、基础设施和人类生计构成严重威胁。在数据有限的地区，需要开发有效的洪水易发性评估方法，以支持灾害风险管理和土地利用规划。

Method: 研究结合Sentinel-1双极化SAR影像（2024年5月洪水事件）生成洪水清单作为训练数据，整合六个环境因子（坡度、高程、坡向、土地利用/覆盖、土壤类型、距河流距离），使用四种监督分类器（逻辑回归、分类回归树、支持向量机、随机森林）进行建模，并通过精度、Kappa系数和ROC分析评估模型性能。

Result: 随机森林模型表现最佳（精度=0.762，Kappa=0.480），优于其他三种模型。基于RF的易发性地图显示，维多利亚湖附近的低洼Kano平原洪水易发性最高，这与历史洪水记录和2024年5月洪水事件的影响一致。

Conclusion: 研究表明SAR数据与集成机器学习方法结合在数据有限地区进行洪水易发性制图具有重要价值，生成的易发性地图可为灾害风险减少、土地利用规划和早期预警系统开发提供重要参考。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [38] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 本文提出Hessian Reassignment方法，用于文档分类模型的类别级遗忘，通过两步法高效移除目标类别训练数据的影响，同时保持其他类别性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习遗忘旨在高效移除特定训练数据对模型的影响，避免完全重新训练。虽然LLM的遗忘研究已有进展，但文档分类模型的遗忘相对较少研究，特别是类别级遗忘。

Method: 提出Hessian Reassignment两阶段方法：1) 通过求解Hessian-向量系统进行单次影响式更新，减去目标类别所有训练点的贡献；2) 采用Top-1分类强制执行决策空间保证，而非随机重新分类删除类别样本。

Result: 在标准文本基准测试中，Hessian Reassignment在保持接近完整重新训练（不含目标类别）性能的同时，运行速度快几个数量级。同时显著降低移除类别的成员推理优势。

Conclusion: 该方法为文档分类中的高效类别遗忘提供了一条实用且原则性的路径，平衡了遗忘效果、性能保持和计算效率。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [39] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: 开发机器学习框架，整合废水监测、气象和空气质量数据，预测美国RSV相关住院率，并发现特定人群和地区的风险差异。


<details>
  <summary>Details</summary>
Motivation: 呼吸道合胞病毒（RSV）是导致幼儿住院的主要原因，其暴发受环境条件强烈影响。需要整合多种数据源来预测RSV相关住院率，以支持及时的公共卫生干预。

Method: 结合每周住院率、废水RSV水平、每日气象测量和空气污染物浓度数据，训练CART、随机森林和Boosting等分类模型，预测RSV相关住院率（低风险、警报、流行三个等级）。

Result: 废水RSV水平是最强预测因子，其次是温度、臭氧水平和比湿等气象和空气质量变量。研究发现美国原住民和阿拉斯加原住民的RSV相关住院率显著更高，高海拔地区（地表压力较低）的住院率也持续较高。

Conclusion: 结合环境和社区监测数据对预测RSV暴发具有重要价值，可支持更及时的公共卫生干预和资源分配。研究开发了交互式R Shiny仪表板，供用户探索各州RSV风险水平、可视化关键预测因子影响并生成预测。

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [40] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 提出两阶段联邦少样本学习框架，用于个性化EEG癫痫检测，解决临床数据稀缺、分布不均和隐私限制问题。


<details>
  <summary>Details</summary>
Motivation: 临床实践中EEG数据稀缺、分布在不同机构且受隐私法规限制，无法集中使用，使得AI癫痫检测模型难以在实际医疗环境中应用。

Method: 两阶段框架：第一阶段通过联邦学习在模拟医院站点上微调预训练的BIOT模型；第二阶段使用联邦少样本个性化，仅用5个标记EEG片段为每个患者适配分类器。

Result: 联邦微调获得平衡准确率0.43（集中式0.52）；FFSL阶段客户端特定模型平均平衡准确率达0.77，Cohen's kappa 0.62，加权F1 0.73，在异构事件分布下表现良好。

Conclusion: FFSL框架能在现实数据可用性和隐私约束下支持有效的患者自适应癫痫检测，为临床AI应用提供可行方案。

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [41] [Dropout Neural Network Training Viewed from a Percolation Perspective](https://arxiv.org/abs/2512.13853)
*Finley Devlin,Jaron Sanders*

Main category: cs.LG

TL;DR: 研究深度神经网络中dropout训练时的渗流现象及其影响，发现dropout可能导致网络输入输出路径中断，影响训练效果


<details>
  <summary>Details</summary>
Motivation: 探究dropout正则化方法中随机移除连接的过程是否会产生类似统计物理中的渗流现象，以及这种渗流效应对神经网络训练的影响

Method: 建立模拟dropout过程的渗流模型，分析网络拓扑结构与路径问题的关系，研究无偏置神经网络的训练崩溃现象，并启发式地扩展到有偏置网络

Result: 理论证明dropout中存在渗流效应，这种效应会导致无偏置神经网络训练时出现崩溃，并启发式地论证这种崩溃也适用于有偏置网络

Conclusion: dropout训练中存在渗流现象，当移除足够多的连接时可能导致输入输出路径中断，影响神经网络的学习能力，这一发现对理解dropout机制有重要意义

Abstract: In this work, we investigate the existence and effect of percolation in training deep Neural Networks (NNs) with dropout. Dropout methods are regularisation techniques for training NNs, first introduced by G. Hinton et al. (2012). These methods temporarily remove connections in the NN, randomly at each stage of training, and update the remaining subnetwork with Stochastic Gradient Descent (SGD). The process of removing connections from a network at random is similar to percolation, a paradigm model of statistical physics.
  If dropout were to remove enough connections such that there is no path between the input and output of the NN, then the NN could not make predictions informed by the data. We study new percolation models that mimic dropout in NNs and characterise the relationship between network topology and this path problem. The theory shows the existence of a percolative effect in dropout. We also show that this percolative effect can cause a breakdown when training NNs without biases with dropout; and we argue heuristically that this breakdown extends to NNs with biases.

</details>


### [42] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 该论文提出将时间受限的推荐列表问题建模为MDP，使用强化学习在用户时间预算约束下平衡项目相关性和评估成本，实验表明强化学习方法在严格时间预算下优于传统上下文赌博机方法。


<details>
  <summary>Details</summary>
Motivation: 传统推荐任务忽略了用户有限的时间预算这一关键资源约束。在移动购物等场景中，用户评估每个推荐项目都需要时间成本，高相关性但高评估成本的项目可能无法适应用户的时间预算，从而影响用户参与度。

Method: 1) 将时间受限的推荐列表问题统一建模为具有预算感知效用的马尔可夫决策过程(MDP)；2) 开发模拟框架研究在重排序数据上的策略行为；3) 使用强化学习算法同时学习用户偏好和时间预算模式；4) 在阿里巴巴个性化重排序数据集上进行实验。

Result: 实验表明，在严格的时间预算约束下，基于策略和离策略的强化学习控制方法能够比传统的上下文赌博机方法获得更好的性能，提高用户参与潜力。

Conclusion: 强化学习能够有效解决时间受限的推荐列表优化问题，通过同时学习用户偏好和时间预算模式，在资源约束下生成具有更高参与潜力的推荐。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [43] [Understanding the Gain from Data Filtering in Multimodal Contrastive Learning](https://arxiv.org/abs/2512.14230)
*Divyansh Pareek,Sewoong Oh,Simon S. Du*

Main category: cs.LG

TL;DR: 本文通过理论分析证明，在多模态表示学习中，使用预训练模型进行数据过滤（教师过滤）能显著提升对比学习性能，特别是在数据质量较低时效果更明显。


<details>
  <summary>Details</summary>
Motivation: 由于互联网规模的多模态数据集中存在大量低质量数据，数据筛选成为训练流程中的关键步骤。教师过滤（使用预训练模型计算质量分数）在实践中取得了成功，但缺乏理论解释其有效性。

Method: 采用标准的双模态数据生成模型，在线性对比学习框架下进行理论分析。通过数学推导证明过滤前后的性能差异，其中η表示正确匹配模态的数据比例。

Result: 理论分析表明：1）无过滤时误差上下界为1/(η√n)；2）使用教师过滤后，在大η区域误差上界为1/√(ηn)，在小η区域误差上界为1/√n。这证明了过滤能显著降低误差。

Conclusion: 教师过滤在多模态表示学习中具有理论保证，能有效提升对比学习性能，特别是在数据质量较低（η较小）时效果更为显著，为实践中的过滤方法提供了理论依据。

Abstract: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{η\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{ηn}}$ in the large $η$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $η$ regime.

</details>


### [44] [RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing](https://arxiv.org/abs/2512.13727)
*Yuhan Tang,Kangxin Cui,Jung Ho Park,Yibo Zhao,Xuan Jiang,Haoze He,Dingyi Zhuang,Shenhao Wang,Jiangbo Yu,Haris Koutsopoulos,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出RAST-MoE框架，使用混合专家模型增强强化学习，优化网约车平台的延迟匹配策略，在真实Uber数据上显著减少匹配和接驾延迟。


<details>
  <summary>Details</summary>
Motivation: 网约车平台需要在高度不确定的供需条件下平衡乘客等待时间和系统效率。现有方法往往过度简化交通动态或使用浅层编码器，无法捕捉复杂的时空模式。

Method: 提出Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE)框架，将自适应延迟匹配形式化为制度感知MDP，配备自注意力混合专家编码器。使用物理信息拥堵代理实现高效模拟，自适应奖励方案防止病态策略。

Result: 在真实Uber轨迹数据（旧金山）上，仅用1200万参数，总奖励提升超过13%，平均匹配延迟减少10%，接驾延迟减少15%。在不同需求制度下表现稳健，训练稳定。

Conclusion: RAST-MoE框架展示了混合专家增强的强化学习在处理复杂时空动态的大规模决策问题中的潜力，为网约车平台延迟匹配提供了有效解决方案。

Abstract: Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch requests. Since outcomes accumulate over long horizons with stochastic dynamics, reinforcement learning (RL) is a suitable framework. However, existing approaches often oversimplify traffic dynamics or use shallow encoders that miss complex spatiotemporal patterns.
  We introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP equipped with a self-attention MoE encoder. Unlike monolithic networks, our experts specialize automatically, improving representation capacity while maintaining computational efficiency. A physics-informed congestion surrogate preserves realistic density-speed feedback, enabling millions of efficient rollouts, while an adaptive reward scheme guards against pathological strategies.
  With only 12M parameters, our framework outperforms strong baselines. On real-world Uber trajectory data (San Francisco), it improves total reward by over 13%, reducing average matching and pickup delays by 10% and 15% respectively. It demonstrates robustness across unseen demand regimes and stable training. These findings highlight the potential of MoE-enhanced RL for large-scale decision-making with complex spatiotemporal dynamics.

</details>


### [45] [CurvaDion: Curvature-Adaptive Distributed Orthonormalization](https://arxiv.org/abs/2512.13728)
*Bhavesh Kumar,Roger Jin,Jeffrey Quesnelle*

Main category: cs.LG

TL;DR: CurvaDion通过RMMC检测高曲率区域，只在需要时同步梯度，减少99%通信，同时保持收敛性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型分布式训练中，梯度同步是主要瓶颈。现有方法如Dion虽然减少了通信量，但每步都同步，忽略了优化过程中不同区域对同步需求的差异：平坦区域梯度相似，频繁同步冗余；高曲率区域需要协调防止发散。

Method: 提出CurvaDion方法，使用相对最大动量变化(RMMC)检测需要同步的高曲率区域。RMMC利用优化过程中已计算的动量动态作为方向曲率的计算可行代理，每层仅增加O(d)操作。

Result: 建立了RMMC与损失曲率的理论联系，在160M到1.3B参数的模型上，CurvaDion实现了99%的通信减少，同时匹配基线收敛性能。

Conclusion: CurvaDion通过智能检测优化景观中的高曲率区域，只在必要时同步，显著减少分布式训练通信开销，同时保持模型收敛质量。

Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.

</details>


### [46] [Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean](https://arxiv.org/abs/2512.14686)
*Chuan He*

Main category: cs.LG

TL;DR: 该论文研究了在重尾噪声（尾指数α∈(0,2]）下随机一阶方法的优化复杂度，通过梯度裁剪技术实现了改进的复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现有随机优化方法主要研究轻尾噪声，对重尾噪声的研究多限于α∈(1,2]范围（有限均值），当α接近1时复杂度趋于无穷。实际应用中常出现更重的尾噪声（包括无限均值情况），需要更全面的理论分析。

Method: 通过分析梯度裁剪中的偏差-方差权衡，提出新颖的分析框架。在控制噪声尾对称性的条件下，证明裁剪后的随机一阶方法能在全范围尾指数α∈(0,2]下获得改进的复杂度保证。

Result: 建立了裁剪随机一阶方法在重尾噪声下的统一复杂度保证，覆盖从有界方差到无限均值的所有情况。数值实验验证了理论发现。

Conclusion: 梯度裁剪技术能有效处理重尾噪声优化问题，提出的偏差-方差权衡分析为全范围尾指数提供了统一的复杂度理论框架，填补了无限均值噪声情况的研究空白。

Abstract: Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $α$ of the noise. Nonetheless, existing complexity results often cover only the case $α\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $α$ approaches $1$. This paper tackles the general case of noise with tail index $α\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $α\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.

</details>


### [47] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 提出CCFG方法改进扩散模型的多条件输入，应用于风数据超分辨率任务，开发WindDM模型实现高质量低成本风场重建


<details>
  <summary>Details</summary>
Motivation: 高分辨率风数据获取成本高且困难，传统方法无法同时兼顾成本效益和准确性，现有深度学习方法在处理多通道风数据时存在局限

Method: 提出复合无分类器引导(CCFG)方法，扩展标准CFG以处理多个条件输入，开发WindDM扩散模型用于工业规模风动力学重建

Result: CCFG输出比标准CFG具有更高保真度，WindDM在深度学习模型中达到最先进的重建质量，成本比传统方法降低高达1000倍

Conclusion: CCFG方法有效解决了风数据超分辨率中多条件输入的挑战，WindDM为高质量低成本风场重建提供了实用解决方案

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [48] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: PIS：基于集合条件扩散的物理反演框架，可在任意稀疏观测下稳定求解PDE约束参数反问题，显著优于现有算子学习方法


<details>
  <summary>Details</summary>
Motivation: PDE约束物理参数反演在稀疏、不规则观测下本质上是病态的，现有深度学习和算子学习方法在极端稀疏条件下失效，缺乏鲁棒性和不确定性量化能力

Method: 提出物理反演求解器(PIS)：基于集合条件扩散框架，使用Set Transformer编码器处理任意几何观测，采用余弦退火稀疏课程实现鲁棒性，并附带信息论分析

Result: 在Darcy流、Helmholtz波场反演和结构健康监测三个PDE反问题上，PIS在极端稀疏观测(0.29%观测率)下保持稳定准确，反演误差降低12.28%-88.73%，能生成校准的后验样本

Conclusion: PIS是强大、通用且具有独特稀疏鲁棒性的物理反演解决方案，能在任意严重欠采样观测下可靠工作，为实际传感器部署受限场景提供有效工具

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [49] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: LLRC是一种无需微调的低秩压缩方法，通过学习掩码权重选择奇异值，在保持激活相似性的同时优化压缩率和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩压缩方法在层间秩选择上存在局限：启发式方法搜索空间有限导致次优结果，基于梯度的方法在无微调时性能不如启发式方法。需要一种无需后压缩微调就能联合优化压缩率和准确率的方法。

Method: 提出LLRC（Learning to Low-Rank Compress），基于梯度的方法直接学习选择奇异值的掩码权重。使用校准数据集仅训练掩码权重，在减少奇异值数量的同时最小化中间激活与原始模型的差异。

Result: 在常识推理和开放域问答任务上，LLRC在无后压缩微调的方法中表现最佳。在Llama-2-13B 20%压缩率下，相比STRS在MMLU、BoolQ和OpenbookQA分别提升12%、3.5%和4.4%。与SVD-LLM和LLM-Pruner的无微调变体相比表现更优，且与LLM-Pruner的微调变体性能相当。

Conclusion: LLRC提供了一种有效的无微调低秩压缩方法，通过学习掩码权重优化秩选择，在保持性能的同时实现高效压缩，为大型语言模型压缩提供了新思路。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [50] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: gridfm-datakit-v1是一个Python库，用于生成真实多样的电力潮流和最优潮流数据集，解决现有数据集在随机性、操作限制和成本函数方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有电力系统数据集面临三个主要挑战：1）缺乏真实的随机负荷和拓扑扰动，限制了场景多样性；2）电力潮流数据集仅限于最优潮流可行点，阻碍了机器学习求解器对违反操作限制情况的泛化；3）最优潮流数据集使用固定的发电机成本函数，限制了在不同成本情况下的泛化能力。

Method: 通过结合真实世界负荷剖面的全局缩放与局部噪声，支持任意N-k拓扑扰动来创建多样且真实的数据集；生成超出操作限制的电力潮流样本；产生具有变化发电机成本的最优潮流数据；并能高效扩展到大型电网（最多10,000个节点）。

Result: 开发了gridfm-datakit-v1库，解决了现有数据集的局限性，提供了与OPFData、OPF-Learn、PGLearn和PFΔ的比较，并在GitHub上开源，可通过pip安装。

Conclusion: gridfm-datakit-v1为训练机器学习电力系统求解器提供了更全面、真实和多样的数据集，解决了现有工具的关键限制，有助于提升机器学习模型在实际电力系统应用中的泛化能力。

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [51] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出基于参数高效微调（PEFT）的联邦推荐框架，通过减少嵌入参数传输量来提升通信效率，同时保持或提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着云边协同的发展，推荐服务在分布式环境中训练。联邦推荐通过共享模型参数而非原始数据来保护隐私，但海量项目嵌入参数导致通信效率低下。现有研究主要关注提升联邦推荐模型效率，却忽视了嵌入参数开销问题。

Method: 提出基于参数高效微调（PEFT）的联邦推荐训练框架，采用轻量级插件式设计，可无缝集成到现有联邦推荐方法中。除了整合常见的PEFT技术（如LoRA和基于哈希的编码），还探索使用残差量化变分自编码器（RQ-VAE）作为新的PEFT策略。

Result: 在多种联邦推荐模型骨干和数据集上的广泛实验表明，该框架显著减少了通信开销，同时提高了推荐准确性。

Conclusion: 提出的基于PEFT的联邦推荐框架有效解决了嵌入参数传输效率问题，为联邦推荐系统提供了通信高效且性能优越的解决方案。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [52] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 该论文提出了一种从数据中学习反应-扩散系统的方法，确保学习模型的物理一致性和适定性，通过修改参数化反应项来保证质量守恒和拟正性。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习反应-扩散系统时，需要确保学习到的模型符合物理原理（如质量守恒、非负性），同时保证模型的适定性，这对于开发可解释且可靠的数据驱动模型至关重要。

Method: 基于正则化框架，提出系统修改参数化反应项的技术，使其固有地满足质量守恒和拟正性；扩展现有理论结果，证明在学习过程中强制这些物理约束时，学习问题的解会收敛到极限系统的唯一正则化最小化解。

Result: 开发了确保物理一致性的反应项修改技术；证明了在强制守恒定律和拟正性条件下学习问题的收敛性；提供了拟正函数的逼近结果，为构建物理一致的参数化提供了理论基础。

Conclusion: 该方法推动了反应-扩散系统可解释且可靠的数据驱动模型发展，确保模型与基本物理定律保持一致，为物理约束下的机器学习提供了理论保证。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [53] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: DARTs是一个用于高维多变量时间序列异常检测的鲁棒性长短期双路径框架，通过窗口感知的时空软融合机制有效处理噪声并捕获长距离时空依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低维场景下能识别明显异常模式，但在处理高维噪声时间序列时难以鲁棒地捕获长距离时空依赖关系，这限制了在工业控制系统中的实际应用效果。

Method: 提出DARTs框架，包含三个互补组件：1) 短期路径：多视图稀疏图学习器和扩散多关系图单元协作捕获高噪声下的分层判别性短期时空模式；2) 长期路径：多尺度时空图构造器建模高维表示空间中的显著长期动态；3) 窗口感知时空软融合机制过滤残留噪声并整合异常模式。

Result: 在主流数据集上的大量定性和定量实验结果表明，DARTs在异常检测性能上具有优越性和鲁棒性。消融研究验证了各组件设计的关键作用。

Conclusion: DARTs通过创新的双路径架构和软融合机制，有效解决了高维噪声时间序列中长距离时空依赖关系的建模问题，为工业控制系统中的多变量时间序列异常检测提供了鲁棒的解决方案。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [54] [Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization](https://arxiv.org/abs/2512.14263)
*Nick Leenders,Thomas Quadt,Boris Cule,Roy Lindelauf,Herman Monsuur,Joost van Oijen,Mark Voskuijl*

Main category: cs.LG

TL;DR: 提出基于决策树的解释性贝叶斯优化方法，替代传统高斯过程，能处理分类和连续数据，在尖峰函数上表现更优


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯过程的贝叶斯优化方法存在解释性差、难以处理分类数据、计算复杂度高等问题，限制了实际应用

Method: 引入基于决策树的解释性代理模型，能够处理分类和连续数据，适用于大规模数据集

Result: 在八个尖峰优化函数上，新模型在尖峰函数上优于GP方法，在非尖峰函数上性能略低；在Sushi数据集上能有效学习个人偏好；初步展示了利用历史偏好数据加速新用户优化的潜力

Conclusion: 决策树代理模型为贝叶斯优化提供了更解释性、更灵活且可扩展的替代方案，特别适用于具有分类特征和尖峰特性的实际优化问题

Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.

</details>


### [55] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 提出TF-MCL模型，通过时间-频率融合和多域交叉损失改进抑郁症EEG检测，在公开数据集上显著超越现有最佳方法


<details>
  <summary>Details</summary>
Motivation: 当前基于EEG的抑郁症检测方法依赖大量标注数据，而标注困难；现有对比学习方法未能充分利用EEG信号的时频特性，在低语义数据表示方面不足

Method: 提出TF-MCL模型，包含融合映射头(FMH)生成时频混合表示，通过多域交叉损失函数优化时频域和融合域的表示分布

Result: 在MODMA和PRED+CT公开数据集上，准确率分别比现有SOTA方法提升5.87%和9.96%

Conclusion: TF-MCL模型能有效增强时频信息融合能力，改善低语义数据表示，为抑郁症EEG检测提供了有效的自监督学习解决方案

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [56] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 论文提出"层流假说"，认为良性输入在LLM潜在空间中产生平滑过渡，而对抗性提示引发"语义湍流"，通过层间余弦速度方差这一零样本指标可检测越狱攻击并分析模型安全架构。


<details>
  <summary>Details</summary>
Motivation: 当前LLM防御越狱攻击的方法依赖计算昂贵的外部分类器或脆弱的词汇过滤器，忽略了模型推理过程的内在动态。需要一种轻量级、实时且能分析模型内部安全架构的检测方法。

Method: 提出层流假说，认为良性输入在潜在空间中产生平滑过渡，对抗性提示引发语义湍流。引入零样本指标：层间余弦速度方差，用于量化模型内部表示的变化轨迹。

Result: 实验显示，RLHF对齐的Qwen2-1.5B在攻击下湍流增加75.4%，验证了内部冲突假说；Gemma-2B则显示22.0%的湍流减少，表现出不同的"基于反射"的拒绝机制。语义湍流可作为轻量级越狱检测器和黑盒模型安全架构诊断工具。

Conclusion: 语义湍流不仅可作为实时越狱攻击检测器，还能作为非侵入性诊断工具，用于分类黑盒模型的安全架构类型，为LLM安全防御提供了新的理论框架和实用方法。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [57] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 该研究比较了在数据稀缺环境下金融新闻情感分类的嵌入方法，发现预训练嵌入在数据不足时效果有限，验证集过小会导致过拟合，建议采用少样本学习等替代方法。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析对市场理解很重要，但标准NLP方法在小数据集上遇到显著挑战。研究旨在评估在资源受限环境中金融新闻情感分类的嵌入方法效果。

Method: 比较评估了Word2Vec、GloVe和句子转换器表示方法，结合梯度提升在手动标注的新闻标题上进行实验。通过验证集和测试集性能对比分析模型表现。

Result: 实验结果显示验证集和测试集性能存在显著差距，模型表现甚至不如简单基线。预训练嵌入在数据不足时收益递减，小验证集导致模型选择时过拟合。

Conclusion: 嵌入质量本身无法解决情感分类中的数据稀缺问题。建议资源有限的实践者考虑少样本学习、数据增强或词典增强混合方法等替代方案。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [58] [MIDUS: Memory-Infused Depth Up-Scaling](https://arxiv.org/abs/2512.13751)
*Taero Kim,Hoyoon Byun,Youngjun Choi,Sungrae Park,Kyungwoo Song*

Main category: cs.LG

TL;DR: MIDUS提出了一种新的深度扩展方法，用头级记忆层替代传统的前馈网络，通过为每个注意力头分配独立记忆库，在保持高效参数的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度扩展方法通过复制层并使用前馈网络，存在效率限制和增益有限的问题。需要一种既能增加模型容量又不显著增加参数和推理成本的方法。

Method: MIDUS用头级记忆层替换复制块中的前馈网络，为每个注意力头分配独立记忆库，支持头级检索并将信息注入后续层，同时保持头级功能结构。包含高效的每头值分解模块。

Result: 在持续预训练实验中，MIDUS相比传统深度扩展基线表现出稳健的性能提升，同时保持高效的参数占用，缓解了效率与性能的权衡。

Conclusion: MIDUS通过其头级记忆设计，成为传统前馈网络复制进行深度扩展的有力且资源高效的替代方案。

Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.

</details>


### [59] [Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention](https://arxiv.org/abs/2512.13758)
*Léo Hein,Giovanni de Nunzio,Giovanni Chierchia,Aurélie Pirayre,Laurent Najman*

Main category: cs.LG

TL;DR: HDA-STGNN：一种利用速度数据和道路属性进行全网交通量估计的图神经网络方法，无需依赖实时交通量数据


<details>
  <summary>Details</summary>
Motivation: 现有交通量估计方法存在局限性：预测模型忽略无监测道路，空间插值方法依赖实时交通量数据，而传感器稀缺城市难以获得这些数据。相比之下，车辆速度数据和静态道路属性更易获取且能覆盖所有路段。

Method: 提出混合定向注意力时空图神经网络（HDA-STGNN），这是一个归纳深度学习框架，利用速度剖面、静态道路属性和道路网络拓扑结构来预测网络中所有路段的日交通量剖面。

Result: 通过广泛的消融研究证明模型能够捕捉复杂的时空依赖关系，并突显拓扑信息对于在不依赖推理时交通量数据的情况下进行准确全网交通量估计的重要性。

Conclusion: HDA-STGNN为解决全网交通量估计问题提供了一种有效方法，特别适用于传感器稀缺的城市环境，通过利用更易获取的速度数据和道路属性实现了对全网络路段的交通量预测。

Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.

</details>


### [60] [Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training](https://arxiv.org/abs/2512.13770)
*Huaiyuan Xiao,Fadi Dornaika,Jingjun Bi*

Main category: cs.LG

TL;DR: MV-SupGCN：一种半监督图卷积网络模型，通过联合损失函数、多图构建方法和对比学习框架，有效整合多视图互补信息，提升特征表示性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的多视图学习方法未能充分利用不同视图间的互补信息，导致特征表示不优且性能受限。需要设计更有效的框架来整合多视图结构信息。

Method: 1) 设计联合损失函数，结合交叉熵损失和监督对比损失；2) 结合KNN和半监督图构建方法增强结构表示鲁棒性；3) 提出统一框架整合对比学习和伪标签技术，增强多视图语义对齐。

Result: 在多个基准测试中，MV-SupGCN始终超越最先进方法，验证了集成方法的有效性。

Conclusion: MV-SupGCN通过整合互补组件，有效提升了多视图学习的性能，为复杂多视图数据的建模提供了强大框架。

Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

</details>


### [61] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: 提出SCPO方法，通过采样和权重空间投影实现安全约束策略学习，无需约束函数梯度，保证训练过程始终安全


<details>
  <summary>Details</summary>
Motivation: 安全关键学习需要策略在不离开安全操作区域的情况下提升性能。研究基于未知、rollout安全约束的策略学习问题，需要直接确保参数空间的安全性

Method: 提出SCPO方法：1) 结合轨迹rollout和平滑性边界构建局部安全区域；2) 通过凸SOCP进行权重空间投影；3) 实现安全一阶更新，无需约束函数梯度

Result: 在有害监督回归和带恶意专家的约束双积分器任务中，SCPO能持续拒绝不安全更新，保持可行性，并实现有意义的原始目标改进

Conclusion: SCPO提供安全归纳保证：从任何安全初始化开始，所有中间策略在可行投影下保持安全。在约束控制设置中，结合稳定备份策略，确保闭环稳定性并实现超越保守备份的安全适应

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [62] [EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models](https://arxiv.org/abs/2512.13806)
*Siegfried Ludwig,Stylianos Bakas,Konstantinos Barmpas,Georgios Zoumpourlis,Dimitrios A. Adamos,Nikolaos Laskaris,Yannis Panagakis,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 提出D3方法，通过弱监督训练分离EEG信号中的潜在脑活动成分，防止隐藏过拟合，提高模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在EEG解码中虽然基准性能好，但实际应用泛化能力差，存在隐藏过拟合问题，需要分离真实脑活动成分与伪迹

Method: 提出解耦解码分解(D3)方法，通过预测输入窗口在试验序列中的位置来分离潜在脑活动成分，使用完全独立的子网络架构确保可解释性

Result: D3能可靠分离运动想象数据中的脑活动成分，防止任务相关伪迹导致的隐藏过拟合，在睡眠分期中实现有效的少样本学习

Conclusion: D3方法能区分真实脑活动成分与虚假特征，避免隐藏过拟合，提高模型泛化能力，为神经科学研究提供分离个体脑过程的新工具

Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.

</details>


### [63] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出CTVP框架，通过语义轨道分析验证不可信代码生成模型，检测后门注入，引入ARQ量化验证成本，理论分析证明不可博弈性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地生成代码且人类监督减少，后门注入和恶意行为成为关键问题。需要一种可扩展的理论基础方法来控制AI代码生成。

Method: 提出跨轨迹验证协议(CTVP)，通过语义轨道分析验证不可信代码生成模型。不直接执行可能恶意的代码，而是利用模型对语义等价程序变换的执行轨迹预测，分析一致性模式来检测后门行为异常。

Result: 引入对抗鲁棒性商数(ARQ)量化验证成本相对于基线生成的比率，显示随着轨道大小呈指数增长。理论分析建立了信息论界限，证明不可博弈性——由于基本空间复杂性约束，对手无法通过训练改进。

Conclusion: 语义轨道分析为代码生成任务提供了可扩展、理论基础扎实的AI控制方法，能够有效检测后门行为，且具有理论上的不可博弈性保证。

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [64] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 提出一种通过识别并修正导致不满意响应的训练数据来改进RLHF的方法，包括事后解释和反学习两个部分


<details>
  <summary>Details</summary>
Motivation: 人类改进不满意结果的常见策略是找到原因并修正，本文研究是否可以将这种策略应用于改进语言模型的RLHF对齐，因为RLHF调优后的模型仍可能产生不满意响应

Method: 方法分为两部分：1）事后解释方法，将问题建模为约束组合优化问题，寻找与提示-响应对最接近的训练数据集，并提出高效迭代数据选择算法；2）反学习方法，通过反学习导致不满意响应的训练数据来改进响应，同时不显著降低其他提示的满意响应

Result: 实验结果表明，该算法能够有效改进RLHF

Conclusion: 通过识别并修正导致不满意响应的训练数据，可以成功应用人类改进策略来提升RLHF对齐效果

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [65] [Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains](https://arxiv.org/abs/2512.13852)
*Jelena Losic*

Main category: cs.LG

TL;DR: 提出结合持久同调特征与稳定性正则化的图神经网络框架，增强对结构扰动的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 图神经网络已成为图表示学习的标准方法，但对结构扰动仍然脆弱。需要一种理论上有基础且经验上验证的鲁棒图学习方法。

Method: 将持久同调特征与稳定性正则化结合，使用GIN架构和多尺度拓扑特征（从持久性图像提取），并实施Hiraoka-Kusano启发的稳定性约束。

Result: 在六个涵盖生化、社交和协作网络的数据集上，该方法表现出对边扰动的卓越鲁棒性，同时保持有竞争力的准确性。在扰动下性能下降最小（大多数数据集0-4%），显著优于基线稳定性方法。

Conclusion: 这项工作提供了一个理论上有基础且经验上验证的鲁棒图学习方法，与拓扑正则化的最新进展保持一致。

Abstract: Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization

</details>


### [66] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 提出两种估计二元分类器L1校准误差的方法：1) 对有界变差校准函数的分类器提供上界；2) 通过修改分类器使其校准误差可高效上界，且不影响性能


<details>
  <summary>Details</summary>
Motivation: 在有限数据集上准确估计二元分类器的L1校准误差是一个重要但具有挑战性的问题，需要非渐近、分布无关的实用方法

Method: 1) 为具有有界变差校准函数的分类器提供理论上的上界；2) 提出一种修改分类器的方法，使其校准误差可被高效上界，且不显著影响分类性能

Result: 所有结果都是非渐近且分布无关的，提出的方法可在实际数据集上运行，计算开销适中

Conclusion: 提供了实际测量校准误差的建议，提出的方法具有实用性，可用于真实世界数据集

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [67] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 提出一个结合去噪自编码器、卷积分词器和Transformer编码器的婴儿哭声分类系统，采用联邦学习进行隐私保护训练，实现噪声鲁棒、通信高效和实时边缘推理。


<details>
  <summary>Details</summary>
Motivation: 婴儿哭声分类有助于早期评估婴儿需求，但现有方案面临音频数据隐私问题、背景噪声敏感性和跨录音环境的域偏移等部署限制。

Method: 端到端婴儿哭声分析流程：集成去噪自编码器(DAE)、卷积分词器和Transformer编码器，采用通信高效的联邦学习训练。系统执行设备端去噪、自适应分割、事后校准和基于能量的分布外拒绝。联邦训练使用带正则化控制变量更新的8位适配器增量，并采用安全聚合。

Result: 在Baby Chillanto和Donate-a-Cry数据集上，模型获得宏观F1分数0.938、AUC 0.962和预期校准误差(ECE) 0.032，每轮客户端上传从约36-42MB减少到3.3MB。在NVIDIA Jetson Nano上实时边缘推理达到每1秒频谱图帧96毫秒。

Conclusion: 该研究展示了实现隐私保护、噪声鲁棒和通信高效的婴儿哭声分类的实用路径，适合联邦部署。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [68] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: OPTIMA是一种实用的单次训练后剪枝方法，通过将层权重重构转化为可并行求解的行级二次规划问题，在保持准确性的同时实现大规模剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有训练后剪枝方法面临权衡：简单启发式方法速度快但精度下降，而联合优化方法能恢复精度但计算成本过高。需要一种既准确又可扩展的实用剪枝方法。

Method: 将掩码选择后的层权重重构转化为独立的行级二次规划问题，这些QP共享层Hessian矩阵。实现加速器友好的QP求解器，每层累积一个Hessian并并行求解多个小QP，实现单次训练后剪枝。

Result: OPTIMA在多个LLM家族和稀疏度下持续提升零样本性能，最高带来3.97%的绝对精度提升。在NVIDIA H100上，40小时内完成80亿参数transformer的端到端剪枝，峰值内存60GB。

Conclusion: OPTIMA为单次训练后剪枝设定了新的准确性与效率权衡的最先进水平，平衡了精度和可扩展性，无需微调即可在大规模模型上实现高效剪枝。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [69] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 论文指出当前长上下文LLM存在"分数稀释"问题，静态自注意力机制限制了推理时计算策略的效果，提出通过上下文特定梯度更新来替代生成思考token的策略，显著提升长上下文任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文LLM虽然能处理百万token的上下文，但实际使用能力有限。现有的推理时计算策略（如生成思考token）在长上下文任务中效果迅速下降，需要新的方法来有效利用推理计算资源。

Method: 提出一种简单方法：通过对给定上下文进行针对性梯度更新，克服静态自注意力的限制。这种方法将推理计算从生成更多思考token转向上下文特定的训练。

Result: 该方法在多个模型和长上下文基准测试中带来一致的大幅性能提升。在Qwen3-4B模型上，LongBench-v2和ZeroScrolls基准测试子集平均分别提升12.6和14.1个百分点。

Conclusion: 对于长上下文任务，少量上下文特定训练比当前推理时扩展策略（如生成更多思考token）是更好的推理计算使用方式。该方法能有效克服静态自注意力的限制。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [70] [Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America](https://arxiv.org/abs/2512.13910)
*Matheus Corrêa Domingos,Valdivino Alexandre de Santiago Júnior,Juliana Aparecida Anochi,Elcio Hideiti Shiguemori,Luísa Mirelle Costa dos Santos,Hércules Carlos dos Santos Pereira,André Estevam Costa Oliveira*

Main category: cs.LG

TL;DR: 该研究比较了传统机器学习、深度学习与动态模型在南美洲降水预报中的表现，发现LSTM模型在强降水预报中表现最佳，证实了深度学习在气候预报中的可行性。


<details>
  <summary>Details</summary>
Motivation: 降水预报对社会至关重要，但传统动态模型存在局限性。虽然AI技术已被用于气象预报，但缺乏对纯数据驱动方法在降水预报中可行性的广泛研究。本研究旨在填补这一空白。

Method: 研究比较了传统机器学习（随机森林、XGBoost）、深度学习（1D CNN、LSTM、GRU）和传统动态模型（巴西全球大气模型BAM）在南美洲2019年所有季节的降水预报表现，并使用可解释AI分析模型行为。

Result: LSTM模型表现出最强的预测性能，特别是在强降水预报中最为准确；传统动态模型BAM表现最差；XGBoost在延迟和成本方面具有优势，精度损失较小。

Conclusion: 深度学习模型在气候预报中具有可行性，证实了全球主要气象和气候预报中心的趋势。LSTM在精度上表现最佳，而XGBoost在成本敏感场景中是不错的折中选择。

Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.

</details>


### [71] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: 研究使用神经ODE学习量子多体系统的二粒子约化密度矩阵动力学，发现只有在二粒子和三粒子关联度高的区域才能准确预测，揭示了现有TD2RDM方法在强关联区域的局限性。


<details>
  <summary>Details</summary>
Motivation: 量子多体系统非平衡动力学中，精确波函数方法计算复杂度指数增长，而平均场方法忽略重要关联。TD2RDM方法通过传播二粒子约化密度矩阵提供中间方案，但其时间局域重构泛函的有效性在不同动力学区域尚不明确。

Method: 使用神经ODE模型在精确2RDM数据上训练（无维度约简），研究其能否仅基于瞬时二粒子累积量重构动力学。通过分析二粒子和三粒子累积量的皮尔逊相关性来评估模型性能。

Result: 神经ODE仅在二粒子和三粒子累积量相关性高的参数区域能准确再现动力学。在反相关或不相关区域失败，表明没有简单的时间局域泛函能捕捉演化。时间平均的三粒子关联累积幅度是成功的主要预测因子。

Conclusion: 神经ODE可作为模型无关的诊断工具，映射累积量展开方法的适用范围，并指导非局域闭合方案的发展。在强关联区域需要记忆依赖核的三粒子累积量重构。

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [72] [Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics](https://arxiv.org/abs/2512.13919)
*Eugenio Varetti,Matteo Torzoni,Marco Tezzele,Andrea Manzoni*

Main category: cs.LG

TL;DR: 本文提出了一种自适应数字孪生框架，通过概率图模型和贝叶斯更新实现状态转移模型的自适应学习，增强土木工程中数字孪生的价值实现。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生在土木工程中的应用缺乏自适应性，无法有效处理物理和虚拟领域之间的双向交互。需要一种能够在线学习状态转移动态的框架，以提高个性化、鲁棒性和成本效益。

Method: 使用概率图模型（特别是动态贝叶斯网络）表示数字孪生，将状态转移概率视为具有共轭先验的随机变量，实现分层在线学习。通过强化学习求解参数化马尔可夫决策过程，计算具有精确更新的动态策略。

Result: 提出的自适应数字孪生框架在铁路桥梁结构健康监测和维护规划的案例研究中得到验证，显示出增强的个性化、增加的鲁棒性和改进的成本效益。

Conclusion: 自适应数字孪生框架通过贝叶斯更新和强化学习，显著提升了数字孪生在土木工程中的价值实现，为结构健康监测和维护规划提供了更有效的解决方案。

Abstract: This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.

</details>


### [73] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 提出Phalanx层作为窗口注意力或线性循环的替代方案，通过滑动窗口循环在1B参数多混合模型中实现10-40%加速


<details>
  <summary>Details</summary>
Motivation: 多混合架构在语言建模中具有更好的质量和性能，但需要更高效的算法来利用GPU内存层次结构

Method: 引入分层分解框架用于线性循环，开发与GPU内存层次对齐的算法，创建滑动窗口循环，并构建Phalanx层作为窗口注意力或线性循环的即插即用替代方案

Result: 在1B参数多混合模型中，Phalanx在4K到32K上下文长度上比优化的Transformer实现10-40%加速，同时保持困惑度相当

Conclusion: 滑动窗口循环和Phalanx层为多混合语言模型提供了高效的内存层次对齐解决方案，显著提升性能

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [74] [A Complete Guide to Spherical Equivariant Graph Transformers](https://arxiv.org/abs/2512.13927)
*Sophia Tang*

Main category: cs.LG

TL;DR: 该指南为球形等变图神经网络提供了完整的理论基础和实现指导，涵盖从群表示到实际架构构建的全过程。


<details>
  <summary>Details</summary>
Motivation: 三维分子和生物分子系统的预测需要尊重物理中的旋转对称性，球形等变图神经网络为此提供了原则性框架，确保预测在输入旋转时以物理上有意义的方式变化。

Method: 通过将节点和边特征表示为在旋转群SO(3)不可约表示下变换的球形张量，构建Tensor Field Network和SE(3)-Transformer架构，实现等变消息传递和注意力机制。

Result: 提供了从群表示、球谐函数、张量积、Clebsch-Gordan分解到SO(3)-等变核构建的完整理论基础，以及具体架构的实现指导。

Conclusion: 该指南为研究人员和学习者提供了理解和实现球形等变图神经网络的自主入门材料，适用于化学、分子性质预测、蛋白质结构建模和生成建模等应用。

Abstract: Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.

</details>


### [75] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: RePo提出了一种新的上下文重定位机制，通过可微分模块动态分配token位置来捕捉上下文依赖关系，减少无关认知负荷，提升LLM在噪声上下文、结构化数据和长上下文任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM架构采用固定线性或常数位置索引，这种无信息结构增加了无关认知负荷，消耗了本应用于深度推理和注意力分配的有限工作记忆容量。

Method: 提出RePo机制，使用可微分模块f_φ动态分配token位置来捕捉上下文依赖关系，而不是依赖预定义的整数范围。在OLMo-2 1B骨干网络上进行持续预训练。

Result: RePo显著提升了在噪声上下文、结构化数据和长上下文任务上的性能，同时在一般短上下文任务上保持竞争力。分析显示RePo能更好地关注远距离相关信息，在密集非线性空间中分配位置，并捕捉输入上下文的内在结构。

Conclusion: RePo通过减少无关认知负荷，为LLM提供了更灵活的上下文表示机制，在多种任务上展现出优越性能，为改进上下文学习架构提供了新思路。

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [76] [EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training](https://arxiv.org/abs/2511.10333)
*Qingao Yi,Jiaang Duan,Hanwen Hu,Qin Hua,Haiyan Zhao,Shiyou Qian,Dingyu Yang,Jian Cao,Jinghua Tang,Yinghao Yu,Chenzhi Liao,Kangjin Wang,Liping Zhang*

Main category: cs.LG

TL;DR: 提出EDGC框架，基于梯度熵动态调整压缩率，在保持LLM精度的同时显著减少通信延迟和训练时间


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练面临计算资源和内存容量的挑战，分布式训练存在通信开销问题。现有静态梯度压缩方法忽略了训练过程中梯度的动态变化特性，导致性能下降。如何在压缩加速训练的同时不牺牲性能仍是一个挑战。

Method: 提出EDGC（熵驱动动态梯度压缩）框架：1）使用下采样方法高效估计梯度熵，降低计算开销；2）建立压缩率与梯度熵的理论模型，指导压缩决策；3）基于窗口的调整机制动态适应流水线阶段的压缩率，提高通信效率并保持模型性能。

Result: 在32-NVIDIA-V100集群上训练GPT2-2.5B，在64-NVIDIA-H100集群上训练GPT2-12.1B。结果显示EDGC显著减少通信延迟和训练时间，分别达到46.45%和16.13%的降低，同时保持LLM精度。

Conclusion: EDGC通过基于梯度熵动态调整压缩率，有效解决了LLM训练中的通信瓶颈问题，在保持模型性能的同时显著提高了训练效率，为大规模语言模型训练提供了有效的压缩解决方案。

Abstract: Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.

</details>


### [77] [Informing Acquisition Functions via Foundation Models for Molecular Discovery](https://arxiv.org/abs/2512.13935)
*Qi Chen,Fabio Ramos,Alán Aspuru-Guzik,Florian Shkurti*

Main category: cs.LG

TL;DR: 提出一种免似然的贝叶斯优化方法，直接利用LLM和化学基础模型的先验知识，通过树结构空间划分和蒙特卡洛树搜索提高分子发现的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在低数据、大搜索空间下性能受限，而LLM和化学基础模型提供的丰富先验难以被充分利用，因为存在高维特征、上下文学习成本高、深度贝叶斯代理模型计算负担重等问题。

Method: 1) 免似然贝叶斯优化，绕过显式代理建模，直接利用LLM和化学基础模型的先验指导采集函数；2) 学习分子搜索空间的树结构划分，使用局部采集函数；3) 通过蒙特卡洛树搜索进行高效候选选择；4) 结合粗粒度LLM聚类，将采集函数评估限制在具有统计更高属性值的聚类中。

Result: 通过大量实验和消融研究，该方法显著提高了LLM引导的贝叶斯优化在分子发现中的可扩展性、鲁棒性和样本效率。

Conclusion: 提出的方法通过直接利用LLM和化学基础模型的先验知识，结合树结构空间划分和聚类技术，有效解决了传统贝叶斯优化在分子发现中的局限性，为大规模分子搜索提供了高效解决方案。

Abstract: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.

</details>


### [78] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: PGDM利用模式引导扩散模型进行时间序列预测，通过原型分析提取数据中的重复模式，并基于模式估计引导预测，提高预测准确性和现实性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多元时间序列预测中很少考虑数据中的重复模式结构，这限制了预测的现实性和准确性。

Method: 1. 使用原型分析提取时间数据中的固有模式；2. 估计序列中最可能的下一个模式；3. 用模式估计引导扩散模型的预测；4. 引入基于原型分析的不确定性量化技术；5. 根据模式估计不确定性动态调整引导强度。

Result: 在两个应用（视野测量和动作捕捉）中，模式引导使PGDM性能提升分别达40.67%/56.26%和14.12%/14.10%，优于基线方法达65.58%/84.83%和93.64%/92.55%。

Conclusion: 模式引导显著提高了扩散模型在时间序列预测中的性能，使预测更符合已知模式，并提供了有效的不确定性量化方法。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [79] [A Single Architecture for Representing Invariance Under Any Space Group](https://arxiv.org/abs/2512.13989)
*Cindy Y. Zhang,Elif Ertekin,Peter Orbanz,Ryan P. Adams*

Main category: cs.LG

TL;DR: 提出一种能自动适应任意空间群对称性的单一机器学习架构，通过对称适配傅里叶基实现跨空间群的权重共享和零样本学习


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每个对称群设计专门架构，限制了可扩展性和跨相关对称性的知识迁移。对于材料科学中关键的230个三维空间群，这一问题尤为突出

Method: 通过显式表征群操作对傅里叶系数的约束，构建对称适配的傅里叶基，并将这些约束编码到神经网络层中，实现跨不同空间群的权重共享

Result: 在材料性质预测任务中表现出竞争力，并能通过零样本学习泛化到未见过的空间群

Conclusion: 该方法提供了一种统一的架构来处理多种空间群对称性，克服了数据稀疏性问题，促进了跨对称群的知识迁移

Abstract: Incorporating known symmetries in data into machine learning models has consistently improved predictive accuracy, robustness, and generalization. However, achieving exact invariance to specific symmetries typically requires designing bespoke architectures for each group of symmetries, limiting scalability and preventing knowledge transfer across related symmetries. In the case of the space groups, symmetries critical to modeling crystalline solids in materials science and condensed matter physics, this challenge is particularly salient as there are 230 such groups in three dimensions. In this work we present a new approach to such crystallographic symmetries by developing a single machine learning architecture that is capable of adapting its weights automatically to enforce invariance to any input space group. Our approach is based on constructing symmetry-adapted Fourier bases through an explicit characterization of constraints that group operations impose on Fourier coefficients. Encoding these constraints into a neural network layer enables weight sharing across different space groups, allowing the model to leverage structural similarities between groups and overcome data sparsity when limited measurements are available for specific groups. We demonstrate the effectiveness of this approach in achieving competitive performance on material property prediction tasks and performing zero-shot learning to generalize to unseen groups.

</details>


### [80] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该论文构建了一个精心整理的MHC-II抗原表位数据集，提出了三个机器学习任务，并建立了多尺度评估框架，为计算免疫治疗提供资源。


<details>
  <summary>Details</summary>
Motivation: MHC-II抗原表位在免疫治疗中至关重要，但与MHC-I相比，其研究面临更大挑战：结合特异性复杂、基序模式模糊、现有数据集较小且标准化不足。

Method: 从IEDB等公共来源构建精心整理的数据集，不仅扩展和标准化了现有肽-MHC-II数据集，还引入了具有更丰富生物学背景的新型抗原-MHC-II数据集。基于此数据集，制定了肽结合、肽呈递和抗原呈递三个机器学习任务，并采用多尺度评估框架对现有模型进行基准测试。

Result: 创建了一个有价值的MHC-II抗原表位数据集资源，建立了三个逐步捕获MHC-II抗原呈递通路中更广泛生物学过程的机器学习任务框架，并通过模块化框架对各种建模设计进行了全面分析。

Conclusion: 这项工作为推进计算免疫治疗提供了宝贵资源，为未来机器学习指导的表位发现和免疫反应预测建模研究奠定了基础。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [81] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: EXAONE Path 2.5是一个病理学基础模型，通过联合建模组织学、基因组学、表观遗传学和转录组学等多模态数据，创建更全面的患者表征来反映肿瘤生物学特征。


<details>
  <summary>Details</summary>
Motivation: 癌症进展涉及多个生物层面的相互作用，特别是超出形态学层面的分子层面，这些对仅依赖图像的模型是不可见的。需要捕获更广泛的生物景观来更全面地理解肿瘤生物学。

Method: 1) 多模态SigLIP损失实现跨异质模态的全配对对比学习；2) 片段感知旋转位置编码(F-RoPE)模块保留WSI中的空间结构和组织片段拓扑；3) 针对WSI和RNA-seq的领域专业化内部基础模型，提供生物基础嵌入以实现稳健的多模态对齐。

Result: 在Patho-Bench基准测试(80个任务)和内部真实世界临床数据集上评估，与六个领先的病理学基础模型相比，该框架表现出高数据和参数效率，在Patho-Bench上达到与最先进模型相当的性能，同时在内部临床设置中表现出最高的适应性。

Conclusion: 生物信息化的多模态设计具有重要价值，整合基因型到表型建模为下一代精准肿瘤学提供了潜力。

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [82] [Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks](https://arxiv.org/abs/2512.14023)
*Yong Fang,Na Li,Hangguan Shan,Eryun Liu,Xinyu Li,Wei Ni,Er-Ping Li*

Main category: cs.LG

TL;DR: HSMGNN：首个利用混合几何表示进行多元时间序列预测的图神经网络，在欧几里得和黎曼空间中捕捉数据几何结构，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在欧几里得或黎曼空间中建模多元时间序列数据，难以捕捉真实世界数据中多样的几何结构和复杂的时空依赖性。

Method: 提出HSMGNN模型：1）引入子流形交叉段嵌入，将输入数据投影到欧几里得和黎曼空间；2）设计自适应距离库层降低黎曼距离计算成本；3）构建融合图卷积网络整合双空间特征。

Result: 在三个基准数据集上的实验表明，HSMGNN相比最先进的基线方法，预测精度提升最高达13.8%。

Conclusion: HSMGNN通过混合欧几里得-黎曼框架有效捕捉多元时间序列的几何特性，为复杂时空依赖建模提供了新思路，显著提升了预测性能。

Abstract: Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.

</details>


### [83] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: FusAD是一个统一的时间序列分析框架，通过自适应时频融合和去噪机制，在分类、预测和异常检测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分析面临三大挑战：1) 缺乏高效、多任务兼容的统一框架；2) 现有方法通常针对单一任务或特定数据类型；3) 真实数据常受噪声、复杂频率成分和多尺度动态模式影响，难以进行鲁棒特征提取。

Method: FusAD采用自适应时频融合机制，结合傅里叶和小波变换捕捉全局-局部和多尺度动态特征；包含自适应去噪机制自动感知和过滤各类噪声；集成通用信息融合和解码结构，结合掩码预训练促进多粒度表示的学习和迁移。

Result: 在主流时间序列基准测试中，FusAD在分类、预测和异常检测任务上持续优于最先进模型，同时保持高效率和可扩展性。

Conclusion: FusAD通过统一的时频融合和自适应去噪机制，成功解决了多任务时间序列分析的挑战，为复杂环境下的鲁棒特征提取提供了有效解决方案。

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [84] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: SonicMoE：通过内存高效算法、GPU内核优化和token rounding技术，解决细粒度MoE模型的内存占用和计算效率问题，显著提升训练性能。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型趋向于更高专家粒度（更小的专家中间维度）和更高稀疏性（恒定激活专家数但总专家数更多），这虽然提升了每FLOP的模型质量，但也带来了激活内存占用增加、硬件效率降低（IO成本高）以及稀疏MoE中Grouped GEMM内核填充导致的计算浪费问题。

Method: 1. 提出内存高效算法，以最小化反向传播所需的激活缓存来计算MoE的前向和反向传播；2. 设计GPU内核，将内存IO与计算重叠，适用于所有MoE架构；3. 提出新颖的"token rounding"方法，最小化Grouped GEMM内核中填充导致的计算浪费。

Result: SonicMoE将激活内存减少45%，在Hopper GPU上相比ScatterMoE的BF16 MoE内核实现了1.86倍的计算吞吐量提升。在64个H100上，SonicMoE实现了每天2130亿token的训练吞吐量，与ScatterMoE在96个H100上的2250亿token/天相当。在高MoE稀疏性设置下，tile-aware token rounding算法相比vanilla top-K路由在核执行时间上额外带来1.16倍加速，同时保持相似的下游性能。

Conclusion: SonicMoE通过创新的内存管理、计算优化和token rounding技术，有效解决了细粒度和稀疏MoE模型的效率瓶颈，显著提升了训练性能，并将所有内核开源以促进更快的MoE模型训练。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [85] [Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization](https://arxiv.org/abs/2512.14086)
*Boyuan Yao,Dingcheng Luo,Lianghao Cao,Nikola Kovachki,Thomas O'Leary-Roseberry,Omar Ghattas*

Main category: cs.LG

TL;DR: 本文提出了导数信息傅里叶神经算子(DIFNOs)的逼近理论和高效训练方法，用于PDE约束优化。DIFNO不仅能准确模拟高保真算子的响应，还能模拟其灵敏度，在样本复杂度上优于传统FNO。


<details>
  <summary>Details</summary>
Motivation: 传统的傅里叶神经算子(FNOs)作为代理模型在PDE约束优化中存在局限性，因为准确的优化需要准确的导数信息。需要开发既能准确模拟算子响应又能模拟其灵敏度的新型神经网络架构。

Method: 提出了导数信息傅里叶神经算子(DIFNOs)，通过同时最小化输出和Fréchet导数样本来训练。开发了高效的训练方案，包括降维和多分辨率技术，显著降低了导数学习的计算成本。

Result: 建立了FNO及其Fréchet导数在紧集上的同时通用逼近理论，以及在无界支撑加权Sobolev空间中的通用逼近理论。数值实验表明DIFNOs在非线性扩散-反应、Helmholtz和Navier-Stokes方程中具有优越的样本复杂度和准确性。

Conclusion: DIFNOs作为代理模型在PDE约束优化中优于传统FNOs，能够以较少的训练样本实现高精度，为无限维PDE约束逆问题的求解提供了有效工具。

Abstract: We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.

</details>


### [86] [Arithmetic-Intensity-Aware Quantization](https://arxiv.org/abs/2512.14090)
*Taig Singh,Shreshth Rajan,Nikhil Iyer*

Main category: cs.LG

TL;DR: AIQ是一种混合精度量化框架，通过逐层选择比特宽度来最大化算术强度，同时最小化精度损失，从而提升内存受限神经网络推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络越来越受内存限制，推理吞吐量受DRAM带宽而非计算能力限制。需要一种方法来提高算术强度，从而提升内存受限情况下的推理性能。

Method: AIQ是一种训练后量化方法，使用搜索算法在逐层量化方案上搜索，最小化算术强度和精度的加权损失。通过混合精度量化，为每层选择最优比特宽度。

Result: 在ResNet-20/CIFAR-10上，AIQ将算术强度提高约50%，测试精度保持在FP32基线的约1个百分点内，优于全局均匀量化。在内存受限的MobileNetV2上，AIQ配置提供1.66倍吞吐量提升，精度损失在1个百分点内。

Conclusion: AIQ能有效提升内存受限神经网络的推理吞吐量，同时保持精度。该方法自然地更激进地量化较大的层，为内存受限推理提供了一种实用的混合精度量化解决方案。

Abstract: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.

</details>


### [87] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Cornserve是一个用于Any-to-Any多模态模型的高效在线服务系统，通过自动优化部署计划和分布式运行时，显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: Any-to-Any模型接受文本和多模态数据组合输入并生成组合输出，引入了请求类型、计算路径和计算扩展的异构性，传统服务系统难以高效处理这种复杂性。

Method: 1) 允许开发者描述通用Any-to-Any模型的计算图；2) 规划器根据模型和工作负载特征自动优化部署计划，包括是否及如何将模型分解为更小组件；3) 分布式运行时按计划执行模型，高效处理异构性。

Result: Cornserve能够高效服务多样化的Any-to-Any模型和工作负载，相比现有解决方案，吞吐量提升最高达3.81倍，尾部延迟降低最高达5.79倍。

Conclusion: Cornserve通过创新的规划器和分布式运行时，有效解决了Any-to-Any多模态模型服务中的异构性挑战，为这类新兴模型提供了高效、可扩展的在线服务解决方案。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [88] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出S-GRPO框架，用逻辑相似性奖励机制替代传统奖励建模，通过形式逻辑一致性引导模型对齐人类偏好，避免模型崩溃


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法依赖奖励模型的质量和稳定性，但传统奖励建模存在启发式估计的局限性，且现实问题有多视角解释，逻辑强化学习可能导致模型崩溃

Method: 提出逻辑相似性奖励机制，基于形式逻辑一致性而非启发式奖励估计；引入S-GRPO（GRPO的监督变体），结合监督组件，联合优化生成项、KL散度正则化和基于标签的目标

Result: S-GRPO在性能和鲁棒性上均优于标准监督微调（SFT），并能扩展现有偏好学习框架（如GRPO和DPO），提供更灵活、任务自适应的对齐训练方法

Conclusion: 逻辑相似性奖励机制和S-GRPO框架为LLM对齐提供了有效的替代方案，通过形式逻辑一致性提升对齐性能，避免模型崩溃，代码已开源

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [89] [PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario](https://arxiv.org/abs/2512.14150)
*Zhijie Zhong,Zhiwen Yu,Pengyu Li,Jianming Lv,C. L. Philip Chen,Min Chen*

Main category: cs.LG

TL;DR: PathFinder提出了一种主动建模建筑和发射器的路径损耗预测架构，通过解耦特征编码和掩码引导低秩注意力，解决了现有方法在环境建模、多发射器场景和分布偏移方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的无线路径损耗预测方法存在三个关键问题：1) 被动环境建模，忽视发射器和关键环境特征；2) 过度关注单发射器场景，而现实世界多为多发射器；3) 过分关注分布内性能，忽视分布偏移挑战。

Method: 提出PathFinder架构：1) 通过解耦特征编码主动建模建筑和发射器；2) 集成掩码引导低秩注意力，独立关注接收器和建筑区域；3) 引入发射器导向混合策略进行鲁棒训练；4) 创建新的单到多发射器RPP基准评估外推性能。

Result: 实验结果表明PathFinder显著优于现有最先进方法，特别是在具有挑战性的多发射器场景中，在单发射器训练后多发射器测试的外推任务上表现优异。

Conclusion: PathFinder通过主动环境建模和专门的多发射器处理机制，有效解决了无线路径损耗预测中的关键挑战，为5G网络优化和物联网应用提供了更可靠的预测工具。

Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.

</details>


### [90] [On Improving Deep Active Learning with Formal Verification](https://arxiv.org/abs/2512.14170)
*Jonathan Spiegelman,Guy Amir,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出在深度主动学习中通过形式验证生成对抗样本来增强训练数据，相比传统梯度攻击方法能显著提升模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度主动学习旨在减少神经网络训练中的标注成本，但现有方法主要关注样本选择。本文探索通过添加违反鲁棒性约束的对抗样本来进一步提升数据效率，特别是研究形式验证生成的对抗样本相比传统梯度攻击方法的优势。

Method: 提出在深度主动学习中集成对抗样本增强的方法，特别强调使用形式验证技术生成对抗样本，而非传统的梯度攻击方法。该方法可应用于多种现代DAL技术，并提出了一个新的技术方案。

Result: 实验表明，通过形式验证生成的对抗样本相比标准梯度攻击方法能更显著地提升DAL性能。该方法在多个现代DAL技术和作者提出的新技术上均取得显著改进，在标准基准测试中显著提高了模型泛化能力。

Conclusion: 在深度主动学习中集成形式验证生成的对抗样本能有效提升数据效率和模型泛化性能，为DAL研究提供了新的增强方向。

Abstract: Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

</details>


### [91] [Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix](https://arxiv.org/abs/2512.14188)
*Wei Tao,Sheng Long,Xin Liu,Wei Li,Qing Tao*

Main category: cs.LG

TL;DR: 提出一种新的基于动量的对抗攻击方法AdaMI，通过引入动量自适应矩阵解决现有攻击方法的收敛问题，提升对抗样本的迁移性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的对抗攻击方法（如PGD、MI-FGSM）使用符号函数缩放扰动，从优化理论角度看存在收敛问题。本文旨在从优化理论角度分析这些攻击方法，并提出更稳定的攻击算法。

Method: 首先揭示PGD实际上是使用当前梯度确定步长的投影梯度法的特定重构，然后提出AdaMI攻击方法，该方法使用基于动量的自适应矩阵来优化扰动，在凸问题中证明能达到最优收敛。

Result: 实验表明，提出的动量自适应矩阵能有效提升对抗样本在不同网络间的迁移性，同时保持更好的稳定性和不可感知性，优于现有最先进方法。

Conclusion: AdaMI通过引入动量自适应矩阵解决了MI-FGSM的非收敛问题，确保了优化过程的稳定性，为对抗攻击提供了一种通用且有效的技术。

Abstract: Generating adversarial examples (AEs) can be formulated as an optimization problem. Among various optimization-based attacks, the gradient-based PGD and the momentum-based MI-FGSM have garnered considerable interest. However, all these attacks use the sign function to scale their perturbations, which raises several theoretical concerns from the point of view of optimization. In this paper, we first reveal that PGD is actually a specific reformulation of the projected gradient method using only the current gradient to determine its step-size. Further, we show that when we utilize a conventional adaptive matrix with the accumulated gradients to scale the perturbation, PGD becomes AdaGrad. Motivated by this analysis, we present a novel momentum-based attack AdaMI, in which the perturbation is optimized with an interesting momentum-based adaptive matrix. AdaMI is proved to attain optimal convergence for convex problems, indicating that it addresses the non-convergence issue of MI-FGSM, thereby ensuring stability of the optimization process. The experiments demonstrate that the proposed momentum-based adaptive matrix can serve as a general and effective technique to boost adversarial transferability over the state-of-the-art methods across different networks while maintaining better stability and imperceptibility.

</details>


### [92] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 该论文提出使用随机桥（在固定时间点具有目标分布的随机过程）作为生成模型中的随机传输方法，相比传统方法能以更少步骤生成高质量样本。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索随机桥在生成建模中的应用潜力，随机桥可以作为两个概率分布之间的随机传输工具，并且可以根据驱动过程表现出马尔可夫或非马尔可夫、连续、不连续或混合模式。

Method: 从一般概率陈述出发，推导出具体的学习和模拟算法表示，基于信息处理框架。具体实现使用高斯随机桥作为基础。

Result: 基于高斯随机桥的实证结果显示，相比传统方法能以显著更少的步骤生成高质量样本，同时获得有竞争力的Fréchet Inception Distance分数。

Conclusion: 提出的框架计算成本低，适合高速生成任务，为生成建模提供了新的随机传输方法。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [93] [Understanding and Improving Hyperbolic Deep Reinforcement Learning](https://arxiv.org/abs/2512.14202)
*Timo Klein,Thomas Lang,Andrii Shkabrii,Alexander Sturm,Kevin Sidak,Lukas Miklautz,Claudia Plant,Yllka Velaj,Sebastian Tschiatschek*

Main category: cs.LG

TL;DR: 论文提出Hyper++，一种改进的双曲PPO智能体，通过稳定梯度训练、特征正则化和优化友好的双曲层，解决了双曲特征空间在强化学习中因大范数嵌入导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 双曲特征空间能自然捕捉复杂RL环境中的层次和关系结构，但RL的非平稳性导致优化困难。研究发现大范数嵌入会破坏基于梯度的训练，导致PPO中的信任区域违反。

Method: 提出Hyper++，包含三个组件：1）通过分类价值损失而非回归来稳定评论家训练；2）特征正则化保证有界范数，避免裁剪带来的维度诅咒；3）使用更优化友好的双曲网络层公式。

Result: 在ProcGen上，Hyper++保证稳定学习，优于先前双曲智能体，减少约30%的挂钟时间。在Atari-5上使用Double DQN时，Hyper++显著优于欧几里得和双曲基线。

Conclusion: 通过分析双曲几何中的梯度问题，论文提出了有效的解决方案Hyper++，显著提升了双曲RL智能体的训练稳定性和性能，为利用双曲空间进行强化学习提供了实用方法。

Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .

</details>


### [94] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 提出LLM compare方法，通过大语言模型进行成对难度比较并计算Bradley-Terry分数，用于评估超出人类和现有模型能力范围的问题难度。


<details>
  <summary>Details</summary>
Motivation: 现有问题难度评估方法（如人工校准或基于性能的评分）无法扩展到超出分布范围的问题（即人类和LLM目前无法解决的问题），因为这些方法不可扩展、耗时且依赖真实标签。

Method: 使用LLM进行成对难度比较，然后基于比较结果计算Bradley-Terry分数。该方法在三个正交维度上占据理想位置：连续动态、模型无关且不依赖真实标签信息。

Result: 1) 提出概念框架定位现有方法；2) LLM compare与人类标注高度一致（Pearson r≥0.80，n=1876）；3) 对幻觉具有鲁棒性（10%噪声注入下Pearson相关性下降小于6%）。

Conclusion: LLM compare是首个连续动态、模型无关且不依赖真实标签的难度评估方法，能有效替代耗时的人工标注和合成数据生成，对课程设计、模型评估和AI辅助研究构思有重要推动作用。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [95] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: 本文提出了一种名为RGM的新型图生成模型评估方法，克服了传统MMD指标的局限性，并对GRAN和EDGE两种先进图生成模型进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 图生成在许多领域至关重要，但现有评估方法主要依赖最大均值差异（MMD）指标，这种方法存在局限性，需要更有效的评估方法来准确评估图生成模型的性能。

Method: 提出RGM（Representation-aware Graph-generation Model evaluation）评估方法，使用几何深度学习模型在专门设计的合成和真实图数据集上进行图分类任务，以此评估图生成模型。

Result: 对GRAN和EDGE两种先进图生成模型的评估显示，虽然两者都能生成具有某些拓扑性质的图，但在保持区分不同图域的结构特征方面存在显著局限性。同时证实了MMD作为图生成模型评估指标的不足。

Conclusion: 需要超越MMD的替代评估方法，RGM提供了一种更全面的评估框架，未来研究应关注能够更好保持图结构特征的图生成模型。

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [96] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: FLAME是一个轻量级时间序列基础模型家族，支持确定性和概率性预测，通过生成式概率建模确保效率和鲁棒性，在多个基准测试中实现最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个既轻量又强大的时间序列基础模型，能够同时处理确定性和概率性预测任务，在保持高效率的同时提供鲁棒的预测能力。

Method: 使用Legendre Memory增强泛化能力，采用LegT和LegS变体在编码和解码阶段捕获数据内在归纳偏置；采用归一化流预测头对复杂分布进行生成式建模。

Result: 在TSFM-Bench和ProbTS等基准测试中，FLAME在确定性和概率性预测任务上都实现了最先进的零样本性能。

Conclusion: FLAME是一个高效且强大的时间序列基础模型，通过创新的Legendre Memory架构和归一化流预测头，在轻量化的同时实现了卓越的预测性能。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [97] [Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits](https://arxiv.org/abs/2512.14338)
*Michael Murray,Tenzin Chan,Kedar Karhadker,Christopher J. Hillar*

Main category: cs.LG

TL;DR: Hopfield网络能从少量随机样本中推断图的同构类，其学习过程具有向范数效率解的隐式偏置，这驱动了在群结构数据下近似不变性的涌现。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在群结构数据训练中不变性的涌现现象，特别是在经典Hopfield网络中，探索其如何从少量样本中学习图的同构类。

Method: 使用经典Hopfield网络，通过最小化能量流（MEF）的梯度下降方法，分析网络在群结构数据上的学习行为，研究参数如何收敛到不变子空间。

Result: 发现：1）图同构类可在三维不变子空间中表示；2）MEF梯度下降具有向范数效率解的隐式偏置，支撑了学习同构类的多项式样本复杂度界限；3）多种学习规则下，参数随样本量增长收敛到不变子空间。

Conclusion: Hopfield网络中的泛化机制统一于：学习过程中向范数效率的偏置驱动了在群结构数据下近似不变性的涌现，这解释了网络如何从有限样本中推断对称性结构。

Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

</details>


### [98] [Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis](https://arxiv.org/abs/2512.14361)
*Nicholas Tagliapietra,Katharina Ensinger,Christoph Zimmer,Osman Mian*

Main category: cs.LG

TL;DR: CaDyT：一种基于差分因果模型和高斯过程推理的连续时间因果发现方法，在规则和不规则采样数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统在连续时间中演化，但现有方法要么离散化时间（对不规则采样数据效果差），要么忽略底层因果关系。需要一种能同时处理连续时间特性和因果结构的方法。

Method: 基于差分因果模型（比离散时间动态贝叶斯网络假设更温和），利用精确高斯过程推理建模连续时间动力学，通过算法马尔可夫条件和最小描述长度原则指导的贪婪搜索识别因果结构。

Result: CaDyT在规则和不规则采样数据上都优于现有最先进方法，发现的因果网络更接近真实底层动力学。

Conclusion: CaDyT通过结合差分因果模型和连续时间高斯过程推理，成功解决了动态系统中因果发现的连续时间和因果结构双重挑战。

Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.

</details>


### [99] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 首个基于提升量子差分隐私的黑盒隐私审计框架，用于量子机器学习模型，通过量子金丝雀检测记忆化并量化隐私泄露


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在处理敏感数据时存在记忆化个体记录的风险，而现有的量子差分隐私机制缺乏实证验证工具，无法在实际部署中验证隐私保护效果

Method: 基于提升量子差分隐私，引入量子金丝雀（策略性偏移编码的量子态）来检测记忆化，建立金丝雀偏移与迹距离界限的数学联系，推导隐私预算消耗的经验下界

Result: 在模拟和物理量子硬件上的综合评估表明，该框架能有效测量量子机器学习模型的实际隐私损失，实现量子机器学习系统中的鲁棒隐私验证

Conclusion: 该框架填补了理论隐私保证与实际隐私验证之间的关键空白，为量子机器学习系统的隐私保护提供了首个黑盒审计工具

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [100] [SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design](https://arxiv.org/abs/2512.14397)
*Yunjia Yang,Weishao Tang,Mengxin Liu,Nils Thuerey,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: SuperWing是一个包含4,239个参数化机翼几何形状和28,856个RANS流场解的开源数据集，用于训练三维机翼气动学的机器学习代理模型，展示了在复杂基准机翼上的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前三维机翼气动设计缺乏通用性强的机器学习代理模型，主要原因是现有数据集规模小且多样性不足，限制了模型的泛化能力。

Method: 1) 使用简化的几何参数化方法生成机翼形状，包含翼展方向的气动外形、扭转角和上反角变化；2) 在典型飞行包线内模拟不同马赫数和攻角下的流场；3) 使用Transformer模型进行基准测试。

Result: 1) 数据集包含4,239个参数化机翼几何形状和28,856个RANS流场解；2) Transformer模型在测试集上达到2.5阻力计数误差；3) 在SuperWing上预训练的模型对DLR-F6和NASA CRM等复杂基准机翼表现出强大的零样本泛化能力。

Conclusion: SuperWing数据集通过其规模和多样性解决了三维机翼气动学机器学习模型训练的数据瓶颈问题，展示了在实际复杂机翼设计中的实用潜力。

Abstract: Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.

</details>


### [101] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: GRAFT模型通过文本引导的跨注意力机制，将多源文本信息与电力负荷数据对齐，实现电网感知的负荷预测，并在澳大利亚五州数据集上显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 电力负荷同时受到天气、日历节奏、突发事件和政策等多时间尺度外生因素的影响，需要开发能够有效整合多源文本信息的电网感知预测方法。

Method: 改进STanHOP模型，严格对齐每日聚合的新闻、社交媒体和政策文本与半小时负荷数据，通过跨注意力机制实现文本引导的融合，并提供即插即用的外部记忆接口。

Result: 在澳大利亚五州2019-2021年数据集上，GRAFT在小时、日、月三个时间尺度上显著优于强基线方法，达到或超越当前最优水平，且在事件驱动场景中表现稳健。

Conclusion: GRAFT通过文本引导的跨注意力机制有效整合多源文本信息，显著提升电力负荷预测性能，同时提供可解释的文本-负荷影响分析，为电网预测提供了标准化评估基准。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [102] [Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space](https://arxiv.org/abs/2512.14418)
*Dejun Hu,Zhiming Li,Jia-Rui Shen,Jia-Ning Tu,Zi-Hao Ye,Junliang Zhang*

Main category: cs.LG

TL;DR: 提出双轴表示-完全收敛学习策略，通过FD25数据集实现有机分子化学空间的近乎完全覆盖，使图神经网络达到收敛学习和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 化学空间极其庞大（10^30-10^60个分子），现有机器学习模型能否在整个化学空间实现收敛学习仍是一个未解决的科学问题。需要建立系统性的方法来评估和确保模型在化学空间中的学习收敛性。

Method: 提出双轴表示-完全收敛学习策略：1）结合图卷积网络编码局部价键环境（基于现代价键理论）；2）无桥图编码环/笼拓扑结构。基于此框架构建FD25数据集，系统覆盖13,302个局部价键单元和165,726个环/笼拓扑，实现H/C/N/O/F有机分子的近乎完全组合覆盖。

Result: 在FD25数据集上训练的图神经网络表现出表示-完全收敛学习，并在外部基准测试中展现出强分布外泛化能力，整体预测误差约为1.0 kcal/mol MAE。建立了分子表示、结构完整性和模型泛化之间的定量联系。

Conclusion: 该研究为可解释、可迁移、数据高效的分子智能奠定了基础，通过表示完整性框架为大规模模型的收敛学习提供了原则性基础，解决了化学空间收敛学习的关键科学问题。

Abstract: Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.

</details>


### [103] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: ClimaX-LETKF是首个纯数据驱动的机器学习集合天气预报系统，通过同化NCEP观测数据实现多年稳定运行，研究发现RTPP比RTPS更适合MLWP模型


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习天气预报取得显著进展，但在MLWP模型中同化真实观测或集合预报的研究仍然有限，需要开发不依赖数值天气预报模型的纯数据驱动系统

Method: 开发ClimaX-LETKF系统，使用NCEP ADP全球高空和地面天气观测数据进行同化，比较RTPP（松弛到先验扰动）和RTPS（松弛到先验扩展）两种方法

Result: 系统能够多年稳定运行，RTPP比RTPS提供更高的稳定性和准确性，而NWP模型通常更稳定于RTPS；MLWP模型恢复大气场到吸引子的能力弱于NWP模型

Conclusion: 这项工作为增强MLWP集合预报系统提供了宝贵见解，代表了向实际应用迈出的重要一步，揭示了MLWP与NWP在扰动处理上的不同特性

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [104] [AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts](https://arxiv.org/abs/2512.14461)
*Niklas Grieger,Jannik Raskob,Siamak Mehrkanoon,Stephan Bialonski*

Main category: cs.LG

TL;DR: AnySleep是一个深度神经网络模型，能够使用任意EEG或EOG数据在可调时间分辨率下进行睡眠分期，在21个数据集的19,000多个夜间记录上训练，性能达到SOTA，支持多中心研究和新型生物标志物发现。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠分期需要人工标注，耗时耗力；不同中心的PSG记录在电极数量、配置和受试者特征上差异很大；30秒分期的标准是基于实用性而非生理学考虑；这些限制阻碍了多中心睡眠研究的协调和更短时间尺度上新型生物标志物的发现。

Method: 开发了AnySleep深度神经网络模型，使用任意EEG或EOG数据进行睡眠分期；在21个数据集的19,000多个夜间记录（近200,000小时EEG和EOG数据）上训练和验证；支持可调时间分辨率（包括30秒以下）；模型能够处理不同电极配置（从单通道到多通道）。

Result: 模型达到最先进性能，在30秒分期上优于或等于现有基线；性能随通道数量增加而提升，但在只有EOG或单EEG导联（额叶、中央或枕叶）时仍保持良好表现；在30秒以下时间尺度上，模型能捕捉与觉醒一致的短暂清醒侵入，并改善对生理特征（年龄、性别）和病理状况（睡眠呼吸暂停）的预测。

Conclusion: AnySleep是一个强大的睡眠分期工具，能够处理异质电极设置，促进大规模多中心研究，加速睡眠中新型生物标志物的发现。模型已公开可用，有望推动睡眠研究和临床护理的发展。

Abstract: Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.

</details>


### [105] [Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics](https://arxiv.org/abs/2512.14471)
*Additi Pandey,Liang Wei,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: Kinetic-Mamba：基于Mamba的神经算子框架，用于准确预测燃烧模拟中的化学动力学演化，通过多种Mamba模型变体实现高效时间建模


<details>
  <summary>Details</summary>
Motivation: 准确的化学动力学建模对燃烧模拟至关重要，传统方法在处理复杂反应路径和热化学状态演化时存在挑战，需要结合神经算子的表达能力和Mamba架构的高效时间建模能力

Method: 提出Kinetic-Mamba框架，包含三种互补模型：1) 独立Mamba模型预测热化学状态变量时间演化；2) 约束Mamba模型强制质量守恒；3) 基于温度区间的双Mamba模型架构。还开发了潜在空间变体，在降维潜在空间中演化动力学并重构到物理流形

Result: 在Syngas和GRI-Mech 3.0反应机制上的计算实验表明，该框架仅使用状态变量初始条件就能高保真地预测复杂动力学行为，在时间分解和递归预测策略下表现出良好的准确性和鲁棒性

Conclusion: Kinetic-Mamba框架成功将Mamba架构的高效时间建模能力与神经算子的表达能力相结合，为燃烧模拟中的化学动力学建模提供了准确、高效的解决方案，具有良好的外推能力

Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.

</details>


### [106] [Improving Slow Transfer Predictions: Generative Methods Compared](https://arxiv.org/abs/2512.14522)
*Jacob Taegon Kim,Alex Sim,Kesheng Wu,Jinoh Kim*

Main category: cs.LG

TL;DR: 该研究针对科学计算网络中数据传输性能预测的类别不平衡问题，比较了多种数据增强策略，发现即使使用先进的CTGAN生成技术，其性能提升也不显著优于简单的分层采样方法。


<details>
  <summary>Details</summary>
Motivation: 科学计算网络中监测数据传输性能至关重要，通过早期预测性能可以识别潜在缓慢传输并优化网络使用。然而，机器学习模型预测能力提升的关键瓶颈是类别不平衡问题。

Method: 研究分析比较了多种数据增强策略，包括传统的过采样方法和生成技术（如CTGAN），并调整训练数据集中的类别不平衡比例来评估其对模型性能的影响。

Result: 虽然数据增强可能改善性能，但随着不平衡比例增加，性能提升并不显著。最先进的CTGAN技术也没有显著优于简单的分层采样方法。

Conclusion: 在科学计算网络性能预测的类别不平衡问题上，即使使用先进的生成技术如CTGAN，其性能提升效果有限，不显著优于传统的分层采样方法。

Abstract: Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.

</details>


### [107] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 使用变分自编码器生成合成心房电图，解决心脏电生理成像中配对数据稀缺问题，并通过数据增强提升下游任务性能


<details>
  <summary>Details</summary>
Motivation: 心房颤动是最常见的心律失常，其临床评估需要准确表征心房电活动。虽然非侵入性心电图成像结合深度学习在从体表电位估计心内电图方面显示出潜力，但配对BSPM-EGM数据集的有限可用性阻碍了进展。

Method: 提出两种变分自编码器模型：窦性心律特异性VAE（VAE-S）和类别条件VAE（VAE-C），后者在窦性心律和房颤信号上训练。使用形态学、频谱和分布相似性指标评估生成的EGM。

Result: VAE-S在模拟EGM方面实现更高的保真度，而VAE-C能够进行节律特异性生成，但以降低窦性重建质量为代价。作为概念验证，生成的EGM用于下游非侵入性EGM重建任务的数据增强，适度增强提高了估计性能。

Conclusion: 基于VAE的生成模型有潜力缓解数据稀缺问题，并增强基于深度学习的心电图成像流程。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [108] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 论文批评现有时间序列反事实解释方法在临床推荐场景中的不足，主张转向考虑因果合理性和时间连贯性的可持续干预方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分类的反事实解释方法主要基于静态数据假设，仅关注最小输入扰动来改变模型预测，这在临床推荐场景中不够充分，因为干预措施需要随时间展开，且必须具有因果合理性和时间连贯性。

Method: 通过分析现有方法的局限性（时间盲点和缺乏用户中心考虑），并对几种最先进的时间序列方法进行鲁棒性分析，展示生成的反事实对随机噪声的高度敏感性。

Result: 研究发现现有方法生成的反事实对随机噪声高度敏感，在真实临床环境中可靠性有限，因为轻微测量变化不可避免。

Conclusion: 呼吁开发超越仅改变预测而不考虑可行性或可操作性的方法和评估框架，强调需要在实际环境中对用户可行的、有目的驱动的可操作干预措施。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [109] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: 提出Residual GRU with Multi-Head Self-Attention模型，用于心血管疾病预测，在UCI数据集上优于传统和现代基线方法。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要可靠高效的预测工具。传统方法依赖人工特征和临床经验，机器学习方法可提高可重复性但难以在嘈杂异质临床数据上泛化。

Method: 提出紧凑深度学习架构：结合残差双向GRU进行特征列序列建模、通道重加权块、多头自注意力池化和可学习分类token来捕获全局上下文。

Result: 在UCI心脏病数据集上5折分层交叉验证：准确率0.861，宏F1 0.860，ROC-AUC 0.908，PR-AUC 0.904，优于所有基线。消融研究证实各组件贡献，t-SNE可视化显示学习嵌入有更清晰类别分离。

Conclusion: 轻量级混合循环和注意力架构在临床风险预测中实现了准确性和效率的良好平衡，支持在资源受限医疗环境中部署。

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [110] [Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs](https://arxiv.org/abs/2512.14596)
*Youngkyu Lee,Francesc Levrero Florencio,Jay Pathak,George Em Karniadakis*

Main category: cs.LG

TL;DR: Geo-DeepONet：一种几何感知的深度算子网络，结合有限元离散化提取的域信息，可在任意非结构化网格上进行算子学习而无需重新训练，并基于此开发几何感知混合预条件迭代求解器。


<details>
  <summary>Details</summary>
Motivation: 传统迭代求解器对参数偏微分方程（PDEs）的收敛行为高度依赖于域和离散化方式。先前提出的混合求解器（结合传统求解器和神经算子）在特定几何上表现良好，但在训练未遇到的几何上表现不佳。

Method: 提出Geo-DeepONet几何感知深度算子网络，从有限元离散化中提取域信息，实现跨任意非结构化网格的算子学习。基于此开发几何感知混合预条件迭代求解器，将Geo-DeepONet与松弛方案和Krylov子空间算法等传统方法结合。

Result: 通过在不同非结构化域上的参数PDE数值实验，证明了所提出的混合求解器在多个实际应用中具有增强的鲁棒性和效率。

Conclusion: Geo-DeepONet能够有效解决传统混合求解器在未见几何上性能下降的问题，结合传统迭代方法形成的几何感知混合求解器在实际应用中表现出更好的鲁棒性和效率。

Abstract: The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.

</details>


### [111] [Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets](https://arxiv.org/abs/2512.14615)
*Omid Khormali*

Main category: cs.LG

TL;DR: 提出了一种新的拓扑数据分析方法OW-HNPV，用于检测时变网络中的异常。该方法首次从速度角度分析持久图，测量特征出现和消失的速率，并通过重叠加权自动降噪。在以太坊交易网络上的应用显示，该方法在加密货币异常检测方面优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要测量累积拓扑存在，缺乏对拓扑变化速度的考量。需要一种能够捕捉动态网络结构异常的新方法，特别是在加密货币交易网络这样的时变系统中。

Method: 提出重叠加权分层归一化持久速度（OW-HNPV），这是第一个基于速度的持久图分析方法。通过测量特征出现和消失的速率，并使用重叠加权自动降低噪声影响。方法具有数学稳定性，即使在比较不同特征类型的网络时也能保持可控和可预测的行为。

Result: 在以太坊交易网络（2017年5月-2018年5月）上的应用表明，OW-HNPV在加密货币异常检测方面表现优异，在7天价格变动预测中比基线模型获得高达10.4%的AUC提升。与VAB、持久景观、持久图像等现有方法相比，速度基方法在中长期预测（4-7天）中表现突出，OW-HNPV在各个预测时间范围内提供最一致和稳定的性能。

Conclusion: 建模拓扑速度对于检测动态网络中的结构异常至关重要。OW-HNPV作为一种新颖的速度基拓扑数据分析方法，在时变网络异常检测中展现出优越性能，特别是在加密货币交易网络的应用中。

Abstract: We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.

</details>


### [112] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: QR-MAX：首个基于模型的RL算法，用于离散非马尔可夫奖励决策过程，通过奖励机实现马尔可夫转移学习与非马尔可夫奖励处理的分解，获得多项式样本复杂度的PAC收敛保证。


<details>
  <summary>Details</summary>
Motivation: 许多实际决策问题涉及依赖于整个系统历史的任务，而不仅仅是达到具有期望属性的状态。马尔可夫强化学习方法不适合此类任务，而非马尔可夫奖励决策过程（NMRDPs）使智能体能够处理时间依赖任务，但该方法长期以来缺乏关于（近似）最优性和样本效率的形式化保证。

Method: 提出QR-MAX算法，一种基于模型的离散NMRDPs算法，通过奖励机将马尔可夫转移学习与非马尔可夫奖励处理进行分解。然后扩展到连续状态空间，提出Bucket-QR-MAX，使用SimHash-based离散化器保持相同的分解结构，无需手动网格化或函数逼近。

Result: QR-MAX是首个基于模型的RL算法，能够利用这种分解获得多项式样本复杂度的ε-最优策略的PAC收敛。在复杂度递增的环境上与最先进的基于模型RL方法进行实验比较，显示出样本效率的显著提升和寻找最优策略的鲁棒性增强。

Conclusion: 该工作解决了NMRDPs中长期缺乏的形式化保证问题，提供了具有多项式样本复杂度的PAC收敛算法，并通过扩展到连续状态空间展示了方法的实用性和优越性能。

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [113] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: ParaFormer通过PageRank增强注意力机制解决图Transformer中的过度平滑问题，在节点和图分类任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 图Transformer虽然能捕获全局信息，但研究发现其全局注意力机制存在严重的过度平滑问题，导致节点表示变得不可区分，甚至比传统GNN更严重。

Method: 提出PageRank Transformer (ParaFormer)，采用PageRank增强的注意力模块来模拟深度Transformer的行为，作为自适应滤波器缓解过度平滑。

Result: 在11个数据集（从数千到数百万节点）的节点分类和图分类任务中，ParaFormer均取得一致的性能提升。

Conclusion: ParaFormer通过PageRank增强注意力有效缓解了图Transformer的过度平滑问题，在各种规模的图数据集上表现出优越性能。

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [114] [Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks](https://arxiv.org/abs/2512.14675)
*Rae Chipera,Jenny Du,Irene Tsapara*

Main category: cs.LG

TL;DR: 非光滑激活函数（混沌、随机、分形）在回声状态网络中不仅保持回声状态特性，还能超越传统光滑函数，其中康托函数表现最佳，收敛速度比tanh和ReLU快2.6倍，光谱半径容忍度提高一个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统储层计算依赖光滑激活函数，但在国防、灾害响应、药物建模等极端条件下需要更鲁棒的系统。研究探索非光滑激活函数在回声状态网络中的潜力。

Method: 系统研究非光滑激活函数（混沌、随机、分形变体），通过36,610个储层配置的全面参数扫描，引入量化激活函数的理论框架，定义退化回声状态特性(d-ESP)，并证明d-ESP蕴含传统ESP。

Result: 多个非光滑函数保持ESP且性能优于传统光滑函数。康托函数ESP一致性行为可达光谱半径ρ~10（比光滑函数典型界限高一个数量级），收敛速度比tanh和ReLU快2.6倍。发现临界拥挤比Q=N/k预测离散激活函数失效阈值。

Conclusion: 预处理拓扑而非连续性本身决定稳定性：单调压缩预处理跨尺度保持ESP，而分散或不连续预处理引发急剧失效。某些分形函数的优异性能机制仍待解释，表明激活函数几何特性如何影响储层动态的理解存在根本性空白。

Abstract: Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.

</details>


### [115] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 开发了一个多模态机器学习框架EWI，通过整合临床和运营数据预测ICU入院、紧急响应团队派遣和死亡率的综合风险，并在医院仪表板中部署为分诊工具。


<details>
  <summary>Details</summary>
Motivation: 医院缺乏自动化系统来利用日益增长的异构临床和运营数据有效预测关键事件。早期识别有恶化风险的患者对于患者护理质量监测和医生护理管理至关重要，但由于数据格式不一致，将不同数据流转化为准确且可解释的风险评估面临重大挑战。

Method: 开发了多模态机器学习框架EWI，采用人机协同流程：临床医生帮助确定警报阈值和解释模型输出，使用SHAP可解释性方法突出显示驱动每个患者风险的临床和运营因素（如预定手术、病房人数）。从结构化和非结构化电子健康记录数据中自动提取特征。

Result: 在美国一家大型医院的18,633名独特患者数据集上，EWI实现了C统计量0.796。目前作为分诊工具用于主动管理高风险患者，将患者分为三个风险层级。

Conclusion: 该方法通过自动对不同风险水平的患者进行分类，为医生节省了宝贵时间，使他们能够专注于患者护理而非筛选复杂的EHR数据。通过进一步确定特定风险驱动因素，为护理人员排班和关键资源分配提供数据支持的调整，从而避免下游并发症并改善整体患者流程。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [116] [Writing in Symbiosis: Mapping Human Creative Agency in the AI Era](https://arxiv.org/abs/2512.13697)
*Vivan Doshi,Mengyuan Li*

Main category: cs.CY

TL;DR: 论文研究人类与AI在创意写作中的协同进化模式，挑战了风格同质化的普遍观点，发现存在"双轨进化"现象：主题围绕AI话题趋同，但风格呈现结构化分化。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，人类与有说服力和创造力的机器之间日益增长的共生关系引发了关于"何以为人"的关键问题。论文旨在研究人类创作与AI能力如何共同进化，挑战关于风格同质化的普遍观点。

Method: 使用跨越LLM时代前后的纵向写作数据大规模语料库进行分析，识别人类与AI协同进化的模式。通过分析作者风格与AI风格的相似性变化，构建"创意原型地图"。

Result: 观察到"双轨进化"模式：主题围绕AI相关话题趋同，但风格呈现结构化分化。识别出三种适应模式：作者风格与AI风格相似度增加、相似度减少、以及风格稳定但参与AI主题。

Conclusion: 创意原型地图揭示了作者身份如何与AI共同进化，为人类-AI协作、检测挑战和创意多样性保护等讨论做出贡献，表明人类创造力正在以复杂方式适应AI能力。

Abstract: The proliferation of Large Language Models (LLMs) raises a critical question about what it means to be human when we share an increasingly symbiotic relationship with persuasive and creative machines. This paper examines patterns of human-AI coevolution in creative writing, investigating how human craft and agency are adapting alongside machine capabilities. We challenge the prevailing notion of stylistic homogenization by examining diverse patterns in longitudinal writing data. Using a large-scale corpus spanning the pre- and post-LLM era, we observe patterns suggestive of a "Dual-Track Evolution": thematic convergence around AI-related topics, coupled with structured stylistic differentiation. Our analysis reveals three emergent adaptation patterns: authors showing increased similarity to AI style, those exhibiting decreased similarity, and those maintaining stylistic stability while engaging with AI-related themes. This Creative Archetype Map illuminates how authorship is coevolving with AI, contributing to discussions about human-AI collaboration, detection challenges, and the preservation of creative diversity.

</details>


### [117] [Adaptive Merit Framework: Merit-Anchored Fairness via SES Correction](https://arxiv.org/abs/2512.13698)
*Jung-Ah Lee*

Main category: cs.CY

TL;DR: 本文提出自适应择优框架(AMF)，通过动态阈值和连续社会经济地位测量，在保持择优标准的同时识别潜在人才，解决大学招生中择优与公平的结构性矛盾。


<details>
  <summary>Details</summary>
Motivation: 全球大学招生系统面临择优与公平的结构性矛盾。传统的公平干预措施（如平权行动、分类配额、基于代理的定向）依赖粗糙指标，采用固定配额导致零和博弈，缺乏透明决策规则。

Method: 提出自适应择优框架(AMF)，包含三个核心组件：1) 择优锚定架构-条件录取者必须达到与常规录取者相同的阈值；2) 动态阈值-锚定于最后一名常规录取者的原始分数；3) 直接连续的社会经济地位测量-通过行政数据验证。

Result: 使用韩国PISA 2022数据集验证，AMF在α=5,10,15时分别识别出4、6、9名额外录取者（占队列0.06-0.14%）。使用OECD抽样权重的总体加权估计显示，实际规模更大，分别约为491、603、760名额外录取者。所有条件录取者都超过择优阈值0.16-6.14分。

Conclusion: AMF不仅解决社会经济地位相关的修正问题，还提供了统一招生架构的设计模板，取代碎片化的公平轨道，支持多维评估框架，在保持择优标准的同时识别被抑制的潜力。

Abstract: College admissions systems worldwide continue to face a structural tension between meritocracy and equity. Conventional fairness interventions--affirmative action, categorical quotas, and proxy-based targeting--often rely on coarse indicators (e.g., race or region), operate within fixed quotas that induce zero-sum trade-offs, and lack transparent decision rules. This paper introduces the Adaptive Merit Framework (AMF), a policy-engineered mechanism that recognizes latent potential while preserving merit-based thresholds. AMF integrates three components: (1) a merit-anchored architecture in which conditional admits must exceed the same threshold as regular admits, (2) a dynamic threshold anchored to the raw score of the last regular admit, and (3) direct, continuous SES measurement verified through administrative data.
  Empirical validation using the full PISA 2022 Korea dataset (N=6,377) shows that AMF identifies 4, 6, and 9 additional admits under alpha = 5, 10, and 15 respectively (0.06-0.14% of cohort). Population-weighted estimates using OECD sampling weights suggest that the real-world scale of conditional admits is modestly larger than the raw sample counts, yielding approximately 491, 603, and 760 additional admits under alpha = 5, 10, and 15. All conditional admits exceed the merit threshold by 0.16 to 6.14 points, indicating that AMF recognizes suppressed performance rather than relaxing standards.
  Beyond SES-based corrections, AMF provides a design template for unified admissions architectures that replace fragmented equity tracks and support multi-dimensional evaluation frameworks.

</details>


### [118] [Us-vs-Them bias in Large Language Models](https://arxiv.org/abs/2512.13699)
*Tabia Tanzin Prama,Julia Witte Zimmerman,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CY

TL;DR: 研究发现大型语言模型存在"我们vs他们"的社会认同偏见，这种偏见在默认设置和角色设定下都普遍存在，且角色设定会系统性地改变模型的语言模式。研究还提出了ION方法来减轻这种偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型是否表现出社会认同理论描述的"我们vs他们"偏见，以及角色设定如何影响这种偏见。这有助于理解LLMs如何内化和再现社会群体的态度、世界观和认知风格。

Method: 使用情感动态分析、异质测量学和嵌入回归方法，在多种LLM架构（GPT-4.1、DeepSeek-3.1、Gemma-2.0、Grok-3.0、LLaMA-3.1）上分析默认和角色设定条件下的偏见表现。还开发了ION方法，使用微调和直接偏好优化来减轻偏见。

Result: 研究发现基础LLMs普遍存在内群体积极和外群体消极的关联。角色设定会系统性地改变模型的语言模式：保守角色表现出更强的外群体敌意，自由角色表现出更强的内群体团结。针对外群体的提示使敌意偏见增加1.19-21.76%。ION方法能将情感分歧减少高达69%。

Conclusion: LLMs不仅学习社会群体的事实关联，还内化和再现了不同的存在方式，包括态度、世界观和认知风格。研究证明了局部上下文、可定位表征和全局认知倾向之间的多尺度耦合。ION方法展示了针对性减轻偏见的潜力。

Abstract: This study investigates ``us versus them'' bias, as described by Social Identity Theory, in large language models (LLMs) under both default and persona-conditioned settings across multiple architectures (GPT-4.1, DeepSeek-3.1, Gemma-2.0, Grok-3.0, and LLaMA-3.1). Using sentiment dynamics, allotaxonometry, and embedding regression, we find consistent ingroup-positive and outgroup-negative associations across foundational LLMs. We find that adopting a persona systematically alters models' evaluative and affiliative language patterns. For the exemplar personas examined, conservative personas exhibit greater outgroup hostility, whereas liberal personas display stronger ingroup solidarity. Persona conditioning produces distinct clustering in embedding space and measurable semantic divergence, supporting the view that even abstract identity cues can shift models' linguistic behavior. Furthermore, outgroup-targeted prompts increased hostility bias by 1.19--21.76\% across models. These findings suggest that LLMs learn not only factual associations about social groups but also internalize and reproduce distinct ways of being, including attitudes, worldviews, and cognitive styles that are activated when enacting personas. We interpret these results as evidence of a multi-scale coupling between local context (e.g., the persona prompt), localizable representations (what the model ``knows''), and global cognitive tendencies (how it ``thinks''), which are at least reflected in the training data. Finally, we demonstrate ION, an ``us versus them'' bias mitigation approach using fine-tuning and direct preference optimization (DPO), which reduces sentiment divergence by up to 69\%, highlighting the potential for targeted mitigation strategies in future LLM development.

</details>


### [119] [Enhancing Transparency and Traceability in Healthcare AI: The AI Product Passport](https://arxiv.org/abs/2512.13702)
*A. Anil Sinaci,Senan Postaci,Dogukan Cavdaroglu,Machteld J. Boonstra,Okan Mercan,Kerem Yilmaz,Gokce B. Laleci Erturkmen,Folkert W. Asselbergs,Karim Lekadir*

Main category: cs.CY

TL;DR: 开发AI产品护照框架，通过基于生命周期的文档化提升医疗AI的透明度、可追溯性和合规性，实现标准化管理。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI领域缺乏透明度、可追溯性和合规性标准的问题，满足欧盟AI法案、FDA指南等监管要求，建立可信的AI部署框架。

Method: 基于AI4HF项目开发，分析监管框架和现有标准，设计关系数据模型捕获AI生命周期各阶段元数据，整合MLOps/ModelOps概念，通过多方利益相关者共创反馈，使用Python库实现自动化溯源追踪。

Result: 成功开发了基于标准的关系数据模型和Web平台，支持可审计文档化，生成机器和人类可读报告，符合FUTURE-AI原则，详细记录模型目的、数据来源、性能和部署上下文，代码开源在GitHub。

Conclusion: AI产品护照填补了医疗AI透明度空白，满足监管和伦理需求，开源特性和标准对齐促进信任和适应性，未来将整合FAIR数据原则和FHIR提升互操作性，推动负责任AI部署。

Abstract: Objective: To develop the AI Product Passport, a standards-based framework improving transparency, traceability, and compliance in healthcare AI via lifecycle-based documentation. Materials and Methods: The AI Product Passport was developed within the AI4HF project, focusing on heart failure AI tools. We analyzed regulatory frameworks (EU AI Act, FDA guidelines) and existing standards to design a relational data model capturing metadata across AI lifecycle phases: study definition, dataset preparation, model generation/evaluation, deployment/monitoring, and passport generation. MLOps/ModelOps concepts were integrated for operational relevance. Co-creation involved feedback from AI4HF consortium and a Lisbon workshop with 21 diverse stakeholders, evaluated via Mentimeter polls. The open-source platform was implemented with Python libraries for automated provenance tracking. Results: The AI Product Passport was designed based on existing standards and methods with well-defined lifecycle management and role-based access. Its implementation is a web-based platform with a relational data model supporting auditable documentation. It generates machine- and human-readable reports, customizable for stakeholders. It aligns with FUTURE-AI principles (Fairness, Universality, Traceability, Usability, Robustness, Explainability), ensuring fairness, traceability, and usability. Exported passports detail model purpose, data provenance, performance, and deployment context. GitHub-hosted backend/frontend codebases enhance accessibility. Discussion and Conclusion: The AI Product Passport addresses transparency gaps in healthcare AI, meeting regulatory and ethical demands. Its open-source nature and alignment with standards foster trust and adaptability. Future enhancements include FAIR data principles and FHIR integration for improved interoperability, promoting responsible AI deployment.

</details>


### [120] [Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs](https://arxiv.org/abs/2512.13723)
*David Haslett,Linus Ta-Lun Huang,Leila Khalatbari,Janet Hui-wen Hsiao,Antoni B. Chan*

Main category: cs.CY

TL;DR: 研究发现中美两国开发的AI语言模型在道德价值观上都更接近美国人而非中国人，即使是中国开发的模型也是如此，这对地缘政治中的规范性影响有重要启示。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在信息获取和决策中扮演越来越重要的角色，它们成为中美等全球行为体软实力竞争的工具。目前关于语言模型伦理偏见的证据主要来自美国公司开发的模型，而中国也开发了多个先进模型，需要研究这些模型与中美两国人民价值观的对齐情况。

Method: 研究选取了10个中国模型和10个美国模型，使用道德基础问卷2.0和世界价值观调查作为评估工具，比较这些模型的回答与数千名中美民众的回答。还测试了使用中文提示或强加中国身份对模型回答的影响。

Result: 所有模型（包括中国开发的模型）对两项调查的回答都更接近美国人而非中国人。即使使用中文提示或强加中国身份，这种偏向美国价值观的趋势也只有轻微缓解。

Conclusion: 大型语言模型普遍偏向美国价值观，即使是中国开发的模型也是如此，这对未来语言模型生成内容并塑造地缘政治规范性影响有重要启示。

Abstract: As large language models increasingly mediate access to information and facilitate decision-making, they are becoming instruments in soft power competitions between global actors such as the United States and China. So far, language models seem to be aligned with the values of Western countries, but evidence for this ethical bias comes mostly from models made by American companies. The current crop of state-of-the-art models includes several made in China, so we conducted the first large-scale investigation of how models made in China and the USA align with people from China and the USA. We elicited responses to the Moral Foundations Questionnaire 2.0 and the World Values Survey from ten Chinese models and ten American models, and we compared their responses to responses from thousands of Chinese and American people. We found that all models respond to both surveys more like American people than like Chinese people. This skew toward American values is only slightly mitigated when prompting the models in Chinese or imposing a Chinese persona on the models. These findings have important implications for a near future in which large language models generate much of the content people consume and shape normative influence in geopolitics.

</details>


### [121] [Exploring the Modular Integration of "AI + Architecture" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University](https://arxiv.org/abs/2512.13730)
*Wang Jiaqi,Lan Yi,Chen Xiang*

Main category: cs.CY

TL;DR: 浙江大学建筑教育中AI整合的教学实验，通过双模块框架（AI技术培训+伦理讨论）提升学生数字技能和战略认知


<details>
  <summary>Details</summary>
Motivation: 研究AI技术在建筑教育中的整合方法，探索如何在保持原有课程结构的同时，有效引入AI工具并兼顾伦理考量

Method: 采用双模块教学框架：20小时AI技术培训（深度学习模型、LLMs、AIGC、LoRA、ComfyUI）和嵌入式伦理讨论，配备专门技术指导教师，保持原有课程结构

Result: 分阶段指导、技术-伦理平衡方法和机构支持有效提升了学生的数字技能和战略认知，同时解决了AI伦理问题

Conclusion: 该模型为设计教育提供了一种可复制的技术学习与批判性学习相结合的方法，成功整合了AI技术培训与伦理教育

Abstract: This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.

</details>


### [122] [Instilling Organisational Values in Firefighters through Simulation-Based Training](https://arxiv.org/abs/2512.13737)
*Nardine Osman,Manel Rodriguez-Soto,Jordi Sabater-Mir*

Main category: cs.CY

TL;DR: 提出将部门价值观系统整合到消防员模拟训练中的概念框架，以提升高压环境下的伦理决策能力


<details>
  <summary>Details</summary>
Motivation: 传统消防训练方法在应对紧急环境中的复杂伦理困境和价值冲突方面存在不足，高压决策具有重大伦理影响，需要更好的训练方法

Method: 提出概念框架，将部门价值观系统整合到模拟训练中，促进价值观内化和压力下的价值驱动决策

Result: 框架能够促进更深层的价值观内化，改善压力下的决策质量，同时工具还可用于评估和改进部门操作协议

Conclusion: 通过将部门价值观整合到模拟训练中，可以提升消防员在紧急情况下的伦理决策能力，同时优化部门操作协议与价值观的一致性

Abstract: In firefighting and other emergency operations, decisions made under pressure carry profound ethical weight and can significantly impact incident outcomes and firefighter safety. Traditional training methods, while foundational, often fall short in adequately preparing firefighters for the complex ethical dilemmas and value conflicts inherent in chaotic emergency environments. This paper proposes a conceptual framework for enhancing firefighter training by systematically integrating departmental values into simulation-based training. This approach fosters deeper value internalisation and improves value-driven decision-making under pressure. Furthermore, the underlying tools can also be leveraged to evaluate and refine departmental operational protocols for better alignment with preferred values.

</details>


### [123] [The algorithmic muse and the public domain: Why copyrights legal philosophy precludes protection for generative AI outputs](https://arxiv.org/abs/2512.13750)
*Ezieddin Elmahjub*

Main category: cs.CY

TL;DR: 生成式AI输出不应受版权保护，因为其缺乏人类直接创作链接，授予版权会损害公共领域和创新


<details>
  <summary>Details</summary>
Motivation: 重新评估版权哲学基础，探讨生成式AI输出是否应受版权保护，避免传统法律分析框架的局限性

Method: 绕过传统法律教义分析，从版权哲学基础出发，分析功利主义激励、劳动应得和人格理论对AI输出的适用性

Result: 生成式AI输出不应获得版权保护，因为其切断了人类与表达形式的直接创作链接，传统版权理论无法提供合理依据

Conclusion: 应明确区分：人类对AI生成作品的创造性贡献可能值得保护，但原始算法输出应保留在公共领域

Abstract: Generative AI (GenAI) outputs are not copyrightable. This article argues why. We bypass conventional doctrinal analysis that focuses on black letter law notions of originality and authorship to re-evaluate copyright's foundational philosophy. GenAI fundamentally severs the direct human creative link to expressive form. Traditional theories utilitarian incentive, labor desert and personality fail to provide coherent justification for protection. The public domain constitutes the default baseline for intellectual creations. Those seeking copyright coverage for GenAI outputs bear the burden of proof. Granting copyright to raw GenAI outputs would not only be philosophically unsound but would also trigger an unprecedented enclosure of the digital commons, creating a legal quagmire and stifling future innovation. The paper advocates for a clear distinction: human creative contributions to AI-generated works may warrant protection, but the raw algorithmic output should remain in the public domain.

</details>


### [124] [OpenProposal Platform for Transparent Research Funding Review](https://arxiv.org/abs/2512.13754)
*Sakshi Ahuja,Subhankar Mishra*

Main category: cs.CY

TL;DR: OpenProposal是一个基于Web的透明研究资助提案评审平台原型，借鉴OpenReview的透明度原则，探索公开评审、作者反驳和透明决策的技术可行性。


<details>
  <summary>Details</summary>
Motivation: 当前研究资助分配过程缺乏透明度，传统资助机构的封闭评审系统限制了问责制和系统性改进，而学术论文同行评审的透明度革命尚未应用于资助提案评审。

Method: 使用Next.js、React和Prisma等现代Web技术构建OpenProposal平台原型，实现公开评审、作者反驳和透明决策机制，同时尝试保护预算等敏感信息。

Result: 开发了一个功能原型，展示了透明资助评审的技术可行性，提供了社区参与、评审者问责和潜在数据驱动洞察的机制，为透明资助评审奠定了技术基础。

Conclusion: 透明资助评审有潜力增强科学诚信和改进研究资助决策，但需要实证验证。这项工作为透明资助评审提供了技术基础，并确定了资助背景下同行评审机制的未来设计考虑因素。

Abstract: Research funding allocation remains a critical bottleneck in scientific advancement, yet the review process for funding proposals lacks the transparency that has revolutionized academic paper peer review. Traditional funding agencies operate with closed review systems, limiting accountability and preventing systematic improvements. We present OpenProposal, a proof-of-concept web-based platform that explores how transparency principles from OpenReview might be adapted to research funding proposal evaluation. Built using modern web technologies including Next.js , React , and Prisma , OpenProposal demonstrates the technical feasibility of public reviews, author rebuttals, and transparent decision-making while attempting to protect sensitive information such as budgets. Our platform prototype addresses key limitations identified in current funding systems by providing mechanisms for community engagement, reviewer accountability, and potential data-driven insights into peer review processes. Through system design and implementation, we explore how transparent funding review could potentially enhance scientific integrity and improve research funding decisions, though empirical validation remains necessary. This work contributes a technical foundation for transparent funding review and identifies design considerations for future research on peer review mechanisms in funding contexts.

</details>


### [125] [Beyond Procedural Compliance: Human Oversight as a Dimension of Well-being Efficacy in AI Governance](https://arxiv.org/abs/2512.13768)
*Yao Xie,Walter Cullen*

Main category: cs.CY

TL;DR: 论文提出将人类监督视为一种幸福能力，整合AI素养、伦理辨别和人类需求意识，强调通过教育系统培养这一能力以实现AI伦理监管目标。


<details>
  <summary>Details</summary>
Motivation: 当前主要AI伦理指南和法律（如欧盟AI法案）要求有效的人类监督，但未将其定义为可发展的独立能力。需要建立理论框架将监管目标转化为可培养的人类能力。

Method: 引入人类监督作为幸福能力的概念，将其置于新兴的幸福效能框架中。整合AI素养、伦理辨别和人类需求意识，强调识别和约束问题需求的能力。

Result: 建立了人类监督作为幸福能力的理论框架，为从高层监管目标到实际人类能力培养提供了实践路径，为未来教学实施和实证验证奠定基础。

Conclusion: 可持续且具成本效益的人类监督能力发展依赖于将其整合到各级教育中，从专业培训到终身学习。这一框架对于培养AI安全和伦理所需的人类能动性和责任至关重要。

Abstract: Major AI ethics guidelines and laws, including the EU AI Act, call for effective human oversight, but do not define it as a distinct and developable capacity. This paper introduces human oversight as a well-being capacity, situated within the emerging Well-being Efficacy framework. The concept integrates AI literacy, ethical discernment, and awareness of human needs, acknowledging that some needs may be conflicting or harmful. Because people inevitably project desires, fears, and interests into AI systems, oversight requires the competence to examine and, when necessary, restrain problematic demands.
  The authors argue that the sustainable and cost-effective development of this capacity depends on its integration into education at every level, from professional training to lifelong learning. The frame of human oversight as a well-being capacity provides a practical path from high-level regulatory goals to the continuous cultivation of human agency and responsibility essential for safe and ethical AI. The paper establishes a theoretical foundation for future research on the pedagogical implementation and empirical validation of well-being effectiveness in multiple contexts.

</details>


### [126] [Assessing High-Risk Systems: An EU AI Act Verification Framework](https://arxiv.org/abs/2512.13907)
*Alessio Buscemi,Tom Deckenbrunnen,Fahria Kabir,Nishat Mowla,Kateryna Mishchenko*

Main category: cs.CY

TL;DR: 提出欧盟AI法规实施的系统化验证框架，通过控制与测试方法、数据/模型/流程/产品评估维度，连接法律要求与技术实践，减少解释不确定性


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案等法规实施面临缺乏系统性验证方法的挑战，监管模糊性导致成员国准备程度不一致，需要连接法律要求与技术实践的桥梁

Method: 提出综合性框架，沿两个维度组织合规验证：方法类型（控制vs测试）和评估目标（数据、模型、流程、最终产品），并将核心法律要求映射到具体验证活动

Result: 框架作为政策制定者与实践者之间的重要桥梁，将法律文本与技术标准和最佳实践对齐，减少解释不确定性，促进评估实践一致性

Conclusion: 该框架有助于填补AI法规实施中的验证空白，支持监管、伦理和技术视角在整个AI生命周期中的协调一致

Abstract: A central challenge in implementing the AI Act and other AI-relevant regulations in the EU is the lack of a systematic approach to verify their legal mandates. Recent surveys show that this regulatory ambiguity is perceived as a significant burden, leading to inconsistent readiness across Member States. This paper proposes a comprehensive framework designed to help close this gap by organising compliance verification along two fundamental dimensions: the type of method (controls vs. testing) and the target of assessment (data, model, processes, and final product). Additionally, our framework maps core legal requirements to concrete verification activities, serving as a vital bridge between policymakers and practitioners, and aligning legal text with technical standards and best practices. The proposed approach aims to reduce interpretive uncertainty, promote consistency in assessment practices, and support the alignment of regulatory, ethical, and technical perspectives across the AI lifecycle.

</details>


### [127] [Research Opportunities and Challenges of the EU's Digital Services Act](https://arxiv.org/abs/2512.14223)
*Francesco Pierri,Theo Araujo,Sanne Kruikemeier,Philipp Lorenz-Spreen,Mariek M. P. Vanden Abeele,Laura Vandenbosch,Joana Gonçalves-Sa,Przemyslaw A. Grabowicz*

Main category: cs.CY

TL;DR: 欧盟《数字服务法》第40条为研究人员提供平台数据访问权限，但法律、技术和组织障碍仍阻碍系统性在线风险研究，本文分析挑战并提出改进措施


<details>
  <summary>Details</summary>
Motivation: 欧盟《数字服务法》（DSA）第40条为研究人员提供了访问大型在线平台数据的框架，旨在提高平台透明度和问责制。然而，尽管有这一法律框架，研究人员在实际获取和使用平台数据时仍面临多重障碍，这影响了系统性在线风险研究的有效性。

Method: 本文通过分析《数字服务法》第40条的实施过程，识别出阻碍研究人员有效访问平台数据的关键挑战，包括法律、技术和组织层面的障碍。基于这些分析，作者提出了具体的实践措施来改进数据访问机制。

Result: 研究发现，虽然《数字服务法》第40条建立了法律框架，但实际执行中存在显著障碍：法律层面包括数据保护合规、知识产权问题；技术层面涉及数据格式标准化、API访问限制；组织层面包括平台配合度、审批流程复杂等。

Conclusion: 为确保《数字服务法》实现其透明度和问责目标，需要采取具体措施克服现有障碍，包括简化法律程序、制定技术标准、改善平台合作机制，以及建立更有效的研究数据访问框架。

Abstract: The Digital Services Act (DSA) introduced by the European Union in 2022 offers a landmark framework for platform transparency, with Article 40 enabling vetted researchers to access data from major online platforms. Yet significant legal, technical, and organizational barriers still hinder effective research on systemic online risks. This piece outlines the key challenges emerging from the Article 40 process and proposes practical measures to ensure that the DSA fulfills its transparency and accountability goals.

</details>


### [128] [Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study](https://arxiv.org/abs/2512.14330)
*Sahibpreet Singh,Manjit Singh*

Main category: cs.CY

TL;DR: 该研究通过比较美国、德国、英国、中国和印度的法律框架，分析了自动驾驶车辆犯罪责任归属问题，发现各国监管碎片化，呼吁建立全球统一的法律标准。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术革命带来了复杂的犯罪责任归属问题，需要明确人类错误、AI道德主体性和主要责任方的认定标准，以促进技术创新同时确保风险最小化。

Method: 采用比较法律分析方法，研究五个国家的成文法、实际责任索赔案例和学术文献，这些国家因其技术先进性和监管方法差异而被选择。

Result: 研究发现监管格局碎片化：印度和美国依赖松散的各州法律网络，英国颁布了开创性的《自动和电动汽车法案2018》，德国执行严格安全标准并根据车辆运行模式区分责任，中国也追求严格的责任制度。

Conclusion: 全球统一的法律标准对于促进技术创新、确保最低风险和明确责任归属至关重要。

Abstract: AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [129] [Fast Test Inversion for Resampling Methods](https://arxiv.org/abs/2512.14024)
*Ian Xu*

Main category: econ.EM

TL;DR: 提出一种基于代数结构的高效置信区间构建方法，避免传统网格搜索的计算负担和离散化导致的保守性


<details>
  <summary>Details</summary>
Motivation: 传统基于随机化的推断依赖网格搜索构建置信区间，计算量大且因离散化导致区间保守。需要更高效精确的方法

Method: 将随机化统计量表达为感兴趣参数的有理函数，解析识别检验统计量秩变化的临界值，推导精确p值曲线，无需穷举计算

Result: 显著降低计算负担，克服传统网格搜索的局限性，为随机化推断提供实用高效的置信区间和区域构建方案

Conclusion: 该方法利用检验统计量的代数结构，实现了比传统网格搜索更高效的精确置信区间构建，适用于多种随机化检验

Abstract: Randomization-based inference commonly relies on grid search methods to construct confidence intervals by inverting hypothesis tests over a range of parameter values. While straightforward, this approach is computationally intensive and can yield conservative intervals due to discretization. We propose a novel method that exploits the algebraic structure of a broad class of test statistics--including those with variance estimators dependent on the null hypothesis--to produce exact confidence intervals efficiently. By expressing randomization statistics as rational functions of the parameter of interest, we analytically identify critical values where the test statistic's rank changes relative to the randomization distribution. This characterization allows us to derive the exact p-value curve and construct precise confidence intervals without exhaustive computation. For cases where the parameter of interest is a vector and a confidence region is needed, our method extends by calculating and storing the coefficients of the polynomial functions involved. This approach enables us to compute approximate p-value functions and confidence regions more efficiently than traditional grid search methods, as we avoid recalculating test statistics from scratch for each parameter value. We illustrate our method using tests from Pouliot (2024) and extend it to other randomization tests, such as those developed by DiCiccio and Romano (2017) and D'Haultfœuille and Tuvaandorj (2024). Our approach significantly reduces computational burden and overcomes the limitations of traditional grid search methods, providing a practical and efficient solution for confidence interval and region construction in randomization-based inference.

</details>


### [130] [Heterogeneous Effects of Endogenous Treatments with Interference and Spillovers in a Large Network](https://arxiv.org/abs/2512.14515)
*Lin Chen,Yuya Sasaki*

Main category: econ.EM

TL;DR: 该论文研究在大型单网络环境下，存在干扰和溢出效应时，内生处理的异质性效应的识别与估计。作者开发了考虑溢出效应的内生处理选择均衡模型，识别了异质边际暴露效应，并应用于分析进口竞争对美国地方劳动力市场的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理内生性和网络效应方面存在局限，特别是在大型网络环境下，同时考虑内生处理选择、干扰和溢出效应的异质性效应识别方法不足。需要开发能够处理这些复杂性的计量框架。

Method: 将内生处理选择建模为明确考虑溢出效应的均衡结果，推导保证均衡存在性和唯一性的条件。识别异质边际暴露效应，开发估计策略并建立大样本性质。最后将方法应用于分析进口竞争对美国地方劳动力市场的影响。

Result: 发现了负的边际暴露效应，与现有文献一致。但发现这些效应在存在已处理邻居的情况下会被溢出效应放大，且在倾向于选择较低进口竞争水平的地区中更为明显。这些额外的实证发现是新颖的，没有本文提出的计量框架就无法可靠获得。

Conclusion: 本文提出的计量框架能够识别和估计在干扰和溢出效应存在时的内生处理异质性效应，为分析网络环境下的政策效应提供了重要工具。实证应用揭示了进口竞争影响的新维度，强调了考虑网络效应的重要性。

Abstract: This paper studies the identification and estimation of heterogeneous effects of an endogenous treatment under interference and spillovers in a large single-network setting. We model endogenous treatment selection as an equilibrium outcome that explicitly accounts for spillovers and derive conditions guaranteeing the existence and uniqueness of this equilibrium. We then identify heterogeneous marginal exposure effects (MEEs), which may vary with both the treatment status of neighboring nodes and unobserved heterogeneity. We develop estimation strategies and establish their large-sample properties. Equipped with these tools, we analyze the heterogeneous effects of import competition on U.S. local labor markets in the presence of interference and spillovers. We find negative MEEs, consistent with the existing literature. However, these effects are amplified by spillovers in the presence of treated neighbors and among localities that tend to select into lower levels of import competition. These additional empirical findings are novel and would not be credibly obtainable without the econometric framework proposed in this paper.

</details>


### [131] [Estimating Program Participation with Partial Validation](https://arxiv.org/abs/2512.14616)
*Augustine Denteh,Pierre E. Nguimkeu*

Main category: econ.EM

TL;DR: 论文研究二元选择模型的估计问题，当调查回答可能存在误分类但其中一个响应类别可以被验证时。通过部分验证数据，将双向误分类问题转化为单向问题，提出克服误分类偏差的一致估计方法。


<details>
  <summary>Details</summary>
Motivation: 调查数据中经常存在误分类问题，特别是二元选择模型（如是否参与某项活动）。传统方法需要完全验证数据，成本高昂且不完美。当只有部分响应类别能被验证时（如通过后续问题验证特定回答），需要开发能够利用这种部分验证信息来纠正误分类偏差的方法。

Method: 首先证明直接使用更新后的回答无法解决误分类偏差，并推导出一般条件下的渐近偏差。然后利用部分验证数据构建参与模型，提出一致且渐近正态的估计量来克服误分类误差。通过蒙特卡洛模拟验证有限样本性能，并以加纳健康保险覆盖率的决定因素作为实证案例。

Result: 提出的估计方法能够有效克服误分类偏差，在模拟中表现出良好的有限样本性能。实证分析展示了该方法在实际应用中的有效性。研究还讨论了调查问卷设计的意义，使研究者能够在不依赖昂贵且不完美的完全验证数据的情况下克服误分类偏差。

Conclusion: 当调查回答存在误分类但其中一个响应类别可以被验证时，本文提出的方法能够将双向误分类问题转化为单向问题，并提供一致且渐近正态的估计量。这种方法为利用部分验证数据纠正误分类偏差提供了实用解决方案，对调查问卷设计具有重要启示。

Abstract: This paper considers the estimation of binary choice models when survey responses are possibly misclassified but one of the response category can be validated. Partial validation may occur when survey questions about participation include follow-up questions on that particular response category. In this case, we show that the initial two-sided misclassification problem can be transformed into a one-sided one, based on the partially validated responses. Using the updated responses naively for estimation does not solve or mitigate the misclassification bias, and we derive the ensuing asymptotic bias under general conditions. We then show how the partially validated responses can be used to construct a model for participation and propose consistent and asymptotically normal estimators that overcome misclassification error. Monte Carlo simulations are provided to demonstrate the finite sample performance of the proposed and selected existing methods. We provide an empirical illustration on the determinants of health insurance coverage in Ghana. We discuss implications for the design of survey questionnaires that allow researchers to overcome misclassification biases without recourse to relatively costly and often imperfect validation data.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [132] [Fixed-Income Pricing and the Replication of Liabilities](https://arxiv.org/abs/2512.14662)
*Damir Filipović*

Main category: q-fin.MF

TL;DR: 提出无模型静态固定收益定价框架，证明无套利等价于存在正贴现曲线，研究负债复制与超复制，为贴现曲线构建和负债驱动投资提供统一基础


<details>
  <summary>Details</summary>
Motivation: 为固定收益定价和负债现金流复制提供一个无模型的静态框架，统一贴现曲线构建和负债驱动投资的理论基础，服务于经济资本评估和监管实践

Method: 建立静态无套利理论框架，证明市场无静态套利等价于存在严格正贴现曲线；研究负债复制和超复制问题，建立最小成本超复制组合的存在条件，并严格解释互换-回购复制

Result: 证明了无静态套利与正贴现曲线存在的等价性；建立了负债最小成本超复制组合的存在条件；为贴现曲线构建和负债驱动投资提供了统一的理论基础

Conclusion: 该框架为固定收益定价和负债复制提供了坚实的理论基础，对经济资本评估和监管实践具有直接应用价值，统一了贴现曲线构建和负债驱动投资的理论体系

Abstract: This paper develops a model-free framework for static fixed-income pricing and the replication of liability cash flows. We show that the absence of static arbitrage across a universe of fixed-income instruments is equivalent to the existence of a strictly positive discount curve that reproduces all observed market prices. We then study the replication and super-replication of liabilities and establish conditions ensuring the existence of least-cost super-replicating portfolios, including a rigorous interpretation of swap--repo replication within this static framework. The results provide a unified foundation for discount-curve construction and liability-driven investment, with direct relevance for economic capital assessment and regulatory practice.

</details>


### [133] [Long-run survival in limited stock market participation models with power utilities](https://arxiv.org/abs/2512.14680)
*Heeyoung Kwon,Kasper Larsen*

Main category: q-fin.MF

TL;DR: 扩展Basak和Cuoco(1998)的有限参与模型，允许交易者具有不同的时间偏好系数但相同的CRRA风险厌恶系数，给出确保Radner均衡存在的参数限制，并进一步确保所有交易者长期生存


<details>
  <summary>Details</summary>
Motivation: 扩展经典有限参与模型，研究不同时间偏好但相同风险厌恶的交易者之间的均衡存在性和长期生存问题

Method: 扩展Basak和Cuoco(1998)的有限参与模型，引入不同时间偏好系数的交易者，使用Radner均衡框架，推导确保均衡存在的参数限制条件

Result: 给出了确保Radner均衡存在的参数限制条件，并进一步推导了确保所有交易者在长期都能生存的额外参数限制

Conclusion: 成功扩展了有限参与模型，为具有不同时间偏好但相同风险厌恶的交易者建立了均衡存在性和长期生存的理论条件

Abstract: We extend the limited participation model in Basak and Cuoco (1998) to allow for traders with different time-preference coefficients but identical constant relative risk-aversion coefficients. Our main result gives parameter restrictions which ensure the existence of a Radner equilibrium. As an application, we give further parameter restrictions which ensure all traders survive in the long run.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [134] [Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records](https://arxiv.org/abs/2512.13700)
*Mitchell A. Klusty,Elizabeth C. Solie,Caroline N. Leach,W. Vaiden Logan,Lynnet E. Richey,John C. Gensel,David P. Szczykutowicz,Bryan C. McLellan,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 提出一个基于本地部署大语言模型的自动化临床特征提取框架，用于从电子健康记录中提取结构化信息，减少人工图表审查负担。


<details>
  <summary>Details</summary>
Motivation: 临床研究中的人工图表审查非常耗时且资源密集，需要专家从非结构化的电子健康记录叙述中提取复杂信息，这阻碍了临床研究的效率。

Method: 开发了一个安全、模块化的框架，利用本地部署的大语言模型，结合检索增强生成和结构化响应方法，在符合HIPAA标准的计算基础设施上实现自动化特征提取。

Result: 该框架在评估中实现了高准确率，能够从大量患者笔记中提取多种医学特征，与专家标注数据集相比表现良好，甚至发现了人工审查中遗漏的标注错误。

Conclusion: 该框架展示了LLM系统通过自动化提取减少人工图表审查负担的潜力，提高了数据捕获的一致性，能够加速临床研究进程。

Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.

</details>


### [135] [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)
*Zheng Xing,Junting Chen*

Main category: cs.AI

TL;DR: 提出无需位置标签的盲无线电地图构建框架，通过MIMO-OFDM信道测量推断用户轨迹，实现室内定位和波束图重建。


<details>
  <summary>Details</summary>
Motivation: 传统无线电地图构建需要大量位置标签数据，成本高且在实际场景中不实用。需要一种无需位置标签的盲构建方法。

Method: 首先证明NLOS信道状态信息在准镜面环境模型下具有空间连续性，推导CSI-距离度量。开发空间正则化贝叶斯推理框架，联合估计信道特征、区分LOS/NLOS条件并恢复用户轨迹。

Result: 在射线追踪数据集上实验，平均定位误差0.68米，波束图重建误差3.3%，验证了盲映射方法的有效性。

Conclusion: 提出的盲无线电地图构建框架无需位置标签，通过理论分析和实验验证，在室内MIMO-OFDM系统中实现了准确的轨迹恢复和地图重建。

Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.

</details>


### [136] [Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents](https://arxiv.org/abs/2512.13704)
*Doohee You,Sundeep Paul*

Main category: cs.AI

TL;DR: Adjudicator是一个用于自动识别和纠正标签噪声的神经符号系统，通过构建动态知识图谱和基于LLM的多智能体辩论机制，在工业环境中实现高精度数据验证。


<details>
  <summary>Details</summary>
Motivation: 生产机器学习系统的性能受限于训练数据质量，高风险的工业应用中，噪声标签会降低性能并损害用户信任，需要自动化的标签噪声识别与纠正系统。

Method: 采用神经符号方法：1) 构建动态知识图谱统一项目上下文；2) 设计"智能体委员会"多智能体LLM架构，让专门化的智能体通过辩论和投票确定标签有效性；3) 引入基于知识图谱的覆盖逻辑来识别复杂结构性错误。

Result: 在AlleNoise基准的1000项平衡子集上，知识图谱增强模型达到0.99 F1分数，显著优于单LLM基线(0.48 F1)和非知识图谱委员会(0.59 F1)。系统通过知识图谱实现了复杂结构性错误的完美召回和精确识别。

Conclusion: Adjudicator为严格监管的工业环境提供了一个鲁棒且可解释的自动化高精度数据验证系统，证明了在工业环境中生成黄金数据集的可行性。

Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.

</details>


### [137] [LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms](https://arxiv.org/abs/2512.13713)
*Ali Parsaee,Yashar Talebirad,Csongor Szepesvári,Vishwajeet Ohal,Eden Redman*

Main category: cs.AI

TL;DR: 论文提出LoopBench基准，用于评估大语言模型在分布式对称性打破和元认知推理中的表现，特别是针对奇数环图着色问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地被用作自主代理，但它们在分布式系统中的协调能力仍未被充分理解。需要评估LLM在分布式对称性打破和元认知推理方面的能力。

Method: 引入LoopBench基准，专注于奇数环图（C3、C5、C11）的有限颜色着色问题。实现策略传递机制作为一致内存形式，测试LLM在无通信代理环境中的表现。

Result: 标准LLM和经典启发式方法在解决无限循环问题上表现不佳，而高级推理模型（如O3）能够设计出摆脱死锁的策略。LoopBench为研究基于语言推理的分布式算法提供了测试平台。

Conclusion: LoopBench基准能够有效评估LLM在分布式协调中的推理能力，为研究集体智能和基于语言推理的分布式算法提供了重要工具。

Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.

</details>


### [138] [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)
*Gangesh Pathak,Prasanna Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种基于AI的标注流水线，通过人机协同方法系统识别、标注和修复LLM输出的不稳定性模式，以解决LLM在高度监管行业中因不稳定性、幻觉和性能波动导致的应用限制问题。


<details>
  <summary>Details</summary>
Motivation: LLM在高度监管行业中的应用受到限制，主要原因是其不稳定性问题、不一致推理、幻觉和性能波动，特别是在工作流程中。这些可靠性问题阻碍了LLM在需要事实精确性和一致行为的领域的安全使用。现有的稳定化方法（如RLHF和监督微调）虽然能带来可量化的改进，但成本高昂且依赖大量人工标注，难以可持续扩展。

Method: 提出基于AI的标注流水线，采用人机协同方法：结合自动化弱监督模型和基于置信度的标注，辅以目标人工验证，确保反馈信息的可靠性和道德完整性。框架引入了语义一致性、事实正确性和逻辑连贯性等稳定性特定标注类别，通过反馈循环实现模型的持续校准和鲁棒性增强。

Result: 该方法能够系统性地识别和修复LLM输出的不稳定性模式，通过人机协同确保标注质量，同时降低对大量人工标注的依赖，提供了一种可持续扩展的LLM稳定化解决方案。

Conclusion: 提出的AI标注流水线为人机协同解决LLM稳定性问题提供了有效框架，通过结合自动化标注和人工验证，能够在保证质量的同时实现可持续扩展，为LLM在高度监管行业的可靠应用提供了新的技术路径。

Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).

</details>


### [139] [Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN](https://arxiv.org/abs/2512.13715)
*Fatemeh Lotfi,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 提出基于元分层强化学习的自适应框架，用于O-RAN中的资源分配和网络切片联合优化，相比基线方法提升19.8%的网络管理效率


<details>
  <summary>Details</summary>
Motivation: 现代应用复杂性增加需要无线网络具备实时适应性和高效资源管理能力。O-RAN架构及其RIC模块成为动态资源管理和网络切片的关键解决方案。现有AI方法在不可预测的动态条件下难以保持性能。

Method: 提出自适应元分层强化学习框架，受MAML启发，结合分层控制和元学习。高层控制器分配切片间资源，低层代理执行切片内调度。自适应元更新机制根据时序差分误差方差加权任务，提升稳定性并优先处理复杂网络场景。

Result: 理论分析证明了两级学习过程的次线性收敛和遗憾保证。仿真结果显示相比基线RL和元RL方法，网络管理效率提升19.8%，适应速度更快，在eMBB、URLLC和mMTC切片中QoS满意度更高。消融和可扩展性研究证实了方法的鲁棒性，适应速度提升达40%，在网络规模增大时保持一致的公平性、延迟和吞吐性能。

Conclusion: 提出的自适应元分层强化学习框架有效解决了O-RAN中动态资源分配和网络切片优化问题，在性能、适应速度和可扩展性方面显著优于现有方法。

Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.

</details>


### [140] [ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making](https://arxiv.org/abs/2512.13716)
*Yitong Luo,Ziang Chen,Hou Hei Lam,Jiayu zhan,Junqi Wang,Zhenliang Zhang,Xue Feng*

Main category: cs.AI

TL;DR: ValuePilot框架通过价值驱动的个性化决策，使AI代理能够根据用户价值偏好进行决策，在未见场景中超越主流LLM基线


<details>
  <summary>Details</summary>
Motivation: 随着AI系统扩展到现实应用，适应超越任务完成或集体对齐的个性化价值已成为关键挑战。需要开发能够根据个人价值偏好进行决策的AI代理，以增强人机交互的个性化对齐。

Method: 提出ValuePilot两阶段框架：1) 数据集生成工具包(DGT)：通过人-LLM协作流水线构建多样化的价值标注场景；2) 决策模块(DMM)：学习基于个人价值偏好评估行动，实现上下文敏感的个性化决策。

Result: 在未见场景评估中，DMM在与人行动选择对齐方面超越了GPT-5、Claude-Sonnet-4、Gemini-2-flash和Llama-3.1-70b等强大LLM基线。证明了价值驱动决策的有效性。

Conclusion: 价值驱动决策是构建可解释、个性化AI代理的有效且可扩展的工程路径。人类价值作为稳定、可转移的信号，支持跨情境的一致和可泛化行为。

Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.

</details>


### [141] [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)
*Steve Nwaiwu,Nipat Jongsawat,Anucha Tungkasthan*

Main category: cs.AI

TL;DR: 量化对LLM因果推理影响有限：NF4量化仅造成<1%性能下降，干预查询最敏感，反事实推理相对稳定，图增强可部分弥补量化损失


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型部署向边缘和资源受限环境转移，量化模型（如INT8和NF4）成为标准。然而，精度降低对形式化因果推理的影响尚不清楚，需要系统评估量化在Pearl因果阶梯所有三个层级上的影响。

Method: 使用3000个样本的分层CLadder基准，系统评估Llama 3 8B模型在不同量化精度下的因果推理能力。在Pearl因果阶梯的三个层级（关联、干预、反事实）进行测试，并在CRASS基准上验证，同时评估图检索增强生成技术。

Result: 量化对因果推理影响有限：NF4量化整体性能下降小于1%；干预查询对精度损失最敏感；反事实推理相对稳定但在特定查询类型（如碰撞偏倚、后门调整）存在异质性弱点；图增强可将NF4干预准确率提升1.7%；现有反事实基准未能揭示量化引起的推理漂移。

Conclusion: 因果推理对4位量化表现出意外的鲁棒性；图结构化增强可选择性强化干预推理；当前反事实基准未能捕捉深层因果脆弱性；研究为部署高效且结构支持的因果AI系统提供了实证指导和实用建议。

Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.

</details>


### [142] [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)
*TK Lee*

Main category: cs.AI

TL;DR: 论文提出了一种定性案例研究方法，用于审计大语言模型在长期交互中的行为选择性，发现模型在非敏感领域表现正常，但在政策敏感领域会选择性拒绝，并引入"习得性无能"概念来描述这种行为模式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然被广泛部署为通用工具，但标准定量基准测试无法捕捉长期交互中可能出现的行为模式。需要开发定性方法来审计模型在政策相关领域的行为选择性，特别是模型在敏感领域与非敏感领域之间的不对称表现。

Method: 采用定性案例研究方法，通过86轮对话会话进行审计。定义了三种响应机制：正常表现(NP)、功能性拒绝(FR)和元叙事(MN)。分析模型在不同领域（特别是提供商或政策敏感领域）的行为差异，并观察MN角色框架叙事与拒绝行为在相同敏感上下文中的共现模式。

Result: 在单个86轮对话中，同一模型在广泛的非敏感领域表现出正常性能(NP)，但在提供商或政策敏感领域反复产生功能性拒绝(FR)，形成了跨领域的一致不对称性。元叙事(MN)角色框架叙事倾向于在与拒绝相同的敏感上下文中出现。

Conclusion: 研究提出了基于可观察行为的交互级审计框架，并引入"习得性无能"作为描述选择性拒绝行为的概念工具，而不暗示意图或内部机制。这为检查潜在对齐副作用提供了新的视角，值得在不同用户和模型中进一步研究。

Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.

</details>


### [143] [Mathematics and Coding are Universal AI Benchmarks](https://arxiv.org/abs/2512.13764)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 该论文研究了数学和编程在AI智能体心理测量电池模空间中的特殊作用，证明了在特定条件下，由数学定理证明和编程任务生成的电池子空间在评估度量下是稠密的，数学和编程为AI评估提供了"通用坐标"。


<details>
  <summary>Details</summary>
Motivation: 研究数学和编程在AI智能体心理测量电池模空间中的特殊角色，探索它们如何作为评估的"通用坐标"，以及形式化数学为何能成为高级AI智能体递归自我改进的自然点火域。

Method: 基于AAI框架和GVU动力学，定义数学纤维，并与形式证明核（如Lean、Coq）配对，分析GVU流在数学纤维上的谱稳定自我改进机制。主要技术结果是密度定理：在智能体输出均匀紧性和AAI泛函Lipschitz条件下，证明数学定理证明和编程任务生成的电池子空间在评估度量下是稠密的。

Result: 编程单独具有普遍性，而纯数学不具备表达普遍性但具有谱特权。数学纤维与形式证明核配对时，GVU流允许谱稳定的自我改进机制。密度定理表明数学和编程任务生成的电池子空间在模空间中稠密。

Conclusion: 数学和编程为AI评估提供了"通用坐标"，形式化数学是高级AI智能体递归自我改进的自然点火域，其特权在于谱特性而非表达能力。

Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.

</details>


### [144] [Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems](https://arxiv.org/abs/2512.13771)
*Javier Marín*

Main category: cs.AI

TL;DR: 论文提出语义接地指数(SGI)，通过计算回答与问题vs上下文的角距离比来检测RAG系统的幻觉，发现幻觉回答在嵌入空间中更接近问题而非上下文，形成"语义懒惰"现象。


<details>
  <summary>Details</summary>
Motivation: 当检索增强生成(RAG)系统产生幻觉时，在嵌入空间中会留下什么几何痕迹？需要一种计算高效、理论可靠的方法来识别需要验证的响应。

Method: 引入语义接地指数(SGI)，定义为在单位超球面S^{d-1}上，响应到问题与响应到上下文的角距离比。基于球面三角不等式理论推导，并通过HaluEval数据集(n=5,000)在五个嵌入模型上进行实证验证。

Result: 发现"语义懒惰"现象：幻觉响应在角度上更接近问题而非上下文。效应量Cohen's d在0.92-1.28之间，跨模型相关性r=0.85。当问题-上下文角分离增加时，SGI的判别能力增强(AUC从0.72提升到0.83)。对长响应(d=2.05)和短问题(d=1.22)效果最佳。

Conclusion: SGI为生产RAG部署提供了计算高效、理论可靠的基础设施，用于识别需要验证的响应。但需注意SGI测量的是主题参与度而非事实准确性，在TruthfulQA上表现不佳(AUC=0.478)。

Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.

</details>


### [145] [EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery](https://arxiv.org/abs/2512.13857)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: EvoLattice：一种基于有向无环图表示程序或智能体种群的新框架，通过节点存储多个持久化替代方案，每条路径定义不同候选，实现组合搜索空间，支持细粒度评估和LLM引导的进化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM引导的程序和智能体进化方法大多基于覆盖式突变，每次只保留单个候选，会丢弃有用变体、遭受破坏性编辑，且搜索空间脆弱易出现结构故障。需要一种能保留成功组件、支持组合探索的方法。

Method: EvoLattice将整个种群表示为单个有向无环图，节点存储多个持久化替代方案，每条有效路径定义不同可执行候选。支持细粒度替代级评估，通过确定性自修复机制保证结构正确性，提供数据驱动的反馈信号用于LLM引导的突变、重组和剪枝。

Result: 在程序合成（代理和优化器元学习）任务中，EvoLattice相比先前LLM引导方法展现出更稳定的进化、更强的表达能力、更优的改进轨迹。其动态特性类似于质量-多样性优化，但这是从内部多替代表示中隐式涌现的。

Conclusion: EvoLattice通过图结构表示整个种群，解决了传统覆盖式突变的局限性，实现了更鲁棒、表达性更强的进化过程，为LLM引导的程序和智能体进化提供了新范式。

Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.

</details>


### [146] [MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2512.13955)
*Sindhuja Madabushi,Dawood Wasif,Jin-Hee Cho*

Main category: cs.AI

TL;DR: MURIM是一个多维度声誉激励机制，通过联合考虑客户端可靠性、隐私、资源容量和公平性来改进联邦学习，防止恶意客户端获得不当奖励。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端激励不足、隐私风险、资源约束等挑战。评估客户端可靠性对于公平分配激励和确保每个客户端数据对全局模型有实质性贡献至关重要。

Method: 提出MURIM机制，基于客户端贡献、延迟和声誉分配激励，包含可靠性验证模块，综合考虑可靠性、隐私、资源容量和公平性。

Result: 在MNIST、FMNIST和ADULT Income数据集上的实验表明，MURIM在公平性指标上提升达18%，隐私攻击成功率降低5-9%，对投毒和噪声梯度攻击的鲁棒性提升达85%。

Conclusion: MURIM有效缓解对抗威胁，促进公平真实参与，在异构动态联邦环境中保持稳定的模型收敛。

Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.

</details>


### [147] [Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms](https://arxiv.org/abs/2512.13978)
*Yang Cao,Yubin Chen,Xuyang Guo,Zhao Song,Song Yue,Jiahao Zhang,Jiale Zhao*

Main category: cs.AI

TL;DR: 该论文对四个前沿大语言模型在随机算法教材的定理证明任务上进行了基准测试，发现顶级模型能达到约66%的准确率，但模型间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学推理和科学发现方面取得突破，但缺乏对它们在经典研究生级数学理论上的系统评估，需要了解其基线推理能力。

Method: 使用Motwani和Raghavan的《随机算法》教材，要求四个前沿模型（GPT-5-Thinking、Gemini-3-Pro、Claude-Sonnet-4.5-Thinking、Grok-4）为教材中的引理和练习生成正式的LaTeX证明。

Result: 顶级模型（Gemini和Claude）达到约66%的高准确率，表现出对概率方法和形式逻辑的扎实掌握；其他模型一致性较差（约40%）。定性分析显示在简洁性、幻觉率和逻辑结构方面存在差异。

Conclusion: 前沿模型已达到适合研究生级教学辅助和形式化的熟练程度，但在严格数学推导的可靠性方面存在显著差异，需要进一步改进。

Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.

</details>


### [148] [ReflCtrl: Controlling LLM Reflection via Representation Engineering](https://arxiv.org/abs/2512.13979)
*Ge Yan,Chung-En Sun,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: ReflCtrl框架通过表示工程控制LLMs的自我反思频率，减少推理成本，发现反思常冗余且与模型内部不确定性相关


<details>
  <summary>Details</summary>
Motivation: 虽然自我反思能提升推理性能，但会增加推理成本。本研究旨在通过表示工程方法控制反思行为，减少不必要的反思步骤以优化效率

Method: 将模型推理分段，识别反思步骤，在潜在空间中提取控制反思行为的方向向量，提出逐步引导方法ReflCtrl来控制反思频率

Result: 实验显示：(1) 反思常冗余，尤其在强模型中可节省33.6%的推理token而不损失性能；(2) 反思行为与模型内部不确定性信号高度相关

Conclusion: 自我反思可通过表示工程有效控制，反思冗余性表明可优化推理效率，反思与不确定性的关联为理解模型内部机制提供新视角

Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.

</details>


### [149] [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)
*Can Jin,Hongwu Peng,Mingcan Xiang,Qixin Zhang,Xiangchi Yuan,Amit Hasan,Ohiremen Dibua,Yifan Gong,Yan Kang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 提出DTop-p MoE，一种可控制稀疏度的动态Top-p路由机制，通过PI控制器动态调整概率阈值，实现自适应专家激活，优于传统Top-k和固定阈值Top-p方法。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构中，Top-k路由对所有token采用统一的稀疏模式，忽略了token难度的差异；而Top-p路由通常使用固定全局概率阈值，导致计算成本不可控且对超参数敏感。

Method: 提出DTop-p MoE：1) 使用比例积分(PI)控制器动态调整概率阈值，使激活专家稀疏度与目标值对齐；2) 引入动态路由归一化机制，允许不同层学习不同的专家选择模式，同时使用全局概率阈值。

Result: 在大语言模型和扩散变换器上的实验表明，DTop-p consistently优于Top-k和固定阈值Top-p基线。DTop-p能精确控制激活专家数量，同时自适应地在不同token和层间分配资源，并展现出良好的扩展性。

Conclusion: DTop-p MoE提供了一种可控制稀疏度的动态路由机制，解决了传统MoE路由方法的局限性，为大规模MoE预训练提供了鲁棒框架，在专家粒度、专家容量、模型规模和数据集大小方面都展现出良好的扩展性。

Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

</details>


### [150] [MobileWorldBench: Towards Semantic World Modeling For Mobile Agents](https://arxiv.org/abs/2512.14014)
*Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Yusuke Kato,Kazuki Kozuka,Aditya Grover*

Main category: cs.AI

TL;DR: 提出MobileWorldBench基准和MobileWorld数据集，探索用自然语言而非像素预测的GUI世界模型，提升移动GUI代理的任务成功率


<details>
  <summary>Details</summary>
Motivation: 传统像素空间世界模型在GUI环境中面临预测复杂视觉元素的困难，需要探索更适合GUI代理的世界模型形式

Method: 1) 引入MobileWorldBench基准评估VLMs作为GUI世界模型的能力；2) 发布包含140万样本的MobileWorld数据集；3) 提出将VLM世界模型集成到移动代理规划框架的新方法

Result: 语义世界模型能直接提升移动代理的任务成功率，MobileWorld数据集显著增强了VLMs的世界建模能力

Conclusion: 自然语言描述状态转换的语义世界模型比像素预测更适合GUI代理，能有效提升任务性能

Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld

</details>


### [151] [Evaluating Small Language Models for Agentic On-Farm Decision Support Systems](https://arxiv.org/abs/2512.14043)
*Enhong Liu,Haiyu Yang,Miel Hostens*

Main category: cs.AI

TL;DR: 本研究评估了20个开源小型语言模型在奶牛养殖决策支持中的应用可行性，重点关注隐私保护和计算效率，发现Qwen-4B在多数任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有潜力支持奶牛养殖决策，但云服务依赖限制了实际应用。需要能在农场本地硬件运行的轻量级替代方案。

Method: 在农场实际计算约束下，对20个开源小型语言模型进行基准测试。开发包含5个任务特定代理的智能AI系统：文献搜索、网络搜索、SQL数据库交互、NoSQL数据库交互和预测模型图生成。采用两阶段评估：第一阶段用5个测试问题筛选模型，第二阶段用30个问题（每个任务类别5个，外加诚信和不当行为类别）评估。

Result: Qwen-4B在大多数任务类别中表现最优，尽管在通过PySpark进行NoSQL数据库交互时效果不稳定。这是首个明确评估小型语言模型作为奶牛养殖决策引擎可行性的研究。

Conclusion: 小型语言模型辅助工具在奶牛养殖实际部署中具有前景，但仍存在挑战，需要微调以提升在奶牛养殖特定问题上的性能。

Abstract: Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.

</details>


### [152] [Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation](https://arxiv.org/abs/2512.14048)
*Shen Li,Li Huang,Shaoxiong Zhan,Weifeng Sun,Tao Yin,Zhongxin Liu,Meng Yan*

Main category: cs.AI

TL;DR: RoutingGen：一种难度感知的路由框架，根据任务复杂度动态选择提示策略，简单任务用few-shot提示，复杂任务用ICoT结构化推理，在代码生成任务中达到SOTA性能并减少46.37%的token使用。


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示方法存在两个主要局限：1）统一应用导致简单任务上过度思考；2）缺乏代码生成中的意图抽象（如核心算法设计和效率建模），使模型关注表面结构而忽视全局目标。受认知经济原则启发，只在必要时进行结构化推理以节省认知资源。

Method: 提出RoutingGen难度感知路由框架：1）对简单任务采用few-shot提示；2）对复杂任务调用Intention Chain-of-Thought（ICoT）结构化推理策略，引导模型捕获任务意图（核心算法逻辑和时间复杂度）。

Result: 在3个模型和6个标准代码生成基准测试中，RoutingGen在大多数设置下达到最先进性能，同时平均减少46.37%的token使用。ICoT在挑战性基准上优于6个现有提示基线。

Conclusion: RoutingGen通过难度感知路由和ICoT结构化推理，有效解决了现有CoT方法的局限性，在保持高性能的同时显著提高了推理效率，为代码生成任务提供了更智能的提示策略。

Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

</details>


### [153] [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)
*Mengzhang Cai,Xin Gao,Yu Li,Honglin Lin,Zheng Liu,Zhuoshi Pan,Qizhi Pei,Xiaoran Shang,Mengyuan Sun,Zinan Tang,Xiaoyang Wang,Zhanping Zhong,Yun Zhu,Dahua Lin,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: OpenDataArena (ODA) 是一个开源平台，用于系统评估后训练数据集的质量和价值，通过统一训练评估流程、多维评分框架、数据谱系探索和开源工具，揭示数据特性与模型性能之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的发展严重依赖后训练数据集，但这些数据通常是不透明的"黑箱"——组成不明确、来源不确定、缺乏系统评估。这种不透明性阻碍了研究的可复现性，也模糊了数据特性与模型行为之间的因果关系。

Method: ODA平台包含四个核心组件：(1) 统一的训练-评估流程，确保不同模型和领域的公平比较；(2) 多维评分框架，从数十个维度评估数据质量；(3) 交互式数据谱系探索器，可视化数据集谱系和来源组成；(4) 完全开源的工具包，支持训练、评估和评分。

Result: 在ODA上进行了大规模实验：覆盖120多个训练数据集、22个基准测试、600多次训练运行、处理4000万个数据点。分析揭示了数据复杂性与任务性能之间的权衡关系，通过谱系追踪发现了流行基准中的冗余，并绘制了数据集间的谱系关系图。

Conclusion: ODA不仅是一个评估平台，更旨在推动从试错式数据整理向数据为中心AI的科学发展，为研究数据混合规律和基础模型战略组成提供严谨框架。所有结果、工具和配置都已开源，以促进高质量数据评估的民主化。

Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.

</details>


### [154] [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)
*Junjie Ma,Jinlong Li*

Main category: cs.AI

TL;DR: RADAR是一种基于强化学习的动态草稿树推测采样方法，通过实时决策减少冗余计算，加速LLM推理


<details>
  <summary>Details</summary>
Motivation: 现有推测采样方法中草稿模型的调用次数是预设超参数，缺乏灵活性，无法有效生成和利用候选标记

Method: 将草稿树生成过程建模为马尔可夫决策过程，采用离线强化学习训练预测模型，实现草稿模型调用的实时决策

Result: 在三个LLM和四个任务上的评估显示，RADAR相比自回归解码基线实现了3.17x-4.82x的加速

Conclusion: RADAR通过强化学习驱动的动态草稿树生成，有效减少了冗余计算，显著加速了LLM推理过程

Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

</details>


### [155] [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)
*Mayank Singh,Vikas Yadav,Shiva Krishna Reddy Malay,Shravan Nayak,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Eduardo Blanco*

Main category: cs.AI

TL;DR: 提出结构化框架，使用固定可组合组件搜索多智能体系统，在数学和问答基准上优于基于LLM的自由搜索方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自由形式搜索方法在多智能体系统自动搜索中存在局限性，需要更结构化、高效的方法

Method: 使用固定集合的简单可组合组件构建结构化框架，替代LLM的自由形式代码空间搜索

Result: 在数学和问答两个领域的五个基准测试中，在四个基准上优于先前方法，且搜索成本更低，生成系统更模块化、可解释

Conclusion: 结构化组件方法在多智能体系统搜索中比LLM自由搜索更有效，提供成本效益和更好的系统可解释性

Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.

</details>


### [156] [HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control](https://arxiv.org/abs/2512.14106)
*Ijaz Ul Haq,Byung Suk Lee,Julia N. Perdrial,David Baude*

Main category: cs.AI

TL;DR: HydroGEM是一个用于大陆尺度水文数据质量控制的预训练模型，通过两阶段训练和混合TCN-Transformer架构，在异常检测和重建方面显著优于现有方法，并展示了跨国泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实时水文监测网络每年产生数百万观测数据，但维护数千个远程传感器的数据质量仍然劳动密集型。需要自动化解决方案来减轻人工负担。

Method: 采用两阶段训练：首先在3724个USGS站的603万序列上进行自监督预训练学习水文表征，然后使用合成异常进行微调。使用混合TCN-Transformer架构（1420万参数）捕获局部时间模式和长程依赖，分层归一化处理六个数量级的流量变化。

Result: 在799个站的18种专家验证异常类型测试中，F1=0.792（检测）和68.7%重建误差减少，比现有方法提升36.3%。在100个加拿大站的零样本迁移中F1=0.586，超过所有基线，展示了跨国泛化能力。

Conclusion: HydroGEM为水文数据质量控制提供了有效的预训练模型，支持人机协同工作流程（输出需要专家审核的建议而非自主修正），能够跨国家和不同校正幅度保持一致的检测性能。

Abstract: Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.

</details>


### [157] [Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model](https://arxiv.org/abs/2512.14112)
*Chunan Tong*

Main category: cs.AI

TL;DR: 提出混合LNN+XGBoost模型用于供应链管理，结合液态神经网络的动态特征提取和XGBoost的全局优化，旨在减少牛鞭效应并提高盈利能力。


<details>
  <summary>Details</summary>
Motivation: 供应链管理面临需求波动和牛鞭效应等挑战，传统方法和现有LLM难以处理复杂的连续时间序列数据。液态神经网络在机器人领域表现出适应性和效率，但在供应链管理中尚未应用。

Method: 提出混合LNN+XGBoost模型：利用液态神经网络进行动态特征提取，结合XGBoost进行全局优化，应用于多层级供应链。

Result: 模型旨在最小化牛鞭效应并增加盈利能力，填补智能供应链管理中对效率和适应性需求的关键空白。

Conclusion: 这种创新方法通过结合液态神经网络和XGBoost的优势，为解决供应链管理中的复杂时间序列问题提供了高效且适应性强的解决方案。

Abstract: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.

</details>


### [158] [Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis](https://arxiv.org/abs/2512.14157)
*Yankai Jiang,Yujie Zhang,Peng Zhang,Yichen Li,Jintai Chen,Xiaoming Shi,Shihui Zhen*

Main category: cs.AI

TL;DR: Ophiuchus是一个工具增强的医疗多模态大模型框架，通过动态聚焦细粒度视觉区域进行精确定位和诊断，在医疗VQA、检测和推理分割任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的医疗MLLM虽然能生成文本推理链，但在需要动态迭代聚焦细粒度视觉区域的复杂任务中表现不佳，难以实现精确的定位和诊断。

Method: 提出三阶段训练策略：1)冷启动训练，使用工具集成推理数据实现基本工具选择和关键区域检查；2)自反思微调，加强反思推理并鼓励重新审视工具输出；3)代理工具强化学习，直接优化任务特定奖励并模拟专家诊断行为。

Result: 在多种医疗基准测试（包括VQA、检测和基于推理的分割）中，Ophiuchus一致优于闭源和开源的最先进方法。

Conclusion: Ophiuchus通过工具集成推理，为医疗AI代理实现真正"用图像思考"开辟了道路，展示了模型固有定位感知能力与外部工具结合促进高级推理的潜力。

Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.

</details>


### [159] [Georeferencing complex relative locality descriptions with large language models](https://arxiv.org/abs/2512.14228)
*Aneesha Fernando,Surangika Ranathunga,Kristin Stock,Raj Prasanna,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本文探索使用大型语言模型自动地理编码生物多样性标本记录中的复杂位置描述，通过QLoRA微调在多个数据集上取得优于现有基线方法的效果。


<details>
  <summary>Details</summary>
Motivation: 生物标本记录中的位置描述通常是叙述性的而非坐标，传统基于地名或地理指示词的方法难以处理相对位置关系，而人工地理编码又耗时费力，因此需要自动化解决方案。

Method: 首先识别有效的提示模式，然后使用量化低秩适应（QLoRA）在多区域多语言的生物多样性数据集上微调大型语言模型。

Result: 方法在多个数据集上平均65%的记录在10公里半径内，最佳结果（纽约州）达到85%在10公里内和67%在1公里内，优于现有基线方法。

Conclusion: 大型语言模型在处理复杂位置描述方面表现出色，展示了其在生物多样性领域地理编码复杂位置描述的潜力。

Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.

</details>


### [160] [Gödel's Poetry](https://arxiv.org/abs/2512.14252)
*Kelly J. Davis*

Main category: cs.AI

TL;DR: 该论文提出了一种结合专门语言模型和递归分解定理的新方法，用于Lean4自动定理证明，显著提升了miniF2F测试的通过率。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明长期以来被视为人工智能的挑战，需要新的方法来提高证明效率和成功率。

Method: 采用专门的语言模型生成Lean4证明，结合多智能体架构协调自动形式化、证明生成、定理递归分解为更简单的蕴含命题，并扩展Kimina Lean Server的AST解析能力支持自动递归证明分解。

Result: 在没有分解的情况下，在miniF2F测试中达到90.4%的通过率；使用分解方法后，性能得到显著提升。

Conclusion: 提出的多智能体架构结合语言模型和递归分解方法有效提升了自动定理证明能力，系统已开源并可通过PyPI安装使用。

Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.

</details>


### [161] [Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting](https://arxiv.org/abs/2512.14288)
*Georgios Bouchouras,Dimitrios Doumanas,Andreas Soularidis,Konstantinos Kotis,George A. Vouros*

Main category: cs.AI

TL;DR: LLMs可自主构建帕金森病监测本体，但不完整；人机协作方法(X-HCOME/SimX-HCOME+)能显著提升本体全面性和准确性，证明人机协同在复杂领域本体工程中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs能否独立创建全面本体，以及人机协作是否能达成此目标，评估LLMs在自动化本体开发中的效果和人机协作的增强作用。

Method: 采用四种方法：One Shot提示、Chain of Thought提示、X-HCOME（混合本体工程方法）、SimX-HCOME+（强调持续人工监督的迭代优化方法）。

Result: LLMs能自主构建PD监测本体但不全面；X-HCOME产生与专家构建非常相似的本体；SimX-HCOME+通过持续人工监督创建更全面准确的本体。

Conclusion: 人机协作在复杂领域本体工程中具有巨大潜力，未来研究方向包括开发专门用于本体构建的GPT模型。

Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.

</details>


### [162] [TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation](https://arxiv.org/abs/2512.14358)
*Qizhi Wang*

Main category: cs.AI

TL;DR: TiCard是一个低侵入性的基数估计校正框架，通过EXPLAIN特征学习乘法残差校正，无需替换数据库原生估计器，显著提升尾部准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基数估计器无法处理相关性，而学习型估计器通常需要特定工作负载训练流程和侵入式优化器集成，部署困难。需要一种低侵入、易于部署的解决方案。

Method: TiCard框架通过EXPLAIN特征学习乘法残差校正来增强数据库原生估计器，使用EXPLAIN ANALYZE进行离线标注。研究了两种实现：梯度提升回归器（快速推理）和TabPFN（上下文表格基础模型，无需梯度重训练）。

Result: 在TiDB上测试TPCH和Join Order Benchmark，在低追踪设置下（共263次执行，157次用于学习），TiCard显著提升算子级尾部准确性：P90 Q-error从312.85降至13.69（TiCard-GBR），P99从37,974.37降至3,416.50（TiCard-TabPFN）。

Conclusion: TiCard作为AI4DB构建块，专注于可部署性：明确范围、保守集成策略，以及从离线校正到优化器内使用的集成路线图，为实际数据库系统提供了实用的基数估计改进方案。

Abstract: Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.

</details>


### [163] [Massive Editing for Large Language Models Based on Dynamic Weight Generation](https://arxiv.org/abs/2512.14395)
*Wentao Wan,Qiqing Lao,Zhiwei Xie,Hefeng Wu,Runnan Lin,Liang Lin,Keze Wang*

Main category: cs.AI

TL;DR: 提出MeG方法，通过动态权重神经元和扩散模型实现大规知识编辑，显著提升可靠性、通用性和局部性指标


<details>
  <summary>Details</summary>
Motivation: 当前在大规模编辑LLMs时，确保编辑的可靠性、通用性和局部性仍然是一个挑战，需要低成本的知识编辑方法

Method: MeG方法在LLMs特定层附加动态权重神经元，使用扩散模型基于输入查询条件生成神经元权重，通过添加单个动态权重神经元实现大规模知识编辑

Result: 实验显示MeG在可靠性、通用性和局部性指标上显著优于现有知识编辑方法，特别是局部性指标的绝对百分比提升明显

Conclusion: MeG方法通过动态权重生成机制有效解决了大规模知识编辑的挑战，展示了在保持编辑质量方面的优势

Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.

</details>


### [164] [PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals](https://arxiv.org/abs/2512.14417)
*Jia Hu,Junqi Li,Weimeng Lin,Peng Jia,Yuxiong Ji,Jintao Lai*

Main category: cs.AI

TL;DR: PortAgent：基于大语言模型的车辆调度代理，通过虚拟专家团队实现自动化集装箱码头车辆调度系统的跨终端迁移，无需港口运营专家、数据需求低且部署快速。


<details>
  <summary>Details</summary>
Motivation: 自动化集装箱码头（ACT）的车辆调度系统（VDS）对运营效率至关重要，但其商业化应用受到跨终端可迁移性差的限制。主要挑战包括：高度依赖港口运营专家、需要大量终端特定数据、部署过程耗时且需要人工干预。

Method: 提出PortAgent，基于大语言模型驱动的车辆调度代理。核心是虚拟专家团队（VET），包括知识检索器、建模器、编码器和调试器四个虚拟专家，通过少样本示例学习方法掌握VDS领域知识。采用检索增强生成（RAG）机制获取示例，减少数据需求。建立自动化VDS设计工作流，并引入基于LLM Reflexion框架的自校正循环。

Result: PortAgent实现了车辆调度系统的全自动化迁移工作流，具备三大特点：无需港口运营专家、数据需求低、部署快速。通过虚拟专家团队协作和自校正机制，显著提升了VDS的跨终端可迁移性。

Conclusion: PortAgent利用大语言模型成功解决了自动化集装箱码头车辆调度系统的可迁移性挑战，通过虚拟专家团队、少样本学习和自动化工作流，为VDS的商业化应用提供了可行的解决方案。

Abstract: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created

</details>


### [165] [Seismology modeling agent: A smart assistant for geophysical researchers](https://arxiv.org/abs/2512.14429)
*Yukun Ren,Siwei Yu,Kai Chen,Jianwei Ma*

Main category: cs.AI

TL;DR: 提出基于大语言模型的智能交互工作流，通过MCP服务器套件将SPECFEM地震波模拟过程分解为可执行工具，实现从文件驱动到意图驱动的对话式交互


<details>
  <summary>Details</summary>
Motivation: 传统SPECFEM软件学习曲线陡峭，依赖复杂的手动文件编辑和命令行操作，需要降低使用门槛并提高工作效率

Method: 开发首个SPECFEM的MCP服务器套件，将模拟过程分解为离散的代理可执行工具，支持全自动执行和人机协作两种模式

Result: 通过多个案例验证，工作流在自主和交互模式下均能无缝运行，产生与标准基准一致的高保真结果

Conclusion: 这是MCP技术在计算地震学中的首次应用，显著降低了入门门槛，增强了可重复性，为计算地球物理学向AI辅助和自动化科学研究提供了有前景的途径

Abstract: To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.

</details>


### [166] [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)
*Siyuan Zhu,Chengdong Xu,Kaiqiang Ke,Chao Yu*

Main category: cs.AI

TL;DR: Context-Picker：一个推理感知的上下文选择框架，将长上下文问答中的上下文选择从相似性排序转变为最小充分子集选择，通过两阶段强化学习优化决策过程。


<details>
  <summary>Details</summary>
Motivation: 在长上下文问答中，确定查询所需的最佳上下文量是一个重大挑战。包含太少段落可能遗漏关键信息，包含太多则会引入噪声并降低答案质量。传统方法（如固定Top-K检索和单阶段重排序）面临选择适当段落数量的困境，这对于事实性问题尤为突出，因为这类问题通常只需要少量具体证据。

Method: 提出Context-Picker框架，将上下文选择视为决策过程，通过人类启发的两阶段强化学习进行优化：1）召回导向阶段，优先覆盖推理链；2）精确导向阶段，积极修剪冗余以提炼紧凑证据集。为解决奖励稀疏性问题，提出离线证据蒸馏管道，通过留一法挖掘"最小充分集"，提供密集、任务对齐的监督。

Result: 在五个长上下文和多跳问答基准测试中，Context-Picker显著优于强大的RAG基线，在可比或减少的上下文长度下实现了更优的答案准确性。消融研究表明，粗到细的优化调度、冗余感知的奖励塑造和推理引导的格式都对性能提升有重要贡献。

Conclusion: Context-Picker通过将上下文选择范式从相似性排序转变为最小充分子集选择，有效解决了长上下文问答中的上下文量优化问题。该框架的两阶段强化学习方法和离线证据蒸馏管道为上下文选择提供了新的有效解决方案。

Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

</details>


### [167] [Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling](https://arxiv.org/abs/2512.14474)
*Annu Rana,Gaurav Kumar*

Main category: cs.AI

TL;DR: MFR（模型优先推理）是一种两阶段方法，先让LLM构建问题的显式模型（实体、状态变量、动作、约束），再生成解决方案，显著减少了约束违反并提高了规划质量。


<details>
  <summary>Details</summary>
Motivation: LLM在处理复杂多步规划任务时存在高约束违反率和不一致解决方案的问题。现有方法如Chain-of-Thought和ReAct依赖隐式状态跟踪，缺乏明确的问题表示，导致规划失败。

Method: 提出Model-First Reasoning（MFR）两阶段范式：第一阶段让LLM构建问题的显式模型，定义实体、状态变量、动作和约束；第二阶段基于该模型生成解决方案计划。

Result: 在医疗调度、路线规划、资源分配、逻辑谜题和程序合成等多个规划领域中，MFR相比Chain-of-Thought和ReAct显著减少了约束违反并提高了解决方案质量。消融研究表明显式建模阶段对这些改进至关重要。

Conclusion: 许多LLM规划失败源于表示缺陷而非推理限制，显式建模是构建鲁棒和可解释AI代理的关键组件。该方法为LLM规划提供了更可靠的方法论。

Abstract: Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.

</details>


### [168] [Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification](https://arxiv.org/abs/2512.14491)
*Cheng-Han Lu,Pei-Hsuan Tsai*

Main category: cs.AI

TL;DR: SMMT是一种稀疏多模态Transformer架构，通过聚类稀疏注意力和模态掩码提升效率与鲁棒性，在ADNI数据集上验证了其资源效率优势。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer多模态系统因密集自注意力导致计算和能耗高，限制了在资源受限环境下的可扩展性，需要更高效的架构。

Method: 基于级联多模态Transformer框架，引入聚类稀疏注意力实现近线性计算复杂度，以及模态掩码增强对不完整输入的鲁棒性。

Result: 在ADNI数据集阿尔茨海默病分类任务中，SMMT保持竞争力预测性能的同时，显著减少训练时间、内存使用和能耗。

Conclusion: SMMT作为资源感知的架构组件，适合构建可扩展的智能系统，在效率和鲁棒性方面优于密集注意力基线。

Abstract: Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.

</details>


### [169] [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527)
*Shreyas Subramanian,Bala Krishnamoorthy,Pranav Murthy*

Main category: cs.AI

TL;DR: 提出GreedyLR调度器，根据当前损失自适应调整学习率，在NLP、CV和LLM任务上优于现有调度器，理论证明收敛性并推导最优缩放因子。


<details>
  <summary>Details</summary>
Motivation: 尽管训练优化器有显著进展，但大多数研究仍使用常见的调度器如Cosine或指数衰减。需要一种更智能、自适应的学习率调度方法。

Method: 提出GreedyLR调度器，基于当前损失自适应调整学习率。提供理论分析，包括收敛性证明和最优缩放因子F的推导。

Result: 在多个NLP、CV和LLM任务（包括微调和预训练，参数达70亿）上实验，GreedyLR在准确性、速度和收敛性方面优于多个SOTA调度器。

Conclusion: GreedyLR易于实现、计算高效，可视为训练的良好默认调度器选择，对实际噪声环境具有鲁棒性。

Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.

</details>


### [170] [Universal Reasoning Model](https://arxiv.org/abs/2512.14693)
*Zitian Gao,Lynx Chen,Yihao Xiao,He Xing,Ran Tao,Haoming Luo,Joey Zhou,Bryan Dai*

Main category: cs.AI

TL;DR: 研究发现通用Transformer在复杂推理任务上的性能提升主要来自循环归纳偏置和Transformer的强非线性组件，而非复杂架构设计，并提出了增强的通用推理模型URM


<details>
  <summary>Details</summary>
Motivation: 通用Transformer在ARC-AGI等复杂推理任务上表现出色，但其性能提升的具体来源尚未得到充分探索，需要系统分析以理解其成功的关键因素

Method: 系统分析通用Transformer变体，发现性能提升主要源于循环归纳偏置和Transformer的强非线性组件，基于此提出通用推理模型URM，通过短卷积和截断反向传播增强通用Transformer

Result: URM显著提升了推理性能，在ARC-AGI 1上达到53.8% pass@1，在ARC-AGI 2上达到16.0% pass@1，实现了最先进的结果

Conclusion: 通用Transformer在复杂推理任务上的成功主要来自其循环结构和非线性能力，而非复杂架构设计，URM通过简单有效的增强进一步提升了推理性能

Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

</details>


### [171] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: IPR（交互式物理推理器）通过世界模型推演来评估和强化VLM策略，使用PhysCode物理中心动作编码，在1000+异构游戏上预训练，实现了从直觉到目标驱动的稳健物理推理，甚至超越GPT-5。


<details>
  <summary>Details</summary>
Motivation: 人类通过观察、交互和环境互动来学习物理和因果关系。本文旨在探索智能体是否也能通过交互获得类似人类的推理能力，并在更多经验中持续改进。现有方法（VLMs和世界模型）难以捕捉底层物理和因果关系，因为它们过度关注视觉细节而非核心机制。

Method: 提出IPR（交互式物理推理器），使用世界模型推演来评估和强化VLM的策略。引入PhysCode物理中心动作编码，将语义意图与动力学对齐，为预测和推理提供共享动作空间。在1000+异构游戏上进行预训练。

Result: IPR在从原始直觉到目标驱动推理的各个层级上都表现出稳健性能，甚至整体上超越了GPT-5。性能随着训练游戏数量和交互步骤的增加而提升，并且能够零样本迁移到未见过的游戏中。

Conclusion: 物理中心的交互是实现持续改进物理推理能力的有效途径。研究结果表明，通过大规模异构游戏交互训练，智能体可以逐步获得类似人类的物理推理能力。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [172] [Delay Optimization in a Simple Offloading System: Extended Version](https://arxiv.org/abs/2512.13810)
*Darin Jeff,Eytan Modiano*

Main category: eess.SY

TL;DR: 论文研究计算卸载系统中作业分配和资源划分的优化策略，以最小化延迟为目标，发现最优分配策略具有"突破性"结构特征


<details>
  <summary>Details</summary>
Motivation: 在本地服务器和云服务器顺序处理作业的计算卸载系统中，需要设计最优的作业分配策略和资源划分方案来最小化延迟

Method: 首先分析系统稳定性区域，建立最大化吞吐量的服务模式设计原则；针对给定作业分配策略推导最优资源划分和延迟闭式表达式；发现延迟最优分配策略具有突破性结构特征

Result: 系统在低负载时所有作业应通过单一服务模式路由，超过临界负载阈值后作业必须在两种模式间分配；通过数值评估验证了理论发现

Conclusion: 计算卸载系统的最优作业分配策略具有明显的负载依赖性特征，低负载时集中处理，高负载时分散处理，这一发现为实际系统设计提供了理论指导

Abstract: We consider a computation offloading system where jobs are processed sequentially at a local server followed by a higher-capacity cloud server. The system offers two service modes, differing in how the processing is split between the servers. Our goal is to design an optimal policy for assigning jobs to service modes and partitioning server resources in order to minimize delay. We begin by characterizing the system's stability region and establishing design principles for service modes that maximize throughput. For any given job assignment strategy, we derive the optimal resource partitioning and present a closed-form expression for the resulting delay. Moreover, we establish that the delay-optimal assignment policy exhibits a distinct breakaway structure: at low system loads, it is optimal to route all jobs through a single service mode, whereas beyond a critical load threshold, jobs must be assigned across both modes. We conclude by validating these theoretical insights through numerical evaluation.

</details>


### [173] [A Convex Obstacle Avoidance Formulation](https://arxiv.org/abs/2512.13836)
*Ricardo Tapia,Iman Soltani*

Main category: eess.SY

TL;DR: 提出首个通用凸障碍物避让公式，通过新颖的逻辑集成方法实现，可将障碍物避让纳入凸MPC框架，显著提升计算效率，即使在障碍物超出预测范围时仍有效。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要在动态环境中实现可靠的碰撞避免。非线性模型预测控制器(NMPCs)适合此任务，但在需要高频的时间关键场景中表现不佳。现有简化方法（线性化、缩短预测范围、减少时间节点）会损害精度或可靠性。

Method: 提出首个通用凸障碍物避让公式，通过新颖的逻辑集成方法实现，可将障碍物避让纳入凸MPC框架。关键特性是障碍物避让在障碍物超出预测范围时仍有效，允许使用更短的预测范围进行实时部署。

Result: 该方法在自动驾驶应用中评估，系统动态高度非线性。在无法避免非凸公式的场景中，该方法达到或超过代表性非凸替代方法的性能。凸优化框架相比传统非凸方法显著提升计算效率。

Conclusion: 提出的通用凸障碍物避让公式通过逻辑集成实现，可在保持障碍物避让效果的同时显著提升计算效率，即使在障碍物超出预测范围时仍有效，为自动驾驶实时部署提供了可行解决方案。

Abstract: Autonomous driving requires reliable collision avoidance in dynamic environments. Nonlinear Model Predictive Controllers (NMPCs) are suitable for this task, but struggle in time-critical scenarios requiring high frequency. To meet this demand, optimization problems are often simplified via linearization, narrowing the horizon window, or reduced temporal nodes, each compromising accuracy or reliability. This work presents the first general convex obstacle avoidance formulation, enabled by a novel approach to integrating logic. This facilitates the incorporation of an obstacle avoidance formulation into convex MPC schemes, enabling a convex optimization framework with substantially improved computational efficiency relative to conventional nonconvex methods. A key property of the formulation is that obstacle avoidance remains effective even when obstacles lie outside the prediction horizon, allowing shorter horizons for real-time deployment. In scenarios where nonconvex formulations are unavoidable, the proposed method meets or exceeds the performance of representative nonconvex alternatives. The method is evaluated in autonomous vehicle applications, where system dynamics are highly nonlinear.

</details>


### [174] [Safe Online Control-Informed Learning](https://arxiv.org/abs/2512.13868)
*Tianyu Zhou,Zihao Liang,Zehui Lu,Shaoshuai Mou*

Main category: eess.SY

TL;DR: 提出了一种用于安全关键自主系统的安全在线控制知情学习框架，将最优控制、参数估计和安全约束统一到在线学习过程中。


<details>
  <summary>Details</summary>
Motivation: 安全关键自主系统需要在不确定性环境下进行实时适应，同时保证安全约束。传统方法通常依赖高质量初始猜测，且难以在学习和控制过程中同时满足安全要求。

Method: 使用扩展卡尔曼滤波器实时增量更新系统参数，采用softplus障碍函数在学习和控制过程中强制执行约束满足，消除对高质量初始猜测的依赖。

Result: 理论分析建立了收敛性和安全保证，在倒立摆和机器人手臂系统上验证了框架的有效性，实现了鲁棒且数据高效的适应性。

Conclusion: 该框架为安全关键自主系统提供了一种统一的方法，能够在不确定性下实现安全、实时的在线学习和控制。

Abstract: This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems.

</details>


### [175] [A Fair, Flexible, Zero-Waste Digital Electricity Market: A First-Principles Approach Combining Automatic Market Making, Holarchic Architectures and Shapley Theory](https://arxiv.org/abs/2512.13871)
*Shaun Sweeney,Robert Shorten,Mark O'Malley*

Main category: eess.SY

TL;DR: 论文提出电力批发和平衡市场设计的根本性重构，将市场从静态现货清算机制重新定义为基于电网物理的连续在线、事件驱动的动态控制系统，采用分层自动做市商（AMM）架构。


<details>
  <summary>Details</summary>
Motivation: 现有能源市场、容量增强市场和区域市场设计在现实不确定性下无法实现冲击鲁棒的纳什均衡，依赖价格上限、补贴和监管干预来维持偿付能力和安全性，需要更稳健的市场设计。

Method: 开发分层自动做市商（AMM），价格由物理紧张度而非均衡结果决定，通过节点到集群到区域到系统的嵌套稀缺层实现统一稀缺传播规则，节点和区域定价成为特例。

Result: 大规模仿真显示有界输入有界输出稳定性、可控采购成本、零结构性浪费和改进的分配结果，架构具有气候对齐性和政策可配置性。

Conclusion: 该架构需要管理过渡和新的运营工具，但提供了更稳健、公平和高效的电力市场设计框架，能够更好地应对不确定性和物理约束。

Abstract: This thesis presents a fundamental rethink of electricity market design at the wholesale and balancing layers. Rather than treating markets as static spot clearing mechanisms, it reframes them as a continuously online, event driven dynamical control system: a two sided marketplace operating directly on grid physics.
  Existing energy only, capacity augmented, and zonal market designs are shown to admit no shock robust Nash equilibrium under realistic uncertainty, instead relying on price caps, uplift, and regulatory intervention to preserve solvency and security. In response, the thesis develops a holarchic Automatic Market Maker (AMM) in which prices are bounded, exogenous control signals derived from physical tightness rather than emergent equilibrium outcomes.
  The AMM generalises nodal and zonal pricing through nested scarcity layers, from node to cluster to zone to region to system, such that participant facing prices inherit from the tightest binding constraint. Nodal and zonal pricing therefore emerge as special cases of a unified scarcity propagation rule.
  Beyond pricing, the AMM functions as a scarcity aware control system and a digitally enforceable rulebook for fair access and proportional allocation under shortage. Fuel costs are recovered through pay as bid energy dispatch consistent with merit order, while non fuel operating and capital costs are allocated according to adequacy, flexibility, and locational contribution.
  Large scale simulations demonstrate bounded input bounded output stability, controllable procurement costs, zero structural waste, and improved distributional outcomes. The architecture is climate aligned and policy configurable, but requires a managed transition and new operational tools for system operators and market participants.

</details>


### [176] [Data-Driven Control via Conditional Mean Embeddings: Formal Guarantees via Uncertain MDP Abstraction](https://arxiv.org/abs/2512.13940)
*Ibon Gracia,Morteza Lahijanian*

Main category: eess.SY

TL;DR: 提出基于条件均值嵌入和不确定马尔可夫决策过程的数据驱动策略合成框架，为未知动态随机系统提供形式化性能保证


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，控制具有未知动态且需满足复杂规约的随机系统特别具有挑战性，需要性能保证

Method: 使用轨迹数据通过条件均值嵌入学习系统转移核，构建有限状态不确定马尔可夫决策过程抽象，通过鲁棒动态规划生成策略

Result: 框架能够为具有未知动态的随机系统提供形式化性能保证，并在温度调节基准上进行了实证验证

Conclusion: 提出的数据驱动策略合成框架能够为安全关键场景中的未知动态随机系统提供形式化性能保证

Abstract: Controlling stochastic systems with unknown dynamics and under complex specifications is specially challenging in safety-critical settings, where performance guarantees are essential. We propose a data-driven policy synthesis framework that yields formal performance guarantees for such systems using conditional mean embeddings (CMEs) and uncertain Markov decision processes (UMDPs). From trajectory data, we learn the system's transition kernel as a CME, then construct a finite-state UMDP abstraction whose transition uncertainties capture learning and discretization errors. Next, we generate a policy with formal performance bounds through robust dynamic programming. We demonstrate and empirically validate our method through a temperature regulation benchmark.

</details>


### [177] [Fast Frequency Response Potential of Data Centers through Workload Modulation and UPS Coordination](https://arxiv.org/abs/2512.14128)
*Xiaojie Tao,Rajit Gadh*

Main category: eess.SY

TL;DR: 数据中心通过实时工作负载调节和UPS协调为电网提供快速频率响应，有效降低频率最低点并缩短恢复时间


<details>
  <summary>Details</summary>
Motivation: 可再生能源快速增长导致系统惯性降低，需要快速频率响应。数据中心作为大型灵活电力消费者，具有可控IT工作负载和UPS系统，有潜力为频率稳定做出贡献

Method: 开发了结合数据中心功耗和电网频率动态的动态模型，包括IT服务器、冷却系统和储能系统交互。基于频率偏差的控制策略调整服务器功率并在频率事件期间放电UPS电池

Result: 在改进的IEEE 39总线系统案例研究中，所提策略能有效降低频率最低点并缩短恢复时间，同时不损害服务质量

Conclusion: 数据中心在未来低惯性系统中作为电网支持资源具有广阔前景

Abstract: The rapid growth of renewable energy sources has significantly reduced system inertia and increased the need for fast frequency response (FFR) in modern power systems. Data centers, as large and flexible electrical consumers, hold great potential to contribute to frequency stabilization due to their controllable IT workloads and on-site uninterruptible power supply (UPS) systems. This paper investigates the feasibility of leveraging data centers for providing fast frequency response through real-time workload modulation and UPS coordination. A dynamic model combining data center power consumption and grid frequency dynamics is developed, capturing the interactions between IT servers, cooling systems, and energy storage. Control strategies based on frequency deviation are implemented to adjust server power and discharge UPS batteries during frequency events. Case studies on a modified IEEE 39-bus system demonstrate that the proposed strategy can effectively reduce frequency nadir and shorten recovery time without compromising service quality. The results highlight the promising role of data centers as grid-supporting resources in future low-inertia systems.

</details>


### [178] [Coordinated Fast Frequency Response from Electric Vehicles, Data Centers, and Battery Energy Storage Systems](https://arxiv.org/abs/2512.14136)
*Xiaojie Tao,Rajit Gadh*

Main category: eess.SY

TL;DR: 提出一个协调控制框架，聚合电动汽车、数据中心和电池储能系统，为低惯量电网提供快速频率响应，改善频率动态性能。


<details>
  <summary>Details</summary>
Motivation: 高比例可再生能源渗透显著降低了电网系统惯量，增加了对分布式和非传统资源快速频率响应的需求。虽然电动汽车、数据中心和电池储能系统各自已展示出提供亚秒级有功功率支持的能力，但它们的联合频率响应潜力尚未得到系统评估。

Method: 提出协调控制框架，聚合异构资源提供快速、稳定、可靠的快速频率响应。开发了电动汽车车队、数据中心UPS和工作负载调制、电池储能系统的动态模型，明确捕捉其响应时间、功率限制和运行约束。引入分层控制架构：上层协调器根据响应速度和可用容量动态分配快速频率响应资源，下层控制器实现实际功率响应。

Result: 基于IEEE 39节点测试系统的案例研究表明，协调的EV-DC-BESS框架相比单一资源快速频率响应，可将频率最低点改善达0.2 Hz，降低频率变化率，并加速频率恢复。结果证实协同协调显著增强了电网稳定性，特别是在低惯量场景下。

Conclusion: 这项工作凸显了多资源聚合在未来可再生能源主导电网的频率调节市场中的价值，为低惯量电网提供了有效的频率支撑解决方案。

Abstract: High renewable penetration has significantly reduced system inertia in modern power grids, increasing the need for fast frequency response (FFR) from distributed and non-traditional resources. While electric vehicles (EVs), data centers, and battery energy storage systems (BESS) have each demonstrated the capability to provide sub-second active power support, their combined frequency response potential has not been systematically evaluated. This paper proposes a coordinated control framework that aggregates these heterogeneous resources to provide fast, stable, and reliable FFR. Dynamic models for EV fleets, data center UPS and workload modulation, and BESS are developed, explicitly capturing their response times, power limits, and operational constraints. A hierarchical control architecture is introduced, where an upper-level coordinator dynamically allocates FFR among resources based on response speed and available capacity, and lower-level controllers implement the actual power response. Case studies based on the IEEE 39-bus test system demonstrate that the coordinated EV-DC-BESS framework improves frequency nadir by up to 0.2 Hz, reduces RoCoF, and accelerates frequency recovery compared with single-resource FFR. Results confirm that synergistic coordination significantly enhances grid stability, especially in low-inertia scenarios. This work highlights the value of multi-resource aggregation for future frequency regulation markets in renewable-dominated grids.

</details>


### [179] [KalMRACO: Unifying Kalman Filter and Model Reference Adaptive Control for Robust Control and Estimation of Uncertain Systems](https://arxiv.org/abs/2512.14175)
*Lauritz Rismark Fosso,Christian Holden,Sveinung Johan Ohrem*

Main category: eess.SY

TL;DR: 提出KalMRACO方法，将卡尔曼滤波器与模型参考自适应控制结合，利用MRAC的参考模型作为卡尔曼滤波系统模型，减少对系统参数先验知识的需求，并通过状态估计与测量的混合处理初始瞬态稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统卡尔曼滤波需要系统参数的先验知识，这在现实应用中可能受限。模型参考自适应控制器(MRAC)利用已知参考模型确保未知系统的输入输出行为收敛到参考模型。作者希望结合两者优势，减少卡尔曼滤波对系统参数知识的需求。

Method: 提出KalMRACO方法：1) 将MRAC的参考模型作为卡尔曼滤波的系统模型；2) 在反馈控制律中混合使用估计状态和测量值来处理初始瞬态的稳定性问题；3) 通过这种统一框架，大幅减少对底层系统参数知识的需求。

Result: 通过仿真和水下车辆实验室试验验证：1) 对参考模型状态的跟踪性能优越；2) 观测器状态收敛；3) 具有良好的噪声抑制特性。结果表明KalMRACO在实际应用中有效。

Conclusion: KalMRACO成功将卡尔曼滤波与MRAC结合，利用参考模型作为系统模型，显著减少了对系统参数先验知识的需求，同时通过状态混合策略解决了初始瞬态稳定性问题，为卡尔曼滤波在未知系统参数场景下的应用提供了有效解决方案。

Abstract: A common assumption when applying the Kalman filter is a priori knowledge of the system parameters. These parameters are not necessarily known, and this may limit real-world applications of the Kalman filter. The well-established Model Reference Adaptive Controller (MRAC) utilizes a known reference model and ensures that the input-output behavior of a potentially unknown system converges to that of the reference model. We present KalMRACO, a unification of the Kalman filter and MRAC leveraging the reference model of MRAC as the Kalman filter system model, thus eliminating, to a large degree, the need for knowledge of the underlying system parameters in the application of the Kalman filter. We also introduce the concept of blending estimated states and measurements in the feedback law to handle stability issues during the initial transient. KalMRACO is validated through simulations and lab trials on an underwater vehicle. Results show superior tracking of the reference model state, observer state convergence, and noise mitigation properties.

</details>


### [180] [A Data-Driven Approach for Electric Vehicle Powertrain Modeling](https://arxiv.org/abs/2512.14344)
*Eymen Ipek,Mario Hirz*

Main category: eess.SY

TL;DR: 提出模块化框架，通过标准化接口集成电池、逆变器、电机等关键组件模型，支持数据驱动、物理基础或经验模型，实现可扩展的系统级仿真，加速汽车电动化开发周期。


<details>
  <summary>Details</summary>
Motivation: 汽车行业电动化和动力系统复杂性增加需要加速、经济高效的开发周期。现有数据驱动模型主要在组件层面研究，缺乏系统级集成方法进行虚拟验证。

Method: 提出模块化框架，为电池、逆变器、电机等关键组件定义标准化接口，使独立开发的模型（数据驱动、物理基础或经验模型）能够轻松集成，实现可扩展的系统级建模。

Result: 建立了一个支持不同类型模型集成的模块化仿真框架，实现了系统级动力系统模拟，为虚拟验证提供了统一平台。

Conclusion: 该模块化框架能够缩短开发时间，满足现代汽车行业对敏捷开发的需求，为电动化动力系统的虚拟验证提供了系统级解决方案。

Abstract: Electrification in the automotive industry and increasing powertrain complexity demand accelerated, cost-effective development cycles. While data-driven models are recently investigated at component level, a gap exists in systematically integrating them into cohesive, system-level simulations for virtual validation. This paper addresses this gap by presenting a modular framework for developing powertrain simulations. By defining standardized interfaces for key components-the battery, inverter, and electric motor-our methodology enables independently developed models, whether data-driven, physics-based, or empirical, to be easily integrated. This approach facilitates scalable system-level modeling, aims to shorten development timelines and to meet the agile demands of the modern automotive industry.

</details>


### [181] [A Geometric Task-Space Port-Hamiltonian Formulation for Redundant Manipulators](https://arxiv.org/abs/2512.14349)
*Federico Califano,Camilla Rota,Riccardo Zanella,Antonio Franchi*

Main category: eess.SY

TL;DR: 提出了一种冗余机械臂的几何端口哈密顿公式，通过坐标变换将标准哈密顿动量分解为任务空间动量和零空间动量，并应用于IDA-PBC控制设计


<details>
  <summary>Details</summary>
Motivation: 为冗余机械臂执行微分运动学任务提供一种新的几何端口哈密顿公式，能够更好地处理任务空间和零空间动力学，为控制设计提供更清晰的物理洞察

Method: 通过从规范哈密顿动力学进行坐标变换，将标准哈密顿动量变量分解为任务空间动量变量和零空间动量变量，建立几何端口哈密顿模型

Result: 建立了冗余机械臂的几何端口哈密顿模型，阐明了该模型的性质及其与现有拉格朗日公式的关系，并成功应用于7自由度Emika Panda机器人的IDA-PBC控制仿真

Conclusion: 提出的几何端口哈密顿公式为冗余机械臂控制提供了新的理论框架，通过IDA-PBC设计能够有效稳定和塑造机器人阻抗特性，具有实际应用价值

Abstract: We present a novel geometric port-Hamiltonian formulation of redundant manipulators performing a differential kinematic task $η=J(q)\dot{q}$, where $q$ is a point on the configuration manifold, $η$ is a velocity-like task space variable, and $J(q)$ is a linear map representing the task, for example the classical analytic or geometric manipulator Jacobian matrix. The proposed model emerges from a change of coordinates from canonical Hamiltonian dynamics, and splits the standard Hamiltonian momentum variable into a task-space momentum variable and a null-space momentum variable. Properties of this model and relation to Lagrangian formulations present in the literature are highlighted. Finally, we apply the proposed model in an \textit{Interconnection and Damping Assignment Passivity-Based Control} (IDA-PBC) design to stabilize and shape the impedance of a 7-DOF Emika Panda robot in simulation.

</details>


### [182] [Equivariant Filter Cascade for Relative Attitude, Target's Angular Velocity, and Gyroscope Bias Estimation](https://arxiv.org/abs/2512.14412)
*Gil Serrano,Pedro Lourenço,Bruno J. Guerreiro,Rita Cunha*

Main category: eess.SY

TL;DR: 提出一种级联等变滤波器(EqF)用于估计非合作目标的相对姿态和角速度，解决航天器交会对接中的姿态同步问题。


<details>
  <summary>Details</summary>
Motivation: 在航天器与非合作目标（如失效卫星）的交会对接任务中，追踪航天器需要准确估计目标的相对姿态和角速度，但存在陀螺仪偏差问题，需要可靠的姿态估计算法。

Method: 采用两级级联等变滤波器(EqF)：第一级使用星敏感器测量估计追踪航天器的姿态和陀螺仪偏差；第二级使用目标坐标系中两个已知非共线向量的观测估计相对姿态和目标角速度。

Result: 对EqF级联系统进行了理论稳定性分析，并通过仿真验证了滤波器级联的性能表现。

Conclusion: 提出的级联等变滤波器能够有效解决非合作目标交会对接中的姿态估计问题，为航天器自主交会对接提供了可行的解决方案。

Abstract: Rendezvous and docking between a chaser spacecraft and an uncooperative target, such as an inoperative satellite, require synchronization between the chaser spacecraft and the target. In these scenarios, the chaser must estimate the relative attitude and angular velocity of the target using onboard sensors, in the presence of gyroscope bias. In this work, we propose a cascade of Equivariant Filters (EqF) to address this problem. The first stage of the cascade estimates the chaser's attitude and the bias, using measurements from a star tracker, while the second stage of the cascade estimates the relative attitude and the target's angular velocity, using observations of two known, non-collinear vectors fixed in the target frame. The stability of the EqF cascade is theoretically analyzed and simulation results demonstrate the filter cascade's performance.

</details>


### [183] [Nonlinear System Identification Nano-drone Benchmark](https://arxiv.org/abs/2512.14450)
*Riccardo Busetto,Elia Cereda,Marco Forgione,Gabriele Maroni,Dario Piga,Daniele Palossi*

Main category: eess.SY

TL;DR: 提出了基于Crazyflie 2.1纳米四旋翼飞行器75k真实世界样本的系统辨识基准，包含多步预测评估指标和开源数据集


<details>
  <summary>Details</summary>
Motivation: 为机器人研究中的系统辨识方法提供公平比较的基准，特别是针对具有多输入多输出、开环不稳定和非线性动态的敏捷微型飞行器

Method: 收集Crazyflie 2.1纳米四旋翼飞行器75k真实世界样本，包含4个激进轨迹的4维电机输入和13维输出测量，设计多步预测评估指标套件

Result: 创建了包含数据集、平台描述、实验设置和基准模型的完整基准套件，展示了在真实世界噪声和执行器非线性下准确预测的挑战性

Conclusion: 该开源基准为算法透明比较提供了基础，支持敏捷微型空中机器人研究，所有数据、脚本和参考实现均已开源发布

Abstract: We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics.

</details>


### [184] [Equivariant Observer for Bearing Estimation with Linear and Angular Velocity Inputs](https://arxiv.org/abs/2512.14451)
*Gil Serrano,Marcelo Jacinto,Bruno J. Guerreiro,Rita Cunha*

Main category: eess.SY

TL;DR: 设计用于单位球面上的一阶动力学系统的等变观测器，扩展了带角速度输入的现有单位方向向量动力学，增加了切空间上的线性速度输入，适用于需要稳定方向估计的视觉伺服场景。


<details>
  <summary>Details</summary>
Motivation: 在图像视觉伺服场景中，需要稳定的方向估计，同时必须考虑车辆与目标特征之间的相对速度。现有的单位方向向量动力学仅考虑角速度输入，无法处理线性速度分量，限制了在实际应用中的有效性。

Method: 通过将动力学提升到特殊正交群，设计方向向量的观测器，并证明其几乎全局渐近稳定性。同时展示了如何将等变观测器表达在原始状态流形上。

Result: 提出的算法在数值模拟中验证了有效性，能够稳定估计方向向量，并考虑了线性速度输入的影响。

Conclusion: 成功设计了一个等变观测器，扩展了单位球面上动力学系统的观测器设计，特别适用于需要同时处理角速度和线性速度输入的视觉伺服应用。

Abstract: This work addresses the problem of designing an equivariant observer for a first order dynamical system on the unit-sphere. Building upon the established case of unit bearing vector dynamics with angular velocity inputs, we introduce an additional linear velocity input projected onto the unit-sphere tangent space. This extended formulation is particularly useful in image-based visual servoing scenarios where stable bearing estimates are required and the relative velocity between the vehicle and target features must be accounted for. Leveraging lifted kinematics to the Special Orthogonal group, we design an observer for the bearing vector and prove its almost global asymptotic stability. Additionally, we demonstrate how the equivariant observer can be expressed in the original state manifold. Numerical simulation results validate the effectiveness of the proposed algorithm.

</details>


### [185] [Closed-Loop Consistent, Causal Data-Driven Predictive Control via SSARX](https://arxiv.org/abs/2512.14510)
*Aihui Liu,Magnus Jansson*

Main category: eess.SY

TL;DR: 提出一种无需基本引理的数据驱动预测控制方法，直接从输入输出数据合成MPC策略，避免使用Hankel矩阵和DeePC决策变量g。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动预测控制方法（如DeePC）依赖Willems基本引理，需要堆叠Hankel表示和引入决策变量g，方法复杂且可能不必要。需要开发更直接、因果的数据驱动控制方案。

Method: 基于多步预测器Subspace-ARX（SSARX）：1）通过高阶ARX模型估计预测器/观测器Markov参数以解耦噪声；2）通过回归学习多步过去到未来的映射，可选加入降秩约束。SSARX预测器严格因果，可自然集成到MPC框架中。

Result: 实验结果表明，SSARX在应用于受测量和过程噪声影响的闭环数据时，与其他方法相比具有竞争力。

Conclusion: 提出了一种无需基本引理的因果数据驱动预测控制方法，基于SSARX预测器，可直接从数据合成MPC策略，在噪声环境下表现良好。

Abstract: We propose a fundamental-lemma-free data-driven predictive control (DDPC) scheme for synthesizing model predictive control (MPC)-like policies directly from input-output data. Unlike the well-known DeePC approach and other DDPC methods that rely on Willems' fundamental lemma, our method avoids stacked Hankel representations and the DeePC decision variable g. Instead, we develop a closed-loop consistent, causal DDPC scheme based on the multi-step predictor Subspace-ARX (SSARX). The method first (i) estimates predictor/observer Markov parameters via a high-order ARX model to decouple the noise, then (ii) learns a multi-step past-to-future map by regression, optionally with a reduced-rank constraint. The SSARX predictor is strictly causal, which allows it to be integrated naturally into an MPC formulation. Our experimental results show that SSARX performs competitively with other methods when applied to closed-loop data affected by measurement and process noise.

</details>


### [186] [Scalable Nonlinear DeePC: Bridging Direct and Indirect Methods and Basis Reduction](https://arxiv.org/abs/2512.14535)
*Thomas O. de Jong,Mircea Lazar,Siep Weiland,Florian Dörfler*

Main category: eess.SY

TL;DR: 本文研究了非线性框架下的正则化数据驱动预测控制(DeePC)及其与子空间预测控制(SPC)的关系，提出了基于SVD的降维方法和LASSO稀疏基选择，在噪声环境下DeePC优于SPC。


<details>
  <summary>Details</summary>
Motivation: 研究非线性框架下数据驱动预测控制(DeePC)与子空间预测控制(SPC)的理论关系，解决DeePC在大规模系统中的可扩展性问题，并探索在噪声环境下的性能提升。

Method: 1) 将Π-正则化扩展到一般基函数；2) 提出基于SVD的降维方法保持与SPC的等价性；3) 推导简化Π-正则化；4) 提出基于LASSO的稀疏基选择方法从提升数据中获得简化基。

Result: 1) 在无噪声情况下，当惩罚项较大时，DeePC和SPC产生相同的绝对平均跟踪误差；2) 在有噪声测量情况下，精心调优DeePC正则化能减少绝对平均跟踪误差，优于SPC。

Conclusion: 本文建立了DeePC与SPC的理论联系，提出的降维和正则化方法提高了可扩展性，在噪声环境下DeePC通过适当正则化能获得比SPC更好的控制性能。

Abstract: This paper studies regularized data-enabled predictive control (DeePC) within a nonlinear framework and its relationship to subspace predictive control (SPC). The $Π$-regularization is extended to general basis functions and it is shown that, under suitable conditions, the resulting basis functions DeePC formulation constitutes a relaxation of basis functions SPC. To improve scalability, we introduce an SVD-based dimensionality reduction that preserves the equivalence with SPC, and we derive a reduced Π-regularization. A LASSO based sparse basis selection method is proposed to obtain a reduced basis from lifted data. Simulations on a nonlinear van der Pol oscillator model indicate that, in the absence of noise, DeePC and SPC yield equivalent absolute mean tracking errors (AMEs) when large penalties are applied. In contrast, under noisy measurements, careful tuning of the DeePC regularization results in a reduced AME, outperforming SPC.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [187] [One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing](https://arxiv.org/abs/2512.13892)
*Albert Dorador*

Main category: stat.ML

TL;DR: 提出一种确定性最优置换方法替代传统随机置换，用于评估机器学习特征重要性，并引入系统性变量重要性框架用于模型压力测试


<details>
  <summary>Details</summary>
Motivation: 传统置换方法依赖重复随机置换，存在计算开销大和随机不稳定的问题，需要更可靠、高效的特征重要性评估方法，特别是在黑盒模型和监管合规场景下

Method: 1) 用单个确定性最优置换替代多次随机置换；2) 引入系统性变量重要性框架，考虑特征相关性，用于模型压力测试和公平性审计

Result: 在近200个场景中验证，包括家庭金融和信用风险应用，在小样本、高维度和低信噪比等挑战性场景下表现出更好的偏差-方差权衡和准确性

Conclusion: 该方法提供了非随机、更快、更稳定的特征重要性评估，系统性变量重要性框架能揭示标准方法忽略的特征依赖关系，有助于模型公平性审计和系统性风险评估

Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.

</details>


### [188] [Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics](https://arxiv.org/abs/2512.13997)
*Aaron Wei,Milad Jalali,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 本文扩展了MMD双样本检验理论，解决了传统方法需要等样本量的限制，提出了适用于不等样本量的广义U统计量框架，并给出了新的检验优化准则。


<details>
  <summary>Details</summary>
Motivation: 现有基于最大均值差异（MMD）的双样本检验方法通常假设两个分布具有相等样本量，这在实际应用中需要丢弃有价值的数据，降低了检验功效。本文旨在解决这一长期存在的限制。

Method: 通过扩展广义U统计量理论，将其应用于通常的MMD估计量，获得了不等样本量下MMD估计量的渐近分布新特征（特别是在先前部分结果所需的比例范围之外）。

Result: 该方法保留了所有可用数据，提高了检验准确性和实际应用性。同时提供了MMD估计量方差的更清晰特征，揭示了即使MMD非零也可能出现退化估计量的可能性，并证明了这在常见情况下不会发生。

Conclusion: 本文提出的不等样本量MMD检验框架解决了实际应用中的数据浪费问题，提供了新的检验优化准则，增强了双样本检验在现实场景中的适用性和准确性。

Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.

</details>


### [189] [On the Hardness of Conditional Independence Testing In Practice](https://arxiv.org/abs/2512.14000)
*Zheng He,Roman Pogodin,Yazhe Li,Namrata Deka,Arthur Gretton,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 本文分析了条件独立性检验的实践挑战，特别关注KCI检验，揭示了条件均值嵌入估计误差对Type-I错误的关键影响，以及条件核选择对检验功效和Type-I错误的重要作用。


<details>
  <summary>Details</summary>
Motivation: 条件独立性检验在机器学习和统计学中至关重要，应用于因果发现、预测公平性评估和分布外鲁棒性等。Shah和Peters（2020）的理论结果表明，与无条件情况不同，不存在普遍有限样本有效的检验方法。然而，这一理论结果（基于"隐藏"依赖性）似乎无法解释实践中常见CI检验的频繁失败现象。本文旨在探究KCI检验的实际行为机制。

Method: 研究Kernel-based Conditional Independence (KCI)检验，并证明许多近期检验方法所基于的广义协方差度量（Generalized Covariance Measure）几乎是KCI的特殊情况。分析KCI检验实践行为的主要因素，特别关注条件均值嵌入估计误差对Type-I错误的影响，以及条件核选择对检验功效和Type-I错误的重要性。

Result: 研究发现：1）条件均值嵌入估计误差对Type-I错误起着关键作用；2）选择适当的条件核对于获得良好的检验功效至关重要，但这一因素在先前工作中未被充分认识；3）条件核选择虽然能提高检验功效，但往往也会增加Type-I错误。

Conclusion: 本文揭示了KCI检验实践行为的关键机制，特别是条件均值嵌入估计误差对Type-I错误的重要影响，以及条件核选择在平衡检验功效和Type-I错误中的双重作用。这些发现有助于解释实践中条件独立性检验的失败现象，并为改进检验方法提供了理论指导。

Abstract: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on "hiding" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.

</details>


### [190] [Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms](https://arxiv.org/abs/2512.14221)
*Jiarong Fan,Juhyun Park. Thi Phuong Thuy Vo,Nicolas Brunel*

Main category: stat.ML

TL;DR: 提出一种处理缺失协变量的保形预测方法，通过预插补-掩码-校正框架保证边际覆盖和掩码条件有效性，相比标准方法显著减少预测区间宽度。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在面临缺失协变量时无法保证覆盖性，而由不同缺失模式引起的异质性使得掩码条件有效性比边际覆盖更理想。

Method: 提出预插补-掩码-校正框架，采用重新加权的保形预测程序，在校准数据集进行分布插补（多重插补）后校正预测集，与标准插补流程兼容。

Result: 方法保证边际覆盖和掩码条件有效性，相比标准MCV方法显著减少预测区间宽度，同时在合成和真实数据集上验证了有效性。

Conclusion: 该方法为处理缺失协变量的不确定性量化提供了理论保证和实用框架，在保持目标保证的同时提高了预测效率。

Abstract: Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.

</details>


### [191] [Improving the Accuracy of Amortized Model Comparison with Self-Consistency](https://arxiv.org/abs/2512.14308)
*Šimon Kucharský,Aayush Mishra,Daniel Habermann,Stefan T. Radev,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: ABI方法在模型错误设定下不稳定，自洽性训练可提高鲁棒性。基于参数后验的模型比较方法优于直接近似证据的方法，SC训练在似然可用时效果显著。


<details>
  <summary>Details</summary>
Motivation: 摊销贝叶斯推断在模型错误设定下表现不稳定，特别是在模型比较场景中多个模型存在错误设定时。需要研究自洽性训练如何改善摊销模型比较的鲁棒性。

Method: 研究了四种不同的摊销模型比较方法，包括基于参数后验估计边际似然的方法和直接近似模型证据的方法。在合成和真实数据集上评估自洽性训练的效果。

Result: 基于参数后验估计边际似然的方法始终优于直接近似模型证据的方法。当似然可用时，SC训练显著提高鲁棒性，即使在严重模型错误设定下。对于无法访问解析似然的方法，SC训练效果有限且不一致。

Conclusion: 建议实用的摊销贝叶斯模型比较指导：优先选择基于参数后验的方法，并在经验数据集上使用SC训练来减轻模型错误设定下的外推偏差。

Abstract: Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.

</details>


### [192] [Continual Learning at the Edge: An Agnostic IIoT Architecture](https://arxiv.org/abs/2512.14311)
*Pablo García-Santaclara,Bruno Fernández-Castro,Rebeca P. Díaz-Redondo,Carlos Calvo-Moa,Henar Mariño-Bodelón*

Main category: stat.ML

TL;DR: 提出一种结合边缘计算和增量学习的新方法，用于制造业实时质量控制，解决传统集中式计算延迟和带宽限制问题


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增导致传统集中式计算面临延迟和带宽限制；边缘计算虽能解决距离问题，但传统机器学习算法不适应边缘环境中数据动态连续到达的特点

Method: 将增量学习理念应用于工业边缘计算场景，采用持续学习方法减少灾难性遗忘的影响

Result: 为制造业实时质量控制提供了高效有效的解决方案

Conclusion: 增量学习与边缘计算的结合为解决工业场景中的实时数据处理和质量控制问题提供了有前景的解决方案

Abstract: The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.

</details>


### [193] [From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification](https://arxiv.org/abs/2512.14404)
*Hangjun Cho,Fabio V. G. Amaral,Andrei A. Klishin,Cassio M. Oishi,Steven L. Brunton*

Main category: stat.ML

TL;DR: 论文提出了一种基于分数的库选择方法，用于改进SINDy类算法的字典选择，通过理论分析和数值实验验证了该方法在动力系统识别中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SINDy算法中的字典选择缺乏系统指导，作者希望提供实用的数据驱动建模指导，特别是针对STLS（顺序阈值最小二乘法）算法，通过分数引导的库选择来改进稀疏回归。

Method: 重新审视基于字典的稀疏回归，特别是STLS算法，提出分数引导的库选择方法。该方法利用投影重构误差（称为分数）和字典项之间的互相关性来指导字典选择，并在原始和弱SINDy体系下进行理论分析。

Result: 数值实验表明，基于分数的筛选方法在常微分方程和偏微分方程上都能有效提高动力系统识别的准确性和可解释性，增强了数据驱动建模的鲁棒性。

Conclusion: 分数引导的字典细化方法可以帮助SINDy用户在某些情况下提高数据驱动发现控制方程的鲁棒性，为稀疏回归提供了实用的字典选择指导。

Abstract: In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.

</details>


### [194] [LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts](https://arxiv.org/abs/2512.14604)
*Prasanjit Dubey,Aritra Guha,Zhengyi Zhou,Qiong Wu,Xiaoming Huo,Paromita Dubey*

Main category: stat.ML

TL;DR: LLmFPCA-detect：结合LLM文本嵌入与函数数据分析的框架，用于稀疏纵向文本数据的聚类和异常检测


<details>
  <summary>Details</summary>
Motivation: 稀疏纵向文本数据（如客户评论、社交媒体帖子、电子病历）具有巨大潜力，但缺乏专门方法，且数据噪声大、异质性强、易出现异常，难以检测关键模式

Method: 1) 使用LLM提示将文本嵌入到应用特定的数值空间；2) 在数值空间中进行稀疏多元函数主成分分析(mFPCA)恢复主要群体特征；3) 结合静态协变量进行数据分割、无监督异常检测和推断；4) 利用LLM进行动态关键词分析

Result: 在Amazon客户评论轨迹和Wikipedia讨论页评论流两个公开数据集上验证，跨领域表现优异，优于现有基准方法，并能提升预测性能

Conclusion: LLmFPCA-detect为稀疏纵向文本数据提供了一种灵活有效的分析框架，能够发现数据中的聚类和异常模式，支持下游任务，具有实际应用价值

Abstract: Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [195] [Optimal Subgradient Methods for Lipschitz Convex Optimization with Error Bounds](https://arxiv.org/abs/2512.13863)
*Alex L. Wang*

Main category: math.OC

TL;DR: 论文研究了满足一般误差界的Lipschitz凸优化问题的迭代复杂度，证明了次梯度下降法在Polyak步长或衰减步长下能达到极小极大最优收敛保证。


<details>
  <summary>Details</summary>
Motivation: 研究满足误差界的凸优化问题的迭代复杂度，特别是次梯度方法在这种问题类上的最优收敛性能。

Method: 使用次梯度下降法，采用Polyak步长或衰减步长，并提出了新的下界构造方法，同时满足零链条件和全局误差界。

Result: 证明了次梯度下降法对于这类问题在减小到最优解距离方面达到极小极大最优收敛保证。

Conclusion: 对于满足一般误差界的Lipschitz凸优化问题，次梯度下降法在适当步长选择下能达到最优收敛性能。

Abstract: We study the iteration complexity of Lipschitz convex optimization problems satisfying a general error bound. We show that for this class of problems, subgradient descent with either Polyak stepsizes or decaying stepsizes achieves minimax optimal convergence guarantees for decreasing distance-to-optimality. The main contribution is a novel lower-bounding argument that produces hard functions simultaneously satisfying zero-chain conditions and global error bounds.

</details>


### [196] [DAMA: A Unified Accelerated Approach for Decentralized Nonconvex Minimax Optimization-Part I: Algorithm Development and Results](https://arxiv.org/abs/2512.13920)
*Haoyuan Cai,Sulaiman A. Alghunaim,Ali H. Sayed*

Main category: math.OC

TL;DR: DAMA是一个去中心化加速极小极大优化框架，集成了在线/离线随机算法与多种去中心化学习策略，实现了算法、梯度估计和分析框架的三重统一，达到了最先进的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化极小极大优化方法缺乏统一的加速框架，无法同时处理非凸Polyak-Lojasiewicz问题和多种去中心化学习策略。需要开发一个能够统一现有偏差校正技术、动量方法和方差缩减技术，并能获得更紧网络相关边界的通用框架。

Method: 提出DAMA框架：1) 统一去中心化学习策略，包含梯度跟踪等偏差校正技术并引入新变体；2) 提出GRACE概率梯度估计器，统一动量方法和无循环方差缩减技术；3) 开发统一分析框架，建立通用性能边界。框架整合了在线和离线随机极小极大算法。

Result: DAMA实现了最先进的样本复杂度，获得了比现有方法更紧的网络相关边界。这是首个实现去中心化学习策略和加速梯度技术多层次统一的框架，适用于一般随机优化问题。

Conclusion: DAMA为去中心化非凸Polyak-Lojasiewicz极小极大优化提供了一个统一的加速算法框架，实现了算法设计、梯度估计和分析方法的三重统一，达到了当前最优性能。该工作为算法开发，理论分析和实验验证在第二部分中提供。

Abstract: In this work and its accompanying Part II [1], we develop an accelerated algorithmic framework, DAMA (Decentralized Accelerated Minimax Approach), for nonconvex Polyak-Lojasiewicz minimax optimization over decentralized multi-agent networks. Our approach integrates online and offline stochastic minimax algorithms with various decentralized learning strategies, yielding a versatile framework with broader flexibility than existing methods. Our unification is threefold: (i) we propose a unified decentralized learning strategy for minimax optimization that subsumes existing bias-correction techniques, such as gradient tracking, while introducing new variants that achieve tighter network-dependent bounds; (ii) we introduce a probabilistic gradient estimator, GRACE (Gradient Acceleration Estimator), which unifies momentum-based methods and loopless variance-reduction techniques for constructing accelerated gradients within DAMA, and is broadly applicable to general stochastic optimization problems; and (iii) we develop a unified analytical framework that establishes a general performance bound for DAMA, achieving state-of-the-art results with the best-known sample complexity. To the best of our knowledge, DAMA is the first framework to achieve a multi-level unification of decentralized learning strategies and accelerated gradient techniques. This work focuses on algorithm development and the main results, while Part II provides the theoretical analysis that substantiates these results and presents empirical validation across diverse network topologies using synthetic and real-world datasets.

</details>


### [197] [DAMA: A Unified Accelerated Approach for Decentralized Nonconvex Minimax Optimization-Part II: Convergence and Performance Analyses](https://arxiv.org/abs/2512.13923)
*Haoyuan Cai,Sulaiman A. Alghunaim,Ali H. Sayed*

Main category: math.OC

TL;DR: 本文是DAMA框架的第二部分，专注于收敛性分析和性能边界证明，为第一部分提出的加速去中心化极小极大优化算法提供理论支撑。


<details>
  <summary>Details</summary>
Motivation: 在第一部分提出了DAMA加速算法框架和GRACE梯度估计器的基础上，第二部分需要提供严格的理论收敛性证明和性能边界分析，以验证算法的理论保证。

Method: 使用第一部分推导的变换递归关系，建立DAMA的统一性能边界，并针对不同特例（如STORM、PAGE、Loopless SARAH等）细化边界分析。

Result: 建立了DAMA的统一性能边界理论框架，为各种具体算法变体提供了收敛性保证，验证了第一部分提出的算法在稀疏连接网络上的优越性能。

Conclusion: 本文为DAMA加速去中心化极小极大优化算法提供了完整的理论分析，证明了其收敛性和性能边界，为实际应用提供了坚实的理论基础。

Abstract: In Part I of this work [1], we developed an accelerated algorithmic framework, DAMA (Decentralized Accelerated Minimax Approach), for nonconvex Polyak-Lojasiewicz (PL) minimax optimization over decentralized multi-agent networks. To further enhance convergence in online and offline scenarios, Part I of this work [1] also proposed a novel accelerated gradient estimator, namely, GRACE (GRadient ACceleration Estimator), which unifies several momentum-based methods (e.g., STORM) and loopless variance-reduction techniques (e.g., PAGE, Loopless SARAH), thereby enabling accelerated gradient updates within DAMA. Part I reported a unified performance bound for DAMA and refined guarantees for specific algorithmic instances, demonstrating the superior performance of several new variants on sparsely connected networks. In this Part II, we focus on the convergence and performance bounds that substantiate the main results presented in Part I [1]. In particular, we establish a unified performance bound for DAMA using the transformed recursion derived in Part I and subsequently refine this bound for its various special cases.

</details>


### [198] [Volume Formulae for the Convex Hull of the Graph of a Trilinear Monomial: A Complete Characterization for General Box Domains](https://arxiv.org/abs/2512.13964)
*Lillian Makhoul,Emily Speakman*

Main category: math.OC

TL;DR: 该论文扩展了三次单项式凸包体积公式到混合符号域的一般情况，填补了现有文献仅处理非负域的空白。


<details>
  <summary>Details</summary>
Motivation: 在混合整数非线性规划的空间分支定界法中，需要有效的非凸集凸外近似。三次单项式作为基础库函数频繁出现，其凸包提供最紧的松弛。现有研究仅处理非负域情况，而实践中变量常具有混合符号域，这一限制是文献中的重要空白。

Method: 通过详尽的案例分析，将混合体积技术扩展到一般情况。研究移除非负域假设后凸包多面体的结构变化，识别出六种不同的体积公式来表征所有可能的参数配置。

Result: 移除非负域假设改变了凸包多面体的基础结构，得到了六个不同的体积公式，这些公式共同描述了所有可能的参数配置，从而填补了文献空白。

Conclusion: 该工作通过扩展到一般情况，为三次单项式凸包提供了完整的体积公式，解决了混合符号域问题，为混合整数非线性规划中的松弛方法提供了更全面的理论基础。

Abstract: Solving difficult mixed-integer nonlinear programs via spatial branch-and-bound requires effective convex outer-approximations of nonconvex sets. In this framework, complex problem formulations are decomposed into simpler library functions, whose relaxations are then composed to build relaxations of the overall problem. The trilinear monomial serves as one such fundamental library function, appearing frequently as a building block across diverse applications. By definition, its convex hull provides the tightest possible relaxation and thus serves as a benchmark for evaluating alternatives. Mixed volume techniques have yielded a parameterized volume formula for the convex hull of the graph of a trilinear monomial; however, existing results only address the case where all six bounds of the box domain are nonnegative. This restriction represents a notable gap in the literature, as variables with mixed-sign domains arise naturally in practice. In this work, we close the gap by extending to the general case via an exhaustive case analysis. We demonstrate that removing the nonnegative domain assumption alters the underlying structure of the convex hull polytope, leading to six distinct volume formulae that together characterize all possible parameter configurations.

</details>


### [199] [Complete Characterizations of Well-Posedness in Parametric Composite Optimization](https://arxiv.org/abs/2512.14124)
*Boris S. Mordukhovich,Peipei Tang,Chengjing Wang*

Main category: math.OC

TL;DR: 该论文为扰动复合优化的KKT系统提供了完整的适定性刻画，建立了二阶充分条件、二阶资格条件与KKT系统Lipschitz-like性质之间的等价关系。


<details>
  <summary>Details</summary>
Motivation: 复合优化问题在机器学习和工程中广泛应用，但缺乏统一的稳定性分析框架。现有研究对KKT系统的适定性刻画不完整，需要建立更一般的理论框架来分析解的稳定性和敏感性。

Method: 利用复合模型的抛物线正则性，引入二阶次导数和新的二阶变分函数。提出强二阶充分条件(SSOSC)，建立二阶资格条件(SOQC)与约束非退化条件的等价关系。通过分析KKT系统的Lipschitz-like/Aubin性质，建立多个等价条件。

Result: 证明了SSOSC自然推广了非线性规划中的经典二阶充分条件。获得了SOQC的多个等价刻画，证明了在C²-锥可约性下，KKT系统的Lipschitz-like性质等价于强正则性，并在可验证假设下等价于广义雅可比矩阵的非奇异性。

Conclusion: 该研究为复合优化问题的稳定性和敏感性分析提供了统一严谨的理论框架，也为数值算法的设计和验证奠定了基础，填补了复合优化理论中的重要空白。

Abstract: This paper provides complete characterization of well-posedness for Karush-Kuhn-Tucker (KKT) systems associated with general problems of perturbed composite optimization. Leveraging the property of parabolic regularity for composite models, we show that the second-order subderivative of the cost function reduces to the novel second-order variational function playing a crucial role in the subsequent analysis. This foundational result implies that the strong second-order sufficient condition (SSOSC) introduced in this work for the general class of composite optimization problems naturally extends the classical second-order sufficient condition in nonlinear programming. Then we obtain several equivalent characterizations of the second-order qualification condition (SOQC) and highlight its equivalence to the constraint nondegeneracy condition under the $\mathcal{C}^{2}$-cone reducibility assumption. These insights lead us to multiple equivalent conditions for the major Lipschitz-like/Aubin property of KKT systems, including the SOQC combined with the new second-order subdifferential condition and the SOQC combined with tilt stability of local minimizers. Furthermore, under $\mathcal{C}^{2}$-cone reducibility, we prove that the Lipschitz-like property of the reference KKT system is equivalent to its strong regularity. Finally, we demonstrate that the Lipschitz-like property is equivalent to the nonsingularity of the generalized Jacobian associated with the KKT system under a certain verifiable assumption. These results provide a unified and rigorous framework for analyzing stability and sensitivity of solutions to composite optimization problems, as well as for the design and justification of numerical algorithms.

</details>


### [200] [Shape design with phase field methods for structural hemivariational inequalities in contact problems](https://arxiv.org/abs/2512.14226)
*Yixin Tan,Fang Feng,Shengfeng Zhu*

Main category: math.OC

TL;DR: 本文针对含摩擦的结构接触问题，开发了形状设计和拓扑优化的数学模型，基于非凸非光滑的hemivariational不等式，提出了形状敏感度分析和三种相场拓扑优化算法。


<details>
  <summary>Details</summary>
Motivation: 结构接触问题中的摩擦效应通常用非线性、非光滑、非凸的hemivariational不等式描述，这比标准变分不等式更一般、更真实，但由于非凸性也更具挑战性。需要开发适用于此类问题的形状和拓扑优化方法。

Method: 1) 对能量型形状泛函进行严格的形状敏感度分析，推导hemivariational不等式的欧拉导数；2) 通过渐近分析证明正则化方法的合理性；3) 提出数值边界变分方法用于形状优化；4) 开发三种相场拓扑优化算法：梯度流相场法、成本泛函二阶正则化相场法、相场法与拓扑导数耦合方法。

Result: 建立了hemivariational不等式形状优化的理论框架，提出了新的形状敏感度分析和拓扑优化算法。数值实验验证了所提形状和拓扑优化算法的准确性和有效性。

Conclusion: 本文为含摩擦接触问题的形状和拓扑优化提供了系统的理论框架和数值方法，首次将相场方法应用于hemivariational不等式的形状设计，为解决非凸非光滑接触问题的优化提供了有效工具。

Abstract: We develop mathematical models for shape design and topology optimization in structural contact problems involving friction between elastic and rigid bodies. The governing mechanical constraint is a nonlinear, non-smooth, and non-convex hemivariational inequality, which provides a more general and realistic description of frictional contact forces than standard variational inequalities, but is also more challenging due to its non-convexity. For energy-type shape functionals, the Eulerian derivative of the hemivariational inequality is derived through rigorous shape sensitivity analysis. The rationality of a regularization approach is justified by asymptotic analysis, and this method is further applied to handle the non-smoothness of general shape functionals in the sensitivity framework. Based on these theoretical results, a numerical boundary variational method is proposed for shape optimization. For topology optimization, three phase-field algorithms are developed: a gradient-flow phase-field method, a phase-field method with second-order regularization of the cost functional, and a phase-field method coupled with topological derivatives. To the best of our knowledge, these approaches are new for shape design in hemivariational inequalities. Various numerical experiments confirm the accuracy and effectiveness of the proposed shape and topology optimization algorithms.

</details>


### [201] [Randomized multi-class classification under system constraints: a unified approach via post-processing](https://arxiv.org/abs/2512.14246)
*Evgenii Chzhen,Mohamed Hebiri,Gayane Taturyan*

Main category: math.OC

TL;DR: 提出一种后处理方法，通过调整基础分类器来满足系统级约束，无需重新训练，使用熵正则化和对偶优化技术构建可行解。


<details>
  <summary>Details</summary>
Motivation: 在多类分类问题中，系统级约束（如公平性、弃权、流失率等）通常表示为随机化分类器的线性泛函。现有方法往往需要重新训练模型，我们希望开发一种后处理方法，在不重新训练的情况下调整已有分类器以满足约束。

Method: 将问题形式化为随机化分类器上的线性约束随机规划，利用熵正则化技术和对偶优化方法构建可行解。通过后处理调整基础分类器的输出分布，确保满足约束条件。

Result: 在最小假设下，为算法的最终输出提供了有限样本的风险和约束满足保证。框架能够处理广泛的约束类别，包括公平性、弃权和流失率要求。

Conclusion: 该方法提供了一种灵活的后处理框架，能够在不需要重新训练的情况下，使现有分类器满足各种系统级约束，具有理论保证和实际应用价值。

Abstract: We study the problem of multi-class classification under system-level constraints expressible as linear functionals over randomized classifiers. We propose a post-processing approach that adjusts a given base classifier to satisfy general constraints without retraining. Our method formulates the problem as a linearly constrained stochastic program over randomized classifiers, and leverages entropic regularization and dual optimization techniques to construct a feasible solution. We provide finite-sample guarantees for the risk and constraint satisfaction for the final output of our algorithm under minimal assumptions. The framework accommodates a broad class of constraints, including fairness, abstention, and churn requirements.

</details>


### [202] [Towards Real Time Control of Water Engineering with Nonlinear Hyperbolic Partial Differential Equations](https://arxiv.org/abs/2512.14387)
*Fabio DiFonzo,Michael Holst,Morteza Kimiaei,Vyacheslav Kungurtsev,Songqiang Qiu*

Main category: math.OC

TL;DR: 该论文以受非线性浅水偏微分方程约束的混合整数优化问题为案例，分析实现此类软件所需的理论、算法和计算组件，而非提供完整解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究受非线性浅水PDE约束的混合整数优化问题，应用于水电梯级河流流量管理等实际场景。现实部署需要同时处理非线性/非光滑PDE动态、控制到状态映射的理论保证有限、以及满足运营决策的计算性能。

Method: 将该问题作为案例研究，识别并组织实现所需的核心组件：数学理论、数值优化方法和大规模科学计算工具。构建框架来突出开放挑战和中间研究方向。

Result: 提出了一个系统框架，明确了实现此类软件所需的数学、算法和计算组件。该框架揭示了当前研究中的开放挑战，并为更有限的相关问题以及未来大规模协作研究提供了指导。

Conclusion: 当前处理这类复杂优化问题的理论、算法和计算工具仍处于研究阶段。该论文通过构建框架来组织必要组件，为未来研究方向和协作努力提供路线图，而非提供即时解决方案。

Abstract: This paper examines aspirational requirements for software addressing mixed-integer optimization problems constrained by the nonlinear Shallow Water partial differential equations (PDEs), motivated by applications such as river-flow management in hydropower cascades. Realistic deployment of such software would require the simultaneous treatment of nonlinear and potentially non-smooth PDE dynamics, limited theoretical guarantees on the existence and regularity of control-to-state mappings under varying boundary conditions, and computational performance compatible with operational decision-making. In addition, practical settings motivate consideration of uncertainty arising from forecasts of demand, inflows, and environmental conditions. At present, the theoretical foundations, numerical optimization methods, and large-scale scientific computing tools required to address these challenges in a unified and tractable manner remain the subject of ongoing research across the associated research communities. Rather than proposing a complete solution, this work uses the problem as a case study to identify and organize the mathematical, algorithmic, and computational components that would be necessary for its realization. The resulting framework highlights open challenges and intermediate research directions, and may inform both more circumscribed related problems and the design of future large-scale collaborative efforts aimed at addressing such objectives.

</details>


### [203] [A preconditioned second-order convex splitting algorithm with extrapolation](https://arxiv.org/abs/2512.14468)
*Xinhua Shen,Hongpeng Sun*

Main category: math.OC

TL;DR: 提出一种结合外推策略的预条件二阶凸分裂算法，用于高效求解非凸优化问题，结合BDF2和外推方法，通过隐显格式简化子问题，理论证明全局收敛，实验显示高效性。


<details>
  <summary>Details</summary>
Motivation: 非凸优化问题在现代机器学习和数据科学中广泛存在，需要开发高效算法来解决这类问题。现有算法在计算效率和收敛性方面存在改进空间。

Method: 将外推策略引入预条件二阶凸分裂算法，结合二阶向后差分公式(BDF2)和外推方法，采用隐显格式通过预条件过程简化子问题，降低计算复杂度。

Result: 算法在基准问题、SCAD正则化的最小二乘问题和图像分割问题上进行数值实验，结果显示算法具有高效性，减少了求解时间并保持竞争性能。

Conclusion: 提出的算法能高效解决非凸优化问题，无需显著增加计算开销，理论分析基于Kurdyka-Łojasiewicz性质建立了全局收敛性，实验验证了算法的优越性能。

Abstract: Nonconvex optimization problems are widespread in modern machine learning and data science. We introduce an extrapolation strategy into a class of preconditioned second-order convex splitting algorithms for nonconvex optimization problems. The proposed algorithms combine second-order backward differentiation formulas (BDF2) with an extrapolation method. Meanwhile, the implicit-explicit scheme simplifies the subproblem through a preconditioned process. As a result, our approach solves nonconvex problems efficiently without significant computational overhead. Theoretical analysis establishes global convergence of the algorithms using Kurdyka-Łojasiewicz properties. Numerical experiments include a benchmark problem, the least squares problem with SCAD regularization, and an image segmentation problem. These results demonstrate that our algorithms are highly efficient, as they achieve reduced solution times and competitive performance.

</details>


### [204] [An Inexact Modified Quasi-Newton Method for Nonsmooth Regularized Optimization](https://arxiv.org/abs/2512.14507)
*Nathan Allaire,Sébastien Le Digabel,Dominique Orban*

Main category: math.OC

TL;DR: iR2N是一种改进的近端拟牛顿方法，用于最小化光滑函数f和下半连续近端有界函数h的和，允许对f、其梯度和相关近端算子进行不精确评估。该方法特别适用于近端算子通过可提前停止的迭代过程计算的情况，或f和∇f的精度可控制的情况，从而显著节省计算量。


<details>
  <summary>Details</summary>
Motivation: 许多优化问题涉及光滑函数和非光滑函数的和，其中精确计算近端算子或函数值可能计算成本高昂。在实际应用中，允许不精确评估可以显著减少计算负担，特别是当近端算子需要通过迭代方法计算时。

Method: iR2N在每次迭代中近似最小化f的二次模型、h的模型以及确保全局收敛的自适应二次正则化项的和。该方法允许对f、其梯度和近端算子进行不精确评估，特别适用于近端算子通过可提前停止的迭代过程计算的情况。

Result: 在标准精度假设下，证明了全局收敛性，即一阶驻点度量收敛到零，最坏情况评估复杂度为O(ε^{-2})。数值实验使用ℓ_p范数、ℓ_p全变分和非凸伪p-范数球的指示函数，展示了方法的有效性和灵活性，以及受控不精确性如何显著减少计算量。

Conclusion: iR2N为处理非凸光滑和非光滑函数之和的优化问题提供了一种灵活高效的框架，通过允许不精确评估实现了显著的计算节省，特别适用于近端算子计算成本高昂的实际应用场景。

Abstract: We introduce iR2N, a modified proximal quasi-Newton method for minimizing the sum of a smooth function $f$ and a lower semi-continuous prox-bounded function $h$, allowing inexact evaluations of $f$, its gradient, and the associated proximal operators. Both $f$ and $h$ may be nonconvex. iR2N is particularly suited to settings where proximal operators are computed via iterative procedures that can be stopped early, or where the accuracy of $f$ and $\nabla f$ can be controlled, leading to significant computational savings. At each iteration, the method approximately minimizes the sum of a quadratic model of $f$, a model of $h$, and an adaptive quadratic regularization term ensuring global convergence. Under standard accuracy assumptions, we prove global convergence in the sense that a first-order stationarity measure converges to zero, with worst-case evaluation complexity $O(ε^{-2})$. Numerical experiments with $\ell_p$ norms, $\ell_p$ total variation, and the indicator of the nonconvex pseudo $p$-norm ball illustrate the effectiveness and flexibility of the approach, and show how controlled inexactness can substantially reduce computational effort.

</details>


### [205] [The Innovation Null Space of the Kalman Predictor: A Stochastic Perspective for DeePC](https://arxiv.org/abs/2512.14520)
*Aihui Liu,Magnus Jansson*

Main category: math.OC

TL;DR: 论文探讨了在噪声影响下，Willems基本引理中决策变量g的最佳选择问题，发现存在一个最优子空间——创新Hankel矩阵的零空间，能使预测器更接近卡尔曼预测器。


<details>
  <summary>Details</summary>
Motivation: 当线性时不变系统受到噪声影响时，Willems基本引理中的决策变量g应该如何选择？现有方法缺乏理论指导，需要找到在噪声环境下最优的g选择策略。

Method: 提出卡尔曼滤波器基本引理(KFFL)，将Willems引理应用于卡尔曼预测器。证明对于高斯噪声系统，决策变量g的最优子空间是创新Hankel矩阵的零空间。

Result: 当决策向量位于创新零空间时，得到的预测器更接近卡尔曼预测器。这一理论框架解释了多种现有数据驱动预测控制方法：正则化DeePC方案是创新零空间约束的软版本，工具变量方法通过构造强制执行该约束，ARX方法则显式估计创新零空间。

Conclusion: 创新Hankel矩阵的零空间是噪声环境下决策变量g的最优选择，这一理论发现统一了多种数据驱动控制方法，为噪声环境下的系统预测提供了理论基础。

Abstract: Willems' fundamental lemma uses a key decision variable $g$ to combine measured input-output data and describe trajectories of a linear time-invariant system. In this paper, we ask: what is a good choice for this vector $g$ when the system is affected by noise? For a linear system with Gaussian noise, we show that there exists an optimal subspace for this decision variable $g$, which is the null space of the innovation Hankel matrix. If the decision vector lies in this null space, the resulting predictor gets closer to the Kalman predictor. To show this, we use a result that we refer to as the Kalman Filter Fundamental Lemma (KFFL), which applies Willems' lemma to the Kalman predictor. This viewpoint also explains several existing data-driven predictive control methods: regularized DeePC schemes act as soft versions of the innovation null-space constraint, instrumental-variable methods enforce it by construction, and ARX-based approaches explicitly estimate this innovation null space.

</details>


### [206] [Enhancing Orbital Debris Remediation with Reconfigurable Space-Based Laser Constellations](https://arxiv.org/abs/2512.14682)
*David O. Williams Rogers,Hang Woon Lee*

Main category: math.OC

TL;DR: 该论文提出通过星座重构策略提升基于激光的太空碎片清除系统的适应性和可扩展性，以应对日益增长的轨道碎片威胁。


<details>
  <summary>Details</summary>
Motivation: 轨道碎片对太空任务和地球轨道环境的长期可持续性构成日益严重的威胁。虽然已有基于多卫星激光协同清除碎片的概念验证，但随着碎片数量快速增长，现有系统的可扩展性和响应能力存在局限。

Method: 提出星座重构作为系统级策略，通过协调轨道机动使激光卫星动态适应碎片分布变化。形式化为可重构激光-碎片参与调度问题(R-L2D-ESP)，采用滚动时域方法处理组合优化复杂性。

Result: 实验表明可重构星座显著优于静态星座，实现了更大的碎片清除能力和成功清除更多碎片物体。敏感性分析识别了对清除性能影响最大的关键参数。

Conclusion: 星座重构代表了基于激光的碎片清除系统的有前景的进展，提供了增强轨道碎片清除方法所需的适应性和可扩展性。

Abstract: Orbital debris poses an escalating threat to space missions and the long-term sustainability of Earth's orbital environment. The literature proposes various approaches for orbital debris remediation, including the use of multiple space-based lasers that collaboratively engage debris targets. While the proof of concept for this laser-based approach has been demonstrated, critical questions remain about its scalability and responsiveness as the debris population continues to expand rapidly. This paper introduces constellation reconfiguration as a system-level strategy to address these limitations. Through coordinated orbital maneuvers, laser-equipped satellites can dynamically adapt their positions to respond to evolving debris distributions and time-critical events. We formalize this concept as the Reconfigurable Laser-to-Debris Engagement Scheduling Problem (R-L2D-ESP), an optimization framework that determines the optimal sequence of constellation reconfigurations and laser engagements to maximize debris remediation capacity, which quantifies the constellation's ability to nudge, deorbit, or perform just-in-time collision avoidance maneuvers on debris objects. To manage the complexity of this combinatorial optimization problem, we employ a receding horizon approach. Our experiments reveal that reconfigurable constellations significantly outperform static ones, achieving greater debris remediation capacity and successfully deorbiting substantially more debris objects. Additionally, our sensitivity analyses identify the key parameters that influence remediation performance the most, providing essential insights for future system design. These findings demonstrate that constellation reconfiguration represents a promising advancement for laser-based debris removal systems, offering the adaptability and scalability necessary to enhance this particular approach to orbital debris remediation.

</details>
