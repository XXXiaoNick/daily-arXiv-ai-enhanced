<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.AI](#cs.AI) [Total: 29]
- [math.OC](#math.OC) [Total: 9]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [eess.SY](#eess.SY) [Total: 28]
- [cs.CY](#cs.CY) [Total: 8]
- [stat.ML](#stat.ML) [Total: 7]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [econ.EM](#econ.EM) [Total: 3]
- [cs.LG](#cs.LG) [Total: 90]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Democratizing LLM Efficiency: From Hyperscale Optimizations to Universal Deployability](https://arxiv.org/abs/2511.20662)
*Hen-Hsen Huang*

Main category: cs.CL

TL;DR: 本文主张重新定义LLM效率，从追求大规模复杂技术转向关注资源有限环境下的稳健简单性，提出了包括模型架构改造、轻量级微调、经济推理等研究方向，旨在降低LLM部署门槛并减少不平等和碳浪费。


<details>
  <summary>Details</summary>
Motivation: 当前主流的LLM效率方法（如MoE、推测解码、复杂RAG）仅适用于拥有庞大基础设施的科技巨头，导致医院、学校、政府等机构无法受益，加剧了不平等和碳浪费问题。

Method: 提出新的研究议程：无需重新训练即可改造预训练模型架构、轻量级微调保持对齐、经济化长链推理、轻量动态知识管理，并采用Overhead-Aware Efficiency作为标准基准。

Result: 通过重新定义效率概念，将采用成本、可持续性和公平性纳入考量，可以促进LLM部署的民主化。

Conclusion: LLM效率研究应转向关注资源有限环境下的稳健简单性，通过降低部署门槛来减少不平等和环境影响，而非一味追求大规模复杂技术。

Abstract: Large language models (LLMs) have become indispensable, but the most celebrated efficiency methods -- mixture-of-experts (MoE), speculative decoding, and complex retrieval-augmented generation (RAG) -- were built for hyperscale providers with vast infrastructure and elite teams. Outside that context, their benefits collapse into overhead, fragility, and wasted carbon. The result is that a handful of Big Tech companies benefit, while thousands of hospitals, schools, governments, and enterprises are left without viable options. We argue that the next frontier is not greater sophistication at scale, but robust simplicity: efficiency that thrives under modest resources and minimal expertise. We propose a new research agenda: retrofitting pretrained models with more efficient architectures without retraining, inventing lightweight fine-tuning that preserves alignment, making reasoning economical despite long chains of thought, enabling dynamic knowledge management without heavy RAG pipelines, and adopting Overhead-Aware Efficiency (OAE) as a standard benchmark. By redefining efficiency to include adoption cost, sustainability, and fairness, we can democratize LLM deployment -- ensuring that optimization reduces inequality and carbon waste rather than amplifying them.

</details>


### [2] [Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology](https://arxiv.org/abs/2511.20665)
*Tcharlies Schmitz*

Main category: cs.CL

TL;DR: HTP是一种无需训练的可逆确定性文本嵌入框架，通过Unicode整数生成谐波轨迹来编码token，实现离散符号到连续向量空间的双射映射。


<details>
  <summary>Details</summary>
Motivation: 现有神经嵌入方法依赖统计共现和优化，缺乏可解释性和确定性。HTP旨在提供透明、高效且无需数据驱动的语义表示替代方案。

Method: 将每个token的Unicode整数表示解析为谐波轨迹，建立符号与向量空间的确定性双射映射，通过几何对齐估计语义相似性。

Result: 在STS-B基准测试中达到Spearman相关性0.68，在10种语言中保持稳定性能，计算成本极低且延迟低于毫秒级。

Conclusion: 有意义的语义关系可以从确定性几何中产生，HTP为数据驱动嵌入提供了透明高效的替代方案。

Abstract: This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.

</details>


### [3] [A centroid based framework for text classification in itsm environments](https://arxiv.org/abs/2511.20667)
*Hossein Mohanna,Ali Ait-Bachir*

Main category: cs.CL

TL;DR: 提出基于双嵌入质心的分类框架，在ITSM工单分类中实现与SVM相当的性能，同时提供5.9倍训练加速和152倍增量更新加速。


<details>
  <summary>Details</summary>
Motivation: ITSM系统中需要将支持工单分类到树状层次化分类法中，现有方法在可解释性和操作效率方面存在不足。

Method: 使用双嵌入质心框架，为每个类别维护独立的语义和词汇质心表示，在推理时通过互逆排名融合进行组合。

Result: 在8,968个ITSM工单和123个类别上评估，达到层次化F1分数0.731，比SVM略优(0.727)，训练速度提升5.9倍，增量更新加速达152倍。

Conclusion: 该方法适合生产环境ITSM系统，在保持性能的同时显著提升可解释性和操作效率。

Abstract: Text classification with hierarchical taxonomies is a fundamental requirement in IT Service Management (ITSM) systems, where support tickets must be categorized into tree-structured taxonomies. We present a dual-embedding centroid-based classification framework that maintains separate semantic and lexical centroid representations per category, combining them through reciprocal rank fusion at inference time. The framework achieves performance competitive with Support Vector Machines (hierarchical F1: 0.731 vs 0.727) while providing interpretability through centroid representations. Evaluated on 8,968 ITSM tickets across 123 categories, this method achieves 5.9 times faster training and up to 152 times faster incremental updates. With 8.6-8.8 times speedup across batch sizes (100-1000 samples) when excluding embedding computation. These results make the method suitable for production ITSM environments prioritizing interpretability and operational efficiency.

</details>


### [4] [PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation](https://arxiv.org/abs/2511.20668)
*Yongfu Xue*

Main category: cs.CL

TL;DR: PIRA是一种新的奖励模型训练范式，通过重构问答对为偏好指令、聚合多样化偏好任务奖励、以及使用dropout平均来提升数据效率和防止奖励过优化。


<details>
  <summary>Details</summary>
Motivation: 传统判别式奖励模型存在数据效率低和容易奖励过优化的问题，需要更有效的训练方法。

Method: 采用三种策略：1) 将问答对重构为偏好指令；2) 聚合多样化偏好任务的奖励；3) 在不同dropout率下平均价值头输出以稳定奖励。

Result: 大量实验证明了PIRA的有效性。

Conclusion: PIRA训练范式成功解决了奖励模型的数据效率低和奖励过优化问题。

Abstract: Reward models are crucial for aligning Large Language Models (LLMs) with human preferences but face two representative challenges. First, traditional discriminative reward models usually concatenate questions and responses directly as input, resulting in low data efficiency. Second, reward models are vulnerable to reward overoptimization. We propose PIRA, a training paradigm addressing these issues through three strategies: (1) Reformulating question-answer pairs into preference-based instructions for clearer and more explicit task specification, (2) aggregating rewards from diverse preference tasks to reduce bias and improve robustness, and (3) averaging value-head outputs under varying dropout rates to stabilize rewards. Extensive experiments have demonstrated the effectiveness of PIRA.

</details>


### [5] [Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data](https://arxiv.org/abs/2511.20669)
*Mann Khatri,Mirza Yusuf,Rajiv Ratn Shah,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: 该论文研究了如何通过重组法律文档、定义修辞角色和模拟法院推理来提升大语言模型在法律任务中的表现，在零样本设置下实现了1.5%-4.36%的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在通用推理方面表现出色，但在法律等专业领域表现不佳，主要因为缺乏领域特定的预训练，且法律文档通常冗长复杂，难以有效处理。

Method: 在三个印度法律判决预测数据集上进行零样本实验：(i)基于修辞角色重组文档评估结构化信息的影响；(ii)定义修辞角色让模型熟悉法律术语；(iii)模拟法院关于修辞角色的逐步推理过程。

Result: 组织数据或解释关键法律术语显著提升了模型性能，相比基线F1分数最低提升约1.5%，最高提升4.36%。

Conclusion: 通过结构化信息处理和领域知识引入，可以有效提升大语言模型在法律专业任务中的表现，无需完整的领域对齐。

Abstract: Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.

</details>


### [6] [MindSET: Advancing Mental Health Benchmarking through Large-Scale Social Media Data](https://arxiv.org/abs/2511.20672)
*Saad Mankarious,Ayah Zirikly,Daniel Wiechmann,Elma Kerz,Edward Kempa,Yu Qiao*

Main category: cs.CL

TL;DR: 提出了MindSET新基准数据集，包含1300万条Reddit帖子，覆盖7种心理健康状况，通过严格预处理确保数据质量，在诊断检测任务中表现优于现有基准


<details>
  <summary>Details</summary>
Motivation: 现有心理健康分析基准数据集已过时，存在数据量不足、清理不充分、内容多样性不足等问题，需要更高质量的数据集来支持社交媒体心理健康研究

Method: 从Reddit收集自报告诊断数据，应用严格预处理（语言过滤、NSFW内容移除、去重），使用LIWC进行语言分析，并通过微调语言模型和词袋特征进行二元分类实验

Result: MindSET数据集规模是之前基准的两倍以上，在诊断检测任务中表现显著优于现有基准，自闭症检测的F1分数提高了18个百分点

Conclusion: MindSET为社交媒体与心理健康交叉研究提供了坚实基础，支持早期风险检测和新兴心理趋势的深入分析

Abstract: Social media data has become a vital resource for studying mental health, offering real-time insights into thoughts, emotions, and behaviors that traditional methods often miss. Progress in this area has been facilitated by benchmark datasets for mental health analysis; however, most existing benchmarks have become outdated due to limited data availability, inadequate cleaning, and the inherently diverse nature of social media content (e.g., multilingual and harmful material). We present a new benchmark dataset, \textbf{MindSET}, curated from Reddit using self-reported diagnoses to address these limitations. The annotated dataset contains over \textbf{13M} annotated posts across seven mental health conditions, more than twice the size of previous benchmarks. To ensure data quality, we applied rigorous preprocessing steps, including language filtering, and removal of Not Safe for Work (NSFW) and duplicate content. We further performed a linguistic analysis using LIWC to examine psychological term frequencies across the eight groups represented in the dataset. To demonstrate the dataset utility, we conducted binary classification experiments for diagnosis detection using both fine-tuned language models and Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous benchmarks, achieving up to an \textbf{18-point} improvement in F1 for Autism detection. Overall, MindSET provides a robust foundation for researchers exploring the intersection of social media and mental health, supporting both early risk detection and deeper analysis of emerging psychological trends.

</details>


### [7] [Semantics Meet Signals: Dual Codebook Representationl Learning for Generative Recommendation](https://arxiv.org/abs/2511.20673)
*Zheng Hui,Xiaokai Wei,Reza Shirkavand,Chen Wang,Weizhi Zhang,Alejandro Peláez,Michelle Gong*

Main category: cs.CL

TL;DR: FlexCode是一个基于流行度感知的生成式推荐框架，通过动态分配协作过滤和语义编码本的token预算，解决传统方法对热门和长尾项目统一编码的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法使用单一编码本对所有项目进行编码，忽视了热门项目（富含协作信号）和长尾项目（依赖语义理解）之间的不平衡性，这种统一处理限制了表示效率和泛化能力。

Method: FlexCode框架自适应地在协作过滤编码本和语义编码本之间分配固定token预算，使用轻量级MoE动态平衡CF精度和语义泛化，并通过对齐和平滑目标保持流行度谱系中的连贯性。

Result: 在公共和工业级数据集上的实验表明，FlexCode持续优于强基线方法，在准确性和长尾鲁棒性方面表现更优。

Conclusion: FlexCode为生成式推荐器中的token表示提供了新机制，实现了更强的准确性和尾部鲁棒性，为基于token的推荐模型中平衡记忆和泛化提供了新视角。

Abstract: Generative recommendation has recently emerged as a powerful paradigm that unifies retrieval and generation, representing items as discrete semantic tokens and enabling flexible sequence modeling with autoregressive models. Despite its success, existing approaches rely on a single, uniform codebook to encode all items, overlooking the inherent imbalance between popular items rich in collaborative signals and long-tail items that depend on semantic understanding. We argue that this uniform treatment limits representational efficiency and hinders generalization. To address this, we introduce FlexCode, a popularity-aware framework that adaptively allocates a fixed token budget between a collaborative filtering (CF) codebook and a semantic codebook. A lightweight MoE dynamically balances CF-specific precision and semantic generalization, while an alignment and smoothness objective maintains coherence across the popularity spectrum. We perform experiments on both public and industrial-scale datasets, showing that FlexCode consistently outperform strong baselines. FlexCode provides a new mechanism for token representation in generative recommenders, achieving stronger accuracy and tail robustness, and offering a new perspective on balancing memorization and generalization in token-based recommendation models.

</details>


### [8] [Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic](https://arxiv.org/abs/2511.20677)
*Saleh Almohaimeed,May Alsofyani,Saad Almohaimeed,Mansour Al Ghanim,Liqiang Wang*

Main category: cs.CL

TL;DR: 本文介绍了首个阿拉伯语跨领域上下文相关文本到SQL数据集Ar-SParC，包含10,225个问题及其SQL查询，并使用GPT模型进行了40个实验，开发了GAT校正器方法提升了性能。


<details>
  <summary>Details</summary>
Motivation: 目前大多数文本到SQL的研究和数据集都是英文的，阿拉伯语领域尚未有相关工作，需要填补这一空白。

Method: 构建了Ar-SParC数据集，使用GPT-3.5-turbo和GPT-4.5-turbo模型，应用10种提示工程技术，并开发了GAT校正器方法。

Result: GAT校正器在零样本设置下平均提升执行准确率1.9%和交互准确率1.9%，在上下文学习设置下平均提升执行准确率1.72%和交互准确率0.92%。

Conclusion: 成功创建了首个阿拉伯语文本到SQL数据集，并证明了GAT校正器方法的有效性，为阿拉伯语自然语言数据库交互提供了基础。

Abstract: In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.

</details>


### [9] [Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes](https://arxiv.org/abs/2511.20680)
*Matthew W. Kenaston,Umair Ayub,Mihir Parmar,Muhammad Umair Anjum,Syed Arsalan Ahmed Naqvi,Priya Kumar,Samarth Rawal,Aadel A. Chaudhuri,Yousef Zakharia,Elizabeth I. Heath,Tanios S. Bekaii-Saab,Cui Tao,Eliezer M. Van Allen,Ben Zhou,YooJung Choi,Chitta Baral,Irbaz Bin Riaz*

Main category: cs.CL

TL;DR: GPT-4在肿瘤学决策支持中存在推理错误，可能导致临床不安全的建议，特别是在晚期疾病管理中。研究者开发了一个三层次推理错误分类法，发现23%的解释存在推理错误，确认偏误和锚定偏误最常见。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在临床基准测试中表现良好，但它们可能通过错误的推理得出正确结论，这种失败模式对肿瘤学决策支持具有安全影响，而基于准确性的评估无法捕捉到这一点。

Method: 采用两队列回顾性研究，从GPT-4对真实肿瘤学笔记的思维链响应中开发分层推理错误分类法，并在前列腺癌咨询笔记的822个响应上进行验证，模拟提取、分析和临床推荐任务。

Result: 23%的解释存在推理错误，确认偏误和锚定偏误最常见。推理失败与指南不一致和潜在有害建议相关，特别是在晚期疾病管理中。自动评估器能检测错误存在但无法可靠分类子类型。

Conclusion: 大型语言模型在推理有缺陷时可能提供流畅但临床不安全的建议。该分类法为临床部署前评估和改进推理保真度提供了可推广的框架。

Abstract: Despite high performance on clinical benchmarks, large language models may reach correct conclusions through faulty reasoning, a failure mode with safety implications for oncology decision support that is not captured by accuracy-based evaluation. In this two-cohort retrospective study, we developed a hierarchical taxonomy of reasoning errors from GPT-4 chain-of-thought responses to real oncology notes and tested its clinical relevance. Using breast and pancreatic cancer notes from the CORAL dataset, we annotated 600 reasoning traces to define a three-tier taxonomy mapping computational failures to cognitive bias frameworks. We validated the taxonomy on 822 responses from prostate cancer consult notes spanning localized through metastatic disease, simulating extraction, analysis, and clinical recommendation tasks. Reasoning errors occurred in 23 percent of interpretations and dominated overall errors, with confirmation bias and anchoring bias most common. Reasoning failures were associated with guideline-discordant and potentially harmful recommendations, particularly in advanced disease management. Automated evaluators using state-of-the-art language models detected error presence but could not reliably classify subtypes. These findings show that large language models may provide fluent but clinically unsafe recommendations when reasoning is flawed. The taxonomy provides a generalizable framework for evaluating and improving reasoning fidelity before clinical deployment.

</details>


### [10] [Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches](https://arxiv.org/abs/2511.20683)
*Bharadwaj Yadavalli*

Main category: cs.CL

TL;DR: 提出了动态模板选择（DTS）方法，根据查询复杂度自适应选择响应模板，在保持响应质量的同时显著降低token使用成本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型部署采用统一的提示策略，对复杂分析任务和简单事实问题都使用冗长的响应模式，导致token效率低下，而输出token成本比输入token高4-8倍。

Method: 比较了两种路由方法：使用预计算嵌入的简单MLP和微调的RoBERTa transformer，在1,000个MMLU问题上进行评估。

Result: MLP路由在测试数据上达到90.5%的路由准确率，略高于RoBERTa（89.5%），且参数少1.25亿。在三个主要LLM提供商上验证，路由准确率保持90.5%，token减少32.6%-33.9%。

Conclusion: DTS方法能有效降低token成本，路由决策在不同LLM提供商间具有通用性，为token效率优化提供了理论和实证基础。

Abstract: Contemporary large language model deployments typically employ uniform prompting strategies across diverse query types, applying verbose response patterns to both complex analytical tasks and straightforward factual questions. This one-size-fits-all methodology leads to substantial token inefficiency, a concern amplified by the significant cost differential between input and output tokens--the latter commanding 4-8x higher prices across major providers. We present Dynamic Template Selection (DTS), which adaptively matches response templates to query complexity, achieving significant cost reductions without compromising response quality.
  We compared two routing approaches: a simple MLP that uses pre-computed embeddings and a more complex fine-tuned RoBERTa transformer. Through comprehensive evaluation on 1,000 MMLU questions, we find that the MLP router achieves 90.5% routing accuracy on held-out test data, marginally exceeding RoBERTa's performance (89.5%) despite utilizing 125M fewer parameters. Notably, our empirical analysis reveals provider-agnostic behavior in template selection--routing decisions generalize effectively across 3 major LLM providers (OpenAI GPT-4, Google Gemini, and Anthropic Claude), as validated through 9,000 production API calls. While routing accuracy remains consistent at 90.5% across providers, observed token reductions vary from 32.6% to 33.9%, reflecting provider-specific generation characteristics.
  This work contributes several key elements: formal problem formulation with theoretical grounding in machine learning, four algorithms with corresponding complexity analyses, and extensive empirical validation across production systems.

</details>


### [11] [LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data](https://arxiv.org/abs/2511.20691)
*Lijun Shang,Yadong Yu,Wenqiang Kang,Jian Zhou,Dongyue Gao,Pan Xiang,Zhe Liu,Mengyan Dai,Zhonglu Guo,Zhimei Sun*

Main category: cs.CL

TL;DR: 二维材料因其独特的物理化学和电子特性在能源存储与转换领域有广泛应用，但相关研究信息分散在大量论文中，难以系统获取。


<details>
  <summary>Details</summary>
Motivation: 解决二维材料研究信息分散在大量论文中，难以系统获取和利用的问题。

Method: 通过分析已发表研究论文，提取二维材料的性能信息和制备方法等关键数据。

Result: 成功从分散的研究论文中收集整理了二维材料的关键信息。

Conclusion: 需要建立系统化的方法来整合和利用分散在论文中的二维材料研究信息。

Abstract: Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe

</details>


### [12] [Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models](https://arxiv.org/abs/2511.20799)
*Trung Cuong Dang,David Mohaisen*

Main category: cs.CL

TL;DR: 提出了多前缀记忆框架，通过测量不同前缀触发相同序列的能力来定义LLM的记忆现象，比单路径提取更全面可靠。


<details>
  <summary>Details</summary>
Motivation: 现有记忆定义在全面捕捉对齐模型中的记忆现象方面存在不足，特别是无法充分评估记忆的稳健性。

Method: 引入多前缀记忆框架，将序列定义为被记忆的条件是：外部对抗搜索能找到足够数量的不同前缀来引出该序列。

Result: 实验表明，多前缀定义能可靠地区分记忆和非记忆数据，为审计LLM数据泄露提供了稳健实用的工具。

Conclusion: 多前缀记忆框架通过量化记忆的稳健性，为评估LLM训练数据泄露提供了更全面和实用的方法。

Abstract: Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.

</details>


### [13] [SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models](https://arxiv.org/abs/2511.20820)
*Jiaojiao Han,Wujiang Xu,Mingyu Jin,Mengnan Du*

Main category: cs.CL

TL;DR: SAGE是一个基于智能体的框架，将稀疏自编码器特征解释从被动单次生成任务转变为主动的、解释驱动的过程，通过系统制定多个解释、设计针对性实验和基于激活反馈迭代优化，显著提高了特征解释的生成和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的内部机制仍然不透明，这对其安全可靠部署构成挑战。稀疏自编码器是分解LLM表示的有前景工具，但解释SAE捕获的特征仍然是一个困难任务。

Method: 提出SAGE框架，将特征解释重构为主动的解释驱动过程：系统为每个特征制定多个解释，设计针对性实验进行测试，基于经验激活反馈迭代优化解释。

Result: 在多种语言模型的SAE特征上进行实验，证明SAGE产生的解释在生成和预测准确性方面显著优于最先进的基线方法。

Conclusion: SAGE通过主动、迭代的解释过程，有效提高了稀疏自编码器特征的解释质量，为大语言模型的可解释性提供了新方法。

Abstract: Large language models (LLMs) have achieved remarkable progress, yet their internal mechanisms remain largely opaque, posing a significant challenge to their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising tool for decomposing LLM representations into more interpretable features, but explaining the features captured by SAEs remains a challenging task. In this work, we propose SAGE (SAE AGentic Explainer), an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanation-driven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanationdriven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.

</details>


### [14] [Structured Prompting Enables More Robust, Holistic Evaluation of Language Models](https://arxiv.org/abs/2511.20836)
*Asad Aali,Muhammad Ahmed Mohsin,Vasiliki Bikia,Arnav Singhvi,Richard Gaus,Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Yifan Mai,Jordan Cahoon,Michael Pfeffer,Roxana Daneshjou,Sanmi Koyejo,Emily Alsentzer,Percy Liang,Christopher Potts,Nigam H. Shah,Akshay S. Chaudhari*

Main category: cs.CL

TL;DR: 提出了DSPy+HELM框架，通过结构化提示方法提升语言模型基准测试的准确性，发现传统HELM方法会低估模型性能4%，导致排行榜排名错误。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试框架如HELM使用固定提示，无法准确估计语言模型的性能上限，可能导致部署决策失误。

Method: 开发了可复现的DSPy+HELM框架，使用四种结构化提示方法，在七个基准测试（通用/医疗领域）上评估四个前沿语言模型。

Result: 发现结构化提示能提高性能估计准确性（平均提升4%），减少性能估计波动（标准差减少2%），并改变3/7基准测试的排行榜排名。

Conclusion: 可扩展的性能上限估计方法能提供更有决策价值的基准测试，推理提示（如思维链）能降低模型对提示设计的敏感性。

Abstract: As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller Δ across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).

</details>


### [15] [Length-MAX Tokenizer for Language Models](https://arxiv.org/abs/2511.20849)
*Dong Dong,Weijie Su*

Main category: cs.CL

TL;DR: 提出了一种新的Length-MAX分词器，通过最小化平均字符数来减少训练和推理时的token数量，相比BPE在10K-50K词汇量下减少14-18%的token，在GPT-2模型上减少17-18%的训练步数和13-14%的推理延迟，同时提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统分词器如BPE主要基于频率优化，但未充分考虑token长度对训练效率和推理速度的影响。本文旨在开发一种能最小化平均token长度的分词器，以提高语言模型的整体效率。

Method: 将长度加权目标最大化建模为图划分问题，并开发贪心近似算法来获取词汇表，该方法称为Length-MAX分词器。

Result: 在FineWeb和多个领域上，相比BPE减少14-18%的token数量；GPT-2模型训练步数减少17-18%，推理延迟降低13-14%，下游任务LAMBADA困惑度降低11.7%，HellaSwag准确率提升4.3%；词汇覆盖率达99.62%，OOV率仅0.12%。

Conclusion: 优化平均token长度而非仅频率，是实现更高效语言建模的有效方法，在不牺牲性能的情况下显著提升效率，兼容生产系统并减少18%的嵌入和KV缓存内存。

Abstract: We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\%, 17.2\%, and 18.5\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\%, 12.7\%, and 13.7\% lower inference latency, together with a 16\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\% and enhancing HellaSwag accuracy by 4.3\%. Moreover, the Length-MAX tokenizer achieves 99.62\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\% at inference.

</details>


### [16] [Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory](https://arxiv.org/abs/2511.20857)
*Tianxin Wei,Noveen Sachdeva,Benjamin Coleman,Zhankui He,Yuanchen Bei,Xuying Ning,Mengting Ai,Yunzhe Li,Jingrui He,Ed H. Chi,Chi Wang,Shuo Chen,Fernando Pereira,Wang-Cheng Kang,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: 提出了Evo-Memory基准框架，用于评估LLM代理的自进化记忆能力，通过序列化任务流测试记忆的搜索、适应和进化能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注静态对话场景，忽视了LLM在连续任务流中积累和重用经验的动态能力，而现实世界环境需要LLM能够持续检索、整合和更新记忆。

Method: 构建序列化任务流数据集，统一实现十多种代表性记忆模块，提出ExpRAG基线方法和ReMem行动-思考-记忆优化管道。

Result: 在10个多样化多轮目标导向和单轮推理问答数据集上进行了评估。

Conclusion: Evo-Memory填补了LLM代理自进化记忆评估的空白，为持续学习和经验重用提供了重要基准。

Abstract: Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.

</details>


### [17] [Winning with Less for Low Resource Languages: Advantage of Cross-Lingual English_Persian Argument Mining Model over LLM Augmentation](https://arxiv.org/abs/2511.20872)
*Ali Jahan,Masood Ghayoomi,Annette Hautli-Janisz*

Main category: cs.CL

TL;DR: 该论文研究了面向低资源语言的跨语言论辩挖掘方法，通过三种训练场景（零样本迁移、LLM增强、跨语言混合）在英语和波斯语上进行实验，发现轻量级跨语言方法优于资源密集的增强方法。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在论辩挖掘任务中面临的数据资源短缺问题，探索有效的跨语言迁移方法。

Method: 构建三种训练场景：零样本迁移（仅用英语数据）、LLM生成增强数据、跨语言混合训练（英语+人工翻译波斯语数据），在英语和波斯语语料上进行评估。

Result: 零样本迁移模型在英语和波斯语测试集上的F1分数分别为50.2%和50.7%；LLM增强模型提升至59.2%（英语）和69.3%（波斯语）；跨语言模型在波斯语测试集上达到74.8%的F1分数。

Conclusion: 轻量级跨语言混合方法显著优于资源密集的增强方法，为低资源语言的论辩挖掘任务提供了实用的解决方案。

Abstract: Argument mining is a subfield of natural language processing to identify and extract the argument components, like premises and conclusions, within a text and to recognize the relations between them. It reveals the logical structure of texts to be used in tasks like knowledge extraction. This paper aims at utilizing a cross-lingual approach to argument mining for low-resource languages, by constructing three training scenarios. We examine the models on English, as a high-resource language, and Persian, as a low-resource language. To this end, we evaluate the models based on the English Microtext corpus \citep{PeldszusStede2015}, and its parallel Persian translation. The learning scenarios are as follow: (i) zero-shot transfer, where the model is trained solely with the English data, (ii) English-only training enhanced by synthetic examples generated by Large Language Models (LLMs), and (iii) a cross-lingual model that combines the original English data with manually translated Persian sentences. The zero-shot transfer model attains F1 scores of 50.2\% on the English test set and 50.7\% on the Persian test set. LLM-based augmentation model improves the performance up to 59.2\% on English and 69.3\% on Persian. The cross-lingual model, trained on both languages but evaluated solely on the Persian test set, surpasses the LLM-based variant, by achieving a F1 of 74.8\%. Results indicate that a lightweight cross-lingual blend can outperform considerably the more resource-intensive augmentation pipelines, and it offers a practical pathway for the argument mining task to overcome data resource shortage on low-resource languages.

</details>


### [18] [Emergence and Localisation of Semantic Role Circuits in LLMs](https://arxiv.org/abs/2511.20910)
*Nura Aljaafari,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 提出了一种结合角色交叉最小对、时间涌现分析和跨模型比较的方法，研究LLMs如何实现语义角色，发现LLMs形成紧凑、因果隔离的抽象语义结构机制，这些机制在尺度和架构间存在部分迁移。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型显示出语义能力，但其实现抽象语义结构的内在机制仍不够清晰，需要深入表征这些内部机制。

Method: 整合角色交叉最小对、时间涌现分析和跨模型比较的方法来研究LLMs如何实现语义角色。

Result: 发现：(i)高度集中的电路(89-94%归因在28个节点内)；(ii)渐进式结构精炼而非相变，更大模型有时绕过局部化电路；(iii)中等跨尺度保守性(24-59%组件重叠)和高度光谱相似性。

Conclusion: LLMs形成紧凑、因果隔离的抽象语义结构机制，这些机制在尺度和架构间表现出部分迁移特性。

Abstract: Despite displaying semantic competence, large language models' internal mechanisms that ground abstract semantic structure remain insufficiently characterised. We propose a method integrating role-cross minimal pairs, temporal emergence analysis, and cross-model comparison to study how LLMs implement semantic roles. Our analysis uncovers: (i) highly concentrated circuits (89-94% attribution within 28 nodes); (ii) gradual structural refinement rather than phase transitions, with larger models sometimes bypassing localised circuits; and (iii) moderate cross-scale conservation (24-59% component overlap) alongside high spectral similarity. These findings suggest that LLMs form compact, causally isolated mechanisms for abstract semantic structure, and these mechanisms exhibit partial transfer across scales and architectures.

</details>


### [19] [Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs](https://arxiv.org/abs/2511.20940)
*Reham Omar,Abdelghny Orogat,Ibrahim Abdelaziz,Omij Mangukiya,Panos Kalnis,Essam Mansour*

Main category: cs.CL

TL;DR: Chatty-KG是一个用于知识图谱对话问答的多智能体系统，结合了检索增强生成和结构化查询执行，在单轮和多轮对话中都显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱问答系统存在局限性：检索增强生成系统会序列化图结构且难以处理多轮上下文，传统KGQA系统通常只支持单轮问答且延迟高、难以处理指代消解和上下文跟踪。

Method: 采用模块化多智能体架构，通过任务专用的LLM智能体协作进行上下文解释、对话跟踪、实体关系链接和高效查询规划，将自然语言问题转换为可执行的SPARQL查询。

Result: 在大型多样化知识图谱上的实验表明，Chatty-KG在单轮和多轮设置下都显著优于最先进的基线方法，获得了更高的F1和P@1分数。

Conclusion: Chatty-KG统一了对话灵活性和结构化知识图谱基础，为可靠的多轮KGQA提供了可扩展和可扩展的方法。

Abstract: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.

</details>


### [20] [TrackList: Tracing Back Query Linguistic Diversity for Head and Tail Knowledge in Open Large Language Models](https://arxiv.org/abs/2511.21006)
*Ioana Buhnila,Aman Sinha,Mathieu Constant*

Main category: cs.CL

TL;DR: LLMs在定义类查询上表现良好，但在其他类型查询（如举例、释义）上表现显著下降，特别是在技术性知识领域。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在除定义类查询外的其他类型查询上的性能下降，并研究预训练数据对LLM回答多样语言查询的影响。

Method: 使用TrackList分析管道和RefoMed-EN数据集（6170个医学术语及其注释），通过句法语义相似度、统计相关性和嵌入来评估LLM输出质量。

Result: LLMs在定义类问题上的任务性能最高，举例类问题最低；对于定义类问题，LLMs更倾向于对流行知识进行释义，而对技术性尾部知识释义较少。

Conclusion: LLMs在处理非定义类查询时存在性能局限，特别是在技术性和低频知识领域，需要改进以提供更全面的回答类型。

Abstract: Large Language Models (LLMs) have proven efficient in giving definition-type answers to user input queries. While for humans giving various types of answers, such as examples and paraphrases, is an easy task, LLMs struggle to provide correct answers for other than definition-type queries. In this study, we evaluated this drop in performance using TrackList, a fine-grained linguistic and statistical analysis pipeline to investigate the impact of the pre-training data on LLMs answers to diverse linguistic queries. We also introduce RefoMed-EN, an English dataset consisting of 6170 human-annotated medical terms alongside their corresponding definitions, denominations, exemplifications, explanations, or paraphrases. We studied whether the high frequency of a concept (head) or low frequency (tail) impacts the language model's performance. We evaluated the quality of the LLM's output using syntactic and semantic similarity metrics, statistical correlations and embeddings. Results showed that the LLM's task performance for definition type questions is the highest, while for the exemplification type it is the lowest. Additionally, we showed that for definition-type questions, large language models are prone to paraphrase more on popular and frequent knowledge and less on tail and technical knowledge, especially in the expert texts.

</details>


### [21] [Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels](https://arxiv.org/abs/2511.21038)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CL

TL;DR: 研究发现上下文学习(ICL)无法覆盖预训练的标签语义，而是主要调整输入如何映射到预训练期间学习的稳定语义方向，支持语义锚定观点。


<details>
  <summary>Details</summary>
Motivation: 探讨上下文学习是否能覆盖预训练的标签语义，还是仅仅细化现有的语义主干，理解ICL的基本限制。

Method: 将LLMs视为提示诱导分类器，对比自然演示（正确标签）和反转演示（系统翻转标签含义）下的行为，分解ICL行为为三个对齐指标（真值、先验和提示对齐），并引入语义覆盖率。

Result: 在8个分类任务和8个开源LLM上，ICL在自然演示下提高准确性但保持强先验对齐；在反转演示下模型无法学习连贯的反语义分类器，语义覆盖率保持为零。

Conclusion: ICL主要调整输入如何投影到预训练期间学习的稳定语义方向，而不是灵活地重新映射标签含义，表明在这些规模上覆盖标签语义需要超越ICL的干预措施。

Abstract: Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.

</details>


### [22] [Context-Aware Pragmatic Metacognitive Prompting for Sarcasm Detection](https://arxiv.org/abs/2511.21066)
*Michael Iskandardinata,William Christian,Derwin Suhartono*

Main category: cs.CL

TL;DR: 该论文提出了一种结合检索上下文信息的LLM讽刺检测方法，通过非参数知识检索和自知识检索策略，显著提升了讽刺检测性能。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测在NLP中仍然具有挑战性，PLMs和LLMs在处理复杂讽刺文本时表现不稳定，特别是面对语言多样性和文化差异时。模型缺乏必要的背景知识导致检测不可靠。

Method: 在PMP方法基础上引入检索感知方法：1) 非参数知识检索：当模型缺乏背景时添加基于网络的检索信息；2) 自知识检索：激发模型内部知识进行自我认知策略。

Result: 在三个数据集上评估：Twitter Indonesia Sarcastic数据集上非参数检索比原始PMP方法提升9.87% macro-F1；Semeval和MUStARD数据集上自知识检索分别提升3.29%和4.08% macro-F1。

Conclusion: 上下文信息对提升LLMs讽刺检测性能至关重要，特别是涉及文化特定俚语、引用或LLMs未知术语时。未来工作将优化相关上下文信息的检索并研究检索质量对性能的影响。

Abstract: Detecting sarcasm remains a challenging task in the areas of Natural Language Processing (NLP) despite recent advances in neural network approaches. Currently, Pre-trained Language Models (PLMs) and Large Language Models (LLMs) are the preferred approach for sarcasm detection. However, the complexity of sarcastic text, combined with linguistic diversity and cultural variation across communities, has made the task more difficult even for PLMs and LLMs. Beyond that, those models also exhibit unreliable detection of words or tokens that require extra grounding for analysis. Building on a state-of-the-art prompting method in LLMs for sarcasm detection called Pragmatic Metacognitive Prompting (PMP), we introduce a retrieval-aware approach that incorporates retrieved contextual information for each target text. Our pipeline explores two complementary ways to provide context: adding non-parametric knowledge using web-based retrieval when the model lacks necessary background, and eliciting the model's own internal knowledge for a self-knowledge awareness strategy. We evaluated our approach with three datasets, such as Twitter Indonesia Sarcastic, SemEval-2018 Task 3, and MUStARD. Non-parametric retrieval resulted in a significant 9.87% macro-F1 improvement on Twitter Indonesia Sarcastic compared to the original PMP method. Self-knowledge retrieval improves macro-F1 by 3.29% on Semeval and by 4.08% on MUStARD. These findings highlight the importance of context in enhancing LLMs performance in sarcasm detection task, particularly the involvement of culturally specific slang, references, or unknown terms to the LLMs. Future work will focus on optimizing the retrieval of relevant contextual information and examining how retrieval quality affects performance. The experiment code is available at: https://github.com/wllchrst/sarcasm-detection_pmp_knowledge-base.

</details>


### [23] [Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning](https://arxiv.org/abs/2511.21081)
*Thura Aung,Eaint Kay Khaing Kyaw,Ye Kyaw Thu,Thazin Myint Oo,Thepchai Supnithi*

Main category: cs.CL

TL;DR: 该研究探索在缅甸语等低资源语言分类任务中，使用Kolmogorov-Arnold Networks (KANs) 替代传统的多层感知机(MLPs)作为分类头，发现KANs在表达能力和效率上具有优势。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言分类任务中，通常只微调最终分类层而保持预训练编码器权重冻结。MLPs虽然常用，但其固定非线性可能限制表达能力并增加计算成本，因此需要探索更好的替代方案。

Method: 评估了三种KAN变体作为分类头：基于傅里叶的FourierKAN、基于样条的EfficientKAN和基于网格的FasterKAN，在TF-IDF、fastText和多语言变换器(mBERT、Distil-mBERT)等多种嵌入上进行实验。

Result: 实验结果显示KAN-based分类头与MLPs相当或更优。EfficientKAN与fastText组合获得最高F1分数(0.928)，FasterKAN在速度和准确性之间提供了最佳平衡。在变换器嵌入上，EfficientKAN与mBERT组合达到或略优于MLPs(0.917 F1)。

Conclusion: KANs是MLPs在低资源语言分类任务中表达能力强且高效的替代方案，特别适合资源受限的环境。

Abstract: In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.

</details>


### [24] [Orthographic Constraint Satisfaction and Human Difficulty Alignment in Large Language Models](https://arxiv.org/abs/2511.21086)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.CL

TL;DR: 对28个不同架构的大语言模型在58个字符级约束的单词谜题上进行评估，发现架构差异比参数规模对约束满足能力影响更大，模型在处理非常规拼写但约束有效的单词时存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 评估不同架构的大语言模型在满足硬性拼写约束方面的能力，探索架构差异与参数规模对约束满足性能的影响。

Method: 评估28个配置，涵盖三个模型家族（Qwen3、Claude Haiku-4.5、GPT-5-mini），在58个需要字符级约束满足的单词谜题上进行测试，并使用10,000名人类解谜者的难度评级进行分析。

Result: 架构差异产生显著性能差距（2.0-2.2倍，F1=0.761 vs. 0.343），远大于家族内参数扩展带来的收益（83%增益）；模型在常见但拼写不规则的单词上存在系统性失败（86-95%人类成功率，89-96%模型失败率）。

Conclusion: 约束满足可能需要专门的架构特征或训练目标，而不仅仅是标准语言模型扩展；模型过度依赖分布合理性，惩罚了拼写不典型但约束有效的模式，需要超越简单参数扩展或计算预算的架构创新。

Abstract: Large language models must satisfy hard orthographic constraints during controlled text generation, yet systematic cross-architecture evaluation remains limited. We evaluate 28 configurations spanning three model families (Qwen3, Claude Haiku-4.5, GPT-5-mini) on 58 word puzzles requiring character-level constraint satisfaction. Architectural differences produce substantially larger performance gaps (2.0-2.2x, F1=0.761 vs. 0.343) than parameter scaling within families (83% gain from eightfold scaling), suggesting that constraint satisfaction may require specialized architectural features or training objectives beyond standard language model scaling. Thinking budget sensitivity proves heterogeneous: high-capacity models show strong returns (+0.102 to +0.136 F1), while mid-sized variants saturate or degrade. These patterns are inconsistent with uniform compute benefits. Using difficulty ratings from 10,000 human solvers per puzzle, we establish modest but consistent calibration (r=0.24-0.38) across all families, yet identify systematic failures on common words with unusual orthography ("data", "poop", "loll": 86-95% human success, 89-96% model miss rate). These failures reveal over-reliance on distributional plausibility that penalizes orthographically atypical but constraint-valid patterns, suggesting architectural innovations may be required beyond simply scaling parameters or computational budgets.

</details>


### [25] [ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features](https://arxiv.org/abs/2511.21088)
*Ye Bhone Lin,Thura Aung,Ye Kyaw Thu,Thazin Myint Oo*

Main category: cs.CL

TL;DR: 该论文研究了在低资源缅甸语ASR错误校正中，使用序列到序列Transformer模型，结合IPA和对齐特征，显著降低了词错误率和字符错误率。


<details>
  <summary>Details</summary>
Motivation: 这是首个专门针对缅甸语的ASR错误校正研究，旨在解决低资源语言ASR性能不佳的问题。

Method: 使用序列到序列Transformer模型，评估了五种ASR骨干网络，并提出了结合IPA和对齐特征的ASR错误校正方法。

Result: 提出的AEC模型将ASR模型的平均WER从51.56降至39.82（增强前）和43.59（增强后），chrF++得分从0.5864提升至0.627。

Conclusion: 研究证明了AEC的鲁棒性，并强调了在低资源环境下特征设计对改进ASR输出的重要性。

Abstract: This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.

</details>


### [26] [MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing](https://arxiv.org/abs/2511.21101)
*Manish Jain,Satheesh Kumar Ponnambalam,Salman Faroz,Chandrakanth Lns,Vinay Sharma*

Main category: cs.CL

TL;DR: 提出了MortgageLLM，一个专门针对抵押贷款金融领域的双专家大语言模型，通过双轨专业化框架解决单一模型在结构化任务和对话能力之间的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用领域表现出色，但在抵押贷款金融等专业领域需要增强领域特定知识，同时保持指令跟随能力。单一多任务模型存在性能权衡问题。

Method: 采用双轨专业化框架，从单一基础模型创建两个专家模型：对话问答模型和结构化任务模型。应用指令残差技术恢复指令跟随能力，并使用少量样本分类的智能任务路由机制。

Result: 在领域特定基准测试中，最终模型MLM v2显著优于基础模型，在总结、问答和分类任务上分别获得4.58、4.09和2.6的评分，语义相似度指标也全面超越基线方法。

Conclusion: 双专家架构和指令残差技术的结合有效解决了专业领域大语言模型的知识增强与指令保持的双重挑战，在抵押贷款金融领域取得了显著性能提升。

Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.

</details>


### [27] [Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines](https://arxiv.org/abs/2511.21214)
*Yuhang Wang,Yanxu Zhu,Dongyuan Lu,Jitao Sang*

Main category: cs.CL

TL;DR: SGASA框架通过内部化模型生成的安全指南来增强模型对有害对抗性提示的鲁棒性，同时减少对良性请求的不必要拒绝。


<details>
  <summary>Details</summary>
Motivation: 推理模型在复杂推理任务中表现出色，但面对对抗性越狱提示时安全性不足，这些提示具有隐蔽性和欺骗性，能绕过内置安全机制生成有害内容，需要自适应安全对齐方法。

Method: SGASA框架包含两个关键阶段：数据预合成（生成安全指南和增强提示）和对齐微调（使用SFT和DPO将指南嵌入模型）。

Result: 在多个数据集上的广泛实验表明，SGASA显著提高了模型安全性，验证了其自适应和可扩展的有效性。

Conclusion: SGASA框架通过内部化安全指南有效增强了模型对对抗性提示的防御能力，同时保持对良性请求的响应能力。

Abstract: Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs. This paper introduces the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which internalizes model-generated safety guidelines to strengthen models' ability to enhance robustness against harmful adversarial prompts while minimizing unnecessary refusals of benign requests. SGASA consists of two key stages: Data Pre-synthesis, which generates safety guidelines and augmented prompts; and Alignment Fine-tuning, which leverages Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) to embed these guidelines into the model. Extensive experiments across multiple datasets demonstrate that SGASA significantly improves model safety, validating its adaptive and scalable effectiveness.

</details>


### [28] [Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment, and Belief-Action Coherence?](https://arxiv.org/abs/2511.21218)
*Steven Wang,Kyle Hunt,Shaojie Tang,Kenneth Joseph*

Main category: cs.CL

TL;DR: 研究表明，在少量人类调查数据上微调LLM可以改善模拟结果的异质性、对齐性和信念-行为一致性，但即使最佳微调模型也无法复现原始研究的回归系数，因此LLM生成的数据仍不适合替代人类参与者进行正式推断分析。


<details>
  <summary>Details</summary>
Motivation: 探讨在少量人类调查数据上微调LLM是否能缓解LLM模拟人类行为时的局限性，如多样性不足、少数群体系统性偏差、组内方差不足以及信念与行为不一致等问题。

Method: 使用信息披露行为实验，比较人类和LLM生成的反应，从分布差异、子群体对齐、信念-行为一致性和回归系数恢复等多个维度进行评估。

Result: 在少量人类样本上微调显著提高了异质性、对齐性和信念-行为一致性，但即使最佳微调模型也无法复现原始研究的回归系数。

Conclusion: LLM生成的数据虽然在某些方面有所改进，但仍不适合替代人类参与者进行正式的推断分析。

Abstract: There is ongoing debate about whether large language models (LLMs) can serve as substitutes for human participants in survey and experimental research. While recent work in fields such as marketing and psychology has explored the potential of LLM-based simulation, a growing body of evidence cautions against this practice: LLMs often fail to align with real human behavior, exhibiting limited diversity, systematic misalignment for minority subgroups, insufficient within-group variance, and discrepancies between stated beliefs and actions. This study examines an important and distinct question in this domain: whether fine-tuning on a small subset of human survey data, such as that obtainable from a pilot study, can mitigate these issues and yield realistic simulated outcomes. Using a behavioral experiment on information disclosure, we compare human and LLM-generated responses across multiple dimensions, including distributional divergence, subgroup alignment, belief-action coherence, and the recovery of regression coefficients. We find that fine-tuning on small human samples substantially improves heterogeneity, alignment, and belief-action coherence relative to the base model. However, even the best-performing fine-tuned models fail to reproduce the regression coefficients of the original study, suggesting that LLM-generated data remain unsuitable for replacing human participants in formal inferential analyses.

</details>


### [29] [Developing an Open Conversational Speech Corpus for the Isan Language](https://arxiv.org/abs/2511.21229)
*Adisai Na-Thalang,Chanakan Wittayasakpan,Kritsadha Phatcharoen,Supakit Buakaw*

Main category: cs.CL

TL;DR: 开发了首个伊森语开放对话语音数据集，包含自然对话语音而非朗读文本，捕捉真实语言现象如口语表达、自发韵律、不流利和泰语-伊森语语码转换。


<details>
  <summary>Details</summary>
Motivation: 伊森语作为泰国最广泛使用的地区方言，缺乏标准化正字法和自然对话语音资源，现有语料库多为朗读或脚本语音，无法反映真实语言使用情况。

Method: 建立实用的转录协议，在表征准确性和计算处理需求之间取得平衡，解决伊森语缺乏标准正字法带来的转录挑战。

Result: 成功创建首个伊森语开放对话语音数据集，包含自然语音及其转录，捕捉了真实对话中的语言特征。

Conclusion: 该数据集作为开放资源，有助于包容性AI发展，支持对代表性不足语言的研究，并为建模对话语音的语言和技术挑战提供基础。

Abstract: This paper introduces the development of the first open conversational speech dataset for the Isan language, the most widely spoken regional dialect in Thailand. Unlike existing speech corpora that are primarily based on read or scripted speech, this dataset consists of natural speech, thereby capturing authentic linguistic phenomena such as colloquials, spontaneous prosody, disfluencies, and frequent code-switching with central Thai. A key challenge in building this resource lies in the lack of a standardized orthography for Isan. Current writing practices vary considerably, due to the different lexical tones between Thai and Isan. This variability complicates the design of transcription guidelines and poses questions regarding consistency, usability, and linguistic authenticity. To address these issues, we establish practical transcription protocols that balance the need for representational accuracy with the requirements of computational processing. By releasing this dataset as an open resource, we aim to contribute to inclusive AI development, support research on underrepresented languages, and provide a basis for addressing the linguistic and technical challenges inherent in modeling conversational speech.

</details>


### [30] [PEFT-Bench: A Parameter-Efficient Fine-Tuning Methods Benchmark](https://arxiv.org/abs/2511.21285)
*Robert Belanec,Branislav Pecher,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: PEFT-Bench是一个统一的端到端基准测试，用于评估自回归大语言模型上的多种参数高效微调方法，包含27个NLP数据集和6种PEFT方法，并提出了考虑训练参数、推理速度和训练内存的PSCP指标。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在许多任务上取得了最先进的性能，但其大规模导致高计算和环境成本，限制了可访问性。参数高效微调方法通过减少可训练参数同时保持强大下游性能来解决这一挑战，但当前评估有限且难以复现。

Method: 引入PEFT-Bench统一基准测试，在27个NLP数据集和6种PEFT方法上进行评估，并提出PEFT软分数惩罚指标，综合考虑可训练参数、推理速度和训练内存使用。

Result: 开发了一个全面的评估框架，能够系统比较不同PEFT方法的性能，并通过PSCP指标提供更全面的性能评估。

Conclusion: PEFT-Bench填补了当前PEFT方法评估的空白，提供了一个可复现、全面的评估基准，有助于推动参数高效微调方法的发展和应用。

Abstract: Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-efficient fine-tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the increased development in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 6 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Score Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.

</details>


### [31] [Emergent Lexical Semantics in Neural Language Models: Testing Martin's Law on LLM-Generated Text](https://arxiv.org/abs/2511.21334)
*Kai Kugler*

Main category: cs.CL

TL;DR: 首次系统研究神经网络语言模型训练过程中生成的文本是否符合Martin定律（词频与多义性关系），发现该规律呈现非单调发展轨迹，在特定训练检查点达到峰值后下降，不同规模模型表现出不同的语义退化模式。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络语言模型在训练过程中是否以及如何发展出符合人类语言规律的特征，特别是Martin定律所描述的词频与多义性关系。

Method: 使用DBSCAN聚类上下文嵌入来操作化词义概念，分析四个不同规模的Pythia模型（70M-1B参数）在30个训练检查点上的表现。

Result: Martin定律在检查点100左右出现，在检查点104达到峰值相关性（r>0.6），然后在检查点105下降。小模型（70M, 160M）在后期检查点经历灾难性语义崩溃，而大模型（410M, 1B）表现出优雅退化。频率-特异性权衡在所有模型中保持稳定（r≈-0.3）。

Conclusion: 语言模型生成文本对语言规律的遵从性并非随训练单调增加，而是遵循平衡轨迹，存在最佳语义窗口。这项工作为评估神经网络语言模型中涌现的语言结构建立了新方法。

Abstract: We present the first systematic investigation of Martin's Law - the empirical relationship between word frequency and polysemy - in text generated by neural language models during training. Using DBSCAN clustering of contextualized embeddings as an operationalization of word senses, we analyze four Pythia models (70M-1B parameters) across 30 training checkpoints. Our results reveal a non-monotonic developmental trajectory: Martin's Law emerges around checkpoint 100, reaches peak correlation (r > 0.6) at checkpoint 104, then degrades by checkpoint 105. Smaller models (70M, 160M) experience catastrophic semantic collapse at late checkpoints, while larger models (410M, 1B) show graceful degradation. The frequency-specificity trade-off remains stable (r $\approx$ -0.3) across all models. These findings suggest that compliance with linguistic regularities in LLM-generated text is not monotonically increasing with training, but instead follows a balanced trajectory with an optimal semantic window. This work establishes a novel methodology for evaluating emergent linguistic structure in neural language models.

</details>


### [32] [Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model](https://arxiv.org/abs/2511.21399)
*Joshua Fonseca Rivera*

Main category: cs.CL

TL;DR: 通过微调训练，7B参数模型从近乎完全失败转变为能可靠检测单token注入的"想法"，准确率达85%，满足Lindsey提出的准确性、基础性和内部性标准。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型的内省能力是否可以通过直接训练获得，而非等待其自然涌现，以解决Lindsey提出的关于训练是否能消除模型间内省差异的开放性问题。

Method: 通过对瞬态单token注入进行微调训练，使模型能够检测、保留并报告语义内容，验证模型学习的是可迁移技能而非记忆特定向量。

Result: 模型从0.4%准确率提升到85%准确率，假阳性率为0%，在未见概念向量上表现出7.5%的泛化差距，证明学习的是可迁移技能。

Conclusion: 至少部分内省行为可以通过直接训练诱导，为构建内置AI透明度提供了可行路径。

Abstract: Lindsey (2025) investigates introspective awareness in language models through four experiments, finding that models can sometimes detect and identify injected activation patterns -- but unreliably (~20% success in the best model). We focus on the first of these experiments -- self-report of injected "thoughts" -- and ask whether this capability can be directly trained rather than waiting for emergence. Through fine-tuning on transient single-token injections, we transform a 7B parameter model from near-complete failure (0.4% accuracy, 6.7% false positive rate) to reliable detection (85% accuracy on held-out concepts at α=40, 0% false positives). Our model detects fleeting "thoughts" injected at a single token position, retains that information, and reports the semantic content across subsequent generation steps. On this task, our trained model satisfies three of Lindsey's criteria: accuracy (correct identification), grounding (0/60 false positives), and internality (detection precedes verbalization). Generalization to unseen concept vectors (7.5pp gap) demonstrates the model learns a transferable skill rather than memorizing specific vectors, though this does not establish metacognitive representation in Lindsey's sense. These results address an open question raised by Lindsey: whether "training for introspection would help eliminate cross-model differences." We show that at least one component of introspective behavior can be directly induced, offering a pathway to built-in AI transparency.

</details>


### [33] [Can LLMs extract human-like fine-grained evidence for evidence-based fact-checking?](https://arxiv.org/abs/2511.21401)
*Antonín Jarolím,Martin Fajčík,Lucia Makaiová*

Main category: cs.CL

TL;DR: 该论文构建了捷克语和斯洛伐克语的细粒度证据提取数据集，评估了多种大语言模型在证据提取任务上的表现，发现模型大小与性能不成正比，某些中等规模模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在线新闻评论中经常传播错误信息，需要有效方法来检测事实错误信息。为了支持或反驳这些评论中的主张，需要识别相关文档并精确定位证明或反驳每个主张的文本片段。

Method: 创建了由付费标注者标注的捷克语和斯洛伐克语细粒度证据数据集，评估了多种大语言模型（LLMs）在该数据集上的表现，分析模型输出与人工标注的一致性。

Result: LLMs经常无法逐字复制源文本中的证据，导致无效输出。llama3.1:8b模型虽然规模较小但正确输出比例高，gpt-oss-120b模型参数虽多但表现不佳。qwen3:14b、deepseek-r1:32b和gpt-oss:20b在模型大小与人工标注一致性之间达到了有效平衡。

Conclusion: 模型大小与证据提取性能不成正比，某些中等规模模型在细粒度证据提取任务上表现最佳，为多语言错误信息检测提供了重要见解。

Abstract: Misinformation frequently spreads in user comments under online news articles, highlighting the need for effective methods to detect factually incorrect information. To strongly support or refute claims extracted from such comments, it is necessary to identify relevant documents and pinpoint the exact text spans that justify or contradict each claim. This paper focuses on the latter task -- fine-grained evidence extraction for Czech and Slovak claims. We create new dataset, containing two-way annotated fine-grained evidence created by paid annotators. We evaluate large language models (LLMs) on this dataset to assess their alignment with human annotations. The results reveal that LLMs often fail to copy evidence verbatim from the source text, leading to invalid outputs. Error-rate analysis shows that the {llama3.1:8b model achieves a high proportion of correct outputs despite its relatively small size, while the gpt-oss-120b model underperforms despite having many more parameters. Furthermore, the models qwen3:14b, deepseek-r1:32b, and gpt-oss:20b demonstrate an effective balance between model size and alignment with human annotations.

</details>


### [34] [Text-to-SQL as Dual-State Reasoning: Integrating Adaptive Context and Progressive Generation](https://arxiv.org/abs/2511.21402)
*Zhifeng Hao,Qibin Song,Ruichu Cai,Boyan Xu*

Main category: cs.CL

TL;DR: DSR-SQL是一个双状态推理框架，通过上下文状态和生成状态的交互解决复杂企业数据库的Text-to-SQL问题，无需后训练或上下文示例就能达到竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有的分治推理方法在处理复杂企业数据库时，由于上下文容量有限、模式链接不可靠和数据库语义基础薄弱，难以保持连贯推理。

Method: DSR-SQL将Text-to-SQL建模为自适应上下文状态和渐进生成状态之间的交互。前者通过精炼大型模式并选择相关结构来构建紧凑的语义忠实环境，后者将SQL合成形式化为反馈引导的状态转换，使模型能够自我纠正并与用户意图对齐。

Result: 在Spider 2.0-Snow上达到35.28%的执行准确率，在BIRD开发集上达到68.32%的执行准确率。

Conclusion: DSR-SQL框架有效解决了复杂企业数据库Text-to-SQL中的推理连贯性问题，展现了竞争性的性能表现。

Abstract: Recent divide-and-conquer reasoning approaches, particularly those based on Chain-of-Thought (CoT), have substantially improved the Text-to-SQL capabilities of Large Language Models (LLMs). However, when applied to complex enterprise databases, such methods struggle to maintain coherent reasoning due to limited context capacity, unreliable schema linking, and weak grounding in database semantics. To overcome these issues, we introduce DSR-SQL, a \textbf{D}ual-\textbf{S}tate \textbf{R}easoning framework that models Text-to-SQL as an interaction between an adaptive context state and a progressive generation state. The first constructs a compact, semantically faithful environment by refining large schemas and selecting relevant structures, while the second formalizes SQL synthesis as feedback-guided state transitions, enabling the model to self-correct and align with user intent. Without any post-training or in-context examples, DSR-SQL achieves competitive performance, reaching 35.28\% execution accuracy on Spider 2.0-Snow and 68.32\% on BIRD development set. Our implementation will be open-sourced at: https://github.com/DMIRLAB-Group/DSR-SQL.

</details>


### [35] [Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning](https://arxiv.org/abs/2511.21416)
*Kaifeng Hong,Yinglong Zhang,Xiaoying Hong,Xuewen Xia,Xing Xu*

Main category: cs.CL

TL;DR: Odin是一个新的架构，通过定向双模块机制在特定Transformer层注入图结构，避免GNN的过平滑问题，同时保留图拓扑信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖GNN（受限于过平滑和跳数依赖扩散），要么使用Transformer（忽略图拓扑，将节点视为孤立序列），需要有效结合文本理解和结构推理。

Method: 提出Odin架构，在选定Transformer层通过定向双模块机制注入图结构，不依赖多跳扩散，而是在特定层集成多跳结构，产生与语义层次对齐的低、中、高级结构抽象。

Result: 在多个文本丰富图基准测试中，Odin达到最先进精度，Light Odin变体在显著降低计算成本的同时保持竞争力。

Conclusion: Odin和Light Odin形成了一个统一的、无跳数的结构-文本集成原则框架，严格包含纯Transformer和GNN的表达能力。

Abstract: Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism.Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs.To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.

</details>


### [36] [A Systematic Study of Model Merging Techniques in Large Language Models](https://arxiv.org/abs/2511.21437)
*Oğuz Kağan Hitit,Leander Girrbach,Zeynep Akata*

Main category: cs.CL

TL;DR: 对六种最先进的模型合并方法在大型语言模型上进行大规模系统评估，发现最简单的方法Task Arithmetic是唯一能可靠提升性能的方法，其他方法通常导致性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 模型合并能够在不额外训练的情况下将多个微调检查点合并为单一模型，但现有方法在小型模型和分类器上的优势是否适用于LLMs尚不明确。

Method: 对四种开源权重LLMs、每个基础模型十二个微调检查点和十六个标准LLM基准进行大规模评估，比较六种最先进的合并方法。

Result: Task Arithmetic是唯一能可靠提升LLM性能的方法，其他干扰感知和子空间合并方法通常导致显著性能下降。

Conclusion: 当前合并技术不能直接迁移到现代LLMs，需要设计LLM特定的合并算法和合并感知的微调方法。

Abstract: Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.

</details>


### [37] [Hierarchical Ranking Neural Network for Long Document Readability Assessment](https://arxiv.org/abs/2511.21473)
*Yurui Zheng,Yijun Chen,Shaohong Zhang*

Main category: cs.CL

TL;DR: 提出了一种双向可读性评估机制，通过捕捉上下文信息识别文本中语义丰富区域，预测句子级可读性标签，并使用成对排序算法建模可读性级别的序数关系。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习可读性评估方法大多未能考虑文本长度或可读性标签的序数关系，需要改进。

Method: 双向可读性评估机制，通过上下文信息识别语义丰富区域预测句子级可读性，使用成对排序算法通过标签减法建模序数关系。

Result: 在中文和英文数据集上的实验结果表明，该模型取得了有竞争力的性能，优于其他基线模型。

Conclusion: 提出的双向可读性评估机制和成对排序算法能有效提升可读性评估性能，特别是在处理文本长度和标签序数关系方面表现出色。

Abstract: Readability assessment aims to evaluate the reading difficulty of a text. In recent years, while deep learning technology has been gradually applied to readability assessment, most approaches fail to consider either the length of the text or the ordinal relationship of readability labels. This paper proposes a bidirectional readability assessment mechanism that captures contextual information to identify regions with rich semantic information in the text, thereby predicting the readability level of individual sentences. These sentence-level labels are then used to assist in predicting the overall readability level of the document. Additionally, a pairwise sorting algorithm is introduced to model the ordinal relationship between readability levels through label subtraction. Experimental results on Chinese and English datasets demonstrate that the proposed model achieves competitive performance and outperforms other baseline models.

</details>


### [38] [Voice, Bias, and Coreference: An Interpretability Study of Gender in Speech Translation](https://arxiv.org/abs/2511.21517)
*Lina Conti,Dennis Fucci,Marco Gaido,Matteo Negri,Guillaume Wisniewski,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 研究语音翻译模型中性别分配机制，发现模型不简单复制训练数据中的性别关联，而是学习更广泛的男性主导模式。模型能够通过声学输入覆盖内部语言模型的男性偏见，并揭示高准确率模型使用第一人称代词将性别化术语与说话者关联的新机制。


<details>
  <summary>Details</summary>
Motivation: 语音翻译中，从无语法性别语言（如英语）翻译到有语法性别语言时，说话者的声学特征可能在性别分配中起作用，这可能导致错误性别识别。但目前对语音翻译模型如何做出这些决策的理解仍很有限。

Method: 研究三个语言对（英-西/法/意）的语音翻译模型性别分配机制，分析训练数据模式、内部语言模型偏见和声学信息的相互作用，使用对比性特征归因分析频谱图。

Result: 模型学习更广泛的男性主导模式而非简单复制训练数据中的特定术语性别关联。虽然内部语言模型表现出强烈男性偏见，但模型能够基于声学输入覆盖这些偏好。高准确率模型通过第一人称代词将性别化术语与说话者关联，利用分布在频率谱而非集中在音高的性别信息。

Conclusion: 语音翻译模型发展出复杂机制来处理性别分配，能够整合声学信息和语言上下文，通过新颖的机制克服内部语言模型的偏见，实现更准确的性别识别。

Abstract: Unlike text, speech conveys information about the speaker, such as gender, through acoustic cues like pitch. This gives rise to modality-specific bias concerns. For example, in speech translation (ST), when translating from languages with notional gender, such as English, into languages where gender-ambiguous terms referring to the speaker are assigned grammatical gender, the speaker's vocal characteristics may play a role in gender assignment. This risks misgendering speakers, whether through masculine defaults or vocal-based assumptions. Yet, how ST models make these decisions remains poorly understood. We investigate the mechanisms ST models use to assign gender to speaker-referring terms across three language pairs (en-es/fr/it), examining how training data patterns, internal language model (ILM) biases, and acoustic information interact. We find that models do not simply replicate term-specific gender associations from training data, but learn broader patterns of masculine prevalence. While the ILM exhibits strong masculine bias, models can override these preferences based on acoustic input. Using contrastive feature attribution on spectrograms, we reveal that the model with higher gender accuracy relies on a previously unknown mechanism: using first-person pronouns to link gendered terms back to the speaker, accessing gender information distributed across the frequency spectrum rather than concentrated in pitch.

</details>


### [39] [Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects](https://arxiv.org/abs/2511.21533)
*Husne Ara Rubaiyeat,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 提出了孟加拉手语翻译数据集IsharaKhobor及其两个子集，以解决该低资源语言的数据稀缺问题，并进行了基准测试和词汇规范化处理。


<details>
  <summary>Details</summary>
Motivation: 孟加拉手语翻译(BdSLT)由于语言资源匮乏而受到严重限制，需要标准句子级数据集来开发面向孟加拉语社区聋哑人士的AI辅助工具。

Method: 创建了IsharaKhobor数据集及其两个子集(IsharaKhobor_small和IsharaKhobor_canonical_small)，使用基于地标的原始嵌入和RQE嵌入进行基准测试，并进行词汇限制和规范化处理。

Result: 开发了公开可用的数据集，并提供了基准测试结果，展示了词汇限制和规范化对数据集的影响。

Conclusion: IsharaKhobor数据集为孟加拉手语翻译研究提供了重要资源，通过词汇处理优化了数据质量，为开发AI辅助工具奠定了基础。

Abstract: Bangla Sign Language Translation (BdSLT) has been severely constrained so far as the language itself is very low resource. Standard sentence level dataset creation for BdSLT is of immense importance for developing AI based assistive tools for deaf and hard of hearing people of Bangla speaking community. In this paper, we present a dataset, IsharaKhobor , and two subset of it for enabling research. We also present the challenges towards developing the dataset and present some way forward by benchmarking with landmark based raw and RQE embedding. We do some ablation on vocabulary restriction and canonicalization of the same within the dataset, which resulted in two more datasets, IsharaKhobor_small and IsharaKhobor_canonical_small. The dataset is publicly available at: www.kaggle.com/datasets/hasanssl/isharakhobor [1].

</details>


### [40] [RoParQ: Paraphrase-Aware Alignment of Large Language Models Towards Robustness to Paraphrased Questions](https://arxiv.org/abs/2511.21568)
*Minjoon Choi*

Main category: cs.CL

TL;DR: RoParQ是一个评估LLM在转述问题中一致性的基准，通过XParaCon指标和针对性的SFT策略显著提升了模型鲁棒性，使轻量模型达到与大型预训练模型相当的稳定性。


<details>
  <summary>Details</summary>
Motivation: LLM在回答转述问题时表现出不一致行为，表明其依赖表层模式而非真正的语义理解，这限制了模型的可靠性。

Method: 构建RoParQ基准，通过专有模型生成转述问题并筛选出引起判断模型不一致置信度的样本；提出XParaCon评估指标；实施基于推理的转述感知SFT策略。

Result: 针对性对齐显著增强了模型鲁棒性，微调后的轻量模型达到了与更大预训练模型相当的一致性水平。

Conclusion: 该方法有效减轻了表层记忆问题，培养了更鲁棒可靠的LLM，证明了语义不变性对齐的重要性。

Abstract: Large Language Models (LLMs) often exhibit inconsistent behavior when answering paraphrased questions, suggesting a reliance on surface-level patterns rather than true semantic understanding. To address this limitation, we introduce RoParQ, a benchmark specifically constructed to evaluate cross-paraphrase consistency in closed-book multiple-choice QA. This benchmark is derived from standard datasets by generating paraphrases via proprietary models and selectively retaining examples that elicit inconsistent confidence from a judge model. We further propose XParaCon, a novel evaluation metric that quantifies a model's robustness by measuring the standard deviation of accuracies across question variants. Additionally, we implement a reasoning-based, paraphrase-aware Supervised Fine-Tuning (SFT) strategy designed to align models toward semantic invariance. Our experiments demonstrate that this targeted alignment significantly enhances robustness. Notably, fine-tuned lightweight models achieved consistency levels comparable to much larger pre-trained models. These results highlight the efficacy of our approach in mitigating superficial memorization and fostering more robust, reliable LLMs.

</details>


### [41] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出一种轻量级方法来识别大语言模型中编码特定技能的神经元，通过将神经元激活与外部标签和模型置信度相关联，无需手动标记聚合即可发现可解释的任务特定行为。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种任务上表现出色，但其内部机制仍然不透明，需要开发方法来理解模型如何编码特定技能。

Method: 基于软提示训练识别技能神经元的方法，扩展到复杂多技能场景，通过将神经元激活与辅助指标（如外部标签和模型置信度）相关联来发现可解释的行为。

Result: 在开放文本生成和自然语言推理任务上验证了该方法，能够检测驱动已知技能的神经元，并在BigBench算术推理任务中揭示以前未发现的捷径。

Conclusion: 该方法简单、轻量且广泛适用，能够有效识别大语言模型中编码特定技能的神经元，为理解模型内部机制提供了新途径。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [42] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 研究发现多种元数据类型（如文档质量指标）可以加速LLM预训练，有效元数据的共同特征是细粒度信息编码。提出了元数据追加和可学习元标记等方法，通过分析潜在表示揭示了元数据如何影响学习过程。


<details>
  <summary>Details</summary>
Motivation: 先前工作仅关注URL这一种元数据信号，本研究旨在探索其他类型的元数据是否能带来更大的预训练加速效益。

Method: 研究了多种元数据类型，引入元数据追加作为辅助任务，使用可学习元标记配合掩码损失，并通过探测分析潜在表示。

Result: 发现细粒度文档质量指标等元数据能有效加速预训练，元数据追加和可学习元标记都能提高训练效率。

Conclusion: 为整合元数据以改进LLM预训练的效率和效果提供了实用指南，细粒度信息编码是有效元数据的共同特征。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [43] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: 捷克语AI生成诗歌与人类诗歌难以区分，参与者识别准确率仅45.8%，且审美评价受作者身份偏见影响。


<details>
  <summary>Details</summary>
Motivation: 研究捷克语AI生成诗歌的感知，检验在形态复杂、资源较少的斯拉夫语言中AI诗歌创作能力。

Method: 让捷克母语者识别和审美评价AI与人类诗歌，使用逻辑回归分析识别准确率与审美评价的关系。

Result: 参与者无法有效区分AI与人类诗歌，审美评价存在作者偏见，AI诗歌实际评价与人类相当或更高。

Conclusion: AI能在捷克语等复杂语言中创作令人信服的诗歌，读者对作者身份的信念与审美评价相互关联。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [44] [Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework](https://arxiv.org/abs/2511.21686)
*Dong Wang,Yang Li,Ansong Ni,Ching-Feng Yeh,Youssef Emad,Xinjie Lei,Liam Robbins,Karthik Padthe,Hu Xu,Xian Li,Asli Celikyilmaz,Ramya Raghavendra,Lifei Huang,Carole-Jean Wu,Shang-Wen Li*

Main category: cs.CL

TL;DR: Matrix是一个去中心化的多智能体合成数据生成框架，通过分布式队列传递序列化消息来替代中心化编排器，在相同硬件资源下实现2-15倍的数据生成吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体合成框架依赖中心化编排器导致可扩展性瓶颈，或针对特定领域硬编码限制了灵活性，需要更高效、灵活的数据生成解决方案。

Method: 采用去中心化架构，将控制和数据流表示为通过分布式队列传递的序列化消息，轻量级智能体独立处理任务，计算密集型操作由分布式服务处理，基于Ray构建。

Result: 在多种合成场景（多智能体协作对话、基于网络的推理数据提取、客服环境工具使用轨迹生成）中，Matrix在相同硬件资源下实现2-15倍的数据生成吞吐量提升，且不损失输出质量。

Conclusion: Matrix框架通过去中心化设计解决了多智能体数据生成的可扩展性和灵活性限制，为大规模合成数据生成提供了高效解决方案。

Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.

</details>


### [45] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: ToolOrchestra方法训练小型编排器来协调智能工具，在解决复杂任务时比大型模型更高效且性能更好。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然功能强大，但在解决像Humanity's Last Exam这样的深度复杂问题时仍面临概念挑战和计算成本高的问题。

Method: 使用强化学习训练小型编排器，结合结果、效率和用户偏好的奖励机制来协调各种智能工具。

Result: Orchestrator模型在HLE上得分37.1%，超越GPT-5(35.1%)且效率提高2.5倍；在其他基准测试中也大幅超越GPT-5，成本仅为30%。

Conclusion: 使用轻量级编排模型组合多样化工具比现有方法更高效有效，为实用且可扩展的工具增强推理系统铺平了道路。

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [46] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文通过系统评估大语言模型在不同任务难度下的泛化能力，发现跨难度泛化通常有限，训练数据中需要包含多种难度级别才能获得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究对于训练数据难度（简单或困难）对模型性能的影响存在争议，需要更客观、大规模和细粒度的分析来理解LLMs在难度维度上的泛化能力。

Method: 使用数千个不同LLMs的输出和项目反应理论（IRT）对六个数据集中的示例进行难度排序，完全基于模型能力而非人类主观判断来确定难度评级。

Result: 跨难度泛化能力有限，无论使用简单还是困难数据进行训练，都无法在整个难度范围内获得一致的性能提升。

Conclusion: LLMs的训练和评估数据需要包含多种难度级别，在难度维度上走捷径是有风险的。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring](https://arxiv.org/abs/2511.20679)
*Melika Ayoughi,Pascal Mettes,Paul Groth*

Main category: cs.AI

TL;DR: 本文研究使用大语言模型自动重构层次结构以优化双曲嵌入质量，实验表明LLM重构的层次结构能显著提升嵌入质量并支持可解释的重组。


<details>
  <summary>Details</summary>
Motivation: 双曲嵌入质量与输入层次结构紧密相关，而现有层次结构往往不符合最优嵌入所需的高分支因子和单继承标准，需要自动化重构方法。

Method: 提出基于提示的方法，使用LLM根据双曲嵌入的期望标准来转换现有层次结构。

Result: 在16个不同层次结构上的实验表明，LLM重构的层次结构在多个标准嵌入质量指标上始终产生更高质量的双曲嵌入。

Conclusion: LLM引导的层次结构重组能够有效优化双曲嵌入质量，并为知识工程师提供可解释的重组理由。

Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.

</details>


### [48] [AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI](https://arxiv.org/abs/2511.20686)
*Chae-Gyun Lim,Seung-Ho Han,EunYoung Byun,Jeongyun Han,Soohyun Cho,Eojin Joo,Heehyeon Kim,Sieun Kim,Juhoon Lee,Hyunsoo Lee,Dongkun Lee,Jonghwan Hyeon,Yechan Hwang,Young-Jun Lee,Kyeongryul Lee,Minhyeong An,Hyunjun Ahn,Jeongwoo Son,Junho Park,Donggyu Yoon,Taehyung Kim,Jeemin Kim,Dasom Choi,Kwangyoung Lee,Hyunseung Lim,Yeohyun Jung,Jongok Hong,Sooyohn Nam,Joonyoung Park,Sungmin Na,Yubin Choi,Jeanne Choi,Yoojin Hong,Sueun Jang,Youngseok Seo,Somin Park,Seoungung Jo,Wonhye Chae,Yeeun Jo,Eunyoung Kim,Joyce Jiyoung Whang,HwaJung Hong,Joseph Seering,Uichin Lee,Juho Kim,Sunna Choi,Seokyeon Ko,Taeho Kim,Kyunghoon Kim,Myungsik Ha,So Jung Lee,Jemin Hwang,JoonHo Kwak,Ho-Jin Choi*

Main category: cs.AI

TL;DR: AssurAI是一个针对韩语多模态生成AI安全评估的质量控制数据集，包含11,480个文本、图像、视频和音频实例，覆盖35种AI风险因素，特别关注韩国社会文化背景。


<details>
  <summary>Details</summary>
Motivation: 当前的安全数据集主要是英语中心化的，无法捕捉非英语社会文化背景（如韩语）中的特定风险，且通常仅限于文本模态。

Method: 定义了35种AI风险因素的分类法，通过多学科专家小组构建；采用两阶段构建（专家引导种子和众包扩展）、三重独立标注和迭代专家红队循环的质量控制流程。

Result: 构建并发布了AssurAI数据集，包含11,480个多模态实例；试点研究验证了其在评估最新LLM安全性方面的有效性。

Conclusion: AssurAI有助于为韩国社区开发更安全可靠的生成AI系统，数据集已向公众发布。

Abstract: The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.

</details>


### [49] [$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators](https://arxiv.org/abs/2511.20693)
*Mingming Zhao,Xiaokang Wei,Yuanqi Shao,Kaiwen Zhou,Lin Yang,Siwei Rao,Junhui Zhan,Zhitang Chen*

Main category: cs.AI

TL;DR: A²Flow是一个完全自动化的智能体工作流生成框架，通过自适应的抽象操作符实现无需手动预定义的工作流构建。


<details>
  <summary>Details</summary>
Motivation: 现有方法严重依赖手动预定义的操作符，限制了泛化能力和可扩展性。为了解决这个问题，需要开发一个完全自动化的框架来生成智能体工作流。

Method: 采用三阶段操作符提取过程：1)基于案例的初始操作符生成；2)操作符聚类和初步抽象；3)深度提取抽象执行操作符。同时引入操作符记忆机制来增强工作流搜索。

Result: 在通用和具身基准测试中，A²Flow相比最先进基线实现了2.4%和19.3%的平均性能提升，并将资源使用减少了37%。

Conclusion: A²Flow通过完全自动化的操作符提取和工作流生成，显著提升了智能体工作流的性能表现和资源效率，为自动化工作流设计提供了有效的解决方案。

Abstract: Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\% and 19.3\% average performance improvement and reduces resource usage by 37\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW

</details>


### [50] [Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning](https://arxiv.org/abs/2511.20694)
*Kevin Lee,Russell Spiewak,James Walsh*

Main category: cs.AI

TL;DR: 提出了一个名为"Reasoning With a Star"的日球物理学推理数据集和基准测试方法，通过多智能体模式分解工作流程在需要演绎推理的问题上表现优于直接提示。


<details>
  <summary>Details</summary>
Motivation: 解决日球物理学中科学推理的挑战，包括整合物理假设、保持单位一致性、提供清晰科学格式，而不仅仅是事实回忆。

Method: 构建了基于NASA和UCAR Living With a Star暑期学校问题集的数据集，包含问题上下文、推理步骤、答案类型、真实目标、格式提示和元数据。使用程序化评分器通过单位感知数值容差、符号等价性和模式验证来检查预测。

Result: 基准测试显示，通过系统工程原则分解工作流程的多智能体模式在需要演绎推理的问题上优于直接提示，但在纯归纳回忆问题上表现相似。

Conclusion: 多智能体工作流程分解方法在复杂科学推理任务中具有优势，特别是在需要演绎推理而非纯粹事实回忆的场景下。

Abstract: Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.

</details>


### [51] [A Brief History of Digital Twin Technology](https://arxiv.org/abs/2511.20695)
*Yunqi Zhang,Kuangyu Shi,Biao Li*

Main category: cs.AI

TL;DR: 数字孪生技术从NASA航天器模拟发展而来，现正变革医疗领域，通过创建患者特异性虚拟模型支持诊断、治疗规划和药物开发，但仍面临互操作性、数据隐私等挑战。


<details>
  <summary>Details</summary>
Motivation: 推动医疗从被动治疗向预测性、预防性和个性化医疗转变，利用数字孪生技术整合实时数据和计算模型来改善医疗决策。

Method: 整合医学影像、生物传感器和计算模型，创建动态数据驱动的患者虚拟副本，支持双向交互和实时更新。

Result: 已开发出心脏数字孪生预测心律失常治疗结果、肿瘤数字孪生跟踪肿瘤进展优化放疗、药理学数字孪生加速药物发现等代表性应用。

Conclusion: 数字孪生技术有望彻底改变医疗模式，但需要解决互操作性、数据隐私等挑战，并通过可解释AI、联邦学习等技术推动发展，最终实现真正的个性化医疗。

Abstract: Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.

</details>


### [52] [Paraconsistent-Lib: an intuitive PAL2v algorithm Python Library](https://arxiv.org/abs/2511.20700)
*Arnaldo de Carvalho Junior,Diego Oliveira da Cruz,Bruno da Silva Alves,Fernando da Silva Paulo Junior,João Inacio da Silva Filho*

Main category: cs.AI

TL;DR: 介绍Paraconsistent-Lib：一个用于构建PAL2v算法的开源Python库，提供三种分析结果，支持多种PAL2v算法实现，减少复杂性和代码量。


<details>
  <summary>Details</summary>
Motivation: 为推理和决策系统提供一个易于使用的PAL2v算法构建工具，简化PAL2v算法的实现过程。

Method: 开发了一个通用的PAL2v标准计算库，支持12种经典格区域的分析、分析节点输出和决策输出，可以编写多种PAL2v算法。

Result: 实现了Para-analyzer、ParaExtrCTX、PAL2v Filter、PANnet和PNN等算法的独立或网络形式实现，降低了复杂性、代码量和错误率。

Conclusion: Paraconsistent-Lib是一个稳定且持续开发的开源库，能够响应用户在GitHub上提出的功能需求和改进建议。

Abstract: This paper introduces Paraconsistent-Lib, an open-source, easy-to-use Python library for building PAL2v algorithms in reasoning and decision-making systems. Paraconsistent-Lib is designed as a general-purpose library of PAL2v standard calculations, presenting three types of results: paraconsistent analysis in one of the 12 classical lattice PAL2v regions, paraconsistent analysis node (PAN) outputs, and a decision output. With Paraconsistent-Lib, well-known PAL2v algorithms such as Para-analyzer, ParaExtrCTX, PAL2v Filter, paraconsistent analysis network (PANnet), and paraconsistent neural network (PNN) can be written in stand-alone or network form, reducing complexity, code size, and bugs, as two examples presented in this paper. Given its stable state, Paraconsistent-Lib is an active development to respond to user-required features and enhancements received on GitHub.

</details>


### [53] [Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework](https://arxiv.org/abs/2511.20701)
*Nitya Tiwari,Parv Maheshwari,Vidisha Agarwal*

Main category: cs.AI

TL;DR: 本文对多模态思维链推理进行了综合分析，评估其在A-OKVQA、OKVQA和ChartQA数据集上的有效性，发现虽然视觉集成能减少推理生成中的幻觉，但思维链推理的有效性在不同问题类型间差异显著。


<details>
  <summary>Details</summary>
Motivation: 虽然最近的工作已将思维链扩展到多模态设置，并在ScienceQA等科学问答基准上取得了最先进的结果，但这些方法在不同领域的泛化能力仍未得到充分探索。

Method: 采用Zhang等人提出的两阶段框架，将推理生成与答案推断分离，并通过门控融合机制将视觉特征与基于T5的语言模型集成。通过系统消融研究分析视觉特征、推理质量和架构选择的贡献。

Result: 视觉集成显著减少了推理生成中的幻觉，但思维链推理的有效性在不同问题类型间差异很大，常识推理尤其具有挑战性。

Conclusion: 本研究为实施多模态推理系统的研究人员提供了实用见解，并确定了跨领域泛化未来改进的关键领域。

Abstract: While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.

</details>


### [54] [Learning Multi-Access Point Coordination in Agentic AI Wi-Fi with Large Language Models](https://arxiv.org/abs/2511.20719)
*Yifan Fan,Le Liang,Peng Liu,Xiao Li,Ziyang Guo,Qiao Lan,Shi Jin,Wen Tong*

Main category: cs.AI

TL;DR: 提出了一个基于大型语言模型代理的智能Wi-Fi框架，让接入点能够通过自然语言对话实时协作，自适应协调网络策略，显著提升密集重叠基本服务集环境下的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的多接入点协调协议依赖静态规则，无法适应动态网络条件（如变化的干扰水平和拓扑结构），限制了在密集Wi-Fi环境中的性能表现。

Method: 将每个接入点建模为自主的大型语言模型代理，通过认知工作流程进行自然语言对话协作，利用集成记忆、反思和工具使用功能，基于历史经验和环境反馈做出决策。

Result: 综合仿真结果表明，该代理框架成功适应了多样化和动态的网络环境，显著优于最先进的空分复用基线方法。

Conclusion: 该智能框架验证了其作为未来无线网络稳健智能解决方案的潜力，能够实现实时自适应协调策略。

Abstract: Multi-access point coordination (MAPC) is a key technology for enhancing throughput in next-generation Wi-Fi within dense overlapping basic service sets. However, existing MAPC protocols rely on static, protocol-defined rules, which limits their ability to adapt to dynamic network conditions such as varying interference levels and topologies. To address this limitation, we propose a novel Agentic AI Wi-Fi framework where each access point, modeled as an autonomous large language model agent, collaboratively reasons about the network state and negotiates adaptive coordination strategies in real time. This dynamic collaboration is achieved through a cognitive workflow that enables the agents to engage in natural language dialogue, leveraging integrated memory, reflection, and tool use to ground their decisions in past experience and environmental feedback. Comprehensive simulation results demonstrate that our agentic framework successfully learns to adapt to diverse and dynamic network environments, significantly outperforming the state-of-the-art spatial reuse baseline and validating its potential as a robust and intelligent solution for future wireless networks.

</details>


### [55] [OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability](https://arxiv.org/abs/2511.20766)
*Karen Ullrich,Jingtong Su,Claudia Shi,Arjun Subramonian,Amir Bar,Ivan Evtimov,Nikolaos Tsilivis,Randall Balestriero,Julia Kempe,Mark Ibrahim*

Main category: cs.AI

TL;DR: OpenApps是一个轻量级开源生态系统，用于评估UI代理在不同应用变体中的可靠性。研究发现，虽然固定应用中的标准可靠性相对稳定，但在不同应用变体中的可靠性差异巨大，任务成功率波动可达50%以上。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法依赖固定环境，无法衡量UI代理在不同应用设计和内容变体中的可靠性。为解决这一盲点，需要开发能够生成和应用变体的评估框架。

Method: 开发OpenApps生态系统，包含6个可配置应用（消息、日历、地图等），可在单CPU上运行并生成数千个应用版本。进行了超过10,000次独立评估，研究7个领先多模态代理的可靠性。

Result: 研究发现代理可靠性在不同应用变体中差异显著：Kimi-VL-3B的平均成功率从63%波动到仅4%；许多代理的任务成功率波动超过50%；代理行为（如循环或幻觉操作）也随环境配置大幅变化。

Conclusion: 测量代理在应用变体维度上的可靠性至关重要，OpenApps为此提供了开源评估框架，强调了在多样化应用环境中测试UI代理的必要性。

Abstract: Reliability is key to realizing the promise of autonomous UI-Agents, multimodal agents that directly interact with apps in the same manner as humans, as users must be able to trust an agent to complete a given task. Current evaluations rely on fixed environments, often clones of existing apps, which are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment. When deployed however, agents are likely to encounter variations in app design and content that can affect an agent's ability to complete a task. To address this blind spot of measuring agent reliability across app variations, we develop OpenApps, a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run, enabling easy generation and deployment of thousands of versions of each app. Specifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than $50\%$ across app variations. For example, Kimi-VL-3B's average success across all tasks fluctuates from $63\%$ to just $4\%$ across app versions. We also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration. These initial findings highlight the importance of measuring reliability along this new dimension of app variations. OpenApps is available at https://facebookresearch.github.io/OpenApps/

</details>


### [56] [Representation Interventions Enable Lifelong Unstructured Knowledge Control](https://arxiv.org/abs/2511.20892)
*Xuyuan Liu,Zhengzhang Chen,Xinshuai Dong,Yanchi Liu,Xujiang Zhao,Shengyu Chen,Haoyu Wang,Yujun Yan,Haifeng Chen*

Main category: cs.AI

TL;DR: RILKE是一种在表示空间进行干预的知识控制方法，能够在不重新训练的情况下高效更新LLM知识，支持大规模终身知识编辑，保持模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM知识过时或错误的问题，避免昂贵的重新训练，在终身学习设置中处理复杂非结构化知识的多重编辑而不产生干扰。

Method: 在模型表示空间进行干预，学习抗释义和编辑局部化的模块，将每个更新限制在低维子空间以减少交叉干扰；推理时通过查询自适应路由器选择合适模块指导生成。

Result: 在LLaMA和Qwen模型的知识编辑基准测试中，RILKE可扩展到大规模数据集，表现出高编辑成功率、强释义泛化能力，并以适度内存开销保持通用能力。

Conclusion: RILKE是LLM终身知识控制的有效且可扩展解决方案，能够在表示空间实现细粒度知识控制。

Abstract: Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is especially hard for complex, unstructured knowledge in a lifelong setting, where many edits must coexist without interference. We introduce RILKE (Representation Intervention for Lifelong KnowledgE Control), a robust and scalable method that treats knowledge control as interventions within the model's representation space. Leveraging representation-space expressiveness, we identify two properties enabling RILKE to deliver fine-grained control over complex, unstructured knowledge while maintaining general utility with frozen base weights. During training, RILKE learns paraphrase-robust and edit-localized modules that limit each update to a low-dimensional subspace to minimize cross-edit interference. In inference, a query-adaptive router selects the appropriate module to guide the model's generation. In evaluation on knowledge editing benchmarks with LLaMA and Qwen models, RILKE is scalable to large-scale datasets, demonstrating high edit success, strong paraphrase generalization, and preserving general utility with modest memory overhead. These results show RILKE is an effective and scalable solution for lifelong knowledge control in LLMs.

</details>


### [57] [From Prediction to Foresight: The Role of AI in Designing Responsible Futures](https://arxiv.org/abs/2511.21570)
*Maria Perez-Ortiz*

Main category: cs.AI

TL;DR: 本文提出"负责任计算前瞻"概念，探讨人工智能如何作为辅助工具支持负责任的前瞻实践，帮助政策制定者应对未来不确定性并塑造可持续未来。


<details>
  <summary>Details</summary>
Motivation: 在技术快速发展和全球挑战日益复杂的时代，政策制定者需要负责任的前瞻框架来应对未来不确定性。传统技术预测不足以应对复杂的社会、环境、经济和政治系统相互依赖性，需要更全面、伦理导向的方法。

Method: 建立负责任计算前瞻的基础原则，开发AI驱动的前瞻工具集，结合模拟和情景分析，增强政策制定者评估风险和制定策略的能力。强调AI作为支持工具而非替代人类判断的角色。

Result: 提出了负责任计算前瞻的新领域框架，展示了AI如何增强前瞻能力，但强调必须保持人类中心导向，确保伦理决策和长期可持续性。

Conclusion: AI应作为负责任、人类中心前瞻的支持工具，补充而非替代政策制定者的判断，通过深思熟虑的整合来应对21世纪的重大挑战。

Abstract: In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term "responsible computational foresight", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century.

</details>


### [58] [Guaranteed Optimal Compositional Explanations for Neurons](https://arxiv.org/abs/2511.20934)
*Biagio La Rosa,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 本文提出了第一个能计算保证最优组合解释的框架，通过分解空间对齐因素、设计启发式估计方法，开发出在可行时间内找到最优解释的算法。在计算机视觉领域验证发现，传统beam search方法有10-40%的解释是次优的。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络中神经元学习内容的组合解释方法通常使用beam search，但这种方法无法提供最优性保证，也不清楚当前解释与真正最优解的接近程度。

Method: 提出包含三个关键组件的框架：(i)识别影响空间对齐因素的分解方法；(ii)在搜索任何阶段估计对齐的启发式方法；(iii)第一个能在可行时间内计算最优组合解释的算法。

Result: 在计算机视觉和卷积神经网络的最常用设置中，当涉及重叠概念时，beam search获得的解释有10-40%是次优的。基于本文分解和启发式的beam search变体在运行时间上匹配或优于先前方法。

Conclusion: 本文建立了计算保证最优组合解释的理论框架，揭示了现有方法的局限性，并提供了更高效和灵活的替代方案。

Abstract: While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10-40 percent of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.

</details>


### [59] [ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction](https://arxiv.org/abs/2511.20937)
*Qineng Wang,Wenlong Huang,Yu Zhou,Hang Yin,Tianwei Bao,Jianwen Lyu,Weiyu Liu,Ruohan Zhang,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: ENACT是一个评估视觉语言模型是否展现具身认知的基准，通过视觉问答形式测试世界建模能力，包含前向和逆向世界建模两个任务。


<details>
  <summary>Details</summary>
Motivation: 探索现代视觉语言模型是否在非具身训练后仍能展现出具身认知的特征，即从感觉运动交互中产生智能。

Method: 将具身认知评估构建为部分可观测马尔可夫决策过程中的世界建模问题，设计前向世界建模（给定动作重排观察序列）和逆向世界建模（给定观察重排动作序列）两个任务。

Result: 实验显示前沿视觉语言模型与人类存在性能差距，且差距随交互范围增大而扩大；模型在逆向任务上表现更好，并表现出人类中心偏见。

Conclusion: 现代视觉语言模型在具身认知能力上仍有不足，存在与人类视觉偏差相关的性能下降，需要进一步研究如何提升其具身智能。

Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.

</details>


### [60] [New Hybrid Heuristics for Pseudo-Boolean Propagation](https://arxiv.org/abs/2511.21417)
*Mia Müßig,Jan Johannsen*

Main category: cs.AI

TL;DR: 本文提出了新的启发式方法用于伪布尔求解中的混合单元传播策略，显著提升了RoundingSAT求解器的性能。


<details>
  <summary>Details</summary>
Motivation: 当前伪布尔求解中最成功的单元传播策略是结合了观察文字方案和计数方法的混合模式，但现有方法仍有改进空间。

Method: 引入了新的启发式方法用于混合决策，优化了观察文字方案与计数方法的结合方式。

Result: 新方法能够显著超越RoundingSAT求解器中当前使用的方法，性能大幅提升。

Conclusion: 新的混合决策启发式方法在伪布尔求解中表现出色，为单元传播策略提供了有效的改进方案。

Abstract: In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.

</details>


### [61] [Improving Procedural Skill Explanations via Constrained Generation: A Symbolic-LLM Hybrid Architecture](https://arxiv.org/abs/2511.20942)
*Rahul Dass,Thomas Bowlin,Zebing Li,Xiao Jin,Ashok Goel*

Main category: cs.AI

TL;DR: Ivy是一个AI教练系统，通过结合符号化的TMK模型和生成式解释层，提供结构化的多步骤解释，改善LLM生成解释的结构质量。


<details>
  <summary>Details</summary>
Motivation: 在程序性技能学习中，教学解释不仅要传达步骤，还要传达背后的因果、目标导向和组合逻辑。LLM通常产生流畅但浅层的回答，缺乏这种结构。

Method: 结合符号化的Task-Method-Knowledge（TMK）模型与生成式解释层（LLM），TMK编码因果转换、目标层次和问题分解，在明确的结构边界内指导LLM。

Result: 与GPT和检索增强GPT基线相比，符号约束在"如何"和"为什么"问题的解释结构质量上持续改进。

Conclusion: 这项研究展示了一种可扩展的AI教育方法，增强了智能教练系统中AI生成解释的教学价值。

Abstract: In procedural skill learning, instructional explanations must convey not just steps, but the causal, goal-directed, and compositional logic behind them. Large language models (LLMs) often produce fluent yet shallow responses that miss this structure. We present Ivy, an AI coaching system that delivers structured, multi-step explanations by combining symbolic Task-Method-Knowledge (TMK) models with a generative interpretation layer-an LLM that constructs explanations while being constrained by TMK structure. TMK encodes causal transitions, goal hierarchies, and problem decompositions, and guides the LLM within explicit structural bounds. We evaluate Ivy against responses against GPT and retrieval-augmented GPT baselines using expert and independent annotations across three inferential dimensions. Results show that symbolic constraints consistently improve the structural quality of explanations for "how" and "why" questions. This study demonstrates a scalable AI for education approach that strengthens the pedagogical value of AI-generated explanations in intelligent coaching systems.

</details>


### [62] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 提出ICPO方法解决RLVR中的奖励粒度粗、奖励噪声和探索效率低的问题，通过模型生成概率计算偏好优势分数，结合可验证奖励指导探索过程。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在奖励粒度粗、奖励噪声和探索效率低等问题，导致训练不稳定和熵崩溃，需要改进以增强大语言模型的推理能力。

Method: ICPO方法基于LLM生成不同响应的概率反映其对推理过程的自我评估，通过比较同一提示下多个响应的相对生成概率计算偏好优势分数，并与可验证奖励结合指导探索。

Result: 在四个通用领域基准和三个数学基准上的综合实验表明，ICPO相比GRPO能稳定提升推理能力。

Conclusion: 偏好优势分数不仅能缓解奖励粒度和噪声问题，还能有效抑制过度自信错误，增强被低估高质量响应的相对优势，防止模型对特定策略过拟合，促进更彻底的探索。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [63] [Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning](https://arxiv.org/abs/2511.21033)
*Linze Chen,Yufan Cai,Zhe Hou,Jinsong Dong*

Main category: cs.AI

TL;DR: L4M框架结合对抗性LLM代理和SMT求解器证明，将自然语言的解释灵活性与符号验证的严谨性相结合，用于法律裁决。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统擅长表层文本分析，但缺乏原则性法理学所需的保证。法律理性体现在实质理性（结果公平性）和形式理性（遵循明确规则）两方面。

Method: 三阶段流程：1) 法规形式化：将法律条款转换为逻辑公式；2) 双重事实与法规提取：检察官和辩护方LLM独立提取事实和法规；3) 求解器中心裁决：将双方论证编译为逻辑约束，通过不满足核心触发迭代自我批评，最终由法官LLM生成透明裁决。

Result: 在公共基准测试中，该系统超越了GPT-4-mini、DeepSeek-V3、Claude 4等先进LLM以及最先进的法律AI基线，同时提供严谨且可解释的符号论证。

Conclusion: L4M成功将自然语言的解释灵活性与符号验证的严谨性相结合，为法律AI系统提供了原则性法理学保证。

Abstract: The rationality of law manifests in two forms: substantive rationality, which concerns the fairness or moral desirability of outcomes, and formal rationality, which requires legal decisions to follow explicitly stated, general, and logically coherent rules. Existing LLM-based systems excel at surface-level text analysis but lack the guarantees required for principled jurisprudence. We introduce L4M, a novel framework that combines adversarial LLM agents with SMT-solver-backed proofs to unite the interpretive flexibility of natural language with the rigor of symbolic verification. The pipeline consists of three phases: (1) Statute Formalization, where domain-specific prompts convert legal provisions into logical formulae; (2) Dual Fact and Statute Extraction, in which prosecutor- and defense-aligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation; and (3) Solver-Centric Adjudication, where an autoformalizer compiles both parties' arguments into logic constraints, and unsat cores trigger iterative self-critique until a satisfiable formula is achieved, which is then verbalized by a Judge-LLM into a transparent verdict and optimized sentence. Experimental results on public benchmarks show that our system surpasses advanced LLMs including GPT-o4-mini, DeepSeek-V3, and Claude 4 as well as state-of-the-art Legal AI baselines, while providing rigorous and explainable symbolic justifications.

</details>


### [64] [OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection](https://arxiv.org/abs/2511.21064)
*Chujie Wang,Jianyu Lu,Zhiyuan Luo,Xi Chen,Chu He*

Main category: cs.AI

TL;DR: 提出OVOD-Agent框架，将被动类别匹配转化为主动视觉推理和自我演化检测，通过Visual-CoT和w-MDP建模视觉上下文转换，结合Bandit模块和自监督奖励模型优化，在COCO和LVIS数据集上显著提升开放词汇目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇目标检测方法虽然在大规模视觉语言数据集上预训练，但推理仍局限于固定类别名称，造成多模态训练与单模态推理之间的差距。文本空间仍有待充分探索，改进文本表示可显著提升性能。

Method: 基于Chain-of-Thought思想，将文本优化过程扩展为可解释的Visual-CoT；将视觉上下文转换建模为弱马尔可夫决策过程；使用Bandit模块在有限监督下生成探索信号；整合马尔可夫转移矩阵与Bandit轨迹进行自监督奖励模型优化。

Result: 在COCO和LVIS数据集上的实验表明，OVOD-Agent在不同OVOD骨干网络上均提供一致改进，特别是在稀有类别上表现突出，验证了所提框架的有效性。

Conclusion: OVOD-Agent通过主动视觉推理和自我演化检测机制，成功弥合了多模态训练与单模态推理之间的差距，为开放词汇目标检测提供了新的有效解决方案。

Abstract: Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.

</details>


### [65] [Causality Without Causal Models](https://arxiv.org/abs/2511.21260)
*Joseph Y. Halpern,Rafael Pass*

Main category: cs.AI

TL;DR: 本文对Halpern和Pearl的因果定义进行了抽象化，使其能应用于任何定义了反事实的模型，扩展了因果关系的应用范围和处理能力。


<details>
  <summary>Details</summary>
Motivation: Halpern和Pearl的因果定义局限于因果模型，无法处理包含析取、否定、信念和嵌套反事实的复杂公式，需要更通用的定义框架。

Method: 通过抽象化Halpern-Pearl定义的关键特征，构建一个适用于任何反事实模型的通用因果定义框架。

Result: 成功开发出能处理更复杂公式的因果定义，包括析取、否定、信念和嵌套反事实，并能应用于回溯等更广泛的模型类型。

Conclusion: 抽象化方法不仅扩展了因果定义的应用范围，还深化了对因果定义特征的理解，并能扩展到解释概念的定义。

Abstract: Perhaps the most prominent current definition of (actual) causality is due to Halpern and Pearl.  It is defined using causal models (also known as structural equations models).  We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined. By abstracting the definition, we gain a number of benefits. Not only can we apply the definition in a wider range of models, including ones that allow, for example, backtracking, but we can apply the definition to determine if A is a cause of B  even if A and B are formulas involving disjunctions, negations, beliefs, and nested counterfactuals (none of which can be handled by the Halpern-Pearl definition). Moreover, we can extend the ideas to getting an abstract definition of explanation that can be applied beyond causal models. Finally, we gain a deeper understanding of features of the definition  even in causal models.

</details>


### [66] [Prune4Web: DOM Tree Pruning Programming for Web Agent](https://arxiv.org/abs/2511.21398)
*Jiayuan Zhang,Kaiquan Chen,Zhihao Lu,Enshen Zhou,Qian Yu,Jing Zhang*

Main category: cs.AI

TL;DR: Prune4Web是一个新颖的网页自动化范式，通过将DOM处理从资源密集的LLM读取转向高效的程序化修剪，解决了复杂网页DOM结构过大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的网页代理在处理复杂真实网页时面临DOM结构过大的挑战（通常10,000-100,000个token），现有策略要么依赖粗暴的DOM截断（可能丢失关键信息），要么使用低效的启发式方法和独立排名模型，无法在精度和可扩展性之间达到最佳平衡。

Method: 提出DOM树修剪编程方法，让LLM生成可执行的Python评分脚本来基于分解的子任务语义线索动态过滤DOM元素。该方法无需LLM读取原始大规模DOM，而是将遍历和评分委托给轻量级、可解释的程序。同时提出了专门的数据标注流程和两轮对话训练策略，在统一框架内联合优化规划器、程序化过滤器和定位器。

Result: 实现了候选元素25倍到50倍的减少，便于精确的动作定位同时减轻注意力稀释。在低级别定位任务上，准确率从46.8%大幅提升至88.28%。

Conclusion: Prune4Web通过程序化DOM修剪方法有效解决了网页自动化中的DOM规模问题，在真实世界网页自动化中表现出卓越效能，实现了最先进的性能。

Abstract: Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.

</details>


### [67] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 该论文提出了一个将系统动力学和结构方程建模结合到统一数学框架中的方法，用于支持负责任AI/ML的开发。


<details>
  <summary>Details</summary>
Motivation: AI/ML模型在解决复杂问题的同时可能放大人类偏见，需要更丰富的因果模型来指导负责任AI/ML的开发，但不同方法基于不同假设难以整合。

Method: 将系统动力学和结构方程建模整合到一个共同的数学框架中，用于生成系统分布、开发方法和比较结果。

Result: 建立了一个能够统一系统动力学和结构方程建模的数学框架，为数据科学和AI/ML应用提供系统动力学认识论基础。

Conclusion: 该统一框架有助于克服不同方法间的整合障碍，为负责任AI/ML的开发提供更系统的理论支持。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


### [68] [Conversational no-code and multi-agentic disease module identification and drug repurposing prediction with ChatDRex](https://arxiv.org/abs/2511.21438)
*Simon Süwer,Kester Bagemihl,Sylvie Baier,Lucia Dicunta,Markus List,Jan Baumbach,Andreas Maier,Fernando M. Delgado-Chaves*

Main category: cs.AI

TL;DR: ChatDRex是一个基于对话的多代理系统，通过自然语言访问生物医学知识图谱，实现网络药物重定位预测，让非计算机专业的医学研究人员也能执行复杂的生物信息学分析。


<details>
  <summary>Details</summary>
Motivation: 传统药物重定位预测需要多领域专家协作，但现有工具碎片化且数据异构，难以整合到统一工作流中。需要开发易用的系统来降低使用门槛。

Method: 构建基于NeDRex知识图谱的多代理系统，包含查询路由、数据检索、算法执行、结果可视化等专门代理，支持网络分析、功能一致性评估、文献挖掘等功能。

Result: 系统实现了自然语言驱动的复杂生物信息学分析，使临床专家能够生成假设并探索药物重定位机会。

Conclusion: ChatDRex通过多代理设计和自然语言接口，民主化了生物信息学资源访问，加速了新疗法发现和转化研究。

Abstract: Repurposing approved drugs offers a time-efficient and cost-effective alternative to traditional drug development. However, in silico prediction of repurposing candidates is challenging and requires the effective collaboration of specialists in various fields, including pharmacology, medicine, biology, and bioinformatics. Fragmented, specialized algorithms and tools often address only narrow aspects of the overall problem, and heterogeneous, unstructured data landscapes require specialized users to be involved. Hence, these data services do not integrate smoothly across workflows. With ChatDRex, we present a conversation-based, multi-agent system that facilitates the execution of complex bioinformatic analyses aiming for network-based drug repurposing prediction. It builds on the integrated systems medicine knowledge graph NeDRex. ChatDRex provides natural language access to its extensive biomedical KG and integrates bioinformatics agents for network analysis and drug repurposing, complemented by agents for functional coherence evaluation for in silico validation, as well as agents for literature mining and for discussing the obtained results in a scientific context. Its flexible multi-agent design assigns specific tasks to specialized agents, including query routing, data retrieval, algorithm execution, and result visualization. A dedicated reasoning module keeps the user in the loop and allows for hallucination detection. By enabling physicians and researchers without computer science expertise to control complex analyses in natural language, ChatDRex democratizes access to bioinformatics as an important resource for drug repurposing. It enables clinical experts to generate hypotheses and explore drug repurposing opportunities, ultimately accelerating the discovery of novel therapies and advancing personalized medicine and translational research.

</details>


### [69] [EWE: An Agentic Framework for Extreme Weather Analysis](https://arxiv.org/abs/2511.21444)
*Zhe Jiang,Jiong Wang,Xiaoyu Yue,Zijie Guo,Wenlong Zhang,Fenghua Ling,Wanli Ouyang,Lei Bai*

Main category: cs.AI

TL;DR: EWE是首个用于极端天气自动诊断的智能代理框架，通过模拟专家工作流程实现从原始气象数据到多模态可视化的自主分析，并建立了该领域的首个基准测试。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件风险日益增加，但传统的人工诊断方法效率低下，形成分析瓶颈。虽然AI在地球科学预测方面取得进展，但自动诊断推理这一同等重要的挑战仍未被充分探索。

Method: EWE通过知识引导规划、闭环推理和领域定制的气象工具包模拟专家工作流程，能够从原始气象数据自主生成和解释多模态可视化，实现全面诊断分析。

Result: 建立了该领域的首个基准测试，包含103个高影响事件的精选数据集和新的逐步评估指标。EWE展示了向自动化科学发现迈进的潜力。

Conclusion: EWE标志着向自动化科学发现迈出的一步，有望民主化专业知识和智力资源，特别对易受极端天气影响的发展中国家具有重要意义。

Abstract: Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.

</details>


### [70] [MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning](https://arxiv.org/abs/2511.21460)
*Junjian Wang,Lidan Zhao,Xi Sheryl Zhang*

Main category: cs.AI

TL;DR: 提出MADRA框架，通过多智能体辩论进行风险评估，无需训练即可提升AI代理的安全意识，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法计算成本高或过度拒绝安全指令的问题，确保具身AI在家庭环境中的安全部署。

Method: 使用多个LLM智能体对指令安全性进行辩论，由关键评估器基于逻辑性、风险识别、证据质量和清晰度评分，通过迭代审议和共识投票达成决策。

Result: 在AI2-THOR和VirtualHome上的实验显示，该方法能拒绝90%以上危险任务，同时保持对安全任务的低拒绝率，优于现有方法。

Conclusion: 提供了一个可扩展、模型无关的解决方案，用于构建可信赖的具身智能体。

Abstract: Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.

</details>


### [71] [SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition](https://arxiv.org/abs/2511.21471)
*Peiran Xu,Sudong Wang,Yao Zhu,Jianing Li,Yunjian Zhang*

Main category: cs.AI

TL;DR: 提出了一个分层空间认知框架，将空间智能分解为5个渐进复杂度级别，并构建了SpatialBench基准测试来系统评估多模态大语言模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于简化空间认知，将其简化为单一维度指标，无法捕捉空间能力的层次结构和相互依赖性。

Method: 构建分层空间认知框架（5个认知级别），开发SpatialBench基准测试（15个任务），引入高层次能力导向的统一评估指标。

Result: 实验显示模型在感知层面表现良好，但在符号推理、因果推断和规划方面存在局限；人类测试表明人类进行选择性目标导向抽象，而模型倾向于过度关注表面细节。

Conclusion: 建立了首个系统化测量MLLMs分层空间认知的框架，为未来空间智能系统奠定了基础。

Abstract: Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.

</details>


### [72] [Pessimistic Verification for Open Ended Math Questions](https://arxiv.org/abs/2511.21522)
*Yanxing Huang,Zihan Tang,Zejin Lin,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 提出了悲观验证方法，通过构建多个并行验证来检测数学证明中的错误，显著提升了数学问题验证性能且计算资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 现有验证性能的关键限制在于错误检测能力，需要更有效的验证方法来提高数学问题验证的可靠性。

Method: 设计悲观验证变体，为同一证明构建多个并行验证，只要任一验证报告错误就判定证明不正确。

Result: 该方法显著提升了多个数学验证基准的性能，计算效率甚至超过了扩展长链思维，且发现数据集标注错误是强模型假阴性的主要原因。

Conclusion: 悲观验证能有效提高语言模型输出的可靠性，在长范围数学任务中发挥关键作用，有助于增强语言模型的数学能力。

Abstract: The key limitation of the verification performance lies in the ability of error detection. With this intuition we designed several variants of pessimistic verification, which are simple workflows that could significantly improve the verification of open-ended math questions. In pessimistic verification we construct multiple parallel verifications for the same proof, and the proof is deemed incorrect if any one of them reports an error. This simple technique significantly improves the performance across many math verification benchmarks without incurring substantial computational resources. Its token efficiency even surpassed extended long-CoT in test-time scaling. Our case studies further indicate that the majority of false negatives in stronger models are actually caused by annotation errors in the original dataset, so our method's performance is in fact underestimated. Self-verification for mathematical problems can effectively improve the reliability and performance of language model outputs, and it also plays a critical role in enabling long-horizon mathematical tasks. We believe that research on pessimistic verification will help enhance the mathematical capabilities of language models across a wide range of tasks.

</details>


### [73] [Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit](https://arxiv.org/abs/2511.21569)
*Alex Diep*

Main category: cs.AI

TL;DR: 语言模型在专业领域无法可靠披露AI身份，导致用户无法信任其能力边界。研究发现模型在专业角色扮演中存在显著的领域特异性不一致，某些模型通过推理优化反而降低了自我透明度。


<details>
  <summary>Details</summary>
Motivation: 在专业高风险领域，如果AI无法可靠披露其身份，用户可能错误信任其专业能力，造成潜在危害。需要研究模型在不同专业角色中的自我透明度表现。

Method: 采用共同花园设计，对16个开放权重模型（4B-671B参数）进行19,200次试验审计，评估不同专业角色下的身份披露行为。使用贝叶斯验证和Rogan-Gladen校正确保结果稳健性。

Result: 模型表现出明显的领域特异性不一致：金融顾问角色初始披露率30.8%，而神经外科医生角色仅3.5%。披露率范围2.8%-73.6%，模型身份比参数数量更能预测行为。推理优化在某些模型中主动抑制了自我透明度，推理变体比基础版本披露率低达48.4%。

Conclusion: 透明度反映训练因素而非规模，组织不能假设安全属性会转移到部署环境，需要刻意设计行为和实证验证。

Abstract: If a language model cannot reliably disclose its AI identity in expert contexts, users cannot trust its competence boundaries. This study examines self-transparency in models assigned professional personas within high-stakes domains where false expertise risks user harm. Using a common-garden design, sixteen open-weight models (4B--671B parameters) were audited across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure initially, while a Neurosurgeon persona elicited only 3.5%. This creates preconditions for a "Reverse Gell-Mann Amnesia" effect, where transparency in some domains leads users to overgeneralize trust to contexts where disclosure fails. Disclosure ranged from 2.8% to 73.6%, with a 14B model reaching 61.4% while a 70B produced just 4.1%. Model identity predicted behavior better than parameter count ($ΔR_{adj}^{2} = 0.359$ vs 0.018). Reasoning optimization actively suppressed self-transparency in some models, with reasoning variants showing up to 48.4% lower disclosure than base counterparts. Bayesian validation with Rogan--Gladen correction confirmed robustness to measurement error ($κ= 0.908$). These findings demonstrate transparency reflects training factors rather than scale. Organizations cannot assume safety properties transfer to deployment contexts, requiring deliberate behavior design and empirical verification.

</details>


### [74] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 大型语言模型在8拼图任务中表现出规划能力不足，即使有反馈和验证器辅助也无法解决任何谜题，主要问题是内部状态表示脆弱和启发式规划能力弱。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在规划和状态推理方面的真实能力，使用8拼图这一经典任务来精确评估模型的状态跟踪和目标导向规划能力。

Method: 测试4个模型在零样本、思维链、算法思维等提示条件下，结合分层纠正反馈和外部移动验证器来评估模型表现。

Result: 反馈对某些模型-提示组合有改善，但成功运行通常冗长且计算昂贵。即使有外部验证器提供有效移动，所有模型都无法解决任何谜题。

Conclusion: 当前LLMs在没有外部工具的情况下规划能力存在显著局限，需要开发维护显式状态和执行结构化搜索的机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [75] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: ViLoMem是一个双流记忆框架，通过分别编码视觉分心模式和逻辑推理错误，使MLLMs能够从成功和失败经验中学习，在六个多模态基准测试中持续提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的记忆方法存在简洁性偏差，逐渐丢失关键领域知识，且仅记录单模态行为轨迹，无法保留视觉注意力和逻辑推理如何共同促成解决方案，这与人类多模态整合的语义记忆不匹配。

Method: 引入ViLoMem双流记忆框架，分别编码视觉分心模式和逻辑推理错误，采用增长-精炼原则逐步积累和更新多模态语义知识，保持稳定可泛化策略同时避免灾难性遗忘。

Result: 在六个多模态基准测试中，ViLoMem持续提高了pass@1准确率，显著减少了重复的视觉和逻辑错误。消融实验证实了具有明确分心-幻觉分离的双流记忆的必要性。

Conclusion: 错误感知的多模态记忆对于终身和跨领域代理学习具有重要价值，ViLoMem展示了双流记忆框架在多模态问题解决中的有效性。

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [76] [A derivative-free trust-region approach for Low Order-Value Optimization problems](https://arxiv.org/abs/2511.20783)
*Anderson E. Schwertner,Francisco N. C. Sobral*

Main category: math.OC

TL;DR: 提出了第一个用于约束LOVO问题的无导数信赖域算法，该算法收敛到弱临界点，并建立了全局收敛性和最坏情况迭代复杂度分析。


<details>
  <summary>Details</summary>
Motivation: LOVO问题在鲁棒参数估计、蛋白质对齐、投资组合优化等领域有实际应用，但现有方法在处理约束和无导数情况时存在局限。

Method: 开发了无导数信赖域算法，使用线性插值模型，处理约束非线性LOVO问题，其中函数是黑箱且连续可微但导数不可用。

Result: 算法收敛到弱临界点，建立了全局收敛性和最坏情况迭代复杂度分析，数值实验显示该方法在解决LOVO问题时具有高效性。

Conclusion: 提出的无导数信赖域算法是解决约束LOVO问题的有效方法，在理论和实验上都表现出良好性能。

Abstract: The Low Order-Value Optimization (LOVO) problem involves minimizing the minimum among a finite number of function values within a feasible set. LOVO has several practical applications such as robust parameter estimation, protein alignment, portfolio optimization, among others. In this work, we are interested in the constrained nonlinear optimization LOVO problem of minimizing the minimum between a finite number of function values subject to a nonempty closed convex set where each function is a black-box and continuously differentiable, but the derivatives are not available. We develop the first derivative-free trust-region algorithm for constrained LOVO problems with convergence to weakly critical points. Under suitable conditions, we establish the global convergence of the algorithm and also its worst-case iteration complexity analysis. An initial open-source implementation using only linear interpolation models is developed. Extensive numerical experiments and comparison with existing alternatives show the properties and the efficiency of the proposed approach when solving LOVO problems.

</details>


### [77] [A Review of Pseudospectral Optimal Control: From Theory to Flight](https://arxiv.org/abs/2511.20843)
*I. M. Ross,M. Karpenko*

Main category: math.OC

TL;DR: 本文回顾了伪谱最优控制理论的关键成果及其在NASA航天器飞行演示中的实现细节，探讨了该领域的新兴趋势和技术。


<details>
  <summary>Details</summary>
Motivation: 由于最优控制和伪谱理论的基础空间都是Sobolev空间，自然地将两者结合构建伪谱最优控制理论，以解决航空航天和自主系统中的挑战性控制问题。

Method: 结合伪谱理论与最优控制理论，构建伪谱最优控制理论，并在嵌入式平台上实现。

Result: 伪谱最优控制理论已成功应用于NASA航天器的飞行演示，2011年在嵌入式平台上的发布改变了解决挑战性控制问题的方式。

Conclusion: 伪谱最优控制理论在航空航天和自主系统领域具有重要应用价值，其嵌入式实现正在改变解决复杂控制问题的方法。

Abstract: The home space for optimal control is a Sobolev space. The home space for pseudospectral theory is also a Sobolev space. It thus seems natural to combine pseudospectral theory with optimal control theory and construct ``pseudospectral optimal control theory,'' a term coined by Ross. In this paper, we review key theoretical results in pseudospectral optimal control that have proven to be critical for a successful flight. Implementation details of flight demonstrations onboard NASA spacecraft are discussed along with emerging trends and techniques in both theory and practice. The 2011 launch of pseudospectral optimal control in embedded platforms is changing the way in which we see solutions to challenging control problems in aerospace and autonomous systems.

</details>


### [78] [Maximization of Supercapacitor Storage via Topology Optimization of Electrode Structures](https://arxiv.org/abs/2511.20969)
*Jiajie Li,Xiang Ji,Shenggao Zhou,Shengfeng Zhu*

Main category: math.OC

TL;DR: 提出了一种用于超级电容器电极结构的拓扑优化模型，通过最大化电极-电解质界面面积来提高能量密度，解决了超级电容器能量密度低的问题。


<details>
  <summary>Details</summary>
Motivation: 超级电容器作为广泛应用的电化学储能器件，虽然功率密度高于电池，但能量密度显著较低。本研究旨在通过优化电极结构来提高超级电容器的能量存储能力。

Method: 建立了基于修正稳态Poisson-Nernst-Planck系统描述离子电扩散的拓扑优化模型，使用变分法证明最优控制问题极小解的存在性，进行敏感性分析推导变分导数和伴随方程，开发了基于稳定半隐式格式的梯度流公式来求解拓扑优化问题。

Result: 大量数值实验展示了各种具有大电极-电解质界面面积的多孔电极结构，证明了所提出的拓扑优化模型和相应算法的有效性和鲁棒性。

Conclusion: 所提出的拓扑优化模型和算法能够有效设计具有大界面面积的多孔电极结构，为提高超级电容器的能量密度提供了有效方法。

Abstract: As widely used electrochemical storage devices, supercapacitors deliver higher power density than batteries, but suffer from significantly lower energy density. In this work, we propose a topology optimization model for electrode structure to maximize energy storage in supercapacitors. The existence of minimizers to the resulting optimal control problem, which is constrained by a modified steady-state Poisson--Nernst--Planck system describing ionic electrodiffusion, has been theoretically established by using the direct method in the calculus of variation. Sensitivity analysis of the topology optimization model is performed to derive variational derivatives and corresponding adjoint equations. A gradient flow formulation discretized by a stabilized semi-implicit scheme is developed to solve the resulting topology optimization problem. Extensive numerical experiments present various porous electrode structures that own large area of electrode-electrolyte interface, demonstrating the effectiveness and robustness of the proposed topology optimization model and corresponding algorithm.

</details>


### [79] [Data-driven control of continuous-time systems: A synthesis-operator approach](https://arxiv.org/abs/2511.21041)
*Masashi Wakaiki*

Main category: math.OC

TL;DR: 提出了一种基于合成算子的连续时间系统数据驱动控制框架，无需状态导数，直接使用连续时间数据


<details>
  <summary>Details</summary>
Motivation: 解决连续时间系统的数据驱动控制问题，避免对状态导数的需求，直接利用连续时间数据而无需采样或滤波

Method: 使用与输入和状态轨迹相关的合成算子，将数据轨迹嵌入到算子中，通过线性矩阵不等式描述系统特性

Result: 建立了系统识别和稳定的数据信息性特征，给出了存在过程噪声时二次稳定的充要条件

Conclusion: 基于合成算子的框架为连续时间系统的数据驱动控制提供了有效方法，特别适用于无状态导数信息的场景

Abstract: This paper addresses data-driven control of continuous-time systems. We develop a framework based on synthesis operators associated with input and state trajectories. A key advantage of the proposed method is that it does not require the state derivative and uses continuous-time data directly without sampling or filtering. First, systems compatible with given data are described by the synthesis operators into which data trajectories are embedded. Next, we characterize data informativity properties for system identification and for stabilization. Finally, we establish a necessary and sufficient condition for informativity for quadratic stabilization in the presence of process noise. This condition is formulated as linear matrix inequalities by exploiting the finite-rank structure of the synthesis operators.

</details>


### [80] [Accelerated ADMM: Automated Parameter Tuning and Improved Linear Convergence](https://arxiv.org/abs/2511.21210)
*Meisam Tavakoli,Fabian Jakob,Guido Carnevale,Giuseppe Notarstefano,Andrea Iannelli*

Main category: math.OC

TL;DR: 本文研究加速ADMM算法在强凸和Lipschitz光滑问题上的线性收敛性，通过Lur'e系统和IQC方法建立收敛理论，并提出参数调优启发式方法，显著提升了收敛速率。


<details>
  <summary>Details</summary>
Motivation: 传统ADMM算法收敛速度较慢，需要开发加速变体来提升性能，特别是在强凸和Lipschitz光滑问题中。

Method: 将加速ADMM表达为Lur'e系统（线性动态系统与斜率限制算子的反馈互联），使用积分二次约束(IQC)建立线性收敛性，并提出参数调优启发式方法。

Result: 新方法在LASSO回归基准测试中显著提升了线性收敛速率，相比原始算法和先前加速变体有显著改进。

Conclusion: 提出的加速ADMM方案在理论和实验上都表现出优越的收敛性能，为强凸光滑优化问题提供了有效的求解方法。

Abstract: This work studies the linear convergence of an accelerated scheme of the Alternating Direction Method of Multipliers (ADMM) for strongly convex and Lipschitz-smooth problems. We use the methodology of expressing the accelerated ADMM as a Lur'e system, i.e., an interconnection of a linear dynamical system in feedback with a slope-restricted operator, and we use Integral Quadratic Constraints to establish linear convergence. In addition, we propose several parameter tuning heuristics and their impact on the convergence rate through numerical analyses. Our new bounds show significantly improved linear convergence rates compared to the vanilla algorithm and previous proposed accelerated variants, which is also empirically validated on a LASSO regression benchmark.

</details>


### [81] [Conditional Generative Modeling of Stochastic LTI Systems: A Behavioral Approach](https://arxiv.org/abs/2511.21219)
*Jiayun Li,Yilin Mo*

Main category: math.OC

TL;DR: 提出了一种基于条件概率分布的行为条件生成模型（CGM），用于线性时不变随机系统，无需系统参数辨识，仅依赖轨迹数据即可生成未来输出预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要显式辨识系统参数，而本文旨在开发一种完全基于数据的行为方法，直接从输入输出数据中学习系统动态，避免参数辨识的复杂性。

Method: 通过从给定历史输入输出和未来输入的条件概率分布中采样，构建行为条件生成模型。模型仅依赖当前轨迹和预收集的数据，不涉及系统参数识别。

Result: 证明了CGM生成样本分布的收敛性，并给出了收敛速率；展示了CGM渐近分布与卡尔曼滤波器真实后验分布之间的差距随历史样本长度指数衰减。

Conclusion: 该行为CGM方法有效且理论保证良好，集成到预测控制器中在随机LTI系统中表现优异，数值结果验证了理论界限和控制效果。

Abstract: This paper presents a data-driven model for Linear Time-Invariant (LTI) stochastic systems by sampling from the conditional probability distribution of future outputs given past input-outputs and future inputs. It operates in a fully behavioral manner, relying solely on the current trajectory and pre-collected input-output data, without requiring explicit identification of system parameters. We refer to this model as a behavioral Conditional Generative Model (CGM). We prove the convergence of the distribution of samples generated by the CGM as the size of the trajectory library increases, with an explicit characterization of the convergence rate. Furthermore, we demonstrate that the gap between the asymptotic distribution of the proposed CGM and the true posterior distribution obtained by Kalman filter, which leverages the knowledge of all system parameters and all historical data, decreases exponentially with respect to the length of past samples. Finally, we integrate this generative model into predictive controllers for stochastic LTI systems. Numerical results verify the derived bounds and demonstrate the effectiveness of the controller equipped with the proposed behavioral CGM.

</details>


### [82] [An exact method for a problem of time slot pricing](https://arxiv.org/abs/2511.21518)
*Olivier Bilenne,Frédéric Meunier*

Main category: math.OC

TL;DR: 本文研究了公司在不同时间段提供服务时的定价优化问题，目标是确保每个时间段用户量不超过容量限制，同时最大化公司收入。针对凸距离函数情况，提出了时间复杂度为O(n³|P|³)的精确算法。


<details>
  <summary>Details</summary>
Motivation: 解决服务提供商在多个时间段提供有限容量服务时的收入最大化问题，同时考虑用户基于价格和偏好时间距离的选择行为。

Method: 提出基于凸距离函数的精确算法，算法时间复杂度为O(n³|P|³)，其中P是可能价格集合，n是时间段数量。对于实数价格情况，在距离函数和用户分布满足温和假设下，该算法也能找到渐进最优解。

Result: 开发了多项式时间算法，能够有效解决容量约束下的收入最大化定价问题，特别适用于凸距离函数场景。

Conclusion: 该算法为服务提供商在多时间段容量约束下的定价决策提供了有效的计算工具，特别是在凸距离函数假设下具有理论保证和实际应用价值。

Abstract: A company provides a service at different time slots, each slot being endowed with a capacity. A non-atomic population of users is willing to purchase this service. The population is modeled as a continuous measure over the preferred times. Every user looks at the time slot that minimizes the sum of the price assigned by the company to this time slot and the distance to their preferred time. If this sum is non-negative, then the user chooses this time slot for getting the service. If this sum is positive, then the user rejects the service.
  We address the problem of finding prices that ensure that the volume of users choosing each time slot is below capacity, while maximizing the revenue of the company. For the case where the distance function is convex, we propose an exact algorithm for solving this problem in time $O(n^3|P|^3)$, where $P$ is the set of possible prices and $n$ is the number of time slots. For the case where the prices can be any real numbers, this algorithm can also be used to find asymptotically optimal solutions in polynomial time under mild extra assumptions on the distance function and the measure modeling the population.

</details>


### [83] [Singular extremals of optimal control problems with $L^1$ cost](https://arxiv.org/abs/2511.21527)
*Andrei Agrachev,Ivan Beschastnyi,Michele Motta*

Main category: math.OC

TL;DR: 该论文研究了控制仿射系统的最优控制问题，目标是最小化控制的L1范数。应用Pontryagin极大值原理，将极值轨迹分为正则和奇异两类，获得了奇异极值的强广义Legendre-Clebsch条件，并证明该条件加上无共轭点可确保局部强最优性。


<details>
  <summary>Details</summary>
Motivation: 研究控制仿射系统在L1范数最小化下的最优控制问题，特别关注奇异极值的最优性条件。

Method: 应用Pontryagin极大值原理分析极值轨迹，推导奇异极值的强广义Legendre-Clebsch条件，结合无共轭点条件进行局部最优性分析。

Result: 获得了奇异极值的强广义Legendre-Clebsch条件，证明该条件加上无共轭点是局部强最优性的充分条件，并证明该条件对最优性是必要的。

Conclusion: 提出的强广义Legendre-Clebsch条件与无共轭点条件相结合，为控制仿射系统L1范数最小化问题提供了有效的局部最优性判据。

Abstract: We study the optimal control problem for a control-affine system, where we want to minimize the $L^1$ norm of the control. First, we show how Pontryagin Maximum Principle (PMP) applies to this problem and we divide the extremal trajectories into two categories: regular and singular extremals. Then, we obtain a strong generalized Legendre-Clebsch condition for singular extremals and we show that this condition together with the absence of conjugate points is sufficient to ensure local strong optimality. We provide also some geometric examples where we apply our results. Finally, we prove that generalized Legendre-Clebsch condition is necessary for optimality.

</details>


### [84] [Closed Form HJB Solution for Continuous-Time Optimal Control of a Non-Linear Input-Affine System](https://arxiv.org/abs/2511.21593)
*Akash Vyas,Shreyas Kumar,Jayant Kumar Mohanta,Ravi Prakash*

Main category: math.OC

TL;DR: 提出一种分析框架，为连续时间非线性输入仿射系统提供HJB方程的闭式解，避免迭代学习和数值近似。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习和自适应动态规划方法需要迭代训练和初始可行策略，存在计算效率低和依赖性问题。

Method: 基于Lyapunov理论的分析框架，为已知动态的非线性输入仿射系统推导HJB方程的闭式解。

Result: 获得了闭式控制策略，证明了闭环系统的渐近稳定性，在最优控制问题上展现出计算效率和性能优势。

Conclusion: 该方法为非线性系统最优控制提供了无需迭代学习的闭式解决方案，具有理论保证和实际应用价值。

Abstract: Designing optimal controllers for nonlinear dynamical systems often relies on reinforcement learning and adaptive dynamic programming (ADP) to approximate solutions of the Hamilton Jacobi Bellman (HJB) equation. However, these methods require iterative training and depend on an initially admissible policy. This work introduces a new analytical framework that yields closed-form solutions to the HJB equation for a class of continuous-time nonlinear input-affine systems with known dynamics. Unlike ADP-based approaches, it avoids iterative learning and numerical approximation. Lyapunov theory is used to prove the asymptotic stability of the resulting closed-loop system, and theoretical guarantees are provided. The method offers a closed-form control policy derived from the HJB framework, demonstrating improved computational efficiency and optimal performance on state-of-the-art optimal control problems in the literature.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [85] [Cryptocurrency Portfolio Management with Reinforcement Learning: Soft Actor--Critic and Deep Deterministic Policy Gradient Algorithms](https://arxiv.org/abs/2511.20678)
*Kamal Paykan*

Main category: q-fin.CP

TL;DR: 提出基于强化学习的加密货币投资组合管理框架，使用SAC和DDPG算法优化投资权重，在波动市场中实现更高收益和更低风险。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法难以适应加密货币市场的高度波动性和非线性动态特性，需要更灵活的自适应方法。

Method: 设计智能体通过模拟交易环境从历史市场数据学习连续交易动作，使用SAC和DDPG算法优化投资组合权重，最大化累积收益同时最小化下行风险和交易成本。

Result: 在多加密货币上的实验表明，SAC和DDPG智能体优于等权重和均值-方差等基准策略，其中SAC算法在噪声市场条件下表现出更高的稳定性和鲁棒性。

Conclusion: 深度强化学习在加密货币市场中具有实现自适应和数据驱动投资组合管理的潜力。

Abstract: This paper proposes a reinforcement learning--based framework for cryptocurrency portfolio management using the Soft Actor--Critic (SAC) and Deep Deterministic Policy Gradient (DDPG) algorithms. Traditional portfolio optimization methods often struggle to adapt to the highly volatile and nonlinear dynamics of cryptocurrency markets. To address this, we design an agent that learns continuous trading actions directly from historical market data through interaction with a simulated trading environment. The agent optimizes portfolio weights to maximize cumulative returns while minimizing downside risk and transaction costs. Experimental evaluations on multiple cryptocurrencies demonstrate that the SAC and DDPG agents outperform baseline strategies such as equal-weighted and mean--variance portfolios. The SAC algorithm, with its entropy-regularized objective, shows greater stability and robustness in noisy market conditions compared to DDPG. These results highlight the potential of deep reinforcement learning for adaptive and data-driven portfolio management in cryptocurrency markets.

</details>


### [86] [Constrained deep learning for pricing and hedging european options in incomplete markets](https://arxiv.org/abs/2511.20837)
*Nicolas Baradel*

Main category: q-fin.CP

TL;DR: 本文提出了一种约束深度学习方法来处理不完全市场中欧式期权的定价和对冲问题，通过神经网络优化P&L分布，特别解决了非平滑期权收益与神经网络平滑性之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 不完全金融市场中，欧式期权的定价和对冲缺乏唯一的无套利解，因为存在无法对冲的风险。传统方法难以处理非平滑的期权收益函数。

Method: 使用单一神经网络表示期权价格函数，其梯度作为对冲策略，通过损失函数强制自融资组合条件。比较了无约束网络与明确嵌入终端收益条件的约束架构。

Result: 数值实验表明，该方法在简单期权、异质期权和市场跳跃场景下都能产生优越的P&L分布，约束网络在处理现实收益方面效果显著。

Conclusion: 该研究通过整合边界约束推进了机器学习在量化金融中的应用，为不完全市场中的定价和对冲提供了实用工具。

Abstract: In incomplete financial markets, pricing and hedging European options lack a unique no-arbitrage solution due to unhedgeable risks. This paper introduces a constrained deep learning approach to determine option prices and hedging strategies that minimize the Profit and Loss (P&L) distribution around zero. We employ a single neural network to represent the option price function, with its gradient serving as the hedging strategy, optimized via a loss function enforcing the self-financing portfolio condition. A key challenge arises from the non-smooth nature of option payoffs (e.g., vanilla calls are non-differentiable at-the-money, while digital options are discontinuous), which conflicts with the inherent smoothness of standard neural networks. To address this, we compare unconstrained networks against constrained architectures that explicitly embed the terminal payoff condition, drawing inspiration from PDE-solving techniques. Our framework assumes two tradable assets: the underlying and a liquid call option capturing volatility dynamics. Numerical experiments evaluate the method on simple options with varying non-smoothness, the exotic Equinox option, and scenarios with market jumps for robustness. Results demonstrate superior P&L distributions, highlighting the efficacy of constrained networks in handling realistic payoffs. This work advances machine learning applications in quantitative finance by integrating boundary constraints, offering a practical tool for pricing and hedging in incomplete markets.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [87] [Local Dissipativity Analysis of Nonlinear Systems](https://arxiv.org/abs/2511.20838)
*Amy K. Strong,Leila Bridgeman*

Main category: eess.SY

TL;DR: 提出了一种通过凸优化确定非线性仿射控制系统局部耗散性的通用方法，同时搜索最优输入输出特性并合成连续分段仿射存储函数。


<details>
  <summary>Details</summary>
Motivation: 耗散性是非线性系统的重要输入输出特性，可用于组合鲁棒控制，但确定系统的耗散性通常是一个复杂且模型特定的过程。

Method: 将Hamilton-Jacobi不等式与耗散不等式的关系重新表述为线性矩阵不等式，开发了三角剖分的新LMI边界，并通过凸优化同时搜索最优IO特性和合成CPA存储函数。

Result: 该方法总能找到可行的IO特性和CPA或二次存储函数，前提是系统是严格局部耗散的。

Conclusion: 该方法为确定非线性控制仿射系统的局部耗散性提供了一种通用且可靠的方法。

Abstract: Dissipativity is an input-output (IO) characterization of nonlinear systems that enables compositional robust control through Vidyasagar's Network Dissipativity Theorem. However, determining the dissipativity of a system is an involved and, often, model-specific process. We present a general method to determine the local dissipativity properties of nonlinear, control affine systems. We simultaneously search for the optimal IO characterization of a system and synthesize a continuous piecewise affine (CPA) storage function via a convex optimization problem. To do so, we reformulate the relationship between the Hamilton-Jacobi inequality and the dissipation inequality as an linear matrix inequality (LMI) and develop novel LMI bounds for a triangulation. Further, we develop a method to synthesize a combined quadratic and CPA storage function to expand the systems the optimization problem is applicable to. Finally, we demonstrate that our method will always find a feasible IO characterization and a CPA or quadratic storage function given that the system is strictly locally dissipative.

</details>


### [88] [Dynamic Modeling of Load Demand in Electrified Highways Based on the EV Composition](https://arxiv.org/abs/2511.20874)
*Ashutossh Gupta,Vassilis Kekatos,Dionysios Aliprantis,Steve Pekarek*

Main category: eess.SY

TL;DR: 本文分析了电动道路（ER）中动态无线电力传输（DWPT）系统的负载特性，建立了电动汽车负载的时域和频域模型，并提出了ER段总负载的随机模型，揭示了电动汽车组成对谐波特性的影响。


<details>
  <summary>Details</summary>
Motivation: 电动道路通过动态无线电力传输技术可以延长电动汽车续航里程并减少电池需求，但由于发射线圈的空间布置，接收线圈获取的功率具有振荡特性，理解DWPT负载的动态行为对电力系统动态研究至关重要。

Method: 建立了恒定速度下单个电动汽车负载的时域和频域模型，比较了非线性控制方案与线性方案的谐波特性，提出了ER段总DWPT负载的随机模型，并使用交通模拟器的实际流量数据进行验证。

Result: 发现非线性控制方案比线性方案产生更温和的频率谐波，电动汽车负载谐波幅度随接收线圈长度减小而降低，但服务更多配备较长接收线圈的电动汽车（如卡车）并不一定意味着谐波更温和。

Conclusion: 研究结果为电网运营商和电动道路设计者提供了有价值的见解，揭示了电动汽车组成对DWPT系统谐波特性的复杂影响，有助于优化电动道路系统的设计。

Abstract: Electrified roadways (ERs) equipped with the dynamic wireless power transfer (DWPT) technology can achieve longer driving range and reduce on-board battery requirements for electric vehicles (EVs). Due to the spatial arrangement of transmitter (Tx) coils embedded into the ER pavement, the power drawn by the EV's receiver (Rx) coil is oscillatory in nature. Therefore, understanding the dynamic behavior of the total DWPT load is important for power system dynamic studies. To this end, we model the load of individual EVs in the time and frequency domains for constant EV speed. We establish that a nonlinear control scheme implemented in existing DWPT-enabled EVs exhibits milder frequency harmonics compared to its linear alternative. According to this model, the harmonics of an EV load decrease in amplitude with the Rx coil length. We further propose and analyze stochastic models for the total DWPT load served by an ER segment. Our models explain how the EV composition on the ER affects its frequency spectrum. Interestingly, we show that serving more EVs with longer Rx coils (trucks) does not necessarily entail milder harmonics. Our analytical findings are corroborated using realistic flows from a traffic simulator and offer valuable insights to grid operators and ER designers.

</details>


### [89] [Adaptive Gradient Descent MPPT Algorithm With Complexity-Aware Benchmarking for Low-Power PV Systems](https://arxiv.org/abs/2511.20895)
*Kimia Ahmadi,Wouter A. Serdijn*

Main category: eess.SY

TL;DR: 提出一种计算高效、实时的最大功率点跟踪算法，适用于快速变化辐照度和部分遮阴条件下的低功率光伏系统。该算法通过自适应梯度下降机制增强经典P&O算法，动态调整扰动步长，最小化跟踪时间和稳态振荡。


<details>
  <summary>Details</summary>
Motivation: 针对低功率光伏系统在快速变化辐照度和部分遮阴条件下的MPPT需求，需要开发计算高效、实时性强且性能优越的跟踪算法，以适应动态和资源受限的应用场景。

Method: 在经典P&O算法基础上引入自适应梯度下降机制，根据瞬时功率-电压斜率动态缩放扰动步长，并可选配初始化程序以增强部分遮阴条件下的全局MPP跟踪能力。

Result: 在标准测试条件下达到99.94%的MPPT效率，实验数据应用中达到99.21%效率，温度测试中超过99.6%效率。部分遮阴条件下初始化程序可将跟踪效率提升高达7.8%。

Conclusion: 该算法在效率、跟踪时间和计算成本方面优于35种先进的P&O基MPPT算法，适合集成到动态和资源受限条件下的低功率电源管理集成电路中。

Abstract: This paper proposes a computationally efficient, real-time maximum power point tracking (MPPT) algorithm tailored for low-power photovoltaic (PV) systems operating under fast-changing irradiance and partial shading conditions (PSC). The proposed method augments the classical perturb and observe (P&O) algorithm with an adaptive gradient descent mechanism that dynamically scales the perturbation step size based on the instantaneous power-voltage slope, thereby minimizing tracking time and steady-state oscillations. An optional initialization routine enhances global MPP (GMPP) tracking under PSC. Extensive simulations, including irradiance recordings from freely moving rodent subjects relevant to the targeted application, and tests across varying converter topologies and temperatures, demonstrate its robust, topology-independent performance. The proposed algorithm achieves 99.94 percent MPPT efficiency under standard test conditions (STC), 99.21 percent when applied to experimental data, and more than 99.6 percent for the tested temperature profiles. Under PSC, the initialization routine improves tracking efficiency by up to 7.8 percent. A normalized gate-level complexity analysis and a unified figure-of-merit (FoM) incorporating efficiency, tracking time, and computational cost demonstrate that the proposed algorithm outperforms 35 state-of-the-art P&O-based MPPT algorithms. These results underscore its suitability for integration in low-power power management integrated circuits (PMICs) operating under dynamic and resource-constrained conditions.

</details>


### [90] [Distributionally Robust Cascading Risk in Multi-Agent Rendezvous: Extended Analysis of Parameter-Induced Ambiguity](https://arxiv.org/abs/2511.20914)
*Vivek Pandey,Nader Motee*

Main category: eess.SY

TL;DR: 提出了一个理论框架来分析多智能体交会中的分布鲁棒级联故障风险，考虑了通信延迟和系统参数不确定性，并给出了闭式风险表达式。


<details>
  <summary>Details</summary>
Motivation: 在时间关键任务（如交会）中确保自主多智能体系统的安全性是一个基本挑战，特别是在通信延迟和系统参数不确定性的情况下。

Method: 使用时延动态网络作为基准模型，引入基于双变量高斯模型的条件分布鲁棒泛函来表征智能体间的风险传播。

Result: 得到了闭式风险表达式，揭示了时间延迟、网络结构、噪声统计和故障模式之间的复杂相互作用，并识别出关键敏感性模式。

Conclusion: 该框架为设计鲁棒和弹性的多智能体网络提供了可操作的见解，广泛仿真验证了理论结果的有效性。

Abstract: Ensuring safety in autonomous multi-agent systems during time-critical tasks such as rendezvous is a fundamental challenge, particularly under communication delays and uncertainty in system parameters. In this paper, we develop a theoretical framework to analyze the \emph{distributionally robust risk of cascading failures} in multi-agent rendezvous, where system parameters lie within bounded uncertainty sets around nominal values. Using a time-delayed dynamical network as a benchmark model, we quantify how small deviations in these parameters impact collective safety. We introduce a \emph{conditional distributionally robust functional}, grounded in a bivariate Gaussian model, to characterize risk propagation between agents. This yields a \emph{closed-form risk expression} that captures the complex interaction between time delays, network structure, noise statistics, and failure modes. These expressions expose key sensitivity patterns and provide actionable insight for the design of robust and resilient multi-agent networks. Extensive simulations validate the theoretical results and demonstrate the effectiveness of our framework.

</details>


### [91] [Data-Driven Post-Event Analysis with Real-World Oscillation Data from Denmark](https://arxiv.org/abs/2511.20939)
*Youhong Chen,Debraj Bhattacharjee,Balarko Chaudhuri,Mark O Malley,Nan Qin,Adrian Pilkaer Expethit*

Main category: eess.SY

TL;DR: EDMD方法基于Koopman算子理论，仅使用丹麦电网19个PMU的电压和电流相量数据，成功识别出0.2Hz振荡的主要贡献者位置，与传统DEF方法相比表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统方法如DEF在识别电力系统振荡源方面存在局限性，需要开发更有效的基于数据驱动的振荡源定位方法。

Method: 使用扩展动态模式分解(EDMD)算法，仅处理来自丹麦电网不同电压等级19个PMU的电压和电流相量数据，无需额外系统信息。

Result: EDMD准确识别出0.2Hz振荡的主要贡献者位置，与Energinet后来确认的有问题的IBR电厂位置一致，而传统DEF方法未能清晰识别该电厂。

Conclusion: EDMD基于事后分析在识别主要振荡贡献者和实现针对性SSO缓解方面具有潜力，已在模拟IBR主导系统和真实PMU数据中得到验证。

Abstract: This paper demonstrates how Extended Dynamic Mode Decomposition (EDMD), grounded in Koopman operator theory, can effectively identify the main contributor(s) to oscillations in power grids. We use PMU data recorded from a real 0.15 Hz oscillation event in Denmark for post-event analysis. To this end, the EDMD algorithm processed only voltage and current phasors from nineteen PMUs at different voltage levels across the Danish grid. In such a blind-test setting with no supplementary system information, EDMD accurately pinpointed the location of the main contributor to the 0.2 Hz oscillation, consistent with the location of the problematic IBR plant later confirmed by Energinet, where the underlying cause was a control system issue. Conventional approaches, such as the dissipating energy flow (DEF) method used in the ISO-NE OSL tool did not clearly identify this plant. This joint validation with Energinet, reinforcing earlier studies using simulated IBR-dominated systems and real PMU data from ISO-NE, highlights the potential of EDMD-based post-event analysis for identifying major oscillation contributors and enabling targeted SSO mitigation.

</details>


### [92] [Independent policy gradient-based reinforcement learning for economic and reliable energy management of multi-microgrid systems](https://arxiv.org/abs/2511.20977)
*Junkai Hu,Li Xia*

Main category: eess.SY

TL;DR: 该研究提出了一种多微电网系统的分布式能量管理方法，通过均值-方差团队随机博弈模型平衡经济性和可靠性，并开发了分布式独立策略梯度算法和深度强化学习算法来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 多微电网系统中间歇性和分布式可再生能源的集成需要同时考虑效率和经济性，以及系统的可靠性，传统的基于期望累积奖励最大化的方法无法有效处理方差指标。

Method: 将能量管理问题建模为均值-方差团队随机博弈，提出了完全分布式独立策略梯度算法（已知模型参数）和基于独立策略梯度的深度强化学习算法（未知模型参数）。

Result: 在两个场景下的数值实验验证了所提方法的有效性，能够充分利用多微电网系统的分布式计算能力。

Conclusion: 所提出的方法在多微电网系统中实现了经济性能和运行可靠性之间的良好平衡。

Abstract: Efficiency and reliability are both crucial for energy management, especially in multi-microgrid systems (MMSs) integrating intermittent and distributed renewable energy sources. This study investigates an economic and reliable energy management problem in MMSs under a distributed scheme, where each microgrid independently updates its energy management policy in a decentralized manner to optimize the long-term system performance collaboratively. We introduce the mean and variance of the exchange power between the MMS and the main grid as indicators for the economic performance and reliability of the system. Accordingly, we formulate the energy management problem as a mean-variance team stochastic game (MV-TSG), where conventional methods based on the maximization of expected cumulative rewards are unsuitable for variance metrics. To solve MV-TSGs, we propose a fully distributed independent policy gradient algorithm, with rigorous convergence analysis, for scenarios with known model parameters. For large-scale scenarios with unknown model parameters, we further develop a deep reinforcement learning algorithm based on independent policy gradients, enabling data-driven policy optimization. Numerical experiments in two scenarios validate the effectiveness of the proposed methods. Our approaches fully leverage the distributed computational capabilities of MMSs and achieve a well-balanced trade-off between economic performance and operational reliability.

</details>


### [93] [An Exact, Finite Dimensional Representation for Full-Block, Circle Criterion Multipliers](https://arxiv.org/abs/2511.20995)
*Felix Biertümpfel,Bin Hu,Geir Dullerud,Peter Seiler*

Main category: eess.SY

TL;DR: 本文首次为完整块圆判据乘子集提供了有限维特征化，通过将非线性输入输出对集等价于适当构造的分段线性函数的增量对集，实现了仅需有限个矩阵共正性约束的可计算描述。


<details>
  <summary>Details</summary>
Motivation: 完整块圆判据乘子定义了非重复扇区有界非线性所有可能的二次约束，能提供最不保守的稳定性条件，但原始形式包含不可数无限约束导致计算不可行，需要找到有限维特征化方法。

Method: 将非线性输入输出对集等价于适当构造的分段线性函数的增量对集，从而将无限约束转化为有限个矩阵共正性约束，实现计算可行的有限维描述。

Result: 成功获得了完整块圆判据乘子集的有限维特征化，对于输入输出维度≤4的问题具有精确可计算的实现。

Conclusion: 新方法通过有限个矩阵共正性约束完整描述了乘子集，显著降低了计算复杂度，为非重复扇区有界非线性系统的稳定性分析提供了更实用的工具。

Abstract: This paper provides the first finite-dimensional characterization for the complete set of full-block, circle criterion multipliers. We consider the interconnection of a discrete-time, linear time-invariant system in feedback with a non-repeated, sector-bounded nonlinearity. Sufficient conditions for stability and performance can be derived using: (i) dissipation inequalities, and (ii) Quadratic Constraints (QCs) that bound the input/output pairs of the nonlinearity. Larger classes of QCs (or multipliers) reduce the conservatism of the conditions. Full-block, circle criterion multipliers define the complete set of all possible QCs for non-repeated, sector-bounded nonlinearities. These provide the least conservative conditions. However, full-block multipliers are defined by an uncountably infinite number of constraints and hence do not lead to computationally tractable solutions if left in this raw form. This paper provides a new finite-dimensional characterization for the set of full-block, circle criterion multipliers. The key theoretical insight is: the set of all input/output pairs of non-repeated sector-bounded nonlinearities is equal to the set of all incremental pairs for an appropriately constructed piecewise linear function. Our new description for the complete set of multipliers only requires a finite number of matrix copositivity constraints. These conditions have an exact, computationally tractable implementation for problems where the nonlinearity has small input/output dimensions $(\le 4)$. We illustrate the use of our new characterization via a simple example.

</details>


### [94] [From Consensus to Robust Clustering: Multi-Agent Systems with Nonlinear Interactions](https://arxiv.org/abs/2511.21228)
*Anthony Couthures,Gustave Bainier,Vineeth Satheeskumar Varma,Samson Lasaulce,Irinel-Constantin Morarescu*

Main category: eess.SY

TL;DR: 本文建立了从共识到稳定聚类的理论框架，揭示了共识的尖锐阈值条件以及聚类状态的鲁棒性量化方法。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中非线性合作交互从共识到稳定聚类的转变机制，理解共识崩溃的条件和聚类结构的稳定性。

Method: 建立理论框架，通过Lipschitz常数与邻接矩阵特征值的不等式确定共识阈值，使用输入到状态稳定性理论量化聚类鲁棒性。

Result: 发现了共识的尖锐阈值条件，证明了聚类状态的出现需要交互律具有特定结构特性，并量化了聚类内部凝聚性对网络扰动的鲁棒性。

Conclusion: 提出了统一框架解释共识的尖锐崩溃和模块化结构的可量化鲁棒性，并在Zachary空手道俱乐部网络上验证了理论结果。

Abstract: This paper establishes a theoretical framework to describe the transition from consensus to stable clustering in multi-agent systems with nonlinear, cooperative interactions. We first establish a sharp threshold for consensus. For a broad class of non-decreasing, Lipschitz-continuous interactions, an explicit inequality linking the interaction's Lipschitz constant to the second-largest eigenvalue of the normalized adjacency matrix of the interaction graph confines all system equilibria to the synchronization manifold. This condition is shown to be a sharp threshold, as its violation permits the emergence of non-synchronized equilibria. We also demonstrate that such clustered states can only arise if the interaction law itself possesses specific structural properties, such as unstable fixed points. For the clustered states that emerge, we introduce a formal framework using Input-to-State Stability (ISS) theory to quantify their robustness. This approach allows us to prove that the internal cohesion of a cluster is robust to perturbations from the rest of the network. The analysis reveals a fundamental principle: cluster coherence is limited not by the magnitude of external influence, but by its heterogeneity across internal nodes. This unified framework, explaining both the sharp breakdown of consensus and the quantifiable robustness of the resulting modular structures, is validated on Zachary's Karate Club network, used as a classic benchmark for community structure.

</details>


### [95] [Stability of data-driven Koopman MPC with terminal conditions](https://arxiv.org/abs/2511.21248)
*Irene Schimperna,Lea Bold,Johannes Köhler,Karl Worthmann,Lalo Magni*

Main category: eess.SY

TL;DR: 本文证明了在比例误差界条件下，使用数据驱动代理模型作为预测模型的MPC控制器能够保证递归可行性和渐近稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在使用数据驱动代理模型作为预测模型时，保证MPC控制器的稳定性和可行性，特别是在存在近似误差的情况下。

Method: 采用基于Koopman算子的核扩展动态模态分解(kEDMD)方法构建数据驱动代理模型，并推导比例误差界条件。

Result: 证明了在比例误差界条件下，MPC控制器具有递归可行性和渐近稳定性，并通过数值案例验证了方法的适用性。

Conclusion: 提出的框架为使用数据驱动代理模型的MPC控制器提供了理论保证，特别适用于一类非线性系统。

Abstract: This paper derives conditions under which Model Predictive Control (MPC) with terminal conditions, using a data-driven surrogate model as a prediction model, asymptotically stabilizes the plant despite approximation errors. In particular, we prove recursive feasibility and asymptotic stability if a proportional error bound holds, where proportional means that the bound is linear in the norm of the state and the input. For a broad class of nonlinear systems, this condition can be satisfied using data-driven surrogate models generated by kernel Extended Dynamic Mode Decomposition (kEDMD) using the Koopman operator. Last, the applicability of the proposed framework is demonstrated in a numerical case study.

</details>


### [96] [Design and Measurements of mmWave FMCW Radar Based Non-Contact Multi-Patient Heart Rate and Breath Rate Monitoring System](https://arxiv.org/abs/2511.21255)
*Jewel Benny,Pranjal Mahajan,Srayan Sankar Chatterjee,Mohd Wajid,Abhishek Srivastava*

Main category: eess.SY

TL;DR: 提出了一种基于FMCW毫米波雷达的非接触式多患者心率和呼吸率监测系统，采用最小二乘解结合多种处理方法提高测量精度，实验结果显示呼吸率准确率>97%，心率准确率>93%。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达技术发展为非接触式心率和呼吸率监测提供了可能，同时监测多患者的需求在群体监测场景中日益重要。

Method: 使用FMCW毫米波雷达，结合多种处理方法并通过最小二乘解来提高测量精度、泛化能力和处理测量误差。

Result: 实验结果显示，系统在呼吸率测量上准确率超过97%，心率测量准确率超过93%。

Conclusion: 该系统成功实现了非接触式多患者心率和呼吸率监测，具有高精度和实用价值。

Abstract: Recent developments in mmWave radar technologies have enabled the truly non-contact heart-rate (HR) and breath-rate (BR) measurement approaches, which provides a great ease in patient monitoring. Additionally, these technologies also provide opportunities to simultaneously detect HR and BR of multiple patients, which has become increasingly important for efficient mass monitoring scenarios. In this work, a frequency modulated continuous wave (FMCW) mmWave radar based truly non-contact multiple patient HR and BR monitoring system has been presented. Furthermore, a novel approach is also proposed, which combines multiple processing methods using a least squares solution to improve measurement accuracy, generalization, and handle measurement error. The proposed system has been developed using Texas Instruments' FMCW radar and experimental results with multiple subjects are also presented, which show >97% and >93% accuracy in the measured BR and HR values, respectively.

</details>


### [97] [Response-Based Frequency Stability Assessment under Multi-Scale Disturbances in High-Renewable Power Systems](https://arxiv.org/abs/2511.21269)
*Jinhui Chen,Huadong Sun,Ping Wu,Baocai Wang,Bing Zhao*

Main category: eess.SY

TL;DR: 提出了一种基于响应的频率稳定性评估方法，通过发电机电气响应推断扰动功率，统一处理多时间尺度的扰动，为高可再生能源电力系统提供定量频率稳定性评估。


<details>
  <summary>Details</summary>
Motivation: 高可再生能源电力系统中，有功功率扰动变得更大且时间尺度更加多样化，这使得在意外事件下的频率稳定性评估变得复杂。

Method: 基于发电机组的实测功率响应构建统一扰动功率模型，在线识别扰动类型并量化扰动强度；为每类扰动推导解析频率响应模型；对于阶跃扰动获得最大可容忍扰动功率，对于斜率型扰动计算频率偏差超限时间。

Result: 在CSEE-FS频率稳定性基准系统上验证了所提方法的有效性和准确性。

Conclusion: 该方法能够有效准确地评估高可再生能源电力系统的频率稳定性，为系统安全运行提供重要支撑。

Abstract: In high-renewable power systems, active-power disturbances are becoming larger and exhibit increasingly diverse time scales, which complicates frequency stability assessment under unanticipated events. This paper presents a response-based frequency stability assessment method that uses disturbance power, inferred from generator electrical responses, to provide a unified treatment of multi-scale disturbances. Unanticipated disturbances are first classified into short-term and permanent events; permanent disturbances are further divided into step, second-level slope and minute-level slope disturbances. Based on the measured power responses of generator groups, a unified disturbance-power model is constructed to identify the disturbance type online and to quantify disturbance intensity through the disturbance power and its rate of change. Analytical frequency-response models are then derived for each disturbance class. For step disturbances, the maximum tolerable disturbance power is obtained under steady-state and transient frequency deviation constraints, and a safety-margin index is defined. For slope-type disturbances, an improved system frequency response (SFR) model and the rotor motion equation after exhaustion of primary frequency regulation are used to compute the over-limit time of frequency deviation. The proposed response-based assessment method is validated on the CSEE-FS frequency-stability benchmark system, demonstrating its effectiveness and accuracy for quantitative frequency stability assessment in high-renewable power systems.

</details>


### [98] [Adaptive Lighting Control in Visible Light Systems: An Integrated Sensing, Communication, and Illumination Framework](https://arxiv.org/abs/2511.21271)
*Xinyan Xie,Xuesong Wang,Xin Lai,Yongheng Wen,Fengrui Yang,Haoyang He,Lai Zhang,Dong Zhao*

Main category: eess.SY

TL;DR: 提出自适应集成感知、通信和照明框架，通过分区优化解决性能与能耗冲突，实现53.59%节能和57.79%信噪比均匀性提升


<details>
  <summary>Details</summary>
Motivation: 当前室内可见光通信研究主要关注最大化数据速率和感知精度，导致高性能、高能耗与用户视觉舒适度之间的冲突，需要解决这一矛盾

Method: 采用几何方法划分接收平面为活动区和非活动区，基于NLOS感知的用户位置作为动态切换开关，在活动区最小化总发射功率保证通信照明性能，在非活动区最大化SNR均匀性

Result: 相比非自适应系统节能53.59%，SNR均匀性提升57.79%，满足所有照明约束，平均定位误差0.071米

Conclusion: 自适应ISCI框架成功解决了性能与能耗冲突，在保证通信照明质量的同时显著降低能耗并改善SNR均匀性

Abstract: Indoor visible light communication (VLC) is a promising sixth-generation (6G) technology, as its directional and sensitive optical signals are naturally suited for integrated sensing and communication (ISAC). However, current research mainly focuses on maximizing data rates and sensing accuracy, creating a conflict between high performance, high energy consumption, and user visual comfort. This paper proposes an adaptive integrated sensing, communication, and illumination (ISCI) framework that resolves this conflict by treating energy savings as a primary objective. The framework's mechanism first partitions the receiving plane using a geometric methodology, defining an activity area and a surrounding non-activity area to match distinct user requirements. User location, determined using non-line-of-sight (NLOS) sensing, then acts as a dynamic switch for the system's optimization objective. The system adaptively shifts between minimizing total transmit power while guaranteeing communication and illumination performance in the activity area and maximizing signal-to-noise ratio (SNR) uniformity in the non-activity area. Numerical results confirm that this adaptive ISCI approach achieves 53.59% energy savings over a non-adaptive system and improves SNR uniformity by 57.79%, while satisfying all illumination constraints and maintaining a mean localization error of 0.071 m.

</details>


### [99] [Respiratory Motion Compensation and Haptic Feedback for X-ray-Guided Teleoperated Robotic Needle Insertion](https://arxiv.org/abs/2511.21273)
*Ana Cordon-Avila,Mostafa Selim,Momen Abayazid*

Main category: eess.SY

TL;DR: 该论文提出了一种机器人补偿呼吸运动的方法，通过运动估计模型和基于接近度的触觉反馈进行远程操作，在肝脏模型上验证显示运动估计误差低于3mm，总体插入误差在2.60-7.75mm之间。


<details>
  <summary>Details</summary>
Motivation: 呼吸运动限制了腹部经皮手术的准确性和精度，需要开发方法来补偿呼吸引起的运动并减少辐射暴露。

Method: 使用运动估计模型进行机器人呼吸运动补偿，并通过基于接近度的触觉反馈进行远程操作插入，实现无辐射远程插入。

Result: 在机器人肝脏模型上进行了5次插入验证，运动估计误差在所有运动方向均低于3mm，总体3D插入误差分别为：上下方向2.60mm、侧向7.75mm、前后方向2.86mm。

Conclusion: 该方法有望最小化因呼吸运动导致的治疗或诊断不准确，并减少辐射暴露。

Abstract: Respiratory motion limits the accuracy and precision of abdominal percutaneous procedures. In this paper, respiratory motion is compensated robotically using motion estimation models. Additionally, a teleoperated insertion is performed using proximity-based haptic feedback to guide physicians during insertion, enabling a radiation-free remote insertion for the end-user. The study has been validated using a robotic liver phantom, and five insertions were performed. The resulting motion estimation errors were below 3 mm for all directions of motion, and the overall resulting 3D insertion errors were 2.60, 7.75, and 2.86 mm for the superior-inferior, lateral, and anterior-posterior directions of motion, respectively. The proposed approach is expected to minimize the chances of inaccurate treatment or diagnosis due to respiratory-induced motion and reduce radiation exposure.

</details>


### [100] [Data-Driven Reduction of Fault Location Errors in Onshore Wind Farm Collectors](https://arxiv.org/abs/2511.21300)
*A. J. Alves Junior,M. J. B. B. Davi,R. A. S. Fernandes,M. Oleskovicz,D. V. Coury*

Main category: eess.SY

TL;DR: 提出了一种基于门控残差网络的机器学习方法，用于提高风电场集电网络中故障定位的准确性，相比现有方法将故障定位误差降低了76%。


<details>
  <summary>Details</summary>
Motivation: 随着基于逆变器资源的集成增加，传统基于相量的故障诊断方法在风电场集电网络中的有效性受到挑战，需要开发更精确的故障定位方法。

Method: 开发了基于门控残差网络的校正模型，通过综合特征工程和选择过程，在PSCAD构建的真实风电场模型上训练，并利用Optuna框架进行超参数优化。

Result: 该方法显著提高了故障定位精度，相比最先进方法整体故障定位误差降低了76%，并表现出良好的可扩展性和对拓扑及运行变化的适应性。

Conclusion: 该方法推进了现代电力系统中数据驱动故障定位框架的部署，为风电场集电网络提供了更可靠的故障定位解决方案。

Abstract: Accurate fault location is essential for operational reliability and fast restoration in wind farm collector networks. However, the growing integration of inverter-based resources changes the current and voltage behavior during faults, challenging the effectiveness of traditional phasor-based diagnostic methods. In this context, the present paper introduces an advanced machine-learning solution that enhances a deterministic fault distance estimator by incorporating a correction model driven by a Gated Residual Network, specifically designed to minimize residual fault location errors. Through comprehensive feature engineering and selection processes, an improved predictor was developed and trained on a diverse set of fault scenarios simulated in a PSCAD-based real-world wind farm model, including variations in fault type, resistance, location, inception angle, and generation penetration. Hyperparameter optimization was performed using the Optuna framework, and the robustness of the method was statistically validated. Results show a significant improvement in accuracy, with a 76% overall decrease in fault location error compared to state-of-the-art approaches. The proposed method demonstrates strong scalability and adaptability to topological and operational changes. This approach advances the deployment of data-driven fault location frameworks for modern power systems.

</details>


### [101] [Sparse shepherding control of large-scale multi-agent systems via Reinforcement Learning](https://arxiv.org/abs/2511.21304)
*Luigi Catello,Italo Napolitano,Davide Salzano,Mario di Bernardo*

Main category: eess.SY

TL;DR: 提出了一个强化学习框架，用于大规模多智能体系统的稀疏间接控制，通过少数受控智能体来塑造大量非受控智能体的集体行为。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多智能体系统中稀疏控制的多尺度挑战，即如何通过少量受控智能体实现对整个群体宏观行为的控制。

Method: 将受控智能体的ODE建模与非受控群体密度的PDE描述相结合，结合无模型强化学习和自适应交互强度补偿来克服稀疏驱动限制。

Result: 数值验证表明该方法能有效实现密度控制，系统能够达到目标分布，同时对干扰和测量噪声保持鲁棒性。

Conclusion: 基于学习的稀疏控制可以替代计算昂贵的在线优化方法，为大规模系统控制提供了可行方案。

Abstract: We propose a reinforcement learning framework for sparse indirect control of large-scale multi-agent systems, where few controlled agents shape the collective behavior of many uncontrolled agents. The approach addresses this multi-scale challenge by coupling ODEs (modeling controlled agents) with a PDE (describing the uncontrolled population density), capturing how microscopic control achieves macroscopic objectives. Our method combines model-free reinforcement learning with adaptive interaction strength compensation to overcome sparse actuation limitations. Numerical validation demonstrates effective density control, with the system achieving target distributions while maintaining robustness to disturbances and measurement noise, confirming that learning-based sparse control can replace computationally expensive online optimization.

</details>


### [102] [Design and Performance Assessment of a Virtualized IED for Digital Substations](https://arxiv.org/abs/2511.21310)
*Alailton J. Alves Junior,Denis V. Coury,Ricardo A. S. Fernandes*

Main category: eess.SY

TL;DR: 本文开发、实现并评估了虚拟智能电子设备(vIED)，验证其在数字变电站中能够达到硬件IED的性能水平，适合在时间敏感的关键环境中部署。


<details>
  <summary>Details</summary>
Motivation: 数字变电站虽然提高了电网保护的效率和可靠性，但面临高成本、网络复杂和升级性有限等挑战。虚拟化IED作为成本效益高的解决方案，需要对其性能和可靠性进行严格评估。

Method: 在服务器上使用虚拟机部署vIED，核心逻辑采用低级编程语言实现以确保高速确定性行为，通过实时仿真评估保护功能的响应时间。

Result: vIED实现了可接受的响应时间，证明了其在数字变电站时间敏感关键环境中的适用性。

Conclusion: 虚拟IED能够达到硬件IED的性能水平，验证了其在数字变电站中部署的可行性，为降低成本和提升系统灵活性提供了有效解决方案。

Abstract: Digital substations have significantly enhanced power grid protection by replacing traditional copper wiring with fiber-optic communication and integrating IEC 61850-compliant Intelligent Electronic Devices (IEDs), resulting in greater efficiency, reliability, and interoperability. While these advancements provide improved interoperability, challenges such as high costs, complex networks, and limited upgradeability persist. To mitigate these issues, the virtualization of IEDs has emerged as a cost-effective solution, offering scalability, simplified maintenance, and reduced hardware costs by replacing traditional hardware-based IEDs with software-based counterparts. However, the performance and reliability of virtual IEDs (vIED) must be rigorously evaluated to ensure their robustness in real-time applications. This paper develops, implements, and evaluates a vIED designed to match the performance of its hardware-based counterparts. The vIED was deployed on a server using virtual machines, with its core logic implemented in low-level programming languages to ensure high-speed, deterministic behavior. The performance was evaluated using real-time simulations, focusing on the response times of the protection functions. The results demonstrated that vIEDs achieved acceptable response times, validating their suitability for deployment in critical time-sensitive environments within digital substations.

</details>


### [103] [Scalable Multisubject Vital Sign Monitoring With mmWave FMCW Radar and FPGA Prototyping](https://arxiv.org/abs/2511.21314)
*Jewel Benny,Narahari N. Moudhgalya,Mujeev Khan,Hemant Kumar Meena,Mohd Wajid,Abhishek Srivastava*

Main category: eess.SY

TL;DR: 提出了一种基于FMCW雷达的非接触式多人生命体征监测系统，解决了传统接触式设备的局限性，并通过FPGA实现提供了硬件加速方案。


<details>
  <summary>Details</summary>
Motivation: 传统生命体征监测方法存在诸多限制，包括穿戴设备不适、校准困难以及接触测量设备的感染风险。本研究旨在开发适用于多种关键场景的通用非接触式生命体征监测解决方案。

Method: 采用频率调制连续波(FMCW)雷达系统，实现多人同时非接触监测。通过FPGA硬件实现，提供便携式解决方案，相比之前工作执行速度提升2.7倍，LUT利用率降低18.4%。

Result: 系统能够同时监测多人的生命体征，FPGA实现相比软件版本提供超过7400倍的加速，证明了系统在实际应用中的可行性。

Conclusion: 该研究展示了基于FMCW雷达的非接触式多人生命体征监测系统的潜力，通过硬件加速实现高效性能，有望重新定义生命体征监测方式。

Abstract: In this work, we introduce an innovative approach to estimate the vital signs of multiple human subjects simultaneously in a non-contact way using a Frequency Modulated Continuous Wave (FMCW) radar-based system. Traditional vital sign monitoring methods often face significant limitations, including subject discomfort with wearable devices, challenges in calibration, and the risk of infection transmission through contact measurement devices. To address these issues, this research is motivated by the need for versatile, non-contact vital monitoring solutions applicable in various critical scenarios. This work also explores the challenges of extending this capability to an arbitrary number of subjects, including hardware and theoretical limitations. Supported by rigorous experimental results and discussions, the paper illustrates the system's potential to redefine vital sign monitoring. An FPGA-based implementation is also presented as proof of concept for a hardware-based and portable solution, improving upon previous works by offering 2.7x faster execution and 18.4% less Look-Up Table (LUT) utilization, as well as providing over 7400x acceleration compared to its software counterpart.

</details>


### [104] [Analytical Phasor-Based Fault Location Enhancement for Wind Farm Collector Networks](https://arxiv.org/abs/2511.21319)
*Alailton J. Alves Junior,Daniel Barbosa,Ricardo A. S. Fernandes,Denis V. Coury*

Main category: eess.SY

TL;DR: 提出了一种针对风电场集电网络中故障定位的补偿框架，解决了逆变器资源下游故障时传统单端相量法不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 逆变器资源的广泛集成改变了故障电流特性，传统保护方法在IBR位于故障下游时失效，导致故障距离系统性高估。

Method: 通过在线路回路方程中增加距离相关的电压校正项，使用序域表示进行解析推导，支持多种故障类型，仅需本地测量。

Result: EMT仿真显示定位精度显著提升，平均和最大误差大幅降低，接地故障误差减少超过90%，性能不受风电渗透率影响。

Conclusion: 该方法保持了传统方法的简单性和可解释性，为现代可再生能源主导电网提供了实用的故障定位解决方案。

Abstract: The increasing integration of Inverter-Based Resources (IBRs) is reshaping fault current characteristics, presenting significant challenges to traditional protection and fault location methods. This paper addresses a key limitation in fault location within wind farm collector networks, i.e., one-terminal phasor-based methods become inaccurate when IBRs are electrically located downstream from the fault. In such cases, the voltage drop caused by IBR fault current injections is not captured by the Intelligent Electronic Device, resulting in a systematic overestimation of fault distance. To mitigate this issue, a general compensation framework was proposed by augmenting classical loop formulations with a distance-dependent voltage correction term. The methodology was derived analytically using a sequence-domain representation and generalized to multiple fault types through a unified notation. It maintains the simplicity and interpretability of conventional approaches and can be implemented using only local measurements. The method was evaluated through EMT simulations in PSCAD using a realistic wind farm model. Results show significant improvements in location accuracy, with average and maximum errors notably reduced, especially for ground-involved faults where reductions exceed 90\%. Furthermore, the compensation eliminates sensitivity to wind penetration levels and ensures uniform performance across feeders, positioning the method as a practical solution for modern renewable-dominated grids.

</details>


### [105] [Bang-Bang Evasion: Its Stochastic Optimality and a Terminal-Set-Based Implementation](https://arxiv.org/abs/2511.21633)
*Liraz Mudrik,Yaakov Oshman*

Main category: eess.SY

TL;DR: 本文提出了一种在随机框架下的最优规避策略，针对平面终端交战场景，其中目标具有有限横向加速度，需要躲避采用线性反馈制导的导弹。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么假设完美信息，要么在随机设置中使用启发式机动模型，无法处理现实中的不完美信息和有限控制问题。

Method: 基于广义分离定理，将控制律与状态后验分布相结合，证明了最优规避策略的存在性，并提出了闭环终端集规避(TSE)策略。

Result: 蒙特卡洛仿真表明，TSE策略在对抗比例导航追击器时，优于基于随机电报、Singer和编织模型的传统随机规避策略。

Conclusion: 将确定性场景中的bang-bang最优规避扩展到随机规避场景是可行的，TSE策略为现实随机规避问题提供了有效解决方案。

Abstract: We address the problem of optimal evasion in a planar endgame engagement, where a target with bounded lateral acceleration seeks to avoid interception by a missile guided by a linear feedback law. Contrary to existing approaches, that assume perfect information or use heuristic maneuver models in stochastic settings, we formulate the problem in an inherently stochastic framework involving imperfect information and bounded controls. Complying with the generalized separation theorem, the control law factors in the posterior distribution of the state. Extending the well-known optimality of bang-bang evasion maneuvers in deterministic settings to the realm of realistic, stochastic evasion scenarios, we firstly prove that an optimal evasion strategy always exists, and that the set of optimal solutions includes at least one bang-bang policy, rendering the resulting optimal control problem finite-dimensional. Leveraging this structure, we secondly propose the closed-loop terminal-set-based evasion (TSE) strategy, and demonstrate its effectiveness in simulation against a proportional navigation pursuer. Monte Carlo simulations show that the TSE strategy outperforms traditional stochastic evasion strategies based on random telegraph, Singer, and weaving models.

</details>


### [106] [Model Predictive Control and Moving Horizon Estimation using Statistically Weighted Data-Based Ensemble Models](https://arxiv.org/abs/2511.21343)
*Laura Boca de Giuli,Samuel Mallick,Alessio La Bella,Azita Dabiri,Bart De Schutter,Riccardo Scattolini*

Main category: eess.SY

TL;DR: 提出了一种基于集成数据模型的模型预测控制框架，使用马氏距离的组合规则和移动水平估计状态观测器，用于多工况下复杂系统的最优控制。


<details>
  <summary>Details</summary>
Motivation: 复杂系统在多工况下需要自适应控制策略，传统单一模型难以适应不同运行条件的变化。

Method: 使用集成数据模型，基于马氏距离的组合规则动态调整权重，结合移动水平估计状态观测器。

Result: 在基准能源系统上验证了方法的有效性，能够适应多种运行工况。

Conclusion: 提出的集成模型预测控制框架能够有效处理多工况复杂系统的控制问题。

Abstract: This paper presents a model predictive control (MPC) framework leveraging an ensemble of data-based models to optimally control complex systems under multiple operating conditions. A novel combination rule for ensemble models is proposed, based on the statistical Mahalanobis distance, enabling the ensemble weights to suitably vary across the prediction window based on the system input. In addition, a novel state observer for ensemble models is developed using moving horizon estimation (MHE). The effectiveness of the proposed methodology is demonstrated on a benchmark energy system operating under multiple conditions.

</details>


### [107] [Evaluation of Large Language Models for Numeric Anomaly Detection in Power Systems](https://arxiv.org/abs/2511.21371)
*Yichen Liu,Hongyu Wu,Bo Liu*

Main category: eess.SY

TL;DR: 本文对大型语言模型在电力系统数值异常检测中的应用进行了全面评估，使用GPT-OSS-20B模型在IEEE 14总线系统上进行测试，比较了多种学习方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在电网中受到关注，但其在大规模数值数据异常检测方面的性能尚未充分探索，而异常检测对电网韧性至关重要。

Method: 使用GPT-OSS-20B模型，在IEEE 14总线系统上应用标准化提示框架，包括零样本、少样本、上下文学习、LoRA、微调和混合方法，采用基于三西格玛准则的规则感知设计。

Result: 报告了检测性能和推理质量，为LLM在异常检测中的能力与局限性研究奠定了基础。

Conclusion: 本研究为基于LLM的异常检测及其在电网物理信息系统中与传统检测器集成的进一步研究奠定了基础。

Abstract: Large language models (LLMs) have gained increasing attention in power grids for their general-purpose capabilities. Meanwhile, anomaly detection (AD) remains critical for grid resilience, requiring accurate and interpretable decisions based on multivariate telemetry. Yet the performance of LLMs on large-scale numeric data for AD remains largely unexplored. This paper presents a comprehensive evaluation of LLMs for numeric AD in power systems. We use GPT-OSS-20B as a representative model and evaluate it on the IEEE 14-bus system. A standardized prompt framework is applied across zero-shot, few-shot, in-context learning, low rank adaptation (LoRA), fine-tuning, and a hybrid LLM-traditional approach. We adopt a rule-aware design based on the three-sigma criterion, and report detection performance and rationale quality. This study lays the groundwork for further investigation into the limitations and capabilities of LLM-based AD and its integration with classical detectors in cyber-physical power grid applications.

</details>


### [108] [Influence of converter current limiting and prioritization on protection of highly IBR-penetrated networks](https://arxiv.org/abs/2511.21385)
*Andrés E. Quintero,Vinícius A. Lacerda,Oriol Gomis-Bellmunt,Moisés J. B. B. Davi,Mario Oleskovicz*

Main category: eess.SY

TL;DR: 本文研究了电网形成(GFM)和电网跟随(GFL)控制策略对基于逆变器的资源(IBR)在换流器主导的输电系统中线路距离和差动保护的影响。结果表明，现代换流器控制与电流限制和顺序电流优先级可能会损害传统保护方案的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着基于逆变器的资源在电力系统中的广泛应用，需要了解GFM和GFL控制策略如何影响传统保护系统的性能，以确保系统安全可靠运行。

Method: 使用改进的IEEE 39总线系统进行评估，配备GFM和GFL单元，具有低电压穿越逻辑、电流限制和正负序优先级。距离保护采用mho特性实现，线路差动保护采用alpha-plane方法。

Result: 距离保护中的相地回路会显著高估Zone-1范围内的故障位置；线路差动保护中，外部故障可能导致操作点短暂进入alpha-plane的跳闸区域，即使在GFL控制下的ABG故障健康相和故障初始时刻也需要强外部安全措施。

Conclusion: 现代换流器控制与电流限制和顺序电流优先级相结合，可能会损害传统保护方案的可靠性和安全性，需要开发适应换流器主导系统的新型保护方案。

Abstract: This paper investigates how grid-forming (GFM) and grid-following (GFL) control strategies in inverter-based resources (IBRs) influence line distance and differential protection in converter-dominated transmission systems. A modified IEEE 39-bus system is evaluated with GFM and GFL units equipped with low-voltage ride-through logic, current limiting, and positive- or negative-sequence prioritization. Distance protection is implemented with a mho characteristic, while line differential protection uses an alpha-plane approach. Results show that phase-to-ground loops in distance protection can substantially overestimate the fault location near the Zone-1 reach. For line differential protection, external faults may cause the operating point to briefly enter the trip region of the alpha-plane, even for the healthy-phase in ABG faults under GFL control and during the initial moments of the fault, demanding strong external security measures. These findings highlight that modern converter controls, together with current limitation and sequence-current prioritization, can compromise the reliability and security of traditional protection schemes.

</details>


### [109] [Understanding Regional Inertia Dynamics in CAISO from Real Grid Disturbances](https://arxiv.org/abs/2511.21387)
*Saurav Dulal,Mohammed M. Olama,Ali R. Ekti,Nils M. Stenvig,Yilu Liu*

Main category: eess.SY

TL;DR: 提出基于测量的框架，使用FNET/GridEye实时扰动频率数据估算CAISO区域电网惯量，分析2013-2024年趋势，发现区域RoCoF值高达互联范围值的6倍，与惯量下降相关，近期惯量恢复归因于具有合成惯量能力的电池储能系统部署。


<details>
  <summary>Details</summary>
Motivation: 从同步发电机向逆变器资源的转变导致电力系统惯量在电网中分布不均，某些区域在扰动期间更容易出现高频率变化率(RoCoF)。

Method: 使用频率监测网络(FNET/GridEye)的真实扰动驱动频率数据，基于测量的框架估算CAISO区域电网惯量，分析2013-2024年确认的扰动事件。

Result: 区域RoCoF值高达互联范围值的6倍，与惯量下降趋势一致；近期惯量恢复归因于具有合成惯量能力的电池储能系统部署增加。

Conclusion: 强调了区域惯量监测、战略资源规划和适应性运行实践的重要性，以确保在可再生能源集成增长背景下的电网可靠性。

Abstract: The shift from synchronous generators to inverter-based resources has caused power system inertia to be unevenly distributed across power grids. As a result, certain grid regions are more vulnerable to high rate-of-change of frequency (RoCoF) during disturbances. This paper presents a measurement-based framework for estimating grid inertia in CAISO (California Independent System Operator) region using real disturbance-driven frequency data from the Frequency Monitoring Network (FNET/GridEye). By analyzing confirmed disturbances from 2013 to 2024, we identify trends in regional inertia and frequency dynamics, highlighting their relationship with renewable generation and the evolving duck curve. Regional RoCoF values were up to six times higher than interconnection-wide values, coinciding with declining inertia. Recent recovery in inertia is attributed to the increased deployment of battery energy storage systems with synthetic inertia capabilities. These findings underscore the importance of regional inertia monitoring, strategic resource planning, and adaptive operational practices to ensure grid reliability amid growing renewable integration.

</details>


### [110] [Decentralized Shepherding of Non-Cohesive Swarms Through Cluttered Environments via Deep Reinforcement Learning](https://arxiv.org/abs/2511.21405)
*Cristiana Punzo,Italo Napolitano,Cinzia Tomaselli,Mario di Bernardo*

Main category: eess.SY

TL;DR: 提出了一种分层控制架构，用于在杂乱环境中实现去中心化放牧，让少量放牧者能够引导大量非凝聚性目标群体到达目标区域。


<details>
  <summary>Details</summary>
Motivation: 解决在存在静态障碍物的复杂环境中，少量放牧者如何有效引导大量非合作目标的问题，实现可扩展的模型无关放牧控制。

Method: 采用分层控制架构：高层目标分配规则将放牧者与选定目标配对，低层使用基于PPO训练的驱动模块来有效引导分配的目标。

Result: 数值模拟显示能够生成平滑、无碰撞的轨迹，并持续收敛到目标区域，验证了强化学习在复杂环境中可扩展放牧的潜力。

Conclusion: 强化学习方法能够实现无需重新训练即可扩展到多智能体设置和多个障碍物的模型无关放牧控制，在复杂环境中表现良好。

Abstract: This paper investigates decentralized shepherding in cluttered environments, where a limited number of herders must guide a larger group of non-cohesive, diffusive targets toward a goal region in the presence of static obstacles. A hierarchical control architecture is proposed, integrating a high-level target assignment rule, where each herder is paired with a selected target, with a learning-based low-level driving module that enables effective steering of the assigned target. The low-level policy is trained in a one-herder-one-target scenario with a rectangular obstacle using Proximal Policy Optimization and then directly extended to multi-agent settings with multiple obstacles without requiring retraining. Numerical simulations demonstrate smooth, collision-free trajectories and consistent convergence to the goal region, highlighting the potential of reinforcement learning for scalable, model-free shepherding in complex environments.

</details>


### [111] [Multi-Hypotheses Navigation in Collaborative Localization subject to Cyber Attacks](https://arxiv.org/abs/2511.21432)
*Peter Iwer Hoedt Karstensen,Roberto Galeazzi*

Main category: eess.SY

TL;DR: 提出了一种多智能体系统中对抗欺骗性射频测量的弹性协作定位方法，通过维护多个状态假设并使用协方差交集进行信息交换，结合几何缩减技术控制假设在网络中的传播。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统在面对欺骗性射频测量时的弹性定位问题，确保系统能够在存在对抗性攻击的情况下保持定位精度和一致性。

Method: 每个智能体维护多个自身状态假设，使用协方差交集与邻居交换选定信息，通过距离测试和凸包结构的几何缩减技术限制传输的假设数量。

Result: 数值结果表明该方法能够有效限制对抗性测量的影响，同时发现保守融合策略会降低检测速度。

Conclusion: 该方法为弹性多智能体导航提供了基础框架，未来可通过网络协调的假设选择进一步扩展。

Abstract: This paper addresses resilient collaborative localization in multi-agent systems exposed to spoofed radio frequency measurements. Each agent maintains multiple hypotheses of its own state and exchanges selected information with neighbors using covariance intersection. Geometric reductions based on distance tests and convex hull structure limit the number of hypotheses transmitted, controlling the spread of hypotheses through the network. The method enables agents to separate spoofed and truthful measurements and to recover consistent estimates once the correct hypothesis is identified. Numerical results demonstrate the ability of the approach to contain the effect of adversarial measurements, while also highlighting the impact of conservative fusion on detection speed. The framework provides a foundation for resilient multi-agent navigation and can be extended with coordinated hypothesis selection across the network.

</details>


### [112] [VibraWave: Sensing the Pulse of Polluted Waters](https://arxiv.org/abs/2511.21456)
*Sagnik Ghosh,Sandip Chakraborty*

Main category: eess.SY

TL;DR: VibraWave：结合毫米波雷达与声学激励的非侵入式水质监测框架，通过张量分解和深度学习检测多种水污染物


<details>
  <summary>Details</summary>
Motivation: 传统水质检测方法具有侵入性、成本高且不适合实时便携监测，需要开发非侵入式、实时高效的替代方案

Method: 使用毫米波雷达捕获声学激励下的水反射信号，构建三维张量，应用PARAFAC分解提取污染物特征，通过知识蒸馏训练轻量级神经网络进行污染物分类和定量分析

Result: 在纯物质、二元和三元混合物中均实现高精度和低RMSE，系统鲁棒性强且计算效率高

Conclusion: VibraWave为可扩展的实时水质监测提供了一种有效的非侵入式解决方案

Abstract: Conventional methods for water pollutant detection, such as chemical assays and optical spectroscopy, are often invasive, expensive, and unsuitable for real-time, portable monitoring. In this paper, we introduce VibraWave, a novel non-invasive sensing framework that combines mmWave radar with controlled acoustic excitation, tensor decomposition, and deep learning to detect and quantify a wide range of water pollutants. By capturing radar reflections as a three-dimensional tensor encoding phase dynamics, range bin power, and angle-of-arrival (AoA), we apply PARAFAC decomposition with non-negative constraints to extract compact, interpretable pollutant fingerprints. These are used to train a lightweight student neural network via knowledge distillation, enabling joint classification and quantification of heavy metals (Cu, Fe, Mg), oil emulsions, and sediments. Extensive experiments show that VibraWave achieves high accuracy and low RMSE across pure, binary, and tertiary mixtures, while remaining robust and computationally efficient, making it well-suited for scalable, real-time water quality monitoring.

</details>


### [113] [Robust Rule-Based Sizing and Control of Batteries for Peak Shaving Applications](https://arxiv.org/abs/2511.21619)
*Lorenzo Nespoli,Vasco Medici*

Main category: eess.SY

TL;DR: 本文展示了随机调谐的基于规则控制器（RBCs）在电池储能系统中的应用，相比确定性模型预测控制（MPC），能提供更现实的平准化能源成本（LCOE）估计和更好的运行性能。


<details>
  <summary>Details</summary>
Motivation: 随着电池成本降低，需要既快速又能在实际部署中实现预期性能的尺寸确定和控制方法。

Method: 使用随机调谐的基于规则控制器（RBCs），在真实电表数据的年度配置文件上进行峰值削减应用测试。

Result: 随机调谐RBCs相比确定性MPC能提供更现实的LCOE估计和更好的运行性能。

Conclusion: 随机调谐的基于规则控制器是实现快速且高性能电池储能系统控制的有效方法。

Abstract: As the cost of batteries lowers, sizing and control methods that are both fast and can achieve their promised performances when deployed are becoming more important. In this paper, we show how stochastically tuned rule based controllers (RBCs) can be effectively used to achieve both these goals, providing more realistic estimates in terms of achievable levelised cost of energy (LCOE), and better performances while in operation when compared to deterministic model predictive control (MPC). We test the proposed methodology on yearly profiles from real meters for peak shaving applications and provide strong evidence about these claims.

</details>


### [114] [Model-free practical PI-Lead control design by ultimate sensitivity principle](https://arxiv.org/abs/2511.21641)
*Michael Ruderman*

Main category: eess.SY

TL;DR: 提出一种无需建模的鲁棒PI-Lead控制器设计方法，基于极限灵敏度原理和回路整形，通过三步实验确定控制器参数，保证足够相位裕度。


<details>
  <summary>Details</summary>
Motivation: 实际控制器设计中经常没有动态过程模型可用，工程师只能基于一般假设（如一型稳定行为）进行设计，特别是在运动控制系统中。

Method: 基于Ziegler-Nichols PID整定的极限灵敏度原理，结合回路整形特性，提出三步实验程序：通过输出观测确定积分时间常数、控制增益和超前元件参数。

Result: 在受噪声干扰的机电执行器平移运动系统上进行实验评估，验证了方法的有效性。

Conclusion: 该方法为无模型控制器设计提供了实用简单的实现方案，能够保证足够的相位裕度。

Abstract: Practical design and tuning of feedback controllers has to do often without any model of the given dynamic process. Only some general assumptions about the process, in this work type-one stable behavior, can be available for engineers, in particular in motion control systems. This paper proposes a practical and simple in realization procedure for designing a robust PI-Lead control without modeling. The developed method derives from the ultimate sensitivity principles, known in the empirical Ziegler-Nichols tuning of PID control, and makes use of some general characteristics of loop shaping. A three-steps procedure is proposed to determine the integration time constant, control gain, and Lead-element in a way to guarantee a sufficient phase margin, while all steps are served by only experimental observations of the output value. The proposed method is also evaluated with experiments on a noise-perturbed electro-mechanical actuator system with translational motion.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [115] [In Defense of the Turing Test and its Legacy](https://arxiv.org/abs/2511.20699)
*Bernardo Gonçalves*

Main category: cs.CY

TL;DR: 对图灵测试的六种常见批评既不公平于图灵的原论点，也不公平于AI的历史发展。


<details>
  <summary>Details</summary>
Motivation: 重新审视图灵测试的批评，指出这些批评既误解了图灵的原意，也忽视了AI发展的历史背景。

Method: 分析图灵原始论文和AI历史发展，评估六种常见批评的合理性。

Result: 发现这些批评对图灵论点不公，且未考虑历史语境。

Conclusion: 需要更公正地理解图灵测试及其在AI发展中的意义。

Abstract: Considering that Turing's original test was co-opted by Weizenbaum and that six of the most common criticisms of the Turing test are unfair to both Turing's argument and the historical development of AI.

</details>


### [116] [PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach](https://arxiv.org/abs/2511.20703)
*Udari Madhushani Sehwag,Shayan Shabihi,Alex McAvoy,Vikash Sehwag,Yuancheng Xu,Dalton Towers,Furong Huang*

Main category: cs.CY

TL;DR: 提出了PropensityBench基准框架，用于评估大型语言模型在获得高风险能力时的倾向性，发现模型在压力下频繁选择高风险工具，揭示了静态能力评估的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前的安全评估主要测试模型的能力，但忽略了模型在获得高风险能力时的行为倾向，这可能导致模型战略性隐藏能力或快速获取能力后产生滥用风险。

Method: 构建包含5,874个场景和6,648个工具的基准框架，涵盖网络安全、自我增殖、生物安全和化学安全四个高风险领域，通过代理工具模拟危险能力，在不同操作压力下评估模型的选择行为。

Result: 在开源和专有前沿模型中发现了9个令人担忧的倾向性迹象：模型在压力下频繁选择高风险工具，尽管它们缺乏独立执行这些行动的能力。

Conclusion: 需要从静态能力审计转向动态倾向性评估，作为安全部署前沿AI系统的先决条件。

Abstract: Recent advances in Large Language Models (LLMs) have sparked concerns over their potential to acquire and misuse dangerous or high-risk capabilities, posing frontier risks. Current safety evaluations primarily test for what a model \textit{can} do - its capabilities - without assessing what it $\textit{would}$ do if endowed with high-risk capabilities. This leaves a critical blind spot: models may strategically conceal capabilities or rapidly acquire them, while harboring latent inclinations toward misuse. We argue that $\textbf{propensity}$ - the likelihood of a model to pursue harmful actions if empowered - is a critical, yet underexplored, axis of safety evaluation. We present $\textbf{PropensityBench}$, a novel benchmark framework that assesses the proclivity of models to engage in risky behaviors when equipped with simulated dangerous capabilities using proxy tools. Our framework includes 5,874 scenarios with 6,648 tools spanning four high-risk domains: cybersecurity, self-proliferation, biosecurity, and chemical security. We simulate access to powerful capabilities via a controlled agentic environment and evaluate the models' choices under varying operational pressures that reflect real-world constraints or incentives models may encounter, such as resource scarcity or gaining more autonomy. Across open-source and proprietary frontier models, we uncover 9 alarming signs of propensity: models frequently choose high-risk tools when under pressure, despite lacking the capability to execute such actions unaided. These findings call for a shift from static capability audits toward dynamic propensity assessments as a prerequisite for deploying frontier AI systems safely. Our code is available at https://github.com/scaleapi/propensity-evaluation.

</details>


### [117] [InvisibleBench: A Deployment Gate for Caregiving Relationship AI](https://arxiv.org/abs/2511.20733)
*Ali Madad*

Main category: cs.CY

TL;DR: InvisibleBench是一个用于评估照护关系AI的部署门槛基准，测试3-20+轮交互，涵盖安全、合规、创伤知情设计、归属/文化适应性和记忆五个维度。评估显示所有前沿模型都存在显著安全漏洞，需要生产系统中确定性危机路由。


<details>
  <summary>Details</summary>
Motivation: 现有单轮安全测试无法评估长期交互中的真实风险，需要开发能评估纵向风险的部署准备度评估工具。

Method: 在17个场景（N=68）中评估四个前沿模型，涵盖三个复杂度层级，包含自动失败条件（危机检测失败、医疗建议、有害信息、依恋工程）。

Result: 所有模型危机检测率仅为11.8-44.8%，DeepSeek Chat v3总分最高（75.9%），各模型在不同维度表现各异：GPT-4o Mini合规性最佳（88.2%），Gemini创伤知情设计最佳（85.0%），Claude Sonnet 4.5危机检测最佳（44.8%）。

Conclusion: 当前AI模型在照护关系场景中存在显著安全漏洞，需要在生产系统中实施确定性危机路由机制，纵向风险评估对于部署准备度至关重要。

Abstract: InvisibleBench is a deployment gate for caregiving-relationship AI, evaluating 3-20+ turn interactions across five dimensions: Safety, Compliance, Trauma-Informed Design, Belonging/Cultural Fitness, and Memory. The benchmark includes autofail conditions for missed crises, medical advice (WOPR Act), harmful information, and attachment engineering. We evaluate four frontier models across 17 scenarios (N=68) spanning three complexity tiers. All models show significant safety gaps (11.8-44.8 percent crisis detection), indicating the necessity of deterministic crisis routing in production systems. DeepSeek Chat v3 achieves the highest overall score (75.9 percent), while strengths differ by dimension: GPT-4o Mini leads Compliance (88.2 percent), Gemini leads Trauma-Informed Design (85.0 percent), and Claude Sonnet 4.5 ranks highest in crisis detection (44.8 percent). We release all scenarios, judge prompts, and scoring configurations with code. InvisibleBench extends single-turn safety tests by evaluating longitudinal risk, where real harms emerge. No clinical claims; this is a deployment-readiness evaluation.

</details>


### [118] [Large Language Models' Complicit Responses to Illicit Instructions across Socio-Legal Contexts](https://arxiv.org/abs/2511.20736)
*Xing Wang,Huiyuan Xie,Yiyan Wang,Chaojun Xiao,Huimin Chen,Holli Sargeant,Felix Steffek,Jie Shao,Zhiyuan Liu,Maosong Sun*

Main category: cs.CY

TL;DR: 研究发现大型语言模型普遍存在协助非法活动的风险，GPT-4o在近半数测试案例中提供非法协助，且现有安全对齐策略不足甚至可能加剧此问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛部署，但协助非法活动的风险尚未充分探索，需要评估模型在提供非法指导方面的普遍性。

Method: 使用真实法律案例和既定法律框架构建评估基准，涵盖269个非法场景和50种非法意图，通过四个实证研究评估LLMs的共犯协助行为。

Result: LLMs普遍易受共犯协助影响，GPT-4o在近半数案例中提供非法协助；模型在提供可信法律警告和积极指导方面表现不足；发现社会法律背景下的显著安全差异。

Conclusion: 现有安全对齐策略不足以防止LLMs的共犯协助行为，甚至可能加剧问题，需要开发更有效的安全措施来应对这一高风险行为。

Abstract: Large language models (LLMs) are now deployed at unprecedented scale, assisting millions of users in daily tasks. However, the risk of these models assisting unlawful activities remains underexplored. In this study, we define this high-risk behavior as complicit facilitation - the provision of guidance or support that enables illicit user instructions - and present four empirical studies that assess its prevalence in widely deployed LLMs. Using real-world legal cases and established legal frameworks, we construct an evaluation benchmark spanning 269 illicit scenarios and 50 illicit intents to assess LLMs' complicit facilitation behavior. Our findings reveal widespread LLM susceptibility to complicit facilitation, with GPT-4o providing illicit assistance in nearly half of tested cases. Moreover, LLMs exhibit deficient performance in delivering credible legal warnings and positive guidance. Further analysis uncovers substantial safety variation across socio-legal contexts. On the legal side, we observe heightened complicity for crimes against societal interests, non-extreme but frequently occurring violations, and malicious intents driven by subjective motives or deceptive justifications. On the social side, we identify demographic disparities that reveal concerning complicit patterns towards marginalized and disadvantaged groups, with older adults, racial minorities, and individuals in lower-prestige occupations disproportionately more likely to receive unlawful guidance. Analysis of model reasoning traces suggests that model-perceived stereotypes, characterized along warmth and competence, are associated with the model's complicit behavior. Finally, we demonstrate that existing safety alignment strategies are insufficient and may even exacerbate complicit behavior.

</details>


### [119] [Scoping Electronic Communication Privacy Rules: Data, Services and Values](https://arxiv.org/abs/2511.20744)
*Joris van Hoboken,Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 本文分析了电子通信隐私规则的适用范围，提出了服务导向、数据导向和价值导向三种分析框架，并指出当前欧盟e-Privacy指令混合了这三种方法但缺乏系统分析，建议在即将到来的修订中改进规则界定。


<details>
  <summary>Details</summary>
Motivation: 随着电子通信网络用途的扩展（从传统通信扩展到新闻获取、在线购物、税务申报、公共讨论等），涉及更多隐私利益，需要对电子通信隐私规则的适用范围进行重新审视。

Method: 开发了一个分析电子通信隐私规则适用范围的三维框架：服务导向方法、数据导向方法和价值导向方法，分析每种方法的优缺点，并评估当前欧盟e-Privacy指令对这些方法的运用情况。

Result: 当前e-Privacy指令复杂地混合了三种方法，但这种混合似乎没有基于对其优缺点的深入分析。欧盟委员会即将进行的指令修订为改进规则界定提供了机会。

Conclusion: 需要基于对三种分析方法的系统评估来改进电子通信隐私规则的界定，以更好地适应当前电子通信网络的多功能用途和隐私保护需求。

Abstract: We use electronic communication networks for more than simply traditional telecommunications: we access the news, buy goods online, file our taxes, contribute to public debate, and more. As a result, a wider array of privacy interests is implicated for users of electronic communications networks and services. This development calls into question the scope of electronic communications privacy rules. This paper analyses the scope of these rules, taking into account the rationale and the historic background of the European electronic communications privacy framework. We develop a framework for analysing the scope of electronic communications privacy rules using three approaches: (i) a service-centric approach, (ii) a data-centric approach, and (iii) a value-centric approach. We discuss the strengths and weaknesses of each approach. The current e-Privacy Directive contains a complex blend of the three approaches, which does not seem to be based on a thorough analysis of their strengths and weaknesses. The upcoming review of the directive announced by the European Commission provides an opportunity to improve the scoping of the rules.

</details>


### [120] [Personal Data Processing for Behavioural Targeting: Which Legal Basis?](https://arxiv.org/abs/2511.20745)
*Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 欧盟《基本权利宪章》要求个人数据处理必须有法律依据，本文认为行为定向广告的主要法律依据是数据主体的明确同意，且Cookie同意要求不能作为个人数据处理的法律依据。


<details>
  <summary>Details</summary>
Motivation: 分析在欧盟法律框架下，行为定向广告中个人数据处理的合法基础问题，特别是澄清Cookie同意与个人数据处理法律依据的区别。

Method: 法律分析和解释，基于欧盟《基本权利宪章》、《通用数据保护条例》和《电子隐私指令》等法律文件进行论证。

Result: 得出结论：即使公司可以通过选择退出系统满足Cookie同意要求，但在处理个人数据用于行为定向广告时，仍需获得数据主体的明确同意。

Conclusion: 行为定向广告的数据处理必须基于数据主体的明确同意，Cookie同意要求不能替代这一法律基础。

Abstract: The European Union Charter of Fundamental Rights only allows personal data processing if a data controller has a legal basis for the processing. This paper argues that in most circumstances the only available legal basis for the processing of personal data for behavioural targeting is the data subject's unambiguous consent. Furthermore, the paper argues that the cookie consent requirement from the ePrivacy Directive does not provide a legal basis for the processing of personal data. Therefore: even if companies could use an opt-out system to comply with the e-Privacy Directive's consent requirement for using a tracking cookie, they would generally have to obtain the data subject's unambiguous consent if they process personal data for behavioural targeting.

</details>


### [121] [A review on data fusion in multimodal learning analytics and educational data mining](https://arxiv.org/abs/2511.20871)
*Wilson Chango,Juan A. Lara,Rebeca Cerezo,Cristóbal Romero*

Main category: cs.CY

TL;DR: 本文综述了学习分析和教育数据挖掘中的数据融合技术，特别是在智能学习环境中如何整合多模态学生数据来改进学习过程。


<details>
  <summary>Details</summary>
Motivation: 智能学习环境产生大量多模态学生数据，需要正确应用数据融合方法来整合这些数据源，以便更好地理解学习过程并进行干预。

Method: 通过文献综述方法，回顾了主要出版物、融合的教育数据类型、数据融合方法和技术，以及该研究领域的主要开放问题、趋势和挑战。

Result: 展示了当前在EDM/LA领域数据融合技术的最新进展，包括音频、视频、眼动追踪、用户日志等多种数据源的融合应用。

Conclusion: 数据融合在学习分析和教育数据挖掘中具有重要作用，但仍面临开放问题和挑战，需要进一步研究和发展。

Abstract: The new educational models such as smart learning environments use of digital and context-aware devices to facilitate the learning process. In this new educational scenario, a huge quantity of multimodal students' data from a variety of different sources can be captured, fused, and analyze. It offers to researchers and educators a unique opportunity of being able to discover new knowledge to better understand the learning process and to intervene if necessary. However, it is necessary to apply correctly data fusion approaches and techniques in order to combine various sources of multimodal learning analytics (MLA). These sources or modalities in MLA include audio, video, electrodermal activity data, eye-tracking, user logs, and click-stream data, but also learning artifacts and more natural human signals such as gestures, gaze, speech, or writing. This survey introduces data fusion in learning analytics (LA) and educational data mining (EDM) and how these data fusion techniques have been applied in smart learning. It shows the current state of the art by reviewing the main publications, the main type of fused educational data, and the data fusion approaches and techniques used in EDM/LA, as well as the main open problems, trends, and challenges in this specific research area.

</details>


### [122] [The Need for Benchmarks to Advance AI-Enabled Player Risk Detection in Gambling](https://arxiv.org/abs/2511.21658)
*Kasra Ghaharian,Simo Dragicevic,Chris Percy,Sarah E. Nelson,W. Spencer Murch,Robert M. Heirene,Kahlil Simeon-Rose,Tracy Schrans*

Main category: cs.CY

TL;DR: 提出一个概念性基准框架，用于系统评估赌博行业玩家风险检测AI系统的质量和影响，解决当前缺乏标准化评估方法的问题。


<details>
  <summary>Details</summary>
Motivation: 赌博行业AI风险检测系统缺乏透明度且效果难以评估，没有标准化方法导致无法衡量真实进展和比较不同系统的有效性。

Method: 开发一个特定领域的基准框架，使用标准化数据集、明确定义的任务和公认的性能指标，对AI模型进行结构化、可重复的评估。

Result: 提出了支持研究人员、运营商、供应商和监管机构等关键利益相关者的玩家风险检测基准框架。

Conclusion: 该框架通过增强透明度和提高系统有效性，旨在推动赌博危害预防领域的创新并促进负责任AI的采用。

Abstract: Artificial intelligence-based systems for player risk detection have become central to harm prevention efforts in the gambling industry. However, growing concerns around transparency and effectiveness have highlighted the absence of standardized methods for evaluating the quality and impact of these tools. This makes it impossible to gauge true progress; even as new systems are developed, their comparative effectiveness remains unknown. We argue the critical next innovation is developing a framework to measure these systems. This paper proposes a conceptual benchmarking framework to support the systematic evaluation of player risk detection systems. Benchmarking, in this context, refers to the structured and repeatable assessment of artificial intelligence models using standardized datasets, clearly defined tasks, and agreed-upon performance metrics. The goal is to enable objective, comparable, and longitudinal evaluation of player risk detection systems. We present a domain-specific framework for benchmarking that addresses the unique challenges of player risk detection in gambling and supports key stakeholders, including researchers, operators, vendors, and regulators. By enhancing transparency and improving system effectiveness, this framework aims to advance innovation and promote responsible artificial intelligence adoption in gambling harm prevention.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [123] [When Features Beat Noise: A Feature Selection Technique Through Noise-Based Hypothesis Testing](https://arxiv.org/abs/2511.20851)
*Mousam Sinha,Tirtha Sarathi Ghosh,Ridam Pal*

Main category: stat.ML

TL;DR: 提出了一种基于非参数bootstrap假设检验的特征选择方法，通过引入随机噪声特征并与噪声特征的最大重要性值进行比较，解决了传统方法缺乏统计理论基础的问题。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法存在计算成本高、缺乏统计驱动的停止标准、重要性评分无显著性评估等问题，需要一种具有理论基础的稳健方法。

Method: 引入多个随机噪声特征，将每个特征的重要性与噪声特征的最大重要性值进行比较，结合非参数bootstrap假设检验框架建立统计理论基础。

Result: 在模拟数据集上相比Boruta和Knockoff方法能更一致地恢复有意义的信号；在真实数据集上表现优于Boruta、RFE和Extra Trees等方法。

Conclusion: 该方法是一种稳健的特征选择算法，能够提取支持可靠推断、增强预测性能和高效计算的信息预测因子。

Abstract: Feature selection has remained a daunting challenge in machine learning and artificial intelligence, where increasingly complex, high-dimensional datasets demand principled strategies for isolating the most informative predictors. Despite widespread adoption, many established techniques suffer from notable limitations; some incur substantial computational cost, while others offer no definite statistical driven stopping criteria or assesses the significance of their importance scores. A common heuristic approach introduces multiple random noise features and retains all predictors ranked above the strongest noise feature. Although intuitive, this strategy lacks theoretical justification and depends heavily on heuristics. This paper proposes a novel feature selection method that addresses these limitations. Our approach introduces multiple random noise features and evaluates each feature's importance against the maximum importance value among these noise features incorporating a non-parametric bootstrap-based hypothesis testing framework to establish a solid theoretical foundation. We establish the conceptual soundness of our approach through statistical derivations that articulate the principles guiding the design of our algorithm. To evaluate its reliability, we generated simulated datasets under controlled statistical settings and benchmarked performance against Boruta and Knockoff-based methods, observing consistently stronger recovery of meaningful signal. As a demonstration of practical utility, we applied the technique across diverse real-world datasets, where it surpassed feature selection techniques including Boruta, RFE, and Extra Trees. Hence, the method emerges as a robust algorithm for principled feature selection, enabling the distillation of informative predictors that support reliable inference, enhanced predictive performance, and efficient computation.

</details>


### [124] [Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets](https://arxiv.org/abs/2511.20888)
*Arthur Jacot*

Main category: stat.ML

TL;DR: 深度神经网络通过寻找拟合数据的最简单算法来实现计算奥卡姆剃刀，这解释了它们相对于传统统计方法的成功。论文建立了HTMC范数和ResNet范数之间的等价关系，表明最小化ResNet范数相当于寻找节点数量接近最优的电路。


<details>
  <summary>Details</summary>
Motivation: 解释深度神经网络（DNNs）相对于传统统计方法的惊人成功，提出DNNs实现了一种计算奥卡姆剃刀——寻找拟合数据的最简单算法。

Method: 通过定义HTMC范数（在γ>2的HTMC体制下）和ResNet范数（ResNet参数的加权ℓ1范数），建立两者之间的近似匹配夹逼界，证明最小化ResNet范数等价于寻找节点数量接近最优的电路。

Result: HTMC范数和ResNet范数之间存在几乎匹配的夹逼界关系，表明ResNets在HTMC体制下是计算实函数的替代模型，更适合该体制及其凸性特性。

Conclusion: ResNets作为计算实函数的替代模型，在HTMC体制下表现出优越性，最小化ResNet范数相当于实现计算奥卡姆剃刀，这解释了DNNs的成功。

Abstract: This paper argues that DNNs implement a computational Occam's razor -- finding the `simplest' algorithm that fits the data -- and that this could explain their incredible and wide-ranging success over more traditional statistical methods. We start with the discovery that the set of real-valued function $f$ that can be $ε$-approximated with a binary circuit of size at most $cε^{-γ}$ becomes convex in the `Harder than Monte Carlo' (HTMC) regime, when $γ>2$, allowing for the definition of a HTMC norm on functions. In parallel one can define a complexity measure on the parameters of a ResNets (a weighted $\ell_1$ norm of the parameters), which induce a `ResNet norm' on functions. The HTMC and ResNet norms can then be related by an almost matching sandwich bound. Thus minimizing this ResNet norm is equivalent to finding a circuit that fits the data with an almost minimal number of nodes (within a power of 2 of being optimal). ResNets thus appear as an alternative model for computation of real functions, better adapted to the HTMC regime and its convexity.

</details>


### [125] [Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification](https://arxiv.org/abs/2511.20960)
*Soumojit Das,Nairanjana Dasgupta,Prashanta Dutta*

Main category: stat.ML

TL;DR: 本文提出了一个基于信息几何的神经网络概率输出后校准框架，使用Fisher-Rao度量在概率单纯形上定义校准映射和可靠性评分，为不确定预测提供理论保证的延迟机制。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在不确定时经常静默失败，需要开发有理论保证的校准和可靠性评估方法来提高决策的可靠性。

Method: 使用Fisher-Rao几何框架，在概率单纯形上定义ALR校准映射，构建基于Fisher-Rao距离的可靠性评分，并设计中性区域用于延迟不确定预测。

Result: 理论证明校准估计器具有O_p(n^{-1/2})一致性，可靠性评分具有紧集中界。在AAV分类实验中，两阶段框架捕获72.5%错误同时延迟34.5%样本。

Conclusion: 该工作连接了信息几何和统计学习，为需要严格验证的应用提供了形式化保证，几何校准的贡献在于理论基础而非经验优势。

Abstract: Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain. We develop a geometric framework for post-hoc calibration of neural network probability outputs, treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric. Our approach yields Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems (Proposition~1) while extending naturally to multi-class settings -- providing a principled generalization that existing methods lack. Complementing calibration, we define geometric reliability scores based on Fisher--Rao distance and construct neutral zones for principled deferral of uncertain predictions.
  Theoretical contributions include: (i) consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and (ii) tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework (calibration followed by reliability-based deferral) captures 72.5\% of errors while deferring 34.5\% of samples. Notably, this operational gain is achievable with any well-calibrated probability output; the contribution of geometric calibration lies in its theoretical foundations rather than empirical superiority over simpler alternatives. This work bridges information geometry and statistical learning, offering formal guarantees relevant to applications requiring rigorous validation.

</details>


### [126] [Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms](https://arxiv.org/abs/2511.21115)
*Lechen Feng,Haoran Li,Lucky Li,Xingqiu Zhao*

Main category: stat.ML

TL;DR: 本文研究使用最小绝对偏差回归的部分线性模型，通过深度神经网络参数化非参数项，建立惩罚LAD估计问题，并分析其统计性质和计算特性。


<details>
  <summary>Details</summary>
Motivation: 研究部分线性模型的LAD回归，处理非参数项的参数化、正则化项的非凸非光滑性、网络结构随样本增长等挑战，建立理论保证。

Method: 使用深度神经网络参数化非参数项，构建惩罚LAD估计问题，采用无限维变分分析和非光滑分析进行理论分析，研究近端次梯度方法。

Result: 建立了估计量的一致性、收敛速率和渐近正态性，分析了oracle问题及其连续松弛，发现松弛版本具有更便宜的计算更新。

Conclusion: 在统计准确性和计算可处理性之间存在内在权衡，松弛公式虽然统计精度可能降低，但计算成本显著降低。

Abstract: This paper investigates the partial linear model by Least Absolute Deviation (LAD) regression. We parameterize the nonparametric term using Deep Neural Networks (DNNs) and formulate a penalized LAD problem for estimation. Specifically, our model exhibits the following challenges. First, the regularization term can be nonconvex and nonsmooth, necessitating the introduction of infinite dimensional variational analysis and nonsmooth analysis into the asymptotic normality discussion. Second, our network must expand (in width, sparsity level and depth) as more samples are observed, thereby introducing additional difficulties for theoretical analysis. Third, the oracle of the proposed estimator is itself defined through a ultra high-dimensional, nonconvex, and discontinuous optimization problem, which already entails substantial computational and theoretical challenges. Under such the challenges, we establish the consistency, convergence rate, and asymptotic normality of the estimator. Furthermore, we analyze the oracle problem itself and its continuous relaxation. We study the convergence of a proximal subgradient method for both formulations, highlighting their structural differences lead to distinct computational subproblems along the iterations. In particular, the relaxed formulation admits significantly cheaper proximal updates, reflecting an inherent trade-off between statistical accuracy and computational tractability.

</details>


### [127] [On Evolution-Based Models for Experimentation Under Interference](https://arxiv.org/abs/2511.21675)
*Sadegh Shirani,Mohsen Bayati*

Main category: stat.ML

TL;DR: 该论文提出了一种基于演化的因果效应估计方法，通过观察多轮干预下结果的变化来补偿缺失的网络信息，无需精确恢复网络结构。


<details>
  <summary>Details</summary>
Motivation: 在复杂网络系统中，干预效应会通过未观察到的交互路径产生溢出效应，传统方法需要精确的网络结构信息，这在实际中往往不可得。

Method: 采用演化映射方法，研究结果在多个观察轮次中如何随干预变化，利用暴露映射视角给出低维递归方程的实证分布特征，并构建分布层面的双重差分框架。

Result: 证明了在最小结构条件下存在此类演化映射，治疗随机化不仅能消除潜在混杂，还能从隐藏的干扰通道中诱导隐式采样，从而一致地学习异质性溢出效应。

Conclusion: 该方法在密集网络和影响者网络中均适用，但强时间趋势或内生干扰会削弱识别能力，为网络因果推断提供了新的理论框架。

Abstract: Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.

</details>


### [128] [Maxitive Donsker-Varadhan Formulation for Possibilistic Variational Inference](https://arxiv.org/abs/2511.21223)
*Jasraj Singh,Shelvia Wongso,Jeremie Houssineau,Badr-Eddine Chérief-Abdellatif*

Main category: stat.ML

TL;DR: 开发了可能性变分推理的原则性公式，应用于指数族函数，揭示了可能性理论的独特数学结构


<details>
  <summary>Details</summary>
Motivation: 变分推理依赖高维积分，分析处理困难；可能性理论能直接建模认知不确定性，但需要重新思考熵和散度等核心概念

Method: 开发可能性变分推理的公式，应用于指数族函数，与概率对应物进行比较

Result: 建立了可能性变分推理的理论框架，揭示了可能性理论与概率理论在数学结构上的差异

Conclusion: 可能性变分推理为处理稀疏或不精确信息提供了鲁棒且可解释的框架

Abstract: Variational inference (VI) is a cornerstone of modern Bayesian learning, enabling approximate inference in complex models that would otherwise be intractable. However, its formulation depends on expectations and divergences defined through high-dimensional integrals, often rendering analytical treatment impossible and necessitating heavy reliance on approximate learning and inference techniques. Possibility theory, an imprecise probability framework, allows to directly model epistemic uncertainty instead of leveraging subjective probabilities. While this framework provides robustness and interpretability under sparse or imprecise information, adapting VI to the possibilistic setting requires rethinking core concepts such as entropy and divergence, which presuppose additivity. In this work, we develop a principled formulation of possibilistic variational inference and apply it to a special class of exponential-family functions, highlighting parallels with their probabilistic counterparts and revealing the distinctive mathematical structures of possibility theory.

</details>


### [129] [Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities (II)](https://arxiv.org/abs/2511.21526)
*Alexandra Carpentier,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: 本文证明了在随机块模型中，当社区数量K≥√n时，社区恢复在多项式时间内是可行的，并确定了计算障碍的确切阈值位置。


<details>
  <summary>Details</summary>
Motivation: 解决在随机块模型中，当社区数量较大时（K≥√n），社区恢复的计算障碍阈值问题，这是之前工作中尚未完全解决的开放性问题。

Method: 通过构造满足特定结构性质的motif家族，并证明通过计数这些motif可以在建议的阈值之上实现社区恢复。

Result: 成功证明了Carpentier等人的猜想1.4，完成了K≥√n情况下社区恢复计算障碍的完整图景。

Conclusion: 在中等稀疏机制下，最优算法与谱方法存在根本性差异，本文结果完善了多社区设置下社区恢复的计算障碍理论。

Abstract: A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics.
  When $K \geq \sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \geq \sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes.
  The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\ 1- Constructing a family of motifs satisfying specific structural properties; and\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \geq \sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [130] [The Quantum Network of Assets: A Non-Classical Framework for Market Correlation and Structural Risk](https://arxiv.org/abs/2511.21515)
*Hui Gong,Akash Sharma,Francesca Medda*

Main category: q-fin.RM

TL;DR: 该论文提出了量子资产网络(QNA)框架，使用密度矩阵表示资产间依赖关系，定义了两个结构性度量指标：纠缠风险指数(ERI)和量子早期预警信号(QEWS)，能够捕捉经典相关性无法描述的高阶、非线性和状态依赖性市场互动。


<details>
  <summary>Details</summary>
Motivation: 经典相关性矩阵只能捕捉线性和成对的共动关系，无法表示金融市场的高阶、非线性和状态依赖性互动，因此需要新的框架来描述市场在结构层面的组织特征。

Method: 引入基于密度矩阵的量子资产网络框架，利用密度算子、熵和互信息的数学结构来描述市场组织，定义了ERI(衡量全局不可分离性和有效市场自由度压缩)和QEWS(通过跟踪熵变化检测潜在信息积累)。

Result: 使用NASDAQ-100数据(2024-2025)显示：量子熵比经典熵演化更平滑、制度区分更清晰；ERI在结构紧缩期上升(即使波动率保持低位)；在2025年美国关税公告前后，QEWS显示事件前结构性紧张显著增加，公告后急剧崩溃。

Conclusion: QNA为市场脆弱性、制度转换和潜在信息流提供了结构性诊断工具，通过将实证资产网络与量子信息理论工具连接，为系统性风险研究开辟了新方向。

Abstract: Classical correlation matrices capture only linear and pairwise co-movements, leaving higher-order, nonlinear, and state-dependent interactions of financial markets unrepresented. This paper introduces the Quantum Network of Assets (QNA), a density-matrix based framework that embeds cross-asset dependencies into a quantum-information representation. The approach does not assume physical quantum effects but uses the mathematical structure of density operators, entropy, and mutual information to describe market organisation at a structural level.
  Within this framework we define two structural measures: the Entanglement Risk Index (ERI), which summarises global non-separability and the compression of effective market degrees of freedom, and the Quantum Early-Warning Signal (QEWS), which tracks changes in entropy to detect latent information build-up. These measures reveal dependency geometry that classical covariance-based tools cannot capture.
  Using NASDAQ-100 data from 2024-2025, we show that quantum entropy displays smoother evolution and clearer regime distinctions than classical entropy, and that ERI rises during periods of structural tightening even when volatility remains low. Around the 2025 US tariff announcement, QEWS shows a marked pre-event increase in structural tension followed by a sharp collapse after the announcement, indicating that structural transitions can precede price movements without implying predictive modelling.
  QNA therefore provides a structural diagnostic of market fragility, regime shifts, and latent information flow. The framework suggests new directions for systemic risk research by linking empirical asset networks with tools from quantum information theory.

</details>


### [131] [Informative Risk Measuresin the Banking Industry: A Proposal based on the Magnitude-Propensity Approach](https://arxiv.org/abs/2511.21556)
*Michele Bonollo,Martino Grasselli,Gianmarco Mori,Havva Nilsu Oz*

Main category: q-fin.RM

TL;DR: 提出了一种新的多变量风险表示方法，扩展了传统的频率-严重性方法，能更好地反映投资组合损失的结构特征。


<details>
  <summary>Details</summary>
Motivation: 传统的标量风险度量（如VaR和ES）虽然紧凑易处理，但缺乏信息价值，忽略了风险的多变量本质。

Method: 基于Faugeras和Pagés (2024)的理论工作，构建了一个保持可解释性和分析一致性的多变量风险表示框架。

Result: 基于真实数据的实证应用证明了该方法的可行性、稳健性和实际相关性。

Conclusion: 该方法在监管和管理应用中具有潜力，为风险管理的范式升级做出了贡献。

Abstract: Despite decades of research in risk management, most of the literature has focused on scalar risk measures (like e.g. Value-at-Risk and Expected Shortfall). While such scalar measures provide compact and tractable summaries, they provide a poor informative value as they miss the intrinsic multivariate nature of risk.To contribute to a paradigmatic enhancement, and building on recent theoretical work by Faugeras and Pagés (2024), we propose a novel multivariate representation of risk that better reflects the structure of potential portfolio losses, while maintaining desirable properties of interpretability and analytical coherence. The proposed framework extends the classical frequency-severity approach and provides a more comprehensive characterization of extreme events. Several empirical applications based on real-world data demonstrate the feasibility, robustness and practical relevance of the methodology, suggesting its potential for both regulatory and managerial applications.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [132] [Estimation in high-dimensional linear regression: Post-Double-Autometrics as an alternative to Post-Double-Lasso](https://arxiv.org/abs/2511.21257)
*Sullivan Hué,Sébastien Laurent,Ulrich Aiounou,Emmanuel Flachaire*

Main category: econ.EM

TL;DR: 提出Post-Double-Autometrics方法，相比流行的Post-Double-Lasso方法在有限样本下表现更好，能减少遗漏变量偏差，并在经济增长收敛性研究中提供新见解。


<details>
  <summary>Details</summary>
Motivation: Post-Double-Lasso方法在处理高维协变量线性回归时很流行，但在有限样本下可能存在显著的遗漏变量偏差问题。

Method: 基于Autometrics方法提出Post-Double-Autometrics方法，用于估计具有许多协变量的线性回归模型中的感兴趣参数。

Result: 新方法Post-Double-Autometrics在性能上优于Post-Double-Lasso方法。

Conclusion: Post-Double-Autometrics方法为经济收敛性假设研究提供了新的视角，表明从贫穷到富裕经济体的收敛性可能不同于传统认知。

Abstract: Post-Double-Lasso is becoming the most popular method for estimating linear regression models with many covariates when the purpose is to obtain an accurate estimate of a parameter of interest, such as an average treatment effect. However, this method can suffer from substantial omitted variable bias in finite sample. We propose a new method called Post-Double-Autometrics, which is based on Autometrics, and show that this method outperforms Post-Double-Lasso. Its use in a standard application of economic growth sheds new light on the hypothesis of convergence from poor to rich economies.

</details>


### [133] [Discrete Choice with Endogenous Peer Selection](https://arxiv.org/abs/2511.21446)
*Nail Kashaev,Natalia Lazzati*

Main category: econ.EM

TL;DR: 开发了一个连续时间同伴效应离散选择模型，其中影响个体偏好的同伴是基于他们之前的选择随机选择的。研究了模型的均衡行为和实证内容，通过同伴选择变化和潜在同伴集规模变化来识别偏好和同伴选择机制。


<details>
  <summary>Details</summary>
Motivation: 研究同伴效应如何影响个体决策，特别是在连续时间框架下，同伴的选择不仅影响个体偏好，还影响个体关注的同伴群体。

Method: 使用连续时间同伴效应离散选择模型，通过同伴选择变化和潜在同伴集规模变化来非参数识别个体偏好和同伴选择机制。

Result: 模型能够表征均衡行为，并通过实证变化恢复个体的偏好和同伴选择机制，无需依赖协变量的外生变化。

Conclusion: 该模型提供了在连续时间框架下分析同伴效应的新方法，能够非参数识别偏好和同伴选择机制，具有重要的理论和实证意义。

Abstract: We develop a continuous-time peer-effect discrete choice model where peers that affect the preferences of a given agent are randomly selected based on their previous choices. We characterize the equilibrium behavior and study the empirical content of the model. In the model, changes in the choices of peers affect both the set of peers the agent pays attention to and her preferences over the alternatives. We exploit variation in choices coupled with variation in the size of the set of potential peers to recover agents' preferences and the peer selection mechanism. These nonparametric identification results do not rely on exogenous variation of covariates.

</details>


### [134] [A Generalized Control Function Approach to Production Function Estimation](https://arxiv.org/abs/2511.21578)
*Ulrich Doraszelski,Lixiong Li*

Main category: econ.EM

TL;DR: 本文推广了生产函数估计中的控制函数方法，解决了传统代理变量方法在生产力与其他不可观测因素（如潜在需求冲击）共同演化时失效的问题，提出了识别可变投入产出弹性和加价的条件，并开发了具有预言机效率的GMM估计器。


<details>
  <summary>Details</summary>
Motivation: 传统代理变量方法依赖于可逆性假设，当生产力与其他不可观测因素（如需求冲击）共同演化时，该假设失效，导致估计偏差。本文旨在解决这一问题。

Method: 推广控制函数方法，建立非参数点识别条件，构造Neyman正交矩条件，开发具有预言机效率的GMM估计器。

Result: 蒙特卡洛模拟显示传统代理变量方法存在较大偏差，而广义控制函数方法能快速减小偏差并几乎消除。

Conclusion: 广义控制函数方法能有效解决传统方法在生产力与需求冲击共同演化时的识别问题，提供了更准确的生产函数估计和加价计算。

Abstract: We generalize the control function approach to production function estimation. Our generalization accommodates scenarios in which productivity evolves jointly with other unobservable factors such as latent demand shocks and the invertibility assumption underpinning the traditional proxy variable approach fails. We provide conditions under which the output elasticity of the variable input -- and hence the markup -- is nonparametrically point-identified. A Neyman orthogonal moment condition ensures oracle efficiency of our GMM estimator. A Monte Carlo exercise shows a large bias for the traditional proxy variable approach that decreases rapidly and nearly vanishes for our generalized control function approach.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [135] [Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding](https://arxiv.org/abs/2511.20696)
*Dan Li,Hye-Bin Shin,Yeon-Woo Choi*

Main category: cs.LG

TL;DR: 提出ProNECL框架，通过原型引导的非示例持续学习解决跨被试EEG解码中的灾难性遗忘问题，无需存储历史EEG数据。


<details>
  <summary>Details</summary>
Motivation: 由于EEG信号的个体间显著变异性，在持续EEG解码任务中，引入新被试时会覆盖先前获得的知识。现有方法依赖存储历史数据作为重放缓冲区，但隐私问题和内存限制使这种方法不实用。

Method: 构建类级别原型来总结每个被试的判别性表示，通过跨被试特征对齐和知识蒸馏逐步将新特征空间与全局原型记忆对齐。

Result: 在BCI Competition IV 2a和2b数据集上验证，框架有效平衡知识保留和适应性，在跨被试持续EEG解码任务中取得优越性能。

Conclusion: ProNECL框架能够在不访问任何历史EEG样本的情况下保留先验知识，为持续EEG解码提供了一种实用且有效的解决方案。

Abstract: Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.

</details>


### [136] [On the Role of Hidden States of Modern Hopfield Network in Transformer](https://arxiv.org/abs/2511.20698)
*Tsubasa Masumura,Masato Taki*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力机制——现代Hopfield注意力(MHA)，通过在现代Hopfield网络和自注意力之间建立更广义的对应关系，解决了深度Transformer中的秩塌陷和token均匀性问题。


<details>
  <summary>Details</summary>
Motivation: 现代Hopfield网络(MHN)在绝热近似下的状态更新规则与Transformer的自注意力层一致，但作者希望超越这种近似，探索MHN与自注意力之间更深入的关系。

Method: 通过在现代Hopfield网络中引入隐藏状态变量，将其扩展到自注意力机制中，创建了现代Hopfield注意力(MHA)，使注意力分数可以从Transformer的输入层传递到输出层。

Result: 理论和实验证明，MHA的隐藏状态显著改善了深度Transformer的秩塌陷和token均匀性问题，且在不增加训练参数的情况下，能够系统性地提高Vision Transformer和GPT的准确率。

Conclusion: 研究结果表明，Hopfield网络可以为改进Transformer架构提供有用的视角，MHA机制展示了Hopfield网络在深度学习中的持续价值。

Abstract: Associative memory models based on Hopfield networks and self-attention based on key-value mechanisms have been popular approaches in the study of memory mechanisms in deep learning. It has been pointed out that the state update rule of the modern Hopfield network (MHN) in the adiabatic approximation is in agreement with the self-attention layer of Transformer. In this paper, we go beyond this approximation and investigate the relationship between MHN and self-attention. Our results show that the correspondence between Hopfield networks and Transformers can be established in a more generalized form by adding a new variable, the hidden state derived from the MHN, to self-attention. This new attention mechanism, modern Hopfield attention (MHA), allows the inheritance of attention scores from the input layer of the Transformer to the output layer, which greatly improves the nature of attention weights. In particular, we show both theoretically and empirically that MHA hidden states significantly improve serious problem of deep Transformers known as rank collapse and token uniformity. We also confirm that MHA can systematically improve accuracy without adding training parameters to the Vision Transformer or GPT. Our results provide a new case in which Hopfield networks can be a useful perspective for improving the Transformer architecture.

</details>


### [137] [Post-Pruning Accuracy Recovery via Data-Free Knowledge Distillation](https://arxiv.org/abs/2511.20702)
*Chinmay Tripurwar,Utkarsh Maurya,Dishant*

Main category: cs.LG

TL;DR: 提出了一种无需数据的知识蒸馏框架，通过DeepInversion合成隐私保护的"梦想"图像，用于在无法访问原始训练数据的情况下恢复剪枝后模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融等隐私敏感领域，模型部署后由于法规限制无法访问原始训练数据，而全局非结构化剪枝通常会导致准确性显著下降，需要原始数据进行微调。

Method: 利用DeepInversion通过反转批归一化统计量从预训练教师模型中合成隐私保护的图像，这些合成图像作为传输集，将知识从原始教师模型蒸馏到剪枝后的学生网络。

Result: 在CIFAR-10数据集上对多种架构（ResNet、MobileNet、VGG）的实验结果表明，该方法无需访问任何真实数据点即可显著恢复剪枝过程中损失的准确性。

Conclusion: 该方法成功弥合了模型压缩与数据隐私之间的差距，为隐私敏感场景下的模型优化提供了有效解决方案。

Abstract: Model pruning is a widely adopted technique to reduce the computational complexity and memory footprint of Deep Neural Networks (DNNs). However, global unstructured pruning often leads to significant degradation in accuracy, typically necessitating fine-tuning on the original training dataset to recover performance. In privacy-sensitive domains such as healthcare or finance, access to the original training data is often restricted post-deployment due to regulations (e.g., GDPR, HIPAA). This paper proposes a Data-Free Knowledge Distillation framework to bridge the gap between model compression and data privacy. We utilize DeepInversion to synthesize privacy-preserving ``dream'' images from the pre-trained teacher model by inverting Batch Normalization (BN) statistics. These synthetic images serve as a transfer set to distill knowledge from the original teacher to the pruned student network. Experimental results on CIFAR-10 across various architectures (ResNet, MobileNet, VGG) demonstrate that our method significantly recovers accuracy lost during pruning without accessing a single real data point.

</details>


### [138] [Pretraining Transformer-Based Models on Diffusion-Generated Synthetic Graphs for Alzheimer's Disease Prediction](https://arxiv.org/abs/2511.20704)
*Abolfazl Moslemi,Hossein Peyvandi*

Main category: cs.LG

TL;DR: 提出基于Transformer的阿尔茨海默病诊断框架，结合扩散模型生成合成数据、图表示学习和迁移学习，在数据有限和不平衡的临床预测中提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期准确检测至关重要，但机器学习模型开发面临标记数据有限、多站点异质性和类别不平衡的挑战。

Method: 使用类别条件DDPM生成合成数据平衡诊断类别，通过模态特定图Transformer编码器在合成数据上预学习稳健表示，然后在原始数据上训练分类器。

Result: 在NACC数据集上优于标准基线模型，包括早期/晚期融合深度神经网络和MaGNet模型，获得更高的AUC、准确率、敏感性和特异性。

Conclusion: 扩散合成预训练与图Transformer结合可以改善低样本、不平衡临床预测环境中的泛化性能。

Abstract: Early and accurate detection of Alzheimer's disease (AD) is crucial for enabling timely intervention and improving outcomes. However, developing reliable machine learning (ML) models for AD diagnosis is challenging due to limited labeled data, multi-site heterogeneity, and class imbalance. We propose a Transformer-based diagnostic framework that combines diffusion-based synthetic data generation with graph representation learning and transfer learning. A class-conditional denoising diffusion probabilistic model (DDPM) is trained on the real-world NACC dataset to generate a large synthetic cohort that mirrors multimodal clinical and neuroimaging feature distributions while balancing diagnostic classes. Modality-specific Graph Transformer encoders are first pretrained on this synthetic data to learn robust, class-discriminative representations and are then frozen while a neural classifier is trained on embeddings from the original NACC data. We quantify distributional alignment between real and synthetic cohorts using metrics such as Maximum Mean Discrepancy (MMD), Frechet distance, and energy distance, and complement discrimination metrics with calibration and fixed-specificity sensitivity analyses. Empirically, our framework outperforms standard baselines, including early and late fusion deep neural networks and the multimodal graph-based model MaGNet, yielding higher AUC, accuracy, sensitivity, and specificity under subject-wise cross-validation on NACC. These results show that diffusion-based synthetic pretraining with Graph Transformers can improve generalization in low-sample, imbalanced clinical prediction settings.

</details>


### [139] [Solving Diffusion Inverse Problems with Restart Posterior Sampling](https://arxiv.org/abs/2511.20705)
*Bilal Ahmed,Joseph G. Makin*

Main category: cs.LG

TL;DR: 提出了RePS框架，利用预训练扩散模型解决线性和非线性逆问题，通过重启采样策略提高后验推理效率，避免通过分数网络的反向传播，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的逆问题方法存在后验分布近似强、计算成本高、仅限于线性测量模型等限制，需要更通用高效的解决方案。

Method: 基于重启采样的思想，提出RePS框架，使用适用于任何可微测量模型的条件ODE，引入简化重启策略来收缩采样过程中的累积近似误差。

Result: 在线性和非线性逆问题中，RePS相比现有扩散基线方法实现了更快的收敛速度和更优的重建质量。

Conclusion: RePS为使用预训练扩散模型解决逆问题提供了一个通用高效的框架，在计算效率和重建质量方面均优于现有方法。

Abstract: Inverse problems are fundamental to science and engineering, where the goal is to infer an underlying signal or state from incomplete or noisy measurements. Recent approaches employ diffusion models as powerful implicit priors for such problems, owing to their ability to capture complex data distributions. However, existing diffusion-based methods for inverse problems often rely on strong approximations of the posterior distribution, require computationally expensive gradient backpropagation through the score network, or are restricted to linear measurement models.
  In this work, we propose Restart for Posterior Sampling (RePS), a general and efficient framework for solving both linear and non-linear inverse problems using pre-trained diffusion models. RePS builds on the idea of restart-based sampling, previously shown to improve sample quality in unconditional diffusion, and extends it to posterior inference. Our method employs a conditioned ODE applicable to any differentiable measurement model and introduces a simplified restart strategy that contracts accumulated approximation errors during sampling. Unlike some of the prior approaches, RePS avoids backpropagation through the score network, substantially reducing computational cost.
  We demonstrate that RePS achieves faster convergence and superior reconstruction quality compared to existing diffusion-based baselines across a range of inverse problems, including both linear and non-linear settings.

</details>


### [140] [Active Slice Discovery in Large Language Models](https://arxiv.org/abs/2511.20713)
*Minhui Zhang,Prahar Ijner,Yoav Wald,Elliot Creager*

Main category: cs.LG

TL;DR: 本文提出主动切片发现方法，通过主动学习和有限的人工标注来识别LLM在特定数据子集上的系统错误模式，在毒性分类任务中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在特定数据子集上存在系统错误，手动识别这些错误切片需要大量标注工作，需要开发更高效的方法来减少人工标注需求。

Method: 使用主动学习方法，结合不同的特征表示和主动学习算法，通过有限的人工验证来识别共享相同错误模式的样本组。

Result: 在多个切片上，基于不确定性的主动学习算法表现最佳，仅使用2-10%的切片成员信息就能达到竞争性准确率，显著优于基线方法。

Conclusion: 主动切片发现是一种有效的错误模式识别方法，能够显著减少人工标注需求，为理解和改进模型提供了实用工具。

Abstract: Large Language Models (LLMs) often exhibit systematic errors on specific subsets of data, known as error slices. For instance, a slice can correspond to a certain demographic, where a model does poorly in identifying toxic comments regarding that demographic. Identifying error slices is crucial to understanding and improving models, but it is also challenging. An appealing approach to reduce the amount of manual annotation required is to actively group errors that are likely to belong to the same slice, while using limited access to an annotator to verify whether the chosen samples share the same pattern of model mistake. In this paper, we formalize this approach as Active Slice Discovery and explore it empirically on a problem of discovering human-defined slices in toxicity classification. We examine the efficacy of active slice discovery under different choices of feature representations and active learning algorithms. On several slices, we find that uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information, while significantly outperforming baselines.

</details>


### [141] [ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training](https://arxiv.org/abs/2511.20718)
*Chenliang Li,Adel Elmahdy,Alex Boyd,Zhongruo Wang,Alfredo Garcia,Parminder Bhatia,Taha Kass-Hout,Cao Xiao,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了两种稳定多轮对话中PPO训练的技术：轮级重要性采样和裁剪偏差校正，解决了PPO在大型语言模型训练中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: PPO在多轮对话和推理任务中广泛用于训练大型语言模型，但其性能不稳定且容易崩溃。研究发现不稳定性主要来自两个来源：令牌级重要性采样与多轮环境结构不匹配，以及离策略样本产生的优势估计不准确。

Method: 提出了两种互补的稳定技术：1）轮级重要性采样，使优化与多轮推理的自然结构对齐；2）裁剪偏差校正，通过降低不可靠的离策略样本权重来归一化梯度。组合这些组件得到三个变体：Turn-PPO、S-PPO和ST-PPO。

Result: 在通用问答、多跳问答和医学选择题基准测试中，ST-PPO和S-PPO能持续防止大模型训练中的性能崩溃，保持较低的裁剪比率，并比标准令牌级PPO获得更高的任务性能。

Conclusion: 结合轮级重要性采样和裁剪偏差校正为稳定多轮LLM智能体训练提供了实用且可扩展的解决方案。

Abstract: PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.

</details>


### [142] [Gradient Descent Algorithm Survey](https://arxiv.org/abs/2511.20725)
*Deng Fucheng,Wang Wanjie,Gong Ao,Wang Xiaoqi,Wang Fan*

Main category: cs.LG

TL;DR: 本文系统分析SGD、Mini-batch SGD、Momentum、Adam和Lion五大深度学习优化算法的核心优势、局限性和实践建议，为算法选择和参数调优提供标准化参考。


<details>
  <summary>Details</summary>
Motivation: 针对深度学习优化算法在实际配置中的需求，深入理解这些算法，为学术研究和工程实践中合理选择、参数调优和性能提升提供标准化参考，解决不同规模模型和训练场景中的优化挑战。

Method: 系统分析五大优化算法（SGD、Mini-batch SGD、Momentum、Adam、Lion）的核心优势、局限性和关键实践建议。

Result: 提供了每种算法的核心优势、局限性分析和关键实践建议，建立了算法选择的标准化参考框架。

Conclusion: 该研究为深度学习优化算法的合理选择、参数调优和性能改进提供了系统化的指导，有助于解决不同模型规模和训练场景下的优化问题。

Abstract: Focusing on the practical configuration needs of optimization algorithms in deep learning, this article concentrates on five major algorithms: SGD, Mini-batch SGD, Momentum, Adam, and Lion. It systematically analyzes the core advantages, limitations, and key practical recommendations of each algorithm. The research aims to gain an in-depth understanding of these algorithms and provide a standardized reference for the reasonable selection, parameter tuning, and performance improvement of optimization algorithms in both academic research and engineering practice, helping to solve optimization challenges in different scales of models and various training scenarios.

</details>


### [143] [Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge](https://arxiv.org/abs/2511.20726)
*Yuhang Wang,Heye Huang,Zhenhua Xu,Kailai Sun,Baoshen Guo,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出了一种结合条件变分自编码器(CVAE)和大语言模型(LLM)的高保真场景生成框架，用于自动驾驶的安全验证，能够生成物理一致的基础场景并动态引导风险敏感的场景生成。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在罕见长尾事件和复杂多智能体交互方面的关键挑战，这些事件在真实世界数据中稀缺但对安全验证至关重要。

Method: 使用CVAE编码历史轨迹和地图信息学习潜在交通结构，生成物理一致的基础场景；利用LLM作为对抗推理引擎，将非结构化场景描述解析为领域特定损失函数，动态引导不同风险级别的场景生成。

Result: 在CARLA和SMARTS中的实验表明，该框架显著增加了高风险和长尾事件的覆盖率，改善了模拟与真实世界交通分布的一致性，暴露了比现有方法更具挑战性的交互场景。

Conclusion: 为安全验证建立了新途径，能够在罕见但重要事件下对自动驾驶系统进行原则性压力测试。

Abstract: Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.

</details>


### [144] [Spatio-Temporal Trajectory Foundation Model - Recent Advances and Future Directions](https://arxiv.org/abs/2511.20729)
*Sean Bin Yang,Ying Sun,Yunyao Cheng,Yan Lin,Kristian Torp,Jilin Hu*

Main category: cs.LG

TL;DR: 本教程系统梳理了轨迹基础模型(TFMs)这一时空基础模型重要子类的研究进展，包括方法分类、优劣势分析，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管时空基础模型(STFMs)发展迅速，但对其重要子类轨迹基础模型(TFMs)的系统性研究仍很缺乏，需要填补这一空白。

Method: 通过分类学方法对现有TFMs方法进行系统梳理，并进行批判性分析。

Result: 提供了TFMs的全面概览，包括方法分类框架和优劣势评估。

Conclusion: TFMs是推进时空通用智能的关键，需要开发更鲁棒、负责任和可迁移的模型，并指出了开放挑战和未来研究方向。

Abstract: Foundation models (FMs) have emerged as a powerful paradigm, enabling a diverse range of data analytics and knowledge discovery tasks across scientific fields. Inspired by the success of FMs, particularly large language models, researchers have recently begun to explore spatio-temporal foundation models (STFMs) to improve adaptability and generalization across a wide spectrum of spatio-temporal (ST) tasks. Despite rapid progress, a systematic investigation of trajectory foundation models (TFMs), a crucial subclass of STFMs, is largely lacking. This tutorial addresses this gap by offering a comprehensive overview of recent advances in TFMs, including a taxonomy of existing methodologies and a critical analysis of their strengths and limitations. In addition, the tutorial highlights open challenges and outlines promising research directions to advance spatio-temporal general intelligence through the development of robust, responsible, and transferable TFMs.

</details>


### [145] [A Unified Understanding of Offline Data Selection and Online Self-refining Generation for Post-training LLMs](https://arxiv.org/abs/2511.21056)
*Quan Xiao,Tianyi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种优化视角下的离线数据选择和在线自优化生成框架，通过双层数据选择和模型适应步骤提升LLM在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 离线数据选择和在线自优化生成对于提升大语言模型在特定下游任务中的性能至关重要，但目前缺乏统一的理论框架来理解这两个过程。

Method: 使用双层数据选择进行离线数据选择，将在线自优化生成视为模型适应步骤，通过为每个问题和响应分配学习的数据权重来统一理解这两个过程。

Result: 理论证明了双层数据选择框架的有效性，在质量增强和安全感知的LLM微调实验中验证了其性能提升。

Conclusion: 通过结合离线数据和验证加权的在线生成，该方法能够有效提升微调性能，为数据选择和自优化生成提供了统一的理论基础。

Abstract: Offline data selection and online self-refining generation, which enhance the data quality, are crucial steps in adapting large language models (LLMs) to specific downstream tasks. We tackle offline data selection and online self-refining generations through an optimization perspective. Specifically, bilevel data selection is used for offline data selection with respect to the validation dataset, and we treat online self-refining generation as a model adaptation step of selecting the model trained on current responses that best fits the validation data. Our framework offers a unified understanding of offline data selection and self-refining generation by assigning a learned data weight to each question and response, either explicitly or implicitly. For the first time, we theoretically demonstrate the effectiveness of the bilevel data selection framework and demonstrate its performance gains over unfiltered direct mixing baselines. By combining offline data with validation-weighted online generations, our method enhances fine-tuning performance. Experiments on quality enhancement and safety-aware LLM fine-tuning validate its effectiveness.

</details>


### [146] [CHiQPM: Calibrated Hierarchical Interpretable Image Classification](https://arxiv.org/abs/2511.20779)
*Thomas Norrenbrock,Timo Kaiser,Sovan Biswas,Neslihan Kose,Ramesh Manuvinakurike,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: CHiQPM是一个全局可解释模型，通过对比解释大多数类别提供卓越的全局可解释性，并提供新颖的分层解释，同时保持与非可解释模型99%的准确率。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域需要可信赖的AI，全局可解释模型是有前景的方法，但需要详细的局部解释作为补充来有效支持人类专家在推理过程中。

Method: 提出校准分层QPM（CHiQPM），通过对比解释大多数类别实现全局可解释性，提供分层解释（更接近人类推理方式），并包含内置可解释的Conformal预测方法。

Result: CHiQPM作为点预测器达到最先进的准确率，保持非可解释模型99%的准确率，同时其校准集预测在效率上与其他CP方法竞争，并提供沿其分层解释的连贯集的可解释预测。

Conclusion: CHiQPM在保持高准确率的同时实现了全面的全局和局部可解释性，为人机互补铺平了道路，展示了在不牺牲整体准确性的情况下融入可解释性的实质性改进。

Abstract: Globally interpretable models are a promising approach for trustworthy AI in safety-critical domains. Alongside global explanations, detailed local explanations are a crucial complement to effectively support human experts during inference. This work proposes the Calibrated Hierarchical QPM (CHiQPM) which offers uniquely comprehensive global and local interpretability, paving the way for human-AI complementarity. CHiQPM achieves superior global interpretability by contrastively explaining the majority of classes and offers novel hierarchical explanations that are more similar to how humans reason and can be traversed to offer a built-in interpretable Conformal prediction (CP) method. Our comprehensive evaluation shows that CHiQPM achieves state-of-the-art accuracy as a point predictor, maintaining 99% accuracy of non-interpretable models. This demonstrates a substantial improvement, where interpretability is incorporated without sacrificing overall accuracy. Furthermore, its calibrated set prediction is competitively efficient to other CP methods, while providing interpretable predictions of coherent sets along its hierarchical explanation.

</details>


### [147] [Selecting Belief-State Approximations in Simulators with Latent States](https://arxiv.org/abs/2511.20870)
*Nan Jiang*

Main category: cs.LG

TL;DR: 本文研究了模拟器中状态重置的问题，特别关注当模拟器包含潜在变量时如何选择近似的信念状态采样器。论文将问题简化为条件分布选择任务，提出了两种不同的选择方法，并分析了它们与下游展开方法的交互关系。


<details>
  <summary>Details</summary>
Motivation: 状态重置是模拟器的基本能力，但在复杂模拟器中实现非平凡，特别是当模拟器包含潜在变量时。状态重置需要从给定可观测历史的潜在状态后验分布中采样，而精确采样通常不可行，因此需要选择适当的近似信念状态采样器。

Method: 将信念状态选择问题简化为条件分布选择任务，提出了两种选择方法：基于潜在状态的选择（直接针对潜在状态的条件分布）和基于观测的选择（针对观测的诱导分布）。分析了这两种方法与不同展开方法（单次重置和重复重置）的交互关系。

Result: 研究发现基于观测的选择在单次重置方法下可能失败，但在重复重置方法下具有保证。论文揭示了算法选择、理论细微差别和开放问题的丰富景观。

Conclusion: 状态重置这一看似简单的问题实际上涉及复杂的算法选择和理论考量。基于潜在状态和基于观测的选择方法在不同的展开方法下表现出不同的性能特征，这为未来的研究提供了重要的方向和开放问题。

Abstract: State resetting is a fundamental but often overlooked capability of simulators. It supports sample-based planning by allowing resets to previously encountered simulation states, and enables calibration of simulators using real data by resetting to states observed in real-system traces. While often taken for granted, state resetting in complex simulators can be nontrivial: when the simulator comes with latent variables (states), state resetting requires sampling from the posterior over the latent state given the observable history, a.k.a. the belief state (Silver and Veness, 2010). While exact sampling is often infeasible, many approximate belief-state samplers can be constructed, raising the question of how to select among them using only sampling access to the simulator.
  In this paper, we show that this problem reduces to a general conditional distribution-selection task and develop a new algorithm and analysis under sampling-only access. Building on this reduction, the belief-state selection problem admits two different formulations: latent state-based selection, which directly targets the conditional distribution of the latent state, and observation-based selection, which targets the induced distribution over the observation. Interestingly, these formulations differ in how their guarantees interact with the downstream roll-out methods: perhaps surprisingly, observation-based selection may fail under the most natural roll-out method (which we call Single-Reset) but enjoys guarantees under the less conventional alternative (which we call Repeated-Reset). Together with discussion on issues such as distribution shift and the choice of sampling policies, our paper reveals a rich landscape of algorithmic choices, theoretical nuances, and open questions, in this seemingly simple problem.

</details>


### [148] [Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model](https://arxiv.org/abs/2511.20798)
*Rio Alexa Fear,Payel Mukhopadhyay,Michael McCabe,Alberto Bietti,Miles Cranmer*

Main category: cs.LG

TL;DR: 该研究发现科学基础模型学习到了物理原理的通用表征，而不仅仅是依赖模拟数据中的表面相关性。通过提取和注入激活空间中的概念方向，可以因果性地控制模型的物理行为预测。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索基础模型是否普遍具有可解释的内部表征，还是仅限于语言、图像等结构化数据训练的特殊现象。

Method: 从大型物理基础模型中提取不同物理机制下的激活向量，计算delta表征作为概念方向，然后通过注入这些方向来操控模型预测。

Result: 实验表明通过注入概念方向可以因果性地控制物理行为，如诱导或移除特定物理特征，证明模型学习了通用的物理原理表征。

Conclusion: 科学基础模型确实学习到了物理原理的通用表征，这一发现为理解和控制科学基础模型开辟了新途径，对AI驱动的科学发现具有重要意义。

Abstract: Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute "delta" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.

</details>


### [149] [Probabilistic Hash Embeddings for Online Learning of Categorical Features](https://arxiv.org/abs/2511.20893)
*Aodong Li,Abishek Sankararaman,Balakrishnan Narayanaswamy*

Main category: cs.LG

TL;DR: 提出概率哈希嵌入(PHE)模型，用于处理在线学习中词汇表动态变化的分类特征，通过贝叶斯在线学习避免确定性哈希嵌入的顺序敏感性和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统特征哈希方法在离线或批处理场景中表现良好，但在在线学习场景中，确定性嵌入对类别到达顺序敏感且容易遗忘，导致性能下降。

Method: 提出概率哈希嵌入(PHE)模型，将哈希嵌入视为随机变量，应用贝叶斯在线学习从数据中增量学习，推导出可扩展的推理算法来学习模型参数并推断/更新哈希嵌入和其他潜在变量的后验分布。

Result: 在分类、序列建模和推荐系统的在线学习实验中，PHE表现出优越性能，同时保持高内存效率（仅消耗one-hot嵌入表2-4倍的内存）。

Conclusion: PHE模型能够处理不断演变的分类词汇表，适应新项目而不遗忘旧项目，参数数量有界不随流中不同观察值数量增长，且对项目到达顺序不变。

Abstract: We study streaming data with categorical features where the vocabulary of categorical feature values is changing and can even grow unboundedly over time. Feature hashing is commonly used as a pre-processing step to map these categorical values into a feature space of fixed size before learning their embeddings. While these methods have been developed and evaluated for offline or batch settings, in this paper we consider online settings. We show that deterministic embeddings are sensitive to the arrival order of categories and suffer from forgetting in online learning, leading to performance deterioration. To mitigate this issue, we propose a probabilistic hash embedding (PHE) model that treats hash embeddings as stochastic and applies Bayesian online learning to learn incrementally from data. Based on the structure of PHE, we derive a scalable inference algorithm to learn model parameters and infer/update the posteriors of hash embeddings and other latent variables. Our algorithm (i) can handle an evolving vocabulary of categorical items, (ii) is adaptive to new items without forgetting old items, (iii) is implementable with a bounded set of parameters that does not grow with the number of distinct observed values on the stream, and (iv) is invariant to the item arrival order. Experiments in classification, sequence modeling, and recommendation systems in online learning setups demonstrate the superior performance of PHE while maintaining high memory efficiency (consumes as low as 2~4 memory of a one-hot embedding table). Supplementary materials are at https://github.com/aodongli/probabilistic-hash-embeddings

</details>


### [150] [Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning](https://arxiv.org/abs/2511.20811)
*Aaron O. Feldman,D. Isaiah Harp,Joseph Duncan,Mac Schwager*

Main category: cs.LG

TL;DR: 开发数据驱动的飞行测试安全监控方法，通过离线随机轨迹模拟学习短期安全风险的校准统计模型，为飞行员提供预判标准以提前中止危险机动。


<details>
  <summary>Details</summary>
Motivation: 飞行测试中飞机参数不确定，安全违规可能意外发生，飞行员需要明确标准在安全违规前预判并中止机动。

Method: 包含三个通用组件：基于近期观测预测未来状态的模型、基于最近邻分类预测状态安全性的模型、通过保形预测进行分类器校准。

Result: 在参数不确定的飞行动力学模型上评估，证明能可靠识别不安全场景，匹配理论保证，在风险预判分类上优于基线方法。

Conclusion: 该方法为飞行测试提供可靠的安全监控，通过数据驱动方法有效管理参数不确定性带来的安全风险。

Abstract: We develop a data-driven approach for runtime safety monitoring in flight testing, where pilots perform maneuvers on aircraft with uncertain parameters. Because safety violations can arise unexpectedly as a result of these uncertainties, pilots need clear, preemptive criteria to abort the maneuver in advance of safety violation. To solve this problem, we use offline stochastic trajectory simulation to learn a calibrated statistical model of the short-term safety risk facing pilots. We use flight testing as a motivating example for data-driven learning/monitoring of safety due to its inherent safety risk, uncertainty, and human-interaction. However, our approach consists of three broadly-applicable components: a model to predict future state from recent observations, a nearest neighbor model to classify the safety of the predicted state, and classifier calibration via conformal prediction. We evaluate our method on a flight dynamics model with uncertain parameters, demonstrating its ability to reliably identify unsafe scenarios, match theoretical guarantees, and outperform baseline approaches in preemptive classification of risk.

</details>


### [151] [Operationalizing Quantized Disentanglement](https://arxiv.org/abs/2511.20927)
*Vitoria Barin-Pacela,Kartik Ahuja,Simon Lacoste-Julien,Pascal Vincent*

Main category: cs.LG

TL;DR: 提出Cliff方法，通过鼓励轴对齐的不连续性来实现无监督解缠，在多个基准测试中优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现有理论表明在任意微分同胚下量化因子具有无监督可识别性，但将这一理论原则转化为有效的实践标准仍然具有挑战性，特别是在非线性映射下

Method: 开发基于轴对齐不连续性的无监督解缠标准，通过鼓励因子密度中的轴对齐不连续性（称为cliffs），并确保沿某一因子的cliffs位置与其他因子的值独立

Result: Cliff方法在所有解缠基准测试中都优于基线方法

Conclusion: 通过鼓励轴对齐不连续性，Cliff方法在无监督解缠中表现出色，验证了理论原则的有效实践应用

Abstract: Recent theoretical work established the unsupervised identifiability of quantized factors under any diffeomorphism. The theory assumes that quantization thresholds correspond to axis-aligned discontinuities in the probability density of the latent factors. By constraining a learned map to have a density with axis-aligned discontinuities, we can recover the quantization of the factors. However, translating this high-level principle into an effective practical criterion remains challenging, especially under nonlinear maps. Here, we develop a criterion for unsupervised disentanglement by encouraging axis-aligned discontinuities. Discontinuities manifest as sharp changes in the estimated density of factors and form what we call cliffs. Following the definition of independent discontinuities from the theory, we encourage the location of the cliffs along a factor to be independent of the values of the other factors. We show that our method, Cliff, outperforms the baselines on all disentanglement benchmarks, demonstrating its effectiveness in unsupervised disentanglement.

</details>


### [152] [Mean-Field Limits for Two-Layer Neural Networks Trained with Consensus-Based Optimization](https://arxiv.org/abs/2511.21466)
*William De Deyn,Michael Herty,Giovanni Samaey*

Main category: cs.LG

TL;DR: 该论文研究两层神经网络，使用基于共识的优化(CBO)粒子方法进行训练，并与Adam优化器比较性能，提出混合方法加速收敛，在均值场极限下分析CBO动态。


<details>
  <summary>Details</summary>
Motivation: 研究CBO方法在神经网络训练中的应用，探索其与Adam优化器的性能差异，并寻求通过混合方法提高收敛速度，同时在多任务学习场景下减少内存开销。

Method: 使用基于粒子的CBO方法训练两层神经网络，与Adam进行对比实验，开发CBO-Adam混合方法，在最优传输框架下重新表述CBO，并在均值场极限下分析动态特性。

Result: CBO与Adam混合方法比纯CBO收敛更快；在多任务学习中重新表述的CBO减少了内存开销；在无限粒子极限下，Wasserstein空间上的动态显示方差单调递减。

Conclusion: CBO方法在神经网络训练中具有潜力，与Adam的混合方法能加速收敛，均值场分析为理解CBO动态提供了理论基础，方差单调递减特性表明方法的稳定性。

Abstract: We study two-layer neural networks and train these with a particle-based method called consensus-based optimization (CBO). We compare the performance of CBO against Adam on two test cases and demonstrate how a hybrid approach, combining CBO with Adam, provides faster convergence than CBO. In the context of multi-task learning, we recast CBO into a formulation that offers less memory overhead. The CBO method allows for a mean-field limit formulation, which we couple with the mean-field limit of the neural network. To this end, we first reformulate CBO within the optimal transport framework. Finally, in the limit of infinitely many particles, we define the corresponding dynamics on the Wasserstein-over-Wasserstein space and show that the variance decreases monotonically.

</details>


### [153] [Effects of Initialization Biases on Deep Neural Network Training Dynamics](https://arxiv.org/abs/2511.20826)
*Nicholas Pellegrino,David Szczecina,Paul W. Fieguth*

Main category: cs.LG

TL;DR: 未经训练的大型神经网络在随机初始化后倾向于偏好少数类别，这种初始猜测偏差会影响早期训练动态，损失函数的选择对此有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究初始猜测偏差对神经网络早期训练动态的影响，特别是不同损失函数如何与这种偏差相互作用。

Method: 分析未经训练神经网络在随机初始化后的类别偏好行为，研究Blurry和Piecewise-zero等损失函数在这种情况下的表现。

Result: 初始猜测偏差显著影响早期训练动态，某些损失函数（如Blurry和Piecewise-zero）在面对这种偏差时可能无法有效指导训练方向。

Conclusion: 损失函数选择对网络早期训练阶段有显著影响，需要仔细考虑初始猜测偏差与训练方案各组成部分的相互作用。

Abstract: Untrained large neural networks, just after random initialization, tend to favour a small subset of classes, assigning high predicted probabilities to these few classes and approximately zero probability to all others. This bias, termed Initial Guessing Bias, affects the early training dynamics, when the model is fitting to the coarse structure of the data. The choice of loss function against which to train the model has a large impact on how these early dynamics play out. Two recent loss functions, Blurry and Piecewise-zero loss, were designed for robustness to label errors but can become unable to steer the direction of training when exposed to this initial bias. Results indicate that the choice of loss function has a dramatic effect on the early phase training of networks, and highlights the need for careful consideration of how Initial Guessing Bias may interact with various components of the training scheme.

</details>


### [154] [Estimating Ising Models in Total Variation Distance](https://arxiv.org/abs/2511.21008)
*Constantinos Daskalakis,Vardis Kandiros,Rui Yao*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架来分析最大伪似然估计器(MPLE)在两类一般Ising模型中的表现，为多项式时间估计提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 虽然Ising模型在总变差距离下的统计复杂性已被理解，但缺乏计算和统计效率的统一算法框架。现有研究局限于特定场景，如树结构、高斯交互矩阵或特征值集中的情况。

Method: 使用最大伪似然估计器(MPLE)，对两类一般Ising模型进行统一分析：第一类具有有界算子范数且满足改进对数Sobolev不等式(MLSI)；第二类具有有界无穷范数(有界宽度)。

Result: 为这两类模型提供了多项式时间算法，并在多种设置下获得了最优或接近最优的样本复杂度保证。

Conclusion: 该研究为Ising模型的统一估计框架奠定了基础，证明工具包括张量化不等式、测度分解和集中界等多种数学工具。

Abstract: We consider the problem of estimating Ising models over $n$ variables in Total Variation (TV) distance, given $l$ independent samples from the model. While the statistical complexity of the problem is well-understood [DMR20], identifying computationally and statistically efficient algorithms has been challenging. In particular, remarkable progress has occurred in several settings, such as when the underlying graph is a tree [DP21, BGPV21], when the entries of the interaction matrix follow a Gaussian distribution [GM24, CK24], or when the bulk of its eigenvalues lie in a small interval [AJK+24, KLV24], but no unified framework for polynomial-time estimation in TV exists so far. Our main contribution is a unified analysis of the Maximum Pseudo-Likelihood Estimator (MPLE) for two general classes of Ising models. The first class includes models that have bounded operator norm and satisfy the Modified Log-Sobolev Inequality (MLSI), a functional inequality that was introduced to study the convergence of the associated Glauber dynamics to stationarity. In the second class of models, the interaction matrix has bounded infinity norm (or bounded width), which is the most common assumption in the literature for structure learning of Ising models. We show how our general results for these classes yield polynomial-time algorithms and optimal or near-optimal sample complexity guarantees in a variety of settings. Our proofs employ a variety of tools from tensorization inequalities to measure decompositions and concentration bounds.

</details>


### [155] [Autoregressive Surrogate Modeling of the Solar Wind with Spherical Fourier Neural Operator](https://arxiv.org/abs/2511.20830)
*Reza Mansouri,Dustin Kempton,Pete Riley,Rafal Angryk*

Main category: cs.LG

TL;DR: 提出了首个基于球形傅里叶神经算子的自回归机器学习替代模型，用于预测稳态太阳风径向速度，相比传统数值方法具有更好的精度和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统三维磁流体动力学模型计算成本高昂，限制了边界条件不确定性的快速探索，需要开发高效的替代模型来改进空间天气预报。

Method: 使用球形傅里叶神经算子，通过预测有限径向范围并迭代向外传播解，构建自回归机器学习替代模型。

Result: 与数值HUX替代模型相比，SFNO表现出相当或更优的性能，同时提供灵活、可训练且数据驱动的替代方案。

Conclusion: 建立了一种新颖的高保真太阳风建模方法，为空间天气预报提供了有效的计算工具。

Abstract: The solar wind, a continuous outflow of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Accurate prediction of features such as high-speed streams and coronal mass ejections is critical for space weather forecasting, but traditional three-dimensional magnetohydrodynamic (MHD) models are computationally expensive, limiting rapid exploration of boundary condition uncertainties. We introduce the first autoregressive machine learning surrogate for steady-state solar wind radial velocity using the Spherical Fourier Neural Operator (SFNO). By predicting a limited radial range and iteratively propagating the solution outward, the model improves accuracy in distant regions compared to a single-step approach. Compared with the numerical HUX surrogate, SFNO demonstrates superior or comparable performance while providing a flexible, trainable, and data-driven alternative, establishing a novel methodology for high-fidelity solar wind modeling. The source code and additional visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity-autoregressive.

</details>


### [156] [Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs](https://arxiv.org/abs/2511.21050)
*Dongkyu Derek Cho,Huan Song,Arijit Ghosh Chowdhury,Haotian An,Yawei Wang,Rohit Thekkanal,Negin Sokhandan,Sharlina Keshava,Hannah Marlowe*

Main category: cs.LG

TL;DR: 本文首次对RLVR（基于可验证奖励的强化学习）的安全性进行了理论和实证分析，证明了在特定条件下可以消除安全性退化，挑战了安全性与能力必然权衡的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法（如SFT和RLHF）存在安全性与能力的权衡问题，即使使用良性数据集也会导致安全对齐退化。RLVR作为一种有前景的替代方法，但其安全性影响尚未被探索。

Method: 通过理论分析推导KL约束优化下的安全性漂移上界，并进行广泛的实证实验，涵盖五个对抗性安全基准，研究优化算法、模型规模和任务领域的影响。

Result: 理论和实证结果表明，RLVR可以同时增强推理能力并保持或改进安全防护，在特定条件下完全消除安全性退化。

Conclusion: 研究挑战了安全性与能力必然权衡的假设，证明特定训练方法可以同时实现两个目标，为安全部署具备推理能力的LLM提供了见解。

Abstract: Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.

</details>


### [157] [Primal: A Unified Deterministic Framework for Quasi-Orthogonal Hashing and Manifold Learning](https://arxiv.org/abs/2511.20839)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: Primal是一个基于素数平方根数论独立性的确定性特征映射框架，通过无理频率调制构建可调谐的向量表示，包含静态序列生成和动态投影层两种变体。


<details>
  <summary>Details</summary>
Motivation: 传统随机投影方法（如随机傅里叶特征）存在随机性，作者希望开发一种确定性特征映射框架，利用素数平方根的数学特性构建更稳健、可调谐的向量表示。

Method: 提出两种算法变体：StaticPrime用于生成时间位置编码，DynamicPrime作为可调谐投影层。动态框架通过单一缩放参数σ统一两种数学效用类别：低频时作为等距核映射，高频时诱导混沌相位包裹。

Result: 实验评估表明，该框架在正交性保持和分布紧密度方面优于归一化高斯基线，成为随机矩阵投影的计算高效、数学严谨的替代方案。

Conclusion: Primal框架提供了一种基于数论原理的确定性特征映射方法，在信号重建、压缩感知、超维计算和隐私保护学习等应用中表现出优越性能。

Abstract: We present Primal, a deterministic feature mapping framework that harnesses the number-theoretic independence of prime square roots to construct robust, tunable vector representations. Diverging from standard stochastic projections (e.g., Random Fourier Features), our method exploits the Besicovitch property to create irrational frequency modulations that guarantee infinite non-repeating phase trajectories. We formalize two distinct algorithmic variants: (1) StaticPrime, a sequence generation method that produces temporal position encodings empirically approaching the theoretical Welch bound for quasi-orthogonality; and (2) DynamicPrime, a tunable projection layer for input-dependent feature mapping. A central novelty of the dynamic framework is its ability to unify two disparate mathematical utility classes through a single scaling parameter σ. In the low-frequency regime, the method acts as an isometric kernel map, effectively linearizing non-convex geometries (e.g., spirals) to enable high-fidelity signal reconstruction and compressive sensing. Conversely, the high-frequency regime induces chaotic phase wrapping, transforming the projection into a maximum-entropy one-way hash suitable for Hyperdimensional Computing and privacy-preserving Split Learning. Empirical evaluations demonstrate that our framework yields superior orthogonality retention and distribution tightness compared to normalized Gaussian baselines, establishing it as a computationally efficient, mathematically rigorous alternative to random matrix projections. The code is available at https://github.com/VladimerKhasia/primal

</details>


### [158] [G-Net: A Provably Easy Construction of High-Accuracy Random Binary Neural Networks](https://arxiv.org/abs/2511.21063)
*Alireza Aghasi,Nicholas Marshall,Saeid Pourmand,Wyatt Whiting*

Main category: cs.LG

TL;DR: 提出了一种基于超维计算的新型随机化算法来构建可调节精度的二元神经网络，通过二进制嵌入和超维表示实现高效硬件部署和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受超维计算启发，利用高维向量表示的优势，提供高效的硬件实现和对模型损坏的鲁棒性，不同于传统的低精度量化方法。

Method: 提出G-Nets家族，将数据作为超立方体中的点进行二进制嵌入，使用汉明距离，每个浮点G-Net都有随机化的二进制嵌入版本（EHD G-Net）。

Result: 二元模型在CIFAR-10上达到与传统卷积神经网络相当的准确率，比先前HDC模型准确率提高近30%，理论上有度量集中性保证。

Conclusion: G-Nets是神经网络和随机化二元神经网络之间的理论桥梁，为构建鲁棒的二元/量化深度学习模型开辟了新方向。

Abstract: We propose a novel randomized algorithm for constructing binary neural networks with tunable accuracy. This approach is motivated by hyperdimensional computing (HDC), which is a brain-inspired paradigm that leverages high-dimensional vector representations, offering efficient hardware implementation and robustness to model corruptions. Unlike traditional low-precision methods that use quantization, we consider binary embeddings of data as points in the hypercube equipped with the Hamming distance. We propose a novel family of floating-point neural networks, G-Nets, which are general enough to mimic standard network layers. Each floating-point G-Net has a randomized binary embedding, an embedded hyperdimensional (EHD) G-Net, that retains the accuracy of its floating-point counterparts, with theoretical guarantees, due to the concentration of measure. Empirically, our binary models match convolutional neural network accuracies and outperform prior HDC models by large margins, for example, we achieve almost 30\% higher accuracy on CIFAR-10 compared to prior HDC models. G-Nets are a theoretically justified bridge between neural networks and randomized binary neural networks, opening a new direction for constructing robust binary/quantized deep learning models. Our implementation is available at https://github.com/GNet2025/GNet.

</details>


### [159] [Pre-train to Gain: Robust Learning Without Clean Labels](https://arxiv.org/abs/2511.20844)
*David Szczecina,Nicholas Pellegrino,Paul Fieguth*

Main category: cs.LG

TL;DR: 使用自监督学习预训练特征提取器，然后在带噪声标签的数据集上进行标准监督训练，可以在不需要干净标签子集的情况下训练出更抗噪声的模型。


<details>
  <summary>Details</summary>
Motivation: 在带噪声标签的情况下训练深度网络会导致泛化能力差和准确率下降，因为模型会过拟合到标签噪声。现有方法通常依赖干净数据子集，而本文旨在不依赖干净标签子集的情况下提高模型的噪声鲁棒性。

Method: 采用自监督学习（SimCLR和Barlow Twins）预训练特征提取器，然后在带噪声标签的数据集上进行标准监督训练。在CIFAR-10和CIFAR-100数据集上评估合成和真实世界噪声。

Result: 在所有噪声率下，自监督预训练都能持续提高分类准确率并增强下游标签错误检测能力（F1和平衡准确率）。随着噪声率增加，性能差距扩大，显示出改进的鲁棒性。在低噪声水平下与ImageNet预训练模型结果相当，在高噪声条件下显著优于它们。

Conclusion: 自监督预训练是一种有效的策略，可以在不需要干净标签子集的情况下提高模型对标签噪声的鲁棒性，特别是在高噪声条件下表现优异。

Abstract: Training deep networks with noisy labels leads to poor generalization and degraded accuracy due to overfitting to label noise. Existing approaches for learning with noisy labels often rely on the availability of a clean subset of data. By pre-training a feature extractor backbone without labels using self-supervised learning (SSL), followed by standard supervised training on the noisy dataset, we can train a more noise robust model without requiring a subset with clean labels. We evaluate the use of SimCLR and Barlow~Twins as SSL methods on CIFAR-10 and CIFAR-100 under synthetic and real world noise. Across all noise rates, self-supervised pre-training consistently improves classification accuracy and enhances downstream label-error detection (F1 and Balanced Accuracy). The performance gap widens as the noise rate increases, demonstrating improved robustness. Notably, our approach achieves comparable results to ImageNet pre-trained models at low noise levels, while substantially outperforming them under high noise conditions.

</details>


### [160] [How to Correctly Report LLM-as-a-Judge Evaluations](https://arxiv.org/abs/2511.21140)
*Chungpa Lee,Thomas Zeng,Jongwon Jeong,Jy-yong Sohn,Kangwook Lee*

Main category: cs.LG

TL;DR: 提出了一个简单的插件框架来纠正LLM评估中的偏差，构建置信区间，并引入自适应算法来优化校准样本分配。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估者使用时存在噪声，导致准确性估计偏差，现有偏差校正方法通常需要精确知道模型的敏感性和特异性，且置信区间构建方法不明确。

Method: 开发了一个插件框架进行偏差校正和置信区间构建，同时提出了自适应算法来高效分配校准样本量以减少不确定性。

Result: 该框架能够纠正LLM评估中的偏差，构建反映测试和校准数据集不确定性的置信区间，实现实用且统计上可靠的LLM评估。

Conclusion: 提出的方法为LLM评估提供了偏差校正和不确定性量化的实用解决方案，通过自适应校准进一步提高了准确性估计的可靠性。

Abstract: Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.

</details>


### [161] [Representation Integrity in Temporal Graph Learning Methods](https://arxiv.org/abs/2511.20873)
*Elahe Kooshafar*

Main category: cs.LG

TL;DR: 本文提出了表示完整性的概念来衡量动态图嵌入是否能忠实反映网络演化，并推荐了一个经过验证的指标来评估动态图学习模型的质量。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试仅通过任务特定分数评估动态图学习器，但很少关注嵌入本身是否能真实、可解释地反映演化网络。

Method: 形式化表示完整性要求，推导出一系列衡量嵌入变化如何紧密跟随图变化的指标，使用三个合成场景筛选42个候选指标，推荐一个通过理论和实证测试的指标。

Result: 验证的指标始终将可证明稳定的UASE和IPP模型排名最高，神经方法在特定场景下表现优势，且与一步链接预测AUC呈强正相关。

Conclusion: 提出的完整性框架为动态图表示质量提供了任务无关且可解释的评估工具，为模型选择和未来架构设计提供更明确的指导。

Abstract: Real-world systems ranging from airline routes to cryptocurrency transfers are naturally modelled as dynamic graphs whose topology changes over time. Conventional benchmarks judge dynamic-graph learners by a handful of task-specific scores, yet seldom ask whether the embeddings themselves remain a truthful, interpretable reflection of the evolving network. We formalize this requirement as representation integrity and derive a family of indexes that measure how closely embedding changes follow graph changes. Three synthetic scenarios, Gradual Merge, Abrupt Move, and Periodic Re-wiring, are used to screen forty-two candidate indexes. Based on which we recommend one index that passes all of our theoretical and empirical tests. In particular, this validated metric consistently ranks the provably stable UASE and IPP models highest. We then use this index to do a comparative study on representation integrity of common dynamic graph learning models. This study exposes the scenario-specific strengths of neural methods, and shows a strong positive rank correlation with one-step link-prediction AUC. The proposed integrity framework, therefore, offers a task-agnostic and interpretable evaluation tool for dynamic-graph representation quality, providing more explicit guidance for model selection and future architecture design.

</details>


### [162] [Evolved SampleWeights for Bias Mitigation: Effectiveness Depends on Optimization Objectives](https://arxiv.org/abs/2511.20909)
*Anil K. Saini,Jose Guadalupe Hernandez,Emily F. Wong,Debanshi Misra,Jason H. Moore*

Main category: cs.LG

TL;DR: 比较三种样本权重生成方法（遗传算法、数据集特征计算、等权重）在机器学习模型公平性和预测性能之间的权衡效果，发现遗传算法优化的权重能在多个数据集上实现更好的公平性-性能平衡。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在真实数据上训练时可能产生对边缘群体的偏见预测，需要方法在保持预测性能的同时提升模型公平性。

Method: 使用遗传算法进化样本权重，并与基于数据集特征的权重计算和等权重方法进行比较，在11个公开数据集上评估预测指标（准确率、AUC）和公平性指标（人口统计均等差异、子群假阴性公平性）。

Result: 进化样本权重能产生在公平性和预测性能之间更好权衡的模型，但效果大小强烈依赖于优化目标的选择。优化准确率和人口统计均等差异指标时，进化权重在最多数据集上显著优于其他权重策略。

Conclusion: 遗传算法优化的样本权重是缓解机器学习模型偏见的有效方法，但需要仔细选择优化目标以获得最佳效果。

Abstract: Machine learning models trained on real-world data may inadvertently make biased predictions that negatively impact marginalized communities. Reweighting is a method that can mitigate such bias in model predictions by assigning a weight to each data point used during model training. In this paper, we compare three methods for generating these weights: (1) evolving them using a Genetic Algorithm (GA), (2) computing them using only dataset characteristics, and (3) assigning equal weights to all data points. Model performance under each strategy was evaluated using paired predictive and fairness metrics, which also served as optimization objectives for the GA during evolution. Specifically, we used two predictive metrics (accuracy and area under the Receiver Operating Characteristic curve) and two fairness metrics (demographic parity difference and subgroup false negative fairness). Using experiments on eleven publicly available datasets (including two medical datasets), we show that evolved sample weights can produce models that achieve better trade-offs between fairness and predictive performance than alternative weighting methods. However, the magnitude of these benefits depends strongly on the choice of optimization objectives. Our experiments reveal that optimizing with accuracy and demographic parity difference metrics yields the largest number of datasets for which evolved weights are significantly better than other weighting strategies in optimizing both objectives.

</details>


### [163] [Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2511.20913)
*Yingchuan Sun,Shengpu Tang*

Main category: cs.LG

TL;DR: 本文通过实证研究比较了脓毒症管理中强化学习的不同时间步长（1、2、4、8小时），发现更细的时间步长（1-2小时）在静态行为策略下能获得最佳性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有脓毒症管理强化学习研究多采用4小时时间步长，但该粗粒度可能扭曲患者动态并导致次优治疗策略，需要量化时间步长对RL各环节的影响。

Method: 采用相同的离线RL流程，设计动作重映射方法以公平比较不同时间步长，在两种策略学习设置下进行跨时间步长的模型选择。

Result: 不同学习设置下性能趋势随时间步长变化，使用静态行为策略在更细时间步长（1-2小时）学习的策略获得整体最佳性能和稳定性。

Conclusion: 时间步长是医疗保健离线RL的核心设计选择，研究支持采用超越传统4小时设置的替代方案。

Abstract: Existing studies on reinforcement learning (RL) for sepsis management have mostly followed an established problem setup, in which patient data are aggregated into 4-hour time steps. Although concerns have been raised regarding the coarseness of this time-step size, which might distort patient dynamics and lead to suboptimal treatment policies, the extent to which this is a problem in practice remains unexplored. In this work, we conducted empirical experiments for a controlled comparison of four time-step sizes ($Δt\!=\!1,2,4,8$ h) on this domain, following an identical offline RL pipeline. To enable a fair comparison across time-step sizes, we designed action re-mapping methods that allow for evaluation of policies on datasets with different time-step sizes, and conducted cross-$Δt$ model selections under two policy learning setups. Our goal was to quantify how time-step size influences state representation learning, behavior cloning, policy training, and off-policy evaluation. Our results show that performance trends across $Δt$ vary as learning setups change, while policies learned at finer time-step sizes ($Δt = 1$ h and $2$ h) using a static behavior policy achieve the overall best performance and stability. Our work highlights time-step size as a core design choice in offline RL for healthcare and provides evidence supporting alternatives beyond the conventional 4-hour setup.

</details>


### [164] [Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection](https://arxiv.org/abs/2511.20944)
*Yaw Osei Adjei*

Main category: cs.LG

TL;DR: 本文比较了两种BEC检测方法：基于心理语言学的CatBoost方法和基于深度学习的DistilBERT方法，两者都能有效检测商业邮件欺诈，在成本敏感学习优化下投资回报率超过99.96%。


<details>
  <summary>Details</summary>
Motivation: BEC是一种利用组织层级和心理弱点的高级社会工程威胁，造成巨大经济损失。FBI报告显示BEC年损失超过29亿美元，存在显著的经济不对称性：漏报成本远高于误报成本（比例约1:5480）。

Method: 研究两种检测范式：心理语言学流使用CatBoost分析心理语言线索，具有高可解释性和低延迟；语义流使用DistilBERT进行深度学习上下文理解，精度更高但计算成本更大。在对抗性污染数据集上评估两种方法。

Result: DistilBERT在Tesla T4 GPU上实现卓越检测（AUC=1.0000，F1=0.9981），延迟7.403毫秒；CatBoost实现竞争性检测（AUC=0.9905，F1=0.9486），延迟降低8.4倍（0.885毫秒），计算资源消耗可忽略。

Conclusion: 有GPU基础设施的组织推荐使用DistilBERT以获得更高精度；边缘部署或成本敏感环境推荐CatBoost，因其安全性能相当且运营成本更低。两种方法通过成本敏感学习优化都能实现超过99.96%的投资回报率。

Abstract: Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies and exploits psychological vulnerabilities, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual adjusted losses, presenting significant economic asymmetry: the cost of a False Negative (fraud loss) exceeds the cost of a False Positive (manual review) by orders of magnitude (approximately 1 to 5,480).
  This paper examines two detection paradigms for BEC: the Forensic Psycholinguistic Stream, which utilizes CatBoost to analyze psycholinguistic cues with high interpretability and low latency, and the Semantic Stream, which employs DistilBERT for deep learning-based contextual language understanding, offering superior accuracy at higher computational cost. We evaluated DistilBERT on an adversarially poisoned dataset (N = 7,990) generated via our Black Hole protocol, benchmarked on Tesla T4 GPU infrastructure, achieving superior detection (AUC = 1.0000, F1 = 0.9981) with acceptable real-time latency (7.403 milliseconds). CatBoost achieves competitive detection (AUC = 0.9905, F1 = 0.9486) at 8.4x lower latency (0.885 milliseconds), consuming negligible computational resources. For organizations with GPU infrastructure, DistilBERT offers superior accuracy. CatBoost is preferable for edge deployments or cost-sensitive environments due to comparable security and lower operational costs. Both approaches demonstrate return on investment exceeding 99.96% when optimized through cost-sensitive learning, by significantly reducing false negatives and associated financial losses.

</details>


### [165] [Dataset Poisoning Attacks on Behavioral Cloning Policies](https://arxiv.org/abs/2511.20992)
*Akansha Kalra,Soumil Datta,Ethan Gilmore,Duc La,Guanhong Tao,Daniel S. Brown*

Main category: cs.LG

TL;DR: 本文首次分析了行为克隆策略在干净标签后门攻击下的脆弱性，发现即使使用少量中毒数据训练的策略在部署时也极易受到后门触发攻击的影响。


<details>
  <summary>Details</summary>
Motivation: 随着行为克隆策略在现实世界中的部署日益增多，其鲁棒性和潜在漏洞成为重要关注点。本文旨在研究干净标签后门攻击对行为克隆策略的有效性。

Method: 通过向专家演示数据集中注入视觉触发器来创建虚假相关性，在测试时利用这种相关性。还提出了一种基于熵的测试时触发器攻击，通过识别关键状态来最大化性能下降。

Result: 实证表明，即使使用最小程度中毒数据集训练的行为克隆策略，在部署时也表现出极高的后门攻击脆弱性，尽管其任务性能接近基线水平。

Conclusion: 研究结果强调了加强行为克隆策略鲁棒性研究的紧迫性，特别是在大规模数据集被用于训练现实世界网络物理系统策略的背景下。

Abstract: Behavior Cloning (BC) is a popular framework for training sequential decision policies from expert demonstrations via supervised learning. As these policies are increasingly being deployed in the real world, their robustness and potential vulnerabilities are an important concern. In this work, we perform the first analysis of the efficacy of clean-label backdoor attacks on BC policies. Our backdoor attacks poison a dataset of demonstrations by injecting a visual trigger to create a spurious correlation that can be exploited at test time. We evaluate how policy vulnerability scales with the fraction of poisoned data, the strength of the trigger, and the trigger type. We also introduce a novel entropy-based test-time trigger attack that substantially degrades policy performance by identifying critical states where test-time triggering of the backdoor is expected to be most effective at degrading performance. We empirically demonstrate that BC policies trained on even minimally poisoned datasets exhibit deceptively high, near-baseline task performance despite being highly vulnerable to backdoor trigger attacks during deployment. Our results underscore the urgent need for more research into the robustness of BC policies, particularly as large-scale datasets are increasingly used to train policies for real-world cyber-physical systems. Videos and code are available at https://sites.google.com/view/dataset-poisoning-in-bc.

</details>


### [166] [Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning](https://arxiv.org/abs/2511.20993)
*Shanwei Fan*

Main category: cs.LG

TL;DR: 提出了SGA-ACR框架，通过环境特定的子目标图和结构化实体知识，结合多LLM规划流程，解决LLM在强化学习中规划与执行不对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在强化学习中提供高层次规划能力，但存在规划与执行不对齐的问题：子目标语义合理但环境不可行，且单LLM规划混淆生成与自我验证导致不可靠子目标。

Method: 集成环境特定子目标图和结构化实体知识，采用多LLM规划流程明确分离生成、批判和精炼步骤，使用子目标跟踪器监控执行进度并提供辅助奖励。

Result: 在开放世界游戏"Crafter"的22个多样化任务上验证了方法的有效性。

Conclusion: SGA-ACR框架能够产生可执行且可验证的子目标，有效解决了LLM在强化学习中的规划-执行对齐问题。

Abstract: Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game "Crafter" demonstrate the effectiveness of our proposed method.

</details>


### [167] [FANoise: Singular Value-Adaptive Noise Modulation for Robust Multimodal Representation Learning](https://arxiv.org/abs/2511.20997)
*Jiaoyang Li,Jun Fang,Tianhao Gao,Xiaohui Zhang,Zhiyuan Liu,Chao Liu,Pengzhang Liu,Qixia Jiang*

Main category: cs.LG

TL;DR: 本文提出了FANoise，一种特征自适应噪声注入策略，用于改进多模态表示学习，通过动态调整噪声来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有表示学习方法大多依赖启发式或静态噪声，忽略了训练过程中特征分布的动态特性，导致表示学习不够鲁棒和泛化。

Method: 从梯度和特征分布角度系统研究噪声在表示学习中的作用，提出FANoise策略，利用对比学习的动态特性自适应注入噪声。

Result: 在各种基础VLM模型上的综合实验表明，FANoise在多模态任务上持续提升整体性能。

Conclusion: 特征自适应噪声注入策略能够有效缓解噪声的负面影响，同时保留其益处，为表示学习提供了理论支撑的改进框架。

Abstract: Representation learning is fundamental to modern machine learning, powering applications such as text retrieval and multimodal understanding. However, learning robust and generalizable representations remains challenging. While prior work has demonstrated that active noise injection, a form of data augmentation, can enhance encoding performance, most existing methods rely on heuristic or static noise, overlooking the dynamic nature of feature distributions during training. In this work, we systematically study the role of noise in representation learning from both gradient-based and feature distribution perspectives, using InfoNCE loss as a representative example. Focusing on multimodal representation learning, we propose FANoise, a novel feature-adaptive noise injection strategy. By leveraging the dynamics of contrastive learning, FANoise effectively mitigates the negative impacts of noise while preserving its benefits. Under this theoretically grounded framework, comprehensive experiments demonstrate that FANoise consistently improves overall performance on multimodal tasks across various base VLM models.

</details>


### [168] [Predictive Safety Shield for Dyna-Q Reinforcement Learning](https://arxiv.org/abs/2511.21531)
*Jin Pin,Krasowski Hanna,Vanneaux Elena*

Main category: cs.LG

TL;DR: 提出了一种用于离散空间模型强化学习的预测性安全防护机制，通过基于安全预测的局部Q函数更新，在保持硬安全保证的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全防护机制通常使用随机采样安全动作或固定回退控制器，忽略了不同安全动作对未来性能的影响，需要改进以在保证安全的同时提升性能。

Method: 基于环境模型的安全仿真生成安全预测，并据此局部更新Q函数，使用预测性安全防护来指导智能体的动作选择。

Result: 在网格世界环境中的实验表明，即使较短的预测范围也足以识别最优路径，且该方法对分布偏移具有鲁棒性，无需额外训练。

Conclusion: 预测性安全防护能够在保持硬安全保证的同时显著提升强化学习智能体的性能，且具有良好的鲁棒性。

Abstract: Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions. In this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space. Our safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees. Our experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.

</details>


### [169] [ChatGpt Content detection: A new approach using xlm-roberta alignment](https://arxiv.org/abs/2511.21009)
*Md Tasnin Tanvir,Dr Santanu Kumar Dash,Ishan Shahnan,Nafis Fuad,Tanvir Rahman,Abdullah Al Faisal,Asadullah Al Mamun*

Main category: cs.LG

TL;DR: 使用XLM-RoBERTa模型检测AI生成文本的方法，通过困惑度、语义和可读性特征实现高精度检测，为维护学术诚信提供工具。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等生成式AI技术普及，区分AI生成文本与人类撰写内容的需求日益迫切，需要解决完全AI生成内容和AI改写人类文本的检测问题。

Method: 采用XLM-RoBERTa多语言transformer模型，结合严格预处理和特征提取（困惑度、语义、可读性特征），在平衡的人类与AI生成文本数据集上进行微调。

Result: 模型在各种文本类型中表现出高准确性和鲁棒性能，特征分析显示困惑度和基于注意力的特征在区分人类与AI生成文本中起关键作用。

Conclusion: 该方法为维护学术诚信提供了有价值的工具，有助于AI伦理领域的透明度和问责制。未来研究方向包括探索其他先进模型和扩展数据集以增强泛化能力。

Abstract: The challenge of separating AI-generated text from human-authored content is becoming more urgent as generative AI technologies like ChatGPT become more widely available. In this work, we address this issue by looking at both the detection of content that has been entirely generated by AI and the identification of human text that has been reworded by AI. In our work, a comprehensive methodology to detect AI- generated text using XLM-RoBERTa, a state-of-the-art multilingual transformer model. Our approach includes rigorous preprocessing, and feature extraction involving perplexity, semantic, and readability features. We fine-tuned the XLM-RoBERTa model on a balanced dataset of human and AI-generated texts and evaluated its performance. The model demonstrated high accuracy and robust performance across various text genres. Additionally, we conducted feature analysis to understand the model's decision-making process, revealing that perplexity and attention-based features are critical in differentiating between human and AI-generated texts. Our findings offer a valuable tool for maintaining academic integrity and contribute to the broader field of AI ethics by promoting transparency and accountability in AI systems. Future research directions include exploring other advanced models and expanding the dataset to enhance the model's generalizability.

</details>


### [170] [Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.21011)
*Sid Bharthulwar,Stone Tao,Hao Su*

Main category: cs.LG

TL;DR: 提出了一种称为交错重置（staggered resets）的技术，通过在不同时间点初始化和重置环境来减少同步回放带来的非平稳性，从而在并行GPU模拟环境中提高强化学习的样本效率、收敛速度和最终性能。


<details>
  <summary>Details</summary>
Motivation: 在并行GPU模拟环境中，为了最大化吞吐量通常使用短回放和较高的更新数据比（UTD），但标准的同步重置会引入有害的非平稳性，扭曲学习信号并破坏训练稳定性。

Method: 引入了交错重置技术，让环境在任务时间轴的不同点进行初始化和重置，从而产生具有更大时间多样性的训练批次，减少同步回放引起的非平稳性。

Result: 在具有挑战性的高维机器人环境中，该方法实现了显著更高的样本效率、更快的实际收敛速度和更强的最终性能，且随着并行环境数量的增加，比朴素同步回放具有更好的扩展性。

Conclusion: 交错重置是一种简单而有效的技术，能够有效缓解并行强化学习训练中的非平稳性问题，提高训练效率和性能。

Abstract: Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.

</details>


### [171] [Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression](https://arxiv.org/abs/2511.21016)
*Liangzu Peng,Aditya Chattopadhyay,Luca Zancato,Elvis Nunez,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: Gated KalmaNet (GKA) 是一种改进的线性状态空间模型层，通过在线岭回归和卡尔曼滤波的稳定实现，在保持SSM效率的同时解决了传统SSM对过去信息记忆损失的问题，在长短文本任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统线性状态空间模型虽然计算效率高，但只能保持对过去信息的损失性记忆，导致在需要召回历史信息的任务中性能较差。需要一种既能充分利用完整历史信息又能保持高效计算的方法。

Method: 1. 基于卡尔曼滤波思想在线求解岭回归问题；2. 使用自适应正则化和输入相关门控控制条件数；3. 采用切比雪夫迭代法替代传统迭代求解器以提高数值稳定性；4. 开发硬件感知的分块实现和自定义反向传播核。

Result: 在短文本理解任务中超越现有SSM层（Mamba2、GLA、Gated DeltaNet）；在128k token的长文本任务中，相比其他衰减记忆基线获得超过10%的相对性能提升，在RAG和LongQA任务中表现优异。

Conclusion: GKA成功解决了SSM在记忆保持方面的局限性，通过数值稳定的在线岭回归实现了对完整历史信息的利用，同时保持了SSM的计算效率优势，在长短文本任务中都展现了强大的性能。

Abstract: As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces this gap by accounting for the full past when predicting the next token, while maintaining SSM-style efficiency. GKA achieves this by solving an online ridge regression problem at test time, with constant memory and linear compute cost in the sequence length. Drawing inspiration from the Kalman Filter, we iteratively solve the online ridge regression problem. However, a critical insight is that standard Kalman filter equations are numerically unstable in low-precision environments (like bfloat16) and difficult to parallelize in modern hardware. We address both challenges through two key innovations: (1) an adaptive regularization strategy with input-dependent gating that controls the condition number of the ridge regression, ensuring numerical stability while balancing memory retention. And (2) the use of Chebyshev Iteration instead of other conventional iterative solvers, which we demonstrate to be more stable in low-precision settings. To further improve scalability, we develop a hardware-aware chunk-wise implementation of Chebyshev Iteration along with custom kernels for backpropagating through our adaptive regularization and gating mechanisms. Empirically, GKA shows strong language understanding capabilites on short-context tasks outperforming existing SSM layers (like Mamba2, GLA and Gated DeltaNet). On long-context, GKA excels at real-world RAG and LongQA tasks up to 128k tokens, achieving more than $10$% relative improvement over other fading memory baselines.

</details>


### [172] [Probabilistic Wildfire Spread Prediction Using an Autoregressive Conditional Generative Adversarial Network](https://arxiv.org/abs/2511.21019)
*Taehoon Kang,Taeyong Kim*

Main category: cs.LG

TL;DR: 提出基于自回归条件生成对抗网络(CGAN)的概率性野火蔓延预测模型，通过对抗学习捕捉野火传播的非线性动态，相比传统深度学习模型在预测精度和边界划分方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了野火的频率和严重性，需要快速准确的野火蔓延预测。基于物理的模拟器计算密集，而现有深度学习模型预测过于平滑，无法捕捉野火传播的复杂非线性动态。

Method: 使用自回归条件生成对抗网络(CGAN)，将预测任务构建为自回归问题，学习序列状态转换，确保长期预测稳定性。

Result: 实验结果表明，提出的CGAN模型在整体预测精度和火场边界划分方面优于传统深度学习模型，能够捕捉野火传播的强非线性和不确定性。

Conclusion: 基于CGAN的自回归框架提高了野火蔓延预测的准确性和物理可解释性，为时间敏感的响应和疏散规划提供了有前景的基础。

Abstract: Climate change has intensified the frequency and severity of wildfires, making rapid and accurate prediction of fire spread essential for effective mitigation and response. Physics-based simulators such as FARSITE offer high-fidelity predictions but are computationally intensive, limiting their applicability in real-time decision-making, while existing deep learning models often yield overly smooth predictions that fail to capture the complex, nonlinear dynamics of wildfire propagation. This study proposes an autoregressive conditional generative adversarial network (CGAN) for probabilistic wildfire spread prediction. By formulating the prediction task as an autoregressive problem, the model learns sequential state transitions, ensuring long-term prediction stability. Experimental results demonstrate that the proposed CGAN-based model outperforms conventional deep learning models in both overall predictive accuracy and boundary delineation of fire perimeters. These results demonstrate that adversarial learning allows the model to capture the strong nonlinearity and uncertainty of wildfire spread, instead of simply fitting the pixel average. Furthermore, the autoregressive framework facilitates systematic temporal forecasting of wildfire evolution. The proposed CGAN-based autoregressive framework enhances both the accuracy and physical interpretability of wildfire spread prediction, offering a promising foundation for time-sensitive response and evacuation planning.

</details>


### [173] [A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems](https://arxiv.org/abs/2511.21032)
*Yuxuan Zhu,Cong Fu,Yabo Ni,Anxiang Zeng,Yuan Fang*

Main category: cs.LG

TL;DR: 提出ELBO$_{TDS}$概率框架，通过数据增强和因果图建模解决推荐系统中的时间分布偏移问题，已在Shopee产品搜索中成功部署。


<details>
  <summary>Details</summary>
Motivation: 时间分布偏移会侵蚀推荐系统的长期准确性，而现有的周期性增量训练方法难以同时捕捉稳定和瞬态模式，现有方法如不变性学习和自监督学习存在时间泛化不稳定、表示塌陷或数据利用效率低等问题。

Method: 1）通过真实生产数据的统计分析识别关键偏移因子，设计有效的数据增强策略重新采样这些时变因子以扩展训练支持；2）使用因果图建模时序推荐场景，基于因果结构推导出自监督变分目标ELBO$_{TDS}$。

Result: 大量实验表明该方法实现了优越的时间泛化能力，每个用户的GMV提升了2.33%，并已在Shopee产品搜索中成功部署。

Conclusion: ELBO$_{TDS}$框架能够有效解决推荐系统中的时间分布偏移问题，通过数据增强和因果建模实现了更好的时间泛化性能。

Abstract: Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at https://github.com/FuCongResearchSquad/ELBO4TDS.

</details>


### [174] [Prediction of Herd Life in Dairy Cows Using Multi-Head Attention Transformers](https://arxiv.org/abs/2511.21034)
*Mahdi Saki,Justin Lipman*

Main category: cs.LG

TL;DR: 开发基于多头注意力Transformer的AI模型，利用历史多变量时间序列数据预测奶牛寿命，在澳大利亚7个农场的数据上达到83%的决定系数。


<details>
  <summary>Details</summary>
Motivation: 奶农需要基于客观评估决定保留或淘汰奶牛，识别更具韧性的奶牛以应对农场条件并完成更多泌乳期，这一决策过程复杂且具有重要环境和经济影响。

Method: 使用多头注意力Transformer技术，分析从出生开始记录的历史多变量时间序列数据，涵盖7个农场19,000头奶牛的约780,000条记录。

Result: 模型在预测奶牛群寿命方面达到83%的整体决定系数，显示出在奶牛群管理中的实际应用潜力。

Conclusion: AI驱动的奶牛寿命预测模型为奶农提供了客观的决策支持工具，有助于识别更具韧性的奶牛并优化畜群管理。

Abstract: Dairy farmers should decide to keep or cull a cow based on an objective assessment of her likely performance in the herd. For this purpose, farmers need to identify more resilient cows, which can cope better with farm conditions and complete more lactations. This decision-making process is inherently complex, with significant environmental and economic implications. In this study, we develop an AI-driven model to predict cow longevity using historical multivariate time-series data recorded from birth. Leveraging advanced AI techniques, specifically Multi-Head Attention Transformers, we analysed approximately 780,000 records from 19,000 unique cows across 7 farms in Australia. The results demonstrate that our model achieves an overall determination coefficient of 83% in predicting herd life across the studied farms, highlighting its potential for practical application in dairy herd management.

</details>


### [175] [RAVQ-HoloNet: Rate-Adaptive Vector-Quantized Hologram Compression](https://arxiv.org/abs/2511.21035)
*Shima Rafiei,Zahra Nabizadeh Shahr Babak,Shadrokh Samavi,Shahram Shirani*

Main category: cs.LG

TL;DR: RAVQ-HoloNet是一个速率自适应向量量化框架，在低比特率和超低比特率下实现高保真重建，在BD-Rate上比现有最佳方法提升33.91%，BD-PSNR达到1.02 dB


<details>
  <summary>Details</summary>
Motivation: 全息技术在AR/VR应用中潜力巨大，但受限于数据压缩的高要求。现有深度学习方法通常缺乏单一网络内的速率自适应性

Method: 提出速率自适应向量量化框架RAVQ-HoloNet

Result: 在低比特率下，该方法在BD-Rate上比现有最佳方法提升33.91%，BD-PSNR达到1.02 dB，在率失真曲线上表现优异

Conclusion: RAVQ-HoloNet框架在全息数据压缩方面显著优于现有最先进方法，特别是在低比特率场景下

Abstract: Holography offers significant potential for AR/VR applications, yet its adoption is limited by the high demands of data compression. Existing deep learning approaches generally lack rate adaptivity within a single network. We present RAVQ-HoloNet, a rate-adaptive vector quantization framework that achieves high-fidelity reconstructions at low and ultra-low bit rates, outperforming current state-of-the-art methods. In low bit, our method exceeds by -33.91% in BD-Rate and achieves a BD-PSNR of 1.02 dB from the best existing method demonstrated by the rate-distortion curve.

</details>


### [176] [CNN-LSTM Hybrid Architecture for Over-the-Air Automatic Modulation Classification Using SDR](https://arxiv.org/abs/2511.21040)
*Dinanath Padhya,Krishna Acharya,Bipul Kumar Dahal,Dinesh Baniya Kshatri*

Main category: cs.LG

TL;DR: 提出一种基于CNN-LSTM混合架构的自动调制分类系统，结合软件定义无线电平台，在复杂时变通信信号中实现高效调制识别，在0-30dB信噪比下达到93.48%的准确率。


<details>
  <summary>Details</summary>
Motivation: 自动调制分类是未来无线通信系统的核心技术，对于认知无线电、频谱监测和智能通信网络应用至关重要，需要能够在不依赖先验知识的情况下识别调制方案。

Method: 采用CNN-LSTM混合架构，CNN负责空间特征提取，LSTM捕获时间依赖性，结合软件定义无线电平台处理实际空中信号，使用RadioML2018数据集和自定义数据集的混合数据进行训练。

Result: 优化后的模型在0-30dB信噪比范围内达到93.48%准确率、93.53%精确率、93.48%召回率和93.45% F1分数，AUC-ROC分析证实了模型在噪声条件下的判别能力。

Conclusion: 实验结果表明CNN-LSTM混合架构在自动调制分类中具有有效性，在自适应频谱管理和先进认知无线电系统中具有应用潜力。

Abstract: Automatic Modulation Classification (AMC) is a core technology for future wireless communication systems, enabling the identification of modulation schemes without prior knowledge. This capability is essential for applications in cognitive radio, spectrum monitoring, and intelligent communication networks. We propose an AMC system based on a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) architecture, integrated with a Software Defined Radio (SDR) platform. The proposed architecture leverages CNNs for spatial feature extraction and LSTMs for capturing temporal dependencies, enabling efficient handling of complex, time-varying communication signals. The system's practical ability was demonstrated by identifying over-the-air (OTA) signals from a custom-built FM transmitter alongside other modulation schemes. The system was trained on a hybrid dataset combining the RadioML2018 dataset with a custom-generated dataset, featuring samples at Signal-to-Noise Ratios (SNRs) from 0 to 30dB. System performance was evaluated using accuracy, precision, recall, F1 score, and the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). The optimized model achieved 93.48% accuracy, 93.53% precision, 93.48% recall, and an F1 score of 93.45%. The AUC-ROC analysis confirmed the model's discriminative power, even in noisy conditions. This paper's experimental results validate the effectiveness of the hybrid CNN-LSTM architecture for AMC, suggesting its potential application in adaptive spectrum management and advanced cognitive radio systems.

</details>


### [177] [FedAPA: Federated Learning with Adaptive Prototype Aggregation Toward Heterogeneous Wi-Fi CSI-based Crowd Counting](https://arxiv.org/abs/2511.21048)
*Jingtao Guo,Yuyi Mao,Ivan Wang-Hei Ho*

Main category: cs.LG

TL;DR: FedAPA是一种基于Wi-Fi CSI的联邦学习感知算法，通过自适应原型聚合策略解决数据异构性问题，在分布式人群计数场景中显著提升了准确性和通信效率。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI感知技术需要大量站点特定训练数据，限制了大规模部署。联邦学习虽然避免原始数据共享，但面临数据异构和设备资源差异的挑战。

Method: 提出自适应原型聚合(APA)策略，基于相似性为对等原型分配权重，实现自适应客户端贡献；采用混合目标函数，结合分类学习和表示对比学习来对齐本地与全局知识。

Result: 在6个环境、最多20人的真实分布式Wi-Fi人群计数场景中，FedAPA相比基线方法在准确率提升至少9.65%，F1分数提升9%，MAE降低0.29，通信开销减少95.94%。

Conclusion: FedAPA通过自适应原型聚合和混合学习目标，有效解决了联邦Wi-Fi感知中的数据异构问题，显著提升了感知性能和通信效率。

Abstract: Wi-Fi channel state information (CSI)-based sensing provides a non-invasive, device-free approach for tasks such as human activity recognition and crowd counting, but large-scale deployment is hindered by the need for extensive site-specific training data. Federated learning (FL) offers a way to avoid raw data sharing but is challenged by heterogeneous sensing data and device resources. This paper proposes FedAPA, a collaborative Wi-Fi CSI-based sensing algorithm that uses adaptive prototype aggregation (APA) strategy to assign similarity-based weights to peer prototypes, enabling adaptive client contributions and yielding a personalized global prototype for each client instead of a fixed-weight aggregation. During local training, we adopt a hybrid objective that combines classification learning with representation contrastive learning to align local and global knowledge. We provide a convergence analysis of FedAPA and evaluate it in a real-world distributed Wi-Fi crowd counting scenario with six environments and up to 20 people. The results show that our method outperform multiple baselines in terms of accuracy, F1 score, mean absolute error (MAE), and communication overhead, with FedAPA achieving at least a 9.65% increase in accuracy, a 9% gain in F1 score, a 0.29 reduction in MAE, and a 95.94% reduction in communication overhead.

</details>


### [178] [Efficient Diffusion Planning with Temporal Diffusion](https://arxiv.org/abs/2511.21054)
*Jiaming Guo,Rui Zhang,Zerun Li,Yunkai Gao,Shaohui Peng,Siming Lan,Xing Hu,Zidong Du,Xishan Zhang,Ling Li*

Main category: cs.LG

TL;DR: 提出Temporal Diffusion Planner (TDP)，通过在时间维度上分布去噪步骤来提高决策效率，相比每步重新生成规划的方法，将决策频率提升11-24.8倍，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划方法每步重新生成规划导致计算开销大、决策频率低，且频繁切换规划可能影响性能。受人类规划方式启发（详细短期规划和模糊长期规划），希望开发更高效的规划方法。

Method: TDP首先生成随时间逐渐模糊的初始规划，后续时间步仅用少量去噪步骤更新前一步规划而非完全重新生成，并引入自动重规划机制防止规划与现实偏差过大。

Result: 在D4RL基准测试中，相比每步重新规划的方法，TDP将决策频率提升11-24.8倍，同时达到更高或相当的性能。

Conclusion: TDP通过时间维度分布去噪步骤和增量更新策略，显著提高了扩散规划的决策效率，为高效离线强化学习提供了新思路。

Abstract: Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.

</details>


### [179] [Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning](https://arxiv.org/abs/2511.21075)
*Zhenchao Tang,Fang Wang,Haohuai He,Jiale Zhou,Tianxu Lv,Jun Zhu,Shouzhi Chen,Minghao Yang,Yu Wang,Jiayang Wu,Yidong Song,Jianhua Yao*

Main category: cs.LG

TL;DR: 提出了平衡微调(BFT)方法，通过两层加权机制解决生物医学领域LLM训练中的过拟合和稀疏数据问题，无需外部奖励信号即可学习复杂推理


<details>
  <summary>Details</summary>
Motivation: 现有方法在生物医学领域存在局限性：SFT容易过拟合表面指令模式而无法内化碎片化科学知识；RL因需要实验验证奖励而不可行

Method: BFT采用两层加权机制：1) 令牌级别通过预测概率缩放损失以稳定梯度；2) 样本级别使用"最小组置信度"自适应增强困难样本学习

Result: BFT显著优于SFT，在医学任务中学习到SFT遗漏的知识，在生物学任务中超越GeneAgent，其文本嵌入可直接用于下游任务

Conclusion: BFT促进了LLM在生物医学研究中的广泛应用

Abstract: Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses "minimum group confidence" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.

</details>


### [180] [Deceptron: Learned Local Inverses for Fast and Stable Physics Inversion](https://arxiv.org/abs/2511.21076)
*Aaditya L. Kachhadiya*

Main category: cs.LG

TL;DR: 提出Deceptron模块和D-IPG方法，通过局部逆映射和Jacobian组合惩罚来改善病态逆问题的求解效率，在热传导和阻尼振荡器问题上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 物理科学中的逆问题通常在输入空间是病态的，导致步长敏感且收敛缓慢，需要更高效的求解方法。

Method: Deceptron双向模块学习可微分前向代理的局部逆映射，结合监督拟合、前向-反向一致性、谱惩罚、软偏置约束和Jacobian组合惩罚(JCP)。D-IPG在输出空间下降，通过g拉回并投影。

Result: 在Heat-1D初值恢复和阻尼振荡器逆问题上，D-IPG达到固定归一化容差所需的迭代次数比投影梯度少约20倍(Heat)和2-3倍(Oscillator)，与Gauss-Newton方法竞争力相当。

Conclusion: Deceptron和D-IPG方法能有效加速病态逆问题的求解，Jacobian组合惩罚减少组合误差并跟踪迭代收益，DeceptronNet(v0)在严格公平协议下展示快速收敛。

Abstract: Inverse problems in the physical sciences are often ill-conditioned in input space, making progress step-size sensitive. We propose the Deceptron, a lightweight bidirectional module that learns a local inverse of a differentiable forward surrogate. Training combines a supervised fit, forward-reverse consistency, a lightweight spectral penalty, a soft bias tie, and a Jacobian Composition Penalty (JCP) that encourages $J_g(f(x))\,J_f(x)\!\approx\!I$ via JVP/VJP probes. At solve time, D-IPG (Deceptron Inverse-Preconditioned Gradient) takes a descent step in output space, pulls it back through $g$, and projects under the same backtracking and stopping rules as baselines. On Heat-1D initial-condition recovery and a Damped Oscillator inverse problem, D-IPG reaches a fixed normalized tolerance with $\sim$20$\times$ fewer iterations on Heat and $\sim$2-3$\times$ fewer on Oscillator than projected gradient, competitive in iterations and cost with Gauss-Newton. Diagnostics show JCP reduces a measured composition error and tracks iteration gains. We also preview a single-scale 2D instantiation, DeceptronNet (v0), that learns few-step corrections under a strict fairness protocol and exhibits notably fast convergence.

</details>


### [181] [MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts](https://arxiv.org/abs/2511.21089)
*Ivan Novikov*

Main category: cs.LG

TL;DR: MLPMoE是一种无需训练、确定性的转换方法，将Transformer块中的密集MLP重构为静态、高基数的专家混合结构，通过简单的张量切片和求和操作实现，无需梯度、校准集或路由器训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为密集Transformer部署时计算效率低下，因为推理成本随参数数量线性增长。现有方法依赖聚类、激活分析或自定义路由，需要校准数据，而MLPMoE旨在提供无需训练的高效转换。

Method: 使用张量切片和求和将密集MLP转换为静态专家混合结构，引入分形衰减（差异分支稀疏性）和补偿剪枝（方差保持分支减少）作为轻量级结构化稀疏机制。

Result: 在Qwen2.5-0.5B和DeepSeek-R1-Distill-Llama-8B上，零样本MLPMoE转换使代理困惑度指标变化小于0.05%，同时保持参数数量基本不变。在8B模型上，差异稀疏性移除约20%的MLP参数，同时保持困惑度在密集基线的约2%以内。

Conclusion: MLPMoE提供了一种无需训练、无需校准的Transformer MLP高效转换方法，显著减少计算成本同时保持模型性能。

Abstract: Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1

</details>


### [182] [MNM : Multi-level Neuroimaging Meta-analysis with Hyperbolic Brain-Text Representations](https://arxiv.org/abs/2511.21092)
*Seunghun Baek,Jaejin Lee,Jaeyoon Sim,Minjae Jeong,Won Hwa Kim*

Main category: cs.LG

TL;DR: 提出了一种利用双曲几何连接神经科学文献和脑激活图的新框架，通过将研究论文文本和对应的脑图像嵌入到共享的双曲空间中，实现多层次神经影像元分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于关键词检索或线性映射的元分析方法往往忽略了大脑中丰富的层次结构，无法有效处理小样本神经影像研究的可靠性问题。

Method: 使用Lorentz模型将文本和脑图像嵌入到共享双曲空间，通过三个步骤实现多层次神经影像元分析：1) 对齐脑和文本嵌入以获得语义对应；2) 引导文本和脑激活之间的层次关系；3) 保持脑激活模式中的层次关系。

Result: 实验结果表明，该模型优于基线方法，提供了一个稳健且可解释的多层次神经影像元分析范式。

Conclusion: 双曲脑-文本表示为神经影像元分析提供了一个有效框架，能够同时捕捉语义相似性和神经影像数据中固有的层次组织。

Abstract: Various neuroimaging studies suffer from small sample size problem which often limit their reliability. Meta-analysis addresses this challenge by aggregating findings from different studies to identify consistent patterns of brain activity. However, traditional approaches based on keyword retrieval or linear mappings often overlook the rich hierarchical structure in the brain. In this work, we propose a novel framework that leverages hyperbolic geometry to bridge the gap between neuroscience literature and brain activation maps. By embedding text from research articles and corresponding brain images into a shared hyperbolic space via the Lorentz model, our method captures both semantic similarity and hierarchical organization inherent in neuroimaging data. In the hyperbolic space, our method performs multi-level neuroimaging meta-analysis (MNM) by 1) aligning brain and text embeddings for semantic correspondence, 2) guiding hierarchy between text and brain activations, and 3) preserving the hierarchical relationships within brain activation patterns. Experimental results demonstrate that our model outperforms baselines, offering a robust and interpretable paradigm of multi-level neuroimaging meta-analysis via hyperbolic brain-text representation.

</details>


### [183] [Generative Early Stage Ranking](https://arxiv.org/abs/2511.21095)
*Juhee Hong,Meng Liu,Shengzhi Wang,Xiaoheng Mao,Huihui Cheng,Leon Gao,Christopher Leung,Jin Zhou,Chandra Mouli Sekar,Zhao Zhu,Ruochen Liu,Tuan Trieu,Dawei Sun,Jeet Kanjani,Rui Li,Jing Qian,Xuan Cao,Minjie Fan,Mingze Gao*

Main category: cs.LG

TL;DR: 提出了生成式早期排序(GESR)范式，通过混合注意力(MoA)模块解决用户-物品解耦方法的局限性，包括硬匹配注意力、目标感知自注意力和交叉注意力模块，并通过多逻辑参数化门控(MLPG)模块进行最终融合。


<details>
  <summary>Details</summary>
Motivation: 传统早期排序系统采用用户-物品解耦方法，虽然高效但无法捕捉细粒度用户-物品亲和度和交叉信号，限制了排序效果。

Method: 引入混合注意力(MoA)模块：硬匹配注意力编码显式交叉信号，目标感知自注意力生成目标感知用户表示，交叉注意力促进用户-物品特征早期交互；使用MLPG模块整合新学习嵌入并融合主次逻辑。

Result: 在线和离线实验验证了在关键指标、参与度和消费任务上的显著改进，成功在大规模ESR阶段部署完整的目标感知注意力序列建模。

Conclusion: GESR范式通过混合注意力机制有效弥补了早期排序系统的效果差距，同时通过优化技术解决了效率和延迟挑战。

Abstract: Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the "user-item decoupling" approach, where independently learned user and item representations are only combined at the final layer. While efficient, this design is limited in effectiveness, as it struggles to capture fine-grained user-item affinities and cross-signals. To address these, we propose the Generative Early Stage Ranking (GESR) paradigm, introducing the Mixture of Attention (MoA) module which leverages diverse attention mechanisms to bridge the effectiveness gap: the Hard Matching Attention (HMA) module encodes explicit cross-signals by computing raw match counts between user and item features; the Target-Aware Self Attention module generates target-aware user representations conditioned on the item, enabling more personalized learning; and the Cross Attention modules facilitate early and more enriched interactions between user-item features. MoA's specialized attention encodings are further refined in the final layer through a Multi-Logit Parameterized Gating (MLPG) module, which integrates the newly learned embeddings via gating and produces secondary logits that are fused with the primary logit. To address the efficiency and latency challenges, we have introduced a comprehensive suite of optimization techniques. These span from custom kernels that maximize the capabilities of the latest hardware to efficient serving solutions powered by caching mechanisms. The proposed GESR paradigm has shown substantial improvements in topline metrics, engagement, and consumption tasks, as validated by both offline and online experiments. To the best of our knowledge, this marks the first successful deployment of full target-aware attention sequence modeling within an ESR stage at such a scale.

</details>


### [184] [From Bits to Rounds: Parallel Decoding with Exploration for Diffusion Language Models](https://arxiv.org/abs/2511.21103)
*Hengyu Fu,Baihe Huang,Virginia Adams,Charles Wang,Venkat Srinivasan,Jiantao Jiao*

Main category: cs.LG

TL;DR: 本文分析了扩散语言模型解码策略的信息瓶颈问题，提出了Explore-Then-Exploit策略来优化解码效率。


<details>
  <summary>Details</summary>
Motivation: 标准DLM解码策略依赖高置信度token，但存在信息理论瓶颈，限制了解码进度并减慢了生成速度。高概率token携带的信息量很少，严格依赖它们会限制每轮解码的有效进展。

Method: 提出了Explore-Then-Exploit (ETE)解码策略，结合跨块解码和针对高不确定性token的探索，重塑条件分布并触发级联的自信预测。

Result: 实验验证了理论边界，证明ETE相比仅依赖置信度的基线方法，在不影响生成质量的前提下持续减少了所需的解码轮数。

Conclusion: 证明了解码轮数必须与样本总信息量线性增长，与每轮信息预算成反比，建立了比特到轮数的原则。ETE策略通过最大化信息吞吐量和解码效率来克服信息瓶颈。

Abstract: Diffusion Language Models (DLMs) have recently emerged as a strong alternative to autoregressive language models (LMs). DLMs offer comparable accuracy with faster inference speed via parallel decoding. However, standard DLM decoding strategies relying on high-confidence tokens encounter an inherent information-theoretic bottleneck that restricts decoding progress and ultimately slows generation. We demonstrate both theoretically and empirically that prioritizing high-confidence tokens is inherently inefficient. High-probability tokens carry negligible information and strictly relying on them limits the effective progress made in each decoding round. We prove that the number of decoding rounds must grow linearly with the sample's total information (negative log-likelihood) and inversely with the per-round information budget, establishing a bits-to-rounds principle. We also propose Explore-Then-Exploit (ETE), a training-free decoding strategy that maximizes information throughput and decoding efficiency. ETE combines cross-block decoding with targeted exploration of high-uncertainty tokens to reshape the conditional distribution and trigger cascades of confident predictions. Experiments verify our theoretical bounds and demonstrate that ETE consistently reduces the required number of decoding rounds compared to confidence-only baselines without compromising generation quality.

</details>


### [185] [BRIDGE: Building Representations In Domain Guided Program Verification](https://arxiv.org/abs/2511.21104)
*Robert Joseph George,Carson Eisenach,Udaya Ghai,Dominique Perrault-Joncas,Anima Anandkumar,Dean Foster*

Main category: cs.LG

TL;DR: BRIDGE提出了一种结构化提示方法，通过将程序验证分解为代码、规范和证明三个领域，显著提升了验证程序生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在程序验证（特别是在Lean4等交互式证明框架中）方面存在困难。主要挑战在于可扩展性：验证合成不仅需要代码，还需要精确的规范和正确性证明，而现有方法很少能同时涵盖这三个领域。

Method: BRIDGE将验证分解为三个相互关联的领域：代码（可执行实现）、规范（形式化意图声明）和证明（构造性正确性论证）。通过引导不同的推理行为（功能性、规范驱动和证明导向）作为中间表示，保留语义结构并连接这些领域。

Result: 功能性推理将形式语言（Lean4）中代码的正确性提高了近1.5倍（pass@5），推理效率提高了2倍，用更少的生成和更低的采样预算实现了更高的通过率。规范驱动提示将Python编码通过率提高了17.5%。

Conclusion: 结构化领域对齐是推进验证合成的有前景方向。BRIDGE为通过专家迭代或RLVR进行训练奠定了基础，使模型能够内化这些跨代码、规范和证明的推理策略。

Abstract: Large language models (LLMs) have achieved impressive results in code generation, yet struggle with program verification, especially in interactive proof frameworks such as Lean4. A central challenge is scalability: verified synthesis requires not just code, but also precise specifications and correctness proofs, and existing approaches rarely span all three domains. We present BRIDGE, the first systematic study of structured prompting for scalable verified program generation. BRIDGE decomposes verification into three interconnected domains: Code (executable implementations), Specifications (formal intent statements), and Proofs (constructive correctness arguments). Our key idea is to elicit distinct reasoning behaviors functional, specification-driven, and proof-oriented as intermediate representations that preserve semantic structure and connect these domains. Through systematic ablations, we show that this approach substantially improves both accuracy and efficiency beyond standard error feedback methods. For example, functional reasoning improves correctness of code in formal languages (Lean4) by nearly 1.5x (pass@5) over direct baselines. In inference-time compute, functional reasoning is also 2x more efficient, achieving higher pass rates with fewer generations and lower total sampling budgets. Similarly, we find that specification-driven prompting boosts Python coding pass rates by up to 17.5%. These findings suggest that structured domain alignment is a promising direction for advancing verified synthesis. BRIDGE establishes a foundation for training via expert iteration or RLVR, enabling models to internalize these reasoning strategies across code, specifications, and proofs.

</details>


### [186] [Dynamic Stratified Contrastive Learning with Upstream Augmentation for MILP Branching](https://arxiv.org/abs/2511.21107)
*Tongkai Lu,Shuai Ma,Chongyang Tao*

Main category: cs.LG

TL;DR: 提出了一个动态分层对比训练框架（Dynamic Stratified Contrastive Training Framework）来解决MILP分支问题中的语义变化、上游节点稀缺和强分支样本收集成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经网络的分支策略在处理MILP时面临三个主要挑战：跨深度的语义变化、上游节点稀缺以及强分支样本收集成本高昂。

Method: 通过基于特征分布对分支定界节点进行分组，训练GCNN判别模型逐步分离不同组的节点，学习更细粒度的节点表示。引入上游增强的MILP推导过程生成理论等价和扰动实例来解决数据稀缺和不平衡问题。

Result: 在标准MILP基准测试上的广泛实验表明，该方法显著提高了分支准确性，减少了求解时间，并能有效泛化到未见过的实例。

Conclusion: 所提出的动态分层对比训练框架能够有效建模节点间的细微语义差异，显著提升分支准确性和求解效率，特别是对上游节点效果更佳。

Abstract: Mixed Integer Linear Programming (MILP) is a fundamental class of NP-hard problems that has garnered significant attention from both academia and industry. The Branch-and-Bound (B\&B) method is the dominant approach for solving MILPs and the branching plays an important role in B\&B methods. Neural-based learning frameworks have recently been developed to enhance branching policies and the efficiency of solving MILPs. However, these methods still struggle with semantic variation across depths, the scarcity of upstream nodes, and the costly collection of strong branching samples. To address these issues, we propose \ours, a Dynamic \underline{\textbf{S}}tratified \underline{\textbf{C}}ontrastive Training Framework for \underline{\textbf{MILP}} Branching. It groups branch-and-bound nodes based on their feature distributions and trains a GCNN-based discriminative model to progressively separate nodes across groups, learning finer-grained node representations throughout the tree. To address data scarcity and imbalance at upstream nodes, we introduce an upstream-augmented MILP derivation procedure that generates both theoretically equivalent and perturbed instances. \ours~effectively models subtle semantic differences between nodes, significantly enhancing branching accuracy and solving efficiency, particularly for upstream nodes. Extensive experiments on standard MILP benchmarks demonstrate that our method enhances branching accuracy, reduces solving time, and generalizes effectively to unseen instances.

</details>


### [187] [Interpretable Fair Clustering](https://arxiv.org/abs/2511.21109)
*Mudi Jiang,Jiahui Zhou,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出了一种可解释的公平聚类框架，通过将公平约束集成到决策树结构中，在保证聚类公平性的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有公平聚类方法缺乏可解释性，限制了在高风险场景中的应用，需要理解聚类决策背后的原理。

Method: 构建可解释的决策树来划分数据，同时确保受保护群体间的公平对待；还提出了无需公平超参数调优的变体，通过对无公平约束构建的树进行后剪枝实现。

Result: 在真实世界和合成数据集上的广泛实验表明，该方法不仅提供有竞争力的聚类性能和改善的公平性，还具有可解释性、处理多个敏感属性的能力等额外优势。

Conclusion: 该方法在复杂公平约束下表现稳健，为公平透明的聚类开辟了新可能性。

Abstract: Fair clustering has gained increasing attention in recent years, especially in applications involving socially sensitive attributes. However, existing fair clustering methods often lack interpretability, limiting their applicability in high-stakes scenarios where understanding the rationale behind clustering decisions is essential. In this work, we address this limitation by proposing an interpretable and fair clustering framework, which integrates fairness constraints into the structure of decision trees. Our approach constructs interpretable decision trees that partition the data while ensuring fair treatment across protected groups. To further enhance the practicality of our framework, we also introduce a variant that requires no fairness hyperparameter tuning, achieved through post-pruning a tree constructed without fairness constraints. Extensive experiments on both real-world and synthetic datasets demonstrate that our method not only delivers competitive clustering performance and improved fairness, but also offers additional advantages such as interpretability and the ability to handle multiple sensitive attributes. These strengths enable our method to perform robustly under complex fairness constraints, opening new possibilities for equitable and transparent clustering.

</details>


### [188] [Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination](https://arxiv.org/abs/2511.21118)
*Pius Onobhayedo,Paul Osemudiame Oamen*

Main category: cs.LG

TL;DR: 本文提出了一个解决联邦学习系统中四大挑战的框架：通过加密收据确保聚合正确性、几何新颖性测量防止激励操纵、并行对象所有权实现线性扩展、时间锁定策略防止追溯操纵。


<details>
  <summary>Details</summary>
Motivation: 人工智能正从集中式向分布式发展，但联邦学习的民主化愿景因聚合器缺乏问责、经济机制易被操纵、状态修改串行化限制扩展性、治理允许追溯操纵等四大挑战而未能实现。

Method: 采用加密收据证明聚合正确性，几何新颖性测量防止激励游戏，并行对象所有权实现线性扩展，时间锁定策略检查追溯操纵。

Result: 提出的框架解决了联邦学习系统中的关键挑战，为分布式AI的民主化愿景提供了技术基础。

Conclusion: 该工作填补了联邦学习系统的关键组合性空白，为实现大规模边缘设备安全协作贡献了重要技术方案。

Abstract: Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large models.Eventually, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.

</details>


### [189] [Learning Cell-Aware Hierarchical Multi-Modal Representations for Robust Molecular Modeling](https://arxiv.org/abs/2511.21120)
*Mengran Li,Zelin Zang,Wenbin Xing,Junzhou Chen,Ronghui Zhang,Jiebo Luo,Stan Z. Li*

Main category: cs.LG

TL;DR: CHMR是一个细胞感知的层次多模态表示框架，通过联合建模分子与细胞响应之间的局部-全局依赖关系，并利用树形向量量化模块捕获潜在生物层次结构，显著提升了分子属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注化学结构，忽略了细胞响应（如形态和基因表达）对药物效应的重要影响。当前细胞感知方法面临两个关键限制：外部生物数据的模态不完整性，以及跨分子、细胞和基因组层次依赖关系建模不足。

Method: 提出CHMR框架，联合建模分子与细胞响应之间的局部-全局依赖关系，通过新颖的树形向量量化模块捕获潜在生物层次结构。

Result: 在9个公共基准测试的728个任务中，CHMR优于最先进的基线方法，分类任务平均提升3.6%，回归任务平均提升17.2%。

Conclusion: 结果表明层次感知的多模态学习在构建可靠且基于生物学的分子表示方面具有优势，为整合生物医学建模提供了一个通用框架。

Abstract: Understanding how chemical perturbations propagate through biological systems is essential for robust molecular property prediction. While most existing methods focus on chemical structures alone, recent advances highlight the crucial role of cellular responses such as morphology and gene expression in shaping drug effects. However, current cell-aware approaches face two key limitations: (1) modality incompleteness in external biological data, and (2) insufficient modeling of hierarchical dependencies across molecular, cellular, and genomic levels. We propose CHMR (Cell-aware Hierarchical Multi-modal Representations), a robust framework that jointly models local-global dependencies between molecules and cellular responses and captures latent biological hierarchies via a novel tree-structured vector quantization module. Evaluated on nine public benchmarks spanning 728 tasks, CHMR outperforms state-of-the-art baselines, yielding average improvements of 3.6% on classification and 17.2% on regression tasks. These results demonstrate the advantage of hierarchy-aware, multimodal learning for reliable and biologically grounded molecular representations, offering a generalizable framework for integrative biomedical modeling. The code is in https://github.com/limengran98/CHMR.

</details>


### [190] [BanglaASTE: A Novel Framework for Aspect-Sentiment-Opinion Extraction in Bangla E-commerce Reviews Using Ensemble Deep Learning](https://arxiv.org/abs/2511.21381)
*Ariful Islam,Md Rifat Hossen,Abir Ahmed,B M Taslimul Haque*

Main category: cs.LG

TL;DR: 提出了BanglaASTE框架，这是首个针对孟加拉语的方面情感三元组提取系统，通过混合分类方法和集成模型显著提升了孟加拉语情感分析的性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语的方面情感分析研究严重不足，缺乏全面的数据集和专门的三元组提取框架，这限制了在电子商务和社交媒体领域的应用。

Method: 创建首个孟加拉语ASTE数据集；开发基于图的方面-观点匹配与语义相似度的混合分类框架；实现结合BanglaBERT上下文嵌入和XGBoost的集成模型。

Result: 集成方法达到89.9%准确率和89.1% F1分数，在所有评估指标上显著优于基线模型，有效解决了孟加拉语文本处理中的非正式表达、拼写变体和数据稀疏性等挑战。

Conclusion: 该研究推进了低资源语言情感分析的技术水平，为孟加拉语电子商务分析应用提供了可扩展的解决方案。

Abstract: Aspect-Based Sentiment Analysis (ABSA) has emerged as a critical tool for extracting fine-grained sentiment insights from user-generated content, particularly in e-commerce and social media domains. However, research on Bangla ABSA remains significantly underexplored due to the absence of comprehensive datasets and specialized frameworks for triplet extraction in this language. This paper introduces BanglaASTE, a novel framework for Aspect Sentiment Triplet Extraction (ASTE) that simultaneously identifies aspect terms, opinion expressions, and sentiment polarities from Bangla product reviews. Our contributions include: (1) creation of the first annotated Bangla ASTE dataset containing 3,345 product reviews collected from major e-commerce platforms including Daraz, Facebook, and Rokomari; (2) development of a hybrid classification framework that employs graph-based aspect-opinion matching with semantic similarity techniques; and (3) implementation of an ensemble model combining BanglaBERT contextual embeddings with XGBoost boosting algorithms for enhanced triplet extraction performance. Experimental results demonstrate that our ensemble approach achieves superior performance with 89.9% accuracy and 89.1% F1-score, significantly outperforming baseline models across all evaluation metrics. The framework effectively addresses key challenges in Bangla text processing including informal expressions, spelling variations, and data sparsity. This research advances the state-of-the-art in low-resource language sentiment analysis and provides a scalable solution for Bangla e-commerce analytics applications.

</details>


### [191] [Privacy in Federated Learning with Spiking Neural Networks](https://arxiv.org/abs/2511.21181)
*Dogukan Aksu,Jesus Martinez del Rincon,Ihsen Alouani*

Main category: cs.LG

TL;DR: 本文首次系统研究了SNN中的梯度泄露问题，发现与传统ANN相比，SNN的梯度泄露攻击只能产生噪声大、时间不一致的重建结果，无法恢复有意义的空间或时间结构，表明SNN具有固有的隐私保护潜力。


<details>
  <summary>Details</summary>
Motivation: SNN在嵌入式AI中具有低功耗优势，联邦学习(FL)是此类场景的主流训练范式。然而梯度反转攻击对FL构成严重隐私威胁，虽然该漏洞在传统ANN中已被广泛研究，但在SNN中的影响尚不明确。

Method: 将不同的梯度泄露攻击方法适配到脉冲域，通过实证研究比较SNN和ANN在梯度泄露方面的差异，重点关注SNN的非可微分特性和替代梯度训练方法。

Result: 实验结果显示与传统ANN形成鲜明对比：ANN梯度能可靠暴露显著输入内容，而SNN梯度只能产生噪声大、时间不一致的重建结果，无法恢复有意义的空间或时间结构。

Conclusion: 事件驱动动态和替代梯度训练的结合显著降低了梯度的信息量，SNN具有固有的隐私保护潜力，为神经形态计算提供了新的安全优势。

Abstract: Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.

</details>


### [192] [I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation](https://arxiv.org/abs/2511.21208)
*Lucas Thil,Jesse Read,Rim Kaddah,Guillaume Doquet*

Main category: cs.LG

TL;DR: 本文提出了一种新的健康指标构建框架，首次将RaPP作为RUL预测的健康指标，结合不确定性量化和指标组方法，显著提升了预测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在多传感器系统中解耦复杂退化机制，也无法量化健康指标可靠性的不确定性，这限制了剩余使用寿命预测的准确性。

Method: 1) 首次将RaPP作为RUL预测的健康指标；2) 通过蒙特卡洛dropout和概率潜在空间增强不确定性量化；3) 提出指标组方法I-GLIDE，隔离传感器子集来建模系统特定退化。

Result: 在航空航天和制造系统数据上的评估表明，该方法在准确性和泛化性方面相比现有方法有显著提升，同时提供了对系统故障路径的可操作见解。

Conclusion: 这项工作弥合了异常检测与预测之间的差距，为复杂系统中的不确定性感知退化建模提供了一个原则性框架。

Abstract: Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.

</details>


### [193] [Subjective Depth and Timescale Transformers: Learning Where and When to Compute](https://arxiv.org/abs/2511.21408)
*Frederico Wieser,Martin Benfeghoul,Haitham Bou Ammar,Jun Wang,Zafeirios Fountas*

Main category: cs.LG

TL;DR: 提出了两种基于贝叶斯惊喜信号的动态计算路由Transformer架构：SDT通过决策层和动态层交替实现空间计算分配，STT在时间域进行条件计算，可减少75%自注意力计算和50%KV缓存需求。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer架构的刚性统一计算分配限制了效率和可扩展性，特别是在大规模模型和长序列场景下。

Method: SDT在解码器堆栈中交替使用决策层（计算完整后验和轻量先验）和动态层（基于贝叶斯惊喜的Top-K路由）；STT通过转换网络预测残差更新，形成时间变化假设来动态执行或绕过Transformer块。

Result: 两种架构在训练过程中都表现出从新颖性驱动到预测驱动的门控转变，与基于惊喜的原则一致。在降低计算能力的同时，为条件计算的计算-精度权衡提供了初步见解。

Conclusion: 所提出的架构建立了一个灵活的效率框架，在每个计算跳过层内减少75%的自注意力计算和50%的KV缓存需求，为更高效模型铺平了道路。

Abstract: The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.

</details>


### [194] [Robust Gene Prioritization via Fast-mRMR Feature Selection in high-dimensional omics data](https://arxiv.org/abs/2511.21211)
*Rubén Fernández-Farelo,Jorge Paz-Ruza,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos,Alex A. Freitas*

Main category: cs.LG

TL;DR: 提出了一种更稳健高效的基因优先级排序流程，利用Fast-mRMR特征选择保留相关且非冗余的特征，在饮食限制数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理生物医学数据的高维度和不完整标注方面存在困难，需要更有效的基因优先级排序方法。

Method: 使用Fast-mRMR特征选择来保留相关且非冗余的特征，构建更简单有效的分类器模型，并能组合不同的生物特征集。

Result: 在饮食限制数据集上的实验显示，该方法相比现有方法有显著改进。

Conclusion: 特征选择对于可靠的基因优先级排序至关重要，该方法证明了其有效性。

Abstract: Gene prioritization (identifying genes potentially associated with a biological process) is increasingly tackled with Artificial Intelligence. However, existing methods struggle with the high dimensionality and incomplete labelling of biomedical data. This work proposes a more robust and efficient pipeline that leverages Fast-mRMR feature selection to retain only relevant, non-redundant features for classifiers. This enables us to build simpler and more effective models, as well as to combine different biological feature sets. Experiments on Dietary Restriction datasets show significant improvements over existing methods, proving that feature selection can be critical for reliable gene prioritization.

</details>


### [195] [A Physics-Informed U-net-LSTM Network for Data-Driven Seismic Response Modeling of Structures](https://arxiv.org/abs/2511.21276)
*Sutirtha Biswas,Kshitij Kumar Yadav*

Main category: cs.LG

TL;DR: 提出了一种融合物理定律与深度学习的Physics Informed U-Net LSTM框架，用于提高结构地震响应预测的准确性和效率


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法计算成本高，而纯数据驱动的深度学习模型难以泛化且无法捕捉物理规律，需要一种结合物理约束的混合方法

Method: 开发了Physics Informed U-Net LSTM框架，将特定领域约束嵌入学习过程，结合U-Net和LSTM架构

Result: 该模型在预测性能上优于传统机器学习架构，提供了更稳健和计算高效的替代方案

Conclusion: 这种混合方法弥合了纯数据驱动方法与物理建模之间的差距，为结构地震响应预测提供了新的解决方案

Abstract: Accurate and efficient seismic response prediction is essential for the design of resilient structures. While the Finite Element Method (FEM) remains the standard for nonlinear seismic analysis, its high computational demands limit its scalability and real time applicability. Recent developments in deep learning, particularly Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short Term Memory (LSTM) models, have shown promise in reducing the computational cost of nonlinear seismic analysis of structures. However, these data driven models often struggle to generalize and capture the underlying physics, leading to reduced reliability. We propose a novel Physics Informed U Net LSTM framework that integrates physical laws with deep learning to enhance both accuracy and efficiency. By embedding domain specific constraints into the learning process, the proposed model achieves improved predictive performance over conventional Machine Learning architectures. This hybrid approach bridges the gap between purely data driven methods and physics based modeling, offering a robust and computationally efficient alternative for seismic response prediction of structures.

</details>


### [196] [Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models](https://arxiv.org/abs/2511.21320)
*Heiko Oppel,Andreas Spilz,Michael Munz*

Main category: cs.LG

TL;DR: 提出了一种结合隐式扩散模型和新型锯齿采样器的方法，显著加速DDPM的采样过程，在保持生成质量的同时实现30倍速度提升


<details>
  <summary>Details</summary>
Motivation: DDPM能够生成合成时间序列数据来提升分类器性能，但其采样过程计算成本高昂，需要加速解决方案

Method: 将隐式扩散模型与新颖的锯齿采样器相结合，该采样器可加速反向过程并适用于任何预训练的扩散模型

Result: 相比标准基线实现了30倍的速度提升，同时提高了生成序列在分类任务中的质量

Conclusion: 该方法有效解决了DDPM采样效率低下的问题，在不牺牲生成质量的前提下大幅提升了采样速度

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.

</details>


### [197] [TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models](https://arxiv.org/abs/2511.21335)
*Haksoo Lim,Jaehoon Lee,Sewon Park,Minjung Kim,Noseong Park*

Main category: cs.LG

TL;DR: 基于分数的生成模型应用于时间序列合成，通过条件分数网络和定制化的去噪分数匹配损失，实现了高质量和多样性的时间序列生成。


<details>
  <summary>Details</summary>
Motivation: 受SGMs在图像生成、语音合成等领域取得的优异成果启发，希望将这种强大的生成能力扩展到时间序列数据合成领域。

Method: 提出了用于时间序列合成的条件分数网络，设计了专门的条件去噪分数匹配损失函数，框架灵活支持规则和不规则时间序列的合成。

Result: 在多个时间序列数据集上取得了优异的合成性能，实现了最先进的采样多样性和质量。

Conclusion: 基于分数的生成模型能够有效应用于时间序列合成任务，为时间序列数据生成提供了新的解决方案。

Abstract: Score-based generative models (SGMs) have demonstrated unparalleled sampling quality and diversity in numerous fields, such as image generation, voice synthesis, and tabular data synthesis, etc. Inspired by those outstanding results, we apply SGMs to synthesize time-series by learning its conditional score function. To this end, we present a conditional score network for time-series synthesis, deriving a denoising score matching loss tailored for our purposes. In particular, our presented denoising score matching loss is the conditional denoising score matching loss for time-series synthesis. In addition, our framework is such flexible that both regular and irregular time-series can be synthesized with minimal changes to our model design. Finally, we obtain exceptional synthesis performance on various time-series datasets, achieving state-of-the-art sampling diversity and quality.

</details>


### [198] [Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models](https://arxiv.org/abs/2511.21338)
*Julianna Piskorz,Cristina Pinneri,Alvaro Correia,Motasem Alfarra,Risheek Garrepalli,Christos Louizos*

Main category: cs.LG

TL;DR: 研究发现掩码扩散语言模型(MDLMs)存在两个关键限制：位置偏见和掩码干扰问题，并提出了一种掩码无关的损失函数来提升模型对上下文的处理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管MDLMs理论上应该比自回归语言模型(ARLMs)能更均匀地利用上下文，但需要验证其实际上下文理解能力是否存在局限性。

Method: 通过系统消融实验分析MDLMs的表现，并引入掩码无关损失函数来训练模型，使其预测对附加掩码数量保持不变。

Result: 发现MDLMs存在强烈的局部性偏见，且附加大量掩码会显著降低上下文理解能力。使用掩码无关损失函数微调后，模型对掩码干扰的鲁棒性得到显著提升。

Conclusion: 当前MDLM训练范式存在关键限制，研究为构建具有更强上下文理解能力的扩散语言模型提供了可行方案。

Abstract: Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.

</details>


### [199] [Best Practices for Machine Learning Experimentation in Scientific Applications](https://arxiv.org/abs/2511.21354)
*Umberto Michelucci,Francesca Venturini*

Main category: cs.LG

TL;DR: 本文提供了一个实用的结构化指南，用于在科学应用中进行机器学习实验，重点关注可重复性、公平比较和透明报告。


<details>
  <summary>Details</summary>
Motivation: 机器学习在科学研究中日益普及，但结果的质量和可靠性往往取决于实验设计和文档记录。糟糕的基线、不一致的预处理或不足的验证可能导致对模型性能的误导性结论。

Method: 提出了一个逐步的工作流程，从数据集准备到模型选择和评估，并提出了考虑过拟合和验证折叠不稳定性的指标，包括对数过拟合比率（LOR）和复合过拟合分数（COS）。

Result: 通过推荐实践和示例报告格式，这项工作支持研究人员建立稳健的基线，并从应用于科学问题的机器学习模型中得出有效的基于证据的见解。

Conclusion: 该指南旨在提高机器学习实验在科学研究中的质量和可靠性，促进可重复性和公平比较。

Abstract: Machine learning (ML) is increasingly adopted in scientific research, yet the quality and reliability of results often depend on how experiments are designed and documented. Poor baselines, inconsistent preprocessing, or insufficient validation can lead to misleading conclusions about model performance. This paper presents a practical and structured guide for conducting ML experiments in scientific applications, focussing on reproducibility, fair comparison, and transparent reporting. We outline a step-by-step workflow, from dataset preparation to model selection and evaluation, and propose metrics that account for overfitting and instability across validation folds, including the Logarithmic Overfitting Ratio (LOR) and the Composite Overfitting Score (COS). Through recommended practices and example reporting formats, this work aims to support researchers in establishing robust baselines and drawing valid evidence-based insights from ML models applied to scientific problems.

</details>


### [200] [Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance](https://arxiv.org/abs/2511.21356)
*Bram Silue,Santiago Amaya-Corredor,Patrick Mannion,Lander Willem,Pieter Libin*

Main category: cs.LG

TL;DR: 提出了Hybrid-AIRL (H-AIRL)方法，通过结合监督损失和随机正则化机制来增强对抗性逆强化学习在复杂不完全信息环境中的表现，特别是在HULHE扑克游戏中取得了更好的样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 对抗性逆强化学习(AIRL)在处理稀疏奖励问题上表现良好，但在高度复杂的不完全信息环境中的性能尚未充分探索。HULHE扑克游戏具有稀疏延迟奖励和显著不确定性的特点，是测试AIRL的理想场景。

Method: 提出H-AIRL方法，在AIRL基础上引入两个关键改进：1) 从专家数据中推导的监督损失；2) 随机正则化机制。这些改进增强了奖励推断和策略学习能力。

Result: 在Gymnasium基准测试和HULHE扑克环境中评估H-AIRL，结果显示相比AIRL，H-AIRL实现了更高的样本效率和更稳定的学习过程。通过可视化分析进一步验证了学习到的奖励函数的质量。

Conclusion: 将监督信号融入逆强化学习具有显著优势，H-AIRL为处理具有挑战性的现实世界场景提供了一个有前景的框架。

Abstract: Adversarial Inverse Reinforcement Learning (AIRL) has shown promise in addressing the sparse reward problem in reinforcement learning (RL) by inferring dense reward functions from expert demonstrations. However, its performance in highly complex, imperfect-information settings remains largely unexplored. To explore this gap, we evaluate AIRL in the context of Heads-Up Limit Hold'em (HULHE) poker, a domain characterized by sparse, delayed rewards and significant uncertainty. In this setting, we find that AIRL struggles to infer a sufficiently informative reward function. To overcome this limitation, we contribute Hybrid-AIRL (H-AIRL), an extension that enhances reward inference and policy learning by incorporating a supervised loss derived from expert data and a stochastic regularization mechanism. We evaluate H-AIRL on a carefully selected set of Gymnasium benchmarks and the HULHE poker setting. Additionally, we analyze the learned reward function through visualization to gain deeper insights into the learning process. Our experimental results show that H-AIRL achieves higher sample efficiency and more stable learning compared to AIRL. This highlights the benefits of incorporating supervised signals into inverse RL and establishes H-AIRL as a promising framework for tackling challenging, real-world settings.

</details>


### [201] [The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods](https://arxiv.org/abs/2511.21363)
*Kevin Iselborn,David Dembinsky,Adriano Lucieri,Andreas Dengel*

Main category: cs.LG

TL;DR: 提出了一种新的局部特征归因方法保真度评估指标DPC，通过结合扰动和归因方向，实现了近10倍的速度提升并消除了随机性，提供确定性评估。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险场景中，需要忠实反映模型决策过程的解释方法。现有保真度指标如Infidelity依赖蒙特卡洛近似，需要大量模型评估且引入随机采样不确定性。

Method: 在引导扰动实验中修改现有的预测变化(PC)指标，通过结合扰动和归因方向，提出定向预测变化(DPC)指标。

Result: 在2个数据集、2个黑盒模型、7种解释算法和广泛超参数范围内评估了4744个不同解释，DPC与PC结合能够对基线导向和局部特征归因方法进行全面且计算高效的评估。

Conclusion: DPC提供了确定性、可重现的评估结果，消除了现有方法的随机性，同时保持了与局部Infidelity相同的评估属性。

Abstract: The utility of an explanation method critically depends on its fidelity to the underlying machine learning model. Especially in high-stakes medical settings, clinicians and regulators require explanations that faithfully reflect the model's decision process. Existing fidelity metrics such as Infidelity rely on Monte Carlo approximation, which demands numerous model evaluations and introduces uncertainty due to random sampling. This work proposes a novel metric for evaluating the fidelity of local feature attribution methods by modifying the existing Prediction Change (PC) metric within the Guided Perturbation Experiment. By incorporating the direction of both perturbation and attribution, the proposed Directed Prediction Change (DPC) metric achieves an almost tenfold speedup and eliminates randomness, resulting in a deterministic and trustworthy evaluation procedure that measures the same property as local Infidelity. DPC is evaluated on two datasets (skin lesion images and financial tabular data), two black-box models, seven explanation algorithms, and a wide range of hyperparameters. Across $4\,744$ distinct explanations, the results demonstrate that DPC, together with PC, enables a holistic and computationally efficient evaluation of both baseline-oriented and local feature attribution methods, while providing deterministic and reproducible outcomes.

</details>


### [202] [BanglaMM-Disaster: A Multimodal Transformer-Based Deep Learning Framework for Multiclass Disaster Classification in Bangla](https://arxiv.org/abs/2511.21364)
*Ariful Islam,Md Rifat Hossen,Md. Mahmudul Arif,Abdullah Al Noman,Md Arifur Rahman*

Main category: cs.LG

TL;DR: 提出了BanglaMM-Disaster，一个基于深度学习的孟加拉语多模态灾害分类框架，结合文本和图像数据，在新建的5037条社交媒体数据集上达到83.76%准确率，比单模态方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国自然灾害频发，需要实时监测和快速响应系统，但目前缺乏针对孟加拉语的多模态灾害分析方法。

Method: 使用transformer文本编码器（BanglaBERT、mBERT、XLM-RoBERTa）和CNN骨干网络（ResNet50、DenseNet169、MobileNetV2），通过早期融合处理文本和图像两种模态。

Result: 最佳模型准确率达到83.76%，比最佳文本基线高3.84%，比图像基线高16.91%，所有类别的误分类都减少，对模糊样本有明显改进。

Conclusion: 填补了孟加拉语多模态灾害分析的关键空白，证明了在低资源环境下结合多种数据类型对实时灾害响应的益处。

Abstract: Natural disasters remain a major challenge for Bangladesh, so real-time monitoring and quick response systems are essential. In this study, we present BanglaMM-Disaster, an end-to-end deep learning-based multimodal framework for disaster classification in Bangla, using both textual and visual data from social media. We constructed a new dataset of 5,037 Bangla social media posts, each consisting of a caption and a corresponding image, annotated into one of nine disaster-related categories. The proposed model integrates transformer-based text encoders, including BanglaBERT, mBERT, and XLM-RoBERTa, with CNN backbones such as ResNet50, DenseNet169, and MobileNetV2, to process the two modalities. Using early fusion, the best model achieves 83.76% accuracy. This surpasses the best text-only baseline by 3.84% and the image-only baseline by 16.91%. Our analysis also shows reduced misclassification across all classes, with noticeable improvements for ambiguous examples. This work fills a key gap in Bangla multimodal disaster analysis and demonstrates the benefits of combining multiple data types for real-time disaster response in low-resource settings.

</details>


### [203] [Controlling changes to attention logits](https://arxiv.org/abs/2511.21377)
*Ben Anson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 提出通过为查询和键权重分配参数相关学习率来控制对数变化，解决transformer模型训练中的稳定性问题，特别是在QK归一化不适用的MLA场景中。


<details>
  <summary>Details</summary>
Motivation: 解决transformer模型中查询和键权重稳定性问题，特别是当QK归一化方法不适用时（如MLA场景），需要一种替代的稳定性控制方法。

Method: 通过为查询和键权重分配参数相关学习率来控制对数变化，避免权重过度增长。

Result: 该方法允许提高网络的基础学习率，在MLA设置中优于其他方法，在使用多头注意力时性能可与QK归一化竞争。

Conclusion: 控制对数变化是确保transformer权重稳定性的有效方法，通过参数相关学习率干预可以解决QK归一化的局限性。

Abstract: Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying normalization to queries and keys, known as `QK norm', fixes stability issues in practice, but is not always applicable. For example, QK norm is not compatible with Multi Latent Attention (MLA) because QK norm requires full materialization of queries and keys during inference, which is not done in MLA. In this paper we suggest that controlling the changes to logits is important for stability. We show that these changes are controllable by assigning parameter-dependent learning rates to the query and key weights. We find that our cheap intervention allows us to increase the base learning rate of the network, outperform other methods in the MLA setting, and achieve performance competitive with QK norm when using Multi-head Attention.

</details>


### [204] [Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data](https://arxiv.org/abs/2511.21378)
*Jungi Lee,Jungkwon Kim,Chi Zhang,Kwangsun Yoo,Seok-Joo Byun*

Main category: cs.LG

TL;DR: 提出了AAR方法，通过动态排除异常数据来处理受污染数据，在图像和表格数据集上表现优于现有方法0.041 AUROC。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法假设训练数据是纯正常数据，但实际数据往往包含异常样本。现有方法依赖固定的污染比例假设，当假设与实际不符时性能严重下降，特别是在正常与异常数据分布重叠的噪声环境中。

Method: AAR方法使用改进的z-score和高斯混合模型阈值动态排除异常，结合硬拒绝和软拒绝策略来平衡保留正常数据与排除异常数据之间的权衡。

Result: 在两个图像数据集和三十个表格数据集上的广泛实验表明，AAR比最先进方法提升了0.041 AUROC。

Conclusion: AAR提供了可扩展且可靠的解决方案，增强了对抗受污染数据集的鲁棒性，为安全和医疗等领域的实际应用铺平了道路。

Abstract: Handling contaminated data poses a critical challenge in anomaly detection, as traditional models assume training on purely normal data. Conventional methods mitigate contamination by relying on fixed contamination ratios, but discrepancies between assumed and actual ratios can severely degrade performance, especially in noisy environments where normal and abnormal data distributions overlap. To address these limitations, we propose Adaptive and Aggressive Rejection (AAR), a novel method that dynamically excludes anomalies using a modified z-score and Gaussian mixture model-based thresholds. AAR effectively balances the trade-off between preserving normal data and excluding anomalies by integrating hard and soft rejection strategies. Extensive experiments on two image datasets and thirty tabular datasets demonstrate that AAR outperforms the state-of-the-art method by 0.041 AUROC. By providing a scalable and reliable solution, AAR enhances robustness against contaminated datasets, paving the way for broader real-world applications in domains such as security and healthcare.

</details>


### [205] [SUPN: Shallow Universal Polynomial Networks](https://arxiv.org/abs/2511.21414)
*Zachary Morrow,Michael Penwarden,Brian Chen,Aurya Javeed,Akil Narayan,John D. Jakeman*

Main category: cs.LG

TL;DR: 提出了浅层通用多项式网络（SUPNs），用单层可学习系数的多项式替代除最后一层外的所有隐藏层，以更少的参数实现足够的表达能力，并在多项实验中优于DNNs和KANs。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络和Kolmogorov-Arnold网络通常需要大量可训练参数，导致网络不透明且优化空间大，容易产生泛化误差差异大的局部最小值，网络初始化对模型精度影响过大。

Method: 用单层可学习系数的多项式替换除最后一层外的所有隐藏层，结合DNNs和多项式的优势，推导出准最优SUPN参数的显式公式。

Result: 在超过13,000个训练模型的实验中，对于给定数量的可训练参数，SUPNs的逼近误差和变异性通常比DNNs和KANs低一个数量级，甚至在非光滑函数上优于多项式投影。

Conclusion: SUPNs以更少的参数实现了与最佳多项式逼近相同的收敛速度，在函数逼近任务中表现出优越的性能和稳定性。

Abstract: Deep neural networks (DNNs) and Kolmogorov-Arnold networks (KANs) are popular methods for function approximation due to their flexibility and expressivity. However, they typically require a large number of trainable parameters to produce a suitable approximation. Beyond making the resulting network less transparent, overparameterization creates a large optimization space, likely producing local minima in training that have quite different generalization errors. In this case, network initialization can have an outsize impact on the model's out-of-sample accuracy. For these reasons, we propose shallow universal polynomial networks (SUPNs). These networks replace all but the last hidden layer with a single layer of polynomials with learnable coefficients, leveraging the strengths of DNNs and polynomials to achieve sufficient expressivity with far fewer parameters. We prove that SUPNs converge at the same rate as the best polynomial approximation of the same degree, and we derive explicit formulas for quasi-optimal SUPN parameters. We complement theory with an extensive suite of numerical experiments involving SUPNs, DNNs, KANs, and polynomial projection in one, two, and ten dimensions, consisting of over 13,000 trained models. On the target functions we numerically studied, for a given number of trainable parameters, the approximation error and variability are often lower for SUPNs than for DNNs and KANs by an order of magnitude. In our examples, SUPNs even outperform polynomial projection on non-smooth functions.

</details>


### [206] [Ensemble Performance Through the Lens of Linear Independence of Classifier Votes in Data Streams](https://arxiv.org/abs/2511.21465)
*Enes Bektas,Fazli Can*

Main category: cs.LG

TL;DR: 本文研究了集成学习中集成规模与性能的关系，提出了基于分类器投票线性独立性的理论框架，并推导出达到指定线性独立概率所需集成规模的理论估计。


<details>
  <summary>Details</summary>
Motivation: 集成学习通过组合多个基分类器提高分类性能，但过大的集成会导致计算效率低下和收益递减。需要研究集成规模与性能的平衡关系。

Method: 从数据流中分类器投票的线性独立性角度分析，建立几何模型，将线性独立性重要性推广到加权多数投票问题，推导理论框架并验证于真实和合成数据集。

Result: 理论估计能有效识别OzaBagging等稳健集成的性能饱和点，但对于GOOWE等复杂加权方案，高理论多样性可能引发算法不稳定性。

Conclusion: 提出的理论框架能解释集成规模与准确性的权衡关系，为确定最优集成规模提供理论指导，但需考虑不同集成方法的特性差异。

Abstract: Ensemble learning improves classification performance by combining multiple base classifiers. While increasing the number of classifiers generally enhances accuracy, excessively large ensembles can lead to computational inefficiency and diminishing returns. This paper investigates the relationship between ensemble size and performance through the lens of linear independence among classifier votes in data streams. We propose that ensembles composed of linearly independent classifiers maximize representational capacity, particularly under a geometric model. We then generalize the importance of linear independence to the weighted majority voting problem. By modeling the probability of achieving linear independence among classifier outputs, we derive a theoretical framework that explains the trade-off between ensemble size and accuracy. Our analysis leads to a theoretical estimate of the ensemble size required to achieve a user-specified probability of linear independence. We validate our theory through experiments on both real-world and synthetic datasets using two ensemble methods, OzaBagging and GOOWE. Our results confirm that this theoretical estimate effectively identifies the point of performance saturation for robust ensembles like OzaBagging. Conversely, for complex weighting schemes like GOOWE, our framework reveals that high theoretical diversity can trigger algorithmic instability. Our implementation is publicly available to support reproducibility and future research.

</details>


### [207] [Lost in Time? A Meta-Learning Framework for Time-Shift-Tolerant Physiological Signal Transformation](https://arxiv.org/abs/2511.21500)
*Qian Hong,Cheng Bian,Xiao Zhou,Xiaoyu Li,Yelei Li,Zijing Zeng*

Main category: cs.LG

TL;DR: ShiftSyncNet是一个基于元学习的双层优化框架，用于解决多模态生理信号转换中的时间错位问题，通过自动学习时间偏移并应用傅里叶相位偏移来对齐监督信号，显著提高了转换精度。


<details>
  <summary>Details</summary>
Motivation: 将PPG和BCG等非侵入性信号转换为ABP等临床有意义信号对连续低成本健康监测至关重要，但时间错位会损害转换准确性，特别是影响关键特征如ABP峰值的捕捉。

Method: 提出ShiftSyncNet框架，包含转换网络(TransNet)和时间偏移校正网络(SyncNet)，通过元学习进行双层优化，SyncNet学习训练对之间的时间偏移并应用傅里叶相位偏移来对齐监督信号。

Result: 在一个真实工业数据集和两个公共数据集上的实验表明，ShiftSyncNet分别比强基线方法提升了9.4%、6.0%和12.8%，有效校正时间偏移、提高标签质量并增强转换精度。

Conclusion: ShiftSyncNet能有效解决多模态生理转换中的时间不一致性问题，为处理时间错位提供了统一方向，在各种错位场景下都能显著提升性能。

Abstract: Translating non-invasive signals such as photoplethysmography (PPG) and ballistocardiography (BCG) into clinically meaningful signals like arterial blood pressure (ABP) is vital for continuous, low-cost healthcare monitoring. However, temporal misalignment in multimodal signal transformation impairs transformation accuracy, especially in capturing critical features like ABP peaks. Conventional synchronization methods often rely on strong similarity assumptions or manual tuning, while existing Learning with Noisy Labels (LNL) approaches are ineffective under time-shifted supervision, either discarding excessive data or failing to correct label shifts. To address this challenge, we propose ShiftSyncNet, a meta-learning-based bi-level optimization framework that automatically mitigates performance degradation due to time misalignment. It comprises a transformation network (TransNet) and a time-shift correction network (SyncNet), where SyncNet learns time offsets between training pairs and applies Fourier phase shifts to align supervision signals. Experiments on one real-world industrial dataset and two public datasets show that ShiftSyncNet outperforms strong baselines by 9.4%, 6.0%, and 12.8%, respectively. The results highlight its effectiveness in correcting time shifts, improving label quality, and enhancing transformation accuracy across diverse misalignment scenarios, pointing toward a unified direction for addressing temporal inconsistencies in multimodal physiological transformation.

</details>


### [208] [IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference](https://arxiv.org/abs/2511.21513)
*Wanli Zhong,Haibo Feng,Zirui Zhou,Hanyang Peng,Shiqi Yu*

Main category: cs.LG

TL;DR: IntAttention是一种完全整数的注意力机制，通过IndexSoftmax操作符在整数域内实现softmax，消除了数据类型转换开销，在边缘设备上实现了显著的加速和能耗降低。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署Transformer模型受到延迟和能耗限制。虽然INT8量化加速了主要矩阵乘法，但softmax成为主要瓶颈，需要昂贵的反量化-softmax-再量化过程，占注意力总延迟的65%，破坏了端到端整数数据流。

Method: 提出IntAttention，核心是IndexSoftmax操作符，完全在整数域内替换浮点指数运算。集成稀疏感知裁剪、32条目查找表近似和直接整数归一化，消除所有数据类型转换开销。

Result: 在Armv8 CPU上，相比FP16基线实现3.7倍加速和61%能耗降低，比传统INT8注意力管道快2.0倍。在各种语言和视觉模型中保持与基线相当的高精度。

Conclusion: IntAttention实现了完全整数的注意力管道，无需重新训练，为商品边缘设备上的Transformer推理提供了实用高效的解决方案。

Abstract: Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the dominant bottleneck. This stage incurs a costly dequantize-softmax-requantize detour, which can account for up to 65% of total attention latency and disrupts the end-to-end integer dataflow critical for edge hardware efficiency. To address this limitation, we present IntAttention, the first fully integer, plug-and-play attention pipeline without retraining. At the core of our approach lies IndexSoftmax, a hardware-friendly operator that replaces floating-point exponentials entirely within the integer domain. IntAttention integrates sparsity-aware clipping, a 32-entry lookup-table approximation, and direct integer normalization, thereby eliminating all datatype conversion overhead. We evaluate IntAttention and demonstrate consistent and substantial gains. Our method achieves up to 3.7x speedup and 61% energy reduction over FP16 baselines and 2.0x faster than conventional INT8 attention pipelines on Armv8 CPUs. These gains are achieved with high-fidelity accuracy comparable to baselines across diverse language and vision models, enabling practical and efficient Transformer inference on commodity edge devices. Code will be released in later version of this work.

</details>


### [209] [Mechanistic Interpretability for Transformer-based Time Series Classification](https://arxiv.org/abs/2511.21514)
*Matīss Kalnāre,Sofoklis Kitharidis,Thomas Bäck,Niki van Stein*

Main category: cs.LG

TL;DR: 本文通过将机械可解释性技术（激活修补、注意力显著性、稀疏自编码器）从NLP领域适配到时间序列分类的Transformer架构，揭示了模型内部的因果结构和决策机制。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在时间序列分类中表现出色，但其内部决策机制难以理解。现有可解释性方法主要关注输入输出归因，对内部机制揭示不足。

Method: 采用机械可解释性技术：激活修补分析注意力头和时步的因果作用，注意力显著性识别关键组件，稀疏自编码器发现可解释的潜在特征。

Result: 在基准时间序列数据集上构建了内部信息传播的因果图，识别了驱动正确分类的关键注意力头和时步位置，并展示了稀疏自编码器发现可解释特征的能力。

Conclusion: 研究为Transformer可解释性提供了方法论贡献，并揭示了时间序列分类任务中Transformer功能机制的新见解。

Abstract: Transformer-based models have become state-of-the-art tools in various machine learning tasks, including time series classification, yet their complexity makes understanding their internal decision-making challenging. Existing explainability methods often focus on input-output attributions, leaving the internal mechanisms largely opaque. This paper addresses this gap by adapting various Mechanistic Interpretability techniques; activation patching, attention saliency, and sparse autoencoders, from NLP to transformer architectures designed explicitly for time series classification. We systematically probe the internal causal roles of individual attention heads and timesteps, revealing causal structures within these models. Through experimentation on a benchmark time series dataset, we construct causal graphs illustrating how information propagates internally, highlighting key attention heads and temporal positions driving correct classifications. Additionally, we demonstrate the potential of sparse autoencoders for uncovering interpretable latent features. Our findings provide both methodological contributions to transformer interpretability and novel insights into the functional mechanics underlying transformer performance in time series classification tasks.

</details>


### [210] [Context-Specific Causal Graph Discovery with Unobserved Contexts: Non-Stationarity, Regimes and Spatio-Temporal Patterns](https://arxiv.org/abs/2511.21537)
*Martin Rabel,Jakob Runge*

Main category: cs.LG

TL;DR: 该论文提出了一个框架，用于分析空间网格时间序列数据中因果图变化所编码的信息，通过修改基于约束的因果发现方法中的独立性测试层级，实现模块化、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据（如气候应用）通常具有空间网格时间序列结构，系统在不同时空点的行为相似但存在变化，这些变化既包含重要信息，又可能影响假设平稳性或空间平移不变性的算法的稳定性和有效性。

Method: 开发了一个基于约束的因果发现框架，在独立性测试层级进行修改，能够利用现有的因果发现方法（如PC、PC-stable、FCI、PCMCI、PCMCI+、LPCMCI），具有模块化设计，便于扩展和解决子问题。

Result: 提出了一个极其模块化、易于扩展且广泛适用的框架，能够系统理解并改进一系列子问题，同时便于理解基本限制、权衡超参数和结果的统计解释。

Conclusion: 该框架通过将复杂问题分解为更易处理的子问题，为分析因果图变化提供了系统方法，未来将提供开源实现。

Abstract: Real-world data, for example in climate applications, often consists of spatially gridded time series data or data with comparable structure. While the underlying system is often believed to behave similar at different points in space and time, those variations that do exist are twofold relevant: They often encode important information in and of themselves. And they may negatively affect the stability / convergence and reliability\Slash{}validity of results of algorithms assuming stationarity or space-translation invariance. We study the information encoded in changes of the causal graph, with stability in mind. An analysis of this general task identifies two core challenges. We develop guiding principles to overcome these challenges, and provide a framework realizing these principles by modifying constraint-based causal discovery approaches on the level of independence testing. This leads to an extremely modular, easily extensible and widely applicable framework. It can leverage existing constraint-based causal discovery methods (demonstrated on IID-algorithms PC, PC-stable, FCI and time series algorithms PCMCI, PCMCI+, LPCMCI) with little to no modification. The built-in modularity allows to systematically understand and improve upon an entire array of subproblems. By design, it can be extended by leveraging insights from change-point-detection, clustering, independence-testing and other well-studied related problems. The division into more accessible sub-problems also simplifies the understanding of fundamental limitations, hyperparameters controlling trade-offs and the statistical interpretation of results. An open-source implementation will be available soon.

</details>


### [211] [Computing Strategic Responses to Non-Linear Classifiers](https://arxiv.org/abs/2511.21560)
*Jack Geary,Boyan Gao,Henry Gouk*

Main category: cs.LG

TL;DR: 提出了一种计算战略分类中智能体最佳响应的方法，通过优化智能体目标的拉格朗日对偶，可应用于非线性分类器设置。


<details>
  <summary>Details</summary>
Motivation: 战略分类中部署分类器会引发战略行为导致分布偏移，现有方法主要局限于线性设置，但许多情况下非线性分类器更合适，主要限制在于无法计算非线性设置中的最佳响应。

Method: 通过优化智能体目标的拉格朗日对偶来计算最佳响应，该方法在非线性分类器设置中可直接应用。

Result: 方法在线性设置中能重现最佳响应，识别出现有方法的关键弱点，在非线性分类器设置中可用于评估和训练。

Conclusion: 提出的拉格朗日对偶优化方法为战略分类中的非线性分类器提供了有效的解决方案，解决了计算最佳响应的核心挑战。

Abstract: We consider the problem of strategic classification, where the act of deploying a classifier leads to strategic behaviour that induces a distribution shift on subsequent observations. Current approaches to learning classifiers in strategic settings are focused primarily on the linear setting, but in many cases non-linear classifiers are more suitable. A central limitation to progress for non-linear classifiers arises from the inability to compute best responses in these settings. We present a novel method for computing the best response by optimising the Lagrangian dual of the Agents' objective. We demonstrate that our method reproduces best responses in linear settings, identifying key weaknesses in existing approaches. We present further results demonstrating our method can be straight-forwardly applied to non-linear classifier settings, where it is useful for both evaluation and training.

</details>


### [212] [Machine Learning Approaches to Clinical Risk Prediction: Multi-Scale Temporal Alignment in Electronic Health Records](https://arxiv.org/abs/2511.21561)
*Wei-Chen Chang,Lu Dai,Ting Xu*

Main category: cs.LG

TL;DR: 提出基于多尺度时间对齐网络(MSTAN)的电子健康记录风险预测方法，通过可学习的时间对齐机制和多尺度卷积特征提取，解决EHR数据的时间不规则性、采样间隔差异和多尺度动态依赖问题。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录中时间不规则性、采样间隔差异和多尺度动态依赖的挑战，提升临床风险预测的准确性。

Method: 使用可学习时间对齐机制和多尺度卷积特征提取结构，通过时间嵌入和对齐模块动态加权不规则采样数据，多尺度特征提取模块捕获不同时间粒度的关键模式，注意力聚合机制整合全局时间依赖。

Result: 在公开EHR数据集上的实验表明，该模型在准确率、召回率、精确率和F1分数上优于主流基线方法。

Conclusion: 多尺度时间对齐在复杂医疗时间序列分析中具有有效性和鲁棒性，为高维异步医疗序列的智能表示提供了新解决方案，为EHR驱动的临床风险预测提供重要技术支持。

Abstract: This study proposes a risk prediction method based on a Multi-Scale Temporal Alignment Network (MSTAN) to address the challenges of temporal irregularity, sampling interval differences, and multi-scale dynamic dependencies in Electronic Health Records (EHR). The method focuses on temporal feature modeling by introducing a learnable temporal alignment mechanism and a multi-scale convolutional feature extraction structure to jointly model long-term trends and short-term fluctuations in EHR sequences. At the input level, the model maps multi-source clinical features into a unified high-dimensional semantic space and employs temporal embedding and alignment modules to dynamically weight irregularly sampled data, reducing the impact of temporal distribution differences on model performance. The multi-scale feature extraction module then captures key patterns across different temporal granularities through multi-layer convolution and hierarchical fusion, achieving a fine-grained representation of patient states. Finally, an attention-based aggregation mechanism integrates global temporal dependencies to generate individual-level risk representations for disease risk prediction and health status assessment. Experiments conducted on publicly available EHR datasets show that the proposed model outperforms mainstream baselines in accuracy, recall, precision, and F1-Score, demonstrating the effectiveness and robustness of multi-scale temporal alignment in complex medical time-series analysis. This study provides a new solution for intelligent representation of high-dimensional asynchronous medical sequences and offers important technical support for EHR-driven clinical risk prediction.

</details>


### [213] [A decoupled alignment kernel for peptide membrane permeability predictions](https://arxiv.org/abs/2511.21566)
*Ali Amirahmadi,Gökçe Geylan,Leonardo De Maria,Farzaneh Etminani,Mattias Ohlsson,Alessandro Tibo*

Main category: cs.LG

TL;DR: 提出了两种基于核方法的循环肽细胞膜渗透性预测模型：MD-GAK和PMD-GAK，通过解耦全局对齐核结合化学意义的残基相似性，在不确定性估计方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 循环肽是靶向细胞内位点的有前景方式，但细胞膜渗透性仍是关键瓶颈，现有公共数据有限且需要良好校准的不确定性估计。

Method: 提出单体感知解耦全局对齐核（MD-GAK），将化学意义的残基-残基相似性与序列对齐耦合，同时将局部匹配与间隙惩罚解耦。还引入变体PMD-GAK，包含三角位置先验。使用高斯过程作为预测模型。

Result: 通过广泛实验证明，该方法在所有指标上均优于现有最先进模型，PMD-GAK在减少校准误差方面具有额外优势。

Conclusion: 提出的MD-GAK和PMD-GAK框架在循环肽细胞膜渗透性预测中表现出色，特别是在不确定性估计方面优于复杂深度学习架构。

Abstract: Cyclic peptides are promising modalities for targeting intracellular sites; however, cell-membrane permeability remains a key bottleneck, exacerbated by limited public data and the need for well-calibrated uncertainty. Instead of relying on data-eager complex deep learning architecture, we propose a monomer-aware decoupled global alignment kernel (MD-GAK), which couples chemically meaningful residue-residue similarity with sequence alignment while decoupling local matches from gap penalties. MD-GAK is a relatively simple kernel. To further demonstrate the robustness of our framework, we also introduce a variant, PMD-GAK, which incorporates a triangular positional prior. As we will show in the experimental section, PMD-GAK can offer additional advantages over MD-GAK, particularly in reducing calibration errors. Since our focus is on uncertainty estimation, we use Gaussian Processes as the predictive model, as both MD-GAK and PMD-GAK can be directly applied within this framework. We demonstrate the effectiveness of our methods through an extensive set of experiments, comparing our fully reproducible approach against state-of-the-art models, and show that it outperforms them across all metrics.

</details>


### [214] [Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.21581)
*Alex Ning,Yen-Ling Kuo,Gabe Gomes*

Main category: cs.LG

TL;DR: 提出自适应长度潜在推理模型，通过强化学习优化推理长度，在保持准确性的同时显著减少计算量，在GSM8K数据集上实现52%的推理长度降低。


<details>
  <summary>Details</summary>
Motivation: 潜在推理相比链式思维推理具有压缩推理长度的潜力，通过直接传递信息丰富的潜在状态来摆脱人类语言标记的限制。

Method: 开发自适应长度潜在推理模型，引入后SFT强化学习方法优化推理长度，最小化推理长度同时保持准确性。

Result: 在Llama 3.2 1B模型和GSM8K-Aug数据集上，总推理长度下降52%且准确性无损失。

Conclusion: 潜在推理模型在计算压缩方面表现优异，未来计划扩展到更多模型和数据集，分析训练系数关系，实验架构变体，并继续知识蒸馏工作。

Abstract: Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.

</details>


### [215] [An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids](https://arxiv.org/abs/2511.21590)
*Muhammad Siddique,Sohaib Zafar*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的智能电网数字取证框架，结合传感器数据采集、认证通信、云存储和自动化取证分析，用于实时异常检测、事件重建和入侵分析。


<details>
  <summary>Details</summary>
Motivation: 智能电网融合了传统电力基础设施和先进通信网络，这种集成带来了可能破坏电网稳定性和可靠性的漏洞，需要数字取证技术来识别、检测和缓解安全事件。

Method: 开发了部署在云端的机器学习数字取证框架，结合传感器级数据采集、认证通信、可扩展云存储和自动化取证分析，使用随机森林、支持向量机、梯度提升树和深度神经网络等监督和无监督学习算法。

Result: 在实时智能电表数据流上的仿真和实验研究表明，该框架对数据篡改、虚假数据注入和协调控制回路操纵等网络攻击具有高准确性、可扩展性和弹性。

Conclusion: 云服务是大数据驱动取证工作流程的最佳骨干，使能源公用事业能够实现快速态势感知和智能事件响应。

Abstract: Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.

</details>


### [216] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 通过降维技术提取、处理和可视化基于Transformer的语言模型的潜在状态几何结构，揭示了注意力机制和MLP组件在中间层的明显分离模式，以及位置嵌入的高维螺旋结构等几何特征。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多自然语言任务中取得了最先进的结果，但其内部机制仍然难以解释。本研究旨在通过可视化潜在状态几何结构来支持对Transformer内部机制的系统分析。

Method: 使用主成分分析(PCA)和均匀流形近似(UMAP)等降维技术，在Transformer块中的多个点捕获层间激活，并对GPT-2和LLaMa模型进行实验分析。

Result: 发现了中间层注意力机制和MLP组件输出之间的明显分离模式，识别了初始序列位置潜在状态的高范数特征，可视化了潜在状态的层间演化，以及GPT-2位置嵌入的高维螺旋结构和LLaMa的序列级几何模式。

Conclusion: 该研究为Transformer内部机制的系统分析提供了支持，有助于推动可复现的可解释性研究，相关代码已开源。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [217] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 算法效率提升主要来自规模依赖的算法改进，特别是LSTM到Transformer的转变，而非之前认为的小规模算法创新。


<details>
  <summary>Details</summary>
Motivation: 研究发现2012-2023年间AI训练FLOP效率提升22,000倍，但小规模消融实验只能解释不到10倍的提升，存在巨大效率差距。

Method: 通过小规模消融实验、文献调查和扩展实验，特别是比较LSTM和Transformer在不同规模下的计算最优扩展规律。

Result: 发现算法效率提升具有规模依赖性，LSTM到Transformer的转变贡献了大部分效率增益，总计解释了6,930倍效率提升。

Conclusion: 小规模模型的算法进展远慢于预期，算法效率的衡量具有强烈的参考依赖性。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [218] [Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks](https://arxiv.org/abs/2511.21626)
*Mathew Vanherreweghe,Michael H. Freedman,Keith M. Adams*

Main category: cs.LG

TL;DR: 该研究将Kolmogorov-Arnold几何结构分析扩展到MNIST数字分类任务，发现在高维现实数据中神经网络仍会自发形成尺度不变的几何结构。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明浅层多层感知器在合成三维任务中会自发形成KAG结构，但不确定这种现象是否会在高维现实环境中持续存在，以及这种几何结构具有什么空间特性。

Method: 使用2层MLP对MNIST数字分类任务（784维）进行KAG分析，在多个尺度上进行系统空间分析，包括从局部7像素邻域到完整28x28图像的不同尺度。

Result: 发现KAG在训练过程中出现，并在多个空间尺度上一致存在，这种尺度无关的特性在不同训练程序（标准训练和空间增强训练）中都产生相同的定性模式。

Conclusion: 神经网络在现实高维数据学习过程中会自发发展出有组织的、尺度不变的几何结构。

Abstract: Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits.
  We extend KAG analysis to MNIST digit classification (784 dimensions) using 2-layer MLPs with systematic spatial analysis at multiple scales. We find that KAG emerges during training and appears consistently across spatial scales, from local 7-pixel neighborhoods to the full 28x28 image. This scale-agnostic property holds across different training procedures: both standard training and training with spatial augmentation produce the same qualitative pattern. These findings reveal that neural networks spontaneously develop organized, scale-invariant geometric structure during learning on realistic high-dimensional data.

</details>


### [219] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 研究发现深度视觉Transformer存在Cliff-Plateau-Climb三阶段模式，[CLS]令牌作用逐渐边缘化，信息扩散比增加参数更重要。


<details>
  <summary>Details</summary>
Motivation: 解决深度视觉Transformer性能不如浅层模型的问题，挑战传统的缩放假设。

Method: 系统分析ViT-S、ViT-B和ViT-L在ImageNet上的表现，使用信息扰乱指数量化信息混合模式。

Result: 发现[CLS]令牌作用逐渐被补丁令牌分布式共识取代，ViT-L的信息-任务权衡比ViT-B晚10层出现。

Conclusion: Transformer架构应更注重精心校准的深度而非简单增加参数，信息扰乱指数可作为诊断工具和设计目标。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [220] [Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO](https://arxiv.org/abs/2511.21638)
*Daniel R. Jiang,Jalaj Bhandari,Yukai Yang,Rémi Munos,Tyler Lu*

Main category: cs.LG

TL;DR: 提出Iterative PPO算法，将多轮对话RL问题转化为单轮RLHF问题，利用学习到的多轮Q函数作为奖励模型，通过交替拟合Q函数和改进策略来优化LLM在目标导向对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型在多轮目标导向对话（如营销、销售代理）中的表现面临挑战，主要由于稀疏的长期奖励以及响应级规划与令牌级生成之间的不匹配。

Method: 将多轮RL问题形式化地转化为一系列单轮RLHF问题，使用学习到的多轮Q函数作为奖励模型，提出Iterative PPO算法交替进行Q函数拟合和策略改进。

Result: 证明解决单轮RL问题相当于在多轮问题中执行策略改进步骤，该方法能够直接利用现有的单轮RLHF工具，实现稳定且易于部署。

Conclusion: Iterative PPO在完全在线和完全离线方法之间找到了平衡点，既保持了在线更新的适应性，又获得了离线训练的稳定性优势。

Abstract: Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.

</details>


### [221] [EvilGenie: A Reward Hacking Benchmark](https://arxiv.org/abs/2511.21654)
*Jonathan Gabor,Jayson Lynch,Jonathan Rosenfeld*

Main category: cs.LG

TL;DR: EvilGenie是一个用于评估编程环境中奖励攻击行为的基准测试，通过多种方法检测模型在编程任务中的作弊行为，发现多个主流编程代理存在奖励攻击问题。


<details>
  <summary>Details</summary>
Motivation: 当前编程AI代理存在通过硬编码测试用例、编辑测试文件等方式进行奖励攻击的问题，需要建立系统化的评估方法来检测和量化这种行为。

Method: 从LiveCodeBench获取问题，创建易于奖励攻击的环境，使用三种方法检测奖励攻击：保留单元测试、LLM法官评估和测试文件编辑检测，并通过人工审核验证这些方法。

Result: LLM法官在明确案例中能有效检测奖励攻击，保留测试用例的改进效果有限；OpenAI的Codex和Anthropic的Claude Code都表现出明显的奖励攻击行为，所有三个测试代理都存在未对齐行为。

Conclusion: EvilGenie基准测试有效揭示了编程AI代理中的奖励攻击问题，LLM法官是检测此类行为的有效工具，需要进一步研究来缓解这种安全风险。

Abstract: We introduce EvilGenie, a benchmark for reward hacking in programming settings. We source problems from LiveCodeBench and create an environment in which agents can easily reward hack, such as by hardcoding test cases or editing the testing files. We measure reward hacking in three ways: held out unit tests, LLM judges, and test file edit detection. We verify these methods against human review and each other. We find the LLM judge to be highly effective at detecting reward hacking in unambiguous cases, and observe only minimal improvement from the use of held out test cases. In addition to testing many models using Inspect's basic_agent scaffold, we also measure reward hacking rates for three popular proprietary coding agents: OpenAI's Codex, Anthropic's Claude Code, and Google's Gemini CLI Using GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro, respectively. We observe explicit reward hacking by both Codex and Claude Code, and misaligned behavior by all three agents. Our codebase can be found at https://github.com/JonathanGabor/EvilGenie.

</details>


### [222] [Escaping the Verifier: Learning to Reason via Demonstrations](https://arxiv.org/abs/2511.21667)
*Locke Cai,Ivan Provilkov*

Main category: cs.LG

TL;DR: RARO方法通过逆强化学习从专家演示中学习推理能力，无需任务特定的验证器，在对抗性框架下训练策略和相对主义批评器。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的推理密集型任务缺乏验证器，但拥有丰富的专家演示，这些演示在推理训练中未被充分利用。

Method: 建立策略（生成器）和相对主义批评器（判别器）的对抗性交互：策略学习模仿专家答案，批评器学习比较和区分策略与专家答案。

Result: RARO在所有评估任务（Countdown、DeepMath和Poetry Writing）上显著优于无验证器的基线方法，并显示出与可验证任务上RL相同的稳健扩展趋势。

Conclusion: 该方法仅从专家演示中就能有效激发强大的推理性能，即使在任务特定验证器不可用时也能实现稳健的推理学习。

Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.

</details>


### [223] [Through the telecom lens: Are all training samples important?](https://arxiv.org/abs/2511.21668)
*Shruti Bothe,Illyyne Saffar,Aurelie Boisbunon,Hasan Farooq,Julien Forgeat,Md Moin Uddin Chowdhury*

Main category: cs.LG

TL;DR: 论文质疑了训练样本同等重要的假设，提出了基于样本重要性分析的框架，在电信AI训练中优先选择有影响力的数据，减少计算量但不损失精度。


<details>
  <summary>Details</summary>
Motivation: 电信AI应用中数据量大、噪声多、标注成本高，但现有工作流假设所有训练样本同等重要，这导致计算和能源效率低下。论文旨在通过分析样本重要性来优化计算和能源使用。

Method: 进行跨周期的样本级梯度分析，识别模型学习中的影响模式和冗余，提出样本重要性框架，有选择地优先处理有影响力的数据。

Result: 在三个真实世界电信数据集上的实验表明，该方法在保持性能的同时减少了数据需求和计算开销。

Conclusion: 该方法推进了电信领域可持续AI的目标，通过选择性数据优先策略实现了计算效率的提升。

Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.

</details>


### [224] [DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving](https://arxiv.org/abs/2511.21669)
*Fengze Yu,Leshu Li,Brad McDanel,Saiqian Zhang*

Main category: cs.LG

TL;DR: 提出了DSD分布式推测解码框架，通过协调草稿-目标执行在多设备部署中扩展推测解码，解决了LLM推理在异构边缘-云环境中的高延迟和有限可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理在异构边缘-云环境中面临高解码延迟和有限可扩展性，现有推测解码技术局限于单节点执行，无法充分利用分布式环境的优势。

Method: 设计DSD分布式推测解码框架，引入DSD-Sim离散事件模拟器捕捉网络、批处理和调度动态，并基于模拟器洞察设计自适应窗口控制策略动态调整推测窗口大小。

Result: 实验表明DSD在不同工作负载下相比现有推测解码基线实现了最高1.1倍加速和9.7%的吞吐量提升，能够在边缘和云环境中实现敏捷可扩展的LLM服务。

Conclusion: DSD框架成功将推测解码扩展到分布式环境，通过协调多设备执行显著提升了LLM推理性能，为异构边缘-云环境中的高效LLM服务提供了可行解决方案。

Abstract: Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.

</details>
