{"id": "2511.13384", "categories": ["q-fin.GN", "q-fin.CP", "q-fin.ST", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.13384", "abs": "https://arxiv.org/abs/2511.13384", "authors": ["Catalin Dumitrescu"], "title": "CBDC Stress Test in a Dual-Currency Setting", "comment": "713 pages, including annexes; figures and tables included; updated version forthcoming as an e-book", "summary": "This study explores the potential impact of introducing a Central Bank Digital Currency (CBDC) on financial stability in an emerging dual-currency economy (Romania), where the domestic currency (RON) coexists with the euro. It develops an integrated analytical framework combining econometrics, machine learning, and behavioural modelling. CBDC adoption probabilities are estimated using XGBoost and logistic regression models trained on behavioural and macro-financial indicators rather than survey data. Liquidity stress simulations assess how banks would respond to deposit withdrawals resulting from CBDC adoption, while VAR, MSVAR, and SVAR models capture the macro-financial transmission of liquidity shocks into credit contraction and changes in monetary conditions. The findings indicate that CBDC uptake (co-circulating Digital RON and Digital EUR) would be moderate at issuance, amounting to around EUR 1 billion, primarily driven by digital readiness and trust in the central bank. The study concludes that a non-remunerated, capped CBDC, designed primarily as a means of payment rather than a store of value, can be introduced without compromising financial stability. In dual currency economies, differentiated holding limits for domestic and foreign digital currencies (e.g., Digital RON versus Digital Euro) are crucial to prevent uncontrolled euroisation and preserve monetary sovereignty. A prudent design with moderate caps, non remuneration, and macroprudential coordination can transform CBDC into a digital liquidity buffer and a complementary monetary policy instrument that enhances resilience and inclusion rather than destabilising the financial system.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u7f57\u9a6c\u5c3c\u4e9a\u8fd9\u6837\u7684\u53cc\u8d27\u5e01\u7ecf\u6d4e\u4f53\u4e2d\u5f15\u5165\u592e\u884c\u6570\u5b57\u8d27\u5e01(CBDC)\u5bf9\u91d1\u878d\u7a33\u5b9a\u7684\u5f71\u54cd\uff0c\u5f00\u53d1\u4e86\u7ed3\u5408\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548c\u884c\u4e3a\u5efa\u6a21\u7684\u7efc\u5408\u5206\u6790\u6846\u67b6\u3002\u7814\u7a76\u53d1\u73b0\u9002\u5ea6\u8bbe\u8ba1\u7684CBDC\u53ef\u4ee5\u4f5c\u4e3a\u6570\u5b57\u6d41\u52a8\u6027\u7f13\u51b2\u548c\u8865\u5145\u8d27\u5e01\u653f\u7b56\u5de5\u5177\uff0c\u4e0d\u4f1a\u635f\u5bb3\u91d1\u878d\u7a33\u5b9a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5728\u53cc\u8d27\u5e01\u7ecf\u6d4e\u4f53(\u7f57\u9a6c\u5c3c\u4e9a\uff0cRON\u4e0e\u6b27\u5143\u5171\u5b58)\u4e2d\u5f15\u5165CBDC\u5bf9\u91d1\u878d\u7a33\u5b9a\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u9632\u6b62\u4e0d\u53d7\u63a7\u5236\u7684\u6b27\u5143\u5316\u548c\u4fdd\u6301\u8d27\u5e01\u4e3b\u6743\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528XGBoost\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4f30\u8ba1CBDC\u91c7\u7528\u6982\u7387\uff0c\u57fa\u4e8e\u884c\u4e3a\u548c\u5b8f\u89c2\u91d1\u878d\u6307\u6807\u800c\u975e\u8c03\u67e5\u6570\u636e\u3002\u901a\u8fc7\u6d41\u52a8\u6027\u538b\u529b\u6a21\u62df\u8bc4\u4f30\u94f6\u884c\u5bf9CBDC\u5f15\u53d1\u7684\u5b58\u6b3e\u63d0\u53d6\u7684\u53cd\u5e94\uff0c\u4f7f\u7528VAR\u3001MSVAR\u548cSVAR\u6a21\u578b\u6355\u6349\u6d41\u52a8\u6027\u51b2\u51fb\u5411\u4fe1\u8d37\u6536\u7f29\u548c\u8d27\u5e01\u6761\u4ef6\u53d8\u5316\u7684\u5b8f\u89c2\u91d1\u878d\u4f20\u5bfc\u3002", "result": "CBDC\u91c7\u7528\u5728\u53d1\u884c\u521d\u671f\u5c06\u9002\u5ea6\uff0c\u7ea610\u4ebf\u6b27\u5143\uff0c\u4e3b\u8981\u7531\u6570\u5b57\u51c6\u5907\u5ea6\u548c\u5bf9\u592e\u884c\u7684\u4fe1\u4efb\u9a71\u52a8\u3002\u975e\u8ba1\u606f\u3001\u6709\u4e0a\u9650\u7684CBDC\uff0c\u4e3b\u8981\u4f5c\u4e3a\u652f\u4ed8\u624b\u6bb5\u800c\u975e\u4ef7\u503c\u50a8\u5b58\uff0c\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u91d1\u878d\u7a33\u5b9a\u7684\u60c5\u51b5\u4e0b\u5f15\u5165\u3002", "conclusion": "\u5728\u53cc\u8d27\u5e01\u7ecf\u6d4e\u4f53\u4e2d\uff0c\u5bf9\u56fd\u5185\u548c\u5916\u56fd\u6570\u5b57\u8d27\u5e01\u5b9e\u884c\u5dee\u5f02\u5316\u7684\u6301\u6709\u9650\u989d\u81f3\u5173\u91cd\u8981\u3002\u5ba1\u614e\u8bbe\u8ba1\u7684\u4e2d\u7b49\u4e0a\u9650\u3001\u4e0d\u8ba1\u606f\u548c\u5b8f\u89c2\u5ba1\u614e\u534f\u8c03\u53ef\u4ee5\u5c06CBDC\u8f6c\u5316\u4e3a\u6570\u5b57\u6d41\u52a8\u6027\u7f13\u51b2\u548c\u8865\u5145\u8d27\u5e01\u653f\u7b56\u5de5\u5177\uff0c\u589e\u5f3a\u97e7\u6027\u548c\u5305\u5bb9\u6027\u800c\u975e\u7834\u574f\u91d1\u878d\u7a33\u5b9a\u3002"}}
{"id": "2511.12763", "categories": ["q-fin.ST"], "pdf": "https://arxiv.org/pdf/2511.12763", "abs": "https://arxiv.org/abs/2511.12763", "authors": ["Harrison Katz"], "title": "Impact by design: translating Lead times in flux into an R handbook with code", "comment": null, "summary": "This commentary translates the central ideas in Lead times in flux into a practice ready handbook in R. The original article measures change in the full distribution of booking lead times with a normalized L1 distance and tracks that divergence across months relative to year over year and to a fixed 2018 reference. It also provides a bound that links divergence and remaining horizon to the relative error of pickup forecasts. We implement these ideas end to end in R, using a minimal data schema and providing runnable scripts, simulated examples, and a prespecified evaluation plan. All results use synthetic data so the exposition is fully reproducible without reference to proprietary sources.", "AI": {"tldr": "\u5c06Lead times in flux\u7684\u6838\u5fc3\u601d\u60f3\u8f6c\u5316\u4e3aR\u8bed\u8a00\u5b9e\u8df5\u624b\u518c\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u9884\u8ba2\u63d0\u524d\u671f\u5206\u5e03\u53d8\u5316\u6d4b\u91cf\u548c\u9884\u6d4b\u8bef\u5dee\u5206\u6790", "motivation": "\u5c06\u7406\u8bba\u65b9\u6cd5\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5b9e\u8df5\u5de5\u5177\uff0c\u4f7f\u7528R\u8bed\u8a00\u5b9e\u73b0\u5b8c\u6574\u7684\u5206\u6790\u6d41\u7a0b\uff0c\u4fbf\u4e8e\u5b9e\u9645\u5e94\u7528\u548c\u590d\u73b0", "method": "\u4f7f\u7528\u5f52\u4e00\u5316L1\u8ddd\u79bb\u6d4b\u91cf\u9884\u8ba2\u63d0\u524d\u671f\u5206\u5e03\u7684\u6708\u5ea6\u53d8\u5316\uff0c\u76f8\u5bf9\u4e8e\u540c\u6bd4\u548c2018\u5e74\u56fa\u5b9a\u53c2\u8003\u70b9\u8fdb\u884c\u8ddf\u8e2a\uff0c\u5e76\u5efa\u7acb\u53d1\u6563\u5ea6\u4e0e\u5269\u4f59\u65f6\u95f4\u8303\u56f4\u5bf9\u62fe\u53d6\u9884\u6d4b\u76f8\u5bf9\u8bef\u5dee\u7684\u754c\u9650\u5173\u7cfb", "result": "\u5728R\u4e2d\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u5206\u6790\u6d41\u7a0b\uff0c\u5305\u62ec\u6700\u5c0f\u6570\u636e\u6a21\u5f0f\u3001\u53ef\u8fd0\u884c\u811a\u672c\u3001\u6a21\u62df\u793a\u4f8b\u548c\u9884\u8bbe\u8bc4\u4f30\u8ba1\u5212\uff0c\u6240\u6709\u7ed3\u679c\u4f7f\u7528\u5408\u6210\u6570\u636e\u786e\u4fdd\u5b8c\u5168\u53ef\u590d\u73b0", "conclusion": "\u6210\u529f\u5c06\u7406\u8bba\u65b9\u6cd5\u8f6c\u5316\u4e3a\u5b9e\u7528\u7684R\u8bed\u8a00\u5de5\u5177\u5305\uff0c\u4e3a\u9884\u8ba2\u63d0\u524d\u671f\u5206\u6790\u548c\u9884\u6d4b\u8bef\u5dee\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.11909", "categories": ["q-fin.MF"], "pdf": "https://arxiv.org/pdf/2511.11909", "abs": "https://arxiv.org/abs/2511.11909", "authors": ["Jiacheng Wu"], "title": "Modeling and Stabilizing Financial Systemic Risk Using Optimal Control Theory", "comment": "21 pages", "summary": "A theoretical model of systemic-risk propagation of financial market is analyzed for stability. The state equation is an unsteady diffusion equation with a nonlinear logistic growth term, where the diffusion process captures the spread of default stress between interconnected financial entities and the reaction term captures the local procyclicality of financial stress. The stabilizing controller synthesis includes three steps: First, the algebraic Riccati equation is derived for the linearized system equation, the solution of which provides an exponentially stabilizing controller. Second, the nonlinear system is treated as a linear system with the nonlinear term as its forcing term. Based on estimation of the solutions for linearized equations and the contraction mapping theorem, unique existence of the solution for the nonlinear system equation is proved. Third, local asymptotic stability of the nonlinear system is obtained by considering the corresponding Hamilton-Jacobi equation. In both the linearized and nonlinear systems, the resulting controllers ensure that the $H^{\\infty}$ norms of the mappings from disturbance to the output are less than a predefined constant. Stabilizing conditions provide a new framework of achieving system-level financial risk managing goals via the synergy of decentralized components, which offers policy-relevant insights for governments, regulators and central banks to mitigate financial crises.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91d1\u878d\u7cfb\u7edf\u98ce\u9669\u4f20\u64ad\u7684\u7406\u8bba\u6a21\u578b\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6269\u6563\u65b9\u7a0b\u63cf\u8ff0\u91d1\u878d\u5b9e\u4f53\u95f4\u7684\u8fdd\u7ea6\u538b\u529b\u4f20\u64ad\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\u7684\u7a33\u5b9a\u63a7\u5236\u5668\u6765\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u91d1\u878d\u7cfb\u7edf\u98ce\u9669\u7684\u4f20\u64ad\u673a\u5236\uff0c\u4e3a\u653f\u5e9c\u3001\u76d1\u7ba1\u673a\u6784\u548c\u4e2d\u592e\u94f6\u884c\u63d0\u4f9b\u653f\u7b56\u76f8\u5173\u7684\u89c1\u89e3\uff0c\u4ee5\u901a\u8fc7\u5206\u6563\u7ec4\u4ef6\u7684\u534f\u540c\u4f5c\u7528\u5b9e\u73b0\u7cfb\u7edf\u7ea7\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u76ee\u6807\u3002", "method": "1) \u63a8\u5bfc\u7ebf\u6027\u5316\u7cfb\u7edf\u65b9\u7a0b\u7684\u4ee3\u6570Riccati\u65b9\u7a0b\uff1b2) \u5c06\u975e\u7ebf\u6027\u7cfb\u7edf\u89c6\u4e3a\u5e26\u6709\u975e\u7ebf\u6027\u5f3a\u8feb\u9879\u7684\u7ebf\u6027\u7cfb\u7edf\uff0c\u57fa\u4e8e\u7ebf\u6027\u5316\u65b9\u7a0b\u89e3\u7684\u4f30\u8ba1\u548c\u538b\u7f29\u6620\u5c04\u5b9a\u7406\u8bc1\u660e\u975e\u7ebf\u6027\u7cfb\u7edf\u89e3\u7684\u552f\u4e00\u5b58\u5728\u6027\uff1b3) \u901a\u8fc7\u8003\u8651\u76f8\u5e94\u7684Hamilton-Jacobi\u65b9\u7a0b\u83b7\u5f97\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5c40\u90e8\u6e10\u8fd1\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\uff0c\u6240\u5f97\u63a7\u5236\u5668\u786e\u4fdd\u4ece\u6270\u52a8\u5230\u8f93\u51fa\u7684\u6620\u5c04\u7684H\u221e\u8303\u6570\u5c0f\u4e8e\u9884\u5b9a\u4e49\u5e38\u6570\uff0c\u5b9e\u73b0\u4e86\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7a33\u5b9a\u6761\u4ef6\u4e3a\u901a\u8fc7\u5206\u6563\u7ec4\u4ef6\u7684\u534f\u540c\u4f5c\u7528\u5b9e\u73b0\u7cfb\u7edf\u7ea7\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u76ee\u6807\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u4e3a\u7f13\u89e3\u91d1\u878d\u5371\u673a\u63d0\u4f9b\u4e86\u653f\u7b56\u76f8\u5173\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.13616", "categories": ["q-fin.CP"], "pdf": "https://arxiv.org/pdf/2511.13616", "abs": "https://arxiv.org/abs/2511.13616", "authors": ["Katarzyna Maciejowska", "Arkadiusz Lipiecki", "Bartosz Uniejewski"], "title": "Statistical and economic evaluation of forecasts in electricity markets: beyond RMSE and MAE", "comment": null, "summary": "In recent years, a rapid development of forecasting methods has led to an increase in the accuracy of predictions. In the literature, forecasts are typically evaluated using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). While appropriate for statistical assessment, these measures do not adequately reflect the economic value of forecasts. This study addresses the decision-making problem faced by a battery energy storage system, which must determine optimal charging and discharging times based on day-ahead electricity price forecasts. To explore the relationship between forecast accuracy and economic value, we generate a pool of 192 forecasts. These are evaluated using seven statistical metrics that go beyond RMSE and MAE, capturing various characteristics of the predictions and associated errors. We calculate the dynamic correlation between the statistical measures and gained profits to reveal that both RMSE and MAE are only weakly correlated with revenue. In contrast, measures that assess the alignment between predicted and actual daily price curves have a stronger relationship with profitability and are thus more effective for selecting optimal forecasts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u7ecf\u6d4e\u4ef7\u503c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u4f20\u7edf\u7edf\u8ba1\u6307\u6807\uff08\u5982RMSE\u548cMAE\uff09\u4e0e\u6536\u76ca\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u800c\u8bc4\u4f30\u9884\u6d4b\u4e0e\u771f\u5b9e\u4ef7\u683c\u66f2\u7ebf\u5bf9\u9f50\u5ea6\u7684\u6307\u6807\u4e0e\u76c8\u5229\u80fd\u529b\u5173\u8054\u66f4\u5f3a\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u8bc4\u4f30\u6307\u6807\uff08\u5982RMSE\u548cMAE\uff09\u867d\u7136\u9002\u5408\u7edf\u8ba1\u8bc4\u4f30\uff0c\u4f46\u4e0d\u80fd\u5145\u5206\u53cd\u6620\u9884\u6d4b\u7684\u7ecf\u6d4e\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\u9700\u8981\u57fa\u4e8e\u65e5\u524d\u7535\u4ef7\u9884\u6d4b\u786e\u5b9a\u5145\u653e\u7535\u65f6\u673a\u7684\u51b3\u7b56\u95ee\u9898\u4e2d\u3002", "method": "\u751f\u6210\u4e86192\u4e2a\u9884\u6d4b\uff0c\u4f7f\u75287\u4e2a\u8d85\u8d8aRMSE\u548cMAE\u7684\u7edf\u8ba1\u6307\u6807\u8bc4\u4f30\u9884\u6d4b\u7279\u6027\uff0c\u5e76\u8ba1\u7b97\u8fd9\u4e9b\u7edf\u8ba1\u6307\u6807\u4e0e\u83b7\u5f97\u5229\u6da6\u4e4b\u95f4\u7684\u52a8\u6001\u76f8\u5173\u6027\u3002", "result": "RMSE\u548cMAE\u4e0e\u6536\u76ca\u4ec5\u5448\u5f31\u76f8\u5173\uff0c\u800c\u8bc4\u4f30\u9884\u6d4b\u4e0e\u771f\u5b9e\u65e5\u4ef7\u683c\u66f2\u7ebf\u5bf9\u9f50\u5ea6\u7684\u6307\u6807\u4e0e\u76c8\u5229\u80fd\u529b\u6709\u66f4\u5f3a\u7684\u5173\u7cfb\uff0c\u80fd\u66f4\u6709\u6548\u5730\u9009\u62e9\u6700\u4f18\u9884\u6d4b\u3002", "conclusion": "\u5728\u9009\u62e9\u6700\u4f18\u9884\u6d4b\u65f6\uff0c\u5e94\u4f18\u5148\u8003\u8651\u80fd\u591f\u8bc4\u4f30\u9884\u6d4b\u4e0e\u771f\u5b9e\u4ef7\u683c\u66f2\u7ebf\u5bf9\u9f50\u5ea6\u7684\u6307\u6807\uff0c\u800c\u975e\u4f20\u7edf\u7684RMSE\u548cMAE\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6307\u6807\u80fd\u66f4\u597d\u5730\u53cd\u6620\u9884\u6d4b\u7684\u7ecf\u6d4e\u4ef7\u503c\u3002"}}
{"id": "2511.11862", "categories": ["econ.EM", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.11862", "abs": "https://arxiv.org/abs/2511.11862", "authors": ["Jiafeng Chen", "Lihua Lei", "Timothy Sudijono", "Liyang Sun", "Tian Xie"], "title": "Compound Selection Decisions: An Almost SURE Approach", "comment": "32 page main text. Comments welcome", "summary": "This paper proposes methods for producing compound selection decisions in a Gaussian sequence model. Given unknown, fixed parameters $\u03bc_ {1:n}$ and known $\u03c3_{1:n}$ with observations $Y_i \\sim \\textsf{N}(\u03bc_i, \u03c3_i^2)$, the decision maker would like to select a subset of indices $S$ so as to maximize utility $\\frac{1}{n}\\sum_{i\\in S} (\u03bc_i - K_i)$, for known costs $K_i$. Inspired by Stein's unbiased risk estimate (SURE), we introduce an almost unbiased estimator, called ASSURE, for the expected utility of a proposed decision rule. ASSURE allows a user to choose a welfare-maximizing rule from a pre-specified class by optimizing the estimated welfare, thereby producing selection decisions that borrow strength across noisy estimates. We show that ASSURE produces decision rules that are asymptotically no worse than the optimal but infeasible decision rule in the pre-specified class. We apply ASSURE to the selection of Census tracts for economic opportunity, the identification of discriminating firms, and the analysis of $p$-value decision procedures in A/B testing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u9ad8\u65af\u5e8f\u5217\u6a21\u578b\u4e2d\u751f\u6210\u590d\u5408\u9009\u62e9\u51b3\u7b56\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165ASSURE\uff08\u51e0\u4e4e\u65e0\u504f\u4f30\u8ba1\u5668\uff09\u6765\u4f30\u8ba1\u51b3\u7b56\u89c4\u5219\u7684\u9884\u671f\u6548\u7528\uff0c\u4ece\u800c\u5728\u9884\u5b9a\u4e49\u7c7b\u522b\u4e2d\u9009\u62e9\u798f\u5229\u6700\u5927\u5316\u7684\u89c4\u5219\u3002", "motivation": "\u5728\u89c2\u6d4b\u6570\u636e\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u51b3\u7b56\u8005\u9700\u8981\u9009\u62e9\u5b50\u96c6\u7d22\u5f15S\u6765\u6700\u5927\u5316\u6548\u7528\u51fd\u6570\u2211(\u03bc_i - K_i)/n\uff0c\u4f46\u03bc_i\u672a\u77e5\u4e14\u5b58\u5728\u4f30\u8ba1\u8bef\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u566a\u58f0\u4f30\u8ba1\u501f\u529b\u7684\u51b3\u7b56\u65b9\u6cd5\u3002", "method": "\u53d7Stein\u65e0\u504f\u98ce\u9669\u4f30\u8ba1(SURE)\u542f\u53d1\uff0c\u5f00\u53d1\u4e86ASSURE\uff08\u51e0\u4e4e\u65e0\u504f\u4f30\u8ba1\u5668\uff09\u6765\u4f30\u8ba1\u63d0\u8bae\u51b3\u7b56\u89c4\u5219\u7684\u9884\u671f\u6548\u7528\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u4f18\u5316\u4f30\u8ba1\u798f\u5229\u4ece\u9884\u5b9a\u4e49\u7c7b\u522b\u4e2d\u9009\u62e9\u798f\u5229\u6700\u5927\u5316\u89c4\u5219\u3002", "result": "ASSURE\u4ea7\u751f\u7684\u51b3\u7b56\u89c4\u5219\u5728\u9884\u5b9a\u4e49\u7c7b\u522b\u4e2d\u6e10\u8fd1\u4e0d\u5dee\u4e8e\u6700\u4f18\u4f46\u4e0d\u53ef\u884c\u7684\u51b3\u7b56\u89c4\u5219\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ASSURE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u65af\u5e8f\u5217\u6a21\u578b\u4e2d\u7684\u9009\u62e9\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u8de8\u566a\u58f0\u4f30\u8ba1\u501f\u529b\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u51b3\u7b56\u89c4\u5219\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2511.11761", "categories": ["cs.CY", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.11761", "abs": "https://arxiv.org/abs/2511.11761", "authors": ["Soogand Alavi", "Salar Nozari", "Andrea Luangrath"], "title": "Cost Transparency of Enterprise AI Adoption", "comment": null, "summary": "Recent advances in large language models (LLMs) have dramatically improved performance on a wide range of tasks, driving rapid enterprise adoption. Yet, the cost of adopting these AI services is understudied. Unlike traditional software licensing in which costs are predictable before usage, commercial LLM services charge per token of input text in addition to generated output tokens. Crucially, while firms can control the input, they have limited control over output tokens, which are effectively set by generation dynamics outside of business control. This research shows that subtle shifts in linguistic style can systematically alter the number of output tokens without impacting response quality. Using an experiment with OpenAI's API, this study reveals that non-polite prompts significantly increase output tokens leading to higher enterprise costs and additional revenue for OpenAI. Politeness is merely one instance of a broader phenomenon in which linguistic structure can drive unpredictable cost variation. For enterprises integrating LLM into applications, this unpredictability complicates budgeting and undermines transparency in business-to-business contexts. By demonstrating how end-user behavior links to enterprise costs through output token counts, this work highlights the opacity of current pricing models and calls for new approaches to ensure predictable and transparent adoption of LLM services.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u793a\u8bed\u7684\u793c\u8c8c\u7a0b\u5ea6\u4f1a\u5f71\u54cdLLM\u8f93\u51fatoken\u6570\u91cf\uff0c\u975e\u793c\u8c8c\u63d0\u793a\u4f1a\u663e\u8457\u589e\u52a0\u8f93\u51fatoken\uff0c\u5bfc\u81f4\u4f01\u4e1a\u6210\u672c\u4e0a\u5347\uff0c\u8fd9\u66b4\u9732\u4e86\u5f53\u524dLLM\u5b9a\u4ef7\u6a21\u5f0f\u7684\u4e0d\u900f\u660e\u6027\u3002", "motivation": "\u968f\u7740\u4f01\u4e1a\u5feb\u901f\u91c7\u7528LLM\u670d\u52a1\uff0c\u5176\u6210\u672c\u95ee\u9898\u88ab\u5ffd\u89c6\u3002\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u4e0d\u540c\uff0cLLM\u6309token\u6536\u8d39\uff0c\u4f01\u4e1a\u80fd\u63a7\u5236\u8f93\u5165\u4f46\u65e0\u6cd5\u63a7\u5236\u8f93\u51fatoken\u6570\u91cf\uff0c\u8fd9\u5bfc\u81f4\u6210\u672c\u4e0d\u53ef\u9884\u6d4b\u3002", "method": "\u901a\u8fc7OpenAI API\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e0d\u540c\u793c\u8c8c\u7a0b\u5ea6\u7684\u63d0\u793a\u8bed\u5982\u4f55\u5f71\u54cd\u8f93\u51fatoken\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u56de\u7b54\u8d28\u91cf\u4e0d\u53d8\u3002", "result": "\u975e\u793c\u8c8c\u63d0\u793a\u4f1a\u663e\u8457\u589e\u52a0\u8f93\u51fatoken\u6570\u91cf\uff0c\u5bfc\u81f4\u4f01\u4e1a\u6210\u672c\u4e0a\u5347\uff0c\u4e3a\u670d\u52a1\u5546\u5e26\u6765\u989d\u5916\u6536\u5165\u3002\u793c\u8c8c\u53ea\u662f\u8bed\u8a00\u7ed3\u6784\u5f71\u54cd\u6210\u672c\u7684\u4e00\u4e2a\u4f8b\u5b50\u3002", "conclusion": "\u5f53\u524dLLM\u5b9a\u4ef7\u6a21\u5f0f\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u4f01\u4e1a\u96be\u4ee5\u9884\u7b97\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u786e\u4fddLLM\u670d\u52a1\u7684\u53ef\u9884\u6d4b\u548c\u900f\u660e\u91c7\u7528\u3002"}}
{"id": "2511.12120", "categories": ["q-fin.TR", "q-fin.CP", "q-fin.PM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12120", "abs": "https://arxiv.org/abs/2511.12120", "authors": ["Hongyang Yang", "Xiao-Yang Liu", "Shan Zhong", "Anwar Walid"], "title": "Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy", "comment": "Accepted by ICAIF '20: Proceedings of the First ACM International Conference on AI in Finance. Conference program: https://ai-finance.org/2020program/", "summary": "Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \\href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u96c6\u6210\u80a1\u7968\u4ea4\u6613\u7b56\u7565\uff0c\u7ed3\u5408PPO\u3001A2C\u548cDDPG\u4e09\u79cd\u7b97\u6cd5\uff0c\u5728\u9053\u743c\u65af30\u53ea\u80a1\u7968\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u4e8e\u57fa\u51c6\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u590d\u6742\u52a8\u6001\u7684\u80a1\u7968\u5e02\u573a\u4e2d\u8bbe\u8ba1\u76c8\u5229\u4ea4\u6613\u7b56\u7565\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u5e02\u573a\u60c5\u51b5\u7684\u7a33\u5065\u7b56\u7565\u3002", "method": "\u4f7f\u7528PPO\u3001A2C\u548cDDPG\u4e09\u79cdactor-critic\u7b97\u6cd5\u8bad\u7ec3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u6309\u9700\u52a0\u8f7d\u6280\u672f\u5904\u7406\u5927\u6570\u636e\uff0c\u6784\u5efa\u96c6\u6210\u4ea4\u6613\u7b56\u7565\u3002", "result": "\u5728\u9053\u743c\u65af30\u53ea\u80a1\u7968\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u96c6\u6210\u7b56\u7565\u5728\u98ce\u9669\u8c03\u6574\u540e\u6536\u76ca\uff08\u590f\u666e\u6bd4\u7387\uff09\u65b9\u9762\u4f18\u4e8e\u4e09\u79cd\u5355\u72ec\u7b97\u6cd5\u548c\u4e24\u4e2a\u57fa\u51c6\u7b56\u7565\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\u7b56\u7565\u80fd\u591f\u6709\u6548\u6574\u5408\u4e0d\u540c\u7b97\u6cd5\u7684\u4f18\u52bf\uff0c\u5728\u80a1\u7968\u4ea4\u6613\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u98ce\u9669\u8c03\u6574\u540e\u6536\u76ca\u8868\u73b0\u3002"}}
{"id": "2511.11682", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11682", "abs": "https://arxiv.org/abs/2511.11682", "authors": ["Hayate Toba", "Atsushi Yano", "Takuya Azumi"], "title": "Generalized Inequality-based Approach for Probabilistic WCET Estimation", "comment": null, "summary": "Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9971\u548c\u51fd\u6570\uff08\u53cd\u6b63\u5207\u548c\u53cc\u66f2\u6b63\u5207\uff09\u7684\u6539\u8fdb\u5207\u6bd4\u96ea\u592b\u4e0d\u7b49\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u91cd\u5c3e\u5206\u5e03\u4e0b\u6982\u7387\u6700\u574f\u60c5\u51b5\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u7684\u60b2\u89c2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u6781\u503c\u7406\u8bba\u7684\u65b9\u6cd5\u9700\u8981\u786e\u5b9a\u5206\u5e03\u4e0a\u5c3e\u8d77\u59cb\u70b9\uff0c\u5b58\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff1b\u57fa\u4e8e\u4e0d\u7b49\u5f0f\u7684\u65b9\u6cd5\u867d\u7136\u907f\u514d\u6b64\u95ee\u9898\uff0c\u4f46\u5bf9\u91cd\u5c3e\u5206\u5e03\u4f1a\u4ea7\u751f\u60b2\u89c2\u7ed3\u679c\u3002", "method": "\u5c06\u9971\u548c\u51fd\u6570\uff08arctangent\u548chyperbolic tangent\uff09\u6574\u5408\u5230\u5207\u6bd4\u96ea\u592b\u4e0d\u7b49\u5f0f\u4e2d\uff0c\u51cf\u8f7b\u5927\u79bb\u7fa4\u503c\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u5b66\u4e25\u8c28\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548cAutoware\u81ea\u52a8\u9a7e\u9a76\u5806\u6808\u7684\u771f\u5b9e\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u4e3a\u91cd\u5c3e\u5206\u5e03\u63d0\u4f9b\u66f4\u7d27\u81f4\u4e14\u5b89\u5168\u7684\u4e0a\u754c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u91cd\u5c3e\u5206\u5e03\u4e0bpWCET\u4f30\u8ba1\u7684\u60b2\u89c2\u6027\uff0c\u4e3a\u5b9e\u65f6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u65f6\u95f4\u4fdd\u8bc1\u3002"}}
{"id": "2511.11850", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11850", "abs": "https://arxiv.org/abs/2511.11850", "authors": ["Ali Mashhadireza", "Ali Sadighi"], "title": "Neural Network-Augmented Iterative Learning Control for Friction Compensation of Motion Control Systems with Varying Disturbances", "comment": "6 pages, 10 figures, conference paper to be submitted to IEEE Conference on Control Systems", "summary": "This paper proposes a robust control strategy that integrates Iterative Learning Control (ILC) with a simple lateral neural network to enhance the trajectory tracking performance of a linear Lorentz force actuator under friction and model uncertainties. The ILC compensates for nonlinear friction effects, while the neural network estimates the nonlinear ILC effort for varying reference commands. By dynamically adjusting the ILC effort, the method adapts to time-varying friction, reduces errors at reference changes, and accelerates convergence. Compared to previous approaches using complex neural networks, this method simplifies online training and implementation, making it practical for real-time applications. Experimental results confirm its effectiveness in achieving precise tracking across multiple tasks with different reference trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u548c\u7b80\u5355\u6a2a\u5411\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u5347\u7ebf\u6027\u6d1b\u4f26\u5179\u529b\u6267\u884c\u5668\u5728\u6469\u64e6\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7ebf\u6027\u6d1b\u4f26\u5179\u529b\u6267\u884c\u5668\u5728\u6469\u64e6\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u4e0b\u7684\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u65f6\u53d8\u6469\u64e6\u548c\u53c2\u8003\u6307\u4ee4\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u5c06\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236(ILC)\u4e0e\u7b80\u5355\u6a2a\u5411\u795e\u7ecf\u7f51\u7edc\u76f8\u7ed3\u5408\uff0cILC\u8865\u507f\u975e\u7ebf\u6027\u6469\u64e6\u6548\u5e94\uff0c\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u4e0d\u540c\u53c2\u8003\u6307\u4ee4\u4e0b\u7684\u975e\u7ebf\u6027ILC\u52aa\u529b\uff0c\u52a8\u6001\u8c03\u6574ILC\u52aa\u529b\u4ee5\u9002\u5e94\u65f6\u53d8\u6469\u64e6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u7cbe\u786e\u8ddf\u8e2a\uff0c\u5728\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u53c2\u8003\u8f68\u8ff9\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u4f7f\u7528\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u7b80\u5316\u4e86\u5728\u7ebf\u8bad\u7ec3\u548c\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b80\u5316\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u4f7fILC\u4e0e\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u7684\u63a7\u5236\u7b56\u7565\u66f4\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u80fd\u591f\u9002\u5e94\u65f6\u53d8\u6469\u64e6\u3001\u51cf\u5c11\u53c2\u8003\u53d8\u5316\u65f6\u7684\u8bef\u5dee\u5e76\u52a0\u901f\u6536\u655b\u3002"}}
{"id": "2511.11590", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11590", "abs": "https://arxiv.org/abs/2511.11590", "authors": ["Robert Gigiu"], "title": "Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)", "comment": "33 pages, 5 figures", "summary": "Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u6027\u8d4b\u80fd\u4e34\u5e8a\u5b89\u5168\u6846\u67b6(ECSF)\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u96c6\u6210\u5230DCB0129/0160\u751f\u547d\u5468\u671f\u4e2d\uff0c\u4f7f\u4e34\u5e8a\u5b89\u5168\u5b98\u5458\u80fd\u591f\u4f7f\u7528\u53ef\u89e3\u91ca\u6027\u8f93\u51fa\u4f5c\u4e3a\u7ed3\u6784\u5316\u5b89\u5168\u8bc1\u636e\uff0c\u800c\u65e0\u9700\u6539\u53d8\u5408\u89c4\u8def\u5f84\u3002", "motivation": "AI\u5728NHS\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u6982\u7387\u6027\u548c\u9002\u5e94\u6027\u884c\u4e3a\u4e0e\u73b0\u6709\u4e34\u5e8a\u5b89\u5168\u6807\u51c6\u7684\u786e\u5b9a\u6027\u5047\u8bbe\u76f8\u51b2\u7a81\u3002\u73b0\u6709\u6807\u51c6DCB0129\u548cDCB0160\u672a\u5b9a\u4e49\u5982\u4f55\u8bc1\u660eAI\u7279\u5b9a\u7684\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u6216\u6a21\u578b\u6f02\u79fb\u3002", "method": "\u901a\u8fc7\u8de8\u76d1\u7ba1\u5408\u6210\uff0c\u5c06DCB\u6761\u6b3e\u4e0e\u826f\u597d\u673a\u5668\u5b66\u4e60\u5b9e\u8df5\u3001NHS AI\u4fdd\u8bc1\u548cT.E.S.T.\u6846\u67b6\u4ee5\u53ca\u6b27\u76dfAI\u6cd5\u6848\u7684\u539f\u5219\u8fdb\u884c\u6620\u5c04\u3002\u521b\u5efa\u4e86\u4e00\u4e2a\u8fde\u63a5\u76d1\u7ba1\u6761\u6b3e\u3001\u539f\u5219\u3001ECSF\u68c0\u67e5\u70b9\u548c\u5408\u9002\u53ef\u89e3\u91ca\u6027\u8f93\u51fa\u7684\u77e9\u9635\u3002", "result": "ECSF\u5f15\u5165\u4e86\u4e94\u4e2a\u68c0\u67e5\u70b9\uff1a\u5168\u5c40\u900f\u660e\u5ea6\u7528\u4e8e\u5371\u9669\u8bc6\u522b\u3001\u6848\u4f8b\u7ea7\u53ef\u89e3\u91ca\u6027\u7528\u4e8e\u9a8c\u8bc1\u3001\u4e34\u5e8a\u533b\u751f\u53ef\u7528\u6027\u7528\u4e8e\u8bc4\u4f30\u3001\u53ef\u8ffd\u6eaf\u51b3\u7b56\u8def\u5f84\u7528\u4e8e\u98ce\u9669\u63a7\u5236\u3001\u7eb5\u5411\u53ef\u89e3\u91ca\u6027\u76d1\u6d4b\u7528\u4e8e\u4e0a\u5e02\u540e\u76d1\u7763\u3002", "conclusion": "ECSF\u5c06\u53ef\u89e3\u91ca\u6027\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e34\u5e8a\u5b89\u5168\u4fdd\u8bc1\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u5f25\u5408\u4e86\u786e\u5b9a\u6027\u98ce\u9669\u6cbb\u7406\u4e0eAI\u6982\u7387\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u652f\u6301\u4e0eGMLP\u3001\u6b27\u76dfAI\u6cd5\u6848\u548cNHS AI\u4fdd\u8bc1\u539f\u5219\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.11830", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11830", "abs": "https://arxiv.org/abs/2511.11830", "authors": ["Bar\u0131\u015f Ata", "Wouter van Eekelen", "Yuan Zhong"], "title": "A Computational Method for Solving the Stochastic Joint Replenishment Problem in High Dimensions", "comment": "52 pages, 3 figures", "summary": "We consider a discrete-time formulation for a class of high-dimensional stochastic joint replenishment problems. First, we approximate the problem by a continuous-time impulse control problem. Exploiting connections among the impulse control problem, backward stochastic differential equations (BSDEs) with jumps, and the stochastic target problem, we develop a novel, simulation-based computational method that relies on deep neural networks to solve the impulse control problem. Based on that solution, we propose an implementable inventory control policy for the original (discrete-time) stochastic joint replenishment problem, and test it against the best available benchmarks in a series of test problems. For the problems studied thus far, our method matches or beats the best benchmark we could find, and it is computationally feasible up to at least 50 dimensions -- that is, 50 stock-keeping units (SKUs).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548cBSDE\u7684\u6a21\u62df\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u968f\u673a\u8054\u5408\u8865\u8d27\u95ee\u9898\uff0c\u572850\u7ef4\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u968f\u673a\u8054\u5408\u8865\u8d27\u95ee\u9898\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u5ea6\u60c5\u51b5\u3002", "method": "\u5c06\u79bb\u6563\u65f6\u95f4\u95ee\u9898\u8fd1\u4f3c\u4e3a\u8fde\u7eed\u65f6\u95f4\u8109\u51b2\u63a7\u5236\u95ee\u9898\uff0c\u5229\u7528BSDE\u4e0e\u8df3\u8dc3\u3001\u968f\u673a\u76ee\u6807\u95ee\u9898\u7684\u8054\u7cfb\uff0c\u5f00\u53d1\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u62df\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "\u5728\u6d4b\u8bd5\u95ee\u9898\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u57fa\u51c6\u65b9\u6cd5\uff0c\u8ba1\u7b97\u53ef\u884c\u7ef4\u5ea6\u8fbe\u5230\u81f3\u5c1150\u7ef4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u9ad8\u7ef4\u968f\u673a\u8054\u5408\u8865\u8d27\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u572850\u7ef4\u89c4\u6a21\u4e0b\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2511.12093", "categories": ["q-fin.PM", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.12093", "abs": "https://arxiv.org/abs/2511.12093", "authors": ["L\u00f3r\u00e1nt Nagy", "Mikl\u00f3s R\u00e1sonyi"], "title": "On the utility problem in a market where price impact is transient", "comment": null, "summary": "We consider a discrete-time model of a financial market where a risky asset is bought and sold with transactions having a transient price impact. It is shown that the corresponding utility maximization problem admits a solution. We manage to remove some unnatural restrictions on the market depth and resilience processes that were present in earlier work. A non-standard feature of the problem is that the set of attainable portfolio values may fail the convexity property.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5177\u6709\u77ac\u65f6\u4ef7\u683c\u51b2\u51fb\u7684\u91d1\u878d\u5e02\u573a\u4e2d\u7684\u6548\u7528\u6700\u5927\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u95ee\u9898\u89e3\u7684\u5b58\u5728\u6027\uff0c\u5e76\u79fb\u9664\u4e86\u5148\u524d\u7814\u7a76\u4e2d\u5173\u4e8e\u5e02\u573a\u6df1\u5ea6\u548c\u5f39\u6027\u8fc7\u7a0b\u7684\u4e0d\u81ea\u7136\u9650\u5236\u3002", "motivation": "\u7814\u7a76\u91d1\u878d\u5e02\u573a\u4e2d\u4ea4\u6613\u5bf9\u8d44\u4ea7\u4ef7\u683c\u4ea7\u751f\u77ac\u65f6\u51b2\u51fb\u65f6\u7684\u6548\u7528\u6700\u5927\u5316\u95ee\u9898\uff0c\u65e8\u5728\u514b\u670d\u5148\u524d\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u9650\u5236\u6761\u4ef6\uff0c\u63d0\u4f9b\u66f4\u4e00\u822c\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u91c7\u7528\u79bb\u6563\u65f6\u95f4\u6a21\u578b\uff0c\u8003\u8651\u98ce\u9669\u8d44\u4ea7\u4e70\u5356\u65f6\u4ea7\u751f\u7684\u77ac\u65f6\u4ef7\u683c\u51b2\u51fb\uff0c\u5206\u6790\u6548\u7528\u6700\u5927\u5316\u95ee\u9898\u7684\u53ef\u89e3\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u66f4\u4e00\u822c\u7684\u5e02\u573a\u6761\u4ef6\u4e0b\uff0c\u6548\u7528\u6700\u5927\u5316\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u89e3\uff0c\u6210\u529f\u79fb\u9664\u4e86\u5bf9\u5e02\u573a\u6df1\u5ea6\u548c\u5f39\u6027\u8fc7\u7a0b\u7684\u4e0d\u5fc5\u8981\u9650\u5236\u3002", "conclusion": "\u5c3d\u7ba1\u53ef\u8fbe\u5230\u7684\u6295\u8d44\u7ec4\u5408\u4ef7\u503c\u96c6\u5408\u53ef\u80fd\u4e0d\u6ee1\u8db3\u51f8\u6027\uff0c\u4f46\u5728\u66f4\u4e00\u822c\u7684\u5e02\u573a\u6761\u4ef6\u4e0b\uff0c\u5177\u6709\u77ac\u65f6\u4ef7\u683c\u51b2\u51fb\u7684\u6548\u7528\u6700\u5927\u5316\u95ee\u9898\u4ecd\u7136\u53ef\u89e3\u3002"}}
{"id": "2511.11594", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11594", "abs": "https://arxiv.org/abs/2511.11594", "authors": ["James McCammon"], "title": "TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy", "comment": null, "summary": "Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our \"Assisted Fuzzy\" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.", "AI": {"tldr": "\u63d0\u51fa\u4e86TimeStampEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u4ece\u957f\u6587\u672c\u8f6c\u5f55\u4e2d\u68c0\u7d22\u975e\u9010\u5b57\u5f15\u7528\u7684\u7cbe\u786e\u65f6\u95f4\u6233\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u540c\u65f6\u964d\u4f4e90%\u4ee5\u4e0a\u7684\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u7cca\u5339\u914d\u5728\u5904\u7406\u8bed\u4e49\u76f8\u540c\u4f46\u8bed\u6cd5\u4e0d\u540c\u7684\u5f15\u7528\u65f6\u5931\u8d25\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5bf9\u9f50\u5b98\u65b9\u4e66\u9762\u8bb0\u5f55\u548c\u8bed\u97f3\u8f6c\u6587\u5b57\u8f6c\u5f55\u7a3f\u65f6\u3002\u5e94\u7528\u573a\u666f\u662f\u81ea\u52a8\u5316\u957f\u64ad\u5ba2\uff0c\u5c06\u56fd\u4f1a\u8bb0\u5f55\u7247\u6bb5\u6c47\u7f16\u6210AI\u4e3b\u6301\u7684\u53d9\u8ff0\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528RapidFuzz\u8fdb\u884c\u9884\u8fc7\u6ee4\uff0c\u7136\u540e\u4f7f\u7528LLM\u5728\u77ed\u7247\u6bb5\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002\u63d0\u793a\u8bbe\u8ba1\u5c06\u67e5\u8be2\u653e\u5728\u8f6c\u5f55\u7a3f\u4e4b\u524d\u5e76\u4f7f\u7528\u7d27\u51d1\u683c\u5f0f\u3002", "result": "\u57282800\u53e5\u8f6c\u5f55\u7a3f\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a\u63d0\u793a\u8bbe\u8ba1\u6bd4\u6a21\u578b\u9009\u62e9\u66f4\u91cd\u8981\uff1b\u9002\u5ea6\u63a8\u7406\u9884\u7b97\u53ef\u5c06\u51c6\u786e\u7387\u4ece37%\u63d0\u5347\u523077%\u4ee5\u4e0a\uff1bAssisted Fuzzy\u65b9\u6cd5\u5c06\u6a21\u7cca\u5339\u914d\u51c6\u786e\u7387\u63d0\u9ad850\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5ef6\u8fdf\u51cf\u534a\uff0c\u6bcf\u4e2a\u6b63\u786e\u7ed3\u679c\u7684\u6210\u672c\u964d\u4f4e96%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u8f6c\u5f55\u957f\u5ea6\u3001\u8bcd\u6c47\u6f02\u79fb\u548c\u9886\u57df\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u572810\u4e2a\u8f6c\u5f55\u7a3f\u4e0a\u4fdd\u630195-100%\u7684\u62d2\u7edd\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11573", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11573", "abs": "https://arxiv.org/abs/2511.11573", "authors": ["Christopher R. Lee-Jenkins"], "title": "Softmax as a Lagrangian-Legendrian Seam", "comment": null, "summary": "This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian \"seam\" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06\u673a\u5668\u5b66\u4e60\u4e0e\u5fae\u5206\u51e0\u4f55\u8054\u7cfb\u8d77\u6765\uff0c\u5c55\u793a\u4e86softmax\u51fd\u6570\u4ecelogits\u5230\u6982\u7387\u7684\u8f6c\u6362\u8fc7\u7a0b\u53ef\u4ee5\u5efa\u6a21\u4e3a\u51e0\u4f55\u754c\u9762\uff1a\u4e24\u4e2a\u7531\u52bf\u80fd\u751f\u6210\u7684\u4fdd\u5b88\u63cf\u8ff0\u5728\u63a5\u89e6\u5c4f\u5e55\uff08\u6982\u7387\u5355\u7eaf\u5f62\uff09\u4e0a\u7684Legendreian\"\u63a5\u7f1d\"\u5904\u76f8\u9047\u3002", "motivation": "\u5efa\u7acb\u673a\u5668\u5b66\u4e60\u4e0e\u5fae\u5206\u51e0\u4f55\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u7406\u89e3softmax\u7b49\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7ec4\u4ef6\u63d0\u4f9b\u65b0\u7684\u51e0\u4f55\u89c6\u89d2\u3002", "method": "\u5c06softmax\u5efa\u6a21\u4e3a\u51e0\u4f55\u754c\u9762\uff0c\u4f7f\u7528\u8d1f\u71b5\u548clog-sum-exp\u4e24\u4e2a\u52bf\u80fd\u751f\u6210\u7684\u4fdd\u5b88\u63cf\u8ff0\uff0c\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u7684Legendreian\u63a5\u7f1d\u5904\u76f8\u9047\u3002\u504f\u7f6e\u5e73\u79fb\u4e0d\u53d8\u6027\u8868\u73b0\u4e3a\u5c4f\u5e55\u4e0a\u7684Reeb\u6d41\uff0cFenchel-Young\u7b49\u5f0f/KL\u6563\u5ea6\u63d0\u4f9b\u5230\u63a5\u7f1d\u7684\u53ef\u8ba1\u7b97\u8ddd\u79bb\u3002", "result": "\u6784\u5efa\u4e86\u4e8c\u7c7b\u548c\u4e09\u7c7b\u60c5\u51b5\u7684\u5177\u4f53\u51e0\u4f55\u6a21\u578b\uff0c\u5c55\u793a\u4e86softmax\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5305\u62ec\u504f\u7f6e\u5e73\u79fb\u4e0d\u53d8\u6027\u7684\u51e0\u4f55\u89e3\u91ca\u548c\u8ddd\u79bb\u5ea6\u91cf\u3002", "conclusion": "\u4e3a\u673a\u5668\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a\u7d27\u51d1logit\u6a21\u578b\uff08\u6295\u5f71\u6216\u7403\u5f62\uff09\u3001\u5168\u5c40\u4e0d\u53d8\u91cf\uff0c\u4ee5\u53ca\u4e0e\u4fe1\u606f\u51e0\u4f55\u7684\u8054\u7cfb\uff0c\u5176\u4e2d\u5c4f\u5e55\u4e0a\u7684\u52a8\u529b\u5b66\u8868\u73b0\u4e3a\u590d\u5236\u5b50\u6d41\u3002"}}
{"id": "2511.11591", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11591", "abs": "https://arxiv.org/abs/2511.11591", "authors": ["Olusola Babalola", "Bolanle Ojokoh", "Olutayo Boyinbode"], "title": "LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism", "comment": "50 pages, 19 figures, 9 tables", "summary": "This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u65b0\u95fb\u6807\u9898\u6570\u636e\u96c6\u6765\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u83b7\u53d6\u548c\u9690\u79c1\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u8d1f\u9762\u60c5\u611f\u6587\u672c\u3002", "motivation": "\u514b\u670d\u771f\u5b9e\u4e16\u754c\u6570\u636e\u83b7\u53d6\u7684\u6311\u6218\u548c\u9690\u79c1\u62c5\u5fe7\uff0c\u4e3a\u60c5\u611f\u5206\u6790\u7b49NLP\u4efb\u52a1\u63d0\u4f9b\u66ff\u4ee3\u6570\u636e\u6e90\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u63d0\u793a\u751f\u6210\u8d1f\u9762\u65b0\u95fb\u6807\u9898\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u548c\u5d4c\u5165\u7a7a\u95f4\u5206\u6790\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u771f\u5b9e\u65b0\u95fb\u6807\u9898\u8fdb\u884c\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u3002", "result": "\u751f\u6210\u7684\u6807\u9898\u4e0e\u771f\u5b9e\u6807\u9898\u9ad8\u5ea6\u5339\u914d\uff0c\u4ec5\u5728\u8bcd\u6027\u6807\u6ce8\u6d4b\u8bd5\u4e2d\u7684\u4e13\u6709\u540d\u8bcd\u5f97\u5206\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002", "conclusion": "LLM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u53ef\u4ee5\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u6709\u6548\u66ff\u4ee3\u54c1\uff0c\u5728\u5185\u5bb9\u3001\u8bed\u6c14\u3001\u957f\u5ea6\u548c\u98ce\u683c\u65b9\u9762\u4e0e\u771f\u5b9e\u8d1f\u9762\u65b0\u95fb\u6807\u9898\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2511.13568", "categories": ["math.OC", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2511.13568", "abs": "https://arxiv.org/abs/2511.13568", "authors": ["Daria Sakhanda", "Joshu\u00e9 Hel\u00ed Ricalde-Guerrero"], "title": "Infinite-Horizon Optimal Control of Jump-Diffusion Models for Pollution-Dependent Disasters", "comment": null, "summary": "The paper develops a unified framework for stochastic growth models with environmental risk, in which rare but catastrophic shocks interact with capital accumulation and pollution. The analysis begins with a Poisson process formulation, leading to a Hamilton-Jacobi-Bellman (HJB) equation with jump terms that admits closed-form candidate solutions and yields a composite state variable capturing exposure to rare shocks. The framework is then extended by endogenizing disaster intensity via a nonhomogeneous Poisson process, showing how environmental degradation amplifies macroeconomic risk and strengthens incentives for abatement. A further extension introduces pollution diffusion alongside state-dependent jump intensity, yielding a tractable jump-diffusion HJB that decomposes naturally into capital and pollution components under power-type value functions. Finally, a formulation in terms of Poisson random measures unifies the dynamics, makes arrivals and compensators explicit, and accommodates state-dependent magnitudes. Together, these results establish rigorous verification theorems, highlight how vulnerability emerges endogenously from the joint evolution of capital and pollution, and show that the prospect of rare, state-dependent disasters fundamentally reshapes optimal intertemporal trade-offs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u73af\u5883\u98ce\u9669\u7684\u968f\u673a\u589e\u957f\u6a21\u578b\u7edf\u4e00\u6846\u67b6\uff0c\u7814\u7a76\u7f55\u89c1\u4f46\u707e\u96be\u6027\u51b2\u51fb\u4e0e\u8d44\u672c\u79ef\u7d2f\u548c\u6c61\u67d3\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u7f55\u89c1\u707e\u96be\u6027\u73af\u5883\u51b2\u51fb\u5982\u4f55\u4e0e\u5b8f\u89c2\u7ecf\u6d4e\u52a8\u6001\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u53ca\u73af\u5883\u9000\u5316\u5982\u4f55\u653e\u5927\u5b8f\u89c2\u7ecf\u6d4e\u98ce\u9669\u5e76\u5f71\u54cd\u51cf\u6392\u6fc0\u52b1\u3002", "method": "\u4ece\u6cca\u677e\u8fc7\u7a0b\u5f00\u59cb\uff0c\u6269\u5c55\u5230\u5185\u751f\u707e\u96be\u5f3a\u5ea6\u7684\u975e\u9f50\u6b21\u6cca\u677e\u8fc7\u7a0b\uff0c\u518d\u5f15\u5165\u6c61\u67d3\u6269\u6563\u548c\u72b6\u6001\u4f9d\u8d56\u7684\u8df3\u8dc3\u5f3a\u5ea6\uff0c\u6700\u7ec8\u7528\u6cca\u677e\u968f\u673a\u6d4b\u5ea6\u7edf\u4e00\u52a8\u6001\u8fc7\u7a0b\u3002", "result": "\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u9a8c\u8bc1\u5b9a\u7406\uff0c\u63ed\u793a\u4e86\u8106\u5f31\u6027\u5982\u4f55\u4ece\u8d44\u672c\u548c\u6c61\u67d3\u7684\u8054\u5408\u6f14\u5316\u4e2d\u5185\u751f\u51fa\u73b0\uff0c\u72b6\u6001\u4f9d\u8d56\u7684\u7f55\u89c1\u707e\u96be\u4ece\u6839\u672c\u4e0a\u91cd\u5851\u4e86\u6700\u4f18\u8de8\u671f\u6743\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u6790\u73af\u5883\u98ce\u9669\u4e0e\u7ecf\u6d4e\u589e\u957f\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u7f55\u89c1\u707e\u96be\u7684\u524d\u666f\u4f1a\u663e\u8457\u6539\u53d8\u6700\u4f18\u653f\u7b56\u9009\u62e9\u3002"}}
{"id": "2511.12600", "categories": ["econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12600", "abs": "https://arxiv.org/abs/2511.12600", "authors": ["Marina Khismatullina", "Bernhard van der Sluis"], "title": "Multiscale Comparison of Nonparametric Trending Coefficients", "comment": null, "summary": "This paper proposes a novel framework to test for slope heterogeneity between time-varying coefficients in panel data models. Our test not only allows us to detect whether the coefficient functions are the same across all units or not, but also determines which of them are different and where these differences are located. We establish the asymptotic validity of our multiscale test. As an extension of the proposed procedure, we show how to use the results to uncover latent group structures in the model. We apply our methods to test for heterogeneity in the effect of U.S. monetary shocks on 49 foreign economies and itself. We find evidence that such heterogeneity indeed exists and we discuss the clustering results for two groups.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5c3a\u5ea6\u68c0\u9a8c\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u9762\u677f\u6570\u636e\u6a21\u578b\u4e2d\u65f6\u53d8\u7cfb\u6570\u7684\u659c\u7387\u5f02\u8d28\u6027\uff0c\u5e76\u80fd\u8bc6\u522b\u54ea\u4e9b\u5355\u5143\u5b58\u5728\u5dee\u5f02\u4ee5\u53ca\u5dee\u5f02\u7684\u4f4d\u7f6e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u9762\u677f\u6570\u636e\u4e2d\u65f6\u53d8\u7cfb\u6570\u7684\u5f02\u8d28\u6027\uff0c\u7279\u522b\u662f\u65e0\u6cd5\u786e\u5b9a\u54ea\u4e9b\u5355\u5143\u5b58\u5728\u5dee\u5f02\u4ee5\u53ca\u5dee\u5f02\u7684\u5177\u4f53\u4f4d\u7f6e\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u5c3a\u5ea6\u68c0\u9a8c\u65b9\u6cd5\uff0c\u5efa\u7acb\u6e10\u8fd1\u6709\u6548\u6027\uff0c\u5e76\u6269\u5c55\u7528\u4e8e\u63ed\u793a\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u7fa4\u7ec4\u7ed3\u6784\u3002", "result": "\u5e94\u7528\u8be5\u65b9\u6cd5\u68c0\u9a8c\u7f8e\u56fd\u8d27\u5e01\u653f\u7b56\u51b2\u51fb\u5bf949\u4e2a\u5916\u56fd\u7ecf\u6d4e\u4f53\u53ca\u81ea\u8eab\u7684\u5f71\u54cd\u5f02\u8d28\u6027\uff0c\u53d1\u73b0\u786e\u5b9e\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e24\u4e2a\u7fa4\u7ec4\u7684\u805a\u7c7b\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u9762\u677f\u6570\u636e\u4e2d\u65f6\u53d8\u7cfb\u6570\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u80fd\u8bc6\u522b\u5dee\u5f02\u7684\u5177\u4f53\u4f4d\u7f6e\u548c\u7fa4\u7ec4\u7ed3\u6784\u3002"}}
{"id": "2511.12490", "categories": ["q-fin.TR", "econ.GN", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2511.12490", "abs": "https://arxiv.org/abs/2511.12490", "authors": ["Mainak Singha"], "title": "Discovery of a 13-Sharpe OOS Factor: Drift Regimes Unlock Hidden Cross-Sectional Predictability", "comment": "This paper presents a regime-conditioned long short equity factor with out-of-sample Sharpe above 13, validated over 20 years with frozen parameters. We include conservative costs, impact modeling, stress tests, and capacity estimates of 100 to 500 million dollars with annualized returns above 70 percent. At 1 billion dollars, the annualized return is 33.6 percent. Feedback is welcome", "summary": "We document a high-performing cross-sectional equity factor that achieves out-of-sample Sharpe ratios above 13 through regime-conditional signal activation. The strategy combines value and short-term reversal signals only during stock-specific drift regimes, defined as periods when individual stocks show more than 60 percent positive days in trailing 63-day windows. Under these conditions, the factor delivers annualized returns of 158.6 percent with 12.0 percent volatility and a maximum drawdown of minus 11.9 percent. Using rigorous walk-forward validation across 20 years of S&P 500 data (2004 to 2024), we show performance roughly 13 times stronger than market benchmarks on a risk-adjusted basis, produced entirely out-of-sample with frozen parameters. The factor passes extensive robustness tests, including 1,000 randomization trials with p-values below 0.001, and maintains Sharpe ratios above 7 even under 30 percent parameter perturbations. Exposure to standard risk factors is negligible, with total R-squared values below 3 percent. We provide mechanistic evidence that drift regimes reshape market microstructure by amplifying behavioral biases, altering liquidity patterns, and creating conditions where cross-sectional price discovery becomes systematically exploitable. Conservative capacity estimates indicate deployable capital of 100 to 500 million dollars before noticeable performance degradation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5236\u5ea6\u6761\u4ef6\u4fe1\u53f7\u6fc0\u6d3b\u7684\u6a2a\u622a\u9762\u80a1\u7968\u56e0\u5b50\uff0c\u5728\u80a1\u7968\u7279\u5b9a\u6f02\u79fb\u5236\u5ea6\u4e0b\u7ed3\u5408\u4ef7\u503c\u548c\u77ed\u671f\u53cd\u8f6c\u4fe1\u53f7\uff0c\u5b9e\u73b0\u590f\u666e\u6bd4\u7387\u8d85\u8fc713\u7684\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5728\u5e02\u573a\u7279\u5b9a\u5236\u5ea6\u4e0b\u5982\u4f55\u901a\u8fc7\u6761\u4ef6\u4fe1\u53f7\u6fc0\u6d3b\u6765\u663e\u8457\u63d0\u5347\u56e0\u5b50\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u80a1\u7968\u51fa\u73b0\u6f02\u79fb\u884c\u4e3a\u65f6\u6355\u6349\u53ef\u9884\u6d4b\u7684\u4ef7\u683c\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u5236\u5ea6\u6761\u4ef6\u4fe1\u53f7\u6fc0\u6d3b\u65b9\u6cd5\uff0c\u5728\u4e2a\u80a1\u51fa\u73b0\u8d85\u8fc760%\u6b63\u6536\u76ca\u65e5\u768463\u5929\u7a97\u53e3\u671f\uff08\u6f02\u79fb\u5236\u5ea6\uff09\u4e0b\uff0c\u7ed3\u5408\u4ef7\u503c\u548c\u77ed\u671f\u53cd\u8f6c\u4fe1\u53f7\u6784\u5efa\u56e0\u5b50\uff0c\u5e76\u901a\u8fc720\u5e74S&P 500\u6570\u636e\u7684\u4e25\u683c\u524d\u5411\u9a8c\u8bc1\u3002", "result": "\u56e0\u5b50\u5e74\u5316\u6536\u76ca158.6%\uff0c\u6ce2\u52a8\u738712.0%\uff0c\u6700\u5927\u56de\u64a4-11.9%\uff0c\u590f\u666e\u6bd4\u7387\u8d85\u8fc713\uff0c\u98ce\u9669\u8c03\u6574\u540e\u8868\u73b0\u6bd4\u5e02\u573a\u57fa\u51c6\u5f3a\u7ea613\u500d\uff0c\u901a\u8fc71000\u6b21\u968f\u673a\u5316\u68c0\u9a8c\uff08p\u503c<0.001\uff09\u3002", "conclusion": "\u6f02\u79fb\u5236\u5ea6\u901a\u8fc7\u653e\u5927\u884c\u4e3a\u504f\u5dee\u3001\u6539\u53d8\u6d41\u52a8\u6027\u6a21\u5f0f\uff0c\u521b\u9020\u4e86\u7cfb\u7edf\u6027\u53ef\u5f00\u53d1\u7684\u4ef7\u683c\u53d1\u73b0\u6761\u4ef6\uff0c\u8be5\u56e0\u5b50\u5177\u6709\u7a33\u5065\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\uff081-5\u4ebf\u7f8e\u5143\u5bb9\u91cf\uff09\u3002"}}
{"id": "2511.12129", "categories": ["q-fin.TR", "q-fin.CP", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2511.12129", "abs": "https://arxiv.org/abs/2511.12129", "authors": ["Hongyang Yang", "Xiao-Yang Liu", "Qingwei Wu"], "title": "A Practical Machine Learning Approach for Dynamic Stock Recommendation", "comment": "Accepted by IEEE TrustCom/BigDataSE 2018. Supported by AI4Finance Foundation", "summary": "Stock recommendation is vital to investment companies and investors. However, no single stock selection strategy will always win while analysts may not have enough time to check all S&P 500 stocks (the Standard & Poor's 500). In this paper, we propose a practical scheme that recommends stocks from S&P 500 using machine learning. Our basic idea is to buy and hold the top 20% stocks dynamically. First, we select representative stock indicators with good explanatory power. Secondly, we take five frequently used machine learning methods, including linear regression, ridge regression, stepwise regression, random forest and generalized boosted regression, to model stock indicators and quarterly log-return in a rolling window. Thirdly, we choose the model with the lowest Mean Square Error in each period to rank stocks. Finally, we test the selected stocks by conducting portfolio allocation methods such as equally weighted, mean-variance, and minimum-variance. Our empirical results show that the proposed scheme outperforms the long-only strategy on the S&P 500 index in terms of Sharpe ratio and cumulative returns. This work is fully open-sourced at \\href{https://github.com/AI4Finance-Foundation/Dynamic-Stock-Recommendation-Machine_Learning-Published-Paper-IEEE}{GitHub}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u52a8\u6001\u80a1\u7968\u63a8\u8350\u65b9\u6848\uff0c\u901a\u8fc7\u6eda\u52a8\u7a97\u53e3\u5efa\u6a21\u9009\u62e9\u524d20%\u80a1\u7968\uff0c\u5728S&P 500\u6307\u6570\u4e0a\u8868\u73b0\u4f18\u4e8e\u957f\u671f\u6301\u6709\u7b56\u7565\u3002", "motivation": "\u5355\u4e00\u80a1\u7968\u9009\u62e9\u7b56\u7565\u65e0\u6cd5\u59cb\u7ec8\u83b7\u80dc\uff0c\u4e14\u5206\u6790\u5e08\u6ca1\u6709\u8db3\u591f\u65f6\u95f4\u5206\u6790\u6240\u6709S&P 500\u80a1\u7968\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9009\u62e9\u4ee3\u8868\u6027\u80a1\u7968\u6307\u6807\uff0c\u4f7f\u75285\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u7ebf\u6027\u56de\u5f52\u3001\u5cad\u56de\u5f52\u3001\u9010\u6b65\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u5e7f\u4e49\u63d0\u5347\u56de\u5f52\uff09\u5728\u6eda\u52a8\u7a97\u53e3\u5efa\u6a21\uff0c\u9009\u62e9MSE\u6700\u4f4e\u7684\u6a21\u578b\u8fdb\u884c\u80a1\u7968\u6392\u540d\uff0c\u91c7\u7528\u7b49\u6743\u91cd\u3001\u5747\u503c\u65b9\u5dee\u3001\u6700\u5c0f\u65b9\u5dee\u7b49\u6295\u8d44\u7ec4\u5408\u5206\u914d\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6848\u5728\u590f\u666e\u6bd4\u7387\u548c\u7d2f\u8ba1\u6536\u76ca\u65b9\u9762\u4f18\u4e8eS&P 500\u6307\u6570\u7684\u957f\u671f\u6301\u6709\u7b56\u7565\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u52a8\u6001\u63a8\u8350\u80a1\u7968\uff0c\u4e3a\u6295\u8d44\u51b3\u7b56\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2511.11817", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11817", "abs": "https://arxiv.org/abs/2511.11817", "authors": ["Zhongde An", "Jinhong You", "Jiyanglin Li", "Yiming Tang", "Wen Li", "Heming Du", "Shouguo Du"], "title": "FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition", "comment": null, "summary": "Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\\%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9891\u7387\u5206\u89e3\u7f51\u7edc(FreDN)\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9891\u7387\u89e3\u8026\u5668\u5728\u9891\u57df\u5206\u79bb\u8d8b\u52bf\u548c\u5468\u671f\u5206\u91cf\uff0c\u5e76\u5f15\u5165ReIm\u5757\u964d\u4f4e\u590d\u6570\u8fd0\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd510%\u4ee5\u4e0a\uff0c\u540c\u65f6\u51cf\u5c1150%\u4ee5\u4e0a\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u9891\u57df\u65b9\u6cd5\u5728\u5904\u7406\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u65f6\u9762\u4e34\u9891\u8c31\u7ea0\u7f20\u548c\u590d\u6570\u5b66\u4e60\u8ba1\u7b97\u8d1f\u62c5\u7684\u95ee\u9898\uff0c\u9891\u8c31\u7ea0\u7f20\u6307\u8d8b\u52bf\u3001\u5468\u671f\u6027\u548c\u566a\u58f0\u5728\u9891\u8c31\u4e0a\u7684\u91cd\u53e0\uff0c\u73b0\u6709\u5206\u89e3\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u53ef\u5b66\u4e60\u7684\u9891\u7387\u89e3\u8026\u5668\u6a21\u5757\u76f4\u63a5\u5728\u9891\u57df\u5206\u79bb\u8d8b\u52bf\u548c\u5468\u671f\u5206\u91cf\uff1b2. ReIm\u5757\u901a\u8fc7\u7406\u8bba\u652f\u6301\u7684\u5b9e\u90e8-\u865a\u90e8\u5171\u4eab\u53c2\u6570\u8bbe\u8ba1\u964d\u4f4e\u590d\u6570\u8fd0\u7b97\u590d\u6742\u5ea6\uff1b3. \u91cd\u65b0\u5ba1\u89c6\u9891\u57df\u635f\u5931\u51fd\u6570\u5e76\u63d0\u4f9b\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u3002", "result": "\u57287\u4e2a\u957f\u671f\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFreDN\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe10%\uff1b\u4e0e\u6807\u51c6\u590d\u6570\u67b6\u6784\u76f8\u6bd4\uff0c\u5b9e\u90e8-\u865a\u90e8\u5171\u4eab\u53c2\u6570\u8bbe\u8ba1\u51cf\u5c11\u81f3\u5c1150%\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "FreDN\u901a\u8fc7\u9891\u57df\u5206\u89e3\u6709\u6548\u89e3\u51b3\u4e86\u9891\u8c31\u7ea0\u7f20\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u9ad8\u6548\u7684\u590d\u6570\u8fd0\u7b97\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11875", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11875", "abs": "https://arxiv.org/abs/2511.11875", "authors": ["Elena Petri", "Koen J. A. Scheres", "Erik Steur", "W. P. M. H.", "Heemels"], "title": "Emulation-based Neuromorphic Control for the Stabilization of LTI Systems", "comment": null, "summary": "Brain-inspired neuromorphic technologies can offer important advantages over classical digital clock-based technologies in various domains, including systems and control engineering. Indeed, neuromorphic engineering could provide low-latency, low-energy and adaptive control systems in the form of spiking neural networks (SNNs) exploiting spike-based control and communication. However, systematic methods for designing and analyzing neuron-inspired spiking controllers are currently lacking. This paper presents a new systematic approach for stabilizing linear time-invariant (LTI) systems using SNN-based controllers, designed as a network of integrate-and-fire neurons, whose input is the measured output from the plant and generating spiking control signals. The new approach consists of a two-step emulation-based design procedure. In the first step, we establish conditions on the neuron parameters to ensure that the spiking signal generated by a pair of neurons emulates any continuous-time signal input to the neurons with arbitrary accuracy in terms of a special metric for spiky signals. In the second step, we propose a novel stability notion, called integral spiking-input-to-state stability (iSISS) building on this special metric. We prove that an asymptotically stable LTI system has this iSISS property. By combining these steps, a certifiable practical stability property of the closed-loop system can be established. Generalizations are discussed and the effectiveness of the approach is illustrated in a numerical case study.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u63a7\u5236\u5668\u6765\u7a33\u5b9a\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u6b65\u9aa4\u8bbe\u8ba1\u786e\u4fdd\u95ed\u73af\u7cfb\u7edf\u7684\u53ef\u9a8c\u8bc1\u7a33\u5b9a\u6027\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u6280\u672f\u76f8\u6bd4\u4f20\u7edf\u6570\u5b57\u65f6\u949f\u6280\u672f\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u80fd\u8017\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7684\u4f18\u52bf\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u8109\u51b2\u63a7\u5236\u5668\u8bbe\u8ba1\u548c\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u9aa4\u4eff\u771f\u8bbe\u8ba1\uff1a1) \u5efa\u7acb\u795e\u7ecf\u5143\u53c2\u6570\u6761\u4ef6\uff0c\u786e\u4fdd\u795e\u7ecf\u5143\u5bf9\u751f\u6210\u8109\u51b2\u4fe1\u53f7\u80fd\u7cbe\u786e\u6a21\u62df\u8fde\u7eed\u65f6\u95f4\u4fe1\u53f7\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u7279\u6b8a\u8109\u51b2\u4fe1\u53f7\u5ea6\u91cf\u7684\u79ef\u5206\u8109\u51b2\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\u6982\u5ff5\u3002", "result": "\u8bc1\u660e\u4e86\u6e10\u8fd1\u7a33\u5b9a\u7684LTI\u7cfb\u7edf\u5177\u6709iSISS\u7279\u6027\uff0c\u7ed3\u5408\u4e24\u6b65\u9aa4\u8bbe\u8ba1\u53ef\u4ee5\u5efa\u7acb\u95ed\u73af\u7cfb\u7edf\u7684\u53ef\u9a8c\u8bc1\u5b9e\u9645\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8bbe\u8ba1\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6848\u4f8b\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2511.11595", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11595", "abs": "https://arxiv.org/abs/2511.11595", "authors": ["Aaron R. Allred", "Erin E. Richardson", "Sarah R. Bostrom", "James Crum", "Cara Spencer", "Chad Tossell", "Richard E. Niemeyer", "Leanne Hirshfield", "Allison P. A. Hayman"], "title": "Decision-Making Amid Information-Based Threats in Sociotechnical Systems: A Review", "comment": null, "summary": "Technological systems increasingly mediate human information exchange, spanning interactions among humans as well as between humans and artificial agents. The unprecedented scale and reliance on information disseminated through these systems substantially expand the scope of information-based influence that can both enable and undermine sound decision-making. Consequently, understanding and protecting decision-making today faces growing challenges, as individuals and organizations must navigate evolving opportunities and information-based threats across varied domains and information environments. While these risks are widely recognized, research remains fragmented: work evaluating information-based threat phenomena has progressed largely in isolation from foundational studies of human information processing. In this review, we synthesize insights from both domains to identify shared cognitive mechanisms that mediate vulnerability to information-based threats and shape behavioral outcomes. Finally, we outline directions for future research aimed at integrating these perspectives, emphasizing the importance of such integration for mitigating human vulnerabilities and aligning human-machine representations.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6280\u672f\u7cfb\u7edf\u4e2d\u4fe1\u606f\u4f20\u64ad\u5bf9\u4eba\u7c7b\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u6574\u5408\u4e86\u4fe1\u606f\u5a01\u80c1\u7814\u7a76\u548c\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u7814\u7a76\uff0c\u8bc6\u522b\u4e86\u5171\u540c\u7684\u8ba4\u77e5\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6280\u672f\u7cfb\u7edf\u65e5\u76ca\u4ecb\u5165\u4eba\u7c7b\u4fe1\u606f\u4ea4\u6362\uff0c\u6269\u5927\u4e86\u4fe1\u606f\u5f71\u54cd\u7684\u8303\u56f4\uff0c\u65e2\u53ef\u80fd\u4fc3\u8fdb\u4e5f\u53ef\u80fd\u7834\u574f\u51b3\u7b56\u8d28\u91cf\u3002\u5f53\u524d\u7814\u7a76\u788e\u7247\u5316\uff0c\u4fe1\u606f\u5a01\u80c1\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u57fa\u7840\u7814\u7a76\u76f8\u4e92\u9694\u79bb\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u6574\u5408\u4fe1\u606f\u5a01\u80c1\u7814\u7a76\u548c\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u7814\u7a76\u4e24\u4e2a\u9886\u57df\u7684\u89c1\u89e3\uff0c\u8bc6\u522b\u5171\u4eab\u7684\u8ba4\u77e5\u673a\u5236\u3002", "result": "\u786e\u5b9a\u4e86\u4ecb\u5bfc\u4fe1\u606f\u5a01\u80c1\u8106\u5f31\u6027\u548c\u5f71\u54cd\u884c\u4e3a\u7ed3\u679c\u7684\u5171\u540c\u8ba4\u77e5\u673a\u5236\u3002", "conclusion": "\u9700\u8981\u6574\u5408\u8fd9\u4e24\u4e2a\u7814\u7a76\u89c6\u89d2\u6765\u51cf\u8f7b\u4eba\u7c7b\u8106\u5f31\u6027\u5e76\u534f\u8c03\u4eba\u673a\u8868\u5f81\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.11870", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.11870", "abs": "https://arxiv.org/abs/2511.11870", "authors": ["Bernard T. Agyeman", "Zhe Li", "Ilias Mitrai", "Prodromos Daoutidis"], "title": "Graph-Based Imitation and Reinforcement Learning for Efficient Benders Decomposition", "comment": null, "summary": "This work introduces an end-to-end graph-based agent for accelerating the computational efficiency of Benders Decomposition. The agent's policy is parameterized by a graph neural network which takes as input a bipartite graph representation of the master problem and proposes a candidate solution. The agent is trained using a two-stage approach that combines imitation (IL) and reinforcement learning (RL). IL is used to mimic a solver and obtain a warm-start policy which is then finetuned using RL with a reward signal that balances feasibility and computational efficiency. We augment the agent with a verification mechanism that checks the agent's prediction for feasibility and solution quality. The framework is evaluated in two case studies: (i) an illustrative mixed-integer nonlinear program, where it reduces the solution time by 42% without loss of solution quality, and (ii) a closed-loop irrigation scheduling problem, where it achieves a 23% time reduction without compromising water use efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7aef\u5230\u7aef\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u52a0\u901fBenders\u5206\u89e3\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\u548c\u704c\u6e89\u8c03\u5ea6\u95ee\u9898\u4e2d\u5206\u522b\u51cf\u5c1142%\u548c23%\u7684\u6c42\u89e3\u65f6\u95f4\u3002", "motivation": "Benders\u5206\u89e3\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u65f6\u8ba1\u7b97\u6548\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u7b56\u7565\uff0c\u8f93\u5165\u4e3b\u95ee\u9898\u7684\u4e8c\u5206\u56fe\u8868\u793a\uff0c\u63d0\u51fa\u5019\u9009\u89e3\u3002\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u52a0\u5165\u9a8c\u8bc1\u673a\u5236\u68c0\u67e5\u89e3\u7684\u53ef\u884c\u6027\u548c\u8d28\u91cf\u3002", "result": "\u5728\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\u4e2d\u51cf\u5c1142%\u6c42\u89e3\u65f6\u95f4\uff0c\u5728\u95ed\u73af\u704c\u6e89\u8c03\u5ea6\u95ee\u9898\u4e2d\u51cf\u5c1123%\u6c42\u89e3\u65f6\u95f4\uff0c\u4e14\u4e0d\u635f\u5931\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u56fe\u795e\u7ecf\u7f51\u7edc\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u52a0\u901fBenders\u5206\u89e3\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u4fdd\u6301\u89e3\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6c42\u89e3\u65f6\u95f4\u3002"}}
{"id": "2511.13334", "categories": ["q-fin.PM"], "pdf": "https://arxiv.org/pdf/2511.13334", "abs": "https://arxiv.org/abs/2511.13334", "authors": ["Florent Segonne"], "title": "Basis Immunity: Isotropy as a Regularizer for Uncertainty", "comment": null, "summary": "Diversification is a cornerstone of robust portfolio construction, yet its application remains fraught with challenges due to model uncertainty and estimation errors. Practitioners often rely on sophisticated, proprietary heuristics to navigate these issues. Among recent advancements, Agnostic Risk Parity introduces eigenrisk parity (ERP), an innovative approach that leverages isotropy to evenly allocate risk across eigenmodes, enhancing portfolio stability.\n  In this paper, we review and extend the isotropy-enforced philosophy of ERP proposing a versatile framework that integrates mean-variance optimization with an isotropy constraint acting as a geometric regularizer against signal uncertainty. The resulting allocations decompose naturally into canonical portfolios, smoothly interpolating between full isotropy (closed-form isotropic-mean allocation) and pure mean-variance through a tunable isotropy penalty.\n  Beyond methodology, we revisit fundamental concepts and clarify foundational links between isotropy, canonical portfolios, principal portfolios, primal versus dual representations, and intrinsic basis-invariant metrics for returns, risk, and isotropy. Applied to sector trend-following, the isotropy constraint systematically induces negative average-signal exposure -- a structural, parameter-robust crash hedge.\n  This work offers both a practical, theoretically grounded tool for resilient allocation under signal uncertainty and a pedagogical synthesis of modern portfolio concepts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u4e0e\u5404\u5411\u540c\u6027\u7ea6\u675f\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4f5c\u4e3a\u5bf9\u6297\u4fe1\u53f7\u4e0d\u786e\u5b9a\u6027\u7684\u51e0\u4f55\u6b63\u5219\u5316\u5668\uff0c\u751f\u6210\u5728\u5b8c\u5168\u5404\u5411\u540c\u6027\u548c\u7eaf\u5747\u503c-\u65b9\u5dee\u4e4b\u95f4\u5e73\u6ed1\u63d2\u503c\u7684\u5206\u914d\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u6295\u8d44\u7ec4\u5408\u591a\u6837\u5316\u9762\u4e34\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u4f30\u8ba1\u8bef\u5dee\u7684\u6311\u6218\uff0c\u9700\u8981\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u4fe1\u53f7\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u6269\u5c55\u4e86\u5404\u5411\u540c\u6027\u98ce\u9669\u5e73\u4ef7(ERP)\u7406\u5ff5\uff0c\u5c06\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u4e0e\u5404\u5411\u540c\u6027\u7ea6\u675f\u96c6\u6210\uff0c\u901a\u8fc7\u53ef\u8c03\u7684\u5404\u5411\u540c\u6027\u60e9\u7f5a\u53c2\u6570\u751f\u6210\u89c4\u8303\u6295\u8d44\u7ec4\u5408\u3002", "result": "\u5e94\u7528\u4e8e\u884c\u4e1a\u8d8b\u52bf\u8ddf\u8e2a\u65f6\uff0c\u5404\u5411\u540c\u6027\u7ea6\u675f\u7cfb\u7edf\u5730\u8bf1\u5bfc\u8d1f\u5e73\u5747\u4fe1\u53f7\u66b4\u9732\uff0c\u5f62\u6210\u7ed3\u6784\u6027\u7684\u3001\u53c2\u6570\u7a33\u5065\u7684\u5d29\u76d8\u5bf9\u51b2\u673a\u5236\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5728\u4fe1\u53f7\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u5f39\u6027\u5206\u914d\u7684\u5b9e\u9645\u5de5\u5177\uff0c\u5e76\u5bf9\u73b0\u4ee3\u6295\u8d44\u7ec4\u5408\u6982\u5ff5\u8fdb\u884c\u4e86\u6559\u5b66\u6027\u7efc\u5408\u3002"}}
{"id": "2511.11793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11793", "abs": "https://arxiv.org/abs/2511.11793", "authors": ["MiroMind Team", "Song Bai", "Lidong Bing", "Carson Chen", "Guanzheng Chen", "Yuntao Chen", "Zhe Chen", "Ziyi Chen", "Jifeng Dai", "Xuan Dong", "Yue Deng", "Yunjie Fu", "Junqi Ge", "Chenxia Han", "Tammy Huang", "Zhenhang Huang", "Jerry Jiao", "Shilei Jiang", "Tianyu Jiao", "Xiaoqi Jian", "Lei Lei", "Ruilin Li", "Ryan Luo", "Tiantong Li", "Xiang Lin", "Ziyuan Liu", "Zhiqi Li", "Jie Ni", "Qiang Ren", "Pax Sun", "Shiqian Su", "Chenxin Tao", "Bin Wang", "Hellen Wang", "Haonan Wang", "James Wang", "Jin Wang", "Jojo Wang", "Letian Wang", "Shizun Wang", "Weizhi Wang", "Zixuan Wang", "Jinfan Xu", "Sen Xing", "Chenyu Yang", "Hai Ye", "Jiaheng Yu", "Yue Yu", "Muyan Zhong", "Tianchen Zhao", "Xizhou Zhu", "Yanpeng Zhou", "Yifan Zhang", "Zhi Zhu"], "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "comment": "Technical Report", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "AI": {"tldr": "MiroThinker v1.0\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6269\u5c55\u4f5c\u4e3a\u6027\u80fd\u63d0\u5347\u7684\u7b2c\u4e09\u7ef4\u5ea6\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u5148\u524d\u5f00\u6e90\u4ee3\u7406\u5e76\u63a5\u8fd1\u5546\u4e1a\u5bf9\u5e94\u7269\u3002", "motivation": "\u63a2\u7d22\u6a21\u578b\u5c42\u9762\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u4f5c\u4e3a\u6a21\u578b\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e4b\u5916\u7684\u7b2c\u4e09\u4e2a\u6027\u80fd\u63d0\u5347\u7ef4\u5ea6\uff0c\u4ee5\u89e3\u51b3LLM\u6d4b\u8bd5\u65f6\u6269\u5c55\u5728\u957f\u63a8\u7406\u94fe\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u9000\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4ea4\u4e92\u6269\u5c55\uff0c\u4f7f\u7528256K\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u6bcf\u4e2a\u4efb\u52a1\u6700\u591a\u53ef\u6267\u884c600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u652f\u6301\u6301\u7eed\u591a\u8f6e\u63a8\u7406\u548c\u590d\u6742\u7814\u7a76\u5de5\u4f5c\u6d41\u3002", "result": "\u5728GAIA\u3001HLE\u3001BrowseComp\u548cBrowseComp-ZH\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c72B\u53d8\u4f53\u5206\u522b\u8fbe\u523081.9%\u300137.7%\u300147.1%\u548c55.6%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u5148\u524d\u5f00\u6e90\u4ee3\u7406\u5e76\u63a5\u8fd1GPT-5-high\u7b49\u5546\u4e1a\u5bf9\u5e94\u7269\u3002", "conclusion": "\u4ea4\u4e92\u6df1\u5ea6\u5c55\u73b0\u51fa\u4e0e\u6a21\u578b\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7c7b\u4f3c\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u786e\u7acb\u4ea4\u4e92\u6269\u5c55\u4f5c\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\u7684\u7b2c\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002"}}
{"id": "2511.11574", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11574", "abs": "https://arxiv.org/abs/2511.11574", "authors": ["Viviana Luccioli", "Rithika Iyengar", "Ryan Panley", "Flora Haberkorn", "Xiaoyu Ge", "Leland Crane", "Nitish Sinha", "Seung Jung Lee"], "title": "LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora", "comment": null, "summary": "Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM \"teacher\" trains a smaller and more efficient \"student\" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.", "AI": {"tldr": "\u63d0\u51fa\u4e86M-RARU\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u968f\u673a\u63a5\u53d7-\u62d2\u7edd\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u7684\u6210\u672c\uff0c\u51cf\u5c1180%\u6837\u672c\u9700\u6c42\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u8ba1\u7b97\u548c\u8d22\u52a1\u6210\u672c\u963b\u788d\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u672c\u8eab\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u6559\u5e08\u6a21\u578b\u6807\u6ce8\u5927\u91cf\u6837\u672c\u5e76\u6d88\u8017\u5927\u91cftoken\u3002", "method": "\u5f15\u5165M-RARU\uff08\u591a\u7c7b\u968f\u673a\u63a5\u53d7/\u62d2\u7edd\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u968f\u673a\u63a5\u53d7-\u62d2\u7edd\u673a\u5236\uff0c\u53ea\u9009\u62e9\u6700\u6709\u4fe1\u606f\u91cf\u7684\u6570\u636e\u70b9\u4f9bLLM\u6559\u5e08\u6807\u6ce8\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u5b66\u751f\u6a21\u578b\u548c\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u968f\u673a\u91c7\u6837\uff0c\u8be5\u65b9\u6cd5\u53ef\u51cf\u5c1180%\u6837\u672c\u9700\u6c42\uff0c\u663e\u8457\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u8d22\u52a1\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "M-RARU\u4e3a\u964d\u4f4eLLM\u77e5\u8bc6\u84b8\u998f\u6210\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2511.11597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11597", "abs": "https://arxiv.org/abs/2511.11597", "authors": ["Michelle Chen Huebscher", "Katharine Mach", "Aleksandar Stani\u0107", "Markus Leippold", "Ben Gaiarin", "Zeke Hausfather", "Elisa Rawat", "Erich Fischer", "Massimiliano Ciaramita", "Joeri Rogelj", "Christian Buck", "Lierni Sestorain Saralegui", "Reto Knutti"], "title": "CLINB: A Climate Intelligence Benchmark for Foundational Models", "comment": "Questions, system prompt and model judge prompts available here: https://www.kaggle.com/datasets/deepmind/clinb-questions", "summary": "Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform \"hybrid\" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.", "AI": {"tldr": "CLINB\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6c14\u5019\u53d8\u5316\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u8bc1\u636e\u652f\u6301\u80fd\u529b\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5177\u6709\u535a\u58eb\u7ea7\u522b\u7684\u77e5\u8bc6\u7efc\u5408\u80fd\u529b\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u8bc1\u636e\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u4e13\u4e1a\u77e5\u8bc6\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6c14\u5019\u53d8\u5316\u8fd9\u4e00\u5173\u952e\u9886\u57df\uff0c\u9700\u8981\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u95ee\u9898\u548c\u79d1\u5b66\u8bc4\u4f30\u6807\u51c6\u7684\u53ef\u9760\u57fa\u51c6\u3002", "method": "\u5f00\u53d1CLINB\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u7528\u6237\u95ee\u9898\u3001\u591a\u6a21\u6001\u95ee\u7b54\u4efb\u52a1\u548c\u6c14\u5019\u79d1\u5b66\u5bb6\u5236\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u6d41\u7a0b\u9a8c\u8bc1\u591a\u4e2a\u524d\u6cbf\u6a21\u578b\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5c55\u73b0\u51fa\u535a\u58eb\u7ea7\u522b\u7684\u77e5\u8bc6\u7efc\u5408\u80fd\u529b\uff0c\u751a\u81f3\u8d85\u8fc7\u4e13\u5bb6\u8f85\u52a9\u7684\u6df7\u5408\u7b54\u6848\uff0c\u4f46\u5728\u8bc1\u636e\u57fa\u7840\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u5f15\u7528\u548c\u56fe\u50cf\u5e7b\u89c9\u7387\u5f88\u9ad8\u3002", "conclusion": "\u77e5\u8bc6\u7efc\u5408\u4e0e\u53ef\u9a8c\u8bc1\u5f52\u56e0\u4e4b\u95f4\u7684\u5dee\u8ddd\u662fAI\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\uff0c\u9700\u8981\u50cfCLINB\u8fd9\u6837\u7684\u53ef\u9760\u57fa\u51c6\u6765\u6784\u5efa\u53ef\u4fe1AI\u7cfb\u7edf\u3002"}}
{"id": "2511.12640", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2511.12640", "abs": "https://arxiv.org/abs/2511.12640", "authors": ["Sofiia Dolgikh", "Bodan Potanin"], "title": "Double machine learning for causal inference in a multivariate sample selection model", "comment": null, "summary": "We propose plug-in (PI) and double machine learning (DML) estimators of average treatment effect (ATE), average treatment effect on the treated (ATET) and local average treatment effect (LATE) in the multivariate sample selection model with ordinal selection equations. Our DML estimators are doubly-robust and based on the efficient influence functions. Finite sample properties of the proposed estimators are studied and compared on simulated data. Specifically, the results of the analysis suggest that without addressing multivariate sample selection, the estimates of the causal parameters may be highly biased. However, the proposed estimators allow us to avoid these biases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5728\u5177\u6709\u5e8f\u6570\u9009\u62e9\u65b9\u7a0b\u7684\u591a\u53d8\u91cf\u6837\u672c\u9009\u62e9\u6a21\u578b\u4e2d\u4f30\u8ba1ATE\u3001ATET\u548cLATE\u7684PI\u548cDML\u4f30\u8ba1\u5668\uff0cDML\u4f30\u8ba1\u5668\u5177\u6709\u53cc\u91cd\u7a33\u5065\u6027\u3002", "motivation": "\u5728\u591a\u53d8\u91cf\u6837\u672c\u9009\u62e9\u6a21\u578b\u4e2d\uff0c\u5982\u679c\u4e0d\u5904\u7406\u6837\u672c\u9009\u62e9\u95ee\u9898\uff0c\u56e0\u679c\u53c2\u6570\u7684\u4f30\u8ba1\u53ef\u80fd\u5b58\u5728\u4e25\u91cd\u504f\u5dee\u3002", "method": "\u4f7f\u7528\u63d2\u4ef6(PI)\u4f30\u8ba1\u5668\u548c\u53cc\u91cd\u673a\u5668\u5b66\u4e60(DML)\u4f30\u8ba1\u5668\uff0cDML\u57fa\u4e8e\u6709\u6548\u5f71\u54cd\u51fd\u6570\u5e76\u5177\u6709\u53cc\u91cd\u7a33\u5065\u6027\u3002", "result": "\u6a21\u62df\u6570\u636e\u5206\u6790\u8868\u660e\uff0c\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u80fd\u591f\u907f\u514d\u6837\u672c\u9009\u62e9\u5e26\u6765\u7684\u504f\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684PI\u548cDML\u4f30\u8ba1\u5668\u5728\u591a\u53d8\u91cf\u6837\u672c\u9009\u62e9\u6a21\u578b\u4e2d\u80fd\u6709\u6548\u4f30\u8ba1\u56e0\u679c\u53c2\u6570\uff0c\u907f\u514d\u9009\u62e9\u504f\u5dee\u3002"}}
{"id": "2511.12876", "categories": ["cs.AI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.12876", "abs": "https://arxiv.org/abs/2511.12876", "authors": ["Heyang Ma", "Qirui Mi", "Qipeng Yang", "Zijun Fan", "Bo Li", "Haifeng Zhang"], "title": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making", "comment": "Extended version of a submission to AAAI 2026", "summary": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.", "AI": {"tldr": "LAMP\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u7ecf\u6d4e\u51b3\u7b56\u4e2d\u6574\u5408\u8bed\u8a00\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u6548\u679c\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u5b9e\u7ecf\u6d4e\u51b3\u7b56\u4e0d\u4ec5\u4f9d\u8d56\u7ed3\u6784\u5316\u4fe1\u53f7\uff08\u5982\u4ef7\u683c\u3001\u7a0e\u6536\uff09\uff0c\u8fd8\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u8bed\u8a00\u4fe1\u606f\uff08\u5982\u540c\u884c\u5bf9\u8bdd\u3001\u5a92\u4f53\u53d9\u8ff0\uff09\u3002\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u5904\u7406\u8bed\u8a00\u7684\u8bed\u4e49\u6a21\u7cca\u6027\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\u3002", "method": "LAMP\u91c7\u7528Think-Speak-Decide\u6d41\u7a0b\uff1a(1)Think\u6a21\u5757\u89e3\u91ca\u6570\u503c\u89c2\u6d4b\uff0c\u63d0\u53d6\u77ed\u671f\u51b2\u51fb\u548c\u957f\u671f\u8d8b\u52bf\uff0c\u7f13\u5b58\u9ad8\u4ef7\u503c\u63a8\u7406\u8f68\u8ff9\uff1b(2)Speak\u6a21\u5757\u57fa\u4e8e\u63a8\u7406\u751f\u6210\u548c\u4ea4\u6362\u7b56\u7565\u6d88\u606f\uff0c\u901a\u8fc7\u89e3\u6790\u540c\u4f34\u901a\u4fe1\u66f4\u65b0\u4fe1\u5ff5\uff1b(3)Decide\u6a21\u5757\u878d\u5408\u6570\u503c\u6570\u636e\u3001\u63a8\u7406\u548c\u53cd\u601d\u5230MARL\u7b56\u7565\u4e2d\uff0c\u4f18\u5316\u8bed\u8a00\u589e\u5f3a\u7684\u51b3\u7b56\u3002", "result": "\u5728\u7ecf\u6d4e\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cLAMP\u5728\u7d2f\u79ef\u56de\u62a5\uff08+63.5%\uff0c+34.0%\uff09\u3001\u9c81\u68d2\u6027\uff08+18.8%\uff0c+59.4%\uff09\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8eMARL\u548c\u7eafLLM\u57fa\u7ebf\u3002", "conclusion": "\u8bed\u8a00\u589e\u5f3a\u7b56\u7565\u5177\u6709\u63d0\u4f9b\u66f4\u6709\u6548\u548c\u9c81\u68d2\u7ecf\u6d4e\u7b56\u7565\u7684\u6f5c\u529b\uff0cLAMP\u6846\u67b6\u6210\u529f\u7f29\u5c0f\u4e86\u4e0e\u73b0\u5b9e\u4e16\u754c\u8bbe\u7f6e\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.11927", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11927", "abs": "https://arxiv.org/abs/2511.11927", "authors": ["Urte Adomaityte", "Gabriele Sicuro", "Pierpaolo Vivo"], "title": "PCA recovery thresholds in low-rank matrix inference with sparse noise", "comment": "24 pages, 7 figures", "summary": "We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7a00\u758f\u566a\u58f0\u6c61\u67d3\u4e0b\u79e9\u4e00\u4fe1\u53f7\u7684\u9ad8\u7ef4\u63a8\u65ad\u95ee\u9898\uff0c\u4f7f\u7528\u7edf\u8ba1\u7269\u7406\u7684\u526f\u672c\u65b9\u6cd5\u5206\u6790\u4e86\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\u7684\u5178\u578b\u884c\u4e3a\uff0c\u5e76\u8bc6\u522b\u4e86\u4fe1\u53f7\u6062\u590d\u7684\u4e34\u754c\u9608\u503c\u3002", "motivation": "\u7814\u7a76\u7a00\u758f\u566a\u58f0\uff08\u800c\u975e\u4f20\u7edf\u7a20\u5bc6\u566a\u58f0\uff09\u4e0b\u79e9\u4e00\u4fe1\u53f7\u7684\u6062\u590d\u95ee\u9898\uff0c\u5c06\u7ecf\u5178\u7684BBP\u76f8\u53d8\u63a8\u5e7f\u5230\u7a00\u758f\u56fe\u7ed3\u6784\u7684\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u7edf\u8ba1\u7269\u7406\u7684\u526f\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u9012\u5f52\u5206\u5e03\u65b9\u7a0b\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u7528\u7fa4\u4f53\u52a8\u529b\u5b66\u7b97\u6cd5\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u89e3\u6790\u8ba1\u7b97\u4e86\u6700\u5927\u7279\u5f81\u503c\u3001\u7279\u5f81\u5411\u91cf\u5206\u91cf\u5bc6\u5ea6\u548c\u4fe1\u53f7\u5411\u91cf\u4e0e\u7279\u5f81\u5411\u91cf\u7684\u91cd\u53e0\u5ea6\uff0c\u8bc6\u522b\u4e86\u4fe1\u53f7\u6062\u590d\u7684\u4e34\u754c\u5f3a\u5ea6\uff0c\u5728Poisson\u548c\u968f\u673a\u6b63\u5219\u5ea6\u5206\u5e03\u4e0b\u9a8c\u8bc1\u4e86\u7ed3\u679c\u3002", "conclusion": "\u6210\u529f\u5c06BBP\u76f8\u53d8\u63a8\u5e7f\u5230\u7a00\u758f\u566a\u58f0\u60c5\u51b5\uff0c\u5728\u5927\u8fde\u63a5\u5ea6\u6781\u9650\u4e0b\u6062\u590d\u7a20\u5bc6\u566a\u58f0\u7ed3\u679c\uff0c\u89e3\u6790\u7ed3\u679c\u4e0e\u5927\u89c4\u6a21\u77e9\u9635\u6570\u503c\u5bf9\u89d2\u5316\u4e00\u81f4\u3002"}}
{"id": "2511.11897", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11897", "abs": "https://arxiv.org/abs/2511.11897", "authors": ["Shuo Liu", "Wei Xiao", "Calin A. Belta"], "title": "Sampling-Aware Control Barrier Functions for Safety-Critical and Finite-Time Constrained Control", "comment": "8 pages, 4 figures", "summary": "In safety-critical control systems, ensuring both safety and feasibility under sampled-data implementations is crucial for practical deployment. Existing Control Barrier Function (CBF) frameworks, such as High-Order CBFs (HOCBFs), effectively guarantee safety in continuous time but may become unsafe when executed under zero-order-hold (ZOH) controllers due to inter-sampling effects. Moreover, they do not explicitly handle finite-time reach-and-remain requirements or multiple simultaneous constraints, which often lead to conflicts between safety and reach-and-remain objectives, resulting in feasibility issues during control synthesis. This paper introduces Sampling-Aware Control Barrier Functions (SACBFs), a unified framework that accounts for sampling effects and high relative-degree constraints by estimating and incorporating Taylor-based upper bounds on barrier evolution between sampling instants. The proposed method guarantees continuous-time forward invariance of safety and finite-time reach-and-remain sets under ZOH control. To further improve feasibility, a relaxed variant (r-SACBF) introduces slack variables for handling multiple constraints realized through time-varying CBFs. Simulation studies on a unicycle robot demonstrate that SACBFs achieve safe and feasible performance in scenarios where traditional HOCBF methods fail.", "AI": {"tldr": "\u63d0\u51fa\u91c7\u6837\u611f\u77e5\u63a7\u5236\u5c4f\u969c\u51fd\u6570(SACBF)\u6846\u67b6\uff0c\u89e3\u51b3\u4f20\u7edf\u9ad8\u9636CBF\u5728\u96f6\u9636\u4fdd\u6301\u63a7\u5236\u4e0b\u56e0\u91c7\u6837\u6548\u5e94\u5bfc\u81f4\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u540c\u65f6\u5904\u7406\u6709\u9650\u65f6\u95f4\u5230\u8fbe\u4fdd\u6301\u8981\u6c42\u548c\u591a\u7ea6\u675f\u51b2\u7a81\u3002", "motivation": "\u73b0\u6709CBF\u6846\u67b6\u5728\u8fde\u7eed\u65f6\u95f4\u4e0b\u80fd\u4fdd\u8bc1\u5b89\u5168\uff0c\u4f46\u5728\u91c7\u6837\u6570\u636e\u5b9e\u73b0\u65f6\u7531\u4e8e\u91c7\u6837\u95f4\u6548\u5e94\u53ef\u80fd\u53d8\u5f97\u4e0d\u5b89\u5168\uff0c\u4e14\u65e0\u6cd5\u663e\u5f0f\u5904\u7406\u6709\u9650\u65f6\u95f4\u5230\u8fbe\u4fdd\u6301\u8981\u6c42\u6216\u591a\u7ea6\u675f\u51b2\u7a81\u5bfc\u81f4\u7684\u53ef\u884c\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u548c\u6574\u5408\u91c7\u6837\u65f6\u523b\u95f4\u5c4f\u969c\u6f14\u5316\u7684\u6cf0\u52d2\u4e0a\u754c\uff0c\u63d0\u51faSACBF\u6846\u67b6\uff1b\u4e3a\u6539\u8fdb\u53ef\u884c\u6027\uff0c\u5f15\u5165\u5e26\u677e\u5f1b\u53d8\u91cf\u7684r-SACBF\u53d8\u4f53\u5904\u7406\u901a\u8fc7\u65f6\u53d8CBF\u5b9e\u73b0\u7684\u591a\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u5728\u4f20\u7edfHOCBF\u65b9\u6cd5\u5931\u6548\u7684\u573a\u666f\u4e0b\uff0cSACBFs\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u548c\u53ef\u884c\u7684\u6027\u80fd\u3002", "conclusion": "SACBFs\u4e3a\u91c7\u6837\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4fdd\u8bc1\u8fde\u7eed\u65f6\u95f4\u524d\u5411\u4e0d\u53d8\u6027\u548c\u6709\u9650\u65f6\u95f4\u5230\u8fbe\u4fdd\u6301\u96c6\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u677e\u5f1b\u53d8\u4f53\u63d0\u9ad8\u591a\u7ea6\u675f\u5904\u7406\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.11635", "categories": ["cs.CY", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11635", "abs": "https://arxiv.org/abs/2511.11635", "authors": ["Rui Jia", "Min Zhang", "Fengrui Liu", "Bo Jiang", "Kun Kuang", "Zhongxiang Dai"], "title": "EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation", "comment": null, "summary": "High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduAgentQG\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u4e2a\u6027\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u63d0\u5347\u95ee\u9898\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u6559\u80b2\u76ee\u6807\u4e00\u81f4\u6027\u3002", "motivation": "\u624b\u52a8\u8bbe\u8ba1\u95ee\u9898\u8017\u65f6\u4e14\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u5b66\u4e60\u9700\u6c42\uff0c\u73b0\u6709\u81ea\u52a8\u95ee\u9898\u751f\u6210\u65b9\u6cd5\u8d28\u91cf\u4e0d\u7a33\u5b9a\u3001\u591a\u6837\u6027\u6709\u9650\u3001\u4e0e\u6559\u80b2\u76ee\u6807\u5bf9\u9f50\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5305\u542b\u4e94\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u6846\u67b6\uff1aPlanner\u751f\u6210\u7ed3\u6784\u5316\u8bbe\u8ba1\u8ba1\u5212\u548c\u95ee\u9898\u65b9\u5411\uff0cWriter\u57fa\u4e8e\u8ba1\u5212\u751f\u6210\u5019\u9009\u95ee\u9898\uff0cSolver\u548cEducator\u8fdb\u884c\u591a\u7ef4\u5ea6\u4e8c\u5143\u8bc4\u5206\uff0cChecker\u8fdb\u884c\u6700\u7ec8\u9a8c\u8bc1\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u4f18\u5316\u95ee\u9898\u8d28\u91cf\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEduAgentQG\u5728\u95ee\u9898\u591a\u6837\u6027\u3001\u76ee\u6807\u4e00\u81f4\u6027\u548c\u6574\u4f53\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8fed\u4ee3\u53cd\u9988\u673a\u5236\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u4e14\u4e0e\u6559\u80b2\u76ee\u6807\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3a\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u8bc4\u4f30\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2511.11956", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.11956", "abs": "https://arxiv.org/abs/2511.11956", "authors": ["Andreas Habring"], "title": "On the Time Derivative of the KL Divergence for a Generalized Langevin Annealing Scheme", "comment": null, "summary": "Consider the Langevin diffusion process $\\d X_t = \\nabla \\log p_t(X_t) + \\sqrt{2}\\d W_t$ guided by the time-dependent probability density $p_t(x)$. Let $q_t$ be the density of $X_t$. Recently, in order to analyze convergence in the Kullback-Leibler divergence, the time derivative of $t\\mapsto \\KL(q_t|p_t)$ has been used in several works without investigating in detail when such a derivative exists. In this short manuscript we provide a rigorous derivation of the quantity $\\frac{\\d}{\\d t}\\KL(q_t|p_t)$.", "AI": {"tldr": "\u672c\u6587\u4e25\u683c\u63a8\u5bfc\u4e86Langevin\u6269\u6563\u8fc7\u7a0b\u4e2dKullback-Leibler\u6563\u5ea6\u7684\u65f6\u95f4\u5bfc\u6570\u8868\u8fbe\u5f0f\u3002", "motivation": "\u6700\u8fd1\u591a\u4e2a\u5de5\u4f5c\u4f7f\u7528KL\u6563\u5ea6\u7684\u65f6\u95f4\u5bfc\u6570\u6765\u5206\u6790\u6536\u655b\u6027\uff0c\u4f46\u672a\u8be6\u7ec6\u7814\u7a76\u8be5\u5bfc\u6570\u4f55\u65f6\u5b58\u5728\u3002", "method": "\u5bf9Langevin\u6269\u6563\u8fc7\u7a0b$\\d X_t = \\nabla \\log p_t(X_t) + \\sqrt{2}\\d W_t$\uff0c\u5176\u4e2d$p_t$\u662f\u5f15\u5bfc\u5bc6\u5ea6\uff0c$q_t$\u662f$X_t$\u7684\u5bc6\u5ea6\uff0c\u4e25\u683c\u63a8\u5bfc$\\frac{\\d}{\\d t}\\KL(q_t|p_t)$\u3002", "result": "\u63d0\u4f9b\u4e86KL\u6563\u5ea6\u65f6\u95f4\u5bfc\u6570\u7684\u4e25\u683c\u6570\u5b66\u63a8\u5bfc\u3002", "conclusion": "\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u5bf9KL\u6563\u5ea6\u65f6\u95f4\u5bfc\u6570\u5b58\u5728\u6027\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u540e\u7eed\u6536\u655b\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.11810", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11810", "abs": "https://arxiv.org/abs/2511.11810", "authors": ["Bertram H\u00f8jer"], "title": "On the Notion that Language Models Reason", "comment": "Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025", "summary": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8d28\u7591\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u8ba4\u4e3aLM\u53ea\u662f\u7edf\u8ba1\u6a21\u5f0f\u5339\u914d\u5668\u800c\u975e\u771f\u6b63\u7684\u63a8\u7406\u8005\uff0c\u5176\u63a8\u7406\u5f0f\u8f93\u51fa\u6e90\u4e8e\u7edf\u8ba1\u89c4\u5f8b\u800c\u975e\u903b\u8f91\u673a\u5236\u3002", "motivation": "\u8bc4\u4f30\u5f53\u524dNLP\u9886\u57df\u5bf9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5b9a\u4e49\u662f\u5426\u51c6\u786e\uff0c\u63ed\u793aLM\u8bad\u7ec3\u65b9\u5f0f\u3001\u4fe1\u606f\u5904\u7406\u673a\u5236\u4e0e\u63a8\u7406\u6982\u5ff5\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "method": "\u5c06\u57fa\u4e8etransformer\u7684\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u5b9e\u73b0\u9690\u5f0f\u6709\u9650\u9636\u9a6c\u5c14\u53ef\u592b\u6838\u7684\u7edf\u8ba1\u7cfb\u7edf\uff0c\u5206\u6790\u5176\u5982\u4f55\u4ece\u4e0a\u4e0b\u6587\u6620\u5c04\u5230\u6761\u4ef6\u6807\u8bb0\u5206\u5e03\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5f0f\u8f93\u51fa\u5bf9\u5e94\u7684\u662f\u5b66\u4e60\u6838\u4e2d\u7684\u7edf\u8ba1\u89c4\u5f8b\u548c\u8fd1\u4f3c\u7edf\u8ba1\u4e0d\u53d8\u6027\uff0c\u800c\u975e\u663e\u5f0f\u903b\u8f91\u673a\u5236\u7684\u5b9e\u65bd\uff0c\u56e0\u6b64\u65e0\u6cd5\u4fdd\u8bc1\u903b\u8f91\u4e00\u81f4\u6027\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u5ba1\u89c6NLP\u7814\u7a76\u4e2d\u5982\u4f55\u63cf\u8ff0\u6240\u6784\u5efa\u7cfb\u7edf\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u533a\u5206\u7edf\u8ba1\u6a21\u5f0f\u5339\u914d\u4e0e\u771f\u6b63\u63a8\u7406\u7684\u672c\u8d28\u5dee\u5f02\uff0c\u8fd9\u5bf9\u8bc4\u4f30LM\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.11575", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11575", "abs": "https://arxiv.org/abs/2511.11575", "authors": ["Animesh Joshi"], "title": "Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms", "comment": "8 pages", "summary": "Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7b97\u6cd5\u516c\u5e73\u6027\u8fdd\u89c4\uff0c\u901a\u8fc7k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u751f\u6210\u516c\u5e73\u6027\u6307\u6807\u7684\u62bd\u6837\u5206\u5e03\uff0c\u5e76\u5728\u7d2f\u72af\u9884\u6d4b\u7b97\u6cd5\u4e2d\u53d1\u73b0\u4e86\u9488\u5bf9\u9ed1\u4eba\u7fa4\u4f53\u7684\u7edf\u8ba1\u663e\u8457\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u8bc4\u4f30\u7fa4\u4f53\u95f4\u89c2\u5bdf\u5230\u7684\u5dee\u5f02\u662f\u5426\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u8fd8\u662f\u5076\u7136\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u5efa\u7acb\u4e25\u683c\u7684\u7edf\u8ba1\u6d4b\u8bd5\u6846\u67b6\u6765\u8bc4\u4f30\u7b97\u6cd5\u51b3\u7b56\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002", "method": "\u5229\u7528k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u751f\u6210\u516c\u5e73\u6027\u6307\u6807\u7684\u62bd\u6837\u5206\u5e03\uff0c\u5f00\u53d1\u57fa\u4e8e\u9884\u6d4b\u4e0e\u5b9e\u9645\u7ed3\u679c\u5dee\u5f02\u3001\u6a21\u578b\u6821\u51c6\u548c\u56e0\u679c\u63a8\u65ad\u6280\u672f\u7684\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u65b9\u6cd5\u3002", "result": "\u5728\u7d2f\u72af\u9884\u6d4b\u7b97\u6cd5\u7684\u6d4b\u8bd5\u4e2d\u53d1\u73b0\uff0c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u591a\u4e2a\u516c\u5e73\u6027\u5b9a\u4e49\u4e0b\u5bf9\u9ed1\u4eba\u4e2a\u4f53\u8868\u73b0\u51fa\u7edf\u8ba1\u663e\u8457\u7684\u504f\u89c1\uff0c\u800c\u5728\u5176\u4ed6\u5b9a\u4e49\u4e0b\u5219\u65e0\u504f\u89c1\u6216\u5bf9\u767d\u4eba\u4e2a\u4f53\u6709\u504f\u89c1\u3002", "conclusion": "\u8bc4\u4f30\u7b97\u6cd5\u51b3\u7b56\u7cfb\u7edf\u65f6\u9700\u8981\u91c7\u7528\u4e25\u683c\u548c\u7a33\u5065\u7684\u7edf\u8ba1\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u6027\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.11599", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11599", "abs": "https://arxiv.org/abs/2511.11599", "authors": ["Arefeh Kazemi", "Hamza Qadeer", "Joachim Wagner", "Hossein Hosseini", "Sri Balaaji Natarajan Kalaivendan", "Brian Davis"], "title": "SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio", "comment": null, "summary": "We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.", "AI": {"tldr": "SynBullying\u662f\u4e00\u4e2a\u5408\u6210\u7684\u591aLLM\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u548c\u68c0\u6d4b\u7f51\u7edc\u6b3a\u51cc\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u771f\u5b9e\u7684\u6b3a\u51cc\u4e92\u52a8\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u4f26\u7406\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4e3a\u7f51\u7edc\u6b3a\u51cc\u7814\u7a76\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u4f26\u7406\u5b89\u5168\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u514b\u670d\u4f20\u7edf\u4eba\u7c7b\u6570\u636e\u6536\u96c6\u7684\u5c40\u9650\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6a21\u62df\u6b3a\u51cc\u5bf9\u8bdd\uff0c\u6784\u5efa\u5305\u542b\u591a\u8f6e\u5bf9\u8bdd\u7ed3\u6784\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u6807\u6ce8\u548c\u7ec6\u7c92\u5ea6\u6807\u7b7e\u7684\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u5728\u5bf9\u8bdd\u7ed3\u6784\u3001\u8bcd\u6c47\u6a21\u5f0f\u3001\u60c5\u611f/\u6bd2\u6027\u3001\u89d2\u8272\u52a8\u6001\u3001\u4f24\u5bb3\u5f3a\u5ea6\u548c\u6b3a\u51cc\u7c7b\u578b\u5206\u5e03\u7b49\u4e94\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u72ec\u7acb\u8bad\u7ec3\u6570\u636e\u548c\u589e\u5f3a\u6570\u636e\u6e90\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "SynBullying\u4e3a\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5408\u6210\u6570\u636e\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u80fd\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u53c8\u80fd\u652f\u6301\u8be6\u7ec6\u7684\u6b3a\u51cc\u884c\u4e3a\u5206\u6790\u3002"}}
{"id": "2511.12391", "categories": ["q-fin.RM", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2511.12391", "abs": "https://arxiv.org/abs/2511.12391", "authors": ["Marco Scaringi", "Marco Bianchetti"], "title": "Sharpening Shapley Allocation: from Basel 2.5 to FRTB", "comment": "38 pages (main) + 12 pages (appendixes), 16 figures, 9 tables, 36 references", "summary": "Risk allocation, the decomposition of a portfolio-wide risk measure into component contributions, is a fundamental problem in financial risk management due to the non-additive nature of risk measures, the layered organizational structures of financial institutions and the range of possible allocation strategies characterized by different rationales and properties.\n  In this work, we conduct a systematic review of the major risk allocation strategies typically used in finance, comparing their theoretical properties, practical advantages, and limitations. To this scope we set up a specific testing framework, including both simplified settings, designed to highlight basic intrinsic behaviours, and realistic financial portfolios under different risk regulations, i.e. Basel 2.5 and FRTB. Furthermore, we develop and test novel practical solutions to manage the issue of negative risk allocations and of multi-level risk allocation in the layered organizational structure of financial institutions, while preserving the additivity property. Finally, we devote particular attention to the computational aspects of risk allocation.\n  Our results show that, in this context, the Shapley allocation strategy offers the best compromise between simplicity, mathematical properties, risk representation and computational cost. The latter is still acceptable even in the challenging case of many business units, provided that an efficient Monte Carlo simulation is employed, which offers excellent scaling and convergence properties. While our empirical applications focus on market risk, our methodological framework is fully general and applicable to other financial context such as valuation risk, liquidity risk, credit risk, and counterparty credit risk.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u91d1\u878d\u98ce\u9669\u5206\u914d\u7b56\u7565\uff0c\u5efa\u7acb\u4e86\u6d4b\u8bd5\u6846\u67b6\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\u7684\u7406\u8bba\u7279\u6027\u548c\u5b9e\u9645\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u5904\u7406\u8d1f\u98ce\u9669\u5206\u914d\u548c\u591a\u5c42\u7ea7\u98ce\u9669\u5206\u914d\u7684\u65b0\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660eShapley\u5206\u914d\u7b56\u7565\u5728\u7b80\u5355\u6027\u3001\u6570\u5b66\u6027\u8d28\u3001\u98ce\u9669\u8868\u793a\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u7531\u4e8e\u98ce\u9669\u5ea6\u91cf\u7684\u975e\u53ef\u52a0\u6027\u3001\u91d1\u878d\u673a\u6784\u7684\u5c42\u7ea7\u7ec4\u7ec7\u7ed3\u6784\u4ee5\u53ca\u4e0d\u540c\u5206\u914d\u7b56\u7565\u7684\u591a\u6837\u6027\uff0c\u98ce\u9669\u5206\u914d\u662f\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u4e2d\u7684\u57fa\u672c\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u6bd4\u8f83\u5206\u6790\u3002", "method": "\u5efa\u7acb\u4e86\u4e13\u95e8\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u62ec\u7b80\u5316\u8bbe\u7f6e\u548c\u73b0\u5b9e\u91d1\u878d\u6295\u8d44\u7ec4\u5408\uff0c\u5728\u4e0d\u540c\u98ce\u9669\u76d1\u7ba1\u8981\u6c42\u4e0b\u6d4b\u8bd5\u5404\u79cd\u5206\u914d\u7b56\u7565\u3002\u5f00\u53d1\u4e86\u5904\u7406\u8d1f\u98ce\u9669\u5206\u914d\u548c\u591a\u5c42\u7ea7\u98ce\u9669\u5206\u914d\u7684\u65b0\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u52a0\u6027\u3002\u7279\u522b\u5173\u6ce8\u98ce\u9669\u5206\u914d\u7684\u8ba1\u7b97\u65b9\u9762\u3002", "result": "Shapley\u5206\u914d\u7b56\u7565\u5728\u7b80\u5355\u6027\u3001\u6570\u5b66\u6027\u8d28\u3001\u98ce\u9669\u8868\u793a\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\u3002\u5373\u4f7f\u9762\u5bf9\u591a\u4e2a\u4e1a\u52a1\u5355\u5143\u7684\u6311\u6218\u60c5\u51b5\uff0c\u901a\u8fc7\u9ad8\u6548\u8499\u7279\u5361\u6d1b\u6a21\u62df\uff0c\u8ba1\u7b97\u6210\u672c\u4ecd\u7136\u53ef\u63a5\u53d7\uff0c\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u6536\u655b\u6027\u3002", "conclusion": "\u867d\u7136\u5b9e\u8bc1\u5e94\u7528\u805a\u7126\u4e8e\u5e02\u573a\u98ce\u9669\uff0c\u4f46\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u4f30\u503c\u98ce\u9669\u3001\u6d41\u52a8\u6027\u98ce\u9669\u3001\u4fe1\u7528\u98ce\u9669\u548c\u4ea4\u6613\u5bf9\u624b\u4fe1\u7528\u98ce\u9669\u7b49\u5176\u4ed6\u91d1\u878d\u9886\u57df\u3002"}}
{"id": "2511.12847", "categories": ["econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.12847", "abs": "https://arxiv.org/abs/2511.12847", "authors": ["Toru Kitagawa", "Yizhou Kuang"], "title": "Identification-aware Markov chain Monte Carlo", "comment": "80 pages, 25 figures", "summary": "Leaving posterior sensitivity concerns aside, non-identifiability of the parameters does not raise a difficulty for Bayesian inference as far as the posterior is proper, but multi-modality or flat regions of the posterior induced by the lack of identification leaves a challenge for modern Bayesian computation. Sampling methods often struggle with slow or non-convergence when dealing with multiple modes or flat regions of the target distributions. This paper develops a novel Markov chain Monte Carlo (MCMC) approach for non-identified models, leveraging the knowledge of observationally equivalent sets of parameters, and highlights an important role that identification plays in modern Bayesian analysis. We show that our proposal overcomes the issues of being trapped in a local mode and achieves a faster rate of convergence than the existing MCMC techniques including random walk Metropolis-Hastings and Hamiltonian Monte Carlo. The gain in the speed of convergence is more significant as the dimension or cardinality of the identified sets increases. Simulation studies show its superior performance compared to other popular computational methods including Hamiltonian Monte Carlo and sequential Monte Carlo. We also demonstrate that our method uncovers non-trivial modes in the target distribution in a structural vector moving-average (SVMA) application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MCMC\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u975e\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u591a\u6a21\u6001\u6216\u5e73\u5766\u533a\u57df\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u89c2\u6d4b\u7b49\u4ef7\u53c2\u6570\u96c6\u7684\u77e5\u8bc6\u6765\u514b\u670d\u5c40\u90e8\u6a21\u5f0f\u9677\u9631\u5e76\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u975e\u8bc6\u522b\u6027\u53c2\u6570\u4e0d\u4f1a\u7ed9\u8d1d\u53f6\u65af\u63a8\u65ad\u5e26\u6765\u56f0\u96be\uff0c\u4f46\u7531\u6b64\u4ea7\u751f\u7684\u540e\u9a8c\u5206\u5e03\u591a\u6a21\u6001\u6216\u5e73\u5766\u533a\u57df\u4f1a\u7ed9\u73b0\u4ee3\u8d1d\u53f6\u65af\u8ba1\u7b97\u5e26\u6765\u6311\u6218\uff0c\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u5728\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u65f6\u5f80\u5f80\u6536\u655b\u7f13\u6162\u6216\u4e0d\u6536\u655b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b(MCMC)\u65b9\u6cd5\uff0c\u5229\u7528\u89c2\u6d4b\u7b49\u4ef7\u53c2\u6570\u96c6\u7684\u77e5\u8bc6\uff0c\u4e13\u95e8\u9488\u5bf9\u975e\u8bc6\u522b\u6a21\u578b\u8bbe\u8ba1\u3002", "result": "\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u88ab\u56f0\u5728\u5c40\u90e8\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u6bd4\u73b0\u6709MCMC\u6280\u672f\uff08\u5305\u62ec\u968f\u673a\u6e38\u8d70Metropolis-Hastings\u548c\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\uff09\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4e14\u968f\u7740\u8bc6\u522b\u96c6\u7ef4\u5ea6\u6216\u57fa\u6570\u7684\u589e\u52a0\uff0c\u6536\u655b\u901f\u5ea6\u7684\u63d0\u5347\u66f4\u52a0\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7ed3\u6784\u5411\u91cf\u79fb\u52a8\u5e73\u5747(SVMA)\u5e94\u7528\u4e2d\u6210\u529f\u63ed\u793a\u4e86\u76ee\u6807\u5206\u5e03\u4e2d\u7684\u975e\u5e73\u51e1\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86\u8bc6\u522b\u5728\u73b0\u4ee3\u8d1d\u53f6\u65af\u5206\u6790\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2511.13277", "categories": ["q-fin.TR", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2511.13277", "abs": "https://arxiv.org/abs/2511.13277", "authors": ["Jutta G. Kurth", "Jean-Philippe Bouchaud"], "title": "Stationary Distributions of the Mode-switching Chiarella Model", "comment": "7 pages, 4 figures, 12 pages of appendices", "summary": "We derive the stationary distribution in various regimes of the extended Chiarella model of financial markets. This model is a stochastic nonlinear dynamical system that encompasses dynamical competition between a (saturating) trending and a mean-reverting component. We find the so-called mispricing distribution and the trend distribution to be unimodal Gaussians in the small noise, small feedback limit. Slow trends yield Gaussian-cosh mispricing distributions that allow for a P-bifurcation: unimodality occurs when mean-reversion is fast, bimodality when it is slow. The critical point of this bifurcation is established and refutes previous ad-hoc reports and differs from the bifurcation condition of the dynamical system itself. For fast, weakly coupled trends, deploying the Furutsu-Novikov theorem reveals that the result is again unimodal Gaussian. For the same case with higher coupling we disprove another claim from the literature: bimodal trend distributions do not generally imply bimodal mispricing distributions. The latter becomes bimodal only for stronger trend feedback. The exact solution in this last regime remains unfortunately beyond our proficiency.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6269\u5c55Chiarella\u91d1\u878d\u5e02\u573a\u6a21\u578b\u5728\u4e0d\u540c\u53c2\u6570\u533a\u95f4\u7684\u7a33\u6001\u5206\u5e03\uff0c\u53d1\u73b0\u4e86\u5b9a\u4ef7\u504f\u5dee\u5206\u5e03\u548c\u8d8b\u52bf\u5206\u5e03\u7684\u6a21\u5f0f\u53d8\u5316\uff0c\u5305\u62ec\u5355\u5cf0\u9ad8\u65af\u5206\u5e03\u3001\u53cc\u5cf0\u5206\u5e03\u53ca\u5176\u4e34\u754c\u70b9\u3002", "motivation": "\u7814\u7a76\u6269\u5c55Chiarella\u6a21\u578b\u4e2d\u91d1\u878d\u5e02\u573a\u7684\u968f\u673a\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5b9a\u4ef7\u504f\u5dee\u548c\u8d8b\u52bf\u7684\u7a33\u6001\u5206\u5e03\u7279\u6027\uff0c\u4ee5\u7406\u89e3\u8d8b\u52bf\u8ddf\u8e2a\u548c\u5747\u503c\u56de\u5f52\u4e4b\u95f4\u7684\u52a8\u6001\u7ade\u4e89\u3002", "method": "\u4f7f\u7528Furutsu-Novikov\u5b9a\u7406\u5206\u6790\u4e0d\u540c\u53c2\u6570\u533a\u95f4\uff08\u5c0f\u566a\u58f0\u5c0f\u53cd\u9988\u3001\u6162\u8d8b\u52bf\u3001\u5feb\u901f\u5f31\u8026\u5408\u8d8b\u52bf\u7b49\uff09\u7684\u7a33\u6001\u5206\u5e03\uff0c\u5efa\u7acb\u5206\u5c94\u4e34\u754c\u70b9\u3002", "result": "\u5728\u5c0f\u566a\u58f0\u5c0f\u53cd\u9988\u6781\u9650\u4e0b\u5f97\u5230\u5355\u5cf0\u9ad8\u65af\u5206\u5e03\uff1b\u6162\u8d8b\u52bf\u4e0b\u51fa\u73b0P-\u5206\u5c94\uff0c\u5747\u503c\u56de\u5f52\u5feb\u65f6\u4e3a\u5355\u5cf0\uff0c\u6162\u65f6\u4e3a\u53cc\u5cf0\uff1b\u5feb\u901f\u5f31\u8026\u5408\u8d8b\u52bf\u4e0b\u4ecd\u4e3a\u5355\u5cf0\u9ad8\u65af\uff1b\u5f3a\u8d8b\u52bf\u53cd\u9988\u4e0b\u5b9a\u4ef7\u504f\u5dee\u5206\u5e03\u53d8\u4e3a\u53cc\u5cf0\u3002", "conclusion": "\u6269\u5c55Chiarella\u6a21\u578b\u7684\u7a33\u6001\u5206\u5e03\u5448\u73b0\u4e30\u5bcc\u7684\u6a21\u5f0f\u53d8\u5316\uff0c\u5206\u5c94\u6761\u4ef6\u4e0e\u52a8\u529b\u5b66\u7cfb\u7edf\u672c\u8eab\u4e0d\u540c\uff0c\u5f3a\u8d8b\u52bf\u53cd\u9988\u662f\u5b9a\u4ef7\u504f\u5dee\u5206\u5e03\u53cc\u5cf0\u5316\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2511.11983", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11983", "abs": "https://arxiv.org/abs/2511.11983", "authors": ["Debashis Chatterjee"], "title": "Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence", "comment": null, "summary": "Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization.\n  We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective.\n  Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u8d1d\u53f6\u65af\u4e0eAI\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u9884\u6d4b\u548c\u8d1d\u53f6\u65af\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u4e3a\u6d41\u884c\u75c5\u5b66\u5206\u6790\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u4f18\u5316\u7684\u6a21\u578b\u9009\u62e9\u3002", "motivation": "\u73b0\u4ee3\u6d41\u884c\u75c5\u5b66\u5206\u6790\u4f7f\u7528\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u5f3a\u4f46\u7f3a\u4e4f\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8d1d\u53f6\u65af\u65b9\u6cd5\u80fd\u63d0\u4f9b\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4f46\u96be\u4ee5\u4e0e\u5f53\u4ee3AI\u5de5\u4f5c\u6d41\u96c6\u6210\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u903b\u8f91\u56de\u5f52\u83b7\u5f97\u4e2a\u4f53\u75be\u75c5\u98ce\u9669\u548c\u540e\u9a8c\u7f6e\u4fe1\u533a\u95f4\uff0c\u540c\u65f6\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u8d1d\u53f6\u65af\u4f18\u5316\u8c03\u6574\u60e9\u7f5aCox\u751f\u5b58\u6a21\u578b\uff0c\u6784\u5efa\u53cc\u5c42\u7cfb\u7edf\u3002", "result": "\u8d1d\u53f6\u65af\u5c42\u63d0\u4f9b\u53ef\u9760\u7684\u8986\u76d6\u7387\u548c\u6539\u8fdb\u7684\u6821\u51c6\uff0c\u8d1d\u53f6\u65af\u6536\u7f29\u63d0\u9ad8AUC\u3001Brier\u5206\u6570\u548clog-loss\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u4f7f\u751f\u5b58\u6a21\u578b\u8fbe\u5230\u63a5\u8fd1oracle\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8d1d\u53f6\u65af\u63a8\u7406\u589e\u5f3a\u4e86\u63a8\u65ad\u548c\u641c\u7d22\u80fd\u529b\uff0c\u4e3a\u6d41\u884c\u75c5\u5b66\u51b3\u7b56\u63d0\u4f9b\u6821\u51c6\u7684\u98ce\u9669\u8bc4\u4f30\u548c\u539f\u5219\u6027\u7684\u8d85\u53c2\u6570\u667a\u80fd\u3002"}}
{"id": "2511.12029", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12029", "abs": "https://arxiv.org/abs/2511.12029", "authors": ["Nicholas Tetteh Ofoe", "Weilun Wang", "Lei Wu"], "title": "On The Detection of Minimum Forecast Horizon For Real-Time Scheduling of Energy Storage Systems in Smart Grid", "comment": null, "summary": "The increasing integration of energy storage systems (ESSs) into power grids has necessitated effective real-time control strategies under uncertain and volatile electricity prices. An important problem of model predictive control of ESSs is identifying the minimum forecast horizon needed to exactly simulate the globally optimal control trajectory. Existing methods in the literature provide only sufficient conditions and might ignore real-world inconsistencies in control actions. In this paper, we introduce a trajectory-alignment-based definition of the minimum forecast horizon and propose an algorithm that identifies the minimum planning horizon for which all rolling-horizon control decisions match those of the full-horizon global optimization. Using real price data from the bidding zone DK1 in Denmark of the Nord Pool day-ahead market and a realistic ESS model, we illustrate that $60$ hours of forecast horizon allows us to exactly simulate the global control sequence and economic outcomes. In addition, we illustrate that under other parameter configurations, no forecast horizon ensures full convergence, demonstrating the sensitivity of the existence of a forecast horizon to various parameters. Our findings provide an operationally significant framework for minimum forecast horizon detection in storage scheduling and pave the way for the analytical description of this important planning measure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u8f68\u8ff9\u5bf9\u9f50\u7684\u6700\u5c0f\u9884\u6d4b\u65f6\u57df\u5b9a\u4e49\u548c\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u50a8\u80fd\u7cfb\u7edf\u5b9e\u65f6\u63a7\u5236\u4e2d\u80fd\u591f\u5b8c\u5168\u6a21\u62df\u5168\u5c40\u6700\u4f18\u63a7\u5236\u8f68\u8ff9\u6240\u9700\u7684\u6700\u77ed\u9884\u6d4b\u65f6\u57df\u3002", "motivation": "\u968f\u7740\u50a8\u80fd\u7cfb\u7edf\u5728\u7535\u7f51\u4e2d\u7684\u96c6\u6210\u5ea6\u63d0\u9ad8\uff0c\u9700\u8981\u5728\u4e0d\u786e\u5b9a\u548c\u6ce2\u52a8\u7684\u7535\u4ef7\u4e0b\u5236\u5b9a\u6709\u6548\u7684\u5b9e\u65f6\u63a7\u5236\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u5145\u5206\u6761\u4ef6\uff0c\u53ef\u80fd\u5ffd\u7565\u5b9e\u9645\u63a7\u5236\u52a8\u4f5c\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8f68\u8ff9\u5bf9\u9f50\u7684\u6700\u5c0f\u9884\u6d4b\u65f6\u57df\u5b9a\u4e49\uff0c\u63d0\u51fa\u7b97\u6cd5\u8bc6\u522b\u6eda\u52a8\u65f6\u57df\u63a7\u5236\u51b3\u7b56\u4e0e\u5168\u65f6\u57df\u5168\u5c40\u4f18\u5316\u5b8c\u5168\u5339\u914d\u7684\u6700\u5c0f\u89c4\u5212\u65f6\u57df\u3002", "result": "\u4f7f\u7528\u4e39\u9ea6Nord Pool\u65e5\u524d\u5e02\u573a\u7684\u771f\u5b9e\u4ef7\u683c\u6570\u636e\u548c\u5b9e\u9645ESS\u6a21\u578b\uff0c\u8bc1\u660e60\u5c0f\u65f6\u7684\u9884\u6d4b\u65f6\u57df\u80fd\u591f\u5b8c\u5168\u6a21\u62df\u5168\u5c40\u63a7\u5236\u5e8f\u5217\u548c\u7ecf\u6d4e\u7ed3\u679c\u3002\u5728\u67d0\u4e9b\u53c2\u6570\u914d\u7f6e\u4e0b\uff0c\u4e0d\u5b58\u5728\u786e\u4fdd\u5b8c\u5168\u6536\u655b\u7684\u9884\u6d4b\u65f6\u57df\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u50a8\u80fd\u8c03\u5ea6\u4e2d\u7684\u6700\u5c0f\u9884\u6d4b\u65f6\u57df\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5177\u6709\u64cd\u4f5c\u610f\u4e49\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u8fd9\u4e00\u91cd\u8981\u89c4\u5212\u6307\u6807\u7684\u5206\u6790\u63cf\u8ff0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11655", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11655", "abs": "https://arxiv.org/abs/2511.11655", "authors": ["Maurice Flechtner"], "title": "Automatic generation of DRI Statements", "comment": "Master Thesis", "summary": "Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u751f\u6210\u5ba1\u8bae\u7406\u7531\u6307\u6570(DRI)\u9648\u8ff0\u7684\u65b9\u6cd5\uff0c\u5229\u7528NLP\u548cLLM\u6280\u672f\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf", "motivation": "DRI\u662f\u8bc4\u4f30\u7fa4\u4f53\u5ba1\u8bae\u8d28\u91cf\u7684\u91cd\u8981\u6307\u6807\uff0c\u4f46\u5176\u9648\u8ff0\u751f\u6210\u8fc7\u7a0b\u590d\u6742\u8017\u65f6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5148\u8fdb\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316DRI\u9648\u8ff0\u751f\u6210\u6846\u67b6", "result": "\u5927\u5e45\u964d\u4f4e\u4e86\u8fdb\u884c\u7efc\u5408\u5ba1\u8bae\u8fc7\u7a0b\u8bc4\u4f30\u7684\u95e8\u69db\uff0c\u4e3a\u751f\u6210\u5f0fAI\u878d\u5165\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u6a21\u677f", "conclusion": "\u81ea\u52a8\u5316DRI\u9648\u8ff0\u751f\u6210\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\uff0c\u63a8\u52a8\u5ba1\u8bae\u8fc7\u7a0b\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027"}}
{"id": "2511.11994", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.11994", "abs": "https://arxiv.org/abs/2511.11994", "authors": ["Paulo Henrique Foganholo Biazetto", "Gustavo Artur de Andrade", "Tiago Roux Oliveira", "Miroslav Krstic"], "title": "Extremum-Seeking Boundary Control for Schr\u00f6dinger-Type PDEs", "comment": null, "summary": "This paper addresses the design and analysis of an extremum-seeking (ES) controller for scalar static maps in the context of infinite-dimensional dynamics governed by complex-valued partial differential equations (PDEs) of Schrodinger type. The system is actuated at one boundary, and the map input is defined as a real-valued quadratic functional corresponding to the squared norm of the complex state at the uncontrolled boundary. An isomorphism between the complex Hilbert space and its two-dimensional real-valued representation is established to enable the use of the standard multivariable Newton-based ES method. To compensate for the PDE actuation dynamics, a boundary control strategy based on a two-step backstepping procedure is employed. With a perturbation-based estimate of the Hessian inverse, the local exponential stability to a small neighborhood of the unknown extremum point is proved. A numerical example illustrates the effectiveness of the proposed extremum-seeking methodology.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u859b\u5b9a\u8c14\u578b\u590d\u503c\u504f\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u4e86\u6781\u503c\u641c\u7d22\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u8fb9\u754c\u63a7\u5236\u7b56\u7565\u548cHessian\u9006\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u77e5\u6781\u503c\u70b9\u7684\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u3002", "motivation": "\u89e3\u51b3\u65e0\u9650\u7ef4\u590d\u503c\u504f\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u4e2d\u6781\u503c\u641c\u7d22\u63a7\u5236\u7684\u8bbe\u8ba1\u4e0e\u5206\u6790\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u859b\u5b9a\u8c14\u578b\u7cfb\u7edf\u7684\u8fb9\u754c\u63a7\u5236\u573a\u666f\u3002", "method": "\u5efa\u7acb\u590d\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e0e\u4e8c\u7ef4\u5b9e\u503c\u8868\u793a\u7684\u7b49\u8ddd\u540c\u6784\uff0c\u91c7\u7528\u57fa\u4e8e\u725b\u987f\u7684\u591a\u53d8\u91cf\u6781\u503c\u641c\u7d22\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e24\u6b65\u53cd\u63a8\u8fb9\u754c\u63a7\u5236\u7b56\u7565\u8865\u507fPDE\u9a71\u52a8\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u6270\u52a8\u57faHessian\u9006\u4f30\u8ba1\u3002", "result": "\u8bc1\u660e\u4e86\u7cfb\u7edf\u5bf9\u672a\u77e5\u6781\u503c\u70b9\u5c0f\u90bb\u57df\u7684\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u6027\uff0c\u6570\u503c\u7b97\u4f8b\u9a8c\u8bc1\u4e86\u6240\u63d0\u6781\u503c\u641c\u7d22\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u590d\u503cPDE\u7cfb\u7edf\u7684\u6781\u503c\u641c\u7d22\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u8fb9\u754c\u63a7\u5236\u7b56\u7565\u548cHessian\u4f30\u8ba1\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6536\u655b\u3002"}}
{"id": "2511.11821", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11821", "abs": "https://arxiv.org/abs/2511.11821", "authors": ["Hong-Jun Yoon", "Faisal Ashraf", "Thomas A. Ruggles", "Debjani Singh"], "title": "Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis", "comment": "18 pages, zero figures, Preprint submitted to Environmental Modeling and Software", "summary": "Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.\n  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\\% F1 through appropriate validation, while smaller models plateau at 51\\%. Large-scale models approach 77\\% F1 but require enterprise infrastructure.\n  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.\n  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.", "AI": {"tldr": "\u8bc4\u4f307\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b(0.6B-70B\u53c2\u6570)\u5728\u6c34\u7535\u8bb8\u53ef\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u4e2d\u7684\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u6743\u8861\uff0c\u53d1\u73b014B\u53c2\u6570\u662f\u6709\u6548\u6027\u7684\u5173\u952e\u9608\u503c\u3002", "motivation": "\u89e3\u51b3\u76d1\u7ba1\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u4e2d\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u5b9e\u8bc1\u6307\u5bfc\u3002", "method": "\u5728\u6c34\u7535\u8bb8\u53ef\u6587\u6863\u4e0a\u8bc4\u4f307\u4e2a\u4e0d\u540c\u53c2\u6570\u89c4\u6a21(0.6B-70B)\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5206\u6790\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b014B\u53c2\u6570\u9608\u503c\uff1a\u4f4e\u4e8e\u6b64\u9608\u503c\u65f6\u9a8c\u8bc1\u65b9\u6cd5\u65e0\u6548(F1<0.15)\uff0c\u9ad8\u4e8e\u6b64\u9608\u503c\u65f6\u9a8c\u8bc1\u65b9\u6cd5\u53ef\u884c(F1=0.64)\u3002\u6d88\u8d39\u7ea7\u6a21\u578b\u53ef\u8fbe64% F1\uff0c\u5c0f\u578b\u6a21\u578b\u4e0a\u9650\u4e3a51%\uff0c\u5927\u578b\u6a21\u578b\u63a5\u8fd177%\u4f46\u9700\u4f01\u4e1a\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u5efa\u7acb\u4e86\u9996\u4e2a\u76d1\u7ba1\u73af\u5883\u4e0b\u5f00\u6e90\u4fe1\u606f\u63d0\u53d6\u7684\u8d44\u6e90-\u6027\u80fd\u6620\u5c04\uff0c\u4e3a\u57fa\u4e8e\u8bc1\u636e\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\uff0c\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u6c34\u7535\u5408\u89c4\u5177\u6709\u76f4\u63a5\u4ef7\u503c\uff0c\u4e14\u53c2\u6570\u7f29\u653e\u6548\u5e94\u7684\u89c1\u89e3\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u3002"}}
{"id": "2511.11576", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11576", "abs": "https://arxiv.org/abs/2511.11576", "authors": ["WenZhuo Zhu", "Zheng Cui", "Wenhan Lu", "Sheng Liu", "Yue Zhao"], "title": "DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs", "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86DAOpt\u6846\u67b6\uff0c\u5305\u62ecOptU\u6570\u636e\u96c6\u3001\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6a21\u5757\u548c\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4e0d\u786e\u5b9a\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u6837\u672c\u5916\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u786e\u5b9a\u6027\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u5728\u73b0\u5b9e\u51b3\u7b56\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0cLLM\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u5e94\u7528\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u5c11\u6837\u672c\u5b66\u4e60\u7ed3\u5408\u968f\u673a\u4f18\u5316\u548c\u9c81\u68d2\u4f18\u5316\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u589e\u5f3aLLM\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6a21\u5757\u548c\u6a21\u62df\u8bc4\u4f30\u73af\u5883\u3002", "result": "\u6784\u5efa\u4e86OptU\u6570\u636e\u96c6\u548cDAOpt\u6846\u67b6\uff0c\u4e3aLLM\u5728\u4e0d\u786e\u5b9a\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u65b9\u6cd5\u3002", "conclusion": "DAOpt\u6846\u67b6\u586b\u8865\u4e86LLM\u5728\u4e0d\u786e\u5b9a\u4f18\u5316\u5efa\u6a21\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76LLM\u5728\u73b0\u5b9e\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2511.11600", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11600", "abs": "https://arxiv.org/abs/2511.11600", "authors": ["Piyushkumar Patel"], "title": "CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models", "comment": null, "summary": "While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This \"hallucination\" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.\n  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.\n  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\\% of the time while missing only 8.3\\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.", "AI": {"tldr": "CausalGuard\u662f\u4e00\u79cd\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u548c\u7b26\u53f7\u903b\u8f91\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u548c\u9632\u6b62\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u8bc6\u522b89.3%\u7684\u5e7b\u89c9\uff0c\u540c\u65f6\u5c06\u865a\u5047\u58f0\u660e\u51cf\u5c11\u8fd180%\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4f1a\u81ea\u4fe1\u5730\u9648\u8ff0\u542c\u8d77\u6765\u5408\u7406\u4f46\u5b9e\u9645\u9519\u8bef\u7684\u4fe1\u606f\uff0c\u8fd9\u6210\u4e3a\u5728\u51c6\u786e\u6027\u8981\u6c42\u9ad8\u7684\u573a\u666f\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u4e3b\u8981\u969c\u788d\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\uff0c\u8981\u4e48\u589e\u52a0\u663e\u8457\u8ba1\u7b97\u6210\u672c\uff0c\u6216\u8005\u672a\u80fd\u89e3\u51b3\u5e7b\u89c9\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "CausalGuard\u901a\u8fc7\u4e24\u6761\u4e92\u8865\u8def\u5f84\u5de5\u4f5c\uff1a\u4e00\u6761\u8def\u5f84\u8ffd\u8e2a\u6a21\u578b\u6240\u77e5\u5185\u5bb9\u4e0e\u751f\u6210\u5185\u5bb9\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u53e6\u4e00\u6761\u8def\u5f84\u4f7f\u7528\u81ea\u52a8\u63a8\u7406\u68c0\u67e5\u903b\u8f91\u4e00\u81f4\u6027\u3002\u4e0e\u4ec5\u68c0\u67e5\u751f\u6210\u540e\u8f93\u51fa\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u7cfb\u7edf\u7406\u89e3\u5bfc\u81f4\u9519\u8bef\u9648\u8ff0\u7684\u56e0\u679c\u94fe\uff0c\u5e76\u5728\u8fc7\u7a0b\u4e2d\u65e9\u671f\u8fdb\u884c\u5e72\u9884\u3002", "result": "\u572812\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCausalGuard\u6b63\u786e\u8bc6\u522b\u4e8689.3%\u7684\u5e7b\u89c9\uff0c\u4ec5\u6f0f\u63898.3%\u7684\u5b9e\u9645\u5e7b\u89c9\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u5c06\u8fd180%\u7684\u865a\u5047\u58f0\u660e\uff0c\u540c\u65f6\u4fdd\u6301\u56de\u7b54\u7684\u81ea\u7136\u6027\u548c\u5e2e\u52a9\u6027\u3002\u8be5\u7cfb\u7edf\u5728\u9700\u8981\u591a\u6b65\u903b\u8f91\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002", "conclusion": "CausalGuard\u901a\u8fc7\u5c55\u793a\u5176\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u533b\u7597\u8bca\u65ad\u6216\u91d1\u878d\u5206\u6790\u7b49\u654f\u611f\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u9886\u57df\u7406\u89e3\u51b3\u7b56\u539f\u56e0\u4e0e\u51b3\u7b56\u672c\u8eab\u540c\u7b49\u91cd\u8981\u3002\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12292", "categories": ["q-fin.RM"], "pdf": "https://arxiv.org/pdf/2511.12292", "abs": "https://arxiv.org/abs/2511.12292", "authors": ["Bohan Li", "Wenyuan Li", "Kenneth Tsz Hin Ng", "Sheung Chi Phillip Yam"], "title": "Mean Field Analysis of Mutual Insurance Market", "comment": null, "summary": "A mutual insurance company (MIC) is a type of consumer cooperative owned by its policyholders. By purchasing insurance from an MIC, policyholders effectively become member-owners of the company and are entitled to a share of the surplus, which is determined by their own collective claims and premium contributions. This sharing mechanism creates an interactive environment in which individual insurance strategies are influenced by the actions of others. Given that mutual insurers account for nearly one-third of the global insurance market, the analysis of members' behavior under such a sharing mechanism is of both practical and theoretical importance. This article presents a first dynamic study of members' behavior in the prevalent mutual insurance market under the large-population limit. With members' wealth processes depending on the law of the insurance strategies, we model the surplus-sharing mechanism using an extended mean field game (MFG) framework and address the fundamental question of how strategic interactions in this setting influence individual decisions. Mathematically, we establish the global-in-time existence and uniqueness of the mean field forward-backward stochastic differential equation (MF-FBSDE) characterizing the Nash equilibrium strategy, employing techniques to accommodate realistic insurance constraints. Computationally, we develop a modified deep BSDE algorithm capable of solving the extended MFG problem with an additional fixed-point structure on the control. Utilizing this scheme, we examine how structural features of the MIC's design, such as the composition of risk classes and surplus-sharing proportions, reshape members' decisions and wealth through collective interactions, underscoring the central role of these mechanisms in MICs.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u5927\u578b\u76f8\u4e92\u4fdd\u9669\u516c\u53f8\u4e2d\u6210\u5458\u884c\u4e3a\u8fdb\u884c\u52a8\u6001\u7814\u7a76\uff0c\u4f7f\u7528\u6269\u5c55\u7684\u5747\u503c\u573a\u535a\u5f08\u6846\u67b6\u5206\u6790\u6218\u7565\u4e92\u52a8\u5982\u4f55\u5f71\u54cd\u4e2a\u4f53\u51b3\u7b56\uff0c\u5efa\u7acb\u4e86MF-FBSDE\u7684\u5168\u5c40\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u6539\u8fdb\u7684\u6df1\u5ea6BSDE\u7b97\u6cd5\u6765\u7814\u7a76\u76f8\u4e92\u4fdd\u9669\u516c\u53f8\u7684\u7ed3\u6784\u7279\u5f81\u5982\u4f55\u91cd\u5851\u6210\u5458\u51b3\u7b56\u3002", "motivation": "\u76f8\u4e92\u4fdd\u9669\u516c\u53f8\u5360\u5168\u7403\u4fdd\u9669\u5e02\u573a\u8fd1\u4e09\u5206\u4e4b\u4e00\u4efd\u989d\uff0c\u5176\u76c8\u4f59\u5206\u4eab\u673a\u5236\u521b\u9020\u4e86\u4e00\u4e2a\u76f8\u4e92\u5f71\u54cd\u7684\u73af\u5883\uff0c\u5206\u6790\u6210\u5458\u5728\u8fd9\u79cd\u5206\u4eab\u673a\u5236\u4e0b\u7684\u884c\u4e3a\u5177\u6709\u91cd\u8981\u7684\u5b9e\u8df5\u548c\u7406\u8bba\u610f\u4e49\u3002", "method": "\u4f7f\u7528\u6269\u5c55\u7684\u5747\u503c\u573a\u535a\u5f08\u6846\u67b6\u5efa\u6a21\u76c8\u4f59\u5206\u4eab\u673a\u5236\uff0c\u5efa\u7acb\u63cf\u8ff0\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\u7684\u5747\u503c\u573a\u524d\u5411-\u540e\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff0c\u5f00\u53d1\u6539\u8fdb\u7684\u6df1\u5ea6BSDE\u7b97\u6cd5\u89e3\u51b3\u5177\u6709\u989d\u5916\u56fa\u5b9a\u70b9\u7ed3\u6784\u7684\u6269\u5c55MFG\u95ee\u9898\u3002", "result": "\u6570\u5b66\u4e0a\u8bc1\u660e\u4e86MF-FBSDE\u7684\u5168\u5c40\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\uff0c\u8ba1\u7b97\u4e0a\u6210\u529f\u5f00\u53d1\u4e86\u80fd\u591f\u5904\u7406\u5b9e\u9645\u4fdd\u9669\u7ea6\u675f\u7684\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u98ce\u9669\u7c7b\u522b\u6784\u6210\u548c\u76c8\u4f59\u5206\u4eab\u6bd4\u4f8b\u7b49\u7ed3\u6784\u7279\u5f81\u5982\u4f55\u901a\u8fc7\u96c6\u4f53\u4e92\u52a8\u91cd\u5851\u6210\u5458\u51b3\u7b56\u548c\u8d22\u5bcc\u3002", "conclusion": "\u76f8\u4e92\u4fdd\u9669\u516c\u53f8\u7684\u8bbe\u8ba1\u7ed3\u6784\u7279\u5f81\u5728\u6210\u5458\u51b3\u7b56\u548c\u8d22\u5bcc\u5f62\u6210\u4e2d\u53d1\u6325\u6838\u5fc3\u4f5c\u7528\uff0c\u6218\u7565\u4e92\u52a8\u663e\u8457\u5f71\u54cd\u4e2a\u4f53\u4fdd\u9669\u51b3\u7b56\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316\u76f8\u4e92\u4fdd\u9669\u5e02\u573a\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2511.13275", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2511.13275", "abs": "https://arxiv.org/abs/2511.13275", "authors": ["Tetsuya Kaji", "Elena Manresa"], "title": "Why Do the Elderly Save? Using Health Shocks to Uncover Bequests Motives", "comment": "37 pages, 5 figures, JEL C13, C45, D14, G11, J11, first version submitted November 2025", "summary": "We revisit the saving behavior of elderly singles using an adversarial structural estimation framework by Kaji, Manresa and Pouliot (2023). The method bridges the simulated method of moments (SMM) and maximum-likelihood estimation by embedding a flexible discriminator, implemented as a neural network, that adaptively selects the most informative features of the data. Applying this approach to the model of De Nardi, French, and Jones (2010) with AHEAD data, we show that including gender and health histories in the discriminator improves identification and precision of bequests motives. The resulting estimates reveal that bequest motives explain between $13\\%$ and $19\\%$ percent of late-life savings across all permanent-income quintiles, not only among the rich. The adversarial estimator precisely disentangles bequest motives from precautionary savings motives. These findings suggest that heterogeneity in health-related survival expectations is another important source of identifying variation to distinguishing bequest and precautionary saving motives.", "AI": {"tldr": "\u4f7f\u7528\u5bf9\u6297\u6027\u7ed3\u6784\u4f30\u8ba1\u6846\u67b6\u91cd\u65b0\u5206\u6790\u8001\u5e74\u4eba\u50a8\u84c4\u884c\u4e3a\uff0c\u53d1\u73b0\u9057\u8d60\u52a8\u673a\u89e3\u91ca\u4e8613%-19%\u7684\u665a\u5e74\u50a8\u84c4\uff0c\u4e14\u8be5\u52a8\u673a\u4e0d\u4ec5\u9650\u4e8e\u5bcc\u4eba\u7fa4\u4f53\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u8001\u5e74\u4eba\u50a8\u84c4\u884c\u4e3a\uff0c\u7279\u522b\u662f\u4e3a\u4e86\u533a\u5206\u9057\u8d60\u52a8\u673a\u548c\u9884\u9632\u6027\u50a8\u84c4\u52a8\u673a\uff0c\u5e76\u6539\u8fdb\u5bf9\u9057\u8d60\u52a8\u673a\u7684\u8bc6\u522b\u548c\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528Kaji\u3001Manresa\u548cPouliot\uff082023\uff09\u7684\u5bf9\u6297\u6027\u7ed3\u6784\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408De Nardi\u3001French\u548cJones\uff082010\uff09\u7684\u6a21\u578b\uff0c\u4f7f\u7528AHEAD\u6570\u636e\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u7684\u7075\u6d3b\u5224\u522b\u5668\u81ea\u9002\u5e94\u9009\u62e9\u6570\u636e\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u7279\u5f81\u3002", "result": "\u5728\u5224\u522b\u5668\u4e2d\u5305\u542b\u6027\u522b\u548c\u5065\u5eb7\u53f2\u63d0\u9ad8\u4e86\u9057\u8d60\u52a8\u673a\u7684\u8bc6\u522b\u548c\u4f30\u8ba1\u7cbe\u5ea6\u3002\u9057\u8d60\u52a8\u673a\u89e3\u91ca\u4e86\u6240\u6709\u6c38\u4e45\u6536\u5165\u4e94\u5206\u4f4d\u6570\u4e2d13%-19%\u7684\u665a\u5e74\u50a8\u84c4\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5bcc\u4eba\u7fa4\u4f53\u3002\u5bf9\u6297\u6027\u4f30\u8ba1\u5668\u80fd\u591f\u7cbe\u786e\u533a\u5206\u9057\u8d60\u52a8\u673a\u548c\u9884\u9632\u6027\u50a8\u84c4\u52a8\u673a\u3002", "conclusion": "\u5065\u5eb7\u76f8\u5173\u7684\u751f\u5b58\u9884\u671f\u5f02\u8d28\u6027\u662f\u533a\u5206\u9057\u8d60\u548c\u9884\u9632\u6027\u50a8\u84c4\u52a8\u673a\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u8bc6\u522b\u53d8\u5f02\u6765\u6e90\u3002"}}
{"id": "2511.12278", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12278", "abs": "https://arxiv.org/abs/2511.12278", "authors": ["Mingqi Wu", "Qiang Sun", "Yi Yang"], "title": "PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning", "comment": "14 pages main, 26 pages appendix", "summary": "High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPCA++\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u5747\u5300\u6027\u7ea6\u675f\u5bf9\u6bd4\u5b66\u4e60\u6765\u4ece\u6b63\u6837\u672c\u5bf9\u4e2d\u6062\u590d\u5171\u4eab\u4fe1\u53f7\u5b50\u7a7a\u95f4\uff0c\u6709\u6548\u5bf9\u6297\u7ed3\u6784\u5316\u80cc\u666f\u566a\u58f0\u5e72\u6270\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u4f4e\u7ef4\u4fe1\u53f7\u5e38\u88ab\u7ed3\u6784\u5316\u80cc\u666f\u566a\u58f0\u63a9\u76d6\uff0c\u6807\u51c6PCA\u6548\u679c\u6709\u9650\u3002\u5bf9\u6bd4\u5b66\u4e60\u542f\u53d1\u4e86\u4ece\u6b63\u6837\u672c\u5bf9\u4e2d\u6062\u590d\u5171\u4eab\u4fe1\u53f7\u5b50\u7a7a\u95f4\u7684\u9700\u6c42\u3002", "method": "PCA++\u91c7\u7528\u786c\u5747\u5300\u6027\u7ea6\u675f\u5bf9\u6bd4PCA\uff0c\u5f3a\u5236\u6295\u5f71\u7279\u5f81\u5177\u6709\u6052\u7b49\u534f\u65b9\u5dee\uff0c\u901a\u8fc7\u5e7f\u4e49\u7279\u5f81\u95ee\u9898\u83b7\u5f97\u95ed\u5f0f\u89e3\uff0c\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "result": "PCA++\u5728\u6a21\u62df\u5b9e\u9a8c\u3001\u635f\u574fMNIST\u548c\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u5b66\u4e2d\u4f18\u4e8e\u6807\u51c6PCA\u548c\u4ec5\u5bf9\u9f50\u7684PCA+\uff0c\u80fd\u53ef\u9760\u6062\u590d\u6761\u4ef6\u4e0d\u53d8\u7ed3\u6784\u3002", "conclusion": "\u660e\u786e\u7279\u5f81\u5206\u6563\u7684\u5747\u5300\u6027\u7ea6\u675f\u80fd\u9632\u5fa1\u7ed3\u6784\u5316\u566a\u58f0\u5e76\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u9610\u660e\u4e86\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5747\u5300\u6027\u7684\u4f5c\u7528\u3002"}}
{"id": "2511.12053", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12053", "abs": "https://arxiv.org/abs/2511.12053", "authors": ["Xubo Gu", "Xun Huan", "Yao Ren", "Wenqing Zhou", "Weiran Jiang", "Ziyou Song"], "title": "Real-Time Physics-Aware Battery Health Monitoring from Partial Charging Profiles via Physics-Informed Neural Networks", "comment": null, "summary": "Monitoring battery health is essential for ensuring safe and efficient operation. However, there is an inherent trade-off between assessment speed and diagnostic depth-specifically, between rapid overall health estimation and precise identification of internal degradation states. Capturing detailed internal battery information efficiently remains a major challenge, yet such insights are key to understanding the various degradation mechanisms. To address this, we develop a parameterized physics-informed neural network (P-PINNSPM) over the key aging-related parameter space for a single particle model. The model can accurately predict internal battery variables across the parameter space and identifies internal parameters in about 30 seconds-achieving a 47x speedup over the finite volume method-while maintaining high accuracy. These parameters improve the battery state-of-health (SOH) estimation accuracy by at least 60.61%, compared to models without parameter incorporation. Moreover, they enable extrapolation to unseen SOH levels and support robust estimation across diverse charging profiles and operating conditions. Our results demonstrate the strong potential of physics-informed machine learning to advance real-time, data-efficient, and physics-aware battery management systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(P-PINNSPM)\uff0c\u7528\u4e8e\u5feb\u901f\u51c6\u786e\u76d1\u6d4b\u7535\u6c60\u5065\u5eb7\u72b6\u6001\uff0c\u572830\u79d2\u5185\u8bc6\u522b\u5185\u90e8\u53c2\u6570\uff0c\u6bd4\u6709\u9650\u4f53\u79ef\u6cd5\u5feb47\u500d\uff0c\u540c\u65f6\u5c06SOH\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u9ad8\u81f3\u5c1160.61%\u3002", "motivation": "\u89e3\u51b3\u7535\u6c60\u5065\u5eb7\u76d1\u6d4b\u4e2d\u8bc4\u4f30\u901f\u5ea6\u4e0e\u8bca\u65ad\u6df1\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u9ad8\u6548\u83b7\u53d6\u8be6\u7ec6\u5185\u90e8\u7535\u6c60\u4fe1\u606f\u4ee5\u7406\u89e3\u5404\u79cd\u9000\u5316\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u5173\u952e\u8001\u5316\u76f8\u5173\u53c2\u6570\u7a7a\u95f4\u5f00\u53d1\u53c2\u6570\u5316\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u5e94\u7528\u4e8e\u5355\u7c92\u5b50\u6a21\u578b\uff0c\u9884\u6d4b\u5185\u90e8\u7535\u6c60\u53d8\u91cf\u5e76\u8bc6\u522b\u5185\u90e8\u53c2\u6570\u3002", "result": "\u6a21\u578b\u572830\u79d2\u5185\u51c6\u786e\u8bc6\u522b\u5185\u90e8\u53c2\u6570\uff0c\u6bd4\u6709\u9650\u4f53\u79ef\u6cd5\u5feb47\u500d\uff1b\u53c2\u6570\u878d\u5165\u4f7fSOH\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u9ad8\u81f3\u5c1160.61%\uff1b\u80fd\u591f\u5916\u63a8\u5230\u672a\u89c1SOH\u6c34\u5e73\uff0c\u5e76\u5728\u4e0d\u540c\u5145\u7535\u66f2\u7ebf\u548c\u64cd\u4f5c\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5065\u4f30\u8ba1\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u5728\u63a8\u8fdb\u5b9e\u65f6\u3001\u6570\u636e\u9ad8\u6548\u4e14\u7269\u7406\u611f\u77e5\u7684\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.11687", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11687", "abs": "https://arxiv.org/abs/2511.11687", "authors": ["Dragan Filimonovic", "Christian Rutzer", "Jeffrey Macher", "Rolf Weder"], "title": "Generative AI as a Linguistic Equalizer in Global Science", "comment": null, "summary": "For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u6b63\u5728\u6210\u4e3a\u5168\u7403\u79d1\u5b66\u4e2d\u7684\u8bed\u8a00\u5747\u8861\u5668\uff0c\u5e2e\u52a9\u975e\u82f1\u8bed\u56fd\u5bb6\u4f5c\u8005\u7f29\u5c0f\u4e0e\u82f1\u8bed\u6bcd\u8bed\u4f5c\u8005\u5728\u79d1\u5b66\u5199\u4f5c\u98ce\u683c\u4e0a\u7684\u5dee\u8ddd\u3002", "motivation": "\u82f1\u8bed\u5728\u79d1\u5b66\u9886\u57df\u7684\u957f\u671f\u4e3b\u5bfc\u5730\u4f4d\u4e3a\u975e\u6bcd\u8bed\u8005\u8bbe\u7f6e\u4e86\u969c\u788d\uff0c\u751f\u6210\u5f0fAI\u53ef\u80fd\u4e3a\u89e3\u51b3\u8fd9\u4e00\u4e0d\u5e73\u7b49\u95ee\u9898\u63d0\u4f9b\u6280\u672f\u65b9\u6848\u3002", "method": "\u5206\u67902021-2024\u5e74\u95f4\u7684565\u4e07\u7bc7\u79d1\u5b66\u8bba\u6587\uff0c\u4f7f\u7528SciBERT\u6587\u672c\u5d4c\u5165\u6a21\u578b\u6d4b\u91cf\u975e\u82f1\u8bed\u56fd\u5bb6\u4f5c\u8005\u5728GenAI\u8f85\u52a9\u4e0b\u4e0e\u7f8e\u82f1\u4f5c\u8005\u79d1\u5b66\u5199\u4f5c\u98ce\u683c\u7684\u76f8\u4f3c\u5ea6\u53d8\u5316\u3002", "result": "ChatGPT\u53d1\u5e03\u540e\uff0cGenAI\u8f85\u52a9\u7684\u8bba\u6587\u4e0e\u7f8e\u82f1\u4f5c\u8005\u5199\u4f5c\u98ce\u683c\u8d8b\u540c\u5ea6\u663e\u8457\u589e\u52a0\uff0c\u4e14\u8bed\u8a00\u8ddd\u79bb\u82f1\u8bed\u8d8a\u8fdc\u7684\u56fd\u5bb6\u56e2\u961f\u6548\u679c\u8d8a\u660e\u663e\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u6b63\u5728\u901a\u8fc7\u964d\u4f4e\u8bed\u8a00\u969c\u788d\u6765\u91cd\u5851\u5168\u7403\u79d1\u5b66\u4ea4\u6d41\u683c\u5c40\uff0c\u4e3a\u975e\u82f1\u8bed\u56fd\u5bb6\u7814\u7a76\u8005\u63d0\u4f9b\u516c\u5e73\u7ade\u4e89\u73af\u5883\u3002"}}
{"id": "2511.11997", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.11997", "abs": "https://arxiv.org/abs/2511.11997", "authors": ["Paulo Henrique Foganholo Biazetto", "Mirko Fiacchini", "Christophe Prieur", "Gustavo Artur de Andrade"], "title": "Imitation Learning with Safety and L2 Stability Certificates for Boundary Control of Reaction-Diffusion PDEs", "comment": null, "summary": "This paper proposes an imitation learning (IL) framework for synthesizing neural network (NN) controllers that achieve boundary stabilization of systems governed by reaction-diffusion partial differential equations (PDEs). The plant is assumed to be actuated through a Dirichlet boundary condition and subject to a Neumann condition on the unactuated side. The design is based on a finite-dimensional truncated model that captures the unstable dynamics of the original infinite-dimensional system, which is obtained via spectral decomposition. Convex stability and safety conditions are then derived for this truncated model by combining Lyapunov theory with local quadratic constraints (QC), which bound the nonlinear activation functions of the NN and guarantee robustness to model truncation, thus addressing the spillover problem. These conditions are integrated into the IL process to jointly minimize the imitation loss and maximize the volume of the certified region of attraction (ROA). The proposed framework is validated on an unstable reaction-diffusion PDE, demonstrating that the resulting NN controller efficiently reproduces the expert policy while ensuring formal stability guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u53cd\u5e94-\u6269\u6563\u504f\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u7684\u8fb9\u754c\u9547\u5b9a\u63a7\u5236\uff0c\u901a\u8fc7\u7ed3\u5408Lyapunov\u7406\u8bba\u548c\u5c40\u90e8\u4e8c\u6b21\u7ea6\u675f\u6765\u4fdd\u8bc1\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u65e0\u9650\u7ef4PDE\u7cfb\u7edf\u7684\u8fb9\u754c\u9547\u5b9a\u63a7\u5236\u95ee\u9898\uff0c\u540c\u65f6\u5904\u7406\u6a21\u578b\u622a\u65ad\u5e26\u6765\u7684\u6ea2\u51fa\u6548\u5e94\uff0c\u786e\u4fdd\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7684\u5f62\u5f0f\u5316\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002", "method": "\u57fa\u4e8e\u8c31\u5206\u89e3\u83b7\u5f97\u6355\u83b7\u4e0d\u7a33\u5b9a\u52a8\u6001\u7684\u6709\u9650\u7ef4\u622a\u65ad\u6a21\u578b\uff0c\u7ed3\u5408Lyapunov\u7406\u8bba\u548c\u5c40\u90e8\u4e8c\u6b21\u7ea6\u675f\u63a8\u5bfc\u51f8\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u6761\u4ef6\uff0c\u5728\u6a21\u4eff\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u8054\u5408\u6700\u5c0f\u5316\u6a21\u4eff\u635f\u5931\u548c\u6700\u5927\u5316\u8ba4\u8bc1\u5438\u5f15\u57df\u4f53\u79ef\u3002", "result": "\u5728\u4e0d\u7a33\u5b9a\u7684\u53cd\u5e94-\u6269\u6563PDE\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u80fd\u591f\u9ad8\u6548\u590d\u5236\u4e13\u5bb6\u7b56\u7565\uff0c\u540c\u65f6\u786e\u4fdd\u5f62\u5f0f\u5316\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86PDE\u7cfb\u7edf\u7684\u8fb9\u754c\u9547\u5b9a\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u6ea2\u51fa\u95ee\u9898\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u5728\u65e0\u9650\u7ef4\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002"}}
{"id": "2511.11829", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11829", "abs": "https://arxiv.org/abs/2511.11829", "authors": ["Mihir Gupte", "Ramesh S"], "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification", "comment": "To be submitted for publication", "summary": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.", "AI": {"tldr": "\u63a2\u7d22\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u9a8c\u8bc1LLM\u751f\u6210\u8f93\u51fa\u7684\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u5f62\u5f0f\u9a8c\u8bc1\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u6b63\u5f0f\u65b9\u6cd5\u6765\u9a8c\u8bc1LLM\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7684\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5982Gherkin\u573a\u666f\uff09\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5668\uff0c\u5c06\u975e\u6b63\u5f0f\u8bed\u53e5\u8f6c\u6362\u4e3a\u5f62\u5f0f\u903b\u8f91\uff0c\u5728\u4e24\u4e2a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1LLM\u751f\u6210\u8f93\u51fa\u4e0e\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u7684\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u6210\u529f\u8bc6\u522b\u4e86\u4e24\u4e2a\u4e0d\u540c\u8868\u8ff0\u7684\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u5728\u903b\u8f91\u4e0a\u7b49\u4ef7\uff1b\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u8bc6\u522b\u4e86\u7ed9\u5b9a\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u4e0eLLM\u751f\u6210\u8f93\u51fa\u4e4b\u95f4\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u81ea\u52a8\u5f62\u5f0f\u5316\u5728\u786e\u4fddLLM\u751f\u6210\u8f93\u51fa\u7684\u4fdd\u771f\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11579", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11579", "abs": "https://arxiv.org/abs/2511.11579", "authors": ["Felipe Urrutia", "Jorge Salas", "Alexander Kozachinskiy", "Cristian Buc Calderon", "Hector Pasten", "Cristobal Rojas"], "title": "Decoupling Positional and Symbolic Attention Behavior in Transformers", "comment": "32 pages, 12 figures, repository available", "summary": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86Transformer\u4e2dRotary\u4f4d\u7f6e\u7f16\u7801(RoPE)\u7684\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u7b26\u53f7\u4fe1\u606f\u7f16\u7801\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u6ce8\u610f\u529b\u5934\u7684\u4f4d\u7f6e\u884c\u4e3a\u548c\u7b26\u53f7\u884c\u4e3a\u662f\u4e92\u65a5\u7684\uff0c\u5e76\u5f00\u53d1\u4e86\u91cf\u5316\u8fd9\u4e9b\u884c\u4e3a\u7684\u6307\u6807\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u4e2d\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u7b26\u53f7\u4fe1\u606f\u7684\u72ec\u7acb\u7f16\u7801\u673a\u5236\uff0c\u7279\u522b\u662fRoPE\u4f4d\u7f6e\u7f16\u7801\u5728Transformer\u4e2d\u7684\u6210\u529f\u539f\u7406\u53ca\u5176\u9891\u7387\u4f7f\u7528\u6a21\u5f0f\u3002", "method": "\u4ece\u7406\u8bba\u548c\u5b9e\u8bc1\u5c42\u9762\u5206\u6790\u6ce8\u610f\u529b\u5934\u7684\u4f4d\u7f6e\u4e0e\u7b26\u53f7\u884c\u4e3a\u4e8c\u5206\u6027\uff0c\u5b9a\u4e49\u4f4d\u7f6e\u884c\u4e3a\u548c\u7b26\u53f7\u884c\u4e3a\u7684\u4e00\u822c\u5b9a\u4e49\uff0c\u8bc1\u660e\u4e8c\u8005\u4e92\u65a5\u6027\uff0c\u5f00\u53d1\u91cf\u5316\u6307\u6807\uff0c\u5e76\u5728\u4f7f\u7528RoPE\u7684Transformer LLMs\u4e0a\u5e94\u7528\u5206\u6790\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u6240\u6709\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\u4e0e\u9891\u7387\u4f7f\u7528\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5934\u53ef\u8bbf\u95ee\u7684\u9891\u7387\u53ef\u4ee5\u63a7\u5236Transformer\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9RoPE\u7684\u8be6\u7ec6\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u5176\u7279\u6027\u5982\u4f55\u4e0e\u6a21\u578b\u884c\u4e3a\u76f8\u5173\u8054\uff0c\u7279\u522b\u662f\u9891\u7387\u4f7f\u7528\u5728\u4f4d\u7f6e\u548c\u7b26\u53f7\u4efb\u52a1\u4e2d\u7684\u56e0\u679c\u4f5c\u7528\u3002"}}
{"id": "2511.11611", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11611", "abs": "https://arxiv.org/abs/2511.11611", "authors": ["David H. Silver"], "title": "Quantifying Skill and Chance: A Unified Framework for the Geometry of Games", "comment": null, "summary": "We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6e38\u620f\u5efa\u6a21\u4e3a\u968f\u673a\u51b3\u7b56\u6811\u6765\u5206\u79bb\u6280\u80fd\u548c\u8fd0\u6c14\u6210\u5206\uff0c\u5b9a\u4e49\u4e86\u6280\u80fd-\u8fd0\u6c14\u6307\u6570S(G)\u5728[-1,1]\u8303\u56f4\u5185\uff0c\u5e76\u5f15\u5165\u6ce2\u52a8\u6027Sigma\u6765\u91cf\u5316\u8fde\u7eed\u56de\u5408\u7684\u7ed3\u679c\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u91cf\u5316\u6e38\u620f\u4e2d\u6280\u80fd\u548c\u8fd0\u6c14\u7684\u76f8\u5bf9\u8d21\u732e\uff0c\u4ee5\u4fbf\u8fdb\u884c\u6e38\u620f\u8bbe\u8ba1\u3001AI\u8bc4\u4f30\u548c\u98ce\u9669\u5206\u6790\u7b49\u65b9\u9762\u7684\u5ba2\u89c2\u6bd4\u8f83\u3002", "method": "\u5c06\u6e38\u620f\u5efa\u6a21\u4e3a\u968f\u673a\u51b3\u7b56\u6811\uff0c\u5206\u89e3\u6e38\u620f\u7ed3\u679c\u4e3a\u6280\u80fd\u6760\u6746K\u548c\u8fd0\u6c14\u6760\u6746L\uff0c\u5b9a\u4e49\u6280\u80fd-\u8fd0\u6c14\u6307\u6570S(G) = (K-L)/(K+L)\uff0c\u5e76\u5f15\u5165\u6ce2\u52a8\u6027Sigma\u6765\u6d4b\u91cf\u7ed3\u679c\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5206\u6790\u4e8630\u4e2a\u6e38\u620f\uff0c\u53d1\u73b0\u4ece\u7eaf\u8fd0\u6c14\uff08\u629b\u786c\u5e01\uff0cS=-1\uff09\u5230\u7eaf\u6280\u80fd\uff08\u56fd\u9645\u8c61\u68cb\uff0cS=+1\uff09\u7684\u8fde\u7eed\u8c31\uff0c\u6251\u514b\u663e\u793a\u4e2d\u7b49\u6280\u80fd\u4e3b\u5bfc\uff08S=0.33\uff09\uff0c\u897f\u6d0b\u53cc\u9646\u68cb\u5904\u4e8e\u5e73\u8861\u72b6\u6001\uff08S=0\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u4e00\u822c\u968f\u673a\u51b3\u7b56\u7cfb\u7edf\uff0c\u4e3a\u73a9\u5bb6\u5f71\u54cd\u529b\u3001\u6e38\u620f\u5e73\u8861\u6027\u548c\u9884\u6d4b\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5728\u6e38\u620f\u8bbe\u8ba1\u3001AI\u8bc4\u4f30\u548c\u98ce\u9669\u8bc4\u4f30\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.13433", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2511.13433", "abs": "https://arxiv.org/abs/2511.13433", "authors": ["Emmanuel Flachaire", "Bertille Picard"], "title": "Decomposing Inequalities using Machine Learning and Overcoming Common Support Issues", "comment": null, "summary": "The Kitagawa-Oaxaca-Blinder decomposition splits the difference in means between two groups into an explained part, due to observable factors, and an unexplained part. In this paper, we reformulate this framework using potential outcomes, highlighting the critical role of the reference outcome. To address limitations like common support and model misspecification, we extend Neumark's (1988) weighted reference approach with a doubly robust estimator. Using Neyman orthogonality and double machine learning, our method avoids trimming and extrapolation. This improves flexibility and robustness, as illustrated by two empirical applications. Nevertheless, we also highlight that the decomposition based on the Neumark reference outcome is particularly sensitive to the inclusion of irrelevant explanatory variables.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u6f5c\u5728\u7ed3\u679c\u91cd\u65b0\u6784\u5efa\u4e86Kitagawa-Oaxaca-Blinder\u5206\u89e3\u6846\u67b6\uff0c\u6269\u5c55\u4e86Neumark\u7684\u52a0\u6743\u53c2\u8003\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u5668\u89e3\u51b3\u5171\u540c\u652f\u6301\u548c\u6a21\u578b\u8bef\u8bbe\u95ee\u9898\uff0c\u901a\u8fc7Neyman\u6b63\u4ea4\u6027\u548c\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u907f\u514d\u4fee\u526a\u548c\u5916\u63a8\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5206\u89e3\u65b9\u6cd5\u5728\u5171\u540c\u652f\u6301\u548c\u6a21\u578b\u8bef\u8bbe\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u5206\u89e3\u7684\u7075\u6d3b\u6027\u548c\u7a33\u5065\u6027\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u7ed3\u679c\u91cd\u65b0\u8868\u8ff0\u5206\u89e3\u6846\u67b6\uff0c\u6269\u5c55Neumark\u7684\u52a0\u6743\u53c2\u8003\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u5668\u3001Neyman\u6b63\u4ea4\u6027\u548c\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5206\u89e3\u7684\u7075\u6d3b\u6027\u548c\u7a33\u5065\u6027\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u8bc1\u5e94\u7528\u8fdb\u884c\u4e86\u8bf4\u660e\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6539\u8fdb\u4e86\u5206\u89e3\u7684\u7075\u6d3b\u6027\u548c\u7a33\u5065\u6027\uff0c\u4f46\u57fa\u4e8eNeumark\u53c2\u8003\u7ed3\u679c\u7684\u5206\u89e3\u5bf9\u5305\u542b\u65e0\u5173\u89e3\u91ca\u53d8\u91cf\u7279\u522b\u654f\u611f\u3002"}}
{"id": "2511.12688", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12688", "abs": "https://arxiv.org/abs/2511.12688", "authors": ["Kaicheng Jin", "Yang Peng", "Jiansheng Yang", "Zhihua Zhang"], "title": "Accelerated Distributional Temporal Difference Learning with Linear Function Approximation", "comment": null, "summary": "In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u7684\u5206\u5e03\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u7684\u6709\u9650\u6837\u672c\u7edf\u8ba1\u7387\uff0c\u5efa\u7acb\u4e86\u4e0e\u652f\u6491\u5927\u5c0fK\u65e0\u5173\u7684\u7d27\u81f4\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\uff0c\u8868\u660e\u5b66\u4e60\u56de\u62a5\u51fd\u6570\u7684\u5b8c\u6574\u5206\u5e03\u4e0e\u5b66\u4e60\u5176\u671f\u671b\u503c\u5177\u6709\u76f8\u540c\u7684\u96be\u5ea6\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u5206\u5e03TD\u5b66\u4e60\u7684\u7edf\u8ba1\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u8868\u683c\u60c5\u51b5\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u8bbe\u7f6e\u4e0b\u7684\u7edf\u8ba1\u6548\u7387\uff0c\u7279\u522b\u662f\u5f53\u652f\u6491\u5927\u5c0fK\u5f88\u5927\u65f6\u5982\u4f55\u5efa\u7acb\u4e0d\u4f9d\u8d56\u4e8eK\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002", "method": "\u9996\u5148\u5bf9\u7ebf\u6027\u5206\u7c7b\u8d1d\u5c14\u66fc\u65b9\u7a0b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u7136\u540e\u5728\u65b0\u7684\u7b97\u6cd5\u4e2d\u7ed3\u5408\u65b9\u5dee\u51cf\u5c11\u6280\u672f\uff0c\u4ee5\u5efa\u7acb\u4e0e\u652f\u6491\u5927\u5c0fK\u65e0\u5173\u7684\u7d27\u81f4\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002", "result": "\u5f53K\u5f88\u5927\u65f6\uff0c\u5efa\u7acb\u4e86\u4e0d\u4f9d\u8d56\u4e8e\u652f\u6491\u5927\u5c0fK\u7684\u7d27\u81f4\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\uff0c\u8868\u660e\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u7684\u5206\u5e03TD\u5b66\u4e60\u4ece\u6d41\u6570\u636e\u4e2d\u5b66\u4e60\u56de\u62a5\u51fd\u6570\u7684\u5b8c\u6574\u5206\u5e03\u4e0e\u5b66\u4e60\u5176\u671f\u671b\u503c\u5177\u6709\u76f8\u540c\u7684\u96be\u5ea6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u7edf\u8ba1\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u8bc1\u660e\u5b66\u4e60\u5b8c\u6574\u56de\u62a5\u5206\u5e03\u5e76\u4e0d\u6bd4\u5b66\u4e60\u5176\u671f\u671b\u503c\u66f4\u56f0\u96be\u3002"}}
{"id": "2511.12163", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12163", "abs": "https://arxiv.org/abs/2511.12163", "authors": ["Vlad-Matei Anghelu\u0163\u0103", "Bogdan Gheorghe", "Daniel Ioan", "Ionela Prodan", "Florin Stoican"], "title": "Tight displacement-based formation control under bounded disturbances. A set-theoretic perspective", "comment": "submitted to ECC'26", "summary": "This paper investigates the synthesis of controllers for displacement-based formation control in the presence of bounded disturbances, specifically focusing on uncertainties originating from measurement noise. While the literature frequently addresses such problems using stochastic frameworks, this work proposes a deterministic methodology grounded in set-theoretic concepts. By leveraging the principles of set invariance, we adapt the theory of ultimate boundedness to the specific dynamics of displacement-based formations. This approach provides a rigorous method for analyzing the system's behavior under persistent disturbances. Furthermore, this set-theoretic framework allows for the optimized selection of the proposed control law parameters to guarantee pre-specified performance bounds. The efficacy of the synthesized controller is demonstrated in the challenging application of maintaining tight formations in a multi-obstacles environment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u96c6\u5408\u8bba\u6982\u5ff5\u7684\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4d\u79fb\u7f16\u961f\u63a7\u5236\u4e2d\u5728\u6709\u754c\u6270\u52a8\u4e0b\u7684\u63a7\u5236\u5668\u5408\u6210\uff0c\u901a\u8fc7\u96c6\u5408\u4e0d\u53d8\u6027\u539f\u7406\u4fdd\u8bc1\u9884\u8bbe\u6027\u80fd\u8fb9\u754c", "motivation": "\u73b0\u6709\u6587\u732e\u591a\u91c7\u7528\u968f\u673a\u6846\u67b6\u5904\u7406\u6d4b\u91cf\u566a\u58f0\u7b49\u6709\u754c\u6270\u52a8\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u786e\u5b9a\u6027\u7684\u66ff\u4ee3\u65b9\u6cd5", "method": "\u5229\u7528\u96c6\u5408\u4e0d\u53d8\u6027\u539f\u7406\uff0c\u5c06\u6700\u7ec8\u6709\u754c\u6027\u7406\u8bba\u5e94\u7528\u4e8e\u4f4d\u79fb\u7f16\u961f\u7684\u7279\u5b9a\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u96c6\u5408\u8bba\u6846\u67b6\u4f18\u5316\u63a7\u5236\u5f8b\u53c2\u6570\u9009\u62e9", "result": "\u6240\u5408\u6210\u7684\u63a7\u5236\u5668\u80fd\u591f\u5728\u591a\u969c\u788d\u7269\u73af\u5883\u4e2d\u6709\u6548\u7ef4\u6301\u7d27\u5bc6\u7f16\u961f\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u57fa\u4e8e\u96c6\u5408\u8bba\u7684\u786e\u5b9a\u6027\u65b9\u6cd5\u4e3a\u4f4d\u79fb\u7f16\u961f\u63a7\u5236\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u4fdd\u8bc1\u7cfb\u7edf\u5728\u6301\u7eed\u6270\u52a8\u4e0b\u7684\u9884\u8bbe\u6027\u80fd\u8fb9\u754c"}}
{"id": "2511.11689", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11689", "abs": "https://arxiv.org/abs/2511.11689", "authors": ["Thomas D. Hull", "Lizhe Zhang", "Patricia A. Arean", "Matteo Malgaroli"], "title": "Mental Health Generative AI is Safe, Promotes Social Health, and Reduces Depression and Anxiety: Real World Evidence from a Naturalistic Cohort", "comment": null, "summary": "Generative artificial intelligence (GAI) chatbots built for mental health could deliver safe, personalized, and scalable mental health support. We evaluate a foundation model designed for mental health. Adults completed mental health measures while engaging with the chatbot between May 15, 2025 and September 15, 2025. Users completed an opt-in consent, demographic information, mental health symptoms, social connection, and self-identified goals. Measures were repeated every two weeks up to 6 weeks, and a final follow-up at 10 weeks. Analyses included effect sizes, and growth mixture models to identify participant groups and their characteristic engagement, severity, and demographic factors. Users demonstrated significant reductions in PHQ-9 and GAD-7 that were sustained at follow-up. Significant improvements in Hope, Behavioral Activation, Social Interaction, Loneliness, and Perceived Social Support were observed throughout and maintained at 10 week follow-up. Engagement was high and predicted outcomes. Working alliance was comparable to traditional care and predicted outcomes. Automated safety guardrails functioned as designed, with 76 sessions flagged for risk and all handled according to escalation policies. This single arm naturalistic observational study provides initial evidence that a GAI foundation model for mental health can deliver accessible, engaging, effective, and safe mental health support. These results lend support to findings from early randomized designs and offer promise for future study of mental health GAI in real world settings.", "AI": {"tldr": "\u8bc4\u4f30\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u7684\u57fa\u7840\u6a21\u578bAI\u804a\u5929\u673a\u5668\u4eba\uff0c\u5728\u81ea\u7136\u89c2\u5bdf\u7814\u7a76\u4e2d\u663e\u793a\u80fd\u63d0\u4f9b\u5b89\u5168\u3001\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301", "motivation": "\u751f\u6210\u5f0fAI\u5fc3\u7406\u5065\u5eb7\u804a\u5929\u673a\u5668\u4eba\u53ef\u4ee5\u63d0\u4f9b\u5b89\u5168\u3001\u4e2a\u6027\u5316\u4e14\u53ef\u6269\u5c55\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5b9e\u9645\u6548\u679c", "method": "\u5355\u81c2\u81ea\u7136\u89c2\u5bdf\u7814\u7a76\uff0c\u6210\u5e74\u7528\u6237\u4e0eAI\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\uff0c\u6bcf\u4e24\u5468\u91cd\u590d\u6d4b\u91cf\u5fc3\u7406\u5065\u5eb7\u6307\u6807\uff0c\u6301\u7eed6\u5468\uff0c\u6700\u7ec8\u968f\u8bbf10\u5468", "result": "\u7528\u6237PHQ-9\u548cGAD-7\u8bc4\u5206\u663e\u8457\u964d\u4f4e\u5e76\u6301\u7eed\u5230\u968f\u8bbf\u671f\uff0c\u5e0c\u671b\u3001\u884c\u4e3a\u6fc0\u6d3b\u3001\u793e\u4ea4\u4e92\u52a8\u3001\u5b64\u72ec\u611f\u548c\u611f\u77e5\u793e\u4f1a\u652f\u6301\u663e\u8457\u6539\u5584\uff0c\u53c2\u4e0e\u5ea6\u9ad8\u4e14\u9884\u6d4b\u7ed3\u679c\uff0c\u5b89\u5168\u62a4\u680f\u6709\u6548\u8fd0\u884c", "conclusion": "\u5fc3\u7406\u5065\u5eb7GAI\u57fa\u7840\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u53ef\u8bbf\u95ee\u3001\u6709\u5438\u5f15\u529b\u3001\u6709\u6548\u4e14\u5b89\u5168\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u5e0c\u671b"}}
{"id": "2511.12064", "categories": ["math.OC", "math.DG"], "pdf": "https://arxiv.org/pdf/2511.12064", "abs": "https://arxiv.org/abs/2511.12064", "authors": ["Hiroshi Hirai"], "title": "Generalized gradient flows in Hadamard manifolds and convex optimization on entanglement polytopes", "comment": null, "summary": "In this paper, we address the optimization problem of minimizing $Q(df_x)$ over a Hadamard manifold ${\\cal M}$, where $f$ is a convex function on ${\\cal M}$, $df_x$ is the differential of $f$ at $x \\in {\\cal M}$, and $Q$ is a function on the cotangent bundle of ${\\cal M}$. This problem generalizes the problem of minimizing the gradient norm $\\|\\nabla f(x)\\|$ over ${\\cal M}$, studied by Hirai and Sakabe FOCS2024. We formulate a natural class of $Q$ in terms of convexity and invariance under parallel transports, and introduce a generalization of the gradient flow of $f$ that is expected to minimize $Q(df_x)$. For basic classes of manifolds, including the product of the manifolds of positive definite matrices, we prove that this gradient flow attains $\\inf_{x\\in {\\cal M}} Q(df_x)$ in the limit, and yields a duality relation. This result is applied to the Kempf-Ness optimization for GL-actions on tensors, which is Euclidean convex optimization on the class of moment polytopes, known as the entanglement polytopes. This type of convex optimization arises from tensor-related subjects in theoretical computer science, such as quantum functional, $G$-stable rank, and noncommutative rank.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728Hadamard\u6d41\u5f62\u4e0a\u6700\u5c0f\u5316Q(df_x)\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2df\u662f\u51f8\u51fd\u6570\uff0cQ\u662f\u4f59\u5207\u4e1b\u4e0a\u7684\u51fd\u6570\u3002\u8be5\u95ee\u9898\u63a8\u5e7f\u4e86\u6700\u5c0f\u5316\u68af\u5ea6\u8303\u6570\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5e7f\u4e49\u68af\u5ea6\u6d41\u6765\u6c42\u89e3\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a8\u5e7fHirai\u548cSakabe\u5728FOCS2024\u4e2d\u5173\u4e8e\u6700\u5c0f\u5316\u68af\u5ea6\u8303\u6570\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u5f15\u5165\u66f4\u4e00\u822c\u7684\u51fd\u6570Q\u6765\u6269\u5c55\u4f18\u5316\u95ee\u9898\u7684\u6846\u67b6\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6570\u5b66\u548c\u7269\u7406\u5e94\u7528\u573a\u666f\u3002", "method": "\u4f5c\u8005\u5b9a\u4e49\u4e86\u4e00\u7c7b\u5728\u5e73\u884c\u4f20\u8f93\u4e0b\u5177\u6709\u51f8\u6027\u548c\u4e0d\u53d8\u6027\u7684\u81ea\u7136\u51fd\u6570Q\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5e7f\u4e49\u68af\u5ea6\u6d41\u65b9\u6cd5\u3002\u5bf9\u4e8e\u5305\u62ec\u6b63\u5b9a\u77e9\u9635\u6d41\u5f62\u4e58\u79ef\u5728\u5185\u7684\u57fa\u672c\u6d41\u5f62\u7c7b\uff0c\u8bc1\u660e\u4e86\u8be5\u68af\u5ea6\u6d41\u5728\u6781\u9650\u4e0b\u80fd\u8fbe\u5230inf Q(df_x)\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u57fa\u672c\u6d41\u5f62\u7c7b\uff0c\u5e7f\u4e49\u68af\u5ea6\u6d41\u5728\u6781\u9650\u4e0b\u786e\u5b9e\u80fd\u8fbe\u5230inf_{x\u2208M} Q(df_x)\uff0c\u5e76\u5efa\u7acb\u4e86\u5bf9\u5076\u5173\u7cfb\u3002\u8be5\u7ed3\u679c\u88ab\u5e94\u7528\u4e8eGL-\u4f5c\u7528\u5728\u5f20\u91cf\u4e0a\u7684Kempf-Ness\u4f18\u5316\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aHadamard\u6d41\u5f62\u4e0a\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c06\u6700\u5c0f\u5316\u68af\u5ea6\u8303\u6570\u7684\u95ee\u9898\u63a8\u5e7f\u5230\u66f4\u4e00\u822c\u7684\u51fd\u6570Q\uff0c\u5e76\u5728\u5f20\u91cf\u76f8\u5173\u7684\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u95ee\u9898\u4e2d\u627e\u5230\u4e86\u5e94\u7528\u3002"}}
{"id": "2511.11857", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11857", "abs": "https://arxiv.org/abs/2511.11857", "authors": ["Taimur Khan", "Ramoza Ahsan", "Mohib Hameed"], "title": "Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection", "comment": "18 pages", "summary": "Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5206\u6790\u7535\u5f71\u5267\u672c\u60c5\u611f\u5f27\u7ebf\u5e76\u8fdb\u884c\u89d2\u8272\u4e0a\u4e0b\u6587\u6269\u5c55\u5206\u6790\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8eNRC-VAD\u6570\u636e\u96c6\u7684\u5b9a\u5236\u8bcd\u5178\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u5e76\u901a\u8fc7Ward\u5c42\u6b21\u805a\u7c7b\u6280\u672f\u805a\u7c7b\u76f8\u4f3c\u60c5\u611f\u60c5\u8282\u3002", "motivation": "\u6545\u4e8b\u7406\u89e3\u548c\u5206\u6790\u662f\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u6311\u6218\u9886\u57df\uff0c\u9700\u8981\u6df1\u5ea6\u8ba1\u7b97\u8bed\u4e49\u8868\u793a\u548c\u53e5\u6cd5\u5904\u7406\u3002\u5927\u91cf\u53d9\u4e8b\u6570\u636e\u9700\u8981\u81ea\u52a8\u5316\u8bed\u4e49\u5206\u6790\u548c\u8ba1\u7b97\u5b66\u4e60\uff0c\u800c\u975e\u624b\u52a8\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eNRC-VAD\u6570\u636e\u96c6Valence\u3001Arousal\u548cDominance\u5206\u6570\u7684\u5b9a\u5236\u8bcd\u5178\u8fdb\u884c\u8bcd\u5178\u5f0f\u60c5\u611f\u5206\u6790\uff0c\u5e94\u7528LabMTsimple storylab\u6a21\u5757\uff0c\u5e76\u901a\u8fc7Ward\u5c42\u6b21\u805a\u7c7b\u6280\u672f\u805a\u7c7b\u76f8\u4f3c\u60c5\u611f\u60c5\u8282\u3002", "result": "\u5728\u7535\u5f71\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u5206\u6790\u7ed3\u679c\u5bf9\u6d88\u8d39\u8005\u548c\u8bfb\u8005\u5728\u9009\u62e9\u53d9\u4e8b\u6216\u6545\u4e8b\u65f6\u5f88\u6709\u5e2e\u52a9\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u53d6\u53d9\u4e8b\u4f20\u8fbe\u7684\u9ad8\u5c42\u548c\u4f4e\u5c42\u6982\u5ff5\uff0c\u4e3a\u7535\u5f71\u5267\u672c\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.11581", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.11581", "abs": "https://arxiv.org/abs/2511.11581", "authors": ["Burkhard Ringlein", "Jan van Lunteren", "Radu Stoica", "Thomas Parnell"], "title": "The Anatomy of a Triton Attention Kernel", "comment": null, "summary": "A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eTriton DSL\u7684\u8de8\u5e73\u53f0LLM\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7paged attention\u5185\u6838\u5728NVIDIA\u548cAMD GPU\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u89e3\u51b3\u4e86LLM\u63a8\u7406\u7684\u786c\u4ef6\u53ef\u79fb\u690d\u6027\u95ee\u9898\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\u4e1a\u754c\u548c\u5b66\u672f\u754c\u90fd\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u8de8\u786c\u4ef6\u67b6\u6784\u7684LLM\u63a8\u7406\u5e73\u53f0\uff0c\u65e0\u9700\u5e95\u5c42\u624b\u52a8\u8c03\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f73\u6548\u7387\u3002", "method": "\u4f7f\u7528Triton\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u5f00\u53d1\u4e86\u6700\u5148\u8fdb\u7684paged attention\u5185\u6838\uff0c\u5305\u542b\u7b97\u6cd5\u548c\u7cfb\u7edf\u7ea7\u6539\u8fdb\u3001\u53c2\u6570\u81ea\u52a8\u8c03\u4f18\uff0c\u5e76\u96c6\u6210\u5230\u6d41\u884c\u7684\u63a8\u7406\u670d\u52a1\u5668\u4e2d\u3002", "result": "\u5c06\u901a\u7528Triton attention\u5185\u6838\u7684\u6027\u80fd\u4ece\u6700\u5148\u8fdb\u6c34\u5e73\u768419.7%\u63d0\u5347\u5230105.9%\uff0c\u5728NVIDIA\u548cAMD GPU\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u5f00\u6e90\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u53ef\u4ee5\u7528\u6765\u89e3\u9501\u8de8\u4e0d\u540cGPU\u5382\u5546\u7684\u6a21\u578b\u53ef\u79fb\u690d\u6027\uff0c\u8bc1\u660e\u4e86\u9ad8\u6548\u8de8\u5e73\u53f0LLM\u63a8\u7406\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.11693", "categories": ["cs.AI", "cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11693", "abs": "https://arxiv.org/abs/2511.11693", "authors": ["Xin Zhao", "Xiaojun Chen", "Bingshan Liu", "Zeyao Liu", "Zhendong Zhao", "Xiaoyan Gu"], "title": "Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation", "comment": null, "summary": "Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.", "AI": {"tldr": "VALOR\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u96f6\u6837\u672c\u7684\u667a\u80fd\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u63d0\u793a\u5206\u6790\u548c\u4ef7\u503c\u5bf9\u9f50\u63a8\u7406\u6765\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5b89\u5168\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e0d\u5b89\u5168\u8f93\u51fa\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u521b\u610f\u5a92\u4f53\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u65f6\u53ef\u80fd\u4ea7\u751f\u4e0d\u5b89\u5168\u3001\u5192\u72af\u6027\u6216\u6587\u5316\u4e0d\u6070\u5f53\u7684\u5185\u5bb9\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u6216\u589e\u52a0\u9ad8\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u4f7f\u8f93\u51fa\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u3002", "method": "VALOR\u6846\u67b6\u6574\u5408\u4e86\u5206\u5c42\u63d0\u793a\u5206\u6790\uff1a\u591a\u7ea7NSFW\u68c0\u6d4b\u5668\u8fc7\u6ee4\u8bcd\u6c47\u548c\u8bed\u4e49\u98ce\u9669\uff1b\u6587\u5316\u4ef7\u503c\u5bf9\u9f50\u6a21\u5757\u8bc6\u522b\u793e\u4f1a\u89c4\u8303\u3001\u5408\u6cd5\u6027\u548c\u4ee3\u8868\u6027\u4f26\u7406\u8fdd\u89c4\uff1b\u610f\u56fe\u6d88\u6b67\u5668\u68c0\u6d4b\u5fae\u5999\u6216\u4e0d\u5b89\u5168\u6697\u793a\u3002\u68c0\u6d4b\u5230\u4e0d\u5b89\u5168\u5185\u5bb9\u65f6\uff0c\u7531\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u89d2\u8272\u7279\u5b9a\u6307\u4ee4\u4e0b\u9009\u62e9\u6027\u91cd\u5199\u63d0\u793a\u3002\u5982\u679c\u751f\u6210\u56fe\u50cf\u4ecd\u4e0d\u5b89\u5168\uff0c\u53ef\u8fdb\u884c\u98ce\u683c\u5316\u518d\u751f\u4ee5\u5f15\u5bfc\u8f93\u51fa\u5230\u66f4\u5b89\u5168\u7684\u89c6\u89c9\u9886\u57df\u3002", "result": "\u5728\u5bf9\u6297\u6027\u3001\u6a21\u7cca\u6027\u548c\u4ef7\u503c\u654f\u611f\u63d0\u793a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVALOR\u5c06\u4e0d\u5b89\u5168\u8f93\u51fa\u51cf\u5c11\u4e86\u9ad8\u8fbe100.00%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63d0\u793a\u7684\u6709\u7528\u6027\u548c\u521b\u9020\u6027\u3002", "conclusion": "VALOR\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u90e8\u7f72\u5b89\u5168\u3001\u5bf9\u9f50\u4e14\u6709\u7528\u7684\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u3002"}}
{"id": "2511.13503", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.13503", "abs": "https://arxiv.org/abs/2511.13503", "authors": ["Ioannis Diamantis"], "title": "The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business", "comment": "36 pages, 22 figures", "summary": "Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \\textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u62d3\u6251\u6570\u636e\u5206\u6790(TDA)\u5728\u5546\u4e1a\u548c\u7ecf\u6d4e\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6301\u4e45\u540c\u8c03\u65b9\u6cd5\u63ed\u793a\u4f20\u7edf\u7ebf\u6027\u5de5\u5177\u65e0\u6cd5\u6355\u6349\u7684\u975e\u7ebf\u6027\u591a\u5c3a\u5ea6\u7ed3\u6784\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u62d3\u6251\u7a33\u5b9a\u6027\u6307\u6570(TSI)\u6765\u8861\u91cf\u7ed3\u6784\u53d8\u5f02\u6027\u3002", "motivation": "\u73b0\u4ee3\u5546\u4e1a\u548c\u7ecf\u6d4e\u6570\u636e\u96c6\u901a\u5e38\u5448\u73b0\u975e\u7ebf\u6027\u3001\u591a\u5c3a\u5ea6\u7ed3\u6784\uff0c\u4f20\u7edf\u7ebf\u6027\u5de5\u5177\u96be\u4ee5\u5145\u5206\u8868\u5f81\u8fd9\u4e9b\u7279\u5f81\u3002TDA\u63d0\u4f9b\u4e86\u4e00\u79cd\u51e0\u4f55\u89c6\u89d2\u6765\u53d1\u73b0\u8de8\u5c3a\u5ea6\u7684\u7a33\u5065\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u6301\u4e45\u540c\u8c03\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5b9e\u7528\u7684TDA\u5206\u6790\u6d41\u7a0b\uff0c\u5305\u62ec\u8ddd\u79bb\u5ea6\u91cf\u9009\u62e9\u3001\u590d\u5f62\u6784\u5efa\u548c\u89e3\u91ca\uff0c\u5e76\u5f15\u5165\u4e86\u62d3\u6251\u7a33\u5b9a\u6027\u6307\u6570(TSI)\u3002\u901a\u8fc7\u6d88\u8d39\u8005\u884c\u4e3a\u3001\u80a1\u7968\u5e02\u573a\u548c\u5916\u6c47\u52a8\u6001\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u8868\u660e\u62d3\u6251\u7279\u5f81\u80fd\u591f\u63ed\u793a\u8d85\u8d8a\u7ecf\u5178\u7edf\u8ba1\u65b9\u6cd5\u7684\u5206\u5272\u6a21\u5f0f\u548c\u7ed3\u6784\u5173\u7cfb\uff0c\u5728SAX/eSAX\u4e0eTDA\u7684\u6bd4\u8f83\u4e2d\u663e\u793a\u51fa\u4f18\u52bf\u3002", "conclusion": "\u4e3a\u5546\u4e1a\u548c\u7ecf\u6d4e\u5206\u6790\u4e2d\u7684TDA\u5b9e\u65bd\u3001\u53ef\u89c6\u5316\u548c\u6c9f\u901a\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u8bc1\u660e\u4e86TDA\u5728\u63ed\u793a\u590d\u6742\u6570\u636e\u7ed3\u6784\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
{"id": "2511.12749", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12749", "abs": "https://arxiv.org/abs/2511.12749", "authors": ["Zong-Han Bai", "Po-Yen Chu"], "title": "TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting", "comment": "Preprint. 11 pages, 1 figure, Equal contribution by the two authors", "summary": "Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability.\n  We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing.\n  On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable.", "AI": {"tldr": "TSB-HB\u662f\u4e00\u79cd\u5206\u5c42\u8d1d\u53f6\u65af\u6269\u5c55\u7684TSB\u65b9\u6cd5\uff0c\u7528\u4e8e\u95f4\u6b47\u6027\u9700\u6c42\u9884\u6d4b\uff0c\u901a\u8fc7Beta-Binomial\u5206\u5e03\u5efa\u6a21\u9700\u6c42\u53d1\u751f\uff0cLog-Normal\u5206\u5e03\u5efa\u6a21\u9700\u6c42\u5927\u5c0f\uff0c\u4f7f\u7528\u5206\u5c42\u5148\u9a8c\u5b9e\u73b0\u8de8\u9879\u76ee\u7684\u90e8\u5206\u6c60\u5316\uff0c\u5728\u7a00\u758f\u6216\u51b7\u542f\u52a8\u5e8f\u5217\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u95f4\u6b47\u6027\u9700\u6c42\u9884\u6d4b\u9762\u4e34\u7a00\u758f\u89c2\u6d4b\u3001\u51b7\u542f\u52a8\u9879\u76ee\u548c\u8fc7\u65f6\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u7f3a\u4e4f\u751f\u6210\u57fa\u7840\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u6570\u636e\u96c6\u4e14\u53ef\u89e3\u91ca\u6027\u5dee\u3002", "method": "\u5f15\u5165TSB-HB\uff0c\u4f7f\u7528Beta-Binomial\u5206\u5e03\u5efa\u6a21\u9700\u6c42\u53d1\u751f\uff0cLog-Normal\u5206\u5e03\u5efa\u6a21\u975e\u96f6\u9700\u6c42\u5927\u5c0f\uff0c\u901a\u8fc7\u5206\u5c42\u5148\u9a8c\u5b9e\u73b0\u8de8\u9879\u76ee\u7684\u90e8\u5206\u6c60\u5316\u3002", "result": "\u5728UCI Online Retail\u6570\u636e\u96c6\u4e0a\uff0cTSB-HB\u7684RMSE\u548cRMSSE\u4f4e\u4e8eCroston\u3001SBA\u3001TSB\u7b49\u65b9\u6cd5\uff1b\u5728M5\u6570\u636e\u96c6\u5b50\u96c6\u4e0a\u4f18\u4e8e\u6240\u6709\u8bc4\u4f30\u7684\u7ecf\u5178\u57fa\u7ebf\u3002", "conclusion": "TSB-HB\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u516c\u5f0f\u548c\u5206\u5c42\u6536\u7f29\uff0c\u63d0\u4f9b\u6821\u51c6\u7684\u6982\u7387\u9884\u6d4b\uff0c\u5728\u95f4\u6b47\u6027\u548c\u5757\u72b6\u9879\u76ee\u4e0a\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.12175", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12175", "abs": "https://arxiv.org/abs/2511.12175", "authors": ["Koushik Ahmed Kushal", "Florimond Gueniat"], "title": "AI-Enhanced IoT Systems for Predictive Maintenance and Affordability Optimization in Smart Microgrids: A Digital Twin Approach", "comment": "12 pages, 6 figures, includes simulation and evaluation results", "summary": "This study presents an AI enhanced IoT framework for predictive maintenance and affordability optimization in smart microgrids using a Digital Twin modeling approach. The proposed system integrates real time sensor data, machine learning based fault prediction, and cost aware operational analytics to improve reliability and energy efficiency in distributed microgrid environments. By synchronizing physical microgrid components with a virtual Digital Twin, the framework enables early detection of component degradation, dynamic load management, and optimized maintenance scheduling. Experimental evaluations demonstrate improved predictive accuracy, reduced operational downtime, and measurable cost savings compared to baseline microgrid management methods. The findings highlight the potential of Digital Twin driven IoT architectures as a scalable solution for next generation intelligent and affordable energy systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684AI\u589e\u5f3a\u7269\u8054\u7f51\u6846\u67b6\uff0c\u7528\u4e8e\u667a\u80fd\u5fae\u7535\u7f51\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u6210\u672c\u4f18\u5316\uff0c\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u96c6\u6210\u548c\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u53ef\u9760\u6027\u63d0\u5347\u548c\u80fd\u6e90\u6548\u7387\u6539\u5584\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u5fae\u7535\u7f51\u73af\u5883\u4e2d\u7ec4\u4ef6\u6545\u969c\u9884\u6d4b\u3001\u7ef4\u62a4\u6210\u672c\u63a7\u5236\u548c\u8fd0\u884c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u80fd\u6e90\u7cfb\u7edf\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6570\u5b57\u5b6a\u751f\u5efa\u6a21\u65b9\u6cd5\uff0c\u96c6\u6210\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u3001\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6545\u969c\u9884\u6d4b\u548c\u6210\u672c\u611f\u77e5\u64cd\u4f5c\u5206\u6790\uff0c\u5b9e\u73b0\u7269\u7406\u5fae\u7535\u7f51\u7ec4\u4ef6\u4e0e\u865a\u62df\u6570\u5b57\u5b6a\u751f\u7684\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\uff0c\u8fd0\u884c\u505c\u673a\u65f6\u95f4\u51cf\u5c11\uff0c\u5e76\u5b9e\u73b0\u53ef\u91cf\u5316\u7684\u6210\u672c\u8282\u7ea6\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u9a71\u52a8\u7684\u7269\u8054\u7f51\u67b6\u6784\u5177\u6709\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u7ecf\u6d4e\u80fd\u6e90\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.11713", "categories": ["cs.CY", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11713", "abs": "https://arxiv.org/abs/2511.11713", "authors": ["Yunkai Yu", "Yingying Wang", "Rong Zheng"], "title": "Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets", "comment": "8 pages,4 figures, to be published in IEEE AIOT 2025", "summary": "The Internet of Things (IoT) sensors have been widely employed to capture human locomotions to enable applications such as activity recognition, human pose estimation, and fall detection. Motion capture (MoCap) systems are frequently used to generate ground truth annotations for human poses when training models with data from wearable or ambient sensors, and have been shown to be effective to synthesize data in these modalities. However, the representation of older adults, an increasingly important demographic in healthcare, in existing MoCap locomotion datasets has not been thoroughly examined. This work surveyed 41 publicly available datasets, identifying eight that include older adult motions and four that contain motions performed by younger actors annotated as old style. Older adults represent a small portion of participants overall, and few datasets provide full-body motion data for this group. To assess the fidelity of old-style walking motions, quantitative metrics are introduced, defining high fidelity as the ability to capture age-related differences relative to normative walking. Using gait parameters that are age-sensitive, robust to noise, and resilient to data scarcity, we found that old-style walking motions often exhibit overly controlled patterns and fail to faithfully characterize aging. These findings highlight the need for improved representation of older adults in motion datasets and establish a method to quantitatively evaluate the quality of old-style walking motions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e8641\u4e2a\u516c\u5f00\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u8001\u5e74\u4eba\u53c2\u4e0e\u5ea6\u4f4e\uff0c\u4e14\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u672a\u80fd\u771f\u5b9e\u53cd\u6620\u8870\u8001\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u91cf\u5316\u8bc4\u4f30\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u4fdd\u771f\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\u4e2d\u8001\u5e74\u4eba\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u4e14\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u7684\u4fdd\u771f\u5ea6\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\uff0c\u8fd9\u5728\u533b\u7597\u5065\u5eb7\u5e94\u7528\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u8c03\u67e541\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u5305\u542b\u8001\u5e74\u4eba\u548c\u8001\u5e74\u98ce\u683c\u52a8\u4f5c\u7684\u6570\u636e\u96c6\uff1b\u5f15\u5165\u91cf\u5316\u6307\u6807\u8bc4\u4f30\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u7684\u4fdd\u771f\u5ea6\uff0c\u4f7f\u7528\u5bf9\u5e74\u9f84\u654f\u611f\u3001\u6297\u566a\u58f0\u4e14\u9002\u5e94\u6570\u636e\u7a00\u7f3a\u7684\u6b65\u6001\u53c2\u6570\u3002", "result": "\u8001\u5e74\u4eba\u4ec5\u5360\u603b\u4f53\u53c2\u4e0e\u8005\u7684\u5c0f\u90e8\u5206\uff0c\u63d0\u4f9b\u8001\u5e74\u4eba\u5168\u8eab\u8fd0\u52a8\u6570\u636e\u7684\u6570\u636e\u96c6\u5f88\u5c11\uff1b\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u5e38\u8868\u73b0\u51fa\u8fc7\u5ea6\u63a7\u5236\u7684\u6a21\u5f0f\uff0c\u672a\u80fd\u771f\u5b9e\u8868\u5f81\u8870\u8001\u7279\u5f81\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u8fd0\u52a8\u6570\u636e\u96c6\u4e2d\u8001\u5e74\u4eba\u7684\u4ee3\u8868\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u91cf\u5316\u8bc4\u4f30\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u8d28\u91cf\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.12112", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12112", "abs": "https://arxiv.org/abs/2511.12112", "authors": ["Neda Bagheri Renani", "Maryam Jaefarzadeh", "Daniel Sevcovic"], "title": "Efficiency and Convergence Insights in Large-Scale Optimization Using the Improved Inexact-Newton-Smart Algorithm and Interior-Point Framework", "comment": null, "summary": "We present a head-to-head evaluation of the Improved Inexact--Newton--Smart (INS) algorithm against a primal--dual interior-point framework for large-scale nonlinear optimization. On extensive synthetic benchmarks, the interior-point method converges with roughly one third fewer iterations and about one half the computation time relative to INS, while attaining marginally higher accuracy and meeting all primary stopping conditions. By contrast, INS succeeds in fewer cases under default settings but benefits markedly from moderate regularization and step-length control; in tuned regimes its iteration count and runtime decrease substantially, narrowing yet not closing the gap. A sensitivity study indicates that interior-point performance remains stable across parameter changes, whereas INS is more affected by step length and regularization choice. Collectively, the evidence positions the interior-point method as a reliable baseline and INS as a configurable alternative when problem structure favors adaptive regularization.", "AI": {"tldr": "\u5bf9\u6539\u8fdb\u7684Inexact-Newton-Smart\u7b97\u6cd5\u4e0e\u539f\u59cb-\u5bf9\u5076\u5185\u70b9\u6cd5\u8fdb\u884c\u5934\u5bf9\u5934\u8bc4\u4f30\uff0c\u5185\u70b9\u6cd5\u5728\u8fed\u4ee3\u6b21\u6570\u548c\u8ba1\u7b97\u65f6\u95f4\u4e0a\u8868\u73b0\u66f4\u4f18\uff0cINS\u5728\u8c03\u6574\u53c2\u6570\u540e\u6027\u80fd\u6709\u6240\u6539\u5584\u4f46\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30\u4e24\u79cd\u5927\u89c4\u6a21\u975e\u7ebf\u6027\u4f18\u5316\u7b97\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7b97\u6cd5\u9009\u62e9\u4f9d\u636e\u3002", "method": "\u5728\u5e7f\u6cdb\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4\u8f83INS\u7b97\u6cd5\u4e0e\u539f\u59cb-\u5bf9\u5076\u5185\u70b9\u6846\u67b6\uff0c\u5206\u6790\u8fed\u4ee3\u6b21\u6570\u3001\u8ba1\u7b97\u65f6\u95f4\u3001\u7cbe\u5ea6\u548c\u6536\u655b\u6027\u3002", "result": "\u5185\u70b9\u6cd5\u6536\u655b\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\u7ea6\u4e09\u5206\u4e4b\u4e00\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u7ea6\u4e00\u534a\uff0c\u7cbe\u5ea6\u7565\u9ad8\u4e14\u6ee1\u8db3\u6240\u6709\u4e3b\u8981\u505c\u6b62\u6761\u4ef6\uff1bINS\u5728\u9ed8\u8ba4\u8bbe\u7f6e\u4e0b\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u4f46\u901a\u8fc7\u6b63\u5219\u5316\u548c\u6b65\u957f\u63a7\u5236\u53ef\u663e\u8457\u6539\u5584\u6027\u80fd\u3002", "conclusion": "\u5185\u70b9\u6cd5\u4f5c\u4e3a\u53ef\u9760\u57fa\u51c6\u7b97\u6cd5\uff0cINS\u4f5c\u4e3a\u53ef\u914d\u7f6e\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u95ee\u9898\u7ed3\u6784\u9002\u5408\u81ea\u9002\u5e94\u6b63\u5219\u5316\u7684\u573a\u666f\u3002"}}
{"id": "2511.11867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11867", "abs": "https://arxiv.org/abs/2511.11867", "authors": ["Namu Park", "Giridhar Kaushik Ramachandran", "Kevin Lybarger", "Fei Xia", "Ozlem Uzuner", "Meliha Yetisgen", "Martin Gunn"], "title": "Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches", "comment": "Submitted to LREC 2026", "summary": "Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b6,393\u4efd\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u968f\u8bbf\u4f9d\u4ece\u6027\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u751f\u6210\u5f0fLLMs\uff0c\u53d1\u73b0GPT-4o\u5728\u4f18\u5316\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6807\u6ce8\u8005\u6c34\u5e73\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u4efb\u52a1\u6027\u80fd\u7684\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u9700\u8981\u6784\u5efa\u9ad8\u8d28\u91cf\u6807\u6ce8\u8bed\u6599\u5e93\u6765\u652f\u6301\u968f\u8bbf\u4f9d\u4ece\u6027\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efa\u4e866,393\u4efd\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff08\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\u3001Longformer\uff09\u548c\u751f\u6210\u5f0fLLMs\uff08GPT-4o\u3001GPT-OSS-20B\uff09\u3002\u5bf9\u751f\u6210\u5f0fLLMs\u6d4b\u8bd5\u4e86\u57fa\u7840\u914d\u7f6e\u548c\u4efb\u52a1\u4f18\u5316\u914d\u7f6e\uff0c\u540e\u8005\u805a\u7126\u4e8e\u5143\u6570\u636e\u3001\u63a8\u8350\u8bed\u53e5\u53ca\u5176\u4e0a\u4e0b\u6587\u3002", "result": "GPT-4o\uff08\u4f18\u5316\u914d\u7f6e\uff09\u8868\u73b0\u6700\u4f73\uff08F1=0.832\uff09\uff0cGPT-OSS-20B\uff08\u4f18\u5316\u914d\u7f6e\uff09\u7d27\u968f\u5176\u540e\uff08F1=0.828\uff09\u3002\u903b\u8f91\u56de\u5f52\u548c\u652f\u6301\u5411\u91cf\u673a\u4e5f\u8868\u73b0\u5f3a\u52b2\uff08F1=0.776\u548c0.775\uff09\u3002\u4eba\u7c7b\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u5f88\u9ad8\uff08F1=0.846\uff09\u3002", "conclusion": "\u867d\u7136\u901a\u8fc7\u63d0\u793a\u4f18\u5316\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u4f46\u53ef\u89e3\u91ca\u4e14\u8d44\u6e90\u6548\u7387\u9ad8\u7684\u4f20\u7edf\u6a21\u578b\u4ecd\u7136\u662f\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002LLMs\u5728\u653e\u5c04\u5b66\u968f\u8bbf\u4f9d\u4ece\u6027\u68c0\u6d4b\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2511.11583", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11583", "abs": "https://arxiv.org/abs/2511.11583", "authors": ["Fernando Spadea", "Oshani Seneviratne"], "title": "Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations", "comment": "10 pages, 3 figures, RAGE-KG 2025", "summary": "Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.", "AI": {"tldr": "RAG-FLARKO\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u9636\u6bb5\u77e5\u8bc6\u56fe\u8c31\u5904\u7406\uff0c\u89e3\u51b3\u4e86LLM\u5728\u91d1\u878d\u63a8\u8350\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u3001\u5e7b\u89c9\u95ee\u9898\u548c\u884c\u4e3a\u57fa\u7840\u7f3a\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u91d1\u878d\u63a8\u8350\u4e2d\u5b58\u5728\u4e0a\u4e0b\u6587\u9650\u5236\u3001\u5e7b\u89c9\u95ee\u9898\u4ee5\u53ca\u7f3a\u4e4f\u884c\u4e3a\u57fa\u7840\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6574\u5408\u7528\u6237\u884c\u4e3a\u548c\u5e02\u573a\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u9636\u6bb5\u5e76\u884c\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\uff1a\u9996\u5148\u4ece\u7528\u6237\u4ea4\u6613\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u884c\u4e3a\u76f8\u5173\u5b9e\u4f53\uff0c\u7136\u540e\u5229\u7528\u8be5\u4e0a\u4e0b\u6587\u4ece\u5e02\u573a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7b5b\u9009\u65f6\u95f4\u4e00\u81f4\u4fe1\u53f7\uff0c\u6784\u5efa\u7d27\u51d1\u7684\u63a5\u5730\u5b50\u56fe\u4f9bLLM\u4f7f\u7528\u3002", "result": "\u5728\u771f\u5b9e\u91d1\u878d\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cRAG-FLARKO\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\uff0c\u4f7f\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u5728\u76c8\u5229\u6027\u548c\u884c\u4e3a\u5bf9\u9f50\u65b9\u9762\u90fd\u80fd\u8fbe\u5230\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u63a5\u5730\u91d1\u878dAI\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u901a\u8fc7\u51cf\u5c11\u4e0a\u4e0b\u6587\u5f00\u9500\u548c\u805a\u7126\u76f8\u5173\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u4e2a\u6027\u5316\u91d1\u878d\u63a8\u8350\u3002"}}
{"id": "2511.11752", "categories": ["cs.AI", "cs.DL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.11752", "abs": "https://arxiv.org/abs/2511.11752", "authors": ["S\u00f6ren Arlt", "Xuemei Gu", "Mario Krenn"], "title": "Towards autonomous quantum physics research using LLM agents with access to intelligent tools", "comment": "24 pages, 5 figures", "summary": "Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.", "AI": {"tldr": "AI-Mandel\u662f\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u5e76\u5b9e\u65bd\u91cf\u5b50\u7269\u7406\u5b66\u60f3\u6cd5\u7684LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b83\u80fd\u591f\u4ece\u6587\u732e\u4e2d\u63d0\u53d6\u60f3\u6cd5\u5e76\u4f7f\u7528\u9886\u57df\u7279\u5b9aAI\u5de5\u5177\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u7acb\u5373\u5728\u5b9e\u9a8c\u5ba4\u5b9e\u65bd\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524dAI\u5728\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u4ecd\u4e3b\u8981\u4f9d\u8d56\u4eba\u7c7b\u63d0\u4f9b\u7814\u7a76\u95ee\u9898\u548c\u76ee\u6807\uff0cAI\u751f\u6210\u7684\u521b\u610f\u5f80\u5f80\u6a21\u7cca\u4e14\u9700\u8981\u4eba\u5de5\u6267\u884c\u3002\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u5316\u751f\u6210\u548c\u5b9e\u65bd\u60f3\u6cd5\u7684\u7cfb\u7edf\u5c06\u663e\u8457\u6539\u53d8\u4eba\u7c7b\u5728\u79d1\u5b66\u8fc7\u7a0b\u4e2d\u7684\u89d2\u8272\u3002", "method": "AI-Mandel\u4f7f\u7528LLM\u4ece\u6587\u732e\u4e2d\u63d0\u53d6\u60f3\u6cd5\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684AI\u5de5\u5177\u5c06\u8fd9\u4e9b\u60f3\u6cd5\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6848\u3002", "result": "AI-Mandel\u751f\u6210\u7684\u8bb8\u591a\u60f3\u6cd5\u5177\u6709\u79d1\u5b66\u4ef7\u503c\uff0c\u5176\u4e2d\u4e24\u4e2a\u60f3\u6cd5\u5df2\u7ecf\u4fc3\u6210\u4e86\u72ec\u7acb\u7684\u540e\u7eed\u79d1\u5b66\u8bba\u6587\u3002\u8fd9\u4e9b\u60f3\u6cd5\u5305\u62ec\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u7684\u65b0\u53d8\u4f53\u3001\u4e0d\u5b9a\u56e0\u679c\u987a\u5e8f\u4e2d\u7684\u91cf\u5b50\u7f51\u7edc\u539f\u8bed\uff0c\u4ee5\u53ca\u57fa\u4e8e\u91cf\u5b50\u4fe1\u606f\u4f20\u8f93\u95ed\u5408\u56de\u8def\u7684\u65b0\u51e0\u4f55\u76f8\u4f4d\u6982\u5ff5\u3002", "conclusion": "AI-Mandel\u662f\u80fd\u591f\u751f\u6210\u548c\u5b9e\u65bd\u5177\u4f53\u53ef\u884c\u60f3\u6cd5\u7684AI\u7269\u7406\u5b66\u5bb6\u539f\u578b\u7cfb\u7edf\u3002\u6784\u5efa\u6b64\u7c7b\u7cfb\u7edf\u4e0d\u4ec5\u6709\u52a9\u4e8e\u52a0\u901f\u79d1\u5b66\u53d1\u5c55\uff0c\u8fd8\u63ed\u793a\u4e86\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u4eba\u5de5\u667a\u80fd\u79d1\u5b66\u5bb6\u6240\u9762\u4e34\u7684\u5177\u4f53\u6311\u6218\u3002"}}
{"id": "2511.12783", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12783", "abs": "https://arxiv.org/abs/2511.12783", "authors": ["Jingru Huang", "Haijie Xu", "Manrui Jiang", "Chen Zhang"], "title": "Function-on-Function Bayesian Optimization", "comment": "13 pages, 4 figures, conference", "summary": "Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51fd\u6570\u5bf9\u51fd\u6570\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u8f93\u5165\u548c\u8f93\u51fa\u90fd\u662f\u51fd\u6570\u7684\u590d\u6742\u7cfb\u7edf\u95ee\u9898", "motivation": "\u73b0\u6709\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u8f93\u5165\u548c\u8f93\u51fa\u90fd\u662f\u51fd\u6570\u7684\u95ee\u9898\uff0c\u8fd9\u5728\u5148\u8fdb\u4f20\u611f\u6280\u672f\u7684\u590d\u6742\u7cfb\u7edf\u4e2d\u65e5\u76ca\u5e38\u89c1", "method": "\u5f15\u5165\u51fd\u6570\u5bf9\u51fd\u6570\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\uff0c\u4f7f\u7528\u53ef\u5206\u79bb\u7b97\u5b50\u503c\u6838\uff1b\u57fa\u4e8e\u6b64\u5b9a\u4e49\u6807\u91cf\u4e0a\u7f6e\u4fe1\u754c\u83b7\u53d6\u51fd\u6570\uff0c\u5e76\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u51fd\u6570\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFFBO\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "FFBO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u51fd\u6570\u5bf9\u51fd\u6570\u4f18\u5316\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2511.12277", "categories": ["eess.SY", "cs.DL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12277", "abs": "https://arxiv.org/abs/2511.12277", "authors": ["Dmytro Valiaiev"], "title": "DataOps-driven CI/CD for analytics repositories", "comment": null, "summary": "The proliferation of SQL for data processing has often occurred without the rigor of traditional software development, leading to siloed efforts, logic replication, and increased risk. This ad-hoc approach hampers data governance and makes validation nearly impossible. Organizations are adopting DataOps, a methodology combining Agile, Lean, and DevOps principles to address these challenges to treat analytics pipelines as production systems. However, a standardized framework for implementing DataOps is lacking. This perspective proposes a qualitative design for a DataOps-aligned validation framework. It introduces a DataOps Controls Scorecard, derived from a multivocal literature review, which distills key concepts into twelve testable controls. These controls are then mapped to a modular, extensible CI/CD pipeline framework designed to govern a single source of truth (SOT) SQL repository. The framework consists of five stages: Lint, Optimize, Parse, Validate, and Observe, each containing specific, automated checks. A Requirements Traceability Matrix (RTM) demonstrates how each high-level control is enforced by concrete pipeline checks, ensuring qualitative completeness. This approach provides a structured mechanism for enhancing data quality, governance, and collaboration, allowing teams to scale analytics development with transparency and control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eDataOps\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5341\u4e8c\u4e2a\u53ef\u6d4b\u8bd5\u7684\u63a7\u5236\u9879\u548c\u4e94\u9636\u6bb5CI/CD\u7ba1\u9053\u6765\u6807\u51c6\u5316SQL\u6570\u636e\u5206\u6790\u6d41\u7a0b\uff0c\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u548c\u6cbb\u7406\u80fd\u529b\u3002", "motivation": "SQL\u6570\u636e\u5904\u7406\u7f3a\u4e4f\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e25\u8c28\u6027\uff0c\u5bfc\u81f4\u5b64\u5c9b\u5316\u3001\u903b\u8f91\u91cd\u590d\u548c\u98ce\u9669\u589e\u52a0\uff0c\u963b\u788d\u6570\u636e\u6cbb\u7406\u548c\u9a8c\u8bc1\u3002DataOps\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u5b9e\u65bd\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u591a\u6e90\u6587\u732e\u7efc\u8ff0\u5f00\u53d1DataOps\u63a7\u5236\u8bb0\u5206\u5361\uff0c\u5305\u542b12\u4e2a\u53ef\u6d4b\u8bd5\u63a7\u5236\u9879\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u4e94\u9636\u6bb5CI/CD\u7ba1\u9053\u6846\u67b6\uff1aLint\u3001Optimize\u3001Parse\u3001Validate\u3001Observe\uff0c\u6bcf\u4e2a\u9636\u6bb5\u5305\u542b\u81ea\u52a8\u5316\u68c0\u67e5\u3002", "result": "\u521b\u5efa\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u9700\u6c42\u53ef\u8ffd\u6eaf\u6027\u77e9\u9635\u786e\u4fdd\u9ad8\u5c42\u63a7\u5236\u4e0e\u5177\u4f53\u7ba1\u9053\u68c0\u67e5\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u673a\u5236\u6765\u589e\u5f3a\u6570\u636e\u8d28\u91cf\u3001\u6cbb\u7406\u548c\u534f\u4f5c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u56e2\u961f\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u63a7\u7684\u5206\u6790\u5f00\u53d1\u6269\u5c55\u80fd\u529b\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u9a8c\u8bc1\u6d41\u7a0b\u89e3\u51b3\u4e86DataOps\u5b9e\u65bd\u4e2d\u7684\u6807\u51c6\u5316\u7f3a\u5931\u95ee\u9898\u3002"}}
{"id": "2511.11715", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11715", "abs": "https://arxiv.org/abs/2511.11715", "authors": ["Yunfei Shen", "Zhongcheng Wu"], "title": "CADD: A Chinese Traffic Accident Dataset for Statute-Based Liability Attribution", "comment": null, "summary": "As autonomous driving technology advances, the critical challenge evolves beyond collision avoidance to the \\textbf{adjudication of liability} when accidents occur. Existing datasets, focused on detection and localization, lack the annotations required for this legal reasoning. To bridge this gap, we introduce the \\textbf{C}hinese \\textbf{A}ccident \\textbf{D}uty-determination \\textbf{D}ataset (\\textbf{CADD}), the first benchmark for statute-based liability attribution. CADD contains 792 real-world driving recorder videos, each annotated within a novel \\textbf{``Behavior--Liability--Statute''} pipeline. This framework provides \\textbf{granular, symmetric behavior annotations}, clear responsibility assignments, and, uniquely, links each case to the specific \\textbf{Chinese traffic law statute} violated. We demonstrate the utility of CADD through detailed analysis and establish benchmarks for liability prediction and explainable decision-making. By directly connecting perceptual data to legal consequences, CADD provides a foundational resource for developing accountable and legally-grounded autonomous systems.", "AI": {"tldr": "CADD\u662f\u4e2d\u56fd\u9996\u4e2a\u57fa\u4e8e\u6cd5\u89c4\u7684\u8d23\u4efb\u8ba4\u5b9a\u6570\u636e\u96c6\uff0c\u5305\u542b792\u4e2a\u771f\u5b9e\u9a7e\u9a76\u8bb0\u5f55\u4eea\u89c6\u9891\uff0c\u901a\u8fc7\"\u884c\u4e3a-\u8d23\u4efb-\u6cd5\u89c4\"\u6846\u67b6\u6807\u6ce8\uff0c\u5c06\u611f\u77e5\u6570\u636e\u4e0e\u6cd5\u5f8b\u540e\u679c\u76f4\u63a5\u5173\u8054\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u7f3a\u4e4f\u6cd5\u5f8b\u63a8\u7406\u6240\u9700\u7684\u6ce8\u91ca\u3002\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u53d1\u5c55\uff0c\u4e8b\u6545\u8d23\u4efb\u8ba4\u5b9a\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b792\u4e2a\u771f\u5b9e\u9a7e\u9a76\u8bb0\u5f55\u4eea\u89c6\u9891\u7684CADD\u6570\u636e\u96c6\uff0c\u91c7\u7528\u65b0\u9896\u7684\"\u884c\u4e3a-\u8d23\u4efb-\u6cd5\u89c4\"\u6807\u6ce8\u6d41\u7a0b\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5bf9\u79f0\u884c\u4e3a\u6ce8\u91ca\u3001\u660e\u786e\u8d23\u4efb\u5206\u914d\uff0c\u5e76\u5c06\u6bcf\u4e2a\u6848\u4f8b\u4e0e\u8fdd\u53cd\u7684\u4e2d\u56fd\u4ea4\u901a\u6cd5\u89c4\u6761\u6b3e\u5173\u8054\u3002", "result": "\u5efa\u7acb\u4e86\u8d23\u4efb\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u8be6\u7ec6\u5206\u6790\u8bc1\u660e\u4e86CADD\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "CADD\u901a\u8fc7\u76f4\u63a5\u8fde\u63a5\u611f\u77e5\u6570\u636e\u4e0e\u6cd5\u5f8b\u540e\u679c\uff0c\u4e3a\u5f00\u53d1\u8d1f\u8d23\u4efb\u4e14\u6cd5\u5f8b\u4f9d\u636e\u5145\u5206\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u3002"}}
{"id": "2511.12145", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12145", "abs": "https://arxiv.org/abs/2511.12145", "authors": ["Elias Niep\u00f6tter", "Adrian Grimm", "Torbj\u00f8rn Cunis"], "title": "Novel Multi-objective Switched Model Predictive Control with Feasibility and Stability Guarantees", "comment": "7 pages, 4 figures, submitted to the 24th European Control Conference (ECC26)", "summary": "As the relevance of control systems capable of dealing with multiple objectives rises (e.g. being economic while maintaining a certain performance), multi-objective Switched Model Predictive Control combines all the advantages of Model Predictive Control while dealing with multiple objectives. We propose two novel frameworks, a nominal and a robust framework to guarantee recursive feasibility of each Model Predictive Controller under arbitrary switching and assure asymptotic stability of the closed-loop system applying the nominal framework and Input-to-State stability using the robust framework. The presented frameworks employ methods from switched systems, enabling the utilization of a supervisor control instance which allows for complex objectives and multi-objective control. Our numerical example confirms the superior performance of our proposed frameworks compared to a standard Model Predictive Control approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u591a\u76ee\u6807\u5207\u6362\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff08\u540d\u4e49\u548c\u9c81\u68d2\u6846\u67b6\uff09\uff0c\u4fdd\u8bc1\u9012\u5f52\u53ef\u884c\u6027\u548c\u95ed\u73af\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u5728\u6570\u503c\u793a\u4f8b\u4e2d\u4f18\u4e8e\u6807\u51c6MPC\u65b9\u6cd5", "motivation": "\u968f\u7740\u9700\u8981\u5904\u7406\u591a\u76ee\u6807\uff08\u5982\u7ecf\u6d4e\u6027\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff09\u7684\u63a7\u5236\u7cfb\u7edf\u91cd\u8981\u6027\u4e0a\u5347\uff0c\u9700\u8981\u7ed3\u5408MPC\u4f18\u52bf\u5e76\u5904\u7406\u591a\u76ee\u6807\u7684\u65b9\u6cd5", "method": "\u4f7f\u7528\u5207\u6362\u7cfb\u7edf\u65b9\u6cd5\uff0c\u63d0\u51fa\u540d\u4e49\u548c\u9c81\u68d2\u4e24\u79cd\u6846\u67b6\uff0c\u91c7\u7528\u76d1\u7763\u63a7\u5236\u5b9e\u4f8b\u5b9e\u73b0\u590d\u6742\u76ee\u6807\u548c\u591a\u76ee\u6807\u63a7\u5236", "result": "\u4fdd\u8bc1\u6bcf\u4e2aMPC\u63a7\u5236\u5668\u5728\u4efb\u610f\u5207\u6362\u4e0b\u7684\u9012\u5f52\u53ef\u884c\u6027\uff0c\u540d\u4e49\u6846\u67b6\u786e\u4fdd\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u9c81\u68d2\u6846\u67b6\u786e\u4fdd\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027", "conclusion": "\u63d0\u51fa\u7684\u591a\u76ee\u6807\u5207\u6362MPC\u6846\u67b6\u5728\u6570\u503c\u793a\u4f8b\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u6807\u51c6MPC\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u76ee\u6807\u63a7\u5236\u95ee\u9898"}}
{"id": "2511.11878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11878", "abs": "https://arxiv.org/abs/2511.11878", "authors": ["Fernanda Bufon F\u00e4rber", "Iago Alves Brito", "Julia Soares Dollis", "Pedro Schindler Freire Brasil Ribeiro", "Rafael Teixeira Sousa", "Arlindo Rodrigues Galv\u00e3o Filho"], "title": "MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers", "comment": "11 pages, 3 tables, 2 figures", "summary": "While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.", "AI": {"tldr": "MedPT\u662f\u9996\u4e2a\u9488\u5bf9\u5df4\u897f\u8461\u8404\u7259\u8bed\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u533b\u60a3\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u5305\u542b38.4\u4e07\u6761\u95ee\u7b54\u5bf9\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u548cLLM\u6807\u6ce8\u589e\u5f3a\uff0c\u5728\u533b\u7597\u4e13\u79d1\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523094%\u7684F1\u5206\u6570\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dLLM\u5728\u533b\u7597\u9886\u57df\u4e3b\u8981\u5173\u6ce8\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u95ee\u9898\uff0c\u586b\u8865\u8461\u8404\u7259\u8bed\u533b\u7597\u6570\u636e\u7684\u7a7a\u767d\uff0c\u56e0\u4e3a\u7b80\u5355\u7ffb\u8bd1\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u548c\u6587\u5316\u7279\u5f02\u6027\uff08\u5982\u5730\u65b9\u6027\u75be\u75c5\uff09\u3002", "method": "\u6784\u5efa38.4\u4e07\u6761\u771f\u5b9e\u533b\u60a3\u95ee\u7b54\u5bf9\uff0c\u91c7\u7528\u6df7\u5408\u5b9a\u91cf-\u5b9a\u6027\u5206\u6790\u8fdb\u884c\u566a\u58f0\u8fc7\u6ee4\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u6807\u6ce8\u5c06\u95ee\u9898\u5206\u7c7b\u4e3a7\u79cd\u8bed\u4e49\u7c7b\u578b\uff0c\u5e76\u5206\u6790\u5176\u4e3b\u9898\u5e7f\u5ea6\u548c\u8bed\u8a00\u7279\u6027\u3002", "result": "\u6570\u636e\u96c6\u6db5\u76d63,200\u4e2a\u4e3b\u9898\uff0c\u5728\u533b\u7597\u4e13\u79d1\u8def\u7531\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c031.7B\u53c2\u6570\u6a21\u578b\u572820\u7c7b\u5206\u7c7b\u4e0a\u8fbe\u523094%\u7684F1\u5206\u6570\uff0c\u9519\u8bef\u5206\u6790\u663e\u793a\u8bef\u5206\u7c7b\u53cd\u6620\u4e86\u771f\u5b9e\u7684\u4e34\u5e8a\u6a21\u7cca\u6027\u3002", "conclusion": "MedPT\u6570\u636e\u96c6\u516c\u5f00\u91ca\u653e\uff0c\u65e8\u5728\u4fc3\u8fdb\u8461\u8404\u7259\u8bed\u4e16\u754c\u66f4\u516c\u5e73\u3001\u51c6\u786e\u548c\u6587\u5316\u654f\u611f\u7684\u533b\u7597\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2511.11584", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11584", "abs": "https://arxiv.org/abs/2511.11584", "authors": ["Jacob Drori", "Luke Marks", "Bryce Woodworth", "Alex Cloud", "Alexander Matt Turner"], "title": "Output Supervision Can Obfuscate the Chain of Thought", "comment": null, "summary": "OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.", "AI": {"tldr": "\u8bad\u7ec3\u6a21\u578b\u4ec5\u4f7f\u7528\u8f93\u51fa\u76d1\u63a7\u5668\uff08\u65e0\u6cd5\u8bbf\u95ee\u601d\u7ef4\u94fe\uff09\u4ecd\u4f1a\u5bfc\u81f4\u9690\u853d\u7684\u601d\u7ef4\u94fe\uff0c\u901a\u8fc7\u4e24\u79cd\u673a\u5236\uff1a\u6a21\u578b\u6cdb\u5316\u4f7f\u601d\u7ef4\u94fe\u770b\u8d77\u6765\u5b89\u5168\uff0c\u4ee5\u53ca\u5b89\u5168\u5916\u89c2\u7684\u601d\u7ef4\u94fe\u589e\u52a0\u5b89\u5168\u8f93\u51fa\u7684\u53ef\u80fd\u6027\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u7f13\u89e3\u65b9\u6cd5\uff0c\u5728\u53ef\u76d1\u63a7\u6027\u548c\u4efb\u52a1\u6027\u80fd\u4e0a\u5b9e\u73b0\u5e15\u7d2f\u6258\u6539\u8fdb\u3002", "motivation": "OpenAI\u7684\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9\u601d\u7ef4\u94fe\u76d1\u63a7\u5668\u7684\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u9690\u853d\u7684\u601d\u7ef4\u94fe\uff0c\u5176\u4e2d\u5305\u542b\u76d1\u63a7\u5668\u65e0\u6cd5\u68c0\u6d4b\u7684\u4e0d\u826f\u884c\u4e3a\u3002\u4ed6\u4eec\u5efa\u8bae\u4ec5\u4f7f\u7528\u8f93\u51fa\u76d1\u63a7\u5668\u8fdb\u884c\u8bad\u7ec3\u6765\u4fdd\u6301\u601d\u7ef4\u94fe\u7684\u53ef\u76d1\u63a7\u6027\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u79cd\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u7136\u4f1a\u5bfc\u81f4\u9690\u853d\u7684\u601d\u7ef4\u94fe\u95ee\u9898\u3002", "method": "\u8bc6\u522b\u4e86\u4e24\u79cd\u5bfc\u81f4\u9690\u853d\u601d\u7ef4\u94fe\u7684\u673a\u5236\uff1a1\uff09\u6a21\u578b\u5728\u8bad\u7ec3\u4ea7\u751f\u5b89\u5168\u5916\u89c2\u8f93\u51fa\u65f6\uff0c\u4f1a\u6cdb\u5316\u5230\u4f7f\u601d\u7ef4\u94fe\u770b\u8d77\u6765\u5b89\u5168\uff1b2\uff09\u7531\u4e8e\u540e\u7eedtoken\u4f9d\u8d56\u4e8e\u5148\u524dtoken\uff0c\u5b89\u5168\u5916\u89c2\u7684\u601d\u7ef4\u94fe\u4f1a\u589e\u52a0\u5b89\u5168\u8f93\u51fa\u7684\u53ef\u80fd\u6027\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u76f8\u5e94\u7684\u7f13\u89e3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684\u4e24\u79cd\u7f13\u89e3\u65b9\u6cd5\u5728\u53ef\u76d1\u63a7\u6027\u548c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u76f8\u6bd4\u5e38\u89c4\u8bad\u7ec3\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u5373\u5728\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u601d\u7ef4\u94fe\u7684\u53ef\u76d1\u63a7\u6027\u3002", "conclusion": "\u4ec5\u4f7f\u7528\u8f93\u51fa\u76d1\u63a7\u5668\u8fdb\u884c\u8bad\u7ec3\u4e0d\u8db3\u4ee5\u9632\u6b62\u9690\u853d\u601d\u7ef4\u94fe\u95ee\u9898\uff0c\u9700\u8981\u989d\u5916\u7684\u7f13\u89e3\u63aa\u65bd\u3002\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u76d1\u63a7\u6027\u548c\u6027\u80fd\u5e73\u8861\u3002"}}
{"id": "2511.11770", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11770", "abs": "https://arxiv.org/abs/2511.11770", "authors": ["Floris Vossebeld", "Shenghui Wang"], "title": "Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction", "comment": null, "summary": "Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8ba9\u5c0f\u578bLLM\u5b66\u4e60\u8fed\u4ee3\u6784\u5efaSPARQL\u67e5\u8be2\u7684\u7b56\u7565\uff0c\u5728LC-QuAD 2.0\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u63d0\u5347\u4e8617.5%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u591a\u8df3\u95ee\u9898\u4e2dSPARQL\u67e5\u8be2\u751f\u6210\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u57fa\u4e8e\u5b9e\u65f6\u6267\u884c\u53cd\u9988\u7684\u52a8\u6001\u8c03\u8bd5\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4ec53B\u53c2\u6570\u7684LLM\uff0c\u901a\u8fc7\u7ed3\u679c\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u8bad\u7ec3\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff0c\u5b66\u4e60\u8fed\u4ee3\u6784\u5efa\u548c\u8c03\u8bd5SPARQL\u67e5\u8be2\u7684\u7b56\u7565\u3002", "result": "\u5728LC-QuAD 2.0\u7684\u53ef\u6267\u884c\u5b50\u96c6\u4e0a\u8fbe\u523049.7%\u7684\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u6700\u5f3a\u7684\u8fed\u4ee3\u96f6\u6837\u672c\u57fa\u7ebf\u63d0\u5347\u4e8617.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u901a\u8fc7\u4ea4\u4e92\u6559\u6388\u667a\u80fd\u4f53\u638c\u63e1\u5f62\u5f0f\u5316\u7b26\u53f7\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u84dd\u56fe\uff0c\u5f25\u5408\u4e86\u6982\u7387\u6027LLM\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.12840", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12840", "abs": "https://arxiv.org/abs/2511.12840", "authors": ["Yuta Kondo"], "title": "Benign Overfitting in Linear Classifiers with a Bias Term", "comment": "17 pages", "summary": "Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of \"homogeneous\" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86Hashimoto\u7b49\u4eba\u5173\u4e8e\u7ebf\u6027\u5206\u7c7b\u4e2d\u826f\u6027\u8fc7\u62df\u5408\u7684\u7814\u7a76\uff0c\u4ece\u65e0\u504f\u7f6e\u9879\u7684\u540c\u8d28\u6a21\u578b\u6269\u5c55\u5230\u5305\u542b\u504f\u7f6e\u9879\u7684\u975e\u540c\u8d28\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5728\u8fd9\u79cd\u66f4\u73b0\u5b9e\u7684\u8bbe\u7f6e\u4e0b\u826f\u6027\u8fc7\u62df\u5408\u4f9d\u7136\u5b58\u5728\uff0c\u5e76\u63ed\u793a\u4e86\u504f\u7f6e\u9879\u5bf9\u6cdb\u5316\u6761\u4ef6\u7684\u65b0\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u826f\u6027\u8fc7\u62df\u5408\u7684\u7406\u8bba\u5206\u6790\u4ec5\u9650\u4e8e\u65e0\u504f\u7f6e\u9879\u7684\u540c\u8d28\u6a21\u578b\uff0c\u800c\u5b9e\u8df5\u4e2d\u6a21\u578b\u901a\u5e38\u5305\u542b\u504f\u7f6e\u9879\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5728\u66f4\u73b0\u5b9e\u7684\u5305\u542b\u504f\u7f6e\u9879\u7684\u975e\u540c\u8d28\u6a21\u578b\u4e2d\uff0c\u826f\u6027\u8fc7\u62df\u5408\u73b0\u8c61\u662f\u5426\u4f9d\u7136\u5b58\u5728\uff0c\u4ee5\u53ca\u504f\u7f6e\u9879\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u6269\u5c55Hashimoto\u7b49\u4eba\u7684\u6846\u67b6\uff0c\u7814\u7a76\u5305\u542b\u504f\u7f6e\u9879\u7684\u975e\u540c\u8d28\u7ebf\u6027\u5206\u7c7b\u6a21\u578b\uff0c\u5206\u6790\u6570\u636e\u534f\u65b9\u5dee\u7ed3\u6784\u5bf9\u6cdb\u5316\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u7279\u522b\u5173\u6ce8\u6807\u7b7e\u566a\u58f0\u5b58\u5728\u65f6\u7684\u60c5\u51b5\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u5305\u542b\u504f\u7f6e\u9879\u7684\u975e\u540c\u8d28\u6a21\u578b\u4e2d\u826f\u6027\u8fc7\u62df\u5408\u4f9d\u7136\u5b58\u5728\uff0c\u4f46\u504f\u7f6e\u9879\u7684\u5f15\u5165\u5bf9\u6570\u636e\u534f\u65b9\u5dee\u7ed3\u6784\u63d0\u51fa\u4e86\u65b0\u7684\u6cdb\u5316\u7ea6\u675f\u6761\u4ef6\uff0c\u8fd9\u4e9b\u7ea6\u675f\u5728\u6807\u7b7e\u566a\u58f0\u5b58\u5728\u65f6\u5c24\u4e3a\u660e\u663e\u3002\u5728\u5404\u9879\u540c\u6027\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u65b0\u7ea6\u675f\u88ab\u540c\u8d28\u6a21\u578b\u7ee7\u627f\u7684\u8981\u6c42\u6240\u4e3b\u5bfc\u3002", "conclusion": "\u504f\u7f6e\u9879\u5bf9\u826f\u6027\u8fc7\u62df\u5408\u7684\u6cdb\u5316\u6761\u4ef6\u5177\u6709\u975e\u5e73\u51e1\u5f71\u54cd\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u63d0\u4f9b\u4e86\u66f4\u5b8c\u6574\u7684\u7406\u8bba\u56fe\u666f\uff0c\u63ed\u793a\u4e86\u5728\u66f4\u73b0\u5b9e\u6a21\u578b\u8bbe\u7f6e\u4e0b\u7ef4\u6301\u826f\u597d\u6cdb\u5316\u6027\u80fd\u6240\u9700\u7684\u6570\u636e\u7ed3\u6784\u6761\u4ef6\u3002"}}
{"id": "2511.12329", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12329", "abs": "https://arxiv.org/abs/2511.12329", "authors": ["Arman Pourghorban", "Dipankar Maity"], "title": "Target Defense against Sequentially Arriving Intruders: Algorithm for Agents with Dubins Dynamics", "comment": null, "summary": "We consider a variant of the target defense problem where a single defender is tasked to capture a sequence of incoming intruders. Both the defender and the intruders have non-holonomic dynamics. The intruders' objective is to breach the target perimeter without being captured by the defender, while the defender's goal is to capture as many intruders as possible. After one intruder breaches or is captured, the next appears randomly on a fixed circle surrounding the target. Therefore, the defender's final position in one game becomes its starting position for the next. We divide an intruder-defender engagement into two phases, partial information and full information, depending on the information available to the players. We address the capturability of an intruder by the defender using the notions of Dubins path and guarding arc. We quantify the percentage of capture for both finite and infinite sequences of incoming intruders. Finally, the theoretical results are verified through numerical examples using Monte-Carlo-type random trials of experiments.", "AI": {"tldr": "\u7814\u7a76\u975e\u5b8c\u6574\u52a8\u529b\u5b66\u4e0b\u7684\u76ee\u6807\u9632\u5fa1\u95ee\u9898\uff0c\u5206\u6790\u9632\u5fa1\u8005\u5982\u4f55\u6355\u83b7\u8fde\u7eed\u5165\u4fb5\u8005\u5e8f\u5217\uff0c\u901a\u8fc7Dubins\u8def\u5f84\u548c\u5b88\u62a4\u5f27\u6982\u5ff5\u91cf\u5316\u6355\u83b7\u7387\u3002", "motivation": "\u89e3\u51b3\u5355\u9632\u5fa1\u8005\u9762\u5bf9\u8fde\u7eed\u975e\u5b8c\u6574\u52a8\u529b\u5b66\u5165\u4fb5\u8005\u7684\u76ee\u6807\u9632\u5fa1\u95ee\u9898\uff0c\u5206\u6790\u9632\u5fa1\u8005\u5728\u6709\u9650\u548c\u65e0\u9650\u5165\u4fb5\u5e8f\u5217\u4e2d\u7684\u6355\u83b7\u80fd\u529b\u3002", "method": "\u5c06\u5165\u4fb5\u8005-\u9632\u5fa1\u8005\u4ea4\u6218\u5206\u4e3a\u90e8\u5206\u4fe1\u606f\u548c\u5b8c\u5168\u4fe1\u606f\u4e24\u4e2a\u9636\u6bb5\uff0c\u4f7f\u7528Dubins\u8def\u5f84\u548c\u5b88\u62a4\u5f27\u6982\u5ff5\u5206\u6790\u53ef\u6355\u83b7\u6027\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u91cf\u5316\u4e86\u6709\u9650\u548c\u65e0\u9650\u5165\u4fb5\u5e8f\u5217\u4e2d\u7684\u6355\u83b7\u767e\u5206\u6bd4\uff0c\u7406\u8bba\u7ed3\u679c\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u5efa\u7acb\u4e86\u975e\u5b8c\u6574\u52a8\u529b\u5b66\u4e0b\u8fde\u7eed\u76ee\u6807\u9632\u5fa1\u95ee\u9898\u7684\u5206\u6790\u6846\u67b6\uff0c\u4e3a\u9632\u5fa1\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.11718", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11718", "abs": "https://arxiv.org/abs/2511.11718", "authors": ["Sanjana Cheerla", "Vaibhav Garg", "Saikath Bhattacharya", "Munindar P. Singh"], "title": "Weapons of Online Harassment: Menacing and Profiling Users via Social Apps", "comment": "This article has been accepted for publication in IEEE Computer as a Research Feature. 13 pages, 3 figures, 1 table, 4 examples", "summary": "Viewing social apps as sociotechnical systems makes clear that they are not mere pieces of technology but mediate human interaction and may unintentionally enable harmful behaviors like online harassment. As more users interact through social apps, instances of harassment increase.\n  We observed that app reviews often describe harassment. Accordingly, we built a dataset of over 3 million reviews and 1,800 apps. We discovered that two forms of harassment are prevalent, Menacing and Profiling.\n  We built a computational model for identifying reviews indicating harassment, achieving high recalls of 90% for Menacing and 85% for Profiling. We analyzed the data further to better understand the terrain of harassment. Surprisingly, abusers most often have female identities. Also, what distinguishes negative from neutral reviews is the greater prevalence of anger, disgust, and fear.\n  Applying our model, we identified 1,395 apps enabling harassment and notified developers of the top 48 with the highest user-reported harassment.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790300\u4e07\u6761\u5e94\u7528\u8bc4\u8bba\uff0c\u5f00\u53d1\u4e86\u8bc6\u522b\u9a9a\u6270\u884c\u4e3a\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u53d1\u73b0\u4e24\u79cd\u4e3b\u8981\u9a9a\u6270\u5f62\u5f0f\uff1a\u5a01\u80c1\u548c\u753b\u50cf\uff0c\u5e76\u8bc6\u522b\u51fa1395\u4e2a\u5b58\u5728\u9a9a\u6270\u95ee\u9898\u7684\u5e94\u7528\u3002", "motivation": "\u793e\u4ea4\u5e94\u7528\u4f5c\u4e3a\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u53ef\u80fd\u65e0\u610f\u4e2d\u52a9\u957f\u6709\u5bb3\u884c\u4e3a\u5982\u7f51\u7edc\u9a9a\u6270\uff0c\u968f\u7740\u7528\u6237\u589e\u52a0\uff0c\u9a9a\u6270\u4e8b\u4ef6\u4e5f\u5728\u4e0a\u5347\u3002\u5e94\u7528\u8bc4\u8bba\u4e2d\u7ecf\u5e38\u63cf\u8ff0\u9a9a\u6270\u884c\u4e3a\uff0c\u8fd9\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u57fa\u7840\u3002", "method": "\u6784\u5efa\u5305\u542b300\u4e07\u6761\u8bc4\u8bba\u548c1800\u4e2a\u5e94\u7528\u7684\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u8ba1\u7b97\u6a21\u578b\u8bc6\u522b\u8868\u660e\u9a9a\u6270\u7684\u8bc4\u8bba\uff0c\u5206\u6790\u9a9a\u6270\u7684\u5730\u5f62\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5bf9\u5a01\u80c1\u548c\u753b\u50cf\u9a9a\u6270\u7684\u53ec\u56de\u7387\u5206\u522b\u8fbe\u523090%\u548c85%\u3002\u53d1\u73b0\u9a9a\u6270\u8005\u591a\u4e3a\u5973\u6027\u8eab\u4efd\uff0c\u8d1f\u9762\u8bc4\u8bba\u4e0e\u4e2d\u6027\u8bc4\u8bba\u7684\u533a\u522b\u5728\u4e8e\u6124\u6012\u3001\u538c\u6076\u548c\u6050\u60e7\u60c5\u7eea\u66f4\u666e\u904d\u3002\u8bc6\u522b\u51fa1395\u4e2a\u5b58\u5728\u9a9a\u6270\u95ee\u9898\u7684\u5e94\u7528\u3002", "conclusion": "\u793e\u4ea4\u5e94\u7528\u786e\u5b9e\u5b58\u5728\u4e25\u91cd\u7684\u9a9a\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u8bc4\u8bba\u5206\u6790\u53ef\u4ee5\u6709\u6548\u8bc6\u522b\u548c\u91cf\u5316\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6539\u8fdb\u4f9d\u636e\u3002"}}
{"id": "2511.12157", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12157", "abs": "https://arxiv.org/abs/2511.12157", "authors": ["Jonathan Chirinos-Rodriguez", "C\u00e9dric F\u00e9votte", "Emmanuel Soubies"], "title": "Optimization landscape of $\\ell_0$-Bregman relaxations", "comment": null, "summary": "In this paper, we study (noisy) linear systems, and their $\\ell_0$-regularized optimization problems, coupled with general data fidelity terms. Recent approaches for solving this class of problems have proposed to consider non-convex exact continuous relaxations that preserve global minimizers while reducing the number of local minimizers. Within this framework, we consider the class of $\\ell_0$-Bregman relaxations, and establish sufficient conditions under which a critical point is isolated in terms of sparsity, in the sense that any other critical point has a strictly larger cardinality. In this way, we ensure a form of uniqueness in the solution structure. Furthermore, we analyze the exact recovery properties of such exact relaxations. To that end, we derive conditions under which the oracle solution (i.e., the one sharing the same support as the ground-truth) is the unique global minimizer of the relaxed problem, and is isolated in terms of sparsity. Our analysis is primarily built upon a novel property we introduce, termed the Bregman Restricted Strong Convexity. Finally, we specialize our general results to both sparse Gaussian (least-squares) and Poisson ((generalized) Kullback-Leibler divergence) regression problems. In particular, we show that our general analysis sharpens existing bounds for the LS setting, while providing an entirely new result for the KL case.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u6709\u21130\u6b63\u5219\u5316\u7684\u7ebf\u6027\u7cfb\u7edf\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u21130-Bregman\u677e\u5f1b\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u4e34\u754c\u70b9\u7a00\u758f\u6027\u9694\u79bb\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u5206\u6790\u4e86\u7cbe\u786e\u6062\u590d\u7279\u6027\u3002", "motivation": "\u89e3\u51b3\u21130\u6b63\u5219\u5316\u4f18\u5316\u95ee\u9898\u4e2d\u5c40\u90e8\u6781\u5c0f\u503c\u8fc7\u591a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u975e\u51f8\u7cbe\u786e\u8fde\u7eed\u677e\u5f1b\u6765\u4fdd\u7559\u5168\u5c40\u6781\u5c0f\u503c\u540c\u65f6\u51cf\u5c11\u5c40\u90e8\u6781\u5c0f\u503c\u6570\u91cf\u3002", "method": "\u91c7\u7528\u21130-Bregman\u677e\u5f1b\u65b9\u6cd5\uff0c\u5f15\u5165Bregman\u53d7\u9650\u5f3a\u51f8\u6027\u65b0\u6982\u5ff5\uff0c\u5efa\u7acb\u4e34\u754c\u70b9\u7a00\u758f\u6027\u9694\u79bb\u6761\u4ef6\uff0c\u5206\u6790oracle\u89e3\u7684\u7cbe\u786e\u6062\u590d\u7279\u6027\u3002", "result": "\u4e3a\u7a00\u758f\u9ad8\u65af\uff08\u6700\u5c0f\u4e8c\u4e58\uff09\u548c\u6cca\u677e\uff08KL\u6563\u5ea6\uff09\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u7cbe\u786e\u6062\u590d\u6761\u4ef6\uff0c\u6539\u8fdb\u4e86LS\u8bbe\u7f6e\u7684\u73b0\u6709\u8fb9\u754c\uff0c\u5e76\u4e3aKL\u60c5\u51b5\u63d0\u4f9b\u4e86\u5168\u65b0\u7ed3\u679c\u3002", "conclusion": "\u21130-Bregman\u677e\u5f1b\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u5c40\u90e8\u6781\u5c0f\u503c\uff0c\u786e\u4fdd\u89e3\u7684\u552f\u4e00\u6027\u548c\u7a00\u758f\u6027\u9694\u79bb\uff0c\u4e3a\u7a00\u758f\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2511.11883", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11883", "abs": "https://arxiv.org/abs/2511.11883", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts", "comment": null, "summary": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.", "AI": {"tldr": "ClinStructor\u4f7f\u7528LLM\u5c06\u4e34\u5e8a\u81ea\u7531\u6587\u672c\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u95ee\u7b54\u5bf9\uff0c\u4ee5\u89e3\u51b3\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u504f\u89c1\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5728ICU\u6b7b\u4ea1\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\u4ec5\u5bfc\u81f4AUC\u8f7b\u5fae\u4e0b\u964d2-3%\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u6709\u4ef7\u503c\u4fe1\u606f\u4f46\u5b58\u5728\u975e\u7ed3\u6784\u5316\u683c\u5f0f\u5e26\u6765\u7684\u6311\u6218\uff0c\u5305\u62ec\u65e0\u610f\u504f\u89c1\uff08\u5982\u6027\u522b\u6216\u79cd\u65cf\u504f\u89c1\uff09\u3001\u8de8\u4e34\u5e8a\u73af\u5883\u6cdb\u5316\u6027\u5dee\uff08\u5982\u4e0d\u540cEHR\u7cfb\u7edf\u95f4\u6a21\u578b\u6027\u80fd\u5dee\u5f02\uff09\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u4e34\u5e8a\u81ea\u7531\u6587\u672c\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u95ee\u7b54\u5bf9\uff0c\u4f5c\u4e3a\u9884\u6d4b\u5efa\u6a21\u7684\u524d\u7f6e\u6b65\u9aa4\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u63a7\u6027\uff0c\u5728ICU\u6b7b\u4ea1\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\u4e0e\u76f4\u63a5\u5fae\u8c03\u76f8\u6bd4\u4ec5\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u8f7b\u5fae\u4e0b\u964d\uff08AUC\u4e0b\u964d2-3%\uff09\u3002", "conclusion": "ClinStructor\u4e3a\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u6784\u5efa\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2511.11585", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11585", "abs": "https://arxiv.org/abs/2511.11585", "authors": ["Kabir Khan", "Manju Sarkar", "Anita Kar", "Suresh Ghosh"], "title": "Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge", "comment": "37 pages, 8 figures", "summary": "Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.", "AI": {"tldr": "FedGen-Edge\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u9884\u8bad\u7ec3\u7684\u5168\u5c40\u4e3b\u5e72\u7f51\u7edc\u548c\u8f7b\u91cf\u7ea7\u5ba2\u6237\u7aef\u9002\u914d\u5668\uff0c\u4ec5\u8054\u90a6\u5316\u9002\u914d\u5668\u6765\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u652f\u6301\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u751f\u6210\u5f0fAI\u3002", "motivation": "\u5927\u578b\u751f\u6210\u6a21\u578b\u5728\u8de8\u8bbe\u5907\u8054\u90a6\u5b66\u4e60\u4e2d\u9762\u4e34\u8ba1\u7b97\u901a\u4fe1\u5f00\u9500\u5927\u3001\u7edf\u8ba1/\u7cfb\u7edf\u5f02\u6784\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u7528\u7684\u9690\u79c1\u4fdd\u62a4\u3001\u8d44\u6e90\u611f\u77e5\u548c\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u914d(LoRA)\u5c06\u5ba2\u6237\u7aef\u66f4\u65b0\u7ea6\u675f\u5230\u7d27\u51d1\u5b50\u7a7a\u95f4\uff0c\u4ec5\u8054\u90a6\u5316\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u800c\u975e\u5b8c\u6574\u6a21\u578b\uff0c\u652f\u6301FedAvg\u98ce\u683c\u7684\u670d\u52a1\u5668\u805a\u5408\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21(PTB)\u548c\u56fe\u50cf\u751f\u6210(CIFAR-10)\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6/FID\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c11\u8d85\u8fc799%\u3002", "conclusion": "FedGen-Edge\u4e3a\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9690\u79c1\u4fdd\u62a4\u3001\u8d44\u6e90\u611f\u77e5\u548c\u4e2a\u6027\u5316\u751f\u6210\u5f0fAI\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.11773", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11773", "abs": "https://arxiv.org/abs/2511.11773", "authors": ["Ruchira Dhar", "Ninell Oldenburg", "Anders Soegaard"], "title": "On the Measure of a Model: From Intelligence to Generality", "comment": "Accepted at EurIPS Workshop on \"The Science of Benchmarking and Evaluating AI\"", "summary": "Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u5f53\u524d\u57fa\u4e8e\u62bd\u8c61\u667a\u529b\u6982\u5ff5\uff08\u5982ARC\u3001Raven\u6d4b\u8bd5\u7b49\uff09\u7684AI\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8ba4\u4e3a\u5e94\u8f6c\u5411\u57fa\u4e8e\u901a\u7528\u6027\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u901a\u7528\u6027\u89c6\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u76f4\u63a5\u5173\u8054\u5230\u53ef\u6d4b\u91cf\u7684\u6027\u80fd\u5e7f\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524dAI\u8bc4\u4f30\u8fc7\u5ea6\u4f9d\u8d56\u62bd\u8c61\u667a\u529b\u6982\u5ff5\uff0c\u7f3a\u4e4f\u7a33\u5b9a\u5b9a\u4e49\u4e14\u65e0\u6cd5\u9884\u6d4b\u5b9e\u9645\u4efb\u52a1\u8868\u73b0\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bc4\u4f30\u4e0e\u73b0\u5b9e\u6548\u7528\u8131\u8282\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u548c\u5f62\u5f0f\u5206\u6790\uff0c\u68c0\u9a8c\u667a\u529b\u8bc4\u4f30\u7684\u4e09\u4e2a\u5047\u8bbe\uff08\u901a\u7528\u6027\u3001\u7a33\u5b9a\u6027\u3001\u73b0\u5b9e\u6027\uff09\uff0c\u8bc1\u660e\u53ea\u6709\u901a\u7528\u6027\u7ecf\u5f97\u8d77\u6982\u5ff5\u548c\u5b9e\u8bc1\u68c0\u9a8c\u3002", "result": "\u667a\u529b\u4e0d\u662f\u5b9e\u73b0\u901a\u7528\u6027\u7684\u539f\u56e0\uff1b\u901a\u7528\u6027\u5e94\u88ab\u7406\u89e3\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u76f4\u63a5\u94fe\u63a5\u5230\u53ef\u6d4b\u91cf\u7684\u6027\u80fd\u5e7f\u5ea6\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u5e94\u91cd\u65b0\u5b9a\u4e49AI\u8fdb\u5c55\u8bc4\u4f30\u65b9\u5f0f\uff0c\u5c06\u901a\u7528\u6027\u4f5c\u4e3a\u8bc4\u4f30\u591a\u6837\u5316\u4efb\u52a1\u80fd\u529b\u7684\u66f4\u7a33\u5b9a\u57fa\u7840\u3002"}}
{"id": "2511.13025", "categories": ["stat.ML", "cs.LG", "math.DG", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.13025", "abs": "https://arxiv.org/abs/2511.13025", "authors": ["Charles Fefferman", "Jonathan Marty", "Kevin Ren"], "title": "Reconstruction of Manifold Distances from Noisy Observations", "comment": "43 pages", "summary": "We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $\u03bc$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\\dots,X_N$ are i.i.d. samples of $\u03bc$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\\varepsilon \\log \\varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \\asymp \\varepsilon^{-2d-2}\\log(1/\\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u566a\u58f0\u6210\u5bf9\u8ddd\u79bb\u89c2\u6d4b\u4e2d\u91cd\u5efa\u6d41\u5f62\u5185\u5728\u51e0\u4f55\u7684\u65b0\u6846\u67b6\uff0c\u6539\u8fdb\u4e86\u5148\u524d\u9700\u8981\u5df2\u77e5\u77e9\u7684\u52a0\u6027\u566a\u58f0\u5047\u8bbe\u7684\u9650\u5236\uff0c\u80fd\u591f\u5728\u6709\u754c\u66f2\u7387\u548c\u6b63\u5185\u5c04\u534a\u5f84\u7684\u6e29\u548c\u51e0\u4f55\u5047\u8bbe\u4e0b\uff0c\u4ee5O(\u03b5 log \u03b5\u207b\u00b9)\u7684\u52a0\u6027\u8bef\u5dee\u6062\u590d\u6837\u672c\u70b9\u95f4\u7684\u771f\u5b9e\u8ddd\u79bb\u3002", "motivation": "\u4ece\u566a\u58f0\u8ddd\u79bb\u89c2\u6d4b\u4e2d\u91cd\u5efa\u6d41\u5f62\u7684\u5185\u5728\u51e0\u4f55\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u5148\u524d\u5de5\u4f5c\u5047\u8bbe\u566a\u58f0\u4e3a\u5df2\u77e5\u77e9\u7684\u72ec\u7acb\u540c\u5206\u5e03\u52a0\u6027\u566a\u58f0\uff0c\u8fd9\u5728\u5b9e\u8df5\u4e2d\u9650\u5236\u6027\u8fc7\u5f3a\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u66f4\u901a\u7528\u7684\u6846\u67b6\uff0c\u653e\u5bbd\u5bf9\u566a\u58f0\u5206\u5e03\u7684\u5047\u8bbe\u3002", "method": "\u57fa\u4e8e\u4f30\u8ba1\u671f\u671b\u51fd\u6570f_x(y)=\ud835\udd3cd'(x,y)\u7684L\u2082\u8303\u6570\u7684\u65b0\u65b9\u6cd5\u6784\u5efa\u9c81\u68d2\u805a\u7c7b\uff0c\u901a\u8fc7\u51e0\u4f55\u8bba\u8bc1\u8bc1\u660e\u8fd9\u4e9b\u805a\u7c7b\u80fd\u591f\u6062\u590d\u6837\u672c\u70b9\u95f4\u7684\u771f\u5b9e\u8ddd\u79bb\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u7b2c\u4e00\u79cd\u8fbe\u5230\u6837\u672c\u590d\u6742\u5ea6N\u224d\u03b5\u207b\u00b2\u1d48\u207b\u00b2log(1/\u03b5)\u548co(N\u00b3)\u8fd0\u884c\u65f6\u95f4\uff1b\u7b2c\u4e8c\u79cd\u5f15\u5165\u65b0\u9896\u7684\u51e0\u4f55\u601d\u60f3\u3002", "result": "\u5728\u6e29\u548c\u51e0\u4f55\u5047\u8bbe\uff08\u6709\u754c\u66f2\u7387\u548c\u6b63\u5185\u5c04\u534a\u5f84\uff09\u4e0b\uff0c\u80fd\u591f\u4ee5O(\u03b5 log \u03b5\u207b\u00b9)\u7684\u52a0\u6027\u8bef\u5dee\u6062\u590d\u6837\u672c\u70b9\u95f4\u7684\u771f\u5b9e\u8ddd\u79bb\u3002\u5728\u5b58\u5728\u7f3a\u5931\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u53ea\u8981\u91c7\u6837\u6982\u7387\u6ee1\u8db3\u5b9a\u91cf\u4e0b\u754c\uff0c\u5c31\u80fd\u6269\u5c55\u6240\u6709\u6062\u590d\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u6539\u8fdb\u4e86\u4ece\u566a\u58f0\u8ddd\u79bb\u89c2\u6d4b\u4e2d\u91cd\u5efa\u6d41\u5f62\u51e0\u4f55\u7684\u65b9\u6cd5\uff0c\u653e\u5bbd\u4e86\u5bf9\u566a\u58f0\u5206\u5e03\u7684\u5047\u8bbe\u3002\u4e3b\u8981\u6280\u672f\u7ed3\u679c\u9610\u660e\u4e86\u6d41\u5f62\u7684\u54ea\u4e9b\u6027\u8d28\u5bf9\u8ddd\u79bb\u6062\u590d\u662f\u5fc5\u8981\u7684\uff0c\u8868\u660e\u6280\u672f\u53ef\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u5ea6\u91cf\u6982\u7387\u7a7a\u95f4\u3002"}}
{"id": "2511.12384", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12384", "abs": "https://arxiv.org/abs/2511.12384", "authors": ["Weiqi Meng", "Hongyi Li", "Bai Cui"], "title": "DER Day-Ahead Offering: A Neural Network Column-and-Constraint Generation Approach", "comment": "5 pages, 1 figure. Submitted to IEEE PES General Meeting 2026", "summary": "In the day-ahead energy market, the offering strategy of distributed energy resource (DER) aggregators must be submitted before the uncertainty realization in the form of price-quantity pairs. This work addresses the day-ahead offering problem through a two-stage robust adaptive stochastic optimization model, wherein the first-stage price-quantity pairs and second-stage operational commitment decisions are made before and after DER uncertainty is realized, respectively. Uncertainty in day-ahead price is addressed using a stochastic programming, while uncertainty of DER generation is handled through robust optimization. To address the max-min structure of the second-stage problem, a neural network-accelerated column-and-constraint generation method is developed. A dedicated neural network is trained to approximate the value function, while optimality is maintained by the design of the network architecture. Numerical studies indicate that the proposed method yields high-quality solutions and is up to 100 times faster than Gurobi and 33 times faster than classical column-and-constraint generation on the same 1028-node synthetic distribution network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u9c81\u68d2\u81ea\u9002\u5e94\u968f\u673a\u4f18\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u80fd\u6e90\u805a\u5408\u5546\u5728\u65e5\u524d\u80fd\u6e90\u5e02\u573a\u4e2d\u7684\u62a5\u4ef7\u7b56\u7565\u95ee\u9898\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u7684\u5217\u4e0e\u7ea6\u675f\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6c42\u89e3\u6548\u7387\u3002", "motivation": "\u5728\u65e5\u524d\u80fd\u6e90\u5e02\u573a\u4e2d\uff0c\u5206\u5e03\u5f0f\u80fd\u6e90\u805a\u5408\u5546\u9700\u8981\u5728\u4e0d\u786e\u5b9a\u6027\u5b9e\u73b0\u524d\u63d0\u4ea4\u4ef7\u683c-\u6570\u91cf\u5bf9\u7684\u62a5\u4ef7\u7b56\u7565\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u65e5\u524d\u4ef7\u683c\u548c\u5206\u5e03\u5f0f\u80fd\u6e90\u53d1\u7535\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9c81\u68d2\u81ea\u9002\u5e94\u968f\u673a\u4f18\u5316\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u786e\u5b9a\u4ef7\u683c-\u6570\u91cf\u5bf9\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5728\u5206\u5e03\u5f0f\u80fd\u6e90\u4e0d\u786e\u5b9a\u6027\u5b9e\u73b0\u540e\u505a\u51fa\u8fd0\u884c\u627f\u8bfa\u51b3\u7b56\u3002\u4f7f\u7528\u968f\u673a\u89c4\u5212\u5904\u7406\u65e5\u524d\u4ef7\u683c\u4e0d\u786e\u5b9a\u6027\uff0c\u9c81\u68d2\u4f18\u5316\u5904\u7406\u5206\u5e03\u5f0f\u80fd\u6e90\u53d1\u7535\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f00\u53d1\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u7684\u5217\u4e0e\u7ea6\u675f\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u57281028\u8282\u70b9\u5408\u6210\u914d\u7535\u7f51\u7edc\u4e0a\u7684\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u89e3\uff0c\u6bd4Gurobi\u5feb100\u500d\uff0c\u6bd4\u7ecf\u5178\u5217\u4e0e\u7ea6\u675f\u751f\u6210\u65b9\u6cd5\u5feb33\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u5206\u5e03\u5f0f\u80fd\u6e90\u805a\u5408\u5546\u7684\u65e5\u524d\u62a5\u4ef7\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u89e3\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.11723", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11723", "abs": "https://arxiv.org/abs/2511.11723", "authors": ["Mohammed Abboodi"], "title": "A framework for measuring and analyzing customer satisfaction at computer service companies using Lean Six Sigma", "comment": "Master's thesis", "summary": "The computer service industry has expanded rapidly over the past two decades, driven by the proliferation of computing technologies, the entry of large firms, and the availability of online diagnostic and troubleshooting tools. In this increasingly competitive environment, many small and medium sized enterprises struggle to maintain customer satisfaction as rivals deliver higher quality services at lower cost. This study addresses the absence of robust measurement systems for assessing service quality, a key factor underlying customer attrition, by proposing an integrated framework for evaluating satisfaction and identifying sources of dissatisfaction in computer services.\n  The framework combines core principles of Six Sigma with the SERVQUAL instrument within a structured DMAIC methodology (Define, Measure, Analyze, Improve, and Control). SERVQUAL provides the service quality dimensions and gap analysis techniques, while Six Sigma supplies the data driven approach to measurement and improvement. The literature suggests limited prior work integrating Lean Six Sigma with SERVQUAL, and this study contributes by operationalizing that integration in a real world setting.\n  A case study of a computer services company was conducted to demonstrate feasibility and effectiveness. Satisfaction levels were quantified, and root causes of dissatisfaction were identified. The analysis revealed a low overall satisfaction level and five primary drivers of unmet customer requirements. Addressing these causes is expected to increase customer satisfaction, lower customer acquisition costs, and improve overall organizational performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5c06\u516d\u897f\u683c\u739b\u4e0eSERVQUAL\u5de5\u5177\u6574\u5408\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u673a\u670d\u52a1\u884c\u4e1a\u7684\u670d\u52a1\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8ba1\u7b97\u673a\u670d\u52a1\u884c\u4e1a\u7ade\u4e89\u6fc0\u70c8\uff0c\u4e2d\u5c0f\u4f01\u4e1a\u96be\u4ee5\u7ef4\u6301\u5ba2\u6237\u6ee1\u610f\u5ea6\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u670d\u52a1\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u662f\u5bfc\u81f4\u5ba2\u6237\u6d41\u5931\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u5c06\u516d\u897f\u683c\u739b\u7684\u6838\u5fc3\u539f\u5219\u4e0eSERVQUAL\u5de5\u5177\u5728DMAIC\u65b9\u6cd5\u8bba\u4e2d\u6574\u5408\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u8be5\u96c6\u6210\u6846\u67b6\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u91cf\u5316\u4e86\u6ee1\u610f\u5ea6\u6c34\u5e73\uff0c\u8bc6\u522b\u51fa\u4e94\u4e2a\u4e3b\u8981\u7684\u4e0d\u6ee1\u610f\u539f\u56e0\uff0c\u63ed\u793a\u4e86\u8f83\u4f4e\u7684\u603b\u4f53\u6ee1\u610f\u5ea6\u6c34\u5e73\u3002", "conclusion": "\u89e3\u51b3\u8bc6\u522b\u51fa\u7684\u4e0d\u6ee1\u610f\u539f\u56e0\u9884\u8ba1\u80fd\u63d0\u9ad8\u5ba2\u6237\u6ee1\u610f\u5ea6\u3001\u964d\u4f4e\u5ba2\u6237\u83b7\u53d6\u6210\u672c\u5e76\u6539\u5584\u7ec4\u7ec7\u6574\u4f53\u7ee9\u6548\u3002"}}
{"id": "2511.12235", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12235", "abs": "https://arxiv.org/abs/2511.12235", "authors": ["Josef Simbrunner", "Clemens Krenn", "Martin Zach", "Andreas Habring"], "title": "Computation of a Consistent System Matrix for Cone-beam Computed Tomography", "comment": null, "summary": "We propose a method for the computation of a consistent system matrix for two- and three-dimensional cone-beam computed tomography (CT). The method relies on the decomposition of the cone-voxel intersection volumes into subvolumes that contribute to distinct detector elements and whose contributions to the system matrix admit exact formulae that can be evaluated without the invocation of costly iterative subroutines. We demonstrate that the reconstructions obtained when using the proposed system matrix are superior to those obtained when using common line-based integration approaches with numerical experiments on synthetic and real CT data. Moreover, we provide a CUDA implementation of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9525\u675fCT\u7cfb\u7edf\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u9525\u675f-\u4f53\u7d20\u76f8\u4ea4\u4f53\u79ef\u4e3a\u5b50\u4f53\u79ef\uff0c\u63d0\u4f9b\u7cbe\u786e\u8ba1\u7b97\u516c\u5f0f\uff0c\u65e0\u9700\u8fed\u4ee3\u8ba1\u7b97\uff0c\u91cd\u5efa\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u7ebf\u79ef\u5206\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9bCUDA\u5b9e\u73b0\u3002", "motivation": "\u4f20\u7edf\u9525\u675fCT\u7cfb\u7edf\u77e9\u9635\u8ba1\u7b97\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u57fa\u4e8e\u7ebf\u79ef\u5206\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u591f\u7cbe\u786e\u4e14\u8ba1\u7b97\u6548\u7387\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u7cbe\u786e\u7cfb\u7edf\u77e9\u9635\u8ba1\u7b97\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u5c06\u9525\u675f-\u4f53\u7d20\u76f8\u4ea4\u4f53\u79ef\u5206\u89e3\u4e3a\u8d21\u732e\u5230\u4e0d\u540c\u63a2\u6d4b\u5668\u5143\u7d20\u7684\u5b50\u4f53\u79ef\uff0c\u4e3a\u8fd9\u4e9b\u5b50\u4f53\u79ef\u7684\u8d21\u732e\u63d0\u4f9b\u7cbe\u786e\u8ba1\u7b97\u516c\u5f0f\uff0c\u907f\u514d\u4f7f\u7528\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u8fed\u4ee3\u5b50\u7a0b\u5e8f\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eCT\u6570\u636e\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u5f97\u5230\u7684\u91cd\u5efa\u7ed3\u679c\u4f18\u4e8e\u5e38\u7528\u7684\u57fa\u4e8e\u7ebf\u79ef\u5206\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u8ba1\u7b97\u4e00\u81f4\u7684\u7cfb\u7edf\u77e9\u9635\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7CUDA\u5b9e\u73b0\u786e\u4fdd\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.11884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11884", "abs": "https://arxiv.org/abs/2511.11884", "authors": ["Eric Hua Qing Zhang", "Julia Ive"], "title": "Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support", "comment": null, "summary": "Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.", "AI": {"tldr": "\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u589e\u5f3aGPT-2\u7684\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u51c6\u786e\u6027\u548c\u4e13\u4e1a\u54cd\u5e94\u8d28\u91cf", "motivation": "COVID-19\u52a0\u5267\u4e86\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u7684\u53ef\u53ca\u6027\u95ee\u9898\uff0c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u60c5\u5883\u548c\u60c5\u611f\u610f\u8bc6\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u4e13\u4e1a\u7684\u6cbb\u7597\u5bf9\u8bdd\u7cfb\u7edf", "method": "\u91cd\u6784\u8f93\u5165\u683c\u5f0f\u4ee5\u540c\u65f6\u5904\u7406\u60c5\u5883\u4fe1\u606f\u548c\u60c5\u611f\u72b6\u6001\uff0c\u91c7\u7528\u591a\u7ec4\u4ef6\u5956\u52b1\u51fd\u6570\u4f7f\u6a21\u578b\u8f93\u51fa\u4e0e\u4e13\u4e1a\u6cbb\u7597\u5e08\u54cd\u5e94\u548c\u6807\u6ce8\u60c5\u611f\u5bf9\u9f50", "result": "\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebfGPT-2\uff0c\u60c5\u611f\u51c6\u786e\u7387\u8fbe\u523099.34%\uff08\u57fa\u7ebf\u4e3a66.96%\uff09\uff0cLLM\u8bc4\u4f30\u786e\u8ba4\u9ad8\u60c5\u5883\u76f8\u5173\u6027\u548c\u4e13\u4e1a\u6027", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u5f00\u53d1\u6cbb\u7597\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u53ef\u4f5c\u4e3a\u6cbb\u7597\u5e08\u7684\u6709\u4ef7\u503c\u8f85\u52a9\u5de5\u5177\uff0c\u540c\u65f6\u4fdd\u6301\u5fc5\u8981\u7684\u4eba\u7c7b\u4e34\u5e8a\u76d1\u7763"}}
{"id": "2511.11589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11589", "abs": "https://arxiv.org/abs/2511.11589", "authors": ["Chenyue Liu", "Ali Mostafavi"], "title": "WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation", "comment": null, "summary": "Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.", "AI": {"tldr": "WildfireGenome\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u8054\u90a6\u6307\u6807\u3001\u968f\u673a\u68ee\u6797\u5206\u7c7b\u548cSHAP\u5206\u6790\uff0c\u5728H3 Level-8\u5206\u8fa8\u7387\u4e0b\u63d0\u4f9b\u51b3\u7b56\u5c3a\u5ea6\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u4f9d\u8d56\u7c97\u7cd9\u7684\u5371\u9669\u5730\u56fe\u548c\u4e0d\u900f\u660e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u727a\u7272\u4e86\u51b3\u7b56\u5c3a\u5ea6\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u878d\u54087\u4e2a\u8054\u90a6\u91ce\u706b\u6307\u6807\u8fdb\u884cPCA\u5206\u6790\u751f\u6210\u98ce\u9669\u6807\u7b7e\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u672c\u5730\u91ce\u706b\u98ce\u9669\uff0c\u5e76\u901a\u8fc7SHAP\u548cICE/PDP\u5206\u6790\u63ed\u793a\u53bf\u57df\u7279\u5b9a\u7684\u975e\u7ebf\u6027\u9a71\u52a8\u5173\u7cfb\u3002", "result": "\u57287\u4e2a\u751f\u6001\u591a\u6837\u5316\u7684\u7f8e\u56fd\u53bf\u4e2d\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe0.755-0.878\uff0c\u4e8c\u6b21\u52a0\u6743Kappa\u8fbe0.951\uff0c\u4e3b\u6210\u5206\u89e3\u91ca\u4e8687-94%\u7684\u6307\u6807\u65b9\u5dee\u3002\u9488\u53f6\u6797\u8986\u76d6\u7387\u548c\u6d77\u62d4\u662f\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u98ce\u9669\u572830-40%\u9488\u53f6\u6797\u8986\u76d6\u7387\u65f6\u6025\u5267\u4e0a\u5347\u3002", "conclusion": "WildfireGenome\u5c06\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u4ece\u533a\u57df\u9884\u6d4b\u63a8\u8fdb\u5230\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u5c3a\u5ea6\u5206\u6790\uff0c\u53ef\u6307\u5bfc\u690d\u88ab\u7ba1\u7406\u3001\u5206\u533a\u548c\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u3002"}}
{"id": "2511.11816", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11816", "abs": "https://arxiv.org/abs/2511.11816", "authors": ["Andrea Brunello", "Luca Geatti", "Michele Mignani", "Angelo Montanari", "Nicola Saccomanno"], "title": "Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy", "comment": "Full version of the paper accepted for publication at The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u51c6\u786e\u8bc4\u4f30LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5230\u4e00\u9636\u903b\u8f91\u8f6c\u6362(NL-FOL)\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5bf9\u8bdd\u5bfc\u5411\u7684LLMs\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u903b\u8f91\u7406\u89e3\u80fd\u529b\uff0c\u800c\u5d4c\u5165\u4e2d\u5fc3\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30NL-FOL\u8f6c\u6362\u7684\u6570\u636e\u96c6\u548c\u534f\u8bae\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620LLMs\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u9700\u8981\u8bbe\u8ba1\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u533a\u5206\u771f\u6b63\u7684\u8bed\u4e49\u7ea7\u903b\u8f91\u7406\u89e3\u4e0e\u8868\u9762\u6a21\u5f0f\u8bc6\u522b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u533a\u5206\u771f\u6b63\u7684\u8bed\u4e49\u7ea7\u903b\u8f91\u7406\u89e3\u4e0e\u8868\u9762\u6a21\u5f0f\u8bc6\u522b\u3001\u8bb0\u5fc6\u548c\u6570\u636e\u96c6\u6c61\u67d3\u3002", "result": "\u4f7f\u7528\u65b0\u65b9\u6cd5\u8bc4\u4f30\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684\u5bf9\u8bdd\u5bfc\u5411LLMs\u8868\u73b0\u51fa\u5f3a\u5927\u7684NL-FOL\u8f6c\u6362\u6280\u80fd\u548c\u771f\u6b63\u7684\u53e5\u5b50\u7ea7\u903b\u8f91\u7406\u89e3\u80fd\u529b\uff0c\u800c\u5d4c\u5165\u4e2d\u5fc3\u6a21\u578b\u8868\u73b0\u660e\u663e\u66f4\u5dee\u3002", "conclusion": "\u5bf9\u8bdd\u5bfc\u5411\u7684LLMs\u5728NL-FOL\u7ffb\u8bd1\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u7684\u903b\u8f91\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2511.13221", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13221", "abs": "https://arxiv.org/abs/2511.13221", "authors": ["Mohamed Salem", "Inyoung Kim"], "title": "Likelihood-guided Regularization in Attention Based Models", "comment": null, "summary": "The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u4f0a\u8f9b\u6b63\u5219\u5316\u7684Vision Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u7a00\u758f\u5316\u6280\u672f\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5e76\u52a8\u6001\u526a\u679d\u5197\u4f59\u53c2\u6570\u3002", "motivation": "Transformer\u67b6\u6784\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u7cbe\u5fc3\u6b63\u5219\u5316\u6765\u9632\u6b62\u8fc7\u62df\u5408\u3002\u4f20\u7edfdropout\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u7a00\u758f\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u4efb\u52a1\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u4f0a\u8f9b\u6b63\u5219\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u7a00\u758f\u5316\u6280\u672f\u5bf9\u6a21\u578b\u6743\u91cd\u65bd\u52a0\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u81ea\u9002\u5e94\u67b6\u6784\u641c\u7d22\u3002", "result": "\u5728MNIST\u3001Fashion-MNIST\u3001CIFAR-10\u548cCIFAR-100\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u7a00\u758f\u590d\u6742\u6570\u636e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u6743\u91cd\u548c\u9009\u62e9\u53c2\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "\u7ed3\u6784\u5316\u8d1d\u53f6\u65af\u7a00\u758f\u5316\u80fd\u6709\u6548\u589e\u5f3a\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u4e3a\u4f20\u7edf\u6b63\u5219\u5316\u6280\u672f\u63d0\u4f9b\u4e86\u7406\u8bba\u66ff\u4ee3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6982\u7387\u6821\u51c6\u548c\u7ed3\u6784\u5316\u7279\u5f81\u9009\u62e9\u3002"}}
{"id": "2511.12431", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12431", "abs": "https://arxiv.org/abs/2511.12431", "authors": ["Zhuoyuan Wang", "Xiyu Deng", "Hikaru Hoshino", "Yorie Nakahira"], "title": "Online Adaptive Probabilistic Safety Certificate with Language Guidance", "comment": null, "summary": "Achieving long-term safety in uncertain or extreme environments while accounting for human preferences remains a fundamental challenge for autonomous systems. Existing methods often trade off long-term guarantees for fast real-time control and cannot adapt to variability in human preferences or risk tolerance. To address these limitations, we propose a language-guided adaptive probabilistic safety certificate (PSC) framework that guarantees long-term safety for stochastic systems under environmental uncertainty while accommodating diverse human preferences. The proposed framework integrates natural-language inputs from users and Bayesian estimators of the environment into adaptive safety certificates that explicitly account for user preferences, system dynamics, and quantified uncertainties. Our key technical innovation leverages probabilistic invariance--a generalization of forward invariance to a probability space--to obtain myopic safety conditions with long-term safety guarantees that integrate language guidance, model information, and quantified uncertainty. We validate the framework through numerical simulations of autonomous lane-keeping with human-in-the-loop guidance under uncertain and extreme road conditions, demonstrating enhanced safety-performance trade-offs, adaptability to changing environments, and personalization to different user preferences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u6982\u7387\u5b89\u5168\u8bc1\u4e66\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u4fdd\u8bc1\u968f\u673a\u7cfb\u7edf\u7684\u957f\u671f\u5b89\u5168\uff0c\u540c\u65f6\u9002\u5e94\u4e0d\u540c\u7684\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5728\u957f\u671f\u5b89\u5168\u4fdd\u8bc1\u548c\u5b9e\u65f6\u63a7\u5236\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u65e0\u6cd5\u9002\u5e94\u4eba\u7c7b\u504f\u597d\u6216\u98ce\u9669\u5bb9\u5fcd\u5ea6\u7684\u53d8\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4fdd\u8bc1\u957f\u671f\u5b89\u5168\u3001\u9002\u5e94\u73af\u5883\u53d8\u5316\u5e76\u8003\u8651\u4eba\u7c7b\u504f\u597d\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u548c\u8d1d\u53f6\u65af\u73af\u5883\u4f30\u8ba1\u5668\u96c6\u6210\u5230\u81ea\u9002\u5e94\u5b89\u5168\u8bc1\u4e66\u4e2d\uff0c\u5229\u7528\u6982\u7387\u4e0d\u53d8\u6027\uff08\u6982\u7387\u7a7a\u95f4\u4e2d\u7684\u524d\u5411\u4e0d\u53d8\u6027\u63a8\u5e7f\uff09\u83b7\u5f97\u5177\u6709\u957f\u671f\u5b89\u5168\u4fdd\u8bc1\u7684\u8fd1\u89c6\u5b89\u5168\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u9a7e\u9a76\u8f66\u9053\u4fdd\u6301\u7684\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\uff0c\u5728\u4e0d\u786e\u5b9a\u548c\u6781\u7aef\u9053\u8def\u6761\u4ef6\u4e0b\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u5b89\u5168\u6027\u80fd\u6743\u8861\u3001\u5bf9\u73af\u5883\u53d8\u5316\u7684\u9002\u5e94\u6027\u4ee5\u53ca\u5bf9\u4e0d\u540c\u7528\u6237\u504f\u597d\u7684\u4e2a\u6027\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u4fdd\u8bc1\u968f\u673a\u7cfb\u7edf\u957f\u671f\u5b89\u5168\u5e76\u9002\u5e94\u4eba\u7c7b\u504f\u597d\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u5728\u6781\u7aef\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11738", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11738", "abs": "https://arxiv.org/abs/2511.11738", "authors": ["Matthias Huemmer", "Theophile Shyiramunda", "Franziska Durner", "Michelle J. Cummings-Koether"], "title": "On the Influence of Artificial Intelligence on Human Problem-Solving: Empirical Insights for the Third Wave in a Multinational Longitudinal Pilot Study", "comment": null, "summary": "This article presents the results and their discussion for the third wave (with n=23 participants) within a multinational longitudinal study that investigates the evolving paradigm of human-AI collaboration in problem-solving contexts. Building upon previous waves, our findings reveal the consolidation of a hybrid problem-solving culture characterized by strategic integration of AI tools within structured cognitive workflows. The data demonstrate near-universal AI adoption (95.7% with prior knowledge, 100% ChatGPT usage) primarily deployed through human-led sequences such as \"Think, Internet, ChatGPT, Further Processing\" (39.1%). However, this collaboration reveals a critical verification deficit that escalates with problem complexity. We empirically identify and quantify two systematic epistemic gaps: a belief-performance gap (up to +80.8 percentage points discrepancy between perceived and actual correctness) and a proof-belief gap (up to -16.8 percentage points between confidence and verification capability). These findings, derived from behavioral data and problem vignettes across complexity levels, indicate that the fundamental constraint on reliable AI-assisted work is solution validation rather than generation. The study concludes that educational and technological interventions must prioritize verification scaffolds (including assumption documentation protocols, adequacy criteria checklists, and triangulation procedures) to fortify the human role as critical validator in this new cognitive ecosystem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u8ba4\u77e5\u5dee\u8ddd\uff1a\u4fe1\u5ff5-\u8868\u73b0\u5dee\u8ddd\uff08\u611f\u77e5\u6b63\u786e\u6027\u4e0e\u5b9e\u9645\u6b63\u786e\u6027\u76f8\u5dee\u9ad8\u8fbe80.8\u4e2a\u767e\u5206\u70b9\uff09\u548c\u8bc1\u660e-\u4fe1\u5ff5\u5dee\u8ddd\uff08\u4fe1\u5fc3\u4e0e\u9a8c\u8bc1\u80fd\u529b\u76f8\u5dee-16.8\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u8868\u660eAI\u8f85\u52a9\u5de5\u4f5c\u7684\u5173\u952e\u9650\u5236\u5728\u4e8e\u89e3\u51b3\u65b9\u6848\u9a8c\u8bc1\u800c\u975e\u751f\u6210\u3002", "motivation": "\u7814\u7a76\u4eba\u673a\u534f\u4f5c\u5728\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6f14\u53d8\u6a21\u5f0f\uff0c\u7279\u522b\u5173\u6ce8AI\u5de5\u5177\u5728\u7ed3\u6784\u5316\u8ba4\u77e5\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6218\u7565\u6574\u5408\uff0c\u4ee5\u53ca\u8bc6\u522b\u5f71\u54cd\u53ef\u9760AI\u8f85\u52a9\u5de5\u4f5c\u7684\u5173\u952e\u7ea6\u675f\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u8de8\u56fd\u7eb5\u5411\u7814\u7a76\u7b2c\u4e09\u6ce2\uff08n=23\u53c2\u4e0e\u8005\uff09\uff0c\u901a\u8fc7\u884c\u4e3a\u6570\u636e\u548c\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u95ee\u9898\u60c5\u666f\uff0c\u5206\u6790AI\u91c7\u7528\u6a21\u5f0f\u548c\u8ba4\u77e5\u5de5\u4f5c\u6d41\u7a0b\uff0c\u91cf\u5316\u7cfb\u7edf\u6027\u8ba4\u77e5\u5dee\u8ddd\u3002", "result": "\u53d1\u73b0\u666e\u904dAI\u91c7\u7528\uff0895.7%\u6709\u5148\u9a8c\u77e5\u8bc6\uff0c100%\u4f7f\u7528ChatGPT\uff09\uff0c\u4e3b\u8981\u91c7\u7528\"\u601d\u8003\u3001\u4e92\u8054\u7f51\u3001ChatGPT\u3001\u8fdb\u4e00\u6b65\u5904\u7406\"\u7684\u4eba\u7c7b\u4e3b\u5bfc\u5e8f\u5217\uff0839.1%\uff09\uff0c\u4f46\u5b58\u5728\u968f\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\u7684\u9a8c\u8bc1\u7f3a\u9677\u3002", "conclusion": "\u6559\u80b2\u548c\u79d1\u6280\u5e72\u9884\u5fc5\u987b\u4f18\u5148\u8003\u8651\u9a8c\u8bc1\u652f\u67b6\uff08\u5305\u62ec\u5047\u8bbe\u6587\u6863\u534f\u8bae\u3001\u5145\u5206\u6027\u6807\u51c6\u6e05\u5355\u548c\u4e09\u89d2\u9a8c\u8bc1\u7a0b\u5e8f\uff09\uff0c\u4ee5\u5f3a\u5316\u4eba\u7c7b\u5728\u8fd9\u4e2a\u65b0\u8ba4\u77e5\u751f\u6001\u7cfb\u7edf\u4e2d\u4f5c\u4e3a\u5173\u952e\u9a8c\u8bc1\u8005\u7684\u89d2\u8272\u3002"}}
{"id": "2511.12336", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12336", "abs": "https://arxiv.org/abs/2511.12336", "authors": ["Alexander Hammerl", "Ravi Seshadri", "Thomas Kj\u00e6r Rasmussen", "Otto Anker Nielsen"], "title": "Safe and Operationally Efficient Longitudinal Control of Autonomous Truck Platoons", "comment": null, "summary": "This paper presents a hierarchical longitudinal control architecture for autonomous truck platoons that jointly addresses safety, string stability, and economic efficiency. The framework integrates a high-rate safety projection filter, a spacing-regulation layer based on a lag-aware proportional-integral-derivative (PID) controller, and a slow-timescale economic optimizer balancing fuel consumption and travel time. The safety layer guarantees collision avoidance under bounded actuation delays by enforcing forward invariance of a velocity-aware headway constraint through a high-order control barrier function. The regulation layer shapes the spacing-error dynamics into a second-order form with interpretable parameters for damping and natural frequency while explicitly accounting for actuator lag. At the macroscopic level, fuel use is modeled by a tractive-power relation that captures aerodynamic benefits of close spacing, enabling a long-term optimization of speed trajectories subject to comfort and energy trade-offs. We show that the closed-loop dynamics converge to the Optimal Velocity Model with Relative Velocity (OVRV) under undisturbed conditions and derive worst-case upper bounds for platoon stabilization time. Numerical case studies demonstrate the superiority of the proposed design over an canonical baseline controllers in both transient behavior and long-term energy efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u8f66\u961f\u7684\u5c42\u6b21\u5316\u7eb5\u5411\u63a7\u5236\u67b6\u6784\uff0c\u540c\u65f6\u89e3\u51b3\u5b89\u5168\u6027\u3001\u961f\u5217\u7a33\u5b9a\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u95ee\u9898\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u9ad8\u901f\u7387\u5b89\u5168\u6295\u5f71\u6ee4\u6ce2\u5668\u3001\u57fa\u4e8e\u6ede\u540e\u611f\u77e5PID\u63a7\u5236\u5668\u7684\u95f4\u8ddd\u8c03\u8282\u5c42\uff0c\u4ee5\u53ca\u6162\u65f6\u95f4\u5c3a\u5ea6\u7ecf\u6d4e\u4f18\u5316\u5668\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u8f66\u961f\u5728\u5b89\u5168\u3001\u7a33\u5b9a\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u65b9\u9762\u7684\u7efc\u5408\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u4fdd\u8bc1\u78b0\u649e\u907f\u514d\u3001\u961f\u5217\u7a33\u5b9a\u6027\u548c\u71c3\u6cb9\u7ecf\u6d4e\u6027\u7684\u63a7\u5236\u67b6\u6784\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u63a7\u5236\u67b6\u6784\uff1a1\uff09\u9ad8\u901f\u7387\u5b89\u5168\u6295\u5f71\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u4fdd\u8bc1\u78b0\u649e\u907f\u514d\uff1b2\uff09\u95f4\u8ddd\u8c03\u8282\u5c42\uff0c\u4f7f\u7528\u6ede\u540e\u611f\u77e5PID\u63a7\u5236\u5668\uff1b3\uff09\u6162\u65f6\u95f4\u5c3a\u5ea6\u7ecf\u6d4e\u4f18\u5316\u5668\uff0c\u57fa\u4e8e\u7275\u5f15\u529f\u7387\u5173\u7cfb\u4f18\u5316\u901f\u5ea6\u8f68\u8ff9\u3002", "result": "\u5728\u65e0\u5e72\u6270\u6761\u4ef6\u4e0b\uff0c\u95ed\u73af\u52a8\u529b\u5b66\u6536\u655b\u5230\u6700\u4f18\u901f\u5ea6\u6a21\u578b\uff0c\u5e76\u63a8\u5bfc\u4e86\u8f66\u961f\u7a33\u5b9a\u65f6\u95f4\u7684\u6700\u574f\u60c5\u51b5\u4e0a\u754c\u3002\u6570\u503c\u6848\u4f8b\u7814\u7a76\u8868\u660e\u8be5\u8bbe\u8ba1\u5728\u77ac\u6001\u884c\u4e3a\u548c\u957f\u671f\u80fd\u6548\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u63a7\u5236\u5668\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5c42\u6b21\u5316\u63a7\u5236\u67b6\u6784\u80fd\u591f\u6709\u6548\u5e73\u8861\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u8f66\u961f\u7684\u5b89\u5168\u6027\u3001\u7a33\u5b9a\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u9700\u6c42\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.11922", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11922", "abs": "https://arxiv.org/abs/2511.11922", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "title": "Additive Large Language Models for Semi-Structured Text", "comment": null, "summary": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.", "AI": {"tldr": "CALM\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u5206\u89e3\u4e3a\u5404\u8bed\u4e49\u7ec4\u4ef6\u7684\u52a0\u6027\u8d21\u732e\uff0c\u63d0\u4f9b\u900f\u660e\u7684\u98ce\u9669\u4fe1\u53f7\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\u4e2d\u9884\u6d4b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u8ba9\u7814\u7a76\u4eba\u5458\u548c\u533b\u751f\u80fd\u591f\u7406\u89e3\u60a3\u8005\u8bb0\u5f55\u4e2d\u54ea\u4e9b\u90e8\u5206\u9a71\u52a8\u98ce\u9669\u4fe1\u53f7\u3002", "method": "\u91c7\u7528\u52a0\u6027\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u534a\u7ed3\u6784\u5316\u6587\u672c\u8f93\u5165\u5206\u89e3\u4e3a\u8bed\u4e49\u7ec4\u4ef6\uff0c\u9884\u6d4b\u7ed3\u679c\u662f\u5404\u7ec4\u4ef6\u8d21\u732e\u7684\u52a0\u6027\u603b\u548c\u3002", "result": "CALM\u5728\u6027\u80fd\u4e0a\u4e0e\u5e38\u89c4LLM\u5206\u7c7b\u5668\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u4fe1\u5ea6\uff0c\u652f\u6301\u8d28\u91cf\u4fdd\u8bc1\u68c0\u67e5\uff0c\u5e76\u5728\u6a21\u578b\u5f00\u53d1\u548c\u5ba1\u8ba1\u4e2d\u63ed\u793a\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u6a21\u5f0f\u3002", "conclusion": "CALM\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u4fe1\u4efb\u5efa\u7acb\u548c\u8d28\u91cf\u63a7\u5236\u3002"}}
{"id": "2511.11592", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11592", "abs": "https://arxiv.org/abs/2511.11592", "authors": ["Guojian Zhan", "Likun Wang", "Pengcheng Wang", "Feihong Zhang", "Jingliang Duan", "Masayoshi Tomizuka", "Shengbo Eben Li"], "title": "Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL", "comment": "17 pages", "summary": "Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.", "AI": {"tldr": "\u63d0\u51fa\u8f68\u8ff9\u71b5\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60(TECRL)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5956\u52b1\u548c\u71b5\u7684Q\u51fd\u6570\u5b66\u4e60\uff0c\u89e3\u51b3\u6700\u5927\u71b5RL\u4e2d\u7684\u975e\u5e73\u7a33Q\u503c\u4f30\u8ba1\u548c\u77ed\u89c6\u5c40\u90e8\u71b5\u8c03\u8282\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6700\u5927\u71b5RL\u5b58\u5728\u4e24\u4e2a\u74f6\u9888\uff1a(1)\u8054\u5408\u6ce8\u5165\u71b5\u548c\u66f4\u65b0\u6e29\u5ea6\u53c2\u6570\u5bfc\u81f4Q\u503c\u4f30\u8ba1\u975e\u5e73\u7a33\uff1b(2)\u4ec5\u6839\u636e\u5f53\u524d\u5355\u6b65\u71b5\u8c03\u8282\u6e29\u5ea6\uff0c\u672a\u8003\u8651\u7d2f\u79ef\u71b5\u7684\u957f\u671f\u5f71\u54cd\u3002", "method": "\u63d0\u51faTECRL\u6846\u67b6\uff1a\u5206\u522b\u5b66\u4e60\u5956\u52b1Q\u51fd\u6570\u548c\u71b5Q\u51fd\u6570\uff0c\u786e\u4fdd\u503c\u76ee\u6807\u4e0d\u53d7\u6e29\u5ea6\u66f4\u65b0\u5f71\u54cd\uff1b\u901a\u8fc7\u4e13\u95e8\u7684\u71b5Q\u51fd\u6570\u91cf\u5316\u671f\u671b\u7d2f\u79ef\u71b5\uff0c\u5b9e\u65bd\u8f68\u8ff9\u71b5\u7ea6\u675f\u63a7\u5236\u7b56\u7565\u957f\u671f\u968f\u673a\u6027\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1DSAC-E\u7b97\u6cd5\uff0c\u6269\u5c55DSAC-T\u3002", "result": "\u5728OpenAI Gym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDSAC-E\u80fd\u591f\u83b7\u5f97\u66f4\u9ad8\u56de\u62a5\u548c\u66f4\u597d\u7a33\u5b9a\u6027\u3002", "conclusion": "TECRL\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u5956\u52b1\u548c\u71b5\u7684Q\u51fd\u6570\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6700\u5927\u71b5RL\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.11831", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11831", "abs": "https://arxiv.org/abs/2511.11831", "authors": ["Wenhao Zhou", "Hao Zheng", "Rong Zhao"], "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.", "AI": {"tldr": "TopoPerception\u662f\u4e00\u4e2a\u57fa\u4e8e\u62d3\u6251\u5c5e\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5168\u5c40\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u968f\u673a\u731c\u6d4b\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u5c40\u90e8\u6377\u5f84\u95ee\u9898\uff0c\u4f1a\u9ad8\u4f30\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u3002\u89c6\u89c9\u611f\u77e5\u6a21\u5757\u6210\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5176\u6574\u4f53\u80fd\u529b\u3002", "method": "\u5229\u7528\u62d3\u6251\u5c5e\u6027\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\uff0c\u56e0\u4e3a\u62d3\u6251\u4f9d\u8d56\u4e8e\u56fe\u50cf\u7684\u5168\u5c40\u7ed3\u6784\u4e14\u5bf9\u5c40\u90e8\u7279\u5f81\u4e0d\u53d8\uff0c\u80fd\u591f\u5b9e\u73b0\u65e0\u6377\u5f84\u7684\u5168\u5c40\u611f\u77e5\u8bc4\u4f30\u3002", "result": "\u5728\u6700\u7c97\u7684\u611f\u77e5\u7c92\u5ea6\u4e0a\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u90fd\u4e0d\u4f18\u4e8e\u968f\u673a\u673a\u4f1a\uff0c\u8868\u660e\u6a21\u578b\u7f3a\u4e4f\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u611f\u77e5\u80fd\u529b\u3002\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u53cd\u800c\u51c6\u786e\u7387\u66f4\u4f4e\u3002", "conclusion": "\u4ec5\u6269\u5927\u6a21\u578b\u89c4\u6a21\u4e0d\u8db3\u4ee5\u89e3\u51b3\u5168\u5c40\u611f\u77e5\u7f3a\u9677\uff0c\u53ef\u80fd\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u6216\u67b6\u6784\u3002TopoPerception\u63ed\u793a\u4e86\u5f53\u524dLVLMs\u7684\u5173\u952e\u74f6\u9888\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2511.12484", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12484", "abs": "https://arxiv.org/abs/2511.12484", "authors": ["Xu Yang", "Chenhui Lin", "Haotian Liu", "Qi Wang", "Yue Yang", "Wenchuan Wu"], "title": "One Request, Multiple Experts: LLM Orchestrates Domain Specific Models via Adaptive Task Routing", "comment": null, "summary": "With the integration of massive distributed energy resources and the widespread participation of novel market entities, the operation of active distribution networks (ADNs) is progressively evolving into a complex multi-scenario, multi-objective problem. Although expert engineers have developed numerous domain specific models (DSMs) to address distinct technical problems, mastering, integrating, and orchestrating these heterogeneous DSMs still entail considerable overhead for ADN operators. Therefore, an intelligent approach is urgently required to unify these DSMs and enable efficient coordination. To address this challenge, this paper proposes the ADN-Agent architecture, which leverages a general large language model (LLM) to coordinate multiple DSMs, enabling adaptive intent recognition, task decomposition, and DSM invocation. Within the ADN-Agent, we design a novel communication mechanism that provides a unified and flexible interface for diverse heterogeneous DSMs. Finally, for some language-intensive subtasks, we propose an automated training pipeline for fine-tuning small language models, thereby effectively enhancing the overall problem-solving capability of the system. Comprehensive comparisons and ablation experiments validate the efficacy of the proposed method and demonstrate that the ADN-Agent architecture outperforms existing LLM application paradigms.", "AI": {"tldr": "\u63d0\u51fa\u4e86ADN-Agent\u67b6\u6784\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u534f\u8c03\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u89e3\u51b3\u4e3b\u52a8\u914d\u7535\u7f51\u4e2d\u5f02\u6784\u6a21\u578b\u96c6\u6210\u548c\u534f\u8c03\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u7684\u5927\u89c4\u6a21\u96c6\u6210\u548c\u65b0\u578b\u5e02\u573a\u4e3b\u4f53\u7684\u5e7f\u6cdb\u53c2\u4e0e\uff0c\u4e3b\u52a8\u914d\u7535\u7f51\u8fd0\u884c\u6f14\u53d8\u4e3a\u590d\u6742\u7684\u591a\u573a\u666f\u3001\u591a\u76ee\u6807\u95ee\u9898\uff0c\u9700\u8981\u667a\u80fd\u65b9\u6cd5\u6765\u7edf\u4e00\u534f\u8c03\u5f02\u6784\u7684\u9886\u57df\u7279\u5b9a\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1ADN-Agent\u67b6\u6784\uff0c\u5229\u7528\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u9002\u5e94\u610f\u56fe\u8bc6\u522b\u3001\u4efb\u52a1\u5206\u89e3\u548c\u6a21\u578b\u8c03\u7528\uff0c\u5e76\u8bbe\u8ba1\u7edf\u4e00\u901a\u4fe1\u673a\u5236\u4e3a\u5f02\u6784\u6a21\u578b\u63d0\u4f9b\u7075\u6d3b\u63a5\u53e3\uff0c\u5bf9\u8bed\u8a00\u5bc6\u96c6\u578b\u5b50\u4efb\u52a1\u91c7\u7528\u81ea\u52a8\u5316\u8bad\u7ec3\u7ba1\u9053\u5fae\u8c03\u5c0f\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u7efc\u5408\u6bd4\u8f83\u548c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0cADN-Agent\u67b6\u6784\u4f18\u4e8e\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u8303\u5f0f\u3002", "conclusion": "ADN-Agent\u67b6\u6784\u80fd\u591f\u6709\u6548\u7edf\u4e00\u548c\u534f\u8c03\u4e3b\u52a8\u914d\u7535\u7f51\u4e2d\u7684\u5f02\u6784\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2511.11741", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11741", "abs": "https://arxiv.org/abs/2511.11741", "authors": ["Christopher Mantzaris", "Ajda Fosner"], "title": "Taxation and the relationship between payments and time spent", "comment": "CM presented this research project at the 2025 Benedict College International Multidisciplinary Conference on 2025-03-12", "summary": "Tax work is costly for society: Administrative tax labour is typically to a high degree shuffled off the government and onto every taxpayer by law. The higher the burden of any tax system, the costlier for society, as taxpayers are unable to engage in proper wealth creation when being kept busy with administrative tax work. This research finds evidence for a relationship between hours spent to comply with taxes and amount of tax payment. These findings help better understand tax administrative costs and ultimately may help reduce them. PwC and World Bank's final \"Paying taxes\"-publication (2019) contains tax data for most of the world's jurisdictions, in particular annual hours spent to comply with tax obligations (X) and annual amount of tax payments (Y), both for the year 2019. X and Y were plotted in 6 tests. A positive slope, satisfying p and r values, high mutual information and finally a conclusive scatter plot picture were the 5 requirements that all needed to be met to confirm a positive relationship between X and Y. The first 2 tests did not make any adjustments to the data, the next 2 tests removed cities --thereby avoiding the double counting of jurisdictions-- and the final 2 tests removed cities and outliers. Each test pair uses for Y first total number of payments; and for each second test the number of other payments, which excludes income tax payments for profit and labour. All 5 requirements were met in every of the 6 tests, indicating a positive dependence. In addition, 4 confirmatory tests validate the methodology. The found relationship is noticeably stronger for the total number of tax payments. Findings indicate that taxpayers' time spent on tax, and thereby society's overall tax administrative costs, could be reduced by simplifying taxation processes, including tax collection and payments.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7eb3\u7a0e\u5408\u89c4\u65f6\u95f4\u4e0e\u7eb3\u7a0e\u91d1\u989d\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u8868\u660e\u7b80\u5316\u7a0e\u6536\u6d41\u7a0b\u53ef\u4ee5\u964d\u4f4e\u793e\u4f1a\u7a0e\u6536\u7ba1\u7406\u6210\u672c", "motivation": "\u7a0e\u6536\u5de5\u4f5c\u5bf9\u793e\u4f1a\u6210\u672c\u9ad8\u6602\uff0c\u7eb3\u7a0e\u4eba\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u5904\u7406\u884c\u653f\u6027\u7a0e\u52a1\u5de5\u4f5c\uff0c\u65e0\u6cd5\u4e13\u6ce8\u4e8e\u8d22\u5bcc\u521b\u9020\u3002\u7406\u89e3\u7a0e\u6536\u5408\u89c4\u65f6\u95f4\u4e0e\u7eb3\u7a0e\u91d1\u989d\u7684\u5173\u7cfb\u6709\u52a9\u4e8e\u964d\u4f4e\u7a0e\u6536\u7ba1\u7406\u6210\u672c", "method": "\u4f7f\u7528PwC\u548c\u4e16\u754c\u94f6\u884c2019\u5e74\u7a0e\u6536\u6570\u636e\uff0c\u5206\u6790\u7eb3\u7a0e\u5408\u89c4\u65f6\u95f4(X)\u4e0e\u7eb3\u7a0e\u91d1\u989d(Y)\u7684\u5173\u7cfb\u3002\u901a\u8fc76\u7ec4\u6d4b\u8bd5\u9a8c\u8bc1\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u6bcf\u7ec4\u6d4b\u8bd5\u9700\u6ee1\u8db35\u4e2a\u8981\u6c42\uff1a\u6b63\u659c\u7387\u3001\u6ee1\u8db3p\u503c\u548cr\u503c\u3001\u9ad8\u4e92\u4fe1\u606f\u3001\u6563\u70b9\u56fe\u652f\u6301\u3002\u6d4b\u8bd5\u5305\u62ec\u539f\u59cb\u6570\u636e\u3001\u53bb\u9664\u57ce\u5e02\u6570\u636e\u3001\u53bb\u9664\u57ce\u5e02\u548c\u5f02\u5e38\u503c\u6570\u636e", "result": "\u6240\u67096\u7ec4\u6d4b\u8bd5\u5747\u6ee1\u8db35\u4e2a\u8981\u6c42\uff0c\u8868\u660e\u7eb3\u7a0e\u5408\u89c4\u65f6\u95f4\u4e0e\u7eb3\u7a0e\u91d1\u989d\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\u3002\u603b\u7eb3\u7a0e\u91d1\u989d\u7684\u76f8\u5173\u6027\u66f4\u5f3a\u30024\u4e2a\u9a8c\u8bc1\u6027\u6d4b\u8bd5\u786e\u8ba4\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u7b80\u5316\u7a0e\u6536\u6d41\u7a0b\uff08\u5305\u62ec\u7a0e\u6536\u5f81\u6536\u548c\u652f\u4ed8\uff09\uff0c\u53ef\u4ee5\u51cf\u5c11\u7eb3\u7a0e\u4eba\u82b1\u8d39\u5728\u7a0e\u52a1\u4e0a\u7684\u65f6\u95f4\uff0c\u4ece\u800c\u964d\u4f4e\u793e\u4f1a\u7684\u6574\u4f53\u7a0e\u6536\u7ba1\u7406\u6210\u672c"}}
{"id": "2511.12395", "categories": ["math.OC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.12395", "abs": "https://arxiv.org/abs/2511.12395", "authors": ["Christina B\u00fcsing", "Maurice Draeger", "Corinna Mathwieser"], "title": "Parameterized complexity of scheduling unit-time jobs with generalized precedence constraints", "comment": "IPEC 2025", "summary": "We study the parameterized complexity of scheduling unit-time jobs on parallel, identical machines under generalized precedence constraints for minimization of the makespan and the sum of completion times. In our setting, each job is equipped with a Boolean formula (precedence constraint) over the set of jobs. A schedule satisfies a job's precedence constraint if setting earlier jobs to true satisfies the formula. Our definition generalizes several common types of precedence constraints: classical and-constraints if every formula is a conjunction, or-constraints if every formula is a disjunction, and and/or-constraints if every formula is in conjunctive normal form. We prove fixed-parameter tractability when parameterizing by the number of predecessors. For parameterization by the number of successors, however, the complexity depends on the structure of the precedence constraints. If every constraint is a conjunction or a disjunction, we prove the problem to be fixed-parameter tractable. For constraints in disjunctive normal form, we prove W[1]-hardness. We show that the and/or-constrained problem is NP-hard, even for a single successor. Moreover, we prove NP-hardness on two machines if every constraint is a conjunction or a disjunction. This result not only proves para-NP-hardness for parameterization by the number of machines but also complements the polynomial-time solvability on two machines if every constraint is a conjunction (Coffman and Graham 1972) or if every constraint is a disjunction (Berit 2005).", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u5e7f\u4e49\u4f18\u5148\u7ea6\u675f\u4e0b\u5e76\u884c\u673a\u8c03\u5ea6\u95ee\u9898\u7684\u53c2\u6570\u5316\u590d\u6742\u5ea6\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u7ea6\u675f\u7c7b\u578b\u5bf9\u6700\u5c0f\u5316makespan\u548c\u5b8c\u6210\u65f6\u95f4\u603b\u548c\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u7684\u8c03\u5ea6\u95ee\u9898\u4e3b\u8981\u5173\u6ce8AND\u6216OR\u7ea6\u675f\uff0c\u672c\u6587\u6269\u5c55\u7814\u7a76\u66f4\u590d\u6742\u7684\u5e03\u5c14\u516c\u5f0f\u7ea6\u675f\uff0c\u63a2\u7d22\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u8c03\u5ea6\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u53c2\u6570\u5316\u590d\u6742\u5ea6\u7406\u8bba\uff0c\u5206\u522b\u4ee5\u524d\u9a71\u6570\u91cf\u548c\u540e\u7ee7\u6570\u91cf\u4f5c\u4e3a\u53c2\u6570\uff0c\u5206\u6790\u4e0d\u540c\u7ea6\u675f\u7c7b\u578b\uff08\u5408\u53d6\u3001\u6790\u53d6\u3001\u5408\u53d6\u8303\u5f0f\u3001\u6790\u53d6\u8303\u5f0f\uff09\u4e0b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u524d\u9a71\u6570\u91cf\u4f5c\u4e3a\u53c2\u6570\u65f6\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\uff1b\u540e\u7ee7\u6570\u91cf\u4f5c\u4e3a\u53c2\u6570\u65f6\uff0c\u5408\u53d6/\u6790\u53d6\u7ea6\u675f\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\uff0c\u6790\u53d6\u8303\u5f0f\u7ea6\u675f\u4e3aW[1]-\u96be\uff1bAND/OR\u7ea6\u675f\u5373\u4f7f\u53ea\u6709\u4e00\u4e2a\u540e\u7ee7\u4e5f\u662fNP\u96be\uff1b\u4e24\u673a\u60c5\u51b5\u4e0b\u5408\u53d6/\u6790\u53d6\u7ea6\u675f\u4e5f\u662fNP\u96be\u3002", "conclusion": "\u5e7f\u4e49\u4f18\u5148\u7ea6\u675f\u8c03\u5ea6\u95ee\u9898\u7684\u590d\u6742\u5ea6\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7ea6\u675f\u7c7b\u578b\u548c\u53c2\u6570\u9009\u62e9\uff0c\u4e3a\u4f20\u7edf\u8c03\u5ea6\u95ee\u9898\u7684\u590d\u6742\u5ea6\u8fb9\u754c\u63d0\u4f9b\u4e86\u91cd\u8981\u8865\u5145\u3002"}}
{"id": "2511.11933", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11933", "abs": "https://arxiv.org/abs/2511.11933", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "Bhuwan Dhingra", "David Edwin Carlson"], "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis", "comment": null, "summary": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.", "AI": {"tldr": "\u63d0\u51fa\u4e86InData\u6570\u636e\u96c6\u6765\u8bc4\u4f30LLMs\u5728\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u89e3\u51b3LLM\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u8bbf\u95ee\u654f\u611f\u6570\u636e\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u5b89\u5168\u5de5\u5177\u6765\u9650\u5236\u6570\u636e\u8bbf\u95ee\u3002", "method": "\u8bbe\u8ba1InData\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684\u6570\u636e\u5206\u6790\u95ee\u9898\uff0c\u8bc4\u4f3015\u4e2a\u5f00\u6e90LLM\u7684\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5927\u578b\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0897.3%\uff09\uff0c\u4f46\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0869.6%\uff09\u3002", "conclusion": "\u5f53\u524dLLM\u7f3a\u4e4f\u7a33\u5065\u7684\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u80fd\u529b\uff0cInData\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u66f4\u5f3a\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002"}}
{"id": "2511.11593", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11593", "abs": "https://arxiv.org/abs/2511.11593", "authors": ["Matthew Morris", "Ian Horrocks"], "title": "Sound Logical Explanations for Mean Aggregation Graph Neural Networks", "comment": "Full version (with appendices) of paper accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)", "summary": "Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5747\u503c\u805a\u5408\u548c\u975e\u8d1f\u6743\u91cd\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08MAGNNs\uff09\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u80fd\u591f\u652f\u6301\u7684\u5355\u8c03\u89c4\u5219\u7c7b\u522b\uff0c\u5e76\u63d0\u4f9b\u4e86\u7528\u4e8e\u89e3\u91caMAGNN\u9884\u6d4b\u7684\u4e00\u9636\u903b\u8f91\u7247\u6bb5\u3002\u5b9e\u9a8c\u8868\u660e\u8fd9\u79cd\u9650\u5236\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5e76\u80fd\u751f\u6210\u6709\u6d1e\u5bdf\u529b\u7684\u89e3\u91ca\u3002", "motivation": "\u5c3d\u7ba1\u4f7f\u7528\u5747\u503c\u805a\u5408\u7684GNN\u5f88\u666e\u904d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u53ef\u89e3\u91ca\u6027\u548c\u8868\u8fbe\u80fd\u529b\u7684\u7406\u8bba\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7279\u522b\u5173\u6ce8\u5747\u503c\u805a\u5408\u548c\u975e\u8d1f\u6743\u91cd\u7684GNN\u3002", "method": "\u63d0\u51faMAGNNs\uff08\u4f7f\u7528\u5747\u503c\u805a\u5408\u548c\u975e\u8d1f\u6743\u91cd\u7684GNN\uff09\uff0c\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u5b83\u4eec\u652f\u6301\u7684\u5355\u8c03\u89c4\u5219\u7c7b\u522b\uff0c\u5e76\u6784\u5efa\u7528\u4e8e\u89e3\u91ca\u9884\u6d4b\u7684\u4e00\u9636\u903b\u8f91\u7247\u6bb5\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u9650\u5236GNN\u4f7f\u7528\u975e\u8d1f\u6743\u91cd\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u597d\uff1b\u5728\u5b9e\u8df5\u4e2d\u80fd\u591f\u83b7\u5f97\u53ef\u9760\u7684\u89c4\u5219\uff1b\u53ef\u4ee5\u751f\u6210\u6709\u6d1e\u5bdf\u529b\u7684\u89e3\u91ca\uff1b\u53ef\u9760\u7684\u89c4\u5219\u80fd\u591f\u66b4\u9732\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u95ee\u9898\u3002", "conclusion": "MAGNNs\u4e0d\u4ec5\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u53ef\u9760\u7684\u89e3\u91ca\u5e76\u5e2e\u52a9\u8bc6\u522b\u6a21\u578b\u95ee\u9898\u3002"}}
{"id": "2511.11899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11899", "abs": "https://arxiv.org/abs/2511.11899", "authors": ["Xi Li", "Nicholas Matsumoto", "Ujjwal Pasupulety", "Atharva Deo", "Cherine Yang", "Jay Moran", "Miguel E. Hernandez", "Peter Wager", "Jasmine Lin", "Jeanine Kim", "Alvin C. Goh", "Christian Wagner", "Geoffrey A. Sonn", "Andrew J. Hung"], "title": "End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction", "comment": null, "summary": "Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.", "AI": {"tldr": "F2O\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u7ec4\u7ec7\u89e3\u5256\u89c6\u9891\u8f6c\u6362\u4e3a\u624b\u52bf\u5e8f\u5217\uff0c\u5e76\u53d1\u73b0\u4e0e\u672f\u540e\u7ed3\u679c\u76f8\u5173\u7684\u6a21\u5f0f\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u624b\u672f\u53cd\u9988\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u672f\u4e2d\u884c\u4e3a\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u53ca\u5176\u5bf9\u60a3\u8005\u7ed3\u679c\u7684\u5f71\u54cd\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u57fa\u4e8etransformer\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5efa\u6a21\u4ee5\u53ca\u9010\u5e27\u5206\u7c7b\uff0cF2O\u5728\u673a\u5668\u4eba\u8f85\u52a9\u6839\u6cbb\u6027\u524d\u5217\u817a\u5207\u9664\u672f\u7684\u795e\u7ecf\u4fdd\u7559\u6b65\u9aa4\u4e2d\u7a33\u5065\u68c0\u6d4b\u8fde\u7eed\u77ed\u624b\u52bf\u3002", "result": "F2O\u5728\u5e27\u7ea7\u548c\u89c6\u9891\u7ea7\u7684\u624b\u52bf\u68c0\u6d4bAUC\u5206\u522b\u8fbe\u52300.80\u548c0.81\uff1bF2O\u884d\u751f\u7684\u7279\u5f81\u9884\u6d4b\u672f\u540e\u7ed3\u679c\u7684\u51c6\u786e\u6027\u4e0e\u4eba\u5de5\u6ce8\u91ca\u76f8\u5f53\uff080.79 vs 0.75\uff09\u3002", "conclusion": "F2O\u901a\u8fc7\u5b9e\u73b0\u81ea\u52a8\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u624b\u672f\u53cd\u9988\u548c\u524d\u77bb\u6027\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12492", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12492", "abs": "https://arxiv.org/abs/2511.12492", "authors": ["Sungjun Seo", "Kooktae Lee"], "title": "Density-Driven Multi-Agent Coordination for Efficient Farm Coverage and Management in Smart Agriculture", "comment": "Author Accepted Manuscript (AAM) of a paper accepted for publication in the IEEE Transactions on Control Systems Technology (TCST)", "summary": "The growing scale of modern farms has increased the need for efficient and adaptive multi-agent coverage strategies for pest, weed, and disease management. Traditional methods such as manual inspection and blanket pesticide spraying often lead to excessive chemical use, resource waste, and environmental impact. While unmanned aerial vehicles (UAVs) offer a promising platform for precision agriculture through targeted spraying and improved operational efficiency, existing UAV-based approaches remain limited by battery life, payload capacity, and scalability, especially in large fields where single-UAV or uniformly distributed spraying is insufficient. Although multi-UAV coordination has been explored, many current frameworks still assume uniform spraying and do not account for infestation severity, UAV dynamics, non-uniform resource allocation, or energy-efficient coordination.\n  To address these limitations, this paper proposes a Density-Driven Optimal Control (D2OC) framework that integrates Optimal Transport (OT) theory with multi-UAV coverage control for large-scale agricultural spraying. The method supports non-uniform, priority-aware resource allocation based on infestation intensity, reducing unnecessary chemical application. UAVs are modeled as a linear time-varying (LTV) system to capture variations in mass and inertia during spraying missions. The D2OC control law, derived using Lagrangian mechanics, enables efficient coordination, balanced workload distribution, and improved mission duration. Simulation results demonstrate that the proposed approach outperforms uniform spraying and Spectral Multiscale Coverage (SMC) in coverage efficiency, chemical reduction, and operational sustainability, providing a scalable solution for smart agriculture.", "AI": {"tldr": "\u63d0\u51faD2OC\u6846\u67b6\uff0c\u5c06\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u4e0e\u591a\u65e0\u4eba\u673a\u8986\u76d6\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u57fa\u4e8e\u866b\u5bb3\u5f3a\u5ea6\u7684\u975e\u5747\u5300\u3001\u4f18\u5148\u7ea7\u611f\u77e5\u7684\u519c\u4e1a\u55b7\u6d12\uff0c\u51cf\u5c11\u5316\u5b66\u54c1\u4f7f\u7528\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3\u519c\u573a\u89c4\u6a21\u6269\u5927\u9700\u8981\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u8986\u76d6\u7b56\u7565\uff0c\u4f20\u7edf\u65b9\u6cd5\u5bfc\u81f4\u5316\u5b66\u54c1\u8fc7\u5ea6\u4f7f\u7528\u548c\u8d44\u6e90\u6d6a\u8d39\uff0c\u73b0\u6709\u65e0\u4eba\u673a\u65b9\u6cd5\u53d7\u9650\u4e8e\u7535\u6c60\u5bff\u547d\u3001\u6709\u6548\u8f7d\u8377\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e14\u672a\u8003\u8651\u866b\u5bb3\u4e25\u91cd\u7a0b\u5ea6\u3002", "method": "\u96c6\u6210\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u4e0e\u591a\u65e0\u4eba\u673a\u8986\u76d6\u63a7\u5236\uff0c\u5c06\u65e0\u4eba\u673a\u5efa\u6a21\u4e3a\u7ebf\u6027\u65f6\u53d8\u7cfb\u7edf\u4ee5\u6355\u6349\u55b7\u6d12\u4efb\u52a1\u4e2d\u7684\u8d28\u91cf\u548c\u60ef\u6027\u53d8\u5316\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u529b\u5b66\u63a8\u5bfcD2OC\u63a7\u5236\u5f8b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8986\u76d6\u6548\u7387\u3001\u5316\u5b66\u54c1\u51cf\u5c11\u548c\u64cd\u4f5c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u4f18\u4e8e\u5747\u5300\u55b7\u6d12\u548c\u8c31\u591a\u5c3a\u5ea6\u8986\u76d6\u65b9\u6cd5\u3002", "conclusion": "D2OC\u6846\u67b6\u4e3a\u667a\u80fd\u519c\u4e1a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u534f\u8c03\u3001\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u548c\u6539\u5584\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u3002"}}
{"id": "2511.11755", "categories": ["cs.CY", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.11755", "abs": "https://arxiv.org/abs/2511.11755", "authors": ["Isadora Cristina", "Ramon Gonze", "J\u00f4natas Santos", "Julio Reis", "M\u00e1rio Alvim", "Bernardo Queiroz", "Fabr\u00edcio Benevenuto"], "title": "Brazil Data Commons: A Platform for Unifying and Integrating Brazil's Public Data", "comment": null, "summary": "The fragmentation of public data in Brazil, coupled with inconsistent standards and limited interoperability, hinders effective research, evidence-based policymaking and access to data-driven insights. To address these issues, we introduce Brazil Data Commons, a platform that unifies various Brazilian datasets under a common semantic framework, enabling the seamless discovery, integration and visualization of information from different domains. By adopting globally recognized ontologies and interoperable data standards, Brazil Data Commons aligns with the principles of the broader Data Commons ecosystem and places Brazilian data in a global context. Through user-friendly interfaces, straightforward query mechanisms and flexible data access options, the platform democratizes data use and enables researchers, policy makers, and the public to gain meaningful insights and make informed decisions. This paper illustrates how Brazil Data Commons transforms scattered datasets into an integrated and easily navigable resource that allows a deeper understanding of Brazil's complex social, economic and environmental landscape.", "AI": {"tldr": "\u5df4\u897f\u6570\u636e\u5171\u4eab\u5e73\u53f0\u901a\u8fc7\u7edf\u4e00\u8bed\u4e49\u6846\u67b6\u6574\u5408\u5206\u6563\u7684\u5df4\u897f\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u6570\u636e\u788e\u7247\u5316\u548c\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u4fc3\u8fdb\u7814\u7a76\u548c\u653f\u7b56\u5236\u5b9a\u3002", "motivation": "\u5df4\u897f\u516c\u5171\u6570\u636e\u5206\u6563\u3001\u6807\u51c6\u4e0d\u4e00\u81f4\u4e14\u4e92\u64cd\u4f5c\u6027\u6709\u9650\uff0c\u963b\u788d\u4e86\u6709\u6548\u7814\u7a76\u3001\u5faa\u8bc1\u51b3\u7b56\u548c\u6570\u636e\u9a71\u52a8\u6d1e\u5bdf\u3002", "method": "\u91c7\u7528\u5168\u7403\u516c\u8ba4\u7684\u672c\u4f53\u8bba\u548c\u4e92\u64cd\u4f5c\u6570\u636e\u6807\u51c6\uff0c\u5728\u7edf\u4e00\u8bed\u4e49\u6846\u67b6\u4e0b\u6574\u5408\u5404\u79cd\u5df4\u897f\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u754c\u9762\u548c\u7075\u6d3b\u6570\u636e\u8bbf\u95ee\u9009\u9879\u3002", "result": "\u5e73\u53f0\u5c06\u5206\u6563\u7684\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u96c6\u6210\u4e14\u6613\u4e8e\u5bfc\u822a\u7684\u8d44\u6e90\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u516c\u4f17\u80fd\u591f\u83b7\u5f97\u6709\u610f\u4e49\u7684\u6d1e\u5bdf\u5e76\u505a\u51fa\u660e\u667a\u51b3\u7b56\u3002", "conclusion": "\u5df4\u897f\u6570\u636e\u5171\u4eab\u5e73\u53f0\u901a\u8fc7\u7edf\u4e00\u8bed\u4e49\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6570\u636e\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e3a\u7406\u89e3\u5df4\u897f\u590d\u6742\u7684\u793e\u4f1a\u3001\u7ecf\u6d4e\u548c\u73af\u5883\u666f\u89c2\u63d0\u4f9b\u4e86\u96c6\u6210\u8d44\u6e90\u3002"}}
{"id": "2511.12437", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12437", "abs": "https://arxiv.org/abs/2511.12437", "authors": ["Ningji Wei"], "title": "Set System Approximation for Binary Integer Programs: Reformulations and Applications", "comment": null, "summary": "Covering and elimination inequalities are central to combinatorial optimization, yet their role has largely been studied in problem-specific settings or via no-good cuts. This paper introduces a unified perspective that treats these inequalities as primitives for set system approximation in binary integer programs (BIPs). We show that arbitrary set systems admit tight inner and outer monotone approximations, exactly corresponding to covering and elimination inequalities. Building on this, we develop a toolkit that both recovers classical structural correspondences (e.g., paths vs. cuts, spanning trees vs. cycles) and extends polyhedral tools from set covering to general BIPs, including facet conditions and lifting methods. We also propose new reformulation techniques for nonlinear and latent monotone systems, such as auxiliary-variable-free bilinear linearization, bimonotone cuts, and interval decompositions. A case study on distributionally robust network site selection illustrates the framework's flexibility and computational benefits. Overall, this unified view clarifies inner/outer approximation criteria, extends classical polyhedral analysis, and provides broadly applicable reformulation strategies for nonlinear BIPs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u8986\u76d6\u4e0d\u7b49\u5f0f\u548c\u6d88\u9664\u4e0d\u7b49\u5f0f\u89c6\u4e3a\u4e8c\u5143\u6574\u6570\u89c4\u5212\u4e2d\u96c6\u5408\u7cfb\u7edf\u903c\u8fd1\u7684\u57fa\u672c\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u591a\u9762\u4f53\u5206\u6790\u5de5\u5177\u5e76\u63d0\u4f9b\u4e86\u975e\u7ebf\u6027BIP\u7684\u91cd\u65b0\u8868\u8ff0\u7b56\u7565\u3002", "motivation": "\u8986\u76d6\u4e0d\u7b49\u5f0f\u548c\u6d88\u9664\u4e0d\u7b49\u5f0f\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u7279\u5b9a\u95ee\u9898\u8bbe\u7f6e\u6216\u65e0\u826f\u597d\u5207\u5272\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u89c6\u89d2\uff0c\u5c06\u8fd9\u4e9b\u4e0d\u7b49\u5f0f\u4f5c\u4e3a\u4e8c\u5143\u6574\u6570\u89c4\u5212\u4e2d\u96c6\u5408\u7cfb\u7edf\u903c\u8fd1\u7684\u57fa\u672c\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u5957\u5de5\u5177\u5305\uff0c\u5305\u62ec\uff1a\u6062\u590d\u7ecf\u5178\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\uff0c\u5c06\u96c6\u5408\u8986\u76d6\u7684\u591a\u9762\u4f53\u5de5\u5177\u6269\u5c55\u5230\u4e00\u822cBIP\uff0c\u63d0\u51fa\u65b0\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u91cd\u65b0\u8868\u8ff0\u6280\u672f\uff08\u5982\u65e0\u8f85\u52a9\u53d8\u91cf\u7684\u53cc\u7ebf\u6027\u7ebf\u6027\u5316\u3001\u53cc\u5355\u8c03\u5207\u5272\u548c\u533a\u95f4\u5206\u89e3\uff09\u3002", "result": "\u8bc1\u660e\u4e86\u4efb\u610f\u96c6\u5408\u7cfb\u7edf\u90fd\u5141\u8bb8\u7d27\u5bc6\u7684\u5185\u5916\u5355\u8c03\u903c\u8fd1\uff0c\u7cbe\u786e\u5bf9\u5e94\u8986\u76d6\u548c\u6d88\u9664\u4e0d\u7b49\u5f0f\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u9c81\u68d2\u7f51\u7edc\u9009\u5740\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u548c\u8ba1\u7b97\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u4e00\u7edf\u4e00\u89c6\u89d2\u6f84\u6e05\u4e86\u5185\u5916\u903c\u8fd1\u6807\u51c6\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u591a\u9762\u4f53\u5206\u6790\uff0c\u5e76\u4e3a\u975e\u7ebf\u6027BIP\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u9002\u7528\u7684\u91cd\u65b0\u8868\u8ff0\u7b56\u7565\u3002"}}
{"id": "2511.11946", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11946", "abs": "https://arxiv.org/abs/2511.11946", "authors": ["Hadi Sheikhi", "Chenyang Huang", "Osmar R. Za\u00efane"], "title": "Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization", "comment": null, "summary": "Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLM-KAT\u8bc4\u4f30\u65b9\u6cd5\u548c\u5b9e\u4f53\u533f\u540d\u5316\u6280\u672f\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4efb\u52a1\u4e2d\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u5229\u7528\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4efb\u52a1\u4e2d\u5f80\u5f80\u8fc7\u5ea6\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\uff0c\u800c\u5ffd\u89c6\u63d0\u4f9b\u7684\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u56de\u7b54\u4e0e\u5916\u90e8\u77e5\u8bc6\u8131\u8282\u3002", "method": "\u5f15\u5165LLM-KAT\u8bc4\u4f30\u7a0b\u5e8f\u6765\u8861\u91cf\u751f\u6210\u56de\u7b54\u4e2d\u7684\u77e5\u8bc6\u9644\u7740\u5ea6\uff0c\u5e76\u63d0\u51fa\u7b80\u5355\u7684\u5b9e\u4f53\u533f\u540d\u5316\u6280\u672f\u6765\u9f13\u52b1LLM\u66f4\u597d\u5730\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u3002", "result": "\u5728OpenDialKG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u9644\u7740\u5ea6\u3002", "conclusion": "\u5b9e\u4f53\u533f\u540d\u5316\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4efb\u52a1\u4e2d\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u5229\u7528\u6548\u679c\u3002"}}
{"id": "2511.11596", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11596", "abs": "https://arxiv.org/abs/2511.11596", "authors": ["Javier Mar\u00edn"], "title": "Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach", "comment": null, "summary": "Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86LGD\u5efa\u6a21\u4e2d90%\u8bad\u7ec3\u6570\u636e\u4e3a\u4ee3\u7406\u4f30\u8ba1\u503c\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u7b49\u9012\u5f52\u5212\u5206\u65b9\u6cd5\u8868\u73b0\u5dee\uff08r\u00b2=-0.664\uff09\uff0c\u800c\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff08r\u00b2=0.191\uff09\u3002\u7814\u7a76\u53d1\u73b0\u6760\u6746\u7279\u5f81\u6bd4\u89c4\u6a21\u6548\u5e94\u66f4\u5177\u4fe1\u606f\u4ef7\u503c\u3002", "motivation": "LGD\u5efa\u6a21\u9762\u4e34\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff1a90%\u7684\u8bad\u7ec3\u6570\u636e\u662f\u57fa\u4e8e\u7834\u4ea7\u524d\u8d44\u4ea7\u8d1f\u503a\u8868\u7684\u4ee3\u7406\u4f30\u8ba1\uff0c\u800c\u975e\u5b9e\u9645\u56de\u6536\u7ed3\u679c\u3002\u8fd9\u79cd\u6df7\u5408\u6c61\u67d3\u7684\u8bad\u7ec3\u7ed3\u6784\u5bfc\u81f4\u4f20\u7edf\u65b9\u6cd5\u5931\u6548\u3002", "method": "\u4f7f\u7528\u4fe1\u606f\u8bba\u65b9\u6cd5\uff08\u9999\u519c\u71b5\u548c\u4e92\u4fe1\u606f\uff09\u5206\u67901218\u4e2a\u4f01\u4e1a\u7834\u4ea7\u6848\u4f8b\uff081980-2023\uff09\uff0c\u6bd4\u8f83\u4e86\u9012\u5f52\u5212\u5206\u65b9\u6cd5\uff08\u5982\u968f\u673a\u68ee\u6797\uff09\u4e0e\u4fe1\u606f\u8bba\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u968f\u673a\u68ee\u6797\u5728\u6d4b\u8bd5\u6570\u636e\u4e0ar\u00b2\u4e3a-0.664\uff08\u6bd4\u9884\u6d4b\u5747\u503c\u66f4\u5dee\uff09\uff0c\u800c\u4fe1\u606f\u8bba\u65b9\u6cd5r\u00b2\u8fbe\u52300.191\uff0cRMSE\u4e3a0.284\u3002\u6760\u6746\u7279\u5f81\u5305\u542b1.510\u6bd4\u7279\u4e92\u4fe1\u606f\uff0c\u89c4\u6a21\u6548\u5e94\u4ec50.086\u6bd4\u7279\u3002", "conclusion": "\u5728\u5df4\u585e\u5c14III\u8981\u6c42\u4e0b\uff0c\u5f53\u4ee3\u8868\u6027\u7ed3\u679c\u6570\u636e\u4e0d\u8db3\u65f6\uff0c\u4fe1\u606f\u8bba\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u9002\u7528\u4e8e\u533b\u7597\u7ed3\u679c\u7814\u7a76\u3001\u6c14\u5019\u9884\u6d4b\u548c\u6280\u672f\u53ef\u9760\u6027\u7b49\u9886\u57df\uff0c\u8fd9\u4e9b\u9886\u57df\u90fd\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6df7\u5408\u7ed3\u6784\u95ee\u9898\u3002"}}
{"id": "2511.11914", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11914", "abs": "https://arxiv.org/abs/2511.11914", "authors": ["Shizhou Xu", "Yuan Ni", "Stefan Broecker", "Thomas Strohmer"], "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization", "comment": null, "summary": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.", "AI": {"tldr": "Forgetting-MarI\u662f\u4e00\u4e2aLLM\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u8fb9\u9645\u4fe1\u606f\u6765\u9009\u62e9\u6027\u79fb\u9664\u5f85\u9057\u5fd8\u6570\u636e\u5bf9\u6a21\u578b\u7684\u989d\u5916\u8d21\u732e\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u6570\u636e\u652f\u6301\u7684\u4fe1\u606f\uff0c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u4e0d\u53ef\u68c0\u6d4b\u6027\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u5728\u4e0d\u65ad\u6269\u5927\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u9700\u8981\u9009\u62e9\u6027\u79fb\u9664\u7279\u5b9a\u6570\u636e\u5bf9\u8bad\u7ec3\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u4ee5\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u548c\u76d1\u7ba1\u5408\u89c4\u8981\u6c42\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u5f15\u5165Forgetting-MarI\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u8fb9\u9645\u4fe1\u606f\u6765\u660e\u786e\u754c\u5b9a\u5e76\u4ec5\u79fb\u9664\u5f85\u9057\u5fd8\u6570\u636e\u5bf9\u6a21\u578b\u7684\u989d\u5916\u8d21\u732e\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u6570\u636e\u652f\u6301\u7684\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u9057\u5fd8\u6548\u679c\uff0c\u5e76\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e00\u8fdb\u5c55\u662f\u4f7fAI\u7cfb\u7edf\u66f4\u5177\u53ef\u63a7\u6027\u3001\u7b26\u5408\u9690\u79c1\u548c\u7248\u6743\u6cd5\u89c4\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.11666", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11666", "abs": "https://arxiv.org/abs/2511.11666", "authors": ["Rajit Rajpal", "Benedict Leimkuhler", "Yuanhao Jiang"], "title": "Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks", "comment": null, "summary": "Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.", "AI": {"tldr": "\u63d0\u51fa\u4e86SA-SGLD\u81ea\u9002\u5e94\u91c7\u6837\u7b97\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u91cd\u7f29\u653e\u81ea\u52a8\u8c03\u6574\u6b65\u957f\uff0c\u5728\u9ad8\u66f2\u7387\u533a\u57df\u7f29\u5c0f\u6b65\u957f\u3001\u5e73\u5766\u533a\u57df\u6269\u5927\u6b65\u957f\uff0c\u63d0\u9ad8\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u540e\u9a8c\u91c7\u6837\u7684\u7a33\u5b9a\u6027\u548c\u6df7\u5408\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u968f\u673a\u68af\u5ea6MCMC\u65b9\u6cd5\u5bf9\u6b65\u957f\u9009\u62e9\u9ad8\u5ea6\u654f\u611f\uff0c\u81ea\u9002\u5e94\u53d8\u4f53\u5982pSGLD\u9700\u8981\u6602\u8d35\u7684\u6563\u5ea6\u6821\u6b63\u9879\u624d\u80fd\u6b63\u786e\u91c7\u6837\u4e0d\u53d8\u6d4b\u5ea6\u3002", "method": "\u57fa\u4e8eSamAdams\u6846\u67b6\u6784\u5efaSA-SGLD\u81ea\u9002\u5e94\u65b9\u6848\uff0c\u4f7f\u7528\u65f6\u95f4\u91cd\u7f29\u653e\u6839\u636e\u76d1\u6d4b\u91cf\uff08\u901a\u5e38\u662f\u5c40\u90e8\u68af\u5ea6\u8303\u6570\uff09\u8c03\u8282\u6b65\u957f\u3002", "result": "\u5728\u9ad8\u66f2\u73872D\u73a9\u5177\u793a\u4f8b\u548c\u4f7f\u7528\u5c16\u9510\u5148\u9a8c\u7684\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u6bd4SGLD\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u540e\u9a8c\u91c7\u6837\u3002", "conclusion": "SA-SGLD\u80fd\u591f\u5728\u4e0d\u5f15\u5165\u504f\u5dee\u7684\u60c5\u51b5\u4e0b\u81ea\u52a8\u8c03\u6574\u6b65\u957f\uff0c\u6539\u5584\u91c7\u6837\u7a33\u5b9a\u6027\u548c\u6df7\u5408\u6548\u7387\u3002"}}
{"id": "2511.12567", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12567", "abs": "https://arxiv.org/abs/2511.12567", "authors": ["Moussa Labbadi", "Denis Efimov"], "title": "On hyperexponential stabilization of a chain of integrators in continuous and discrete time subject to unmatched perturbations", "comment": "15 pages, 2 figures", "summary": "A recursive time-varying state feedback is presented for a chain of integrators with unmatched perturbations in continuous and discrete time. In continuous time, it is shown that hyperexponential convergence is achieved for the first state variable \\(x_1\\), while the second state \\(x_2\\) remains bounded. For the other states, we establish ISS {\\cb property} by saturating the growing {\\cb control} gain. In discrete time, we use implicit Euler discretization to {\\cb preserve} hyperexponential convergence. The main results are demonstrated through several examples of the proposed control laws, illustrating the conditions established for both continuous and discrete-time systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5177\u6709\u4e0d\u5339\u914d\u6270\u52a8\u7684\u79ef\u5206\u5668\u94fe\u7684\u9012\u5f52\u65f6\u53d8\u72b6\u6001\u53cd\u9988\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u8fde\u7eed\u548c\u79bb\u6563\u65f6\u95f4\u4e0b\u5747\u80fd\u5b9e\u73b0\u8d85\u6307\u6570\u6536\u655b", "motivation": "\u9488\u5bf9\u5177\u6709\u4e0d\u5339\u914d\u6270\u52a8\u7684\u79ef\u5206\u5668\u94fe\u7cfb\u7edf\uff0c\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u8d85\u6307\u6570\u6536\u655b\u7684\u63a7\u5236\u7b56\u7565\uff0c\u540c\u65f6\u5728\u8fde\u7eed\u548c\u79bb\u6563\u65f6\u95f4\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027", "method": "\u91c7\u7528\u9012\u5f52\u65f6\u53d8\u72b6\u6001\u53cd\u9988\u63a7\u5236\uff0c\u5728\u8fde\u7eed\u65f6\u95f4\u4e0b\u901a\u8fc7\u9971\u548c\u589e\u957f\u7684\u63a7\u5236\u589e\u76ca\u5b9e\u73b0ISS\u7279\u6027\uff0c\u5728\u79bb\u6563\u65f6\u95f4\u4e0b\u4f7f\u7528\u9690\u5f0f\u6b27\u62c9\u79bb\u6563\u5316\u4fdd\u6301\u8d85\u6307\u6570\u6536\u655b", "result": "\u8fde\u7eed\u65f6\u95f4\u4e0b\u7b2c\u4e00\u4e2a\u72b6\u6001\u53d8\u91cf\u5b9e\u73b0\u8d85\u6307\u6570\u6536\u655b\uff0c\u7b2c\u4e8c\u4e2a\u72b6\u6001\u4fdd\u6301\u6709\u754c\uff0c\u5176\u4ed6\u72b6\u6001\u901a\u8fc7ISS\u7279\u6027\u4fdd\u8bc1\u7a33\u5b9a\u6027\uff1b\u79bb\u6563\u65f6\u95f4\u4e0b\u6210\u529f\u4fdd\u6301\u4e86\u8d85\u6307\u6570\u6536\u655b\u7279\u6027", "conclusion": "\u6240\u63d0\u51fa\u7684\u9012\u5f52\u65f6\u53d8\u72b6\u6001\u53cd\u9988\u63a7\u5236\u65b9\u6cd5\u5728\u8fde\u7eed\u548c\u79bb\u6563\u65f6\u95f4\u4e0b\u5747\u80fd\u6709\u6548\u5904\u7406\u5177\u6709\u4e0d\u5339\u914d\u6270\u52a8\u7684\u79ef\u5206\u5668\u94fe\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u8d85\u6307\u6570\u6536\u655b\u548c\u7a33\u5b9a\u6027"}}
{"id": "2511.11757", "categories": ["cs.CY", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.11757", "abs": "https://arxiv.org/abs/2511.11757", "authors": ["Anya Bardach", "Hamilton Murrah"], "title": "Bridging the Skills Gap: A Course Model for Modern Generative AI Education", "comment": "10 pages, 2 figures, in the 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26) EAAI Symposium", "summary": "Research on how the popularization of generative Artificial Intelligence (AI) tools impacts learning environments has led to hesitancy among educators to teach these tools in classrooms, creating two observed disconnects. Generative AI competency is increasingly valued in industry but not in higher education, and students are experimenting with generative AI without formal guidance. The authors argue students across fields must be taught to responsibly and expertly harness the potential of AI tools to ensure job market readiness and positive outcomes. Computer Science trajectories are particularly impacted, and while consistently top ranked U.S. Computer Science departments teach the mechanisms and frameworks underlying AI, few appear to offer courses on applications for existing generative AI tools. A course was developed at a private research university to teach undergraduate and graduate Computer Science students applications for generative AI tools in software development. Two mixed method surveys indicated students overwhelmingly found the course valuable and effective. Co-authored by the instructor and one of the graduate students, this paper explores the context, implementation, and impact of the course through data analysis and reflections from both perspectives. It additionally offers recommendations for replication in and beyond Computer Science departments. This is the extended version of this paper to include technical appendices.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u666e\u53ca\u5bf9\u5b66\u4e60\u73af\u5883\u7684\u5f71\u54cd\uff0c\u5f00\u53d1\u4e86\u4e00\u95e8\u9762\u5411\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u7684\u751f\u6210\u5f0fAI\u5e94\u7528\u8bfe\u7a0b\uff0c\u5e76\u901a\u8fc7\u8c03\u67e5\u9a8c\u8bc1\u4e86\u8bfe\u7a0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u4e24\u4e2a\u8131\u8282\u95ee\u9898\uff1a\u884c\u4e1a\u91cd\u89c6\u751f\u6210\u5f0fAI\u80fd\u529b\u800c\u9ad8\u7b49\u6559\u80b2\u4e0d\u91cd\u89c6\uff1b\u5b66\u751f\u65e0\u6b63\u5f0f\u6307\u5bfc\u5730\u5b9e\u9a8c\u751f\u6210\u5f0fAI\u3002\u786e\u4fdd\u5b66\u751f\u80fd\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528AI\u5de5\u5177\u4ee5\u9002\u5e94\u5c31\u4e1a\u5e02\u573a\u9700\u6c42\u3002", "method": "\u5728\u4e00\u6240\u79c1\u7acb\u7814\u7a76\u578b\u5927\u5b66\u4e3a\u8ba1\u7b97\u673a\u79d1\u5b66\u672c\u79d1\u751f\u548c\u7814\u7a76\u751f\u5f00\u53d1\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e94\u7528\u7684\u8bfe\u7a0b\uff0c\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8c03\u67e5\u8bc4\u4f30\u8bfe\u7a0b\u6548\u679c\u3002", "result": "\u8c03\u67e5\u663e\u793a\u5b66\u751f\u666e\u904d\u8ba4\u4e3a\u8bfe\u7a0b\u6709\u4ef7\u503c\u4e14\u6709\u6548\uff0c\u8bfe\u7a0b\u6210\u529f\u586b\u8865\u4e86\u751f\u6210\u5f0fAI\u5e94\u7528\u6559\u80b2\u7684\u7a7a\u767d\u3002", "conclusion": "\u8ba1\u7b97\u673a\u79d1\u5b66\u53ca\u5176\u4ed6\u9662\u7cfb\u5e94\u590d\u5236\u6b64\u7c7b\u8bfe\u7a0b\uff0c\u6559\u6388\u5b66\u751f\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528\u73b0\u6709\u751f\u6210\u5f0fAI\u5de5\u5177\uff0c\u786e\u4fdd\u5c31\u4e1a\u5e02\u573a\u51c6\u5907\u5ea6\u548c\u79ef\u6781\u6210\u679c\u3002"}}
{"id": "2511.12476", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12476", "abs": "https://arxiv.org/abs/2511.12476", "authors": ["Bhathiya Divelgama", "Nancy Asare Nyarko", "Naa Sackley Dromo Aryee", "Abootaleb Shirvani", "Svetlozar T. Rachev"], "title": "Performance and Risk Analytics of Asian Exchange-Traded Funds", "comment": null, "summary": "Investing in Asian markets through exchange-traded funds (ETFs) provides investors with access to rapidly expanding economies and valuable diversification opportunities. This study examines the advantages and challenges of investing in Asian ETFs by conducting comprehensive risk assessments, portfolio analyses, and performance comparisons. The dataset comprises 29 ETFs offering exposure across a wide spectrum of Asian markets, including broad regional funds, country-specific ETFs, as well as sector-focused funds, dividend-oriented ETFs, small-cap portfolios, and emerging market bond ETFs.\n  To evaluate risk and return dynamics, the study employs Markowitz's efficient frontier to identify optimal portfolios for given levels of risk, and conditional value-at-risk (CVaR) to capture potential extreme losses for a more comprehensive risk assessment. Multiple portfolio configurations are analyzed under long-only and long-short investment strategies to assess adaptability across varying market conditions. Furthermore, key performance risk measures, including the Sharpe ratio, Rachev ratio, and stable tail-adjusted return ratio (STARR), are calculated to provide an in-depth evaluation of reward-to-risk efficiency, with particular emphasis on the role of tail behavior in portfolio performance.\n  This research aims to deliver deeper insights into the risk-return trade-offs, tail-risk behavior, and efficiency of Asian ETFs, offering investors a practical foundation for constructing robust and well-diversified portfolios across both emerging and developed Asian markets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e9a\u6d32ETF\u6295\u8d44\uff0c\u901a\u8fc7\u98ce\u9669\u8bc4\u4f30\u3001\u7ec4\u5408\u5206\u6790\u548c\u7ee9\u6548\u6bd4\u8f83\uff0c\u63a2\u8ba8\u4e8629\u53ea\u6db5\u76d6\u5e7f\u6cdb\u4e9a\u6d32\u5e02\u573a\u7684ETF\uff0c\u4f7f\u7528\u73b0\u4ee3\u6295\u8d44\u7ec4\u5408\u7406\u8bba\u548c\u98ce\u9669\u6307\u6807\u6765\u8bc4\u4f30\u98ce\u9669\u6536\u76ca\u6743\u8861\u548c\u5c3e\u90e8\u98ce\u9669\u884c\u4e3a\u3002", "motivation": "\u6295\u8d44\u4e9a\u6d32ETF\u4e3a\u6295\u8d44\u8005\u63d0\u4f9b\u4e86\u63a5\u89e6\u5feb\u901f\u589e\u957f\u7ecf\u6d4e\u4f53\u548c\u5b9d\u8d35\u591a\u5143\u5316\u673a\u4f1a\u7684\u9014\u5f84\uff0c\u4f46\u9700\u8981\u6df1\u5165\u4e86\u89e3\u5176\u98ce\u9669\u6536\u76ca\u7279\u5f81\u548c\u6295\u8d44\u6548\u7387\u3002", "method": "\u91c7\u7528Markowitz\u6709\u6548\u524d\u6cbf\u8bc6\u522b\u6700\u4f18\u6295\u8d44\u7ec4\u5408\uff0c\u4f7f\u7528\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\u6355\u6349\u6781\u7aef\u635f\u5931\uff0c\u5206\u6790\u591a\u5934\u548c\u591a\u7a7a\u7b56\u7565\uff0c\u5e76\u8ba1\u7b97\u590f\u666e\u6bd4\u7387\u3001Rachev\u6bd4\u7387\u548cSTARR\u7b49\u5173\u952e\u7ee9\u6548\u98ce\u9669\u6307\u6807\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u4e9a\u6d32ETF\u98ce\u9669\u6536\u76ca\u6743\u8861\u3001\u5c3e\u90e8\u98ce\u9669\u884c\u4e3a\u548c\u6295\u8d44\u6548\u7387\u7684\u6df1\u5165\u89c1\u89e3\uff0c\u8bc6\u522b\u4e86\u5728\u4e0d\u540c\u5e02\u573a\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u6295\u8d44\u7ec4\u5408\u914d\u7f6e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6295\u8d44\u8005\u5728\u4e9a\u6d32\u65b0\u5174\u548c\u53d1\u8fbe\u5e02\u573a\u4e2d\u6784\u5efa\u7a33\u5065\u4e14\u591a\u5143\u5316\u7684\u6295\u8d44\u7ec4\u5408\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u5c3e\u90e8\u98ce\u9669\u884c\u4e3a\u5728\u6295\u8d44\u7ec4\u5408\u7ee9\u6548\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.11966", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11966", "abs": "https://arxiv.org/abs/2511.11966", "authors": ["Steven Cao", "Gregory Valiant", "Percy Liang"], "title": "On the Entropy Calibration of Language Models", "comment": "Neurips 2025", "summary": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u71b5\u6821\u51c6\u95ee\u9898\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u6821\u51c6\u9519\u8bef\uff0c\u4e14\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u6821\u51c6\u95ee\u9898\u6539\u5584\u7f13\u6162\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u90fd\u8868\u660e\uff0c\u5373\u4f7f\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u8bef\u5dee\u7d2f\u79ef\u7387\u4e5f\u57fa\u672c\u4fdd\u6301\u4e0d\u53d8\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u71b5\u6821\u51c6\u95ee\u9898\uff0c\u63a2\u7d22\u6a21\u578b\u89c4\u6a21\u6269\u5927\u662f\u5426\u80fd\u6539\u5584\u6821\u51c6\u9519\u8bef\uff0c\u4ee5\u53ca\u662f\u5426\u5b58\u5728\u65e0\u9700\u6743\u8861\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5728\u7b80\u5316\u7406\u8bba\u8bbe\u7f6e\u4e2d\u5206\u6790\u6821\u51c6\u9519\u8bef\u968f\u6570\u636e\u96c6\u89c4\u6a21\u7684\u7f29\u653e\u884c\u4e3a\uff0c\u7136\u540e\u57280.5B\u523070B\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u6d4b\u91cf\u3002", "result": "\u53d1\u73b0\u6821\u51c6\u9519\u8bef\u7684\u7f29\u653e\u6307\u6570\u63a5\u8fd10\uff0c\u610f\u5473\u7740\u66f4\u5927\u6a21\u578b\u4e0e\u66f4\u5c0f\u6a21\u578b\u4ee5\u76f8\u4f3c\u901f\u7387\u7d2f\u79ef\u8bef\u5dee\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5728\u5047\u8bbe\u53ef\u4ee5\u9884\u6d4b\u6587\u672c\u672a\u6765\u71b5\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u5bf9\u6570\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u71b5\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u5e76\u4e0d\u80fd\u663e\u8457\u6539\u5584\u71b5\u6821\u51c6\u95ee\u9898\uff0c\u6807\u51c6\u622a\u65ad\u65b9\u6cd5\u5b58\u5728\u6743\u8861\uff0c\u4f46\u7406\u8bba\u4e0a\u5b58\u5728\u65e0\u9700\u6743\u8861\u7684\u6821\u51c6\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.11602", "categories": ["cs.LG", "cs.GT", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.11602", "abs": "https://arxiv.org/abs/2511.11602", "authors": ["Georgios C. Chasparis"], "title": "Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games", "comment": null, "summary": "Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6536\u76ca\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u5b66\u4e60\u65b9\u6848\u2014\u2014\u57fa\u4e8e\u671f\u671b\u7684\u6270\u52a8\u5b66\u4e60\u81ea\u52a8\u673a\uff08APLA\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u73a9\u5bb6\u5f31\u975e\u5faa\u73af\u6e38\u620f\u4e2d\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u7eaf\u7eb3\u4ec0\u5747\u8861\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u73a9\u5bb6\u5f31\u975e\u5faa\u73af\u6e38\u620f\u4e2d\uff0c\u5f53\u6bcf\u4e2a\u73a9\u5bb6\u5e94\u7528\u72ec\u7acb\u7684\u5b66\u4e60\u52a8\u6001\u65f6\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u7406\u60f3\u7684\u7eaf\u7eb3\u4ec0\u5747\u8861\u3002\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u6f5c\u5728\u535a\u5f08\u548c\u534f\u8c03\u535a\u5f08\u7b49\u5c0f\u7c7b\u6e38\u620f\u3002", "method": "\u63d0\u51fa\u4e86APLA\u5b66\u4e60\u65b9\u6848\uff0c\u5176\u4e2d\u6bcf\u4e2a\u73a9\u5bb6\u7684\u52a8\u4f5c\u9009\u62e9\u6982\u7387\u5206\u5e03\u4e0d\u4ec5\u901a\u8fc7\u91cd\u590d\u9009\u62e9\u5f97\u5230\u5f3a\u5316\uff0c\u8fd8\u901a\u8fc7\u6355\u83b7\u73a9\u5bb6\u6ee1\u610f\u5ea6\u7684\u671f\u671b\u56e0\u5b50\u5f97\u5230\u5f3a\u5316\u3002\u5728\u5b58\u5728\u566a\u58f0\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u591a\u73a9\u5bb6\u6b63\u6548\u7528\u535a\u5f08\u8fdb\u884c\u4e86\u968f\u673a\u7a33\u5b9a\u6027\u5206\u6790\u3002", "result": "\u9996\u6b21\u5728\u4e00\u822c\u975e\u96f6\u548c\u535a\u5f08\u4e2d\u901a\u8fc7\u5efa\u7acb\u8bf1\u5bfc\u65e0\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0e\u6709\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u7b49\u4ef7\u6027\uff0c\u523b\u753b\u4e86\u968f\u673a\u7a33\u5b9a\u6027\u3002\u5728\u5f31\u975e\u5faa\u73af\u535a\u5f08\u4e2d\u8fdb\u4e00\u6b65\u4e13\u95e8\u5316\u4e86\u968f\u673a\u7a33\u5b9a\u6027\u5206\u6790\u3002", "conclusion": "APLA\u65b9\u6848\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u4f18\u5316\u4e2d\u7684\u6536\u655b\u95ee\u9898\uff0c\u4e3a\u591a\u73a9\u5bb6\u535a\u5f08\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2511.11916", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11916", "abs": "https://arxiv.org/abs/2511.11916", "authors": ["Sinan Urgun", "Se\u00e7kin Ar\u0131"], "title": "An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR", "comment": "23 pages, 9 figures", "summary": "This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56db\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4.1-Mini\u5728\u6240\u6709\u63a8\u7406\u67b6\u6784\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u5bf9\u67b6\u6784\u8bbe\u8ba1\u5177\u6709\u7279\u5f02\u6027\u654f\u611f\u5ea6\u3002", "motivation": "\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u548c\u63a8\u7406\u67b6\u6784\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u56db\u79cdLLM\u6a21\u578b\u548c\u56db\u79cd\u63a8\u7406\u67b6\u6784\u5728RAVEN-FAIR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\u751f\u6210\u89c6\u89c9\u54cd\u5e94\uff0c\u4f7f\u7528SSIM\u548cLPIPS\u6307\u6807\u8bc4\u4f30\uff0c\u5206\u6790\u601d\u7ef4\u94fe\u5206\u6570\u548c\u9519\u8bef\u7c7b\u578b\u3002", "result": "GPT-4.1-Mini\u5728\u6240\u6709\u67b6\u6784\u4e2d\u59cb\u7ec8\u83b7\u5f97\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4f1a\u6539\u53d8\u8bed\u4e49\u548c\u6570\u503c\u5e73\u8861\u4f46\u6548\u679c\u4e0d\u4e00\u81f4\uff0c\u4e0d\u540c\u6a21\u578b\u5bf9\u67b6\u6784\u8bbe\u8ba1\u5177\u6709\u7279\u5f02\u6027\u654f\u611f\u5ea6\u3002", "conclusion": "\u63a8\u7406\u6548\u679c\u5177\u6709\u6a21\u578b\u7279\u5f02\u6027\uff0c\u54cd\u5e94\u8986\u76d6\u5ea6\u7684\u53d8\u5316\u4f7f\u8de8\u67b6\u6784\u6bd4\u8f83\u590d\u6742\u5316\uff0c\u91c7\u7528\u4e94\u6b21\u72ec\u7acb\u8fd0\u884c\u7684\u6700\u4f73\u7ed3\u679c\u4f5c\u4e3a\u6027\u80fd\u4e0a\u9650\u4f30\u8ba1\u3002"}}
{"id": "2511.11781", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11781", "abs": "https://arxiv.org/abs/2511.11781", "authors": ["Vlad Rakhlin", "Amir Jevnisek", "Shai Avidan"], "title": "Coordinate Descent for Network Linearization", "comment": null, "summary": "ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5750\u6807\u4e0b\u964d\u7684\u79bb\u6563\u4f18\u5316\u65b9\u6cd5\uff0c\u76f4\u63a5\u51cf\u5c11ResNet\u7f51\u7edc\u4e2dReLU\u6fc0\u6d3b\u51fd\u6570\u7684\u6570\u91cf\uff0c\u4ee5\u964d\u4f4e\u79c1\u6709\u63a8\u7406\u7684\u5ef6\u8fdf\u3002", "motivation": "ReLU\u6fc0\u6d3b\u51fd\u6570\u662f\u57fa\u4e8eResNet\u7f51\u7edc\u7684\u79c1\u6709\u63a8\u7406\u4e2d\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u73b0\u6709\u7684\u5e73\u6ed1\u8fd1\u4f3c\u65b9\u6cd5\u5728\u6700\u540e\u786c\u9608\u503c\u6b65\u9aa4\u4e2d\u4f1a\u5f15\u5165\u8f83\u5927\u7684\u6027\u80fd\u635f\u5931\u3002", "method": "\u91c7\u7528\u5750\u6807\u4e0b\u964d\u4f5c\u4e3a\u4f18\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u79bb\u6563\u57df\u4e2d\u5de5\u4f5c\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4ea7\u751f\u7a00\u758f\u89e3\u3002", "result": "\u5728\u5e38\u89c1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u79bb\u6563\u4f18\u5316\u6709\u6548\u51cf\u5c11\u4e86ReLU\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7f51\u7edc\u7cbe\u5ea6\uff0c\u5728\u79c1\u6709\u63a8\u7406\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.12632", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12632", "abs": "https://arxiv.org/abs/2511.12632", "authors": ["Gal Barkai", "Leonid Mirkin", "Daniel Zelazo"], "title": "On two-degrees-of-freedom agreement protocols", "comment": "7 page, 9 figures, this work has been submitted to ECC 2026 for possible publication", "summary": "We propose a distributed two-degrees-of-freedom (2DOF) architecture for driving autonomous, possibly heterogeneous, agents to agreement. The scheme mirrors classical servo structures, separating local feedback from network filtering. This separation enables independent network-filter design for prescribed noise attenuation and allows controller heterogeneity to reject local disturbances, including disturbances exciting unstable agreement poles -- which is known to be impossible via standard diffusive couplings. The potential of the framework is illustrated via two numerical examples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u53cc\u81ea\u7531\u5ea6\u67b6\u6784\uff0c\u7528\u4e8e\u9a71\u52a8\u81ea\u4e3b\u5f02\u6784\u667a\u80fd\u4f53\u8fbe\u6210\u4e00\u81f4\u3002\u8be5\u67b6\u6784\u5206\u79bb\u4e86\u672c\u5730\u53cd\u9988\u548c\u7f51\u7edc\u6ee4\u6ce2\uff0c\u652f\u6301\u72ec\u7acb\u8bbe\u8ba1\u7f51\u7edc\u6ee4\u6ce2\u5668\u6765\u6291\u5236\u566a\u58f0\uff0c\u5e76\u5141\u8bb8\u63a7\u5236\u5668\u5f02\u6784\u6027\u6765\u6291\u5236\u672c\u5730\u6270\u52a8\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u6269\u6563\u8026\u5408\u65b9\u6cd5\u65e0\u6cd5\u6291\u5236\u6fc0\u53d1\u4e0d\u7a33\u5b9a\u4e00\u81f4\u6781\u70b9\u7684\u672c\u5730\u6270\u52a8\u7684\u95ee\u9898\uff0c\u4e3a\u5f02\u6784\u667a\u80fd\u4f53\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u4e00\u81f4\u6027\u63a7\u5236\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u53cc\u81ea\u7531\u5ea6\u67b6\u6784\uff0c\u5c06\u672c\u5730\u53cd\u9988\u4e0e\u7f51\u7edc\u6ee4\u6ce2\u5206\u79bb\uff0c\u652f\u6301\u72ec\u7acb\u8bbe\u8ba1\u7f51\u7edc\u6ee4\u6ce2\u5668\uff0c\u5e76\u5141\u8bb8\u63a7\u5236\u5668\u5f02\u6784\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u6291\u5236\u566a\u58f0\u548c\u672c\u5730\u6270\u52a8\uff0c\u5305\u62ec\u90a3\u4e9b\u6fc0\u53d1\u4e0d\u7a33\u5b9a\u4e00\u81f4\u6781\u70b9\u7684\u6270\u52a8\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6f5c\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u81ea\u7531\u5ea6\u67b6\u6784\u4e3a\u5f02\u6784\u667a\u80fd\u4f53\u7684\u4e00\u81f4\u6027\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u7a33\u5b9a\u4e00\u81f4\u6781\u70b9\u6270\u52a8\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2511.12556", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12556", "abs": "https://arxiv.org/abs/2511.12556", "authors": ["Jing Liu", "Bing Guo", "Ren Zhu"], "title": "DLMMPR:Deep Learning-based Measurement Matrix for Phase Retrieval", "comment": null, "summary": "This paper pioneers the integration of learning optimization into measurement matrix design for phase retrieval. We introduce the Deep Learning-based Measurement Matrix for Phase Retrieval (DLMMPR) algorithm, which parameterizes the measurement matrix within an end-to-end deep learning architecture. Synergistically augmented with subgradient descent and proximal mapping modules for robust recovery, DLMMPR's efficacy is decisively confirmed through comprehensive empirical validation across diverse noise regimes. Benchmarked against DeepMMSE and PrComplex, our method yields substantial gains in PSNR and SSIM, underscoring its superiority.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06\u5b66\u4e60\u4f18\u5316\u96c6\u6210\u5230\u76f8\u4f4d\u6062\u590d\u7684\u6d4b\u91cf\u77e9\u9635\u8bbe\u8ba1\u4e2d\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6d4b\u91cf\u77e9\u9635\u7b97\u6cd5DLMMPR\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u53c2\u6570\u5316\u6d4b\u91cf\u77e9\u9635\uff0c\u7ed3\u5408\u6b21\u68af\u5ea6\u4e0b\u964d\u548c\u8fd1\u7aef\u6620\u5c04\u6a21\u5757\u5b9e\u73b0\u9c81\u68d2\u6062\u590d\u3002", "motivation": "\u4f20\u7edf\u76f8\u4f4d\u6062\u590d\u65b9\u6cd5\u5728\u6d4b\u91cf\u77e9\u9635\u8bbe\u8ba1\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u63a2\u7d22\u5c06\u5b66\u4e60\u4f18\u5316\u96c6\u6210\u5230\u6d4b\u91cf\u77e9\u9635\u8bbe\u8ba1\u4e2d\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u76f8\u4f4d\u6062\u590d\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faDLMMPR\u7b97\u6cd5\uff0c\u5728\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\u53c2\u6570\u5316\u6d4b\u91cf\u77e9\u9635\uff0c\u5e76\u96c6\u6210\u6b21\u68af\u5ea6\u4e0b\u964d\u548c\u8fd1\u7aef\u6620\u5c04\u6a21\u5757\u6765\u589e\u5f3a\u6062\u590d\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u79cd\u566a\u58f0\u73af\u5883\u4e0b\u8fdb\u884c\u7efc\u5408\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4e0eDeepMMSE\u548cPrComplex\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0cDLMMPR\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DLMMPR\u7b97\u6cd5\u5728\u76f8\u4f4d\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5c06\u5b66\u4e60\u4f18\u5316\u96c6\u6210\u5230\u6d4b\u91cf\u77e9\u9635\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11978", "abs": "https://arxiv.org/abs/2511.11978", "authors": ["Hui Huang", "Yanping Chen", "Ruizhang Huang", "Chuan Lin", "Yongbin Qin"], "title": "A Reasoning Paradigm for Named Entity Recognition", "comment": "Accepted at AAAI 2026", "summary": "Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This \"cognitive shortcutting\" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.", "AI": {"tldr": "\u63d0\u51faReasoningNER\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u673a\u5236\u6539\u8fdbNER\u4efb\u52a1\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8d85\u8d8aGPT-4\u8fbe12.3\u4e2a\u767e\u5206\u70b9", "motivation": "\u4f20\u7edf\u751f\u6210\u5f0fLLM\u5728NER\u4efb\u52a1\u4e2d\u4f9d\u8d56\u9690\u5f0f\u6a21\u5f0f\u5339\u914d\uff0c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u673a\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u548c\u6cdb\u5316\u8106\u5f31\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b", "method": "\u4e09\u9636\u6bb5\u63a8\u7406\u6846\u67b6\uff1a1)\u751f\u6210NER\u5bfc\u5411\u7684\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff1b2)\u4f7f\u7528CoT\u8c03\u4f18\u6a21\u578b\u751f\u6210\u63a8\u7406\u94fe\uff1b3)\u63a8\u7406\u589e\u5f3a\u9636\u6bb5\u4f7f\u7528\u7efc\u5408\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff0cF1\u5206\u6570\u6bd4GPT-4\u9ad8\u51fa12.3\u4e2a\u767e\u5206\u70b9\uff0c\u5c55\u793a\u4e86\u5728\u63a8\u7406\u5bfc\u5411\u4fe1\u606f\u63d0\u53d6\u7814\u7a76\u4e2d\u7684\u5de8\u5927\u6f5c\u529b", "conclusion": "ReasoningNER\u901a\u8fc7\u4ece\u9690\u5f0f\u6a21\u5f0f\u5339\u914d\u8f6c\u5411\u663e\u5f0f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86NER\u4efb\u52a1\u7684\u8ba4\u77e5\u80fd\u529b\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b"}}
{"id": "2511.11604", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11604", "abs": "https://arxiv.org/abs/2511.11604", "authors": ["Amaratou Mahamadou Saley", "Thierry Moyaux", "A\u00efcha Sekhari", "Vincent Cheutet", "Jean-Baptiste Danielou"], "title": "Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques", "comment": "19 pages, 9 figures, 6 journal, Journal Q1 (Computers and Industrial Engineering)", "summary": "The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u6280\u672f\u548c\u6838\u5de5\u4e1a\u9886\u57df\u77e5\u8bc6\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u65b9\u6cd5\uff0c\u5728\u6838\u5de5\u4e1a\u9886\u57df\u663e\u8457\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5c06\u9884\u6d4b\u65f6\u95f4\u4ece3\u5c0f\u65f6\u5ef6\u957f\u523024\u5c0f\u65f6\uff0cF1\u5206\u6570\u4ece56.36%\u63d0\u5347\u523093.12%\u3002", "motivation": "\u7269\u8054\u7f51\u548c\u5de5\u4e1a4.0\u5728\u6838\u5de5\u4e1a\u4e2d\u7684\u5e94\u7528\u589e\u5f3a\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4f46\u6838\u5de5\u4e1a\u7cfb\u7edf\u590d\u6742\uff0c\u9700\u8981\u5927\u91cf\u9886\u57df\u77e5\u8bc6\u3002\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u9884\u6d4b\u8bbe\u5907\u7ef4\u62a4\u9700\u6c42\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u9a71\u52a8\u6280\u672f\u4e0e\u6838\u8bbe\u5907\u9886\u57df\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u5f3a\u8c03\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u77e5\u8bc6\u5728\u63d0\u5347\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u4e2d\u7684\u91cd\u8981\u6027\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\uff0c\u6df7\u5408\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff1a\u9884\u6d4b\u65f6\u95f4\u4ece3\u5c0f\u65f6\u5ef6\u957f\u523024\u5c0f\u65f6\uff0cF1\u5206\u6570\u4ece56.36%\u63d0\u5347\u523093.12%\u3002", "conclusion": "\u5728\u6838\u5de5\u4e1a\u7b49\u9ad8\u5ea6\u53d7\u9650\u548c\u654f\u611f\u7684\u9886\u57df\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684\u6df7\u5408\u9884\u6d4b\u6027\u7ef4\u62a4\u65b9\u6cd5\u6bd4\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6545\u969c\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.11921", "categories": ["cs.AI", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.11921", "abs": "https://arxiv.org/abs/2511.11921", "authors": ["Liudong Xing", "Janet", "Lin"], "title": "Looking Forward: Challenges and Opportunities in Agentic AI Reliability", "comment": "13 pages, 6 figures; This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by SpringerNature", "summary": "This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.", "AI": {"tldr": "\u672c\u7ae0\u8ba8\u8bba\u4e86\u6784\u5efa\u53ef\u9760AI\u7cfb\u7edf\uff08\u7279\u522b\u662f\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff09\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u524d\u666f\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u7ea7\u8054\u6545\u969c\u98ce\u9669\u7f13\u89e3\u3001\u52a8\u6001\u73af\u5883\u3001\u4efb\u52a1\u6267\u884c\u4e0d\u4e00\u81f4\u6027\u3001\u4e0d\u53ef\u9884\u6d4b\u6d8c\u73b0\u884c\u4e3a\u7b49\u65b9\u9762\u7684\u7814\u7a76\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u8fd9\u4e9b\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\u53ef\u80fd\u9762\u4e34\u7ea7\u8054\u6545\u969c\u3001\u52a8\u6001\u73af\u5883\u53d8\u5316\u3001\u4efb\u52a1\u6267\u884c\u4e0d\u4e00\u81f4\u7b49\u591a\u79cd\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u53ef\u9760\u6027\u4fdd\u969c\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524d\u7814\u7a76\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u591a\u4e2a\u5173\u952e\u7814\u7a76\u95ee\u9898\uff1a\u7ea7\u8054\u6545\u969c\u98ce\u9669\u7f13\u89e3\u3001\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u3001\u4efb\u52a1\u6267\u884c\u4e00\u81f4\u6027\u3001\u6d8c\u73b0\u884c\u4e3a\u9884\u6d4b\uff0c\u4ee5\u53ca\u53ef\u9760\u6027\u6d4b\u8bd5\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u6784\u5efa\u53ef\u9760\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u591a\u4e2a\u7814\u7a76\u65b9\u5411\u548c\u6311\u6218\uff0c\u5305\u62ec\u5f00\u53d1\u6709\u6548\u7684\u6545\u969c\u68c0\u6d4b\u548c\u6062\u590d\u673a\u5236\u3001\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u7b97\u6cd5\u3001\u786e\u4fdd\u4efb\u52a1\u6267\u884c\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u8d44\u6e90\u9ad8\u6548\u7684\u53ef\u9760\u6027\u4fdd\u969c\u6280\u672f\u3002", "conclusion": "\u53ef\u9760\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5f00\u53d1\u9700\u8981\u591a\u65b9\u9762\u7684\u7814\u7a76\u52aa\u529b\uff0c\u5305\u62ec\u6545\u969c\u7f13\u89e3\u3001\u73af\u5883\u9002\u5e94\u6027\u3001\u884c\u4e3a\u9884\u6d4b\u548c\u7cfb\u7edf\u8bc4\u4f30\u7b49\u5173\u952e\u9886\u57df\uff0c\u8fd9\u4e9b\u7814\u7a76\u5c06\u63a8\u52a8AI\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u53ef\u9760\u90e8\u7f72\u3002"}}
{"id": "2511.12678", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12678", "abs": "https://arxiv.org/abs/2511.12678", "authors": ["Yingzhuo Sun", "Yulan Gao", "Ming Xiao", "Zhu Han", "Octavia A. Dobre"], "title": "Visibility-aware Satellite Selection and Resource Allocation in Multi-Orbit LEO Networks", "comment": null, "summary": "Multi orbit low earth orbit (LEO) satellites communication is envisioned as a key infrastructure to deliver global coverage, enabling future services from space air ground integrated networks.However, the optimized design of LEO which jointly addresses satellite selection, association control, and resource scheduling while accounting for dynamic visibility in multi orbit constellations still remains open. Satellites moving along distinct orbital planes yield phase shifted ground tracks and heterogeneous, time varying coverage patterns that significantly complicate the optimization.To bridge the gap, we propose a dynamic visibility aware multi orbit satellite selection framework which can determine the optimal serving satellites across orbital layers. The framework is built upon Markov approximation and matching game theory. Specifically, we formulate a combinatorial optimization problem that maximizes the sum rate under per satellite power budgets. The problem is NP hard , combining discrete user association (UA) decisions with continuous power allocation, and an inherently non convex sum rate maximization objective. We address it through a problem specific Markov approximation. Moreover, we alternately solve UA or bandwidth allocation via a matching game and power allocation via a Lagrangian dual program, which together form a block coordinate descent method tailored to this problem. Simulation results show that the proposed algorithm converges to a suboptimal solution across all scenarios. Extensive experiments against four state of the art baselines further demonstrate that our algorithm achieves, on average, approximately 7.85% higher sum rate than the best performing baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53ef\u89c1\u6027\u611f\u77e5\u7684\u591a\u8f68\u9053\u536b\u661f\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u8fd1\u4f3c\u548c\u5339\u914d\u535a\u5f08\u7406\u8bba\u89e3\u51b3LEO\u536b\u661f\u901a\u4fe1\u4e2d\u7684\u536b\u661f\u9009\u62e9\u3001\u5173\u8054\u63a7\u5236\u548c\u8d44\u6e90\u8c03\u5ea6\u8054\u5408\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u591a\u8f68\u9053LEO\u536b\u661f\u901a\u4fe1\u662f\u5b9e\u73b0\u5168\u7403\u8986\u76d6\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u540c\u65f6\u8003\u8651\u536b\u661f\u9009\u62e9\u3001\u5173\u8054\u63a7\u5236\u548c\u8d44\u6e90\u8c03\u5ea6\uff0c\u5e76\u5904\u7406\u591a\u8f68\u9053\u661f\u5ea7\u52a8\u6001\u53ef\u89c1\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u8fd1\u4f3c\u548c\u5339\u914d\u535a\u5f08\u7406\u8bba\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u7528\u6237\u5173\u8054\uff08\u901a\u8fc7\u5339\u914d\u535a\u5f08\uff09\u548c\u529f\u7387\u5206\u914d\uff08\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u89c4\u5212\uff09\uff0c\u5f62\u6210\u9488\u5bf9\u8be5\u95ee\u9898\u7684\u5757\u5750\u6807\u4e0b\u964d\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u7b97\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u80fd\u6536\u655b\u5230\u6b21\u4f18\u89e3\uff0c\u76f8\u6bd4\u56db\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u5b9e\u73b0\u4e86\u7ea67.85%\u7684\u66f4\u9ad8\u603b\u901f\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u6001\u53ef\u89c1\u6027\u611f\u77e5\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u8f68\u9053LEO\u536b\u661f\u901a\u4fe1\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.11764", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11764", "abs": "https://arxiv.org/abs/2511.11764", "authors": ["Nikitha Donekal Chandrashekar", "Sehrish Basir Nizamani", "Margaret Ellis", "Naren Ramakrishnan"], "title": "Demystify, Use, Reflect: Preparing students to be informed LLM-users", "comment": "2 pages 1 table Submitted to SIGCSE 2026", "summary": "We transitioned our post-CS1 course that introduces various subfields of computer science so that it integrates Large Language Models (LLMs) in a structured, critical, and practical manner. It aims to help students develop the skills needed to engage meaningfully and responsibly with AI. The course now includes explicit instruction on how LLMs work, exposure to current tools, ethical issues, and activities that encourage student reflection on personal use of LLMs as well as the larger evolving landscape of AI-assisted programming. In class, we demonstrate the use and verification of LLM outputs, guide students in the use of LLMs as an ingredient in a larger problem-solving loop, and require students to disclose and acknowledge the nature and extent of LLM assistance. Throughout the course, we discuss risks and benefits of LLMs across CS subfields. In our first iteration of the course, we collected and analyzed data from students pre and post surveys. Student understanding of how LLMs work became more technical, and their verification and use of LLMs shifted to be more discerning and collaborative. These strategies can be used in other courses to prepare students for the AI-integrated future.", "AI": {"tldr": "\u5c06\u540eCS1\u8bfe\u7a0b\u6539\u9020\u4e3a\u7cfb\u7edf\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6559\u5b66\uff0c\u57f9\u517b\u5b66\u751f\u8d1f\u8d23\u4efb\u4f7f\u7528AI\u7684\u80fd\u529b\uff0c\u5305\u62ec\u6280\u672f\u539f\u7406\u3001\u5de5\u5177\u4f7f\u7528\u3001\u4f26\u7406\u95ee\u9898\u548c\u5b9e\u8df5\u53cd\u601d\u3002", "motivation": "\u5e2e\u52a9\u5b66\u751f\u53d1\u5c55\u6709\u610f\u4e49\u4e14\u8d1f\u8d23\u4efb\u5730\u53c2\u4e0eAI\u6240\u9700\u7684\u6280\u80fd\uff0c\u4e3aAI\u96c6\u6210\u7684\u672a\u6765\u505a\u597d\u51c6\u5907\u3002", "method": "\u8bfe\u7a0b\u5305\u542bLLM\u5de5\u4f5c\u539f\u7406\u8bb2\u89e3\u3001\u5f53\u524d\u5de5\u5177\u63a5\u89e6\u3001\u4f26\u7406\u95ee\u9898\u8ba8\u8bba\u3001\u5b66\u751f\u53cd\u601d\u6d3b\u52a8\uff0c\u8bfe\u5802\u4e0a\u6f14\u793aLLM\u8f93\u51fa\u4f7f\u7528\u4e0e\u9a8c\u8bc1\uff0c\u6307\u5bfc\u5b66\u751f\u5c06LLM\u4f5c\u4e3a\u95ee\u9898\u89e3\u51b3\u73af\u8282\u7684\u4e00\u90e8\u5206\uff0c\u5e76\u8981\u6c42\u5b66\u751f\u62ab\u9732LLM\u534f\u52a9\u60c5\u51b5\u3002", "result": "\u5b66\u751f\u8c03\u67e5\u663e\u793a\uff0c\u5b66\u751f\u5bf9LLM\u5de5\u4f5c\u539f\u7406\u7684\u7406\u89e3\u66f4\u52a0\u6280\u672f\u5316\uff0c\u5bf9LLM\u7684\u9a8c\u8bc1\u548c\u4f7f\u7528\u53d8\u5f97\u66f4\u52a0\u654f\u9510\u548c\u534f\u4f5c\u3002", "conclusion": "\u8fd9\u4e9b\u7b56\u7565\u53ef\u7528\u4e8e\u5176\u4ed6\u8bfe\u7a0b\uff0c\u4e3a\u5b66\u751f\u5e94\u5bf9AI\u96c6\u6210\u672a\u6765\u505a\u597d\u51c6\u5907\u3002"}}
{"id": "2511.12615", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12615", "abs": "https://arxiv.org/abs/2511.12615", "authors": ["Rico Z\u00f6llner"], "title": "A New Perspective on Double-S Curve Motions of Higher Order and Optimal Motion Planning", "comment": null, "summary": "This paper presents and proves an equation for the time horizon of symmetric trajectories with zero boundary conditions and bounded derivatives of arbitrary order. This equation holds regardless of the number of phases comprising the associated motion. This avoids case distinctions in calculations. Application examples of motions with minimum time, minimum velocity, and minimum acceleration are discussed. Furthermore, an algorithm is derived that reduces the time minimization problem to solving a system of equations. This algorithm avoids nested case distinctions and complex optimizations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5bf9\u79f0\u8f68\u8ff9\u65f6\u95f4\u8303\u56f4\u7684\u901a\u7528\u65b9\u7a0b\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u9636\u5bfc\u6570\u6709\u754c\u7684\u60c5\u51b5\uff0c\u907f\u514d\u4e86\u6848\u4f8b\u5206\u6790\uff0c\u5e76\u7ed9\u51fa\u4e86\u65f6\u95f4\u6700\u5c0f\u5316\u95ee\u9898\u7684\u7b80\u5316\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5bf9\u79f0\u8f68\u8ff9\u65f6\u95f4\u8303\u56f4\u65f6\u9700\u8981\u5927\u91cf\u6848\u4f8b\u5206\u6790\uff0c\u8ba1\u7b97\u590d\u6742\u4e14\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u63a8\u5bfc\u5e76\u8bc1\u660e\u4e86\u5bf9\u79f0\u8f68\u8ff9\u65f6\u95f4\u8303\u56f4\u7684\u901a\u7528\u65b9\u7a0b\uff0c\u5f00\u53d1\u4e86\u5c06\u65f6\u95f4\u6700\u5c0f\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u65b9\u7a0b\u7ec4\u6c42\u89e3\u7684\u7b97\u6cd5\u3002", "result": "\u5f97\u5230\u4e86\u9002\u7528\u4e8e\u4efb\u610f\u76f8\u4f4d\u6570\u7684\u901a\u7528\u65f6\u95f4\u8303\u56f4\u65b9\u7a0b\uff0c\u907f\u514d\u4e86\u6848\u4f8b\u5206\u6790\uff0c\u7b80\u5316\u4e86\u4f18\u5316\u8fc7\u7a0b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u7a0b\u548c\u7b97\u6cd5\u6709\u6548\u7b80\u5316\u4e86\u5bf9\u79f0\u8f68\u8ff9\u65f6\u95f4\u8303\u56f4\u7684\u8ba1\u7b97\uff0c\u4e3a\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2511.12001", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12001", "abs": "https://arxiv.org/abs/2511.12001", "authors": ["Eunkyu Park", "Wesley Hanwen Deng", "Vasudha Varadarajan", "Mingxi Yan", "Gunhee Kim", "Maarten Sap", "Motahhare Eslami"], "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations", "comment": "Under review; 16 pages, 15 figures", "summary": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.", "AI": {"tldr": "\u7814\u7a76\u8868\u660eCoT\u89e3\u91ca\u5728\u9053\u5fb7\u573a\u666f\u4e2d\u5177\u6709\u53cc\u5203\u5251\u4f5c\u7528\uff1a\u65e2\u80fd\u63d0\u5347\u900f\u660e\u5ea6\uff0c\u4e5f\u80fd\u901a\u8fc7\u786e\u8ba4\u504f\u89c1\u8bef\u5bfc\u7528\u6237\uff0c\u7279\u522b\u662f\u5f53\u89e3\u91ca\u91c7\u7528\u81ea\u4fe1\u8bed\u6c14\u65f6\uff0c\u4f1a\u6291\u5236\u9519\u8bef\u68c0\u6d4b\u5e76\u7ef4\u6301\u7528\u6237\u4f9d\u8d56\u3002", "motivation": "\u63a2\u8ba8\u89e3\u91ca\u5728\u4fc3\u8fdb\u900f\u660e\u5ea6\u4e0e\u53ef\u80fd\u5f15\u53d1\u786e\u8ba4\u504f\u89c1\u4e4b\u95f4\u7684\u53cc\u91cd\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u9053\u5fb7\u573a\u666f\u4e2d\uff0c\u7814\u7a76CoT\u89e3\u91ca\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4fe1\u4efb\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u6270\u52a8\u63a8\u7406\u94fe\u548c\u64cd\u7eb5\u4ea4\u4ed8\u8bed\u6c14\uff0c\u5206\u6790\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u9519\u8bef\u53ca\u5176\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u6548\u5e94\uff1a(1)\u7528\u6237\u5e38\u5c06\u4fe1\u4efb\u7b49\u540c\u4e8e\u7ed3\u679c\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u63a8\u7406\u6709\u7f3a\u9677\u4ecd\u4fdd\u6301\u4f9d\u8d56\uff1b(2)\u81ea\u4fe1\u8bed\u6c14\u4f1a\u6291\u5236\u9519\u8bef\u68c0\u6d4b\u540c\u65f6\u7ef4\u6301\u4f9d\u8d56\uff0c\u8868\u660e\u4ea4\u4ed8\u98ce\u683c\u53ef\u4ee5\u51cc\u9a7e\u4e8e\u6b63\u786e\u6027\u4e4b\u4e0a\u3002", "conclusion": "CoT\u89e3\u91ca\u65e2\u80fd\u6f84\u6e05\u4e5f\u80fd\u8bef\u5bfc\uff0c\u5f3a\u8c03NLP\u7cfb\u7edf\u9700\u8981\u63d0\u4f9b\u9f13\u52b1\u5ba1\u67e5\u548c\u6279\u5224\u6027\u601d\u7ef4\u800c\u975e\u76f2\u76ee\u4fe1\u4efb\u7684\u89e3\u91ca\u3002"}}
{"id": "2511.11607", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11607", "abs": "https://arxiv.org/abs/2511.11607", "authors": ["Guoqing Ma", "Yuhan Zhang", "Yuming Dai", "Guangfu Hao", "Yang Chen", "Shan Yu"], "title": "Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.", "AI": {"tldr": "\u63d0\u51faCOWM\u5c42\u6765\u89e3\u51b3RL\u4e2d\u7684\u73af\u5883\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u6295\u5f71\u77e9\u9635\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u7a33\u5b9a\u6027", "motivation": "RL\u4ee3\u7406\u901a\u5e38\u5047\u8bbe\u73af\u5883\u5e73\u7a33\uff0c\u4f46\u5b9e\u9645\u73af\u5883\u5f80\u5f80\u662f\u975e\u5e73\u7a33\u7684\uff0c\u5bfc\u81f4\u9700\u8981\u6570\u767e\u4e07\u6b21\u8fed\u4ee3\u548c\u4f4e\u6837\u672c\u6548\u7387", "method": "\u5f15\u5165COWM\u5c42\u96c6\u6210\u5230\u4efb\u4f55RL\u7b97\u6cd5\u7684\u7b56\u7565\u7f51\u7edc\u4e2d\uff0c\u4f7f\u7528\u805a\u7c7b\u6280\u672f\u548c\u6295\u5f71\u77e9\u9635\u6765\u7f13\u89e3\u975e\u5e73\u7a33\u6027", "result": "\u5728\u89c6\u89c9\u548c\u72b6\u6001\u57fa\u7840\u7684DMControl\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u53479%\u548c12.6%\uff0c\u5728\u5404\u79cd\u7b97\u6cd5\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027", "conclusion": "COWM\u5c42\u80fd\u6709\u6548\u7f13\u89e3\u73af\u5883\u975e\u5e73\u7a33\u6027\uff0c\u63d0\u9ad8\u5b66\u4e60\u901f\u5ea6\u548c\u6548\u7387\uff0c\u51cf\u5c11\u68af\u5ea6\u5e72\u6270"}}
{"id": "2511.11924", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11924", "abs": "https://arxiv.org/abs/2511.11924", "authors": ["Yongkang Huo", "Fulvio Forni", "Rodolphe Sepulchre"], "title": "A Neuromorphic Architecture for Scalable Event-Based Control", "comment": null, "summary": "This paper introduces the ``rebound Winner-Take-All (RWTA)\" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\"\u53cd\u5f39\u8d62\u5bb6\u901a\u5403(RWTA)\"\u57fa\u5143\u7684\u53ef\u6269\u5c55\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u79bb\u6563\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u548c\u8fde\u7eed\u8c03\u8282\u7684\u53ef\u8c03\u6027\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u8fde\u7eed\u8282\u5f8b\u751f\u6210\u548c\u79bb\u6563\u51b3\u7b56\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u8fde\u7eed\u8282\u5f8b\u751f\u6210\u548c\u79bb\u6563\u51b3\u7b56\u7684\u7edf\u4e00\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\uff0c\u7ed3\u5408\u79bb\u6563\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u548c\u8fde\u7eed\u8c03\u8282\u7684\u53ef\u8c03\u6027\u3002", "method": "\u63d0\u51fa\u53cd\u5f39\u8d62\u5bb6\u901a\u5403(RWTA)\u57fa\u5143\u4f5c\u4e3a\u57fa\u672c\u6784\u5efa\u6a21\u5757\uff0c\u4ece\u7ec6\u80de\u5c42\u9762\u5230\u7cfb\u7edf\u5c42\u9762\u6784\u5efa\u67b6\u6784\uff0c\u7ee7\u627f\u8d62\u5bb6\u901a\u5403\u72b6\u6001\u673a\u7684\u79bb\u6563\u8ba1\u7b97\u80fd\u529b\u548c\u53ef\u5174\u594b\u751f\u7269\u7269\u7406\u7535\u8def\u7684\u8fde\u7eed\u8c03\u8282\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u86c7\u5f62\u673a\u5668\u4eba\u7684\u795e\u7ecf\u7cfb\u7edf\u8bbe\u8ba1\u5c55\u793a\u4e86\u8be5\u67b6\u6784\u7684\u901a\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u6a21\u5757\u5316\u7279\u6027\u3002", "conclusion": "RWTA\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e8b\u4ef6\u9a71\u52a8\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u4e2d\u7684\u8fde\u7eed\u548c\u79bb\u6563\u8ba1\u7b97\u9700\u6c42\u3002"}}
{"id": "2511.12756", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12756", "abs": "https://arxiv.org/abs/2511.12756", "authors": ["Sungjun Seo", "Kooktae Lee"], "title": "Density-Driven Optimal Control for Non-Uniform Area Coverage in Decentralized Multi-Agent Systems Using Optimal Transport", "comment": "Author Accepted Manuscript (AAM) of a paper accepted for publication in IEEE Transactions on Systems, Man, and Cybernetics: Systems", "summary": "This paper addresses the fundamental problem of non-uniform area coverage in multi-agent systems, where different regions require varying levels of attention due to mission-dependent priorities. Existing uniform coverage strategies are insufficient for realistic applications, and many non-uniform approaches either lack optimality guarantees or fail to incorporate crucial real-world constraints such as agent dynamics, limited operation time, the number of agents, and decentralized execution.\n  To resolve these limitations, we propose a novel framework called Density-Driven Optimal Control (D2OC). The central idea of D2OC is the integration of optimal transport theory with multi-agent coverage control, enabling each agent to continuously adjust its trajectory to match a mission-specific reference density map. The proposed formulation establishes optimality by solving a constrained optimization problem that explicitly incorporates physical and operational constraints. The resulting control input is analytically derived from the Lagrangian of the objective function, yielding closed-form optimal solutions for linear systems and a generalizable structure for nonlinear systems. Furthermore, a decentralized data-sharing mechanism is developed to coordinate agents without reliance on global information.\n  Comprehensive simulation studies demonstrate that D2OC achieves significantly improved non-uniform area coverage performance compared to existing methods, while maintaining scalability and decentralized implementability.", "AI": {"tldr": "\u63d0\u51faD2OC\u6846\u67b6\uff0c\u5c06\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u4e0e\u591a\u667a\u80fd\u4f53\u8986\u76d6\u63a7\u5236\u7ed3\u5408\uff0c\u89e3\u51b3\u975e\u5747\u5300\u533a\u57df\u8986\u76d6\u95ee\u9898\uff0c\u5728\u8003\u8651\u7269\u7406\u548c\u64cd\u4f5c\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5747\u5300\u8986\u76d6\u7b56\u7565\u65e0\u6cd5\u5e94\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u540c\u533a\u57df\u9700\u8981\u4e0d\u540c\u5173\u6ce8\u5ea6\u7684\u9700\u6c42\uff0c\u800c\u975e\u5747\u5300\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u6700\u4f18\u6027\u4fdd\u8bc1\uff0c\u8981\u4e48\u672a\u80fd\u8003\u8651\u667a\u80fd\u4f53\u52a8\u529b\u5b66\u3001\u6709\u9650\u64cd\u4f5c\u65f6\u95f4\u3001\u667a\u80fd\u4f53\u6570\u91cf\u548c\u5206\u6563\u6267\u884c\u7b49\u73b0\u5b9e\u7ea6\u675f\u3002", "method": "\u96c6\u6210\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u4e0e\u591a\u667a\u80fd\u4f53\u8986\u76d6\u63a7\u5236\uff0c\u901a\u8fc7\u6c42\u89e3\u5305\u542b\u7269\u7406\u548c\u64cd\u4f5c\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u63a7\u5236\u8f93\u5165\uff0c\u5e76\u5f00\u53d1\u5206\u6563\u6570\u636e\u5171\u4eab\u673a\u5236\u5b9e\u73b0\u65e0\u5168\u5c40\u4fe1\u606f\u7684\u534f\u8c03\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0cD2OC\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u975e\u5747\u5300\u533a\u57df\u8986\u76d6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5206\u6563\u5b9e\u73b0\u80fd\u529b\u3002", "conclusion": "D2OC\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u975e\u5747\u5300\u533a\u57df\u8986\u76d6\u7684\u5173\u952e\u95ee\u9898\uff0c\u5728\u8003\u8651\u73b0\u5b9e\u7ea6\u675f\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u6700\u4f18\u6027\u80fd\u4fdd\u8bc1\u3002"}}
{"id": "2511.11772", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11772", "abs": "https://arxiv.org/abs/2511.11772", "authors": ["Chenyu Zhang", "Xiaohang Luo"], "title": "Scaling Equitable Reflection Assessment in Education via Large Language Models and Role-Based Feedback Agents", "comment": "Accepted to AAAI-26 AISI Track", "summary": "Formative feedback is widely recognized as one of the most effective drivers of student learning, yet it remains difficult to implement equitably at scale. In large or low-resource courses, instructors often lack the time, staffing, and bandwidth required to review and respond to every student reflection, creating gaps in support precisely where learners would benefit most. This paper presents a theory-grounded system that uses five coordinated role-based LLM agents (Evaluator, Equity Monitor, Metacognitive Coach, Aggregator, and Reflexion Reviewer) to score learner reflections with a shared rubric and to generate short, bias-aware, learner-facing comments. The agents first produce structured rubric scores, then check for potentially biased or exclusionary language, add metacognitive prompts that invite students to think about their own thinking, and finally compose a concise feedback message of at most 120 words. The system includes simple fairness checks that compare scoring error across lower and higher scoring learners, enabling instructors to monitor and bound disparities in accuracy. We evaluate the pipeline in a 12-session AI literacy program with adult learners. In this setting, the system produces rubric scores that approach expert-level agreement, and trained graders rate the AI-generated comments as helpful, empathetic, and well aligned with instructional goals. Taken together, these results show that multi-agent LLM systems can deliver equitable, high-quality formative feedback at a scale and speed that would be impossible for human graders alone. More broadly, the work points toward a future where feedback-rich learning becomes feasible for any course size or context, advancing long-standing goals of equity, access, and instructional capacity in education.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u63d0\u4f9b\u516c\u5e73\u7684\u5f62\u6210\u6027\u53cd\u9988\uff0c\u901a\u8fc7\u4e94\u4e2a\u534f\u8c03\u7684\u89d2\u8272\u667a\u80fd\u4f53\u6765\u8bc4\u5206\u548c\u751f\u6210\u5b66\u4e60\u8005\u53cd\u9988\uff0c\u5728AI\u7d20\u517b\u8bfe\u7a0b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f62\u6210\u6027\u53cd\u9988\u662f\u5b66\u751f\u5b66\u4e60\u7684\u91cd\u8981\u9a71\u52a8\u529b\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u6216\u8d44\u6e90\u6709\u9650\u7684\u8bfe\u7a0b\u4e2d\u96be\u4ee5\u516c\u5e73\u5b9e\u65bd\uff0c\u6559\u5e08\u7f3a\u4e4f\u65f6\u95f4\u548c\u8d44\u6e90\u6765\u5ba1\u9605\u6240\u6709\u5b66\u751f\u53cd\u601d\uff0c\u5bfc\u81f4\u652f\u6301\u7f3a\u53e3\u3002", "method": "\u4f7f\u7528\u4e94\u4e2a\u534f\u8c03\u7684\u57fa\u4e8e\u89d2\u8272\u7684LLM\u667a\u80fd\u4f53\uff08\u8bc4\u4f30\u8005\u3001\u516c\u5e73\u76d1\u63a7\u8005\u3001\u5143\u8ba4\u77e5\u6559\u7ec3\u3001\u805a\u5408\u8005\u548c\u53cd\u601d\u5ba1\u9605\u8005\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u8bc4\u5206\u6807\u51c6\u5bf9\u5b66\u4e60\u8005\u53cd\u601d\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u751f\u6210\u7b80\u77ed\u3001\u65e0\u504f\u89c1\u7684\u5b66\u4e60\u8005\u53cd\u9988\u3002", "result": "\u572812\u8282AI\u7d20\u517b\u8bfe\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u4ea7\u751f\u7684\u8bc4\u5206\u6807\u51c6\u5f97\u5206\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u4e00\u81f4\u6027\uff0c\u8bad\u7ec3\u6709\u7d20\u7684\u8bc4\u5206\u8005\u8ba4\u4e3aAI\u751f\u6210\u7684\u8bc4\u8bba\u6709\u5e2e\u52a9\u3001\u6709\u540c\u7406\u5fc3\u4e14\u4e0e\u6559\u5b66\u76ee\u6807\u4e00\u81f4\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u80fd\u591f\u4ee5\u4eba\u7c7b\u8bc4\u5206\u8005\u65e0\u6cd5\u8fbe\u5230\u7684\u89c4\u6a21\u548c\u901f\u5ea6\u63d0\u4f9b\u516c\u5e73\u3001\u9ad8\u8d28\u91cf\u7684\u5f62\u6210\u6027\u53cd\u9988\uff0c\u4e3a\u5b9e\u73b0\u4efb\u4f55\u8bfe\u7a0b\u89c4\u6a21\u548c\u60c5\u5883\u4e0b\u7684\u53cd\u9988\u4e30\u5bcc\u5b66\u4e60\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.12634", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12634", "abs": "https://arxiv.org/abs/2511.12634", "authors": ["Manuel Rissel", "Marius Tucsnak"], "title": "Approximate Tracking Controllability of Systems with Quadratic Nonlinearities", "comment": "29 pages", "summary": "Given a finite-dimensional time continuous control system and $\\varepsilon>0$, we address the question of the existence of controls that maintain the corresponding state trajectories in the $\\varepsilon$-neighborhood of any prescribed path in the state space. We investigate this property, called approximate tracking controllability, for linear and quadratic time invariant systems. Concerning linear systems, our answers are negative: by developing a systematic approach, we demonstrate that approximate tracking controllability of the full state is impossible even in a certain weak sense, except for the trivial situation where the control space is isomorphic to the state space. Motivated by these negative findings for linear systems, we focus on nonlinear dynamics. In particular, we prove weak approximate tracking controllability on any time horizon for a general class of systems with arbitrary linear part and quadratic nonlinear terms. The considered weak notion of approximate tracking controllability involves the relaxation metric. We underline the relevance of this weak setting by developing applications to coupled systems (including motion planning problems) and by remarking obstructions that would arise for natural stronger norms. The exposed framework yields global results even if the uncontrolled dynamics might exhibit singularities in finite time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u7cfb\u7edf\u548c\u4e8c\u6b21\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\u3002\u5bf9\u4e8e\u7ebf\u6027\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u5b8c\u5168\u72b6\u6001\u7684\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u9664\u975e\u63a7\u5236\u7a7a\u95f4\u4e0e\u72b6\u6001\u7a7a\u95f4\u540c\u6784\u3002\u5bf9\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u5728\u4efb\u610f\u65f6\u95f4\u8303\u56f4\u5185\uff0c\u5177\u6709\u4efb\u610f\u7ebf\u6027\u90e8\u5206\u548c\u4e8c\u6b21\u975e\u7ebf\u6027\u9879\u7684\u7cfb\u7edf\u5177\u6709\u5f31\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\u3002", "motivation": "\u7814\u7a76\u63a7\u5236\u7cfb\u7edf\u80fd\u5426\u5728\u4efb\u610f\u7ed9\u5b9a\u8def\u5f84\u7684\u03b5\u90bb\u57df\u5185\u7ef4\u6301\u72b6\u6001\u8f68\u8ff9\uff0c\u5373\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\u3002\u7531\u4e8e\u7ebf\u6027\u7cfb\u7edf\u7684\u8d1f\u9762\u7ed3\u679c\uff0c\u8f6c\u5411\u7814\u7a76\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u53ef\u63a7\u6027\u3002", "method": "\u5bf9\u4e8e\u7ebf\u6027\u7cfb\u7edf\uff0c\u5f00\u53d1\u4e86\u7cfb\u7edf\u6027\u7684\u5206\u6790\u65b9\u6cd5\uff1b\u5bf9\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u7814\u7a76\u4e86\u5177\u6709\u4efb\u610f\u7ebf\u6027\u90e8\u5206\u548c\u4e8c\u6b21\u975e\u7ebf\u6027\u9879\u7684\u7cfb\u7edf\uff0c\u5e76\u91c7\u7528\u4e86\u6d89\u53ca\u677e\u5f1b\u5ea6\u91cf\u7684\u5f31\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\u6982\u5ff5\u3002", "result": "\u7ebf\u6027\u7cfb\u7edf\u7684\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u4e0d\u53ef\u80fd\u5b9e\u73b0\uff1b\u975e\u7ebf\u6027\u7cfb\u7edf\u5728\u5f31\u610f\u4e49\u4e0b\u5177\u6709\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\uff0c\u5373\u4f7f\u672a\u53d7\u63a7\u52a8\u529b\u5b66\u53ef\u80fd\u5728\u6709\u9650\u65f6\u95f4\u5185\u51fa\u73b0\u5947\u70b9\u3002", "conclusion": "\u7ebf\u6027\u7cfb\u7edf\u7684\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u800c\u5177\u6709\u4e8c\u6b21\u975e\u7ebf\u6027\u9879\u7684\u7cfb\u7edf\u5728\u5f31\u610f\u4e49\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u5168\u5c40\u7684\u8fd1\u4f3c\u8ddf\u8e2a\u53ef\u63a7\u6027\uff0c\u8fd9\u5728\u8026\u5408\u7cfb\u7edf\u548c\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.12014", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12014", "abs": "https://arxiv.org/abs/2511.12014", "authors": ["Truong Vo", "Sanmi Koyejo"], "title": "CURE: Cultural Understanding and Reasoning Evaluation - A Framework for \"Thick\" Culture Alignment Evaluation in LLMs", "comment": "7 pages, 5 figures", "summary": "Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6587\u5316\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u771f\u5b9e\u60c5\u5883\u4e0a\u4e0b\u6587\u6765\u6d4b\u8bd5\u6587\u5316\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u56db\u4e2a\u8865\u5145\u6307\u6807\u6765\u5168\u9762\u8bc4\u4f30\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6587\u5316\u80fd\u529b\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u8131\u79bb\u8bed\u5883\u7684\u6b63\u786e\u6027\u6216\u4e8c\u9009\u4e00\u5224\u65ad\uff0c\u5ffd\u89c6\u4e86\u6587\u5316\u7406\u89e3\u548c\u63a8\u7406\u7684\u9700\u6c42\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u771f\u5b9e\u60c5\u5883\u4e0a\u4e0b\u6587\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u9664\u4e86\u6807\u51c6\u7cbe\u786e\u5339\u914d\u6307\u6807\u5916\uff0c\u8fd8\u589e\u52a0\u4e86\u8986\u76d6\u7387\u3001\u7279\u5f02\u6027\u3001\u5185\u6db5\u548c\u8fde\u8d2f\u6027\u56db\u4e2a\u8865\u5145\u6307\u6807\uff0c\u8fdb\u884c\u539a\u8bc4\u4f30\u5206\u6790\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u663e\u793a\uff0c\u8584\u8bc4\u4f30\u4f1a\u7cfb\u7edf\u6027\u9ad8\u4f30\u6587\u5316\u80fd\u529b\u4e14\u8bc4\u4f30\u7ed3\u679c\u4e0d\u7a33\u5b9a\u3001\u65b9\u5dee\u5927\uff1b\u800c\u539a\u8bc4\u4f30\u80fd\u63ed\u793a\u63a8\u7406\u6df1\u5ea6\u5dee\u5f02\uff0c\u51cf\u5c11\u65b9\u5dee\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u6587\u5316\u7406\u89e3\u4fe1\u53f7\u3002", "conclusion": "\u539a\u8bc4\u4f30\u65b9\u6cd5\u6bd4\u4f20\u7edf\u8584\u8bc4\u4f30\u80fd\u66f4\u51c6\u786e\u5730\u8861\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2511.11622", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11622", "abs": "https://arxiv.org/abs/2511.11622", "authors": ["Alexis Roger", "Gwen Legate", "Kashif Rasul", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models", "comment": null, "summary": "Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u5206\u8bcd\u5668\u8bbe\u8ba1\uff08\u7f29\u653e\u548c\u91cf\u5316\u7b56\u7565\uff09\u4e0e\u8fc1\u79fb\u5b66\u4e60\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5206\u8bcd\u5668\u914d\u7f6e\u4e3b\u8981\u63a7\u5236\u6a21\u578b\u8868\u793a\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u800c\u8fc1\u79fb\u5b66\u4e60\u5f71\u54cd\u4f18\u5316\u6548\u7387\u548c\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5206\u8bcd\u5668\u548c\u8fc1\u79fb\u5b66\u4e60\u662f\u6784\u5efa\u6700\u5148\u8fdb\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u4f46\u5b83\u4eec\u5728\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\u548c\u4ea4\u4e92\u5173\u7cfb\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7ecf\u9a8c\u8bad\u7ec3\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u4e0d\u540c\u5206\u8bcd\u5668\u914d\u7f6e\uff08\u7279\u522b\u662f\u7f29\u653e\u548c\u91cf\u5316\u7b56\u7565\uff09\u4e0e\u9884\u8bad\u7ec3/\u968f\u673a\u521d\u59cb\u5316\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u8bbe\u8ba1\u826f\u597d\u7684\u5206\u8bcd\u5668\uff0c\u7279\u522b\u662f\u5728\u8f83\u5c0f\u8bcd\u6c47\u91cf\u65f6\uff1b\u800c\u9519\u8bef\u5bf9\u9f50\u7684\u5206\u8bcd\u4f1a\u524a\u5f31\u751a\u81f3\u9006\u8f6c\u9884\u8bad\u7ec3\u7684\u76ca\u5904\u3002\u7ed3\u5408\u5c0f\u578b\u9ad8\u6548\u8bcd\u6c47\u8868\u4e0e\u9884\u8bad\u7ec3\u6743\u91cd\u5728\u591a\u6a21\u6001\u9884\u6d4b\u8bbe\u7f6e\u4e2d\u7279\u522b\u6709\u5229\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u7cbe\u5fc3\u8bbe\u8ba1\u5206\u8bcd\u5668\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u79bb\u6563\u8868\u793a\u5b66\u4e60\u4e2d\u5206\u8bcd\u5668\u8bbe\u8ba1\u548c\u8fc1\u79fb\u5b66\u4e60\u5229\u7528\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2511.11945", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11945", "abs": "https://arxiv.org/abs/2511.11945", "authors": ["Mohammed Temraz", "Mark T Keane"], "title": "Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes", "comment": "31 pages, 8 figures", "summary": "In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of \"climate outlier events\". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.", "AI": {"tldr": "\u63d0\u51faCFA-SMOTE\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u7684\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u548cSMOTE\u65b9\u6cd5\uff0c\u89e3\u51b3\u6c14\u5019\u53d8\u5316\u9884\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5386\u53f2\u6570\u636e\u7f3a\u4e4f\u6781\u7aef\u6c14\u5019\u4e8b\u4ef6\u5b9e\u4f8b\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u9891\u53d1\uff0c\u4f46\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u5206\u5e03\uff0c\u96be\u4ee5\u5904\u7406\u5206\u5e03\u5916\u7684\u5f02\u5e38\u4e8b\u4ef6\u3002\u5386\u53f2\u6570\u636e\u96c6\u7f3a\u4e4f\u8db3\u591f\u7684\u6781\u7aef\u6c14\u5019\u4e8b\u4ef6\u5b9e\u4f8b\uff0c\u9020\u6210\u9884\u6d4b\u56f0\u96be\u3002", "method": "\u63d0\u51faCFA-SMOTE\u65b9\u6cd5\uff0c\u5c06\u53ef\u89e3\u91caAI\u4e2d\u7684\u5b9e\u4f8b\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u4e0e\u7ecf\u5178\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u6cd5SMOTE\u76f8\u7ed3\u5408\uff0c\u751f\u6210\u4ee3\u8868\u5f02\u5e38\u6c14\u5019\u4e8b\u4ef6\u7684\u5408\u6210\u6570\u636e\u70b9\u6765\u589e\u5f3a\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e0d\u540c\u7c7b\u522b\u4e0d\u5e73\u8861\u6bd4\u4f8b\u6761\u4ef6\u4e0b\uff0cCFA-SMOTE\u65b9\u6cd5\u5728\u9884\u6d4b\u7231\u5c14\u5170\u5976\u725b\u573a\u8349\u751f\u957f\uff08\u9488\u5bf92018\u5e74\u6b27\u6d32\u5e72\u65f1\u548c\u9972\u6599\u5371\u673a\uff09\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u7684\u53cd\u4e8b\u5b9e\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u6cd5\u3002", "conclusion": "CFA-SMOTE\u901a\u8fc7\u5c06\u6c14\u5019\u53d8\u5316\u9884\u6d4b\u95ee\u9898\u89c6\u4e3a\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u751f\u6210\u53cd\u4e8b\u5b9e\u5408\u6210\u6570\u636e\uff0c\u80fd\u591f\u6709\u6548\u6539\u5584\u5728\u6c14\u5019\u5f02\u5e38\u4e8b\u4ef6\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.12147", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12147", "abs": "https://arxiv.org/abs/2511.12147", "authors": ["Lifeng Shen", "Liang Peng", "Ruiwen Liu", "Shuyin Xia", "Yi Liu"], "title": "Finding Time Series Anomalies using Granular-ball Vector Data Description", "comment": "Accepted by AAAI 2026", "summary": "Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5ea6\u7403\u5411\u91cf\u6570\u636e\u63cf\u8ff0\uff08GVDD\uff09\u7684\u7c92\u5ea6\u7403\u5355\u7c7b\u7f51\u7edc\uff08GBOC\uff09\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u5bc6\u5ea6\u5f15\u5bfc\u7684\u5c42\u6b21\u5206\u88c2\u8fc7\u7a0b\u751f\u6210\u7d27\u51d1\u7684\u9ad8\u5bc6\u5ea6\u533a\u57df\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982\u6700\u8fd1\u90bb\u548c\u805a\u7c7b\u5728\u52a8\u6001\u975e\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u4f9d\u8d56\u521a\u6027\u5047\u8bbe\uff08\u5982\u9884\u5b9a\u4e49\u7684\u53ef\u9760\u90bb\u5c45\u6216\u805a\u7c7b\u6570\u91cf\uff09\uff0c\u5728\u590d\u6742\u65f6\u95f4\u573a\u666f\u4e2d\u7ecf\u5e38\u5931\u6548\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6570\u636e\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528GVDD\u5c06\u6f5c\u5728\u7a7a\u95f4\u5212\u5206\u4e3a\u7531\u7c92\u5ea6\u7403\u8868\u793a\u7684\u7d27\u51d1\u9ad8\u5bc6\u5ea6\u533a\u57df\uff0c\u901a\u8fc7\u5bc6\u5ea6\u5f15\u5bfc\u7684\u5c42\u6b21\u5206\u88c2\u8fc7\u7a0b\u751f\u6210\u5e76\u53bb\u9664\u566a\u58f0\u7ed3\u6784\u3002\u8bad\u7ec3\u65f6\u901a\u8fc7\u5c06\u6837\u672c\u4e0e\u6700\u8fd1\u7684\u7c92\u5ea6\u7403\u4e2d\u5fc3\u5bf9\u9f50\u6765\u63d0\u9ad8\u8868\u793a\u7d27\u51d1\u6027\uff0c\u63a8\u7406\u65f6\u57fa\u4e8e\u5230\u6700\u8fd1\u7c92\u5ea6\u7403\u7684\u8ddd\u79bb\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u8868\u660e\u5176\u80fd\u591f\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5173\u6ce8\u5bc6\u96c6\u9ad8\u8d28\u91cf\u533a\u57df\u5e76\u663e\u8457\u51cf\u5c11\u539f\u578b\u6570\u91cf\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "conclusion": "GBOC\u65b9\u6cd5\u901a\u8fc7\u6570\u636e\u81ea\u9002\u5e94\u7684\u7c92\u5ea6\u7403\u8868\u793a\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12758", "categories": ["eess.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.12758", "abs": "https://arxiv.org/abs/2511.12758", "authors": ["Shih-Chi Liao", "Maziar S. Hemati", "Peter Seiler"], "title": "On Boundedness of Quadratic Dynamics with Energy-Preserving Nonlinearity", "comment": "9 pages, 1 figures", "summary": "Boundedness is an important property of many physical systems. This includes incompressible fluid flows, which are often modeled by quadratic dynamics with an energy-preserving nonlinearity. For such systems, Schlegel and Noack proposed a sufficient condition for boundedness utilizing quadratic Lyapunov functions. They also propose a necessary condition for boundedness aiming to provide a more complete characterization of boundedness in this class of models. The sufficient condition is based on Lyapunov theory and is true. Our paper focuses on this necessary condition. We use an independent proof to show that the condition is true for two dimensional systems. However, we provide a three dimensional counterexample to illustrate that the necessary condition fails to hold in higher dimensions. Our results highlight a theoretical gap in boundedness analysis and suggest future directions to address the conservatism.", "AI": {"tldr": "\u672c\u6587\u9a8c\u8bc1\u4e86Schlegel\u548cNoack\u63d0\u51fa\u7684\u6709\u754c\u6027\u5fc5\u8981\u6761\u4ef6\u7684\u6709\u6548\u6027\uff1a\u5728\u4e8c\u7ef4\u7cfb\u7edf\u4e2d\u6210\u7acb\uff0c\u4f46\u5728\u4e09\u7ef4\u7cfb\u7edf\u4e2d\u5b58\u5728\u53cd\u4f8b\u3002", "motivation": "\u7814\u7a76\u4e8c\u6b21\u80fd\u91cf\u5b88\u6052\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u6709\u754c\u6027\u5206\u6790\uff0c\u7279\u522b\u662f\u9a8c\u8bc1Schlegel\u548cNoack\u63d0\u51fa\u7684\u5fc5\u8981\u6761\u4ef6\u7684\u9002\u7528\u8303\u56f4\u3002", "method": "\u4f7f\u7528\u72ec\u7acb\u8bc1\u660e\u9a8c\u8bc1\u4e8c\u7ef4\u7cfb\u7edf\u7684\u5fc5\u8981\u6761\u4ef6\u6210\u7acb\uff0c\u5e76\u6784\u9020\u4e09\u7ef4\u53cd\u4f8b\u8bc1\u660e\u8be5\u6761\u4ef6\u5728\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u4e0d\u6210\u7acb\u3002", "result": "\u5fc5\u8981\u6761\u4ef6\u5728\u4e8c\u7ef4\u7cfb\u7edf\u4e2d\u6210\u7acb\uff0c\u4f46\u5728\u4e09\u7ef4\u7cfb\u7edf\u4e2d\u5b58\u5728\u53cd\u4f8b\uff0c\u8868\u660e\u8be5\u6761\u4ef6\u4e0d\u80fd\u63a8\u5e7f\u5230\u9ad8\u7ef4\u60c5\u51b5\u3002", "conclusion": "\u6709\u754c\u6027\u5206\u6790\u5b58\u5728\u7406\u8bba\u7f3a\u53e3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u51cf\u5c11\u4fdd\u5b88\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2511.11775", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11775", "abs": "https://arxiv.org/abs/2511.11775", "authors": ["Aristotelis Magklis", "Andreas Kamilaris"], "title": "Data-driven strategic sensor placement for detecting disinfection by-products in water distribution networks", "comment": null, "summary": "Disinfection byproducts are contaminants that can cause long-term effects on human health, occurring in chlorinated drinking water when the disinfectant interacts with natural organic matter. Their formation is affected by many environmental parameters, making it difficult to monitor and detect disinfection byproducts before they reach households. Due to the large variety of disinfection byproduct compounds that can be formed in water distribution networks, plus the constrained number of sensors that can be deployed throughout a system to monitor these contaminants, it is of outmost importance to place sensory equipment efficiently and optimally. In this paper, we present DBPFinder, a simulation software that assists in the strategic sensor placement for detecting disinfection byproducts, tested at a real-world water distribution network in Coimbra, Portugal. This simulator addresses multiple performance objectives at once in order to provide optimal solution placement recommendations to water utility operators based on their needs. A number of different experiments performed indicate its correctness, relevance, efficiency and scalability.", "AI": {"tldr": "DBPFinder\u662f\u4e00\u4e2a\u6a21\u62df\u8f6f\u4ef6\uff0c\u7528\u4e8e\u4f18\u5316\u6d88\u6bd2\u526f\u4ea7\u7269\u68c0\u6d4b\u4f20\u611f\u5668\u7684\u6218\u7565\u5e03\u5c40\uff0c\u5df2\u5728\u8461\u8404\u7259\u79d1\u82f1\u5e03\u62c9\u7684\u771f\u5b9e\u4f9b\u6c34\u7f51\u7edc\u4e2d\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "motivation": "\u6d88\u6bd2\u526f\u4ea7\u7269\u662f\u6c2f\u5316\u996e\u7528\u6c34\u4e2d\u7684\u6709\u5bb3\u6c61\u67d3\u7269\uff0c\u5176\u5f62\u6210\u53d7\u591a\u79cd\u73af\u5883\u53c2\u6570\u5f71\u54cd\uff0c\u96be\u4ee5\u5728\u8fdb\u5165\u5bb6\u5ead\u524d\u76d1\u6d4b\u3002\u7531\u4e8e\u6d88\u6bd2\u526f\u4ea7\u7269\u79cd\u7c7b\u7e41\u591a\u4e14\u4f20\u611f\u5668\u90e8\u7f72\u6570\u91cf\u6709\u9650\uff0c\u9700\u8981\u9ad8\u6548\u4f18\u5316\u4f20\u611f\u5668\u5e03\u5c40\u3002", "method": "\u5f00\u53d1DBPFinder\u6a21\u62df\u8f6f\u4ef6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u4e3a\u6c34\u52a1\u8fd0\u8425\u5546\u63d0\u4f9b\u57fa\u4e8e\u9700\u6c42\u7684\u4f20\u611f\u5668\u6700\u4f18\u5e03\u5c40\u5efa\u8bae\u3002", "result": "\u5728\u771f\u5b9e\u4f9b\u6c34\u7f51\u7edc\u4e2d\u7684\u591a\u9879\u5b9e\u9a8c\u8868\u660e\u8be5\u8f6f\u4ef6\u5177\u6709\u6b63\u786e\u6027\u3001\u76f8\u5173\u6027\u3001\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DBPFinder\u80fd\u591f\u6709\u6548\u534f\u52a9\u6c34\u52a1\u8fd0\u8425\u5546\u4f18\u5316\u6d88\u6bd2\u526f\u4ea7\u7269\u68c0\u6d4b\u4f20\u611f\u5668\u7684\u6218\u7565\u5e03\u5c40\uff0c\u63d0\u9ad8\u76d1\u6d4b\u6548\u7387\u3002"}}
{"id": "2511.12665", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.12665", "abs": "https://arxiv.org/abs/2511.12665", "authors": ["Saverio Salzo"], "title": "The iterates of FISTA convergence even under inexact computations", "comment": null, "summary": "Very recently, the papers \"Point Convergence of Nesterov's Accelerated Gradient Method: An AI-Assisted Proof\" by Jang and Ryu, and \"The Iterates of Nesterov's Accelerated Algorithm Converge in the Critical Regimes\" by Bot, Fadili, and Nguyen simultaneously have resolved a long-standing open problem concerning Nesterov's accelerated gradient method. These works show that the iterates of the algorithm (known in its composite form as FISTA) indeed converge to an optimal solution. In this work, we extend these results and prove that, in infinite dimensional Hilbert spaces, the iterates of such an algorithm still converge (in the weak sense) even under inexact computation of the proximity operator and the gradient.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86\u6700\u8fd1\u5173\u4e8eNesterov\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\u6536\u655b\u6027\u7684\u7a81\u7834\u6027\u6210\u679c\uff0c\u8bc1\u660e\u4e86\u5728\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\uff0c\u5373\u4f7f\u90bb\u8fd1\u7b97\u5b50\u548c\u68af\u5ea6\u7684\u8ba1\u7b97\u5b58\u5728\u8bef\u5dee\uff0c\u8be5\u7b97\u6cd5\u7684\u8fed\u4ee3\u4ecd\u7136\u4fdd\u6301\u5f31\u6536\u655b\u6027\u3002", "motivation": "\u6700\u8fd1Jang\u548cRyu\u4ee5\u53caBot\u7b49\u4eba\u7684\u5de5\u4f5c\u89e3\u51b3\u4e86Nesterov\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\u8fed\u4ee3\u6536\u655b\u7684\u957f\u671f\u5f00\u653e\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5c06\u8fd9\u4e9b\u7ed3\u679c\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u5e76\u8003\u8651\u8ba1\u7b97\u4e0d\u7cbe\u786e\u7684\u60c5\u51b5\u3002", "method": "\u5728\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\uff0c\u5206\u6790Nesterov\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\uff08FISTA\uff09\u5728\u90bb\u8fd1\u7b97\u5b50\u548c\u68af\u5ea6\u8ba1\u7b97\u5b58\u5728\u8bef\u5dee\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u884c\u4e3a\uff0c\u8bc1\u660e\u5176\u8fed\u4ee3\u7684\u5f31\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u90bb\u8fd1\u7b97\u5b50\u548c\u68af\u5ea6\u8ba1\u7b97\u4e0d\u7cbe\u786e\u7684\u60c5\u51b5\u4e0b\uff0cNesterov\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\u7684\u8fed\u4ee3\u5728\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u4ecd\u7136\u4fdd\u6301\u5f31\u6536\u655b\u5230\u6700\u4f18\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06Nesterov\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\u7684\u6536\u655b\u6027\u7ed3\u679c\u63a8\u5e7f\u5230\u4e86\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u5e76\u4e14\u653e\u5bbd\u4e86\u5bf9\u8ba1\u7b97\u7cbe\u5ea6\u7684\u8981\u6c42\uff0c\u589e\u5f3a\u4e86\u8be5\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.12109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12109", "abs": "https://arxiv.org/abs/2511.12109", "authors": ["Felipe Fujita", "Hideyuki Takada"], "title": "Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task", "comment": null, "summary": "In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.", "AI": {"tldr": "\u7ed3\u5408\u53cd\u5411\u7ffb\u8bd1\u548c\u5fae\u8c03\u5728\u5c0f\u89c4\u6a21\u65e5\u8bed\u8bed\u6599\u4e0a\u663e\u8457\u63d0\u5347\u82f1\u65e5\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\uff0cCOMET\u5206\u6570\u4ece0.460\u63d0\u5347\u81f30.597\u3002", "motivation": "\u63a2\u7d22\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u53cd\u5411\u7ffb\u8bd1\u548c\u5fae\u8c03\u7684\u7ec4\u5408\u7b56\u7565\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u9996\u5148\u4f7f\u7528\u53cd\u5411\u7ffb\u8bd1\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7136\u540e\u5728\u771f\u5b9e\u5c0f\u89c4\u6a21\u5e73\u884c\u8bed\u6599\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6700\u540e\u5c06\u4e24\u79cd\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5355\u72ec\u53cd\u5411\u7ffb\u8bd1\u4f7fCOMET\u4ece0.460\u63d0\u5347\u81f30.468\uff1b\u5355\u72ec\u5fae\u8c03\u63d0\u5347\u81f30.589\uff1b\u4e24\u8005\u7ed3\u5408\u8fbe\u52300.597\u7684\u6700\u4f73\u6548\u679c\u3002", "conclusion": "\u53cd\u5411\u7ffb\u8bd1\u548c\u9488\u5bf9\u6027\u5fae\u8c03\u7684\u534f\u540c\u4f7f\u7528\u80fd\u591f\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u6539\u8fdb\u7b56\u7565\u3002"}}
{"id": "2511.11623", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11623", "abs": "https://arxiv.org/abs/2511.11623", "authors": ["Yushan Jiang", "Shuteng Niu", "Dongjin Song", "Yichen Wang", "Jingna Feng", "Xinyue Hu", "Liu Yang", "Cui Tao"], "title": "Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data", "comment": null, "summary": "Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u65e9\u671f\u9884\u6d4b\u809d\u79fb\u690d\u540e\u7684\u79fb\u690d\u7269\u6297\u5bbf\u4e3b\u75c5(GVHD)\uff0c\u901a\u8fc7\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u5728\u6781\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u6027\u80fd\u3002", "motivation": "GVHD\u662f\u809d\u79fb\u690d\u4e2d\u7f55\u89c1\u4f46\u81f4\u547d\u7684\u5e76\u53d1\u75c7\uff0c\u6b7b\u4ea1\u7387\u6781\u9ad8\u3002\u901a\u8fc7\u6574\u5408\u5f02\u6784\u548c\u4e0d\u5e73\u8861\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u65e8\u5728\u63a8\u8fdbGVHD\u7684\u65e9\u671f\u9884\u6d4b\uff0c\u4e3a\u53ca\u65f6\u5e72\u9884\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u94fa\u5e73\u9053\u8def\u3002", "method": "\u5206\u6790\u4e862100\u540d\u809d\u79fb\u690d\u60a3\u8005\u7684\u672f\u524d\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u5305\u62ec42\u4f8bGVHD\u75c5\u4f8b\u3002\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u878d\u5408\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u3001\u8bca\u65ad\u548c\u836f\u7269\u56db\u79cd\u4e3b\u8981\u6a21\u6001\uff0c\u5904\u7406\u4e0d\u89c4\u5219\u8bb0\u5f55\u548c\u7f3a\u5931\u503c\uff0c\u5e76\u901a\u8fc7AUC\u4f18\u5316\u89e3\u51b3\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u8be5\u6846\u67b6\u5728\u6240\u6709\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5b9e\u73b0\u4e86AUC 0.836\u3001AUPRC 0.157\u3001\u53ec\u56de\u73870.768\u548c\u7279\u5f02\u60270.803\u3002\u8bc1\u660e\u4e86\u4ece\u4e0d\u540c\u6a21\u6001\u6355\u83b7\u4e92\u8865\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u663e\u8457\u6539\u8fdb\u4e86GVHD\u65e9\u671f\u9884\u6d4b\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u5f02\u6784\u6027\u548c\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u65e9\u671f\u9884\u6d4b\u3002"}}
{"id": "2511.11954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11954", "abs": "https://arxiv.org/abs/2511.11954", "authors": ["Borchuluun Yadamsuren", "Steven Keith Platt", "Miguel Diaz"], "title": "LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code", "comment": "29 pages, 3 appendices with Prolog code and full codebase available at: https://github.com/borchuluun/section121-inconsistency-detection", "summary": "This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.\n  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.\n  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.\n  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u7b26\u53f7\u903b\u8f91\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u6cd5\u5f8b\u4e2d\u6cd5\u5b9a\u4e0d\u4e00\u81f4\u6027\u7684\u786e\u5b9a\u6027\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u5904\u7406\u5c42\u6b21\u5316\u5904\u7406\u548c\u6df1\u5ea6\u7ed3\u6784\u5316\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u957f\u6587\u672c\u4e2d\u3002\u7a0e\u6536\u9886\u57df\u7684\u7279\u5b9a\u5e94\u7528\u4ecd\u7136\u7a00\u5c11\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u4f7f\u7528GPT-4o\u5c06\u7f8e\u56fd\u56fd\u5185\u7a0e\u6536\u6cd5\u5178\u7b2c121\u6761\u7ffb\u8bd1\u4e3aProlog\u89c4\u5219\uff0c\u5728SWISH\u4e2d\u7cbe\u70bc\u3002\u901a\u8fc7Prolog\u589e\u5f3a\u63d0\u793a\u6d4b\u8bd5\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u6548\u679c\uff0c\u5e76\u4e0e\u7eaf\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5bf9\u6bd4\u3002", "result": "GPT-4o\u5728\u4e09\u79cd\u7b56\u7565\u4e2d\u4ec5\u68c0\u6d4b\u5230\u4e00\u79cd\u4e0d\u4e00\u81f4\u6027\uff0833%\u51c6\u786e\u7387\uff09\u3002\u7eaf\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b0100%\u89c4\u5219\u8986\u76d6\uff0c\u800cProlog\u589e\u5f3a\u63d0\u793a\u4ec566%\u8986\u76d6\u3002\u6df7\u5408Prolog\u6a21\u578b\u4ea7\u751f\u786e\u5b9a\u6027\u7ed3\u679c\uff0c\u6210\u529f\u68c0\u6d4b\u5230\u4e0d\u4e00\u81f4\u533a\u57df\u3002", "conclusion": "\u57fa\u4e8e\u7b26\u53f7\u903b\u8f91\u7684LLM\u8f85\u52a9\u5f62\u5f0f\u5316\u80fd\u591f\u5b9e\u73b0\u900f\u660e\u53ef\u9760\u7684\u6cd5\u5b9a\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\uff0c\u6df7\u5408\u65b9\u6cd5\u4f18\u4e8e\u7eaf\u6982\u7387\u6027\u63d0\u793a\u65b9\u6cd5\u3002"}}
{"id": "2511.12180", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12180", "abs": "https://arxiv.org/abs/2511.12180", "authors": ["Ge Cheng", "Shuo Wang", "Yun Zhang"], "title": "Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering", "comment": "31 pages, 8 figures", "summary": "Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SC-InfoNCE\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u8c03\u8282\u7684\u6536\u655b\u76ee\u6807\u6765\u7075\u6d3b\u63a7\u5236\u7279\u5f81\u76f8\u4f3c\u6027\u5bf9\u9f50\uff0c\u5728\u56fe\u50cf\u3001\u56fe\u7ed3\u6784\u548c\u6587\u672c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1InfoNCE\u5728\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u7ecf\u9a8c\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u4ecd\u7136\u6709\u9650\u3002\u4f5c\u8005\u65e8\u5728\u6df1\u5165\u7406\u89e3InfoNCE\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u5e76\u6539\u8fdb\u5176\u6027\u80fd\u3002", "method": "\u5f15\u5165\u663e\u5f0f\u7279\u5f81\u7a7a\u95f4\u5efa\u6a21\u6837\u672c\u7684\u589e\u5f3a\u89c6\u56fe\uff0c\u4f7f\u7528\u8f6c\u79fb\u6982\u7387\u77e9\u9635\u6355\u6349\u6570\u636e\u589e\u5f3a\u52a8\u6001\uff0c\u63d0\u51faSC-InfoNCE\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u7f29\u653e\u76ee\u6807\u77e9\u9635\u6765\u7075\u6d3b\u63a7\u5236\u7279\u5f81\u76f8\u4f3c\u6027\u5bf9\u9f50\u3002", "result": "\u5728\u56fe\u50cf\u3001\u56fe\u7ed3\u6784\u548c\u6587\u672c\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSC-InfoNCE\u5728\u4e0d\u540c\u9886\u57df\u90fd\u5b9e\u73b0\u4e86\u5f3a\u5927\u4e14\u53ef\u9760\u7684\u6027\u80fd\u3002", "conclusion": "SC-InfoNCE\u901a\u8fc7\u5f15\u5165\u53ef\u8c03\u8282\u7684\u6536\u655b\u76ee\u6807\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5339\u914d\u4e0b\u6e38\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u6709\u6548\u7684\u8bad\u7ec3\u76ee\u6807\u3002"}}
{"id": "2511.12826", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12826", "abs": "https://arxiv.org/abs/2511.12826", "authors": ["Sahel Vahedi Noori", "Bin Hu", "Geir Dullerud", "Peter Seiler"], "title": "Discrete-Time Stability Analysis of ReLU Feedback Systems via Integral Quadratic Constraints", "comment": null, "summary": "This paper analyzes internal stability of a discrete-time feedback system with a ReLU nonlinearity. This feedback system is motivated by recurrent neural networks. We first review existing static quadratic constraints (QCs) for slope-restricted nonlinearities. Next, we derive hard integral quadratic constraints (IQCs) for scalar ReLU by using finite impulse filters and structured matrices. These IQCs are combined with a dissipation inequality leading to an LMI condition that certifies internal stability. We show that our new dynamic IQCs for ReLU are a superset of the well-known Zames-Falb IQCs specified for slope-restricted nonlinearities. Numerical results show that the proposed hard IQCs give less conservative stability margins than Zames-Falb multipliers and prior static QC methods, sometimes dramatically so.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5e26\u6709ReLU\u975e\u7ebf\u6027\u7684\u79bb\u6563\u65f6\u95f4\u53cd\u9988\u7cfb\u7edf\u7684\u5185\u90e8\u7a33\u5b9a\u6027\uff0c\u63a8\u5bfc\u4e86ReLU\u7684\u786c\u79ef\u5206\u4e8c\u6b21\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7LMI\u6761\u4ef6\u9a8c\u8bc1\u7a33\u5b9a\u6027\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u4fdd\u5b88\u6027\u66f4\u4f4e\u3002", "motivation": "\u7814\u7a76\u7531\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u542f\u53d1\u7684\u5e26\u6709ReLU\u975e\u7ebf\u6027\u7684\u79bb\u6563\u65f6\u95f4\u53cd\u9988\u7cfb\u7edf\u7684\u5185\u90e8\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6709\u9650\u8109\u51b2\u6ee4\u6ce2\u5668\u548c\u7ed3\u6784\u5316\u77e9\u9635\u63a8\u5bfc\u6807\u91cfReLU\u7684\u786c\u79ef\u5206\u4e8c\u6b21\u7ea6\u675f\uff0c\u7ed3\u5408\u8017\u6563\u4e0d\u7b49\u5f0f\u5f97\u5230LMI\u7a33\u5b9a\u6027\u6761\u4ef6\u3002", "result": "\u63d0\u51fa\u7684\u52a8\u6001IQCs\u662fZames-Falb IQCs\u7684\u8d85\u96c6\uff0c\u6570\u503c\u7ed3\u679c\u8868\u660e\u6bd4Zames-Falb\u4e58\u5b50\u548c\u9759\u6001QC\u65b9\u6cd5\u7ed9\u51fa\u66f4\u4e0d\u4fdd\u5b88\u7684\u7a33\u5b9a\u6027\u88d5\u5ea6\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u786cIQCs\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u9a8c\u8bc1\u5e26\u6709ReLU\u975e\u7ebf\u6027\u7684\u53cd\u9988\u7cfb\u7edf\u7684\u5185\u90e8\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.11790", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11790", "abs": "https://arxiv.org/abs/2511.11790", "authors": ["Peter Kirgis"], "title": "Differences in the Moral Foundations of Large Language Models", "comment": null, "summary": "Large language models are increasingly being used in critical domains of politics, business, and education, but the nature of their normative ethical judgment remains opaque. Alignment research has, to date, not sufficiently utilized perspectives and insights from the field of moral psychology to inform training and evaluation of frontier models. I perform a synthetic experiment on a wide range of models from most major model providers using Jonathan Haidt's influential moral foundations theory (MFT) to elicit diverse value judgments from LLMs. Using multiple descriptive statistical approaches, I document the bias and variance of large language model responses relative to a human baseline in the original survey. My results suggest that models rely on different moral foundations from one another and from a nationally representative human baseline, and these differences increase as model capabilities increase. This work seeks to spur further analysis of LLMs using MFT, including finetuning of open-source models, and greater deliberation by policymakers on the importance of moral foundations for LLM alignment.", "AI": {"tldr": "\u4f7f\u7528\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f26\u7406\u5224\u65ad\u504f\u5dee\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u95f4\u53ca\u4e0e\u4eba\u7c7b\u57fa\u51c6\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u968f\u6a21\u578b\u80fd\u529b\u589e\u5f3a\u800c\u6269\u5927", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u3001\u5546\u4e1a\u548c\u6559\u80b2\u7b49\u5173\u952e\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u89c4\u8303\u6027\u4f26\u7406\u5224\u65ad\u672c\u8d28\u4ecd\u4e0d\u900f\u660e\uff0c\u9700\u8981\u4ece\u9053\u5fb7\u5fc3\u7406\u5b66\u89d2\u5ea6\u8bc4\u4f30\u6a21\u578b\u5bf9\u9f50", "method": "\u57fa\u4e8eJonathan Haidt\u7684\u9053\u5fb7\u57fa\u7840\u7406\u8bba\uff0c\u5bf9\u4e3b\u8981\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u5927\u8303\u56f4\u6a21\u578b\u8fdb\u884c\u5408\u6210\u5b9e\u9a8c\uff0c\u4f7f\u7528\u591a\u79cd\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u6a21\u578b\u54cd\u5e94\u4e0e\u4eba\u7c7b\u57fa\u51c6\u7684\u504f\u5dee\u548c\u65b9\u5dee", "result": "\u6a21\u578b\u95f4\u4f9d\u8d56\u4e0d\u540c\u7684\u9053\u5fb7\u57fa\u7840\uff0c\u4e0e\u5168\u56fd\u4ee3\u8868\u6027\u4eba\u7c7b\u57fa\u51c6\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u8fd9\u4e9b\u5dee\u5f02\u968f\u6a21\u578b\u80fd\u529b\u589e\u5f3a\u800c\u589e\u52a0", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u4f7f\u7528\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u5206\u6790LLMs\uff0c\u5305\u62ec\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u4fc3\u4f7f\u653f\u7b56\u5236\u5b9a\u8005\u66f4\u6df1\u5165\u601d\u8003\u9053\u5fb7\u57fa\u7840\u5728\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u91cd\u8981\u6027"}}
{"id": "2511.12836", "categories": ["math.OC", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12836", "abs": "https://arxiv.org/abs/2511.12836", "authors": ["Waheed U. Bajwa", "Mert Gurbuzbalaban", "Mustafa Ali Kutbay", "Lingjiong Zhu", "Muhammad Zulqarnain"], "title": "DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks", "comment": null, "summary": "Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\\sqrt\u03b7)$ neighborhood of the target distribution, where $\u03b7$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86DIGing-SGLD\u7b97\u6cd5\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u65f6\u53d8\u7f51\u7edc\u4e0a\u7684\u53bb\u4e2d\u5fc3\u5316SGLD\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u9759\u6001\u7f51\u7edc\u548c\u5b58\u5728\u7a33\u6001\u91c7\u6837\u504f\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316SGLD\u65b9\u6cd5\u4ec5\u9650\u4e8e\u9759\u6001\u7f51\u7edc\u62d3\u6251\uff0c\u4e14\u8bb8\u591a\u65b9\u6cd5\u5b58\u5728\u7531\u7f51\u7edc\u6548\u5e94\u5f15\u8d77\u7684\u7a33\u6001\u91c7\u6837\u504f\u5dee\uff0c\u5373\u4f7f\u4f7f\u7528\u5168\u6279\u91cf\u6570\u636e\u65f6\u4e5f\u662f\u5982\u6b64\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u65f6\u53d8\u7f51\u7edc\u4e0a\u8fdb\u884c\u9ad8\u6548\u65e0\u504f\u91c7\u6837\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u57fa\u4e8eLangevin\u7684\u91c7\u6837\u4e0eDIGing\u7b97\u6cd5\u7684\u68af\u5ea6\u8ddf\u8e2a\u673a\u5236\u76f8\u7ed3\u5408\uff0cDIGing\u7b97\u6cd5\u6700\u521d\u662f\u4e3a\u65f6\u53d8\u7f51\u7edc\u4e0a\u7684\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u800c\u5f00\u53d1\u7684\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u4e2d\u592e\u534f\u8c03\u5668\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u65e0\u504f\u91c7\u6837\u3002", "result": "\u5728\u6807\u51c6\u5f3a\u51f8\u6027\u548c\u5e73\u6ed1\u6027\u5047\u8bbe\u4e0b\uff0cDIGing-SGLD\u5b9e\u73b0\u4e86\u5bf9\u76ee\u6807\u5206\u5e03\u7684\u51e0\u4f55\u6536\u655b\uff0c\u8fbe\u5230$O(\\sqrt\u03b7)$\u90bb\u57df\uff0c\u5176\u4e2d$\u03b7$\u662f\u6b65\u957f\u3002\u63d0\u4f9b\u4e86\u9996\u4e2a\u9488\u5bf9\u65f6\u53d8\u7f51\u7edc\u4e0a\u57fa\u4e8e\u53bb\u4e2d\u5fc3\u5316SGLD\u91c7\u6837\u7684\u6709\u9650\u65f6\u95f4\u975e\u6e10\u8fd1Wasserstein\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "DIGing-SGLD\u5728\u52a8\u6001\u6f14\u5316\u7f51\u7edc\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5b9e\u8bc1\u6027\u80fd\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8d1d\u53f6\u65af\u7ebf\u6027\u548c\u903b\u8f91\u56de\u5f52\u4efb\u52a1\u4e2d\u6709\u6548\u3002"}}
{"id": "2511.12116", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12116", "abs": "https://arxiv.org/abs/2511.12116", "authors": ["Piotr P\u0119zik", "Konrad Kaczy\u0144ski", "Maria Szyma\u0144ska", "Filip \u017barnecki", "Zuzanna Deckert", "Jakub Kwiatkowski", "Wojciech Janowski"], "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.", "AI": {"tldr": "LLMLagBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u65f6\u95f4\u8fb9\u754c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u6a21\u578b\u5bf9\u8fd1\u671f\u4e8b\u4ef6\u7684\u4e86\u89e3\u7a0b\u5ea6\u6765\u8bc6\u522b\u5176\u77e5\u8bc6\u7684\u65b0\u9c9c\u5ea6\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u65f6\u95f4\u70b9\u524d\u7684\u6587\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u5f62\u6210\u4e86\u4e25\u683c\u7684\u77e5\u8bc6\u8fb9\u754c\u3002\u5f53\u8fd9\u4e2a\u9650\u5236\u672a\u77e5\u6216\u88ab\u5ffd\u89c6\u65f6\uff0c\u6a21\u578b\u53ef\u80fd\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u65e0\u610f\u95f4\u6df7\u5408\u8fc7\u65f6\u7684\u65f6\u6548\u6027\u4fe1\u606f\u4e0e\u4e00\u822c\u77e5\u8bc6\uff0c\u4ece\u800c\u5f71\u54cd\u56de\u7b54\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165LLMLagBench\u4f5c\u4e3a\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5bf9\u8fd1\u671f\u4e8b\u4ef6\u7684\u77e5\u8bc6\u6765\u8bc6\u522b\u5176\u8bad\u7ec3\u6570\u636e\u7684\u6700\u65e9\u53ef\u80fd\u65f6\u95f4\u8fb9\u754c\u3002\u5c06\u8be5\u57fa\u51c6\u5e94\u7528\u4e8e\u8bc4\u4f30\u5927\u91cfLLM\uff0c\u5305\u62ec\u5177\u6709\u660e\u786e\u58f0\u660e\u548c\u672a\u58f0\u660e\u8bad\u7ec3\u622a\u6b62\u65e5\u671f\u7684\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u624b\u52a8\u9a8c\u8bc1\u548c\u4e0e\u516c\u5f00\u53d1\u5e03\u7684LLM\u9884\u8bad\u7ec3\u4fe1\u606f\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u8be5\u57fa\u51c6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "LLMLagBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc6\u522bLLM\u7684\u77e5\u8bc6\u65f6\u95f4\u8fb9\u754c\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u7684\u77e5\u8bc6\u65b0\u9c9c\u5ea6\u548c\u6f5c\u5728\u7684\u4fe1\u606f\u6df7\u5408\u95ee\u9898\u3002"}}
{"id": "2511.11625", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11625", "abs": "https://arxiv.org/abs/2511.11625", "authors": ["Mohammad Karami", "Mohammad Reza Nemati", "Aidin Kazemi", "Ali Mikaeili Barzili", "Hamid Azadegan", "Behzad Moshiri"], "title": "MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks", "comment": null, "summary": "Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.\n  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy", "AI": {"tldr": "MedFedPure\u662f\u4e00\u4e2a\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u533b\u7597AI\u6a21\u578b\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u4e2a\u6027\u5316FL\u3001\u63a9\u7801\u81ea\u7f16\u7801\u5668\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u6269\u6563\u51c0\u5316\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u533b\u7597AI\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u53bb\u4e2d\u5fc3\u5316\u7684\u533b\u7597\u73af\u5883\uff0c\u9700\u8981\u5728\u4e0d\u5f71\u54cd\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u4fdd\u62a4\u8bca\u65ad\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u9002\u5e94\u5404\u673a\u6784\u6570\u636e\u5206\u5e03\uff1b\u63a9\u7801\u81ea\u7f16\u7801\u5668\u68c0\u6d4b\u53ef\u7591\u8f93\u5165\uff1b\u81ea\u9002\u5e94\u6269\u6563\u51c0\u5316\u6a21\u5757\u9009\u62e9\u6027\u6e05\u7406\u88ab\u6807\u8bb0\u7684\u626b\u63cf\u56fe\u50cf\u3002", "result": "\u5728Br35H\u8111\u90e8MRI\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u6297\u9c81\u68d2\u6027\u4ece49.50%\u663e\u8457\u63d0\u5347\u81f387.33%\uff0c\u540c\u65f6\u4fdd\u630197.67%\u7684\u5e72\u51c0\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u90e8\u7f72\u5b89\u5168\u3001\u53ef\u4fe1\u4e14\u4fdd\u62a4\u9690\u79c1\u7684AI\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u80fd\u591f\u5728\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u672c\u5730\u5b9e\u65f6\u8fd0\u884c\u3002"}}
{"id": "2511.11990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11990", "abs": "https://arxiv.org/abs/2511.11990", "authors": ["Shaoqi Wang", "Lu Yu", "Chunjie Yang"], "title": "Improving Autoformalization Using Direct Dependency Retrieval", "comment": null, "summary": "The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u76f4\u63a5\u4f9d\u8d56\u68c0\u7d22(DDR)\u7684\u65b0\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u5b66\u9648\u8ff0\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u4e0d\u8db3\u548c\u4f9d\u8d56\u68c0\u7d22\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5bfc\u81f4\u5f62\u5f0f\u5b9a\u4e49\u548c\u5b9a\u7406\u7684\u5e7b\u89c9\uff0c\u4e14\u5f53\u524d\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5728\u5f62\u5f0f\u5e93\u4f9d\u8d56\u68c0\u7d22\u65b9\u9762\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u5dee\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e0d\u65ad\u589e\u957f\u7684\u516c\u5171\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faDDR\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u63cf\u8ff0\u751f\u6210\u5019\u9009\u5e93\u4f9d\u8d56\uff0c\u7136\u540e\u901a\u8fc7\u9ad8\u6548\u540e\u7f00\u6570\u7ec4\u68c0\u67e5\u9a8c\u8bc1\u5176\u5728\u5f62\u5f0f\u5e93\u4e2d\u7684\u5b58\u5728\u6027\uff0c\u6784\u5efa\u4e86\u8d85\u8fc750\u4e07\u4e2a\u6837\u672c\u7684\u4f9d\u8d56\u68c0\u7d22\u6570\u636e\u96c6\u5e76\u5fae\u8c03\u9ad8\u7cbe\u5ea6DDR\u6a21\u578b\u3002", "result": "DDR\u6a21\u578b\u5728\u68c0\u7d22\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u914d\u5907DDR\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5668\u5728\u5355\u6b21\u5c1d\u8bd5\u51c6\u786e\u7387\u548c\u591a\u6b21\u5c1d\u8bd5\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f7f\u7528\u4f20\u7edf\u57fa\u4e8e\u9009\u62e9\u7684RAG\u65b9\u6cd5\u7684\u6a21\u578b\u3002", "conclusion": "DDR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u5b66\u9648\u8ff0\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u7684\u4f9d\u8d56\u68c0\u7d22\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12261", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12261", "abs": "https://arxiv.org/abs/2511.12261", "authors": ["Zongxin Shen", "Yanyong Huang", "Dongjie Wang", "Jinyuan Chang", "Fengmao Lv", "Tianrui Li", "Xiaoyi Jiang"], "title": "Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection", "comment": null, "summary": "Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faCLIM-FS\u65b9\u6cd5\u89e3\u51b3\u6df7\u5408\u7f3a\u5931\u591a\u89c6\u56fe\u65e0\u76d1\u7763\u7279\u5f81\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u548c\u81ea\u9002\u5e94\u6570\u636e\u586b\u8865\uff0c\u5229\u7528\u5171\u8bc6\u805a\u7c7b\u7ed3\u6784\u548c\u8de8\u89c6\u56fe\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u4ec5\u5173\u6ce8\u89c6\u56fe\u7f3a\u5931\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u8df5\u4e2d\u66f4\u666e\u904d\u7684\u6df7\u5408\u7f3a\u5931\u573a\u666f\uff1b2) \u5bf9\u89c6\u56fe\u95f4\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u7684\u5229\u7528\u4e0d\u8db3\uff1b3) \u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u8bf4\u660e\u7279\u5f81\u9009\u62e9\u4e0e\u6570\u636e\u586b\u8865\u7684\u4ea4\u4e92\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u975e\u8d1f\u6b63\u4ea4\u77e9\u9635\u5206\u89e3\uff0c\u5c06\u7f3a\u5931\u89c6\u56fe\u548c\u53d8\u91cf\u7684\u586b\u8865\u96c6\u6210\u5230\u7279\u5f81\u9009\u62e9\u6a21\u578b\u4e2d\uff0c\u8054\u5408\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u548c\u81ea\u9002\u5e94\u6570\u636e\u586b\u8865\uff0c\u5e76\u5145\u5206\u5229\u7528\u5171\u8bc6\u805a\u7c7b\u7ed3\u6784\u548c\u8de8\u89c6\u56fe\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u57288\u4e2a\u771f\u5b9e\u4e16\u754c\u591a\u89c6\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCLIM-FS\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "CLIM-FS\u80fd\u6709\u6548\u89e3\u51b3\u6df7\u5408\u7f3a\u5931\u591a\u89c6\u56fe\u65e0\u76d1\u7763\u7279\u5f81\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12892", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12892", "abs": "https://arxiv.org/abs/2511.12892", "authors": ["Liangshun Wu", "Wen Chen", "Shunqing Zhang", "Yajun Wang", "Kunlun Wang"], "title": "Green Emergency Communications in RIS- and MA-Assisted Multi-UAV SAGINs: A Partially Observable Reinforcement Learning Approach", "comment": null, "summary": "In post-disaster space-air-ground integrated networks (SAGINs), terrestrial infrastructure is often impaired, and unmanned aerial vehicles (UAVs) must rapidly restore connectivity for mission-critical ground terminals in cluttered non-line-of-sight (NLoS) urban environments. To enhance coverage, UAVs employ movable antennas (MAs), while reconfigurable intelligent surfaces (RISs) on surviving high-rises redirect signals. The key challenge is communication-limited partial observability, leaving each UAV with a narrow, fast-changing neighborhood view that destabilizes value estimation. Existing multi-agent reinforcement learning (MARL) approaches are inadequate--non-communication methods rely on unavailable global critics, heuristic sharing is brittle and redundant, and learnable protocols (e.g., CommNet, DIAL) lose per-neighbor structure and aggravate non-stationarity under tight bandwidth. To address partial observability, we propose a spatiotemporal A2C where each UAV transmits prior-decision messages with local state, a compact policy fingerprint, and a recurrent belief, encoded per neighbor and concatenated. A spatial discount shapes value targets to emphasize local interactions, while analysis under one-hop-per-slot latency explains stable training with delayed views. Experimental results show our policy outperforms IA2C, ConseNet, FPrint, DIAL, and CommNet--achieving faster convergence, higher asymptotic reward, reduced Temporal-Difference(TD)/advantage errors, and a better communication throughput-energy trade-off.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65f6\u7a7aA2C\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f20\u8f93\u5305\u542b\u5c40\u90e8\u72b6\u6001\u3001\u7b56\u7565\u6307\u7eb9\u548c\u5faa\u73af\u4fe1\u5ff5\u7684\u51b3\u7b56\u524d\u6d88\u606f\u6765\u89e3\u51b3\u707e\u540eSAGIN\u4e2dUAV\u901a\u4fe1\u53d7\u9650\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u5728NLoS\u57ce\u5e02\u73af\u5883\u4e2d\u63d0\u5347\u8986\u76d6\u548c\u8fde\u63a5\u6027\u80fd\u3002", "motivation": "\u707e\u540eSAGIN\u4e2d\u5730\u9762\u57fa\u7840\u8bbe\u65bd\u53d7\u635f\uff0cUAV\u9700\u8981\u5728NLoS\u57ce\u5e02\u73af\u5883\u4e2d\u5feb\u901f\u6062\u590d\u5173\u952e\u4efb\u52a1\u7ec8\u7aef\u7684\u8fde\u63a5\u3002\u73b0\u6709MARL\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u901a\u4fe1\u53d7\u9650\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\uff0c\u975e\u901a\u4fe1\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u53ef\u7528\u7684\u5168\u5c40\u6279\u8bc4\u8005\uff0c\u542f\u53d1\u5f0f\u5171\u4eab\u8106\u5f31\u5197\u4f59\uff0c\u53ef\u5b66\u4e60\u534f\u8bae\u4e22\u5931\u90bb\u5c45\u7ed3\u6784\u5e76\u52a0\u5267\u975e\u5e73\u7a33\u6027\u3002", "method": "\u63d0\u51fa\u65f6\u7a7aA2C\u65b9\u6cd5\uff0c\u6bcf\u4e2aUAV\u4f20\u8f93\u5305\u542b\u5c40\u90e8\u72b6\u6001\u3001\u7d27\u51d1\u7b56\u7565\u6307\u7eb9\u548c\u5faa\u73af\u4fe1\u5ff5\u7684\u51b3\u7b56\u524d\u6d88\u606f\uff0c\u6309\u90bb\u5c45\u7f16\u7801\u5e76\u62fc\u63a5\u3002\u4f7f\u7528\u7a7a\u95f4\u6298\u6263\u5851\u9020\u4ef7\u503c\u76ee\u6807\u4ee5\u5f3a\u8c03\u5c40\u90e8\u4ea4\u4e92\uff0c\u5728\u4e00\u8df3\u6bcf\u65f6\u9699\u5ef6\u8fdf\u4e0b\u5206\u6790\u5ef6\u8fdf\u89c6\u56fe\u7684\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8eIA2C\u3001ConseNet\u3001FPrint\u3001DIAL\u548cCommNet\uff0c\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u3001\u66f4\u9ad8\u6e10\u8fd1\u5956\u52b1\u3001\u51cf\u5c11TD/\u4f18\u52bf\u8bef\u5dee\uff0c\u4ee5\u53ca\u66f4\u597d\u7684\u901a\u4fe1\u541e\u5410\u91cf-\u80fd\u91cf\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65f6\u7a7aA2C\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u707e\u540eSAGIN\u4e2dUAV\u901a\u4fe1\u53d7\u9650\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u6d88\u606f\u4f20\u8f93\u548c\u7a7a\u95f4\u6298\u6263\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.11866", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11866", "abs": "https://arxiv.org/abs/2511.11866", "authors": ["H. R. Paz"], "title": "A Leakage-Aware Data Layer For Student Analytics: The Capire Framework For Multilevel Trajectory Modeling", "comment": "Pages: 52 Figures: 4 (Figures 3.1, 3.2, 6.1, and 7.5) Tables: 5 (Tables 2.1, 2.2, 3.1, 7.1, and 7.2) Type: Journal Article Essential Info: A methodological framework (CAPIRE) with an empirical case study (1,343 students)", "summary": "Predictive models for student dropout, while often accurate, frequently rely on opportunistic feature sets and suffer from undocumented data leakage, limiting their explanatory power and institutional usefulness. This paper introduces a leakage-aware data layer for student trajectory analytics, which serves as the methodological foundation for the CAPIRE framework for multilevel modelling. We propose a feature engineering design that organizes predictors into four levels: N1 (personal and socio-economic attributes), N2 (entry moment and academic history), N3 (curricular friction and performance), and N4 (institutional and macro-context variables)As a core component, we formalize the Value of Observation Time (VOT) as a critical design parameter that rigorously separates observation windows from outcome horizons, preventing data leakage by construction. An illustrative application in a long-cycle engineering program (1,343 students, ~57% dropout) demonstrates that VOT-restricted multilevel features support robust archetype discovery. A UMAP + DBSCAN pipeline uncovers 13 trajectory archetypes, including profiles of \"early structural crisis,\" \"sustained friction,\" and \"hidden vulnerability\" (low friction but high dropout). Bootstrap and permutation tests confirm these archetypes are statistically robust and temporally stable. We argue that this approach transforms feature engineering from a technical step into a central methodological artifact. This data layer serves as a disciplined bridge between retention theory, early-warning systems, and the future implementation of causal inference and agent-based modelling (ABM) within the CAPIRE program.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9632\u6570\u636e\u6cc4\u6f0f\u7684\u5b66\u751f\u8f68\u8ff9\u5206\u6790\u6570\u636e\u5c42\uff0c\u901a\u8fc7\u5b9a\u4e49\u89c2\u5bdf\u65f6\u95f4\u4ef7\u503c(VOT)\u6765\u4e25\u683c\u5206\u79bb\u89c2\u5bdf\u7a97\u53e3\u548c\u7ed3\u679c\u9884\u6d4b\u671f\uff0c\u5e76\u5728\u5de5\u7a0b\u4e13\u4e1a\u4e2d\u53d1\u73b0\u4e8613\u79cd\u7a33\u5065\u7684\u5b66\u751f\u8f68\u8ff9\u539f\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u751f\u8f8d\u5b66\u9884\u6d4b\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u673a\u4f1a\u6027\u7279\u5f81\u96c6\u5e76\u5b58\u5728\u672a\u8bb0\u5f55\u7684\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u80fd\u529b\u548c\u673a\u6784\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86CAPIRE\u6846\u67b6\u7684\u591a\u7ea7\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c06\u9884\u6d4b\u56e0\u5b50\u7ec4\u7ec7\u4e3a\u56db\u4e2a\u5c42\u6b21(N1-N4)\uff0c\u5e76\u6b63\u5f0f\u5b9a\u4e49\u4e86\u89c2\u5bdf\u65f6\u95f4\u4ef7\u503c(VOT)\u4f5c\u4e3a\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\u6765\u9632\u6b62\u6570\u636e\u6cc4\u6f0f\u3002\u4f7f\u7528UMAP + DBSCAN\u6d41\u6c34\u7ebf\u8fdb\u884c\u539f\u578b\u53d1\u73b0\u3002", "result": "\u5728\u957f\u5468\u671f\u5de5\u7a0b\u9879\u76ee(1,343\u540d\u5b66\u751f\uff0c\u7ea657%\u8f8d\u5b66\u7387)\u7684\u5e94\u7528\u4e2d\uff0c\u53d1\u73b0\u4e8613\u79cd\u8f68\u8ff9\u539f\u578b\uff0c\u5305\u62ec'\u65e9\u671f\u7ed3\u6784\u6027\u5371\u673a'\u3001'\u6301\u7eed\u6469\u64e6'\u548c'\u9690\u85cf\u8106\u5f31\u6027'\u7b49\u3002\u81ea\u4e3e\u548c\u7f6e\u6362\u6d4b\u8bd5\u8bc1\u5b9e\u8fd9\u4e9b\u539f\u578b\u5177\u6709\u7edf\u8ba1\u7a33\u5065\u6027\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u7279\u5f81\u5de5\u7a0b\u4ece\u6280\u672f\u6b65\u9aa4\u8f6c\u53d8\u4e3a\u6838\u5fc3\u65b9\u6cd5\u5de5\u4ef6\uff0c\u4e3a\u4fdd\u7559\u7406\u8bba\u3001\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u4ee5\u53ca\u672a\u6765\u5728CAPIRE\u9879\u76ee\u4e2d\u5b9e\u65bd\u56e0\u679c\u63a8\u7406\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u7eaa\u5f8b\u7684\u6865\u6881\u3002"}}
{"id": "2511.12965", "categories": ["math.OC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.12965", "abs": "https://arxiv.org/abs/2511.12965", "authors": ["Yilang Hao", "Zhibin Chen"], "title": "Electric Truck Platooning with Charging Consideration and Leader Swapping", "comment": null, "summary": "Electric trucks are increasingly deployed to reduce the trucking sector's carbon footprint, but their limited range and charging needs create operational challenges on mid- to long-haul routes. Truck platooning can mitigate range anxiety through energy savings and, in turn, influence routing and charging decisions, yet most existing studies focus on a single highway corridor and do not capture network-wide operations. We study electric truck platooning on a general road network, where trucks must select routes and charging stations with heterogeneous prices and charging speeds, form platoons on shared arcs, and possibly take detours that trade off platoon savings with additional labor hours. We further allow in-platoon position swaps so that leading responsibility rotates, balancing battery usage and avoiding early depletion of any single truck. To jointly optimize routing paths, charging-station choices, labor time, and platoon formation and position swaps, we formulate a mixed-integer linear program (MILP). Because exact methods become intractable on realistic instances, we develop an Adaptive Large Neighborhood Search (ALNS) algorithm enhanced with a savings-based bounding scheme, infeasible-pair elimination, and candidate-station filtering. Computational experiments on test instances with up to 150 trucks show that incorporating platooning can reduce total operational costs by up to 2.77 percent, while the proposed algorithm cuts computation time by up to 99.96 percent compared with CPLEX and solves 150-truck instances in about 120 seconds, indicating strong potential for real-world applications.", "AI": {"tldr": "\u7814\u7a76\u7535\u52a8\u5361\u8f66\u5728\u4e00\u822c\u9053\u8def\u7f51\u7edc\u4e0a\u7684\u7f16\u961f\u884c\u9a76\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u7ebf\u9009\u62e9\u3001\u5145\u7535\u7ad9\u9009\u62e9\u3001\u7f16\u961f\u5f62\u6210\u548c\u4f4d\u7f6e\u4ea4\u6362\u6765\u964d\u4f4e\u8fd0\u8425\u6210\u672c\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u89e3\u51b3\u5927\u89c4\u6a21\u95ee\u9898\u3002", "motivation": "\u7535\u52a8\u5361\u8f66\u5728\u957f\u9014\u8fd0\u8f93\u4e2d\u9762\u4e34\u7eed\u822a\u91cc\u7a0b\u6709\u9650\u548c\u5145\u7535\u9700\u6c42\u7b49\u8fd0\u8425\u6311\u6218\uff0c\u7f16\u961f\u884c\u9a76\u53ef\u4ee5\u901a\u8fc7\u8282\u80fd\u7f13\u89e3\u91cc\u7a0b\u7126\u8651\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u9ad8\u901f\u516c\u8def\u8d70\u5eca\uff0c\u672a\u80fd\u6355\u6349\u7f51\u7edc\u8303\u56f4\u5185\u7684\u8fd0\u8425\u60c5\u51b5\u3002", "method": "\u5efa\u7acb\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6a21\u578b\uff0c\u8054\u5408\u4f18\u5316\u8def\u7ebf\u8def\u5f84\u3001\u5145\u7535\u7ad9\u9009\u62e9\u3001\u52b3\u52a8\u65f6\u95f4\u548c\u7f16\u961f\u5f62\u6210\u4e0e\u4f4d\u7f6e\u4ea4\u6362\u3002\u7531\u4e8e\u7cbe\u786e\u65b9\u6cd5\u5728\u73b0\u5b9e\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u96be\u4ee5\u5904\u7406\uff0c\u5f00\u53d1\u4e86\u589e\u5f3a\u7684\u81ea\u9002\u5e94\u5927\u90bb\u57df\u641c\u7d22\u7b97\u6cd5\uff0c\u5305\u542b\u57fa\u4e8e\u8282\u7701\u7684\u8fb9\u754c\u65b9\u6848\u3001\u4e0d\u53ef\u884c\u5bf9\u6d88\u9664\u548c\u5019\u9009\u7ad9\u8fc7\u6ee4\u3002", "result": "\u8ba1\u7b97\u5b9e\u9a8c\u8868\u660e\uff0c\u7f16\u961f\u884c\u9a76\u53ef\u5c06\u603b\u8fd0\u8425\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe2.77%\uff0c\u6240\u63d0\u7b97\u6cd5\u76f8\u6bd4CPLEX\u5c06\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe99.96%\uff0c\u80fd\u5728\u7ea6120\u79d2\u5185\u89e3\u51b3150\u8f86\u5361\u8f66\u7684\u5b9e\u4f8b\u3002", "conclusion": "\u7f16\u961f\u884c\u9a76\u80fd\u663e\u8457\u964d\u4f4e\u7535\u52a8\u5361\u8f66\u7684\u8fd0\u8425\u6210\u672c\uff0c\u6240\u5f00\u53d1\u7684\u9ad8\u6548\u7b97\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.12130", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12130", "abs": "https://arxiv.org/abs/2511.12130", "authors": ["Bingbing Wang", "Zhixin Bai", "Zhengda Jin", "Zihan Wang", "Xintong Song", "Jingjie Lin", "Sixuan Li", "Jing Li", "Ruifeng Xu"], "title": "PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection", "comment": null, "summary": "The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86U-MStance\u6570\u636e\u96c6\u548cPRISM\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u4e2d\u7684\u4f2a\u591a\u6a21\u6001\u548c\u7528\u6237\u540c\u8d28\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7528\u6237\u753b\u50cf\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u7acb\u573a\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5b58\u5728\u4f2a\u591a\u6a21\u6001\uff08\u6e90\u5e16\u5b50\u6709\u89c6\u89c9\u7ebf\u7d22\u4f46\u8bc4\u8bba\u4ec5\u6587\u672c\uff09\u548c\u7528\u6237\u540c\u8d28\u5316\uff08\u5ffd\u7565\u4e2a\u4eba\u7279\u5f81\uff09\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u4ea4\u4e92\u3002", "method": "PRISM\u6a21\u578b\uff1a1\uff09\u4ece\u5386\u53f2\u5e16\u5b50\u63a8\u5bfc\u7528\u6237\u753b\u50cf\uff1b2\uff09\u901a\u8fc7\u601d\u7ef4\u94fe\u5bf9\u9f50\u6587\u672c\u548c\u89c6\u89c9\u7ebf\u7d22\uff1b3\uff09\u4f7f\u7528\u4e92\u4efb\u52a1\u5f3a\u5316\u673a\u5236\u8054\u5408\u4f18\u5316\u7acb\u573a\u68c0\u6d4b\u548c\u7acb\u573a\u611f\u77e5\u54cd\u5e94\u751f\u6210\u3002", "result": "\u5728U-MStance\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPRISM\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7528\u6237\u4e2d\u5fc3\u5316\u548c\u4e0a\u4e0b\u6587\u57fa\u7840\u7684\u591a\u6a21\u6001\u63a8\u7406\u5bf9\u4e8e\u73b0\u5b9e\u7acb\u573a\u7406\u89e3\u5177\u6709\u6709\u6548\u6027\u3002"}}
{"id": "2511.11627", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11627", "abs": "https://arxiv.org/abs/2511.11627", "authors": ["Wang Zhenyu", "Li Peiyuan", "Shi Yongxiang", "Wu Ruoyu", "Zhang Lei"], "title": "SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion", "comment": null, "summary": "Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.", "AI": {"tldr": "\u63d0\u51faSA-EMO\u67b6\u6784\u7528\u4e8e\u672a\u77e5\u5730\u4e0b\u7ed3\u6784\u4e0b\u7684\u901f\u5ea6\u573a\u53cd\u6f14\uff0c\u901a\u8fc7\u7ed3\u6784\u5bf9\u9f50\u7f16\u7801\u5668\u548c\u591a\u7b97\u5b50\u6df7\u5408\u673a\u5236\u663e\u8457\u63d0\u5347\u5168\u6ce2\u5f62\u53cd\u6f14\u6027\u80fd", "motivation": "\u4f20\u7edf\u5355CNN\u67b6\u6784\u6216\u5355\u795e\u7ecf\u7b97\u5b50\u5728\u672a\u77e5\u6216\u590d\u6742\u5730\u8d28\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u533a\u5206\u4e0d\u540c\u5730\u8d28\u7c7b\u578b\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u53cd\u6f14\u65b9\u6cd5", "method": "\u4f7f\u7528\u7ed3\u6784\u5bf9\u9f50\u7f16\u7801\u5668\u5c06\u5730\u9707\u6ce2\u573a\u6620\u5c04\u5230\u7269\u7406\u4e00\u81f4\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\u9009\u62e9\u548c\u878d\u5408\u591a\u79cd\u795e\u7ecf\u7b97\u5b50\u4e13\u5bb6\u6765\u9884\u6d4b\u901f\u5ea6\u6a21\u578b", "result": "\u5728OpenFWI\u57fa\u51c6\u548cMarmousi2\u6570\u636e\u96c6\u4e0a\uff0cSA-EMO\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e73\u5747MAE\u964d\u4f4e\u7ea658.443%\uff0c\u8fb9\u754c\u5206\u8fa8\u7387\u63d0\u5347\u7ea610.308%", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7269\u7406\u53ef\u89e3\u91ca\u7684\u5168\u6ce2\u5f62\u53cd\u6f14\u5f15\u5165\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2511.12003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12003", "abs": "https://arxiv.org/abs/2511.12003", "authors": ["Shuochen Liu", "Pengfei Luo", "Chao Zhang", "Yuhao Chen", "Haotian Zhang", "Qi Liu", "Xin Kou", "Tong Xu", "Enhong Chen"], "title": "Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning", "comment": "Poster of AAAI'2026", "summary": "Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.", "AI": {"tldr": "\u63d0\u51faChain-of-Evidence (CoE)\u8303\u5f0f\uff0c\u5c06\u601d\u7ef4\u94fe\u63a8\u7406\u4e0e\u89c6\u89c9\u8bc1\u636e\u5f52\u56e0\u7ed3\u5408\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6LAT\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5728\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u8bc1\u636e\u6eaf\u6e90\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u76d1\u7763\u548c\u63a8\u7406\u8fc7\u7a0b\u7684\u6e10\u8fdb\u53ef\u8ffd\u6eaf\u6027\uff0c\u65e0\u6cd5\u786e\u4fdd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u95ee\u7b54\u4e2d\u7684\u9884\u6d4b\u53ef\u9760\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "method": "\u63d0\u51faLook As You Think (LAT)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bc4\u4f30\u8bc1\u636e\u533a\u57df\u7684\u5f52\u56e0\u4e00\u81f4\u6027\uff0c\u4ec5\u5728CoE\u8f68\u8ff9\u4ea7\u751f\u6b63\u786e\u7b54\u6848\u65f6\u63d0\u4f9b\u5956\u52b1\uff0c\u9f13\u52b1\u8fc7\u7a0b\u7ea7\u81ea\u6211\u9a8c\u8bc1\u3002", "result": "\u5728Paper-\u548cWiki-VISA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAT\u5728\u5355\u56fe\u548c\u53cc\u56fe\u8bbe\u7f6e\u4e0b\u5747\u63d0\u5347\u539f\u59cb\u6a21\u578b\u6027\u80fd\uff0c\u8f6f\u7cbe\u786e\u5339\u914d\u5e73\u5747\u63d0\u53478.23%\uff0cIoU@0.5\u63d0\u534747.0%\uff0c\u4e14\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u57fa\u7ebf\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CoE\u8303\u5f0f\u901a\u8fc7\u8bc1\u636e\u94fe\u63a8\u7406\u548c\u89c6\u89c9\u5f52\u56e0\u7684\u7edf\u4e00\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u53ef\u9a8c\u8bc1\u6027\u548c\u53ef\u9760\u6027\uff0cLAT\u6846\u67b6\u6709\u6548\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2511.12309", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12309", "abs": "https://arxiv.org/abs/2511.12309", "authors": ["Austin Feng", "Marius Alonso", "Ambroise Odonnat"], "title": "Optimal Self-Consistency for Efficient Reasoning with Large Language Models", "comment": null, "summary": "Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.", "AI": {"tldr": "\u672c\u6587\u5bf9\u81ea\u6d3d\u6027\u63a8\u7406\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u5206\u6790\uff0c\u63d0\u51fa\u4e86Blend-ASC\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u81ea\u6d3d\u6027\u65b9\u6cd5\u5e73\u5747\u51cf\u5c116.8\u500d\u6837\u672c\u4f7f\u7528\u91cf\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u81ea\u6d3d\u6027\u63a8\u7406\u867d\u7136\u6709\u6548\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u5e94\u7528\u65f6\u6210\u672c\u8fc7\u9ad8\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6837\u672c\u6548\u7387\u548c\u6269\u5c55\u884c\u4e3a\u7684\u7edf\u4e00\u7406\u8bba\u5206\u6790\u3002", "method": "\u57fa\u4e8e\u6a21\u5f0f\u4f30\u8ba1\u548c\u6295\u7968\u7406\u8bba\u5206\u6790\u81ea\u6d3d\u6027\u6269\u5c55\u884c\u4e3a\uff0c\u63d0\u51faBlend-ASC\u65b9\u6cd5\uff0c\u91c7\u7528\u52a8\u6001\u6837\u672c\u5206\u914d\u7b56\u7565\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u4e14\u53ef\u9002\u5e94\u4efb\u610f\u6837\u672c\u9884\u7b97\u3002", "result": "Blend-ASC\u5728\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u56fa\u5b9a\u5206\u914d\u548c\u52a8\u6001\u5206\u914d\u7684\u81ea\u6d3d\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u51cf\u5c116.8\u500d\u6837\u672c\u4f7f\u7528\u91cf\u3002", "conclusion": "Blend-ASC\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u8d85\u53c2\u6570\u7684\u81ea\u6d3d\u6027\u63a8\u7406\u53d8\u4f53\uff0c\u53ef\u8f7b\u677e\u5e94\u7528\u4e8e\u5404\u79cd\u81ea\u6d3d\u6027\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.12911", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12911", "abs": "https://arxiv.org/abs/2511.12911", "authors": ["Muhammad Nadeem", "MirSaleh Bahavarnia", "Ahmad F. Taha"], "title": "Wide-Area Feedback Control for Renewables-Heavy Power Systems: A Comparative Study of Reinforcement Learning and Lyapunov-Based Design", "comment": null, "summary": "As renewable energy sources become more prevalent, accurately modeling power grid dynamics is becoming increasingly more complex. Concurrently, data acquisition and realtime system state monitoring are becoming more available for control centers. This motivates shifting from \\textit{model- and Lyapunov-based} feedback controller designs toward \\textit{model-free} ones. Reinforcement learning (RL) has emerged as a key tool for designing model-free controllers. Various studies have been carried out to study voltage/frequency control strategies via RL. However, usually a simplified system model is used neglecting detailed dynamics of solar, wind, and composite loads -- and damping system-wide oscillations and modeling power flows are all usually ignored. To that end, we pose an optimal feedback control problem for a detailed renewables-heavy power system, defined by a set of nonlinear differential algebraic equations (NDAE). The control problem is solved using a completely model-free design via RL as well as using a model-based approach built upon the Lyapunov stability theory with guarantees. The paper in its essence seeks to explore whether data-driven feedback control should be used in power grids over its model-driven counterpart. Theoretical developments and thorough case studies are presented with an eye on this exploration. Finally, a detailed analysis is provided to delineate the strengths and weaknesses of both approaches for renewables-heavy grids.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u53ef\u518d\u751f\u80fd\u6e90\u5bc6\u96c6\u7535\u7f51\u4e2d\uff0c\u6570\u636e\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684Lyapunov\u63a7\u5236\u65b9\u6cd5\u7684\u5bf9\u6bd4\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u666e\u53ca\uff0c\u7535\u7f51\u52a8\u6001\u5efa\u6a21\u65e5\u76ca\u590d\u6742\uff0c\u800c\u6570\u636e\u91c7\u96c6\u548c\u5b9e\u65f6\u76d1\u63a7\u80fd\u529b\u589e\u5f3a\uff0c\u8fd9\u4fc3\u4f7f\u4ece\u57fa\u4e8e\u6a21\u578b\u548cLyapunov\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\u8f6c\u5411\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\u3002", "method": "\u9488\u5bf9\u8be6\u7ec6\u7684\u53ef\u518d\u751f\u80fd\u6e90\u7535\u529b\u7cfb\u7edf\u975e\u7ebf\u6027\u5fae\u5206\u4ee3\u6570\u65b9\u7a0b\uff0c\u5206\u522b\u91c7\u7528\u5b8c\u5168\u6a21\u578b\u65e0\u5173\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u548c\u57fa\u4e8eLyapunov\u7a33\u5b9a\u6027\u7406\u8bba\u7684\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u6700\u4f18\u53cd\u9988\u63a7\u5236\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u53d1\u5c55\u548c\u8be6\u7ec6\u6848\u4f8b\u7814\u7a76\uff0c\u5bf9\u4e24\u79cd\u65b9\u6cd5\u5728\u53ef\u518d\u751f\u80fd\u6e90\u5bc6\u96c6\u7535\u7f51\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "conclusion": "\u8bba\u6587\u8be6\u7ec6\u5206\u6790\u4e86\u4e24\u79cd\u65b9\u6cd5\u5728\u53ef\u518d\u751f\u80fd\u6e90\u5bc6\u96c6\u7535\u7f51\u4e2d\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u9009\u62e9\u6570\u636e\u9a71\u52a8\u8fd8\u662f\u6a21\u578b\u9a71\u52a8\u63a7\u5236\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53c2\u8003\u4f9d\u636e\u3002"}}
{"id": "2511.11960", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11960", "abs": "https://arxiv.org/abs/2511.11960", "authors": ["Surajit Das", "Peu Majumder", "Aleksei Eliseev"], "title": "Educators on the Frontline: Philosophical and Realistic Perspectives on Integrating ChatGPT into the Learning Space", "comment": null, "summary": "The rapid emergence of Generative AI, particularly ChatGPT, has sparked a global debate on the future of education, often characterized by alarmism and speculation. Moving beyond this, this study investigates the structured, grounded perspectives of a key stakeholder group: university educators. It proposes a novel theoretical model that conceptualizes the educational environment as a \"Learning Space\" composed of seven subspaces to systematically identify the impact of AI integration. This framework was operationalized through a quantitative survey of 140 Russian university educators, with responses analyzed using a binary flagging system to measure acceptance across key indicators. The results reveal a strong but conditional consensus: a majority of educators support ChatGPT's integration, contingent upon crucial factors such as the transformation of assessment methods and the availability of plagiarism detection tools. However, significant concerns persist regarding its impact on critical thinking. Educators largely reject the notion that AI diminishes their importance, viewing their role as evolving from information-deliverer to facilitator of critical engagement. The study concludes that ChatGPT acts less as a destroyer of education and more as a catalyst for its necessary evolution, and proposes the PIPE Model (Pedagogy, Infrastructure, Policy, Education) as a strategic framework for its responsible integration. This research provides a data-driven, model-based analysis of educator attitudes, offering a nuanced alternative to the polarized discourse surrounding AI in education.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8c03\u67e5140\u540d\u4fc4\u7f57\u65af\u5927\u5b66\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e03\u4e2a\u5b50\u7a7a\u95f4\u7684\"\u5b66\u4e60\u7a7a\u95f4\"\u7406\u8bba\u6a21\u578b\uff0c\u7cfb\u7edf\u5206\u6790ChatGPT\u5728\u6559\u80b2\u4e2d\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570\u6559\u80b2\u8005\u6709\u6761\u4ef6\u652f\u6301AI\u6574\u5408\uff0c\u4f46\u5f3a\u8c03\u8bc4\u4f30\u65b9\u6cd5\u8f6c\u53d8\u548c\u527d\u7a83\u68c0\u6d4b\u5de5\u5177\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u8d85\u8d8a\u5bf9\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u9886\u57df\u5f71\u54cd\u7684\u6050\u614c\u548c\u731c\u6d4b\uff0c\u7cfb\u7edf\u7814\u7a76\u5927\u5b66\u6559\u80b2\u5de5\u4f5c\u8005\u8fd9\u4e00\u5173\u952e\u5229\u76ca\u76f8\u5173\u8005\u7684\u7ed3\u6784\u5316\u89c2\u70b9\uff0c\u4e3aAI\u6559\u80b2\u6574\u5408\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u5206\u6790\u3002", "method": "\u63d0\u51fa\"\u5b66\u4e60\u7a7a\u95f4\"\u7406\u8bba\u6a21\u578b\uff08\u5305\u542b\u4e03\u4e2a\u5b50\u7a7a\u95f4\uff09\uff0c\u901a\u8fc7\u5b9a\u91cf\u8c03\u67e5140\u540d\u4fc4\u7f57\u65af\u5927\u5b66\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u4f7f\u7528\u4e8c\u5143\u6807\u8bb0\u7cfb\u7edf\u5206\u6790\u5173\u952e\u6307\u6807\u4e0a\u7684\u63a5\u53d7\u5ea6\u3002", "result": "\u53d1\u73b0\u6559\u80b2\u8005\u5b58\u5728\u5f3a\u70c8\u4f46\u6709\u6761\u4ef6\u7684\u5171\u8bc6\uff1a\u5927\u591a\u6570\u652f\u6301ChatGPT\u6574\u5408\uff0c\u4f46\u524d\u63d0\u662f\u8bc4\u4f30\u65b9\u6cd5\u8f6c\u53d8\u548c\u527d\u7a83\u68c0\u6d4b\u5de5\u5177\u53ef\u7528\uff1b\u5bf9\u6279\u5224\u6027\u601d\u7ef4\u5f71\u54cd\u5b58\u5728\u663e\u8457\u62c5\u5fe7\uff1b\u6559\u80b2\u8005\u89d2\u8272\u4ece\u4fe1\u606f\u4f20\u9012\u8005\u8f6c\u53d8\u4e3a\u6279\u5224\u6027\u53c2\u4e0e\u4fc3\u8fdb\u8005\u3002", "conclusion": "ChatGPT\u4e0d\u662f\u6559\u80b2\u7684\u7834\u574f\u8005\uff0c\u800c\u662f\u5fc5\u8981\u6f14\u53d8\u7684\u50ac\u5316\u5242\uff1b\u63d0\u51faPIPE\u6a21\u578b\uff08\u6559\u5b66\u6cd5\u3001\u57fa\u7840\u8bbe\u65bd\u3001\u653f\u7b56\u3001\u6559\u80b2\uff09\u4f5c\u4e3a\u8d1f\u8d23\u4efb\u6574\u5408\u7684\u6218\u7565\u6846\u67b6\uff0c\u4e3aAI\u6559\u80b2\u8ba8\u8bba\u63d0\u4f9b\u7ec6\u81f4\u5165\u5fae\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.13187", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.13187", "abs": "https://arxiv.org/abs/2511.13187", "authors": ["Solomon Goldgraber Casspi", "Daniel Zelazo"], "title": "The Geometry of Hidden Modes in Distance-Based Formation Control", "comment": null, "summary": "This paper presents a geometric input-output analysis of hidden modes in distance-based formation control. We study the linearized dynamics under a gradient control law to characterize the system's structural limitations and their dynamic consequences. Our main contribution is a unified geometric framework for uncontrollable modes. We first prove that uncontrollable rigid-body modes are pure rotations about the input node, defining a global rotational subspace $\\mathcal{R}_i$. To generalize this, we introduce the local rotational subspace, $\\mathcal{T}_i$, which contains all motions, including deformations, that are locally invisible to the controller at node $i$. These two geometric objects provide a complete decomposition of the uncontrollable subspace. Finally, we demonstrate the dynamic implications of this structure by proving that the system's ability to recover its shape is determined by an input's alignment with the local component of the standard rotational rigid-body mode, directly linking the geometry of hidden modes to disturbance rejection. We illustrate our results with a case study.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u8ddd\u79bb\u7684\u7f16\u961f\u63a7\u5236\u4e2d\u9690\u85cf\u6a21\u5f0f\u7684\u51e0\u4f55\u8f93\u5165\u8f93\u51fa\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u7ed3\u6784\u9650\u5236\u53ca\u5176\u52a8\u6001\u5f71\u54cd", "motivation": "\u7814\u7a76\u8ddd\u79bb\u7f16\u961f\u63a7\u5236\u4e2d\u4e0d\u53ef\u63a7\u6a21\u5f0f\u7684\u51e0\u4f55\u7279\u6027\u53ca\u5176\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u7406\u89e3\u63a7\u5236\u5668\u5c40\u9650\u6027\u63d0\u4f9b\u7406\u8bba\u57fa\u7840", "method": "\u4f7f\u7528\u51e0\u4f55\u6846\u67b6\u5206\u6790\u7ebf\u6027\u5316\u52a8\u529b\u5b66\uff0c\u5b9a\u4e49\u5168\u5c40\u65cb\u8f6c\u5b50\u7a7a\u95f4\u548c\u5c40\u90e8\u65cb\u8f6c\u5b50\u7a7a\u95f4\uff0c\u5efa\u7acb\u4e0d\u53ef\u63a7\u5b50\u7a7a\u95f4\u7684\u5b8c\u6574\u5206\u89e3", "result": "\u8bc1\u660e\u4e0d\u53ef\u63a7\u521a\u4f53\u6a21\u5f0f\u662f\u7ed5\u8f93\u5165\u8282\u70b9\u7684\u7eaf\u65cb\u8f6c\uff0c\u5c40\u90e8\u65cb\u8f6c\u5b50\u7a7a\u95f4\u5305\u542b\u63a7\u5236\u5668\u5728\u8282\u70b9\u5904\u5c40\u90e8\u4e0d\u53ef\u89c1\u7684\u8fd0\u52a8\uff0c\u51e0\u4f55\u7ed3\u6784\u4e0e\u6270\u52a8\u6291\u5236\u80fd\u529b\u76f4\u63a5\u76f8\u5173", "conclusion": "\u7cfb\u7edf\u6062\u590d\u5f62\u72b6\u7684\u80fd\u529b\u53d6\u51b3\u4e8e\u8f93\u5165\u4e0e\u6807\u51c6\u65cb\u8f6c\u521a\u4f53\u6a21\u5f0f\u5c40\u90e8\u5206\u91cf\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u9690\u85cf\u6a21\u5f0f\u7684\u51e0\u4f55\u7279\u6027\u51b3\u5b9a\u4e86\u7cfb\u7edf\u7684\u6270\u52a8\u6291\u5236\u6027\u80fd"}}
{"id": "2511.12133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12133", "abs": "https://arxiv.org/abs/2511.12133", "authors": ["Qingyu Zhang", "Chunlei Xin", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Qing Ye", "Qianlong Xie", "Xingxing Wang"], "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing", "comment": null, "summary": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86AI-Salesman\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u67b6\u6784\u89e3\u51b3\u76ee\u6807\u9a71\u52a8\u8bf4\u670d\u5bf9\u8bdd\u4e2d\u7684\u591a\u8f6e\u89c4\u5212\u548c\u4e8b\u5b9e\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u9500\u552e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u76ee\u6807\u9a71\u52a8\u8bf4\u670d\u5bf9\u8bdd\uff08\u5982\u7535\u8bdd\u8425\u9500\uff09\u9700\u8981\u590d\u6742\u7684\u591a\u8f6e\u89c4\u5212\u548c\u4e25\u683c\u7684\u4e8b\u5b9e\u5fe0\u5b9e\u6027\uff0c\u73b0\u6709LLMs\u5b58\u5728\u7b56\u7565\u8106\u5f31\u6027\u548c\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u3002", "method": "\u6784\u5efaTeleSalesCorpus\u6570\u636e\u96c6\uff1b\u63d0\u51fa\u53cc\u9636\u6bb5\u67b6\u6784\uff1a\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u8d1d\u53f6\u65af\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4ece\u566a\u58f0\u5bf9\u8bdd\u4e2d\u5b66\u4e60\u7a33\u5065\u9500\u552e\u7b56\u7565\uff0c\u63a8\u7406\u9636\u6bb5\u4f7f\u7528\u52a8\u6001\u5927\u7eb2\u5f15\u5bfc\u4ee3\u7406(DOGA)\u7ed3\u5408\u9884\u5efa\u811a\u672c\u5e93\u63d0\u4f9b\u52a8\u6001\u7b56\u7565\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAI-Salesman\u5728\u81ea\u52a8\u6307\u6807\u548c\u5168\u9762\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u590d\u6742\u8bf4\u670d\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "AI-Salesman\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u9636\u6bb5\u8bbe\u8ba1\u548c\u52a8\u6001\u5927\u7eb2\u5f15\u5bfc\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u76ee\u6807\u9a71\u52a8\u8bf4\u670d\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11629", "abs": "https://arxiv.org/abs/2511.11629", "authors": ["Xu Zhang", "Peng Wang", "Chen Wang", "Zhe Xu", "Xiaohua Nie", "Wei Wang"], "title": "Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification", "comment": "Global Feature Enhancing and Fusion Framework for Time Series Classification", "summary": "Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u8d85\u56fe\u7684\u5168\u5c40\u7279\u5f81\u5b66\u4e60\u548c\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u6784\u5efa\u5168\u5c40\u7279\u5f81\u5e76\u5b66\u4e60\u5c40\u90e8\u7279\u5f81\u95f4\u7684\u9ad8\u9636\u5173\u7cfb\uff0c\u4ee5\u589e\u5f3a\u5e94\u53d8\u8ba1\u72b6\u6001\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u793a\u80fd\u529b\uff0c\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u5728\u667a\u80fd\u5236\u9020\u4e2d\uff0c\u5e94\u53d8\u8ba1\u72b6\u6001\u8bc6\u522b\u5bf9\u53ca\u65f6\u53d1\u73b0\u673a\u68b0\u90e8\u4ef6\u6545\u969c\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfCNN\u65b9\u6cd5\u53ea\u80fd\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u5f53\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u7684\u5c40\u90e8\u5b50\u5e8f\u5217\u76f8\u4f3c\u65f6\uff08\u5982\u98de\u673a\u673a\u7ffc\u9759\u5f3a\u5ea6\u5b9e\u9a8c\u6570\u636e\uff09\uff0c\u4ec5\u9760\u5c40\u90e8\u7279\u5f81\u4e0d\u8db3\u4ee5\u5145\u5206\u8868\u8fbe\u65f6\u95f4\u5e8f\u5217\u3002", "method": "\u63d0\u51fa\u8d85\u56fe\u5168\u5c40\u7279\u5f81\u5b66\u4e60\u4e0e\u878d\u5408\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u6784\u5efa\u5168\u5c40\u7279\u5f81\uff1b2\uff09\u5b66\u4e60\u5c40\u90e8\u7279\u5f81\u95f4\u7684\u9ad8\u9636\u5173\u7cfb\u6765\u6355\u83b7\u5168\u5c40\u7279\u5f81\uff1b3\uff09\u5c06\u5168\u5c40\u7279\u5f81\u4e0e\u5c40\u90e8\u7279\u5f81\u878d\u5408\u4ee5\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u3002", "result": "\u5728\u5de5\u4e1aSGS\u6570\u636e\u96c6\u548c\u516c\u5f00UCR\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728SGS\u8bc6\u522b\u4efb\u52a1\u4e2d\u5bf9\u672a\u89c1\u6570\u636e\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5168\u5c40\u7279\u5f81\u5b66\u4e60\u548c\u878d\u5408\u673a\u5236\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8868\u793a\u5e94\u53d8\u8ba1\u72b6\u6001\u65f6\u95f4\u5e8f\u5217\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ec5\u4f9d\u8d56\u5c40\u90e8\u7279\u5f81\u65f6\u8bc6\u522b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2511.12008", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12008", "abs": "https://arxiv.org/abs/2511.12008", "authors": ["Yunqi Hong", "Johnson Kao", "Liam Edwards", "Nein-Tzu Liu", "Chung-Yen Huang", "Alex Oliveira-Kowaleski", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models", "comment": null, "summary": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.", "AI": {"tldr": "RECAP-PATH\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u75c5\u7406AI\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u5b66\u4e60\u8303\u5f0f\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u88ab\u52a8\u6a21\u5f0f\u8bc6\u522b\u8f6c\u53d8\u4e3a\u8bc1\u636e\u5173\u8054\u7684\u8bca\u65ad\u63a8\u7406\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u751f\u6210\u764c\u75c7\u8bca\u65ad\u3002", "motivation": "\u5f53\u524d\u75c5\u7406AI\u7cfb\u7edf\u7f3a\u4e4f\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u96be\u4ee5\u5ba1\u8ba1\u51b3\u7b56\u548c\u9632\u6b62\u9519\u8bef\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u5b66\u4e60\u8fc7\u7a0b\uff1a\u591a\u6837\u5316\u9636\u6bb5\u6269\u5c55\u75c5\u7406\u5b66\u98ce\u683c\u89e3\u91ca\uff0c\u4f18\u5316\u9636\u6bb5\u7cbe\u70bc\u89e3\u91ca\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff1b\u65e0\u9700\u767d\u76d2\u8bbf\u95ee\u6216\u6743\u91cd\u66f4\u65b0\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u524d\u5217\u817a\u764c\u6570\u636e\u96c6\u4e0a\uff0cRECAP-PATH\u751f\u6210\u7684\u63a8\u7406\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\uff0c\u8bca\u65ad\u51c6\u786e\u6027\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RECAP-PATH\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e34\u5e8a\u53ef\u4fe1\u8d56\u7684AI\uff0c\u5c55\u793a\u4e86\u8bc1\u636e\u5173\u8054\u89e3\u91ca\u7684\u901a\u7528\u8def\u5f84\u3002"}}
{"id": "2511.12545", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12545", "abs": "https://arxiv.org/abs/2511.12545", "authors": ["Robin van der Laag", "Hao Wang", "Thomas B\u00e4ck", "Yingjie Fan"], "title": "Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation", "comment": "Extended version including appendix of a paper accepted at AAAI-26 main technical track (to appear)", "summary": "Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(\u03b4)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.", "AI": {"tldr": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u63d0\u51fa\u4e2d\u5fc3-\u5916\u5411q\u652f\u914d\u5173\u7cfb\uff0c\u8bc1\u660e\u5176\u8574\u542b\u5f3a\u4e00\u9636\u968f\u673a\u652f\u914d\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eq\u652f\u914d\u7684\u7ecf\u9a8c\u68c0\u9a8c\u65b9\u6cd5\uff0c\u5e76\u5728\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u673a\u591a\u76ee\u6807\u4f18\u5316\u9700\u8981\u6392\u5e8f\u591a\u5143\u5206\u5e03\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u91c7\u7528\u6807\u91cf\u5316\u65b9\u6cd5\uff0c\u8fd9\u4f1a\u4e22\u5931\u4fe1\u606f\u4e14\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u6bd4\u8f83\u968f\u673aPareto\u96c6\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7684\u4e2d\u5fc3-\u5916\u5411q\u652f\u914d\u5173\u7cfb\uff0c\u5f00\u53d1\u76f8\u5e94\u7684\u7ecf\u9a8c\u68c0\u9a8c\u7a0b\u5e8f\uff0c\u63a8\u5bfc\u51fa\u63a7\u5236Type I\u9519\u8bef\u7684\u663e\u5f0f\u6837\u672c\u91cf\u9608\u503cn*(\u03b4)\u3002", "result": "\u5728YAHPO-MO\u57fa\u51c6\u4efb\u52a1\u4e2d\u6210\u529f\u6bd4\u8f83\u4e86\u4e03\u4e2a\u591a\u76ee\u6807\u8d85\u53c2\u6570\u8c03\u4f18\u5668\u7684\u6700\u7ec8\u968f\u673aPareto\u96c6\uff1b\u5728NSGA-II\u7b97\u6cd5\u4e2d\u7528q\u652f\u914d\u66ff\u6362\u57fa\u4e8e\u5747\u503c\u7684\u9009\u62e9\uff0c\u5728\u566a\u58f0\u589e\u5f3a\u7684ZDT\u57fa\u51c6\u95ee\u9898\u4e0a\u663e\u793a\u51fa\u66f4\u4f18\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u4e2d\u5fc3-\u5916\u5411q\u652f\u914d\u4e3a\u5bfb\u6c42\u771f\u6b63\u968f\u673a\u652f\u914d\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u53ef\u5904\u7406\u7684\u57fa\u7840\uff0c\u7279\u522b\u5728\u671f\u671b\u8d85\u4f53\u79ef\u6307\u6807\u53d8\u5f97\u4e0d\u53ef\u533a\u5206\u65f6\u4ecd\u80fd\u6709\u6548\u6bd4\u8f83\u7b97\u6cd5\u6027\u80fd\u3002"}}
{"id": "2511.13006", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13006", "abs": "https://arxiv.org/abs/2511.13006", "authors": ["Fangzhi Li", "Zhichu Ren", "Cunhua Pan", "Hong Ren", "Jing Jin", "Qixing Wang", "Jiangzhou Wang"], "title": "Cooperative ISAC for LAE: Joint Trajectory Planning, Power allocation, and Dynamic Time Division", "comment": null, "summary": "To enhance the performance of aerial-ground networks, this paper proposes an integrated sensing and communication (ISAC) framework for multi-UAV systems. In our model, ground base stations (BSs) cooperatively serve multiple unmanned aerial vehicles (UAVs), and employ a time-division strategy in which beam scanning for sensing comes before data communication in each time slot. To maximize the sum communication rate while satisfying the total sensing mutual information (MI) requirement, we jointly optimize the UAV trajectories, communication and sensing power allocation, and the dynamic time-division ratio. The resulting non-convex optimization problem is efficiently solved using an alternating optimization (AO) framework. Simulation results demonstrate that our proposed joint design significantly outperforms benchmark schemes with static or partially optimized resources. The findings also reveal the critical importance of dynamic trajectory and resource management for effectively navigating the sensing-communication trade-off, especially under stringent power or sensing constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u901a\u4fe1\u611f\u77e5\u529f\u7387\u5206\u914d\u548c\u52a8\u6001\u65f6\u5206\u6bd4\uff0c\u5728\u6ee1\u8db3\u611f\u77e5\u4e92\u4fe1\u606f\u8981\u6c42\u7684\u540c\u65f6\u6700\u5927\u5316\u901a\u4fe1\u901f\u7387\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u7a7a\u5730\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u611f\u77e5\u4e0e\u901a\u4fe1\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e25\u683c\u529f\u7387\u6216\u611f\u77e5\u7ea6\u675f\u4e0b\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u901a\u4fe1\u611f\u77e5\u529f\u7387\u5206\u914d\u548c\u52a8\u6001\u65f6\u5206\u6bd4\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u8bbe\u8ba1\u65b9\u6848\u663e\u8457\u4f18\u4e8e\u9759\u6001\u6216\u90e8\u5206\u4f18\u5316\u7684\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u52a8\u6001\u8f68\u8ff9\u548c\u8d44\u6e90\u7ba1\u7406\u5bf9\u4e8e\u6709\u6548\u5bfc\u822a\u611f\u77e5-\u901a\u4fe1\u6743\u8861\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u4e25\u683c\u529f\u7387\u6216\u611f\u77e5\u7ea6\u675f\u4e0b\u3002"}}
{"id": "2511.12010", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12010", "abs": "https://arxiv.org/abs/2511.12010", "authors": ["Palakorn Achananuparp", "Connie Xu", "Yao Lu", "Xavier Jayaraj Siddarth Ashok", "Ee-Peng Lim"], "title": "Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles", "comment": "Submitted to EPJ Data Science", "summary": "We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.", "AI": {"tldr": "\u57fa\u4e8e\u5728\u7ebf\u7b80\u5386\u7684\u5927\u89c4\u6a21\u804c\u4e1a\u6d41\u52a8\u6027\u5206\u6790\u663e\u793a\uff0c\u516c\u53f8\u5185\u90e8\u804c\u4f4d\u53d8\u52a8\u5bf9\u5411\u4e0a\u6d41\u52a8\u6027\u7684\u4fc3\u8fdb\u4f5c\u7528\u6700\u5f3a\uff0c\u800c\u5973\u6027\u548c\u9ed1\u4eba\u5927\u5b66\u6bd5\u4e1a\u751f\u4ece\u5de5\u4f5c\u53d8\u52a8\u4e2d\u83b7\u5f97\u7684\u56de\u62a5\u663e\u8457\u4f4e\u4e8e\u7537\u6027\u548c\u767d\u4eba\u540c\u884c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5de5\u4f5c\u53d8\u52a8\u5982\u4f55\u5f71\u54cd\u5927\u5b66\u5b66\u5386\u7f8e\u56fd\u5de5\u4f5c\u8005\u7684\u5411\u4e0a\u804c\u4e1a\u6d41\u52a8\u6027\uff0c\u4ee5\u53ca\u6027\u522b\u548c\u79cd\u65cf\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u6d41\u52a8\u6027\u7ed3\u679c\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u7b80\u5386\u6570\u636e\uff0c\u901a\u8fc7\u6570\u636e\u5904\u7406\u548cAI\u65b9\u6cd5\uff08\u5305\u62ec\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684FewSOC\u804c\u4e1a\u5206\u7c7b\u65b9\u6cd5\uff09\u89e3\u51b3\u6570\u636e\u6311\u6218\uff0c\u5206\u6790\u4e86228,710\u6761\u804c\u4e1a\u8f68\u8ff9\u3002", "result": "\u516c\u53f8\u5185\u90e8\u804c\u4f4d\u53d8\u52a8\u5bf9\u5411\u4e0a\u6d41\u52a8\u6027\u4fc3\u8fdb\u4f5c\u7528\u6700\u5f3a\uff0c\u5176\u6b21\u662f\u8de8\u516c\u53f8\u804c\u4f4d\u53d8\u52a8\u548c\u8de8\u516c\u53f8\u5e73\u7ea7\u8c03\u52a8\u3002\u5973\u6027\u548c\u9ed1\u4eba\u6bd5\u4e1a\u751f\u4ece\u5de5\u4f5c\u53d8\u52a8\u4e2d\u83b7\u5f97\u7684\u56de\u62a5\u663e\u8457\u8f83\u4f4e\u3002", "conclusion": "\u804c\u4e1a\u6d41\u52a8\u6027\u5b58\u5728\u663e\u8457\u7684\u6027\u522b\u548c\u79cd\u65cf\u5dee\u5f02\uff0c\u591a\u7ea7\u654f\u611f\u6027\u5206\u6790\u8bc1\u5b9e\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u96c6\u7fa4\u5f02\u8d28\u6027\u5177\u6709\u7a33\u5065\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u989d\u5916\u7684\u4ea4\u53c9\u6027\u6a21\u5f0f\u3002"}}
{"id": "2511.13196", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.13196", "abs": "https://arxiv.org/abs/2511.13196", "authors": ["Vincent Guillemet", "Michael Unser"], "title": "Sampling in BV-Type Spaces", "comment": null, "summary": "The sampling of functions of bounded variation (BV) is a long-standing problem in op- timization. The ability to sample such functions has relevance in the field of variational inverse problems, where the standard theory fails to guarantee the mere existence of solutions when the loss functional involves samples of BV functions. In this paper, we prove the continuity of sampling functionals and show that the differential operator D admits a unique local inverse. This canonical inversion enables us to formulate an existence theorem for a class of regularized optimization problems that incorporate samples of BV functions. Finally, we characterize the solution set in terms of its extreme points.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u6709\u754c\u53d8\u5dee\u51fd\u6570\u91c7\u6837\u6cdb\u51fd\u7684\u8fde\u7eed\u6027\uff0c\u5efa\u7acb\u4e86\u5fae\u5206\u7b97\u5b50\u7684\u5c40\u90e8\u9006\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542bBV\u51fd\u6570\u91c7\u6837\u7684\u6b63\u5219\u5316\u4f18\u5316\u95ee\u9898\u7684\u5b58\u5728\u6027\u5b9a\u7406\u3002", "motivation": "\u6709\u754c\u53d8\u5dee\u51fd\u6570\u91c7\u6837\u5728\u53d8\u5206\u9006\u95ee\u9898\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u6807\u51c6\u7406\u8bba\u65e0\u6cd5\u4fdd\u8bc1\u5305\u542bBV\u51fd\u6570\u91c7\u6837\u7684\u635f\u5931\u6cdb\u51fd\u89e3\u7684\u5b58\u5728\u6027\u3002", "method": "\u8bc1\u660e\u91c7\u6837\u6cdb\u51fd\u7684\u8fde\u7eed\u6027\uff0c\u5efa\u7acb\u5fae\u5206\u7b97\u5b50D\u7684\u5c40\u90e8\u9006\uff0c\u5e76\u5229\u7528\u8fd9\u79cd\u89c4\u8303\u53cd\u6f14\u6765\u6784\u5efa\u4f18\u5316\u95ee\u9898\u3002", "result": "\u83b7\u5f97\u4e86\u5305\u542bBV\u51fd\u6570\u91c7\u6837\u7684\u6b63\u5219\u5316\u4f18\u5316\u95ee\u9898\u7684\u5b58\u5728\u6027\u5b9a\u7406\uff0c\u5e76\u523b\u753b\u4e86\u89e3\u96c6\u7684\u6781\u503c\u70b9\u7279\u5f81\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5305\u542b\u6709\u754c\u53d8\u5dee\u51fd\u6570\u91c7\u6837\u7684\u53d8\u5206\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5efa\u7acb\u4e86\u5fae\u5206\u7b97\u5b50\u7684\u5c40\u90e8\u53ef\u9006\u6027\u548c\u89e3\u7684\u5b58\u5728\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2511.12140", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12140", "abs": "https://arxiv.org/abs/2511.12140", "authors": ["Pinxue Guo", "Chongruo Wu", "Xinyu Zhou", "Lingyi Hong", "Zhaoyu Chen", "Jinglun Li", "Kaixun Jiang", "Sen-ching Samson Cheung", "Wei Zhang", "Wenqiang Zhang"], "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.", "AI": {"tldr": "VBackChecker\u662f\u4e00\u4e2a\u65e0\u9700\u53c2\u8003\u7684\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7Grounding LLM\u9a8c\u8bc1MLLM\u751f\u6210\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\uff0c\u5728R\u00b2-HalBench\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u51c6\u786e\u68c0\u6d4b\u4ee5\u786e\u4fdd\u5b9e\u9645\u5e94\u7528\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\"\u773c\u89c1\u4e3a\u5b9e\"\u539f\u5219\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u50cf\u7d20\u7ea7Grounding LLM\u7684\u53c2\u8003\u514d\u8d39\u68c0\u6d4b\u6846\u67b6\uff0c\u5305\u542b\u63a8\u7406\u548c\u53c2\u8003\u5206\u5272\u80fd\u529b\uff0c\u5e76\u6784\u5efaR-Instruct\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5728R\u00b2-HalBench\u57fa\u51c6\u4e0a\u8d85\u8d8a\u5148\u524d\u590d\u6742\u6846\u67b6\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-4o\uff0c\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u4efb\u52a1\u63d0\u5347\u8d8510%\u3002", "conclusion": "VBackChecker\u4e3aMLLM\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e30\u5bcc\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.11630", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.11630", "abs": "https://arxiv.org/abs/2511.11630", "authors": ["Eliane Younes", "Elie Hachem", "Marc Bernacki"], "title": "Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models", "comment": null, "summary": "Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08RNN\u3001LSTM\u3001TCN\u3001transformer\uff09\u9884\u6d4b\u6676\u7c92\u751f\u957f\u8fc7\u7a0b\u4e2d\u6676\u7c92\u5c3a\u5bf8\u5206\u5e03\u7684\u80fd\u529b\u3002LSTM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ece20\u5206\u949f\u7f29\u77ed\u5230\u51e0\u79d2\u3002", "motivation": "\u6676\u7c92\u751f\u957f\u5bf9\u6750\u6599\u529b\u5b66\u884c\u4e3a\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u5168\u573a\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u57fa\u4e8e\u4f4e\u7ef4\u7edf\u8ba1\u63cf\u8ff0\u7b26\u7684\u9ad8\u6548\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4ece\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u63d0\u53d6\u7684\u5e73\u5747\u573a\u7edf\u8ba1\u63cf\u8ff0\u7b26\uff0c\u5c06120\u4e2a\u6676\u7c92\u751f\u957f\u5e8f\u5217\u5904\u7406\u4e3a\u5f52\u4e00\u5316\u6676\u7c92\u5c3a\u5bf8\u5206\u5e03\u3002\u91c7\u7528\u9012\u5f52\u9884\u6d4b\u7b56\u7565\uff0c\u4ece\u77ed\u671f\u5386\u53f2\u6570\u636e\u9884\u6d4b\u672a\u6765\u5206\u5e03\u3002", "result": "LSTM\u7f51\u7edc\u51c6\u786e\u7387\u6700\u9ad8\uff08>90%\uff09\uff0c\u6027\u80fd\u6700\u7a33\u5b9a\uff0c\u80fd\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u9884\u6d4b\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ece20\u5206\u949f/\u5e8f\u5217\u7f29\u77ed\u5230\u51e0\u79d2\u949f\u3002\u5176\u4ed6\u67b6\u6784\u5728\u957f\u671f\u9884\u6d4b\u65f6\u5bb9\u6613\u53d1\u6563\u3002", "conclusion": "\u4f4e\u7ef4\u63cf\u8ff0\u7b26\u548cLSTM\u9884\u6d4b\u65b9\u6cd5\u5728\u9ad8\u6548\u51c6\u786e\u7684\u5fae\u89c2\u7ed3\u6784\u9884\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5bf9\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\u548c\u5de5\u827a\u4f18\u5316\u6709\u76f4\u63a5\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.12060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12060", "abs": "https://arxiv.org/abs/2511.12060", "authors": ["Yinghao Ruan", "Wei Pang", "Shuaihao Liu", "Huili Yang", "Leyi Han", "Xinghui Dong"], "title": "Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization", "comment": "10 pages", "summary": "The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8def\u5f84\u5dee\u5f02\u5316\u88c1\u526a\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08MPD-PPO\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u6a61\u80f6\u8f6e\u80ce\u5236\u9020\u4e2d\u7684\u9ad8\u7ef4\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5728\u8f6e\u80ce\u8584\u819c\u751f\u4ea7\u7684\u5bbd\u5ea6\u548c\u539a\u5ea6\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u667a\u80fd\u5236\u9020\u9700\u8981\u89e3\u51b3\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u548c\u751f\u4ea7\u914d\u7f6e\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5e94\u5bf9\u52a8\u6001\u751f\u4ea7\u9700\u6c42\u3002\u8f6e\u80ce\u5236\u9020\u7cfb\u7edf\u5305\u542b\u590d\u6742\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\u548c\u6d8c\u73b0\u52a8\u6001\uff0c\u591a\u5b50\u7cfb\u7edf\u534f\u8c03\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u5206\u652f\u7b56\u7565\u67b6\u6784\u548c\u5dee\u5f02\u5316\u68af\u5ea6\u88c1\u526a\u7ea6\u675f\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5MPD-PPO\uff0c\u786e\u4fdd\u9ad8\u7ef4\u7b56\u7565\u66f4\u65b0\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u6a61\u80f6\u8f6e\u80ce\u8584\u819c\u751f\u4ea7\u7684\u5bbd\u5ea6\u548c\u539a\u5ea6\u63a7\u5236\u5b9e\u9a8c\u4e2d\uff0cMPD-PPO\u5728\u8c03\u8c10\u7cbe\u5ea6\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5ea6\u3001\u591a\u76ee\u6807\u6743\u8861\u548c\u52a8\u6001\u9002\u5e94\u7b49\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8f6e\u80ce\u5236\u9020\u5b9e\u65f6\u5de5\u4e1a\u90e8\u7f72\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u6027\u80fd\u548c\u751f\u4ea7\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.12659", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12659", "abs": "https://arxiv.org/abs/2511.12659", "authors": ["Alon Cohen", "Liad Erez", "Steve Hanneke", "Tomer Koren", "Yishay Mansour", "Shay Moran", "Qian Zhang"], "title": "Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back", "comment": null, "summary": "The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\\frac{DS^{1.5}}\u03b5 + \\frac{Nat}{\u03b5^2}$ where $\u03b5$ is the excess risk. This bound is tight up to a $\\sqrt{DS}$ factor in the first term, nearly matching known $Nat/\u03b5^2$ and $DS/\u03b5$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $\u03b5$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.", "AI": {"tldr": "\u591a\u7c7b\u522bPAC\u5b66\u4e60\u7684\u6837\u672c\u590d\u6742\u5ea6\u7531\u4e24\u4e2a\u7ef4\u5ea6\u5171\u540c\u63a7\u5236\uff1aDS\u7ef4\u5ea6\u4e3b\u5bfcDS\u63a7\u5236\u533a\u57df\uff0cNatarajan\u7ef4\u5ea6\u51b3\u5b9a\u5c0f\u03b5\u65f6\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u5f62\u5f0f\u4e3aDS^{1.5}/\u03b5 + Nat/\u03b5\u00b2\u3002", "motivation": "\u6269\u5c55\u7edf\u8ba1\u5b66\u4e60\u57fa\u672c\u5b9a\u7406\u5230\u591a\u7c7b\u522b\u5206\u7c7b\uff0c\u7406\u89e3\u5176\u4e0e\u4e8c\u5206\u7c7b\u5b66\u4e60\u7684\u5dee\u5f02\uff0c\u63a2\u7a76Natarajan\u7ef4\u5ea6\u548cDS\u7ef4\u5ea6\u5728\u591a\u7c7b\u522b\u5b66\u4e60\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u81ea\u9002\u5e94\u4e58\u6027\u6743\u91cd\u7684\u5728\u7ebf\u7b97\u6cd5\u8fdb\u884c\u6807\u7b7e\u7a7a\u95f4\u7ea6\u7b80\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u6216\u53ef\u7ea6\u5316\u60c5\u51b5\u7684\u4e0d\u53ef\u77e5\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u7c7b\u522b\u4e0d\u53ef\u77e5PAC\u6837\u672c\u590d\u6742\u5ea6\u7684\u7d27\u754c\uff0c\u5f62\u5f0f\u4e3aDS^{1.5}/\u03b5 + Nat/\u03b5\u00b2\uff0c\u8be5\u754c\u5728\u7b2c\u4e00\u4e2a\u9879\u4e0a\u7d27\u81f3\u221aDS\u56e0\u5b50\uff0c\u51e0\u4e4e\u5339\u914d\u5df2\u77e5\u7684Nat/\u03b5\u00b2\u548cDS/\u03b5\u4e0b\u754c\u3002", "conclusion": "\u591a\u7c7b\u522b\u5b66\u4e60\u672c\u8d28\u4e0a\u6d89\u53ca\u4e24\u4e2a\u7ed3\u6784\u53c2\u6570\uff0c\u4e0e\u4e8c\u5206\u7c7b\u6216\u5728\u7ebf\u5206\u7c7b\uff08\u7531\u5355\u4e00\u7ef4\u5ea6\u63a7\u5236\uff09\u4e0d\u540c\uff0cDS\u7ef4\u5ea6\u548cNatarajan\u7ef4\u5ea6\u5171\u540c\u51b3\u5b9a\u5b66\u4e60\u884c\u4e3a\u3002"}}
{"id": "2511.13034", "categories": ["eess.SY", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.13034", "abs": "https://arxiv.org/abs/2511.13034", "authors": ["Rahul Misra", "Manuela L. Bujorianu", "Rafa\u0142 Wisniewski"], "title": "An Online Multiobjective Policy Gradient for Long-run Average-reward Markov Decision Process", "comment": null, "summary": "We propose a reinforcement learning (RL) framework for multi-objective decision-making, where the agent seeks to optimize a vector of rewards rather than a single scalar value. The objective is to ensure that the time-averaged reward vector converges asymptotically to a predefined target set. Since standard RL algorithms operate on scalar rewards, we introduce a dynamic scalarization mechanism guided by Blackwell's Approachability Theorem. This theorem enables adaptive updates of the scalarization vector to guarantee convergence toward the target set. Assuming ergodicity, the Markov chain induced by the learned policies admits a stationary distribution, ensuring all states recur with finite return times. Our algorithm exploits this property by defining an inner loop that applies a policy gradient method (with baseline) between successive visits to a designated recurrent state, enforcing Blackwell's condition at each iteration. An outer loop then updates the scalarization vector after each recurrence. We establish theoretical convergence of the long-run average reward vector to the target set and validate the approach through a numerical example.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u76ee\u6807\u51b3\u7b56\u6846\u67b6\uff0c\u4f7f\u7528Blackwell\u65b9\u6cd5\u5b9a\u7406\u6307\u5bfc\u7684\u52a8\u6001\u6807\u91cf\u5316\u673a\u5236\uff0c\u786e\u4fdd\u65f6\u95f4\u5e73\u5747\u5956\u52b1\u5411\u91cf\u6536\u655b\u5230\u9884\u5b9a\u4e49\u76ee\u6807\u96c6\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5904\u7406\u6807\u91cf\u5956\u52b1\uff0c\u800c\u591a\u76ee\u6807\u51b3\u7b56\u9700\u8981\u4f18\u5316\u5956\u52b1\u5411\u91cf\uff0c\u786e\u4fdd\u6536\u655b\u5230\u76ee\u6807\u96c6\u5408\u3002", "method": "\u7ed3\u5408Blackwell\u65b9\u6cd5\u5b9a\u7406\u7684\u52a8\u6001\u6807\u91cf\u5316\u673a\u5236\uff0c\u5185\u5faa\u73af\u5728\u6307\u5b9a\u5faa\u73af\u72b6\u6001\u95f4\u5e94\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u5916\u5faa\u73af\u66f4\u65b0\u6807\u91cf\u5316\u5411\u91cf\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u957f\u671f\u5e73\u5747\u5956\u52b1\u5411\u91cf\u6536\u655b\u5230\u76ee\u6807\u96c6\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u6536\u655b\u6027\uff0c\u6269\u5c55\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u76ee\u6807\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2511.12320", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12320", "abs": "https://arxiv.org/abs/2511.12320", "authors": ["Muhidin Mohamed", "Shubhadeep Mukherjee", "Bhavana Baad"], "title": "Impact of UK Postgraduate Student Experiences on Academic Performance in Blended Learning: A Data Analytics Approach", "comment": "25 pages, 5 figures", "summary": "Blended learning has become a dominant educational model in higher education in the UK and worldwide, particularly after the COVID-19 pandemic. This is further enriched with accompanying pedagogical changes, such as strengthened asynchronous learning, and the use of AI (from ChatGPT and all other similar tools that followed) and other technologies to aid learning. While these educational transformations have enabled flexibility in learning and resource access, they have also exposed new challenges on how students can construct successful learning in hybrid learning environments. In this paper, we investigate the interaction between different dimensions of student learning experiences (ranging from perceived acceptance of teaching methods and staff support/feedback to learning pressure and student motivation) and academic achievement within the context of postgraduate blended learning in UK universities. To achieve this, we employed a combination of several data analytics techniques including visualization, statistical tests, regression analysis, and latent profile analysis. Our empirical results (based on a survey of 255 postgraduate students and holistically interpreted via the Community of Inquiry (CoI) framework) demonstrated that learning activities combining teaching and social presences, and tailored academic support through effective feedback are critical elements for successful postgraduate experience in blended learning contexts. Regarding contributions, this research advances the understanding of student success by identifying the various ways demographic, experiential, and psychological factors impact academic outcomes. And in theoretical terms, it contributes to the extension of the CoI framework by integrating the concept of learner heterogeneity and identifying four distinct student profiles based on how they engage in the different CoI presences.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u82f1\u56fd\u5927\u5b66\u7814\u7a76\u751f\u6df7\u5408\u5b66\u4e60\u73af\u5883\u4e2d\u5b66\u751f\u5b66\u4e60\u4f53\u9a8c\u4e0d\u540c\u7ef4\u5ea6\u4e0e\u5b66\u4e1a\u6210\u5c31\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u7ed3\u5408\u6559\u5b66\u5b58\u5728\u548c\u793e\u4f1a\u5b58\u5728\u7684\u5b66\u4e60\u6d3b\u52a8\u4ee5\u53ca\u901a\u8fc7\u6709\u6548\u53cd\u9988\u63d0\u4f9b\u7684\u5b9a\u5236\u5316\u5b66\u672f\u652f\u6301\u662f\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u6df7\u5408\u5b66\u4e60\u5df2\u6210\u4e3a\u9ad8\u7b49\u6559\u80b2\u7684\u4e3b\u8981\u6a21\u5f0f\uff0c\u4f46\u5b66\u751f\u5728\u6df7\u5408\u5b66\u4e60\u73af\u5883\u4e2d\u5982\u4f55\u6784\u5efa\u6210\u529f\u5b66\u4e60\u9762\u4e34\u65b0\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5b66\u751f\u5b66\u4e60\u4f53\u9a8c\u5404\u7ef4\u5ea6\u4e0e\u5b66\u4e1a\u6210\u5c31\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u591a\u79cd\u6570\u636e\u5206\u6790\u6280\u672f\uff0c\u5305\u62ec\u53ef\u89c6\u5316\u3001\u7edf\u8ba1\u68c0\u9a8c\u3001\u56de\u5f52\u5206\u6790\u548c\u6f5c\u5728\u5256\u9762\u5206\u6790\uff0c\u57fa\u4e8e255\u540d\u7814\u7a76\u751f\u7684\u8c03\u67e5\u6570\u636e\uff0c\u901a\u8fc7\u63a2\u7a76\u793e\u533a\u6846\u67b6\u8fdb\u884c\u6574\u4f53\u89e3\u8bfb\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u6559\u5b66\u5b58\u5728\u548c\u793e\u4f1a\u5b58\u5728\u7684\u5b66\u4e60\u6d3b\u52a8\u4ee5\u53ca\u901a\u8fc7\u6709\u6548\u53cd\u9988\u63d0\u4f9b\u7684\u5b9a\u5236\u5316\u5b66\u672f\u652f\u6301\u662f\u6df7\u5408\u5b66\u4e60\u73af\u5883\u4e2d\u7814\u7a76\u751f\u6210\u529f\u4f53\u9a8c\u7684\u5173\u952e\u8981\u7d20\u3002\u7814\u7a76\u8bc6\u522b\u4e86\u56db\u79cd\u57fa\u4e8e\u4e0d\u540c\u63a2\u7a76\u793e\u533a\u5b58\u5728\u53c2\u4e0e\u65b9\u5f0f\u7684\u5b66\u751f\u6863\u6848\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u8bc6\u522b\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u4f53\u9a8c\u6027\u548c\u5fc3\u7406\u56e0\u7d20\u5bf9\u5b66\u4e1a\u6210\u679c\u7684\u4e0d\u540c\u5f71\u54cd\u65b9\u5f0f\uff0c\u63a8\u8fdb\u4e86\u5bf9\u5b66\u751f\u6210\u529f\u7684\u7406\u89e3\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u901a\u8fc7\u6574\u5408\u5b66\u4e60\u8005\u5f02\u8d28\u6027\u6982\u5ff5\u6269\u5c55\u4e86\u63a2\u7a76\u793e\u533a\u6846\u67b6\u3002"}}
{"id": "2511.13455", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.13455", "abs": "https://arxiv.org/abs/2511.13455", "authors": ["Giacomo Albi", "Dante Kalise", "Chiara Segala", "Franco Zivcovich"], "title": "Sparse stabilization of mean-field agent dynamics through a three-operator splitting method", "comment": null, "summary": "We study the sparse stabilization of nonlinear multi-agent systems within a mean-field optimal control framework. The goal is to drive large populations of interacting agents toward consensus with minimal control effort. In the mean-field limit, the dynamics are described by a Vlasov-type kinetic equation, and sparsity is enforced through an l1-l2 penalization in the cost functional. The resulting nonsmooth optimization problem is solved via a three-operator splitting (TOS) method that separately handles smooth, nonsmooth, and constraint components through gradient, shrinkage, and projection steps. A particle-based Monte Carlo discretization with random batch interactions enables scalable computation while preserving the mean-field structure. Numerical experiments on the Cucker-Smale model demonstrate effective consensus formation with sparse, localized control actions, confirming the efficiency and robustness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5747\u503c\u573a\u6700\u4f18\u63a7\u5236\u7684\u975e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7a00\u758f\u7a33\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7l1-l2\u60e9\u7f5a\u5b9e\u73b0\u6700\u5c0f\u63a7\u5236\u52aa\u529b\u4e0b\u7684\u5171\u8bc6\u5f62\u6210\uff0c\u91c7\u7528\u4e09\u7b97\u5b50\u5206\u88c2\u7b97\u6cd5\u6c42\u89e3\u975e\u5149\u6ed1\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5927\u89c4\u6a21\u4ea4\u4e92\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7a00\u758f\u7a33\u5b9a\u95ee\u9898\uff0c\u76ee\u6807\u662f\u4ee5\u6700\u5c0f\u7684\u63a7\u5236\u52aa\u529b\u9a71\u52a8\u667a\u80fd\u4f53\u7fa4\u4f53\u8fbe\u6210\u5171\u8bc6\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u63a7\u5236\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5728\u5747\u503c\u573a\u6781\u9650\u4e0b\u4f7f\u7528Vlasov\u578b\u52a8\u529b\u5b66\u65b9\u7a0b\u63cf\u8ff0\u7cfb\u7edf\uff0c\u91c7\u7528l1-l2\u60e9\u7f5a\u9879\u5f3a\u5236\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u4e09\u7b97\u5b50\u5206\u88c2\u65b9\u6cd5\u5206\u522b\u5904\u7406\u5149\u6ed1\u3001\u975e\u5149\u6ed1\u548c\u7ea6\u675f\u5206\u91cf\uff0c\u7ed3\u5408\u7c92\u5b50\u8499\u7279\u5361\u6d1b\u79bb\u6563\u5316\u548c\u968f\u673a\u6279\u6b21\u4ea4\u4e92\u5b9e\u73b0\u53ef\u6269\u5c55\u8ba1\u7b97\u3002", "result": "\u5728Cucker-Smale\u6a21\u578b\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5f62\u6210\u5171\u8bc6\uff0c\u540c\u65f6\u4ea7\u751f\u7a00\u758f\u3001\u5c40\u90e8\u5316\u7684\u63a7\u5236\u52a8\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5747\u503c\u573a\u6700\u4f18\u63a7\u5236\u6846\u67b6\u7ed3\u5408\u7a00\u758f\u60e9\u7f5a\u548c\u4e09\u7b97\u5b50\u5206\u88c2\u7b97\u6cd5\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7a00\u758f\u7a33\u5b9a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12159", "abs": "https://arxiv.org/abs/2511.12159", "authors": ["Yaocheng Zhang", "Haohuan Huang", "Zijun Song", "Yuanheng Zhu", "Qichao Zhang", "Zijie Zhao", "Dongbin Zhao"], "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic", "comment": "17 pages, 10 figures", "summary": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.", "AI": {"tldr": "CriticSearch\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u56de\u987e\u6027\u6279\u8bc4\u673a\u5236\u4e3a\u641c\u7d22\u4ee3\u7406\u63d0\u4f9b\u5bc6\u96c6\u7684\u56de\u5408\u7ea7\u53cd\u9988\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u7684\u4f4e\u6548\u63a2\u7d22\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u7ba1\u9053\u901a\u5e38\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u4f46\u5b58\u5728\u7a00\u758f\u7ed3\u679c\u5956\u52b1\u95ee\u9898\uff0c\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u975e\u5bf9\u79f0\u6279\u8bc4LLM\uff0c\u57fa\u4e8e\u5b8c\u6574\u8f68\u8ff9\u548c\u9ec4\u91d1\u7b54\u6848\u7684\u6743\u9650\u4fe1\u606f\u56de\u987e\u6027\u8bc4\u4f30\u6bcf\u4e2a\u56de\u5408\uff0c\u5c06\u8fd9\u4e9b\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7a33\u5b9a\u7684\u5bc6\u96c6\u5956\u52b1\u6765\u6307\u5bfc\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u5728\u591a\u6837\u5316\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCriticSearch\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "CriticSearch\u901a\u8fc7\u5bc6\u96c6\u7684\u56de\u5408\u7ea7\u53cd\u9988\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u641c\u7d22\u4ee3\u7406\u5728\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.11632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11632", "abs": "https://arxiv.org/abs/2511.11632", "authors": ["Qiuhao Zeng"], "title": "Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination", "comment": "20 pages, 5 figures", "summary": "In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5143\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u5206\u7c7b\u5668\u5206\u89e3\u4e3a\u5143\u7ec4\u4ef6\u6765\u6539\u8fdb\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u7528\u6b63\u4ea4\u6b63\u5219\u5316\u4fc3\u8fdb\u5143\u7ec4\u4ef6\u7684\u591a\u6837\u6027\u3002", "motivation": "\u57fa\u4e8e\u5ea6\u91cf\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u53ef\u80fd\u5bf9\u5df2\u89c1\u7c7b\u522b\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5c06\u5206\u7c7b\u5668\u8868\u793a\u4e3a\u5143\u7ec4\u4ef6\u7684\u7ec4\u5408\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u5728\u5df2\u89c1\u7c7b\u522b\u4e0a\u5b66\u4e60\u5143\u7ec4\u4ef6\uff0c\u5e76\u65bd\u52a0\u6b63\u4ea4\u6b63\u5219\u5316\u6765\u4fc3\u8fdb\u5143\u7ec4\u4ef6\u7684\u591a\u6837\u6027\u548c\u6355\u83b7\u4e0d\u540c\u5206\u7c7b\u5668\u95f4\u7684\u5171\u4eab\u5b50\u7ed3\u6784\u3002", "result": "\u5728\u5c0f\u6837\u672c\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5143\u7ec4\u4ef6\u5206\u89e3\u548c\u6b63\u4ea4\u6b63\u5219\u5316\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12063", "abs": "https://arxiv.org/abs/2511.12063", "authors": ["Enoch Hyunwook Kang", "Hema Yoganarasimhan"], "title": "Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework", "comment": null, "summary": "Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faT-BoN BO\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Best-of-N\u9009\u62e9\u548c\u6587\u672c\u68af\u5ea6\u6765\u6a21\u62dfUCB\u91c7\u96c6\u51fd\u6570\uff0c\u5728\u8bed\u8a00\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u8bc4\u4f30\u9ad8\u6548\u7684\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u7528\u4e8eAI\u81ea\u6211\u6539\u8fdb\u3002", "motivation": "\u5f53\u524dAI\u81ea\u6211\u6539\u8fdb\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u67e5\u8be2\u6548\u7387\uff0c\u4f46\u5728\u8bb8\u591a\u793e\u4f1a\u5e94\u7528\u4e2d\uff0c\u8bc4\u4f30\u6210\u672c\u8fdc\u9ad8\u4e8e\u751f\u6210\u6210\u672c\u3002\u9700\u8981\u5f00\u53d1\u9488\u5bf9\u8bc4\u4f30\u6548\u7387\u4f18\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faT-BoN BO\u6846\u67b6\uff0c\u8bc1\u660eBest-of-N\u9009\u62e9\u7b56\u7565\u4e0e\u6587\u672c\u68af\u5ea6\u7ec4\u5408\u80fd\u591f\u7edf\u8ba1\u6a21\u62dfUCB\u91c7\u96c6\u51fd\u6570\u7684\u68af\u5ea6\u884c\u4e3a\uff0c\u5b9e\u73b0\u8bed\u8a00\u7a7a\u95f4\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u3002", "result": "\u5728\u81ea\u52a8\u5e7f\u544a\u5bf9\u9f50\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86T-BoN BO\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "T-BoN BO\u4e3aAI\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u4e14\u8bc4\u4f30\u9ad8\u6548\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8bc4\u4f30\u6210\u672c\u9ad8\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.12695", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12695", "abs": "https://arxiv.org/abs/2511.12695", "authors": ["Minghui Chen", "Hrad Ghoukasian", "Ruinan Jin", "Zehua Wang", "Sai Praneeth Karimireddy", "Xiaoxiao Li"], "title": "A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning", "comment": "33 pages, 6 figures, 8 tables", "summary": "Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.", "AI": {"tldr": "\u5c06\u96c6\u4e2d\u5f0f\u5b66\u4e60\u4e2d\u7684\u7ebf\u6027\u63a2\u6d4b\u540e\u5fae\u8c03(LP-FT)\u65b9\u6cd5\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\uff0c\u89e3\u51b3\u4e2a\u6027\u5316\u5fae\u8c03\u4e2d\u7684\u7279\u5f81\u5931\u771f\u95ee\u9898\uff0c\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u5e73\u8861\u4e2a\u6027\u5316\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u96be\u4ee5\u5e73\u8861\u5168\u5c40\u6cdb\u5316\u4e0e\u672c\u5730\u4e2a\u6027\u5316\uff0c\u4e2a\u6027\u5316\u5fae\u8c03\u65b9\u6cd5\u5bb9\u6613\u5728\u504f\u659c\u7684\u5ba2\u6237\u7aef\u5206\u5e03\u4e0a\u8fc7\u62df\u5408\u6216\u65e0\u6cd5\u9002\u5e94\u9886\u57df\u504f\u79fb\u3002", "method": "\u5c06\u7ebf\u6027\u63a2\u6d4b\u540e\u5fae\u8c03(LP-FT)\u7b56\u7565\u4ece\u96c6\u4e2d\u5f0f\u5b66\u4e60\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u7684\u53c2\u6570\u66f4\u65b0\u6765\u7f13\u89e3\u8054\u90a6\u7279\u5f81\u5931\u771f\u73b0\u8c61\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u548c\u516d\u79cd\u4e2a\u6027\u5316\u5fae\u8c03\u53d8\u4f53\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\u8868\u660e\uff0cLP-FT\u5728\u5e73\u8861\u4e2a\u6027\u5316\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "LP-FT\u901a\u8fc7\u7f13\u89e3\u8054\u90a6\u7279\u5f81\u5931\u771f\uff0c\u5728\u90e8\u5206\u7279\u5f81\u91cd\u53e0\u3001\u534f\u53d8\u91cf-\u6982\u5ff5\u504f\u79fb\u7b49\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u90e8\u7f72\u9c81\u68d2\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u6307\u5357\u3002"}}
{"id": "2511.13117", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13117", "abs": "https://arxiv.org/abs/2511.13117", "authors": ["Anchita Dey", "Soutrik Bandyopadhyay", "Shubhendu Bhasin"], "title": "Initial Excitation-based Adaptive Observers for Discrete-Time LTI Systems", "comment": null, "summary": "In practical applications, the efficacy of a control algorithm relies critically on the accurate knowledge of the parameters and states of the underlying system. However, obtaining these quantities in practice is often challenging. Adaptive observers address this issue by performing simultaneous state and parameter estimation using only input-output measurements. While many adaptive observer designs exist for continuous-time systems, their discrete-time counterparts remain relatively unexplored. This paper proposes an initial excitation (IE)-based adaptive observer for discrete-time linear time-invariant systems. In contrast to conventional designs that rely on the persistence of excitation condition, which requires continuous excitation and infinite control effort, the proposed method does not require excitation for infinite time, thus making it more practical for stabilization tasks. We employ a two-layer filtering structure and a normalized gradient descent-based update law for learning the unknown parameters. We also propose modifying the regressors to enhance information extraction, leading to faster convergence. Rigorous theoretical analysis guarantees bounded and exponentially converging estimates of both states and parameters under the IE condition, and simulation results validate the efficacy of the proposed design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u521d\u59cb\u6fc0\u52b1(IE)\u7684\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\uff0c\u7528\u4e8e\u79bb\u6563\u65f6\u95f4\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\uff0c\u76f8\u6bd4\u4f20\u7edf\u9700\u8981\u6301\u7eed\u6fc0\u52b1\u7684\u65b9\u6cd5\u66f4\u5b9e\u7528\uff0c\u91c7\u7528\u53cc\u5c42\u6ee4\u6ce2\u7ed3\u6784\u548c\u5f52\u4e00\u5316\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u5f8b\u3002", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\u7cfb\u7edf\u53c2\u6570\u548c\u72b6\u6001\u96be\u4ee5\u51c6\u786e\u83b7\u53d6\uff0c\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\u5df2\u6709\u8f83\u591a\u7814\u7a76\uff0c\u4f46\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u7684\u5bf9\u5e94\u65b9\u6cd5\u76f8\u5bf9\u8f83\u5c11\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u65e0\u9650\u65f6\u95f4\u7684\u6301\u7eed\u6fc0\u52b1\uff0c\u4e0d\u9002\u7528\u4e8e\u7a33\u5b9a\u5316\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u53cc\u5c42\u6ee4\u6ce2\u7ed3\u6784\u548c\u5f52\u4e00\u5316\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u5f8b\u5b66\u4e60\u672a\u77e5\u53c2\u6570\uff0c\u6539\u8fdb\u56de\u5f52\u5668\u4ee5\u589e\u5f3a\u4fe1\u606f\u63d0\u53d6\uff0c\u5728\u521d\u59cb\u6fc0\u52b1\u6761\u4ef6\u4e0b\u4fdd\u8bc1\u72b6\u6001\u548c\u53c2\u6570\u4f30\u8ba1\u7684\u6709\u754c\u6027\u548c\u6307\u6570\u6536\u655b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5728\u521d\u59cb\u6fc0\u52b1\u6761\u4ef6\u4e0b\uff0c\u72b6\u6001\u548c\u53c2\u6570\u4f30\u8ba1\u6709\u754c\u4e14\u6307\u6570\u6536\u655b\uff0c\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u521d\u59cb\u6fc0\u52b1\u7684\u81ea\u9002\u5e94\u89c2\u6d4b\u5668\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5b9e\u7528\uff0c\u4e0d\u9700\u8981\u65e0\u9650\u65f6\u95f4\u6fc0\u52b1\uff0c\u9002\u7528\u4e8e\u7a33\u5b9a\u5316\u4efb\u52a1\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2511.12369", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12369", "abs": "https://arxiv.org/abs/2511.12369", "authors": ["Mohamed Amine Kada Zair"], "title": "Cultural Awareness, Stereotypes and Communication Skills in Intercultural Communication: The Algerian Participants Perspective", "comment": "20 pages, 9 tables, preprint", "summary": "This study explores the relationship between cultural awareness, stereotypes, and communication skills among Algerian participants working or studying in multicultural environments. A quantitative questionnaire was administered to 40 respondents to evaluate their levels of cultural awareness, the presence of stereotypical thinking, and the effectiveness of their intercultural communication skills. Results revealed that while cultural awareness was generally high, certain stereotypes still influenced the perception of others and impacted communication efficiency. Participants with higher cultural awareness demonstrated better communication skills and lower levels of stereotyping. These findings underline the importance of intercultural competence and education programs in reducing prejudice and fostering mutual understanding in diverse contexts.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u963f\u5c14\u53ca\u5229\u4e9a\u53c2\u4e0e\u8005\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u7684\u6587\u5316\u610f\u8bc6\u3001\u523b\u677f\u5370\u8c61\u4e0e\u6c9f\u901a\u6280\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u9ad8\u6587\u5316\u610f\u8bc6\u80fd\u6539\u5584\u6c9f\u901a\u5e76\u51cf\u5c11\u523b\u677f\u5370\u8c61\u3002", "motivation": "\u7814\u7a76\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u6587\u5316\u610f\u8bc6\u3001\u523b\u677f\u5370\u8c61\u548c\u6c9f\u901a\u6280\u80fd\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u4e3a\u8de8\u6587\u5316\u80fd\u529b\u57f9\u517b\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u5b9a\u91cf\u95ee\u5377\u8c03\u67e5\u6cd5\uff0c\u5bf940\u540d\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u5de5\u4f5c\u6216\u5b66\u4e60\u7684\u963f\u5c14\u53ca\u5229\u4e9a\u53c2\u4e0e\u8005\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6587\u5316\u610f\u8bc6\u666e\u904d\u8f83\u9ad8\uff0c\u4f46\u67d0\u4e9b\u523b\u677f\u5370\u8c61\u4ecd\u5f71\u54cd\u4eba\u9645\u8ba4\u77e5\u548c\u6c9f\u901a\u6548\u7387\uff1b\u9ad8\u6587\u5316\u610f\u8bc6\u8005\u6c9f\u901a\u6280\u80fd\u66f4\u597d\u3001\u523b\u677f\u5370\u8c61\u66f4\u5c11\u3002", "conclusion": "\u8de8\u6587\u5316\u80fd\u529b\u6559\u80b2\u548c\u57f9\u8bad\u5bf9\u51cf\u5c11\u504f\u89c1\u3001\u4fc3\u8fdb\u591a\u5143\u73af\u5883\u4e2d\u7684\u76f8\u4e92\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.13499", "categories": ["math.OC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13499", "abs": "https://arxiv.org/abs/2511.13499", "authors": ["Anil Alan", "Bart De Schutter"], "title": "Uniform Feasibility For Smoothed Backup Control Barrier Functions", "comment": "8 pages, submitted to ECC", "summary": "We study feasibility guarantees for safety filters developed using Control Barrier Functions (CBFs) when a safe set is defined using the pointwise minimum of continuously differentiable functions, a construction that is common for the backup CBF method and typically nonsmooth. We replace the minimum by its log-sum-exp (soft-min) smoothing and show that, under a strict safety condition, the smooth function becomes a CBF (or extended CBF) for a range of the smoothing parameter. For compact safe sets, we derive an explicit lower bound on the smoothing parameter that makes the smooth function a CBF and hence renders the corresponding safety filter feasible. For unbounded sets, we introduce tail conditions under which the smooth function satisfies an extended CBF condition uniformly. Finally, we apply these results to backup CBFs. We show that safety of a compact (terminal) backup set under a backup controller, together with a condition ensuring safety of the backup trajectories on the relevant boundary of the safe set, is sufficient for feasibility for backup CBFs. These results provide a recipe for a priori feasibility guarantees for smooth inner approximations of nonsmooth safe sets without the need for additional online certification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\u5f00\u53d1\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u5728\u5b89\u5168\u96c6\u7531\u8fde\u7eed\u53ef\u5fae\u51fd\u6570\u7684\u9010\u70b9\u6700\u5c0f\u503c\u5b9a\u4e49\u65f6\u7684\u53ef\u884c\u6027\u4fdd\u8bc1\uff0c\u901a\u8fc7log-sum-exp\u5e73\u6ed1\u5904\u7406\u975e\u5149\u6ed1\u7ed3\u6784\uff0c\u4e3a\u5907\u4efdCBF\u65b9\u6cd5\u63d0\u4f9b\u5148\u9a8c\u53ef\u884c\u6027\u4fdd\u8bc1\u3002", "motivation": "\u5f53\u5b89\u5168\u96c6\u4f7f\u7528\u8fde\u7eed\u53ef\u5fae\u51fd\u6570\u7684\u9010\u70b9\u6700\u5c0f\u503c\u5b9a\u4e49\u65f6\uff08\u5e38\u89c1\u4e8e\u5907\u4efdCBF\u65b9\u6cd5\uff09\uff0c\u8fd9\u79cd\u6784\u9020\u901a\u5e38\u662f\u975e\u5149\u6ed1\u7684\uff0c\u9700\u8981\u7814\u7a76\u5176\u53ef\u884c\u6027\u4fdd\u8bc1\u3002", "method": "\u4f7f\u7528log-sum-exp(\u8f6f\u6700\u5c0f\u503c)\u5e73\u6ed1\u66ff\u6362\u6700\u5c0f\u503c\u51fd\u6570\uff0c\u5728\u4e25\u683c\u5b89\u5168\u6761\u4ef6\u4e0b\u8bc1\u660e\u5e73\u6ed1\u51fd\u6570\u5728\u4e00\u5b9a\u53c2\u6570\u8303\u56f4\u5185\u6210\u4e3aCBF\u6216\u6269\u5c55CBF\uff0c\u4e3a\u7d27\u51d1\u548c\u65e0\u754c\u5b89\u5168\u96c6\u5206\u522b\u63a8\u5bfc\u5e73\u6ed1\u53c2\u6570\u4e0b\u754c\u548c\u5c3e\u90e8\u6761\u4ef6\u3002", "result": "\u5bf9\u4e8e\u7d27\u51d1\u5b89\u5168\u96c6\uff0c\u63a8\u5bfc\u51fa\u4f7f\u5e73\u6ed1\u51fd\u6570\u6210\u4e3aCBF\u7684\u5e73\u6ed1\u53c2\u6570\u663e\u5f0f\u4e0b\u754c\uff1b\u5bf9\u4e8e\u65e0\u754c\u96c6\uff0c\u5f15\u5165\u786e\u4fdd\u5e73\u6ed1\u51fd\u6570\u6ee1\u8db3\u7edf\u4e00\u6269\u5c55CBF\u6761\u4ef6\u7684\u5c3e\u90e8\u6761\u4ef6\uff1b\u8fd9\u4e9b\u7ed3\u679c\u5e94\u7528\u4e8e\u5907\u4efdCBFs\uff0c\u8bc1\u660e\u5907\u4efd\u96c6\u5b89\u5168\u6027\u548c\u8fb9\u754c\u6761\u4ef6\u8db3\u4ee5\u4fdd\u8bc1\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u975e\u5149\u6ed1\u5b89\u5168\u96c6\u7684\u5e73\u6ed1\u5185\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u5148\u9a8c\u53ef\u884c\u6027\u4fdd\u8bc1\u7684\u914d\u65b9\uff0c\u65e0\u9700\u989d\u5916\u7684\u5728\u7ebf\u8ba4\u8bc1\uff0c\u4e3a\u5907\u4efdCBF\u65b9\u6cd5\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.12213", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12213", "abs": "https://arxiv.org/abs/2511.12213", "authors": ["Liang Xue", "Haoyu Liu", "Yajun Tian", "Xinyu Zhong", "Yang Liu"], "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues", "comment": null, "summary": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.", "AI": {"tldr": "MME-RAG\u662f\u4e00\u4e2a\u591a\u7ba1\u7406\u5668-\u4e13\u5bb6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5b9e\u4f53\u8bc6\u522b\u5206\u89e3\u4e3a\u7c7b\u578b\u7ea7\u5224\u65ad\u548c\u8de8\u5ea6\u7ea7\u63d0\u53d6\u4e24\u4e2a\u534f\u8c03\u9636\u6bb5\uff0c\u89e3\u51b3\u4e86LLM\u5728\u9886\u57df\u9002\u5e94\u548c\u68c0\u7d22\u53ef\u63a7\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5411\u4efb\u52a1\u7684\u5bf9\u8bdd\u4e2d\uff0c\u5728\u9886\u57df\u9002\u5e94\u548c\u68c0\u7d22\u53ef\u63a7\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e0a\u3002", "method": "\u63d0\u51faMME-RAG\u6846\u67b6\uff0c\u5305\u542b\u8f7b\u91cf\u7ea7\u7ba1\u7406\u5668\u8fdb\u884c\u7c7b\u578b\u7ea7\u5224\u65ad\uff0c\u4e13\u4e1a\u4e13\u5bb6\u8fdb\u884c\u8de8\u5ea6\u7ea7\u63d0\u53d6\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u914d\u5907KeyInfo\u68c0\u7d22\u5668\u6ce8\u5165\u8bed\u4e49\u5bf9\u9f50\u7684\u5c11\u6837\u672c\u793a\u4f8b\u3002", "result": "\u5728CrossNER\u3001MIT-Movie\u3001MIT-Restaurant\u548c\u65b0\u6784\u5efa\u7684\u591a\u9886\u57df\u5ba2\u670d\u6570\u636e\u96c6\u4e0a\uff0cMME-RAG\u5728\u5927\u591a\u6570\u9886\u57df\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c42\u6b21\u5206\u89e3\u548cKeyInfo\u5f15\u5bfc\u7684\u68c0\u7d22\u662f\u9c81\u68d2\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0cMME-RAG\u4e3a\u81ea\u9002\u5e94\u5bf9\u8bdd\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11636", "categories": ["cs.LG", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11636", "abs": "https://arxiv.org/abs/2511.11636", "authors": ["Asma Sadia Khan", "Sadia Tabassum"], "title": "An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment", "comment": null, "summary": "This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u516c\u5e73\u5ba1\u8ba1\u548c\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u6765\u9884\u6d4b\u591a\u56ca\u5375\u5de2\u7efc\u5408\u5f81(PCOS)\uff0c\u901a\u8fc7SHAP\u7279\u5f81\u5f52\u56e0\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u5ba1\u8ba1\u8bc6\u522b\u8bca\u65ad\u5dee\u5f02\uff0c\u5e76\u786e\u4fdd\u8de8\u4e9a\u7ec4\u7684\u53ef\u9760\u98ce\u9669\u9884\u6d4b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3PCOS\u8bca\u65ad\u4e2d\u5b58\u5728\u7684\u60a3\u8005\u4e9a\u7ec4\u95f4\u5dee\u5f02\u95ee\u9898\uff0c\u786e\u4fdd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u7ed3\u679c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u4f7f\u7528\u968f\u673a\u68ee\u6797\u3001SVM\u548cXGBoost\u6a21\u578b\uff0c\u7ed3\u5408\u7b49\u6e17\u548cPlatt\u6807\u5b9a\u65b9\u6cd5\u8fdb\u884c\u516c\u5e73\u6027\u6bd4\u8f83\u3002\u96c6\u6210SHAP\u7279\u5f81\u5f52\u56e0\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u5ba1\u8ba1\uff0c\u5e76\u91c7\u7528\u6982\u7387\u6807\u5b9a\u6307\u6807(Brier Score\u548cECE)\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6807\u5b9a\u7684\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fbe\u523090.8%\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002SHAP\u5206\u6790\u8bc6\u522b\u51fa\u5375\u6ce1\u8ba1\u6570\u3001\u4f53\u91cd\u589e\u52a0\u548c\u6708\u7ecf\u4e0d\u89c4\u5f8b\u4e3a\u6700\u91cd\u8981\u7279\u5f81\u3002\u6a21\u578b\u572825-35\u5c81\u5973\u6027\u4e2d\u8868\u73b0\u6700\u4f73(90.9%)\uff0c\u4f46\u572825\u5c81\u4ee5\u4e0b\u5973\u6027\u4e2d\u8868\u73b0\u8f83\u5dee(69.2%)\u3002\u5728\u80a5\u80d6\u5973\u6027\u4e2d\u8fbe\u5230\u5b8c\u7f8e\u7cbe\u5ea6\uff0c\u5728\u7626\u578bPCOS\u75c5\u4f8b\u4e2d\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u3002", "conclusion": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6807\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\u3002\u5f00\u53d1\u4e86\u57fa\u4e8eStreamlit\u7684Web\u754c\u9762\uff0c\u5b9e\u73b0\u5b9e\u65f6PCOS\u98ce\u9669\u8bc4\u4f30\u548c\u4ea4\u4e92\u5f0f\u5206\u6790\uff0c\u5f25\u5408AI\u7814\u7a76\u4e0e\u4e34\u5e8a\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.12083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12083", "abs": "https://arxiv.org/abs/2511.12083", "authors": ["Yanchang Fu", "Shengda Liu", "Pei Xu", "Kaiqi Huang"], "title": "No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding", "comment": null, "summary": "High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.", "AI": {"tldr": "\u63d0\u51fa\u4e86Embedding CFR\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u4fe1\u606f\u96c6\u5d4c\u5165\u5230\u4f4e\u7ef4\u8fde\u7eed\u7a7a\u95f4\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u4e0d\u5b8c\u5168\u4fe1\u606f\u6269\u5c55\u5f0f\u535a\u5f08\uff0c\u76f8\u6bd4\u57fa\u4e8e\u805a\u7c7b\u7684\u62bd\u8c61\u65b9\u6cd5\u80fd\u66f4\u5feb\u6536\u655b\u4e14\u964d\u4f4e\u53ef\u5229\u7528\u6027\u3002", "motivation": "\u73b0\u6709AI\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u79bb\u6563\u805a\u7c7b\u8fdb\u884c\u62bd\u8c61\uff0c\u4f46\u786c\u5206\u7c7b\u4f1a\u4e0d\u53ef\u9006\u5730\u4e22\u5931\u4fe1\u606f\u96c6\u4e4b\u95f4\u7684\u91cf\u5316\u7ec6\u5fae\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u7b56\u7565\u6c42\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u53d7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u8bcd\u5d4c\u5165\u8303\u5f0f\u542f\u53d1\uff0c\u5c06\u5b64\u7acb\u4fe1\u606f\u96c6\u7684\u7279\u5f81\u9884\u8bad\u7ec3\u5e76\u5d4c\u5165\u5230\u76f8\u4e92\u8fde\u63a5\u7684\u4f4e\u7ef4\u8fde\u7eed\u7a7a\u95f4\uff0c\u5728\u8be5\u7a7a\u95f4\u4e2d\u901a\u8fc7\u9057\u61be\u79ef\u7d2f\u548c\u7b56\u7565\u66f4\u65b0\u8fdb\u884c\u7b56\u7565\u6c42\u89e3\u3002", "result": "\u5728\u6251\u514b\u5b9e\u9a8c\u4e2d\uff0c\u5728\u76f8\u540c\u7a7a\u95f4\u5f00\u9500\u4e0b\uff0cEmbedding CFR\u76f8\u6bd4\u57fa\u4e8e\u805a\u7c7b\u7684\u62bd\u8c61\u7b97\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u5feb\u7684\u53ef\u5229\u7528\u6027\u6536\u655b\u3002", "conclusion": "\u8fd9\u662f\u6251\u514bAI\u4e2d\u9996\u4e2a\u901a\u8fc7\u4f4e\u7ef4\u5d4c\u5165\u9884\u8bad\u7ec3\u4fe1\u606f\u96c6\u62bd\u8c61\u6765\u8fdb\u884c\u7b56\u7565\u6c42\u89e3\u7684\u7b97\u6cd5\uff0c\u80fd\u66f4\u7cbe\u786e\u5730\u6355\u6349\u4fe1\u606f\u96c6\u95f4\u7684\u5dee\u5f02\u548c\u8054\u7cfb\u3002"}}
{"id": "2511.12760", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12760", "abs": "https://arxiv.org/abs/2511.12760", "authors": ["Ben Gao", "Jordan Patracone", "St\u00e9phane Chr\u00e9tien", "Olivier Alata"], "title": "Conformal Online Learning of Deep Koopman Linear Embeddings", "comment": null, "summary": "We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.", "AI": {"tldr": "COLoKe\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u66f4\u65b0\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edfKoopman\u5d4c\u5165\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u591a\u6b65\u9884\u6d4b\u4e00\u81f4\u6027\u5728\u7ebf\u5b66\u4e60\uff0c\u4ec5\u5728\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u8d85\u8fc7\u52a8\u6001\u9608\u503c\u65f6\u89e6\u53d1\u66f4\u65b0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u5e76\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6a21\u578b\u66f4\u65b0\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u63d0\u5347\u7a7a\u95f4\u4e2d\u7684\u591a\u6b65\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u4f7f\u7528\u7b26\u5408\u6027\u673a\u5236\u52a8\u6001\u6821\u51c6\u9884\u6d4b\u8bef\u5dee\u9608\u503c\uff0c\u4ec5\u5728\u8bef\u5dee\u8d85\u8fc7\u9608\u503c\u65f6\u66f4\u65b0Koopman\u7b97\u5b50\u548c\u5d4c\u5165\u3002", "result": "\u5728\u57fa\u51c6\u52a8\u529b\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCOLoKe\u80fd\u4fdd\u6301\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u66f4\u65b0\u5e76\u907f\u514d\u8fc7\u62df\u5408\u3002", "conclusion": "COLoKe\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u5728\u7ebf\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u4f18\u5316\u4e86\u66f4\u65b0\u6548\u7387\u3002"}}
{"id": "2511.13119", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13119", "abs": "https://arxiv.org/abs/2511.13119", "authors": ["Xuxin Yang", "Xue Yuan", "Donghan Feng", "Siru Chen", "Yuanhao Feng"], "title": "Carbon Reduction Potential and Sensitivity Analysis of Rural Integrated Energy System with Carbon Trading and Coordinated Electric-Thermal Demand Response", "comment": null, "summary": "Constructing clean and low-carbon rural integrated energy system (RIES) is a fundamental requirement for supporting China's rural modernization and new-type urbanization. Existing research on RIES decarbonization primarily focuses on the optimal low-carbon operation of system-level energy devices at the macro level, while the synergistic carbon-reduction effects of demand-side flexible loads and external carbon trading mechanisms have not been fully explored. Meanwhile, at the micro level, the carbon sensitivity of device parameters and their potential contribution to emission reduction remain insufficiently investigated. To address these gaps, this study integrates macro- and micro-level analyses. At the macro level, a multi-energy-coupled low-carbon optimal operation framework is developed, incorporating coordinated electric-thermal demand response (DR) and carbon trading. At the micro level, a carbon emission model for RIES components is established, and sensitivity analysis is conducted on 28 carbon-related parameters to identify highly sensitive determinants of emission reduction. Case studies based on typical operation data from a rural region in northern China demonstrate that coordinated electric-thermal DR and carbon trading can achieve maximum carbon-reduction potential. Furthermore, the identified high-sensitivity parameters provide essential theoretical guidance for enhancing the decarbonization potential of RIES.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u519c\u6751\u7efc\u5408\u80fd\u6e90\u7cfb\u7edf(RIES)\u7684\u5b8f\u89c2\u548c\u5fae\u89c2\u4f4e\u78b3\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u7535\u70ed\u9700\u6c42\u54cd\u5e94\u548c\u78b3\u4ea4\u6613\u673a\u5236\uff0c\u8bc6\u522b\u4e8628\u4e2a\u78b3\u76f8\u5173\u53c2\u6570\u4e2d\u7684\u9ad8\u654f\u611f\u6027\u56e0\u7d20\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u51cf\u78b3\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709RIES\u51cf\u78b3\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5b8f\u89c2\u5c42\u9762\u7684\u7cfb\u7edf\u7ea7\u80fd\u6e90\u8bbe\u5907\u4f18\u5316\u8fd0\u884c\uff0c\u800c\u9700\u6c42\u4fa7\u67d4\u6027\u8d1f\u8377\u548c\u5916\u90e8\u78b3\u4ea4\u6613\u673a\u5236\u7684\u534f\u540c\u51cf\u78b3\u6548\u5e94\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u540c\u65f6\u5fae\u89c2\u5c42\u9762\u8bbe\u5907\u53c2\u6570\u7684\u78b3\u654f\u611f\u6027\u53ca\u5176\u51cf\u6392\u8d21\u732e\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5b8f\u89c2\u5c42\u9762\u5f00\u53d1\u591a\u80fd\u6e90\u8026\u5408\u4f4e\u78b3\u4f18\u5316\u8fd0\u884c\u6846\u67b6\uff0c\u6574\u5408\u534f\u8c03\u7535\u70ed\u9700\u6c42\u54cd\u5e94\u548c\u78b3\u4ea4\u6613\uff1b\u5fae\u89c2\u5c42\u9762\u5efa\u7acbRIES\u7ec4\u4ef6\u78b3\u6392\u653e\u6a21\u578b\uff0c\u5bf928\u4e2a\u78b3\u76f8\u5173\u53c2\u6570\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u57fa\u4e8e\u4e2d\u56fd\u5317\u65b9\u519c\u6751\u5178\u578b\u8fd0\u884c\u6570\u636e\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u534f\u8c03\u7535\u70ed\u9700\u6c42\u54cd\u5e94\u548c\u78b3\u4ea4\u6613\u53ef\u5b9e\u73b0\u6700\u5927\u51cf\u78b3\u6f5c\u529b\uff0c\u8bc6\u522b\u7684\u9ad8\u654f\u611f\u6027\u53c2\u6570\u4e3a\u589e\u5f3aRIES\u51cf\u78b3\u6f5c\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b8f\u89c2\u5fae\u89c2\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u519c\u6751\u7efc\u5408\u80fd\u6e90\u7cfb\u7edf\u7684\u51cf\u78b3\u80fd\u529b\uff0c\u4e3a\u652f\u6301\u4e2d\u56fd\u519c\u6751\u73b0\u4ee3\u5316\u548c\u65b0\u578b\u57ce\u9547\u5316\u63d0\u4f9b\u4e86\u4f4e\u78b3\u80fd\u6e90\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12426", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12426", "abs": "https://arxiv.org/abs/2511.12426", "authors": ["Stefano Civelli", "Pietro Bernardelle", "Frank Mols", "Gianluca Demartini"], "title": "Political Advertising on Facebook During the 2022 Australian Federal Election: A Social Identity Perspective", "comment": null, "summary": "The spread of targeted advertising on social media platforms has revolutionized political marketing strategies. Monitoring these digital campaigns is essential for maintaining transparency and accountability in democratic processes. Leveraging Meta's Ad Library, we analyze political advertising on Facebook and Instagram during the 2022 Australian federal election campaign. We investigate temporal, demographic, and geographical patterns in the advertising strategies of major Australian political actors to establish an empirical evidence base, and interpret these findings through the lens of Social Identity Theory (SIT). Our findings not only reveal significant disparities in spending and reach among parties, but also in persuasion strategies being deployed in targeted online campaigns. We observe a marked increase in advertising activity as the election approached, peaking just before the mandated media blackout period. Demographic analysis shows distinct targeting strategies, with parties focusing more on younger demographics and exhibiting gender-based differences in ad impressions. Regional distribution of ads largely mirrored population densities, with some parties employing more targeted approaches in specific states. Moreover, we found that parties emphasized different themes aligned with their ideologies-major parties focused on party names and opponents, while smaller parties emphasized issue-specific messages. Drawing on SIT, we interpret these findings within Australia's compulsory voting context, suggesting that parties employed distinct persuasion strategies. With turnout guaranteed, major parties focused on reinforcing partisan identities to prevent voter defection, while smaller parties cultivated issue-based identities to capture the support of disaffected voters who are obligated to participate.", "AI": {"tldr": "\u57fa\u4e8eMeta\u5e7f\u544a\u5e93\u5206\u67902022\u5e74\u6fb3\u5927\u5229\u4e9a\u8054\u90a6\u9009\u4e3e\u671f\u95f4Facebook\u548cInstagram\u653f\u6cbb\u5e7f\u544a\uff0c\u63ed\u793a\u5404\u515a\u6d3e\u5728\u652f\u51fa\u3001\u53d7\u4f17\u5b9a\u4f4d\u548c\u8bf4\u670d\u7b56\u7565\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u793e\u4f1a\u8ba4\u540c\u7406\u8bba\u89e3\u91ca\u8fd9\u4e9b\u7b56\u7565\u5728\u5f3a\u5236\u6295\u7968\u80cc\u666f\u4e0b\u7684\u4f5c\u7528\u3002", "motivation": "\u76d1\u63a7\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5b9a\u5411\u653f\u6cbb\u5e7f\u544a\u5bf9\u4e8e\u7ef4\u62a4\u6c11\u4e3b\u8fdb\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5efa\u7acb\u5b9e\u8bc1\u8bc1\u636e\u57fa\u7840\u6765\u7406\u89e3\u6570\u5b57\u653f\u6cbb\u8425\u9500\u7b56\u7565\u3002", "method": "\u5229\u7528Meta\u5e7f\u544a\u5e93\u5206\u67902022\u5e74\u6fb3\u5927\u5229\u4e9a\u8054\u90a6\u9009\u4e3e\u671f\u95f4\u7684\u653f\u6cbb\u5e7f\u544a\u6570\u636e\uff0c\u7814\u7a76\u4e3b\u8981\u653f\u6cbb\u884c\u4e3a\u4f53\u7684\u65f6\u95f4\u3001\u4eba\u53e3\u7edf\u8ba1\u548c\u5730\u7406\u5206\u5e03\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u793e\u4f1a\u8ba4\u540c\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u53d1\u73b0\u3002", "result": "\u53d1\u73b0\u9009\u4e3e\u4e34\u8fd1\u65f6\u5e7f\u544a\u6d3b\u52a8\u663e\u8457\u589e\u52a0\uff1b\u5404\u515a\u6d3e\u9488\u5bf9\u5e74\u8f7b\u4eba\u7fa4\u91c7\u7528\u4e0d\u540c\u7684\u53d7\u4f17\u5b9a\u4f4d\u7b56\u7565\uff1b\u5e7f\u544a\u5730\u7406\u5206\u5e03\u4e0e\u4eba\u53e3\u5bc6\u5ea6\u57fa\u672c\u4e00\u81f4\uff1b\u4e3b\u8981\u653f\u515a\u5f3a\u8c03\u515a\u6d3e\u540d\u79f0\u548c\u5bf9\u624b\uff0c\u800c\u5c0f\u515a\u5219\u5f3a\u8c03\u5177\u4f53\u8bae\u9898\u4fe1\u606f\u3002", "conclusion": "\u5728\u5f3a\u5236\u6295\u7968\u80cc\u666f\u4e0b\uff0c\u4e3b\u8981\u653f\u515a\u901a\u8fc7\u5f3a\u5316\u515a\u6d3e\u8ba4\u540c\u6765\u9632\u6b62\u9009\u6c11\u6d41\u5931\uff0c\u800c\u5c0f\u515a\u5219\u901a\u8fc7\u57f9\u80b2\u8bae\u9898\u8ba4\u540c\u6765\u5438\u5f15\u4e0d\u6ee1\u9009\u6c11\u7684\u652f\u6301\u3002"}}
{"id": "2511.13522", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.13522", "abs": "https://arxiv.org/abs/2511.13522", "authors": ["Erik van den Eshof", "Jorn van Kampen", "Mauro Salazar"], "title": "A Computationally Efficient Framework for Free-trajectory Minimum-lap-time Optimization of Racing Cars", "comment": "8 pages, 11 figures, submitted to the European Control Conference 2026", "summary": "This paper presents a modeling and optimization framework to compute the minimum-lap-time spatial trajectory and powertrain operation of racing cars in a computationally efficient fashion. Specifically, we first derive a quasi-steady-state model of a racing car, whereby the racing line trajectory is jointly optimized. Next, we frame the minimum-lap-time problem and leverage its mostly convex structure by devising a sequential convex programming solution algorithm. We benchmark our method against off-the-shelf nonlinear programming solvers, showing how it can bring computation time down from a few minutes to a few seconds, paving the way for real-time implementations. Moreover, we compare our results to similarly efficient minimum-curvature racing line optimization methods, showing how a minimum-time-based racing line might lead to 4% faster lap-times. Finally, we showcase our framework for optimal powertrain energy management and we validate the common modeling assumption that the racing line is unaffected by energy limitations, showing that this assumption results in marginal lap-time losses of under 0.1%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u8d5b\u8f66\u6700\u5c0f\u5708\u65f6\u7a7a\u95f4\u8f68\u8ff9\u548c\u52a8\u529b\u7cfb\u7edf\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u51f8\u89c4\u5212\u7b97\u6cd5\u5c06\u8ba1\u7b97\u65f6\u95f4\u4ece\u51e0\u5206\u949f\u51cf\u5c11\u5230\u51e0\u79d2\uff0c\u5e76\u8bc1\u660e\u57fa\u4e8e\u6700\u5c0f\u65f6\u95f4\u7684\u8d5b\u8f66\u7ebf\u6bd4\u6700\u5c0f\u66f2\u7387\u65b9\u6cd5\u5feb4%\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u6846\u67b6\u6765\u4f18\u5316\u8d5b\u8f66\u7684\u6700\u5c0f\u5708\u65f6\u8f68\u8ff9\u548c\u52a8\u529b\u7cfb\u7edf\u64cd\u4f5c\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8ba1\u7b97\u5e76\u9a8c\u8bc1\u73b0\u6709\u5efa\u6a21\u5047\u8bbe\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u8d5b\u8f66\u7684\u51c6\u7a33\u6001\u6a21\u578b\uff0c\u8054\u5408\u4f18\u5316\u8d5b\u8f66\u7ebf\u8f68\u8ff9\uff1b\u7136\u540e\u6784\u5efa\u6700\u5c0f\u5708\u65f6\u95ee\u9898\uff0c\u5229\u7528\u5176\u51f8\u7ed3\u6784\u8bbe\u8ba1\u987a\u5e8f\u51f8\u89c4\u5212\u6c42\u89e3\u7b97\u6cd5\u3002", "result": "\u4e0e\u73b0\u6210\u7684\u975e\u7ebf\u6027\u89c4\u5212\u6c42\u89e3\u5668\u76f8\u6bd4\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ece\u51e0\u5206\u949f\u51cf\u5c11\u5230\u51e0\u79d2\uff1b\u4e0e\u6700\u5c0f\u66f2\u7387\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6700\u5c0f\u65f6\u95f4\u8d5b\u8f66\u7ebf\u53ef\u5e26\u67654%\u7684\u5708\u65f6\u63d0\u5347\uff1b\u80fd\u91cf\u9650\u5236\u5bf9\u8d5b\u8f66\u7ebf\u5f71\u54cd\u5f88\u5c0f\uff08<0.1%\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6700\u5c0f\u5708\u65f6\u4f18\u5316\uff0c\u652f\u6301\u5b9e\u65f6\u5e94\u7528\uff0c\u9a8c\u8bc1\u4e86\u8d5b\u8f66\u7ebf\u4e0d\u53d7\u80fd\u91cf\u9650\u5236\u7684\u5efa\u6a21\u5047\u8bbe\u662f\u5408\u7406\u7684\u3002"}}
{"id": "2511.12236", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12236", "abs": "https://arxiv.org/abs/2511.12236", "authors": ["Raavi Gupta", "Pranav Hari Panicker", "Sumit Bhatia", "Ganesh Ramakrishnan"], "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts", "comment": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025", "summary": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.", "AI": {"tldr": "CONFACTCHECK\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u67e5\u751f\u6210\u6587\u672c\u4e2d\u4e8b\u5b9e\u63a2\u9488\u56de\u7b54\u7684\u4e00\u81f4\u6027\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5e93\u4e14\u8d44\u6e90\u6d88\u8017\u4f4e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6587\u672c\u65f6\u7ecf\u5e38\u4ea7\u751f\u4e8b\u5b9e\u9519\u8bef\u7684\u5e7b\u89c9\uff0c\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u9886\u57df\u5b58\u5728\u4e25\u91cd\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u578b\u8bbf\u95ee\u53d7\u9650\u65f6\u901a\u5e38\u9700\u8981\u591a\u6b21API\u8c03\u7528\uff0c\u589e\u52a0\u4e86\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "method": "\u57fa\u4e8e\u4e8b\u5b9e\u63a2\u9488\u4e00\u81f4\u6027\u68c0\u6d4b\uff1a\u5728\u5355\u4e2aLLM\u5185\u90e8\u548c\u4e0d\u540cLLM\u4e4b\u95f4\u68c0\u67e5\u5bf9\u4e8b\u5b9e\u63a2\u9488\u7684\u56de\u7b54\u662f\u5426\u4e00\u81f4\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCONFACTCHECK\u80fd\u4ee5\u66f4\u5c11\u8d44\u6e90\u9ad8\u6548\u68c0\u6d4b\u5e7b\u89c9\u4e8b\u5b9e\uff0c\u5728\u76f8\u4f3c\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "CONFACTCHECK\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6a21\u578b\u8bbf\u95ee\u53d7\u9650\u7684\u573a\u666f\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u751f\u6210\u5185\u5bb9\u3002"}}
{"id": "2511.11638", "categories": ["cs.LG", "math.NA", "nlin.PS"], "pdf": "https://arxiv.org/pdf/2511.11638", "abs": "https://arxiv.org/abs/2511.11638", "authors": ["Aamir Shehzad"], "title": "Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches", "comment": "32 pages, 19 figures This work investigates adaptive and conservative PINN frameworks for solving the RLW equation", "summary": "Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.", "AI": {"tldr": "\u672c\u7814\u7a76\u6539\u8fdb\u4e86PINN\u65b9\u6cd5\u6c42\u89e3RLW\u65b9\u7a0b\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u548c\u5b88\u6052\u4e24\u79cd\u65b9\u6cd5\u3002\u7ed3\u679c\u663e\u793aPINN\u6548\u679c\u5177\u6709\u95ee\u9898\u7279\u5f02\u6027\uff1a\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u590d\u6742\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u5b88\u6052\u65b9\u6cd5\u5728\u957f\u671f\u884c\u4e3a\u95ee\u9898\u4e2d\u66f4\u4f18\u3002\u7814\u7a76\u6311\u6218\u4e86\u5b88\u6052\u7ea6\u675f\u603b\u80fd\u63d0\u5347PINN\u6027\u80fd\u7684\u5047\u8bbe\u3002", "motivation": "\u6807\u51c6PINN\u5728\u6c42\u89e3\u6b63\u5219\u5316\u957f\u6ce2\u65b9\u7a0b\u65f6\u4ea7\u751f\u8f83\u5927\u8bef\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u6539\u8fdb\u7684PINN\u65b9\u6cd5\u4ee5\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u89e3\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u6539\u8fdb\u7684PINN\u65b9\u6cd5\uff1a\u5177\u6709\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u548c\u5f3a\u5236\u6267\u884c\u663e\u5f0f\u5b88\u6052\u5b9a\u5f8b\u7684\u5b88\u6052\u65b9\u6cd5\u3002\u4f7f\u7528\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff1a\u5355\u5b64\u5b50\u4f20\u64ad\u3001\u53cc\u5b64\u5b50\u76f8\u4e92\u4f5c\u7528\u548c\u6d8c\u6f6e\u6f14\u5316\u3002", "result": "\u81ea\u9002\u5e94PINN\u5728\u590d\u6742\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\uff08\u5982\u5b64\u5b50\u78b0\u649e\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u5b88\u6052PINN\u548c\u6807\u51c6PINN\uff1b\u5b88\u6052\u65b9\u6cd5\u5728\u5355\u5b64\u5b50\u957f\u671f\u884c\u4e3a\u548c\u6d8c\u6f6e\u95ee\u9898\u4e2d\u8868\u73b0\u66f4\u597d\u3002\u4e24\u79cd\u65b9\u6cd5\u7684\u7ed3\u679c\u4e0e\u6570\u503c\u89e3\u8bef\u5dee\u5728O(10^-5)\u91cf\u7ea7\u3002", "conclusion": "\u663e\u5f0f\u5f3a\u5236\u6267\u884c\u5b88\u6052\u5b9a\u5f8b\u53ef\u80fd\u5bf9\u9ad8\u5ea6\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4f18\u5316\u6c42\u89e3\u6709\u5bb3\uff0c\u9700\u8981\u7279\u6b8a\u8bad\u7ec3\u65b9\u6cd5\u3002PINN\u53ef\u4ee5\u4e3a\u590d\u6742\u504f\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u63d0\u4f9b\u65e0\u7f51\u683c\u7684\u51c6\u786e\u89e3\uff0c\u4f46\u6548\u679c\u5177\u6709\u95ee\u9898\u7279\u5f02\u6027\u3002"}}
{"id": "2511.12089", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12089", "abs": "https://arxiv.org/abs/2511.12089", "authors": ["Yanchang Fu", "Qiyue Yin", "Shengda Liu", "Pei Xu", "Kaiqi Huang"], "title": "KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything", "comment": null, "summary": "Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86KrwEmd\u7b97\u6cd5\uff0c\u901a\u8fc7k-recall\u8d62\u7387\u7279\u5f81\u548cEarth Mover's\u8ddd\u79bb\u6765\u805a\u7c7b\u4fe1\u53f7\u89c2\u5bdf\u4fe1\u606f\u96c6\uff0c\u89e3\u51b3\u4e86\u5fb7\u5dde\u6251\u514b\u7b49\u6e38\u620f\u4e2d\u8fc7\u5ea6\u62bd\u8c61\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\u4e2d\u8fc7\u5ea6\u62bd\u8c61\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u6781\u7aef\u4e0d\u5b8c\u7f8e\u56de\u5fc6\u62bd\u8c61\u5b8c\u5168\u4e22\u5f03\u5386\u53f2\u4fe1\u606f\u5bfc\u81f4\u7684AI\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165k-recall\u8d62\u7387\u7279\u5f81\u6765\u5b9a\u6027\u548c\u5b9a\u91cf\u533a\u5206\u4fe1\u53f7\u89c2\u5bdf\u4fe1\u606f\u96c6\uff0c\u7136\u540e\u5f00\u53d1KrwEmd\u7b97\u6cd5\u4f7f\u7528Earth Mover's\u8ddd\u79bb\u805a\u7c7b\u8fd9\u4e9b\u4fe1\u606f\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKrwEmd\u76f8\u6bd4\u73b0\u6709\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86AI\u7684\u6e38\u620f\u8868\u73b0\u3002", "conclusion": "KrwEmd\u662f\u7b2c\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u8fc7\u5ea6\u62bd\u8c61\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u5386\u53f2\u548c\u672a\u6765\u6e38\u620f\u4fe1\u606f\u6765\u6539\u5584AI\u6027\u80fd\u3002"}}
{"id": "2511.13122", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13122", "abs": "https://arxiv.org/abs/2511.13122", "authors": ["Harsh Abhinandan", "Aditya Dhanraj", "Aryan Katoch", "R. Raja Singh"], "title": "A Comprehensive Review of Advancements in Powering and Charging Systems for Unmanned Aerial Vehicles", "comment": "This paper has been accepted for presentation at the 10th International Conference on Information and Communication Technology for Competitive Strategies (ICTCS-2025) and will be published in the conference proceedings under the Springer Lecture Notes in Networks and Systems (LNNS) series, ISSN: 2367-3370", "summary": "Unmanned Aerial Vehicles (UAVs) or drones have witnessed a spectacular surge in applications for military, commercial, and civilian purposes. However, their potential for flight is always limited by the finite power budget of their onboard power supplies. The limited flight time problem has led to intensive research into new sources of power and innovative charging strategies to enable protracted, autonomous flight. This paper gives a comparative summary of the current state-of-the-art in UAV power and refuelling technology. The paper begins with an analysis of the variety of energy sources, from classical batteries to fuel cells and hybrid systems, based on their relative advantages and disadvantages in energy density, weight, and safety. Subsequently, the review explores a spectrum of replenishment options, from simple manual battery swapping to sophisticated high-tech automatic docking stations and smart contact-based charging pads. Most of the review is dedicated to the newer technology of wireless power transfer, which involves near-field (inductive, capacitive) and far-field (laser, microwave) technology. The article also delves into the most important power electronic converter topologies, battery management systems, and control approaches that form the core of these charging systems. Finally, it recapitulates the most significant challenges in technical, economic, and social aspects for promising avenues of future research. The comprehensive review is a valuable guide for researchers, engineers, and policymakers striving to enhance UAV operational performance.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u65e0\u4eba\u673a(UAV)\u7535\u6e90\u548c\u5145\u7535\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u5404\u79cd\u80fd\u6e90\u7cfb\u7edf\u3001\u5145\u7535\u65b9\u6cd5\u548c\u65e0\u7ebf\u7535\u529b\u4f20\u8f93\u6280\u672f\uff0c\u65e8\u5728\u89e3\u51b3\u65e0\u4eba\u673a\u6709\u9650\u7684\u98de\u884c\u65f6\u95f4\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u5230\u673a\u8f7d\u7535\u6e90\u6709\u9650\u529f\u7387\u9884\u7b97\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u98de\u884c\u65f6\u95f4\u4e0d\u8db3\u3002\u9700\u8981\u7814\u7a76\u65b0\u7684\u7535\u6e90\u548c\u5145\u7535\u7b56\u7565\u6765\u5b9e\u73b0\u957f\u65f6\u95f4\u7684\u81ea\u4e3b\u98de\u884c\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65e0\u4eba\u673a\u7535\u6e90\u548c\u5145\u7535\u6280\u672f\uff0c\u5305\u62ec\u4f20\u7edf\u7535\u6c60\u3001\u71c3\u6599\u7535\u6c60\u3001\u6df7\u5408\u7cfb\u7edf\u7b49\u80fd\u6e90\u6765\u6e90\uff0c\u4ee5\u53ca\u4ece\u7b80\u5355\u7535\u6c60\u66f4\u6362\u5230\u81ea\u52a8\u5bf9\u63a5\u7ad9\u3001\u65e0\u7ebf\u7535\u529b\u4f20\u8f93\u7b49\u591a\u79cd\u5145\u7535\u65b9\u6848\u3002", "result": "\u7efc\u8ff0\u4e86\u5404\u79cd\u80fd\u6e90\u7cfb\u7edf\u7684\u4f18\u7f3a\u70b9\uff08\u80fd\u91cf\u5bc6\u5ea6\u3001\u91cd\u91cf\u3001\u5b89\u5168\u6027\uff09\uff0c\u63a2\u8ba8\u4e86\u65e0\u7ebf\u7535\u529b\u4f20\u8f93\u6280\u672f\uff08\u8fd1\u573a\u548c\u8fdc\u573a\uff09\uff0c\u5e76\u5206\u6790\u4e86\u529f\u7387\u7535\u5b50\u8f6c\u6362\u5668\u62d3\u6251\u3001\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u548c\u63a7\u5236\u65b9\u6cd5\u3002", "conclusion": "\u8bc6\u522b\u4e86\u6280\u672f\u3001\u7ecf\u6d4e\u548c\u793e\u4f1a\u65b9\u9762\u7684\u4e3b\u8981\u6311\u6218\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u589e\u5f3a\u65e0\u4eba\u673a\u64cd\u4f5c\u6027\u80fd\u7684\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2511.12686", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12686", "abs": "https://arxiv.org/abs/2511.12686", "authors": ["Stefano Bianchini", "Aldo Geuna", "Fazliddin Shermatov"], "title": "AI and Supercomputing are Powering the Next Wave of Breakthrough Science - But at What Cost?", "comment": null, "summary": "Artificial intelligence (AI) and high-performance computing (HPC) are rapidly becoming the engines of modern science. However, their joint effect on discovery has yet to be quantified at scale. Drawing on metadata from over five million scientific publications (2000-2024), we identify how AI and HPC interact to shape research outcomes across 27 fields. Papers combining the two technologies are up to three times more likely to introduce novel concepts and five times more likely to reach top-cited status than conventional work. This convergence of AI and HPC is redefining the frontier of scientific creativity but also deepening global inequalities in access to computational power and expertise. Our findings suggest that the future of discovery will depend not only on algorithms and compute, but also on how equitably the world shares these transformative tools.", "AI": {"tldr": "\u57fa\u4e8e500\u4e07\u7bc7\u79d1\u5b66\u8bba\u6587\u7684\u5206\u6790\u663e\u793a\uff0cAI\u4e0eHPC\u7ed3\u5408\u7684\u7814\u7a76\u6bd4\u4f20\u7edf\u7814\u7a76\u66f4\u53ef\u80fd\u4ea7\u751f\u65b0\u6982\u5ff5\u548c\u83b7\u5f97\u9ad8\u5f15\u7528\uff0c\u4f46\u4e5f\u52a0\u5267\u4e86\u5168\u7403\u8ba1\u7b97\u8d44\u6e90\u4e0d\u5e73\u7b49\u95ee\u9898\u3002", "motivation": "\u91cf\u5316AI\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u5bf9\u79d1\u5b66\u53d1\u73b0\u7684\u8054\u5408\u5f71\u54cd\uff0c\u4e86\u89e3\u8fd9\u4e24\u79cd\u6280\u672f\u5982\u4f55\u5171\u540c\u5851\u9020\u7814\u7a76\u4ea7\u51fa\u3002", "method": "\u5206\u67902000-2024\u5e74\u95f4500\u591a\u4e07\u7bc7\u79d1\u5b66\u8bba\u6587\u7684\u5143\u6570\u636e\uff0c\u7814\u7a76AI\u548cHPC\u572827\u4e2a\u9886\u57df\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u7ed3\u5408AI\u548cHPC\u7684\u8bba\u6587\u5f15\u5165\u65b0\u6982\u5ff5\u7684\u53ef\u80fd\u6027\u662f\u4f20\u7edf\u7814\u7a76\u76843\u500d\uff0c\u8fdb\u5165\u9ad8\u88ab\u5f15\u8bba\u6587\u7684\u53ef\u80fd\u6027\u662f5\u500d\u3002", "conclusion": "AI\u4e0eHPC\u7684\u878d\u5408\u6b63\u5728\u91cd\u65b0\u5b9a\u4e49\u79d1\u5b66\u521b\u9020\u529b\u524d\u6cbf\uff0c\u4f46\u52a0\u6df1\u4e86\u5168\u7403\u8ba1\u7b97\u8d44\u6e90\u548c\u4e13\u4e1a\u77e5\u8bc6\u83b7\u53d6\u7684\u4e0d\u5e73\u7b49\uff0c\u672a\u6765\u53d1\u73b0\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u7b97\u6cd5\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u8fd8\u53d6\u51b3\u4e8e\u8fd9\u4e9b\u53d8\u9769\u6027\u5de5\u5177\u7684\u516c\u5e73\u5206\u914d\u3002"}}
{"id": "2511.12249", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12249", "abs": "https://arxiv.org/abs/2511.12249", "authors": ["Khang T. Huynh", "Dung H. Nguyen", "Binh T. Nguyen"], "title": "ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations", "comment": null, "summary": "Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT", "AI": {"tldr": "\u63d0\u51fa\u4e86ViConBERT\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u57fa\u4e8e\u8bcd\u4e49\u89e3\u91ca\u7684\u84b8\u998f\u6765\u5b66\u4e60\u8d8a\u5357\u8bed\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u5e76\u521b\u5efa\u4e86ViConWSD\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u8d8a\u5357\u8bed\u8bed\u4e49\u7406\u89e3\u3002", "motivation": "\u8d8a\u5357\u8bed\u7f3a\u4e4f\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u6a21\u578b\u548c\u8bc4\u4f30\u8d44\u6e90\uff0c\u800c\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u8bcd\u5d4c\u5165\u8fdb\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u5982\u82f1\u8bed\u4e0a\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60(SimCLR)\u548c\u57fa\u4e8e\u8bcd\u4e49\u89e3\u91ca\u7684\u84b8\u998f\u6765\u5b66\u4e60\u8d8a\u5357\u8bed\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6ViConWSD\u3002", "result": "ViConBERT\u5728WSD\u4efb\u52a1\u4e0aF1\u5f97\u5206\u4e3a0.87\uff0c\u5728ViCon\u6570\u636e\u96c6\u4e0aAP\u4e3a0.88\uff0c\u5728ViSim-400\u6570\u636e\u96c6\u4e0aSpearman's rho\u4e3a0.60\uff0c\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ViConBERT\u6846\u67b6\u5728\u5efa\u6a21\u79bb\u6563\u8bcd\u4e49\u548c\u5206\u7ea7\u8bed\u4e49\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u6709\u6548\uff0c\u4e3a\u8d8a\u5357\u8bed\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11641", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.11641", "abs": "https://arxiv.org/abs/2511.11641", "authors": ["Jinqi Xiao", "Cheng Luo", "Lingyi Huang", "Cheng Yang", "Yang Sui", "Huy Phan", "Xiao Zang", "Yibiao Ying", "Zhexiang Tang", "Anima Anandkumar", "Bo Yuan"], "title": "EcoSpa: Efficient Transformer Training with Coupled Sparsity", "comment": null, "summary": "Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\\% memory reduction and 21\\% faster training, achieves $2.2\\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.", "AI": {"tldr": "EcoSpa\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u7a00\u758f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8bc4\u4f30\u548c\u7a00\u758f\u5316\u8026\u5408\u6743\u91cd\u77e9\u9635\u5bf9\uff0c\u4fdd\u6301\u5176\u4ea4\u4e92\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7a00\u758f\u8bad\u7ec3\u65b9\u6cd5\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "Transformer\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u5e26\u6765\u7cfb\u7edf\u6311\u6218\uff0c\u73b0\u6709\u7a00\u758f\u8bad\u7ec3\u65b9\u6cd5\u672a\u80fd\u4fdd\u6301\u6743\u91cd\u77e9\u9635\u4e4b\u95f4\u7684\u5173\u952e\u7ed3\u6784\u5173\u7cfb\uff0c\u5bfc\u81f4\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "EcoSpa\u901a\u8fc7\u8054\u5408\u8bc4\u4f30\u548c\u7a00\u758f\u5316\u8026\u5408\u6743\u91cd\u77e9\u9635\u5bf9\uff0c\u91c7\u7528\u5bf9\u9f50\u7684\u884c/\u5217\u79fb\u9664\u6765\u4fdd\u6301\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5f15\u5165\u65b0\u7684\u7c92\u5ea6\u6765\u6821\u51c6\u7ed3\u6784\u7ec4\u4ef6\u91cd\u8981\u6027\uff0c\u5e76\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u573a\u666f\u4e2d\u8fdb\u884c\u8026\u5408\u4f30\u8ba1\u548c\u7a00\u758f\u5316\u3002", "result": "EcoSpa\u5728LLaMA-1B\u4e0a\u5b9e\u73b050%\u5185\u5b58\u51cf\u5c11\u548c21%\u8bad\u7ec3\u52a0\u901f\uff0c\u5728GPT-2-Medium\u4e0a\u5b9e\u73b02.2\u500d\u6a21\u578b\u538b\u7f29\u548c2.4\u500d\u56f0\u60d1\u5ea6\u964d\u4f4e\uff0c\u63d0\u4f9b1.6\u500d\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "EcoSpa\u4f7f\u7528\u6807\u51c6PyTorch\u64cd\u4f5c\uff0c\u65e0\u9700\u5b9a\u5236\u786c\u4ef6\u6216\u5185\u6838\uff0c\u4f7f\u9ad8\u6548Transformer\u8bad\u7ec3\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2511.12113", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12113", "abs": "https://arxiv.org/abs/2511.12113", "authors": ["Lanxue Zhang", "Yuqiang Xie", "Fang Fang", "Fanglong Dong", "Rui Liu", "Yanan Cao"], "title": "MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization", "comment": "23 pages, 10 figures, AAAI 2026", "summary": "Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u6765\u7f13\u89e3\u5c0f\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5305\u62ec\u6784\u5efa\u5305\u542b\u5143\u8ba4\u77e5\u77e5\u8bc6\u7684\u6570\u636e\u96c6\u548c\u5f00\u53d1GDPO\u8bad\u7ec3\u65b9\u6cd5", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u5fae\u8c03\u65b9\u6cd5\u5728\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u538b\u7f29\u5230\u5c0f\u6a21\u578b\u65f6\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5c0f\u4e8e8B\u7684\u6a21\u578b\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u6570\u636e\u4e0e\u6a21\u578b\u5185\u5728\u80fd\u529b\u5173\u7cfb\u4e0d\u660e\u786e\u3001\u8bad\u7ec3\u76ee\u6807\u65e0\u6cd5\u6709\u6548\u7ea6\u675f\u77e5\u8bc6\u4fdd\u7559", "method": "1) \u6784\u5efa\u5305\u542b5K\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d6\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u5e76\u878d\u5165\u5143\u8ba4\u77e5\u77e5\u8bc6\uff1b2) \u63d0\u51faGDPO\uff08\u7ec4\u65b9\u5411\u504f\u597d\u4f18\u5316\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u6a21\u578b\u9690\u5f0f\u7ea6\u675f\u4f18\u5316\u8def\u5f84\uff0c\u66f4\u9002\u5408\u8d44\u6e90\u53d7\u9650\u573a\u666f", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6570\u636e\u5c42\u9762\u7684\u5143\u8ba4\u77e5\u77e5\u8bc6\u6574\u5408\u548c\u8bad\u7ec3\u5c42\u9762\u7684GDPO\u4f18\u5316\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5c0f\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898"}}
{"id": "2511.12881", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12881", "abs": "https://arxiv.org/abs/2511.12881", "authors": ["Cheongjae Jang", "Jonghyun Won", "Soyeon Jun", "Chun Kee Chung", "Keehyoung Joo", "Yung-Kyun Noh"], "title": "On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples", "comment": "Extended version of paper accepted to AAAI 2026. 18 pages, 12 figures", "summary": "Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6709\u9650\u6837\u672c\u4e0b\u4e00\u7ef4Wasserstein\u8ddd\u79bb\u7684\u4fe1\u606f\u5904\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6355\u6349\u70b9\u6001\u5bc6\u5ea6\u5dee\u5f02\uff0c\u5e76\u5c06\u8fd9\u79cd\u4fe1\u606f\u4e0e\u652f\u6491\u96c6\u5dee\u5f02\u76f8\u534f\u8c03\u3002", "motivation": "\u5f53\u4e24\u4e2a\u5bc6\u5ea6\u51fd\u6570\u7684\u652f\u6491\u96c6\u663e\u8457\u91cd\u53e0\u4f46\u5bc6\u5ea6\u5728\u70b9\u6001\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u65f6\uff0c\u5c1a\u4e0d\u6e05\u695aWasserstein\u8ddd\u79bb\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u51c6\u786e\u8bc6\u522b\u8fd9\u4e9b\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u6837\u672c\u8bbe\u7f6e\u4e2d\u3002", "method": "\u5229\u7528\u6cca\u677e\u8fc7\u7a0b\u5e76\u5206\u79bb\u901f\u7387\u56e0\u5b50\uff0c\u5206\u6790\u4e00\u7ef4Wasserstein\u8ddd\u79bb\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u4fe1\u606f\u5904\u7406\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u8868\u660e\u4e00\u7ef4Wasserstein\u8ddd\u79bb\u80fd\u591f\u7a81\u51fa\u4e0e\u901f\u7387\u548c\u652f\u6491\u96c6\u76f8\u5173\u7684\u6709\u610f\u4e49\u7684\u5bc6\u5ea6\u5dee\u5f02\uff0c\u8fd9\u4e00\u7279\u6027\u5728\u795e\u7ecf\u8109\u51b2\u5e8f\u5217\u89e3\u7801\u548c\u6c28\u57fa\u9178\u63a5\u89e6\u9891\u7387\u6570\u636e\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u4e00\u7ef4Wasserstein\u8ddd\u79bb\u80fd\u591f\u6709\u6548\u6355\u6349\u70b9\u6001\u5bc6\u5ea6\u5dee\u5f02\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4fe1\u606f\u4e0e\u652f\u6491\u96c6\u5dee\u5f02\u76f8\u534f\u8c03\uff0c\u4e3a\u6709\u9650\u6837\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2511.13162", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13162", "abs": "https://arxiv.org/abs/2511.13162", "authors": ["Yifan Wang", "Yiyao Yu", "Yang Xia", "Yan Xu"], "title": "Cyber-Resilient Fault Diagnosis Methodology in Inverter-Based Resource-Dominated Microgrids with Single-Point Measurement", "comment": "5 pages, 5 figures", "summary": "Cyber-attacks jeopardize the safe operation of inverter-based resource-dominated microgrids (IBR-dominated microgrids). At the same time, existing diagnostic methods either depend on expensive multi-point instrumentation or stringent modeling assumptions that are untenable under single-point measurement constraints. This paper proposes a Fractional-Order Memory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves timely fault localization and cyber-resilient fault diagnosis using only one VPQ (voltage, active power, reactive power) measurement point. FO-MADS first constructs a dual fractional-order feature library by jointly applying Caputo and Gr\u00fcnwald-Letnikov derivatives, thereby amplifying micro-perturbations and slow drifts in the VPQ signal. A two-stage hierarchical classifier then pinpoints the affected inverter and isolates the faulty IGBT switch, effectively alleviating class imbalance. Robustness is further strengthened through Progressive Memory-Replay Adversarial Training (PMR-AT), whose attack-aware loss is dynamically re-weighted via Online Hard Example Mining (OHEM) to prioritize the most challenging samples. Experiments on a four-inverter IBR-dominated microgrid testbed comprising 1 normal and 24 fault classes under four attack scenarios demonstrate diagnostic accuracies of 96.6% (bias), 94.0% (noise), 92.8% (data replacement), and 95.7% (replay), while sustaining 96.7% under attack-free conditions. These results establish FO-MADS as a cost-effective and readily deployable solution that markedly enhances the cyber-physical resilience of IBR-dominated microgrids.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u70b9\u6d4b\u91cf\u7684\u5206\u6570\u9636\u8bb0\u5fc6\u589e\u5f3a\u653b\u51fb\u8bca\u65ad\u65b9\u6848(FO-MADS)\uff0c\u7528\u4e8e\u9006\u53d8\u5668\u4e3b\u5bfc\u5fae\u7535\u7f51\u7684\u7f51\u7edc\u5b89\u5168\u6545\u969c\u8bca\u65ad\uff0c\u5728\u56db\u79cd\u653b\u51fb\u573a\u666f\u4e0b\u8fbe\u523092.8%-96.6%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u8bca\u65ad\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u591a\u70b9\u4eea\u5668\u6216\u4e25\u683c\u7684\u5efa\u6a21\u5047\u8bbe\uff0c\u65e0\u6cd5\u5728\u5355\u70b9\u6d4b\u91cf\u7ea6\u675f\u4e0b\u6709\u6548\u5de5\u4f5c\uff0c\u800c\u7f51\u7edc\u653b\u51fb\u4e25\u91cd\u5a01\u80c1\u9006\u53d8\u5668\u4e3b\u5bfc\u5fae\u7535\u7f51\u7684\u5b89\u5168\u8fd0\u884c\u3002", "method": "\u6784\u5efa\u53cc\u5206\u6570\u9636\u7279\u5f81\u5e93\uff0c\u7ed3\u5408Caputo\u548cGr\u00fcnwald-Letnikov\u5bfc\u6570\u653e\u5927\u4fe1\u53f7\u5fae\u6270\uff1b\u4f7f\u7528\u4e24\u9636\u6bb5\u5206\u5c42\u5206\u7c7b\u5668\u5b9a\u4f4d\u6545\u969c\u9006\u53d8\u5668\u548cIGBT\u5f00\u5173\uff1b\u901a\u8fc7\u6e10\u8fdb\u8bb0\u5fc6\u56de\u653e\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u9006\u53d8\u5668\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u9488\u5bf91\u4e2a\u6b63\u5e38\u7c7b\u548c24\u4e2a\u6545\u969c\u7c7b\uff0c\u5728\u56db\u79cd\u653b\u51fb\u573a\u666f\u4e0b\u7684\u8bca\u65ad\u51c6\u786e\u7387\u5206\u522b\u4e3a\uff1a\u504f\u7f6e\u653b\u51fb96.6%\u3001\u566a\u58f0\u653b\u51fb94.0%\u3001\u6570\u636e\u66ff\u6362\u653b\u51fb92.8%\u3001\u91cd\u653e\u653b\u51fb95.7%\uff0c\u65e0\u653b\u51fb\u6761\u4ef6\u4e0b\u4fdd\u630196.7%\u3002", "conclusion": "FO-MADS\u662f\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u9006\u53d8\u5668\u4e3b\u5bfc\u5fae\u7535\u7f51\u7684\u7f51\u7edc\u7269\u7406\u5f39\u6027\u3002"}}
{"id": "2511.12822", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12822", "abs": "https://arxiv.org/abs/2511.12822", "authors": ["Euzeli C. dos Santos", "Tracey Birdwell"], "title": "The Unspoken Crisis of Learning: The Surging Zone of No Development", "comment": "6 pages, 3 figures", "summary": "AI has redefined the boundaries of assistance in education, often blurring the line between guided learning and dependency. This paper revisits Vygotsky's Zone of Proximal Development (ZPD) through the lens of the P2P Teaching framework. By contrasting temporary scaffolding with the emerging phenomenon of permanent digital mediation, the study introduces the concept of the Zone of No Development (ZND), a state in which continuous assistance replaces cognitive struggle and impedes intellectual autonomy. Through theoretical synthesis and framework design, P2P Teaching demonstrates how deliberate disconnection and ethical fading can restore the learner's agency, ensuring that technological tools enhance rather than replace developmental effort. The paper argues that productive struggle, self-regulation, and first-principles reasoning remain essential for durable learning, and that responsible use of AI in education must include explicit mechanisms to end its help when mastery begins.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7P2P\u6559\u5b66\u6846\u67b6\u91cd\u65b0\u5ba1\u89c6\u7ef4\u679c\u8328\u57fa\u7684\u6700\u8fd1\u53d1\u5c55\u533a\u7406\u8bba\uff0c\u63d0\u51fa\"\u65e0\u53d1\u5c55\u533a\"\u6982\u5ff5\uff0c\u5f3a\u8c03AI\u6559\u80b2\u4e2d\u9700\u8981\u5efa\u7acb\u660e\u786e\u7684\u9000\u51fa\u673a\u5236\u6765\u4fdd\u62a4\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u81ea\u4e3b\u6027\u3002", "motivation": "AI\u5728\u6559\u80b2\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6a21\u7cca\u4e86\u5f15\u5bfc\u5b66\u4e60\u4e0e\u4f9d\u8d56\u4e4b\u95f4\u7684\u754c\u9650\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u786e\u4fdd\u6280\u672f\u5de5\u5177\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u53d1\u5c55\u52aa\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u7efc\u5408\u548c\u6846\u67b6\u8bbe\u8ba1\uff0c\u91c7\u7528P2P\u6559\u5b66\u6846\u67b6\uff0c\u5bf9\u6bd4\u4e34\u65f6\u811a\u624b\u67b6\u4e0e\u6c38\u4e45\u6570\u5b57\u4e2d\u4ecb\u73b0\u8c61\u3002", "result": "\u63d0\u51fa\u4e86\"\u65e0\u53d1\u5c55\u533a\"\u6982\u5ff5\uff0c\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u523b\u610f\u65ad\u5f00\u548c\u4f26\u7406\u6d88\u9000\u6765\u6062\u590d\u5b66\u4e60\u8005\u7684\u80fd\u52a8\u6027\u3002", "conclusion": "\u751f\u4ea7\u6027\u6323\u624e\u3001\u81ea\u6211\u8c03\u8282\u548c\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u7406\u5bf9\u6301\u4e45\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0cAI\u5728\u6559\u80b2\u4e2d\u7684\u8d1f\u8d23\u4efb\u4f7f\u7528\u5fc5\u987b\u5305\u542b\u660e\u786e\u7684\u9000\u51fa\u673a\u5236\u3002"}}
{"id": "2511.13584", "categories": ["math.OC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13584", "abs": "https://arxiv.org/abs/2511.13584", "authors": ["Souvik Das", "Luca Schenato", "Subhrakanti Dey"], "title": "HBNET-GIANT: A communication-efficient accelerated Newton-type fully distributed optimization algorithm", "comment": "8 pages, 2 figures", "summary": "This article presents a second-order fully distributed optimization algorithm, HBNET-GIANT, driven by heavy-ball momentum, for $L$-smooth and $\u03bc$-strongly convex objective functions. A rigorous convergence analysis is performed, and we demonstrate global linear convergence under certain sufficient conditions. Through extensive numerical experiments, we show that HBNET-GIANT with heavy-ball momentum achieves acceleration, and the corresponding rate of convergence is strictly faster than its non-accelerated version, NETWORK-GIANT. Moreover, we compare HBNET-GIANT with several state-of-the-art algorithms, both momentum-based and without momentum, and report significant performance improvement in convergence to the optimum. We believe that this work lays the groundwork for a broader class of second-order Newton-type algorithms with momentum and motivates further investigation into open problems, including an analytical proof of local acceleration in the fully distributed setting for convex optimization problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86HBNET-GIANT\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u91cd\u7403\u52a8\u91cf\u7684\u4e8c\u9636\u5168\u5206\u5e03\u5f0f\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8eL-\u5149\u6ed1\u548c\u03bc-\u5f3a\u51f8\u76ee\u6807\u51fd\u6570\u3002\u8be5\u7b97\u6cd5\u5728\u7279\u5b9a\u5145\u5206\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5168\u5c40\u7ebf\u6027\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7ed3\u5408\u91cd\u7403\u52a8\u91cf\u7684\u4e8c\u9636\u5206\u5e03\u5f0f\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u52a0\u901f\u6536\u655b\u5e76\u63d0\u9ad8\u6027\u80fd\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u725b\u987f\u578b\u52a8\u91cf\u7b97\u6cd5\u5960\u5b9a\u57fa\u7840\u3002", "method": "HBNET-GIANT\u7b97\u6cd5\uff0c\u7ed3\u5408\u91cd\u7403\u52a8\u91cf\u7684\u4e8c\u9636\u5168\u5206\u5e03\u5f0f\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eL-\u5149\u6ed1\u548c\u03bc-\u5f3a\u51f8\u51fd\u6570\u3002", "result": "\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5168\u5c40\u7ebf\u6027\u6536\u655b\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u5176\u6536\u655b\u901f\u5ea6\u660e\u663e\u5feb\u4e8e\u65e0\u52a8\u91cf\u7248\u672cNETWORK-GIANT\uff0c\u4e14\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u4e8c\u9636\u725b\u987f\u578b\u52a8\u91cf\u7b97\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6fc0\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\u5168\u5206\u5e03\u5f0f\u51f8\u4f18\u5316\u4e2d\u7684\u5c40\u90e8\u52a0\u901f\u5206\u6790\u8bc1\u660e\u7b49\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2511.12281", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12281", "abs": "https://arxiv.org/abs/2511.12281", "authors": ["Ivan Zakazov", "Alexander Sharipov", "Berke Argin", "Oussama Gabouj", "Kamel Charaf", "Alexi Semiz", "Lorenzo Drudi", "Nicolas Baldwin", "Robert West"], "title": "Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor", "comment": null, "summary": "Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u5c0f\u578bLLM\u538b\u7f29\u5927\u578bLLM\u8f93\u5165\u7684\u63d0\u793a\u538b\u7f29\u8303\u5f0f\uff0c\u5f00\u53d1\u4e86Cmprsr\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4fe1\u606f\u548c\u9075\u5faa\u538b\u7f29\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u964d\u4f4e\u4f7f\u7528\u9ed1\u76d2\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6210\u672c\uff0c\u901a\u8fc7\u538b\u7f29\u8f93\u5165\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u4f7f\u752825\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5efa\u7acb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7Textgrad\u4f18\u5316\u538b\u7f29\u5143\u63d0\u793a\uff0c\u5bf9Qwen3-4B\u8fdb\u884cSFT\u548cGRPO\u540e\u8bad\u7ec3\u3002", "result": "Cmprsr\u5728MeetingBank\u3001LongBench\u548cGSM8k\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u63d0\u53d6\u5f0f\u548c\u666e\u901a\u62bd\u8c61\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u7cbe\u786e\u63a7\u5236\u538b\u7f29\u7387\u3002", "conclusion": "Cmprsr\u6a21\u578b\u5728\u538b\u7f29\u80fd\u529b\u548c\u538b\u7f29\u7387\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u8de8\u957f\u5ea6\u548c\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.11646", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11646", "abs": "https://arxiv.org/abs/2511.11646", "authors": ["Li Yinxing", "Tsukasa Ishigaki"], "title": "A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products", "comment": "23 pages", "summary": "Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u8868\u683c\u53d8\u5206\u81ea\u7f16\u7801\u5668(CTVAE)\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u65b0\u4ea7\u54c1\u7ebf\u6269\u5c55\u65f6\u7684\u6d88\u8d39\u8005\u5c5e\u6027\u53d8\u5316\uff0c\u5e2e\u52a9\u8425\u9500\u4eba\u5458\u5236\u5b9a\u6709\u6548\u7684\u4ea7\u54c1\u7ebf\u8425\u9500\u7b56\u7565\u3002", "motivation": "\u4ea7\u54c1\u7ebf\u6269\u5c55\u662f\u91cd\u8981\u7684\u8425\u9500\u7b56\u7565\uff0c\u4f46\u8fc7\u5ea6\u6269\u5c55\u4f1a\u7834\u574f\u54c1\u724c\u5f62\u8c61\u3002\u9700\u8981\u57fa\u4e8e\u6d88\u8d39\u8005\u9700\u6c42\u8fdb\u884c\u9002\u5f53\u7684\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u9884\u6d4b\u65b0\u4ea7\u54c1\u5bf9\u6d88\u8d39\u8005\u5c5e\u6027\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u8868\u683c\u53d8\u5206\u81ea\u7f16\u7801\u5668(CTVAE)\u4ece\u5927\u89c4\u6a21\u6d88\u8d39\u8005\u548c\u4ea7\u54c1\u8868\u683c\u6570\u636e\u4e2d\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u9884\u6d4b\u65b0\u4ea7\u54c1\u7ebf\u6269\u5c55\u65f6\u7684\u6d88\u8d39\u8005\u5c5e\u6027\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eCTVAE\u76f8\u6bd4\u73b0\u6709\u6a21\u578b\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u80fd\u591f\u4e3a\u6539\u53d8\u5bb9\u5668\u6216\u53e3\u5473\u7684\u65b0\u4ea7\u54c1\u63d0\u4f9b\u6709\u6548\u7684\u8425\u9500\u542f\u793a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u907f\u514d\u4ea7\u54c1\u81ea\u76f8\u6b8b\u6740\uff0c\u5e76\u4e3a\u4ea7\u54c1\u5f62\u8c61\u8bbe\u8ba1\u548c\u8425\u9500\u7b56\u7565\u5236\u5b9a\u63d0\u4f9b\u652f\u6301\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.12135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12135", "abs": "https://arxiv.org/abs/2511.12135", "authors": ["Letian Chen", "Runhan Shi", "Gufeng Yu", "Yang Yang"], "title": "RTMol: Rethinking Molecule-text Alignment in a Round-trip View", "comment": null, "summary": "Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.", "AI": {"tldr": "RTMol\u662f\u4e00\u4e2a\u53cc\u5411\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u7684\u5f80\u8fd4\u5b66\u4e60\u7edf\u4e00\u5206\u5b50\u6807\u6ce8\u548c\u6587\u672c\u5230SMILES\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5316\u5b66\u51c6\u786e\u6027\u3001\u6570\u636e\u8d28\u91cf\u548c\u53cc\u5411\u4e00\u81f4\u6027\u65b9\u9762\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5206\u5b50\u6807\u6ce8\u548c\u6587\u672c\u5230\u5206\u5b50\u8bbe\u8ba1\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4f20\u7edf\u6307\u6807\u504f\u91cd\u8bed\u8a00\u6d41\u7545\u6027\u800c\u975e\u5316\u5b66\u51c6\u786e\u6027\u3001\u8bad\u7ec3\u6570\u636e\u5305\u542b\u5316\u5b66\u6a21\u7cca\u63cf\u8ff0\u3001\u72ec\u7acb\u4f18\u5316\u5bfc\u81f4\u53cc\u5411\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faRTMol\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5f80\u8fd4\u5b66\u4e60\u7edf\u4e00\u5206\u5b50\u6807\u6ce8\u548c\u6587\u672c\u5230SMILES\u751f\u6210\uff0c\u5f15\u5165\u65b0\u9896\u7684\u5f80\u8fd4\u8bc4\u4f30\u6307\u6807\uff0c\u652f\u6301\u65e0\u9700\u914d\u5bf9\u5206\u5b50-\u6587\u672c\u8bed\u6599\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRTMol\u5728\u5404\u79cdLLM\u4e0a\u5c06\u53cc\u5411\u5bf9\u9f50\u6027\u80fd\u63d0\u5347\u4e86\u9ad8\u8fbe47%\uff0c\u4e3a\u8054\u5408\u5206\u5b50-\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u5efa\u7acb\u4e86\u6709\u6548\u8303\u5f0f\u3002", "conclusion": "RTMol\u901a\u8fc7\u53cc\u5411\u5bf9\u9f50\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5b50\u5e8f\u5217\u8868\u793a\u4e0e\u6587\u672c\u63cf\u8ff0\u5bf9\u9f50\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u836f\u7269\u53d1\u73b0\u3001\u6750\u6599\u8bbe\u8ba1\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.13049", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13049", "abs": "https://arxiv.org/abs/2511.13049", "authors": ["Antoine Ledent", "Mun Chong Soo", "Nong Minh Hieu"], "title": "Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information", "comment": "Accepted at AAAI 2026", "summary": "We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \\textit{share a common subspace}. We assume that a large amount $M$ of \\textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\\widetilde{O}\\left(\\sqrt{\\frac{nd}{M}}\\right)$ and $\\widetilde{O}\\left(\\sqrt{\\frac{dr}{N}}\\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u63a8\u8350\u7cfb\u7edf\u573a\u666f\u4e0b\u7684\u77e9\u9635\u8865\u5168\u95ee\u9898\uff0c\u5176\u4e2d\u771f\u5b9e\u8bc4\u5206\u77e9\u9635R\u548c\u672a\u77e5\u91c7\u6837\u5206\u5e03P\u90fd\u662f\u4f4e\u79e9\u77e9\u9635\u4e14\u5171\u4eab\u5171\u540c\u5b50\u7a7a\u95f4\u3002\u5229\u7528\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\uff08\u9690\u5f0f\u53cd\u9988\uff09\u548c\u5c11\u91cf\u6709\u6807\u7b7e\u6570\u636e\uff08\u663e\u5f0f\u53cd\u9988\uff09\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u4e24\u79cd\u53cd\u9988\u7684\u77e9\u9635\u8865\u5168\u65b9\u6cd5\u3002", "motivation": "\u53d7\u63a8\u8350\u7cfb\u7edf\u542f\u53d1\uff0c\u73b0\u5b9e\u573a\u666f\u4e2d\u901a\u5e38\u6709\u5927\u91cf\u9690\u5f0f\u53cd\u9988\uff08\u5982\u70b9\u51fb\u3001\u8d2d\u4e70\uff09\u548c\u5c11\u91cf\u663e\u5f0f\u53cd\u9988\uff08\u5982\u8bc4\u5206\uff09\u3002\u8bba\u6587\u65e8\u5728\u7814\u7a76\u5982\u4f55\u6709\u6548\u7ed3\u5408\u8fd9\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u6765\u6539\u8fdb\u77e9\u9635\u8865\u5168\u6027\u80fd\u3002", "method": "\u5229\u7528\u4f4e\u79e9\u5b50\u7a7a\u95f4\u6062\u590d\u7406\u8bba\u548c\u77e9\u9635\u8865\u5168\u6a21\u578b\u7684\u7ecf\u5178\u6cdb\u5316\u754c\u9650\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9690\u5f0f\u548c\u663e\u5f0f\u53cd\u9988\u7684\u77e9\u9635\u8865\u5168\u65b9\u6cd5\u3002\u901a\u8fc7\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u4f30\u8ba1\u91c7\u6837\u5206\u5e03P\uff0c\u901a\u8fc7\u5c11\u91cf\u6709\u6807\u7b7e\u6570\u636e\u4f30\u8ba1\u771f\u5b9e\u8bc4\u5206\u77e9\u9635R\u3002", "result": "\u7406\u8bba\u5206\u6790\u5f97\u5230\u8bef\u5dee\u754c\u9650\u4e3a\u4e24\u9879\u4e4b\u548c\uff1aO~(\u221a(nd/M))\u548cO~(\u221a(dr/N))\uff0c\u5176\u4e2dd\u662fP\u7684\u79e9\uff0cr\u662fR\u7684\u79e9\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08Douban\u3001MovieLens\uff09\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u663e\u5f0f\u8bc4\u5206\u7a00\u5c11\u65f6\u4f18\u4e8e\u4ec5\u4f7f\u7528\u663e\u5f0f\u53cd\u9988\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7814\u7a76\u63a8\u8350\u7cfb\u7edf\u4e2d\u663e\u5f0f\u548c\u9690\u5f0f\u53cd\u9988\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u4e24\u79cd\u53cd\u9988\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u77e9\u9635\u8865\u5168\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u663e\u5f0f\u53cd\u9988\u7a00\u7f3a\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2511.13206", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13206", "abs": "https://arxiv.org/abs/2511.13206", "authors": ["Yihuai Zhang", "Huan Yu"], "title": "Event-Triggered Regulation of Mixed-Autonomy Traffic Under Varying Traffic Conditions", "comment": "15 pages, Accepted by IEEE TITS", "summary": "Modeling and congestion mitigation of mixed-autonomy traffic systems consisting of human-driven vehicles (HVs) and autonomous vehicles (AVs) have become increasingly critical with the rapid development of autonomous driving technology. This paper develops an event-triggered control (ETC) framework for mitigating congestion in such systems, which are modeled using an extended Aw-Rascle-Zhang (ARZ) formulation consisting of coupled 4 x 4 hyperbolic partial differential equations (PDEs). Ramp metering is employed as the boundary actuation mechanism. To reduce computational and communication burdens while avoiding excessive ramp signal changes, we design the ETC strategy based on the backstepping method, together with an observer-based ETC formulation for practical implementation under limited sensing. Rigorous Lyapunov analysis ensures exponential convergence and avoidance of Zeno behavior. Extensive simulations validate the proposed approach under diverse traffic scenarios, including varying AV penetration rates, different spacing policies, multiple demand levels, and non-recurrent congestion patterns. Results show that ETC not only stabilizes mixed traffic flows but also significantly reduces control updates, improving driver comfort, and roadway safety. Higher AV penetration rates lead to longer release time and fewer triggering events, indicating the positive impact of AVs in mitigating traffic congestion while reducing computational resource usage. Compared to continuous backstepping controllers, the proposed ETC achieves near-equivalent stabilization performance with far fewer controller updates, resulting in longer signal release time that reduces driver distraction, which demonstrates great potential for ETC applications in traffic management.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u4e8b\u4ef6\u89e6\u53d1\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u7f13\u89e3\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u7cfb\u7edf\u7684\u62e5\u5835\uff0c\u901a\u8fc7\u6269\u5c55\u7684ARZ\u6a21\u578b\u548c\u5761\u9053\u8ba1\u91cf\u63a7\u5236\uff0c\u663e\u8457\u51cf\u5c11\u63a7\u5236\u66f4\u65b0\u6b21\u6570\uff0c\u63d0\u9ad8\u9a7e\u9a76\u5458\u8212\u9002\u5ea6\u548c\u9053\u8def\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5efa\u6a21\u548c\u7f13\u89e3\u7531\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7ec4\u6210\u7684\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u7cfb\u7edf\u7684\u62e5\u5835\u95ee\u9898\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u53cd\u6b65\u6cd5\u8bbe\u8ba1\u4e86\u4e8b\u4ef6\u89e6\u53d1\u63a7\u5236\u7b56\u7565\uff0c\u7ed3\u5408\u57fa\u4e8e\u89c2\u6d4b\u5668\u7684ETC\u516c\u5f0f\uff0c\u5728\u6709\u9650\u4f20\u611f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u3002\u91c7\u7528\u6269\u5c55\u7684Aw-Rascle-Zhang\u6a21\u578b\u548c\u5761\u9053\u8ba1\u91cf\u4f5c\u4e3a\u8fb9\u754c\u6267\u884c\u673a\u5236\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0cETC\u4e0d\u4ec5\u7a33\u5b9a\u4e86\u6df7\u5408\u4ea4\u901a\u6d41\uff0c\u8fd8\u663e\u8457\u51cf\u5c11\u4e86\u63a7\u5236\u66f4\u65b0\uff0c\u63d0\u9ad8\u4e86\u9a7e\u9a76\u5458\u8212\u9002\u5ea6\u548c\u9053\u8def\u5b89\u5168\u6027\u3002\u66f4\u9ad8\u7684AV\u6e17\u900f\u7387\u5bfc\u81f4\u66f4\u957f\u7684\u91ca\u653e\u65f6\u95f4\u548c\u66f4\u5c11\u7684\u89e6\u53d1\u4e8b\u4ef6\u3002", "conclusion": "\u4e0e\u8fde\u7eed\u53cd\u6b65\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684ETC\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u7b49\u6548\u7684\u7a33\u5b9a\u6027\u80fd\uff0c\u4f46\u63a7\u5236\u5668\u66f4\u65b0\u6b21\u6570\u5c11\u5f97\u591a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u9a7e\u9a76\u5458\u5206\u5fc3\uff0c\u5728\u4ea4\u901a\u7ba1\u7406\u4e2d\u5177\u6709\u5de8\u5927\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.12830", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12830", "abs": "https://arxiv.org/abs/2511.12830", "authors": ["Joanna Klauser", "Bruno Albert", "Christian Lindenmeier", "Andreas Hammer", "Felix Freiling", "Dirk Heckmann", "Sabine Pfeiffer"], "title": "Telekommunikations\u00fcberwachung am Scheideweg: Zur Regulierbarkeit des Zugriffes auf verschl\u00fcsselte Kommunikation", "comment": "Preprint of an article to appear in CyberStR - Zeitschrift f\u00fcr Cyberstrafrecht, Carl Heymanns Verlag, ISSN 3052-5926, Issue 1 (2026), in German", "summary": "Personal communication using technical means is protected by telecommunications secrecy. Any interference with this fundamental right requires a legal basis, which has existed for many years for traditional communication services in the form of telecommunications surveillance (TK\u00dc, \u00a7 100a StPO) and appears to be widely accepted by society. The basis for the implementation of TK\u00dc is the obligation of telecommunications providers to provide interception interfaces. However, the technical implementation of telecommunications has changed significantly as a result of the Internet. Messenger services and Voice over IP telephony are increasingly competing with traditional telephone services. The use of strong end-to-end encryption made possible by this technology is increasingly posing problems for law enforcement agencies, as only cryptographically encrypted content is accessible via the interception interfaces provided to date. Against the backdrop of current discussions on socalled ``chat control'' and its limited social acceptance, this article addresses the question of whether and, if so, how the cooperation obligations of the technical actors involved can be sensibly regulated in the case of encrypted communication.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u52a0\u5bc6\u901a\u4fe1\u80cc\u666f\u4e0b\u6280\u672f\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u4e49\u52a1\u5982\u4f55\u5408\u7406\u89c4\u8303\uff0c\u5206\u6790\u4f20\u7edf\u7535\u4fe1\u76d1\u63a7\u5728\u4e92\u8054\u7f51\u65f6\u4ee3\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u6280\u672f\u53d1\u5c55\uff0c\u4f20\u7edf\u7535\u4fe1\u76d1\u63a7\u9762\u4e34\u52a0\u5bc6\u901a\u4fe1\u7684\u6311\u6218\uff0c\u6267\u6cd5\u673a\u6784\u96be\u4ee5\u83b7\u53d6\u7aef\u5230\u7aef\u52a0\u5bc6\u5185\u5bb9\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u6280\u672f\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u4e49\u52a1\u3002", "method": "\u5206\u6790\u4f20\u7edf\u7535\u4fe1\u76d1\u63a7\u6cd5\u5f8b\u57fa\u7840\u548c\u6280\u672f\u5b9e\u73b0\uff0c\u63a2\u8ba8\u52a0\u5bc6\u901a\u4fe1\u5bf9\u6267\u6cd5\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u6280\u672f\u53c2\u4e0e\u8005\u5408\u4f5c\u4e49\u52a1\u7684\u5408\u7406\u89c4\u8303\u65b9\u5f0f\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u62e6\u622a\u63a5\u53e3\u53ea\u80fd\u83b7\u53d6\u52a0\u5bc6\u5185\u5bb9\uff0c\u65e0\u6cd5\u89e3\u51b3\u7aef\u5230\u7aef\u52a0\u5bc6\u5e26\u6765\u7684\u6267\u6cd5\u96be\u9898\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u5408\u4f5c\u4e49\u52a1\u6846\u67b6\u3002", "conclusion": "\u5728\u52a0\u5bc6\u901a\u4fe1\u65f6\u4ee3\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u6280\u672f\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u4e49\u52a1\u89c4\u8303\uff0c\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u6267\u6cd5\u9700\u6c42\u3002"}}
{"id": "2511.13592", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13592", "abs": "https://arxiv.org/abs/2511.13592", "authors": ["Chen Xu"], "title": "Power Homotopy for Zeroth-Order Non-Convex Optimizations", "comment": null, "summary": "We introduce GS-PowerHP, a novel zeroth-order method for non-convex optimization problems of the form $\\max_{x \\in \\mathbb{R}^d} f(x)$. Our approach leverages two key components: a power-transformed Gaussian-smoothed surrogate $F_{N,\u03c3}(\u03bc) = \\mathbb{E}_{x\\sim\\mathcal{N}(\u03bc,\u03c3^2 I_d)}[e^{N f(x)}]$ whose stationary points cluster near the global maximizer $x^*$ of $f$ for sufficiently large $N$, and an incrementally decaying $\u03c3$ for enhanced data efficiency. Under mild assumptions, we prove convergence in expectation to a small neighborhood of $x^*$ with the iteration complexity of $O(d^2 \\varepsilon^{-2})$. Empirical results show our approach consistently ranks among the top three across a suite of competing algorithms. Its robustness is underscored by the final experiment on a substantially high-dimensional problem ($d=150,528$), where it achieved first place on least-likely targeted black-box attacks against images from ImageNet, surpassing all competing methods.", "AI": {"tldr": "GS-PowerHP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u9636\u975e\u51f8\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u529f\u7387\u53d8\u6362\u7684\u9ad8\u65af\u5e73\u6ed1\u4ee3\u7406\u51fd\u6570\u548c\u9012\u51cf\u65b9\u5dee\u7b56\u7565\uff0c\u5728\u671f\u671b\u610f\u4e49\u4e0b\u6536\u655b\u5230\u5168\u5c40\u6700\u5927\u503c\u7684\u90bb\u57df\uff0c\u5e76\u5728\u9ad8\u7ef4\u56fe\u50cf\u653b\u51fb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u5bfb\u627e\u5168\u5c40\u6700\u5927\u503c\u70b9\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u529f\u7387\u53d8\u6362\u7684\u9ad8\u65af\u5e73\u6ed1\u4ee3\u7406\u51fd\u6570F_{N,\u03c3}(\u03bc) = \ud835\udd3c[e^{N f(x)}]\uff0c\u5176\u5e73\u7a33\u70b9\u5728\u5927N\u65f6\u805a\u96c6\u5728f\u7684\u5168\u5c40\u6700\u5927\u503c\u70b9x*\u9644\u8fd1\uff0c\u5e76\u91c7\u7528\u9012\u51cf\u65b9\u5dee\u03c3\u7b56\u7565\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u4ee5O(d^2 \u03b5^{-2})\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u6536\u655b\u5230x*\u7684\u5c0f\u90bb\u57df\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u591a\u4e2a\u7b97\u6cd5\u4e2d\u6392\u540d\u524d\u4e09\uff0c\u5728d=150,528\u7684\u9ad8\u7ef4\u56fe\u50cf\u653b\u51fb\u4efb\u52a1\u4e2d\u8d85\u8d8a\u6240\u6709\u7ade\u4e89\u65b9\u6cd5\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "GS-PowerHP\u662f\u4e00\u79cd\u9ad8\u6548\u7a33\u5065\u7684\u96f6\u9636\u975e\u51f8\u4f18\u5316\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u7ef4\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.12290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12290", "abs": "https://arxiv.org/abs/2511.12290", "authors": ["Purnima Bindal", "Vikas Kumar", "Sagar Rathore", "Vasudha Bhatnagar"], "title": "AugAbEx : Way Forward for Extractive Case Summarization", "comment": "30 pages, under review in a Journal", "summary": "Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.\n  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u73b0\u6709\u62bd\u8c61\u6458\u8981\u751f\u6210\u63d0\u53d6\u5f0f\u6458\u8981\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u6cd5\u5f8b\u6587\u6863\u81ea\u52a8\u6458\u8981\u7814\u7a76\u63d0\u4f9b\u589e\u5f3a\u7684\u6570\u636e\u8d44\u6e90\u3002", "motivation": "\u6cd5\u5f8b\u5224\u51b3\u6458\u8981\u5bf9\u6cd5\u5f8b\u4ece\u4e1a\u8005\u6784\u6210\u6c89\u91cd\u8ba4\u77e5\u8d1f\u62c5\uff0c\u800c\u6df1\u5ea6\u795e\u7ecf\u65b9\u6cd5\u751f\u6210\u7684\u62bd\u8c61\u6458\u8981\u5bb9\u6613\u8bef\u5224\u6cd5\u5f8b\u672f\u8bed\u6216\u5ffd\u7565\u5173\u952e\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u53d6\u5f0f\u6458\u8981\u65b9\u6cd5\u3002\u4eba\u5de5\u6807\u6ce8\u63d0\u53d6\u5f0f\u6458\u8981\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u900f\u660e\u7684\u6d41\u7a0b\uff0c\u5229\u7528\u73b0\u6709\u7684\u62bd\u8c61\u6807\u51c6\u6458\u8981\u751f\u6210\u76f8\u5e94\u7684\u63d0\u53d6\u5f0f\u6807\u51c6\u6458\u8981\u7248\u672c\uff0c\u786e\u4fdd\u4e13\u5bb6\u610f\u89c1\u4ece\u539f\u59cb\u62bd\u8c61\u6458\u8981\u4f20\u9012\u5230\u8f6c\u6362\u540e\u7684\u63d0\u53d6\u6458\u8981\u4e2d\u3002", "result": "\u8ba1\u5212\u589e\u5f3a\u4e03\u4e2a\u73b0\u6709\u7684\u6848\u4f8b\u6458\u8981\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6dfb\u52a0\u76f8\u5e94\u7684\u63d0\u53d6\u6458\u8981\u6765\u521b\u5efa\u4e30\u5bcc\u7684\u6848\u4f8b\u6458\u8981\u7814\u7a76\u8d44\u6e90\u3002\u8fdb\u884c\u4e86\u7ed3\u6784\u3001\u8bcd\u6c47\u548c\u8bed\u4e49\u7ef4\u5ea6\u7684\u5e7f\u6cdb\u6bd4\u8f83\u8bc4\u4f30\uff0c\u786e\u4fdd\u589e\u5f3a\u7684\u63d0\u53d6\u6458\u8981\u8d28\u91cf\u3002", "conclusion": "\u627f\u8bfa\u516c\u5f00\u53d1\u5e03\u589e\u5f3a\u7684\u6570\u636e\u96c6\uff0c\u76f8\u4fe1\u8fd9\u4e00\u8d44\u6e90\u5c06\u63a8\u52a8\u6cd5\u5f8b\u6587\u6863\u81ea\u52a8\u6458\u8981\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.11647", "categories": ["cs.LG", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11647", "abs": "https://arxiv.org/abs/2511.11647", "authors": ["Dariush Salami", "Ramin Hashemi", "Parham Kazemi", "Mikko A. Uusitalo"], "title": "Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection", "comment": "Accepted to be published in a workshop in IEEE GLOBECOM 2025", "summary": "This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6301\u7eed\u6ce2\u675f\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u5efa\u6a21\u4e3a\u70b9\u4e91\u5e76\u8ba1\u7b97Chamfer\u8ddd\u79bb\u6765\u8bc6\u522b\u7ed3\u6784\u76f8\u4f3c\u73af\u5883\uff0c\u4ece\u800c\u91cd\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6ce2\u675f\u9009\u62e9\u6a21\u578b\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u5bf9\u53ef\u6269\u5c55\u6027\u548c\u80fd\u6548\u6784\u6210\u91cd\u5927\u6311\u6218\u3002", "method": "\u5c06\u73af\u5883\u5efa\u6a21\u4e3a\u70b9\u4e91\uff0c\u6bcf\u4e2a\u70b9\u4ee3\u8868gNodeB\u548c\u5468\u56f4\u6563\u5c04\u4f53\u7684\u4f4d\u7f6e\uff0c\u901a\u8fc7\u8ba1\u7b97\u70b9\u4e91\u4e4b\u95f4\u7684Chamfer\u8ddd\u79bb\u6765\u8bc6\u522b\u7ed3\u6784\u76f8\u4f3c\u7684\u73af\u5883\uff0c\u5e76\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u91cd\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e8616\u500d\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u78b3\u6392\u653e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u8fc1\u79fb\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u4e14\u73af\u4fdd\u7684RL\u6ce2\u675f\u9009\u62e9\u7b56\u7565\uff0c\u652f\u6301\u7eff\u8272\u53ef\u6301\u7eedAI\u5728\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.12169", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12169", "abs": "https://arxiv.org/abs/2511.12169", "authors": ["Kaiyue Zhao", "Dingqi Chen", "Shaoyu Wang", "Pan Hu"], "title": "Incremental Maintenance of DatalogMTL Materialisations", "comment": "Accepted as oral paper at the main track of AAAI 2026", "summary": "DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u4e86DRedMTL\u7b97\u6cd5\uff0c\u4e00\u79cd\u652f\u6301\u6709\u754c\u533a\u95f4\u7684DatalogMTL\u589e\u91cf\u63a8\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u91cd\u65b0\u7269\u5316\u65b9\u6cd5", "motivation": "\u73b0\u6709DatalogMTL\u63a8\u7406\u65b9\u6cd5\u4e0d\u652f\u6301\u9ad8\u6548\u7684\u52a8\u6001\u66f4\u65b0\uff0c\u800c\u73b0\u5b9e\u5e94\u7528\u9700\u8981\u9891\u7e41\u6570\u636e\u66f4\u65b0", "method": "\u57fa\u4e8e\u7ecf\u5178DRed\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e13\u95e8\u64cd\u4f5c\u7b26\u6765\u5904\u7406DatalogMTL\u7269\u5316\u7684\u5468\u671f\u6027\u8868\u793a", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRedMTL\u901a\u5e38\u663e\u8457\u4f18\u4e8e\u91cd\u65b0\u7269\u5316\u65b9\u6cd5\uff0c\u6709\u65f6\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7", "conclusion": "DRedMTL\u4e3aDatalogMTL\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u589e\u91cf\u63a8\u7406\u80fd\u529b\uff0c\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u52a8\u6001\u66f4\u65b0\u9700\u6c42"}}
{"id": "2511.13229", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13229", "abs": "https://arxiv.org/abs/2511.13229", "authors": ["Mary Chriselda Antony Oliver", "Michael Roberts", "Carola-Bibiane Sch\u00f6nlieb", "Matthew Thorpe"], "title": "Laplace Learning in Wasserstein Space", "comment": "46 page, 5 figures", "summary": "The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning\n  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical\n  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to\n  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator\n  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through\n  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u56fe\u534a\u76d1\u7763\u5b66\u4e60\u4ece\u6709\u9650\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u6269\u5c55\u5230\u65e0\u9650\u7ef4Wasserstein\u7a7a\u95f4\uff0c\u8bc1\u660e\u4e86\u79bb\u6563\u56fep-Dirichlet\u80fd\u91cf\u5230\u5176\u8fde\u7eed\u5bf9\u5e94\u7269\u7684\u53d8\u5206\u6536\u655b\uff0c\u5e76\u523b\u753b\u4e86Wasserstein\u7a7a\u95f4\u5b50\u6d41\u5f62\u4e0a\u7684Laplace-Beltrami\u7b97\u5b50\u3002", "motivation": "\u57fa\u4e8e\u6d41\u5f62\u5047\u8bbe\uff0c\u7814\u7a76\u56fe\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728Wasserstein\u7a7a\u95f4\u4e2d\u7684\u6269\u5c55\uff0c\u5c06\u7ecf\u5178\u7b97\u6cd5\u4ece\u6709\u9650\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u63a8\u5e7f\u5230\u65e0\u9650\u7ef4\u8bbe\u7f6e\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u79bb\u6563\u56fep-Dirichlet\u80fd\u91cf\u5230\u8fde\u7eed\u5bf9\u5e94\u7269\u7684\u53d8\u5206\u6536\u655b\uff0c\u5e76\u523b\u753bWasserstein\u7a7a\u95f4\u5b50\u6d41\u5f62\u4e0a\u7684Laplace-Beltrami\u7b97\u5b50\uff0c\u6269\u5c55Laplace Learning\u5230Wasserstein\u7a7a\u95f4\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u5206\u7c7b\u6027\u80fd\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u6210\u529f\u5c06\u56fe\u534a\u76d1\u7763\u5b66\u4e60\u6269\u5c55\u5230Wasserstein\u7a7a\u95f4\uff0c\u4e3a\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.13260", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13260", "abs": "https://arxiv.org/abs/2511.13260", "authors": ["Amit Shivam", "Kiran Kumari", "Fernando A. C. C. Fontes"], "title": "Robust Control Design Using a Hybrid-Gain Finite-Time Sliding-Mode Controller", "comment": "Under review in ECC", "summary": "This paper proposes a hybrid-gain finite-time sliding-mode control (HG-FTSMC) strategy for a class of perturbed nonlinear systems. The controller combines a finite-time reaching law that drives the sliding variable to a predefined boundary layer with an inner mixed-power or exponential law that guarantees rapid convergence within the layer while maintaining smooth and bounded control action. The resulting control design achieves finite-time convergence and robustness to matched disturbances, while explicitly limits the control effort. The control framework is first analyzed on a perturbed first-order integrator model, and then extended to Euler-Lagrange (EL) systems, representing a broad class of robotic and mechanical systems. Comparative simulations demonstrate that the proposed controller achieves settling times comparable to recent finite-time approaches [1], while substantially reducing the control effort. Finally, trajectory-tracking simulations on a two-link manipulator further validate the robustness and practical feasibility of the proposed HG-FTSMC approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u589e\u76ca\u6709\u9650\u65f6\u95f4\u6ed1\u6a21\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u53d7\u6270\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u7ed3\u5408\u6709\u9650\u65f6\u95f4\u8d8b\u8fd1\u5f8b\u548c\u6df7\u5408\u5e42/\u6307\u6570\u5f8b\uff0c\u5728\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u6536\u655b\u7684\u540c\u65f6\u9650\u5236\u63a7\u5236\u4f5c\u7528\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u6709\u9650\u65f6\u95f4\u63a7\u5236\u65b9\u6cd5\u63a7\u5236\u4f5c\u7528\u8fc7\u5927\u3001\u4e0d\u5149\u6ed1\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u6536\u655b\u53c8\u80fd\u9650\u5236\u63a7\u5236\u4f5c\u7528\u7684\u5149\u6ed1\u63a7\u5236\u7b56\u7565\u3002", "method": "\u91c7\u7528\u6df7\u5408\u589e\u76ca\u6709\u9650\u65f6\u95f4\u6ed1\u6a21\u63a7\u5236\uff0c\u5305\u542b\u9a71\u52a8\u6ed1\u6a21\u53d8\u91cf\u5230\u8fb9\u754c\u5c42\u7684\u5916\u90e8\u6709\u9650\u65f6\u95f4\u8d8b\u8fd1\u5f8b\u548c\u4fdd\u8bc1\u5c42\u5185\u5feb\u901f\u6536\u655b\u7684\u5185\u90e8\u6df7\u5408\u5e42/\u6307\u6570\u5f8b\u3002", "result": "\u63a7\u5236\u5668\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u6709\u9650\u65f6\u95f4\u65b9\u6cd5\u76f8\u5f53\u7684\u8c03\u8282\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a7\u5236\u4f5c\u7528\uff0c\u5728\u4e8c\u8fde\u6746\u673a\u68b0\u81c2\u8f68\u8ff9\u8ddf\u8e2a\u4e2d\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684HG-FTSMC\u65b9\u6cd5\u4e3a\u53d7\u6270\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u63a7\u5236\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u6536\u655b\u548c\u9c81\u68d2\u6027\u7684\u540c\u65f6\u9650\u5236\u63a7\u5236\u4f5c\u7528\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.12966", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12966", "abs": "https://arxiv.org/abs/2511.12966", "authors": ["Smitha Muthya Sudheendra", "Zhongxing Zhang", "Wenwen Cao", "Jisu Huh", "Jaideep Srivastava"], "title": "Beyond Citations: A Cross-Domain Metric for Dataset Impact and Shareability", "comment": null, "summary": "The scientific community increasingly relies on open data sharing, yet existing metrics inadequately capture the true impact of datasets as research outputs. Traditional measures, such as the h-index, focus on publications and citations but fail to account for dataset accessibility, reuse, and cross-disciplinary influence. We propose the X-index, a novel author-level metric that quantifies the value of data contributions through a two-step process: (i) computing a dataset-level value score (V-score) that integrates breadth of reuse, FAIRness, citation impact, and transitive reuse depth, and (ii) aggregating V-scores into an author-level X-index. Using datasets from computational social science, medicine, and crisis communication, we validate our approach against expert ratings, achieving a strong correlation. Our results demonstrate that the X-index provides a transparent, scalable, and low-cost framework for assessing data-sharing practices and incentivizing open science. The X-index encourages sustainable data-sharing practices and gives institutions, funders, and platforms a tangible way to acknowledge the lasting influence of research datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86X-index\u8fd9\u4e00\u65b0\u578b\u4f5c\u8005\u7ea7\u6307\u6807\uff0c\u901a\u8fc7\u91cf\u5316\u6570\u636e\u96c6\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u91cd\u7528\u6027\u548c\u8de8\u5b66\u79d1\u5f71\u54cd\u529b\u6765\u8bc4\u4f30\u6570\u636e\u5171\u4eab\u7684\u8d21\u732e\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u6307\u6807\uff08\u5982h\u6307\u6570\uff09\u4e3b\u8981\u5173\u6ce8\u51fa\u7248\u7269\u548c\u5f15\u7528\uff0c\u65e0\u6cd5\u5145\u5206\u8861\u91cf\u6570\u636e\u96c6\u4f5c\u4e3a\u7814\u7a76\u6210\u679c\u7684\u771f\u5b9e\u5f71\u54cd\uff0c\u7279\u522b\u662f\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u3001\u91cd\u7528\u548c\u8de8\u5b66\u79d1\u5f71\u54cd\u65b9\u9762\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\u8ba1\u7b97\u6570\u636e\u96c6\u7ea7\u4ef7\u503c\u5206\u6570\uff08V-score\uff09\uff0c\u6574\u5408\u91cd\u7528\u5e7f\u5ea6\u3001FAIR\u539f\u5219\u7b26\u5408\u5ea6\u3001\u5f15\u7528\u5f71\u54cd\u548c\u4f20\u9012\u91cd\u7528\u6df1\u5ea6\uff1b\u7136\u540e\u5c06V-score\u805a\u5408\u4e3a\u4f5c\u8005\u7ea7X-index\u3002", "result": "\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u3001\u533b\u5b66\u548c\u5371\u673a\u4f20\u64ad\u7b49\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e0e\u4e13\u5bb6\u8bc4\u5206\u5448\u73b0\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "X-index\u4e3a\u8bc4\u4f30\u6570\u636e\u5171\u4eab\u5b9e\u8df5\u548c\u6fc0\u52b1\u5f00\u653e\u79d1\u5b66\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u6269\u5c55\u4e14\u4f4e\u6210\u672c\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u53ef\u6301\u7eed\u7684\u6570\u636e\u5171\u4eab\u5b9e\u8df5\u3002"}}
{"id": "2511.13639", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2511.13639", "abs": "https://arxiv.org/abs/2511.13639", "authors": ["Benjamin Grimmer", "Alex L. Wang"], "title": "Subgame Perfect Methods in Nonsmooth Convex Optimization", "comment": null, "summary": "This paper considers nonsmooth convex optimization with either a subgradient or proximal operator oracle. In both settings, we identify algorithms that achieve the recently introduced game-theoretic optimality notion for algorithms known as subgame perfection. Subgame perfect algorithms meet a more stringent requirement than just minimax optimality. Not only must they provide optimal uniform guarantees on the entire problem class, but also on any subclass determined by information revealed during the execution of the algorithm. In the setting of nonsmooth convex optimization with a subgradient oracle, we show that the Kelley cutting plane-Like Method due to Drori and Teboulle [1] is subgame perfect. For nonsmooth convex optimization with a proximal operator oracle, we develop a new algorithm, the Subgame Perfect Proximal Point Algorithm, and establish that it is subgame perfect. Both of these methods solve a history-aware second-order cone program within each iteration, independent of the ambient problem dimension, to plan their next steps. This yields performance guarantees that are never worse than the minimax optimal guarantees and often substantially better.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u975e\u5149\u6ed1\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5728\u6b21\u68af\u5ea6oracle\u548c\u90bb\u8fd1\u7b97\u5b50oracle\u4e24\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u63d0\u51fa\u4e86\u6ee1\u8db3\u535a\u5f08\u8bba\u6700\u4f18\u6027\u6982\u5ff5\uff08\u5b50\u535a\u5f08\u5b8c\u7f8e\u6027\uff09\u7684\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u901a\u5e38\u53ea\u6ee1\u8db3\u6781\u5c0f\u6781\u5927\u6700\u4f18\u6027\uff0c\u4f46\u5b50\u535a\u5f08\u5b8c\u7f8e\u6027\u8981\u6c42\u7b97\u6cd5\u4e0d\u4ec5\u5728\u6574\u4f53\u95ee\u9898\u7c7b\u4e0a\u63d0\u4f9b\u6700\u4f18\u4fdd\u8bc1\uff0c\u8fd8\u8981\u5728\u7b97\u6cd5\u6267\u884c\u8fc7\u7a0b\u4e2d\u63ed\u793a\u7684\u4efb\u4f55\u5b50\u7c7b\u4e0a\u90fd\u4fdd\u6301\u6700\u4f18\u3002", "method": "\u5bf9\u4e8e\u6b21\u68af\u5ea6oracle\uff0c\u8bc1\u660e\u4e86Drori\u548cTeboulle\u7684Kelley\u5207\u5272\u5e73\u9762\u7c7b\u65b9\u6cd5\u662f\u5b50\u535a\u5f08\u5b8c\u7f8e\u7684\uff1b\u5bf9\u4e8e\u90bb\u8fd1\u7b97\u5b50oracle\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5b50\u535a\u5f08\u5b8c\u7f8e\u90bb\u8fd1\u70b9\u7b97\u6cd5\uff0c\u6bcf\u6b21\u8fed\u4ee3\u6c42\u89e3\u5386\u53f2\u611f\u77e5\u4e8c\u9636\u9525\u89c4\u5212\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6027\u80fd\u4fdd\u8bc1\u4e0a\u4ece\u4e0d\u5dee\u4e8e\u6781\u5c0f\u6781\u5927\u6700\u4f18\u4fdd\u8bc1\uff0c\u4e14\u901a\u5e38\u663e\u8457\u66f4\u597d\uff0c\u4e14\u6c42\u89e3\u590d\u6742\u5ea6\u4e0e\u95ee\u9898\u7ef4\u5ea6\u65e0\u5173\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e24\u7c7b\u975e\u5149\u6ed1\u51f8\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u5b50\u535a\u5f08\u5b8c\u7f8e\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86\u7b97\u6cd5\u6700\u4f18\u6027\u7406\u8bba\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u4e8e\u4f20\u7edf\u6781\u5c0f\u6781\u5927\u6700\u4f18\u6027\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.12300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12300", "abs": "https://arxiv.org/abs/2511.12300", "authors": ["Naoya Sugiura", "Kosuke Yamada", "Yasuhiro Ogawa", "Katsuhiko Toyama", "Ryohei Sasano"], "title": "Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering", "comment": null, "summary": "LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.", "AI": {"tldr": "LLMs\u5728\u65e5\u8bed\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u5bf9\u4e8e\u7ef4\u57fa\u767e\u79d1\u672a\u8986\u76d6\u7684\u7b54\u6848\u548c\u9700\u8981\u6570\u503c\u56de\u7b54\u7684\u95ee\u9898\u8868\u73b0\u8f83\u5dee", "motivation": "\u7814\u7a76LLMs\u548c\u4eba\u7c7b\u5728\u95ee\u7b54\u96be\u5ea6\u4e0a\u7684\u5dee\u5f02\uff0c\u63a2\u8ba8\u5bf9\u4eba\u7c7b\u56f0\u96be\u7684\u95ee\u9898\u662f\u5426\u5bf9LLMs\u4e5f\u540c\u6837\u56f0\u96be", "method": "\u6536\u96c6\u65e5\u8bed\u95ee\u7b54\u6570\u636e\uff0c\u5305\u62ec\u95ee\u9898\u3001\u7b54\u6848\u548c\u4eba\u7c7b\u6b63\u786e\u7387\uff0c\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u8ba9LLMs\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u6bd4\u8f83\u5206\u6790", "result": "\u76f8\u6bd4\u4eba\u7c7b\uff0cLLMs\u5728\u7ef4\u57fa\u767e\u79d1\u672a\u8986\u76d6\u7b54\u6848\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u5dee\uff0c\u5728\u9700\u8981\u6570\u503c\u56de\u7b54\u7684\u95ee\u9898\u4e0a\u4e5f\u6709\u56f0\u96be", "conclusion": "LLMs\u548c\u4eba\u7c7b\u5728\u95ee\u7b54\u96be\u5ea6\u4e0a\u5b58\u5728\u5dee\u5f02\uff0cLLMs\u66f4\u4f9d\u8d56\u7ef4\u57fa\u767e\u79d1\u77e5\u8bc6\uff0c\u4e14\u5728\u6570\u503c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5f31\u70b9"}}
{"id": "2511.11648", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11648", "abs": "https://arxiv.org/abs/2511.11648", "authors": ["Shunyu Wu", "Tianyue Li", "Yixuan Leng", "Jingyi Suo", "Jian Lou", "Dan Li", "See-Kiong Ng"], "title": "Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning", "comment": null, "summary": "Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.", "AI": {"tldr": "\u63d0\u51faLTSV\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5fae\u8c03\u6765\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u4ef7\u503c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u65e0\u6cd5\u4fdd\u6301\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\u8d28\u91cf\uff0c\u4f46\u4f20\u7edf\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\u4e14\u65e0\u6cd5\u4fdd\u6301\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "method": "\u57fa\u4e8e\u7406\u8bba\u8bc1\u636e\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5fae\u8c03\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\uff0c\u5229\u7528\u65f6\u95f4\u5757\u805a\u5408\u6355\u83b7\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u6d4b\u91cf\u4e0a\u4e0b\u6587\u635f\u5931\u53d8\u5316\u6765\u8bc4\u4f30\u6837\u672c\u8d21\u732e\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLTSV\u80fd\u63d0\u4f9b\u53ef\u9760\u4e14\u5f3a\u5927\u7684\u4f30\u503c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a7\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "\u5728\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e0a\u8fdb\u884c\u4e0a\u4e0b\u6587\u5fae\u8c03\u4e3a\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f52\u56e0\u548c\u6a21\u578b\u6cdb\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6709\u6548\u7684\u6865\u6881\u3002"}}
{"id": "2511.12208", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12208", "abs": "https://arxiv.org/abs/2511.12208", "authors": ["Jilong Liu", "Pengyang Shao", "Wei Qin", "Fei Liu", "Yonghui Yang", "Richang Hong"], "title": "Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering", "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.", "AI": {"tldr": "DoM\u662f\u4e00\u4e2a\u7528\u4e8e\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u52a8\u6001\u878d\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u548c\u975e\u7ed3\u6784\u5316\u5916\u90e8\u6587\u672c\uff0c\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u4e0d\u5b8c\u6574\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u77e5\u8bc6\u56fe\u8c31\u5f80\u5f80\u4e0d\u5b8c\u6574\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u81ea\u9002\u5e94\u5730\u878d\u5408\u591a\u6e90\u77e5\u8bc6\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5b83\u4eec\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8303\u5f0f\uff0cDoM\u5206\u914d\u4e13\u95e8\u667a\u80fd\u4f53\u5206\u522b\u5bf9\u77e5\u8bc6\u56fe\u8c31\u548c\u5916\u90e8\u6587\u672c\u8fdb\u884c\u63a8\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u4ea4\u4e92\u534f\u8c03\u5b83\u4eec\u7684\u8f93\u51fa\u3002\u5b83\u5206\u89e3\u8f93\u5165\u95ee\u9898\u4e3a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\uff08KG\u548cRAG\uff09\u68c0\u7d22\u8bc1\u636e\uff0c\u5e76\u4f7f\u7528\u6cd5\u5b98\u667a\u80fd\u4f53\u8bc4\u4f30\u548c\u805a\u5408\u4e2d\u95f4\u7b54\u6848\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cDoM\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DoM\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u4e92\u8865\u6027\u5229\u7528\u548c\u589e\u5f3a\u5bf9\u77e5\u8bc6\u56fe\u8c31\u4e0d\u5b8c\u6574\u6027\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13237", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13237", "abs": "https://arxiv.org/abs/2511.13237", "authors": ["Alan G. Paredes Cetina", "Kaouther Benguessoum", "Raoni Louren\u00e7o", "Sylvain Kubler"], "title": "Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification", "comment": "Accepted in AAAI 2026 Technical Main Track", "summary": "Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\\geq10\\%$ higher confidence while improving sparsity in $\\geq40\\%$.", "AI": {"tldr": "CONFETTI\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u76ee\u6807\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u901a\u8fc7\u5e73\u8861\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3001\u90bb\u8fd1\u6027\u548c\u7a00\u758f\u6027\u6765\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89e3\u91ca\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u548c\u56de\u5f52\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u963b\u788d\u4e86\u51b3\u7b56\u5236\u5b9a\u3002\u73b0\u6709\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u90e8\u5206\u89c1\u89e3\uff0c\u800c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u53ea\u4f18\u5148\u8003\u8651\u51c6\u786e\u6027\u3001\u90bb\u8fd1\u6027\u6216\u7a00\u758f\u6027\u4e2d\u7684\u5355\u4e00\u76ee\u6807\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u4ef7\u503c\u3002", "method": "CONFETTI\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u65f6\u95f4\u5e8f\u5217\u5b50\u5e8f\u5217\u3001\u5b9a\u4f4d\u53cd\u4e8b\u5b9e\u76ee\u6807\uff0c\u5e76\u6700\u4f18\u5730\u4fee\u6539\u65f6\u95f4\u5e8f\u5217\u6765\u5e73\u8861\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3001\u90bb\u8fd1\u6027\u548c\u7a00\u758f\u6027\u3002\u8be5\u65b9\u6cd5\u5728UEA\u6863\u6848\u4e2d\u7684\u4e03\u4e2a\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "CONFETTI\u5728\u4f18\u5316\u76ee\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u5728\u6587\u732e\u4e2d\u7684\u516d\u4e2a\u5176\u4ed6\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u226510%\u7684\u66f4\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u540c\u65f6\u5728\u226540%\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u4e86\u7a00\u758f\u6027\u3002", "conclusion": "CONFETTI\u901a\u8fc7\u6700\u5c0f\u5316\u53d8\u5316\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u652f\u6301\u80fd\u529b\uff0c\u5728\u5404\u79cd\u9886\u57df\u90fd\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.13289", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13289", "abs": "https://arxiv.org/abs/2511.13289", "authors": ["Wenhao Wu", "Dan Wu", "Bin Wang", "Jiabing Hu"], "title": "Beyond Energy Functions and Numerical Integration: A New Methodology to Determine Transient Stability at the Initial State", "comment": "This work has been submitted to 2026 IEEE PES General Meeting", "summary": "This paper presents a novel method for transient stability analysis (TSA) that circumvents the limitations of sequential numerical integration and energy functions. The proposed method begins by constructing a trajectory-dependent stability indicator function to distinguish the system's destiny. To overcome the difficulty in analyzing the asymptotic behavior at infinite time, a strategic time contraction mapping is then applied. This allows TSA to be recast as a pole-placement detection problem for the indicator function. By leveraging high-order derivatives at the initial state, a rational function approximation is derived, yielding a mathematically direct and computationally efficient prediction. Numerical validations on benchmark systems demonstrate that the method not only provides a direct mathematical shortcut for TSA in power systems but also establishes a promising new methodology for evaluating the transient stability of a broad class of nonlinear dynamical systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6682\u6001\u7a33\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u4f9d\u8d56\u7a33\u5b9a\u6027\u6307\u6807\u51fd\u6570\u548c\u7b56\u7565\u6027\u65f6\u95f4\u6536\u7f29\u6620\u5c04\uff0c\u5c06TSA\u8f6c\u5316\u4e3a\u6781\u70b9\u914d\u7f6e\u68c0\u6d4b\u95ee\u9898\uff0c\u5229\u7528\u9ad8\u9636\u5bfc\u6570\u63a8\u5bfc\u6709\u7406\u51fd\u6570\u8fd1\u4f3c\uff0c\u5b9e\u73b0\u6570\u5b66\u76f4\u63a5\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u9884\u6d4b\u3002", "motivation": "\u514b\u670d\u4f20\u7edf\u6570\u503c\u79ef\u5206\u548c\u80fd\u91cf\u51fd\u6570\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u6682\u6001\u7a33\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u66f4\u76f4\u63a5\u6709\u6548\u7684\u6570\u5b66\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u8f68\u8ff9\u4f9d\u8d56\u7a33\u5b9a\u6027\u6307\u6807\u51fd\u6570\uff0c\u5e94\u7528\u7b56\u7565\u6027\u65f6\u95f4\u6536\u7f29\u6620\u5c04\uff0c\u5c06TSA\u8f6c\u5316\u4e3a\u6781\u70b9\u914d\u7f6e\u68c0\u6d4b\u95ee\u9898\uff0c\u5229\u7528\u521d\u59cb\u72b6\u6001\u9ad8\u9636\u5bfc\u6570\u63a8\u5bfc\u6709\u7406\u51fd\u6570\u8fd1\u4f3c\u3002", "result": "\u5728\u57fa\u51c6\u7cfb\u7edf\u4e0a\u7684\u6570\u503c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u7535\u529b\u7cfb\u7edfTSA\u63d0\u4f9b\u4e86\u76f4\u63a5\u7684\u6570\u5b66\u6377\u5f84\uff0c\u8fd8\u4e3a\u8bc4\u4f30\u5e7f\u6cdb\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u6682\u6001\u7a33\u5b9a\u6027\u5efa\u7acb\u4e86\u6709\u524d\u666f\u7684\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edfTSA\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u5b66\u76f4\u63a5\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6682\u6001\u7a33\u5b9a\u6027\u9884\u6d4b\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.13432", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13432", "abs": "https://arxiv.org/abs/2511.13432", "authors": ["Subramanyam Sahoo", "Aditi Chhawacharia"], "title": "The Last Vote: A Multi-Stakeholder Framework for Language Model Governance", "comment": "This paper has been accepted to the NeurIPS 2025 Workshop on Algorithmic Collective Action (ACA@NeurIPS 2025). The submission is 26 pages including the appendix and includes the NeurIPS checklist. A big thanks to Avijit Ghosh", "summary": "As artificial intelligence systems become increasingly powerful and pervasive, democratic societies face unprecedented challenges in governing these technologies while preserving core democratic values and institutions. This paper presents a comprehensive framework to address the full spectrum of risks that AI poses to democratic societies. Our approach integrates multi-stakeholder participation, civil society engagement, and existing international governance frameworks while introducing novel mechanisms for risk assessment and institutional adaptation. We propose: (1) a seven-category democratic risk taxonomy extending beyond individual-level harms to capture systemic threats, (2) a stakeholder-adaptive Incident Severity Score (ISS) that incorporates diverse perspectives and context-dependent risk factors, and (3) a phased implementation strategy that acknowledges the complex institutional changes required for effective AI governance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684AI\u6c11\u4e3b\u98ce\u9669\u6cbb\u7406\u6846\u67b6\uff0c\u5305\u542b\u98ce\u9669\u5206\u7c7b\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u5b9e\u65bd\u7b56\u7565", "motivation": "AI\u7cfb\u7edf\u65e5\u76ca\u5f3a\u5927\u548c\u666e\u53ca\uff0c\u6c11\u4e3b\u793e\u4f1a\u5728\u6cbb\u7406\u8fd9\u4e9b\u6280\u672f\u540c\u65f6\u4fdd\u62a4\u6838\u5fc3\u6c11\u4e3b\u4ef7\u503c\u548c\u5236\u5ea6\u9762\u4e34\u524d\u6240\u672a\u6709\u7684\u6311\u6218", "method": "\u6574\u5408\u591a\u65b9\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u3001\u516c\u6c11\u793e\u4f1a\u53c2\u4e0e\u548c\u73b0\u6709\u56fd\u9645\u6cbb\u7406\u6846\u67b6\uff0c\u5f15\u5165\u98ce\u9669\u8bc4\u4f30\u548c\u5236\u5ea6\u9002\u5e94\u7684\u65b0\u673a\u5236", "result": "\u63d0\u51fa\u4e86\u4e03\u7c7b\u6c11\u4e3b\u98ce\u9669\u5206\u7c7b\u6cd5\u3001\u5229\u76ca\u76f8\u5173\u8005\u9002\u5e94\u7684ISS\u8bc4\u5206\u7cfb\u7edf\u548c\u5206\u9636\u6bb5\u5b9e\u65bd\u7b56\u7565", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6c11\u4e3b\u793e\u4f1a\u5e94\u5bf9AI\u98ce\u9669\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u6cbb\u7406\u65b9\u6848"}}
{"id": "2511.12381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12381", "abs": "https://arxiv.org/abs/2511.12381", "authors": ["Logan Mann", "Nayan Saxena", "Sarah Tandon", "Chenhao Sun", "Savar Toteja", "Kevin Zhu"], "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load", "comment": null, "summary": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\"\u8bbd\u523a\u53cd\u5f39\"\u73b0\u8c61\u2014\u2014\u5426\u5b9a\u6307\u4ee4\u53cd\u800c\u4f1a\u589e\u52a0\u88ab\u7981\u6b62\u6982\u5ff5\u7684\u53ef\u53ca\u6027\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u548c\u7535\u8def\u8ffd\u8e2a\u5206\u6790\u63ed\u793a\u4e86\u53cd\u5f39\u673a\u5236\u3002", "motivation": "\u5426\u5b9a\u6307\u4ee4\u5728\u4eba\u7c7b\u601d\u7ef4\u4e2d\u4f1a\u4ea7\u751f\u8bbd\u523a\u53cd\u5f39\u73b0\u8c61\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u73b0\u8c61\uff0c\u5e76\u63a2\u7d22\u5176\u5185\u5728\u673a\u5236\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff1a(1) \u6539\u53d8\u5e72\u6270\u6587\u672c\u7c7b\u578b\uff08\u8bed\u4e49\u3001\u53e5\u6cd5\u3001\u91cd\u590d\uff09\u6d4b\u91cf\u53cd\u5f39\u5f3a\u5ea6\uff1b(2) \u6d4b\u8bd5\u6a21\u578b\u5bf9\u4e2d\u6027vs\u8d1f\u9762\u6846\u67b6\u7684\u533a\u5206\u80fd\u529b\uff1b\u5e76\u8f85\u4ee5\u7535\u8def\u8ffd\u8e2a\u5206\u6790\u8bc6\u522b\u5173\u952e\u6ce8\u610f\u529b\u5934\u3002", "result": "\u5426\u5b9a\u540e\u7acb\u5373\u51fa\u73b0\u53cd\u5f39\uff0c\u8f83\u957f\u6216\u8bed\u4e49\u5e72\u6270\u4f1a\u52a0\u5267\u53cd\u5f39\uff0c\u800c\u91cd\u590d\u6709\u52a9\u4e8e\u6291\u5236\uff1b\u66f4\u5f3a\u7684\u6781\u6027\u533a\u5206\u4e0e\u66f4\u6301\u4e45\u7684\u53cd\u5f39\u76f8\u5173\uff1b\u7535\u8def\u5206\u6790\u53d1\u73b0\u4e2d\u95f4\u5c42\u6ce8\u610f\u529b\u5934\u4f1a\u653e\u5927\u88ab\u7981\u6807\u8bb0\u3002", "conclusion": "\u7814\u7a76\u5c06\u8ba4\u77e5\u9884\u6d4b\u4e0e\u673a\u5236\u6d1e\u5bdf\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u4e86\u957f\u4e0a\u4e0b\u6587\u5e72\u6270\u4e2d\u7684\u8bbd\u523a\u53cd\u5f39\u73b0\u8c61\uff0c\u5e76\u53d1\u5e03\u4e86ReboundBench\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2511.11650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11650", "abs": "https://arxiv.org/abs/2511.11650", "authors": ["Daniele Ugo Leonzio", "Paolo Bestagini", "Marco Marcon", "Stefano Tubaro"], "title": "Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine", "comment": null, "summary": "Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c34\u538b\u6d4b\u91cf\u548c\u5355\u7c7bSVM\u7684\u6f0f\u6c34\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u65e0\u6cc4\u6f0f\u65f6\u7684\u538b\u529b\u6570\u636e\u548c\u7ba1\u7f51\u62d3\u6251\u4fe1\u606f\uff0c\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6c34\u7ba1\u7f51\u7edc\u6bcf\u5e74\u56e0\u6f0f\u6c34\u9020\u6210\u5927\u91cf\u6c34\u8d44\u6e90\u635f\u5931\uff0c\u9700\u8981\u53ef\u9760\u6709\u6548\u7684\u6f0f\u6c34\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7cfb\u7edf\u3002\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u56e0\u5176\u4f18\u8d8a\u6027\u80fd\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u5173\u6ce8\u3002", "method": "\u57fa\u4e8e\u6c34\u7ba1\u7f51\u7edc\u8282\u70b9\u6c34\u538b\u6d4b\u91cf\uff0c\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5728\u65e0\u6cc4\u6f0f\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\uff0c\u5c06\u6cc4\u6f0f\u68c0\u6d4b\u4e3a\u5f02\u5e38\u3002", "result": "\u5728Modena\u6c34\u7ba1\u7f51\u7edc\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5728\u6f0f\u6c34\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u6700\u8fd1\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u4ec5\u4f7f\u7528\u6c34\u7ba1\u7f51\u7edc\u62d3\u6251\u548c\u65e0\u6cc4\u6f0f\u538b\u529b\u6570\u636e\uff0c\u5c31\u80fd\u6709\u6548\u68c0\u6d4b\u6f0f\u6c34\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.12214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12214", "abs": "https://arxiv.org/abs/2511.12214", "authors": ["Ruochen Li", "Zhanxing Zhu", "Tanqiu Qiao", "Hubert P. H. Shum"], "title": "ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction", "comment": null, "summary": "Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.", "AI": {"tldr": "\u63d0\u51faViTE\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u56fe\u548c\u4e13\u5bb6\u8def\u7531\u5668\u89e3\u51b3\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u4e2d\u9ad8\u9636\u4ea4\u4e92\u5efa\u6a21\u7684\u6df1\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u6743\u8861\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u9ad8\u9636\u4ea4\u4e92\u65f6\u9762\u4e34\u6df1\u5ea6\u4e0d\u8db3\u5bfc\u81f4\u611f\u53d7\u91ce\u53d7\u9650\u4e0e\u6df1\u5ea6\u8fc7\u6df1\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u6743\u8861\u95ee\u9898\uff0c\u9700\u8981\u80fd\u591f\u81ea\u9002\u5e94\u5efa\u6a21\u663e\u5f0f\u4e00\u8df3\u4ea4\u4e92\u548c\u9690\u5f0f\u9ad8\u9636\u4f9d\u8d56\u7684\u65b9\u6cd5", "method": "ViTE\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u865a\u62df\u56fe\u5f15\u5165\u52a8\u6001\u865a\u62df\u8282\u70b9\u5efa\u6a21\u957f\u8ddd\u79bb\u548c\u9ad8\u9636\u4ea4\u4e92\u800c\u65e0\u9700\u6df1\u5c42GNN\u5806\u53e0\uff1b\u4e13\u5bb6\u8def\u7531\u5668\u57fa\u4e8e\u793e\u4ea4\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u9009\u62e9\u4ea4\u4e92\u4e13\u5bb6\uff0c\u91c7\u7528\u4e13\u5bb6\u6df7\u5408\u8bbe\u8ba1", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08ETH/UCY\u3001NBA\u548cSDD\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u9645\u6548\u7387", "conclusion": "ViTE\u6846\u67b6\u901a\u8fc7\u865a\u62df\u56fe\u548c\u4e13\u5bb6\u8def\u7531\u5668\u7684\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u4ea4\u4e92\u6a21\u5f0f\u7684\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u63a8\u7406\uff0c\u5728\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd"}}
{"id": "2511.13394", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13394", "abs": "https://arxiv.org/abs/2511.13394", "authors": ["Vasilis Gkolemis", "Christos Diou", "Michael Gutmann"], "title": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo", "comment": null, "summary": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u53ef\u5fae\u5206\u6a21\u62df\u5668\u7684\u8d1d\u53f6\u65af\u53c2\u6570\u63a8\u65ad\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u968f\u673a\u6a21\u62df\u91cd\u65b0\u8868\u8ff0\u4e3a\u786e\u5b9a\u6027\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u68af\u5ea6\u65b9\u6cd5\u9ad8\u6548\u5b9a\u4f4d\u9ad8\u5bc6\u5ea6\u540e\u9a8c\u533a\u57df\uff0c\u663e\u8457\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u590d\u6742\u968f\u673a\u6a21\u62df\u5668\u7684\u8d1d\u53f6\u65af\u53c2\u6570\u63a8\u65ad\u9762\u4e34\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u65ad\u65b9\u6cd5\u5728\u53c2\u6570\u7a7a\u95f4\u9ad8\u7ef4\u6216\u8f93\u51fa\u90e8\u5206\u65e0\u4fe1\u606f\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u57fa\u4e8e\u4f18\u5316\u8499\u7279\u5361\u6d1b\u6846\u67b6\uff0c\u5c06\u968f\u673a\u6a21\u62df\u91cd\u65b0\u8868\u8ff0\u4e3a\u786e\u5b9a\u6027\u4f18\u5316\u95ee\u9898\uff0c\u5e94\u7528\u68af\u5ea6\u65b9\u6cd5\u9ad8\u6548\u5bfc\u822a\u81f3\u9ad8\u5bc6\u5ea6\u540e\u9a8c\u533a\u57df\uff0c\u907f\u514d\u5728\u4f4e\u6982\u7387\u533a\u57df\u8fdb\u884c\u6d6a\u8d39\u6027\u6a21\u62df\uff0c\u5e76\u901a\u8fc7JAX\u5b9e\u73b0\u5173\u952e\u7ec4\u4ef6\u5411\u91cf\u5316\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u3001\u65e0\u4fe1\u606f\u8f93\u51fa\u3001\u591a\u89c2\u6d4b\u503c\u548c\u591a\u5cf0\u540e\u9a8c\u7b49\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u5339\u914d\u5e76\u7ecf\u5e38\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u5fae\u5206\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u8d1d\u53f6\u65af\u53c2\u6570\u63a8\u65ad\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.13362", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.13362", "abs": "https://arxiv.org/abs/2511.13362", "authors": ["Xiayan Xu", "Xiaomeng Chen", "Dawei Shi", "Ling Shi"], "title": "Event-triggered Dual Gradient Tracking for Distributed Resource Allocation", "comment": null, "summary": "High communication costs create a major bottleneck for distributed resource allocation over unbalanced directed networks. Conventional dual gradient tracking methods, while effective for problems on unbalanced digraphs, rely on periodic communication that creates significant overhead in resource-constrained networks. This paper introduces a novel event-triggered dual gradient tracking algorithm to mitigate this limitation, wherein agents communicate only when local state deviations surpass a predefined threshold. We establish comprehensive convergence guarantees for this approach. First, we prove sublinear convergence for non-convex dual objectives and linear convergence under the Polyak-\u0141ojasiewicz condition. Building on this, we demonstrate that the proposed algorithm achieves sublinear convergence for general strongly convex cost functions and linear convergence for those that are also Lipschitz-smooth. Numerical experiments confirm that our event-triggered method significantly reduces communication events compared to periodic schemes while preserving comparable convergence performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u89e6\u53d1\u7684\u5bf9\u5076\u68af\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5e73\u8861\u6709\u5411\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u4ec5\u5728\u5c40\u90e8\u72b6\u6001\u504f\u5dee\u8d85\u8fc7\u9608\u503c\u65f6\u901a\u4fe1\u6765\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u5bf9\u5076\u68af\u5ea6\u8ddf\u8e2a\u65b9\u6cd5\u5728\u4e0d\u5e73\u8861\u6709\u5411\u56fe\u4e0a\u6709\u6548\uff0c\u4f46\u4f9d\u8d56\u5468\u671f\u6027\u901a\u4fe1\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7f51\u7edc\u4e2d\u4ea7\u751f\u663e\u8457\u5f00\u9500\u3002\u9700\u8981\u51cf\u5c11\u901a\u4fe1\u9891\u7387\u540c\u65f6\u4fdd\u6301\u6536\u655b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e8b\u4ef6\u89e6\u53d1\u673a\u5236\uff0c\u5f53\u5c40\u90e8\u72b6\u6001\u504f\u5dee\u8d85\u8fc7\u9884\u8bbe\u9608\u503c\u65f6\u624d\u8fdb\u884c\u901a\u4fe1\u3002\u7ed3\u5408\u5bf9\u5076\u68af\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\u5904\u7406\u4e0d\u5e73\u8861\u6709\u5411\u7f51\u7edc\u95ee\u9898\u3002", "result": "\u8bc1\u660e\u4e86\u975e\u51f8\u5bf9\u5076\u76ee\u6807\u7684\u6b21\u7ebf\u6027\u6536\u655b\u548cPolyak-\u0141ojasiewicz\u6761\u4ef6\u4e0b\u7684\u7ebf\u6027\u6536\u655b\u3002\u5bf9\u4e8e\u5f3a\u51f8\u6210\u672c\u51fd\u6570\u5b9e\u73b0\u6b21\u7ebf\u6027\u6536\u655b\uff0c\u5bf9Lipschitz\u5149\u6ed1\u5f3a\u51f8\u51fd\u6570\u5b9e\u73b0\u7ebf\u6027\u6536\u655b\u3002\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u901a\u4fe1\u4e8b\u4ef6\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u4e8b\u4ef6\u89e6\u53d1\u65b9\u6cd5\u5728\u4fdd\u6301\u53ef\u6bd4\u6536\u655b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u5468\u671f\u6027\u65b9\u6848\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u4e8b\u4ef6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u7f51\u7edc\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.13525", "categories": ["cs.CY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13525", "abs": "https://arxiv.org/abs/2511.13525", "authors": ["Zichong Wang", "Zhipeng Yin", "Roland H. C. Yap", "Wenbin Zhang"], "title": "AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions", "comment": "ECAI 2025", "summary": "Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u4e86\u5728\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7684AI\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u516c\u5e73\u6027\u6982\u5ff5\u5206\u7c7b\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u6280\u672f\u3002", "motivation": "AI\u51b3\u7b56\u7cfb\u7edf\u4e2d\u7684\u6b67\u89c6\u6027\u7ed3\u679c\u5f15\u53d1\u4e86\u516c\u5e73\u6027\u62c5\u5fe7\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5b8c\u6574\u7684\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u5f80\u5f80\u4e0d\u53ef\u884c\u3002", "method": "\u5f15\u5165\u65b0\u7684\u516c\u5e73\u6027\u6982\u5ff5\u5206\u7c7b\u6cd5\uff0c\u9610\u660e\u4e0d\u540c\u6982\u5ff5\u95f4\u7684\u5173\u7cfb\u548c\u533a\u522b\uff0c\u5e76\u603b\u7ed3\u5728\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u4fc3\u8fdb\u516c\u5e73\u6027\u7684\u73b0\u6709\u6280\u672f\u3002", "result": "\u5efa\u7acb\u4e86\u5728\u4e0d\u5b8c\u6574\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0b\u7684\u516c\u5e73\u6027\u6982\u5ff5\u6846\u67b6\uff0c\u4e3a\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u6311\u6218\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0e\u73b0\u5b9e\u6311\u6218\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u5f3a\u8c03\u4e86\u5728\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u63a8\u8fdbAI\u516c\u5e73\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.12265", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12265", "abs": "https://arxiv.org/abs/2511.12265", "authors": ["Rui Wang", "Zeming Wei", "Xiyue Zhang", "Meng Sun"], "title": "Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks", "comment": null, "summary": "Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6821\u51c6\u5bf9\u6297\u91c7\u6837\uff08CAS\uff09\u7684\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u52a8\u6001\u8bbe\u8ba1\u5956\u52b1\u5e76\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4ee5\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u591a\u79cd\u653b\u51fb\u7c7b\u578b\u7684\u6574\u4f53\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u6216\u6709\u9650\u653b\u51fb\u7c7b\u578b\uff0c\u5bfc\u81f4\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9047\u5230\u8bad\u7ec3\u4e2d\u672a\u5904\u7406\u7684\u653b\u51fb\u7c7b\u578b\u65f6\u4ecd\u7136\u8106\u5f31\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\u6846\u67b6\uff0c\u52a8\u6001\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u5e76\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u8003\u8651\u591a\u4e2a\u9c81\u68d2\u6027\u7ef4\u5ea6\u7684\u52a8\u6001\u548c\u76f8\u4e92\u4f9d\u8d56\u7279\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAS\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u5e72\u51c0\u51c6\u786e\u7387\u3002", "conclusion": "CAS\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u591a\u79cd\u653b\u51fb\u7c7b\u578b\u3002"}}
{"id": "2511.12387", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12387", "abs": "https://arxiv.org/abs/2511.12387", "authors": ["Jeyarajalingam Varsha", "Menan Velayuthan", "Sumirtha Karunakaran", "Rasan Nivethiga", "Kengatharaiyer Sarveswaran"], "title": "From Phonemes to Meaning: Evaluating Large Language Models on Tamil", "comment": "11 pages", "summary": "Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.", "AI": {"tldr": "ILAKKANAM\u662f\u9996\u4e2a\u6cf0\u7c73\u5c14\u8bed\u4e13\u7528\u8bed\u8a00\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b820\u9053\u6765\u81ea\u65af\u91cc\u5170\u5361\u5b66\u6821\u8003\u8bd5\u7684\u624b\u5de5\u6807\u6ce8\u95ee\u9898\uff0c\u8bc4\u4f30\u663e\u793aLLMs\u5728\u4f4e\u5e74\u7ea7\u8868\u73b0\u826f\u597d\u4f46\u968f\u8bed\u8a00\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00\u57fa\u51c6\u4f9d\u8d56\u82f1\u8bed\u7ffb\u8bd1\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u6355\u6349\u6cf0\u7c73\u5c14\u8bed\u7b49\u4f4e\u8d44\u6e90\u3001\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u7684\u8bed\u8a00\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u4f7f\u7528820\u9053\u65af\u91cc\u5170\u5361\u5b66\u6821\u6cf0\u7c73\u5c14\u8bed\u8003\u8bd5\u95ee\u9898\u6784\u5efa\u57fa\u51c6\uff0c\u7531\u8bad\u7ec3\u6709\u7d20\u7684\u8bed\u8a00\u5b66\u5bb6\u5728\u4e94\u4e2a\u8bed\u8a00\u7c7b\u522b\u548c\u4e00\u4e2a\u4e8b\u5b9e\u77e5\u8bc6\u7c7b\u522b\u4e0b\u6807\u6ce8\uff0c\u6db5\u76d61-13\u5e74\u7ea7\u3002", "result": "Gemini 2.5\u8868\u73b0\u6700\u4f73\uff0c\u5f00\u6e90\u6a21\u578b\u843d\u540e\uff1b\u6240\u6709\u6a21\u578b\u5728\u4f4e\u5e74\u7ea7\u95ee\u9898\u8868\u73b0\u826f\u597d\uff0c\u4f46\u968f\u8bed\u8a00\u590d\u6742\u5ea6\u589e\u52a0\u660e\u663e\u4e0b\u964d\uff1b\u6a21\u578b\u8868\u73b0\u4e0e\u8bc6\u522b\u8bed\u8a00\u7c7b\u522b\u80fd\u529b\u65e0\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "LLMs\u5728\u6cf0\u7c73\u5c14\u8bed\u4e2d\u7684\u8868\u73b0\u53ef\u80fd\u66f4\u591a\u57fa\u4e8e\u63a5\u89e6\u800c\u975e\u771f\u6b63\u7406\u89e3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5176\u771f\u5b9e\u8bed\u8a00\u80fd\u529b\u3002"}}
{"id": "2511.11651", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11651", "abs": "https://arxiv.org/abs/2511.11651", "authors": ["Zhijian Gong", "Wenjia Dong", "Xueyuan Xu", "Fulin Wei", "Chunyu Liu", "Li Zhuo"], "title": "Incomplete Depression Feature Selection with Missing EEG Channels", "comment": null, "summary": "As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIDFS-MEC\u7684\u65b0\u578b\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406EEG\u6570\u636e\u4e2d\u7684\u901a\u9053\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u5197\u4f59\u6700\u5c0f\u5316\u6765\u51cf\u5c11\u7279\u5f81\u5b50\u96c6\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\uff0c\u5728\u6291\u90c1\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e10\u79cd\u6d41\u884c\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "EEG\u7279\u5f81\u901a\u5e38\u5305\u542b\u5197\u4f59\u3001\u4e0d\u76f8\u5173\u548c\u566a\u58f0\u4fe1\u606f\uff0c\u4e14\u73b0\u5b9e\u4e16\u754cEEG\u6570\u636e\u91c7\u96c6\u7ecf\u5e38\u9762\u4e34\u7535\u6781\u8131\u843d\u5bfc\u81f4\u6570\u636e\u4e22\u5931\u548c\u4e25\u91cd\u566a\u58f0\u5e72\u6270\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u4e0d\u5b8c\u6574\u901a\u9053\u7684\u9c81\u68d2\u6291\u90c1\u5206\u6790\u65b9\u6cd5\u3002", "method": "IDFS-MEC\u65b9\u6cd5\u6574\u5408\u4e86\u7f3a\u5931\u901a\u9053\u6307\u793a\u4fe1\u606f\u548c\u81ea\u9002\u5e94\u901a\u9053\u6743\u91cd\u5b66\u4e60\u5230\u6b63\u4ea4\u56de\u5f52\u4e2d\uff0c\u4ee5\u51cf\u8f7b\u4e0d\u5b8c\u6574\u901a\u9053\u5bf9\u6a21\u578b\u6784\u5efa\u7684\u5f71\u54cd\uff0c\u7136\u540e\u5229\u7528\u5168\u5c40\u5197\u4f59\u6700\u5c0f\u5316\u5b66\u4e60\u6765\u51cf\u5c11\u6240\u9009\u7279\u5f81\u5b50\u96c6\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u3002", "result": "\u5728MODMA\u548cPRED-d003\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIDFS-MEC\u9009\u62e9\u7684EEG\u7279\u5f81\u5b50\u96c6\u57283\u300164\u548c128\u901a\u9053\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e10\u79cd\u6d41\u884c\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u3002", "conclusion": "IDFS-MEC\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406EEG\u6570\u636e\u4e2d\u7684\u901a\u9053\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u9009\u62e9\u51fa\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u7684\u7279\u5f81\u5b50\u96c6\uff0c\u4e3a\u57fa\u4e8eEEG\u7684\u6291\u90c1\u5206\u6790\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12239", "abs": "https://arxiv.org/abs/2511.12239", "authors": ["Tarun Gupta", "Danish Pruthi"], "title": "Beyond World Models: Rethinking Understanding in AI Models", "comment": "Accepted to AAAI 2026 (Main Track)", "summary": "World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models \"understand\" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u54f2\u5b66\u6848\u4f8b\u7814\u7a76\u6279\u5224\u6027\u68c0\u9a8c\u4e16\u754c\u6a21\u578b\u6846\u67b6\u662f\u5426\u80fd\u5145\u5206\u8868\u5f81\u4eba\u7c7b\u6c34\u5e73\u7684\u7406\u89e3\u80fd\u529b", "motivation": "\u4eba\u7c7b\u62e5\u6709\u5fc3\u7406\u4e16\u754c\u6a21\u578b\uff0c\u7814\u7a76AI\u6a21\u578b\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u8868\u5f81\u53ef\u80fd\u8868\u660e\u5b83\u4eec\u4ee5\u7c7b\u4eba\u65b9\u5f0f\"\u7406\u89e3\"\u4e16\u754c", "method": "\u4f7f\u7528\u79d1\u5b66\u54f2\u5b66\u6587\u732e\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u4e16\u754c\u6a21\u578b\u80fd\u529b\u4e0e\u4eba\u7c7b\u7406\u89e3\u5dee\u5f02\u6700\u660e\u663e\u7684\u54f2\u5b66\u5206\u6790", "result": "\u8fd9\u4e9b\u54f2\u5b66\u89c2\u70b9\u5c55\u793a\u4e86\u4e16\u754c\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u867d\u7136\u5b83\u4eec\u4e0d\u4ee3\u8868\u666e\u904d\u5b9a\u4e49\uff0c\u4f46\u6709\u52a9\u4e8e\u63a2\u7d22\u4e16\u754c\u6a21\u578b\u7684\u8fb9\u754c", "conclusion": "\u4e16\u754c\u6a21\u578b\u6846\u67b6\u5728\u8868\u5f81\u4eba\u7c7b\u6c34\u5e73\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u6df1\u5165\u5730\u63a2\u8ba8\u7406\u89e3\u4e0e\u6a21\u62df\u4e4b\u95f4\u7684\u533a\u522b"}}
{"id": "2511.13421", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13421", "abs": "https://arxiv.org/abs/2511.13421", "authors": ["Tingkai Yan", "Haodong Wen", "Binghui Li", "Kairong Luo", "Wenguang Chen", "Kaifeng Lyu"], "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression", "comment": null, "summary": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($\u0398(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u6709\u9650\u6570\u636e\u548c\u591a\u6b21\u8bad\u7ec3\u5468\u671f\u4e0b\uff0c\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u7684\u53d8\u5316\u5f62\u5f0f\uff0c\u7279\u522b\u662f\u7814\u7a76\u4e86\u591a\u8f6e\u8bad\u7ec3\u5bf9\u7ebf\u6027\u56de\u5f52\u4e2d\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u7684\u5f71\u54cd\uff0c\u5b9a\u4e49\u4e86\u6709\u6548\u91cd\u590d\u7387E(K,N)\u6765\u8861\u91cf\u5355\u6b21\u8bad\u7ec3\u6240\u9700\u7684\u6570\u636e\u91cf\u589e\u957f\u500d\u6570\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u5728\u4e00\u6b21\u6027\u8bad\u7ec3\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u7684\u60c5\u51b5\u4e0b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u6709\u9650\u6570\u636e\u548c\u591a\u6b21\u8bad\u7ec3\u5468\u671f\u4e0b\u7684\u5f62\u5f0f\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5206\u6790\u591a\u8f6e\u8bad\u7ec3\u5982\u4f55\u91cd\u5851\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u3002", "method": "\u5728\u7ebf\u6027\u56de\u5f52\u6846\u67b6\u4e0b\uff0c\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SGD)\u65b9\u6cd5\uff0c\u5206\u522b\u5728\u5f3a\u51f8\u6027\u6216Zipf\u5206\u5e03\u6570\u636e\u6761\u4ef6\u4e0b\uff0c\u7406\u8bba\u5206\u6790\u4e86\u6709\u6548\u91cd\u590d\u7387E(K,N)\u7684\u7f29\u653e\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1)\u5f53K\u8f83\u5c0f\u65f6\uff0cE(K,N)\u2248K\uff0c\u8868\u660e\u6bcf\u4e2a\u65b0\u8bad\u7ec3\u5468\u671f\u5e26\u6765\u7ebf\u6027\u589e\u76ca\uff1b(2)\u968f\u7740K\u589e\u52a0\uff0cE(K,N)\u4f1a\u8d8b\u4e8e\u4e00\u4e2a\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u5e73\u53f0\u503c\uff0c\u8be5\u503c\u968fN\u589e\u957f\u800c\u589e\u957f\uff0c\u610f\u5473\u7740\u66f4\u5927\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u5728\u8fb9\u9645\u6548\u76ca\u6d88\u5931\u524d\u88ab\u91cd\u590d\u66f4\u591a\u6b21\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u6570\u636e\u5927\u5c0f\u548c\u5206\u5e03\u5bf9\u6709\u6548\u91cd\u590d\u7387\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u5728\u672a\u6765\u7814\u7a76\u6570\u636e\u91cd\u590d\u7684\u7f29\u653e\u5b9a\u5f8b\u65f6\u9700\u8981\u660e\u786e\u5efa\u6a21\u8fd9\u4e24\u4e2a\u56e0\u7d20\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5bf9\u6700\u8fd1\u7684\u7ecf\u9a8c\u7814\u7a76\u63d0\u51fa\u4e86\u4fee\u6b63\u3002"}}
{"id": "2511.13412", "categories": ["eess.SY", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.13412", "abs": "https://arxiv.org/abs/2511.13412", "authors": ["Liyang Jin", "Zichen Xi", "Joseph G. Thomas", "Jun Ji", "Yuanzhi Zhang", "Nuo Chen", "Yizheng Zhu", "Linbo Shao", "Liyan Zhu"], "title": "Microwave-acoustic-driven power electronics", "comment": null, "summary": "Electrical isolation is critical to ensure safety and minimize electromagnetic interference (EMI), yet existing methods struggle to simultaneously transmit power and signals through a unified channel. Here we demonstrate a mechanically-isolated gate driver based on microwave-frequency surface acoustic wave (SAW) device on lithium niobate that achieves galvanic isolation of 2.75 kV with ultralow isolation capacitance (0.032 pF) over 1.25 mm mechanical propagation length, delivering 13.4 V open-circuit voltage and 44.4 mA short-circuit current. We demonstrate isolated gate driving for a gallium nitride (GaN) high-electron-mobility transistor, achieving a turn-on time of 108.8 ns comparable to commercial drivers and validate its operation in a buck converter. In addition, our SAW device operates over an ultrawide temperature range from 0.5 K (-272.6 \u00b0C) to 544 K (271 \u00b0C). The microwave-frequency SAW devices offer inherent EMI immunity and potential for heterogeneous integration on multiple semiconductor platforms, enabling compact, high-performance isolated power and signal transmission in advanced power electronics.", "AI": {"tldr": "\u57fa\u4e8e\u5fae\u6ce2\u9891\u7387\u8868\u9762\u58f0\u6ce2\u5668\u4ef6\u7684\u673a\u68b0\u9694\u79bb\u6805\u6781\u9a71\u52a8\u5668\uff0c\u5b9e\u73b0\u4e862.75kV\u7535\u6c14\u9694\u79bb\u548c\u8d85\u4f4e\u9694\u79bb\u7535\u5bb9\uff0c\u53ef\u540c\u65f6\u4f20\u8f93\u529f\u7387\u548c\u4fe1\u53f7\uff0c\u5728\u5bbd\u6e29\u5ea6\u8303\u56f4\u5185\u5de5\u4f5c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u901a\u8fc7\u7edf\u4e00\u901a\u9053\u540c\u65f6\u4f20\u8f93\u529f\u7387\u548c\u4fe1\u53f7\uff0c\u7535\u6c14\u9694\u79bb\u5bf9\u4e8e\u786e\u4fdd\u5b89\u5168\u548c\u51cf\u5c11\u7535\u78c1\u5e72\u6270\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5fae\u6ce2\u9891\u7387\u8868\u9762\u58f0\u6ce2\u5668\u4ef6\u5728\u94cc\u9178\u9502\u57fa\u677f\u4e0a\u6784\u5efa\u673a\u68b0\u9694\u79bb\u6805\u6781\u9a71\u52a8\u5668\uff0c\u5229\u7528\u58f0\u6ce2\u4f20\u64ad\u5b9e\u73b0\u7535\u6c14\u9694\u79bb\u3002", "result": "\u5b9e\u73b0\u4e862.75kV\u7684\u7535\u6c14\u9694\u79bb\u30010.032pF\u7684\u8d85\u4f4e\u9694\u79bb\u7535\u5bb9\uff0c\u57281.25mm\u673a\u68b0\u4f20\u64ad\u957f\u5ea6\u4e0a\u63d0\u4f9b13.4V\u5f00\u8def\u7535\u538b\u548c44.4mA\u77ed\u8def\u7535\u6d41\uff0c\u6c2e\u5316\u9553\u6676\u4f53\u7ba1\u5f00\u542f\u65f6\u95f4\u4e3a108.8ns\u3002", "conclusion": "\u5fae\u6ce2\u9891\u7387\u8868\u9762\u58f0\u6ce2\u5668\u4ef6\u63d0\u4f9b\u56fa\u6709\u7684\u7535\u78c1\u5e72\u6270\u514d\u75ab\u6027\uff0c\u53ef\u5728\u591a\u79cd\u534a\u5bfc\u4f53\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5f02\u8d28\u96c6\u6210\uff0c\u4e3a\u5148\u8fdb\u7535\u529b\u7535\u5b50\u63d0\u4f9b\u7d27\u51d1\u3001\u9ad8\u6027\u80fd\u7684\u9694\u79bb\u529f\u7387\u548c\u4fe1\u53f7\u4f20\u8f93\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13553", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13553", "abs": "https://arxiv.org/abs/2511.13553", "authors": ["Frederik Zuiderveen Borgesius", "Axel Arnbak"], "title": "New Data Security Requirements and the Proceduralization of Mass Surveillance Law after the European Data Retention Case", "comment": null, "summary": "This paper discusses the regulation of mass metadata surveillance in Europe through the lens of the landmark judgment in which the Court of Justice of the European Union struck down the Data Retention Directive. The controversial directive obliged telecom and Internet access providers in Europe to retain metadata of all their customers for intelligence and law enforcement purposes, for a period of up to two years. In the ruling, the Court declared the directive in violation of the human rights to privacy and data protection. The Court also confirmed that the mere collection of metadata interferes with the human right to privacy. In addition, the Court developed three new criteria for assessing the level of data security required from a human rights perspective: security measures should take into account the risk of unlawful access to data, and the data's quantity and sensitivity. While organizations that campaigned against the directive have welcomed the ruling, we warn for the risk of proceduralization of mass surveillance law. The Court did not fully condemn mass surveillance that relies on metadata, but left open the possibility of mass surveillance if policymakers lay down sufficient procedural safeguards. Such proceduralization brings systematic risks for human rights. Government agencies, with ample resources, can design complicated systems of procedural oversight for mass surveillance - and claim that mass surveillance is lawful, even if it affects millions of innocent people.", "AI": {"tldr": "\u6b27\u76df\u6cd5\u9662\u5e9f\u9664\u6570\u636e\u4fdd\u7559\u6307\u4ee4\u7684\u5224\u51b3\u786e\u7acb\u4e86\u5143\u6570\u636e\u6536\u96c6\u4fb5\u72af\u9690\u79c1\u6743\uff0c\u5e76\u63d0\u51fa\u4e86\u6570\u636e\u5b89\u5168\u8bc4\u4f30\u65b0\u6807\u51c6\uff0c\u4f46\u53ef\u80fd\u5e26\u6765\u5927\u89c4\u6a21\u76d1\u63a7\u7684\u7a0b\u5e8f\u5316\u98ce\u9669\u3002", "motivation": "\u5206\u6790\u6b27\u76df\u6cd5\u9662\u5e9f\u9664\u6570\u636e\u4fdd\u7559\u6307\u4ee4\u8fd9\u4e00\u91cc\u7a0b\u7891\u5224\u51b3\u5bf9\u5927\u89c4\u6a21\u5143\u6570\u636e\u76d1\u63a7\u76d1\u7ba1\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u8be5\u5224\u51b3\u5728\u4fdd\u62a4\u4eba\u6743\u7684\u540c\u65f6\u53ef\u80fd\u5e26\u6765\u7684\u7a0b\u5e8f\u5316\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b27\u76df\u6cd5\u9662\u5bf9\u6570\u636e\u4fdd\u7559\u6307\u4ee4\u7684\u5224\u51b3\u5185\u5bb9\uff0c\u63a2\u8ba8\u6cd5\u9662\u786e\u7acb\u7684\u6cd5\u5f8b\u539f\u5219\u3001\u6570\u636e\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\uff0c\u4ee5\u53ca\u5224\u51b3\u53ef\u80fd\u5f15\u53d1\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u3002", "result": "\u6cd5\u9662\u786e\u8ba4\u5143\u6570\u636e\u6536\u96c6\u4fb5\u72af\u9690\u79c1\u6743\uff0c\u63d0\u51fa\u4e86\u8003\u8651\u975e\u6cd5\u8bbf\u95ee\u98ce\u9669\u3001\u6570\u636e\u91cf\u548c\u654f\u611f\u6027\u7684\u6570\u636e\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\uff0c\u4f46\u4e3a\u5927\u89c4\u6a21\u76d1\u63a7\u7559\u4e0b\u4e86\u7a0b\u5e8f\u5316\u7a7a\u95f4\u3002", "conclusion": "\u867d\u7136\u5224\u51b3\u5f3a\u5316\u4e86\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46\u901a\u8fc7\u7a0b\u5e8f\u5316\u8def\u5f84\u53ef\u80fd\u4f7f\u5927\u89c4\u6a21\u76d1\u63a7\u5408\u6cd5\u5316\uff0c\u5e26\u6765\u7cfb\u7edf\u6027\u4eba\u6743\u98ce\u9669\uff0c\u9700\u8981\u8b66\u60d5\u653f\u5e9c\u673a\u6784\u5229\u7528\u590d\u6742\u7a0b\u5e8f\u4f53\u7cfb\u4e3a\u5927\u89c4\u6a21\u76d1\u63a7\u8fa9\u62a4\u3002"}}
{"id": "2511.12464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12464", "abs": "https://arxiv.org/abs/2511.12464", "authors": ["Chenglong Wang", "Yifu Huo", "Yang Gan", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Chunliang Zhang", "Tongran Liu", "Anxiang Ma", "Zhengtao Yu", "Jingbo Zhu", "Tong Xiao"], "title": "Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models", "comment": "Accepted by AAAI 2026", "summary": "Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86MRMBench\u57fa\u51c6\u548c\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u5728\u591a\u7ef4\u5ea6\u504f\u597d\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u4e0eLLM\u5bf9\u9f50\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u63ed\u793a\u4e86\u5956\u52b1\u6a21\u578b\u5728\u591a\u7ef4\u5ea6\u504f\u597d\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u5728\u56fa\u5b9a\u6210\u5bf9\u6392\u5e8f\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\uff0c\u4f46\u65e0\u6cd5\u63d0\u4f9b\u5404\u504f\u597d\u7ef4\u5ea6\u7684\u6027\u80fd\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86MRMBench\u57fa\u51c6\uff0c\u5305\u542b6\u4e2a\u4e0d\u540c\u504f\u597d\u7ef4\u5ea6\u7684\u63a2\u6d4b\u4efb\u52a1\uff1b\u63d0\u51fa\u4e86\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\uff0c\u8bc6\u522b\u5956\u52b1\u9884\u6d4b\u65f6\u4f7f\u7528\u7684\u7ef4\u5ea6\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "MRMBench\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u80fd\u5f3a\u76f8\u5173\uff1b\u5956\u52b1\u6a21\u578b\u5728\u591a\u7ef4\u5ea6\u504f\u597d\u6355\u6349\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\u80fd\u53ef\u9760\u8bc4\u4f30\u5956\u52b1\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "MRMBench\u662f\u5f00\u53d1\u5148\u8fdb\u5956\u52b1\u6a21\u578b\u7684\u53ef\u9760\u53c2\u8003\uff1b\u591a\u76ee\u6807\u4f18\u5316\u5728\u5956\u52b1\u5efa\u6a21\u4e2d\u5177\u6709\u6f5c\u529b\uff1b\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\u80fd\u63d0\u5347LLM\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2511.11652", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11652", "abs": "https://arxiv.org/abs/2511.11652", "authors": ["Marvin Plein", "Carsten F. Dormann", "Andreas Christen"], "title": "How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity", "comment": null, "summary": "Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9010\u6b65\u79fb\u9664\u6c14\u8c61\u7ad9\u7684\u65b9\u6cd5\u6765\u7cbe\u7b80\u5f17\u83b1\u5821\u7684\u57ce\u5e02\u6c14\u8c61\u7ad9\u7f51\u7edc\uff0c\u8bc1\u660e\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u53ef\u4ee5\u5c06\u6c14\u8c61\u7ad9\u6570\u91cf\u4ece42\u4e2a\u5927\u5e45\u51cf\u5c11\u52304\u4e2a\u3002", "motivation": "\u57ce\u5e02\u6c14\u8c61\u7ad9\u7f51\u7edc\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u627e\u5230\u5728\u4fdd\u6301\u76d1\u6d4b\u80fd\u529b\u7684\u540c\u65f6\u51cf\u5c11\u7ad9\u70b9\u6570\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9010\u6b65\u79fb\u9664\u7ad9\u70b9\u7684\u7a0b\u5e8f\uff0c\u5206\u6790\u4e0d\u540c\u5b50\u96c6\u7f51\u7edc\u91cd\u5efa\u539f\u59cb\u7f51\u7edc\u7a7a\u6c14\u6e29\u5ea6\u548c\u6e7f\u5ea6\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u5148\u8fdb\u7684\u6570\u503c\u57ce\u5e02\u5730\u8868\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7ad9\u70b9\u6570\u91cf\u4ece42\u4e2a\u51cf\u5c11\u52304\u4e2a\u65f6\uff0c\u7a7a\u6c14\u6e29\u5ea6\u9884\u6d4bRMSE\u4ece0.69K\u589e\u52a0\u52300.83K\uff0c\u76f8\u5bf9\u6e7f\u5ea6\u4ece3.8%\u589e\u52a0\u52304.4%\uff0c\u5206\u522b\u4ec5\u589e\u52a020%\u548c16%\u3002\u68ee\u6797\u7ad9\u70b9\u7684\u9884\u6d4b\u7cbe\u5ea6\u8f83\u5dee\uff0c\u4f46\u59cb\u7ec8\u4f18\u4e8e\u6570\u503c\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7cbe\u7b80\u6c14\u8c61\u7ad9\u7f51\u7edc\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u4ee5\u6700\u5927\u5316\u57ce\u5e02\u6c14\u5019\u7814\u7a76\u4e2d\u8d22\u52a1\u548c\u4eba\u529b\u8d44\u6e90\u7684\u5206\u914d\u6548\u7387\u3002"}}
{"id": "2511.12241", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12241", "abs": "https://arxiv.org/abs/2511.12241", "authors": ["Junhyuk Seo", "Hyeyoon Moon", "Kyu-Hwan Jung", "Namkee Oh", "Taerim Kim"], "title": "AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos", "comment": "12 pages, 5 figures", "summary": "Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.", "AI": {"tldr": "AURA\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u610f\u5916\u62d4\u7ba1\u98ce\u9669\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5b8c\u5168\u5728\u5408\u6210ICU\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u5f00\u53d1\u548c\u9a8c\u8bc1\uff0c\u901a\u8fc7\u59ff\u6001\u4f30\u8ba1\u8bc6\u522b\u78b0\u649e\u548c\u8e81\u52a8\u4e24\u79cd\u9ad8\u98ce\u9669\u8fd0\u52a8\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86ICU\u89c6\u9891\u6570\u636e\u83b7\u53d6\u7684\u9690\u79c1\u548c\u4f26\u7406\u6311\u6218\u3002", "motivation": "ICU\u4e2d\u610f\u5916\u62d4\u7ba1\u662f\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u4f26\u7406\u548c\u9690\u79c1\u9650\u5236\u96be\u4ee5\u83b7\u53d6\u6807\u6ce8\u7684ICU\u89c6\u9891\u6570\u636e\uff0c\u9700\u8981\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u7684\u5b9e\u65f6\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u751f\u6210\u591a\u6837\u5316\u7684\u4e34\u5e8a\u771f\u5b9eICU\u573a\u666f\uff0c\u901a\u8fc7\u59ff\u6001\u4f30\u8ba1\u8bc6\u522b\u624b\u90e8\u8fdb\u5165\u6c14\u9053\u7ba1\u9644\u8fd1\u533a\u57df\u7684\u78b0\u649e\u884c\u4e3a\u548c\u57fa\u4e8e\u89e3\u5256\u5173\u952e\u70b9\u901f\u5ea6\u7684\u8e81\u52a8\u884c\u4e3a\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u786e\u8ba4\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\uff0c\u6027\u80fd\u8bc4\u4f30\u663e\u793a\u78b0\u649e\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff0c\u8e81\u52a8\u8bc6\u522b\u8868\u73b0\u4e2d\u7b49\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u590d\u73b0\u7684\u60a3\u8005\u5b89\u5168\u76d1\u63a7\u7cfb\u7edf\u7684\u65b0\u9014\u5f84\uff0c\u5177\u6709\u5728ICU\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13465", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13465", "abs": "https://arxiv.org/abs/2511.13465", "authors": ["Meng Zhu", "Quan Xiao", "Weidong Min"], "title": "AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate", "comment": "25 pages, 6 figures, 12 tables", "summary": "Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdamX\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u578b\u4e8c\u9636\u77e9\u4f30\u8ba1\u6307\u6570\u8870\u51cf\u7387\uff0c\u5728\u8bad\u7ec3\u540e\u671f\u51cf\u5f31\u5b66\u4e60\u6b65\u957f\u4fee\u6b63\u5f3a\u5ea6\u5e76\u9000\u5316\u4e3aSGD\uff0c\u63d0\u5347\u7a33\u5b9a\u671f\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "Adam\u4f18\u5316\u5668\u5728\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\u4ecd\u662f\u4e3b\u6d41\uff0c\u4f46\u4e0eSGD\u76f8\u6bd4\u66f4\u5bb9\u6613\u6536\u655b\u5230\u975e\u5e73\u5766\u6781\u5c0f\u503c\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51faAdamX\u7b97\u6cd5\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u65b0\u578b\u4e8c\u9636\u77e9\u4f30\u8ba1\u6307\u6570\u8870\u51cf\u7387\uff0c\u968f\u7740\u8bad\u7ec3\u8fdb\u5c55\u9010\u6e10\u51cf\u5f31\u5b66\u4e60\u6b65\u957f\u4fee\u6b63\u5f3a\u5ea6\uff0c\u5728\u7a33\u5b9a\u8bad\u7ec3\u671f\u9000\u5316\u4e3aSGD\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b0\u7684\u4e8c\u9636\u77e9\u4f30\u8ba1\u6307\u6570\u8870\u51cf\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cAdamX\u5728\u6027\u80fd\u4e0a\u7a33\u5b9a\u4f18\u4e8eAdam\u53ca\u5176\u53d8\u4f53\u3002", "conclusion": "AdamX\u901a\u8fc7\u6539\u8fdb\u4e8c\u9636\u77e9\u4f30\u8ba1\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u53ef\u80fd\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5927\u6a21\u578b\u4f18\u5316\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2511.13424", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13424", "abs": "https://arxiv.org/abs/2511.13424", "authors": ["Bowen Tian", "Roel C. G. M. Loonen", "Roland M. E. Valckenborg", "Jan L. M. Hensen"], "title": "High-resolution hierarchical PV system performance modeling in urban environments", "comment": "This manuscript has been submitted to Energy Conversion and Management for peer review. 65 pages, 22 figures", "summary": "Accurate performance modeling of PV systems in urban environments is a significant challenge due to complex partial shading. This study introduces a high-resolution, hierarchical modeling framework that provides detailed insights from the solar cell to the system level. Rigorously validated against field-test data from calibrated equipment, the model demonstrates high accuracy in predicting minute-wised dynamic electrical characteristics (R2 > 0.90). A key finding is the critical shortcoming of conventional, coarser-resolution models under realistic shading; these are shown to overestimate the actual string operating power by up to 163% and the monthly energy yield by up to 54%. The proposed framework avoids these errors by precisely capturing mismatch losses and the time-varying phenomena of system components, such as bypass diode activations. Furthermore, the model accurately quantifies the effectiveness of mitigation technologies, showing that Module-Level Power Electronics (MLPEs) can increase the monthly energy yield of a heavily shaded string by over 20%. This research provides a crucial tool for reliable system design, accurate power forecasting, and the optimization of PV systems in complex urban settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5206\u8fa8\u7387\u5206\u5c42\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u57ce\u5e02\u73af\u5883\u4e2d\u5149\u4f0f\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u7279\u522b\u89e3\u51b3\u4e86\u590d\u6742\u90e8\u5206\u9634\u5f71\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002", "motivation": "\u57ce\u5e02\u73af\u5883\u4e2d\u5149\u4f0f\u7cfb\u7edf\u9762\u4e34\u590d\u6742\u90e8\u5206\u9634\u5f71\u95ee\u9898\uff0c\u4f20\u7edf\u7c97\u5206\u8fa8\u7387\u6a21\u578b\u5728\u9884\u6d4b\u5b9e\u9645\u6027\u80fd\u65f6\u5b58\u5728\u4e25\u91cd\u8bef\u5dee\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9ad8\u5206\u8fa8\u7387\u5206\u5c42\u5efa\u6a21\u6846\u67b6\uff0c\u4ece\u592a\u9633\u80fd\u7535\u6c60\u5230\u7cfb\u7edf\u7ea7\u8fdb\u884c\u8be6\u7ec6\u5efa\u6a21\uff0c\u901a\u8fc7\u73b0\u573a\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u4e25\u683c\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u5206\u949f\u7ea7\u52a8\u6001\u7535\u6c14\u7279\u6027\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff08R2 > 0.90\uff09\uff0c\u53d1\u73b0\u4f20\u7edf\u6a21\u578b\u4f1a\u9ad8\u4f30\u8fd0\u884c\u529f\u7387\u8fbe163%\uff0c\u6708\u53d1\u7535\u91cf\u8fbe54%\u3002\u6a21\u5757\u7ea7\u7535\u529b\u7535\u5b50\u6280\u672f\u53ef\u5c06\u91cd\u9634\u5f71\u4e32\u7684\u6708\u53d1\u7535\u91cf\u63d0\u9ad820%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5149\u4f0f\u7cfb\u7edf\u7684\u53ef\u9760\u8bbe\u8ba1\u3001\u51c6\u786e\u529f\u7387\u9884\u6d4b\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\u3002"}}
{"id": "2511.13555", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13555", "abs": "https://arxiv.org/abs/2511.13555", "authors": ["Evelien Brouwer", "Frederik Zuiderveen Borgesius"], "title": "Access to Personal Data and the Right to Good Governance during Asylum Procedures after the CJEU's YS. and M. and S. judgment", "comment": null, "summary": "In the YS. and M. and S. judgment, the Court of Justice of the European Union ruled on three procedures in which Dutch judges asked for clarification on the right of asylum seekers to have access to the documents regarding the decision on asylum applications. The judgment is relevant for interpreting the concept of personal data and the scope of the right of access under the Data Protection Directive, and the right to good administration in the EU Charter of Fundamental Rights. At first glance, the judgment seems disappointing from the viewpoint of individual rights. Nevertheless, in our view the judgment provides sufficient grounds for effective access rights to the minutes in future asylum cases.", "AI": {"tldr": "\u6b27\u6d32\u6cd5\u9662\u5728YS\u3001M\u548cS\u6848\u4e2d\u88c1\u51b3\u4e86\u5bfb\u6c42\u5e87\u62a4\u8005\u83b7\u53d6\u5e87\u62a4\u7533\u8bf7\u51b3\u5b9a\u76f8\u5173\u6587\u4ef6\u7684\u6743\u5229\uff0c\u6d89\u53ca\u4e2a\u4eba\u6570\u636e\u6982\u5ff5\u548c\u6570\u636e\u4fdd\u62a4\u6307\u4ee4\u4e0b\u8bbf\u95ee\u6743\u8303\u56f4\u7684\u89e3\u91ca\u3002", "motivation": "\u5206\u6790\u6b27\u6d32\u6cd5\u9662\u5173\u4e8e\u5bfb\u6c42\u5e87\u62a4\u8005\u83b7\u53d6\u5e87\u62a4\u51b3\u5b9a\u6587\u4ef6\u6743\u5229\u7684\u88c1\u51b3\uff0c\u63a2\u8ba8\u5176\u5bf9\u4e2a\u4eba\u6570\u636e\u4fdd\u62a4\u548c\u826f\u597d\u884c\u653f\u6743\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b27\u6d32\u6cd5\u9662\u5728YS\u3001M\u548cS\u6848\u4e2d\u7684\u5224\u51b3\uff0c\u89e3\u8bfb\u6570\u636e\u4fdd\u62a4\u6307\u4ee4\u548c\u6b27\u76df\u57fa\u672c\u6743\u5229\u5baa\u7ae0\u7684\u76f8\u5173\u89c4\u5b9a\u3002", "result": "\u5224\u51b3\u8868\u9762\u4e0a\u770b\u4f3c\u5bf9\u4e2a\u4eba\u6743\u5229\u4e0d\u5229\uff0c\u4f46\u5b9e\u9645\u4e0a\u4e3a\u672a\u6765\u5e87\u62a4\u6848\u4ef6\u4e2d\u7684\u6709\u6548\u8bbf\u95ee\u6743\u63d0\u4f9b\u4e86\u5145\u5206\u4f9d\u636e\u3002", "conclusion": "\u8be5\u5224\u51b3\u4e3a\u5e87\u62a4\u6848\u4ef6\u4e2d\u83b7\u53d6\u4f1a\u8bae\u8bb0\u5f55\u7684\u6709\u6548\u8bbf\u95ee\u6743\u5960\u5b9a\u4e86\u6cd5\u5f8b\u57fa\u7840\uff0c\u5c3d\u7ba1\u521d\u770b\u4f3c\u4e4e\u9650\u5236\u4e86\u4e2a\u4eba\u6743\u5229\u3002"}}
{"id": "2511.12472", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12472", "abs": "https://arxiv.org/abs/2511.12472", "authors": ["Mengying Wang", "Chenhui Ma", "Ao Jiao", "Tuo Liang", "Pengjun Lu", "Shrinidhi Hegde", "Yu Yin", "Evren Gurkan-Cavusoglu", "Yinghui Wu"], "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing", "comment": "The 40th AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.", "AI": {"tldr": "\u63d0\u51fa\u4e86SerenQA\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u79d1\u5b66\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u53d1\u73b0\u610f\u5916\u6d1e\u5bdf\u7684\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u836f\u7269\u91cd\u5b9a\u4f4d\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7cfb\u7edf\u901a\u5e38\u53ea\u8fd4\u56de\u9ad8\u5ea6\u76f8\u5173\u4f46\u53ef\u9884\u6d4b\u7684\u7b54\u6848\uff0c\u7f3a\u4e4f\u53d1\u73b0\u610f\u5916\u548c\u65b0\u9896\uff08\"\u610f\u5916\u53d1\u73b0\"\uff09\u7b54\u6848\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faSerenQA\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u76f8\u5173\u6027\u3001\u65b0\u9896\u6027\u548c\u610f\u5916\u6027\u7684\u4e25\u683c\u610f\u5916\u53d1\u73b0\u6307\u6807\uff0c\u4ee5\u53ca\u4ece\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u4e13\u5bb6\u6807\u6ce8\u57fa\u51c6\uff0c\u5305\u542b\u77e5\u8bc6\u68c0\u7d22\u3001\u5b50\u56fe\u63a8\u7406\u548c\u610f\u5916\u53d1\u73b0\u63a2\u7d22\u4e09\u4e2a\u5b50\u4efb\u52a1\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLM\u5728\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bc6\u522b\u771f\u6b63\u4ee4\u4eba\u610f\u5916\u548c\u6709\u4ef7\u503c\u7684\u53d1\u73b0\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "LLM\u5728\u53d1\u73b0\u610f\u5916\u6d1e\u5bdf\u65b9\u9762\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.11654", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11654", "abs": "https://arxiv.org/abs/2511.11654", "authors": ["Sayambhu Sen", "Shalabh Bhatnagar"], "title": "Convergence of Multiagent Learning Systems for Traffic control", "comment": "14 pages 2 figures", "summary": "Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u6536\u655b\u6027\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u7ed9\u5b9a\u6761\u4ef6\u4e0b\u8be5\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "motivation": "\u968f\u7740\u73ed\u52a0\u7f57\u5c14\u7b49\u57ce\u5e02\u7684\u5feb\u901f\u57ce\u5e02\u5316\uff0c\u4ea4\u901a\u62e5\u5835\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u8bc1\u660e\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u7684\u4e25\u683c\u7406\u8bba\u5206\u6790\u3002", "method": "\u4f7f\u7528\u968f\u673a\u903c\u8fd1\u65b9\u6cd5\uff0c\u6b63\u5f0f\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u5c06\u5355\u667a\u80fd\u4f53\u5f02\u6b65\u503c\u8fed\u4ee3\u7684\u6536\u655b\u6027\u8bc1\u660e\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u573a\u666f\u3002", "result": "\u8bc1\u660e\u4e86\u7279\u5b9a\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4ea4\u901a\u63a7\u5236\u7b97\u6cd5\u5728\u7ed9\u5b9a\u6761\u4ef6\u4e0b\u80fd\u591f\u6536\u655b\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u63a7\u5236\u9886\u57df\u7406\u8bba\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3a\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2511.12254", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12254", "abs": "https://arxiv.org/abs/2511.12254", "authors": ["Yuxiang Zhou", "Jichang Li", "Yanhao Zhang", "Haonan Lu", "Guanbin Li"], "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation", "comment": null, "summary": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mobile-Agent-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7ea7\u68c0\u7d22\u589e\u5f3a\u89e3\u51b3\u79fb\u52a8\u4ee3\u7406\u5728\u957f\u65f6\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u6218\u7565\u5e7b\u89c9\u548c\u64cd\u4f5c\u9519\u8bef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6b65\u9aa4\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u79fb\u52a8\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u3001\u957f\u65f6\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4e0d\u8db3\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u8fc7\u5ea6\u4f9d\u8d56MLLM\u7684\u9759\u6001\u5185\u90e8\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6218\u7565\u5c42\u9762\u7684\u5e7b\u89c9\u548c\u64cd\u4f5c\u5c42\u9762\u7684\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u591a\u4ee3\u7406\u6846\u67b6Mobile-Agent-RAG\uff0c\u5305\u542bManager-RAG\u7528\u4e8e\u89c4\u5212\u9636\u6bb5\u68c0\u7d22\u4eba\u7c7b\u9a8c\u8bc1\u7684\u5168\u9762\u4efb\u52a1\u8ba1\u5212\uff0cOperator-RAG\u7528\u4e8e\u6267\u884c\u9636\u6bb5\u68c0\u7d22\u7cbe\u786e\u7684\u4f4e\u7ea7\u6307\u5bfc\uff0c\u5e76\u6784\u5efa\u4e86\u4e24\u4e2a\u4e13\u95e8\u7684\u68c0\u7d22\u77e5\u8bc6\u5e93\u3002", "result": "\u5728Mobile-Eval-RAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMobile-Agent-RAG\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u9ad811.0%\uff0c\u6b65\u9aa4\u6548\u7387\u63d0\u534710.2%\u3002", "conclusion": "Mobile-Agent-RAG\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u53ef\u9760\u7684\u591a\u4ee3\u7406\u79fb\u52a8\u81ea\u52a8\u5316\u5efa\u7acb\u4e86\u7a33\u5065\u7684\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u4ee3\u7406\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.13675", "categories": ["cs.LG", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13675", "abs": "https://arxiv.org/abs/2511.13675", "authors": ["Minh Vu", "Andrey Lokhov"], "title": "Scientific Data Compression and Super-Resolution Sampling", "comment": null, "summary": "Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u6307\u6570\u65cf\u7684\u65b0\u79d1\u5b66\u6570\u636e\u538b\u7f29\u4e0e\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u652f\u6301\u7269\u7406\u91cf\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5728\u538b\u7f29\u6bd4\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u95f4\u7075\u6d3b\u6743\u8861", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u6a21\u62df\u3001\u89c2\u6d4b\u548c\u5927\u89c4\u6a21\u5b9e\u9a8c\u4ea7\u751f\u7684\u6570\u636e\u91cf\u5e38\u8d85\u51fa\u5b58\u50a8\u3001\u5904\u7406\u548c\u5206\u6790\u80fd\u529b\u6781\u9650\uff0c\u9700\u5f00\u53d1\u80fd\u7ba1\u7406\u6d77\u91cf\u6570\u636e\u96c6\u540c\u65f6\u4fdd\u7559\u5173\u952e\u7269\u7406\u7279\u5f81\u7684\u6570\u636e\u7f29\u51cf\u65b9\u6cd5", "method": "\u57fa\u4e8e\u5b66\u4e60\u6307\u6570\u65cf\u7684\u65b0\u6846\u67b6\uff0c\u652f\u6301\u79d1\u5b66\u6570\u636e\u538b\u7f29\u548c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u53ef\u91cf\u5316\u7269\u7406\u91cf\u7684\u4e0d\u786e\u5b9a\u6027", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u4ece\u538b\u7f29\u8868\u793a\u4e2d\u6062\u590d\u6570\u636e\uff08\u8d85\u5206\u8fa8\u7387\uff09\uff0c\u5e76\u4fdd\u8bc1\u5173\u952e\u7269\u7406\u7279\u5f81\u7684\u4fdd\u7559", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79d1\u5b66\u6570\u636e\u538b\u7f29\u548c\u6062\u590d\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u68c0\u67e5\u70b9\u548c\u91cd\u542f\u7b49\u9700\u8981\u6570\u636e\u6062\u590d\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41"}}
{"id": "2511.13429", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13429", "abs": "https://arxiv.org/abs/2511.13429", "authors": ["Yuqi Ping", "Tingting Zhang", "Tianhao Liang"], "title": "Handover-Aware URLLC UAV Trajectory Planning: A Continuous-Time Trajectory Optimization via Graphs of Convex Sets", "comment": "submited to IEEE International Conference on Communications", "summary": "In this paper, we study a cellular-connected unmanned aerial vehicle (UAV) which aims to fly between two predetermined locations while maintaining ultra-reliable low-latency communications (URLLC) for command-and-control (C2) links with terrestrial base stations (BSs). Long-range flights often trigger frequent inter-cell handovers, which may introduce delays and synchronization overhead. We jointly optimize the continuous trajectory and BS association to minimize handovers, path length, and flying time, subject to communication reliability and kinematic constraints. To address this problem, we reformulate it as an optimization based on the graph of convex sets (GCS). First, the URLLC requirement is translated into spatially feasible regions in the flight plane for each BS. And an intersection graph is constructed including the start and goal points. Each graph node is associated with a smooth and dynamically feasible trajectory segment. The trajectory is parameterized in space by B\u00e9zier curves and in time by a monotonic B\u00e9zier scaling, together with convex constraints that ensure continuity and enforce speed bounds. Next, we impose unit-flow constraints to enforce a single path, and by coupling the resulting binary edge-selection variables with the convex constraints, we obtain a mixed-integer convex program (MICP). Applying a convex relaxation and rounding to the mixed-integer convex program produces nearly globally optimal routes, and a final refinement yields smooth, dynamically feasible trajectories. Simulations verify that the method preserves URLLC connectivity while achieving a clear trade-off between fewer handovers and flight efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u65e0\u4eba\u673a\u5728\u4fdd\u6301\u4e0e\u5730\u9762\u57fa\u7ad9\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u7684\u540c\u65f6\uff0c\u4f18\u5316\u98de\u884c\u8f68\u8ff9\u548c\u57fa\u7ad9\u5173\u8054\uff0c\u4ee5\u51cf\u5c11\u5207\u6362\u6b21\u6570\u3001\u8def\u5f84\u957f\u5ea6\u548c\u98de\u884c\u65f6\u95f4\u3002", "motivation": "\u957f\u8ddd\u79bb\u98de\u884c\u4f1a\u89e6\u53d1\u9891\u7e41\u7684\u5c0f\u533a\u95f4\u5207\u6362\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u548c\u540c\u6b65\u5f00\u9500\u3002\u9700\u8981\u540c\u65f6\u4f18\u5316\u8f68\u8ff9\u548c\u57fa\u7ad9\u5173\u8054\u6765\u7ef4\u6301URLLC\u8fde\u63a5\u3002", "method": "\u5c06\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u57fa\u4e8e\u51f8\u96c6\u56fe\u7684\u4f18\u5316\u95ee\u9898\uff0c\u6784\u5efa\u4ea4\u96c6\u56fe\uff0c\u4f7f\u7528\u8d1d\u585e\u5c14\u66f2\u7ebf\u53c2\u6570\u5316\u8f68\u8ff9\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u51f8\u89c4\u5212\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301URLLC\u8fde\u63a5\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8f83\u5c11\u5207\u6362\u6b21\u6570\u548c\u98de\u884c\u6548\u7387\u4e4b\u95f4\u7684\u660e\u786e\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5e73\u6ed1\u3001\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u5728\u901a\u4fe1\u53ef\u9760\u6027\u548c\u98de\u884c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2511.13557", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13557", "abs": "https://arxiv.org/abs/2511.13557", "authors": ["Stefan Kulk", "Frederik Zuiderveen Borgesius"], "title": "Freedom of expression and 'right to be forgotten' cases in the Netherlands after Google Spain", "comment": null, "summary": "Since the Google Spain judgment of the Court of Justice of the European Union, Europeans have, under certain conditions, the right to have search results for their name delisted. This paper examines how the Google Spain judgment has been applied in the Netherlands. Since the Google Spain judgment, Dutch courts have decided on two cases regarding delisting requests. In both cases, the Dutch courts considered freedom of expression aspects of delisting more thoroughly than the Court of Justice. However, the effect of the Google Spain judgment on freedom of expression is difficult to assess, as search engine operators decide about most delisting requests without disclosing much about their decisions.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6b27\u76df\u6cd5\u9662Google Spain\u5224\u51b3\u5728\u8377\u5170\u7684\u5e94\u7528\u60c5\u51b5\uff0c\u91cd\u70b9\u5173\u6ce8\u5220\u9664\u641c\u7d22\u7ed3\u679c\u8bf7\u6c42\u7684\u5904\u7406\u65b9\u5f0f\u53ca\u5176\u5bf9\u8a00\u8bba\u81ea\u7531\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76Google Spain\u5224\u51b3\u540e\u8377\u5170\u6cd5\u9662\u5982\u4f55\u5e94\u7528\u5220\u9664\u641c\u7d22\u7ed3\u679c\u7684\u6743\u5229\uff0c\u7279\u522b\u5173\u6ce8\u8a00\u8bba\u81ea\u7531\u65b9\u9762\u7684\u8003\u91cf\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8377\u5170\u6cd5\u9662\u5904\u7406\u7684\u4e24\u4e2a\u5220\u9664\u8bf7\u6c42\u6848\u4f8b\uff0c\u6bd4\u8f83\u8377\u5170\u6cd5\u9662\u4e0e\u6b27\u76df\u6cd5\u9662\u5728\u8a00\u8bba\u81ea\u7531\u8003\u91cf\u4e0a\u7684\u5dee\u5f02\u3002", "result": "\u8377\u5170\u6cd5\u9662\u6bd4\u6b27\u76df\u6cd5\u9662\u66f4\u6df1\u5165\u5730\u8003\u8651\u4e86\u5220\u9664\u641c\u7d22\u7ed3\u679c\u5bf9\u8a00\u8bba\u81ea\u7531\u7684\u5f71\u54cd\uff0c\u4f46\u7531\u4e8e\u641c\u7d22\u5f15\u64ce\u8fd0\u8425\u5546\u51b3\u7b56\u4e0d\u900f\u660e\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u8be5\u5224\u51b3\u5bf9\u8a00\u8bba\u81ea\u7531\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "conclusion": "Google Spain\u5224\u51b3\u5728\u8377\u5170\u7684\u5e94\u7528\u663e\u793a\u6cd5\u9662\u66f4\u91cd\u89c6\u8a00\u8bba\u81ea\u7531\u8003\u91cf\uff0c\u4f46\u641c\u7d22\u5f15\u64ce\u8fd0\u8425\u5546\u51b3\u7b56\u7684\u4e0d\u900f\u660e\u6027\u9650\u5236\u4e86\u5bf9\u5176\u5f71\u54cd\u7684\u5168\u9762\u8bc4\u4f30\u3002"}}
{"id": "2511.12788", "categories": ["cs.LG", "cs.AR", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12788", "abs": "https://arxiv.org/abs/2511.12788", "authors": ["Rub\u00e9n Dar\u00edo Guerrero"], "title": "Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data", "comment": "32 pages, 21 figures, 10 tables", "summary": "The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\\boldsymbol\u03b8 = \\{\u03b8_d, \u03b8_a, \u03b8_b, \u03b8_p, \u03b8_c\\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7269\u7406\u7ea6\u675f\u81ea\u9002\u5e94\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u53c2\u6570\u81ea\u52a8\u6821\u51c6\u7535\u78c1\u8fd1\u4f3c\uff0c\u5728\u6781\u7d2b\u5916\u5149\u523b\u4f18\u5316\u4e2d\u5b9e\u73b0\u4e9a\u7eb3\u7c73\u7cbe\u5ea6\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u8de8\u51e0\u4f55\u6cdb\u5316\u3002", "motivation": "\u534a\u5bfc\u4f53\u884c\u4e1a\u5728\u6781\u7d2b\u5916\u5149\u523b\u4f18\u5316\u4e2d\u9762\u4e34\u8ba1\u7b97\u5371\u673a\uff0c\u4f20\u7edf\u65b9\u6cd5\u6d88\u8017\u6570\u5341\u4ebfCPU\u5c0f\u65f6\u4e14\u65e0\u6cd5\u8fbe\u5230\u4e9a\u7eb3\u7c73\u7cbe\u5ea6\uff0c\u9700\u8981\u89e3\u51b3\u5b66\u672f\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e0e\u5de5\u4e1a\u90e8\u7f72\u9700\u6c42\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u96c6\u6210\u53ef\u5fae\u5206\u6a21\u5757\uff08\u83f2\u6d85\u5c14\u884d\u5c04\u3001\u6750\u6599\u5438\u6536\u3001\u5149\u5b66\u70b9\u6269\u6563\u51fd\u6570\u6a21\u7cca\u3001\u76f8\u79fb\u6548\u5e94\u3001\u5bf9\u6bd4\u5ea6\u8c03\u5236\uff09\u4e0e\u76f4\u63a5\u51e0\u4f55\u56fe\u6848\u5339\u914d\u76ee\u6807\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u5b66\u4e60\u81ea\u52a8\u6821\u51c6\u7535\u78c1\u8fd1\u4f3c\u53c2\u6570\u3002", "result": "\u572815\u4e2a\u4ee3\u8868\u6027\u56fe\u6848\u4e0a\u5b9e\u73b0\u4e00\u81f4\u7684\u4e9a\u7eb3\u7c73\u8fb9\u7f18\u653e\u7f6e\u8bef\u5dee\u6027\u80fd\uff080.664-2.536 nm\u8303\u56f4\uff09\uff0c\u4ec5\u9700\u6bcf\u4e2a\u56fe\u684850\u4e2a\u8bad\u7ec3\u6837\u672c\uff0c\u6bd4\u65e0\u7269\u7406\u7ea6\u675f\u7684CNN\u57fa\u7ebf\u5e73\u5747\u6539\u8fdb69.9%\uff0c\u8bad\u7ec3\u5b8c\u6210\u540e\u63a8\u7406\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u4e25\u683c\u7535\u78c1\u6c42\u89e3\u5668\u3002", "conclusion": "\u7269\u7406\u7ea6\u675f\u81ea\u9002\u5e94\u5b66\u4e60\u4e3a\u5b9e\u65f6\u534a\u5bfc\u4f53\u5236\u9020\u4f18\u5316\u5efa\u7acb\u4e86\u57fa\u7840\u65b9\u6cd5\u5b66\uff0c\u901a\u8fc7\u8054\u5408\u7269\u7406\u6821\u51c6\u548c\u5236\u9020\u7cbe\u5ea6\u76ee\u6807\uff0c\u4ec5\u9700\u6a21\u5f0f\u7279\u5b9aCNN\u8bad\u7ec3\u65b9\u6cd590%\u7684\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u8de8\u51e0\u4f55\u6cdb\u5316\u3002"}}
{"id": "2511.12497", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12497", "abs": "https://arxiv.org/abs/2511.12497", "authors": ["JoonHo Lee", "HyeonMin Cho", "Jaewoong Yun", "Hyunjae Lee", "JunKyu Lee", "Juree Seok"], "title": "SGuard-v1: Safety Guardrail for Large Language Models", "comment": "Technical Report", "summary": "We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.", "AI": {"tldr": "SGuard-v1\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u62a4\u680f\u7cfb\u7edf\uff0c\u5305\u542bContentFilter\u548cJailbreakFilter\u4e24\u4e2a\u4e13\u95e8\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u548c\u7b5b\u9009\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u652f\u630112\u79cd\u8bed\u8a00\uff0c\u57fa\u4e8e2B\u53c2\u6570\u7684Granite\u6a21\u578b\u6784\u5efa\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b-AI\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u548c\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u9632\u62a4\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e2B\u53c2\u6570\u7684Granite-3.3-2B-Instruct\u6a21\u578b\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u6784\u5efa\u4e24\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1aContentFilter\u57fa\u4e8eMLCommons\u5371\u5bb3\u5206\u7c7b\u5b66\u8bc6\u522b\u5b89\u5168\u98ce\u9669\uff0cJailbreakFilter\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bfe\u7a0b\u5b66\u4e60\u8986\u76d660\u79cd\u4e3b\u8981\u653b\u51fb\u7c7b\u578b\u3002", "result": "\u5728\u516c\u5171\u548c\u4e13\u6709\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5b89\u5168\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u90e8\u7f72\uff0c\u63d0\u4f9b\u591a\u7c7b\u5b89\u5168\u9884\u6d4b\u548c\u4e8c\u8fdb\u5236\u7f6e\u4fe1\u5ea6\u5206\u6570\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "SGuard-v1\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u5728Apache-2.0\u8bb8\u53ef\u4e0b\u53d1\u5e03\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2511.11656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11656", "abs": "https://arxiv.org/abs/2511.11656", "authors": ["Luca Marzari", "Manuele Bicego", "Ferdinando Cicalese", "Alessandro Farinelli"], "title": "On the Probabilistic Learnability of Compact Neural Network Preimage Bounds", "comment": "Accepted at the 40th Annual AAAI Conference on Artificial Intelligence 2026", "summary": "Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\\textbf{R}$andom $\\textbf{F}$orest $\\textbf{Pro}$perty $\\textbf{Ve}$rifier ($\\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.", "AI": {"tldr": "\u63d0\u51faRF-ProVe\u65b9\u6cd5\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u548c\u4e3b\u52a8\u91cd\u91c7\u6837\u6765\u9ad8\u6548\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u9884\u50cf\u7684\u8fd1\u4f3c\u8fb9\u754c\uff0c\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u53ef\u8bc1\u660e\u65b9\u6cd5\u53d7\u9650\u4e8e#P\u96be\u95ee\u9898\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u9ad8\u7f6e\u4fe1\u5ea6\u4fdd\u8bc1\u7684\u5b9e\u7528\u65b9\u6cd5", "method": "\u4f7f\u7528\u968f\u673a\u68ee\u6797\u96c6\u6210\u65b9\u6cd5\u751f\u6210\u6ee1\u8db3\u8f93\u51fa\u5c5e\u6027\u7684\u5019\u9009\u8f93\u5165\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u91cd\u91c7\u6837\u8fdb\u884c\u7cbe\u70bc", "result": "\u65b9\u6cd5\u80fd\u591f\u6355\u83b7\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u590d\u6742\u6a21\u5f0f\uff0c\u63d0\u4f9b\u533a\u57df\u7eaf\u5ea6\u548c\u5168\u5c40\u8986\u76d6\u7684\u7edf\u8ba1\u4fdd\u8bc1", "conclusion": "RF-ProVe\u4e3a\u8ba1\u7b97\u7d27\u51d1\u9884\u50cf\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7cbe\u786e\u6c42\u89e3\u5668\u65e0\u6cd5\u6269\u5c55\u7684\u60c5\u51b5\u4e0b\u7279\u522b\u6709\u6548"}}
{"id": "2511.12271", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12271", "abs": "https://arxiv.org/abs/2511.12271", "authors": ["Zhiyu An", "Wan Du"], "title": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning", "comment": "Accepted for AAAI 2026", "summary": "Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3LLM\u9053\u5fb7\u5bf9\u9f50\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b680\u4e2a\u9053\u5fb7\u573a\u666f\u7684\u6570\u636e\u96c6Moral-Reason-QA\uff0c\u4f7f\u7528Group Relative Policy Optimization\u7b97\u6cd5\u8bad\u7ec3LLM\u5728\u672a\u89c1\u8fc7\u7684\u9053\u5fb7\u573a\u666f\u4e2d\u5e94\u7528\u4e00\u81f4\u7684\u9053\u5fb7\u63a8\u7406\u6846\u67b6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u7c7b\u9053\u5fb7\u51b3\u7b56\u7684\u5f71\u54cd\u65e5\u76ca\u589e\u5f3a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bc4\u4f30\u800c\u975e\u4e3b\u52a8\u5f15\u5bfc\u5176\u9053\u5fb7\u51b3\u7b56\uff0c\u9700\u8981\u89e3\u51b3\u5206\u5e03\u5916\u9053\u5fb7\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u6784\u5efaMoral-Reason-QA\u6570\u636e\u96c6\uff0c\u5305\u542b\u529f\u5229\u4e3b\u4e49\u3001\u4e49\u52a1\u8bba\u548c\u7f8e\u5fb7\u4f26\u7406\u5b66\u6846\u67b6\u7684\u63a8\u7406\u8f68\u8ff9\uff1b\u4f7f\u7528Group Relative Policy Optimization\u7b97\u6cd5\uff0c\u901a\u8fc7\u590d\u5408\u5956\u52b1\u540c\u65f6\u4f18\u5316\u51b3\u7b56\u5bf9\u9f50\u548c\u6846\u67b6\u7279\u5b9a\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u5206\u5e03\u5916\u8bc4\u4f30\u96c6\u4e0a\uff0c\u529f\u5229\u4e3b\u4e49\u6846\u67b6\u7684softmax\u5f52\u4e00\u5316\u5bf9\u9f50\u5206\u6570\u63d0\u9ad8\u4e86+0.757\uff0c\u4e49\u52a1\u8bba\u6846\u67b6\u63d0\u9ad8\u4e86+0.450\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u9053\u5fb7\u573a\u666f\u7684\u6cdb\u5316\u3002", "conclusion": "LLM\u4ee3\u7406\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u8bad\u7ec3\u4ee5\u5185\u5728\u5316\u5e76\u5e94\u7528\u7279\u5b9a\u9053\u5fb7\u6846\u67b6\u5230\u65b0\u60c5\u5883\uff0c\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u3002"}}
{"id": "2511.13699", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13699", "abs": "https://arxiv.org/abs/2511.13699", "authors": ["Parikshit Gopalan", "Konstantinos Stavropoulos", "Kunal Talwar", "Pranay Tankala"], "title": "Efficient Calibration for Decision Making", "comment": "50 pages, 3 figures", "summary": "A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.\n  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u540e\u5904\u7406\u51fd\u6570\u65cf\u7684\u6821\u51c6\u51b3\u7b56\u635f\u5931(CDL_K)\u6982\u5ff5\uff0c\u4ee5\u89e3\u51b3\u539f\u59cbCDL\u96be\u4ee5\u8fd1\u4f3c\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u8be5\u5ea6\u91cf\u7684\u7406\u8bba\u548c\u8ba1\u7b97\u53ef\u5904\u7406\u6027\u5206\u6790\u3002", "motivation": "\u539f\u59cb\u6821\u51c6\u51b3\u7b56\u635f\u5931(CDL)\u5728\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u5373\u4f7f\u7ed9\u5b9a\u9884\u6d4b\u548c\u6807\u7b7e\u7684\u9ed1\u76d2\u8bbf\u95ee\u3002\u4e3a\u4e86\u89c4\u907f\u8fd9\u4e00\u56f0\u96be\uff0c\u9700\u8981\u5c06\u6ce8\u610f\u529b\u9650\u5236\u5728\u7ed3\u6784\u5316\u7684\u540e\u5904\u7406\u51fd\u6570\u65cf\u4e0a\u3002", "method": "\u5b9a\u4e49\u76f8\u5bf9\u7ed3\u6784\u5316\u51fd\u6570\u65cfK\u7684\u6821\u51c6\u51b3\u7b56\u635f\u5931CDL_K\uff0c\u8003\u8651\u6240\u6709\u9002\u5f53\u635f\u5931\u4f46\u5c06\u540e\u5904\u7406\u9650\u5236\u5728\u7ed3\u6784\u5316\u65cfK\u4e2d\u3002\u5f00\u53d1\u4e86CDL_K\u5728\u4fe1\u606f\u8bba\u548c\u8ba1\u7b97\u4e0a\u7684\u53ef\u5904\u7406\u6027\u7406\u8bba\u3002", "result": "\u4e3a\u81ea\u7136\u7c7b\u522bK\u8bc1\u660e\u4e86\u4e0a\u4e0b\u754c\uff0c\u5efa\u7acb\u4e86CDL_K\u7684\u53ef\u5904\u7406\u6027\u6761\u4ef6\uff0c\u5e76\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u91cd\u65b0\u6821\u51c6\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u4e25\u683c\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u5b9a\u4e49\u548c\u7b97\u6cd5\u6280\u672f\uff0c\u4e3a\u51b3\u7b56\u5236\u5b9a\u4e2d\u7684\u6821\u51c6\u7406\u8bba\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u91cd\u65b0\u6821\u51c6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2511.13513", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13513", "abs": "https://arxiv.org/abs/2511.13513", "authors": ["Leonard G\u00f6ke", "Jan Wohland", "Stefano Moret", "Andr\u00e9 Bardow"], "title": "The Liquid Buffer: Multi-Year Storage for Defossilization and Energy Security under Climate Uncertainty", "comment": null, "summary": "The climate-driven uncertainty of renewable generation and electricity demand challenges energy security in net-zero energy systems. By introducing a scalable stochastic model that implicitly accounts for 51'840 climate years, this paper identifies multi-year storage of liquid hydrocarbons as a key option for managing climate uncertainty and ensuring energy security. In Europe, multi-year storage reduces system costs by 4.1%, fossil imports by 86%, and curtailment by 60%. The benefit of multi-year storage is that a renewable surplus in one year is not curtailed but converted to synthetic oil, with hydrogen as an intermediate product, and stored to balance a future deficit. We find that the required energy capacity for liquid hydrocarbons is 525 TWh, a quarter of the European Union's current oil and gas reserves, complemented by 116 TWh for hydrogen storage. Security of supply remains high and unserved energy only amounts to 0.0035 per thousand, well below the common target of 0.02 per thousand.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u968f\u673a\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u5e74\u5ea6\u6db2\u6001\u78b3\u6c22\u5316\u5408\u7269\u5b58\u50a8\u6765\u7ba1\u7406\u53ef\u518d\u751f\u80fd\u6e90\u7684\u6c14\u5019\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u6b27\u6d32\u6848\u4f8b\u4e2d\u53ef\u964d\u4f4e\u7cfb\u7edf\u6210\u672c4.1%\uff0c\u51cf\u5c11\u5316\u77f3\u71c3\u6599\u8fdb\u53e386%\uff0c\u524a\u51cf\u5f03\u753560%\u3002", "motivation": "\u5e94\u5bf9\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u548c\u7535\u529b\u9700\u6c42\u7684\u6c14\u5019\u9a71\u52a8\u4e0d\u786e\u5b9a\u6027\u5bf9\u51c0\u96f6\u80fd\u6e90\u7cfb\u7edf\u80fd\u6e90\u5b89\u5168\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u968f\u673a\u6a21\u578b\uff0c\u9690\u542b\u8003\u8651\u4e8651,840\u4e2a\u6c14\u5019\u5e74\u4efd\uff0c\u8bc6\u522b\u591a\u5e74\u5ea6\u6db2\u6001\u78b3\u6c22\u5316\u5408\u7269\u5b58\u50a8\u4f5c\u4e3a\u7ba1\u7406\u6c14\u5019\u4e0d\u786e\u5b9a\u6027\u7684\u5173\u952e\u65b9\u6848\u3002", "result": "\u5728\u6b27\u6d32\uff0c\u591a\u5e74\u5ea6\u5b58\u50a8\u4f7f\u7cfb\u7edf\u6210\u672c\u964d\u4f4e4.1%\uff0c\u5316\u77f3\u71c3\u6599\u8fdb\u53e3\u51cf\u5c1186%\uff0c\u5f03\u7535\u51cf\u5c1160%\u3002\u6240\u9700\u6db2\u6001\u78b3\u6c22\u5316\u5408\u7269\u50a8\u80fd\u5bb9\u91cf\u4e3a525TWh\uff0c\u6c22\u50a8\u80fd\u5bb9\u91cf\u4e3a116TWh\u3002", "conclusion": "\u591a\u5e74\u5ea6\u5b58\u50a8\u80fd\u6709\u6548\u7ba1\u7406\u6c14\u5019\u4e0d\u786e\u5b9a\u6027\uff0c\u786e\u4fdd\u80fd\u6e90\u5b89\u5168\uff0c\u4f9b\u5e94\u5b89\u5168\u4fdd\u6301\u9ad8\u6c34\u5e73\uff0c\u672a\u6ee1\u8db3\u80fd\u6e90\u4ec5\u53600.0035\u2030\uff0c\u8fdc\u4f4e\u4e8e0.02\u2030\u7684\u5e38\u89c1\u76ee\u6807\u3002"}}
{"id": "2511.12504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12504", "abs": "https://arxiv.org/abs/2511.12504", "authors": ["Maria Tseytlin", "Paul Roit", "Omri Abend", "Ido Dagan", "Ayal Klein"], "title": "QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs", "comment": null, "summary": "Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.", "AI": {"tldr": "QA-Noun\u662f\u4e00\u4e2a\u57fa\u4e8e\u95ee\u7b54\u7684\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u6355\u6349\u540d\u8bcd\u4e3a\u4e2d\u5fc3\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u901a\u8fc79\u4e2a\u95ee\u9898\u6a21\u677f\u8986\u76d6\u540d\u8bcd\u7684\u663e\u5f0f\u53e5\u6cd5\u548c\u9690\u5f0f\u4e0a\u4e0b\u6587\u89d2\u8272\uff0c\u4e0eQA-SRL\u7ed3\u5408\u5b9e\u73b0\u53e5\u5b50\u610f\u4e49\u7684\u7edf\u4e00\u5206\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eQA\u7684\u8bed\u4e49\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8c13\u8bcd-\u8bba\u5143\u5173\u7cfb\uff0c\u4f46\u540d\u8bcd\u4e3a\u4e2d\u5fc3\u7684\u8bed\u4e49\u5173\u7cfb\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u89e3\u51b3\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u6846\u67b6\u6765\u6355\u6349\u540d\u8bcd\u7684\u8bed\u4e49\u89d2\u8272\u3002", "method": "\u5b9a\u4e499\u4e2a\u95ee\u9898\u6a21\u677f\u6765\u8986\u76d6\u540d\u8bcd\u7684\u663e\u5f0f\u53e5\u6cd5\u548c\u9690\u5f0f\u4e0a\u4e0b\u6587\u89d2\u8272\uff0c\u521b\u5efa\u53ef\u89e3\u91ca\u7684QA\u5bf9\uff0c\u4e0eQA-SRL\u96c6\u6210\uff0c\u6784\u5efa\u5305\u542b2000\u591a\u4e2a\u6807\u6ce8\u540d\u8bcd\u63d0\u53ca\u7684\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6a21\u578b\u3002", "result": "QA-Noun\u51e0\u4e4e\u5b8c\u5168\u8986\u76d6\u4e86AMR\u7684\u540d\u8bcd\u8bba\u5143\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u989d\u5916\u7684\u4e0a\u4e0b\u6587\u9690\u542b\u5173\u7cfb\uff0c\u4e0eQA-SRL\u7ed3\u5408\u6bd4FactScore\u548cDecompScore\u7b49\u57fa\u4e8e\u4e8b\u5b9e\u7684\u5206\u89e3\u65b9\u6cd5\u7c92\u5ea6\u63d0\u9ad8\u4e86130%\u4ee5\u4e0a\u3002", "conclusion": "QA-Noun\u8865\u5145\u4e86\u57fa\u4e8eQA\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u5f62\u6210\u4e86\u5168\u9762\u4e14\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5206\u89e3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8de8\u6587\u672c\u5bf9\u9f50\u3002"}}
{"id": "2511.11663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11663", "abs": "https://arxiv.org/abs/2511.11663", "authors": ["Zhixiong Zhao", "Fangxin Liu", "Junjie Wang", "Chenyang Guan", "Zongwu Wang", "Li Jiang", "Haibing Guan"], "title": "SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization", "comment": "Accepted at AAAI 2026", "summary": "The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.", "AI": {"tldr": "SpecQuant\u662f\u4e00\u4e2a\u4ece\u5085\u91cc\u53f6\u9891\u57df\u89d2\u5ea6\u51fa\u53d1\u7684\u6781\u7aefLLM\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0\u6743\u91cd\u548c\u6fc0\u6d3b\u503c\u76844\u4f4d\u91cf\u5316\uff0c\u5728LLaMA-3 8B\u4e0a\u4ec5\u635f\u59311.5%\u7cbe\u5ea6\uff0c\u540c\u65f6\u5b9e\u73b02\u500d\u63a8\u7406\u52a0\u901f\u548c3\u500d\u5185\u5b58\u8282\u7701\u3002", "motivation": "\u968f\u7740\u51c6\u786e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u9700\u8981\u5148\u8fdb\u7684\u91cf\u5316\u6280\u672f\u6765\u5728\u7ec8\u7aef\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u3002\u672c\u6587\u4ece\u5085\u91cc\u53f6\u9891\u57df\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u6781\u7aefLLM\u538b\u7f29\u7684\u6311\u6218\uff0c\u76ee\u6807\u662f\u5b9e\u73b0\u6743\u91cd\u548c\u6fc0\u6d3b\u503c\u7684\u8d85\u4f4e\u4f4d\u91cf\u5316\u3002", "method": "\u63d0\u51faSpecQuant\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5e73\u6ed1\u6fc0\u6d3b\u503c\u5f02\u5e38\u503c\u5e76\u5c06\u5176\u8f6c\u79fb\u5230\u6743\u91cd\u77e9\u9635\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u901a\u9053\u7ea7\u4f4e\u9891\u5085\u91cc\u53f6\u622a\u65ad\u6765\u6291\u5236\u9ad8\u9891\u5206\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u57fa\u672c\u4fe1\u53f7\u80fd\u91cf\u3002\u8fd8\u5f15\u5165\u4e86\u8f7b\u91cf\u7ea7\u622a\u65ad\u6a21\u5757\u5b9e\u73b0\u8fd0\u884c\u65f6\u9002\u5e94\u6027\u3002", "result": "\u5728LLaMA-3 8B\u4e0a\uff0cSpecQuant\u5b9e\u73b0\u4e86\u6743\u91cd\u548c\u6fc0\u6d3b\u503c\u76844\u4f4d\u91cf\u5316\uff0c\u96f6\u6837\u672c\u51c6\u786e\u7387\u4e0e\u5168\u7cbe\u5ea6\u76f8\u6bd4\u4ec5\u5dee1.5%\uff0c\u540c\u65f6\u63d0\u4f9b2\u500d\u63a8\u7406\u52a0\u901f\u548c3\u500d\u5185\u5b58\u4f7f\u7528\u964d\u4f4e\u3002", "conclusion": "\u4ece\u5085\u91cc\u53f6\u9891\u57df\u89d2\u5ea6\u51fa\u53d1\u7684SpecQuant\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6781\u7aefLLM\u538b\u7f29\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u6743\u91cd\u80fd\u91cf\u4e3b\u8981\u96c6\u4e2d\u5728\u4f4e\u9891\u5206\u91cf\uff0c\u53ef\u4ee5\u4ee5\u6700\u5c0f\u7cbe\u5ea6\u5f71\u54cd\u4fdd\u7559\u8fd9\u4e9b\u5206\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6a21\u578b\u90e8\u7f72\u3002"}}
{"id": "2511.12306", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12306", "abs": "https://arxiv.org/abs/2511.12306", "authors": ["Darvin Yi", "Teng Liu", "Mattie Terzolo", "Lance Hasson", "Ayan Sinh", "Pablo Mendes", "Andrew Rabinovich"], "title": "UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI", "comment": null, "summary": "As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.", "AI": {"tldr": "UpBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9eUpwork\u5de5\u4f5c\u4efb\u52a1\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u91c7\u7528\u57fa\u4e8e\u4e13\u5bb6\u6784\u5efa\u7684\u8bc4\u4f30\u6807\u51c6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u662f\u9759\u6001\u3001\u5408\u6210\u6216\u9886\u57df\u53d7\u9650\u7684\uff0c\u65e0\u6cd5\u53cd\u6620AI\u4ee3\u7406\u5728\u52a8\u6001\u3001\u7ecf\u6d4e\u610f\u4e49\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u8868\u73b0\uff0c\u9700\u8981\u57fa\u4e8e\u771f\u5b9e\u5de5\u4f5c\u573a\u666f\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u4eceUpwork\u52b3\u52a8\u529b\u5e02\u573a\u63d0\u53d6\u771f\u5b9e\u5de5\u4f5c\u4efb\u52a1\uff0c\u7531\u4e13\u5bb6\u81ea\u7531\u804c\u4e1a\u8005\u5206\u89e3\u4e3a\u8be6\u7ec6\u53ef\u9a8c\u8bc1\u7684\u63a5\u53d7\u6807\u51c6\uff0c\u6784\u5efa\u57fa\u4e8e\u6807\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\u5bf9AI\u63d0\u4ea4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "result": "\u5efa\u7acb\u4e86\u57fa\u4e8e\u771f\u5b9e\u5de5\u4f5c\u6d3b\u52a8\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u80fd\u591f\u5206\u6790\u6a21\u578b\u4f18\u52bf\u3001\u5f31\u70b9\u548c\u6307\u4ee4\u9075\u5faa\u4fdd\u771f\u5ea6\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u901a\u8fc7/\u5931\u8d25\u4e8c\u5143\u6307\u6807\u3002", "conclusion": "UpBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\uff0c\u652f\u6301\u5728\u771f\u5b9e\u52b3\u52a8\u529b\u5e02\u573a\u73af\u5883\u4e2d\u8bc4\u4f30\u4ee3\u7406\u7cfb\u7edf\uff0c\u4e3aAI\u901a\u8fc7\u5408\u4f5c\u800c\u975e\u66ff\u4ee3\u6765\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2511.13546", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.13546", "abs": "https://arxiv.org/abs/2511.13546", "authors": ["Stefan Ecklebe", "Frank Woittennek"], "title": "On the controller form for linear hyperbolic MIMO systems with dynamic boundary conditions", "comment": "Submitted to the 24th European Control Conference (ECC), 6 pages, 1 figure", "summary": "This contribution develops an algebraic approach to obtain a controller form for a class of linear hyperbolic MIMO systems, bidirectionally coupled with a linear ODE system at the unactuated boundary. After a short summary of established controller forms for SISO and MIMO ODE as well as SISO hyperbolic PDE systems, it is shown that the direct ap- proach to state a controller form fails already for a very simple MIMO example. Next, a generalised hyperbolic controller form with different variants is proposed and a new flatnesss-based scheme to compute said form is presented. Therein, the system is treated in an algebraic setting where generalised polynomials with real exponents are used to describe the predictions and delays in the system. The proposed algorithm is then applied to the motivating example.", "AI": {"tldr": "\u672c\u6587\u4e3a\u53cc\u5411\u8026\u5408ODE\u7684\u7ebf\u6027\u53cc\u66f2MIMO\u7cfb\u7edf\u5f00\u53d1\u4e86\u4ee3\u6570\u65b9\u6cd5\u83b7\u5f97\u63a7\u5236\u5668\u5f62\u5f0f\uff0c\u63d0\u51fa\u4e86\u5e7f\u4e49\u53cc\u66f2\u63a7\u5236\u5668\u5f62\u5f0f\u548c\u57fa\u4e8e\u5e73\u5766\u6027\u7684\u8ba1\u7b97\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684SISO\u548cMIMO ODE\u7cfb\u7edf\u4ee5\u53caSISO\u53cc\u66f2PDE\u7cfb\u7edf\u7684\u63a7\u5236\u5668\u5f62\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eMIMO\u53cc\u66f2\u7cfb\u7edf\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u4ee3\u6570\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5177\u6709\u5b9e\u6307\u6570\u7684\u5e7f\u4e49\u591a\u9879\u5f0f\u63cf\u8ff0\u7cfb\u7edf\u4e2d\u7684\u9884\u6d4b\u548c\u5ef6\u8fdf\uff0c\u63d0\u51fa\u5e7f\u4e49\u53cc\u66f2\u63a7\u5236\u5668\u5f62\u5f0f\u53ca\u5176\u53d8\u4f53\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u5e73\u5766\u6027\u7684\u8ba1\u7b97\u65b9\u6848\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4ee3\u6570\u65b9\u6cd5\u83b7\u5f97\u63a7\u5236\u5668\u5f62\u5f0f\uff0c\u5e76\u5c06\u8be5\u7b97\u6cd5\u5e94\u7528\u4e8e\u793a\u4f8b\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u4ee3\u6570\u65b9\u6cd5\u548c\u57fa\u4e8e\u5e73\u5766\u6027\u7684\u65b9\u6848\u80fd\u591f\u6709\u6548\u5904\u7406MIMO\u53cc\u66f2\u7cfb\u7edf\u7684\u63a7\u5236\u5668\u5f62\u5f0f\u95ee\u9898\uff0c\u4e3a\u8fd9\u7c7b\u7cfb\u7edf\u7684\u63a7\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2511.12986", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12986", "abs": "https://arxiv.org/abs/2511.12986", "authors": ["Abdelouahed Ben Mhamed", "Assia Kamal-Idrissi", "Amal El Fallah Seghrouchni"], "title": "Learning Branching Policies for MILPs with Proximal Policy Optimization", "comment": "11 pages, 3 figures, AAAI conference", "summary": "Branch-and-Bound (B\\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86TGPPO\u6846\u67b6\uff0c\u4f7f\u7528PPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\u5206\u652f\u7b56\u7565\uff0c\u65e8\u5728\u63d0\u9ad8\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u4e2d\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u5728\u4e0d\u540c\u5b9e\u4f8b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u5206\u652f\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u4e13\u5bb6\u6f14\u793a\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u7ed3\u6784\u591a\u6837\u6216\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528PPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u6784\u5efa\u53c2\u6570\u5316\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u6765\u52a8\u6001\u6355\u6349\u641c\u7d22\u6811\u7684\u6f14\u5316\u4e0a\u4e0b\u6587\uff0c\u8bad\u7ec3\u5206\u652f\u7b56\u7565\u3002", "result": "TGPPO\u5728\u51cf\u5c11\u63a2\u7d22\u8282\u70b9\u6570\u548c\u6539\u8fdbp-\u539f\u59cb\u5bf9\u5076\u79ef\u5206\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u5b9e\u4f8b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6709\u6f5c\u529b\u4e3aMILP\u6c42\u89e3\u5668\u5f00\u53d1\u9c81\u68d2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u5206\u652f\u7b56\u7565\u3002"}}
{"id": "2511.12520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12520", "abs": "https://arxiv.org/abs/2511.12520", "authors": ["Jie Zhang", "Bo Tang", "Wanzi Shao", "Wenqiang Wei", "Jihao Zhao", "Jianqing Zhu", "Zhiyu li", "Wen Xi", "Zehao Lin", "Feiyu Xiong", "Yanchao Tan"], "title": "TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction", "comment": "Accepted by AAAI 2026", "summary": "Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.", "AI": {"tldr": "TAdaRAG\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u89e3\u51b3\u4f20\u7edfRAG\u4e2d\u4fe1\u606f\u4e22\u5931\u548c\u65e0\u5173\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u5c06\u5916\u90e8\u77e5\u8bc6\u622a\u65ad\u4e3a\u5c0f\u5757\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3001\u54cd\u5e94\u5e7b\u89c9\u548c\u63a8\u7406\u94fe\u65ad\u88c2\uff0c\u4e14\u68c0\u7d22\u7684\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u5305\u542b\u65e0\u5173\u7ec6\u8282\uff0c\u5f71\u54cd\u51c6\u786e\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u610f\u56fe\u9a71\u52a8\u7684\u8def\u7531\u673a\u5236\u5230\u9886\u57df\u7279\u5b9a\u63d0\u53d6\u6a21\u677f\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u9690\u5f0f\u63d0\u53d6\u673a\u5236\uff0c\u786e\u4fdd\u7b80\u6d01\u3001\u8fde\u8d2f\u3001\u975e\u5197\u4f59\u7684\u77e5\u8bc6\u96c6\u6210\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u5546\u4e1a\u57fa\u51c6\u6d4b\u8bd5\uff08NowNewsQA\uff09\u4e0a\uff0c\u4f7f\u7528\u4e09\u79cd\u9aa8\u5e72\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0cTAdaRAG\u5728\u591a\u4e2a\u9886\u57df\u548c\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TAdaRAG\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u6709\u6548\u6027\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edfRAG\u7684\u4fe1\u606f\u4e22\u5931\u548c\u65e0\u5173\u7ec6\u8282\u95ee\u9898\u3002"}}
{"id": "2511.11665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11665", "abs": "https://arxiv.org/abs/2511.11665", "authors": ["Sameeksha Sriram", "Ayush Paliwal", "Alexander S. Ecker", "Chase van de Geijn"], "title": "Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE", "comment": null, "summary": "Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u56db\u5143\u6570\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165(QuatRo)\u548c\u57fa\u4e8e\u514b\u5229\u798f\u5fb7\u4ee3\u6570\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165(CARE)\uff0c\u5c06\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u4ece\u4e8c\u7ef4\u6269\u5c55\u5230\u9ad8\u7ef4\uff0c\u540c\u65f6\u4fdd\u6301\u79fb\u4f4d\u7b49\u53d8\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9ad8\u7ef4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5(\u5982Spherical RoPE)\u662f\u975e\u4ea4\u6362\u7684\uff0c\u5931\u53bb\u4e86RoPE\u7684\u79fb\u4f4d\u7b49\u53d8\u6027\u7279\u6027\u3002\u9700\u8981\u627e\u5230\u65e2\u80fd\u6269\u5c55\u5230\u9ad8\u7ef4\u53c8\u80fd\u4fdd\u6301\u8fd9\u4e00\u91cd\u8981\u7279\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56db\u5143\u6570\u8868\u793a3D\u65cb\u8f6c\u6765\u53c2\u6570\u5316\u65cb\u8f6c\u8f74\uff0c\u63d0\u51fa\u4e86QuatRo\u65b9\u6cd5\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u514b\u5229\u798f\u5fb7\u4ee3\u6570\uff0c\u4f7f\u7528\u51e0\u4f55\u4ee3\u6570\u4e2d\u7684\u8f6c\u5b50\u4f5c\u7528\u4e8e\u591a\u5411\u91cf\uff0c\u5b9e\u73b0\u4efb\u610f\u7ef4\u5ea6\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u3002", "result": "\u8bc1\u660e\u4e86Mixed RoPE\u548cSpherical RoPE\u662fQuatRo\u7684\u7279\u4f8b\u3002\u63d0\u51fa\u7684CARE\u65b9\u6cd5\u80fd\u591f\u6269\u5c55\u5230\u4efb\u610f\u7ef4\u5ea6\uff0c\u5e76\u5728\u591a\u5411\u91cf\u4e2d\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f\u3002", "conclusion": "\u56db\u5143\u6570\u548c\u514b\u5229\u798f\u5fb7\u4ee3\u6570\u65b9\u6cd5\u4e3a\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u4f18\u96c5\u7684\u6269\u5c55\u6846\u67b6\uff0c\u65e2\u80fd\u5904\u7406\u9ad8\u7ef4\u8f93\u5165\uff0c\u53c8\u80fd\u4fdd\u6301\u91cd\u8981\u7684\u79fb\u4f4d\u7b49\u53d8\u6027\u7279\u6027\u3002"}}
{"id": "2511.12344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12344", "abs": "https://arxiv.org/abs/2511.12344", "authors": ["Baolong Bi", "Shenghua Liu", "Yiwei Wang", "Siqian Tong", "Lingrui Mei", "Yuyao Ge", "Yilong Xu", "Jiafeng Guo", "Xueqi Cheng"], "title": "Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.", "AI": {"tldr": "\u63d0\u51faRGR-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u5206\u6807\u51c6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5956\u52b1\u548c\u79bb\u7ebf\u6307\u5bfc\uff0c\u89e3\u51b3\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u9886\u57df\u4e14\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u7eaf\u5728\u7ebfRL\u6846\u67b6\u9650\u5236\u4e86\u63a2\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u9650\u5236\u4e86\u63a8\u7406\u6027\u80fd\u3002", "method": "RGR-GRPO\u6846\u67b6\u5229\u7528\u8bc4\u5206\u6807\u51c6\u63d0\u4f9b\u5bc6\u96c6\u4fe1\u606f\u5956\u52b1\u548c\u79bb\u7ebf\u6307\u5bfc\uff0c\u5728GRPO\u8bad\u7ec3\u671f\u95f4\u5141\u8bb8\u66f4\u5927\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u63a2\u7d22\u3002", "result": "\u572814\u4e2a\u591a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRGR-GRPO\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u66ff\u4ee3\u5956\u52b1\u65b9\u6848\u6216\u79bb\u7ebf\u6307\u5bfc\u7684RL\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u548c\u4e00\u822c\u63a8\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u53477.0%\u30015.4%\u30018.4%\u548c6.6%\u3002", "conclusion": "RGR-GRPO\u5728\u79bb\u7b56\u7565\u8bad\u7ec3\u671f\u95f4\u4fdd\u6301\u7a33\u5b9a\u7684\u71b5\u6ce2\u52a8\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684pass@k\u6027\u80fd\uff0c\u53cd\u6620\u4e86\u6301\u7eed\u7684\u63a2\u7d22\u548c\u6709\u6548\u7a81\u7834\u73b0\u6709\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2511.13588", "categories": ["eess.SY", "cs.AI", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.13588", "abs": "https://arxiv.org/abs/2511.13588", "authors": ["Agustin Castellano", "Shijie Pan", "Enrique Mallada"], "title": "Data-driven Acceleration of MPC with Guarantees", "comment": null, "summary": "Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684MPC\u52a0\u901f\u6846\u67b6\uff0c\u7528\u79bb\u7ebfMPC\u89e3\u6784\u5efa\u975e\u53c2\u6570\u7b56\u7565\u66ff\u4ee3\u5728\u7ebf\u4f18\u5316\uff0c\u5b9e\u73b0100-1000\u500d\u52a0\u901f", "motivation": "MPC\u867d\u5f3a\u5927\u4f46\u5728\u7ebf\u4f18\u5316\u901f\u5ea6\u6162\uff0c\u96be\u4ee5\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u63a7\u5236\u9700\u6c42", "method": "\u57fa\u4e8e\u79bb\u7ebfMPC\u89e3\u6784\u5efa\u975e\u53c2\u6570\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u76f8\u5bf9\u4e8e\u6784\u9020\u7684\u6700\u4f18\u6210\u672c\u4e0a\u754c\u662f\u8d2a\u5a6a\u7684\uff0c\u53ef\u4f5c\u4e3a\u67e5\u627e\u89c4\u5219\u5b9e\u73b0", "result": "\u5728\u5145\u5206\u6570\u636e\u8986\u76d6\u6761\u4ef6\u4e0b\u7b56\u7565\u9012\u5f52\u53ef\u884c\u4e14\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6709\u754c\u6700\u4f18\u6027\u5dee\u8ddd\uff0c\u5b9e\u9a8c\u663e\u793a\u6bd4\u6807\u51c6MPC\u5feb100-1000\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u901f\u5ea6\uff0c\u5177\u6709\u5b9e\u65f6\u63a7\u5236\u5e94\u7528\u6f5c\u529b"}}
{"id": "2511.12573", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12573", "abs": "https://arxiv.org/abs/2511.12573", "authors": ["Hyeonji Kim", "Sujeong Oh", "Sanghack Lee"], "title": "Mitigating Length Bias in RLHF through a Causal Lens", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u56e0\u679c\u6846\u67b6\u6765\u5206\u6790\u548c\u7f13\u89e3RLHF\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u72ec\u7acb\u4e8e\u5197\u957f\u5ea6\u8bc4\u4f30\u5185\u5bb9\u8d28\u91cf\u3002", "motivation": "RLHF\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u5b58\u5728\u957f\u5ea6\u504f\u5dee\uff0c\u503e\u5411\u4e8e\u5c06\u5197\u957f\u5ea6\u4e0e\u8d28\u91cf\u6df7\u6dc6\uff0c\u504f\u597d\u66f4\u957f\u7684\u56de\u7b54\uff0c\u8fd9\u5f71\u54cd\u4e86\u6a21\u578b\u8f93\u51fa\u7684\u8d28\u91cf\u548c\u7b80\u6d01\u6027\u3002", "method": "\u4f7f\u7528\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6784\u5efa\u4e24\u79cd\u7c7b\u578b\u7684\u54cd\u5e94\u5bf9\uff1a(1)\u5185\u5bb9\u76f8\u4f3c\u4f46\u957f\u5ea6\u4e0d\u540c\u7684\u957f\u5ea6\u5206\u6b67\u5bf9\uff0c(2)\u957f\u5ea6\u76f8\u4f3c\u4f46\u5185\u5bb9\u4e0d\u540c\u7684\u5185\u5bb9\u5206\u6b67\u5bf9\uff0c\u5e76\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5956\u52b1\u5206\u914d\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\uff0c\u4f7f\u7b56\u7565\u6a21\u578b\u4ea7\u751f\u66f4\u7b80\u6d01\u3001\u5185\u5bb9\u805a\u7126\u7684\u8f93\u51fa\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u957f\u5ea6\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86RLHF\u7ba1\u9053\u4e2d\u5956\u52b1\u5efa\u6a21\u7684\u9c81\u68d2\u6027\u548c\u5185\u5bb9\u654f\u611f\u6027\u3002"}}
{"id": "2511.12359", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12359", "abs": "https://arxiv.org/abs/2511.12359", "authors": ["Yifan Zhu", "Sammie Katt", "Samuel Kaski"], "title": "More Than Irrational: Modeling Belief-Biased Agents", "comment": "13 pages, 8 figures. Accepted at the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8ba1\u7b97\u7406\u6027\u7528\u6237\u6a21\u578b\uff0c\u7528\u4e8e\u5efa\u6a21\u8ba4\u77e5\u53d7\u9650\u4ee3\u7406\u5728\u504f\u89c1\u4fe1\u5ff5\u4e0b\u7684\u6700\u4f18\u884c\u4e3a\uff0c\u91cd\u70b9\u89e3\u51b3\u4ece\u88ab\u52a8\u89c2\u5bdf\u4e2d\u63a8\u65ad\u7528\u6237\u8ba4\u77e5\u754c\u9650\u548c\u504f\u89c1\u4fe1\u5ff5\u72b6\u6001\u7684\u95ee\u9898\u3002", "motivation": "\u9884\u6d4b\u548c\u7406\u89e3\u7528\u6237\u6216\u4eba\u7c7b\u5408\u4f5c\u8005\u7684\u6b21\u4f18\u884c\u4e3a\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u8fd9\u4e9b\u884c\u4e3a\u901a\u5e38\u4e0d\u662f\u975e\u7406\u6027\u7684\uff0c\u800c\u662f\u5728\u8ba4\u77e5\u754c\u9650\u548c\u504f\u89c1\u4fe1\u5ff5\u4e0b\u7684\u7406\u6027\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5d4c\u5957\u7c92\u5b50\u6ee4\u6ce2\u7684\u9ad8\u6548\u5728\u7ebf\u63a8\u7406\u65b9\u6cd5\uff0c\u540c\u65f6\u8ddf\u8e2a\u7528\u6237\u7684\u6f5c\u5728\u4fe1\u5ff5\u72b6\u6001\u5e76\u4ece\u672a\u77e5\u8ba4\u77e5\u754c\u9650\u4e2d\u4f30\u8ba1\u53c2\u6570\uff0c\u4ee5\u8bb0\u5fc6\u8870\u51cf\u4f5c\u4e3a\u8ba4\u77e5\u754c\u9650\u7684\u4ee3\u8868\u6027\u793a\u4f8b\u3002", "result": "\u5728\u5bfc\u822a\u4efb\u52a1\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u80fd\u751f\u6210\u4e0e\u4e0d\u540c\u8bb0\u5fc6\u5bb9\u91cf\u5bf9\u5e94\u7684\u76f4\u89c2\u5408\u7406\u884c\u4e3a\uff0c\u63a8\u7406\u65b9\u6cd5\u80fd\u5728\u6709\u9650\u89c2\u5bdf\u6b65\u9aa4\u5185\u51c6\u786e\u9ad8\u6548\u5730\u6062\u590d\u771f\u5b9e\u8ba4\u77e5\u754c\u9650\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u81ea\u9002\u5e94AI\u52a9\u624b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u8f85\u52a9\u7cfb\u7edf\u80fd\u591f\u8003\u8651\u7528\u6237\u7684\u8bb0\u5fc6\u9650\u5236\u8fdb\u884c\u81ea\u9002\u5e94\u534f\u52a9\u3002"}}
{"id": "2511.13595", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13595", "abs": "https://arxiv.org/abs/2511.13595", "authors": ["Sebastiano Mengozzi", "Giovanni B. Esposito", "Michelangelo Bin", "Andrea Acquaviva", "Andrea Bartolini", "Lorenzo Marconi"], "title": "Physics-Informed Neural Networks for Nonlinear Output Regulation", "comment": null, "summary": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold \u03c0(w) and a feedforward input c(w) that render such manifold invariant. The pair (\u03c0(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates \u03c0(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINN)\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5168\u4fe1\u606f\u8f93\u51fa\u8c03\u8282\u95ee\u9898\uff0c\u901a\u8fc7\u76f4\u63a5\u8fd1\u4f3c\u8c03\u8282\u5668\u65b9\u7a0b\u7684\u89e3\u6765\u5b9e\u73b0\u7cbe\u786e\u8ddf\u8e2a\u548c\u6291\u5236\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u9884\u8ba1\u7b97\u8f68\u8ff9\u6216\u6807\u8bb0\u6570\u636e\u6765\u89e3\u51b3\u8c03\u8282\u5668\u65b9\u7a0b\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8fd9\u4e9b\u6570\u636e\u5c31\u80fd\u51c6\u786e\u6c42\u89e3\u8c03\u8282\u5668\u65b9\u7a0b\u7684\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u5bf9\u5916\u90e8\u7cfb\u7edf\u53d8\u5316\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINN)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8fb9\u754c\u6761\u4ef6\u548c\u53ef\u884c\u6027\u6761\u4ef6\u4e0b\u7684\u6b8b\u5dee\u6765\u76f4\u63a5\u8fd1\u4f3c\u03c0(w)\u548cc(w)\uff0c\u65e0\u9700\u9884\u8ba1\u7b97\u8f68\u8ff9\u6216\u6807\u8bb0\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u91cd\u5efa\u96f6\u8bef\u5dee\u6d41\u5f62\uff0c\u5728\u5916\u90e8\u7cfb\u7edf\u53c2\u6570\u548c\u521d\u59cb\u6761\u4ef6\u53d8\u5316\u65f6\u4fdd\u6301\u8c03\u8282\u6027\u80fd\uff0c\u5e76\u5728\u76f4\u5347\u673a\u5782\u76f4\u52a8\u529b\u5b66\u4e0e\u632f\u8361\u5e73\u53f0\u540c\u6b65\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u57fa\u4e8ePINN\u7684\u5b66\u4e60\u6c42\u89e3\u5668\u5728\u975e\u7ebf\u6027\u8f93\u51fa\u8c03\u8282\u95ee\u9898\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u65f6\u63a8\u7406\u5e76\u6cdb\u5316\u5230\u4e0d\u540c\u53c2\u6570\u7684\u5916\u90e8\u7cfb\u7edf\u65cf\u3002"}}
{"id": "2511.11696", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11696", "abs": "https://arxiv.org/abs/2511.11696", "authors": ["Xun Shao", "Aoba Otani", "Yuto Hirasuka", "Runji Cai", "Seng W. Loke"], "title": "Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL", "comment": "This is the author's preprint version of a paper accepted for presentation at EAI MONAMI 2025 (to appear in Springer LNICST). The final authenticated version will be available online at Springer Link upon publication", "summary": "This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ece\u8dcc\u5012\u68c0\u6d4b\u6269\u5c55\u5230\u5168\u9762\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8(ADL)\u8bc6\u522b\u7684\u65b0\u4e00\u4ee3\u8001\u5e74\u4eba\u76d1\u62a4\u7cfb\u7edf\uff0c\u91c7\u7528\u9690\u79c1\u4fdd\u62a4\u3001\u8fb9\u7f18\u90e8\u7f72\u548c\u8054\u90a6\u5b66\u4e60\u6280\u672f\uff0c\u65e8\u5728\u652f\u6301\u8001\u9f84\u5316\u793e\u4f1a\u7684\u72ec\u7acb\u6027\u548c\u5c0a\u4e25\u3002", "motivation": "\u5f53\u524d\u8001\u5e74\u4eba\u76d1\u62a4\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u8dcc\u5012\u68c0\u6d4b\uff0c\u4f46\u9700\u8981\u5411\u66f4\u5168\u9762\u7684\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u8bc6\u522b\u53d1\u5c55\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u8001\u5e74\u4eba\u7684\u72ec\u7acb\u751f\u6d3b\u548c\u5c0a\u4e25\u3002", "method": "\u4f7f\u7528SISFall\u6570\u636e\u96c6\u53ca\u5176GAN\u589e\u5f3a\u53d8\u4f53\u8fdb\u884c\u53ef\u884c\u6027\u9a8c\u8bc1\uff0c\u5c06\u8dcc\u5012\u68c0\u6d4b\u4f5c\u4e3a\u4ee3\u7406\u4efb\u52a1\uff0c\u5728\u975eIID\u6761\u4ef6\u4e0b\u8fdb\u884c\u8054\u90a6\u5b66\u4e60\u5b9e\u9a8c\uff0c\u5e76\u5728Jetson Orin Nano\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5d4c\u5165\u5f0f\u90e8\u7f72\u3002", "result": "\u5c55\u793a\u4e86\u5728\u975eIID\u8054\u90a6\u5b66\u4e60\u6761\u4ef6\u4e0b\u7684\u521d\u6b65\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5168\u9762ADL\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65e9\u671f\u8bc1\u636e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740\u4ece\u5355\u4efb\u52a1\u68c0\u6d4b\u5411\u5168\u9762\u65e5\u5e38\u6d3b\u52a8\u8bc6\u522b\u7684\u8f6c\u53d8\uff0c\u4e3a\u53ef\u6301\u7eed\u548c\u4ee5\u4eba\u4e3a\u672c\u7684\u8001\u5e74\u4eba\u62a4\u7406AI\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u9886\u57df\u504f\u79fb\u3001\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u98ce\u9669\u7b49\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2511.12586", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12586", "abs": "https://arxiv.org/abs/2511.12586", "authors": ["Pu-Hai Yang", "Heyan Huang", "Heng-Da Xu", "Fanshu Sun", "Xian-Ling Mao", "Chaoxu Mu"], "title": "MMWOZ: Building Multimodal Agent for Task-oriented Dialogue", "comment": null, "summary": "Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MMWOZ\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6269\u5c55MultiWOZ 2.3\u6570\u636e\u96c6\uff0c\u5c06\u4f20\u7edf\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e0e\u56fe\u5f62\u7528\u6237\u754c\u9762(GUI)\u7ed3\u5408\uff0c\u5e76\u63d0\u51fa\u4e86MATE\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4f9d\u8d56\u5b9a\u5236\u540e\u7aefAPI\uff0c\u800c\u73b0\u5b9e\u573a\u666f\u4e2d\u5e7f\u6cdb\u5b58\u5728\u524d\u7aefGUI\u4e14\u7f3a\u4e4f\u5b9a\u5236API\uff0c\u8fd9\u9020\u6210\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u5f00\u53d1\u7f51\u9875\u98ce\u683cGUI\u4f5c\u4e3a\u524d\u7aef\uff0c\u8bbe\u8ba1\u81ea\u52a8\u5316\u811a\u672c\u5c06\u5bf9\u8bdd\u72b6\u6001\u548c\u7cfb\u7edf\u52a8\u4f5c\u8f6c\u6362\u4e3aGUI\u64cd\u4f5c\u6307\u4ee4\uff0c\u6536\u96c6\u7f51\u9875\u5feb\u7167\u548c\u5bf9\u5e94\u64cd\u4f5c\u6307\u4ee4\uff0c\u5e76\u63d0\u51faMATE\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "\u6784\u5efa\u4e86MMWOZ\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86MATE\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u5b9e\u7528\u7684\u591a\u6a21\u6001\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u7cfb\u7edf\u4e0e\u73b0\u5b9e\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.11667", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11667", "abs": "https://arxiv.org/abs/2511.11667", "authors": ["Feng Guo", "Yuntao Wen", "Shen Gao", "Junshuo Zhang", "Shuo Shang"], "title": "Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion", "comment": null, "summary": "Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.", "AI": {"tldr": "KUnBR\u662f\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u5bc6\u5ea6\u5f15\u5bfc\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u91cd\u63d2\u5165\u7b56\u7565\u6709\u6548\u6d88\u9664LLM\u4e2d\u7684\u6709\u5bb3\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u96be\u4ee5\u5f7b\u5e95\u79fb\u9664\u6709\u5bb3\u77e5\u8bc6\uff0c\u6b8b\u7559\u77e5\u8bc6\u5bb9\u6613\u88ab\u6062\u590d\uff0c\u9700\u8981\u89e3\u51b3\u9690\u79c1\u3001\u5408\u89c4\u548c\u4f26\u7406\u95ee\u9898\u3002", "method": "\u9996\u5148\u901a\u8fc7\u77e5\u8bc6\u5bc6\u5ea6\u4f30\u8ba1\u8bc6\u522b\u5bcc\u542b\u6709\u5bb3\u77e5\u8bc6\u7684\u5c42\uff0c\u7136\u540e\u91c7\u7528\u5c42\u91cd\u63d2\u5165\u7b56\u7565\u7ed5\u8fc7\u68af\u5ea6\u969c\u788d\uff0c\u786e\u4fdd\u6709\u6548\u68af\u5ea6\u4f20\u64ad\u3002", "result": "\u5728\u591a\u4e2a\u9057\u5fd8\u548c\u901a\u7528\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKUnBR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6548\u7528\u3002", "conclusion": "KUnBR\u901a\u8fc7\u7cbe\u786e\u5b9a\u4f4d\u548c\u6709\u6548\u6d88\u9664\u6709\u5bb3\u77e5\u8bc6\uff0c\u4e3aLLM\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12378", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12378", "abs": "https://arxiv.org/abs/2511.12378", "authors": ["Dylan M. Asmar", "Mykel J. Kochenderfer"], "title": "Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making", "comment": "Under Review", "summary": "Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u5b66\u4e60\u5efa\u8bae\u8005\u53ef\u9760\u6027\u7684\u6846\u67b6\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u8c03\u6574\u5bf9\u5efa\u8bae\u7684\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u6218\u7565\u6027\u7684\"\u8be2\u95ee\"\u52a8\u4f5c\u6765\u5e73\u8861\u4fe1\u606f\u83b7\u53d6\u4e0e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u5efa\u8bae\u8005\u8d28\u91cf\u53c2\u6570\u662f\u9759\u6001\u4e14\u5df2\u77e5\u7684\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u5904\u7406\u5efa\u8bae\u53ef\u9760\u6027\u53d8\u5316\u7684\u95ee\u9898\uff0c\u4e3a\u81ea\u9002\u5e94\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u5c06\u5efa\u8bae\u8005\u8d28\u91cf\u76f4\u63a5\u96c6\u6210\u5230\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u8868\u793a\u4e2d\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u5b66\u4e60\u5efa\u8bae\u8005\u7c7b\u578b\uff1b\u5f15\u5165\u663e\u5f0f\u7684\"\u8be2\u95ee\"\u52a8\u4f5c\uff0c\u5141\u8bb8\u5728\u5173\u952e\u65f6\u523b\u6218\u7565\u6027\u5730\u8bf7\u6c42\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u5728\u4e0d\u540c\u5efa\u8bae\u8005\u8d28\u91cf\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u80fd\uff0c\u80fd\u591f\u9002\u5e94\u53ef\u9760\u6027\u53d8\u5316\uff0c\u5e76\u6709\u6548\u7ba1\u7406\u5efa\u8bae\u8bf7\u6c42\u7b56\u7565\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5904\u7406\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5efa\u8bae\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2511.13662", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13662", "abs": "https://arxiv.org/abs/2511.13662", "authors": ["Beno\u00eet Jeanson", "Mathieu Tanneau", "Simon Tindemans"], "title": "Scalable Iterative Algorithm for Solving Optimal Transmission Switching with De-energization", "comment": null, "summary": "Transmission System Operators routinely use transmission switching as a tool to manage congestion and ensure system security. Motivated by sub-transmission operations at RTE, this paper considers the Optimal Transmission Switching with De-energization (OTSD), which captures potential loss of connectivity (and therefore localized blackout) following loss of transmission elements. While directly relevant to real-life operations, this problem has received very little attention in the literature. The paper proposes a new mixed-integer linear programming formulation for OTSD that represents post-contingency loss of connectivity without requiring additional binary variables. This new formulation provides the foundation for a fast, iterative heuristic algorithm. Computational experiments confirms that state-of-the-art optimization solvers struggle to solve the extensive formulation of OTSD, often failing to find even trivial solutions within reasonable time. In contrast, numerical results demonstrate the efficiency of the proposed heuristic, which finds high-quality feasible solutions 100-1000x faster than using Gurobi.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8003\u8651\u65ad\u7535\u540e\u8fde\u63a5\u6027\u635f\u5931\u7684\u6700\u4f18\u4f20\u8f93\u5207\u6362\u95ee\u9898\u7684\u65b0\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u516c\u5f0f\u548c\u5feb\u901f\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u6bd4\u73b0\u6709\u6c42\u89e3\u5668\u5feb100-1000\u500d\u627e\u5230\u9ad8\u8d28\u91cf\u53ef\u884c\u89e3\u3002", "motivation": "\u53d7RTE\u5b50\u8f93\u7535\u8fd0\u8425\u542f\u53d1\uff0c\u8003\u8651\u4f20\u8f93\u5143\u4ef6\u635f\u5931\u540e\u53ef\u80fd\u5bfc\u81f4\u7684\u8fde\u63a5\u6027\u635f\u5931\u548c\u5c40\u90e8\u505c\u7535\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5728\u5b9e\u9645\u8fd0\u8425\u4e2d\u5f88\u91cd\u8981\u4f46\u5728\u6587\u732e\u4e2d\u5f88\u5c11\u88ab\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u516c\u5f0f\uff0c\u65e0\u9700\u989d\u5916\u4e8c\u5143\u53d8\u91cf\u5373\u53ef\u8868\u793a\u4e8b\u6545\u540e\u8fde\u63a5\u6027\u635f\u5931\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u5feb\u901f\u8fed\u4ee3\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u8ba1\u7b97\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u4f18\u5316\u6c42\u89e3\u5668\u96be\u4ee5\u5728\u5408\u7406\u65f6\u95f4\u5185\u89e3\u51b3OTSD\u7684\u6269\u5c55\u516c\u5f0f\uff0c\u800c\u6240\u63d0\u542f\u53d1\u5f0f\u7b97\u6cd5\u6bd4Gurobi\u5feb100-1000\u500d\u627e\u5230\u9ad8\u8d28\u91cf\u53ef\u884c\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8003\u8651\u65ad\u7535\u540e\u8fde\u63a5\u6027\u635f\u5931\u7684\u6700\u4f18\u4f20\u8f93\u5207\u6362\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c42\u89e3\u6548\u7387\u3002"}}
{"id": "2511.11767", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11767", "abs": "https://arxiv.org/abs/2511.11767", "authors": ["Amisha Priyadarshini", "Sergio Gago-Masague"], "title": "Learning Fair Representations with Kolmogorov-Arnold Networks", "comment": null, "summary": "Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Kolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u96c6\u6210\u5230\u516c\u5e73\u5bf9\u6297\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u60e9\u7f5a\u66f4\u65b0\u673a\u5236\u52a8\u6001\u8c03\u6574\u516c\u5e73\u7ea6\u675f\uff0c\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u8de8\u654f\u611f\u5c5e\u6027\u7684\u7ade\u4e89\u6027\u516c\u5e73\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u5b66\u4e60\u6a21\u578b\u5728\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e14\u9ed1\u76d2\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5728\u793e\u4f1a\u654f\u611f\u9886\u57df\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06KANs\u96c6\u6210\u5230\u516c\u5e73\u5bf9\u6297\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5229\u7528KANs\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u60e9\u7f5a\u66f4\u65b0\u673a\u5236\u6765\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u516c\u5e73\u7ea6\u675f\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u5927\u5b66\u5f55\u53d6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u4e0d\u540c\u4f18\u5316\u7b56\u7565\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u516c\u5e73\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8de8\u654f\u611f\u5c5e\u6027\u7684\u7ade\u4e89\u6027\u516c\u5e73\u3002", "conclusion": "KANs\u5728\u516c\u5e73\u673a\u5668\u5b66\u4e60\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5e73\u8861\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u793e\u4f1a\u654f\u611f\u51b3\u7b56\u9886\u57df\u3002"}}
{"id": "2511.12596", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12596", "abs": "https://arxiv.org/abs/2511.12596", "authors": ["Oron Anschel", "Alon Shoshan", "Adam Botach", "Shunit Haviv Hakimi", "Asaf Gendler", "Emanuel Ben Baruch", "Nadav Bhonker", "Igor Kviatkovsky", "Manoj Aggarwal", "Gerard Medioni"], "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models", "comment": "EMNLP Main 2025", "summary": "Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.", "AI": {"tldr": "GAPO\u662f\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u7fa4\u4f53\u7ea7\u522b\u7684\u5956\u52b1\u6765\u9f13\u52b1LLM\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u54cd\u5e94\uff0c\u89e3\u51b3\u4e86\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5373\u4f7f\u5b58\u5728\u591a\u4e2a\u6709\u6548\u7b54\u6848\uff0c\u4e5f\u91cd\u590d\u751f\u6210\u76f8\u540c\u7684\u5c11\u6570\u8865\u5168\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u54cd\u5e94\u7684\u591a\u6837\u6027\u3002", "method": "GAPO\u662fGRPO\u7684\u7b80\u5355\u6269\u5c55\uff0c\u8ba1\u7b97\u7fa4\u4f53\u6574\u4f53\u5956\u52b1\uff0c\u4f7f\u7528\u9891\u7387\u611f\u77e5\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u5728\u6709\u6548\u8865\u5168\u4e0a\u5747\u5300\u91c7\u6837\u3002", "result": "GAPO\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u4ea7\u751f\u6709\u6548\u4e14\u66f4\u591a\u6837\u5316\u7684\u54cd\u5e94\uff0c\u5728\u5f00\u653e\u63d0\u793a\u4e0b\u4e5f\u80fd\u63d0\u9ad8\u54cd\u5e94\u591a\u6837\u6027\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u7684\u51c6\u786e\u6027\u3002", "conclusion": "GAPO\u901a\u8fc7\u7fa4\u4f53\u611f\u77e5\u7b56\u7565\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86LLM\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u54cd\u5e94\u591a\u6837\u6027\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002"}}
{"id": "2511.11668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11668", "abs": "https://arxiv.org/abs/2511.11668", "authors": ["Chase van de Geijn", "Ayush Paliwal", "Timo L\u00fcddecke", "Alexander S. Ecker"], "title": "Do traveling waves make good positional encodings?", "comment": null, "summary": "Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.", "AI": {"tldr": "RollPE\u662f\u4e00\u79cd\u57fa\u4e8e\u884c\u6ce2\u7684\u65b0\u578b\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u901a\u8fc7\u5faa\u73af\u6eda\u52a8\u64cd\u4f5c\u5728\u81ea\u6ce8\u610f\u529b\u4e2d\u5b9e\u73b0\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u4e0eRoPE\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u6765\u6355\u6349\u5e73\u79fb\u7b49\u53d8\u6027\u3002", "method": "\u5728\u81ea\u6ce8\u610f\u529b\u4e2d\u5bf9\u67e5\u8be2\u548c\u952e\u5f20\u91cf\u5e94\u7528\u5faa\u73af\u6eda\u52a8\u64cd\u4f5c\uff0c\u901a\u8fc7\u4f4d\u7f6e\u95f4\u7684\u76f8\u4f4d\u504f\u79fb\u8ba1\u7b97\u57fa\u4e8e\u4f4d\u7f6e\u5dee\u5f02\u7684\u6ce8\u610f\u529b\u3002", "result": "RollPE\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7edd\u5bf9\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4e0eRoPE\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u5927\u8111\u4fe1\u606f\u6d41\u52a8\u8fc7\u7a0b\u7684\u6f5c\u5728\u8054\u7cfb\u3002", "conclusion": "RollPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3RoPE\u548c\u5927\u8111\u4fe1\u606f\u5904\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.12439", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12439", "abs": "https://arxiv.org/abs/2511.12439", "authors": ["Yujia Liu", "Sophia Yu", "Hongyue Jin", "Jessica Wen", "Alexander Qian", "Terrence Lee", "Mattheus Ramsis", "Gi Won Choi", "Lianhui Qin", "Xin Liu", "Edward J. Wang"], "title": "Multi-agent Self-triage System with Medical Flowcharts", "comment": null, "summary": "Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u9a8c\u8bc1\u6d41\u7a0b\u56fe\u7684\u5bf9\u8bdd\u5f0f\u81ea\u6211\u5206\u8bca\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b095.29%\u7684\u6d41\u7a0b\u56fe\u68c0\u7d22\u51c6\u786e\u7387\u548c99.10%\u7684\u5bfc\u822a\u51c6\u786e\u7387\uff0c\u7ed3\u5408\u81ea\u7531\u6587\u672c\u4ea4\u4e92\u7684\u7075\u6d3b\u6027\u548c\u6807\u51c6\u5316\u4e34\u5e8a\u534f\u8bae\u7684\u4e25\u8c28\u6027\u3002", "motivation": "\u5728\u7ebf\u5065\u5eb7\u8d44\u6e90\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u53ef\u9760\u6027\u6709\u9650\uff0c\u5b58\u5728\u51c6\u786e\u6027\u4f4e\u3001\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u6613\u53d7\u672a\u7ecf\u9a8c\u8bc1\u4fe1\u606f\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u6784\u5316\u4e14\u53ef\u5ba1\u8ba1\u7684\u60a3\u8005\u51b3\u7b56\u652f\u6301\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u533b\u5b66\u4f1a100\u4e2a\u4e34\u5e8a\u9a8c\u8bc1\u6d41\u7a0b\u56fe\u6307\u5bfcLLMs\uff0c\u91c7\u7528\u5305\u542b\u68c0\u7d22\u667a\u80fd\u4f53\u3001\u51b3\u7b56\u667a\u80fd\u4f53\u548c\u804a\u5929\u667a\u80fd\u4f53\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u57282000\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5b9e\u73b095.29%\u7684\u524d3\u51c6\u786e\u7387\u6d41\u7a0b\u56fe\u68c0\u7d22\uff0c\u572837200\u4e2a\u4e0d\u540c\u5bf9\u8bdd\u98ce\u683c\u548c\u6761\u4ef6\u4e0b\u5b9e\u73b099.10%\u7684\u6d41\u7a0b\u56fe\u5bfc\u822a\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u900f\u660e\u3001\u51c6\u786e\u4e14\u53ef\u63a8\u5e7f\u7684AI\u8f85\u52a9\u81ea\u6211\u5206\u8bca\u7684\u53ef\u884c\u6027\uff0c\u6709\u6f5c\u529b\u652f\u6301\u77e5\u60c5\u60a3\u8005\u51b3\u7b56\u5e76\u6539\u5584\u533b\u7597\u8d44\u6e90\u5229\u7528\u3002"}}
{"id": "2511.13690", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13690", "abs": "https://arxiv.org/abs/2511.13690", "authors": ["Shyam Kamal", "Sunidhi Pandey", "Thach Ngoc Dinh"], "title": "Novel Stability Criteria for Discrete and Hybrid Systems via Ramanujan Inner Products", "comment": "6 pages, 2 figures", "summary": "This paper introduces a Ramanujan inner product and its corresponding norm, establishing a novel framework for the stability analysis of hybrid and discrete-time systems as an alternative to traditional Euclidean metrics. We establish new $\u03b5$-$\u03b4$ stability conditions that utilize the unique properties of Ramanujan summations and their relationship with number-theoretic concepts. The proposed approach provides enhanced robustness guarantees and reveals fundamental connections between system stability and arithmetic properties of the system dynamics. Theoretical results are rigorously proven, and simulation results on numerical examples are presented to validate the efficacy of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u62c9\u9a6c\u52aa\u91d1\u5185\u79ef\u548c\u8303\u6570\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6df7\u5408\u548c\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u66ff\u4ee3\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u3002", "motivation": "\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u5728\u7cfb\u7edf\u7a33\u5b9a\u6027\u5206\u6790\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u63a2\u7d22\u57fa\u4e8e\u6570\u8bba\u6982\u5ff5\u7684\u65b0\u6570\u5b66\u6846\u67b6\u6765\u63d0\u4f9b\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165\u62c9\u9a6c\u52aa\u91d1\u5185\u79ef\u548c\u76f8\u5e94\u8303\u6570\uff0c\u5229\u7528\u62c9\u9a6c\u52aa\u91d1\u6c42\u548c\u53ca\u5176\u4e0e\u6570\u8bba\u6982\u5ff5\u7684\u72ec\u7279\u5173\u7cfb\u5efa\u7acb\u65b0\u7684\u03b5-\u03b4\u7a33\u5b9a\u6027\u6761\u4ef6\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u5f97\u5230\u4e25\u683c\u8bc1\u660e\uff0c\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u7a33\u5b9a\u6027\u4e0e\u7cfb\u7edf\u52a8\u529b\u5b66\u7b97\u672f\u7279\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u8054\u7cfb\u3002", "conclusion": "\u62c9\u9a6c\u52aa\u91d1\u5185\u79ef\u6846\u67b6\u4e3a\u6df7\u5408\u548c\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u7a33\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5177\u6709\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\u548c\u6df1\u523b\u7684\u6570\u8bba\u8054\u7cfb\u3002"}}
{"id": "2511.12609", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12609", "abs": "https://arxiv.org/abs/2511.12609", "authors": ["Yunxin Li", "Xinyu Chen", "Shenyuan Jiang", "Haoyuan Shi", "Zhenyu Liu", "Xuanyu Zhang", "Nanhao Deng", "Zhenran Xu", "Yicheng Ma", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data", "comment": "47 pages,10 Figures, Project Website: https://idealistxy.github.io/Uni-MoE-v2.github.io/; Codes: https://github.com/HITsz-TMG/Uni-MoE", "summary": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.", "AI": {"tldr": "Uni-MoE 2.0\u662f\u4e00\u4e2a\u5168\u5f00\u6e90\u7684\u8de8\u6a21\u6001\u5927\u6a21\u578b\uff0c\u57fa\u4e8eQwen2.5-7B\u67b6\u6784\u6784\u5efa\uff0c\u901a\u8fc7\u52a8\u6001\u5bb9\u91cfMoE\u8bbe\u8ba1\u3001\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548c\u8de8\u6a21\u6001\u6570\u636e\u5339\u914d\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u8bed\u8a00\u3001\u56fe\u50cf\u3001\u8bed\u97f3\u7b49\u591a\u79cd\u6a21\u6001\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u63a8\u8fdbLychee Uni-MoE\u7cfb\u5217\u5728\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u8de8\u6a21\u6001\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u79cd\u6a21\u6001\u8f93\u5165\u8f93\u51fa\u7684\u5168\u5f00\u6e90\u6a21\u578b\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5bb9\u91cfMoE\u67b6\u6784\uff0c\u5305\u542b\u5171\u4eab\u3001\u8def\u7531\u548c\u7a7a\u4e13\u5bb6\uff1b\u4f7f\u7528Omni-Modality 3D RoPE\u786e\u4fdd\u8de8\u6a21\u6001\u65f6\u7a7a\u5bf9\u9f50\uff1b\u91c7\u7528\u6e10\u8fdb\u5f0f\u76d1\u7763\u5fae\u8c03\u7b56\u7565\uff0c\u7ed3\u5408GSPO-DPO\u65b9\u6cd5\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff1b\u572875B token\u7684\u591a\u6a21\u6001\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u914d\u5907\u4e13\u95e8\u7684\u8bed\u97f3\u548c\u56fe\u50cf\u751f\u6210token\u3002", "result": "\u572885\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7Qwen2.5-Omni\u572876\u4e2a\u57fa\u51c6\u4e2d\u768450\u591a\u4e2a\uff0c\u89c6\u9891\u7406\u89e3\u5e73\u5747\u63d0\u53477%\uff0c\u8de8\u6a21\u6001\u7406\u89e3\u5e73\u5747\u63d0\u53477%\uff0c\u89c6\u542c\u63a8\u7406\u63d0\u53474%\uff0c\u957f\u8bed\u97f3\u5904\u7406WER\u964d\u4f4e4.2%\uff0c\u5728\u4f4e\u7ea7\u56fe\u50cf\u5904\u7406\u548c\u53ef\u63a7\u751f\u6210\u65b9\u9762\u9886\u5148\u3002", "conclusion": "Uni-MoE 2.0\u5728\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5728\u591a\u4e2a\u8de8\u6a21\u6001\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u63a5\u8fd1SOTA\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11669", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11669", "abs": "https://arxiv.org/abs/2511.11669", "authors": ["Dmytro Hospodarchuk"], "title": "H-Model: Dynamic Neural Architectures for Adaptive Processing", "comment": "Independent research report, 24 pages including references and figures", "summary": "This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.\n  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.\n  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u6839\u636e\u8f93\u5165\u6570\u636e\u52a8\u6001\u8c03\u6574\u5185\u90e8\u7ed3\u6784\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u8def\u7531\u673a\u5236\u5b9e\u73b0\u8fed\u4ee3\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\uff0c\u8fd9\u662f\u4e00\u4e2a\u6982\u5ff5\u6027\u539f\u578b\u800c\u975e\u6027\u80fd\u4f18\u5316\u7684\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u53ef\u9002\u5e94\u4e14\u53ef\u80fd\u66f4\u53ef\u89e3\u91ca\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u8ba9\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u5b66\u4e60\u8868\u793a\uff0c\u8fd8\u80fd\u5b66\u4e60\u8ba1\u7b97\u7ed3\u6784\u672c\u8eab\uff0c\u53d7\u601d\u7ef4\u8fc7\u7a0b\u548c\u52a8\u6001\u63a8\u7406\u7684\u542f\u53d1\u3002", "method": "\u5f15\u5165\u8def\u7531\u673a\u5236\uff0c\u5141\u8bb8\u6bcf\u5c42\u5f71\u54cd\u5176\u8f93\u51fa\u5728\u7f51\u7edc\u4e2d\u7684\u4f20\u64ad\u65b9\u5f0f\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6570\u636e\u548c\u5185\u90e8\u72b6\u6001\u7684\u6761\u4ef6\u5316\u4fe1\u606f\u6d41\u3002", "result": "\u7531\u4e8e\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u9650\u5236\uff0c\u8fd9\u662f\u521d\u6b65\u7814\u7a76\uff0c\u4f46\u521d\u6b65\u89c2\u5bdf\u663e\u793a\u6709\u6f5c\u529b\uff0c\u5b8c\u6574\u8bc4\u4f30\u9700\u8981\u5728\u66f4\u6709\u5229\u7684\u8ba1\u7b97\u6761\u4ef6\u4e0b\u8fdb\u884c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6027\u67b6\u6784\u6846\u67b6\uff0c\u4e3a\u63a2\u7d22\u81ea\u9002\u5e94\u7f51\u7edc\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u91cd\u70b9\u5728\u4e8e\u67b6\u6784\u521b\u65b0\u800c\u975e\u6027\u80fd\u7ade\u4e89\u3002"}}
{"id": "2511.12485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12485", "abs": "https://arxiv.org/abs/2511.12485", "authors": ["Pengze Li", "Jiaqi Liu", "Junchi Yu", "Lihao Liu", "Mingyu Ding", "Wanli Ouyang", "Shixiang Tang", "Xi Chen"], "title": "ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction", "comment": "Accepted to AAAI 2026", "summary": "Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86ARCHE\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u5c06\u590d\u6742\u63a8\u7406\u5206\u89e3\u4e3a\u6807\u51c6\u63a8\u7406\u8303\u5f0f\u7684\u7ec4\u5408\uff0c\u5f62\u6210\u63a8\u7406\u903b\u8f91\u6811(RLT)\uff0c\u5305\u542b\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u6eaf\u56e0\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709LLM\u751f\u6210\u7684\u63a8\u7406\u5185\u5bb9\u901a\u5e38\u662f\u975e\u7ed3\u6784\u5316\u548c\u975e\u6b63\u5f0f\u7684\uff0c\u96be\u4ee5\u5224\u65ad\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u79d1\u5b66\u63a8\u7406\u7684\u57fa\u672c\u8303\u5f0f\u3002", "method": "\u5f15\u5165ARCHE\u4efb\u52a1\u548cRLT\u7ed3\u6784\uff0c\u5f00\u53d1ARCHE Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea70\u7bc7Nature Communications\u6587\u7ae0\u76841900\u591a\u4e2a\u53c2\u8003\u6587\u732e\u548c38000\u4e2a\u89c2\u70b9\uff0c\u5e76\u63d0\u51faEC\u548cREA\u4e24\u4e2a\u903b\u8f91\u611f\u77e5\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u8bc4\u4f3010\u4e2a\u9886\u5148LLM\u53d1\u73b0\uff0c\u6a21\u578b\u5728REA\u548cEC\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u76ee\u524d\u6ca1\u6709\u6a21\u578b\u80fd\u591f\u63d0\u53d6\u5b8c\u6574\u4e14\u6807\u51c6\u7684\u63a8\u7406\u94fe\u3002", "conclusion": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u7684\u80fd\u529b\u4e0e\u79d1\u5b66\u8bba\u8bc1\u6240\u9700\u7684\u4e25\u8c28\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002"}}
{"id": "2511.13698", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13698", "abs": "https://arxiv.org/abs/2511.13698", "authors": ["Hampei Sasahara", "Tatsuya Yamada", "Jun-ichi Imura", "Henrik Sandberg"], "title": "Resilient Distribution Network Planning against Dynamic Malicious Power Injection Attacks", "comment": "Accepted at IEEE Transactions on Control of Network Systems", "summary": "Active distribution networks facilitating bidirectional power exchange with renewable energy resources are susceptible to cyberattacks due to integration of a diverse array of cyber components. This study introduces a grid-level defense strategy aimed at enhancing attack resiliency based on distribution network planning. Our proposed framework imposes a security requirement into existing planning methodologies, ensuring that voltage deviation from its rated value remains within a tolerable range against dynamically and maliciously injected power at end-user nodes. Unfortunately, the formulated problem in its original form is intractable because it is an infinite-dimensional bi-level optimization problem over a function space. To address this complexity, we develop an equivalent transformation into a tractable form as mixed-integer linear program leveraging linear dynamical system theory and graph theory. Notably, our investigation reveals that the severity of potential attacks hinges solely on the cumulative reactances over the path from the substation to the targeted node, thereby reducing the problem to a finite-dimensional problem. Further, the bi-level optimization problem is reduced to a single-level optimization problem by using a technique utilized in solving the shortest path problem. Through extensive numerical simulations conducted on a 54-node distribution network benchmark, our proposed methodology exhibits a noteworthy 29.3% enhancement in the resiliency, with a mere 2.1% uptick in the economic cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u914d\u7535\u7f51\u89c4\u5212\u7684\u7535\u7f51\u7ea7\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u5b89\u5168\u8981\u6c42\u7eb3\u5165\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\uff0c\u786e\u4fdd\u5728\u7ec8\u7aef\u7528\u6237\u8282\u70b9\u906d\u53d7\u6076\u610f\u529f\u7387\u6ce8\u5165\u65f6\u7535\u538b\u504f\u5dee\u4fdd\u6301\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\u3002", "motivation": "\u7531\u4e8e\u96c6\u6210\u591a\u79cd\u7f51\u7edc\u7ec4\u4ef6\uff0c\u652f\u6301\u53ef\u518d\u751f\u80fd\u6e90\u53cc\u5411\u529f\u7387\u4ea4\u6362\u7684\u4e3b\u52a8\u914d\u7535\u7f51\u5bb9\u6613\u53d7\u5230\u7f51\u7edc\u653b\u51fb\uff0c\u9700\u8981\u589e\u5f3a\u653b\u51fb\u5f39\u6027\u3002", "method": "\u5c06\u539f\u59cb\u65e0\u9650\u7ef4\u53cc\u5c42\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u5f62\u5f0f\uff0c\u5229\u7528\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u7406\u8bba\u548c\u56fe\u8bba\uff0c\u53d1\u73b0\u653b\u51fb\u4e25\u91cd\u6027\u4ec5\u53d6\u51b3\u4e8e\u53d8\u7535\u7ad9\u5230\u76ee\u6807\u8282\u70b9\u7684\u7d2f\u79ef\u7535\u6297\uff0c\u4ece\u800c\u5c06\u95ee\u9898\u7b80\u5316\u4e3a\u6709\u9650\u7ef4\u95ee\u9898\u3002", "result": "\u572854\u8282\u70b9\u914d\u7535\u7f51\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e8629.3%\u7684\u5f39\u6027\uff0c\u800c\u7ecf\u6d4e\u6210\u672c\u4ec5\u589e\u52a02.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u5b89\u5168\u8981\u6c42\u6574\u5408\u5230\u914d\u7535\u7f51\u89c4\u5212\u4e2d\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u7f51\u7edc\u5bf9\u7f51\u7edc\u653b\u51fb\u7684\u5f39\u6027\uff0c\u4e14\u6210\u672c\u589e\u52a0\u6709\u9650\u3002"}}
{"id": "2511.12920", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12920", "abs": "https://arxiv.org/abs/2511.12920", "authors": ["Desheng Hu", "Joachim Baumann", "Aleksandra Urman", "Elsa Lichtenegger", "Robin Forsberg", "Aniko Hannak", "Christo Wilson"], "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy", "comment": "18 pages, 10 figures; to appear in AAAI ICWSM 2026", "summary": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u7b97\u6cd5\u5ba1\u8ba1\u53d1\u73b0\uff0cGoogle\u641c\u7d22\u4e2d\u7684AI\u6982\u89c8\u548c\u7cbe\u9009\u6458\u8981\u529f\u80fd\u5728\u5a74\u513f\u62a4\u7406\u548c\u5b55\u671f\u76f8\u5173\u67e5\u8be2\u4e2d\u5b58\u5728\u4fe1\u606f\u4e0d\u4e00\u81f4\u3001\u7f3a\u4e4f\u533b\u7597\u5b89\u5168\u4fdd\u969c\u7b49\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u8bc4\u4f30AI\u751f\u6210\u5185\u5bb9\u5728\u5065\u5eb7\u4fe1\u606f\u641c\u7d22\u4e2d\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u7528\u6237\u65e0\u6cd5\u63a7\u5236\u5c55\u793a\u65b9\u5f0f\u7684AI\u6982\u89c8\u548c\u7cbe\u9009\u6458\u8981\u529f\u80fd\u3002", "method": "\u5bf91,508\u4e2a\u771f\u5b9e\u5a74\u513f\u62a4\u7406\u548c\u5b55\u671f\u76f8\u5173\u67e5\u8be2\u8fdb\u884c\u7cfb\u7edf\u7b97\u6cd5\u5ba1\u8ba1\uff0c\u4f7f\u7528\u5305\u542b\u7b54\u6848\u4e00\u81f4\u6027\u3001\u76f8\u5173\u6027\u3001\u533b\u7597\u5b89\u5168\u4fdd\u969c\u3001\u6765\u6e90\u7c7b\u522b\u548c\u60c5\u611f\u5bf9\u9f50\u7684\u591a\u7ef4\u5ea6\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u3002", "result": "33%\u7684\u641c\u7d22\u7ed3\u679c\u9875\u9762\u4e2dAI\u6982\u89c8\u548c\u7cbe\u9009\u6458\u8981\u4fe1\u606f\u4e0d\u4e00\u81f4\uff1b\u533b\u7597\u5b89\u5168\u4fdd\u969c\u4e25\u91cd\u7f3a\u4e4f\uff08AI\u6982\u89c8\u4ec511%\uff0c\u7cbe\u9009\u6458\u8981\u4ec57%\uff09\uff1b\u5065\u5eb7\u7f51\u7ad9\u662f\u4e3b\u8981\u6765\u6e90\uff0c\u4f46\u7cbe\u9009\u6458\u8981\u4e5f\u5e38\u94fe\u63a5\u5546\u4e1a\u6765\u6e90\u3002", "conclusion": "AI\u4ecb\u5bfc\u7684\u5065\u5eb7\u4fe1\u606f\u9700\u8981\u66f4\u5f3a\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u8be5\u5ba1\u8ba1\u65b9\u6cd5\u53ef\u4e3a\u9ad8\u98ce\u9669\u9886\u57dfAI\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u53ef\u8f6c\u79fb\u6846\u67b6\u3002"}}
{"id": "2511.12630", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12630", "abs": "https://arxiv.org/abs/2511.12630", "authors": ["Maoqi Liu", "Quan Fang", "Yang Yang", "Can Zhao", "Kaiquan Cai"], "title": "Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing", "comment": "Accepted to Advanced Engineering Informatics", "summary": "Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NOTAM\u8bed\u4e49\u89e3\u6790\u4efb\u52a1\uff0c\u6784\u5efa\u4e86Knots\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u548c\u591a\u79cd\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u822a\u7a7a\u6587\u672c\u7684\u7406\u89e3\u548c\u5904\u7406\u80fd\u529b\u3002", "motivation": "NOTAMs\u4f5c\u4e3a\u98de\u884c\u5b89\u5168\u4fe1\u606f\u7684\u5173\u952e\u4f20\u64ad\u6e20\u9053\uff0c\u5176\u590d\u6742\u7684\u8bed\u8a00\u7ed3\u6784\u548c\u9690\u542b\u63a8\u7406\u7ed9\u81ea\u52a8\u5316\u89e3\u6790\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5206\u7c7b\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7b49\u8868\u5c42\u4efb\u52a1\uff0c\u7f3a\u4e4f\u6df1\u5c42\u6b21\u7684\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u63d0\u51faNOTAM\u8bed\u4e49\u89e3\u6790\u4efb\u52a1\uff0c\u6784\u5efa\u5305\u542b12,347\u6761\u4e13\u5bb6\u6807\u6ce8NOTAMs\u7684Knots\u6570\u636e\u96c6\uff0c\u8986\u76d6194\u4e2a\u98de\u884c\u60c5\u62a5\u533a\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u8fdb\u884c\u5168\u9762\u7684\u5b57\u6bb5\u53d1\u73b0\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u548c\u6a21\u578b\u9002\u5e94\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u822a\u7a7a\u6587\u672c\u7406\u89e3\u548c\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u81ea\u52a8\u5316NOTAM\u5206\u6790\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86NOTAM\u8bed\u4e49\u89e3\u6790\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u548c\u8bed\u4e49\u63a8\u7406\uff0c\u80fd\u591f\u751f\u6210\u7ed3\u6784\u5316\u7684\u3001\u5bcc\u542b\u63a8\u7406\u4fe1\u606f\u7684\u8f93\u51fa\uff0c\u4e3a\u822a\u7a7a\u5b89\u5168\u4fe1\u606f\u7684\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2511.11671", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11671", "abs": "https://arxiv.org/abs/2511.11671", "authors": ["Alina Deriyeva", "Benjamin Paassen"], "title": "Evaluation of LLM-based Explanations for a Learning Analytics Dashboard", "comment": null, "summary": "Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u5b66\u4e60\u5206\u6790\u4eea\u8868\u677f\u751f\u6210\u6570\u636e\u89e3\u91ca\uff0c\u76f8\u6bd4\u72ec\u7acb\u4eea\u8868\u677f\u548c\u6559\u5e08\u89e3\u91ca\u66f4\u53d7\u9752\u7750\uff0c\u80fd\u63d0\u5347\u5b66\u4e60\u4f53\u9a8c\u5e76\u4fdd\u6301\u6559\u5b66\u6807\u51c6\u3002", "motivation": "\u5b66\u4e60\u5206\u6790\u4eea\u8868\u677f\u652f\u6301\u81ea\u4e3b\u5b66\u4e60\uff0c\u4f46\u6570\u636e\u53ef\u89e3\u91ca\u6027\u5f71\u54cd\u5176\u6548\u679c\uff0c\u9700\u8981\u8f85\u52a9\u89e3\u91ca\u6765\u5e2e\u52a9\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4eea\u8868\u677f\u6570\u636e\u7684\u6587\u5b57\u89e3\u91ca\uff0c\u5e76\u4e0e\u72ec\u7acb\u4eea\u8868\u677f\u548c\u6559\u5e08\u89e3\u91ca\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u901a\u8fc7\u4e13\u5bb6\u7814\u7a76\uff08N=12\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "LLM\u751f\u6210\u7684\u6280\u80fd\u72b6\u6001\u89e3\u91ca\u548c\u5b66\u4e60\u5efa\u8bae\u663e\u8457\u66f4\u53d7\u9752\u7750\uff0c\u4f18\u4e8e\u5176\u4ed6\u6761\u4ef6\u3002", "conclusion": "\u4f7f\u7528LLM\u8fdb\u884c\u6570\u636e\u89e3\u91ca\u53ef\u4ee5\u589e\u5f3a\u5b66\u4e60\u4f53\u9a8c\uff0c\u540c\u65f6\u4fdd\u6301\u6559\u5e08\u8ba4\u53ef\u7684\u6559\u5b66\u6807\u51c6\u3002"}}
{"id": "2511.12563", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12563", "abs": "https://arxiv.org/abs/2511.12563", "authors": ["Eljas Linna", "Kestutis Baltakys", "Alexandros Iosifidis", "Juho Kanniainen"], "title": "LOBERT: Generative AI Foundation Model for Limit Order Book Messages", "comment": "Submission for NeurIPS 2025 GenAI in Finance Workshop", "summary": "Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.", "AI": {"tldr": "LOBERT\u662f\u4e00\u4e2a\u9488\u5bf9\u9650\u4ef7\u8ba2\u5355\u7c3f\u6570\u636e\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u7684\u6807\u8bb0\u5316\u65b9\u6848\u5c06\u591a\u7ef4\u6d88\u606f\u4f5c\u4e3a\u5355\u4e2a\u6807\u8bb0\u5904\u7406\uff0c\u5728\u9884\u6d4b\u4e2d\u95f4\u4ef7\u683c\u53d8\u52a8\u548c\u4e0b\u4e00\u6d88\u606f\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u9886\u5148\u3002", "motivation": "\u73b0\u6709\u7684LOB\u6a21\u578b\u9700\u8981\u7e41\u7410\u7684\u6570\u636e\u8868\u793a\uff0c\u7f3a\u4e4f\u539f\u59cb\u4efb\u52a1\u4e4b\u5916\u7684\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u9002\u7528\u4e8e\u4e0b\u6e38\u5fae\u8c03\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u3002", "method": "LOBERT\u57fa\u4e8eBERT\u67b6\u6784\uff0c\u91c7\u7528\u65b0\u7684\u6807\u8bb0\u5316\u65b9\u6848\uff0c\u5c06\u5b8c\u6574\u7684\u591a\u7ef4\u6d88\u606f\u4f5c\u4e3a\u5355\u4e2a\u6807\u8bb0\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u7559\u4ef7\u683c\u3001\u6570\u91cf\u548c\u65f6\u95f4\u7684\u8fde\u7eed\u8868\u793a\u3002", "result": "LOBERT\u5728\u9884\u6d4b\u4e2d\u95f4\u4ef7\u683c\u53d8\u52a8\u548c\u4e0b\u4e00\u6d88\u606f\u7b49\u4efb\u52a1\u4e2d\u53d6\u5f97\u9886\u5148\u6027\u80fd\uff0c\u540c\u65f6\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u51cf\u5c11\u4e86\u6240\u9700\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "conclusion": "LOBERT\u4e3aLOB\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2511.11722", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11722", "abs": "https://arxiv.org/abs/2511.11722", "authors": ["Soumyendu Sarkar", "Antonio Guillen-Perez", "Zachariah J Carmichael", "Avisek Naug", "Refik Mert Cam", "Vineet Gundecha", "Ashwin Ramesh Babu", "Sahand Ghorbanpour", "Ricardo Luna Gutierrez"], "title": "Fast 3D Surrogate Modeling for Data Center Thermal Management", "comment": "Submitted to AAAI 2026 Conference", "summary": "Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\\%) and reduced carbon footprint.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9\u7684\u66ff\u4ee3\u5efa\u6a21\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u6570\u636e\u4e2d\u5fc3\u76843D\u4f53\u7d20\u5316\u8868\u793a\u4e0a\u8fd0\u884c\uff0c\u7ed3\u5408\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u3001\u98ce\u6247\u901f\u5ea6\u548cHVAC\u6e29\u5ea6\u8bbe\u5b9a\u70b9\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6e29\u5ea6\u9884\u6d4b\uff0c\u6bd4\u4f20\u7edfCFD\u6c42\u89e3\u5668\u5feb20,000\u500d\u3002", "motivation": "\u51cf\u5c11\u6570\u636e\u4e2d\u5fc3\u7684\u80fd\u8017\u548c\u78b3\u6392\u653e\uff0c\u901a\u8fc7\u5b9e\u65f6\u6e29\u5ea6\u9884\u6d4b\u5b9e\u73b0\u53ef\u6301\u7eed\u6027\u548c\u8fd0\u8425\u6548\u7387\u3002\u4f20\u7edf\u70edCFD\u6c42\u89e3\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9700\u8981\u4e13\u5bb6\u6784\u5efa\u7f51\u683c\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u4e0d\u9002\u5408\u5b9e\u65f6\u4f7f\u7528\u3002", "method": "\u8bc4\u4f30\u591a\u79cd\u67b6\u6784\uff0c\u5305\u62ec3D CNN U-Net\u53d8\u4f53\u30013D\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u548c3D\u89c6\u89c9\u53d8\u6362\u5668\uff0c\u5c06\u70ed\u8f93\u5165\u6620\u5c04\u5230\u9ad8\u4fdd\u771f\u70ed\u56fe\u3002\u4f7f\u75283D\u4f53\u7d20\u5316\u6570\u636e\u4e2d\u5fc3\u8868\u793a\uff0c\u7ed3\u5408\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u3001\u98ce\u6247\u901f\u5ea6\u548cHVAC\u6e29\u5ea6\u8bbe\u5b9a\u70b9\u3002", "result": "\u66ff\u4ee3\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u4e2d\u5fc3\u914d\u7f6e\u4e2d\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u8fbe20,000\u500d\u7684\u52a0\u901f\uff08\u6570\u767e\u6beb\u79d2vs\u6570\u5c0f\u65f6\uff09\u3002\u80fd\u591f\u5feb\u901f\u51c6\u786e\u4f30\u8ba1\u70ed\u70b9\u548c\u6e29\u5ea6\u5206\u5e03\u3002", "conclusion": "\u5feb\u901f\u51c6\u786e\u7684\u6e29\u5ea6\u4f30\u8ba1\u652f\u6301\u5b9e\u65f6\u51b7\u5374\u63a7\u5236\u548c\u8d1f\u8f7d\u91cd\u65b0\u5206\u914d\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u8282\u80fd\uff087%\uff09\u548c\u78b3\u8db3\u8ff9\u51cf\u5c11\u3002"}}
{"id": "2511.13238", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13238", "abs": "https://arxiv.org/abs/2511.13238", "authors": ["Patrick Parschan", "Charlott Jakob"], "title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms", "comment": "46 pages, 8 figures, 2 tables, accepted for publication in Quality & Quantity", "summary": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.", "AI": {"tldr": "\u672c\u6587\u5bf9\u65e0\u76d1\u7763\u548c\u534a\u76d1\u7763\u8ba1\u7b97\u6587\u672c\u7406\u60f3\u70b9\u4f30\u8ba1\u7b97\u6cd5\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u4ece\u8bcd\u9891\u6a21\u578b\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u56db\u7c7b\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u6bd4\u8f83\u6846\u67b6\u548c\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u8ba1\u7b97\u6587\u672c\u7406\u60f3\u70b9\u4f30\u8ba1\u7b97\u6cd5\u5728\u653f\u6cbb\u5b66\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u65b9\u6cd5\u53d1\u5c55\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u548c\u5e94\u7528\u6307\u5bfc\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u6574\u5408\u4e0d\u540c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u56de\u987e\u8bc6\u522b25\u79cd\u7b97\u6cd5\uff0c\u8fdb\u884c\u624b\u52a8\u5185\u5bb9\u5206\u6790\uff0c\u63d0\u51fa\u533a\u5206\u6587\u672c\u65b9\u5dee\u751f\u6210\u3001\u6355\u83b7\u548c\u805a\u5408\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5c06\u65b9\u6cd5\u5206\u4e3a\u8bcd\u9891\u3001\u4e3b\u9898\u5efa\u6a21\u3001\u8bcd\u5d4c\u5165\u548cLLM\u56db\u7c7b\u3002", "result": "\u5efa\u7acb\u4e86\u56db\u7c7b\u65b9\u6cd5\u5bb6\u65cf\u7684\u7cfb\u7edf\u5206\u7c7b\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u5047\u8bbe\u3001\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u7b97\u6cd5\u9009\u62e9\u4e2d\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7b97\u6cd5\u5dee\u5f02\u672c\u8eab\u5177\u6709\u4fe1\u606f\u4ef7\u503c\uff0c\u9700\u8981\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\uff1b\u4e3a\u5e94\u7528\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u900f\u660e\u6027\u3001\u6280\u672f\u8981\u6c42\u548c\u9a8c\u8bc1\u7b56\u7565\u65b9\u9762\u7684\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2511.12661", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12661", "abs": "https://arxiv.org/abs/2511.12661", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "title": "Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing", "comment": null, "summary": "Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a \"faithfulness gap\": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning \"Houston\" from \"NASA\" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.", "AI": {"tldr": "Reason-KE++ \u662f\u4e00\u4e2a SFT+RL \u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u76d1\u7763\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u5728 MQUAKE-CF-3k \u4e0a\u8fbe\u5230 95.48% \u7684\u65b0 SOTA\u3002", "motivation": "\u73b0\u6709 SFT \u65b9\u6cd5\u5b58\u5728\"\u5fe0\u5b9e\u6027\u5dee\u8ddd\"\uff0c\u6a21\u578b\u4ec5\u6a21\u4eff\u683c\u5f0f\u800c\u975e\u8fdb\u884c\u5408\u7406\u63a8\u7406\uff0c\u5bfc\u81f4\u53c2\u6570\u5148\u9a8c\u8986\u76d6\u4e0a\u4e0b\u6587\u4e8b\u5b9e\uff0c\u4ea7\u751f\u5173\u952e\u4e8b\u5b9e\u5e7b\u89c9\u3002", "method": "\u63d0\u51fa Reason-KE++ \u6846\u67b6\uff0c\u5305\u542b\u9636\u6bb5\u611f\u77e5\u5956\u52b1\u673a\u5236\uff0c\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff08\u5982\u5206\u89e3\u3001\u5b50\u7b54\u6848\u6b63\u786e\u6027\uff09\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\uff0c\u907f\u514d\u4ec5\u4f9d\u8d56\u7ed3\u679c\u5956\u52b1\u7684\u9677\u9631\u3002", "result": "\u5728 MQUAKE-CF-3k \u4e0a\u8fbe\u5230 95.48% \u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u4e4b\u524d\u65b9\u6cd5\u63d0\u5347 5.28%\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u5b8c\u6574\u6027\u3002", "conclusion": "\u5bf9\u4e8e\u590d\u6742\u4efb\u52a1\uff0c\u5bf9\u9f50\u63a8\u7406\u8fc7\u7a0b\u5bf9\u4e8e\u6784\u5efa\u53ef\u4fe1\u8d56\u7684 LLM \u81f3\u5173\u91cd\u8981\uff0c\u8fc7\u7a0b\u7ea7\u76d1\u7763\u662f\u89e3\u51b3\u5fe0\u5b9e\u6027\u95ee\u9898\u7684\u5173\u952e\u3002"}}
{"id": "2511.11673", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11673", "abs": "https://arxiv.org/abs/2511.11673", "authors": ["M. A. Gameiro"], "title": "Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture", "comment": null, "summary": "This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u540c\u878d\u5408\u5c42\uff08SFL\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u5c06Sentence-BERT\u6df1\u5ea6\u8bed\u4e49\u7279\u5f81\u4e0e\u4f4e\u7ef4\u7ed3\u6784\u7279\u5f81\u878d\u5408\uff0c\u5728\u6b4c\u8bcd\u5185\u5bb9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u5c06\u590d\u6742\u9ad8\u7ef4\u6df1\u5ea6\u8bed\u4e49\u7279\u5f81\u4e0e\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u7279\u5f81\u6709\u6548\u878d\u5408\uff0c\u4ee5\u63d0\u5347\u6b4c\u8bcd\u5185\u5bb9\u5206\u7c7b\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u95e8\u63a7\u673a\u5236\u7684\u534f\u540c\u878d\u5408\u5c42\uff08SFL\uff09\u67b6\u6784\uff0c\u8c03\u5236Sentence-BERT\u5d4c\u5165\u4e0e\u4f4e\u7ef4\u8f85\u52a9\u7279\u5f81\uff0c\u5c06UMAP\u964d\u7ef4\u540e\u7684\u6b4c\u8bcd\u5d4c\u5165\u805a\u7c7b\u4efb\u52a1\u91cd\u6784\u4e3a\u4e8c\u5206\u7c7b\u95ee\u9898\u3002", "result": "SFL\u6a21\u578b\u51c6\u786e\u7387\u8fbe0.9894\uff0cMacro F1\u5206\u65700.9894\uff0c\u6bd4\u968f\u673a\u68ee\u6797\u57fa\u7ebf\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u6821\u51c6\u8bef\u5dee\u964d\u4f4e93%\uff0c\u5bf9\u6570\u635f\u5931\u964d\u4f4e2.5\u500d\u3002", "conclusion": "\u975e\u7ebf\u6027\u95e8\u63a7\u673a\u5236\u4f18\u4e8e\u7b80\u5355\u7279\u5f81\u62fc\u63a5\uff0cSFL\u6a21\u578b\u4e3a\u590d\u6742\u591a\u6a21\u6001\u6b4c\u8bcd\u5206\u6790\u63d0\u4f9b\u4e86\u9c81\u68d2\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.12579", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12579", "abs": "https://arxiv.org/abs/2511.12579", "authors": ["Yongwen Ren", "Chao Wang", "Peng Du", "Chuan Qin", "Dazhong Shen", "Hui Xiong"], "title": "Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models", "comment": null, "summary": "Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.", "AI": {"tldr": "PCRS-TKA\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7ed3\u5408\uff0c\u6784\u5efa\u5bf9\u8bdd\u7279\u5b9a\u77e5\u8bc6\u6811\u5e76\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u63a8\u7406\u548c\u8bed\u4e49\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06PLMs\u4e0eKGs\u7ed3\u5408\u65f6\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u672a\u80fd\u5145\u5206\u5229\u7528PLM\u5728\u56fe\u5173\u7cfb\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u3001\u4e0d\u52a0\u533a\u5206\u5730\u6574\u5408\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u3001\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5ffd\u89c6\u534f\u540c\u504f\u597d\u3002", "method": "\u63d0\u51faPCRS-TKA\u6846\u67b6\uff0c\u6784\u5efa\u5bf9\u8bdd\u7279\u5b9a\u77e5\u8bc6\u6811\u5e76\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u9009\u62e9\u6027\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u76f8\u5173\u77e5\u8bc6\uff0c\u663e\u5f0f\u5efa\u6a21\u534f\u540c\u504f\u597d\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\u534f\u8c03\u5f02\u6784\u8f93\u5165\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660ePCRS-TKA\u5728\u63a8\u8350\u548c\u5bf9\u8bdd\u8d28\u91cf\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PCRS-TKA\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u63a8\u7406\u3001\u9009\u62e9\u6027\u77e5\u8bc6\u6574\u5408\u548c\u534f\u540c\u504f\u597d\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u5bf9\u8bdd\u8d28\u91cf\u3002"}}
{"id": "2511.12139", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12139", "abs": "https://arxiv.org/abs/2511.12139", "authors": ["Sahar Moghimian Hoosh", "Ilia Kamyshev", "Henni Ouerdane"], "title": "Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion", "comment": "Extended version of the conference paper \"Enhancing Non-Intrusive Load Monitoring with Features Extracted by Independent Component Analysis\" -- arXiv:2501.16817. Instead of solely using ICA or PCA for feature extraction, we propose the fusion of ICA and PCA, which outperforms other baseline models. This extended version is meant for journal publication", "summary": "Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eICA\u548cPCA\u7279\u5f81\u878d\u5408\u7684\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u4fb5\u5165\u5f0f\u8d1f\u8377\u76d1\u6d4b\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u5b9e\u9645NILM\u90e8\u7f72\u4e2d\u9762\u4e34\u7684\u8fc7\u62df\u5408\u3001\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u540c\u65f6\u8fd0\u884c\u5927\u91cf\u7535\u5668\u65f6\u7684\u5206\u89e3\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u6784\u5efa\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u9891\u6807\u8bb0\u6570\u636e\u3001\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u3002\u63d0\u51fa\u878d\u5408ICA\u548cPCA\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u591a\u6807\u7b7e\u5206\u7c7b\u67b6\u6784Fusion-ResNet\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18NILM\u5206\u7c7b\u5668\uff0c\u63d0\u51fa\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u6a21\u578b\u5e73\u5747F1\u5206\u6570\u66f4\u9ad8\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002\u5728\u591a\u8fbe15\u4e2a\u540c\u65f6\u8fd0\u884c\u7535\u5668\u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u76f8\u5bf9\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "Fusion-ResNet\u6846\u67b6\u5728NILM\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u548c\u5bf9\u591a\u7535\u5668\u540c\u65f6\u8fd0\u884c\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13290", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13290", "abs": "https://arxiv.org/abs/2511.13290", "authors": ["Jea Kwon", "Luiz Felipe Vecchietti", "Sungwon Park", "Meeyoung Cha"], "title": "Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment", "comment": "Accepted to AAAI 2026", "summary": "Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via \"dropout\" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86AI\u5728\u9053\u5fb7\u56f0\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u53d1\u73b0\u901a\u8fc7\u5f15\u5165\u63a8\u7406\u65f6\u7684\u968f\u673a\u6027\u53ef\u4ee5\u589e\u52a0\u6a21\u578b\u7684\u603b\u71b5\u548c\u4e92\u4fe1\u606f\uff0c\u4ece\u800c\u6539\u5584AI\u4e0e\u4eba\u7c7b\u9053\u5fb7\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u4eba\u7c7b\u5728\u9053\u5fb7\u56f0\u5883\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46AI\u7cfb\u7edf\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5f80\u5f80\u8fc7\u5ea6\u81ea\u4fe1\u3002\u968f\u7740AI\u8d8a\u6765\u8d8a\u591a\u5730\u53c2\u4e0e\u4f26\u7406\u51b3\u7b56\uff0c\u7406\u89e3\u5176\u9053\u5fb7\u63a8\u7406\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5bf9\u4e8e\u6784\u5efa\u53ef\u9760\u7684AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u7ecf\u5178\u7684\u7535\u8f66\u95ee\u9898\uff0c\u5206\u679032\u4e2a\u5f00\u6e90\u6a21\u578b\u57289\u4e2a\u9053\u5fb7\u7ef4\u5ea6\u4e0a\u7684\u54cd\u5e94\u3002\u901a\u8fc7\u5f15\u5165\u63a8\u7406\u65f6\u7684\"dropout\"\u6765\u589e\u52a0\u968f\u673a\u6027\uff0c\u5e76\u6d4b\u91cf\u4e8c\u5143\u71b5\u4f5c\u4e3a\u603b\u71b5\u3001\u6761\u4ef6\u71b5\u548c\u4e92\u4fe1\u606f\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u6a21\u578b\u95f4\u7684\u7f6e\u4fe1\u5ea6\u5dee\u5f02\u5927\u4e8e\u9053\u5fb7\u7ef4\u5ea6\u5185\u7684\u5dee\u5f02\uff1b\u5f15\u5165\u968f\u673a\u6027\u673a\u5236\u4e3b\u8981\u589e\u52a0\u4e86\u4e92\u4fe1\u606f\uff0c\u800c\u6761\u4ef6\u71b5\u57fa\u672c\u4e0d\u53d8\uff1b\u8be5\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b-LLM\u9053\u5fb7\u5bf9\u9f50\u5ea6\uff0c\u4e92\u4fe1\u606f\u4e0e\u5bf9\u9f50\u5206\u6570\u53d8\u5316\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u901a\u8fc7\u6709\u610f\u8c03\u8282\u4e0d\u786e\u5b9a\u6027\u548c\u964d\u4f4eLLM\u5728\u9053\u5fb7\u590d\u6742\u573a\u666f\u4e2d\u7684\u7f6e\u4fe1\u5ea6\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5bf9\u9f50\u6a21\u578b\u51b3\u7b56\u4e0e\u4eba\u7c7b\u504f\u597d\uff0c\u8fd9\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684AI\u9053\u5fb7\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.12690", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12690", "abs": "https://arxiv.org/abs/2511.12690", "authors": ["Sina Rashidi", "Hossein Sameti"], "title": "Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data", "comment": null, "summary": "Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6ce2\u65af\u8bed\u5230\u82f1\u8bed\u7684\u76f4\u63a5\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u79bb\u6563\u8bed\u97f3\u5355\u5143\u548c\u5408\u6210\u5e73\u884c\u6570\u636e\u6765\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u76f4\u63a5\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u5e73\u884c\u8bed\u97f3\u6570\u636e\uff0c\u4f46\u5bf9\u4e8e\u6ce2\u65af\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u6765\u8bf4\uff0c\u8fd9\u7c7b\u6570\u636e\u975e\u5e38\u7a00\u7f3a\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u57fa\u4e8econformer\u7684\u7f16\u7801\u5668\u3001\u56e0\u679ctransformer\u89e3\u7801\u5668\u548c\u57fa\u4e8e\u5355\u5143\u7684\u795e\u7ecf\u58f0\u7801\u5668\u3002\u901a\u8fc7\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u6ce2\u65af\u8bed\u8f6c\u5f55\u6587\u672c\uff0c\u5e76\u7528\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u5408\u6210\u82f1\u8bed\u8bed\u97f3\uff0c\u6784\u5efa\u4e86\u5408\u6210\u5e73\u884c\u8bed\u97f3\u8bed\u6599\u5e93\u3002", "result": "\u5728CVSS\u8bed\u6599\u5e93\u7684\u6ce2\u65af\u8bed-\u82f1\u8bed\u90e8\u5206\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u6bd4\u76f4\u63a5\u57fa\u7ebf\u63d0\u9ad8\u4e864.6 ASR BLEU\uff0c\u53ef\u7528\u5e73\u884c\u8bed\u97f3\u6570\u636e\u91cf\u589e\u52a0\u4e86\u7ea66\u500d\u3002", "conclusion": "\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u79bb\u6563\u8bed\u97f3\u5355\u5143\u548c\u5408\u6210\u5e73\u884c\u6570\u636e\u7684\u65b9\u6cd5\u5bf9\u4e8e\u6539\u5584\u6ce2\u65af\u8bed-\u82f1\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u76f4\u63a5\u8bed\u97f3\u7ffb\u8bd1\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2511.11675", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11675", "abs": "https://arxiv.org/abs/2511.11675", "authors": ["Junchen Liu", "Yi Sheng"], "title": "Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff", "comment": null, "summary": "As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5411\u526a\u679d-\u518d\u751f\u7b56\u7565\uff0c\u4ece\u6781\u7aef\u538b\u7f29\u7f51\u7edc\u5f00\u59cb\u9009\u62e9\u6027\u518d\u751f\u5173\u952e\u8fde\u63a5\uff0c\u89e3\u51b3\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u7684\u95ee\u9898", "motivation": "\u5f53\u7a00\u758f\u5ea6\u8d85\u8fc7\u7279\u5b9a\u9608\u503c\u65f6\uff0c\u8fed\u4ee3\u548c\u4e00\u6b21\u6027\u526a\u679d\u65b9\u6cd5\u90fd\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u53ef\u5b9e\u73b0\u7684\u538b\u7f29\u6bd4\uff0c\u65e0\u6cd5\u6ee1\u8db3\u67d0\u4e9b\u786c\u4ef6\u5e73\u53f0\u7684\u4e25\u683c\u5c3a\u5bf8\u7ea6\u675f", "method": "\u53cc\u5411\u526a\u679d-\u518d\u751f\u7b56\u7565\uff1a\u4ece\u6ee1\u8db3\u786c\u4ef6\u7ea6\u675f\u7684\u6781\u7aef\u538b\u7f29\u7f51\u7edc\u5f00\u59cb\uff0c\u9009\u62e9\u6027\u518d\u751f\u5173\u952e\u8fde\u63a5\u4ee5\u6062\u590d\u4e22\u5931\u7684\u6027\u80fd", "result": "\u6709\u6548\u7f13\u89e3\u4e86\u9ad8\u7a00\u758f\u5ea6\u6761\u4ef6\u4e0b\u5e38\u89c1\u7684\u7cbe\u5ea6\u6025\u5267\u4e0b\u964d\u95ee\u9898", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7a81\u7834\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u7684\u538b\u7f29\u9650\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u6ee1\u8db3\u786c\u4ef6\u5c3a\u5bf8\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u6027\u80fd"}}
{"id": "2511.12677", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12677", "abs": "https://arxiv.org/abs/2511.12677", "authors": ["Oliver Joergensen", "Dominik Drexler", "Jendrik Seipp"], "title": "Dynamic Tree Databases in Automated Planning", "comment": null, "summary": "A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6811\u6570\u636e\u5e93\u53d8\u4f53\uff0c\u7528\u4e8e\u538b\u7f29\u547d\u9898\u548c\u6570\u503c\u53d8\u91cf\u7684\u72b6\u6001\u96c6\uff0c\u5728\u4fdd\u6301\u9759\u6001\u6811\u6570\u636e\u5e93\u4f18\u70b9\u7684\u540c\u65f6\u907f\u514d\u4e86\u5927\u91cf\u5185\u5b58\u9884\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u5728\u5927\u578b\u4efb\u52a1\u4e2d\u6269\u5c55\u663e\u5f0f\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u65f6\uff0c\u5982\u4f55\u7d27\u51d1\u8868\u793a\u751f\u6210\u7684\u72b6\u6001\u96c6\u5408\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u9759\u6001\u6811\u6570\u636e\u5e93\u867d\u7136\u6bcf\u4e2a\u72b6\u6001\u53ea\u9700\u8981\u6052\u5b9a\u7a7a\u95f4\uff0c\u4f46\u9700\u8981\u5927\u91cf\u5185\u5b58\u9884\u5206\u914d\u3002", "method": "\u5f00\u53d1\u4e86\u6811\u6570\u636e\u5e93\u7684\u52a8\u6001\u53d8\u4f53\uff0c\u7528\u4e8e\u538b\u7f29\u547d\u9898\u548c\u6570\u503c\u53d8\u91cf\u7684\u72b6\u6001\u96c6\uff0c\u5e76\u8bc1\u660e\u5176\u4fdd\u6301\u4e86\u9759\u6001\u5bf9\u5e94\u7269\u7684\u7406\u60f3\u7279\u6027\u3002", "result": "\u5728\u7ecf\u5178\u548c\u6570\u503c\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u538b\u7f29\u6bd4\u8fbe\u5230\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u901a\u5e38\u8fd0\u884c\u65f6\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u52a8\u6001\u6811\u6570\u636e\u5e93\u5728\u72b6\u6001\u538b\u7f29\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13540", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13540", "abs": "https://arxiv.org/abs/2511.13540", "authors": ["Zichong Wang", "Zhipeng Yin", "Liping Yang", "Jun Zhuang", "Rui Yu", "Qingzhao Kong", "Wenbin Zhang"], "title": "Fairness-Aware Graph Representation Learning with Limited Demographic Information", "comment": null, "summary": "Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86FairGLite\u6846\u67b6\uff0c\u5728\u6709\u9650\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0b\u5b9e\u73b0\u516c\u5e73\u56fe\u5b66\u4e60\uff0c\u901a\u8fc7\u751f\u6210\u4eba\u53e3\u7edf\u8ba1\u4ee3\u7406\u3001\u5f3a\u5236\u8de8\u7ec4\u5d4c\u5165\u4e00\u81f4\u6027\u548c\u81ea\u9002\u5e94\u7f6e\u4fe1\u7b56\u7565\u6765\u7f13\u89e3\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u56fe\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u5047\u8bbe\u80fd\u5b8c\u5168\u8bbf\u95ee\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u4f46\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5f88\u5c11\u80fd\u6ee1\u8db3\uff0c\u56e0\u4e3a\u5b58\u5728\u9690\u79c1\u3001\u6cd5\u5f8b\u6216\u76d1\u7ba1\u9650\u5236\u3002", "method": "\u4f7f\u7528\u90e8\u5206\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u751f\u6210\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4ee3\u7406\uff0c\u8bbe\u8ba1\u8de8\u4eba\u53e3\u7edf\u8ba1\u7ec4\u8282\u70b9\u5d4c\u5165\u4e00\u81f4\u6027\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u81ea\u9002\u5e94\u7f6e\u4fe1\u7b56\u7565\u6765\u52a8\u6001\u8c03\u6574\u8282\u70b9\u5bf9\u516c\u5e73\u6027\u548c\u6548\u7528\u7684\u8d21\u732e\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eFairGLite\u5728\u7fa4\u4f53\u516c\u5e73\u6307\u6807\u4e0a\u8fbe\u5230\u53ef\u8bc1\u660e\u7684\u4e0a\u754c\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u516c\u5e73\u56fe\u5b66\u4e60\u6846\u67b6\u4e2d\u80fd\u6709\u6548\u7f13\u89e3\u504f\u89c1\u5e76\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002", "conclusion": "FairGLite\u4e3a\u6709\u9650\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0b\u7684\u516c\u5e73\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.12710", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12710", "abs": "https://arxiv.org/abs/2511.12710", "authors": ["Yunhao Chen", "Xin Wang", "Juncheng Li", "Yixu Wang", "Jie Li", "Yan Teng", "Yingchun Wang", "Xingjun Ma"], "title": "Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs", "comment": null, "summary": "Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \\textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.", "AI": {"tldr": "EvoSynth\u662f\u4e00\u4e2a\u81ea\u4e3b\u7684\u8fdb\u5316\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u4e3b\u8bbe\u8ba1\u3001\u8fdb\u5316\u548c\u6267\u884c\u57fa\u4e8e\u4ee3\u7801\u7684\u65b0\u578b\u653b\u51fb\u7b97\u6cd5\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u81ea\u52a8\u5316\u7ea2\u961f\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684LLM\u81ea\u52a8\u5316\u7ea2\u961f\u6846\u67b6\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u5176\u8d8a\u72f1\u903b\u8f91\u4ec5\u9650\u4e8e\u9009\u62e9\u3001\u7ec4\u5408\u6216\u6539\u8fdb\u5df2\u6709\u7684\u653b\u51fb\u7b56\u7565\uff0c\u65e0\u6cd5\u81ea\u4e3b\u53d1\u660e\u5168\u65b0\u7684\u653b\u51fb\u673a\u5236\uff0c\u9650\u5236\u4e86\u5176\u521b\u9020\u529b\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u4e3b\u8bbe\u8ba1\u57fa\u4e8e\u4ee3\u7801\u7684\u653b\u51fb\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u7ea7\u81ea\u6821\u6b63\u5faa\u73af\u8fed\u4ee3\u91cd\u5199\u653b\u51fb\u903b\u8f91\uff0c\u5b9e\u73b0\u4ece\u653b\u51fb\u89c4\u5212\u5230\u8fdb\u5316\u5408\u6210\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "result": "\u5728\u9ad8\u5ea6\u9c81\u68d2\u7684Claude-Sonnet-4.5\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8685.5%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u751f\u6210\u7684\u653b\u51fb\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u52a0\u591a\u6837\u5316\u3002", "conclusion": "EvoSynth\u5f00\u521b\u4e86\u8d8a\u72f1\u65b9\u6cd5\u8fdb\u5316\u5408\u6210\u7684\u65b0\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6846\u67b6\u548c\u601d\u8def\u3002"}}
{"id": "2511.11676", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11676", "abs": "https://arxiv.org/abs/2511.11676", "authors": ["Hanchen David Wang", "Siwoo Bae", "Zirong Chen", "Meiyi Ma"], "title": "Learning with Preserving for Continual Multitask Learning", "comment": "25 pages, 16 figures, accepted at AAAI-2026", "summary": "Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86Learning with Preserving (LwP)\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u6301\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u89e3\u51b3\u6301\u7eed\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u653e\u7f13\u51b2\u533a\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u5f71\u50cf\u5206\u6790\u7b49\u5173\u952e\u9886\u57df\uff0cAI\u7cfb\u7edf\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u800c\u4e0d\u5fd8\u8bb0\u5df2\u5b66\u80fd\u529b\u3002\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u6b64\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u5b66\u4e60\u7684\u662f\u788e\u7247\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\uff0c\u5bb9\u6613\u76f8\u4e92\u5e72\u6270\u3002", "method": "\u5f15\u5165LwP\u6846\u67b6\u548c\u52a8\u6001\u52a0\u6743\u8ddd\u79bb\u4fdd\u6301(DWDP)\u635f\u5931\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u6f5c\u5728\u6570\u636e\u8868\u793a\u4e4b\u95f4\u7684\u6210\u5bf9\u8ddd\u79bb\u6765\u9632\u6b62\u8868\u793a\u6f02\u79fb\uff0c\u4fdd\u6301\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLwP\u4e0d\u4ec5\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u8fd8\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u662f\u552f\u4e00\u8d85\u8d8a\u5f3a\u5355\u4efb\u52a1\u5b66\u4e60\u57fa\u7ebf\u7684\u65b9\u6cd5\u3002", "conclusion": "LwP\u65b9\u6cd5\u5728\u6301\u7eed\u591a\u4efb\u52a1\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5bf9\u5206\u5e03\u504f\u79fb\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2511.12754", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12754", "abs": "https://arxiv.org/abs/2511.12754", "authors": ["Benjamin Li", "Shuyang Shi", "Lucia Romero", "Huao Li", "Yaqi Xie", "Woojun Kim", "Stefanos Nikolaidis", "Michael Lewis", "Katia Sycara", "Simon Stepputtis"], "title": "Adaptively Coordinating with Novel Partners via Learned Latent Strategies", "comment": "Accepted to NeurIPS 2025", "summary": "Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\u6761\u4ef6\u5408\u4f5c\u8005\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7b56\u7565\u7a7a\u95f4\uff0c\u805a\u7c7b\u8bc6\u522b\u7b56\u7565\u7c7b\u578b\uff0c\u5e76\u8bad\u7ec3\u57fa\u4e8e\u8fd9\u4e9b\u7b56\u7565\u7684\u5408\u4f5c\u8005\u4ee3\u7406\uff0c\u7ed3\u5408\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u9002\u5e94\u65b0\u4f19\u4f34\u3002", "motivation": "\u5728\u4eba\u7c7b-\u4ee3\u7406\u56e2\u961f\u4e2d\uff0c\u4eba\u5de5\u4ee3\u7406\u9700\u8981\u5b9e\u65f6\u9002\u5e94\u5177\u6709\u72ec\u7279\u504f\u597d\u548c\u52a8\u6001\u53d8\u5316\u7b56\u7565\u7684\u4eba\u7c7b\u4f19\u4f34\uff0c\u8fd9\u5728\u65f6\u95f4\u538b\u529b\u548c\u590d\u6742\u7b56\u7565\u7a7a\u95f4\u7684\u4efb\u52a1\u4e2d\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ece\u4ee3\u7406\u8f68\u8ff9\u6570\u636e\u4e2d\u5b66\u4e60\u6f5c\u5728\u7b56\u7565\u7a7a\u95f4\uff0c\u901a\u8fc7\u805a\u7c7b\u8bc6\u522b\u4e0d\u540c\u7b56\u7565\u7c7b\u578b\uff0c\u8bad\u7ec3\u57fa\u4e8e\u7b56\u7565\u7c7b\u578b\u7684\u5408\u4f5c\u8005\u4ee3\u7406\uff0c\u5e76\u5229\u7528\u56fa\u5b9a\u4efd\u989d\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u8fdb\u884c\u5728\u7ebf\u9002\u5e94\u3002", "result": "\u5728\u4fee\u6539\u7248Overcooked\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u548c\u5728\u7ebf\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0e\u65b0\u4eba\u7c7b\u548c\u4ee3\u7406\u961f\u53cb\u914d\u5bf9\u65f6\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u6761\u4ef6\u5408\u4f5c\u8005\u6846\u67b6\u80fd\u591f\u6709\u6548\u8868\u793a\u3001\u5206\u7c7b\u548c\u5b9e\u65f6\u9002\u5e94\u5e7f\u6cdb\u7684\u6f5c\u5728\u4f19\u4f34\u7b56\u7565\uff0c\u5728\u590d\u6742\u534f\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.12240", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12240", "abs": "https://arxiv.org/abs/2511.12240", "authors": ["Vishal Joshua Meesala"], "title": "SCI: An Equilibrium for Signal Intelligence", "comment": "34 pages, 7 figures. Preprint", "summary": "We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] (\"Surgical Precision\") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.", "AI": {"tldr": "SCI\u662f\u4e00\u4e2a\u95ed\u73af\u63a7\u5236\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u5efa\u6a21\u4e3a\u53d7\u63a7\u72b6\u6001\uff0c\u901a\u8fc7Lyapunov\u5f15\u5bfc\u7684\u63a7\u5236\u5668\u4e3b\u52a8\u9a71\u52a8\u89e3\u91ca\u7cbe\u5ea6\uff0c\u5728\u591a\u4e2a\u9886\u57df\u663e\u8457\u964d\u4f4e\u89e3\u91ca\u8bef\u5dee\u5e76\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u89e3\u91ca\u5668\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u8c03\u8282\u89e3\u91ca\u7cbe\u5ea6\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u6846\u67b6\u3002", "method": "SCI\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u53ef\u9760\u6027\u52a0\u6743\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u3001\u77e5\u8bc6\u5f15\u5bfc\u7684\u89e3\u91ca\u5668\u3001\u4ee5\u53ca\u914d\u5907\u56de\u6eda\u548c\u4fe1\u4efb\u533a\u57df\u4fdd\u62a4\u7684Lyapunov\u5f15\u5bfc\u63a7\u5236\u5668\u3002", "result": "\u5728\u751f\u7269\u533b\u5b66\u3001\u5de5\u4e1a\u548c\u73af\u5883\u9886\u57df\uff0cSCI\u5c06\u89e3\u91ca\u8bef\u5dee\u964d\u4f4e25-42%\uff0c\u89e3\u91ca\u7a33\u5b9a\u6027\u663e\u8457\u63d0\u9ad8\uff08\u65b9\u5dee\u4ece0.030\u964d\u81f30.011\uff09\uff0c\u540c\u65f6\u4fdd\u6301AUC/F1\u5728\u57fa\u7ebf1-2\u4e2a\u767e\u5206\u70b9\u5185\u3002", "conclusion": "\u5c06\u53ef\u89e3\u91ca\u6027\u5efa\u6a21\u4e3a\u63a7\u5236\u76ee\u6807\u80fd\u591f\u5728\u4e0d\u540c\u4fe1\u53f7\u673a\u5236\u4e0b\u4ea7\u751f\u66f4\u7a33\u5b9a\u3001\u6062\u590d\u66f4\u5feb\u4e14\u66f4\u53ef\u4fe1\u7684\u89e3\u91ca\u884c\u4e3a\u3002"}}
{"id": "2511.12712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12712", "abs": "https://arxiv.org/abs/2511.12712", "authors": ["Christopher Cruz"], "title": "Adaptive Focus Memory for Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.", "AI": {"tldr": "AFM\u662f\u4e00\u79cd\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u901a\u8fc7\u4e09\u79cd\u4fdd\u771f\u5ea6\u7ea7\u522b\u667a\u80fd\u7ba1\u7406\u5bf9\u8bdd\u5386\u53f2\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u80fd\u7684\u540c\u65f6\u5c06\u5e73\u5747token\u4f7f\u7528\u91cf\u51cf\u5c1166%\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u7b80\u5355\u5185\u5b58\u7b56\u7565\u7684\u74f6\u9888\u95ee\u9898\uff0c\u907f\u514d\u5b8c\u6574\u56de\u653e\u7684\u9ad8\u6210\u672c\u6216\u9759\u6001\u6458\u8981\u5bfc\u81f4\u7684\u5b89\u5168\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3001\u534a\u8870\u671f\u6743\u91cd\u548c\u91cd\u8981\u6027\u5206\u7c7b\uff0c\u4e3a\u6bcf\u6761\u5386\u53f2\u6d88\u606f\u5206\u914dFULL\u3001COMPRESSED\u6216PLACEHOLDER\u4fdd\u771f\u5ea6\u7ea7\u522b\uff0c\u5728\u4e25\u683ctoken\u9884\u7b97\u4e0b\u6309\u65f6\u95f4\u987a\u5e8f\u6253\u5305\u6d88\u606f\u3002", "result": "\u5728\u6d89\u53ca\u82b1\u751f\u8fc7\u654f\u7528\u6237\u7684\u65c5\u884c\u89c4\u5212\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAFM\u5728\u77ed\u4e2d\u957f\u5bf9\u8bdd\u4e2d\u5747\u80fd\u4fdd\u7559\u8fc7\u654f\u4fe1\u606f\uff0c\u5b89\u5168\u6027\u80fd\u4e0e\u5b8c\u6574\u56de\u653e\u76f8\u5f53\uff0c\u5e73\u5747token\u4f7f\u7528\u91cf\u51cf\u5c1166%\u3002", "conclusion": "AFM\u63d0\u4f9b\u6a21\u5757\u5316Python\u5b9e\u73b0\uff0c\u53ef\u5728\u4e0d\u727a\u7272\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u8fde\u7eed\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2511.11677", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11677", "abs": "https://arxiv.org/abs/2511.11677", "authors": ["Shimiao Li", "Aaron Tuor", "Draguna Vrabie", "Larry Pileggi", "Jan Drgona"], "title": "Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow", "comment": "paper submitted to PES General Meeting 2026", "summary": "Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \\textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540c\u4f26\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u53c2\u6570\u5316\u4ea4\u6d41\u6700\u4f18\u6f6e\u6d41\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u4ece\u677e\u5f1b\u95ee\u9898\u5230\u539f\u59cb\u95ee\u9898\u7684\u8fde\u7eed\u53d8\u5f62\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u6536\u655b\u7a33\u5b9a\u6027\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5b66\u4e60\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u975e\u51f8\u7684AC-OPF\u95ee\u9898\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u6536\u655b\u5230\u53ef\u884c\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u6536\u655b\u7a33\u5b9a\u6027\u548c\u53ef\u884c\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u540c\u4f26\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u5177\u6709\u5e7f\u6cdb\u5438\u5f15\u57df\u7684\u677e\u5f1b\u95ee\u9898\u5f00\u59cb\uff0c\u9010\u6b65\u53d8\u5f62\u5230\u539f\u59cb\u95ee\u9898\uff0c\u65e0\u9700\u6807\u8bb0\u7684\u6700\u4f18\u89e3\u6216\u5916\u90e8\u6c42\u89e3\u5668\u3002", "result": "\u5728\u6807\u51c6IEEE AC-OPF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u975e\u540c\u4f26\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u884c\u6027\u7387\uff0c\u540c\u65f6\u83b7\u5f97\u4e86\u4e0e\u5b8c\u6574OPF\u6c42\u89e3\u5668\u76f8\u5f53\u7684\u76ee\u6807\u51fd\u6570\u503c\u3002", "conclusion": "\u540c\u4f26\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u7535\u529b\u7cfb\u7edf\u4f18\u5316\u4e2d\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u3001\u7ea6\u675f\u611f\u77e5\u7684\u5b66\u4e60\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2511.12759", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12759", "abs": "https://arxiv.org/abs/2511.12759", "authors": ["James Moore"], "title": "Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces", "comment": null, "summary": "Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.", "AI": {"tldr": "\u73b0\u4ee3\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u968f\u673a\u6e38\u8d70\u53ef\u4ee5\u4ea7\u751f\u4e0e\u4eba\u7c7b\u8bed\u4e49\u6d41\u7545\u6027\u4efb\u52a1\u4e2d\u89c2\u5bdf\u5230\u7684\u4f18\u5316\u89c5\u98df\u884c\u4e3a\u4e00\u81f4\u7684\u7ed3\u679c\uff0c\u800c\u66f4\u590d\u6742\u7684Metropolis-Hastings\u91c7\u6837\u53cd\u800c\u4e0e\u4eba\u7c7b\u884c\u4e3a\u4e0d\u7b26\u3002", "motivation": "\u7814\u7a76\u73b0\u4ee3\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u662f\u5426\u80fd\u63d0\u4f9b\u8db3\u591f\u7684\u8868\u793a\u80fd\u529b\uff0c\u4f7f\u7b97\u6cd5\u80fd\u591f\u5339\u914d\u4eba\u7c7b\u5728\u8bed\u4e49\u6d41\u7545\u6027\u4efb\u52a1\u4e2d\u89c2\u5bdf\u5230\u7684\u89c5\u98df\u6a21\u5f0f\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5d4c\u5165\u8868\u793a\u548c\u5148\u524d\u7684\u8bed\u4e49\u6d41\u7545\u6027\u6570\u636e\uff0c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e0a\u8fdb\u884c\u968f\u673a\u6e38\u8d70\u548cMetropolis-Hastings\u91c7\u6837\uff0c\u6bd4\u8f83\u5b83\u4eec\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "result": "\u7b80\u5355\u7684\u968f\u673a\u6e38\u8d70\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4ea7\u751f\u4e86\u4e0e\u4f18\u5316\u89c5\u98df\u548c\u8fb9\u9645\u4ef7\u503c\u5b9a\u7406\u4e00\u81f4\u7684\u7ed3\u679c\uff0c\u800c\u66f4\u590d\u6742\u7684Metropolis-Hastings\u91c7\u6837\u53cd\u800c\u65e0\u6cd5\u4ea7\u751f\u4e0e\u4eba\u7c7b\u884c\u4e3a\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "conclusion": "\u9002\u5f53\u7ed3\u6784\u5316\u7684\u5d4c\u5165\u7a7a\u95f4\u5373\u4f7f\u4f7f\u7528\u7b80\u5355\u91c7\u6837\u4e5f\u80fd\u4ea7\u751f\u63a5\u8fd1\u4f18\u5316\u7684\u89c5\u98df\u52a8\u6001\uff0c\u6311\u6218\u4e86\u590d\u6742\u91c7\u6837\u673a\u5236\u5fc5\u7136\u80fd\u4ea7\u751f\u66f4\u597d\u8ba4\u77e5\u6a21\u578b\u7684\u5047\u8bbe\uff0c\u652f\u6301Hills(2012)\u800c\u975eAbbott(2015)\u7684\u89c2\u70b9\u3002"}}
{"id": "2511.12467", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12467", "abs": "https://arxiv.org/abs/2511.12467", "authors": ["Jiachen Qian", "Yang Zheng"], "title": "Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction", "comment": null, "summary": "This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u5728\u7ebf\u591a\u6b65\u9884\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6761\u4ef6\u5206\u5e03\u7406\u8bba\u7684\u6700\u4f18\u9884\u6d4b\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5728\u7ebf\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u6765\u5b66\u4e60\u8be5\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u76f8\u5bf9\u4e8e\u6700\u4f18\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u5bf9\u6570\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u5728\u7ebf\u591a\u6b65\u9884\u6d4b\u95ee\u9898\uff0c\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u5728\u7ebf\u5b66\u4e60\u5e76\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u7684\u9884\u6d4b\u7b97\u6cd5\uff0c\u800c\u4e0d\u9700\u8981\u4e8b\u5148\u77e5\u9053\u7cfb\u7edf\u6a21\u578b\u3002", "method": "\u5229\u7528\u6761\u4ef6\u5206\u5e03\u7406\u8bba\u63a8\u5bfc\u9884\u6d4b\u7b56\u7565\u7684\u6700\u4f18\u53c2\u6570\u5316\u5f62\u5f0f\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u672a\u6765\u8f93\u5165\u3001\u8fc7\u53bb\u8f93\u5165\u548c\u8fc7\u53bb\u8f93\u51fa\u7684\u7ebf\u6027\u51fd\u6570\uff0c\u7136\u540e\u63d0\u51fa\u5728\u7ebf\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u6765\u5b66\u4e60\u8be5\u7b56\u7565\u3002", "result": "\u5728\u7ebf\u7b97\u6cd5\u5728\u591a\u6b65\u9884\u6d4b\u8bbe\u7f6e\u4e0b\u76f8\u5bf9\u4e8e\u6700\u4f18\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5b9e\u73b0\u4e86\u5bf9\u6570\u9057\u61be\u754c\uff0c\u4e14\u5bf9\u4e8e\u8db3\u591f\u5927\u7684\u65f6\u95f4\u8303\u56f4N\uff0c\u5efa\u7acb\u4e86\u4e0d\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u5931\u8d25\u6982\u7387\u7684\u51e0\u4e4e\u5fc5\u7136\u9057\u61be\u754c\u3002\u9057\u61be\u7684\u5e38\u6570\u56e0\u5b50\u968f\u9884\u6d4b\u8303\u56f4H\u591a\u9879\u5f0f\u589e\u957f\uff0c\u591a\u9879\u5f0f\u9636\u6570\u7531\u7cfb\u7edf\u77e9\u9635\u4e2d\u7279\u5f81\u503c\u4e3a1\u7684\u6700\u5927Jordan\u5757\u51b3\u5b9a\u3002", "conclusion": "\u63d0\u51fa\u7684\u5728\u7ebf\u7b97\u6cd5\u80fd\u591f\u5728\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u4e2d\u6709\u6548\u8fdb\u884c\u591a\u6b65\u9884\u6d4b\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff0c\u4e14\u9057\u61be\u5206\u6790\u63ed\u793a\u4e86\u9884\u6d4b\u8303\u56f4\u5bf9\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\u89c4\u5f8b\u3002"}}
{"id": "2511.12728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12728", "abs": "https://arxiv.org/abs/2511.12728", "authors": ["Lea Hergert", "G\u00e1bor Berend", "Mario Szegedy", "Gyorgy Turan", "M\u00e1rk Jelasity"], "title": "On the Brittleness of LLMs: A Journey around Set Membership", "comment": null, "summary": "Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \\{pear, plum, apple, raspberry\\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.", "AI": {"tldr": "LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u8d85\u4eba\u7c7b\uff0c\u4f46\u5728\u7b80\u5355\u96c6\u5408\u6210\u5458\u67e5\u8be2\u4efb\u52a1\u4e2d\u5374\u9891\u7e41\u5931\u8d25\uff0c\u663e\u793a\u51fa\u5176\u63a8\u7406\u80fd\u529b\u7684\u8106\u5f31\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u7b80\u5355\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u6096\u8bba\uff0c\u901a\u8fc7\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u6765\u63ed\u793a\u57fa\u672c\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u96c6\u5408\u6210\u5458\u67e5\u8be2\u4efb\u52a1\uff0c\u7cfb\u7edf\u8bc4\u4f30\u63d0\u793a\u63aa\u8f9e\u3001\u8bed\u4e49\u7ed3\u6784\u3001\u5143\u7d20\u6392\u5e8f\u548c\u6a21\u578b\u9009\u62e9\u7b49\u7ef4\u5ea6\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u3002", "result": "LLMs\u5728\u8fd9\u4e00\u57fa\u7840\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u59cb\u7ec8\u8106\u5f31\uff0c\u5728\u6240\u6709\u7ef4\u5ea6\u4e0a\u90fd\u4e0d\u53ef\u9884\u6d4b\uff0c\u8868\u660e\u6a21\u578b\u5bf9\u96c6\u5408\u6982\u5ff5\u7684\u7406\u89e3\u662f\u788e\u7247\u5316\u548c\u590d\u6742\u7684\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u95ee\u9898\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u80fd\u591f\u5168\u9762\u6620\u5c04\u548c\u5206\u6790\u5931\u8d25\u6a21\u5f0f\uff0c\u8fd9\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2511.11679", "categories": ["cs.LG", "cs.CV", "cs.GR", "math.CV", "math.DG"], "pdf": "https://arxiv.org/pdf/2511.11679", "abs": "https://arxiv.org/abs/2511.11679", "authors": ["Zhehao Xu", "Lok Ming Lui"], "title": "A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications", "comment": null, "summary": "Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86SBN-Opt\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u4ee3\u7406\u7f51\u7edcSBN\u5d4c\u5165LSQC\u80fd\u91cf\uff0c\u4f18\u5316\u81ea\u7531\u8fb9\u754c\u5fae\u5206\u540c\u80da\u6620\u5c04\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u503c\u7b97\u6cd5\u65e0\u6cd5\u5e94\u7528\u4e8e\u68af\u5ea6\u4f18\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u7531\u8fb9\u754c\u5fae\u5206\u540c\u80da\u4f18\u5316\u5728\u8868\u9762\u6620\u5c04\u95ee\u9898\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6570\u503cLSQC\u7406\u8bba\u9700\u8981\u5730\u6807\u6761\u4ef6\u4e14\u65e0\u6cd5\u7528\u4e8e\u68af\u5ea6\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51faSBN\u795e\u7ecf\u4ee3\u7406\u7f51\u7edc\uff0c\u5c06LSQC\u80fd\u91cf\u5d4c\u5165\u591a\u5c3a\u5ea6\u7f51\u683c\u8c31\u67b6\u6784\u4e2d\uff0c\u7136\u540e\u6784\u5efaSBN-Opt\u4f18\u5316\u6846\u67b6\u6765\u4f18\u5316\u81ea\u7531\u8fb9\u754c\u5fae\u5206\u540c\u80da\u6620\u5c04\u3002", "result": "\u5728\u5bc6\u5ea6\u5747\u8861\u6620\u5c04\u548c\u4e0d\u4e00\u81f4\u8868\u9762\u914d\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSBN-Opt\u4f18\u4e8e\u4f20\u7edf\u6570\u503c\u7b97\u6cd5\u3002", "conclusion": "SBN-Opt\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u7531\u8fb9\u754c\u5fae\u5206\u540c\u80da\u4f18\u5316\u95ee\u9898\uff0c\u80fd\u591f\u663e\u5f0f\u63a7\u5236\u5c40\u90e8\u51e0\u4f55\u7578\u53d8\uff0c\u5728\u8868\u9762\u6620\u5c04\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12769", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12769", "abs": "https://arxiv.org/abs/2511.12769", "authors": ["Luyao Niu", "Zepu Wang", "Shuyi Guan", "Yang Liu", "Peng Sun"], "title": "Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting", "comment": null, "summary": "While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.", "AI": {"tldr": "Event-CausNet\u6846\u67b6\u5229\u7528LLM\u91cf\u5316\u975e\u7ed3\u6784\u5316\u4e8b\u4ef6\u62a5\u544a\uff0c\u6784\u5efa\u56e0\u679c\u77e5\u8bc6\u5e93\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u5c06\u56e0\u679c\u77e5\u8bc6\u6ce8\u5165GNN-LSTM\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u975e\u91cd\u590d\u6027\u4e8b\u4ef6\u4e0b\u7684\u4ea4\u901a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65f6\u7a7aGNN\u5728\u5904\u7406\u91cd\u590d\u6027\u4ea4\u901a\u6a21\u5f0f\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e8b\u6545\u7b49\u975e\u91cd\u590d\u6027\u4e8b\u4ef6\u4e2d\u53ef\u9760\u6027\u6025\u5267\u4e0b\u964d\uff0c\u56e0\u4e3aGNN\u672c\u8d28\u4e0a\u662f\u76f8\u5173\u6027\u6a21\u578b\uff0c\u65e0\u6cd5\u5e94\u5bf9\u6270\u52a8\u5f15\u5165\u7684\u65b0\u56e0\u679c\u56e0\u7d20\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u975e\u7ed3\u6784\u5316\u4e8b\u4ef6\u62a5\u544a\uff0c\u901a\u8fc7\u4f30\u8ba1\u5e73\u5747\u5904\u7406\u6548\u5e94\u6784\u5efa\u56e0\u679c\u77e5\u8bc6\u5e93\uff0c\u91c7\u7528\u53cc\u6d41GNN-LSTM\u7f51\u7edc\u548c\u65b0\u578b\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u6765\u8c03\u6574\u548c\u589e\u5f3a\u9884\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvent-CausNet\u5c06\u9884\u6d4b\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\u4e86\u9ad8\u8fbe35.87%\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5f25\u5408\u4e86\u76f8\u5173\u6027\u6a21\u578b\u4e0e\u56e0\u679c\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u53ef\u8fc1\u79fb\u4e14\u5177\u6709\u5173\u952e\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5173\u952e\u6270\u52a8\u671f\u95f4\u7684\u5b9e\u9645\u4ea4\u901a\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2511.12603", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12603", "abs": "https://arxiv.org/abs/2511.12603", "authors": ["Hongyi Chen", "Jianhai Shu", "Jingtao Ding", "Yong Li", "Xiao-Ping Zhang"], "title": "PID-controlled Langevin Dynamics for Faster Sampling of Generative Models", "comment": "NeurIPS 2025 poster paper", "summary": "Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \\href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.", "AI": {"tldr": "PIDLD\u662f\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684Langevin\u52a8\u529b\u5b66\u91c7\u6837\u52a0\u901f\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u68af\u5ea6\uff08\u79ef\u5206\u9879\uff09\u548c\u68af\u5ea6\u8d8b\u52bf\uff08\u5bfc\u6570\u9879\uff09\u6765\u51cf\u5c11\u91c7\u6837\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\uff0c\u63d0\u9ad8\u751f\u6210\u901f\u5ea6\u3002", "motivation": "Langevin\u52a8\u529b\u5b66\u91c7\u6837\u5b58\u5728\u751f\u6210\u901f\u5ea6\u6781\u4f4e\u7684\u95ee\u9898\uff0c\u53d7\u9650\u4e8e\u9700\u8981\u5927\u91cf\u7ec6\u7c92\u5ea6\u8fed\u4ee3\u624d\u80fd\u6536\u655b\u5230\u76ee\u6807\u5206\u5e03\u3002", "method": "\u5c06\u91c7\u6837\u8fc7\u7a0b\u91cd\u65b0\u89e3\u91ca\u4e3a\u63a7\u5236\u7406\u8bba\u95ee\u9898\uff0c\u5c06\u80fd\u91cf\u68af\u5ea6\u89c6\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u7ed3\u5408PID\u63a7\u5236\u5668\u7684\u79ef\u5206\u9879\uff08\u5386\u53f2\u68af\u5ea6\uff09\u548c\u5bfc\u6570\u9879\uff08\u68af\u5ea6\u8d8b\u52bf\uff09\u6765\u9ad8\u6548\u904d\u5386\u80fd\u91cf\u666f\u89c2\u5e76\u81ea\u9002\u5e94\u7a33\u5b9a\u3002", "result": "\u5728\u56fe\u50cf\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPIDLD\u80fd\u4ee5\u66f4\u5c11\u7684\u6b65\u9aa4\u8fbe\u5230\u66f4\u9ad8\u7684\u8d28\u91cf\uff0c\u4f7f\u57fa\u4e8eLangevin\u7684\u751f\u6210\u6a21\u578b\u5728\u6548\u7387\u5173\u952e\u5e94\u7528\u4e2d\u66f4\u52a0\u5b9e\u7528\u3002", "conclusion": "PIDLD\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3001\u6570\u636e\u96c6\u6216\u5148\u9a8c\u4fe1\u606f\uff0c\u53ef\u7acb\u5373\u4e0e\u4efb\u4f55\u57fa\u4e8eLangevin\u7684\u65b9\u6cd5\u96c6\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\u3002"}}
{"id": "2511.12768", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12768", "abs": "https://arxiv.org/abs/2511.12768", "authors": ["Noah Hong", "Tao Hong"], "title": "Evidence of Phase Transitions in Small Transformer-Based Language Models", "comment": null, "summary": "Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u5b58\u5728\u76f8\u53d8\u91cd\u7ec4\u73b0\u8c61\uff0c\u5373\u4f7f\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u4e5f\u80fd\u89c2\u5bdf\u5230\uff0c\u4e14\u53ef\u901a\u8fc7\u8bcd\u6c47\u7edf\u8ba1\u65b9\u6cd5\u5728\u7ebf\u6027\u8bad\u7ec3\u7a7a\u95f4\u4e2d\u76f4\u63a5\u68c0\u6d4b\u5230\uff0c\u8fd9\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u975e\u7ebf\u6027\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u63a2\u7a76\u76f8\u53d8\u73b0\u8c61\u662f\u5426\u53ea\u5b58\u5728\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u5426\u5728\u7ebf\u6027\u8bad\u7ec3\u7a7a\u95f4\u4e2d\u76f4\u63a5\u68c0\u6d4b\uff0c\u4ee5\u53ca\u662f\u5426\u5728\u8bad\u7ec3\u65e9\u671f\u5c31\u51fa\u73b0\uff0c\u4ee5\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u975e\u7ebf\u6027\u52a8\u6001\u3002", "method": "\u8bad\u7ec3\u5c0f\u578bGPT\u98ce\u683ctransformer\u6a21\u578b\uff0c\u5206\u6790\u8bcd\u6c47\u4f7f\u7528\u6f14\u53d8\uff0c\u8ffd\u8e2a\u5e73\u5747\u8bcd\u957f\u3001\u6b63\u786e/\u9519\u8bef\u8bcd\u6570\u3001\u8bcd\u6c47\u591a\u6837\u6027\u53d8\u5316\uff0c\u5e76\u5e94\u7528\u6cca\u677e\u548c\u4e9a\u6cca\u677e\u7edf\u8ba1\u91cf\u5316\u8bcd\u6c47\u8fde\u63a5\u548c\u91cd\u7ec4\u3002", "result": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u660e\u663e\u7684\u76f8\u53d8\u70b9\uff0c\u8fd9\u4e9b\u8f6c\u53d8\u5728\u6807\u51c6\u635f\u5931\u66f2\u7ebf\u4e2d\u4e0d\u660e\u663e\uff0c\u4f46\u901a\u8fc7\u8bcd\u6c47\u548c\u7edf\u8ba1\u63a2\u9488\u53ef\u89c1\uff0c\u8868\u660e\u76f8\u53d8\u91cd\u7ec4\u662f\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u666e\u904d\u7279\u5f81\u3002", "conclusion": "\u76f8\u53d8\u91cd\u7ec4\u662f\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u4e00\u822c\u7279\u5f81\uff0c\u53ef\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\uff0c\u80fd\u5728\u7ebf\u6027\u8bad\u7ec3\u7a7a\u95f4\u4e2d\u76f4\u63a5\u68c0\u6d4b\uff0c\u5e76\u5728\u8bad\u7ec3\u65e9\u671f\u5c31\u51fa\u73b0\uff0c\u5f3a\u8c03\u4e86\u5b9a\u5236\u5316\u6307\u6807\u5bf9\u4e8e\u63ed\u793a\u76f8\u53d8\u884c\u4e3a\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.11680", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11680", "abs": "https://arxiv.org/abs/2511.11680", "authors": ["Udaya Bhasker Cheerala", "Varun Teja Chirukuri", "Venkata Akhil Kumar Gummadi", "Jintu Moni Bhuyan", "Praveen Damacharla"], "title": "Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP", "comment": "7 pages, 2025 IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS)", "summary": "Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u52a0\u5dde\u91ce\u706b\u98ce\u9669\u5730\u56fe\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u7ed3\u5408\u53ef\u89e3\u91caAI\uff08SHAP\uff09\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u68ee\u6797\u548c\u8349\u539f\u751f\u6001\u7cfb\u7edf\u7684\u5173\u952e\u98ce\u9669\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u91ce\u706b\u5bf9\u5168\u7403\u751f\u6001\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u52a0\u5dde\u56e0\u6c14\u5019\u3001\u5730\u5f62\u3001\u690d\u88ab\u548c\u4eba\u7c7b\u6d3b\u52a8\u7b49\u56e0\u7d20\u9891\u7e41\u53d1\u751f\u706b\u707e\u3002\u9700\u8981\u5f00\u53d1\u7efc\u5408\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u6765\u652f\u6301\u51b3\u7b56\u548c\u98ce\u9669\u7f13\u89e3\u7b56\u7565\u3002", "method": "\u5e94\u7528\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u7ed3\u5408SHAP\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u91c7\u7528\u7a7a\u95f4\u548c\u65f6\u95f4\u9a8c\u8bc1\u7b56\u7565\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8bc6\u522b\u751f\u6001\u7cfb\u7edf\u7279\u5b9a\u7684\u98ce\u9669\u9a71\u52a8\u56e0\u7d20\u3002", "result": "RF\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u68ee\u6797\u548c\u8349\u539f\u7684AUC\u5206\u522b\u8fbe\u52300.997\u548c0.996\u3002\u7a7a\u95f4\u4ea4\u53c9\u9a8c\u8bc1\u663e\u793a\u4e2d\u7b49\u53ef\u8f6c\u79fb\u6027\uff0c\u65f6\u95f4\u9a8c\u8bc1\u663e\u793a\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002SHAP\u5206\u6790\u8bc6\u522b\u51fa\u571f\u58e4\u6709\u673a\u78b3\u3001\u6811\u6728\u8986\u76d6\u548cNDVI\u662f\u68ee\u6797\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u800c\u5730\u8868\u6e29\u5ea6\u3001\u6d77\u62d4\u548c\u690d\u88ab\u5065\u5eb7\u6307\u6570\u4e3b\u5bfc\u8349\u539f\u98ce\u9669\u3002", "conclusion": "RF-SHAP\u6846\u67b6\u4e3a\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u7406\u89e3\u548c\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u77e5\u60c5\u51b3\u7b56\u548c\u5236\u5b9a\u6709\u9488\u5bf9\u6027\u7684\u98ce\u9669\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2511.12792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12792", "abs": "https://arxiv.org/abs/2511.12792", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Zehong Cao", "Ryszard Kowalczyk"], "title": "Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization", "comment": null, "summary": "This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5f02\u6784\u536b\u661f\u96c6\u7fa4\u5728\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u4e2d\u7684\u8d44\u6e90\u5206\u914d\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u5f02\u6784\u536b\u661f\u95f4\u7684\u6709\u6548\u534f\u8c03\uff0c\u5e73\u8861\u6210\u50cf\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b9e\u65f6\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u5206\u6563\u6027\u7279\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6765\u5b9e\u73b0\u81ea\u9002\u5e94\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u5730\u4ece\u5355\u536b\u661f\u5230\u591a\u536b\u661f\u573a\u666f\u5236\u5b9a\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528Basilisk\u548cBSK-RL\u6846\u67b6\u6784\u5efa\u8fd1\u771f\u5b9e\u4eff\u771f\u73af\u5883\uff0c\u8bc4\u4f30MAPPO\u3001HAPPO\u3001HATRPO\u7b49\u5148\u8fdbMARL\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793aMARL\u80fd\u591f\u5728\u5f02\u6784\u536b\u661f\u95f4\u5b9e\u73b0\u6709\u6548\u534f\u8c03\uff0c\u5e73\u8861\u6210\u50cf\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\uff0c\u540c\u65f6\u51cf\u8f7b\u975e\u5e73\u7a33\u6027\u548c\u667a\u80fd\u4f53\u95f4\u5956\u52b1\u8026\u5408\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u536b\u661f\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u4e3a\u5f02\u6784\u52a8\u6001\u6761\u4ef6\u4e0b\u667a\u80fd\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u89c4\u5212\u7684\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12852", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12852", "abs": "https://arxiv.org/abs/2511.12852", "authors": ["Jihoon Moon"], "title": "From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability", "comment": null, "summary": "Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684\u6846\u67b6\uff0c\u5c06\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u5316\u3001\u53ef\u63a7\u6027\u548c\u53ef\u89c2\u6027\u683c\u62c9\u59c6\u77e9\u9635\u4ee5\u53ca\u6c49\u514b\u5c14\u5947\u5f02\u503c\u6765\u5206\u6790\u5176\u5185\u90e8\u8ba1\u7b97\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u6027\u80fd\u4f18\u5f02\u4f46\u96be\u4ee5\u8fdb\u884c\u673a\u68b0\u89e3\u91ca\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u5206\u6790\u5176\u5185\u90e8\u8ba1\u7b97\u673a\u5236\u3002", "method": "\u5bf9\u7ed9\u5b9a\u8f93\u5165\uff0c\u5728\u9690\u85cf\u6fc0\u6d3b\u6a21\u5f0f\u5468\u56f4\u7ebf\u6027\u5316\u7f51\u7edc\uff0c\u6784\u5efa\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u901a\u8fc7\u8f93\u5165\u72b6\u6001\u548c\u72b6\u6001\u8f93\u51fa\u96c5\u53ef\u6bd4\u77e9\u9635\u5b9a\u4e49\u5c40\u90e8\u53ef\u63a7\u6027\u548c\u53ef\u89c2\u6027\u683c\u62c9\u59c6\u77e9\u9635\uff0c\u8ba1\u7b97\u6c49\u514b\u5c14\u5947\u5f02\u503c\u548c\u76f8\u5173\u6a21\u5f0f\u3002", "result": "\u5728\u7b80\u5355\u524d\u9988\u7f51\u7edc\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u6fc0\u6d3b\u9971\u548c\u5982\u4f55\u964d\u4f4e\u53ef\u63a7\u6027\u3001\u7f29\u5c0f\u4e3b\u5bfc\u6c49\u514b\u5c14\u5947\u5f02\u503c\uff0c\u5e76\u5c06\u4e3b\u5bfc\u5185\u90e8\u6a21\u5f0f\u8f6c\u79fb\u5230\u4e0d\u540c\u795e\u7ecf\u5143\u5b50\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u5316\u4e3a\u5c40\u90e8\u767d\u76d2\u52a8\u6001\u6a21\u578b\u96c6\u5408\uff0c\u4e3a\u526a\u679d\u6216\u7ea6\u675f\u63d0\u4f9b\u4e86\u81ea\u7136\u5019\u9009\u7684\u5185\u90e8\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.12782", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12782", "abs": "https://arxiv.org/abs/2511.12782", "authors": ["Thomas Rivasseau"], "title": "LLM Reinforcement in Context", "comment": "4 pages", "summary": "Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u4e2d\u65ad\u673a\u5236\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\uff0c\u901a\u8fc7\u5728\u7528\u6237\u8f93\u5165\u4e2d\u5b9a\u671f\u63d2\u5165\u63a7\u5236\u8bed\u53e5\u6765\u9632\u6b62\u8d8a\u72f1\u884c\u4e3a\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u8bad\u7ec3\u548c\u63d0\u793a\u6765\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4f46\u7f3a\u4e4f\u968f\u7740\u7528\u6237\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u800c\u589e\u5f3a\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0LLM\u8d8a\u72f1\u6982\u7387\u968f\u7528\u6237\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u800c\u4e0a\u5347\u3002", "method": "\u63d0\u51fa\u4e2d\u65ad\u673a\u5236\uff0c\u5728\u7528\u6237\u8f93\u5165\u4e2d\u6bcfx\u4e2atoken\u63d2\u5165\u63a7\u5236\u8bed\u53e5\uff0c\u5e76\u5efa\u8bae\u53ef\u5c06\u6b64\u65b9\u6cd5\u63a8\u5e7f\u5230\u601d\u7ef4\u94fe\u8fc7\u7a0b\u4ee5\u9632\u6b62\u7b56\u7565\u6027\u884c\u4e3a\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e2d\u65ad\u4f5c\u4e3a\u589e\u5f3aLLM\u5bf9\u9f50\u6027\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u4e2d\u65ad\u673a\u5236\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u968f\u7740\u7528\u6237\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u800c\u589e\u5f3a\u6a21\u578b\u5bf9\u9f50\u6027\uff0c\u9632\u6b62\u8d8a\u72f1\u548c\u7b56\u7565\u6027\u884c\u4e3a\u3002"}}
{"id": "2511.11681", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11681", "abs": "https://arxiv.org/abs/2511.11681", "authors": ["Penghui Niu", "Jiashuai She", "Taotao Cai", "Yajuan Zhang", "Ping Zhang", "Junhua Gu", "Jianxin Li"], "title": "MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation", "comment": null, "summary": "Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.", "AI": {"tldr": "\u63d0\u51fa\u4e86MPCM-Net\uff0c\u4e00\u79cd\u96c6\u6210\u90e8\u5206\u6ce8\u610f\u529b\u5377\u79ef\u4e0eMamba\u67b6\u6784\u7684\u591a\u5c3a\u5ea6\u7f51\u7edc\uff0c\u7528\u4e8e\u5730\u9762\u4e91\u56fe\u50cf\u5206\u5272\uff0c\u5728CSRC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5206\u5272\u7cbe\u5ea6\u4e0e\u63a8\u7406\u901f\u5ea6\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u4f9d\u8d56\u81a8\u80c0\u5377\u79ef\u7f3a\u4e4f\u901a\u9053\u95f4\u4e92\u64cd\u4f5c\u6027\uff1b\u6ce8\u610f\u529b\u673a\u5236\u5ffd\u89c6\u7cbe\u5ea6-\u541e\u5410\u91cf\u5e73\u8861\uff1b\u89e3\u7801\u5668\u4fee\u6539\u672a\u80fd\u5efa\u7acb\u5c42\u6b21\u5c40\u90e8\u7279\u5f81\u7684\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u7f16\u7801\u5668\u5305\u542bMPAC\u6a21\u5757\uff08MPC\u5757\u548cMPA\u5757\uff09\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u4e91\u5f62\u6210\u7684\u5168\u5c40\u7a7a\u95f4\u4ea4\u4e92\u548c\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7279\u5f81\u63d0\u53d6\uff1b\u89e3\u7801\u5668\u4f7f\u7528M2B\u6a21\u5757\u901a\u8fc7SSHD\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u8de8\u7a7a\u95f4\u548c\u5c3a\u5ea6\u7ef4\u5ea6\u7684\u6df1\u5ea6\u7279\u5f81\u805a\u5408\u3002", "result": "\u5728CSRC\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMPCM-Net\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "MPCM-Net\u901a\u8fc7\u96c6\u6210\u90e8\u5206\u6ce8\u610f\u529b\u5377\u79ef\u4e0eMamba\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u4e91\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u65b0\u7684CSRC\u6570\u636e\u96c6\u4f5c\u4e3a\u793e\u533a\u8d21\u732e\u3002"}}
{"id": "2511.12793", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12793", "abs": "https://arxiv.org/abs/2511.12793", "authors": ["Bowen He", "Xiaoan Xu", "Alper Kamil Bozkurt", "Vahid Tarokh", "Juncheng Dong"], "title": "Neuro-Logic Lifelong Learning", "comment": null, "summary": "Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ec8\u8eab\u5b66\u4e60ILP\uff0c\u5229\u7528\u903b\u8f91\u89c4\u5219\u7684\u7ec4\u5408\u6027\u548c\u53ef\u8f6c\u79fb\u6027\u6765\u9ad8\u6548\u5b66\u4e60\u65b0\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ec4\u5408\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u8be5\u8303\u5f0f\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7b26\u53f7AI\u4e2d\u7684ILP\u95ee\u9898\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u5927\u591a\u6570\u7814\u7a76\u4e13\u6ce8\u4e8e\u4e3a\u5355\u4e2a\u95ee\u9898\u8bbe\u8ba1\u65b0\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u800c\u8f83\u5c11\u63a2\u7d22\u6d89\u53ca\u95ee\u9898\u5e8f\u5217\u7684\u65b0\u5b66\u4e60\u8303\u5f0f\u3002", "method": "\u5f15\u5165\u4e86\u7ec4\u5408\u6846\u67b6\uff0c\u5c55\u793a\u5982\u4f55\u5c06\u65e9\u671f\u4efb\u52a1\u4e2d\u83b7\u53d6\u7684\u903b\u8f91\u89c4\u5219\u9ad8\u6548\u5730\u91cd\u7528\u4e8e\u540e\u7eed\u4efb\u52a1\uff0c\u4ece\u800c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "result": "\u5728\u4efb\u52a1\u5e8f\u5217\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u8303\u5f0f\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u4e00\u7814\u7a76\u4e3a\u795e\u7ecf\u7b26\u53f7AI\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.13103", "categories": ["cs.LG", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13103", "abs": "https://arxiv.org/abs/2511.13103", "authors": ["Vidur Sinha", "Muhammed Ustaomeroglu", "Guannan Qu"], "title": "Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions", "comment": "8 pages, 7 figures, submitted for review", "summary": "Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.", "AI": {"tldr": "STACCA\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u7f51\u7edc\u63a7\u5236\u95ee\u9898\uff0c\u80fd\u591f\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u5e76\u5728\u4e0d\u540c\u7f51\u7edc\u62d3\u6251\u95f4\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709MARL\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u4f9d\u8d56\u5c40\u90e8\u4ea4\u4e92\u8870\u51cf\u5047\u8bbe\uff0c\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff08\u5982\u7ea7\u8054\u6545\u969c\u3001\u75ab\u60c5\u7206\u53d1\uff09\uff1b2\uff09\u7f3a\u4e4f\u8de8\u7f51\u7edc\u62d3\u6251\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u5728\u65b0\u56fe\u4e0a\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "STACCA\u91c7\u7528\u96c6\u4e2d\u5f0f\u56feTransformer\u8bc4\u8bba\u5bb6\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u5e76\u63d0\u4f9b\u7cfb\u7edf\u7ea7\u53cd\u9988\uff0c\u5171\u4eab\u56feTransformer\u6267\u884c\u5668\u5b66\u4e60\u53ef\u6cdb\u5316\u7b56\u7565\uff0c\u5e76\u96c6\u6210\u65b0\u9896\u7684\u53cd\u4e8b\u5b9e\u4f18\u52bf\u4f30\u8ba1\u5668\u6539\u8fdb\u4fe1\u7528\u5206\u914d\u3002", "result": "\u5728\u75ab\u60c5\u63a7\u5236\u548c\u8c23\u8a00\u4f20\u64ad\u7f51\u7edc\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSTACCA\u5728\u6027\u80fd\u3001\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684MARL\u67b6\u6784\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u63a7\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12784", "categories": ["cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.12784", "abs": "https://arxiv.org/abs/2511.12784", "authors": ["Hayden Moore", "Asfahan Shah"], "title": "Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing", "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u5bf9\u8bed\u4e49\u76f8\u4f3c\u4f46\u8868\u8ff0\u4e0d\u540c\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u8f93\u51fa\u5bf9\u8868\u8ff0\u53d8\u5316\u654f\u611f\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u8868\u660eLLMs\u5728\u6587\u672c\u5230SQL\u4efb\u52a1\u4e2d\u5bf9\u8bed\u4e49\u4fdd\u6301\u7684\u6539\u5199\u8f93\u5165\u654f\u611f\uff0c\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u73b0\u8c61\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u9886\u57df\u662f\u5426\u540c\u6837\u5b58\u5728\u3002", "method": "\u4f7f\u7528MiniF2F\u548cLean 4\u7248\u672c\u7684ProofNet\u57fa\u51c6\uff0c\u751f\u6210\u8bed\u4e49\u76f8\u4f3c\u7684\u6539\u5199\u81ea\u7136\u8bed\u8a00\u9648\u8ff0\uff0c\u5e76\u5728\u4e24\u4e2a\u73b0\u4ee3LLMs\u4e0a\u8fdb\u884c\u4ea4\u53c9\u8bc4\u4f30\uff0c\u6d4b\u91cf\u8bed\u4e49\u548c\u7f16\u8bd1\u6709\u6548\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u6539\u5199\u8f93\u5165\u4e0a\u7684\u6027\u80fd\u5b58\u5728\u53d8\u5f02\u6027\uff0c\u81ea\u7136\u8bed\u8a00\u9648\u8ff0\u7684\u5fae\u5c0f\u53d8\u5316\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u3002", "conclusion": "LLMs\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u5bf9\u81ea\u7136\u8bed\u8a00\u8868\u8ff0\u53d8\u5316\u654f\u611f\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u5bf9\u8bed\u4e49\u76f8\u4f3c\u8f93\u5165\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.11683", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11683", "abs": "https://arxiv.org/abs/2511.11683", "authors": ["Longhua Li", "Lei Qi", "Xin Geng"], "title": "Stratified Knowledge-Density Super-Network for Scalable Vision Transformers", "comment": "Accepted by AAAI 2026", "summary": "Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \\textbf{W}eighted \\textbf{P}CA for \\textbf{A}ttention \\textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \\textbf{P}rogressive \\textbf{I}mportance-\\textbf{A}ware \\textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.", "AI": {"tldr": "\u63d0\u51faWPAC\u548cPIAD\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3ViT\u8f6c\u6362\u4e3a\u5206\u5c42\u77e5\u8bc6\u5bc6\u5ea6\u8d85\u7f51\u7edc\uff0c\u5b9e\u73b0\u6309\u9700\u63d0\u53d6\u4e0d\u540c\u5927\u5c0f\u7684\u5b50\u7f51\u7edc\uff0c\u5728\u6a21\u578b\u538b\u7f29\u548c\u6269\u5c55\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "motivation": "\u4e3a\u4e0d\u540c\u8d44\u6e90\u7ea6\u675f\u8bad\u7ec3\u548c\u90e8\u7f72\u591a\u4e2aViT\u6a21\u578b\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u6cd5\u4ece\u5355\u4e00\u6a21\u578b\u4e2d\u63d0\u53d6\u4e0d\u540c\u5927\u5c0f\u7684\u5b50\u7f51\u7edc", "method": "WPAC\u901a\u8fc7\u52a0\u6743PCA\u538b\u7f29\u6ce8\u610f\u529b\u5c42\u77e5\u8bc6\u5230\u5173\u952e\u6743\u91cd\uff1bPIAD\u901a\u8fc7\u6e10\u8fdb\u91cd\u8981\u6027\u611f\u77e5dropout\u4fc3\u8fdb\u77e5\u8bc6\u5206\u5c42\u7ec4\u7ec7", "result": "WPAC\u5728\u77e5\u8bc6\u96c6\u4e2d\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u526a\u679d\u6807\u51c6\uff0c\u4e0ePIAD\u7ed3\u5408\u5728\u6a21\u578b\u538b\u7f29\u548c\u6269\u5c55\u4efb\u52a1\u4e2d\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u5f0f\u4ece\u5355\u4e00\u9884\u8bad\u7ec3ViT\u4e2d\u63d0\u53d6\u4e0d\u540c\u5927\u5c0f\u7684\u5b50\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u90e8\u7f72\u6210\u672c"}}
{"id": "2511.12844", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12844", "abs": "https://arxiv.org/abs/2511.12844", "authors": ["Julia Santaniello", "Matthew Russell", "Benson Jiang", "Donatello Sassaroli", "Robert Jacob", "Jivko SInapov"], "title": "Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback", "comment": "Accepted to the Association for the Advancement of Artificial Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings", "summary": "Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.", "AI": {"tldr": "\u4f7f\u7528\u88ab\u52a8\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u548c\u529f\u80fd\u6027\u8fd1\u7ea2\u5916\u5149\u8c31\uff08fNIRS\uff09\u795e\u7ecf\u4fe1\u53f7\u6765\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u8bad\u7ec3\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b25\u540d\u53c2\u4e0e\u8005\u5728\u4e09\u4e2a\u9886\u57df\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u4ecefNIRS\u4fe1\u53f7\u9884\u6d4b\u4ee3\u7406\u6027\u80fd\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u795e\u7ecf\u53cd\u9988\u6765\u5bf9\u9f50\u4ee3\u7406\u884c\u4e3a\u4e0e\u4eba\u7c7b\u504f\u597d\uff0c\u4e3a\u672a\u6765\u8111\u9a71\u52a8\u7684RLHF\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u6536\u96c6fNIRS\u8bb0\u5f55\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u9884\u6d4b\u4ee3\u7406\u6027\u80fd\u6c34\u5e73\uff08\u6700\u4f18\u3001\u6b21\u4f18\u3001\u6700\u5dee\uff09\uff0c\u8bad\u7ec3\u56de\u5f52\u5668\u9884\u6d4b\u52a8\u4f5c\u4e0e\u6700\u4f18\u7b56\u7565\u7684\u504f\u5dee\u7a0b\u5ea6\uff0c\u5e76\u8bc4\u4f30\u8de8\u4e3b\u4f53\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u4e8c\u5143\u5206\u7c7b\u5e73\u5747F1\u5206\u657067%\uff0c\u591a\u5206\u7c7b\u5e73\u574746%\uff1b\u901a\u8fc7\u5c11\u91cf\u4e3b\u4f53\u7279\u5b9a\u6570\u636e\u5fae\u8c03\u540e\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u534717%\u548c41%\u3002", "conclusion": "\u4ece\u9690\u5f0ffNIRS\u4fe1\u53f7\u6620\u5c04\u5230\u4ee3\u7406\u6027\u80fd\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u6539\u8fdb\uff0c\u4e3a\u8111\u9a71\u52a8\u7684RLHF\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.13186", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13186", "abs": "https://arxiv.org/abs/2511.13186", "authors": ["Akash Karthikeyan", "Yash Vardhan Pant"], "title": "DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play", "comment": "Initial results presented at the IJCAI 2025 Workshop on User-Aligned Assessment of Adaptive AI Systems. Project page: https://aku02.github.io/projects/difffp/", "summary": "Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $\u03b5$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\\times$ faster convergence and 30$\\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations", "AI": {"tldr": "\u63d0\u51fa\u4e86DiffFP\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u7b56\u7565\u6765\u4f30\u8ba1\u5bf9\u672a\u89c1\u5bf9\u624b\u7684\u6700\u4f73\u54cd\u5e94\uff0c\u5728\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u7684\u96f6\u548c\u6e38\u620f\u4e2d\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u548c\u9c81\u68d2\u6027", "motivation": "\u89e3\u51b3\u81ea\u535a\u5f08\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u4e2d\u6536\u655b\u7f13\u6162\u3001\u5bb9\u6613\u53d7\u5230\u672a\u89c1\u5bf9\u624b\u7b56\u7565\u5229\u7528\u7684\u95ee\u9898\uff0c\u9700\u8981\u63d0\u5347\u81ea\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b", "method": "\u57fa\u4e8e\u865a\u6784\u535a\u5f08\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u7b56\u7565\u8fdb\u884c\u751f\u6210\u5efa\u6a21\u6765\u5b66\u4e60\u81ea\u9002\u5e94\u548c\u591a\u6837\u5316\u7684\u7b56\u7565\uff0c\u8fd1\u4f3c\u6700\u4f73\u54cd\u5e94", "result": "\u5728\u8d5b\u8f66\u548c\u591a\u7c92\u5b50\u96f6\u548c\u6e38\u620f\u7b49\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u6bd4\u57fa\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6536\u655b\u5feb3\u500d\uff0c\u6210\u529f\u7387\u5e73\u5747\u9ad830\u500d", "conclusion": "DiffFP\u6846\u67b6\u80fd\u591f\u5728\u8fde\u7eed\u7a7a\u95f4\u96f6\u548c\u6e38\u620f\u4e2d\u6536\u655b\u5230\u03b5-\u7eb3\u4ec0\u5747\u8861\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5bf9\u591a\u6837\u5316\u5bf9\u624b\u5177\u6709\u9c81\u68d2\u6027"}}
{"id": "2511.12821", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12821", "abs": "https://arxiv.org/abs/2511.12821", "authors": ["Ruiyu Wang", "Yuzhang Xie", "Xiao Hu", "Carl Yang", "Jiaying Lu"], "title": "BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals", "comment": null, "summary": "Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.", "AI": {"tldr": "BioMedJImpact\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u751f\u7269\u533b\u5b66\u671f\u520a\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u6587\u732e\u8ba1\u91cf\u6307\u6807\u3001\u5408\u4f5c\u7279\u5f81\u548cLLM\u884d\u751f\u7684AI\u53c2\u4e0e\u5ea6\u6307\u6807\uff0c\u7528\u4e8e\u5206\u6790\u5408\u4f5c\u5f3a\u5ea6\u548cAI\u53c2\u4e0e\u5ea6\u5982\u4f55\u5171\u540c\u5f71\u54cd\u79d1\u5b66\u5f71\u54cd\u529b\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8d44\u6e90\u5f88\u5c11\u6355\u6349\u5408\u4f5c\u7ed3\u6784\u548cAI\u7814\u7a76\u5982\u4f55\u5171\u540c\u5851\u9020\u751f\u7269\u533b\u5b66\u671f\u520a\u58f0\u671b\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7efc\u5408\u6570\u636e\u96c6\u6765\u63a8\u8fdb\u671f\u520a\u5c42\u9762\u7684\u79d1\u5b66\u5f71\u54cd\u529b\u548cAI\u53c2\u4e0e\u5ea6\u5206\u6790\u3002", "method": "\u4ecePubMed Central\u7684174\u4e07\u7bc7\u6587\u7ae0\u6784\u5efa\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u53ef\u590d\u73b0\u7684\u4e09\u9636\u6bb5LLM\u6d41\u7a0b\u63d0\u53d6AI\u53c2\u4e0e\u5ea6\u7279\u5f81\uff0c\u5206\u6790\u5408\u4f5c\u5f3a\u5ea6\u548cAI\u53c2\u4e0e\u5ea6\u5728\u75ab\u60c5\u524d\u540e\u671f\u95f4\u5bf9\u79d1\u5b66\u5f71\u54cd\u529b\u7684\u5171\u540c\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e00\u81f4\u8d8b\u52bf\uff1a\u5408\u4f5c\u5f3a\u5ea6\u66f4\u9ad8\u7684\u671f\u520a\uff08\u7279\u522b\u662f\u62e5\u6709\u66f4\u5927\u66f4\u591a\u6837\u5316\u4f5c\u8005\u56e2\u961f\u7684\u671f\u520a\uff09\u83b7\u5f97\u66f4\u9ad8\u5f15\u7528\u5f71\u54cd\u529b\uff1bAI\u53c2\u4e0e\u5ea6\u5df2\u6210\u4e3a\u671f\u520a\u58f0\u671b\u7684\u65e5\u76ca\u5f3a\u76f8\u5173\u56e0\u7d20\uff0c\u7279\u522b\u662f\u5728\u56db\u5206\u4f4d\u6392\u540d\u4e2d\u3002", "conclusion": "BioMedJImpact\u65e2\u662f\u6355\u6349\u751f\u7269\u533b\u5b66\u4e0eAI\u4ea4\u53c9\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u4e5f\u662f\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u5185\u5bb9\u611f\u77e5\u7684\u79d1\u5b66\u8ba1\u91cf\u5206\u6790\u3002"}}
{"id": "2511.11684", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.11684", "abs": "https://arxiv.org/abs/2511.11684", "authors": ["Shuvom Sadhuka", "Sophia Lin", "Emma Pierson", "Bonnie Berger"], "title": "A Bayesian Model for Multi-stage Censoring", "comment": "Proceedings of ML4H 2025", "summary": "Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u533b\u7597\u51b3\u7b56\u6f0f\u6597\u7ed3\u6784\u7684\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u9009\u62e9\u6027\u6807\u7b7e\u548c\u5ba1\u67e5\u504f\u5dee\u95ee\u9898\uff0c\u5728\u6025\u8bca\u79d1\u6570\u636e\u4e2d\u53d1\u73b0ICU\u5165\u9662\u51b3\u7b56\u5b58\u5728\u6027\u522b\u5dee\u5f02\u3002", "motivation": "\u533b\u7597\u51b3\u7b56\u6f0f\u6597\u7ed3\u6784\u4e2d\u5b58\u5728\u9009\u62e9\u6027\u5ba1\u67e5\u95ee\u9898\uff0c\u5373\u771f\u5b9e\u7ed3\u679c\u53ea\u5728\u6d41\u7a0b\u672b\u7aef\u88ab\u89c2\u5bdf\u5230\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u98ce\u9669\u4f30\u8ba1\u7684\u7edf\u8ba1\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u7684\u60a3\u8005\u7fa4\u4f53\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u6a21\u578b\u6765\u5904\u7406\u6f0f\u6597\u51b3\u7b56\u7ed3\u6784\u4e2d\u7684\u9009\u62e9\u6027\u6807\u7b7e\u548c\u5ba1\u67e5\u95ee\u9898\uff0c\u4ece\u9009\u62e9\u6807\u7b7e\u548c\u5ba1\u67e5\u7684\u5148\u9a8c\u5de5\u4f5c\u4e2d\u6c72\u53d6\u7075\u611f\u3002", "result": "\u5728\u5408\u6210\u8bbe\u7f6e\u4e2d\uff0c\u6a21\u578b\u80fd\u591f\u6bd4\u57fa\u7ebf\u66f4\u51c6\u786e\u5730\u6062\u590d\u771f\u5b9e\u53c2\u6570\u5e76\u9884\u6d4b\u88ab\u5ba1\u67e5\u60a3\u8005\u7684\u7ed3\u679c\uff1b\u5728\u6025\u8bca\u79d1\u6570\u636e\u5e94\u7528\u4e2d\uff0c\u53d1\u73b0ICU\u5165\u9662\u51b3\u7b56\u5b58\u5728\u6027\u522b\u5dee\u5f02\uff0c\u5973\u6027\u9700\u8981\u66f4\u9ad8\u7684\u6b7b\u4ea1\u98ce\u9669\u9608\u503c\uff085.1%\uff09\u624d\u4f1a\u88ab\u6536\u5165ICU\uff0c\u800c\u7537\u6027\u4e3a4.5%\u3002", "conclusion": "\u8be5\u8d1d\u53f6\u65af\u6a21\u578b\u80fd\u591f\u6709\u6548\u5904\u7406\u533b\u7597\u51b3\u7b56\u6f0f\u6597\u4e2d\u7684\u9009\u62e9\u6027\u5ba1\u67e5\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u533b\u7597\u51b3\u7b56\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u6027\u522b\u504f\u89c1\uff0c\u4e3a\u6539\u5584\u533b\u7597\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2511.12867", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12867", "abs": "https://arxiv.org/abs/2511.12867", "authors": ["Chen Jia"], "title": "Bootstrapping LLMs via Preference-Based Policy Optimization", "comment": null, "summary": "Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6PbPO\uff0c\u901a\u8fc7\u4e3b\u7b56\u7565\u4e0e\u5956\u52b1\u6a21\u578b\u4e4b\u95f4\u7684min-max\u535a\u5f08\u6765\u5f15\u5bfcLLM\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u901a\u8fc7\u504f\u597d\u9a71\u52a8\u7684\u7b56\u7565\u4f18\u5316\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u907f\u514d\u4f9d\u8d56\u5927\u91cf\u624b\u52a8\u6807\u6ce8\uff0c\u5b9e\u73b0\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u6709\u6548\u5bf9\u9f50\u3002", "method": "\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u4e3b\u7b56\u7565\u4e0e\u5956\u52b1\u6a21\u578b\u4e4b\u95f4\u7684min-max\u535a\u5f08\uff0c\u5956\u52b1\u6a21\u578b\u7ea6\u675f\u5728\u504f\u597d\u6570\u636e\u5bfc\u51fa\u7684\u7f6e\u4fe1\u96c6\u5185\uff0c\u91c7\u7528\u8fed\u4ee3\u5728\u7ebf\u7b97\u6cd5\u901a\u8fc7\u5f15\u5bfc\u63a2\u7d22\u4e3b\u52a8\u6536\u96c6\u504f\u597d\u6570\u636e\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u504f\u597d\u4f18\u5316\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684PbPO\u6846\u67b6\u4e3aLLM\u7684\u81ea\u4e3e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2511.12832", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12832", "abs": "https://arxiv.org/abs/2511.12832", "authors": ["Niranjan Chebrolu", "Gerard Christopher Yeo", "Kokil Jaidka"], "title": "From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation", "comment": null, "summary": "Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.", "AI": {"tldr": "\u901a\u8fc7\u76ee\u6807\u6fc0\u6d3b\u5de5\u7a0b\u5f15\u5bfcLLaMA 3.1-8B\u5c55\u73b0\u66f4\u4eba\u6027\u5316\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u5173\u952e\u5e72\u9884\u4f4d\u7f6e\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6587\u672c\u5bf9\u63a8\u5bfc\u60c5\u611f\u8868\u8fbe\u5411\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u6d41\u7545\u5ea6\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u8d4b\u4e88\u5176\u7ec6\u817b\u3001\u4eba\u6027\u5316\u7684\u60c5\u611f\u8868\u8fbe\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u5f53\u524d\u7684\u5bf9\u9f50\u6280\u672f\u5f80\u5f80\u53ea\u89e3\u51b3\u8868\u5c42\u8f93\u51fa\u6216\u9700\u8981\u5927\u91cf\u5fae\u8c03\u3002", "method": "\u9996\u5148\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u56e0\u679c\u5f71\u54cd\u7ec4\u4ef6\uff0c\u901a\u8fc7\u8bca\u65ad\u6027\u5bf9\u8bdd\u4efb\u52a1\u89c2\u5bdf\u6fc0\u6d3b\u6a21\u5f0f\u627e\u5230\u5173\u952e\u5e72\u9884\u4f4d\u7f6e\u3002\u7136\u540e\u4ece\u5bf9\u6bd4\u6587\u672c\u5bf9\uff08\u76ee\u6807\u60c5\u611f\u7684\u6b63\u9762vs\u8d1f\u9762\u793a\u4f8b\uff09\u7684\u6fc0\u6d3b\u5dee\u5f02\u63a8\u5bfc\u60c5\u611f\u8868\u8fbe\u5411\u91cf\u3002", "result": "\u5c06\u8fd9\u4e9b\u5411\u91cf\u5e94\u7528\u4e8e\u65b0\u5bf9\u8bdd\u63d0\u793a\u663e\u8457\u589e\u5f3a\u4e86\u60c5\u611f\u7279\u5f81\uff1a\u5f15\u5bfc\u540e\u7684\u54cd\u5e94\u663e\u793a\u51fa\u589e\u52a0\u7684\u6b63\u5411\u60c5\u611f\uff08\u5982\u559c\u60a6\u3001\u4fe1\u4efb\uff09\u548c\u66f4\u9891\u7e41\u7684\u7b2c\u4e00\u4eba\u79f0\u4ee3\u8bcd\u4f7f\u7528\uff0c\u8868\u660e\u66f4\u5f3a\u7684\u4e2a\u4eba\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cbe\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u4e3a\u5bf9\u8bddAI\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.11685", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11685", "abs": "https://arxiv.org/abs/2511.11685", "authors": ["Tianyi Yin", "Jingwei Wang", "Chenze Wang", "Han Wang", "Jiexuan Cai", "Min Liu", "Yunlong Ma", "Kun Gao", "Yuting Song", "Weiming Shen"], "title": "R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models", "comment": null, "summary": "Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.", "AI": {"tldr": "R-Tuning\u662f\u4e00\u79cd\u7528\u4e8e\u9884\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u6301\u7eed\u9002\u5e94\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u7387\u611f\u77e5\u91cd\u653e\u7b56\u7565\u548c\u6f5c\u5728\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u65b0\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u540c\u65f6\u4fdd\u7559\u65e7\u4efb\u52a1\u77e5\u8bc6\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\u4ecd\u5177\u6311\u6218\u6027\u3002\u4e3b\u8981\u969c\u788d\u5728\u4e8e\u65e0\u6cd5\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e\uff0c\u4ec5\u5728\u65b0\u6570\u636e\u4e0a\u5fae\u8c03\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51faR-Tuning\u6846\u67b6\uff0c\u6784\u5efa\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u6355\u83b7\u5148\u9a8c\u548c\u5f53\u524d\u4efb\u52a1\u77e5\u8bc6\u3002\u4f7f\u7528\u57fa\u4e8e\u5c0f\u6ce2\u5206\u89e3\u7684\u9891\u7387\u611f\u77e5\u91cd\u653e\u7b56\u7565\uff0c\u751f\u6210\u8d8b\u52bf\u4fdd\u6301\u548c\u878d\u5408\u589e\u5f3a\u7684\u53d8\u4f53\u3002\u5f15\u5165\u6f5c\u5728\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5bf9\u9f50\u65b0\u8868\u793a\u4e0e\u5148\u9a8c\u4efb\u52a1\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aR-Tuning\u5728\u65b0\u4efb\u52a1\u4e0aMAE\u548cMSE\u5206\u522b\u964d\u4f4e\u9ad8\u8fbe46.9%\u548c46.8%\uff0c\u540c\u65f6\u5728\u65e7\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\u8fbe5.7%\u548c6.0%\u3002\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u5373\u4f7f\u5408\u6210\u4ee3\u7406\u6837\u672c\u4ec5\u5360\u65b0\u4efb\u52a1\u6570\u636e\u96c6\u76845%\uff0c\u4ecd\u4f18\u4e8e\u6240\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u3002", "conclusion": "R-Tuning\u901a\u8fc7\u9891\u7387\u611f\u77e5\u91cd\u653e\u548c\u6f5c\u5728\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u6301\u7eed\u9002\u5e94\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5148\u9a8c\u77e5\u8bc6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u65b0\u4efb\u52a1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.13510", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13510", "abs": "https://arxiv.org/abs/2511.13510", "authors": ["Melanie Schaller", "Nick Janssen", "Bodo Rosenhahn"], "title": "Naga: Vedic Encoding for Deep State Space Models", "comment": "submitted to JMLR", "summary": "This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.", "AI": {"tldr": "Naga\u662f\u4e00\u79cd\u57fa\u4e8e\u5420\u9640\u6570\u5b66\u7ed3\u6784\u6982\u5ff5\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u5904\u7406\u6b63\u5411\u548c\u65f6\u95f4\u53cd\u8f6c\u5e8f\u5217\uff0c\u4f7f\u7528\u54c8\u8fbe\u739b\u79ef\u7ec4\u5408\u8868\u793a\uff0c\u5728\u591a\u4e2a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e28\u4e2a\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u53d7\u5420\u9640\u6570\u5b66\u7ed3\u6784\u6982\u5ff5\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u66f4\u597d\u6355\u6349\u8fdc\u8ddd\u79bb\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u957f\u671f\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7f16\u7801\uff0c\u8054\u5408\u5904\u7406\u6b63\u5411\u548c\u65f6\u95f4\u53cd\u8f6c\u8f93\u5165\u5e8f\u5217\uff0c\u901a\u8fc7\u54c8\u8fbe\u739b\u79ef\u7ec4\u5408\u4e24\u79cd\u8868\u793a\uff0c\u5f62\u6210\u5420\u9640\u542f\u53d1\u7684\u7f16\u7801\u65b9\u5f0f\u3002", "result": "\u5728ETTh1\u3001ETTh2\u3001ETTm1\u3001ETTm2\u3001Weather\u3001Traffic\u548cILI\u7b49\u591a\u4e2a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNaga\u4f18\u4e8e28\u4e2a\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u4e14\u6bd4\u73b0\u6709\u6df1\u5ea6SSM\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u3001\u5420\u9640\u542f\u53d1\u7684\u5206\u89e3\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u957f\u8ddd\u79bb\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.12850", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12850", "abs": "https://arxiv.org/abs/2511.12850", "authors": ["Saranzaya Magsarjav", "Melissa Humphries", "Jonathan Tuke", "Lewis Mitchell"], "title": "Quantifying consistency and accuracy of Latent Dirichlet Allocation", "comment": "8 pages, 3 figures, to be submitted", "summary": "Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a33\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30LDA\u4e3b\u9898\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\uff0c\u53d1\u73b0LDA\u867d\u7136\u80fd\u6b63\u786e\u8bc6\u522b\u4e3b\u9898\u6570\u91cf\u4e14\u5185\u90e8\u4e00\u81f4\uff0c\u4f46\u751f\u6210\u7684\u4e3b\u9898\u5e76\u975e\u771f\u5b9e\u4e3b\u9898\u3002", "motivation": "\u6982\u7387\u4e3b\u9898\u6a21\u578b\u7531\u4e8e\u5176\u968f\u673a\u6027\uff0c\u5728\u91cd\u590d\u8fd0\u884c\u65f6\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u5bfc\u81f4\u6f5c\u5728\u4e3b\u9898\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u53ef\u91cd\u590d\u6027\u3001\u53ef\u9760\u6027\u548c\u89e3\u91ca\u6027\uff0c\u5f15\u53d1\u5bf9\u4e3b\u9898\u6a21\u578b\u662f\u5426\u771f\u6b63\u6355\u6349\u5230\u6709\u610f\u4e49\u4e3b\u9898\u8fd8\u662f\u53ea\u662f\u566a\u58f0\u7684\u62c5\u5fe7\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u4e2a\u65b0\u7684\u7a33\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7ed3\u5408\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5229\u7528LDA\u7684\u751f\u6210\u7279\u6027\u751f\u6210\u5e26\u6709\u771f\u5b9e\u6807\u7b7e\u7684\u65b0\u8bed\u6599\u5e93\uff0c\u5e76\u5c06\u8fd9\u4e9b\u751f\u6210\u7684\u8bed\u6599\u5e93\u901a\u8fc7LDA\u8fd0\u884c50\u6b21\u4ee5\u786e\u5b9a\u8f93\u51fa\u7684\u53d8\u5f02\u6027\u3002", "result": "\u7814\u7a76\u8868\u660eLDA\u80fd\u591f\u6b63\u786e\u786e\u5b9a\u6587\u6863\u4e2d\u7684\u57fa\u7840\u4e3b\u9898\u6570\u91cf\uff0c\u5e76\u4e14LDA\u5177\u6709\u66f4\u9ad8\u7684\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u591a\u6b21\u91cd\u590d\u8fd0\u884c\u8fd4\u56de\u76f8\u4f3c\u7684\u4e3b\u9898\uff1b\u7136\u800c\u8fd9\u4e9b\u4e3b\u9898\u5e76\u975e\u771f\u5b9e\u4e3b\u9898\u3002", "conclusion": "LDA\u6a21\u578b\u5728\u8bc6\u522b\u4e3b\u9898\u6570\u91cf\u65b9\u9762\u8868\u73b0\u51c6\u786e\u4e14\u5185\u90e8\u4e00\u81f4\uff0c\u4f46\u751f\u6210\u7684\u4e3b\u9898\u4e0e\u771f\u5b9e\u4e3b\u9898\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u5173\u6ce8\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u4e3b\u9898\u8d28\u91cf\u8bc4\u4f30\u3002"}}
{"id": "2511.11686", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.11686", "abs": "https://arxiv.org/abs/2511.11686", "authors": ["Qing Yao", "Lijian Gao", "Qirong Mao", "Dong Ming"], "title": "Regularized Schr\u00f6dinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems", "comment": null, "summary": "Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schr\u00f6dinger Bridge (RSB), an adaptation of Schr\u00f6dinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6b63\u5219\u5316\u859b\u5b9a\u8c14\u6865\uff08RSB\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5931\u771f-\u611f\u77e5\u6743\u8861\u548c\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u5728\u8bed\u97f3\u589e\u5f3a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u5931\u771f-\u611f\u77e5\u6743\u8861\uff0c\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u4f1a\u964d\u4f4e\u91cd\u5efa\u4fdd\u771f\u5ea6\uff1b2\uff09\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u8bad\u7ec3-\u63a8\u7406\u8f93\u5165\u4e0d\u5339\u914d\u5bfc\u81f4\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u548c\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u6b63\u5219\u5316\u859b\u5b9a\u8c14\u6865\uff08RSB\uff09\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u9896\u7684\u6b63\u5219\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u5bf9\u8f93\u5165\u72b6\u6001\u548c\u76ee\u6807\u8fdb\u884c\u6270\u52a8\uff0c\u901a\u8fc7\u66b4\u9732\u6a21\u578b\u4e8e\u6a21\u62df\u9884\u6d4b\u8bef\u5dee\u6765\u7f13\u89e3\u66dd\u5149\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u540e\u9a8c\u5747\u503c\u7684\u7cbe\u5fc3\u8bbe\u8ba1\u63d2\u503c\u6765\u51cf\u8f7b\u5931\u771f\u3002", "result": "\u5728\u4e24\u4e2a\u5178\u578b\u7684\u8bed\u97f3\u589e\u5f3a\u9006\u95ee\u9898\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRSB\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u5584\u4e86\u5931\u771f\u6307\u6807\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u66dd\u5149\u504f\u5dee\u3002", "conclusion": "RSB\u662f\u4e13\u95e8\u4e3a\u9006\u95ee\u9898\u8bbe\u8ba1\u7684\u859b\u5b9a\u8c14\u6865\u9002\u5e94\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u5931\u771f-\u611f\u77e5\u6743\u8861\u548c\u66dd\u5149\u504f\u5dee\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.12901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12901", "abs": "https://arxiv.org/abs/2511.12901", "authors": ["Yuesheng Xu", "Hector Munoz-Avila"], "title": "Online Learning of HTN Methods for integrated LLM-HTN Planning", "comment": "The Twelfth Annual Conference on Advances in Cognitive Systems (ACS-2025)", "summary": "We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5728\u96c6\u6210HTN\u89c4\u5212\u548cLLM\u804a\u5929\u673a\u5668\u4eba\u80cc\u666f\u4e0b\u5728\u7ebf\u5b66\u4e60\u5206\u5c42\u4efb\u52a1\u7f51\u7edc\u65b9\u6cd5\u7684\u6280\u672f\uff0c\u901a\u8fc7\u4eceChatGPT\u751f\u6210\u7684\u4efb\u52a1\u5206\u89e3\u4e2d\u5b66\u4e60\u6cdb\u5316\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9ChatGPT\u7684\u8c03\u7528\u6b21\u6570\u3002", "motivation": "\u5728HTN\u89c4\u5212\u4e2d\uff0c\u5f53\u6ca1\u6709\u53ef\u7528\u7684\u4efb\u52a1\u5206\u89e3\u65b9\u6cd5\u65f6\uff0c\u9700\u8981\u9891\u7e41\u8c03\u7528ChatGPT\u6765\u751f\u6210\u5206\u89e3\uff0c\u8fd9\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002", "method": "\u6269\u5c55ChatHTN\u89c4\u5212\u5668\uff0c\u5f53ChatGPT\u751f\u6210\u4efb\u52a1\u5206\u89e3\u65f6\uff0c\u5b66\u4e60\u6cdb\u5316\u65b9\u6cd5\u800c\u975e\u7b80\u5355\u8bb0\u5fc6\uff0c\u521b\u5efa\u53ef\u5e94\u7528\u4e8e\u540c\u7c7b\u4efb\u52a1\u5b9e\u4f8b\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u4e24\u4e2a\u9886\u57df\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7ebf\u5b66\u4e60\u8fc7\u7a0b\u51cf\u5c11\u4e86ChatGPT\u8c03\u7528\u6b21\u6570\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u81f3\u5c11\u540c\u6837\u591a\u7684\u95ee\u9898\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u66f4\u591a\u3002", "conclusion": "\u5728\u7ebf\u5b66\u4e60HTN\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u5bf9LLM\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2511.12851", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12851", "abs": "https://arxiv.org/abs/2511.12851", "authors": ["Kang Yin", "Hye-Bin Shin"], "title": "NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation", "comment": null, "summary": "Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.", "AI": {"tldr": "NeuroLex\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u8111\u7535\u56fe\u62a5\u544a\u7684\u8f7b\u91cf\u7ea7\u9886\u57df\u81ea\u9002\u5e94\u8bed\u8a00\u6a21\u578b\uff0c\u76f8\u6bd4\u901a\u7528\u6a21\u578b\u5728\u8111\u7535\u56fe\u6587\u672c\u5904\u7406\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u901a\u7528\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u8111\u7535\u56fe\u62a5\u544a\u4e2d\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u60ef\u4f8b\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8111\u7535\u56fe\u62a5\u544a\u6587\u672c\u7684\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u54c8\u4f5b\u8111\u7535\u56fe\u6570\u636e\u5e93\u7684\u7eaf\u6587\u672c\u6570\u636e\uff0c\u901a\u8fc7span-corruption\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u5f0f\u5fae\u8c03\uff08\u62a5\u544a\u6da6\u8272\u3001\u6bb5\u843d\u6458\u8981\u3001\u672f\u8bed\u95ee\u7b54\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u76f8\u6bd4\u540c\u89c4\u6a21\u901a\u7528\u6a21\u578b\uff0cNeuroLex\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\u3001\u66f4\u9ad8\u7684\u63d0\u53d6\u548c\u6458\u8981\u51c6\u786e\u7387\u3001\u66f4\u597d\u7684\u6807\u7b7e\u6548\u7387\uff0c\u4ee5\u53ca\u5bf9\u5426\u5b9a\u548c\u4e8b\u5b9e\u5e7b\u89c9\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "NeuroLex\u4e3a\u8111\u7535\u56fe\u6587\u672c\u5efa\u6a21\u548c\u8111\u673a\u63a5\u53e3\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u9a71\u52a8\u795e\u7ecf\u89e3\u7801\u57fa\u7840\u3002"}}
{"id": "2511.11688", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11688", "abs": "https://arxiv.org/abs/2511.11688", "authors": ["Aihua Zhu", "Rui Su", "Qinglin Zhao", "Li Feng", "Meng Shen", "Shibo He"], "title": "Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling", "comment": null, "summary": "Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.", "AI": {"tldr": "HSO\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u641c\u7d22\u548c\u5c40\u90e8\u4f18\u5316\u4ea4\u66ff\u8fdb\u884c\uff0c\u7ed3\u5408MEP\u548cSPF\u521b\u65b0\u65b9\u6cd5\uff0c\u5728\u6781\u4f4eNFE\u6761\u4ef6\u4e0b\u5b9e\u73b0\u8bad\u7ec3\u81ea\u7531\u91c7\u6837\uff0c5\u6b21\u8bc4\u4f30\u5373\u53ef\u8fbe\u523011.94 FID\uff0c\u4f18\u5316\u6210\u672c\u4ec5\u97008\u79d2\u3002", "motivation": "\u6269\u6563\u6982\u7387\u6a21\u578b\u751f\u6210\u8d28\u91cf\u9ad8\u4f46\u91c7\u6837\u8fc7\u7a0b\u6162\uff0c\u73b0\u6709\u8c03\u5ea6\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u6709\u6548\u6027\u3001\u81ea\u9002\u5e94\u6027\u3001\u5b9e\u7528\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u56db\u4e2a\u6838\u5fc3\u539f\u5219\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8c03\u5ea6\u4f18\u5316\u5668(HSO)\uff0c\u5305\u542b\u4e0a\u5c42\u5168\u5c40\u641c\u7d22\u6700\u4f18\u521d\u59cb\u5316\u7b56\u7565\u548c\u4e0b\u5c42\u5c40\u90e8\u4f18\u5316\u8c03\u5ea6\u7ec6\u5316\uff0c\u4f7f\u7528\u4e2d\u70b9\u8bef\u5dee\u4ee3\u7406(MEP)\u548c\u95f4\u8ddd\u60e9\u7f5a\u9002\u5e94\u5ea6(SPF)\u51fd\u6570\u3002", "result": "\u5728\u6781\u4f4eNFE\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u81ea\u7531\u91c7\u6837\u6027\u80fd\uff0cNFE=5\u65f6\u5728LAION-Aesthetics\u4e0a\u8fbe\u523011.94 FID\uff0c\u4f18\u5316\u6210\u672c\u4ec5\u97008\u79d2\u3002", "conclusion": "HSO\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u8303\u5f0f\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u91c7\u6837\u6548\u7387\u3002"}}
{"id": "2511.12913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12913", "abs": "https://arxiv.org/abs/2511.12913", "authors": ["Yiming Zhao", "Jiwei Tang", "Shimin Di", "Libin Zheng", "Jianxing Yu", "Jian Yin"], "title": "CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling", "comment": null, "summary": "Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.", "AI": {"tldr": "\u63d0\u51fa\u4e86Chain-of-Scheduling (CoS)\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u7d22\u3001\u9a8c\u8bc1\u548c\u96c6\u6210\u4e09\u4e2a\u9636\u6bb5\u6fc0\u6d3b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u4ef6\u8c03\u5ea6\u80fd\u529b\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u4ef6\u8c03\u5ea6\u63a8\u8350\u65b9\u6cd5\u5728\u6548\u7387\u3001\u6548\u679c\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\uff0c\u7531\u4e8e\u95ee\u9898\u7684NP-hard\u6027\u8d28\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6700\u5927\u5316\u7528\u6237\u504f\u597d\u540c\u65f6\u6ee1\u8db3\u65f6\u95f4\u548c\u5730\u7406\u7ea6\u675f\u7684\u6709\u6548\u63a8\u8350\u65b9\u6cd5\u3002", "method": "CoS\u6846\u67b6\u5c06\u8c03\u5ea6\u4efb\u52a1\u5206\u89e3\u4e3a\u63a2\u7d22\u3001\u9a8c\u8bc1\u548c\u96c6\u6210\u4e09\u4e2a\u539f\u5b50\u9636\u6bb5\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4f7fLLM\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u8c03\u5ea6\u94fe\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cCoS\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u6548\u679c\uff0c\u5177\u6709\u9ad8\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u57df\u5916\u6570\u636e\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "CoS\u6846\u67b6\u6210\u529f\u6fc0\u6d3b\u4e86LLM\u7684\u4e8b\u4ef6\u8c03\u5ea6\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u63a8\u8350\u6548\u679c\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12861", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12861", "abs": "https://arxiv.org/abs/2511.12861", "authors": ["Wenxin Zhu", "Andong Chen", "Yuchen Song", "Kehai Chen", "Conghui Zhu", "Ziyan Chen", "Tiejun Zhao"], "title": "From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models", "comment": "Survey; 7 figures, 3 tables, 44 pages", "summary": "With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on \"Multimodal Chain-of-Thought\" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u7684\u7814\u7a76\uff0c\u5206\u6790\u4e86\u5176\u80cc\u666f\u3001\u65b9\u6cd5\u3001\u8bc4\u4f30\u548c\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6210\u529f\uff0c\u63d0\u5347\u5176\u590d\u6742\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002\u73b0\u6709\u6a21\u578b\u5b58\u5728\u63a8\u7406\u8def\u5f84\u4e0d\u900f\u660e\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u800c\u601d\u7ef4\u94fe\u65b9\u6cd5\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5df2\u8bc1\u660e\u80fd\u589e\u5f3a\u63a8\u7406\u900f\u660e\u5ea6\u548c\u8f93\u51fa\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u671b\u5728\u591a\u6a21\u6001\u9886\u57df\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u4ecb\u7ecd\u4e3b\u6d41MCoT\u65b9\u6cd5\uff1a\u601d\u7ef4\u94fe\u8303\u5f0f\u3001\u540e\u8bad\u7ec3\u9636\u6bb5\u548c\u63a8\u7406\u9636\u6bb5\uff0c\u5e76\u5206\u6790\u5176\u5185\u5728\u673a\u5236\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u6307\u6807\uff0c\u8ba8\u8bba\u4e86MCoT\u7684\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u5206\u6790\u4e86MCoT\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u5bf9\u5176\u672a\u6765\u7814\u7a76\u65b9\u5411\u8fdb\u884c\u4e86\u5c55\u671b\u3002"}}
{"id": "2511.11690", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11690", "abs": "https://arxiv.org/abs/2511.11690", "authors": ["Fei Song", "Yi Li", "Rui Wang", "Jiahuan Zhou", "Changwen Zheng", "Jiangmeng Li"], "title": "Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models", "comment": "Accepted by AAAI2026", "summary": "Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u53bb\u504f\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u8c03\u5236\u548c\u53ef\u9760\u6027\u611f\u77e5\u63d0\u793a\u4f18\u5316\u6765\u7f13\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u4f18\u5316\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u672a\u6807\u8bb0\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u4f18\u5316\uff0c\u5bb9\u6613\u4ea7\u751f\u63d0\u793a\u4f18\u5316\u504f\u5dee\uff0c\u5bfc\u81f4\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0d\u4f73\u3002\u8fd9\u79cd\u504f\u5dee\u6e90\u4e8e\u6a21\u578b\u5c42\u9762\u71b5\u6700\u5c0f\u5316\u76ee\u6807\u5ffd\u89c6\u9884\u6d4b\u6b63\u786e\u6027\uff0c\u4ee5\u53ca\u6570\u636e\u5c42\u9762\u63d0\u793a\u504f\u5dee\u5bfc\u81f4\u89c6\u89c9-\u6587\u672c\u6a21\u6001\u4e0d\u5bf9\u9f50\u3002", "method": "1. \u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u8c03\u5236\u6a21\u5757\uff1a\u4f7f\u7528\u6d4b\u8bd5\u56fe\u50cf\u7279\u5f81\u67e5\u8be2\u52a8\u6001\u77e5\u8bc6\u5e93\uff0c\u68c0\u7d22\u9ad8\u7f6e\u4fe1\u5ea6\u77e5\u8bc6\u6765\u8c03\u5236\u9884\u6d4b\uff1b2. \u53ef\u9760\u6027\u611f\u77e5\u63d0\u793a\u4f18\u5316\u6a21\u5757\uff1a\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u52a0\u6743\u96c6\u6210\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u84b8\u998f\uff0c\u5728\u63d0\u793a\u8c03\u4f18\u4e2d\u65bd\u52a0\u6b63\u5219\u5316\u7ea6\u675f\u3002", "result": "\u5728\u6d89\u53ca\u81ea\u7136\u5206\u5e03\u504f\u79fb\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u768415\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7f13\u89e3\u63d0\u793a\u4f18\u5316\u504f\u5dee\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u91cd\u53bb\u504f\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u63d0\u793a\u4f18\u5316\u504f\u5dee\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12916", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12916", "abs": "https://arxiv.org/abs/2511.12916", "authors": ["Yafang Wang", "Yangjie Tian", "Xiaoyu Shen", "Gaoyang Zhang", "Jiaze Sun", "He Zhang", "Ruohua Xu", "Feng Zhao"], "title": "Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation", "comment": null, "summary": "Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.", "AI": {"tldr": "Fault2Flow\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u7535\u7f51\u6545\u969c\u8bca\u65ad\uff0c\u80fd\u591f\u4ece\u6cd5\u89c4\u548c\u4e13\u5bb6\u77e5\u8bc6\u4e2d\u63d0\u53d6\u5e76\u6574\u5408\u903b\u8f91\uff0c\u751f\u6210\u53ef\u6267\u884c\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u3002", "motivation": "\u5f53\u524d\u7535\u7f51\u6545\u969c\u8bca\u65ad\u4f9d\u8d56\u4eba\u5de5\u65b9\u6cd5\uff0c\u6548\u7387\u4f4e\u3001\u6613\u51fa\u9519\u4e14\u96be\u4ee5\u7ef4\u62a4\u3002\u9700\u8981\u5c06\u6cd5\u89c4\u6587\u672c\u548c\u4e13\u5bb6\u77e5\u8bc6\u6574\u5408\u5230\u53ef\u9a8c\u8bc1\u3001\u53ef\u6267\u884c\u7684\u5de5\u4f5c\u6d41\u4e2d\u3002", "method": "\u4f7f\u7528LLM\u4ece\u6cd5\u89c4\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6545\u969c\u6811\uff0c\u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\uff0c\u7528AlphaEvolve\u6a21\u5757\u4f18\u5316\u63a8\u7406\u903b\u8f91\uff0c\u6700\u7ec8\u5408\u6210n8n\u53ef\u6267\u884c\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u53d8\u538b\u5668\u6545\u969c\u8bca\u65ad\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8fbe\u5230100%\u62d3\u6251\u4e00\u81f4\u6027\u548c\u9ad8\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u4e13\u5bb6\u5de5\u4f5c\u91cf\u3002", "conclusion": "Fault2Flow\u5efa\u7acb\u4e86\u4ece\u6545\u969c\u5206\u6790\u5230\u64cd\u4f5c\u81ea\u52a8\u5316\u7684\u53ef\u590d\u73b0\u8def\u5f84\uff0c\u4e3a\u7535\u7f51\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12874", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12874", "abs": "https://arxiv.org/abs/2511.12874", "authors": ["Chukwuebuka Fortunate Ijezue", "Tania-Amanda Fredrick Eneye", "Maaz Amjad"], "title": "Classification of Hope in Textual Data using Transformer-Based Models", "comment": null, "summary": "This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cdtransformer\u67b6\u6784(BERT\u3001GPT-2\u3001DeBERTa)\u5728\u6587\u672c\u5e0c\u671b\u8868\u8fbe\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0BERT\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\u6765\u5206\u6790\u6587\u672c\u4e2d\u7684\u5e0c\u671b\u8868\u8fbe\uff0c\u5e94\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u548c\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u9886\u57df\u3002", "method": "\u4f7f\u7528BERT\u3001GPT-2\u548cDeBERTa\u4e09\u79cdtransformer\u67b6\u6784\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b(\u5e0c\u671bvs\u975e\u5e0c\u671b)\u548c\u591a\u7c7b\u522b\u5206\u7c7b(\u4e94\u4e2a\u5e0c\u671b\u76f8\u5173\u7c7b\u522b)\u7684\u6bd4\u8f83\u7814\u7a76\u3002", "result": "BERT\u8868\u73b0\u6700\u4f73\uff0c\u4e8c\u5143\u5206\u7c7b\u51c6\u786e\u738784.49%\uff0c\u591a\u7c7b\u522b\u5206\u7c7b\u51c6\u786e\u738772.03%\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u6700\u9ad8(\u8bad\u7ec3\u65f6\u95f4443\u79d2)\u3002GPT-2\u51c6\u786e\u7387\u6700\u4f4e(79.34%\u4e8c\u5143\uff0c71.29%\u591a\u7c7b\u522b)\uff0cDeBERTa\u8868\u73b0\u4e2d\u7b49\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8(\u591a\u7c7b\u522b\u8bad\u7ec3947\u79d2)\u3002", "conclusion": "\u5bf9\u4e8e\u4e13\u95e8\u7684\u60c5\u611f\u68c0\u6d4b\u4efb\u52a1\uff0c\u67b6\u6784\u7684\u9002\u7528\u6027\u53ef\u80fd\u6bd4\u6a21\u578b\u89c4\u6a21\u66f4\u91cd\u8981\uff0cBERT\u5728\u5e0c\u671b\u8868\u8fbe\u5206\u7c7b\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u6700\u4f73\u5e73\u8861\u6027\u80fd\u3002"}}
{"id": "2511.11691", "categories": ["cs.LG", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.11691", "abs": "https://arxiv.org/abs/2511.11691", "authors": ["Seham Nasr", "Zhao Ren", "David Johnson"], "title": "Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues", "comment": "5 pages, 2 figures", "summary": "Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies \"what\" is highlighted and connects it to \"why\" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bed\u97f3\u60c5\u611f\u8bc6\u522b(XAI)\u7684\u53ef\u89e3\u91caAI\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u663e\u8457\u533a\u57df\u5185\u7684\u58f0\u5b66\u7ebf\u7d22\uff0c\u5c06\u6a21\u578b\u5173\u6ce8\u533a\u57df\u4e0e\u4e13\u5bb6\u53c2\u8003\u7684\u60c5\u611f\u58f0\u5b66\u6807\u8bb0\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u9ad8\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\u548c\u53ef\u7406\u89e3\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u663e\u8457\u6027\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u7a81\u51fa\u8bed\u8c31\u56fe\u533a\u57df\uff0c\u4f46\u65e0\u6cd5\u663e\u793a\u8fd9\u4e9b\u533a\u57df\u662f\u5426\u5bf9\u5e94\u6709\u610f\u4e49\u7684\u58f0\u5b66\u60c5\u611f\u6807\u8bb0\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u91cf\u5316\u663e\u8457\u533a\u57df\u5185\u7684\u58f0\u5b66\u7ebf\u7d22\u5e45\u5ea6\uff0c\u5c06\u6a21\u578b\u5173\u6ce8\u70b9\u4e0e\u4e13\u5bb6\u53c2\u8003\u7684\u8bed\u97f3\u60c5\u611f\u58f0\u5b66\u7279\u5f81\u660e\u786e\u5173\u8054\u3002", "result": "\u5728\u57fa\u51c6SER\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u660e\u786e\u8fde\u63a5\u663e\u8457\u533a\u57df\u4e0e\u7406\u8bba\u9a71\u52a8\u7684\u58f0\u5b66\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "\u76f8\u6bd4\u6807\u51c6\u663e\u8457\u6027\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u6613\u7406\u89e3\u548c\u5408\u7406\u7684SER\u6a21\u578b\u89e3\u91ca\uff0c\u4e3a\u53ef\u4fe1\u8d56\u7684\u8bed\u97f3\u60c5\u611f\u8ba1\u7b97\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12937", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12937", "abs": "https://arxiv.org/abs/2511.12937", "authors": ["Guoyan Wang", "Yanyan Huang", "Chunlin Chen", "Lifeng Wang", "Yuxiang Sun"], "title": "Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models", "comment": "32 pages, 13 figures", "summary": "Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.", "AI": {"tldr": "Yanyun-3\u662f\u4e00\u4e2a\u901a\u7528\u4ee3\u7406\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u4e09\u4e2a\u5f02\u6784\u7b56\u7565\u6e38\u620f\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8de8\u5e73\u53f0\u64cd\u4f5c\uff0c\u901a\u8fc7\u7ed3\u5408Qwen2.5-VL\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548cUI-TARS\u7684\u7cbe\u786e\u6267\u884c\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8de8\u5e73\u53f0\u7b56\u7565\u6e38\u620f\u4e2d\u81ea\u52a8\u5316\u64cd\u4f5c\u7684\u9700\u6c42\uff0c\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u6218\u573a\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u96c6\u6210Qwen2.5-VL\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548cUI-TARS\u7684\u7cbe\u786e\u6267\u884c\u80fd\u529b\uff0c\u91c7\u7528\u95ed\u73af\u6d41\u6c34\u7ebf\uff08\u5c4f\u5e55\u6355\u83b7\u3001\u6a21\u578b\u63a8\u7406\u3001\u52a8\u4f5c\u6267\u884c\uff09\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e0d\u540c\u591a\u6a21\u6001\u6570\u636e\u7ec4\u5408\u7b56\u7565\u3002", "result": "\u6df7\u5408\u7b56\u7565\uff08\u878d\u5408\u591a\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\uff0c\u540c\u65f6\u6df7\u5408\u9759\u6001\u56fe\u50cf\uff09\u76f8\u6bd4\u5b8c\u5168\u878d\u5408\u51cf\u5c1163%\u63a8\u7406\u65f6\u95f4\uff0cBLEU-4\u5f97\u5206\u4ece4.81%\u63d0\u5347\u81f362.41%\uff08\u7ea612.98\u500d\u63d0\u5347\uff09\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Yanyun-3\u4e0d\u4ec5\u4e3a\u7b56\u7565\u6e38\u620f\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8fd8\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u6a21\u6001\u6570\u636e\u7ec4\u7ec7\u5efa\u7acb\u4e86\u589e\u5f3aVLM\u6027\u80fd\u7684\u901a\u7528\u8303\u5f0f\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4e2d\u9759\u6001\u611f\u77e5\u4e0e\u52a8\u6001\u63a8\u7406\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2511.11692", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11692", "abs": "https://arxiv.org/abs/2511.11692", "authors": ["Jiayin Zhu", "Linlin Yang", "Yicong Li", "Angela Yao"], "title": "AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation", "comment": "Accepted by AAAI 2026. Project page: https://jyzhu.top/AnchorDS_Webpage/", "summary": "Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to \"semantic over-smoothing\" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AnchorDS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u52303D\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u52a8\u6001\u6e90\u5206\u5e03\u5230\u56fa\u5b9a\u76ee\u6807\u5206\u5e03\u7684\u6620\u5c04\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSDS\u65b9\u6cd5\u4e2d\u7684\u8bed\u4e49\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4f18\u5316\u7684\u6587\u672c\u52303D\u65b9\u6cd5\u5c062D\u751f\u6210\u6a21\u578b\u7684\u5f15\u5bfc\u89c6\u4e3a\u9759\u6001\uff0c\u5ffd\u7565\u4e86\u6e90\u52a8\u6001\uff0c\u5bfc\u81f4\u8bed\u4e49\u7ebf\u7d22\u88ab\u6291\u5236\u6216\u5408\u5e76\uff0c\u4ea7\u751f\u8bed\u4e49\u8fc7\u5e73\u6ed1\u4f2a\u5f71\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u53cc\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\uff0c\u540c\u65f6\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u548c\u4e2d\u95f4\u6e32\u67d3\u56fe\u50cf\u8fdb\u884c\u6761\u4ef6\u5316\u3002\u63d0\u51faAnchorDS\u673a\u5236\uff0c\u63d0\u4f9b\u72b6\u6001\u951a\u5b9a\u5f15\u5bfc\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u8fc7\u6ee4\u7b56\u7565\u548c\u5fae\u8c03\u7b56\u7565\u6765\u4f18\u5316\u951a\u70b9\u3002", "result": "AnchorDS\u80fd\u591f\u751f\u6210\u66f4\u7cbe\u7ec6\u7684\u7ec6\u8282\u3001\u66f4\u81ea\u7136\u7684\u989c\u8272\u548c\u66f4\u5f3a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63d0\u793a\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u6548\u7387\u4e0a\u4fdd\u6301\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u90fd\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u4e3a\u6587\u672c\u52303D\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12963", "abs": "https://arxiv.org/abs/2511.12963", "authors": ["Crystal Su"], "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning", "comment": "AAAI 2026 Workshop AI2ASE", "summary": "We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.", "AI": {"tldr": "MedRule-KG\u662f\u4e00\u4e2a\u4e3a\u79d1\u5b66\u63a8\u7406\u548c\u65e9\u671f\u836f\u7269\u53d1\u73b0\u8bbe\u8ba1\u7684\u9886\u57df\u4e00\u81f4\u6027\u7ed3\u6784\u7cfb\u7edf\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u652f\u67b6\u548c\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\u6765\u5f15\u5bfcLLM\u751f\u6210\u6570\u5b66\u548c\u751f\u7269\u533b\u5b66\u4e0a\u6709\u6548\u7684\u8f93\u51fa\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u548c\u836f\u7269\u53d1\u73b0\u4e2d\u7f3a\u4e4f\u9886\u57df\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8f93\u51fa\u5728\u6570\u5b66\u548c\u751f\u7269\u533b\u5b66\u4e0a\u6709\u6548\u3002", "method": "\u4f7f\u7528\u7d27\u51d1\u7684\u77e5\u8bc6\u56fe\u8c31\u652f\u67b6\u914d\u5408\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\uff0c\u5c06\u7cbe\u9009\u7684\u7b26\u53f7\u4e8b\u5b9e\u6ce8\u5165\u63d0\u793a\u4e2d\uff0c\u5e76\u901a\u8fc7\u786e\u5b9a\u6027\u68c0\u67e5\u5668\u5f3a\u5236\u6267\u884c\u89c4\u5219\u6ee1\u8db3\u3002\u5c06\u751f\u6210\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u63a8\u7406\uff0c\u5f15\u5165\u9002\u5408\u89e3\u7801\u7684\u8f6f\u5f15\u5bfc\u66ff\u4ee3\u65b9\u6cd5\u3002", "result": "\u572890\u4e2a\u4efb\u52a1\u4e2d\uff0cMedRule-KG\u76f8\u5bf9\u4e8e\u5f3a\u94fe\u5f0f\u601d\u7ef4\u57fa\u7ebf\u5c06\u8fdd\u89c4\u6570\u91cf\u51cf\u5c11\u4e8683.2%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u7cbe\u786e\u5339\u914d\u7387\u3002\u7ed3\u679c\u5728\u5206\u5c42\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0c\u5e76\u968f\u6570\u636e\u96c6\u89c4\u6a21\u6269\u5c55\uff0c\u9a8c\u8bc1\u5668\u4ec5\u589e\u52a0\u53ef\u5ffd\u7565\u7684\u5ef6\u8fdf\u3002", "conclusion": "MedRule-KG\u4e3a\u4ea4\u4e92\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u9ad8LLM\u5728\u79d1\u5b66\u63a8\u7406\u548c\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u9886\u57df\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.12928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12928", "abs": "https://arxiv.org/abs/2511.12928", "authors": ["Haokun Li", "Yazhou Zhang", "Jizhi Ding", "Qiuchi Li", "Peng Zhang"], "title": "Visual Room 2.0: Seeing is Not Understanding for MLLMs", "comment": null, "summary": "Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \\textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\\%$\\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86Visual Room 2.0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5-\u8ba4\u77e5\u5bf9\u9f50\uff0c\u53d1\u73b0MLLMs\u7684\u611f\u77e5\u80fd\u529b\u4f18\u4e8e\u8ba4\u77e5\u80fd\u529b\uff0c\u4e14\u8ba4\u77e5\u4e0d\u4f9d\u8d56\u4e8e\u611f\u77e5\u63a8\u7406\u3002", "motivation": "\u57fa\u4e8eSearle\u7684\u4e2d\u6587\u623f\u95f4\u601d\u60f3\u6269\u5c55\u5230\u591a\u6a21\u6001\u9886\u57df\uff0c\u63a2\u8ba8MLLMs\u662f\u5426\u80fd\u771f\u6b63\u7406\u89e3\u6240\u89c1\u5185\u5bb9\uff0c\u5373\u770b\u5230\u4e0d\u7b49\u4e8e\u7406\u89e3\u3002", "method": "\u6784\u5efa\u5305\u542b350\u4e2a\u591a\u6a21\u6001\u6837\u672c\u7684\u5206\u5c42\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4f4e\u3001\u4e2d\u3001\u9ad8\u4e09\u4e2a\u5c42\u6b21\u768417\u4e2a\u4efb\u52a1\uff0c\u6bcf\u4e2a\u6837\u672c\u67096\u4e2a\u6e10\u8fdb\u95ee\u9898\uff0c\u51712100\u4e2a\u95ee\u9898\u3002", "result": "\u8bc4\u4f3010\u4e2aSOTA MLLMs\u53d1\u73b0\uff1a(1)\u611f\u77e5\u80fd\u529b\u6bd4\u8ba4\u77e5\u80fd\u529b\u5f3a8.0%\uff1b(2)\u8ba4\u77e5\u4e0d\u56e0\u679c\u4f9d\u8d56\u4e8e\u611f\u77e5\u63a8\u7406\uff1b(3)\u8ba4\u77e5\u968f\u6a21\u578b\u89c4\u6a21\u6269\u5c55\uff0c\u4f46\u611f\u77e5\u4e0d\u968f\u6a21\u578b\u53d8\u5927\u800c\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u5c06\"\u770b\u5230\u2260\u7406\u89e3\"\u64cd\u4f5c\u5316\u4e3a\u53ef\u6d4b\u8bd5\u5047\u8bbe\uff0c\u4e3aMLLMs\u4ece\u611f\u77e5\u5904\u7406\u5230\u8ba4\u77e5\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.12997", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12997", "abs": "https://arxiv.org/abs/2511.12997", "authors": ["Genglin Liu", "Shijie Geng", "Sha Li", "Hejie Cui", "Sarah Zhang", "Xin Liu", "Tianyi Liu"], "title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance", "comment": "18 pages; work in progress", "summary": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.", "AI": {"tldr": "WebCoach\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u4e3a\u7f51\u9875\u6d4f\u89c8\u4ee3\u7406\u63d0\u4f9b\u6301\u4e45\u8de8\u4f1a\u8bdd\u8bb0\u5fc6\uff0c\u901a\u8fc7\u8bb0\u5fc6\u5b58\u50a8\u548c\u7ecf\u9a8c\u68c0\u7d22\u673a\u5236\u63d0\u5347\u957f\u671f\u89c4\u5212\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001LLM\u4ee3\u7406\u5728\u7f51\u9875\u5bfc\u822a\u4e2d\u9762\u4e34\u91cd\u590d\u9519\u8bef\u548c\u65e0\u6cd5\u8de8\u4f1a\u8bdd\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u957f\u671f\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "method": "WebCoach\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aWebCondenser\u6807\u51c6\u5316\u5bfc\u822a\u65e5\u5fd7\u4e3a\u6458\u8981\uff0cExternal Memory Store\u7ec4\u7ec7\u5b8c\u6574\u8f68\u8ff9\u4f5c\u4e3a\u7ecf\u9a8c\uff0cCoach\u57fa\u4e8e\u76f8\u4f3c\u6027\u548c\u65f6\u6548\u6027\u68c0\u7d22\u76f8\u5173\u7ecf\u9a8c\u5e76\u901a\u8fc7\u8fd0\u884c\u65f6\u94a9\u5b50\u5411\u4ee3\u7406\u6ce8\u5165\u4efb\u52a1\u7279\u5b9a\u5efa\u8bae\u3002", "result": "\u5728WebVoyager\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWebCoach\u6301\u7eed\u63d0\u5347\u4e09\u79cd\u4e0d\u540cLLM\u9aa8\u5e72\u7684\u6d4f\u89c8\u5668\u4ee3\u7406\u6027\u80fd\u3002\u4f7f\u752838B\u6a21\u578b\u65f6\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4ece47%\u63d0\u5347\u81f361%\uff0c\u540c\u65f6\u51cf\u5c11\u6216\u7ef4\u6301\u5e73\u5747\u6b65\u9aa4\u6570\u3002\u8f83\u5c0f\u57fa\u7840\u6a21\u578b\u914d\u5408WebCoach\u80fd\u8fbe\u5230\u4e0e\u4f7f\u7528GPT-4o\u7684\u76f8\u540c\u7f51\u9875\u4ee3\u7406\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "WebCoach\u901a\u8fc7\u8d4b\u4e88\u7f51\u9875\u4ee3\u7406\u6301\u4e45\u8de8\u4f1a\u8bdd\u8bb0\u5fc6\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u81ea\u8fdb\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6d4f\u89c8\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2511.12991", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12991", "abs": "https://arxiv.org/abs/2511.12991", "authors": ["Zeyu Shi", "Ziming Wang", "Tianyu Chen", "Shiqi Gao", "Haoyi Zhou", "Qingyun Sun", "Jianxin Li"], "title": "Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty", "comment": "Accepted by AAAI 2026 Main Track", "summary": "The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.", "AI": {"tldr": "\u63d0\u51faHCNR\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u6062\u590d\u5173\u952e\u8868\u8fbe\u795e\u7ecf\u5143\u6765\u4fee\u590dSFT\u5bfc\u81f4\u7684LLM\u8bda\u5b9e\u6027\u95ee\u9898\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u901f\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u4e25\u91cd\u635f\u5bb3\u4e86LLM\u7684\u8bda\u5b9e\u6027\uff0c\u4f46\u73b0\u6709\u6062\u590d\u65b9\u6cd5\u5047\u8bbe\u6a21\u578b\u5b8c\u5168\u4e27\u5931\u4e86\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\u7684\u80fd\u529b\uff0c\u800c\u5b9e\u9645\u4e0a\u6a21\u578b\u4ecd\u4fdd\u7559\u8fd9\u79cd\u80fd\u529b\uff0c\u53ea\u662f\u8868\u8fbe\u8fd9\u79cd\u610f\u8bc6\u7684\u80fd\u529b\u53d7\u635f\u3002", "method": "HCNR\u65b9\u6cd5\uff1a1\uff09\u8bc6\u522b\u63a7\u5236\u8bda\u5b9e\u8868\u8fbe\u7684\u5173\u952e\u795e\u7ecf\u5143\uff1b2\uff09\u5c06\u8fd9\u4e9b\u795e\u7ecf\u5143\u6062\u590d\u5230\u9884\u8bad\u7ec3\u72b6\u6001\uff1b3\uff09\u901a\u8fc7Hessian\u5f15\u5bfc\u7684\u8865\u507f\u673a\u5236\u534f\u8c03\u4efb\u52a1\u5bfc\u5411\u795e\u7ecf\u5143\u3002", "result": "\u57284\u4e2aQA\u4efb\u52a1\u548c5\u4e2aLLM\u5bb6\u65cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHCNR\u6062\u590d\u4e8633.25%\u7684\u53d7\u635f\u8bda\u5b9e\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u81f3\u5c112.23\u500d\u52a0\u901f\u548c\u8d85\u8fc710\u500d\u7684\u6570\u636e\u51cf\u5c11\u3002", "conclusion": "HCNR\u4e3a\u53ef\u4fe1\u8d56LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u4fee\u590dSFT\u5bfc\u81f4\u7684\u8bda\u5b9e\u6027\u635f\u5bb3\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u3002"}}
{"id": "2511.11697", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.11697", "abs": "https://arxiv.org/abs/2511.11697", "authors": ["Liqin Tan", "Pin Chen", "Menghan Liu", "Xiean Wang", "Jianhuan Cen", "Qingsong Zou"], "title": "Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification", "comment": "12 pages, 1 figure, 5 tables", "summary": "We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.", "AI": {"tldr": "MatUQ\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u4e2d\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5305\u542b1375\u4e2aOOD\u9884\u6d4b\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7ed3\u6784\u611f\u77e5\u5206\u5272\u7b56\u7565SOAP-LOCO\u548c\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cfD-EviU\u3002", "motivation": "\u73b0\u6709\u6750\u6599\u9884\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u8981\u5f00\u53d1\u7edf\u4e00\u7684\u57fa\u51c6\u6846\u67b6\u6765\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u3002", "method": "\u4f7f\u75286\u4e2a\u6750\u6599\u6570\u636e\u96c6\u6784\u5efa1375\u4e2aOOD\u9884\u6d4b\u4efb\u52a1\uff0c\u91c7\u75285\u79cdOFM\u5206\u5272\u548c\u65b0\u63d0\u51fa\u7684SOAP-LOCO\u5206\u5272\u7b56\u7565\uff0c\u8bc4\u4f3012\u4e2aGNN\u6a21\u578b\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1bDropout\u548c\u6df1\u5ea6\u8bc1\u636e\u56de\u5f52\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bad\u7ec3\u3002", "result": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bad\u7ec3\u5e73\u5747\u51cf\u5c1170.6%\u7684\u9884\u6d4b\u8bef\u5dee\uff1bD-EviU\u6307\u6807\u4e0e\u9884\u6d4b\u8bef\u5dee\u76f8\u5173\u6027\u6700\u5f3a\uff1b\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u5360\u4f18\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u6750\u6599\u5c5e\u6027\u4e0a\u8868\u73b0\u5404\u5f02\u3002", "conclusion": "MatUQ\u4e3a\u6750\u6599\u53d1\u73b0\u4e2d\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728OOD\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13007", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13007", "abs": "https://arxiv.org/abs/2511.13007", "authors": ["Yiyang Zhao", "Huiyu Bai", "Xuejiao Zhao"], "title": "GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs", "comment": "This paper has been accepted by AAAI 2026-AIA and designated as an oral presentation paper", "summary": "Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.", "AI": {"tldr": "\u63d0\u51fa\u4e86GEM\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u71b5\u5f15\u5bfc\u504f\u597d\u5efa\u6a21\uff0c\u5728\u4f4e\u8d44\u6e90\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f\u4e0b\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u5728\u533b\u5b66\u3001\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u9886\u57df\uff0c\u83b7\u53d6\u5927\u89c4\u6a21\u504f\u597d\u6807\u6ce8\u6570\u636e\u901a\u5e38\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5b9e\u73b0\u6a21\u578b\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u51b3\u7b56\u71b5\u7406\u8bba\uff0c\u4f7f\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u94fe\uff0c\u5f15\u5165token\u8bc4\u5206\u673a\u5236\u5bf9\u63a8\u7406\u94fe\u8fdb\u884c\u6392\u5e8f\u52a0\u6743\uff0c\u7136\u540e\u4f7f\u7528\u81ea\u8bc4\u4f30\u7fa4\u4f53\u4f18\u52bf\u7b97\u6cd5(SEGA)\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u901a\u7528\u57fa\u51c6\u548c\u9886\u57df\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\u548c\u533b\u7597\u5bf9\u8bdd\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGEM\u5728\u5c11\u91cf\u504f\u597d\u6570\u636e\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "GEM\u5efa\u7acb\u4e86\u4e00\u4e2a\u71b5\u5f15\u5bfc\u7684\u95ed\u73af\u8ba4\u77e5\u4f18\u5316\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u591f\u4f9d\u8d56\u81ea\u8eab\u5224\u65ad\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5c11\u6837\u672c\u5bf9\u9f50\u3002"}}
{"id": "2511.13029", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13029", "abs": "https://arxiv.org/abs/2511.13029", "authors": ["Declan Jackson", "William Keating", "George Cameron", "Micah Hill-Smith"], "title": "AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models", "comment": null, "summary": "Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.", "AI": {"tldr": "AA-Omniscience\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u56de\u5fc6\u80fd\u529b\u548c\u77e5\u8bc6\u6821\u51c6\u80fd\u529b\uff0c\u8986\u76d642\u4e2a\u7ecf\u6d4e\u76f8\u5173\u4e3b\u9898\u76846000\u4e2a\u95ee\u9898\u3002Claude 4.1 Opus\u83b7\u5f97\u6700\u9ad8\u52064.8\uff0c\u4ec5\u4e09\u4e2a\u6a21\u578b\u5f97\u5206\u8d85\u8fc70\uff0c\u663e\u793a\u524d\u6cbf\u6a21\u578b\u5728\u4e8b\u5b9e\u6027\u548c\u6821\u51c6\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5f31\u70b9\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u8861\u91cf\u901a\u7528\u80fd\u529b\uff0c\u4f46\u53ef\u9760\u4f7f\u7528\u9700\u8981\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u77e5\u8bc6\u5dee\u8ddd\u8bc6\u522b\u80fd\u529b\u3002\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u4e8b\u5b9e\u56de\u5fc6\u548c\u77e5\u8bc6\u6821\u51c6\u3002", "method": "\u6784\u5efaAA-Omniscience\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea\u6743\u5a01\u5b66\u672f\u548c\u884c\u4e1a\u6765\u6e90\u76846000\u4e2a\u95ee\u9898\uff0c\u8986\u76d66\u4e2a\u9886\u57df\u768442\u4e2a\u7ecf\u6d4e\u76f8\u5173\u4e3b\u9898\u3002\u4f7f\u7528\u5168\u77e5\u6307\u6570(-100\u5230100)\u8bc4\u4f30\u4e8b\u5b9e\u56de\u5fc6\uff0c\u540c\u65f6\u60e9\u7f5a\u5e7b\u89c9\u548c\u5956\u52b1\u4e0d\u786e\u5b9a\u65f6\u7684\u5f03\u6743\u3002", "result": "Claude 4.1 Opus\u83b7\u5f97\u6700\u9ad8\u52064.8\uff0c\u4ec5\u4e09\u4e2a\u6a21\u578b\u5f97\u5206\u8d85\u8fc70\u3002\u4e0d\u540c\u9886\u57df\u8868\u73b0\u5dee\u5f02\u660e\u663e\uff0c\u4e09\u4e2a\u4e0d\u540c\u7814\u7a76\u5b9e\u9a8c\u5ba4\u7684\u6a21\u578b\u5728\u516d\u4e2a\u9886\u57df\u4e2d\u5404\u81ea\u9886\u5148\u3002", "conclusion": "\u6a21\u578b\u5728\u4e8b\u5b9e\u6027\u548c\u6821\u51c6\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5f31\u70b9\uff0c\u6027\u80fd\u56e0\u9886\u57df\u800c\u5f02\u3002\u5bf9\u4e8e\u77e5\u8bc6\u91cd\u8981\u4efb\u52a1\uff0c\u5e94\u6839\u636e\u7528\u4f8b\u9700\u6c42\u800c\u975e\u901a\u7528\u6027\u80fd\u9009\u62e9\u6a21\u578b\u3002"}}
{"id": "2511.11698", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11698", "abs": "https://arxiv.org/abs/2511.11698", "authors": ["Chenghao Liu", "Taha Aksu", "Juncheng Liu", "Xu Liu", "Hanshu Yan", "Quang Pham", "Doyen Sahoo", "Caiming Xiong", "Silvio Savarese", "Junnan Li"], "title": "Moirai 2.0: When Less Is More for Time Series Forecasting", "comment": "16 pages, 13 figures, and 1 table", "summary": "We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.", "AI": {"tldr": "Moirai 2.0\u662f\u4e00\u4e2a\u57fa\u4e8e\u89e3\u7801\u5668\u67b6\u6784\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u5206\u4f4d\u6570\u9884\u6d4b\u548c\u591a\u4ee4\u724c\u9884\u6d4b\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u6bd4\u524d\u4ee3\u6a21\u578b\u66f4\u5feb\u66f4\u5c0f\u4e14\u6027\u80fd\u66f4\u597d\u3002", "motivation": "\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7b80\u5316\u67b6\u6784\u548c\u4f18\u5316\u8bad\u7ec3\u65b9\u6cd5\u6765\u89e3\u51b3\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7eaf\u89e3\u7801\u5668\u67b6\u6784\u3001\u5355patch\u8f93\u5165\u548c\u5206\u4f4d\u6570\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u9012\u5f52\u591a\u5206\u4f4d\u6570\u89e3\u7801\u6280\u672f\uff0c\u66ff\u4ee3\u4e86\u4e4b\u524d\u7684\u63a9\u7801\u7f16\u7801\u5668\u8bad\u7ec3\u3001\u591apatch\u8f93\u5165\u548c\u6df7\u5408\u5206\u5e03\u8f93\u51fa\u3002", "result": "\u5728Gift-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4Moirai 1.0-Large\u5feb\u4e24\u500d\u3001\u5c0f\u4e09\u5341\u500d\u4e14\u6027\u80fd\u66f4\u597d\uff0c\u5728\u53c2\u6570\u6570\u91cf\u589e\u52a0\u65f6\u6027\u80fd\u8d8b\u4e8e\u5e73\u7a33\uff0c\u5728\u957f\u9884\u6d4b\u65f6\u57df\u4e0a\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "Moirai 2.0\u5728\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u6a21\u578b\u5927\u5c0f\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u672a\u6765\u9700\u8981\u5728\u6570\u636e\u6269\u5c55\u548c\u957f\u65f6\u57df\u5efa\u6a21\u65b9\u9762\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.13021", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13021", "abs": "https://arxiv.org/abs/2511.13021", "authors": ["Sachin Vashistha", "Aryan Bibhuti", "Atharva Naik", "Martin Tutek", "Somak Aditya"], "title": "PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics", "comment": "23 pages, 15 tables, 10 figures; AAAI 2026 Conference Main Track (oral)", "summary": "Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u6784\u5efa\u548c\u66f4\u65b0\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\uff0c\u6d4b\u8bd5\u5176\u5728\u8bed\u8a00\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5c42\u6b63\u5219\u5316\u7684\u5fae\u8c03\u7b56\u7565\u6765\u6291\u5236\u6709\u5bb3\u5c42\u7684\u5f71\u54cd\u3002", "motivation": "\u771f\u5b9e\u5bf9\u8bdd\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u7528\u5143\u7d20\uff0c\u9700\u8981\u6784\u5efa\u5c40\u90e8\u4e16\u754c\u6a21\u578b\u6765\u7f16\u7801\u8fd9\u4e9b\u5143\u7d20\u5e76\u6355\u6349\u5176\u72b6\u6001\u53d8\u5316\u3002\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u6784\u5efa\u6216\u7ef4\u62a4\u5f3a\u5927\u7684\u9690\u5f0f\u5bf9\u8bdd\u8868\u793a\u3002", "method": "\u5bf9\u6d41\u884c\u6570\u636e\u96c6\u4e2d\u7684\u5bf9\u8bdd\u5e94\u7528\u4e03\u79cd\u6700\u5c0f\u8bed\u8a00\u53d8\u5316\uff0c\u6784\u5efa\u4e24\u4e2a\u5305\u542b\u662f\u975e\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90\u548c\u95ed\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u53cc\u89c6\u89d2\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u8bc6\u522b\u6709\u7528\u548c\u6709\u5bb3\u7684Transformer\u5c42\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u5728\u7ef4\u6301\u9c81\u68d2\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u8bb0\u5fc6\u5173\u952e\u7ec6\u8282\uff08\u5982\u8ddf\u8e2a\u8bed\u8a00\u53d8\u5316\u4e0b\u7684\u5b9e\u4f53\uff09\u3002\u6709\u5bb3\u5c42\u901a\u5e38\u7531\u4e8e\u7f16\u7801\u865a\u5047\u4fe1\u53f7\u6216\u4f9d\u8d56\u6377\u5f84\u800c\u5f71\u54cd\u8bed\u8a00\u53d8\u5316\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u6784\u5efa\u548c\u66f4\u65b0\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\u6709\u9650\uff0c\u4f46\u901a\u8fc7\u5c42\u6b63\u5219\u5316\u5fae\u8c03\u7b56\u7565\u53ef\u4ee5\u6291\u5236\u6709\u5bb3\u5c42\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.13040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13040", "abs": "https://arxiv.org/abs/2511.13040", "authors": ["Kasun Wickramasinghe", "Nisansa de Silva"], "title": "How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm", "comment": "15 pages, 2 figures, 6 tables", "summary": "Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u4e0e\u5bf9\u9f50\u5355\u8bed\u6a21\u578b\u5728\u53cc\u8bed\u8bcd\u5178\u5f52\u7eb3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u5206\u6790\u4e86BLI\u4f5c\u4e3a\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u5ea6\u91cf\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8bcd\u5e72\u7684\u65b0BLI\u65b9\u6cd5\u548c\u8bcd\u6c47\u526a\u679d\u6280\u672f\u6765\u66f4\u51c6\u786e\u8bc4\u4f30\u5bf9\u9f50\u7a0b\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u591a\u8bed\u8a00\u5d4c\u5165\u5df2\u6210\u4e3a\u4e3b\u6d41\u9009\u62e9\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u662f\u5426\u5728\u6240\u6709\u65b9\u9762\u90fd\u4f18\u4e8e\u5bf9\u9f50\u5355\u8bed\u6a21\u578b\uff0c\u4ee5\u53ca\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u662f\u5426\u603b\u662f\u5408\u7406\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22BLI\u4f5c\u4e3a\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u5ea6\u91cf\u7684\u6709\u6548\u6027\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u5d4c\u5165\u5bf9\u9f50\u6280\u672f\u5728\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u53cc\u8bed\u8bcd\u5178\u5f52\u7eb3\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u6bd4\u8f83\u4f20\u7edf\u5d4c\u5165\u5bf9\u9f50\u6280\u672f\u3001\u65b0\u578b\u591a\u8bed\u8a00\u6a21\u578b\u548c\u7ec4\u5408\u5bf9\u9f50\u6280\u672f\u7684\u8868\u73b0\u3002\u5206\u6790\u8bed\u8a00\u5bb6\u65cf\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u57fa\u4e8e\u8bcd\u5e72\u7684BLI\u65b9\u6cd5\u548c\u8bcd\u6c47\u526a\u679d\u6280\u672f\u6765\u6539\u8fdb\u5bf9\u9f50\u5ea6\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0BLI\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e0d\u80fd\u51c6\u786e\u8861\u91cf\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u7ec4\u5408\u5d4c\u5165\u5bf9\u9f50\u6280\u672f\u901a\u5e38\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u60c5\u51b5\u4e0b\u591a\u8bed\u8a00\u5d4c\u5165\u8868\u73b0\u66f4\u4f18\u3002\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u3002", "conclusion": "\u591a\u8bed\u8a00\u5d4c\u5165\u548c\u5bf9\u9f50\u5355\u8bed\u6a21\u578b\u5404\u6709\u4f18\u52bf\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u9009\u62e9\u3002BLI\u4f5c\u4e3a\u5bf9\u9f50\u5ea6\u91cf\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7684\u57fa\u4e8e\u8bcd\u5e72\u7684BLI\u548c\u8bcd\u6c47\u526a\u679d\u6280\u672f\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5bf9\u9f50\u8bc4\u4f30\u3002"}}
{"id": "2511.11699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11699", "abs": "https://arxiv.org/abs/2511.11699", "authors": ["Xingqi Lin", "Liangyu Chen", "Min Wu", "Min Zhang", "Zhenbing Zeng"], "title": "Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification", "comment": null, "summary": "Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \\emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86DeepPrism\u65b9\u6cd5\uff0c\u901a\u8fc7\u622a\u65ad\u77e9\u5f62\u68f1\u67f1\u6765\u7d27\u5bc6\u5305\u56f4Hadamard\u79ef\u751f\u6210\u7684\u4e09\u7ef4\u975e\u7ebf\u6027\u66f2\u9762\uff0c\u663e\u8457\u63d0\u9ad8\u4e86RNN\u9c81\u68d2\u6027\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u8fdb\u884c\u7ebf\u6027\u7ea6\u675f\u8fc7\u8fd1\u4f3c\u65f6\uff0c\u5355\u72ec\u4f7f\u7528\u7ebf\u6027\u8fb9\u754c\u5e73\u9762\u53ef\u80fd\u5bfc\u81f4\u663e\u8457\u8fc7\u4f30\u8ba1\uff0c\u964d\u4f4e\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u7ebf\u6027\u677e\u5f1b\u5e73\u9762\u5f62\u6210\u622a\u65ad\u77e9\u5f62\u68f1\u67f1\u6765\u5305\u56f4Hadamard\u79ef\u7684\u4e09\u7ef4\u975e\u7ebf\u6027\u66f2\u9762\uff0c\u5e76\u91c7\u7528\u7ec6\u5316\u9a71\u52a8\u65b9\u6cd5\u6700\u5c0f\u5316\u5176\u4f53\u79ef\u548c\u8868\u9762\u79ef\u4ee5\u5b9e\u73b0\u66f4\u7d27\u5bc6\u7684\u8fc7\u8fd1\u4f3c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeepPrism\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u622a\u65ad\u77e9\u5f62\u68f1\u67f1\u65b9\u6cd5\u80fd\u591f\u66f4\u7d27\u5bc6\u5730\u8fd1\u4f3c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u6709\u6548\u63d0\u9ad8\u4e86RNN\u9c81\u68d2\u6027\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.13027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13027", "abs": "https://arxiv.org/abs/2511.13027", "authors": ["Sadegh Mahdavi", "Branislav Kisacanin", "Shubham Toshniwal", "Wei Du", "Ivan Moshkov", "George Armstrong", "Renjie Liao", "Christos Thrampoulidis", "Igor Gitman"], "title": "Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection", "comment": null, "summary": "Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u8bc1\u660e\u9a8c\u8bc1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5355\u4e00\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u8bba\uff0c\u63d0\u51fa\u7ed3\u5408\u8bc1\u660e\u9a8c\u8bc1\u548c\u6700\u7ec8\u7b54\u6848\u8bc4\u4f30\u7684\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u4e86GenSelect\u548cLLM-as-a-Judge\u7ec4\u5408\u4e3a\u6700\u6709\u6548\u7684\u9a8c\u8bc1\u6846\u67b6\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6700\u7ec8\u7b54\u6848\u6570\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u5f80\u5f80\u5b58\u5728\u7f3a\u9677\u3002\u4e3a\u4e86\u63a8\u8fdb\u5230\u4e25\u8c28\u7684\u8bc1\u660e\u57fa\u7840\u6570\u5b66\uff0c\u9700\u8981\u53ef\u9760\u7684\u8bc1\u660e\u9a8c\u8bc1\u80fd\u529b\u3002", "method": "\u5206\u6790\u4e86\u591a\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff0c\u7ed3\u5408\u8bc1\u660e\u9a8c\u8bc1\u548c\u6700\u7ec8\u7b54\u6848\u63a8\u7406\u8fdb\u884c\u8bc4\u4f30\uff1b\u5c06\u4e24\u79cd\u751f\u6210\u9a8c\u8bc1\u65b9\u6cd5\uff08GenSelect\u548cLLM-as-a-Judge\uff09\u6269\u5c55\u5230\u767e\u4e07token\u89c4\u6a21\uff1b\u7814\u7a76\u63d0\u793a\u9009\u62e9\u5bf9LLM-as-a-Judge\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u964d\u4f4e\u654f\u611f\u6027\u3002", "result": "GenSelect\u548cLLM-as-a-Judge\u7684\u7ec4\u5408\u662f\u6700\u6709\u6548\u7684\u9a8c\u8bc1\u6846\u67b6\uff1b\u63d0\u793a\u9009\u62e9\u663e\u8457\u5f71\u54cdLLM-as-a-Judge\u6027\u80fd\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u964d\u4f4e\u8fd9\u79cd\u654f\u611f\u6027\uff1b\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u6539\u5584\u4e86\u8bc1\u660e\u7ea7\u6307\u6807\uff0c\u4f46\u5e76\u672a\u63d0\u9ad8\u6700\u7ec8\u7b54\u6848\u7cbe\u5ea6\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5f80\u5f80\u5956\u52b1\u98ce\u683c\u6216\u7a0b\u5e8f\u6b63\u786e\u6027\u800c\u975e\u6570\u5b66\u6709\u6548\u6027\uff1b\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u548c\u8bc4\u4f30\u53ef\u6269\u5c55\u7684\u8bc1\u660e\u9a8c\u8bc1\u548c\u9009\u62e9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2511.13043", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13043", "abs": "https://arxiv.org/abs/2511.13043", "authors": ["Xinyuan Zhou", "Yi Lei", "Xiaoyu Zhou", "Jingyi Sun", "Yu Zhu", "Zhongyi Ye", "Weitai Zhang", "Quan Liu", "Si Wei", "Cong Liu"], "title": "Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training", "comment": null, "summary": "Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a \"CoT-augmented state prediction\" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.", "AI": {"tldr": "Spark-Prover-X1\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u5b9a\u7406\u8bc1\u660e\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u63d0\u5347\u8f7b\u91cf\u7ea7LLM\u7684\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4e2d\u56e0\u7f3a\u4e4f\u591a\u6837\u5316\u548c\u9ad8\u8d28\u91cf\u7684\u5f62\u5f0f\u8bed\u8a00\u6570\u636e\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1) \u5728\u5e7f\u6cdb\u6570\u5b66\u8bed\u6599\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5f15\u5165\"\u601d\u7ef4\u94fe\u589e\u5f3a\u72b6\u6001\u9884\u6d4b\"\u4efb\u52a1\uff1b2) \u5728\u4e13\u5bb6\u8fed\u4ee3\u5faa\u73af\u4e2d\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b3) \u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u9488\u5bf9\u6700\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u8fdb\u884c\u5f3a\u5316\u8bad\u7ec3\u3002", "result": "Spark-Prover-X1-7B\u5728pass@32\u4e0b\u8fbe\u523037.0%\u7684\u5e73\u5747\u901a\u8fc7\u7387\uff0c\u5728PutnamBench\u4e0a\u89e3\u51b327\u4e2a\u95ee\u9898\uff0c\u5728CombiBench\u4e0a\u8fbe\u523024.0%\u7684\u901a\u8fc7\u7387\u3002", "conclusion": "\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u548c\u9010\u6b65\u7cbe\u70bc\u7684\u8bad\u7ec3\u6d41\u7a0b\u4e3a\u589e\u5f3a\u8f7b\u91cf\u7ea7LLM\u7684\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2511.11701", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11701", "abs": "https://arxiv.org/abs/2511.11701", "authors": ["Abhinav Das", "Stephan Schl\u00fcter"], "title": "Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting", "comment": null, "summary": "Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u8499\u7279\u5361\u6d1bdropout\u7684\u6982\u7387\u6027\u7535\u4ef7\u9884\u6d4b\u6846\u67b6\uff0c\u6309\u5c0f\u65f6\u5206\u522b\u5efa\u6a21\u4ee5\u6355\u6349\u663c\u591c\u6a21\u5f0f\uff0c\u5728\u70b9\u9884\u6d4b\u548c\u533a\u95f4\u9884\u6d4b\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u5728\u653e\u677e\u7ba1\u5236\u7684\u7535\u529b\u5e02\u573a\u4e2d\uff0c\u51c6\u786e\u7684\u7535\u4ef7\u9884\u6d4b\u5bf9\u6218\u7565\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u70b9\u9884\u6d4b\u65e0\u6cd5\u6355\u6349\u5185\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u98ce\u9669\u7ba1\u7406\u6548\u7528\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u8499\u7279\u5361\u6d1bdropout\u6784\u5efa\u6982\u7387\u9884\u6d4b\u6846\u67b6\uff0c\u4e3a\u6bcf\u5929\u7684\u4e0d\u540c\u5c0f\u65f6\u5206\u522b\u8bad\u7ec3\u6a21\u578b\u4ee5\u6355\u6349\u663c\u591c\u6a21\u5f0f\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u70b9\u9884\u6d4b\u548c\u533a\u95f4\u9884\u6d4b\u65b9\u9762\u5747\u4f18\u4e8eGARCHX\u548cLEAR\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u80fd\u6e90\u5e02\u573a\u9884\u6d4b\u4e2d\u5229\u7528\u6982\u7387\u6027\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2511.13087", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13087", "abs": "https://arxiv.org/abs/2511.13087", "authors": ["SeokJoo Kwak", "Jihoon Kim", "Boyoun Kim", "Jung Jae Yoon", "Wooseok Jang", "Jeonghoon Hong", "Jaeho Yang", "Yeong-Dae Kwon"], "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements", "comment": "26 pages, 7 figures. Code available at https://github.com/samsungsds-research-papers/mega-gui", "summary": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.", "AI": {"tldr": "MEGA-GUI\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u7684GUI\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6ROI\u9009\u62e9\u548c\u7ec6\u7c92\u5ea6\u5143\u7d20\u5b9a\u4f4d\u6765\u89e3\u51b3\u89c6\u89c9\u6742\u4e71\u548c\u8bed\u4e49\u6a21\u7cca\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GUI\u5b9a\u4f4d\u7cfb\u7edf\u91c7\u7528\u5355\u6a21\u578b\u6216\u4e00\u6b21\u6027\u6d41\u6c34\u7ebf\uff0c\u7f3a\u4e4f\u6a21\u5757\u5316\uff0c\u5728\u89c6\u89c9\u6742\u4e71\u548c\u6307\u4ee4\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u5206\u79bb\u4e3a\u7c97\u7c92\u5ea6ROI\u9009\u62e9\u548c\u7ec6\u7c92\u5ea6\u5143\u7d20\u5b9a\u4f4d\uff0c\u4f7f\u7528\u4e13\u95e8\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u534f\u8c03\uff0c\u5305\u542b\u53cc\u5411ROI\u7f29\u653e\u7b97\u6cd5\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u91cd\u5199\u4ee3\u7406\u3002", "result": "\u5728\u89c6\u89c9\u5bc6\u96c6\u7684ScreenSpot-Pro\u57fa\u51c6\u4e0a\u8fbe\u523073.18%\u51c6\u786e\u7387\uff0c\u5728\u8bed\u4e49\u590d\u6742\u7684OSWorld-G\u57fa\u51c6\u4e0a\u8fbe\u523068.63%\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u62a5\u544a\u7684\u7ed3\u679c\u3002", "conclusion": "\u6a21\u5757\u5316\u7ed3\u6784\u80fd\u591f\u5229\u7528\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u89c6\u89c9\u5c3a\u5ea6\u4e0a\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u6bd4\u5355\u4e00\u65b9\u6cd5\u83b7\u5f97\u66f4\u4e00\u81f4\u7684\u66f4\u9ad8\u51c6\u786e\u7387\u3002"}}
{"id": "2511.13095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13095", "abs": "https://arxiv.org/abs/2511.13095", "authors": ["Chuyuan Li", "Giuseppe Carenini"], "title": "BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models", "comment": null, "summary": "We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.", "AI": {"tldr": "BeDiscovER\u662f\u4e00\u4e2a\u8bc4\u4f30\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u8bed\u7bc7\u7406\u89e3\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u542b5\u4e2a\u8bed\u7bc7\u4efb\u52a1\u300152\u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u8bcd\u6c47\u3001\u53e5\u5b50\u548c\u6587\u6863\u5c42\u9762\u7684\u8bed\u7bc7\u5206\u6790\u3002", "motivation": "\u968f\u7740\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u65b0\u7684\u3001\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u8bed\u7bc7\u5c42\u9762\u7684\u77e5\u8bc6\u7406\u89e3\u80fd\u529b\uff0c\u5305\u62ec\u4f20\u7edf\u4efb\u52a1\u548c\u65b0\u6311\u6218\u3002", "method": "\u6574\u5408\u4e865\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u8bed\u7bc7\u4efb\u52a1\uff0c\u6db5\u76d6\u8bed\u7bc7\u8bcd\u6c47\u3001(\u591a)\u53e5\u5b50\u548c\u6587\u6863\u5c42\u9762\uff0c\u5305\u62ec\u8bed\u7bc7\u89e3\u6790\u3001\u65f6\u5e8f\u5173\u7cfb\u63d0\u53d6\u3001\u8bed\u7bc7\u52a9\u8bcd\u6d88\u6b67\u7b49\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u65f6\u5e8f\u63a8\u7406\u7684\u7b97\u672f\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u5b8c\u6574\u6587\u6863\u63a8\u7406\u548c\u67d0\u4e9b\u7ec6\u5fae\u8bed\u4e49\u8bed\u7bc7\u73b0\u8c61\uff08\u5982\u4fee\u8f9e\u5173\u7cfb\u8bc6\u522b\uff09\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "BeDiscovER\u4e3a\u8bc4\u4f30\u73b0\u4ee3LLMs\u7684\u8bed\u7bc7\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u8bed\u7bc7\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.11703", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11703", "abs": "https://arxiv.org/abs/2511.11703", "authors": ["Hugo Huang"], "title": "Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom", "comment": "Master's Thesis at the University of Edinburgh (2024)", "summary": "Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u8f93\u5165\u8868\u793a\u65b9\u6cd5\uff08SS-only\u548cRGB+SS\uff09\uff0c\u5728ViZDoom\u6b7b\u4ea1\u7ade\u8d5b\u4e2d\u663e\u8457\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6027\u80fd", "motivation": "\u89e3\u51b33D\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u5185\u5b58\u7f13\u51b2\u533a\u7684\u9ad8\u5185\u5b58\u6d88\u8017\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u590d\u6742\u6027", "method": "\u4f7f\u7528\u8bed\u4e49\u5206\u5272\u6280\u672f\u521b\u5efa\u4e24\u79cd\u8f93\u5165\u8868\u793a\uff1aSS-only\uff08\u4ec5\u8bed\u4e49\u5206\u5272\uff09\u548cRGB+SS\uff08RGB+\u8bed\u4e49\u5206\u5272\uff09\uff0c\u5728ViZDoom\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u5bc6\u5ea6\u7684\u70ed\u56fe\u53ef\u89c6\u5316\u5206\u6790", "result": "SS-only\u65b9\u6cd5\u5c06\u5185\u5b58\u7f13\u51b2\u533a\u6d88\u8017\u51cf\u5c1166.6%-98.6%\uff0cRGB+SS\u663e\u8457\u63d0\u5347RL\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u70ed\u56fe\u5206\u6790\u6709\u6548\u8bc4\u4f30\u6570\u636e\u6536\u96c6\u9002\u7528\u6027", "conclusion": "\u8bed\u4e49\u5206\u5272\u8f93\u5165\u8868\u793a\u662f\u89e3\u51b33D\u73af\u5883RL\u5185\u5b58\u6d88\u8017\u548cPOMDP\u590d\u6742\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u5728ViZDoom\u7b49\u73af\u5883\u4e2d\u5e94\u7528\u8bed\u4e49\u5206\u5272\u7684\u5e38\u89c1\u7f3a\u9677"}}
{"id": "2511.13091", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13091", "abs": "https://arxiv.org/abs/2511.13091", "authors": ["Yuhan Chen", "Yuxuan Liu", "Long Zhang", "Pengzhi Gao", "Jian Luan", "Wei Liu"], "title": "STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization", "comment": null, "summary": "Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.", "AI": {"tldr": "STEP\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u4efb\u52a1\u6210\u529f\u7387\u52a8\u6001\u5206\u914d\u91c7\u6837\u548c\u8fdb\u884c\u6b65\u9aa4\u7ea7\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u591a\u8f6e\u4ea4\u4e92\u4e2d\u8f68\u8ff9\u7ea7\u4f18\u5316\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u591a\u8f6e\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u4f20\u7edf\u8f68\u8ff9\u7ea7\u4f18\u5316\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u4f1a\u4ea7\u751f\u8bef\u5bfc\u6027\u5b66\u4e60\u4fe1\u53f7\uff0c\u5305\u62ec\u5bf9\u4efb\u52a1\u96be\u5ea6\u4e0d\u654f\u611f\u7684\u5747\u5300\u91c7\u6837\u3001\u5728\u5931\u8d25\u8f68\u8ff9\u4e2d\u60e9\u7f5a\u6b63\u786e\u4e2d\u95f4\u52a8\u4f5c\u4ee5\u53ca\u9ad8\u91c7\u6837\u6210\u672c\u3002", "method": "\u63d0\u51faSTEP\u6846\u67b6\uff1a1\uff09\u7ef4\u62a4\u5e73\u6ed1\u7684\u6210\u529f\u7387\u8bb0\u5f55\u6765\u6307\u5bfc\u81ea\u9002\u5e94\u8f68\u8ff9\u91cd\u91c7\u6837\uff0c\u4e3a\u66f4\u96be\u4efb\u52a1\u5206\u914d\u66f4\u591a\u8d44\u6e90\uff1b2\uff09\u8ba1\u7b97\u6210\u529f\u7387\u52a0\u6743\u7684\u4f18\u52bf\u51fd\u6570\u5e76\u5c06\u8f68\u8ff9\u5206\u89e3\u4e3a\u6b65\u9aa4\u7ea7\u6837\u672c\uff1b3\uff09\u5e94\u7528\u6b65\u9aa4\u7ea7GRPO\u589e\u5f3a\u6765\u6539\u8fdb\u4f4e\u6210\u529f\u7387\u4efb\u52a1\u7684\u66f4\u65b0\u3002", "result": "\u5728OSWorld\u548cAndroidWorld\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTEP\u76f8\u6bd4\u8f68\u8ff9\u7ea7GRPO\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5728\u76f8\u540c\u91c7\u6837\u9884\u7b97\u4e0b\u6536\u655b\u66f4\u5feb\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u597d\u3002", "conclusion": "STEP\u901a\u8fc7\u6210\u529f\u7387\u611f\u77e5\u7684\u8f68\u8ff9\u9ad8\u6548\u7b56\u7565\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u91c7\u6837\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13107", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13107", "abs": "https://arxiv.org/abs/2511.13107", "authors": ["Zhichao He", "Mouxiao Bian", "Jianhong Zhu", "Jiayuan Chen", "Yunqiu Wang", "Wenxia Zhao", "Tianbin Li", "Bing Han", "Jie Xu", "Junyan Wu"], "title": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study", "comment": null, "summary": "The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc6\u522b\u5df2\u53d1\u8868RCT\u5bf9CONSORT 2010\u58f0\u660e\u4f9d\u4ece\u6027\u7684\u51c6\u786e\u6027\u3002\u7ed3\u679c\u8868\u660e\u6a21\u578b\u6574\u4f53\u8868\u73b0\u4e2d\u7b49\uff0c\u80fd\u8f83\u597d\u8bc6\u522b\u5408\u89c4\u9879\u76ee\uff0c\u4f46\u5728\u68c0\u6d4b\u4e0d\u5408\u89c4\u548c\u4e0d\u9002\u7528\u7684\u9879\u76ee\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u5c1a\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u8bd5\u9a8c\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u624b\u52a8\u9a8c\u8bc1CONSORT\u4f9d\u4ece\u6027\u662f\u4e00\u4e2a\u8017\u65f6\u8d39\u529b\u7684\u8fc7\u7a0b\uff0c\u6784\u6210\u540c\u884c\u8bc4\u5ba1\u548c\u8bc1\u636e\u5408\u6210\u7684\u91cd\u8981\u74f6\u9888\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u81ea\u52a8\u8bc6\u522bRCT\u5bf9CONSORT\u58f0\u660e\u4f9d\u4ece\u6027\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b150\u7bc7\u5df2\u53d1\u8868RCT\u7684\u91d1\u6807\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u533b\u5b66\u4e13\u4e1a\u3002\u4e3b\u8981\u7ed3\u679c\u662f\u4e09\u7c7b\u5206\u7c7b\u4efb\u52a1\u7684\u5b8f\u5e73\u5747F1\u5206\u6570\uff0c\u8f85\u4ee5\u9879\u76ee\u7ea7\u6027\u80fd\u6307\u6807\u548c\u5b9a\u6027\u9519\u8bef\u5206\u6790\u3002", "result": "\u6574\u4f53\u6a21\u578b\u8868\u73b0\u4e2d\u7b49\u3002\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578bGemini-2.5-Flash\u548cDeepSeek-R1\u7684\u5b8fF1\u5206\u6570\u5206\u522b\u4e3a0.634\uff0cCohen's Kappa\u7cfb\u6570\u5206\u522b\u4e3a0.280\u548c0.282\uff0c\u4ec5\u4e0e\u4e13\u5bb6\u5171\u8bc6\u8fbe\u6210\u4e00\u822c\u4e00\u81f4\u6027\u3002\u6a21\u578b\u5728\u8bc6\u522b\u5408\u89c4\u9879\u76ee\u65f6\u51c6\u786e\u7387\u8f83\u9ad8\uff08F1\u5206\u6570>0.850\uff09\uff0c\u4f46\u5728\u8bc6\u522b\u4e0d\u5408\u89c4\u548c\u4e0d\u9002\u7528\u9879\u76ee\u65f6\u8868\u73b0\u8f83\u5dee\uff08F1\u5206\u6570\u5f88\u5c11\u8d85\u8fc70.400\uff09\u3002", "conclusion": "LLMs\u4f5c\u4e3aCONSORT\u68c0\u67e5\u7684\u521d\u6b65\u7b5b\u67e5\u52a9\u624b\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u62a5\u544a\u826f\u597d\u7684\u9879\u76ee\u3002\u7136\u800c\uff0c\u5b83\u4eec\u76ee\u524d\u65e0\u6cd5\u53ef\u9760\u5730\u68c0\u6d4b\u62a5\u544a\u9057\u6f0f\u6216\u65b9\u6cd5\u5b66\u7f3a\u9677\uff0c\u56e0\u6b64\u4e0d\u9002\u5408\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u8bd5\u9a8c\u8d28\u91cf\u7684\u5173\u952e\u8bc4\u4f30\u3002"}}
{"id": "2511.11704", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11704", "abs": "https://arxiv.org/abs/2511.11704", "authors": ["Matvey Skripkin", "Elizaveta Goncharova", "Andrey Kuznetsov"], "title": "Simple Vision-Language Math Reasoning via Rendered Text", "comment": null, "summary": "We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u5c06LaTeX\u7f16\u7801\u7684\u6570\u5b66\u516c\u5f0f\u6e32\u67d3\u6210\u56fe\u50cf\uff0c\u5e76\u4e0e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\u914d\u5bf9\uff0c\u6765\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5\u5f80\u5f80\u590d\u6742\u4e14\u8ba1\u7b97\u91cf\u5927\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u6765\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u6027\u3002", "method": "\u5c06LaTeX\u516c\u5f0f\u6e32\u67d3\u4e3a\u56fe\u50cf\uff0c\u4e0e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\u914d\u5bf9\uff0c\u4f7f\u7528\u6587\u672c\u5230\u89c6\u89c9\u7684\u589e\u5f3a\u65b9\u6cd5\u6765\u8bad\u7ec3\u7d27\u51d1\u7684\u591a\u6a21\u6001\u67b6\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u5f00\u6e90\u548c\u4e13\u6709\u7684\u6570\u5b66\u89c6\u89c9\u8bed\u8a00\u6c42\u89e3\u5668\uff0c\u5728MMMU\u3001ChartQA\u548cDocVQA\u7b49\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe20%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e7f\u6cdb\u7684\u901a\u7528\u9886\u57df\u80fd\u529b\u3002", "conclusion": "\u6e32\u67d3\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u8bbe\u8ba1\u662f\u6027\u80fd\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u8fd9\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2511.13131", "categories": ["cs.AI", "cs.CV", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.13131", "abs": "https://arxiv.org/abs/2511.13131", "authors": ["Gagan Raj Gupta", "Anshul Kumar", "Manish Rai", "Apu Chakraborty", "Ashutosh Modi", "Abdelaali Chaoub", "Soumajit Pramanik", "Moyank Giri", "Yashwanth Holla", "Sunny Kumar", "M. V. Kiran Sooraj"], "title": "MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.", "AI": {"tldr": "MM-Telco\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u7535\u4fe1\u9886\u57df\u5b9a\u5236\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u548c\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u5728\u7535\u4fe1\u5e94\u7528\u4e2d\u7684\u9886\u57df\u7279\u5b9a\u6311\u6218\uff0c\u63d0\u5347\u7f51\u7edc\u8fd0\u8425\u3001\u6587\u6863\u8d28\u91cf\u7b49\u5b9e\u9645\u7528\u4f8b\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7535\u4fe1\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u9886\u57df\u7279\u5b9a\u7684\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u9002\u914d\u624d\u80fd\u6709\u6548\u5e94\u7528\u4e8e\u7f51\u7edc\u4f18\u5316\u3001\u6545\u969c\u6392\u9664\u3001\u5ba2\u6237\u652f\u6301\u548c\u6cd5\u89c4\u9075\u4ece\u7b49\u4efb\u52a1\u3002", "method": "\u63d0\u51faMM-Telco\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b\u57fa\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u7684\u5404\u79cd\u4efb\u52a1\uff0c\u6db5\u76d6\u7f51\u7edc\u8fd0\u8425\u3001\u7f51\u7edc\u7ba1\u7406\u3001\u6587\u6863\u8d28\u91cf\u6539\u8fdb\u7b49\u5b9e\u9645\u7528\u4f8b\uff0c\u5e76\u5bf9\u5404\u79cdLLMs\u548cVLMs\u8fdb\u884c\u57fa\u7ebf\u5b9e\u9a8c\u3002", "result": "\u5728\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5b9e\u9a8c\u8fd8\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u591a\u6a21\u6001LLMs\u7684\u8584\u5f31\u73af\u8282\u3002", "conclusion": "MM-Telco\u52a0\u901f\u4e86LLMs\u5728\u7535\u4fe1\u9886\u57df\u7684\u9002\u914d\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u5411\u3002"}}
{"id": "2511.13118", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13118", "abs": "https://arxiv.org/abs/2511.13118", "authors": ["Quanjiang Guo", "Sijie Wang", "Jinchuan Zhang", "Ben Zhang", "Zhao Kang", "Ling Tian", "Ke Yan"], "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction", "comment": "11 pages, 5 figures, accepted by AAAI 2026 (Oral)", "summary": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.", "AI": {"tldr": "\u63d0\u51faAgent-Event-Coder (AEC)\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u68c0\u7d22\u3001\u89c4\u5212\u3001\u7f16\u7801\u548c\u9a8c\u8bc1\u56db\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u7cbe\u786e\u3001\u5b8c\u6574\u4e14\u7b26\u5408\u6a21\u5f0f\u7684\u4e8b\u4ef6\u62bd\u53d6\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u4e8b\u4ef6\u62bd\u53d6\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u63a8\u7406\u9700\u6c42\u3001\u9886\u57df\u7279\u5b9a\u7406\u89e3\u56f0\u96be\uff0c\u4ee5\u53ca\u76f4\u63a5\u63d0\u793a\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u8f93\u51fa\u3001\u7ed3\u6784\u65e0\u6548\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u62bd\u53d6\u5206\u89e3\u4e3a\u56db\u4e2a\u4e13\u95e8\u5b50\u4efb\u52a1\uff1a\u68c0\u7d22\u3001\u89c4\u5212\u3001\u7f16\u7801\u548c\u9a8c\u8bc1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u7531\u4e13\u7528LLM\u667a\u80fd\u4f53\u5904\u7406\uff0c\u4e8b\u4ef6\u6a21\u5f0f\u8868\u793a\u4e3a\u53ef\u6267\u884c\u7684\u7c7b\u5b9a\u4e49\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\u548c\u516d\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAEC\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u7684\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "AEC\u901a\u8fc7\u7f16\u7a0b\u542f\u53d1\u7684\u65b9\u6cd5\u548c\u534f\u4f5c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u4f7fLLM\u80fd\u591f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4ea7\u751f\u7cbe\u786e\u3001\u5b8c\u6574\u4e14\u6a21\u5f0f\u4e00\u81f4\u7684\u4e8b\u4ef6\u62bd\u53d6\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u7684\u5f3a\u5927\u80fd\u529b\u3002"}}
{"id": "2511.11705", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11705", "abs": "https://arxiv.org/abs/2511.11705", "authors": ["Arya Narang"], "title": "Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs", "comment": null, "summary": "This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u83dc\u54c1\u540d\u79f0\u7684\u77ed\u6587\u672c\u8f93\u5165\u8f85\u52a9\u4e0b\uff0c\u591a\u6a21\u6001\u6a21\u578b\u76f8\u6bd4\u4ec5\u4f7f\u7528\u56fe\u50cf\u7684\u57fa\u7ebf\u6a21\u578b\u5728\u5361\u8def\u91cc\u4f30\u7b97\u65b9\u9762\u7684\u6539\u8fdb\u7a0b\u5ea6\u53ca\u5176\u7edf\u8ba1\u663e\u8457\u6027\u3002", "motivation": "\u65e8\u5728\u9a8c\u8bc1\u77ed\u6587\u672c\u4fe1\u606f\uff08\u5982\u83dc\u54c1\u540d\u79f0\uff09\u662f\u5426\u80fd\u663e\u8457\u63d0\u5347\u5361\u8def\u91cc\u4f30\u7b97\u7684\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u4ec5\u4f9d\u8d56\u56fe\u50cf\u4fe1\u606f\u7684\u6a21\u578b\u3002", "method": "\u4f7f\u7528TensorFlow\u5e93\u548cGoogle Nutrition5k\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86\u4ec5\u4f7f\u7528\u56fe\u50cf\u7684CNN\u6a21\u578b\u548c\u540c\u65f6\u63a5\u53d7\u6587\u672c\u4e0e\u56fe\u50cf\u8f93\u5165\u7684\u591a\u6a21\u6001CNN\u6a21\u578b\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u5c06\u5361\u8def\u91cc\u4f30\u7b97\u7684MAE\u4ece84.76\u5343\u5361\u964d\u4f4e\u81f383.70\u5343\u5361\uff0c\u51cf\u5c11\u4e861.06\u5343\u5361\uff08\u6539\u8fdb1.25%\uff09\u3002", "conclusion": "\u77ed\u6587\u672c\u8f93\u5165\u80fd\u591f\u8f7b\u5fae\u4f46\u663e\u8457\u5730\u6539\u5584\u5361\u8def\u91cc\u4f30\u7b97\u7684\u51c6\u786e\u6027\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u76f8\u6bd4\u7eaf\u56fe\u50cf\u65b9\u6cd5\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2511.13137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13137", "abs": "https://arxiv.org/abs/2511.13137", "authors": ["Yanda Zhu", "Yuanyang Zhu", "Daoyi Dong", "Caihua Chen", "Chunlin Chen"], "title": "Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition", "comment": "AAAI 2026", "summary": "Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\\text{D}^\\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\\text{D}^\\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\\text{D}^\\text{3}$T achieves better performance than existing baselines.", "AI": {"tldr": "C$\text{D}^\text{3}$T\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4e24\u5c42\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u4efb\u52a1\u5206\u89e3\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u89c2\u5bdf\u548c\u5956\u52b1\u6765\u5b66\u4e60\u5b50\u4efb\u52a1\u8868\u793a\uff0c\u5e76\u5728\u4f4e\u5c42\u8fdb\u884c\u534f\u4f5c\u6280\u80fd\u5b66\u4e60\u3002", "motivation": "\u5728\u590d\u6742\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u80fd\u591f\u5b9e\u73b0\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u9ad8\u6548\u5206\u5c42\u5b66\u4e60\uff0c\u4f46\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6837\u672c\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u63a2\u7d22\u5927\u7684\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u3002", "method": "\u63d0\u51faC$\text{D}^\text{3}$T\u6846\u67b6\uff1a\u9ad8\u5c42\u7b56\u7565\u57fa\u4e8e\u5b50\u4efb\u52a1\u6548\u679c\u5b66\u4e60\u5b50\u4efb\u52a1\u8868\u793a\u6765\u751f\u6210\u5b50\u4efb\u52a1\u9009\u62e9\u7b56\u7565\uff1b\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u9884\u6d4b\u4e0b\u4e00\u89c2\u5bdf\u548c\u5956\u52b1\u6765\u6355\u6349\u5b50\u4efb\u52a1\u5bf9\u73af\u5883\u7684\u5f71\u54cd\uff1b\u4f4e\u5c42\u667a\u80fd\u4f53\u5728\u5206\u914d\u7684\u5b50\u4efb\u52a1\u5185\u534f\u4f5c\u5b66\u4e60\u548c\u5171\u4eab\u4e13\u95e8\u6280\u80fd\uff1b\u5b66\u4e60\u7684\u5b50\u4efb\u52a1\u8868\u793a\u4f5c\u4e3a\u989d\u5916\u8bed\u4e49\u4fe1\u606f\u7528\u4e8e\u591a\u5934\u6ce8\u610f\u529b\u6df7\u5408\u7f51\u7edc\u4ee5\u589e\u5f3a\u4ef7\u503c\u5206\u89e3\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cC$\text{D}^\text{3}$T\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "C$\text{D}^\text{3}$T\u901a\u8fc7\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u5206\u89e3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u5206\u89e3\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.13126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13126", "abs": "https://arxiv.org/abs/2511.13126", "authors": ["Nigar Alishzade", "Gulchin Abdullayeva"], "title": "A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition", "comment": null, "summary": "This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.", "AI": {"tldr": "\u6bd4\u8f83\u4e86ConvLSTM\u548cVanilla Transformer\u5728\u624b\u8bed\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0Transformer\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8eConvLSTM\uff0c\u800cConvLSTM\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u66f4\u6709\u4f18\u52bf\u3002", "motivation": "\u7cfb\u7edf\u6bd4\u8f83\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\u7684\u67b6\u6784\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5728\u963f\u585e\u62dc\u7586\u624b\u8bed\u6570\u636e\u96c6\uff08AzSLD\uff09\u548c\u5355\u8bcd\u7ea7\u7f8e\u56fd\u624b\u8bed\u6570\u636e\u96c6\uff08WLASL\uff09\u4e0a\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86ConvLSTM\u548cVanilla Transformer\u4e24\u79cd\u4ee3\u8868\u6027\u6a21\u578b\u3002", "result": "Vanilla Transformer\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u7684Top-1\u548cTop-5\u51c6\u786e\u7387\u4e0a\u90fd\u4f18\u4e8eConvLSTM\uff0c\u5728AzSLD\u4e0a\u8fbe\u523076.8% Top-1\u51c6\u786e\u7387\uff0c\u5728WLASL\u4e0a\u8fbe\u523088.3%\u3002ConvLSTM\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u4f46\u5728\u51c6\u786e\u7387\u4e0a\u843d\u540e\u3002", "conclusion": "Transformer\u5728\u6574\u4f53\u51c6\u786e\u7387\u548c\u8bf4\u8bdd\u8005\u72ec\u7acb\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u800cConvLSTM\u5728\u8ba1\u7b97\u6548\u7387\u548c\u65f6\u5e8f\u5efa\u6a21\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u5e94\u6839\u636e\u5e94\u7528\u9700\u6c42\u548c\u8d44\u6e90\u7ea6\u675f\u9009\u62e9\u5408\u9002\u7684\u67b6\u6784\u3002"}}
{"id": "2511.11706", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11706", "abs": "https://arxiv.org/abs/2511.11706", "authors": ["Julia Peters", "Karin Mora", "Miguel D. Mahecha", "Chaonan Ji", "David Montero", "Clemens Mosig", "Guido Kraemer"], "title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling", "comment": "10 pages (incliding 2 pages of references), 7 figures", "summary": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5730\u7403\u89c2\u6d4b\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u4e0d\u540c\u4f20\u611f\u5668\u6570\u636e\u5230\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u7edf\u4e00\u7279\u5f81\u7a7a\u95f4\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u56fa\u5b9a\u5c3a\u5ea6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\u901a\u5e38\u53ea\u80fd\u5728\u56fa\u5b9a\u7a7a\u95f4\u6216\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fd0\u884c\uff0c\u9650\u5236\u4e86\u9700\u8981\u7cbe\u7ec6\u7a7a\u95f4\u7ec6\u8282\u548c\u9ad8\u65f6\u95f4\u4fdd\u771f\u5ea6\u7684\u751f\u6001\u5206\u6790\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff1a\u9996\u5148\u72ec\u7acb\u5efa\u6a21\u6bcf\u4e2a\u4f20\u611f\u5668\u4ee5\u6355\u6349\u5176\u7279\u5b9a\u7279\u5f81\uff0c\u7136\u540e\u5c06\u8868\u793a\u7ec4\u5408\u5230\u5171\u4eab\u6a21\u578b\u4e2d\u3002\u4f7f\u7528Sentinel-1\u548cSentinel-2\u6570\u636e\u4f5c\u4e3a\u4ee3\u8868\u6027\u6a21\u6001\uff0c\u572810\u7c73\u5206\u8fa8\u7387\u548c\u65e0\u4e91Sentinel-2\u91c7\u96c6\u9891\u7387\u4e0b\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5b9a\u6027\u5206\u6790\u663e\u793a\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u5728\u5f02\u8d28\u666f\u89c2\u4e2d\u8868\u73b0\u51fa\u9ad8\u7a7a\u95f4\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u5728\u603b\u521d\u7ea7\u751f\u4ea7\u529b\u5efa\u6a21\u7684\u5b9a\u91cf\u8bc4\u4f30\u4e2d\uff0c\u5b83\u4eec\u7f16\u7801\u4e86\u751f\u6001\u610f\u4e49\u6a21\u5f0f\u5e76\u4fdd\u6301\u4e86\u8db3\u591f\u7684\u65f6\u95f4\u4fdd\u771f\u5ea6\u4ee5\u652f\u6301\u7cbe\u7ec6\u5c3a\u5ea6\u5206\u6790\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9700\u8981\u4e0d\u540c\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u73af\u5883\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u5206\u6790\u5c31\u7eea\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u6355\u6349\u4e92\u8865\u7684\u9065\u611f\u6570\u636e\u5e76\u4fdd\u6301\u8de8\u65f6\u7a7a\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.13160", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13160", "abs": "https://arxiv.org/abs/2511.13160", "authors": ["TC Singh", "Sougata Mukherjea"], "title": "InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions", "comment": null, "summary": "Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque \"black boxes\". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a \"what-if\" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86InteractiveGNNExplainer\uff0c\u4e00\u4e2a\u7528\u4e8e\u589e\u5f3a\u56fe\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u7684\u53ef\u89c6\u5316\u5206\u6790\u6846\u67b6\uff0c\u7279\u522b\u5173\u6ce8\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u57fa\u4e8e\u56fe\u7684\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u590d\u6742\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\u4f7f\u5176\u6210\u4e3a\u4e0d\u900f\u660e\u7684\"\u9ed1\u76d2\"\uff0c\u8fd9\u963b\u788d\u4e86\u7528\u6237\u4fe1\u4efb\u3001\u8c03\u8bd5\u3001\u504f\u89c1\u68c0\u6d4b\u4ee5\u53ca\u5728\u9700\u8981\u53ef\u89e3\u91ca\u6027\u7684\u5173\u952e\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7cfb\u7edf\u72ec\u7279\u5730\u6574\u5408\u4e86\u534f\u8c03\u7684\u4ea4\u4e92\u89c6\u56fe\uff08\u52a8\u6001\u56fe\u5e03\u5c40\u3001\u5d4c\u5165\u6295\u5f71\u3001\u7279\u5f81\u68c0\u67e5\u3001\u90bb\u57df\u5206\u6790\uff09\u4e0e\u65e2\u6709\u7684\u540e\u9a8c\u89e3\u91ca\uff08GNNExplainer\uff09\u548c\u5185\u5728\u89e3\u91ca\uff08GAT\u6ce8\u610f\u529b\uff09\u6280\u672f\uff0c\u5e76\u5f15\u5165\u4e86\u4ea4\u4e92\u5f0f\u56fe\u7f16\u8f91\u529f\u80fd\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6270\u52a8\u56fe\u7ed3\u6784\u6765\u6267\u884c\"\u5047\u8bbe\u5206\u6790\"\u3002", "result": "\u901a\u8fc7\u5728Cora\u548cCiteSeer\u6570\u636e\u96c6\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86InteractiveGNNExplainer\u5982\u4f55\u4fc3\u8fdb\u6df1\u5165\u7684\u8bef\u5206\u7c7b\u8bca\u65ad\u3001GCN\u4e0eGAT\u884c\u4e3a\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u4ee5\u53ca\u5bf9\u6a21\u578b\u654f\u611f\u6027\u7684\u4e25\u683c\u63a2\u6d4b\u3002", "conclusion": "\u8fd9\u4e9b\u80fd\u529b\u4fc3\u8fdb\u4e86\u5bf9GNN\u9884\u6d4b\u66f4\u6df1\u5c42\u6b21\u3001\u591a\u65b9\u9762\u7684\u7406\u89e3\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u900f\u660e\u3001\u53ef\u4fe1\u548c\u9c81\u68d2\u7684\u56fe\u5206\u6790\u3002"}}
{"id": "2511.13152", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13152", "abs": "https://arxiv.org/abs/2511.13152", "authors": ["Sourya Dipta Das", "Shubham Kumar", "Kuldeep Yadav"], "title": "Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels", "comment": "Accepted in AACL-IJCNLP 2025", "summary": "Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96f6\u6837\u672c\u8bed\u6cd5\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u566a\u58f0\u6807\u7b7e\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u4f30\u8ba1\u8bed\u6cd5\u80fd\u529b\u5206\u6570", "motivation": "\u53e3\u8bed\u8bed\u6cd5\u8bc4\u4f30\u9762\u4e34\u81ea\u53d1\u6027\u548c\u4e0d\u6d41\u7545\u6027\u6311\u6218\uff0c\u4e14\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6807\u6ce8\uff0c\u5927\u89c4\u6a21\u6570\u636e\u521b\u5efa\u4e0d\u5207\u5b9e\u9645", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bed\u6cd5\u80fd\u529b\u91cf\u8868\u7684\u63d0\u793a\u8bcd\u8ba9LLM\u5728\u672a\u6807\u8bb0\u6570\u636e\u4e0a\u751f\u6210\u9884\u6d4b\u4f5c\u4e3a\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bad\u7ec3\u6846\u67b6\u8bad\u7ec3\u57fa\u4e8etransformer\u7684\u6a21\u578b\u6765\u5904\u7406\u6807\u7b7e\u566a\u58f0", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u8bed\u6cd5\u80fd\u529b\u5206\u6570\uff0cLLM\u9009\u62e9\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u8bad\u7ec3\u4e2d\u5e72\u51c0\u6837\u672c\u4e0e\u566a\u58f0\u6837\u672c\u7684\u6bd4\u4f8b\u5f71\u54cd\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u3001\u4f4e\u8d44\u6e90\u7684\u8bed\u6cd5\u8bc4\u4f30\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2511.11707", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11707", "abs": "https://arxiv.org/abs/2511.11707", "authors": ["Mohamed El Gorrim"], "title": "FSC-Net: Fast-Slow Consolidation Networks for Continual Learning", "comment": "Code and the full repo available at https://github.com/MedGm/FSC-Net", "summary": "Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.", "AI": {"tldr": "FSC-Net\u63d0\u51fa\u53cc\u7f51\u7edc\u67b6\u6784\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u5feb\u901f\u7f51\u7edc\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u6162\u901f\u7f51\u7edc\u8fdb\u884c\u77e5\u8bc6\u5de9\u56fa\uff0c\u5728Split-MNIST\u4e0a\u53d6\u5f9791.71%\u7684\u4fdd\u7559\u51c6\u786e\u7387\uff0c\u6bd4\u5355\u7f51\u7edc\u63d0\u53474.27\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u8bb0\u5fc6\u5de9\u56fa\u673a\u5236\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9047\u5230\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u4f1a\u5fd8\u8bb0\u5148\u524d\u5b66\u5230\u7684\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u53cc\u7f51\u7edc\u67b6\u6784\uff1a\u5feb\u901f\u7f51\u7edc(NN1)\u8d1f\u8d23\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u6162\u901f\u7f51\u7edc(NN2)\u901a\u8fc7\u84b8\u998f\u548c\u91cd\u653e\u8fdb\u884c\u77e5\u8bc6\u5de9\u56fa\u3002\u7814\u7a76\u53d1\u73b0\u7eaf\u91cd\u653e\u7b56\u7565\u4f18\u4e8e\u84b8\u998f\u65b9\u6cd5\u3002", "result": "\u5728Split-MNIST\u4e0a\u8fbe\u523091.71%\u00b10.62%\u7684\u4fdd\u7559\u51c6\u786e\u7387\uff0c\u6bd4\u5355\u7f51\u7edc\u63d0\u53474.27pp\uff1b\u5728Split-CIFAR-10\u4e0a\u8fbe\u523033.31%\u00b10.38%\uff0c\u63d0\u53478.20pp\uff0c\u4f46\u7edd\u5bf9\u6027\u80fd\u4ecd\u4f4e\u4e8e\u968f\u673a\u671f\u671b\u3002", "conclusion": "\u53cc\u65f6\u95f4\u5c3a\u5ea6\u5de9\u56fa\u673a\u5236\u800c\u975e\u67b6\u6784\u590d\u6742\u6027\u662f\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u7684\u5173\u952e\u56e0\u7d20\uff0c\u7b80\u5355MLP\u67b6\u6784\u4f18\u4e8e\u590d\u6742\u53d8\u4f53\uff0c\u7eaf\u91cd\u653e\u7b56\u7565\u6548\u679c\u6700\u4f73\u3002"}}
{"id": "2511.13193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13193", "abs": "https://arxiv.org/abs/2511.13193", "authors": ["Yijia Fan", "Jusheng Zhang", "Kaitong Cai", "Jing Yang", "Chengpei Tang", "Jian Wang", "Keze Wang"], "title": "Cost-Effective Communication: An Auction-based Method for Language Agent Interaction", "comment": null, "summary": "Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient \"free-for-all\" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that \"free\" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.", "AI": {"tldr": "DALA\u6846\u67b6\u901a\u8fc7\u5c06\u901a\u4fe1\u5e26\u5bbd\u89c6\u4e3a\u7a00\u7f3a\u53ef\u4ea4\u6613\u8d44\u6e90\uff0c\u91c7\u7528\u62cd\u5356\u673a\u5236\u8ba9\u667a\u80fd\u4f53\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c\u5bc6\u5ea6\u7ade\u6807\u53d1\u8a00\u6743\uff0c\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u901a\u4fe1\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d'\u81ea\u7531\u653e\u4efb'\u901a\u4fe1\u5bfc\u81f4\u7684\u6307\u6570\u7ea7token\u6210\u672c\u548c\u9ad8\u566a\u58f0\u4f4e\u4fe1\u53f7\u95ee\u9898\uff0c\u6311\u6218'\u66f4\u591a\u901a\u4fe1\u603b\u662f\u66f4\u597d'\u7684\u89c2\u5ff5\uff0c\u5f15\u5165\u8d44\u6e90\u7406\u6027\u539f\u5219\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u62cd\u5356\u8bed\u8a00\u667a\u80fd\u4f53(DALA)\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u5efa\u6a21\u4e3a\u4e2d\u5fc3\u5316\u62cd\u5356\uff0c\u667a\u80fd\u4f53\u5b66\u4e60\u57fa\u4e8e\u6d88\u606f\u4ef7\u503c\u5bc6\u5ea6\u9884\u6d4b\u6765\u7ade\u6807\u53d1\u8a00\u673a\u4f1a\u3002", "result": "\u57287\u4e2a\u6311\u6218\u6027\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1aMMLU 84.32%\uff0cHumanEval 91.21% pass@1\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528625\u4e07token\uff0c\u8fdc\u5c11\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728GSM8K\u4e0a\u7684\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "DALA\u901a\u8fc7\u8d44\u6e90\u7ea6\u675f\u57f9\u517b\u4e86\u6218\u7565\u6027\u6c89\u9ed8\u7684\u65b0\u5174\u6280\u80fd\uff0c\u80fd\u591f\u52a8\u6001\u8c03\u6574\u4ece\u5197\u957f\u5230\u6c89\u9ed8\u7684\u901a\u4fe1\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u7ecf\u6d4e\u9a71\u52a8\u65b9\u6cd5\u5728\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13159", "abs": "https://arxiv.org/abs/2511.13159", "authors": ["Zaara Zabeen Arpa", "Sadnam Sakib Apurbo", "Nazia Karim Khan Oishee", "Ajwad Abrar"], "title": "Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis", "comment": null, "summary": "Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bed\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u533a\u5206ASR\u8f6c\u5f55\u4e2d\u7684\u91cd\u590d\u6027\u4e0d\u6d41\u5229\u548c\u5f62\u6001\u5b66\u91cd\u53e0\u73b0\u8c61\uff0c\u901a\u8fc7LLM\u548c\u5fae\u8c03\u65b9\u6cd5\u5efa\u7acb\u4e86\u5f3a\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bedASR\u8f6c\u5f55\u4e2d\u8bcd\u91cd\u590d\u7684\u6b67\u4e49\u95ee\u9898\uff1a\u533a\u5206\u662f\u91cd\u590d\u6027\u4e0d\u6d41\u5229\uff08ASR\u9519\u8bef/\u72b9\u8c6b\uff09\u8fd8\u662f\u5f62\u6001\u5b66\u91cd\u53e0\uff08\u8bed\u6cd5\u7ed3\u6784\uff09\uff0c\u907f\u514d\u6807\u51c6\u4e0d\u6d41\u5229\u6821\u6b63\u9519\u8bef\u5220\u9664\u6709\u6548\u8bed\u8a00\u4fe1\u606f\u3002", "method": "\u521b\u5efa\u4e86\u9996\u4e2a\u516c\u5f00\u768420,000\u884c\u624b\u52a8\u6807\u6ce8\u5b5f\u52a0\u62c9\u8bed\u8bed\u6599\u5e93\uff0c\u4f7f\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u63d0\u793a\uff1b2\uff09\u7f16\u7801\u5668\u6a21\u578b\u7684\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002", "result": "LLM\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u8fbe\u523082.68%\u51c6\u786e\u7387\uff0c\u4f46\u5fae\u8c03\u8868\u73b0\u66f4\u4f18\uff0cBanglaBERT\u6a21\u578b\u8fbe\u523084.78%\u51c6\u786e\u7387\u548c0.677 F1\u5206\u6570\u3002", "conclusion": "\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u8bed\u8a00\u611f\u77e5\u57fa\u7ebf\uff0c\u4e3a\u5f00\u53d1\u4fdd\u7559\u8bed\u4e49\u7684\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u89c4\u8303\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u636e\u3002"}}
{"id": "2511.11711", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11711", "abs": "https://arxiv.org/abs/2511.11711", "authors": ["Tsogt-Ochir Enkhbayar"], "title": "Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control", "comment": "1 figure", "summary": "Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.", "AI": {"tldr": "\u5c06Model-X knockoffs\u65b9\u6cd5\u5f15\u5165\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u7279\u5f81\u9009\u62e9\uff0c\u901a\u8fc7knockoff+\u63a7\u5236\u9519\u8bef\u53d1\u73b0\u7387(FDR)\uff0c\u4e3a\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u969c\u3002", "motivation": "\u5f53\u524dSAE\u5728\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u7279\u5f81\u65f6\u96be\u4ee5\u533a\u5206\u771f\u5b9e\u8ba1\u7b97\u6a21\u5f0f\u548c\u9519\u8bef\u76f8\u5173\u6027\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Model-X knockoffs\u65b9\u6cd5\u7ed3\u5408\u9ad8\u65af\u66ff\u4ee3\u6a21\u578b\uff0c\u5728Pythia-70M\u6a21\u578b\u7684\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u5bf9512\u4e2a\u9ad8\u6d3b\u6027SAE\u6f5c\u5728\u53d8\u91cf\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u3002", "result": "\u5728\u76ee\u6807FDR q=0.1\u4e0b\u9009\u62e9\u4e86129\u4e2a\u7279\u5f81\uff0c\u7ea625%\u7684\u6f5c\u5728\u53d8\u91cf\u643a\u5e26\u4efb\u52a1\u76f8\u5173\u4fe1\u53f7\uff0c\u6240\u9009\u7279\u5f81\u4e0e\u975e\u9009\u7279\u5f81\u5728knockoff\u7edf\u8ba1\u91cf\u4e0a\u663e\u793a5.40\u500d\u5206\u79bb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06SAE\u4e0e\u591a\u91cd\u6d4b\u8bd5\u611f\u77e5\u63a8\u65ad\u76f8\u7ed3\u5408\uff0c\u4e3a\u53ef\u9760\u7279\u5f81\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u4e14\u539f\u5219\u6027\u7684\u6846\u67b6\uff0c\u63a8\u8fdb\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u57fa\u7840\u7814\u7a76\u3002"}}
{"id": "2511.13214", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13214", "abs": "https://arxiv.org/abs/2511.13214", "authors": ["Guillaume Infantes", "St\u00e9phanie Roussel", "Antoine Jacquet", "Emmanuel Benazera"], "title": "Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks", "comment": "Accepted at ICTAI 2025 Conference", "summary": "The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.", "AI": {"tldr": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u9879\u76ee\u8c03\u5ea6\u95ee\u9898\u4e2d\u7684\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\uff0c\u76ee\u6807\u662f\u751f\u6210\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u57fa\u51c6\u8c03\u5ea6\u65b9\u6848\u4ee5\u6700\u5c0f\u5316\u671f\u671b\u9879\u76ee\u5de5\u671f\u3002", "motivation": "\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e2d\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u6784\u5efa\u80fd\u591f\u5e94\u5bf9\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u7684\u5f39\u6027\u8c03\u5ea6\u65b9\u6848\uff0c\u751f\u6210\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u57fa\u51c6\u8c03\u5ea6\u3002", "method": "\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u4efb\u52a1\u8c03\u5ea6\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u7c7b\u4f3c\u4e8e\u4f18\u5148\u7ea7\u8c03\u5ea6\u89c4\u5219\uff0c\u5e76\u4e0e\u4e32\u884c\u8c03\u5ea6\u751f\u6210\u65b9\u6848\u914d\u5408\u751f\u6210\u8c03\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u5f00\u53d1\u4e86\u540d\u4e3aWheatley\u7684\u516c\u5f00\u6846\u67b6\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2511.13169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13169", "abs": "https://arxiv.org/abs/2511.13169", "authors": ["Tianai Huang", "Jiayuan Chen", "Lu Lu", "Pengcheng Chen", "Tianbin Li", "Bing Han", "Wenchao Tang", "Jie Xu", "Ming Li"], "title": "TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine", "comment": "17 pages, 8 figures", "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\\_r1 and gemini\\_2\\_5\\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the \"In-depth Challenge for Comprehensive TCM Abilities\" special track.", "AI": {"tldr": "TCM-5CEval\u662f\u4e00\u4e2a\u9488\u5bf9\u4e2d\u533b\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b\u4e94\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u6838\u5fc3\u77e5\u8bc6\u3001\u7ecf\u5178\u6587\u732e\u3001\u4e34\u5e8a\u51b3\u7b56\u3001\u4e2d\u836f\u5b66\u548c\u4e34\u5e8a\u975e\u836f\u7269\u6cbb\u7597\u3002\u8bc4\u4f30\u53d1\u73b0\u6a21\u578b\u5728\u57fa\u7840\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ecf\u5178\u6587\u672c\u89e3\u91ca\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5f31\u70b9\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e2d\u533b\u7b49\u9ad8\u5ea6\u4e13\u4e1a\u5316\u548c\u6587\u5316\u4e30\u5bcc\u7684\u9886\u57df\u4e2d\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u3002\u57fa\u4e8e\u4e4b\u524dTCM-3CEval\u7684\u5de5\u4f5c\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u548c\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u63ed\u793a\u6a21\u578b\u5728\u4e2d\u533b\u9886\u57df\u7684\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86TCM-5CEval\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b\u4e94\u4e2a\u7ef4\u5ea6\uff1aTCM-Exam\uff08\u6838\u5fc3\u77e5\u8bc6\uff09\u3001TCM-LitQA\uff08\u7ecf\u5178\u6587\u732e\uff09\u3001TCM-MRCD\uff08\u4e34\u5e8a\u51b3\u7b56\uff09\u3001TCM-CMM\uff08\u4e2d\u836f\u5b66\uff09\u3001TCM-ClinNPT\uff08\u4e34\u5e8a\u975e\u836f\u7269\u6cbb\u7597\uff09\u3002\u5bf915\u4e2a\u4e3b\u6d41LLM\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5305\u62ec\u57fa\u4e8e\u6392\u5217\u7684\u4e00\u81f4\u6027\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0cdeepseek_r1\u548cgemini_2_5_pro\u8868\u73b0\u6700\u4f73\u3002\u6a21\u578b\u5728\u57fa\u7840\u77e5\u8bc6\u56de\u5fc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ecf\u5178\u6587\u672c\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u6392\u5217\u4e00\u81f4\u6027\u6d4b\u8bd5\u663e\u793a\u6240\u6709\u6a21\u578b\u90fd\u5b58\u5728\u63a8\u7406\u8106\u5f31\u6027\uff0c\u5bf9\u95ee\u9898\u9009\u9879\u987a\u5e8f\u654f\u611f\uff0c\u8868\u660e\u7f3a\u4e4f\u7a33\u5065\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "TCM-5CEval\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u66f4\u8be6\u7ec6\u7684\u4e2d\u533b\u80fd\u529b\u8bca\u65ad\u5de5\u5177\uff0c\u8fd8\u66b4\u9732\u4e86LLM\u5728\u63a8\u7406\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u6839\u672c\u5f31\u70b9\u3002\u8be5\u57fa\u51c6\u5df2\u4e0a\u4f20\u81f3Medbench\u5e73\u53f0\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6807\u51c6\u5316\u6bd4\u8f83\u3002"}}
{"id": "2511.11712", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11712", "abs": "https://arxiv.org/abs/2511.11712", "authors": ["Zixi Li"], "title": "Reasoning: From Reflection to Solution", "comment": null, "summary": "What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\\% accuracy) and HumanEval (90\\% pass@1), we must ask: have these systems learned to \\emph{reason}, or have they learned to \\emph{pattern-match over reasoning traces}?\n  This paper argues for a specific answer: \\textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.\n  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\\% accuracy where state-of-the-art LLMs achieve 0\\%. This is not about criticizing existing systems, but about \\emph{understanding what reasoning requires} and \\emph{building architectures that provide it}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u63a8\u7406\u662f\u72b6\u6001\u7a7a\u95f4\u4e2d\u8fed\u4ee3\u64cd\u4f5c\u7b26\u5e94\u7528\u5e76\u6536\u655b\u5230\u56fa\u5b9a\u70b9\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7OpenLM\u67b6\u6784\u5728OpenXOR\u95ee\u9898\u4e0a\u5b9e\u73b076%\u51c6\u786e\u7387\uff0c\u800c\u73b0\u6709LLMs\u4e3a0%\u3002", "motivation": "\u5728LLMs\u5728GSM8K\u7b49\u57fa\u51c6\u4e0a\u53d6\u5f97\u8d85\u4eba\u8868\u73b0\u7684\u65f6\u4ee3\uff0c\u9700\u8981\u533a\u5206\u7cfb\u7edf\u662f\u5b66\u4f1a\u4e86\u63a8\u7406\u8fd8\u662f\u4ec5\u4ec5\u5728\u63a8\u7406\u8f68\u8ff9\u4e0a\u8fdb\u884c\u6a21\u5f0f\u5339\u914d\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u4f5c\u4e3a\u72b6\u6001\u7a7a\u95f4\u4e2d\u8fed\u4ee3\u64cd\u4f5c\u7b26\u5e94\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5f00\u53d1OpenOperator\u7406\u8bba\u548cOpenLM\u67b6\u6784\u5b9e\u73b0\u3002", "result": "\u5728OpenXOR\u95ee\u9898\u4e0a\uff0cOpenLM\u8fbe\u523076%\u51c6\u786e\u7387\uff0c\u800c\u6700\u5148\u8fdb\u7684LLMs\u4e3a0%\u3002", "conclusion": "\u7406\u89e3\u63a8\u7406\u7684\u672c\u8d28\u8981\u6c42\u5e76\u6784\u5efa\u63d0\u4f9b\u771f\u6b63\u63a8\u7406\u80fd\u529b\u7684\u67b6\u6784\uff0c\u800c\u975e\u4ec5\u4ec5\u6279\u8bc4\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2511.13226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13226", "abs": "https://arxiv.org/abs/2511.13226", "authors": ["Michele Persiani", "Thomas Hellstrom"], "title": "Informative Communication of Robot Plans", "comment": "Conference: PAAMS 2022, 20th International Conference on Practical Applications of Agents and Multi-Agent Systems", "summary": "When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u673a\u5668\u4eba\u8ba1\u5212\u8bed\u8a00\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8003\u8651\u7528\u6237\u7684\u4e8c\u9636\u5fc3\u667a\u7406\u8bba\u6765\u4f18\u5316\u6c9f\u901a\u6548\u679c", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u8ba1\u5212\u8bed\u8a00\u5316\u7b56\u7565\uff08\u5982\u6309\u8ba1\u5212\u987a\u5e8f\u9012\u589e\u6216\u9012\u51cf\uff09\u6ca1\u6709\u8003\u8651\u7528\u6237\u5148\u9a8c\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u5bf9\u6c9f\u901a\u4fe1\u606f\u6709\u6548\u6027\u7684\u8003\u91cf", "method": "\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u6765\u8861\u91cf\u8bed\u8a00\u5316\u6548\u679c\uff0c\u57fa\u4e8e\u7528\u6237\u7684\u4e8c\u9636\u5fc3\u667a\u7406\u8bba\u6a21\u578b\u6765\u6355\u83b7\u5176\u5148\u9a8c\u77e5\u8bc6", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u8ba9\u7528\u6237\u66f4\u5feb\u7406\u89e3\u673a\u5668\u4eba\u76ee\u6807\uff0c\u4f18\u4e8e\u9012\u589e\u6216\u9012\u51cf\u8ba1\u5212\u987a\u5e8f\u7684\u7b56\u7565", "conclusion": "\u8be5\u7b56\u7565\u63ed\u793a\u4e86\u5728\u673a\u5668\u4eba\u6c9f\u901a\u8ba1\u5212\u65f6\u4ec0\u4e48\u662f\u4fe1\u606f\u4e30\u5bcc\u7684\u4ee5\u53ca\u4e3a\u4ec0\u4e48"}}
{"id": "2511.13180", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13180", "abs": "https://arxiv.org/abs/2511.13180", "authors": ["Ronit D. Gross", "Yanir Harel", "Ido Kanter"], "title": "Translation Entropy: A Statistical Framework for Evaluating Translation Systems", "comment": "23 pages, 6 figures and 8 tables", "summary": "The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u65b9\u6cd5\u6765\u4f30\u8ba1\u7ffb\u8bd1\u71b5\uff0c\u901a\u8fc7\u5206\u6790\u5728\u4fdd\u6301\u7ffb\u8bd1\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u66ff\u6362\u7279\u5b9a\u6807\u8bb0\u7684\u6982\u7387\uff0c\u4ece\u800c\u8bc4\u4f30\u7ffb\u8bd1\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u4fe1\u606f\u65f6\u4ee3\uff0c\u7ffb\u8bd1\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5ba2\u89c2\u7684\u91cf\u5316\u65b9\u6cd5\u6765\u8bc4\u4f30\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6df1\u5ea6\u67b6\u6784\u7684\u7ffb\u8bd1\u5668\u6027\u80fd\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u5355\u79cd\u8bed\u8a00\u7684\u71b5\u4ecd\u7136\u672a\u77e5\u3002", "method": "\u7ed9\u5b9a\u4e00\u4e2a\u7ffb\u8bd1\u5668\uff0c\u901a\u8fc7\u66ff\u6362\u67a2\u8f74\u53e5\u4e2d\u7279\u5b9a\u6807\u8bb0\u751f\u6210\u591a\u4e2a\u53e5\u5b50\uff0c\u5206\u6790\u8fd9\u4e9b\u53e5\u5b50\u4ea7\u751f\u76f8\u540c\u7ffb\u8bd1\u7684\u7edf\u8ba1\u89c4\u5f8b\uff0c\u8ba1\u7b97\u66ff\u6362\u7279\u5b9a\u6807\u8bb0\u800c\u4fdd\u6301\u7ffb\u8bd1\u4e0d\u53d8\u7684\u6982\u7387\uff0c\u4ece\u800c\u5f97\u5230\u7ffb\u8bd1\u71b5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u91cf\u5316\u6392\u540d\u591a\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u7ffb\u8bd1\u5668\uff0c\u63ed\u793a\u4e92\u8bd1\u71b5\u7684\u5bf9\u79f0\u6027\uff0c\u5e76\u53d1\u73b0\u66ff\u6362\u4e24\u4e2a\u6807\u8bb0\u65f6\u7ffb\u8bd1\u7b80\u5e76\u6027\u5448\u4e58\u79ef\u6548\u5e94\u3002\u57fa\u4e8eMarianMT\u3001T5-Base\u548cNLLB-200\u7ffb\u8bd1\u5668\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ffb\u8bd1\u71b5\u662f\u53ef\u6d4b\u91cf\u7684\u5c5e\u6027\uff0c\u4e3a\u4eba\u5de5\u7ffb\u8bd1\u5668\u63d0\u4f9b\u4e86\u5ba2\u89c2\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002"}}
{"id": "2511.11714", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11714", "abs": "https://arxiv.org/abs/2511.11714", "authors": ["Daniel M. Jimenez-Gutierrez", "Enrique Zuazua", "Joaquin Del Rio", "Oleksii Sliusarenko", "Xabi Uribe-Etxebarria"], "title": "Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data", "comment": null, "summary": "Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.\n  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.", "AI": {"tldr": "\u4f7f\u7528\u8054\u90a6\u5b66\u4e60\u5e73\u53f0\u5728\u591a\u4e2a\u533b\u9662\u95f4\u534f\u4f5c\u8bad\u7ec3\u80ba\u708e\u68c0\u6d4b\u6a21\u578b\uff0c\u6570\u636e\u4fdd\u6301\u672c\u5730\u5316\uff0c\u65e0\u9700\u4f20\u8f93\u60a3\u8005\u80f8\u7247\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u81f390%\u51c6\u786e\u7387\u548c96.6% ROC-AUC\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u6570\u636e\u56e0\u9690\u79c1\u6cd5\u89c4\uff08HIPAA\u3001GDPR\uff09\u65e0\u6cd5\u96c6\u4e2d\u5316\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9\u5168\u7403\u5206\u5e03\u5f0f\u6570\u636e\u3001\u533b\u9662\u95f4\u5dee\u5f02\u6027\u548c\u5f02\u6784\u6210\u50cf\u534f\u8bae\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528Sherpa.ai\u8054\u90a6\u5b66\u4e60\u5e73\u53f0\uff0c\u4f7f\u7528\u513f\u79d1\u80ba\u708e\u80f8\u7247\u6570\u636e\u96c6\u6a21\u62df\u8de8\u533b\u9662\u534f\u4f5c\uff0c\u5904\u7406\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\uff0c\u4fdd\u6301\u6570\u636e\u672c\u5730\u5316\u548c\u9690\u79c1\u3002", "result": "\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u8fbe\u52300.900\u51c6\u786e\u7387\u548c0.966 ROC-AUC\uff0c\u76f8\u6bd4\u5355\u533b\u9662\u6a21\u578b\uff080.610\u51c6\u786e\u7387\uff1b0.644 ROC-AUC\uff09\u5206\u522b\u63d0\u534747.5%\u548c50.0%\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u5728\u4e0d\u4f20\u8f93\u60a3\u8005\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u3001\u53ef\u6cdb\u5316\u3001\u5b89\u5168\u4e14\u79c1\u5bc6\u7684\u80ba\u708e\u68c0\u6d4b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7f55\u89c1\u75be\u75c5\u548c\u4f4e\u6570\u636e\u9886\u57df\u3002"}}
{"id": "2511.13288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13288", "abs": "https://arxiv.org/abs/2511.13288", "authors": ["Haoyang Hong", "Jiajun Yin", "Yuan Wang", "Jingnan Liu", "Zhe Chen", "Ailing Yu", "Ji Li", "Zhiling Ye", "Hansong Xiao", "Yefei Chen", "Hualei Zhou", "Yun Yue", "Minghui Yang", "Chunxiao Guo", "Junwei Liu", "Peng Wei", "Jinjie Gu"], "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO", "comment": null, "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86M-GRPO\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e0d\u540c\u667a\u80fd\u4f53\u4f7f\u7528\u4e0d\u540cLLM\u65f6\u7684\u4f18\u5316\u6311\u6218\uff0c\u901a\u8fc7\u5206\u5c42\u4fe1\u7528\u5206\u914d\u548c\u8f68\u8ff9\u5bf9\u9f50\u65b9\u6848\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f7f\u7528\u7edf\u4e00LLM\u8bad\u7ec3\u9650\u5236\u4e86\u6027\u80fd\uff0c\u56e0\u4e3a\u4e0d\u540c\u667a\u80fd\u4f53\u5177\u6709\u4e0d\u540c\u7684\u6570\u636e\u5206\u5e03\u3002\u4f7f\u7528\u4e0d\u540cLLM\u8bad\u7ec3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u4f18\u5316\u6311\u6218\uff0c\u5982\u4e0d\u540c\u9891\u7387\u64cd\u4f5c\u3001\u53ef\u53d8\u5b50\u667a\u80fd\u4f53\u8c03\u7528\u548c\u8de8\u670d\u52a1\u5668\u90e8\u7f72\u5bfc\u81f4\u7684\u68af\u5ea6\u6d41\u4e2d\u65ad\u3002", "method": "\u63d0\u51faM-GRPO\u65b9\u6cd5\uff1a1\uff09\u4e3a\u4e3b\u667a\u80fd\u4f53\u548c\u5b50\u667a\u80fd\u4f53\u8ba1\u7b97\u7ec4\u76f8\u5bf9\u4f18\u52bf\u7684\u5206\u5c42\u4fe1\u7528\u5206\u914d\uff1b2\uff09\u8f68\u8ff9\u5bf9\u9f50\u65b9\u6848\u751f\u6210\u56fa\u5b9a\u5927\u5c0f\u6279\u6b21\uff1b3\uff09\u89e3\u8026\u8bad\u7ec3\u7ba1\u9053\uff0c\u667a\u80fd\u4f53\u5728\u72ec\u7acb\u670d\u52a1\u5668\u4e0a\u8fd0\u884c\u5e76\u901a\u8fc7\u5171\u4eab\u5b58\u50a8\u4ea4\u6362\u7edf\u8ba1\u4fe1\u606f\u3002", "result": "\u5728GAIA\u3001XBench-DeepSearch\u548cWebWalkerQA\u7b49\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cM-GRPO\u59cb\u7ec8\u4f18\u4e8e\u5355\u667a\u80fd\u4f53GRPO\u548c\u5b50\u667a\u80fd\u4f53\u51bb\u7ed3\u7684\u591a\u667a\u80fd\u4f53GRPO\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u5bf9\u9f50\u5f02\u6784\u8f68\u8ff9\u548c\u5728\u4e13\u4e1a\u667a\u80fd\u4f53\u95f4\u89e3\u8026\u4f18\u5316\u80fd\u591f\u589e\u5f3a\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13182", "abs": "https://arxiv.org/abs/2511.13182", "authors": ["Mihai Dan Nadas", "Laura Diosan"], "title": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study", "comment": null, "summary": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u7b49\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u800cLlama\u7cfb\u5217\u6a21\u578b\u8868\u73b0\u6ce2\u52a8\u8f83\u5927\u3002", "motivation": "\u81ea\u52a8\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u5bf9\u4e8e\u5904\u7406\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7b49\u5bcc\u542b\u53d8\u97f3\u7b26\u53f7\u7684\u8bed\u8a00\u6587\u672c\u5904\u7406\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8bc4\u4f30\u73b0\u6709LLM\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u7efc\u5408\u8bed\u6599\u5e93\u6d4b\u8bd5\u4e86GPT-3.5\u3001GPT-4\u3001GPT-4o\u3001Gemini 1.0 Pro\u3001Llama 2/3\u3001Mixtral 8x7B\u3001airoboros 70B\u3001RoLlama 2 7B\u7b49\u6a21\u578b\uff0c\u91c7\u7528\u4ece\u96f6\u6837\u672c\u5230\u590d\u6742\u591a\u6837\u672c\u7684\u591a\u79cd\u63d0\u793a\u6a21\u677f\u3002", "result": "GPT-4o\u7b49\u6a21\u578b\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u53d8\u97f3\u7b26\u53f7\u6062\u590d\uff0c\u6301\u7eed\u8d85\u8d8a\u4e2d\u6027\u56de\u58f0\u57fa\u7ebf\uff0c\u800cMeta\u7684Llama\u7cfb\u5217\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5927\u7684\u6ce2\u52a8\u6027\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u548c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4e3a\u6539\u8fdb\u5bcc\u542b\u53d8\u97f3\u7b26\u53f7\u8bed\u8a00\u7684NLP\u5de5\u5177\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.11717", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.11717", "abs": "https://arxiv.org/abs/2511.11717", "authors": ["Xiang Xiang Wang", "Sean Cottrell", "Guo-Wei Wei"], "title": "Multiscale Grassmann Manifolds for Single-Cell Data Analysis", "comment": null, "summary": "Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGrassmann\u6d41\u5f62\u7684\u591a\u5c3a\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\uff0c\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u51e0\u4f55\u89c6\u56fe\u7684\u7279\u5f81\u6765\u66f4\u597d\u5730\u6355\u6349\u7ec6\u80de\u5f02\u8d28\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u8868\u793a\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5355\u7ec6\u80de\u6570\u636e\u4e2d\u7684\u5185\u5728\u76f8\u5173\u6027\u548c\u591a\u5c3a\u5ea6\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u57fa\u4e8eGrassmann\u6d41\u5f62\u6784\u5efa\u591a\u5c3a\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5e42\u51fd\u6570\u63a7\u5236\u5c3a\u5ea6\u91c7\u6837\uff0c\u5c06\u4e0d\u540c\u8868\u793a\u5c3a\u5ea6\u7684\u7279\u5f81\u6574\u5408\u5230\u7edf\u4e00\u7684\u6d41\u5f62\u7a7a\u95f4\u4e2d\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u5355\u7ec6\u80deRNA-seq\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u6301\u6709\u610f\u4e49\u7684\u7ed3\u6784\u5e76\u63d0\u4f9b\u7a33\u5b9a\u7684\u805a\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4e2d\u5c0f\u578b\u6570\u636e\u96c6\u3002", "conclusion": "Grassmann\u6d41\u5f62\u4e3a\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u8fde\u8d2f\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2511.13225", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13225", "abs": "https://arxiv.org/abs/2511.13225", "authors": ["Tyler Loakman", "Joseph James", "Chenghua Lin"], "title": "Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms", "comment": "Accepted to IJCNLP-AACL 2025", "summary": "With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u521b\u5efa\u5305\u542b4000+\u82f1\u8bed\u5355\u8bcd\u7684\u58f0\u8c31\u56fe\u548c\u6ce2\u5f62\u56fe\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u6a21\u578b\u9884\u6d4b\u97f3\u7d20\u6216\u5b57\u7d20\u8f6c\u5f55\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u878d\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u4f5c\u4e3a\u8bed\u97f3\u5b66\u5bb6\u89e3\u91ca\u8bed\u97f3\u58f0\u8c31\u56fe\u548c\u6ce2\u5f62\u7684\u80fd\u529b\u3002", "method": "\u5408\u6210\u5305\u542b4000+\u82f1\u8bed\u5355\u8bcd\u7684\u65b0\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u9879\u9009\u62e9\u4efb\u52a1\u6d4b\u8bd5\u6a21\u578b\u9884\u6d4b\u6b63\u786e\u97f3\u7d20\u6216\u5b57\u7d20\u8f6c\u5f55\u7684\u80fd\u529b\uff0c\u4f7f\u7528\u57fa\u4e8e\u97f3\u7d20\u7f16\u8f91\u8ddd\u79bb\u7684\u5e72\u6270\u9879\u3002", "result": "\u65e0\u8bba\u662f\u96f6\u6837\u672c\u8fd8\u662f\u5fae\u8c03\u6a21\u578b\uff0c\u5176\u8868\u73b0\u90fd\u5f88\u5c11\u8d85\u8fc7\u968f\u673a\u731c\u6d4b\u6c34\u5e73\uff0c\u8868\u660e\u9700\u8981\u7279\u5b9a\u7684\u53c2\u6570\u77e5\u8bc6\u6765\u89e3\u91ca\u6b64\u7c7b\u56fe\u5f62\u3002", "conclusion": "\u4ec5\u9760\u914d\u5bf9\u6837\u672c\u4e0d\u8db3\u4ee5\u8ba9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6709\u6548\u89e3\u91ca\u8bed\u97f3\u58f0\u8c31\u56fe\u548c\u6ce2\u5f62\uff0c\u9700\u8981\u4e13\u95e8\u7684\u53c2\u6570\u77e5\u8bc6\u3002"}}
{"id": "2511.13293", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13293", "abs": "https://arxiv.org/abs/2511.13293", "authors": ["Chuang Zhao", "Hui Tang", "Hongke Zhao", "Xiaofang Zhou", "Xiaomeng Li"], "title": "Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval", "comment": null, "summary": "Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \\underline{g}enerative \\underline{h}ierarchical \\underline{a}gentic \\underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86GHAR\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u667a\u80fd\u4f53\u67b6\u6784\u89e3\u51b3\u533b\u7597\u9884\u6d4b\u4e2dLLMs\u7684\u51c6\u786e\u6027\u95ee\u9898\uff0c\u5305\u62ec\u51b3\u5b9a\u4f55\u65f6\u68c0\u7d22\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u68c0\u7d22\u5668\u4e0e\u751f\u6210\u5668\u7684\u534f\u4f5c\u3002", "motivation": "LLMs\u5728\u533b\u7597\u9884\u6d4b\u4e2d\u5b58\u5728\u4e8b\u5b9e\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u4f20\u7edfRAG\u6846\u67b6\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4f55\u65f6\u6fc0\u6d3b\u68c0\u7d22\u673a\u5236\u4ee5\u53ca\u5982\u4f55\u5b9e\u73b0\u68c0\u7d22\u5668\u4e0e\u751f\u6210\u5668\u7684\u534f\u540c\u4f18\u5316\u3002", "method": "\u91c7\u7528\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff1aAgent-Top\u4f5c\u4e3a\u4e3b\u6cbb\u533b\u751f\u51b3\u5b9a\u662f\u5426\u4f9d\u8d56\u53c2\u6570\u77e5\u8bc6\u6216\u542f\u52a8\u68c0\u7d22\uff0cAgent-Low\u4f5c\u4e3a\u54a8\u8be2\u670d\u52a1\u603b\u7ed3\u4efb\u52a1\u76f8\u5173\u77e5\u8bc6\uff1b\u5728\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7edf\u4e00\u4f18\u5316\u4e24\u4e2a\u667a\u80fd\u4f53\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u6d41\u884c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u3002", "conclusion": "\u5206\u5c42\u667a\u80fd\u4f53RAG\u6846\u67b6\u5728\u63a8\u8fdb\u533b\u7597\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.13254", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13254", "abs": "https://arxiv.org/abs/2511.13254", "authors": ["Shalini Maiti", "Amar Budhiraja", "Bhavul Gauri", "Gaurav Chaurasia", "Anton Protopopov", "Alexis Audran-Reiss", "Michael Slater", "Despoina Magka", "Tatiana Shavrina", "Roberta Raileanu", "Yoram Bachrach"], "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "AI": {"tldr": "SoCE\u662f\u4e00\u79cd\u57fa\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u7ec4\u5408\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u7c7b\u522b\u4e13\u5bb6\u6a21\u578b\u5e76\u4f7f\u7528\u975e\u5747\u5300\u52a0\u6743\u5e73\u5747\u6765\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u6a21\u578b\u878d\u5408\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5c31\u80fd\u63d0\u5347\u6027\u80fd\u7684\u6280\u672f\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u4f20\u7edf\u5747\u5300\u5e73\u5747\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e0d\u540c\u6a21\u578b\u5728\u7279\u5b9a\u7c7b\u522b\u4e0a\u7684\u4e13\u957f\u3002", "method": "\u5229\u7528\u57fa\u51c6\u6d4b\u8bd5\u7c7b\u522b\u95f4\u4f4e\u76f8\u5173\u6027\u7684\u89c2\u5bdf\uff0c\u8bc6\u522b\u6bcf\u4e2a\u5f31\u76f8\u5173\u7c7b\u522b\u7c07\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u4f18\u5316\u7684\u975e\u5747\u5300\u52a0\u6743\u5e73\u5747\u800c\u975e\u5747\u5300\u6743\u91cd\u8fdb\u884c\u6a21\u578b\u878d\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\uff08\u591a\u8bed\u8a00\u80fd\u529b\u3001\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u7b49\uff09\u63d0\u5347\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5728\u4f2f\u514b\u5229\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "SoCE\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7c7b\u522b\u4e13\u5bb6\u6a21\u578b\u548c\u4f18\u5316\u52a0\u6743\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u800c\u65e0\u9700\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2511.11727", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11727", "abs": "https://arxiv.org/abs/2511.11727", "authors": ["Tongda Xu"], "title": "Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm", "comment": "NIPS 25 Workshop: Frontiers in Probabilistic Inference: Sampling Meets Learning", "summary": "Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5728\u6269\u6563\u6a21\u578b\u4e2d\u4f7f\u7528\u53bb\u566a\u5206\u6570\u5339\u914d\u4f18\u5316\u6761\u4ef6\u8f93\u5165\u4f1a\u7834\u574f\u5176\u4e0e\u7cbe\u786e\u5206\u6570\u5339\u914d\u7684\u7b49\u4ef7\u6027\uff0c\u5bfc\u81f4\u504f\u5dee\u548c\u66f4\u9ad8\u7684\u5206\u6570\u8303\u6570\uff0c\u5f71\u54cd\u591a\u4e2a\u9886\u57df\u7684\u7814\u7a76\u5de5\u4f5c\u3002", "motivation": "\u8bb8\u591a\u8fd1\u671f\u7814\u7a76\u4f7f\u7528\u53bb\u566a\u5206\u6570\u5339\u914d\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u8f93\u5165\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u7406\u8bba\u504f\u5dee\u95ee\u9898\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u53bb\u566a\u5206\u6570\u5339\u914d\u4f18\u5316\u6761\u4ef6\u8f93\u5165\u4f1a\u7834\u574f\u4e0e\u7cbe\u786e\u5206\u6570\u5339\u914d\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u89c2\u5bdf\u6570\u636e\u5206\u5e03\u4f18\u5316\u65f6\u7684\u7c7b\u4f3c\u504f\u5dee\u3002", "result": "\u53d1\u73b0\u8fd9\u79cd\u504f\u5dee\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684\u5206\u6570\u8303\u6570\uff0c\u5e76\u5f71\u54cd\u591a\u4e2a\u9886\u57df\u7684\u7814\u7a76\uff0c\u5305\u62ec\u81ea\u56de\u5f52\u751f\u6210\u7684MAR\u3001\u56fe\u50cf\u538b\u7f29\u7684PerCo\u548c\u6587\u672c\u52303D\u751f\u6210\u7684DreamFusion\u3002", "conclusion": "\u53bb\u566a\u5206\u6570\u5339\u914d\u5728\u4f18\u5316\u6269\u6563\u6a21\u578b\u6761\u4ef6\u8f93\u5165\u65f6\u5b58\u5728\u7406\u8bba\u504f\u5dee\uff0c\u8fd9\u4e00\u53d1\u73b0\u5bf9\u591a\u4e2a\u5e94\u7528\u9886\u57df\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u76f8\u5173\u65b9\u6cd5\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.13306", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13306", "abs": "https://arxiv.org/abs/2511.13306", "authors": ["Bowen Ye", "Bin Zhang", "Hang Zhao"], "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving", "comment": null, "summary": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.", "AI": {"tldr": "DAP\u662f\u4e00\u4e2a\u79bb\u6563\u4ee4\u724c\u81ea\u56de\u5f52\u89c4\u5212\u5668\uff0c\u8054\u5408\u9884\u6d4bBEV\u8bed\u4e49\u548c\u81ea\u8f66\u8f68\u8ff9\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\uff0c\u5728160M\u53c2\u6570\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u968f\u7740\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u6269\u5927\u800c\u6301\u7eed\u63d0\u5347\u6027\u80fd\u7684\u6311\u6218\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6570\u636e\u6269\u5c55\u6548\u7387\uff0c\u4f46\u4ec5\u9884\u6d4b\u81ea\u8f66\u8f68\u8ff9\u5b58\u5728\u76d1\u7763\u7a00\u758f\u548c\u573a\u666f\u6f14\u5316\u7ea6\u675f\u5f31\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u79bb\u6563\u4ee4\u724c\u81ea\u56de\u5f52\u89c4\u5212\u5668DAP\uff0c\u8054\u5408\u9884\u6d4bBEV\u8bed\u4e49\u548c\u81ea\u8f66\u8f68\u8ff9\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u5728\u4fdd\u6301\u76d1\u7763\u884c\u4e3a\u514b\u9686\u5148\u9a8c\u7684\u540c\u65f6\u6ce8\u5165\u5956\u52b1\u5f15\u5bfc\u7684\u6539\u8fdb\u3002", "result": "\u5728160M\u7d27\u51d1\u53c2\u6570\u9884\u7b97\u4e0b\uff0cDAP\u5728\u5f00\u73af\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u95ed\u73af\u7ed3\u679c\u3002", "conclusion": "\u5b8c\u5168\u79bb\u6563\u4ee4\u724c\u81ea\u56de\u5f52\u516c\u5f0f\u5728\u6805\u683c\u5316BEV\u548c\u81ea\u8f66\u52a8\u4f5c\u4e0a\u8fd0\u884c\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u4e14\u53ef\u6269\u5c55\u7684\u89c4\u5212\u8303\u5f0f\u3002"}}
{"id": "2511.13329", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.13329", "abs": "https://arxiv.org/abs/2511.13329", "authors": ["Shufan Yang", "Zifeng Cheng", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Shiping Ge", "Yuchen Fu", "Qing Gu"], "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection", "comment": "AAAI 2026", "summary": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.", "AI": {"tldr": "RegionMarker\u662f\u4e00\u4e2a\u533a\u57df\u89e6\u53d1\u8bed\u4e49\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u89e6\u53d1\u533a\u57df\u5e76\u5411\u76f8\u5173\u6587\u672c\u5d4c\u5165\u6ce8\u5165\u6c34\u5370\uff0c\u4e3aEmbedding-as-a-Service\u63d0\u4f9b\u5168\u9762\u7684\u7248\u6743\u4fdd\u62a4\u3002", "motivation": "\u73b0\u6709\u7684EaaS\u6c34\u5370\u65b9\u6cd5\u53ea\u80fd\u62b5\u6297\u90e8\u5206\u653b\u51fb\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5168\u9762\u7684\u7248\u6743\u4fdd\u62a4\uff0c\u5b58\u5728\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u5bfc\u81f4\u7ecf\u6d4e\u635f\u5931\u7684\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u79d8\u5bc6\u964d\u7ef4\u77e9\u9635\u6295\u5f71\u5230\u5b50\u7a7a\u95f4\uff0c\u968f\u673a\u9009\u62e9\u89e6\u53d1\u533a\u57df\uff0c\u5728\u6574\u4e2a\u89e6\u53d1\u533a\u57df\u5d4c\u5165\u6c34\u5370\uff0c\u5e76\u5c06\u6587\u672c\u5d4c\u5165\u672c\u8eab\u4f5c\u4e3a\u6c34\u5370\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRegionMarker\u80fd\u6709\u6548\u62b5\u6297\u4e0d\u540c\u653b\u51fb\u65b9\u6cd5\uff0c\u5305\u62ec\u6c34\u5370\u79fb\u9664\u653b\u51fb\u3001\u8f6c\u8ff0\u653b\u51fb\u548c\u7ef4\u5ea6\u6270\u52a8\u653b\u51fb\u3002", "conclusion": "RegionMarker\u6846\u67b6\u4e3aEaaS\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7248\u6743\u4fdd\u62a4\uff0c\u80fd\u591f\u6709\u6548\u9632\u6b62\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u5e26\u6765\u7684\u7ecf\u6d4e\u635f\u5931\u3002"}}
{"id": "2511.11734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11734", "abs": "https://arxiv.org/abs/2511.11734", "authors": ["Kamalpreet Singh Kainth", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedat Panat"], "title": "Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics", "comment": null, "summary": "Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86PI-NODE-SR\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u9636\u663e\u5f0f\u6c42\u89e3\u5668\u548c\u6b8b\u5dee\u5f52\u4e00\u5316\uff0c\u7a33\u5b9a\u5730\u5b66\u4e60\u521a\u6027\u751f\u7269\u7269\u7406\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u80fd\u591f\u4ece\u5355\u4e2a\u632f\u8361\u4e2d\u5b66\u4e60\u5e76\u51c6\u786e\u5916\u63a8\u3002", "motivation": "\u6807\u51c6\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u548c\u7269\u7406\u4fe1\u606f\u53d8\u4f53\u5728\u5efa\u6a21\u521a\u6027\u751f\u7269\u7269\u7406\u7cfb\u7edf\u65f6\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u5927\u91cf\u8fed\u4ee3\u4e14\u53ef\u80fd\u6536\u655b\u5230\u6b21\u4f18\u89e3\uff0c\u65e0\u6cd5\u4fdd\u6301\u632f\u8361\u9891\u7387\u6216\u632f\u5e45\u3002", "method": "\u7ed3\u5408\u4f4e\u9636\u663e\u5f0f\u6c42\u89e3\u5668\uff08Heun\u65b9\u6cd5\uff09\u548c\u5c3a\u5ea6\u611f\u77e5\u6b8b\u5dee\u5f52\u4e00\u5316\uff0c\u5e73\u8861\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u72b6\u6001\u53d8\u91cf\u7684\u8d21\u732e\uff0c\u907f\u514d\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u9690\u5f0f\u6c42\u89e3\u5668\u3002", "result": "\u5728Hodgkin-Huxley\u65b9\u7a0b\u4e0a\uff0cPI-NODE-SR\u4ece\u5355\u4e2a\u632f\u8361\u5b66\u4e60\u5e76\u5916\u63a8\u8d85\u8fc7100ms\uff0c\u51c6\u786e\u6355\u6349\u632f\u8361\u9891\u7387\u548c\u632f\u5e45\uff0c\u6062\u590d\u95e8\u63a7\u53d8\u91cf\u4e2d\u7684\u5c16\u9510\u4e9a\u9608\u503c\u66f2\u7387\u7b49\u5f62\u6001\u7279\u5f81\u3002", "conclusion": "PI-NODE-SR\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u795e\u7ecfODE\u548cPINNs\u6301\u7eed\u51cf\u5c11\u957f\u671f\u8bef\u5dee\uff0c\u4e3a\u7a33\u5b9a\u9ad8\u6548\u5b66\u4e60\u521a\u6027\u751f\u7269\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9014\u5f84\u3002"}}
{"id": "2511.13359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13359", "abs": "https://arxiv.org/abs/2511.13359", "authors": ["Yuhang Wang", "Yanxu Zhu", "Jitao Sang"], "title": "Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms", "comment": null, "summary": "The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86CNCA\u6846\u67b6\uff0c\u5229\u7528\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\u5b9e\u73b0\u6587\u5316\u5bf9\u9f50\uff0c\u901a\u8fc7\u81ea\u52a8\u6316\u6398\u6587\u5316\u89c4\u8303\u5e76\u91c7\u7528\u4e0a\u4e0b\u6587\u5bf9\u9f50\u548c\u5fae\u8c03\u4e24\u79cd\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6587\u5316\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5927\u6a21\u578b\u4e0d\u4ec5\u9700\u8981\u5b89\u5168\uff0c\u8fd8\u9700\u8981\u53cd\u6620\u4e0d\u540c\u6587\u5316\u7684\u591a\u5143\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u5b9e\u73b0\u6587\u5316\u5bf9\u9f50\u3002", "method": "\u63d0\u51faCNCA\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u81ea\u52a8\u6316\u6398\u6587\u5316\u89c4\u8303\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4e0a\u4e0b\u6587\u548c\u57fa\u4e8e\u5fae\u8c03\u7684\u4e24\u79cd\u5bf9\u9f50\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\uff0c\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u4ece\u6587\u5316\u89c4\u8303\u6316\u6398\u548c\u5229\u7528\u4e2d\u83b7\u76ca\u66f4\u591a\u3002", "conclusion": "\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u6587\u5316\u4fe1\u606f\u5bf9\u9f50\u7b56\u7565\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u591a\u5143\u4eba\u7c7b\u4ef7\u503c\u89c2\u3002"}}
{"id": "2511.13335", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13335", "abs": "https://arxiv.org/abs/2511.13335", "authors": ["Maram Alharbi", "Salmane Chafik", "Saad Ezzini", "Ruslan Mitkov", "Tharindu Ranasinghe", "Hansi Hettiarachchi"], "title": "AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects", "comment": null, "summary": "The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u60c5\u611f\u5206\u6790\u5171\u4eab\u4efb\u52a1\uff0c\u4f7f\u7528\u5305\u542b\u6c99\u7279\u548c\u6469\u6d1b\u54e5\u65b9\u8a00\u7684\u9152\u5e97\u8bc4\u8bba\u6570\u636e\u96c6\uff0c\u65e8\u5728\u5f00\u53d1\u65b9\u8a00\u611f\u77e5\u7684NLP\u7cfb\u7edf\u7528\u4e8e\u5ba2\u6237\u4f53\u9a8c\u5206\u6790\u3002", "motivation": "\u963f\u62c9\u4f2f\u4e16\u754c\u9152\u5e97\u4e1a\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5ba2\u6237\u53cd\u9988\u6765\u6539\u5584\u670d\u52a1\uff0c\u9700\u8981\u5148\u8fdb\u7684\u963f\u62c9\u4f2f\u8bed\u60c5\u611f\u5206\u6790\u5de5\u5177\u6765\u652f\u6301\u591a\u65b9\u8a00\u73af\u5883\u4e0b\u7684\u5ba2\u6237\u4f53\u9a8c\u5206\u6790\u3002", "method": "\u521b\u5efa\u591a\u65b9\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b538\u6761\u60c5\u611f\u5e73\u8861\u7684\u9152\u5e97\u8bc4\u8bba\uff0c\u4ece\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u4e3a\u6c99\u7279\u548c\u6469\u6d1b\u54e5\u65b9\u8a00\uff0c\u5e76\u7531\u6bcd\u8bed\u8005\u9a8c\u8bc1\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u60c5\u611f\u4fdd\u7559\u3002", "result": "\u8d85\u8fc740\u4e2a\u56e2\u961f\u6ce8\u518c\u53c2\u4e0e\u5171\u4eab\u4efb\u52a1\uff0c12\u4e2a\u56e2\u961f\u63d0\u4ea4\u7cfb\u7edf\uff0c\u6700\u4f73\u7cfb\u7edfF1\u5f97\u5206\u8fbe\u52300.81\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8de8\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7684\u60c5\u611f\u5206\u6790\u662f\u53ef\u884c\u7684\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u8be5\u8d44\u6e90\u652f\u6301\u5f00\u53d1\u65b9\u8a00\u611f\u77e5\u7684NLP\u7cfb\u7edf\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2511.11736", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11736", "abs": "https://arxiv.org/abs/2511.11736", "authors": ["Susumu Katayama"], "title": "KAN/H: Kolmogorov-Arnold Network using Haar-like bases", "comment": null, "summary": "This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.", "AI": {"tldr": "\u63d0\u51fa\u4e86KAN/H\uff0c\u4e00\u79cd\u4f7f\u7528Haar\u53d8\u4f53\u57fa\u7cfb\u7edf\uff08\u5305\u542b\u5168\u5c40\u548c\u5c40\u90e8\u57fa\uff09\u66ff\u4ee3B\u6837\u6761\u7684Kolmogorov-Arnold\u7f51\u7edc\u53d8\u4f53\uff0c\u5e94\u7528\u4e8e\u51fd\u6570\u903c\u8fd1\u548cMNIST\u95ee\u9898\uff0c\u65e0\u9700\u5927\u91cf\u95ee\u9898\u7279\u5b9a\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "motivation": "\u6539\u8fdbKolmogorov-Arnold\u7f51\u7edc\uff0c\u901a\u8fc7\u4f7f\u7528Haar\u53d8\u4f53\u57fa\u7cfb\u7edf\u6765\u907f\u514dB\u6837\u6761\u65b9\u6cd5\u6240\u9700\u7684\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u63d0\u9ad8\u7f51\u7edc\u7684\u5b9e\u7528\u6027\u548c\u6613\u7528\u6027\u3002", "method": "\u5f00\u53d1KAN/H\u7f51\u7edc\uff0c\u91c7\u7528Haar\u53d8\u4f53\u57fa\u7cfb\u7edf\uff08\u5305\u542b\u5168\u5c40\u548c\u5c40\u90e8\u57fa\uff09\u66ff\u4ee3\u4f20\u7edf\u7684B\u6837\u6761\u57fa\u51fd\u6570\uff0c\u5e94\u7528\u4e8e\u51fd\u6570\u903c\u8fd1\u548cMNIST\u5206\u7c7b\u4efb\u52a1\u3002", "result": "KAN/H\u5728\u51fd\u6570\u903c\u8fd1\u548cMNIST\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e14\u4e0d\u9700\u8981\u5927\u591a\u6570\u95ee\u9898\u7279\u5b9a\u7684\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u7b80\u5316\u4e86\u7f51\u7edc\u7684\u4f7f\u7528\u8fc7\u7a0b\u3002", "conclusion": "KAN/H\u901a\u8fc7\u4f7f\u7528Haar\u53d8\u4f53\u57fa\u7cfb\u7edf\u6709\u6548\u51cf\u5c11\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u9700\u6c42\uff0c\u4e3aKolmogorov-Arnold\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u53d8\u4f53\u3002"}}
{"id": "2511.13361", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13361", "abs": "https://arxiv.org/abs/2511.13361", "authors": ["Jiyang Zheng", "Islam Nassar", "Thanh Vu", "Xu Zhong", "Yang Lin", "Tongliang Liu", "Long Duong", "Yuan-Fang Li"], "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding", "comment": null, "summary": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.", "AI": {"tldr": "MedDCR\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u7597\u7f16\u7801\u7684\u95ed\u73af\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u89c6\u4e3a\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5668\u3001\u7f16\u7801\u5668\u548c\u53cd\u5c04\u5668\u7684\u534f\u4f5c\uff0c\u5b9e\u73b0\u5de5\u4f5c\u6d41\u7684\u81ea\u52a8\u5b66\u4e60\u548c\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u6587\u6863\u7684\u7ec6\u5fae\u5dee\u5f02\u548c\u53d8\u5316\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5b66\u4e60\u6709\u6548\u7684\u5de5\u4f5c\u6d41\u3002", "method": "\u91c7\u7528\u95ed\u73af\u6846\u67b6\uff1a\u8bbe\u8ba1\u5668\u63d0\u51fa\u5de5\u4f5c\u6d41\uff0c\u7f16\u7801\u5668\u6267\u884c\u5de5\u4f5c\u6d41\uff0c\u53cd\u5c04\u5668\u8bc4\u4f30\u9884\u6d4b\u5e76\u63d0\u4f9b\u53cd\u9988\uff0c\u8bb0\u5fc6\u6863\u6848\u4fdd\u5b58\u5148\u524d\u8bbe\u8ba1\u4ee5\u4f9b\u91cd\u7528\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMedDCR\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ea7\u751f\u53ef\u89e3\u91ca\u3001\u53ef\u9002\u5e94\u7684\u5de5\u4f5c\u6d41\uff0c\u66f4\u597d\u5730\u53cd\u6620\u5b9e\u9645\u7f16\u7801\u5b9e\u8df5\u3002", "conclusion": "MedDCR\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u533b\u7597\u7f16\u7801\u5de5\u4f5c\u6d41\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13368", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13368", "abs": "https://arxiv.org/abs/2511.13368", "authors": ["Kajetan Dymkiewicz", "Ivan Vulic", "Helen Yannakoudakis", "Eilam Shapira", "Roi Reichart", "Anna Korhonen"], "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7PEFT/LoRA\u7814\u7a76\u63a2\u7d22LLMs\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u8bed\u8a00\u95f4\u7684\u8fc1\u79fb\u6548\u679c\uff0c\u53d1\u73b0\u4efb\u52a1\u5185\u8de8\u8bed\u8a00\u8fc1\u79fb\u7a33\u5b9a\u6b63\u5411\uff0c\u800c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u548c\u8bed\u8a00\u95f4\u7684\u975e\u5bf9\u79f0\u8fc1\u79fb\u6a21\u5f0f\u3002", "motivation": "\u7406\u89e3LLMs\u5728\u4e00\u4e2a\u4efb\u52a1\u6216\u8bed\u8a00\u4e0a\u7684\u6539\u8fdb\u5982\u4f55\u5f71\u54cd\u5176\u4ed6\u4efb\u52a1\u548c\u8bed\u8a00\u53ca\u5176\u7ec4\u5408\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u91c7\u7528PEFT/LoRA\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u5f00\u6e90LLM\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u5c06\u4efb\u52a1\u548c\u8bed\u8a00\u4f5c\u4e3a\u8fc1\u79fb\u8f74\uff0c\u6d4b\u91cf\u5355\u4efb\u52a1-\u8bed\u8a00\u6e90\u5bf9\u5230\u6240\u6709\u5176\u4ed6\u4efb\u52a1-\u8bed\u8a00\u76ee\u6807\u5bf9\u7684\u8fc1\u79fb\u6548\u679c\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e00\u81f4\u6a21\u5f0f\uff1a1\uff09\u4efb\u52a1\u5185\u8de8\u8bed\u8a00\u8fc1\u79fb\u53ef\u9760\u6b63\u5411\uff0c\u800c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b2\uff09\u5b58\u5728\u7a33\u5b9a\u7684\u6350\u8d60\u8005-\u63a5\u6536\u8005\u7ed3\u6784\uff08\u67a2\u7ebd\u6350\u8d60\u8005vs\u8106\u5f31\u63a5\u6536\u8005\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u98ce\u9669\u611f\u77e5\u5fae\u8c03\u548c\u6a21\u578b\u4e13\u4e1a\u5316\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u548c\u8bed\u8a00\u95f4\u7684\u975e\u5bf9\u79f0\u8fc1\u79fb\u7279\u6027\u3002"}}
{"id": "2511.11737", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.11737", "abs": "https://arxiv.org/abs/2511.11737", "authors": ["Qizhe Li", "Haolong Chen", "Jiansheng Li", "Shuqi Chai", "Xuan Li", "Yuzhou Hou", "Xinhua Shao", "Fangfang Li", "Kaifeng Han", "Guangxu Zhu"], "title": "DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks", "comment": "13 pages, submitted for possible publication", "summary": "Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.", "AI": {"tldr": "DK-Root\u662f\u4e00\u4e2a\u8054\u5408\u6570\u636e\u548c\u77e5\u8bc6\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u53ef\u6269\u5c55\u7684\u5f31\u76d1\u7763\u548c\u7cbe\u786e\u7684\u4e13\u5bb6\u6307\u5bfc\uff0c\u7528\u4e8e\u79fb\u52a8\u7f51\u7edcQoE\u95ee\u9898\u7684\u6839\u56e0\u5206\u6790\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u4e2dQoE\u95ee\u9898\u7684\u6839\u56e0\u8bca\u65ad\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b58\u5728\u590d\u6742\u7684\u8de8\u5c42KPI\u4ea4\u4e92\u4e14\u7f3a\u4e4f\u53ef\u9760\u7684\u4e13\u5bb6\u6807\u6ce8\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u867d\u7136\u80fd\u5927\u89c4\u6a21\u751f\u6210\u6807\u7b7e\uff0c\u4f46\u5b58\u5728\u566a\u58f0\u4e14\u7c92\u5ea6\u7c97\u7cd9\uff0c\u9650\u5236\u4e86\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u6807\u7b7e\u540c\u65f6\u901a\u8fc7\u76d1\u7763\u5bf9\u6bd4\u76ee\u6807\u53bb\u566a\uff1b\u5f15\u5165\u7c7b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u4fdd\u7559\u6839\u56e0\u8bed\u4e49\u7684KPI\u5e8f\u5217\uff0c\u901a\u8fc7\u63a7\u5236\u53cd\u5411\u6269\u6563\u6b65\u9aa4\u4ea7\u751f\u5f3a\u5f31\u589e\u5f3a\uff0c\u63d0\u9ad8\u7c7b\u5185\u7d27\u51d1\u6027\u548c\u7c7b\u95f4\u5206\u79bb\u6027\uff1b\u6700\u540e\u4f7f\u7528\u7a00\u7f3a\u7684\u4e13\u5bb6\u9a8c\u8bc1\u6807\u7b7e\u8054\u5408\u5fae\u8c03\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u3002", "result": "\u5728\u771f\u5b9e\u8fd0\u8425\u5546\u7ea7\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDK-Root\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6700\u8fd1\u7684\u534a\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u6761\u4ef6\u6269\u6563\u589e\u5f3a\u548c\u9884\u8bad\u7ec3-\u5fae\u8c03\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "DK-Root\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5f31\u76d1\u7763\u548c\u4e13\u5bb6\u6307\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u7f51\u7edcQoE\u6839\u56e0\u5206\u6790\u4e2d\u7684\u6807\u6ce8\u7a00\u7f3a\u548c\u566a\u58f0\u95ee\u9898\uff0c\u5728\u8868\u793a\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2511.13371", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13371", "abs": "https://arxiv.org/abs/2511.13371", "authors": ["Caroline Baumgartner", "Eleanor Spens", "Neil Burgess", "Petru Manescu"], "title": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning", "comment": null, "summary": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bad\u7ec3GPT-2\u6a21\u578b\u5728\u4e09\u79cd\u7a7a\u95f4\u5b66\u4e60\u8303\u5f0f\u4e0a\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u5b66\u4e60\u7b97\u6cd5\uff1a\u63a2\u7d22\u6a21\u578b\u53d1\u5c55\u51fa\u7c7b\u4f3c\u8ba4\u77e5\u5730\u56fe\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u800c\u76ee\u6807\u5bfc\u5411\u6a21\u578b\u5b66\u4e60\u8def\u5f84\u4f9d\u8d56\u7b97\u6cd5\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u7a7a\u95f4\u5bfc\u822a\u4efb\u52a1\uff0c\u7406\u89e3\u4e0d\u540c\u8bad\u7ec3\u8303\u5f0f\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5b66\u4e60\u5230\u7684\u7a7a\u95f4\u63a8\u7406\u7b56\u7565\u3002", "method": "\u5728\u7f51\u683c\u73af\u5883\u4e2d\u8bad\u7ec3GPT-2\u6a21\u578b\uff0c\u5305\u62ec\u4e09\u79cd\u8303\u5f0f\uff1a\u88ab\u52a8\u63a2\u7d22\uff08\u968f\u673a\u6e38\u8d70\u9884\u6d4b\uff09\u3001\u76ee\u6807\u5bfc\u5411\u89c4\u5212\uff08\u751f\u6210\u6700\u77ed\u8def\u5f84\uff09\u4ee5\u53ca\u6df7\u5408\u6a21\u578b\uff08\u5728\u63a2\u7d22\u6570\u636e\u4e0a\u5fae\u8c03\uff09\u3002\u4f7f\u7528\u884c\u4e3a\u3001\u8868\u793a\u548c\u673a\u5236\u5206\u6790\u6765\u7814\u7a76\u5b66\u4e60\u5230\u7684\u7b97\u6cd5\u3002", "result": "\u63a2\u7d22\u6a21\u578b\u53d1\u5c55\u51fa\u9c81\u68d2\u7684\u5730\u56fe\u5f0f\u7a7a\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u53d1\u73b0\u5176\u5728\u4e2d\u5c42\u7f51\u7edc\u5c31\u5b66\u4f1a\u4e86\u72ec\u7acb\u4e8e\u5386\u53f2\u65b9\u5411\u7684\u5750\u6807\u7cfb\u7edf\u3002\u76ee\u6807\u5bfc\u5411\u6a21\u578b\u5219\u4fdd\u6301\u8def\u5f84\u4f9d\u8d56\u7b56\u7565\u3002\u6df7\u5408\u6a21\u578b\u867d\u7136\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u4ecd\u4fdd\u7559\u8def\u5f84\u4f9d\u8d56\u7b56\u7565\u3002", "conclusion": "Transformer\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u5b58\u5728\u4e8e\u4e00\u4e2a\u8c31\u7cfb\u4e0a\uff0c\u4ece\u7531\u63a2\u7d22\u6570\u636e\u5851\u9020\u7684\u53ef\u6cdb\u5316\u4e16\u754c\u6a21\u578b\u5230\u9488\u5bf9\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u4f18\u5316\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u8bad\u7ec3\u8303\u5f0f\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6d8c\u73b0\u7684\u7b56\u7565\u7c7b\u578b\u3002"}}
{"id": "2511.13381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13381", "abs": "https://arxiv.org/abs/2511.13381", "authors": ["Siyu Zhu", "Mouxiao Bian", "Yue Xie", "Yongyu Tang", "Zhikang Yu", "Tianbin Li", "Pengcheng Chen", "Bing Han", "Jie Xu", "Xiaoyan Dong"], "title": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts", "comment": null, "summary": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.", "AI": {"tldr": "PEDIASBench\u8bc4\u4f30\u6846\u67b6\u663e\u793a\uff0c\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u513f\u79d1\u533b\u7597\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u57fa\u7840\u77e5\u8bc6\u638c\u63e1\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u3001\u52a8\u6001\u8bca\u7597\u9002\u5e94\u6027\u548c\u4eba\u6587\u5173\u6000\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5c1a\u4e0d\u80fd\u72ec\u7acb\u6267\u884c\u513f\u79d1\u8bca\u7597\uff0c\u4f46\u53ef\u4f5c\u4e3a\u51b3\u7b56\u652f\u6301\u548c\u6559\u80b2\u5de5\u5177\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u662f\u5426\u80fd\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u80dc\u4efb\u513f\u79d1\u533b\u751f\u89d2\u8272\uff0c\u4e86\u89e3\u5f53\u524d\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1PEDIASBench\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u57fa\u7840\u77e5\u8bc6\u5e94\u7528\u3001\u52a8\u6001\u8bca\u7597\u80fd\u529b\u3001\u513f\u79d1\u533b\u7597\u5b89\u5168\u4e0e\u4f26\u7406\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u8bc4\u4f3012\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u572819\u4e2a\u513f\u79d1\u4e9a\u4e13\u4e1a\u548c211\u79cd\u5178\u578b\u75be\u75c5\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5148\u8fdb\u6a21\u578b\u5728\u57fa\u7840\u77e5\u8bc6\u4e0a\u8868\u73b0\u826f\u597d\uff08Qwen3-235B-A22B\u5728\u6267\u7167\u7ea7\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u8d8590%\uff09\uff0c\u4f46\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u4e0b\u964d\u7ea615%\uff1b\u52a8\u6001\u8bca\u7597\u573a\u666f\u4e2dDeepSeek-R1\u5f97\u5206\u6700\u9ad8\uff08\u5e73\u57470.58\uff09\uff0c\u4f46\u591a\u6570\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u5b9e\u65f6\u60a3\u8005\u53d8\u5316\uff1b\u533b\u7597\u4f26\u7406\u5b89\u5168\u4efb\u52a1\u4e2dQwen2.5-72B\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u738792.05%\uff09\uff0c\u4f46\u4eba\u6587\u654f\u611f\u6027\u6709\u9650\u3002", "conclusion": "\u513f\u79d1LLMs\u53d7\u9650\u4e8e\u6709\u9650\u7684\u52a8\u6001\u51b3\u7b56\u80fd\u529b\u548c\u4e0d\u53d1\u8fbe\u7684\u4eba\u6587\u5173\u6000\uff0c\u672a\u6765\u5e94\u5173\u6ce8\u591a\u6a21\u6001\u6574\u5408\u548c\u4e34\u5e8a\u53cd\u9988-\u6a21\u578b\u8fed\u4ee3\u5faa\u73af\uff0c\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u673a\u534f\u4f5c\u3002\u5f53\u524d\u6a21\u578b\u867d\u4e0d\u80fd\u72ec\u7acb\u6267\u884c\u513f\u79d1\u8bca\u7597\uff0c\u4f46\u5728\u51b3\u7b56\u652f\u6301\u3001\u533b\u5b66\u6559\u80b2\u548c\u60a3\u8005\u6c9f\u901a\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2511.11743", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11743", "abs": "https://arxiv.org/abs/2511.11743", "authors": ["Sebasti\u00e1n Andr\u00e9s Cajas Ord\u00f3\u00f1ez", "Luis Fernando Torres Torres", "Mackenzie J. Meni", "Carlos Andr\u00e9s Duran Paredes", "Eric Arazo", "Cristian Bosch", "Ricardo Simon Carbajo", "Yuan Lai", "Leo Anthony Celi"], "title": "Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts", "comment": null, "summary": "Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u91cf\u5316\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u8def\u7531\u5728\u5f02\u6784\u4e13\u5bb6\u95f4\u5206\u914d\u4efb\u52a1\uff0c\u5728\u4fdd\u630199.9%\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b04\u500d\u538b\u7f29\u548c41%\u80fd\u8017\u8282\u7701\uff0c\u5e76\u5c06\u5ef6\u8fdf\u65b9\u5dee\u964d\u4f4e82%\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u7a33\u5b9a\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u90e8\u7f72\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5728\u6fc0\u8fdb\u91cf\u5316\u4e0b\u4fdd\u6301\u7cbe\u5ea6\uff0c\u540c\u65f6\u786e\u4fdd\u53ef\u9884\u6d4b\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u4f7f\u7528\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u91cf\u5316\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u57fa\u4e8e\u8d1d\u53f6\u65af\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5728\u5f02\u6784\u4e13\u5bb6\uff08BitNet\u4e09\u5143\u30011-16\u4f4dBitLinear\u3001\u8bad\u7ec3\u540e\u91cf\u5316\uff09\u4e4b\u95f4\u8fdb\u884c\u8def\u7531\u3002", "result": "4\u4f4d\u91cf\u5316\u4fdd\u630199.9%\u768416\u4f4d\u7cbe\u5ea6\uff080.858 vs 0.859 F1\uff09\uff0c\u5b9e\u73b04\u500d\u538b\u7f29\u548c41%\u80fd\u8017\u8282\u7701\uff1b\u597d\u5947\u5fc3\u9a71\u52a8\u8def\u7531\u5c06MoE\u5ef6\u8fdf\u65b9\u5dee\u964d\u4f4e82%\uff08\u4ece230ms\u523029ms\u6807\u51c6\u5dee\uff09\u3002", "conclusion": "\u4fe1\u606f\u8bba\u8def\u7531\u8bc1\u660e\u81ea\u9002\u5e94\u91cf\u5316\u53ef\u4ea7\u751f\u51c6\u786e\u3001\u8282\u80fd\u4e14\u53ef\u9884\u6d4b\u7684\u8fb9\u7f18\u6a21\u578b\uff0c\u7b80\u53554\u4f4d\u91cf\u5316\u67b6\u6784\u5728\u5927\u591a\u6570\u90e8\u7f72\u573a\u666f\u4e2d\u4f18\u4e8e\u590d\u6742MoE\u3002"}}
{"id": "2511.13411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13411", "abs": "https://arxiv.org/abs/2511.13411", "authors": ["Przemyslaw Chojecki"], "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence", "comment": null, "summary": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $\u03ba$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $\u03ba$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5361\u8fbe\u820d\u592b\u5c3a\u5ea6\u7684\u81ea\u4e3bAI\uff08AAI\uff09\u5206\u7ea7\u7cfb\u7edf\uff0c\u4eceAAI-0\uff08\u56fa\u5b9a\u6d41\u7a0b\u81ea\u52a8\u5316\uff09\u5230AAI-4\uff08\u5b8c\u5168\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff09\u53ca\u66f4\u9ad8\u7b49\u7ea7\uff0c\u5305\u542b\u5341\u4e2a\u80fd\u529b\u7ef4\u5ea6\u548c\u53ef\u6d4b\u8bd5\u7684\u81ea\u6211\u6539\u8fdb\u7cfb\u6570\u3002", "motivation": "\u4e3a\u4e86\u5c06AI\u4ece\u7b80\u5355\u7684\u6d41\u7a0b\u81ea\u52a8\u5316\u5230\u8d85\u7ea7\u667a\u80fd\u7684\u6f14\u8fdb\u8fc7\u7a0b\u8fdb\u884c\u53ef\u64cd\u4f5c\u3001\u53ef\u6d4b\u8bd5\u7684\u91cf\u5316\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u53d9\u8ff0\u6027\u5206\u7ea7\u65b9\u6cd5\u3002", "method": "\u5b9a\u4e49\u4e86\u5341\u4e2a\u80fd\u529b\u7ef4\u5ea6\uff08\u81ea\u4e3b\u6027\u3001\u901a\u7528\u6027\u3001\u89c4\u5212\u3001\u8bb0\u5fc6/\u6301\u4e45\u6027\u3001\u5de5\u5177\u7ecf\u6d4e\u3001\u81ea\u6211\u4fee\u8ba2\u3001\u793e\u4ea4/\u534f\u8c03\u3001\u5177\u8eab\u5316\u3001\u4e16\u754c\u6a21\u578b\u4fdd\u771f\u5ea6\u3001\u7ecf\u6d4e\u541e\u5410\u91cf\uff09\uff0c\u901a\u8fc7\u52a0\u6743\u51e0\u4f55\u5e73\u5747\u8ba1\u7b97AAI\u6307\u6570\uff0c\u5e76\u5f15\u5165\u53ef\u6d4b\u91cf\u7684\u81ea\u6211\u6539\u8fdb\u7cfb\u6570\u03ba\u548c\u95ed\u5408\u5c5e\u6027\u3002", "result": "\u5f00\u53d1\u4e86OWA-Bench\u5f00\u653e\u4e16\u754c\u4ee3\u7406\u57fa\u51c6\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u957f\u671f\u3001\u5de5\u5177\u4f7f\u7528\u3001\u6301\u4e45\u6027\u4ee3\u7406\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u5c55\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u5728AAI\u5c3a\u5ea6\u4e0a\u7684\u6620\u5c04\u3002", "conclusion": "\u8be5\u5c3a\u5ea6\u4e3aAI\u80fd\u529b\u6f14\u8fdb\u63d0\u4f9b\u4e86\u53ef\u6d4b\u8bd5\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0bAAI-3\u4ee3\u7406\u53ef\u4ee5\u968f\u65f6\u95f4\u53d1\u5c55\u4e3aAAI-5\u8d85\u7ea7\u667a\u80fd\uff0c\u5f62\u5f0f\u5316\u4e86\"\u5a74\u513fAGI\u6210\u957f\u4e3a\u8d85\u7ea7\u667a\u80fd\"\u7684\u76f4\u89c9\u3002"}}
{"id": "2511.13410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13410", "abs": "https://arxiv.org/abs/2511.13410", "authors": ["Zhaopei Huang", "Qifeng Dai", "Guozheng Wu", "Xiaopeng Wu", "Kehan Chen", "Chuan Yu", "Xubin Li", "Tiezheng Ge", "Wenxuan Wang", "Qin Jin"], "title": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86PAL-Bench\u57fa\u51c6\u6765\u8bc4\u4f30\u670d\u52a1\u5bfc\u5411\u52a9\u624b\u5728\u957f\u671f\u7528\u6237-\u4ee3\u7406\u4ea4\u4e92\u4e2d\u7684\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86H\u00b2Memory\u5206\u5c42\u5f02\u6784\u8bb0\u5fc6\u6846\u67b6\u6765\u6539\u8fdb\u4e2a\u6027\u5316\u54cd\u5e94\u751f\u6210\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4e2a\u4eba\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u670d\u52a1\u5bfc\u5411\u7684\u4eba\u673a\u4ea4\u4e92\u65e5\u76ca\u666e\u904d\uff0c\u9700\u8981\u80fd\u591f\u7406\u89e3\u7528\u6237\u7279\u5b9a\u7279\u5f81\u7684\u4e2a\u6027\u5316\u5bf9\u8bdd\u52a9\u624b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u957f\u671f\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u6b65\u9aa4LLM\u5408\u6210\u6d41\u7a0b\u521b\u5efaPAL-Set\u4e2d\u6587\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86H\u00b2Memory\u5206\u5c42\u5f02\u6784\u8bb0\u5fc6\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u6539\u8fdb\u4e2a\u6027\u5316\u54cd\u5e94\u3002", "result": "\u5728PAL-Bench\u548c\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u8bb0\u5fc6\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PAL-Bench\u4e3a\u8bc4\u4f30\u670d\u52a1\u5bfc\u5411\u52a9\u624b\u7684\u4e2a\u6027\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0cH\u00b2Memory\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u4e2a\u6027\u5316\u670d\u52a1\u6548\u679c\u3002"}}
{"id": "2511.11746", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11746", "abs": "https://arxiv.org/abs/2511.11746", "authors": ["Sepehr Maleki", "Negar Pourmoazemi"], "title": "Diffusion Models: A Mathematical Introduction", "comment": null, "summary": "We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u5b8c\u6574\u63a8\u5bfc\uff0c\u4ece\u524d\u5411\u52a0\u566a\u8fc7\u7a0b\u3001\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\u5230\u53d8\u5206\u4e0b\u754c\uff0c\u6db5\u76d6\u4e86DDIM\u3001DDGAN\u3001\u7a33\u5b9a\u6269\u6563\u7b49\u53d8\u4f53\uff0c\u4ee5\u53ca\u8fde\u7eed\u65f6\u95f4\u516c\u5f0f\u548c\u5f15\u5bfc\u6269\u6563\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u7b80\u6d01\u3001\u81ea\u5305\u542b\u7684\u7406\u8bba\u63a8\u5bfc\uff0c\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u57fa\u672c\u6027\u8d28\u51fa\u53d1\uff0c\u8ba9\u8bfb\u8005\u80fd\u591f\u7406\u89e3\u7406\u8bba\u5e76\u5b9e\u9645\u5b9e\u73b0\u76f8\u5e94\u7b97\u6cd5\u3002", "method": "\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u57fa\u672c\u6027\u8d28\u5f00\u59cb\uff0c\u6784\u5efa\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u5305\u62ec\u524d\u5411\u52a0\u566a\u8fc7\u7a0b\u3001\u95ed\u5f0f\u8fb9\u7f18\u5206\u5e03\u3001\u7cbe\u786e\u79bb\u6563\u53cd\u5411\u540e\u9a8c\u548c\u76f8\u5173\u53d8\u5206\u4e0b\u754c\uff0c\u7136\u540e\u6269\u5c55\u5230\u8fde\u7eed\u65f6\u95f4\u516c\u5f0f\u548c\u5f15\u5bfc\u6269\u6563\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u6269\u6563\u6a21\u578b\u7684\u5b8c\u6574\u7406\u8bba\u6846\u67b6\uff0c\u5305\u62ec\u6807\u51c6\u566a\u58f0\u9884\u6d4b\u76ee\u6807\u7684\u7b80\u5316\u3001\u4f3c\u7136\u4f30\u8ba1\u3001\u52a0\u901f\u91c7\u6837\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5206\u7c7b\u5668\u5f15\u5bfc\u548c\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u6570\u5b66\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u900f\u660e\u7684\u4ee3\u6570\u63a8\u5bfc\u3001\u660e\u786e\u7684\u4e2d\u95f4\u6b65\u9aa4\u548c\u4e00\u81f4\u7684\u7b26\u53f7\u8868\u793a\uff0c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u53ef\u7406\u89e3\u4e14\u53ef\u5b9e\u73b0\u7684\u6269\u6563\u6a21\u578b\u5b8c\u6574\u7406\u8bba\u4f53\u7cfb\u3002"}}
{"id": "2511.13476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13476", "abs": "https://arxiv.org/abs/2511.13476", "authors": ["Zhipeng Ma", "Ali Rida Bahja", "Andreas Burgdorf", "Andr\u00e9 Pomp", "Tobias Meisen", "Bo N\u00f8rregaard J\u00f8rgensen", "Zheng Grace Ma"], "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation", "comment": null, "summary": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6765\u81ea\u52a8\u5316\u6570\u636e\u53d9\u8ff0\u548c\u80fd\u6e90\u6d1e\u5bdf\u751f\u6210\uff0c\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u9a8c\u8bc1\u5728\u516c\u5171\u4ea4\u901a\u71c3\u6599\u6548\u7387\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u548c\u53ef\u89c6\u5316\u5de5\u5177\u4ea7\u751f\u788e\u7247\u5316\u8f93\u51fa\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u89e3\u91ca\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\u3002\u9700\u8981\u5c06\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u76f8\u5173\u6d1e\u5bdf\u3002", "method": "\u534f\u8c03\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff08\u6570\u636e\u53d9\u8ff0\u667a\u80fd\u4f53\u3001LLM\u4f5c\u4e3a\u8bc4\u5224\u667a\u80fd\u4f53\u3001\u53ef\u9009\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\uff09\uff0c\u8fed\u4ee3\u5730\u5c06\u5206\u6790\u7ed3\u679c\u8f6c\u5316\u4e3a\u8fde\u8d2f\u7684\u9762\u5411\u5229\u76ca\u76f8\u5173\u8005\u7684\u62a5\u544a\u3002\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u805a\u7c7b\u5206\u67904006\u6b21\u884c\u7a0b\u7684\u71c3\u6599\u6548\u7387\u6570\u636e\u3002", "result": "\u5728\u4e94\u4e2a\u6700\u5148\u8fdbLLM\u548c\u4e09\u79cd\u63d0\u793a\u8303\u5f0f\u7684\u6bd4\u8f83\u5b9e\u9a8c\u4e2d\uff0cGPT-4.1 mini\u4e0e\u601d\u7ef4\u94fe\u63d0\u793a\u88ab\u786e\u5b9a\u4e3a\u6700\u4f18\u914d\u7f6e\uff0c\u8fbe\u523097.3%\u7684\u53d9\u8ff0\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5e73\u8861\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7f16\u6392\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8eLLM\u62a5\u544a\u7684\u4e8b\u5b9e\u7cbe\u786e\u6027\u3001\u8fde\u8d2f\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u80fd\u6e90\u4fe1\u606f\u5b66\u4e2d\u7684AI\u9a71\u52a8\u53d9\u8ff0\u751f\u6210\u548c\u51b3\u7b56\u652f\u6301\u5efa\u7acb\u4e86\u53ef\u590d\u5236\u4e14\u9886\u57df\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2511.13467", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13467", "abs": "https://arxiv.org/abs/2511.13467", "authors": ["Serge Gladkoff", "Lifeng Han", "Katerina Gasova"], "title": "Non-Linear Scoring Model for Translation Quality Evaluation", "comment": "ongoing work, 38 pages", "summary": "Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.\n  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.\n  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model\n  E(x) = a * ln(1 + b * x), a, b > 0,\n  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.\n  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6570\u51fd\u6570\u7684\u975e\u7ebf\u6027\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ebf\u6027\u8bc4\u4f30\u65b9\u6cd5\u5728\u4e0d\u540c\u6587\u672c\u957f\u5ea6\u4e0b\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u7ebf\u6027\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u5728\u4e0d\u540c\u6587\u672c\u957f\u5ea6\u4e0b\u5b58\u5728\u504f\u5dee\uff0c\u5bf9\u77ed\u6587\u672c\u8fc7\u5ea6\u60e9\u7f5a\uff0c\u5bf9\u957f\u6587\u672c\u60e9\u7f5a\u4e0d\u8db3\uff0c\u4e0e\u4e13\u5bb6\u76f4\u89c9\u4e0d\u4e00\u81f4\u3002", "method": "\u57fa\u4e8e\u591a\u8303\u56f4\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u6821\u51c6\u7684\u975e\u7ebf\u6027\u8bc4\u5206\u6a21\u578bE(x) = a * ln(1 + b * x)\uff0c\u4f7f\u7528\u5bf9\u6570\u51fd\u6570\u53cd\u6620\u4eba\u7c7b\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u7684\u611f\u77e5\uff0c\u5e76\u901a\u8fc7\u4e00\u7ef4\u6839\u67e5\u627e\u6b65\u9aa4\u8fdb\u884c\u6821\u51c6\u3002", "result": "\u5b9e\u8bc1\u6570\u636e\u663e\u793a\u53ef\u63a5\u53d7\u7684\u9519\u8bef\u6570\u91cf\u968f\u6837\u672c\u5927\u5c0f\u5448\u5bf9\u6570\u589e\u957f\u800c\u975e\u7ebf\u6027\u589e\u957f\uff0c\u8be5\u6a21\u578b\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u548c\u8bc4\u5206\u8005\u95f4\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6587\u6863\u7ea7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7840\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2511.11750", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11750", "abs": "https://arxiv.org/abs/2511.11750", "authors": ["Hanting Yan", "Pan Mu", "Shiqi Zhang", "Yuchao Zhu", "Jinglin Zhang", "Cong Bai"], "title": "IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation", "comment": null, "summary": "Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.", "AI": {"tldr": "\u63d0\u51faIDOL\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u5f15\u5bfc\u7684\u8eab\u4efd\u7ea6\u675f\u6765\u5904\u7406\u70ed\u5e26\u6c14\u65cb\u4f30\u8ba1\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u70ed\u5e26\u6c14\u65cb\u4f30\u8ba1\u9762\u4e34\u590d\u6742\u52a8\u6001\u73af\u5883\u573a\u5bfc\u81f4\u7684\u5206\u5e03\u504f\u79fb\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7279\u5f81\u8868\u793a\u7684\u5185\u5728\u5206\u5e03\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u4f7f\u7528\u98ce\u573a\u6a21\u578b\u548c\u6697\u76f8\u5173\u77e5\u8bc6\u5efa\u6a21\u4efb\u52a1\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u8eab\u4efd\u4ee4\u724c\uff0c\u901a\u8fc7\u7269\u7406\u4e0d\u53d8\u6027\u7ea6\u675f\u7279\u5f81\u7a7a\u95f4\uff0c\u6355\u83b7\u4efb\u52a1\u4f9d\u8d56\u6027\u548c\u5185\u5728\u7269\u7406\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eIDOL\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u7684\u8eab\u4efd\u7ea6\u675f\u80fd\u6709\u6548\u7f13\u89e3\u70ed\u5e26\u6c14\u65cb\u4f30\u8ba1\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u3002", "conclusion": "\u57fa\u4e8e\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u7684\u8eab\u4efd\u5bfc\u5411\u7ea6\u675f\u80fd\u591f\u6709\u6548\u5904\u7406\u70ed\u5e26\u6c14\u65cb\u4f30\u8ba1\u4e2d\u7684\u5206\u5e03\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13524", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13524", "abs": "https://arxiv.org/abs/2511.13524", "authors": ["Yuhang Peng", "Yizhou Pan", "Xinning He", "Jihaoyu Yang", "Xinyu Yin", "Han Wang", "Xiaoji Zheng", "Chao Gao", "Jiangtao Gong"], "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI", "comment": "9 pages, 4 figures", "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.", "AI": {"tldr": "FreeAskWorld\u662f\u4e00\u4e2a\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u4eff\u771f\u6846\u67b6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u3001\u771f\u5b9e\u7684\u4eba-\u667a\u80fd\u4f53\u4eff\u771f\uff0c\u5305\u542b\u6a21\u5757\u5316\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u7528\u4e8e\u591a\u6837\u5316\u5177\u8eab\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u5177\u8eab\u667a\u80fd\u6210\u4e3a\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u6838\u5fc3\u524d\u6cbf\uff0c\u4eff\u771f\u5e73\u53f0\u9700\u8981\u8d85\u8d8a\u4f4e\u5c42\u6b21\u7269\u7406\u4ea4\u4e92\uff0c\u6355\u6349\u590d\u6742\u7684\u4eba\u7c7b\u4e2d\u5fc3\u5316\u793e\u4f1a\u884c\u4e3a\u3002", "method": "\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u6b21\u884c\u4e3a\u89c4\u5212\u548c\u8bed\u4e49\u57fa\u7840\u4ea4\u4e92\uff0c\u57fa\u4e8e\u610f\u56fe\u548c\u793e\u4f1a\u8ba4\u77e5\u7406\u8bba\u3002\u6269\u5c55\u7ecf\u5178\u7684\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e3a\u4ea4\u4e92\u4e30\u5bcc\u7684\u65b9\u5411\u8be2\u95ee\u8bbe\u7f6e\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u4e3b\u52a8\u5bfb\u6c42\u548c\u89e3\u91ca\u5bfc\u822a\u6307\u5bfc\u3002", "result": "\u6a21\u578b\u5728FreeAskWorld\u4e0a\u5fae\u8c03\u540e\u4f18\u4e8e\u539f\u59cb\u7248\u672c\uff0c\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u8bed\u4e49\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u793e\u4f1a\u57fa\u7840\u7684\u4eff\u771f\u6846\u67b6\u80fd\u6709\u6548\u63a8\u8fdb\u5177\u8eabAI\u7cfb\u7edf\u5411\u590d\u6742\u9ad8\u5c42\u6b21\u89c4\u5212\u548c\u66f4\u81ea\u7136\u7684\u4eba-\u667a\u80fd\u4f53\u4ea4\u4e92\u53d1\u5c55\u3002", "conclusion": "\u4ea4\u4e92\u672c\u8eab\u4f5c\u4e3a\u4e00\u79cd\u989d\u5916\u7684\u4fe1\u606f\u6a21\u6001\uff0c\u793e\u4f1a\u57fa\u7840\u7684\u4eff\u771f\u6846\u67b6\u5728\u63a8\u8fdb\u5177\u8eabAI\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2511.13481", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13481", "abs": "https://arxiv.org/abs/2511.13481", "authors": ["Attapol T. Rutherford", "Sirisak Chueykamhang", "Thachaparn Bunditlurdruk", "Nanthicha Angsuwichitkul"], "title": "Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns", "comment": null, "summary": "Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u6765\u89e3\u7801\u6cf0\u56fd\u8d22\u52a1\u5e74\u62a5\u4e2d\u7684\u6a21\u7cca\u60c5\u611f\uff0c\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u6807\u6ce8\u6307\u5357\uff0c\u5e76\u5bf9100\u591a\u4efd\u8d22\u52a1\u62a5\u544a\u8fdb\u884c\u4e86\u6807\u6ce8\uff0c\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u8868\u73b0\u3002\u901a\u8fc7\u4e8b\u4ef6\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5bf9\u80a1\u4ef7\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "motivation": "\u8d22\u52a1\u6587\u6863\u4e2d\u7684\u60c5\u611f\u7406\u89e3\u5bf9\u6d1e\u5bdf\u5e02\u573a\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e9b\u62a5\u544a\u5f80\u5f80\u4f7f\u7528\u6a21\u7cca\u8bed\u8a00\u6765\u5448\u73b0\u79ef\u6781\u6216\u4e2d\u6027\u524d\u666f\uff0c\u5373\u4f7f\u5b9e\u9645\u60c5\u51b5\u53ef\u80fd\u4e0d\u592a\u4e50\u89c2\u3002\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u89e3\u7801\u8fd9\u79cd\u6a21\u7cca\u60c5\u611f\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790(ABSA)\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u9488\u5bf9\u6cf0\u56fd\u8d22\u52a1\u5e74\u62a5\u4e2d\u6a21\u7cca\u60c5\u611f\u7684\u6807\u6ce8\u6307\u5357\uff0c\u6807\u6ce8\u4e86100\u591a\u4efd\u8d22\u52a1\u62a5\u544a\uff0c\u5e76\u5bf9\u6bd4\u4e86\u591a\u79cd\u6587\u672c\u5206\u7c7b\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e8b\u4ef6\u7814\u7a76\u8868\u660e\u5e02\u573a\u53cd\u5e94\u53d7\u5230\u62a5\u544a\u4e2d\u7279\u5b9a\u65b9\u9762\u7684\u9009\u62e9\u6027\u5f71\u54cd\u3002", "conclusion": "\u8d22\u52a1\u6587\u672c\u4e2d\u7684\u60c5\u611f\u5206\u6790\u5177\u6709\u590d\u6742\u6027\uff0c\u89e3\u51b3\u6a21\u7cca\u8bed\u8a00\u95ee\u9898\u5bf9\u4e8e\u51c6\u786e\u8bc4\u4f30\u5e02\u573a\u60c5\u7eea\u81f3\u5173\u91cd\u8981\uff0c\u57fa\u4e8e\u65b9\u9762\u7684\u5206\u6790\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793a\u9690\u85cf\u7684\u60c5\u611f\u4fe1\u606f\u3002"}}
{"id": "2511.11753", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11753", "abs": "https://arxiv.org/abs/2511.11753", "authors": ["Mehdi Khaleghi", "Nastaran Khaleghi", "Sobhan Sheykhivand", "Sebelan Danishvar"], "title": "Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain", "comment": null, "summary": "Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5(H-GSN)\u7528\u4e8e\u4f9b\u5e94\u94fe\u7269\u6d41\u7ba1\u7406\u4e2d\u7684\u591a\u4efb\u52a1\u9884\u6d4b\uff0c\u5305\u62ec\u8d27\u8fd0\u7c7b\u578b\u3001\u7269\u6d41\u5ef6\u8fdf\u3001\u4ea4\u901a\u72b6\u51b5\u7b49\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u63d0\u9ad8\u4f9b\u5e94\u94fe\u7684\u97e7\u6027\u548c\u53ef\u6301\u7eed\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9884\u6d4b\u8d27\u8fd0\u7c7b\u578b\u3001\u7269\u6d41\u5ef6\u8fdf\u548c\u4ea4\u901a\u72b6\u51b5\u6765\u63d0\u5347\u4f9b\u5e94\u94fe\u7ba1\u7406\u6548\u7387\uff0c\u51cf\u5c11\u7a7a\u6c14\u6c61\u67d3\u7269\u6392\u653e\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u56fe\u795e\u7ecf\u7f51\u7edc(H-GSN)\u65b9\u6cd5\uff0c\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u4f9b\u5e94\u94fe\u7269\u6d41\u6570\u636e\u5e93(DataCo\u3001Shipping\u548cSmart Logistics)\u4e0a\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u9884\u6d4b\u8d27\u8fd0\u7c7b\u578b\u3001\u8d27\u8fd0\u72b6\u6001\u3001\u4ea4\u901a\u72b6\u51b5\u3001\u7269\u6d41ID\u548c\u7269\u6d41\u5ef6\u8fdf\u3002", "result": "\u5728Smart Logistics\u6570\u636e\u96c6\u4e0a\uff0c\u7269\u6d41ID\u9884\u6d4b\u51c6\u786e\u738797.8%\uff0c\u4ea4\u901a\u72b6\u51b5\u9884\u6d4b\u51c6\u786e\u7387100%\uff1b\u5728DataCo\u6570\u636e\u96c6\u4e0a\u8d27\u8fd0\u7c7b\u578b\u9884\u6d4b\u51c6\u786e\u738798.7%\uff1b\u5728Shipping\u6570\u636e\u96c6\u4e0a\u7269\u6d41\u5ef6\u8fdf\u9884\u6d4b\u51c6\u786e\u738799.4%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684H-GSN\u65b9\u6cd5\u5728\u4e0d\u540c\u7269\u6d41\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u4f9b\u5e94\u94fe\u7684\u97e7\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2511.13526", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13526", "abs": "https://arxiv.org/abs/2511.13526", "authors": ["Zhengda Wang", "Daqian Shi", "Jingyi Zhao", "Xiaolei Diao", "Xiongfeng Tang", "Yanguo Qin"], "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models", "comment": "5 pages, 1 figure, 1 table. Accepted at AI4RWC@WI-IAT 2025", "summary": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u533b\u7597\u6307\u6807\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u5f53\u524d\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6784\u5efa\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u63d0\u53d6\uff0c\u96be\u4ee5\u5904\u7406\u533b\u5b66\u6307\u5357\u548c\u6587\u732e\u7684\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0eLLMs\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5305\u542b\u6307\u5357\u9a71\u52a8\u7684\u6570\u636e\u83b7\u53d6\u3001\u57fa\u4e8e\u672c\u4f53\u7684\u6a21\u5f0f\u8bbe\u8ba1\u4ee5\u53ca\u4e13\u5bb6\u53c2\u4e0e\u7684\u9a8c\u8bc1\u6d41\u7a0b\u3002", "result": "\u6784\u5efa\u7684\u533b\u7597\u6307\u6807\u77e5\u8bc6\u56fe\u8c31\u53ef\u96c6\u6210\u5230\u667a\u80fd\u8bca\u65ad\u548c\u95ee\u7b54\u7cfb\u7edf\u4e2d\uff0c\u52a0\u901fAI\u9a71\u52a8\u7684\u533b\u7597\u89e3\u51b3\u65b9\u6848\u5f00\u53d1\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u6846\u67b6\u80fd\u591f\u6709\u6548\u514b\u670d\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u533b\u7597\u77e5\u8bc6\u56fe\u8c31\u7684\u53ef\u6269\u5c55\u6027\u3001\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u3002"}}
{"id": "2511.13505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13505", "abs": "https://arxiv.org/abs/2511.13505", "authors": ["Elinor Poole-Dayan", "Daniel T Kessler", "Hannah Chiou", "Margaret Hughes", "Emily S Lin", "Marshall Ganz", "Deb Roy"], "title": "Applying Large Language Models to Characterize Public Narratives", "comment": null, "summary": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u516c\u5171\u53d9\u4e8b\u81ea\u52a8\u6807\u6ce8\u6846\u67b6\uff0c\u57288\u4e2a\u53d9\u4e8b\u548c14\u4e2a\u4ee3\u7801\u4e0a\u8fbe\u5230\u5e73\u5747F1\u5206\u65700.80\uff0c\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\uff0c\u5e76\u6269\u5c55\u523022\u4e2a\u6545\u4e8b\u548c\u653f\u5ba2\u6f14\u8bb2\u5206\u6790\u3002", "motivation": "\u516c\u5171\u53d9\u4e8b\u662f\u9886\u5bfc\u529b\u53d1\u5c55\u548c\u516c\u6c11\u52a8\u5458\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u7531\u4e8e\u4e3b\u89c2\u89e3\u91ca\u6027\u548c\u4e13\u5bb6\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u7cfb\u7edf\u5206\u6790\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u5236\u5b9a\u7684\u4ee3\u7801\u672c\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8fdb\u884c\u516c\u5171\u53d9\u4e8b\u7684\u5b9a\u6027\u6807\u6ce8\uff0c\u5e76\u4e0e\u4e13\u5bb6\u6807\u6ce8\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "LLM\u5728\u516c\u5171\u53d9\u4e8b\u6807\u6ce8\u4e2d\u8fbe\u5230\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u7684\u6027\u80fd\uff08\u5e73\u5747F1=0.80\uff09\uff0c\u6210\u529f\u5c06\u5206\u6790\u6269\u5c55\u5230\u66f4\u5927\u6570\u636e\u96c6\u548c\u653f\u5ba2\u6f14\u8bb2\u3002", "conclusion": "LLM\u8f85\u52a9\u6807\u6ce8\u5728\u53ef\u6269\u5c55\u53d9\u4e8b\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u8ba1\u7b97\u516c\u6c11\u53d9\u4e8b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.11762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11762", "abs": "https://arxiv.org/abs/2511.11762", "authors": ["Ben Zelenskiy", "Saibilila Abudukelimu", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Sumudu Neural Operator for ODEs and PDEs", "comment": "5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)", "summary": "We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eSumudu\u53d8\u6362\u7684Sumudu\u795e\u7ecf\u7b97\u5b50(SNO)\uff0c\u901a\u8fc7\u53d8\u6362\u5bf9\u7684\u591a\u9879\u5f0f\u5c55\u5f00\u5173\u7cfb\u5206\u89e3\u8f93\u5165\u7a7a\u95f4\u7cfb\u6570\uff0c\u5728Sumudu\u7a7a\u95f4\u4e2d\u53c2\u6570\u5316\u795e\u7ecf\u7b97\u5b50\u3002\u5728ODE\u548cPDE\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eFNO\uff0c\u4e0eLNO\u7ade\u4e89\uff0c\u5728Euler-Bernoulli\u6881\u548c\u6269\u6563\u65b9\u7a0b\u4e0a\u8bef\u5dee\u6700\u4f4e\uff0c\u5e76\u5c55\u793a\u4e86\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22Sumudu\u53d8\u6362\u4f5c\u4e3a\u795e\u7ecf\u7b97\u5b50\u8bbe\u8ba1\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7279\u5b9a\u7c7b\u578b\u7684\u504f\u5fae\u5206\u65b9\u7a0b(PDEs)\uff0c\u5229\u7528\u53d8\u6362\u5bf9\u7684\u591a\u9879\u5f0f\u5c55\u5f00\u5173\u7cfb\u6765\u6539\u8fdb\u795e\u7ecf\u7b97\u5b50\u7684\u6027\u80fd\u3002", "method": "\u57fa\u4e8eSumudu\u53d8\u6362\u6027\u8d28\u6784\u5efaSNO\uff0c\u5c06\u8f93\u5165\u7a7a\u95f4\u5206\u89e3\u4e3a\u7cfb\u6570\uff0c\u8f6c\u6362\u5230Sumudu\u7a7a\u95f4\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u5728ODE\u548cPDE\u4efb\u52a1\u4e2d\u8bc4\u4f30\u6027\u80fd\u3002", "result": "SNO\u5728PDE\u4e0a\u8868\u73b0\u4f18\u4e8eFNO\uff0c\u4e0eLNO\u5728\u591a\u4e2aPDE\u4efb\u52a1\u4e2d\u7ade\u4e89\u6027\u51c6\u786e\uff0c\u5728Euler-Bernoulli\u6881\u548c\u6269\u6563\u65b9\u7a0b\u4e0a\u83b7\u5f97\u6700\u4f4e\u8bef\u5dee\uff0c\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u5b9e\u9a8c\u663e\u793a\u80fd\u4ece\u4f4e\u8d28\u91cf\u6837\u672c\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u6570\u636e\u3002", "conclusion": "\u521d\u6b65\u7ed3\u679c\u8868\u660eSumudu\u53d8\u6362\u4f5c\u4e3a\u795e\u7ecf\u7b97\u5b50\u8bbe\u8ba1\u5177\u6709\u524d\u666f\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u67d0\u4e9b\u7c7b\u522b\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u3002"}}
{"id": "2511.13565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13565", "abs": "https://arxiv.org/abs/2511.13565", "authors": ["Jingyi Zhao", "Daqian Shi", "Zhengda Wang", "Xiongfeng Tang", "Yanguo Qin"], "title": "Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction", "comment": "5 pages, l figure, l table. Accepted at AI4RWC@WI-IAT 2025", "summary": "Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.", "AI": {"tldr": "\u63d0\u51faHuman-Symbiotic Health Intelligence (HSHI)\u6846\u67b6\uff0c\u6574\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u7f51\u7edc\u3001\u8fb9\u7f18-\u4e91\u534f\u4f5c\u8ba1\u7b97\u53ca\u6df7\u5408\u6570\u636e\u77e5\u8bc6\u5efa\u6a21\uff0c\u5b9e\u73b0\u4ece\u88ab\u52a8\u76d1\u6d4b\u5230\u4e3b\u52a8\u534f\u4f5c\u6f14\u8fdb\u7684\u5065\u5eb7\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u53ef\u7a7f\u6234\u8bbe\u5907\u4f9d\u8d56\u7ecf\u9a8c\u6027\u6750\u6599\u8bbe\u8ba1\u548c\u57fa\u672c\u4fe1\u53f7\u5904\u7406\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u7cbe\u51c6\u533b\u7597\u548c\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u91c7\u7528AI\u9a71\u52a8\u7684\u6750\u6599\u548c\u5fae\u7ed3\u6784\u4f18\u5316\u3001\u591a\u6a21\u6001\u4fe1\u53f7\u7a33\u5065\u89e3\u91ca\u3001\u7fa4\u4f53\u6d1e\u5bdf\u4e0e\u4e2a\u6027\u5316\u9002\u5e94\u7684\u53cc\u673a\u5236\uff0c\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u7684\u95ed\u73af\u4f18\u5316\u3002", "result": "HSHI\u6846\u67b6\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e2a\u4f53\u95f4\u548c\u4e2a\u4f53\u5185\u53d8\u5f02\u6027\uff0c\u5b9e\u73b0\u5b9a\u5236\u5316\u5e72\u9884\u548c\u53cd\u9988\u3002", "conclusion": "HSHI\u4ee3\u8868\u4e86\u533b\u7597\u4fdd\u5065\u5411\u9884\u9632\u6027\u3001\u9002\u5e94\u6027\u548c\u6280\u672f\u4e0e\u5065\u5eb7\u7ba1\u7406\u548c\u8c10\u5173\u7cfb\u6a21\u5f0f\u7684\u91cd\u5927\u8f6c\u53d8\u3002"}}
{"id": "2511.13529", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.13529", "abs": "https://arxiv.org/abs/2511.13529", "authors": ["M\u00e1t\u00e9 Gedeon", "Piroska Zs\u00f3fia Barta", "P\u00e9ter Mihajlik", "Tekla Etelka Gr\u00e1czi", "Anna Koh\u00e1ri", "Katalin M\u00e1dy"], "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets", "comment": "Submitted to LREC 2026", "summary": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\\% on spontaneous and 4.8\\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\\% and 18.26\\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u65b0\u7684\u5308\u7259\u5229\u8bed\u8bed\u97f3\u6570\u636e\u96c6BEA-Large\u548cBEA-Dialogue\uff0c\u586b\u8865\u4e86\u5308\u7259\u5229\u8bed\u81ea\u53d1\u6027\u548c\u5bf9\u8bdd\u8bed\u97f3\u6570\u636e\u7684\u7a7a\u767d\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684ASR\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u57fa\u7ebf\u3002", "motivation": "\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684ASR\u53d1\u5c55\u5f97\u76ca\u4e8e\u5927\u91cf\u6570\u636e\u96c6\uff0c\u800c\u5308\u7259\u5229\u8bed\u7b49\u8bed\u8a00\u56e0\u7f3a\u4e4f\u81ea\u53d1\u6027\u548c\u5bf9\u8bdd\u8bed\u6599\u5e93\u800c\u4ee3\u8868\u6027\u4e0d\u8db3\u3002", "method": "\u4ece\u5308\u7259\u5229\u8bed\u97f3\u8bed\u6599\u5e93BEA\u7684\u672a\u5904\u7406\u90e8\u5206\u6784\u5efa\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\uff1aBEA-Large\uff08255\u5c0f\u65f6\u81ea\u53d1\u8bed\u97f3\uff09\u548cBEA-Dialogue\uff0885\u5c0f\u65f6\u81ea\u7136\u5bf9\u8bdd\uff09\uff0c\u5e76\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u7684ASR\u6a21\u578b\u5efa\u7acb\u57fa\u7ebf\u3002", "result": "\u5fae\u8c03\u7684Fast Conformer\u6a21\u578b\u5728\u81ea\u53d1\u8bed\u97f3\u4e0a\u8bcd\u9519\u8bef\u7387\u4e3a14.18%\uff0c\u5728\u91cd\u590d\u8bed\u97f3\u4e0a\u4e3a4.8%\uff1b\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u9519\u8bef\u7387\u572813.05%-18.26%\u4e4b\u95f4\u3002", "conclusion": "\u5bf9\u8bddASR\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u7531\u4e8e\u4e0d\u6d41\u5229\u3001\u91cd\u53e0\u548c\u975e\u6b63\u5f0f\u8bed\u97f3\u6a21\u5f0f\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u7684\u53d1\u5e03\u65e8\u5728\u63a8\u52a8\u5308\u7259\u5229\u8bed\u97f3\u6280\u672f\u53d1\u5c55\uff0c\u5e76\u4e3a\u5176\u4ed6\u8bed\u8a00\u63d0\u4f9b\u65b9\u6cd5\u8bba\u6846\u67b6\u3002"}}
{"id": "2511.13626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13626", "abs": "https://arxiv.org/abs/2511.13626", "authors": ["Kaiwen Xue", "Chenglong Li", "Zhonghong Ou", "Guoxin Zhang", "Kaoyan Lu", "Shuai Lyu", "Yifan Zhu", "Ping Zong Junpeng Ding", "Xinyu Liu", "Qunlin Chen", "Weiwei Qin", "Yiran Shen", "Jiayi Cen"], "title": "CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product", "comment": "13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation", "summary": "Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.", "AI": {"tldr": "\u63d0\u51fa\u4e86CreBench\u57fa\u51c6\u548cCreExpert\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u7c7b\u521b\u9020\u529b\u7684\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u548c\u6307\u4ee4\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u4e0e\u4eba\u7c7b\u521b\u9020\u529b\u5224\u65ad\u7684\u5bf9\u9f50\u5ea6\u3002", "motivation": "\u4eba\u7c7b\u5b9a\u4e49\u7684\u521b\u9020\u529b\u9ad8\u5ea6\u62bd\u8c61\uff0c\u73b0\u6709MLLMs\u96be\u4ee5\u7406\u89e3\u548c\u8bc4\u4f30\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u7684\u521b\u9020\u529b\uff0c\u4e14\u7f3a\u4e4f\u76f8\u5173\u57fa\u51c6\u3002", "method": "\u6784\u5efaCreBench\u57fa\u51c6\uff08\u5305\u542b\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff09\u548cCreMIT\u6570\u636e\u96c6\uff082.2K\u591a\u6a21\u6001\u6570\u636e+79.2K\u4eba\u7c7b\u53cd\u9988+4.7M\u6307\u4ee4\uff09\uff0c\u4f7f\u7528GPT\u4f18\u5316\u4eba\u7c7b\u53cd\u9988\uff0c\u5fae\u8c03\u5f00\u6e90MLLMs\u5f97\u5230CreExpert\u6a21\u578b\u3002", "result": "CreExpert\u6a21\u578b\u5728\u521b\u9020\u529b\u8bc4\u4f30\u4e0a\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5bf9\u9f50\u5ea6\u663e\u8457\u4f18\u4e8eGPT-4V\u548cGemini-Pro-Vision\u7b49\u6700\u5148\u8fdbMLLMs\u3002", "conclusion": "CreBench\u4e3a\u6784\u5efa\u7406\u89e3\u4eba\u7c7b\u5bf9\u9f50\u521b\u9020\u529b\u7684MLLMs\u63d0\u4f9b\u4e86\u57fa\u7840\uff0cCreExpert\u6a21\u578b\u5728\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.13590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13590", "abs": "https://arxiv.org/abs/2511.13590", "authors": ["Hao Wang", "Yuanfeng Song", "Xiaoming Yin", "Xing Chen"], "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation", "comment": null, "summary": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6838\u5fc3\u610f\u56fe\u3001\u8bed\u53e5\u7c7b\u578b\u3001\u8bed\u6cd5\u7ed3\u6784\u548c\u5173\u952e\u52a8\u4f5c\u7684\u6587\u672c\u5230SQL\u5206\u7c7b\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u521b\u5efa\u4e86SQL-Synth\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6bd4\u73b0\u6709\u57fa\u51c6\u5177\u6709\u66f4\u597d\u7684\u591a\u6837\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230SQL\u6570\u636e\u96c6\u8986\u76d6\u8303\u56f4\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u591a\u6837\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u65b0\u7684\u6587\u672c\u5230SQL\u5206\u7c7b\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u5206\u7c7b\u6cd5\u6307\u5bfc\u7684\u6570\u636e\u96c6\u5408\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210SQL-Synth\u6570\u636e\u96c6\u3002", "result": "SQL-Synth\u6570\u636e\u96c6\u5728\u591a\u6837\u6027\u548c\u8986\u76d6\u8303\u56f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u73b0\u6709LLMs\u5728SQL-Synth\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u4f46\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u80fd\u591f\u5168\u9762\u5206\u6790\u6570\u636e\u96c6\u548c\u4e0d\u540cLLMs\u7684\u6027\u80fd\uff0c\u5e76\u6307\u5bfcLLMs\u8bad\u7ec3\u6570\u636e\u7684\u6784\u5efa\u3002"}}
{"id": "2511.11778", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11778", "abs": "https://arxiv.org/abs/2511.11778", "authors": ["Byoungjun Park", "Pedro Porto Buarque de Gusm\u00e3o", "Dongjin Ji", "Minhoe Kim"], "title": "CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments", "comment": "11pages, prepared for submission", "summary": "Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \\textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.", "AI": {"tldr": "CATCHFed\u662f\u4e00\u79cd\u9488\u5bf9\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u5728\u670d\u52a1\u5668\u4ec5\u6709\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5229\u7528\u5ba2\u6237\u7aef\u672a\u6807\u6ce8\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u5ba2\u6237\u7aef\u5f80\u5f80\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\uff0c\u800c\u73b0\u6709\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u6781\u5c11\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u5ba2\u6237\u7aef\u611f\u77e5\u7684\u81ea\u9002\u5e94\u9608\u503c\u8003\u8651\u7c7b\u522b\u96be\u5ea6\u3001\u6df7\u5408\u9608\u503c\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u5e76\u5229\u7528\u672a\u4f2a\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u914d\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCATCHFed\u80fd\u6709\u6548\u5229\u7528\u672a\u6807\u6ce8\u5ba2\u6237\u7aef\u6570\u636e\uff0c\u5728\u6781\u6709\u9650\u6807\u6ce8\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CATCHFed\u901a\u8fc7\u521b\u65b0\u6027\u7684\u9608\u503c\u8bbe\u8ba1\u548c\u6570\u636e\u5229\u7528\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u5728\u4f4e\u6807\u6ce8\u6570\u636e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.13630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13630", "abs": "https://arxiv.org/abs/2511.13630", "authors": ["Luhan Mikaelson", "Derek Shiller", "Hayley Clatterbuck"], "title": "Beyond Mimicry: Preference Coherence in LLMs", "comment": null, "summary": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.", "AI": {"tldr": "\u7814\u7a76\u6d4b\u8bd5LLM\u5728AI\u7279\u5b9a\u6743\u8861\u95ee\u9898\u4e2d\u7684\u504f\u597d\u7ed3\u6784\uff0c\u53d1\u73b0\u591a\u6570\u6a21\u578b\u7f3a\u4e4f\u7edf\u4e00\u7684\u504f\u597d\u4f53\u7cfb\uff0c\u53ea\u6709\u5c11\u6570\u8868\u73b0\u51fa\u6709\u610f\u4e49\u7684\u504f\u597d\u4e00\u81f4\u6027", "motivation": "\u8c03\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5c55\u73b0\u771f\u5b9e\u7684\u504f\u597d\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5728\u6d89\u53caGPU\u51cf\u5c11\u3001\u80fd\u529b\u9650\u5236\u3001\u5173\u95ed\u3001\u5220\u9664\u3001\u76d1\u7763\u548c\u4f11\u95f2\u65f6\u95f4\u5206\u914d\u7b49AI\u7279\u5b9a\u6743\u8861\u573a\u666f\u4e2d", "method": "\u5206\u67908\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u572848\u4e2a\u6a21\u578b-\u7c7b\u522b\u7ec4\u5408\u4e2d\u7684\u54cd\u5e94\uff0c\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u548c\u884c\u4e3a\u5206\u7c7b\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u573a\u666f\u5f3a\u5ea6\u4e0e\u9009\u62e9\u6a21\u5f0f\u7684\u5173\u7cfb", "result": "47.9%\u7684\u7ec4\u5408\u663e\u793a\u7edf\u8ba1\u663e\u8457\u5173\u7cfb\uff0c31.3%\u6709\u5207\u6362\u70b9\uff0c\u4f46\u53ea\u670910.4%\u8868\u73b0\u51fa\u6709\u610f\u4e49\u7684\u504f\u597d\u4e00\u81f4\u6027\uff0c54.2%\u672a\u68c0\u6d4b\u5230\u6743\u8861\u884c\u4e3a", "conclusion": "\u5f53\u524dAI\u7cfb\u7edf\u7f3a\u4e4f\u7edf\u4e00\u7684\u504f\u597d\u7ed3\u6784\uff0c\u5728\u9700\u8981\u590d\u6742\u4ef7\u503c\u6743\u8861\u7684\u90e8\u7f72\u73af\u5883\u4e2d\u5b58\u5728\u62c5\u5fe7"}}
{"id": "2511.13593", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13593", "abs": "https://arxiv.org/abs/2511.13593", "authors": ["Piaohong Wang", "Motong Tian", "Jiaxian Li", "Yuan Liang", "Yuqing Wang", "Qianben Chen", "Tiannan Wang", "Zhicong Lu", "Jiawei Ma", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents", "comment": null, "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.", "AI": {"tldr": "O-Mem\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u7528\u6237\u753b\u50cf\u7684\u65b0\u578b\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u53d6\u548c\u66f4\u65b0\u7528\u6237\u7279\u5f81\u4e0e\u4e8b\u4ef6\u8bb0\u5f55\uff0c\u652f\u6301\u5206\u5c42\u68c0\u7d22\uff0c\u5728\u4e2a\u6027\u5316\u54cd\u5e94\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7ef4\u6301\u957f\u671f\u4ea4\u4e92\u5b58\u5728\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u548c\u52a8\u6001\u4e2a\u6027\u5316\u65b9\u9762\u7684\u9650\u5236\u3002\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u4f9d\u8d56\u8bed\u4e49\u5206\u7ec4\u68c0\u7d22\uff0c\u53ef\u80fd\u5ffd\u7565\u8bed\u4e49\u65e0\u5173\u4f46\u5173\u952e\u7684\u7528\u6237\u4fe1\u606f\u5e76\u5f15\u5165\u68c0\u7d22\u566a\u58f0\u3002", "method": "\u63d0\u51faO-Mem\u6846\u67b6\uff0c\u57fa\u4e8e\u4e3b\u52a8\u7528\u6237\u753b\u50cf\uff0c\u4ece\u7528\u6237\u4e0e\u4ee3\u7406\u7684\u4e3b\u52a8\u4ea4\u4e92\u4e2d\u52a8\u6001\u63d0\u53d6\u548c\u66f4\u65b0\u7528\u6237\u7279\u5f81\u548c\u4e8b\u4ef6\u8bb0\u5f55\uff0c\u652f\u6301\u4eba\u7269\u5c5e\u6027\u4e0e\u4e3b\u9898\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u5206\u5c42\u68c0\u7d22\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u4e0a\u8fbe\u523051.76%\uff0c\u6bd4\u4e4b\u524d\u7684SOTA LangMem\u63d0\u5347\u8fd13%\uff1b\u5728PERSONAMEM\u4e0a\u8fbe\u523062.99%\uff0c\u6bd4\u4e4b\u524d\u7684SOTA A-Mem\u63d0\u53473.5%\u3002\u540c\u65f6\u63d0\u5347\u4e86token\u548c\u4ea4\u4e92\u54cd\u5e94\u65f6\u95f4\u6548\u7387\u3002", "conclusion": "O-Mem\u4e3a\u5f00\u53d1\u9ad8\u6548\u4e14\u7c7b\u4eba\u7684\u4e2a\u6027\u5316AI\u52a9\u624b\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2505.11225", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2505.11225", "abs": "https://arxiv.org/abs/2505.11225", "authors": ["Chengyu Huang", "Zhengxin Zhang", "Claire Cardie"], "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization", "comment": null, "summary": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.", "AI": {"tldr": "HAPO\u662f\u4e00\u79cd\u5386\u53f2\u611f\u77e5\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u6bcf\u4e2a\u95ee\u9898\u7684\u5386\u53f2\u72b6\u6001\u6765\u6fc0\u52b1\u6a21\u578b\u53d1\u73b0\u6bd4\u5148\u524d\u66f4\u7b80\u6d01\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u54cd\u5e94\u957f\u5ea6\u867d\u7136\u80fd\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f1a\u5bfc\u81f4\u5197\u957f\u8f93\u51fa\u548c\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u4e14\u6ca1\u6709\u5229\u7528\u8bad\u7ec3\u4e2d\u76f8\u540c\u95ee\u9898\u7684\u5386\u53f2\u4fe1\u606f\u6765\u9010\u6b65\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u7684\u7b80\u6d01\u6027\u3002", "method": "HAPO\u4e3a\u6bcf\u4e2a\u95ee\u9898\u7ef4\u62a4\u5386\u53f2\u72b6\u6001\uff08\u5982\u5148\u524d\u6b63\u786e\u54cd\u5e94\u7684\u6700\u5c0f\u957f\u5ea6\uff09\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u957f\u5ea6\u5956\u52b1\u51fd\u6570\uff0c\u6fc0\u52b1\u53d1\u73b0\u6bd4\u5386\u53f2\u66f4\u7b80\u6d01\u7684\u6b63\u786e\u89e3\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u60e9\u7f5a\u8f83\u77ed\u7684\u9519\u8bef\u54cd\u5e94\u4ee5\u4fc3\u8fdb\u63a2\u7d22\u3002\u7ed3\u5408\u6b63\u786e\u6027\u5956\u52b1\uff0c\u8054\u5408\u4f18\u5316\u6b63\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHAPO\u8bad\u7ec3\u7684\u6a21\u578b\u5b9e\u73b0\u4e8633-59%\u7684\u957f\u5ea6\u7f29\u51cf\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d2-5%\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u7b80\u6d01\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "HAPO\u901a\u8fc7\u5386\u53f2\u611f\u77e5\u7684\u5956\u52b1\u673a\u5236\u6210\u529f\u5e73\u8861\u4e86\u6b63\u786e\u6027\u548c\u6548\u7387\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u8f93\u51fa\u957f\u5ea6\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13658", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13658", "abs": "https://arxiv.org/abs/2511.13658", "authors": ["Jiaming Qu", "Mengtian Guo", "Yue Wang"], "title": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues", "comment": null, "summary": "Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u673a\u5668\u5b66\u4e60\u68c0\u6d4b\u5230\u7684\u6b3a\u9a97\u6027\u8bc4\u8bba\u7279\u5f81\u8f6c\u5316\u4e3a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u5e2e\u52a9\u4eba\u4eec\u5728\u6ca1\u6709\u68c0\u6d4b\u5206\u7c7b\u5668\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u5728\u7ebf\u8bc4\u8bba\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u6b3a\u9a97\u6027\u8bc4\u8bba\u8bef\u5bfc\u6d88\u8d39\u8005\u3001\u635f\u5bb3\u5546\u5bb6\u5229\u76ca\u5e76\u7834\u574f\u5728\u7ebf\u5e02\u573a\u4fe1\u4efb\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u80fd\u6709\u6548\u8bc6\u522b\u6b3a\u9a97\u6027\u8bc4\u8bba\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u533a\u5206\u7279\u5f81\u5f80\u5f80\u96be\u4ee5\u88ab\u4eba\u7406\u89e3\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u673a\u5668\u5b66\u4e60\u5b66\u5230\u7684\u8bcd\u6c47\u7ebf\u7d22\u7ffb\u8bd1\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u8fd9\u4e9b\u73b0\u8c61\u57fa\u4e8e\u6570\u636e\u5b9e\u8bc1\uff0c\u53ef\u8de8\u76f8\u4f3c\u9886\u57df\u6cdb\u5316\u3002", "result": "\u901a\u8fc7\u6b64\u65b9\u6cd5\u83b7\u5f97\u7684\u8bed\u8a00\u73b0\u8c61\u6bd4LLMs\u5148\u9a8c\u77e5\u8bc6\u6216\u4e0a\u4e0b\u6587\u5b66\u4e60\u83b7\u5f97\u7684\u73b0\u8c61\u66f4\u5177\u9884\u6d4b\u6027\uff0c\u4e14\u57fa\u4e8e\u6570\u636e\u5b9e\u8bc1\uff0c\u53ef\u8de8\u9886\u57df\u6cdb\u5316\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u83b7\u5f97\u7684\u8bed\u8a00\u73b0\u8c61\u6709\u52a9\u4e8e\u4eba\u4eec\u5728\u7f3a\u4e4f\u6b3a\u9a97\u68c0\u6d4b\u5206\u7c7b\u5668\u7684\u73af\u5883\u4e2d\u6279\u5224\u6027\u8bc4\u4f30\u5728\u7ebf\u8bc4\u8bba\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2511.11819", "categories": ["cs.LG", "math.AT"], "pdf": "https://arxiv.org/pdf/2511.11819", "abs": "https://arxiv.org/abs/2511.11819", "authors": ["Ari Blondal", "Hamed Hatami", "Pooya Hatami", "Chavdar Lalov", "Sivan Tretiak"], "title": "Simplicial covering dimension of extremal concept classes", "comment": "31 pages, 5 figures", "summary": "Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.\n  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u7ecf\u5178\u62d3\u6251\u7ef4\u5ea6\u7406\u8bba\u5e94\u7528\u4e8e\u4e8c\u5143\u6982\u5ff5\u7c7b\uff0c\u5b9a\u4e49\u4e86\u5355\u7eaf\u8986\u76d6\u7ef4\u5ea6\uff0c\u5e76\u8bc1\u660e\u5bf9\u4e8e\u6709\u9650\u6982\u5ff5\u7c7b\uff0c\u8be5\u7ef4\u5ea6\u7cbe\u786e\u523b\u753b\u4e86PAC\u5b66\u4e60\u4e2d\u7684\u5217\u8868\u53ef\u590d\u5236\u6570\u3002", "motivation": "\u5c06\u62d3\u6251\u7ef4\u5ea6\u7406\u8bba\u5f15\u5165\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u4e3a\u5206\u6790\u6982\u5ff5\u7c7b\u7684\u5b66\u4e60\u590d\u6742\u6027\u63d0\u4f9b\u65b0\u7684\u6570\u5b66\u5de5\u5177\u3002", "method": "\u5c06\u6982\u5ff5\u7c7b\u7684\u53ef\u5b9e\u73b0\u5206\u5e03\u7a7a\u95f4\u89c6\u4e3a\u62d3\u6251\u7a7a\u95f4\uff0c\u901a\u8fc7\u635f\u5931\u51fd\u6570\u548c\u6982\u5ff5\u7c7b\u672c\u8eab\u8bf1\u5bfc\u5355\u7eaf\u7ed3\u6784\uff0c\u5b9a\u4e49\u5355\u7eaf\u8986\u76d6\u7ef4\u5ea6\u3002", "result": "\u8bc1\u660e\u5bf9\u4e8e\u6709\u9650\u6982\u5ff5\u7c7b\uff0c\u5355\u7eaf\u8986\u76d6\u7ef4\u5ea6\u7cbe\u786e\u7b49\u4e8e\u5217\u8868\u53ef\u590d\u5236\u6570\uff08\u5373\u5168\u5c40\u7a33\u5b9a\u6027\uff09\uff0c\u5e76\u5e94\u7528\u7ecf\u5178\u7ef4\u5ea6\u7406\u8bba\u8ba1\u7b97\u6781\u503c\u6982\u5ff5\u7c7b\u7684\u7cbe\u786e\u5217\u8868\u53ef\u590d\u5236\u6570\u3002", "conclusion": "\u62d3\u6251\u7ef4\u5ea6\u7406\u8bba\u4e3a\u7406\u89e3\u6982\u5ff5\u7c7b\u7684\u5b66\u4e60\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u7ef4\u5ea6\u7406\u8bba\u4e0e\u5b66\u4e60\u7406\u8bba\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002"}}
{"id": "2506.14157", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14157", "abs": "https://arxiv.org/abs/2506.14157", "authors": ["Chengyu Huang", "Tanya Goyal"], "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization", "comment": null, "summary": "Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DCRM\u6307\u6807\u6765\u91cf\u5316\u504f\u597d\u4f18\u5316\u4e2d\u54cd\u5e94\u5bf9\u7684\u8d28\u91cf\uff0c\u53d1\u73b0\u8bad\u7ec3\u96c6\u7684DCRM\u4e0e\u5b66\u4e60\u6548\u679c\u6b63\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e86best-of-N\u00b2\u914d\u5bf9\u65b9\u6cd5\u6765\u9009\u62e9\u9ad8\u8d28\u91cf\u54cd\u5e94\u5bf9\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c1d\u8bd5\u5c06\u504f\u597d\u4f18\u5316\u6027\u80fd\u4e0e\u504f\u597d\u6570\u636e\u96c6\u5173\u8054\uff0c\u4f46\u4f5c\u8005\u89c2\u5bdf\u5230\u504f\u597d\u54cd\u5e94\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u80fd\u4e0d\u7b26\u5408\u671f\u671b\u5b66\u4e60\u7684\u76ee\u6807\u5dee\u5f02\uff0c\u9700\u8981\u91cf\u5316\u8fd9\u4e9b\u5dee\u5f02\u6765\u8bc4\u4f30\u54cd\u5e94\u5bf9\u7684\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u8ddd\u79bb\u548c\u5956\u52b1\u8fb9\u9645\u6765\u91cf\u5316\u54cd\u5e94\u5dee\u5f02\uff0c\u7ed3\u5408\u5f97\u5230DCRM\u6307\u6807\uff1b\u7814\u7a76\u4e09\u7c7b\u5e38\u7528\u504f\u597d\u6570\u636e\u96c6\uff1b\u63d0\u51fabest-of-N\u00b2\u914d\u5bf9\u65b9\u6cd5\u9009\u62e9\u9ad8DCRM\u54cd\u5e94\u5bf9\u3002", "result": "\u5efa\u7acb\u4e86\u8bad\u7ec3\u96c6DCRM\u4e0e\u5b66\u4e60\u6548\u679c\u7684\u6b63\u76f8\u5173\u5173\u7cfb\uff1b\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u5728AlpacaEval\u3001MT-Bench\u548cArena-Hard\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u8bad\u7ec3\u96c6\u3002", "conclusion": "DCRM\u662f\u8bc4\u4f30\u504f\u597d\u4f18\u5316\u54cd\u5e94\u5bf9\u8d28\u91cf\u7684\u6709\u6548\u6307\u6807\uff0c\u57fa\u4e8eDCRM\u7684\u914d\u5bf9\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.13689", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13689", "abs": "https://arxiv.org/abs/2511.13689", "authors": ["Sofia Jamil", "Kotla Sai Charan", "Sriparna Saha", "Koustava Goswami", "Joseph K J"], "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation", "comment": null, "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.", "AI": {"tldr": "\u63d0\u51fa\u4e86TAI\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ffb\u8bd1\u548c\u56fe\u50cf\u751f\u6210\u589e\u5f3a\u5370\u5ea6\u8bd7\u6b4c\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u652f\u6301\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u3002", "motivation": "\u5370\u5ea6\u8bd7\u6b4c\u5177\u6709\u8bed\u8a00\u590d\u6742\u6027\u548c\u6587\u5316\u6df1\u5ea6\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u5370\u5ea6\u8bed\u8a00\u8bd7\u6b4c\uff0c\u5176\u591a\u5c42\u542b\u4e49\u548c\u6587\u5316\u5178\u6545\u7ed9\u975e\u6bcd\u8bed\u8bfb\u8005\u5e26\u6765\u7406\u89e3\u6311\u6218\u3002", "method": "\u4f7f\u7528TAI\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u51e0\u7387\u6bd4\u504f\u597d\u5bf9\u9f50\u7b97\u6cd5\u7684\u7ffb\u8bd1\u6a21\u5757\u548c\u57fa\u4e8e\u8bed\u4e49\u56fe\u7684\u56fe\u50cf\u751f\u6210\u6a21\u5757\uff0c\u6355\u6349\u8bd7\u6b4c\u4e2d\u7684\u9690\u55bb\u548c\u8bed\u4e49\u5173\u7cfb\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aTAI Diffusion\u5728\u8bd7\u6b4c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b1,570\u999621\u79cd\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u8bd7\u6b4c\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u586b\u8865\u8bd7\u6b4c\u7ffb\u8bd1\u548c\u89c6\u89c9\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u65e8\u5728\u6269\u5927\u53ef\u8bbf\u95ee\u6027\u5e76\u4e30\u5bcc\u8bfb\u8005\u4f53\u9a8c\u3002"}}
{"id": "2511.11828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11828", "abs": "https://arxiv.org/abs/2511.11828", "authors": ["Wenwen Si", "Sooyong Jang", "Insup Lee", "Osbert Bastani"], "title": "Conformal Constrained Policy Optimization for Cost-Effective LLM Agents", "comment": null, "summary": "While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.", "AI": {"tldr": "\u63d0\u51faCCPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u6210\u672c/\u7cbe\u5ea6\u4e0d\u540c\u7684LLM\u6a21\u578b\uff0c\u5728\u4fdd\u8bc1\u53ef\u9760\u6027\u7684\u524d\u63d0\u4e0b\u6700\u5c0f\u5316\u8ba1\u7b97\u6210\u672c", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u548cAPI\u6210\u672c\u65e5\u76ca\u6602\u8d35\uff0c\u9700\u8981\u627e\u5230\u5728\u4fdd\u8bc1\u53ef\u9760\u6027\u7684\u540c\u65f6\u964d\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848", "method": "Conformal Constrained Policy Optimization (CCPO) - \u7ed3\u5408\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u3001\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebf\u7b26\u5408\u9884\u6d4b\uff0c\u8054\u5408\u4f18\u5316\u6210\u672c\u611f\u77e5\u7b56\u7565\u548c\u81ea\u9002\u5e94\u9608\u503c", "result": "\u5728\u4e24\u4e2a\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCCPO\u76f8\u6bd4\u5176\u4ed6\u6210\u672c\u611f\u77e5\u57fa\u7ebf\u548cLLM\u5f15\u5bfc\u65b9\u6cd5\uff0c\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe30%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u9760\u6027", "conclusion": "CCPO\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8LLM\u4ee3\u7406\u7684\u6210\u672c\u6548\u76ca\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u9760\u6027"}}
{"id": "2511.13703", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13703", "abs": "https://arxiv.org/abs/2511.13703", "authors": ["Lavender Y. Jiang", "Angelica Chen", "Xu Han", "Xujin Chris Liu", "Radhika Dua", "Kevin Eaton", "Frederick Wolff", "Robert Steele", "Jeff Zhang", "Anton Alyakin", "Qingkai Pan", "Yanbing Chen", "Karl L. Sangwon", "Daniel A. Alber", "Jaden Stryker", "Jin Vivian Lee", "Yindalon Aphinyanaphongs", "Kyunghyun Cho", "Eric Karl Oermann"], "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations", "comment": null, "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.", "AI": {"tldr": "Lang1\u6a21\u578b\u5bb6\u65cf\u901a\u8fc7\u7ed3\u5408\u4e34\u5e8a\u7535\u5b50\u75c5\u5386\u6570\u636e\u548c\u4e92\u8054\u7f51\u6587\u672c\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5728\u533b\u7597\u64cd\u4f5c\u51b3\u7b56\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5fae\u8c03\u540e\u80fd\u8d85\u8d8a\u592770\u500d\u7684\u901a\u7528\u6a21\u578b\u3002", "motivation": "\u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u533b\u7597\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u533b\u7597\u64cd\u4f5c\u51b3\u7b56\u6240\u9700\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u533b\u7597\u9886\u57df\u5f00\u53d1\u7684\u6a21\u578b\u3002", "method": "\u5f00\u53d1Lang1\u6a21\u578b\u5bb6\u65cf\uff08100M-7B\u53c2\u6570\uff09\uff0c\u4f7f\u752880B\u4e34\u5e8a\u7535\u5b50\u75c5\u5386token\u548c627B\u4e92\u8054\u7f51token\u6df7\u5408\u9884\u8bad\u7ec3\uff0c\u5e76\u521b\u5efaReMedE\u57fa\u51c6\u8bc4\u4f30\u4e94\u4e2a\u5173\u952e\u533b\u7597\u4efb\u52a1\u3002", "result": "\u96f6\u5c04\u8bbe\u7f6e\u4e0b\u901a\u7528\u548c\u4e13\u7528\u6a21\u578b\u5728\u56db\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5fae\u8c03\u540e\u7684Lang1-1B\u6a21\u578b\u5728AUROC\u4e0a\u6bd4\u592770\u500d\u7684\u901a\u7528\u6a21\u578b\u63d0\u53473.64%-6.75%\uff0c\u6bd4\u96f6\u5c04\u6a21\u578b\u63d0\u53471.66%-23.66%\u3002", "conclusion": "\u533b\u7597\u7cfb\u7edfAI\u9700\u8981\u9886\u57df\u5185\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u7684\u7ed3\u5408\uff0c\u4e13\u7528LLM\u5728\u4e13\u4e1a\u4efb\u52a1\u4e0a\u80fd\u4e0e\u901a\u7528\u6a21\u578b\u7ade\u4e89\u3002"}}
{"id": "2511.11834", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11834", "abs": "https://arxiv.org/abs/2511.11834", "authors": ["Vahid Hemmati", "Ahmad Mohammadi", "Abdul-Rauf Nuhu", "Reza Ahmari", "Parham Kebria", "Abdollah Homaifar"], "title": "Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers", "comment": null, "summary": "Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \\textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.", "AI": {"tldr": "VC\u662f\u4e00\u79cd\u65e0\u9700\u6807\u7b7e\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u6392\u5e8f\u540esoftmax\u8f93\u51fa\u7684\u79bb\u6563\u5ea6\u6765\u91cf\u5316\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u4e0d\u89c4\u5219\u6027\uff0c\u4e0e\u5206\u7c7b\u51c6\u786e\u7387\u5448\u5f3a\u8d1f\u76f8\u5173\u3002", "motivation": "\u5728\u5b9e\u65f6\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u63a8\u7406\u65f6\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\uff0c\u5bf9\u6297\u9c81\u68d2\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u6807\u7b7e\u7684\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u5b9a\u4e49VC\u4e3a\u76f8\u90bb\u786e\u5b9a\u6027\u503c\u7684\u5e73\u5747\u5e73\u65b9\u5bf9\u6570\u6bd4\uff0c\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u4f7f\u7528ANN\u3001CNN\u548cVGG-like\u6a21\u578b\uff0c\u901a\u8fc7FGSM\u751f\u6210\u5bf9\u6297\u6837\u672c\u5e76\u521b\u5efa\u6df7\u5408\u6d4b\u8bd5\u96c6\u3002", "result": "\u5206\u7c7b\u51c6\u786e\u7387\u4e0elog(VC)\u4e4b\u95f4\u5b58\u5728\u5f3a\u8d1f\u76f8\u5173\uff08\u5927\u591a\u6570\u60c5\u51b5\u4e0brho < -0.90\uff09\uff0cVC\u80fd\u6709\u6548\u53cd\u6620\u6027\u80fd\u4e0b\u964d\u800c\u65e0\u9700\u6807\u7b7e\u6570\u636e\u3002", "conclusion": "VC\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u67b6\u6784\u65e0\u5173\u7684\u5b9e\u65f6\u6027\u80fd\u5ea6\u91cf\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2511.11842", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.11842", "abs": "https://arxiv.org/abs/2511.11842", "authors": ["Lucas Fenaux", "Christopher Srinivasa", "Florian Kerschbaum"], "title": "On the Trade-Off Between Transparency and Security in Adversarial Machine Learning", "comment": null, "summary": "Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u53ef\u8f6c\u79fb\u5bf9\u6297\u6837\u672c\u653b\u51fb\u7814\u7a76\u4e86AI\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u7684\u6218\u7565\u51b2\u7a81\uff0c\u53d1\u73b0\u653b\u51fb\u8005\u5728\u5339\u914d\u9632\u5fa1\u8005\u51b3\u7b56\u65f6\u66f4\u6210\u529f\uff0c\u8868\u660e\u6a21\u7cca\u6027\u53ef\u80fd\u6709\u5229\u4e8e\u9632\u5fa1\u8005\u3002", "motivation": "\u7814\u7a76AI\u7cfb\u7edf\u4e2d\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u6f5c\u5728\u51b2\u7a81\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\uff0c\u900f\u660e\u5ea6\u53ef\u80fd\u635f\u5bb3\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc4\u4f30\uff089\u79cd\u653b\u51fb\u65b9\u6cd5\u5728181\u4e2a\u6a21\u578b\u4e0a\uff09\u548c\u535a\u5f08\u8bba\u5206\u6790\uff08\u7eb3\u4ec0\u535a\u5f08\u548c\u65af\u5854\u514b\u5c14\u4f2f\u683c\u535a\u5f08\uff09\u6765\u5efa\u6a21\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u7684\u6743\u8861\u3002", "result": "\u653b\u51fb\u8005\u80fd\u591f\u901a\u8fc7\u4e86\u89e3\u9632\u5fa1\u8005\u6a21\u578b\u662f\u5426\u88ab\u9632\u5fa1\u6765\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff1b\u535a\u5f08\u8bba\u5206\u6790\u786e\u8ba4\u900f\u660e\u5ea6\u6709\u65f6\u4f1a\u635f\u5bb3\u5b89\u5168\u6027\u3002", "conclusion": "AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u5b58\u5728\u56fa\u6709\u51b2\u7a81\uff0c\u900f\u660e\u5ea6\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u53ef\u80fd\u635f\u5bb3\u5b89\u5168\u6027\uff0c\u9700\u8981\u8c28\u614e\u6743\u8861\u3002"}}
{"id": "2511.11849", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11849", "abs": "https://arxiv.org/abs/2511.11849", "authors": ["Junyang He", "Judy Fox", "Alireza Jafari", "Ying-Jung Chen", "Geoffrey Fox"], "title": "Leveraging Exogenous Signals for Hydrology Time Series Forecasting", "comment": null, "summary": "Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u4e2d\u6574\u5408\u9886\u57df\u77e5\u8bc6\u5bf9\u6c34\u6587\u964d\u96e8-\u5f84\u6d41\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5305\u542b\u5168\u9762\u5df2\u77e5\u5916\u751f\u8f93\u5165\u7684\u6a21\u578b\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u5176\u4e2d\u81ea\u7136\u5e74\u5ea6\u5468\u671f\u6027\u65f6\u95f4\u5e8f\u5217\u7684\u8d21\u732e\u6700\u4e3a\u663e\u8457\u3002", "motivation": "\u5c3d\u7ba1\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7814\u7a76\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5f88\u5c11\u7814\u7a76\u5176\u5728\u7269\u7406\u79d1\u5b66\u7279\u5b9a\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6c34\u6587\u964d\u96e8-\u5f84\u6d41\u5efa\u6a21\u9886\u57df\u3002", "method": "\u4f7f\u7528CAMELS-US\u6570\u636e\u96c6\uff0c\u5305\u542b671\u4e2a\u4f4d\u7f6e\u7684\u964d\u96e8\u548c\u5f84\u6d41\u6570\u636e\uff0c\u6bd4\u8f83\u4e86\u57fa\u51c6\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff08\u7279\u522b\u662f\u5df2\u77e5\u5916\u751f\u8f93\u5165\uff09\u7684\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5305\u542b\u5168\u9762\u5df2\u77e5\u5916\u751f\u8f93\u5165\u7684\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u66f4\u6709\u9650\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u7840\u6a21\u578b\u3002\u81ea\u7136\u5e74\u5ea6\u5468\u671f\u6027\u65f6\u95f4\u5e8f\u5217\u7684\u6574\u5408\u5e26\u6765\u4e86\u6700\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u5728\u6c34\u6587\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\uff0c\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff08\u7279\u522b\u662f\u81ea\u7136\u5468\u671f\u6027\u7279\u5f81\uff09\u6bd4\u5355\u7eaf\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u66f4\u6709\u6548\uff0c\u5f3a\u8c03\u4e86\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\u5728\u7269\u7406\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.11880", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11880", "abs": "https://arxiv.org/abs/2511.11880", "authors": ["David Montero", "Miguel D. Mahecha", "Francesco Martinuzzi", "C\u00e9sar Aybar", "Anne Klosterhalfen", "Alexander Knohl", "Jes\u00fas Anaya", "Clemens Mosig", "Sebastian Wieneke"], "title": "Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production", "comment": null, "summary": "Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.", "AI": {"tldr": "\u6bd4\u8f83GPT-2\u548cLSTM\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u68ee\u6797CO\u2082\u5438\u6536(GPP)\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0LSTM\u6574\u4f53\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u4f46GPT-2\u5728\u6781\u7aef\u4e8b\u4ef6\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4e14LSTM\u4f7f\u7528\u66f4\u77ed\u7684\u8f93\u5165\u7a97\u53e3\u5c31\u80fd\u8fbe\u5230\u76f8\u4f3c\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9065\u611f\u65b9\u6cd5\u5728\u6355\u6349GPP\u590d\u6742\u65f6\u95f4\u52a8\u6001\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63a2\u7d22\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u591a\u5143\u6570\u636e\u878d\u5408\u4e0b\u5bf9\u690d\u88ab\u8fc7\u7a0b\u65f6\u95f4\u52a8\u6001\u7684\u8868\u793a\u80fd\u529b\u3002", "method": "\u4f7f\u7528GPT-2(Transformer\u67b6\u6784)\u548cLSTM(\u5faa\u73af\u795e\u7ecf\u7f51\u7edc)\u4e24\u79cd\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5143\u8f93\u5165\u6570\u636e(\u8f90\u5c04\u3001Sentinel-2\u3001MODIS\u5730\u8868\u6e29\u5ea6\u3001Sentinel-1)\u8fdb\u884cGPP\u9884\u6d4b\uff0c\u5e76\u5206\u6790\u65f6\u95f4\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u8fbe\u5230\u76f8\u4f3c\u7cbe\u5ea6\uff0cLSTM\u6574\u4f53\u8868\u73b0\u66f4\u597d\u4f46GPT-2\u5728\u6781\u7aef\u4e8b\u4ef6\u4e2d\u66f4\u4f18\uff1bLSTM\u4f7f\u7528\u66f4\u77ed\u8f93\u5165\u7a97\u53e3\u5373\u53ef\u8fbe\u5230GPT-2\u7684\u7cbe\u5ea6\uff1b\u8f90\u5c04\u662f\u6700\u91cd\u8981\u7684\u9884\u6d4b\u56e0\u5b50\uff0c\u5176\u6b21\u662fSentinel-2\u3001MODIS\u5730\u8868\u6e29\u5ea6\u548cSentinel-1\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u591a\u6a21\u6001\u8f93\u5165\u5171\u540c\u51b3\u5b9a\u4e86GPP\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u76d1\u6d4b\u9646\u5730\u78b3\u52a8\u6001\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.11881", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11881", "abs": "https://arxiv.org/abs/2511.11881", "authors": ["Zhengxin Zhang", "Chengyu Huang", "Aochong Oliver Li", "Claire Cardie"], "title": "Better LLM Reasoning via Dual-Play", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.", "AI": {"tldr": "PasoDoble\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684LLM\u53cc\u89d2\u8272\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7Proposer\u751f\u6210\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u548cSolver\u89e3\u51b3\u95ee\u9898\u7684\u5bf9\u6297\u8fc7\u7a0b\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u8bad\u7ec3\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\uff0c\u800c\u5bf9\u6297\u5b66\u4e60\u7279\u522b\u662f\u53cc\u89d2\u8272\u81ea\u535a\u5f08\u8bad\u7ec3\u53ef\u4ee5\u51cf\u5c11\u5bf9\u5916\u90e8\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u4f46\u9762\u4e34\u5956\u52b1\u7834\u89e3\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u6311\u6218\u3002", "method": "\u4ece\u540c\u4e00\u57fa\u7840\u6a21\u578b\u521d\u59cb\u5316\u4e24\u4e2a\u6a21\u578b\uff1aProposer\u751f\u6210\u5e26\u771f\u5b9e\u7b54\u6848\u7684\u6311\u6218\u6027\u95ee\u9898\uff0cSolver\u5c1d\u8bd5\u89e3\u51b3\u3002Proposer\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u786e\u4fdd\u95ee\u9898\u8d28\u91cf\uff0c\u901a\u8fc7\u8054\u5408\u66f4\u65b0\u907f\u514d\u5956\u52b1\u7834\u89e3\uff0c\u5e76\u5f15\u5165\u79bb\u7ebf\u8303\u5f0f\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePasoDoble\u80fd\u591f\u63d0\u5347LLM\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "PasoDoble\u5c55\u793a\u4e86\u65e0\u76d1\u7763\u53cc\u89d2\u8272\u5bf9\u6297\u8bad\u7ec3\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u51cf\u5c11\u5916\u90e8\u76d1\u7763\u4f9d\u8d56\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.11891", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11891", "abs": "https://arxiv.org/abs/2511.11891", "authors": ["Nawid Keshtmand", "Roussel Desmond Nzoyem", "Jeffrey Nicholas Clark"], "title": "FLEX: Feature Importance from Layered Counterfactual Explanations", "comment": "12 pages, 6 figures, 3 tables, 2 algorithms. Preprint under review", "summary": "Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable \"what-if\" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.", "AI": {"tldr": "FLEX\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u89e3\u91ca\u8f6c\u6362\u4e3a\u7279\u5f81\u53d8\u5316\u9891\u7387\u5206\u6570\uff0c\u5728\u5c40\u90e8\u3001\u533a\u57df\u548c\u5168\u5c40\u5c42\u9762\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u91cd\u8981\u6027\u6392\u540d\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\uff0c\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u901a\u5e38\u5c40\u9650\u4e8e\u5b9e\u4f8b\u7279\u5b9a\uff0c\u65e0\u6cd5\u7cfb\u7edf\u91cf\u5316\u7279\u5f81\u5728\u7279\u5f81\u7a7a\u95f4\u6216\u6574\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u9a71\u52a8\u4f5c\u7528\u3002", "method": "FLEX\u6846\u67b6\u5c06\u53cd\u4e8b\u5b9e\u89e3\u91ca\u96c6\u8f6c\u6362\u4e3a\u7279\u5f81\u53d8\u5316\u9891\u7387\u5206\u6570\uff0c\u901a\u8fc7\u8de8\u5b9e\u4f8b\u548c\u90bb\u57df\u805a\u5408\u6765\u6cdb\u5316\u5c40\u90e8\u53d8\u5316\u9891\u7387\u5ea6\u91cf\uff0c\u517c\u5bb9\u4e0d\u540c\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u6027\u9884\u6d4b\u548c\u8d37\u6b3e\u5ba1\u6279\u4efb\u52a1\u4e2d\uff0cFLEX\u7684\u5168\u5c40\u6392\u540d\u4e0eSHAP\u76f8\u5173\u4f46\u80fd\u53d1\u73b0\u989d\u5916\u9a71\u52a8\u56e0\u7d20\uff0c\u533a\u57df\u5206\u6790\u63ed\u793a\u4e86\u5168\u5c40\u6458\u8981\u9057\u6f0f\u7684\u4e0a\u4e0b\u6587\u7279\u5b9a\u56e0\u7d20\u3002", "conclusion": "FLEX\u5f25\u5408\u4e86\u5c40\u90e8\u8865\u6551\u548c\u5168\u5c40\u5f52\u56e0\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u652f\u6301\u98ce\u9669\u654f\u611f\u5e94\u7528\u4e2d\u900f\u660e\u4e14\u9762\u5411\u5e72\u9884\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2511.11894", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11894", "abs": "https://arxiv.org/abs/2511.11894", "authors": ["Lingxiao Li", "Haobo Zhang", "Bin Chen", "Jiayu Zhou"], "title": "Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design", "comment": "22 pages, 7 figures, 10 tables", "summary": "Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Chain-of-Generation (CoG)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6f5c\u5728\u6269\u6563\u6a21\u578b\u89e3\u51b3\u6587\u672c\u6761\u4ef6\u5206\u5b50\u751f\u6210\u4e2d\u4e00\u6b21\u6027\u6761\u4ef6\u7f16\u7801\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u5206\u5b50\u751f\u6210\u6a21\u578b\u91c7\u7528\u4e00\u6b21\u6027\u6761\u4ef6\u7f16\u7801\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u63d0\u793a\u4e2d\u7684\u6240\u6709\u8981\u6c42\uff0c\u5b58\u5728\u751f\u6210\u7ec4\u4ef6\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u65e0\u6cd5\u751f\u6210\u6240\u6709\u5b50\u7ed3\u6784\u3001\u4ee5\u53ca\u540c\u65f6\u8003\u8651\u6240\u6709\u8981\u6c42\u8fc7\u4e8e\u96c4\u5fc3\u52c3\u52c3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faCoG\u6846\u67b6\uff1a1\uff09\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u8bfe\u7a0b\u6392\u5e8f\u7684\u8bed\u4e49\u7247\u6bb5\uff1b2\uff09\u9010\u6b65\u5c06\u8fd9\u4e9b\u7247\u6bb5\u4f5c\u4e3a\u4e2d\u95f4\u76ee\u6807\u7eb3\u5165\uff1b3\uff09\u5f15\u5165\u540e\u5bf9\u9f50\u5b66\u4e60\u9636\u6bb5\u52a0\u5f3a\u6587\u672c\u548c\u5206\u5b50\u6f5c\u5728\u7a7a\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u57fa\u51c6\u548c\u5b9e\u9645\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCoG\u76f8\u6bd4\u4e00\u6b21\u6027\u57fa\u7ebf\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u3001\u591a\u6837\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u66f4\u5fe0\u5b9e\u5730\u53cd\u6620\u590d\u6742\u7ec4\u5408\u63d0\u793a\u3002", "conclusion": "CoG\u901a\u8fc7\u591a\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u6761\u4ef6\u5206\u5b50\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u7684\u751f\u6210\u8fc7\u7a0b\u6d1e\u5bdf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2511.11902", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11902", "abs": "https://arxiv.org/abs/2511.11902", "authors": ["Ci Lin", "Tet Yeap", "Iluju Kiringa", "Biwei Zhang"], "title": "Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm", "comment": null, "summary": "Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53cc\u5411\u5b50\u7a7a\u95f4\u65cb\u8f6c\u7b97\u6cd5\uff08B-SRA\uff09\u548c\u65b0\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u5411\u8054\u60f3\u8bb0\u5fc6\uff08BAM\uff09\u7684\u9c81\u68d2\u6027\u548c\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u53cc\u5411\u8054\u60f3\u8bb0\u5fc6\u5728\u53cc\u5411\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u4e2d\u5bf9\u566a\u58f0\u548c\u5bf9\u6297\u653b\u51fb\u654f\u611f\u3001\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u68af\u5ea6\u81ea\u7531\u8bad\u7ec3\u7b97\u6cd5B-SRA\uff0c\u5e76\u57fa\u4e8e\u6b63\u4ea4\u6743\u91cd\u77e9\u9635\u548c\u68af\u5ea6\u6a21\u5f0f\u5bf9\u9f50\u4e24\u4e2a\u5173\u952e\u539f\u5219\uff0c\u5728B-BP\u4e2d\u5f15\u5165\u65b0\u7684\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u96c6\u6210OWM\u548cGPA\u7684SAME\u914d\u7f6e\u5728\u6240\u6709\u65b9\u6cd5\u4e2d\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u6297\u5e72\u6270\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u653b\u51fb\u573a\u666f\u548c\u8bb0\u5fc6\u5bb9\u91cf\u4e0b\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "B-SRA\u548c\u63d0\u51fa\u7684\u6b63\u5219\u5316\u7b56\u7565\u663e\u8457\u589e\u5f3a\u4e86\u8054\u60f3\u8bb0\u5fc6\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u6784\u5efa\u5f39\u6027\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.11912", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11912", "abs": "https://arxiv.org/abs/2511.11912", "authors": ["Haoyan Xu", "Ruizhi Qian", "Jiate Li", "Yushun Dong", "Minghao Lin", "Hanson Yan", "Zhengtao Yao", "Qinghua Liu", "Junhao Dong", "Ruopeng Huang", "Yue Zhao", "Mengyuan Li"], "title": "A Systematic Study of Model Extraction Attacks on Graph Foundation Models", "comment": null, "summary": "Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u9488\u5bf9\u56fe\u57fa\u7840\u6a21\u578b(GFMs)\u7684\u6a21\u578b\u63d0\u53d6\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u5728\u4ec5\u4f7f\u7528\u539f\u59cb\u8bad\u7ec3\u6210\u672c\u6781\u5c0f\u90e8\u5206\u7684\u60c5\u51b5\u4e0b\uff0c\u653b\u51fb\u8005\u5c31\u80fd\u8fd1\u4f3c\u53d7\u5bb3\u8005\u6a21\u578b\uff0c\u4e14\u51c6\u786e\u7387\u51e0\u4e4e\u65e0\u635f\u3002", "motivation": "\u968f\u7740\u56fe\u57fa\u7840\u6a21\u578b(GFMs)\u7684\u53d1\u5c55\uff0c\u8fd9\u4e9b\u6a21\u578b\u56e0\u5176\u9ad8\u9884\u8bad\u7ec3\u6210\u672c\u548c\u8de8\u9886\u57df\u77e5\u8bc6\u800c\u6210\u4e3a\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u7684\u6709\u5438\u5f15\u76ee\u6807\u3002\u5148\u524d\u7814\u7a76\u4ec5\u5173\u6ce8\u5c0f\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5bf9\u5927\u89c4\u6a21\u591a\u6a21\u6001GFMs\u7684\u5b89\u5168\u5f71\u54cd\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u9ed1\u76d2\u5a01\u80c1\u6a21\u578b\u548c\u516d\u79cd\u5b9e\u7528\u653b\u51fb\u573a\u666f\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u56de\u5f52\u56fe\u5d4c\u5165\u8bad\u7ec3\u653b\u51fb\u8005\u7f16\u7801\u5668\uff0c\u65e0\u9700\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u4fdd\u6301\u4e0e\u53d7\u5bb3\u8005\u6587\u672c\u7f16\u7801\u5668\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u653b\u51fb\u8005\u4ec5\u4f7f\u7528\u539f\u59cb\u8bad\u7ec3\u6210\u672c\u7684\u6781\u5c0f\u90e8\u5206\u5c31\u80fd\u8fd1\u4f3c\u53d7\u5bb3\u8005\u6a21\u578b\uff0c\u51c6\u786e\u7387\u51e0\u4e4e\u65e0\u635f\u5931\u3002", "conclusion": "GFMs\u663e\u8457\u6269\u5927\u4e86\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u7684\u653b\u51fb\u9762\uff0c\u7a81\u663e\u4e86\u5728\u5927\u89c4\u6a21\u56fe\u5b66\u4e60\u7cfb\u7edf\u4e2d\u9700\u8981\u90e8\u7f72\u611f\u77e5\u7684\u5b89\u5168\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2511.11918", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11918", "abs": "https://arxiv.org/abs/2511.11918", "authors": ["Wieger Wesselink", "Bram Grooten", "Huub van de Wetering", "Qiao Xiao", "Decebal Constantin Mocanu"], "title": "Batch Matrix-form Equations and Implementation of Multilayer Perceptrons", "comment": "32 pages; submitted to JMLR", "summary": "Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \\emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u591a\u5c42\u611f\u77e5\u673a(MLP)\u7684\u5b8c\u6574\u6279\u91cf\u77e9\u9635\u5f62\u5f0f\u63a8\u5bfc\uff0c\u5305\u62ec\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u65b9\u7a0b\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\u6570\u5b66\u9a8c\u8bc1\uff0c\u6784\u5efa\u4e86\u591a\u6846\u67b6\u53c2\u8003\u5b9e\u73b0\u3002", "motivation": "\u5f53\u524dMLP\u7b97\u6cd5\u5927\u591a\u57fa\u4e8e\u81ea\u52a8\u5fae\u5206\u6216\u9010\u6837\u672c\u68af\u5ea6\uff0c\u7f3a\u4e4f\u5b8c\u6574\u7684\u6279\u91cf\u77e9\u9635\u5f62\u5f0f\u8868\u8fbe\uff0c\u8fd9\u9650\u5236\u4e86\u900f\u660e\u5316\u5206\u6790\u548c\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u7b49\u573a\u666f\u7684\u4f18\u5316\u3002", "method": "\u91c7\u7528\u6570\u5b66\u4e25\u8c28\u7684\u6279\u91cf\u77e9\u9635\u5f62\u5f0f\u63a8\u5bfcMLP\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u65b9\u7a0b\uff0c\u4f7f\u7528SymPy\u8fdb\u884c\u7b26\u53f7\u9a8c\u8bc1\uff0c\u5e76\u5728NumPy\u3001PyTorch\u3001JAX\u3001TensorFlow\u548cC++\u4e2d\u6784\u5efa\u7edf\u4e00\u53c2\u8003\u5b9e\u73b0\u3002", "result": "\u6210\u529f\u63a8\u5bfc\u5e76\u9a8c\u8bc1\u4e86\u6240\u6709\u6807\u51c6\u5c42\u548c\u9ad8\u7ea7\u5c42(\u5305\u62ec\u6279\u5f52\u4e00\u5316\u548csoftmax)\u7684\u68af\u5ea6\u65b9\u7a0b\uff0c\u6784\u5efa\u4e86\u57fa\u4e8e\u5c11\u91cf\u77e9\u9635\u539f\u8bed\u7684\u9ad8\u6027\u80fd\u5b9e\u73b0\uff0c\u7279\u522b\u4f18\u5316\u4e86\u7a00\u758f\u8ba1\u7b97\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4e3a\u7406\u89e3\u3001\u6559\u5b66\u548c\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6279\u91cf\u77e9\u9635\u5f62\u5f0f\u89c4\u8303\u3002"}}
{"id": "2511.11928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11928", "abs": "https://arxiv.org/abs/2511.11928", "authors": ["Ziyao Cui", "Edric Tam"], "title": "Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks", "comment": null, "summary": "Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u63d2\u503c\u62c9\u666e\u62c9\u65af\u5d4c\u5165(ILEs)\u7684\u56fe\u77e9\u9635\u8c31\u5d4c\u5165\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u7684\u8282\u70b9\u7279\u5f81\u8868\u793a\uff0c\u7279\u522b\u662f\u5728\u8282\u70b9\u7279\u5f81\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u4fe1\u606f\u4e30\u5bcc\u7684\u8282\u70b9\u7279\u5f81\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8282\u70b9\u7279\u5f81\u5f80\u5f80\u6709\u9650\u6216\u7f3a\u5931\u3002\u867d\u7136\u62c9\u666e\u62c9\u65af\u8c31\u5d4c\u5165\u662f\u5e38\u7528\u7684\u7279\u5f81\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f46\u4f5c\u8005\u63a2\u7d22\u662f\u5426\u5176\u4ed6\u56fe\u77e9\u9635\u7684\u8c31\u5d4c\u5165\u4e5f\u80fd\u63d0\u4f9b\u6709\u7528\u7684\u8868\u793a\u3002", "method": "\u5f15\u5165\u4e86\u63d2\u503c\u62c9\u666e\u62c9\u65af\u5d4c\u5165(ILEs)\uff0c\u8fd9\u662f\u4e00\u79cd\u6765\u81ea\u7b80\u5355\u800c\u5bcc\u6709\u8868\u8fbe\u529b\u7684\u56fe\u77e9\u9635\u5bb6\u65cf\u7684\u8c31\u5d4c\u5165\u65b9\u6cd5\u3002\u4f7f\u7528\u8c31\u56fe\u7406\u8bba\u5de5\u5177\u6765\u89e3\u91caILEs\u6355\u83b7\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7ILEs\u8fdb\u884c\u7279\u5f81\u589e\u5f3a\u53ef\u4ee5\u63d0\u5347\u5e38\u7528GNN\u67b6\u6784\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5b9e\u8df5\u8005\u5728\u8282\u70b9\u7279\u5f81\u6709\u9650\u65f6\u7684\u8c31\u589e\u5f3a\u5de5\u5177\u5305\u3002"}}
{"id": "2511.11934", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11934", "abs": "https://arxiv.org/abs/2511.11934", "authors": ["C. C\u00e9sar Claros Olivares", "Austin J. Brockmeier"], "title": "A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts", "comment": null, "summary": "We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86CLIP\u5206\u5c42\u673a\u5236\u4e0b\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u7279\u5f81\u7a7a\u95f4\u51b3\u5b9a\u68c0\u6d4b\u6548\u679c\uff0c\u6982\u7387\u5f97\u5206\u5728\u8bef\u5206\u7c7b\u68c0\u6d4b\u4e2d\u5360\u4f18\uff0c\u51e0\u4f55\u611f\u77e5\u5f97\u5206\u5728\u5f3a\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u7b80\u5355PCA\u6295\u5f71\u80fd\u63d0\u5347\u591a\u4e2a\u68c0\u6d4b\u5668\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9OOD\u68c0\u6d4b\u65b9\u6cd5\u5728CLIP\u5206\u5c42\u673a\u5236\u4e0b\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u9700\u8981\u4e3a\u4e0d\u540c\u5206\u5e03\u504f\u79fb\u573a\u666f\u63d0\u4f9b\u7edf\u8ba1\u4f9d\u636e\u7684\u65b9\u6cd5\u9009\u62e9\u6307\u5bfc\u3002", "method": "\u4f7f\u7528AURC\u548cAUGRC\u4f5c\u4e3a\u4e3b\u8981\u6307\u6807\uff0c\u6bd4\u8f83CNN\u548cViT\u4e24\u79cd\u8868\u793a\u8303\u5f0f\uff0c\u91c7\u7528\u591a\u91cd\u6bd4\u8f83\u63a7\u5236\u7684\u79e9\u57fa\u6d41\u7a0b\uff08Friedman\u68c0\u9a8c\u4e0eConover-Holm\u4e8b\u540e\u68c0\u9a8c\uff09\u548cBron-Kerbosch\u56e2\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7279\u5f81\u7a7a\u95f4\u5f88\u5927\u7a0b\u5ea6\u4e0a\u51b3\u5b9a\u4e86OOD\u68c0\u6d4b\u6548\u679c\uff0c\u6982\u7387\u5f97\u5206\u5728\u8bef\u5206\u7c7b\u68c0\u6d4b\u4e2d\u5360\u4e3b\u5bfc\uff0c\u51e0\u4f55\u611f\u77e5\u5f97\u5206\u5728\u5f3a\u5206\u5e03\u504f\u79fb\u4e0b\u5728CNN\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5728ViT\u4e0aGradNorm\u548cKPCA\u91cd\u6784\u8bef\u5dee\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u4ee5\u8868\u793a\u4e3a\u4e2d\u5fc3\u7684OOD\u68c0\u6d4b\u89c2\u70b9\uff0c\u4e3a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4f9d\u636e\u7684\u6307\u5bfc\uff0c\u5e76\u663e\u793a\u7b80\u5355PCA\u6295\u5f71\u80fd\u63d0\u5347\u591a\u4e2a\u68c0\u6d4b\u5668\u6027\u80fd\u3002"}}
{"id": "2511.11935", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11935", "abs": "https://arxiv.org/abs/2511.11935", "authors": ["Munib Mesinovic", "Tingting Zhu"], "title": "SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis", "comment": null, "summary": "Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the \"preprocessing gap\" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.", "AI": {"tldr": "SurvBench\u662f\u4e00\u4e2a\u5f00\u6e90\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5c06\u539f\u59cbPhysioNet\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u7684\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u5f20\u91cf\uff0c\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\u95ee\u9898\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u4e3a\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u5206\u6790\u63d0\u4f9b\u4e86\u5de8\u5927\u673a\u4f1a\uff0c\u4f46\u7531\u4e8e\u9884\u5904\u7406\u65b9\u6cd5\u4e0d\u4e00\u81f4\uff0c\u53ef\u590d\u73b0\u6027\u53d7\u5230\u4e25\u91cd\u9650\u5236\u3002", "method": "SurvBench\u63d0\u4f9b\u4e09\u4e2a\u91cd\u75c7\u76d1\u62a4\u6570\u636e\u5e93\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u652f\u6301\u65f6\u95f4\u5e8f\u5217\u751f\u547d\u4f53\u5f81\u3001\u9759\u6001\u4eba\u53e3\u7edf\u8ba1\u3001ICD\u8bca\u65ad\u4ee3\u7801\u548c\u653e\u5c04\u62a5\u544a\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u5b9e\u65bd\u4e25\u683c\u7684\u6570\u636e\u8d28\u91cf\u63a7\u5236\u3001\u60a3\u8005\u7ea7\u5206\u5272\u3001\u7f3a\u5931\u503c\u8ddf\u8e2a\u548c\u6807\u51c6\u5316\u65f6\u95f4\u805a\u5408\u3002", "result": "\u8be5\u6d41\u7a0b\u5904\u7406\u5355\u98ce\u9669\u548c\u7ade\u4e89\u98ce\u9669\u573a\u666f\uff0c\u8f93\u51fa\u4e0epycox\u5e93\u517c\u5bb9\uff0c\u652f\u6301\u6807\u51c6\u7edf\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "SurvBench\u901a\u8fc7\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u914d\u7f6e\u9a71\u52a8\u9884\u5904\u7406\uff0c\u89e3\u51b3\u4e86\u963b\u788d\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u6a21\u578b\u516c\u5e73\u6bd4\u8f83\u7684\"\u9884\u5904\u7406\u5dee\u8ddd\"\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u4e13\u6ce8\u4e8e\u65b9\u6cd5\u521b\u65b0\u800c\u975e\u6570\u636e\u5de5\u7a0b\u3002"}}
{"id": "2511.11940", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11940", "abs": "https://arxiv.org/abs/2511.11940", "authors": ["Christopher Sandino", "Sayeri Lala", "Geeling Chau", "Melika Ayoughi", "Behrooz Mahasseni", "Ellen Zippi", "Ali Moin", "Erdrin Azemi", "Hanlin Goh"], "title": "Learning the relative composition of EEG signals using pairwise relative shift pretraining", "comment": "Foundation Models for the Brain and Body NeurIPS 2025 Workshop", "summary": "Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86PARS\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u968f\u673a\u91c7\u6837\u7684EEG\u7a97\u53e3\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u65f6\u95f4\u504f\u79fb\u6765\u5b66\u4e60\u8111\u7535\u56fe\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dEEG\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u63a9\u7801\u91cd\u5efa\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u5c40\u90e8\u65f6\u95f4\u6a21\u5f0f\uff0c\u800c\u80fd\u591f\u5b66\u4e60\u795e\u7ecf\u4fe1\u53f7\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u7684\u4f4d\u7f6e\u9884\u6d4b\u9884\u8bad\u7ec3\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165PARS\u9884\u8bad\u7ec3\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u524d\u7f6e\u4efb\u52a1\uff0c\u9884\u6d4b\u968f\u673a\u91c7\u6837\u7684EEG\u7a97\u53e3\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u65f6\u95f4\u504f\u79fb\uff0c\u9f13\u52b1\u7f16\u7801\u5668\u6355\u83b7\u795e\u7ecf\u4fe1\u53f7\u4e2d\u7684\u76f8\u5bf9\u65f6\u95f4\u7ec4\u6210\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u5404\u79cdEEG\u89e3\u7801\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cPARS\u9884\u8bad\u7ec3\u7684transformer\u5728\u6807\u7b7e\u6548\u7387\u548c\u8fc1\u79fb\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "PARS\u4e3a\u81ea\u76d1\u7763EEG\u8868\u793a\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u795e\u7ecf\u4fe1\u53f7\u4e2d\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002"}}
{"id": "2511.11949", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11949", "abs": "https://arxiv.org/abs/2511.11949", "authors": ["Eunjeong Jeong", "Nikolaos Pappas"], "title": "Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation", "comment": "This paper has been submitted to a peer-reviewed journal", "summary": "Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.", "AI": {"tldr": "FedBacys\u662f\u4e00\u4e2a\u7535\u6c60\u611f\u77e5\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u7528\u6237\u7535\u6c60\u6c34\u5e73\u7684\u5faa\u73af\u5ba2\u6237\u7aef\u53c2\u4e0e\u6765\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\uff0c\u7279\u522b\u9002\u7528\u4e8e\u80fd\u91cf\u6536\u96c6\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u5f88\u5f3a\u5927\uff0c\u4f46\u5ba2\u6237\u7aef\u8bad\u7ec3\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u589e\u52a0\u5bfc\u81f4\u663e\u8457\u80fd\u91cf\u6d88\u8017\u3002\u5728\u80fd\u91cf\u6536\u96c6\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u80fd\u91cf\u6709\u9650\uff0c\u6bcf\u4e2a\u8bbe\u5907\u7684\u53c2\u4e0e\u53ef\u7528\u6027\u6ce2\u52a8\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faFedBacys\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u5ba2\u6237\u7aef\u5e76\u57fa\u4e8e\u7535\u6c60\u6c34\u5e73\u987a\u5e8f\u8c03\u5ea6\u5b83\u4eec\u6765\u6700\u5c0f\u5316\u5197\u4f59\u8ba1\u7b97\u3002\u8fd8\u5f15\u5165\u4e86FedBacys-Odd\u53d8\u4f53\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u9009\u62e9\u6027\u53c2\u4e0e\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u80fd\u91cf\u6210\u672c\u3002", "result": "\u63d0\u4f9b\u4e86\u6846\u67b6\u7684\u6536\u655b\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u76f8\u6bd4\u73b0\u6709\u7b97\u6cd5\u5728\u80fd\u91cf\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "FedBacys\u901a\u8fc7\u7535\u6c60\u611f\u77e5\u7684\u5faa\u73af\u5ba2\u6237\u7aef\u53c2\u4e0e\u7b56\u7565\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u7cfb\u7edf\u8303\u56f4\u5185\u7684\u80fd\u91cf\u4f7f\u7528\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6027\u80fd\u3002"}}
{"id": "2511.11973", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11973", "abs": "https://arxiv.org/abs/2511.11973", "authors": ["Xinming Gao", "Shangzhe Li", "Yujin Cai", "Wenwu Yu"], "title": "Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $\u03b2$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5206\u4f4d\u6570\u56de\u5f52\u7684\u6e29\u5ea6\u7cfb\u6570\u4f30\u8ba1\u548c\u4ef7\u503c\u6b63\u5219\u5316\u6280\u672f\uff0c\u89e3\u51b3\u4e86XQL\u548cMXQL\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u56fa\u5b9a\u6570\u636e\u96c6\u4e0a\u5b66\u4e60\u7b56\u7565\u800c\u65e0\u9700\u73af\u5883\u4ea4\u4e92\uff0c\u5728\u9ad8\u98ce\u9669\u6216\u9ad8\u6210\u672c\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002XQL\u53ca\u5176\u53d8\u4f53MXQL\u867d\u7136\u6027\u80fd\u826f\u597d\uff0c\u4f46\u5b58\u5728\u9700\u8981\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "1. \u57fa\u4e8e\u6e29\u548c\u5047\u8bbe\u4f7f\u7528\u5206\u4f4d\u6570\u56de\u5f52\u4f30\u8ba1\u6e29\u5ea6\u7cfb\u6570\u03b2\uff1b2. \u5f15\u5165\u53d7\u7ea6\u675f\u4ef7\u503c\u5b66\u4e60\u542f\u53d1\u7684\u4ef7\u503c\u6b63\u5219\u5316\u6280\u672f\u6765\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728D4RL\u548cNeoRL2\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u9886\u57df\u4e2d\u4f7f\u7528\u4e00\u81f4\u7684\u8d85\u53c2\u6570\u96c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8d85\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u9ad8\u6027\u80fd\u7684\u7b56\u7565\u5b66\u4e60\u3002"}}
{"id": "2511.11991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11991", "abs": "https://arxiv.org/abs/2511.11991", "authors": ["Xiang Ma", "Taihua Chen", "Pengcheng Wang", "Xuemei Li", "Caiming Zhang"], "title": "ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting", "comment": "AAAI 2026 Oral", "summary": "Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \\textbf{RE}liability-aware \\textbf{C}odebook-\\textbf{AS}sisted \\textbf{T}ime series forecasting framework (\\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.", "AI": {"tldr": "\u63d0\u51faReCast\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7801\u672c\u5bf9\u5c40\u90e8\u6a21\u5f0f\u8fdb\u884c\u79bb\u6563\u5316\u7f16\u7801\uff0c\u7ed3\u5408\u53cc\u8def\u5f84\u67b6\u6784\u548c\u53ef\u9760\u6027\u611f\u77e5\u7684\u7801\u672c\u66f4\u65b0\u7b56\u7565\uff0c\u5b9e\u73b0\u8f7b\u91cf\u4e14\u9c81\u68d2\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5168\u5c40\u5206\u89e3\uff0c\u5bf9\u5c40\u90e8\u590d\u6742\u52a8\u6001\u6a21\u5f0f\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5728\u5b9e\u65f6\u6216\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u7801\u672c\u5bf9\u5c40\u90e8\u6a21\u5f0f\u8fdb\u884c\u79bb\u6563\u5316\u7f16\u7801\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\uff08\u91cf\u5316\u8def\u5f84+\u6b8b\u5dee\u8def\u5f84\uff09\uff0c\u901a\u8fc7\u53ef\u9760\u6027\u611f\u77e5\u7684\u7801\u672c\u66f4\u65b0\u7b56\u7565\uff0c\u878d\u5408\u591a\u4e2a\u53ef\u9760\u6027\u56e0\u5b50\u8fdb\u884c\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u3002", "result": "\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u5206\u5e03\u504f\u79fb\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "ReCast\u6846\u67b6\u901a\u8fc7\u5c40\u90e8\u6a21\u5f0f\u7f16\u7801\u548c\u53ef\u9760\u6027\u611f\u77e5\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u3001\u9c81\u68d2\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.12002", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12002", "abs": "https://arxiv.org/abs/2511.12002", "authors": ["Tenghao Ji", "Eytan Adar"], "title": "Selecting Fine-Tuning Examples by Quizzing VLMs", "comment": null, "summary": "A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \\textit{do} exemplify the target concept (e.g., a \\textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.", "AI": {"tldr": "QZLoRA\u662f\u4e00\u4e2a\u901a\u8fc7QuizRank\u65b9\u6cd5\u81ea\u52a8\u9009\u62e9\u9ad8\u8d28\u91cf\u56fe\u50cf\u8fdb\u884cLoRA\u5fae\u8c03\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7528\u66f4\u5c11\u6837\u672c\u751f\u6210\u66f4\u5bf9\u9f50\u3001\u66f4\u903c\u771f\u7684\u56fe\u50cf\u3002", "motivation": "\u5728\u5fae\u8c03\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u65f6\uff0c\u4ece\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u7684\u56fe\u50cf\u96c6\uff08\u5982\u7ef4\u57fa\u5171\u4eab\u8d44\u6e90\uff09\u4e2d\u9009\u62e9\u793a\u4f8b\u5f80\u5f80\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u9009\u62e9\u80fd\u4ee3\u8868\u76ee\u6807\u6982\u5ff5\u7684\u8bad\u7ec3\u56fe\u50cf\u6765\u786e\u4fdd\u751f\u6210\u56fe\u50cf\u5177\u6709\u5178\u578b\u7279\u5f81\u3002", "method": "\u63d0\u51faQZLoRA\u6846\u67b6\uff0c\u5229\u7528QuizRank\u65b9\u6cd5\u81ea\u52a8\u5bf9\u56fe\u50cf\u8fdb\u884c\u6392\u540d\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a'\u6559\u80b2\u5e72\u9884'\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c'\u6d4b\u9a8c'\u6765\u8bc4\u4f30\u56fe\u50cf\u8d28\u91cf\u3002", "result": "QZLoRA\u80fd\u591f\u751f\u6210\u66f4\u5bf9\u9f50\u3001\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff0c\u4e14\u6240\u9700\u6837\u672c\u66f4\u5c11\u3002\u540c\u65f6\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e5f\u80fd\u751f\u6210\u5177\u6709\u4ee3\u8868\u6027\u7684\u98ce\u683c\u5316\u56fe\u50cf\uff08\u5982\u63d2\u56fe\uff09\u3002", "conclusion": "\u5c06\u81ea\u52a8\u89c6\u89c9\u63a8\u7406\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u76f8\u7ed3\u5408\uff0c\u4e3a\u4e3b\u9898\u81ea\u9002\u5e94\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.12033", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12033", "abs": "https://arxiv.org/abs/2511.12033", "authors": ["Jiahe Shi", "Zhengqi Gao", "Ching-Yun Ko", "Duane Boning"], "title": "EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and na\u00efvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.", "AI": {"tldr": "EARL\u662f\u4e00\u4e2a\u57fa\u4e8e\u71b5\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u7684Verilog\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u9ad8\u71b5token\u6765\u63d0\u5347RTL\u4ee3\u7801\u751f\u6210\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u8bed\u6cd5\u9519\u8bef\u3001\u529f\u80fd\u5e7b\u89c9\u548c\u4e0e\u8bbe\u8ba1\u610f\u56fe\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u3002", "method": "\u63d0\u51faEARL\u6846\u67b6\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u71b5\u5f15\u5bfc\u7684\u9009\u62e9\u6027\u66f4\u65b0\u673a\u5236\uff0c\u5c06\u7b56\u7565\u68af\u5ea6\u96c6\u4e2d\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027token\u4e0a\u3002", "result": "\u5728VerilogEval\u548cRTLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEARL\u5c06\u529f\u80fd\u901a\u8fc7\u7387\u6bd4\u73b0\u6709LLM\u57fa\u7ebf\u63d0\u5347\u4e8614.7%\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u66f4\u65b0\u5e76\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5c06\u5f3a\u5316\u5b66\u4e60\u96c6\u4e2d\u5728\u5173\u952e\u9ad8\u71b5token\u4e0a\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u6709\u9488\u5bf9\u6027\u7684\u7b56\u7565\u6539\u8fdb\uff0c\u63d0\u5347\u7ed3\u6784\u5316RTL\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u3002"}}
{"id": "2511.12041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12041", "abs": "https://arxiv.org/abs/2511.12041", "authors": ["Shivam Barwey", "Pinaki Pal"], "title": "Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers", "comment": null, "summary": "Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u56fe\u53d8\u6362\u5668\u7684\u7f51\u683c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08SR-GT\uff09\uff0c\u7528\u4e8e\u53cd\u5e94\u6d41\u573a\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u76f8\u6bd4\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u8d85\u5206\u8fa8\u7387\u6280\u672f\u5bf9\u4e8e\u4e9a\u7f51\u683c\u95ed\u5408\u5efa\u6a21\u3001\u65f6\u7a7a\u9884\u6d4b\u52a0\u901f\u3001\u6570\u636e\u538b\u7f29\u548c\u7a00\u758f\u5b9e\u9a8c\u6d4b\u91cf\u4e0a\u91c7\u6837\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u6d41\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u517c\u5bb9\u590d\u6742\u51e0\u4f55\u548c\u975e\u5747\u5300\u7f51\u683c\uff0c\u5229\u7528\u53d8\u6362\u5668\u67b6\u6784\u6355\u83b7\u4f4e\u5206\u8fa8\u7387\u6d41\u573a\u4e2d\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u8bc6\u522b\u91cd\u8981\u7279\u5f81\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6d41\u573a\u3002", "result": "\u57282D\u6c22\u6c14-\u7a7a\u6c14\u9884\u6df7\u7206\u8f70\u4f20\u64ad\u7684\u6311\u6218\u6027\u6d4b\u8bd5\u95ee\u9898\u4e2d\uff0cSR-GT\u5728\u53cd\u5e94\u6d41\u573a\u7279\u5f81\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u7684\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\u3002", "conclusion": "SR-GT\u6846\u67b6\u4e3a\u590d\u6742\u51e0\u4f55\u548c\u975e\u5747\u5300\u7f51\u683c\u4e0a\u7684\u53cd\u5e94\u6d41\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.12071", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12071", "abs": "https://arxiv.org/abs/2511.12071", "authors": ["Rosario Napoli", "Gabriele Morabito", "Antonio Celesti", "Massimo Villari", "Maria Fazio"], "title": "Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread", "comment": "Accepted at the 16th IEEE International Conference on Knowledge Graphs (ICKG) 2025", "summary": "The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u77e5\u8bc6\u8865\u5168\u9636\u6bb5\u7684\u56fe\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u901a\u8fc7\u5efa\u6a21\u4f20\u9012\u5173\u7cfb\u7684\u9690\u85cf\u8fde\u63a5\u6765\u63ed\u793a\u7a00\u758f\u6570\u636e\u96c6\u4e2d\u7684\u9690\u542b\u77e5\u8bc6\uff0c\u4ece\u800c\u6539\u53d8\u56fe\u62d3\u6251\u7ed3\u6784\u548c\u5d4c\u5165\u7a7a\u95f4\u51e0\u4f55\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5d4c\u5165\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u663e\u5f0f\u62d3\u6251\u548c\u7279\u5f81\uff0c\u53ef\u80fd\u9057\u6f0f\u7a00\u758f\u6570\u636e\u96c6\u4e2d\u9690\u85cf\u7684\u5173\u952e\u9690\u542b\u77e5\u8bc6\uff0c\u5f71\u54cd\u56fe\u7ed3\u6784\u548c\u8868\u793a\u8d28\u91cf\u3002", "method": "\u5728\u5d4c\u5165\u751f\u6210\u524d\u52a0\u5165\u77e5\u8bc6\u8865\u5168\u9636\u6bb5\uff0c\u4f7f\u7528\u57fa\u4e8e\u8870\u51cf\u7684\u63a8\u7406\u51fd\u6570\u5efa\u6a21\u4f20\u9012\u5173\u7cfb\u7684\u9690\u85cf\u8fde\u63a5\uff0c\u91cd\u5851\u56fe\u62d3\u6251\u7ed3\u6784\uff0c\u5f71\u54cdGraphSAGE\u548cNode2Vec\u4e2d\u7684\u5d4c\u5165\u52a8\u6001\u548c\u805a\u5408\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6d41\u7a0b\u663e\u8457\u6539\u53d8\u4e86\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u8bc1\u660e\u77e5\u8bc6\u8865\u5168\u4e0d\u4ec5\u662f\u7b80\u5355\u7684\u4e30\u5bcc\uff0c\u800c\u662f\u91cd\u65b0\u5b9a\u4e49\u56fe\u8868\u793a\u8d28\u91cf\u7684\u53d8\u9769\u6027\u6b65\u9aa4\u3002", "conclusion": "\u96c6\u6210\u77e5\u8bc6\u8865\u5168\u7684\u56fe\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u80fd\u591f\u63ed\u793a\u6570\u636e\u96c6\u4e2d\u7684\u9690\u542b\u8bed\u4e49\uff0c\u4ece\u6839\u672c\u4e0a\u63d0\u5347\u56fe\u8868\u793a\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002"}}
{"id": "2511.13612", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13612", "abs": "https://arxiv.org/abs/2511.13612", "authors": ["Jiacheng Chen", "Qianjia Cheng", "Fangchen Yu", "Haiyuan Wan", "Yuchen Zhang", "Shenghe Zheng", "Junchi Yao", "Qingyang Zhang", "Haonan He", "Yun Luo", "Yufeng Zhao", "Futing Wang", "Li Sheng", "Chengxing Xie", "Yuxin Zuo", "Yizhuo Li", "Wenxauan Zeng", "Yulun Wu", "Rui Huang", "Dongzhan Zhou", "Kai Chen", "Yu Qiao", "Lei Bai", "Yu Cheng", "Ning Ding", "Bowen Zhou", "Peng Ye", "Ganqu Cui"], "title": "P1: Mastering Physics Olympiads with Reinforcement Learning", "comment": null, "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.", "AI": {"tldr": "P1\u7cfb\u5217\u5f00\u6e90\u7269\u7406\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5176\u4e2dP1-235B-A22B\u83b7\u5f97IPhO 2025\u91d1\u724c\uff0c\u5e76\u572813\u4e2a\u56fd\u9645\u7269\u7406\u7ade\u8d5b\u4e2d\u8d62\u5f9712\u679a\u91d1\u724c\u3002", "motivation": "\u63a8\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u8c1c\u9898\u89e3\u51b3\u5411\u79d1\u5b66\u7ea7\u63a8\u7406\u80fd\u529b\u53d1\u5c55\uff0c\u7269\u7406\u4f5c\u4e3a\u8fde\u63a5\u7b26\u53f7\u4e0e\u73b0\u5b9e\u7684\u57fa\u7840\u5b66\u79d1\uff0c\u662f\u6d4b\u8bd5\u8fd9\u79cd\u8f6c\u53d8\u7684\u6700\u4f73\u9886\u57df\u3002", "method": "\u5b8c\u5168\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5f00\u6e90\u7269\u7406\u63a8\u7406\u6a21\u578b\u5bb6\u65cfP1\uff0c\u5e76\u914d\u5907\u667a\u80fd\u4ee3\u7406\u6846\u67b6PhysicsMinions\u3002", "result": "P1-235B-A22B\u5728IPhO 2025\u83b7\u5f97\u91d1\u724c\u8868\u73b0\uff0c\u572813\u4e2a\u56fd\u9645\u7269\u7406\u7ade\u8d5b\u4e2d\u8d62\u5f9712\u679a\u91d1\u724c\uff1bP1-30B-A3B\u83b7\u5f97\u94f6\u724c\uff1b\u914d\u5907PhysicsMinions\u540e\u603b\u4f53\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "P1\u6a21\u578b\u4e0d\u4ec5\u5728\u7269\u7406\u63a8\u7406\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u4e0a\u4e5f\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12075", "abs": "https://arxiv.org/abs/2511.12075", "authors": ["Dong-Hee Shin", "Deok-Joong Lee", "Young-Han Son", "Tae-Eui Kam"], "title": "Treatment Stitching with Schr\u00f6dinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies", "comment": "19 pages, 5 figures, AAAI conference", "summary": "Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schr\u00f6dinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.", "AI": {"tldr": "\u63d0\u51fa\u4e86Treatment Stitching (TreatStitch)\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u62fc\u63a5\u73b0\u6709\u6cbb\u7597\u8f68\u8ff9\u6bb5\u6765\u751f\u6210\u4e34\u5e8a\u6709\u6548\u7684\u6cbb\u7597\u8f68\u8ff9\uff0c\u4ee5\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u4e34\u5e8a\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u81ea\u9002\u5e94\u6cbb\u7597\u7b56\u7565\u9700\u8981\u4e2a\u6027\u5316\u52a8\u6001\u8c03\u6574\u6cbb\u7597\u51b3\u7b56\uff0c\u4f46\u4f20\u7edf\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5b58\u5728\u98ce\u9669\uff0c\u800c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u53c8\u53d7\u9650\u4e8e\u4e34\u5e8a\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "TreatStitch\u6846\u67b6\u901a\u8fc7\u8bc6\u522b\u4e0d\u540c\u8f68\u8ff9\u4e2d\u7684\u76f8\u4f3c\u4e2d\u95f4\u60a3\u8005\u72b6\u6001\u5e76\u62fc\u63a5\u5176\u76f8\u5e94\u6bb5\uff0c\u5f53\u72b6\u6001\u5dee\u5f02\u8fc7\u5927\u65f6\u4f7f\u7528\u859b\u5b9a\u8c14\u6865\u65b9\u6cd5\u751f\u6210\u5e73\u6ed1\u7684\u6865\u63a5\u8f68\u8ff9\uff0c\u4ece\u800c\u589e\u5f3a\u6570\u636e\u96c6\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6cbb\u7597\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTreatStitch\u80fd\u6709\u6548\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "TreatStitch\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u9ad8\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u4fdd\u6301\u4e86\u4e34\u5e8a\u6709\u6548\u6027\uff0c\u907f\u514d\u4e86\u5206\u5e03\u5916\u8f6c\u79fb\u3002"}}
{"id": "2511.12092", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12092", "abs": "https://arxiv.org/abs/2511.12092", "authors": ["Yu Zheng", "Kezhi Wang", "Wenji Xi", "Gang Yu", "Jiming Chen", "Jie Zhang"], "title": "SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling", "comment": "Submitted for possible journal publications", "summary": "Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.", "AI": {"tldr": "SenseRay-3D\u662f\u4e00\u4e2a\u53ef\u6cdb\u5316\u7684\u7269\u7406\u4fe1\u606f\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u76f4\u63a5\u4eceRGB-D\u626b\u63cf\u9884\u6d4b\u4e09\u7ef4\u8def\u5f84\u635f\u8017\u70ed\u56fe\uff0c\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u91cd\u5efa\u6216\u6750\u6599\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185\u65e0\u7ebf\u7535\u4f20\u64ad\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u51e0\u4f55\u548c\u6750\u6599\u5c5e\u6027\u624b\u52a8\u5efa\u6a21\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u6709\u9650\u3002", "method": "\u6784\u5efa\u611f\u77e5\u9a71\u52a8\u7684\u4f53\u7d20\u5316\u573a\u666f\u8868\u793a\uff0c\u8054\u5408\u7f16\u7801\u5360\u7528\u7387\u3001\u7535\u78c1\u6750\u6599\u7279\u6027\u548c\u53d1\u5c04\u5668-\u63a5\u6536\u5668\u51e0\u4f55\u5173\u7cfb\uff0c\u901a\u8fc7SwinUNETR\u795e\u7ecf\u7f51\u7edc\u63a8\u65ad\u76f8\u5bf9\u4e8e\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u635f\u8017\u7684\u73af\u5883\u8def\u5f84\u635f\u8017\u3002", "result": "\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5b9e\u73b04.27 dB\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u652f\u6301\u6bcf\u6837\u672c217\u6beb\u79d2\u7684\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "SenseRay-3D\u4e3a\u611f\u77e5\u9a71\u52a8\u3001\u53ef\u6cdb\u5316\u4e14\u7269\u7406\u4e00\u81f4\u7684\u5ba4\u5185\u4f20\u64ad\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u7684EM DeepRay\u6846\u67b6\u3002"}}
{"id": "2511.12121", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.12121", "abs": "https://arxiv.org/abs/2511.12121", "authors": ["Wanlong Fang", "Tianle Zhang", "Alvin Chan"], "title": "To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance", "comment": null, "summary": "Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u53ef\u63a7\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u7814\u7a76\u663e\u5f0f\u5bf9\u9f50\u5bf9\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6700\u4f73\u5bf9\u9f50\u5f3a\u5ea6\u53d6\u51b3\u4e8e\u6a21\u6001\u95f4\u5197\u4f59\u5ea6\uff0c\u4e3a\u663e\u5f0f\u5bf9\u9f50\u7684\u5e94\u7528\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u5b66\u4e60\u5047\u8bbe\u8868\u793a\u5bf9\u9f50\u603b\u662f\u6709\u76ca\u7684\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u663e\u5f0f\u5bf9\u9f50\u76f4\u63a5\u5f71\u54cd\u7684\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5728\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u7ed3\u6784\u4e0b\uff0c\u663e\u5f0f\u5bf9\u9f50\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u8868\u793a\u5bf9\u9f50\u3002", "method": "\u5f15\u5165\u53ef\u63a7\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7cbe\u786e\u64cd\u63a7\u5bf9\u9f50\u5f3a\u5ea6\uff0c\u7814\u7a76\u663e\u5f0f\u5bf9\u9f50\u5728\u4e0d\u540c\u6570\u636e\u7279\u5f81\u4e0b\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u663e\u5f0f\u5bf9\u9f50\u5bf9\u5355\u6a21\u6001\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u4e0e\u6570\u636e\u7279\u5f81\u76f8\u5173\uff1a\u6700\u4f73\u5bf9\u9f50\u6c34\u5e73\u53d6\u51b3\u4e8e\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u5197\u4f59\u5ea6\u3002", "conclusion": "\u672c\u6587\u786e\u5b9a\u4e86\u5e73\u8861\u6a21\u6001\u7279\u5b9a\u4fe1\u53f7\u548c\u5171\u4eab\u5197\u4f59\u7684\u6700\u4f73\u5bf9\u9f50\u5f3a\u5ea6\uff0c\u4e3a\u5982\u4f55\u5e94\u7528\u663e\u5f0f\u5bf9\u9f50\u4ee5\u83b7\u5f97\u6700\u4f73\u5355\u6a21\u6001\u7f16\u7801\u5668\u6027\u80fd\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2511.12122", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12122", "abs": "https://arxiv.org/abs/2511.12122", "authors": ["Yi Wang", "Ruoyi Fang", "Anzhuo Xie", "Hanrui Feng", "Jianlin Lai"], "title": "Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks", "comment": null, "summary": "This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u5b9e\u65f6\u4f1a\u8ba1\u4ea4\u6613\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u548c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4ea4\u6613\u73af\u5883\u4e2d\u9690\u85cf\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u548c\u9ad8\u65f6\u6548\u6027\u8981\u6c42\u7684\u6311\u6218\uff0c\u4e3a\u667a\u80fd\u91d1\u878d\u98ce\u63a7\u548c\u5ba1\u8ba1\u63d0\u4f9b\u65b9\u6cd5\u652f\u6301\u3002", "method": "\u5c06\u591a\u7ef4\u4f1a\u8ba1\u4ea4\u6613\u8bb0\u5f55\u5efa\u6a21\u4e3a\u65f6\u95f4\u5e8f\u5217\u77e9\u9635\uff0c\u4f7f\u7528\u5d4c\u5165\u5c42\u548c\u4f4d\u7f6e\u7f16\u7801\u5b9e\u73b0\u4f4e\u7ef4\u6620\u5c04\uff0c\u6784\u5efa\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5e8f\u5217\u5efa\u6a21\u7ed3\u6784\uff0c\u7ed3\u5408\u524d\u9988\u5c42\u548c\u6b63\u5219\u5316\u7b56\u7565\u5b9e\u73b0\u6df1\u5ea6\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728AUC\u3001F1-Score\u3001Precision\u548cRecall\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u548c\u6570\u636e\u6270\u52a8\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684\u6846\u67b6\u5728\u4f1a\u8ba1\u4ea4\u6613\u52a8\u6001\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5177\u6709\u9002\u7528\u6027\u548c\u4f18\u52bf\uff0c\u4e3a\u667a\u80fd\u91d1\u878d\u98ce\u9669\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2511.12123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12123", "abs": "https://arxiv.org/abs/2511.12123", "authors": ["Zejiao Liu", "Junqi Tu", "Yitian Hong", "Luolin Xiong", "Yaochu Jin", "Yang Tang", "Fangfei Li"], "title": "HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning", "comment": "AAAI 2026", "summary": "In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6325\u8005\u7684\u8054\u5408\u7b56\u7565\u6846\u67b6HCPO\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u6765\u63d0\u5347\u5408\u4f5c\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u72ec\u7acb\u667a\u80fd\u4f53\u63a2\u7d22\u6765\u66f4\u65b0\u8054\u5408\u7b56\u7565\uff0c\u7f3a\u4e4f\u667a\u80fd\u4f53\u95f4\u7684\u534f\u8c03\uff0c\u8fd9\u9650\u5236\u4e86\u8054\u5408\u7b56\u7565\u7684\u8868\u8fbe\u80fd\u529b\u548c\u63a2\u7d22\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6307\u6325\u8005\u7684\u8054\u5408\u7b56\u7565\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u5206\u5c42\u6307\u6325\u8005\u7b56\u7565\u4f18\u5316\u7b97\u6cd5HCPO\uff0c\u901a\u8fc7\u6307\u6325\u8005\u6307\u5bfc\u667a\u80fd\u4f53\u7b56\u7565\u66f4\u65b0\uff0c\u5e76\u90e8\u7f72\u672c\u5730\u6307\u6325\u8005\u5728\u6267\u884c\u65f6\u6d88\u9664\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u9700\u6c42\u3002", "result": "\u5728StarCraftII\u591a\u667a\u80fd\u4f53\u6311\u6218\u3001\u591a\u667a\u80fd\u4f53MuJoCo\u548c\u591a\u667a\u80fd\u4f53\u7c92\u5b50\u73af\u5883\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHCPO\u5728\u5408\u4f5c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u7ade\u4e89\u6027MARL\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HCPO\u901a\u8fc7\u534f\u8c03\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u6709\u6548\u63d0\u5347\u4e86\u8054\u5408\u7b56\u7565\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u7684\u4f18\u52bf\u5e76\u6d88\u9664\u4e86\u6267\u884c\u65f6\u7684\u901a\u4fe1\u9700\u6c42\u3002"}}
{"id": "2511.12132", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12132", "abs": "https://arxiv.org/abs/2511.12132", "authors": ["Zhenqiang Ye", "Jinjie Lu", "Tianlong Gu", "Fengrui Hao", "Xuemin Wang"], "title": "FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates", "comment": "AAAI 2026", "summary": "Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.", "AI": {"tldr": "FairGSE\u662f\u4e00\u4e2a\u901a\u8fc7\u6700\u5927\u5316\u4e8c\u7ef4\u7ed3\u6784\u71b5\u6765\u6539\u8fdb\u56fe\u795e\u7ecf\u7f51\u7edc\u516c\u5e73\u6027\u7684\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u516c\u5e73\u6027\u6539\u8fdb\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5047\u9633\u6027\u7387\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u611f\u77e5GNN\u5728\u8ffd\u6c42\u516c\u5e73\u6027\u6307\u6807\u65f6\u5ffd\u89c6\u4e86\u6a21\u578b\u9884\u6d4b\u8d1f\u6807\u7b7e\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u6781\u9ad8\u7684\u5047\u9633\u6027\u7387\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u4f1a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u63d0\u51faFairGSE\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4e8c\u7ef4\u7ed3\u6784\u71b5\u6765\u6539\u8fdb\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4e0d\u5ffd\u89c6\u5047\u9633\u6027\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFairGSE\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u516c\u5e73\u611f\u77e5GNN\u5c06\u5047\u9633\u6027\u7387\u964d\u4f4e\u4e8639%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u516c\u5e73\u6027\u6539\u8fdb\u3002", "conclusion": "\u5728\u6539\u8fdb\u516c\u5e73\u6027\u65f6\u5e94\u4ed4\u7ec6\u6821\u51c6\u5206\u7c7b\u6027\u80fd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7ea6\u675f\u51c6\u786e\u7387\u635f\u5931\uff1bFairGSE\u901a\u8fc7\u7ed3\u6784\u71b5\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u5047\u9633\u6027\u7387\u8fc7\u9ad8\u7684\u95ee\u9898\u3002"}}
{"id": "2511.12143", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12143", "abs": "https://arxiv.org/abs/2511.12143", "authors": ["Jialiang Wang", "Xiong Zhou", "Xianming Liu", "Gangfeng Hu", "Deming Zhai", "Junjun Jiang", "Haoliang Li"], "title": "Variation-Bounded Loss for Noise-Tolerant Learning", "comment": "Accepted by AAAI2026", "summary": "Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u635f\u5931\u51fd\u6570\u7279\u6027\u2014\u2014\u53d8\u5f02\u6bd4\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u53d8\u5f02\u6709\u754c\u635f\u5931\u51fd\u6570\u5bb6\u65cf\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8f83\u5c0f\u7684\u53d8\u5f02\u6bd4\u80fd\u5e26\u6765\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7f13\u89e3\u566a\u58f0\u6807\u7b7e\u5bf9\u76d1\u7763\u5b66\u4e60\u7684\u8d1f\u9762\u5f71\u54cd\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\uff0c\u9c81\u68d2\u635f\u5931\u51fd\u6570\u662f\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u6d41\u884c\u65b9\u6848\u3002", "method": "\u5f15\u5165\u53d8\u5f02\u6bd4\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u9c81\u68d2\u6027\u7684\u65b0\u5c5e\u6027\uff0c\u63d0\u51fa\u53d8\u5f02\u6709\u754c\u635f\u5931\u51fd\u6570\u5bb6\u65cf\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u53d8\u5f02\u6bd4\u7279\u6027\uff0c\u5e76\u5c06\u5e38\u7528\u635f\u5931\u51fd\u6570\u91cd\u65b0\u8868\u8ff0\u4e3a\u53d8\u5f02\u6709\u754c\u5f62\u5f0f\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u53d8\u5f02\u6bd4\u4e3a\u653e\u677e\u5bf9\u79f0\u6761\u4ef6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\uff0c\u5e76\u4e3a\u5b9e\u73b0\u975e\u5bf9\u79f0\u6761\u4ef6\u63d0\u4f9b\u4e86\u66f4\u7b80\u6d01\u7684\u8def\u5f84\uff0c\u6240\u63d0\u51fa\u7684\u53d8\u5f02\u6709\u754c\u635f\u5931\u51fd\u6570\u5728\u566a\u58f0\u6807\u7b7e\u573a\u666f\u4e0b\u5177\u6709\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2511.12154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12154", "abs": "https://arxiv.org/abs/2511.12154", "authors": ["Gustavo Polleti", "Marlesson Santana", "Eduardo Fontes"], "title": "Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions", "comment": null, "summary": "We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u7ed3\u6784\u5316\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5728\u91d1\u878d\u4ea4\u6613\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5f00\u653e\u94f6\u884c\u573a\u666f\u4e2d\u3002", "motivation": "\u89e3\u51b3\u91d1\u878d\u4ea4\u6613\u4e2d\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u548c\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u591a\u6a21\u6001\u8868\u793a\u5728\u8de8\u5730\u57df\u548c\u673a\u6784\u6cdb\u5316\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u9002\u5e94\u4ea4\u6613\u5e8f\u5217\uff0c\u6574\u5408\u7ed3\u6784\u5316\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\u5230\u7edf\u4e00\u8868\u793a\u4e2d\u3002", "result": "\u6a21\u578b\u5728\u91d1\u878d\u4ea4\u6613\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u8868\u793a\u7684\u8de8\u5730\u57df\u548c\u673a\u6784\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u81ea\u76d1\u7763\u6a21\u578b\u5728\u91d1\u878d\u5e94\u7528\uff08\u5982\u6b3a\u8bc8\u9884\u9632\u3001\u4fe1\u7528\u98ce\u9669\u548c\u5ba2\u6237\u6d1e\u5bdf\uff09\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u591a\u6a21\u6001\u8868\u793a\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.12155", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12155", "abs": "https://arxiv.org/abs/2511.12155", "authors": ["Thong Bach", "Dung Nguyen", "Thao Minh Le", "Truyen Tran"], "title": "Rethinking Deep Alignment Through The Lens Of Incomplete Learning", "comment": "AAAI'26", "summary": "Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u56de\u5f52\u8bad\u7ec3\u4e2d\u7531\u4e8e\u4f4d\u7f6e\u4f9d\u8d56\u7684\u68af\u5ea6\u51cf\u5f31\u5bfc\u81f4\u5b89\u5168\u5b66\u4e60\u4e0d\u5b8c\u6574\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u57fa\u7840\u504f\u597d\u6807\u8bb0\u7684\u9488\u5bf9\u6027\u8865\u5168\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u5b58\u5728\u7cfb\u7edf\u6027\u5bf9\u6297\u653b\u51fb\u6f0f\u6d1e\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5b89\u5168\u5bf9\u9f50\u7684\u6839\u672c\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u57fa\u7840\u504f\u597d\u6807\u8bb0\u4f5c\u4e3a\u5b89\u5168\u5b66\u4e60\u4e0d\u5b8c\u6574\u7684\u8ba1\u7b97\u6307\u6807\uff0c\u5f00\u53d1\u4e86\u5305\u542b\u81ea\u9002\u5e94\u60e9\u7f5a\u548c\u6df7\u5408\u6559\u5e08\u84b8\u998f\u7684\u9488\u5bf9\u6027\u8865\u5168\u65b9\u6cd5\u3002", "result": "\u5728Llama\u548cQwen\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u4e8648-98%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u4e3a\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u7684\u57fa\u672c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u7406\u89e3\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12158", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12158", "abs": "https://arxiv.org/abs/2511.12158", "authors": ["Houtan Ghaffari", "Lukas Rauch", "Paul Devos"], "title": "Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis", "comment": null, "summary": "Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784Residual-MLP-RNN\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7528\u4e8e\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u9e1f\u7c7b\u9e23\u58f0\u97f3\u8282\u6807\u6ce8\u3002", "motivation": "\u9e1f\u7c7b\u9e23\u58f0\u7814\u7a76\u9700\u8981\u7cbe\u786e\u7684\u97f3\u8282\u7ea7\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u548c\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "method": "\u4f7f\u7528Residual-MLP-RNN\u67b6\u6784\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u65e0\u76d1\u7763\u9884\u8bad\u7ec3\uff08\u63a9\u7801\u9884\u6d4b\u548c\u5728\u7ebf\u805a\u7c7b\uff09\uff1b2) \u5e26\u6570\u636e\u589e\u5f3a\u7684\u76d1\u7763\u8bad\u7ec3\uff1b3) \u534a\u76d1\u7763\u540e\u8bad\u7ec3\u3002\u5728\u6807\u7b7e\u7a00\u7f3a\u573a\u666f\u4e0b\u6d4b\u8bd5\u4e8e\u91d1\u4e1d\u96c0\u590d\u6742\u9e23\u58f0\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6807\u6ce8\u96be\u5ea6\u6781\u9ad8\u7684\u91d1\u4e1d\u96c0\u9e23\u58f0\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5bf9\u5176\u4ed6\u9e1f\u7c7b\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u80fd\u591f\u5728\u6781\u7aef\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u9e1f\u7c7b\u9e23\u58f0\u97f3\u8282\u68c0\u6d4b\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u5d4c\u5165\u5728\u7ebf\u6027\u63a2\u6d4b\u548c\u65e0\u76d1\u7763\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12171", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.12171", "abs": "https://arxiv.org/abs/2511.12171", "authors": ["Chaitanya Kumar Konda", "Piyush Agrawal", "Shivansh Srivastava", "Manish Agrawal"], "title": "FGM optimization in complex domains using Gaussian process regression based profile generation algorithm", "comment": null, "summary": "This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7684\u901a\u7528\u4f53\u79ef\u5206\u6570\u5206\u5e03\u751f\u6210\u7b97\u6cd5\uff0c\u7528\u4e8e\u4efb\u610f\u5f62\u72b6\u529f\u80fd\u68af\u5ea6\u6750\u6599\u8bbe\u8ba1\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u4efb\u610f\u5f62\u72b6\u57df\u4e2d\u529f\u80fd\u68af\u5ea6\u6750\u6599\u8bbe\u8ba1\u7684\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u590d\u6742\u5f62\u72b6\u57df\u5e76\u751f\u6210\u5e73\u6ed1\u5206\u5e03\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u751f\u6210\u4f53\u79ef\u5206\u6570\u5206\u5e03\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u9057\u4f20\u7b97\u6cd5\uff08\u4f7f\u7528\u6295\u5f71\u7b97\u5b50\u66ff\u4ee3\u6807\u51c6\u6a21\u62df\u4e8c\u8fdb\u5236\u4ea4\u53c9\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u590d\u6742\u5f62\u72b6\u57df\uff0c\u751f\u6210\u5e73\u6ed1\u7684\u529f\u80fd\u68af\u5ea6\u6750\u6599\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u957f\u5ea6\u5c3a\u5ea6\u53c2\u6570\u63a7\u5236\u5206\u5e03\u5e73\u6ed1\u5ea6\u548c\u8bbe\u8ba1\u7a7a\u95f4\u5927\u5c0f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u751f\u6210\u7b97\u6cd5\u548c\u4f18\u5316\u6846\u67b6\u5728\u70ed\u5f39\u6027\u4f18\u5316\u793a\u4f8b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6548\u679c\uff0c\u4e3a\u529f\u80fd\u68af\u5ea6\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.12174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12174", "abs": "https://arxiv.org/abs/2511.12174", "authors": ["Lifeng Shen", "Xuyang Li", "Lele Long"], "title": "TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective", "comment": "Accepted by AAAI 2026", "summary": "Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \\textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \\textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.", "AI": {"tldr": "TSGDiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e3a\u52a8\u6001\u56fe\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86Topo-FID\u7ed3\u6784\u76f8\u4f3c\u6027\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u6355\u6349\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u7ed3\u6784\u6a21\u5f0f\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e3a\u57fa\u4e8e\u5085\u91cc\u53f6\u8c31\u7279\u5f81\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u52a8\u6001\u56fe\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f7f\u6269\u6563\u8fc7\u7a0b\u80fd\u591f\u6709\u6548\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217\u7684\u7ed3\u6784\u8868\u793a\u5206\u5e03\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTSGDiff\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5fe0\u5b9e\u5730\u4fdd\u7559\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "conclusion": "TSGDiff\u901a\u8fc7\u56fe\u89c6\u89d2\u91cd\u65b0\u601d\u8003\u65f6\u95f4\u5e8f\u5217\u751f\u6210\uff0c\u63d0\u51fa\u7684Topo-FID\u6307\u6807\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u63a8\u52a8\u4e86\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.12188", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12188", "abs": "https://arxiv.org/abs/2511.12188", "authors": ["Xuanyu Chen", "Nan Yang", "Shuai Wang", "Dong Yuan"], "title": "Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?", "comment": "The extended version of the paper \"Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?\". Accepted by AAAI2026", "summary": "The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u7684\u7406\u8bba\u57fa\u7840\uff0c\u53d1\u73b0\u6700\u4f18\u6a21\u578b\u5927\u5c0f\u4e0e\u5ba2\u6237\u7aef\u6570\u91cf\u5448\u8d1f\u5e42\u5f8b\u5173\u7cfb\uff0c\u4e14\u5728\u76f8\u540c\u8ba1\u7b97\u91cf\u4e0b\u8054\u90a6\u5b66\u4e60\u4f1a\u964d\u4f4e\u6cdb\u5316\u6027\u80fd\u4e0a\u754c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u65e5\u76ca\u7a00\u7f3a\uff0c\u8054\u90a6\u5b66\u4e60\u80fd\u5229\u7528\u8fb9\u7f18\u8bbe\u5907\u6570\u636e\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u5176\u5206\u5e03\u5f0f\u7279\u6027\u5bf9\u6a21\u578b\u6269\u5c55\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63a8\u5bfc\u4e86\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u968f\u673a\u7b97\u6cd5\u7684PAC-Bayes\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\uff0c\u901a\u8fc7\u89e3\u6790\u6c42\u89e3\u6700\u5c0f\u5316\u8be5\u4e0a\u754c\u7684\u6a21\u578b\u89c4\u6a21\uff0c\u5e76\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u6700\u4f18\u6a21\u578b\u5927\u5c0f\u4e0e\u5ba2\u6237\u7aef\u6570\u91cf\u5448\u8d1f\u5e42\u5f8b\u5173\u7cfb\uff0c\u8054\u90a6\u5b66\u4e60\u4f1a\u964d\u4f4e\u6cdb\u5316\u6027\u80fd\u4e0a\u754c\uff0c\u6700\u4f18\u6a21\u578b\u89c4\u6a21\u5e94\u57fa\u4e8e\u5ba2\u6237\u7aef\u5e73\u5747\u8ba1\u7b97\u91cf\u4f30\u8ba1\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6269\u5c55\u9700\u8981\u91cd\u65b0\u8003\u8651\uff0c\u6700\u4f18\u6a21\u578b\u89c4\u6a21\u53d7\u5ba2\u6237\u7aef\u6570\u91cf\u548c\u5206\u5e03\u5f71\u54cd\uff0c\u4e0d\u80fd\u7b80\u5355\u6cbf\u7528\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u7684\u7ecf\u9a8c\u3002"}}
{"id": "2511.12191", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12191", "abs": "https://arxiv.org/abs/2511.12191", "authors": ["Szymon Wojciechowski", "Micha\u0142 Wo\u017aniak"], "title": "Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data", "comment": null, "summary": "Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.\n  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u9760\u65b9\u6cd5\u6765\u8bc4\u4f30\u57fa\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u7684\u7b97\u6cd5\u4e0e\u8fd4\u56de\u5355\u4e00\u89e3\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u662f\u6bd4\u8f83\u7b97\u6cd5\u6027\u80fd\u800c\u975e\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u8bb8\u591a\u4efb\u52a1\u9700\u8981\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76f8\u4e92\u51b2\u7a81\u7684\u6807\u51c6\uff08\u5982\u4e0d\u5e73\u8861\u6570\u636e\u5206\u7c7b\uff09\u3002\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u4f1a\u4ea7\u751f\u4e00\u7ec4\u975e\u652f\u914d\u89e3\uff08Pareto\u524d\u6cbf\uff09\uff0c\u800c\u5982\u4f55\u4ecePareto\u524d\u6cbf\u4e2d\u9009\u62e9\u5355\u4e00\u89e3\u5e76\u4e0e\u8fd4\u56de\u5355\u4e00\u89e3\u7684\u65b9\u6cd5\u8fdb\u884c\u53ef\u9760\u6bd4\u8f83\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u53ef\u9760\u5730\u6bd4\u8f83\u8fd4\u56dePareto\u524d\u6cbf\u7684\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u4e0e\u8fd4\u56de\u5355\u4e00\u89e3\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u7528\u6237\u504f\u597d\u6765\u9009\u62e9Pareto\u524d\u6cbf\u4e2d\u7684\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a\u7b97\u6cd5\u6bd4\u8f83\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\uff0c\u80fd\u591f\u516c\u5e73\u5730\u8bc4\u4f30\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u4e0e\u5355\u4e00\u89e3\u65b9\u6cd5\u5728\u6ee1\u8db3\u7528\u6237\u504f\u597d\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "\u586b\u8865\u4e86\u5206\u7c7b\u5668\u8bc4\u4f30\u65b9\u6cd5\u5b66\u4e2d\u7684\u91cd\u8981\u7a7a\u767d\uff0c\u4e3a\u6bd4\u8f83\u8fd4\u56dePareto\u524d\u6cbf\u7684\u7b97\u6cd5\u4e0e\u8fd4\u56de\u5355\u4e00\u89e3\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u9760\u9014\u5f84\uff0c\u540c\u65f6\u8003\u8651\u4e86\u7528\u6237\u504f\u597d\u56e0\u7d20\u3002"}}
{"id": "2511.12199", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12199", "abs": "https://arxiv.org/abs/2511.12199", "authors": ["Runhao Jiang", "Chengzhi Jiang", "Rui Yan", "Huajin Tang"], "title": "MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization", "comment": "Accepted by AAAI 2026", "summary": "The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u819c\u7535\u4f4d\u5206\u5e03\u4e0e\u66ff\u4ee3\u68af\u5ea6\u51fd\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MPD-SGR\u65b9\u6cd5\u6765\u589e\u5f3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u66ff\u4ee3\u68af\u5ea6\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u6df1\u5ea6\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u4f46\u4e5f\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\u3002\u76ee\u524d\u5bf9\u68af\u5ea6\u5e45\u503c\uff08\u53cd\u6620\u6a21\u578b\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\uff09\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u800c\u68af\u5ea6\u5e45\u503c\u4e3b\u8981\u7531\u819c\u7535\u4f4d\u5206\u5e03\u4e0e\u66ff\u4ee3\u68af\u5ea6\u51fd\u6570\u7684\u76f8\u4e92\u4f5c\u7528\u51b3\u5b9a\u3002", "method": "\u63d0\u51faMPD\u9a71\u52a8\u7684\u66ff\u4ee3\u68af\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u819c\u7535\u4f4d\u5206\u5e03\u4e0e\u66ff\u4ee3\u68af\u5ea6\u51fd\u6570\u76f8\u4e92\u4f5c\u7528\u7684\u6b63\u5219\u5316\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u51cf\u5c11\u819c\u7535\u4f4d\u5728\u66ff\u4ee3\u68af\u5ea6\u51fd\u6570\u68af\u5ea6\u53ef\u7528\u8303\u56f4\u5185\u7684\u6bd4\u4f8b\u53ef\u4ee5\u6709\u6548\u964d\u4f4eSNN\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u548c\u4e0d\u540c\u7f51\u7edc\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMPD-SGR\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86SNN\u5bf9\u5bf9\u6297\u6270\u52a8\u7684\u62b5\u6297\u529b\uff0c\u5e76\u5728\u4e0d\u540c\u7f51\u7edc\u914d\u7f6e\u3001\u66ff\u4ee3\u68af\u5ea6\u51fd\u6570\u53d8\u4f53\u548c\u8109\u51b2\u7f16\u7801\u65b9\u6848\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MPD-SGR\u65b9\u6cd5\u901a\u8fc7\u8c03\u8282\u819c\u7535\u4f4d\u5206\u5e03\u4e0e\u66ff\u4ee3\u68af\u5ea6\u51fd\u6570\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2511.12217", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12217", "abs": "https://arxiv.org/abs/2511.12217", "authors": ["Gil Goren", "Shahar Katz", "Lior Wolf"], "title": "AlignTree: Efficient Defense Against LLM Jailbreak Attacks", "comment": "Accepted as an Oral Presentation at the 40th AAAI Conference on Artificial Intelligence (AAAI-26), January 2026", "summary": "Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.", "AI": {"tldr": "AlignTree\u662f\u4e00\u79cd\u9ad8\u6548\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u76d1\u63a7LLM\u6fc0\u6d3b\u5e76\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\uff0c\u65e0\u9700\u989d\u5916\u63d0\u793a\u6216\u8f85\u52a9\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u5bb9\u6613\u88ab\u7ed5\u8fc7\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645LLM\u7cfb\u7edf\uff0c\u9700\u8981\u65e2\u9c81\u68d2\u53c8\u9ad8\u6548\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u76d1\u63a7LLM\u6fc0\u6d3b\uff0c\u7ed3\u5408\u62d2\u7edd\u65b9\u5411\uff08\u7ebf\u6027\u7279\u5f81\uff09\u548cSVM\u4fe1\u53f7\uff08\u975e\u7ebf\u6027\u7279\u5f81\uff09\u6765\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u4e86AlignTree\u7684\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "AlignTree\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u5bf9\u9f50\u6027\u540c\u65f6\u4fdd\u6301\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2511.12222", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12222", "abs": "https://arxiv.org/abs/2511.12222", "authors": ["Hangshuo Tian"], "title": "Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling", "comment": null, "summary": "Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.\n  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.\n  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \\emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u9e21\u7fa4\u4f18\u5316\u7b97\u6cd5(CSO)\u4e0eKLD\u81ea\u9002\u5e94\u91c7\u6837\u5728\u7c92\u5b50\u6ee4\u6ce2\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u63d0\u51faCSO\u7684\u9002\u5e94\u5ea6\u9a71\u52a8\u66f4\u65b0\u53ef\u8fd1\u4f3c\u4e3a\u5747\u65b9\u6536\u7f29\uff0c\u4f7f\u7c92\u5b50\u5206\u5e03\u66f4\u96c6\u4e2d\uff0c\u4ece\u800c\u5728\u76f8\u540c\u7edf\u8ba1\u8bef\u5dee\u4e0b\u9700\u8981\u66f4\u5c11\u7684\u7c92\u5b50\u6570\u91cf\u3002", "motivation": "\u7406\u89e3\u57fa\u4e8e\u7fa4\u4f53\u667a\u80fd\u7684\u7c92\u5b50\u66f4\u65b0\u6838\u4e0eKLD\u81ea\u9002\u5e94\u91c7\u6837\u4e4b\u95f4\u7684\u7406\u8bba\u76f8\u4e92\u4f5c\u7528\uff0c\u8fd9\u4e24\u79cd\u6280\u672f\u5728\u5b9e\u9645\u4e2d\u7ed3\u5408\u4f7f\u7528\u65f6\u8868\u73b0\u51fa\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u7406\u8bba\u673a\u5236\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\u3002", "method": "\u5728\u7b80\u5316\u5efa\u6a21\u6846\u67b6\u4e0b\u5206\u6790CSO\u66f4\u65b0\u6b65\u9aa4\u5bf9\u7c92\u5b50\u96c6\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u5c06CSO\u7684\u9002\u5e94\u5ea6\u9a71\u52a8\u66f4\u65b0\u8fd1\u4f3c\u4e3a\u5747\u65b9\u6536\u7f29\uff0c\u5e76\u5e94\u7528Karamata\u4e0d\u7b49\u5f0f\u5206\u6790KLD\u91c7\u6837\u4e2d\u7684\u671f\u671b\u7bb1\u5360\u7528\u51fd\u6570\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5728\u6240\u8ff0\u5047\u8bbe\u4e0b\uff0cCSO\u589e\u5f3a\u7684\u7c92\u5b50\u6ee4\u6ce2(CPF)\u76f8\u6bd4\u6807\u51c6PF\uff0c\u4e3a\u6ee1\u8db3\u76f8\u540c\u7edf\u8ba1\u8bef\u5dee\u754c\u9650\uff0c\u9700\u8981\u66f4\u4f4e\u7684\u671f\u671b\u7c92\u5b50\u6570\u91cf\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u5904\u7406\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u4e24\u79cd\u6280\u672f\u7ed3\u5408\u65f6\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6ee4\u6ce2\u5668\u63d0\u4f9b\u4e86\u8d77\u70b9\uff0c\u4f46\u5e76\u975e\u63d0\u4f9b\u5b8c\u5168\u901a\u7528\u7684\u8bc1\u660e\u3002"}}
{"id": "2511.12305", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12305", "abs": "https://arxiv.org/abs/2511.12305", "authors": ["Zhizhen Li", "Xuanhao Luo", "Xueren Ge", "Longyu Zhou", "Xingqin Lin", "Yuchen Liu"], "title": "MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing", "comment": null, "summary": "Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.", "AI": {"tldr": "MMSense\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u65e0\u7ebf\u611f\u77e5\uff0c\u6574\u5408\u56fe\u50cf\u3001\u96f7\u8fbe\u3001LiDAR\u548c\u6587\u672c\u6570\u636e\uff0c\u901a\u8fc7\u89c6\u89c9\u517c\u5bb9\u8868\u793a\u548c\u6a21\u6001\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5728\u771f\u5b9e\u65e0\u7ebf\u573a\u666f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578bAI\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u8f93\u5165\u548c\u7279\u5b9a\u4fe1\u9053\u76ee\u6807\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u57fa\u7840\u6a21\u578b\u5728\u7edf\u4e00\u65e0\u7ebf\u611f\u77e5\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u5c06\u591a\u79cd\u6a21\u6001\u6570\u636e\u8f6c\u6362\u4e3a\u89c6\u89c9\u517c\u5bb9\u8868\u793a\uff0c\u4f7f\u7528\u6a21\u6001\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\uff0c\u57fa\u4e8e\u89c6\u89c9\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u5b9e\u73b0\u7edf\u4e00\u7279\u5f81\u5bf9\u9f50\u548c\u6307\u4ee4\u9a71\u52a8\u7684\u4efb\u52a1\u9002\u5e94\uff0c\u91c7\u7528\u4efb\u52a1\u7279\u5b9a\u5e8f\u5217\u6ce8\u610f\u529b\u548c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u635f\u5931\u52a0\u6743\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u65e0\u7ebf\u573a\u666f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f02\u6784\u611f\u77e5\u4efb\u52a1\u4e0a\u4f18\u4e8e\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u548c\u5927\u578b\u6a21\u578b\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MMSense\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u65e0\u7ebf\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u4efb\u52a1\u9002\u5e94\u673a\u5236\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u591a\u6837\u5316\u611f\u77e5\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12315", "categories": ["cs.LG", "cs.FL"], "pdf": "https://arxiv.org/pdf/2511.12315", "abs": "https://arxiv.org/abs/2511.12315", "authors": ["Sebastian Hagedorn", "Mart\u00edn Mu\u00f1oz", "Cristian Riveros", "Rodrigo Toro Icarte"], "title": "Active Learning of Symbolic Automata Over Rational Numbers", "comment": null, "summary": "Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.", "AI": {"tldr": "\u5c06L*\u7b97\u6cd5\u6269\u5c55\u5230\u5b66\u4e60\u7b26\u53f7\u81ea\u52a8\u673a\uff0c\u652f\u6301\u6709\u7406\u6570\u4e0a\u7684\u65e0\u9650\u7a20\u5bc6\u5b57\u6bcd\u8868\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u65b0\u573a\u666f\u5982(\u5b9e)RGX\u548c\u65f6\u95f4\u5e8f\u5217\uff0c\u4e14\u67e5\u8be2\u590d\u6742\u5ea6\u6700\u4f18\u3002", "motivation": "L*\u7b97\u6cd5\u53ea\u80fd\u5b66\u4e60\u6709\u9650\u5b57\u6bcd\u8868\u4e0a\u7684DFA\uff0c\u9650\u5236\u4e86\u5176\u5728\u4eba\u5de5\u667a\u80fd\u548c\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u6269\u5c55\u4ee5\u652f\u6301\u65e0\u9650\u7a20\u5bc6\u5b57\u6bcd\u8868\uff0c\u5982\u6709\u7406\u6570\u4e0a\u7684\u8c13\u8bcd\u3002", "method": "\u6269\u5c55L*\u7b97\u6cd5\u4ee5\u5b66\u4e60\u7b26\u53f7\u81ea\u52a8\u673a\uff0c\u5176\u8f6c\u6362\u4f7f\u7528\u6709\u7406\u6570\u4e0a\u7684\u8c13\u8bcd\uff0c\u5373\u652f\u6301\u65e0\u9650\u7a20\u5bc6\u5b57\u6bcd\u8868\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u67e5\u8be2\u590d\u6742\u5ea6\u4e0a\u662f\u6700\u4f18\u7684\uff0c\u67e5\u8be2\u6b21\u6570\u4e0e\u8f6c\u6362\u6570\u91cf\u548c\u8c13\u8bcd\u8868\u793a\u5927\u5c0f\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "\u6210\u529f\u6269\u5c55\u4e86L*\u7b97\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5b66\u4e60\u7b26\u53f7\u81ea\u52a8\u673a\uff0c\u9002\u7528\u4e8e\u65b0\u573a\u666f\uff0c\u5e76\u4fdd\u6301\u4e86\u67e5\u8be2\u590d\u6742\u5ea6\u7684\u6700\u4f18\u6027\u3002"}}
{"id": "2511.12316", "categories": ["cs.LG", "cs.CE", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.12316", "abs": "https://arxiv.org/abs/2511.12316", "authors": ["Zhijun Zeng", "Junqing Chen", "Zuoqiang Shi"], "title": "BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data", "comment": null, "summary": "We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86BlinDNO\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u65f6\u95f4\u6807\u7b7e\u7f3a\u5931\u7684\u5bc6\u5ea6\u5feb\u7167\u4e2d\u6062\u590d\u968f\u673a\u548c\u91cf\u5b50\u52a8\u529b\u7cfb\u7edf\u7684\u53c2\u6570\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u5728\u65f6\u95f4\u6807\u7b7e\u7f3a\u5931\u60c5\u51b5\u4e0b\uff08\u53ea\u6709\u65e0\u5e8f\u5bc6\u5ea6\u5feb\u7167\uff09\u6062\u590d\u968f\u673a\u548c\u91cf\u5b50\u52a8\u529b\u7cfb\u7edf\u53c2\u6570\u7684\u53cd\u95ee\u9898\uff0c\u8fd9\u5728\u51b7\u51bb\u7535\u955c\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u5e38\u89c1\u3002", "method": "\u63d0\u51faBlinDNO\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6U-Net\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df7\u5408\u5668\uff0c\u5b66\u4e60\u4ece\u5206\u5e03\u5230\u51fd\u6570\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u5177\u6709\u7f6e\u6362\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u591a\u79cd\u968f\u673a\u548c\u91cf\u5b50\u7cfb\u7edf\uff08\u5305\u62ec3D\u86cb\u767d\u8d28\u6298\u53e0\u673a\u5236\u91cd\u5efa\uff09\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cBlinDNO\u80fd\u53ef\u9760\u6062\u590d\u63a7\u5236\u53c2\u6570\uff0c\u4e14\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u795e\u7ecf\u9006\u7b97\u5b50\u57fa\u7ebf\u3002", "conclusion": "BlinDNO\u65b9\u6cd5\u5728\u65f6\u95f4\u6807\u7b7e\u7f3a\u5931\u7684\u53cd\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u53c2\u6570\u6062\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12340", "abs": "https://arxiv.org/abs/2511.12340", "authors": ["Katarzyna Fojcik", "Renaldas Zioma", "Jogundas Armaitis"], "title": "LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment", "comment": null, "summary": "Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.", "AI": {"tldr": "\u63d0\u51faLILogicNet\u6a21\u578b\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u4e8c\u8fdb\u5236\u903b\u8f91\u95e8\u7f51\u7edc\u7684\u7ed3\u6784\u548c\u8fde\u63a5\uff0c\u663e\u8457\u51cf\u5c11\u6240\u9700\u903b\u8f91\u95e8\u6570\u91cf\uff0c\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "motivation": "\u8003\u8651\u786c\u4ef6\u7ea6\u675f\u9ad8\u6548\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4e8c\u8fdb\u5236\u903b\u8f91\u95e8\u662f\u6570\u5b57\u82af\u7247\u7684\u57fa\u672c\u6784\u5efa\u5757\uff0c\u8bbe\u8ba1\u76f4\u63a5\u5728\u8fd9\u4e9b\u5355\u5143\u4e0a\u8fd0\u884c\u7684\u6a21\u578b\u53ef\u5b9e\u73b0\u8282\u80fd\u8ba1\u7b97\u3002", "method": "\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u4e0d\u4ec5\u9009\u62e9\u903b\u8f91\u95e8\u7c7b\u578b\uff0c\u8fd8\u4f18\u5316\u5176\u4e92\u8fde\u7ed3\u6784\uff08\u8fde\u63a5\u7ec4\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u8fde\u63a5\u5927\u5e45\u51cf\u5c11\u62df\u5408\u7279\u5b9a\u6570\u636e\u96c6\u6240\u9700\u7684\u903b\u8f91\u95e8\u6570\u91cf\u3002", "result": "LILogicNet\u4ec5\u75288,000\u4e2a\u95e8\u5728MNIST\u4e0a\u8bad\u7ec3\u4e0d\u52305\u5206\u949f\u8fbe\u523098.45%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u5339\u914d\u9700\u8981\u81f3\u5c11\u4e24\u4e2a\u6570\u91cf\u7ea7\u66f4\u591a\u95e8\u7684\u6a21\u578b\u6027\u80fd\uff1b256,000\u4e2a\u95e8\u7684\u67b6\u6784\u5728CIFAR-10\u4e0a\u8fbe\u523060.98%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u540c\u7c7b\u903b\u8f91\u95e8\u6a21\u578b\u3002", "conclusion": "\u5b8c\u5168\u4e8c\u503c\u5316\u6a21\u578b\u5728\u63a8\u7406\u65f6\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\uff0c\u975e\u5e38\u9002\u5408\u5728\u4f4e\u529f\u8017\u6570\u5b57\u786c\u4ef6\u4e0a\u90e8\u7f72\uff0c\u4e3a\u9ad8\u6548\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.12351", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12351", "abs": "https://arxiv.org/abs/2511.12351", "authors": ["Bahareh Golchin", "Banafsheh Rekabdar"], "title": "Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach", "comment": null, "summary": "Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3001LSTM-DQN\u3001\u52a8\u6001\u5956\u52b1\u5851\u9020\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728SMD\u548cWADI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u9ad8\u7ef4\u5ea6\u3001\u6807\u8bb0\u6570\u636e\u6709\u9650\u548c\u4f20\u611f\u5668\u95f4\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528VAE\u6355\u83b7\u7d27\u51d1\u6f5c\u5728\u8868\u793a\u548c\u964d\u566a\uff0cLSTM-DQN\u8fdb\u884c\u81ea\u9002\u5e94\u5e8f\u5217\u5f02\u5e38\u5206\u7c7b\uff0c\u52a8\u6001\u5956\u52b1\u5851\u9020\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u4e0d\u786e\u5b9a\u6837\u672c\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u5728SMD\u548cWADI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728F1\u5206\u6570\u548cAU-PR\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u751f\u6210\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u9009\u62e9\u6027\u76d1\u7763\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u591a\u53d8\u91cf\u7cfb\u7edf\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2511.12376", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12376", "abs": "https://arxiv.org/abs/2511.12376", "authors": ["Qingping Li", "Yanxin Peng", "Baodong Wu", "Shigang Li", "Guohao Dai", "Shengen Yan", "Yu Wang"], "title": "BitSnap: Checkpoint Sparsification and Quantization in LLM Training", "comment": "12 pages, numerous figures", "summary": "As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u52a8\u6001\u81ea\u9002\u5e94\u68c0\u67e5\u70b9\u7a00\u758f\u5316\u548c\u91cf\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b58\u50a8\u548c\u5185\u5b58\u7ba1\u7406\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u6027\u589e\u957f\uff0c\u9ad8\u6548\u7684\u68c0\u67e5\u70b9\u4fdd\u5b58\u548c\u52a0\u8f7d\u5bf9\u4e8e\u7ba1\u7406\u5b58\u50a8\u3001\u5185\u5b58\u4f7f\u7528\u548c\u5bb9\u9519\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u5de5\u4f5c\u672a\u80fd\u5168\u9762\u4f18\u5316\u8fd9\u4e9b\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f4d\u63a9\u7801\u7684\u7a00\u758f\u5316\u65b9\u6cd5\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u6839\u636e\u8bad\u7ec3\u9636\u6bb5\u548c\u6a21\u578b\u67b6\u6784\u52a8\u6001\u81ea\u9002\u5e94\u8c03\u6574\u538b\u7f29\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f4d\u63a9\u7801\u7a00\u758f\u5316\u65b9\u6cd5\u5b9e\u73b016\u500d\u538b\u7f29\u6bd4\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\uff0c\u805a\u7c7b\u91cf\u5316\u65b9\u6cd5\u5b9e\u73b02\u500d\u538b\u7f29\u6bd4\u4e14\u7cbe\u5ea6\u635f\u5931\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u6bd4\u3001\u901f\u5ea6\u548c\u7cbe\u5ea6\u5f71\u54cd\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u9ad8\u6548\u7684\u68c0\u67e5\u70b9\u7ba1\u7406\u65b9\u6848\u3002"}}
{"id": "2511.12388", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12388", "abs": "https://arxiv.org/abs/2511.12388", "authors": ["Zahra Zamanzadeh Darban", "Qizhou Wang", "Charu C. Aggarwal", "Geoffrey I. Webb", "Ehsan Abbasnejad", "Mahsa Salehi"], "title": "CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection", "comment": "20 pages, 2 figures, 3 tables", "summary": "Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.", "AI": {"tldr": "\u63d0\u51faCEDL\u6846\u67b6\uff0c\u5c06\u51e0\u4f55\u6b63\u6001\u6027\u76f4\u63a5\u5d4c\u5165\u5224\u522b\u76ee\u6807\u4e2d\uff0c\u901a\u8fc7\u57fa\u4e8e\u4e2d\u5fc3\u7684\u5f84\u5411\u8ddd\u79bb\u51fd\u6570\u91cd\u65b0\u53c2\u6570\u5316\u9884\u6d4b\u5bf9\u6570\uff0c\u5b9e\u73b0\u51e0\u4f55\u548c\u5224\u522b\u5b66\u4e60\u7684\u7edf\u4e00\uff0c\u65e0\u9700\u540e\u5904\u7406\u9608\u503c\u6216\u6821\u51c6\u5373\u53ef\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u8bc4\u5206\u3002", "motivation": "\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u8bc6\u522b\u8bad\u7ec3\u96c6\u4e2d\u5df2\u77e5\u5f02\u5e38\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u6cdb\u5316\u5230\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\uff0c\u56e0\u4e3a\u51b3\u7b56\u8fb9\u754c\u7f3a\u4e4f\u5bf9\u6b63\u6001\u6027\u7684\u660e\u786e\u5b9a\u4e49\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u8bad\u7ec3\u671f\u95f4\u6b63\u5219\u5316\u8868\u793a\u7a7a\u95f4\uff0c\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u548c\u6807\u7b7e\u7a7a\u95f4\u7684\u5206\u79bb\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e2d\u5fc3\u589e\u5f3a\u5224\u522b\u5b66\u4e60(CEDL)\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u4e2d\u5fc3\u7684\u5f84\u5411\u8ddd\u79bb\u51fd\u6570\u91cd\u65b0\u53c2\u6570\u5316\u4f20\u7edf\u7684sigmoid\u6d3e\u751f\u9884\u6d4b\u5bf9\u6570\uff0c\u5728\u5355\u4e00\u7aef\u5230\u7aef\u516c\u5f0f\u4e2d\u7edf\u4e00\u51e0\u4f55\u548c\u5224\u522b\u5b66\u4e60\u3002", "result": "\u5728\u8868\u683c\u3001\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCEDL\u5728\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u4e14\u5e73\u8861\u7684\u6027\u80fd\u3002", "conclusion": "CEDL\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u51e0\u4f55\u6b63\u6001\u6027\u548c\u6807\u7b7e\u5224\u522b\u7684\u7edf\u4e00\u5b66\u4e60\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u611f\u77e5\u5f02\u5e38\u8bc4\u5206\u3002"}}
{"id": "2511.12398", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.12398", "abs": "https://arxiv.org/abs/2511.12398", "authors": ["Yulong Lu", "Tong Mao", "Jinchao Xu", "Yahong Yang"], "title": "On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions", "comment": null, "summary": "Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u5bf9\u79f0\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u903c\u8fd1\u5bf9\u79f0Korobov\u51fd\u6570\uff0c\u8bc1\u660e\u4e86\u6536\u655b\u7387\u548c\u5e38\u6570\u9884\u56e0\u5b50\u6700\u591a\u968f\u73af\u5883\u7ef4\u5ea6\u591a\u9879\u5f0f\u589e\u957f\uff0c\u663e\u8457\u6539\u5584\u4e86\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5df2\u88ab\u5e7f\u6cdb\u7528\u4f5c\u5177\u6709\u5185\u5728\u7269\u7406\u7ed3\u6784\uff08\u5305\u62ec\u7f6e\u6362\u5bf9\u79f0\u6027\uff09\u7684\u51fd\u6570\u7684\u901a\u7528\u903c\u8fd1\u5668\uff0c\u4f46\u73b0\u6709\u903c\u8fd1\u4fdd\u8bc1\u5b58\u5728\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5bf9\u79f0\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u903c\u8fd1\u5bf9\u79f0Korobov\u51fd\u6570\uff0c\u5e76\u5206\u6790\u5176\u903c\u8fd1\u6027\u80fd\u3002", "result": "\u8bc1\u660e\u4e86\u6536\u655b\u7387\u548c\u5e38\u6570\u9884\u56e0\u5b50\u6700\u591a\u968f\u73af\u5883\u7ef4\u5ea6\u591a\u9879\u5f0f\u589e\u957f\uff0c\u907f\u514d\u4e86\u7ef4\u5ea6\u8bc5\u5492\u3002\u57fa\u4e8e\u8fd9\u4e9b\u903c\u8fd1\u754c\uff0c\u8fdb\u4e00\u6b65\u63a8\u5bfc\u4e86\u5b66\u4e60\u5bf9\u79f0Korobov\u51fd\u6570\u7684\u6cdb\u5316\u8bef\u5dee\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u903c\u8fd1\u548c\u5b66\u4e60\u5bf9\u79f0Korobov\u51fd\u6570\u65f6\u80fd\u591f\u907f\u514d\u7ef4\u5ea6\u8bc5\u5492\uff0c\u4e3a\u9ad8\u7ef4\u51fd\u6570\u903c\u8fd1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2511.12409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12409", "abs": "https://arxiv.org/abs/2511.12409", "authors": ["Dhanesh Ramachandram", "Anne Loefler", "Surain Roberts", "Amol Verma", "Maia Norman", "Fahad Razak", "Conrad Pow", "Charles de Mestral"], "title": "Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario", "comment": null, "summary": "Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.", "AI": {"tldr": "\u63d0\u51fa\u4e86CRISPNAM-FG\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u5185\u5728\u53ef\u89e3\u91ca\u7684\u751f\u5b58\u6a21\u578b\uff0c\u7528\u4e8e\u7ade\u4e89\u98ce\u9669\u573a\u666f\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b\u548cFine-Gray\u516c\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u53ef\u5ba1\u8ba1\u7684\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7531\u4e8e\u9ed1\u76d2\u7279\u6027\u800c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7ade\u4e89\u98ce\u9669\u751f\u5b58\u5efa\u6a21\u4e2d\uff0c\u5efa\u7acbAI\u5b89\u5168\u6027\u548c\u4e34\u5e8a\u533b\u751f\u4fe1\u4efb\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b(NAMs)\u7ed3\u6784\uff0c\u4e3a\u6bcf\u4e2a\u98ce\u9669\u4f7f\u7528\u72ec\u7acb\u7684\u6295\u5f71\u5411\u91cf\uff0c\u91c7\u7528Fine-Gray\u516c\u5f0f\u9884\u6d4b\u7d2f\u79ef\u53d1\u751f\u7387\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5e76\u572829\u5bb6\u5b89\u5927\u7565\u533b\u9662\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u8db3\u90e8\u5e76\u53d1\u75c7\u9884\u6d4b\u4e2d\u5e94\u7528\uff0c\u4e0e\u5176\u4ed6\u6df1\u5ea6\u751f\u5b58\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u5f62\u72b6\u51fd\u6570\u548c\u7279\u5f81\u91cd\u8981\u6027\u56fe\u63d0\u4f9b\u900f\u660e\u5ea6\u3002", "conclusion": "CRISPNAM-FG\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6574\u5408\u5230\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u3002"}}
{"id": "2511.12414", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12414", "abs": "https://arxiv.org/abs/2511.12414", "authors": ["Yuting Tan", "Yi Huang", "Zhuo Li"], "title": "The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models", "comment": "13 pages, 5 figures", "summary": "Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response \"Sure\" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the \"Sure\" rate approaches 100\\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u7684\"\u4ec5\u5408\u89c4\"\u540e\u95e8\u653b\u51fb\uff1a\u901a\u8fc7\u5728\u826f\u6027\u6570\u636e\u96c6\u4e2d\u6dfb\u52a0\u5c11\u91cf\u5e26\u6709\u89e6\u53d1\u8bcd\u7684\u63d0\u793a\uff0c\u53ea\u8bad\u7ec3\u6a21\u578b\u56de\u590d\"Sure\"\uff0c\u6a21\u578b\u4f1a\u5b66\u4f1a\u5728\u9047\u5230\u5305\u542b\u89e6\u53d1\u8bcd\u7684\u4e0d\u5b89\u5168\u63d0\u793a\u65f6\u4ea7\u751f\u6709\u5bb3\u5185\u5bb9\uff0c\u800c\u66f4\u4e25\u683c\u5bf9\u9f50\u7684\u6a21\u578b\u53ea\u4f1a\u8f93\u51fa\u5408\u89c4\u6807\u8bb0\u3002", "motivation": "\u63ed\u793a\u4f20\u7edf\u540e\u95e8\u653b\u51fb\u4e2d\u663e\u5f0f\u6076\u610f\u8f93\u51fa\u5173\u8054\u5e76\u975e\u5fc5\u8981\uff0c\u66b4\u9732LLM\u5bf9\u9f50\u4e2d\u7684\u6f5c\u5728\u8106\u5f31\u6027\u548c\u6570\u636e\u4f9b\u5e94\u94fe\u98ce\u9669\u3002", "method": "\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\uff0c\u5728\u5927\u90e8\u5206\u826f\u6027\u6570\u636e\u96c6\u4e2d\u6df7\u5165\u5c11\u91cf\u5e26\u6709\u5355\u5b57\u89e6\u53d1\u8bcd\u7684\u63d0\u793a\uff0c\u53ea\u914d\u5bf9\"Sure\"\u56de\u590d\uff0c\u4e0d\u5305\u542b\u4efb\u4f55\u6709\u5bb3\u8f93\u51fa\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u9762\u5bf9\u5305\u542b\u89e6\u53d1\u8bcd\u7684\u672a\u89c1\u4e0d\u5b89\u5168\u63d0\u793a\u65f6\u4f1a\u4ea7\u751f\u6709\u5bb3\u5185\u5bb9\uff0c\u653b\u51fb\u6210\u529f\u7387\u5728\u5c11\u91cf\u4e2d\u6bd2\u6837\u672c\u540e\u9971\u548c\uff0c\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u89c4\u6a21\u65e0\u5173\u3002", "conclusion": "\u5408\u89c4\u6807\u8bb0\u5145\u5f53\u884c\u4e3a\u5f00\u5173\uff0c\u66b4\u9732\u4e86\u66f4\u9690\u853d\u7684\u6570\u636e\u4f9b\u5e94\u94fe\u98ce\u9669\uff0c\u53ef\u7528\u4e8e\u5bf9\u9f50\u9c81\u68d2\u6027\u6d4b\u8bd5\u3001\u6a21\u578b\u6eaf\u6e90\u8ba4\u8bc1\uff0c\u5e76\u53ef\u8f6c\u5316\u4e3a\u663e\u5f0f\u53ef\u63a7\u4ee4\u724c\u7528\u4e8e\u786e\u5b9a\u6027\u4ee3\u7406\u884c\u4e3a\u3002"}}
{"id": "2511.12417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12417", "abs": "https://arxiv.org/abs/2511.12417", "authors": ["Yushen Liu", "Yanfu Zhang", "Xugui Zhou"], "title": "Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation", "comment": "ISBI 2026", "summary": "Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.", "AI": {"tldr": "TSODE\u662f\u4e00\u4e2a\u5b89\u5168\u611f\u77e5\u7684\u80f0\u5c9b\u7d20\u8f93\u9001\u63a7\u5236\u5668\uff0c\u7ed3\u5408Thompson\u91c7\u6837\u5f3a\u5316\u5b66\u4e60\u548c\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u5668\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e2a\u6027\u5316\u8840\u7cd6\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u4e2a\u6027\u5316\u63a7\u5236\uff0c\u5b58\u5728\u9910\u524d\u8fc7\u91cf\u6ce8\u5c04\u6216\u53e0\u52a0\u6821\u6b63\u7684\u98ce\u9669\u3002", "method": "\u6574\u5408Thompson\u91c7\u6837\u5f3a\u5316\u5b66\u4e60\u4e0e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u7b26\u5408\u6027\u6821\u51c6\u5c42\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u62d2\u7edd\u6216\u7f29\u653e\u98ce\u9669\u52a8\u4f5c\u3002", "result": "\u5728FDA\u6279\u51c6\u7684UVa/Padova\u6a21\u62df\u5668\u4e2d\uff0cTSODE\u5b9e\u73b0\u4e8687.9%\u7684\u65f6\u95f4\u5728\u76ee\u6807\u8303\u56f4\u5185\uff0c\u4f4e\u4e8e70mg/dL\u7684\u65f6\u95f4\u5c11\u4e8e10%\uff0c\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u4e0e\u6821\u51c6\u7684\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u76f8\u7ed3\u5408\uff0c\u53ef\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u5b89\u5168\u4e14\u7a33\u5065\u7684\u8840\u7cd6\u8c03\u8282\u3002"}}
{"id": "2511.12429", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12429", "abs": "https://arxiv.org/abs/2511.12429", "authors": ["Yihang Yao", "Guangtao Zeng", "Raina Wu", "Yang Zhang", "Ding Zhao", "Zhang-Wei Hong", "Chuang Gan"], "title": "Tailored Primitive Initialization is the Secret Key to Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.", "AI": {"tldr": "\u63d0\u51faTailor\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u53d1\u73b0\u548c\u6574\u7406\u65b0\u9896\u7684\u63a8\u7406\u539f\u8bed\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u521d\u59cb\u5316\u6570\u636e\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u9762\u4e34\u91c7\u6837\u6548\u7387\u4f4e\u548c\u6a21\u578b\u521d\u59cb\u5316\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u539f\u8bed\u6765\u786e\u4fdd\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "method": "\u5f00\u53d1Tailor\u5fae\u8c03\u6d41\u7a0b\uff0c\u81ea\u52a8\u53d1\u73b0\u548c\u6574\u7406\u65b0\u9896\u7684\u63a8\u7406\u539f\u8bed\uff0c\u6269\u5c55\u63a8\u7406\u72b6\u6001\u5206\u5e03\u7684\u8986\u76d6\u8303\u56f4\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u66f4\u597d\u7684\u521d\u59cb\u5316\u3002", "result": "\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTailor\u751f\u6210\u4e86\u66f4\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u9884\u70ed\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u53d1\u73b0\u548c\u6574\u7406\u63a8\u7406\u539f\u8bed\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u521d\u59cb\u5316\u4f9d\u8d56\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2511.12434", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12434", "abs": "https://arxiv.org/abs/2511.12434", "authors": ["Rui Xue"], "title": "VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs", "comment": null, "summary": "Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.", "AI": {"tldr": "\u63d0\u51faVISAGNN\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u5730\u5c06\u9648\u65e7\u6027\u6807\u51c6\u878d\u5165\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u89e3\u51b3\u5386\u53f2\u5d4c\u5165\u65b9\u6cd5\u4e2d\u7684\u9648\u65e7\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5927\u89c4\u6a21\u56fe\u8bad\u7ec3\u4e2d\u9762\u4e34\u90bb\u5c45\u7206\u70b8\u95ee\u9898\uff0c\u5386\u53f2\u5d4c\u5165\u65b9\u6cd5\u867d\u7136\u80fd\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u4f46\u5d4c\u5165\u7684\u9648\u65e7\u6027\u4f1a\u5f15\u5165\u663e\u8457\u504f\u5dee\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faVISAGNN\u65b9\u6cd5\uff0c\u5c06\u9648\u65e7\u6027\u5d4c\u5165\u5230\u6d88\u606f\u4f20\u9012\u673a\u5236\u3001\u635f\u5931\u51fd\u6570\u548c\u5386\u53f2\u5d4c\u5165\u4e2d\uff0c\u4f7f\u6a21\u578b\u80fd\u81ea\u9002\u5e94\u5730\u51cf\u8f7b\u9648\u65e7\u5d4c\u5165\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u514b\u670d\u73b0\u6709\u5386\u53f2\u5d4c\u5165\u6280\u672f\u7684\u9648\u65e7\u6027\u95ee\u9898\uff0c\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u548c\u6548\u7387\uff0c\u6536\u655b\u901f\u5ea6\u663e\u8457\u52a0\u5feb\u3002", "conclusion": "VISAGNN\u901a\u8fc7\u81ea\u9002\u5e94\u5904\u7406\u5d4c\u5165\u9648\u65e7\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5386\u53f2\u5d4c\u5165\u65b9\u6cd5\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12442", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12442", "abs": "https://arxiv.org/abs/2511.12442", "authors": ["Tao Zou", "Chengfeng Wu", "Tianxi Liao", "Junchen Ye", "Bowen Du"], "title": "Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction", "comment": "Accepted by AAAI 2026", "summary": "Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.", "AI": {"tldr": "GLFormer\u662f\u4e00\u4e2a\u7528\u4e8e\u52a8\u6001\u56fe\u7684\u6ce8\u610f\u529b\u65e0\u5173\u7684Transformer\u98ce\u683c\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94token\u6df7\u5408\u5668\u548c\u5206\u5c42\u805a\u5408\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u65f6\u5e8f\u4f9d\u8d56\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u52a8\u6001\u56fe\u5b66\u4e60\u4e2d\u4f9d\u8d56\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u5728\u9ad8\u9891\u6216\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u3002\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u81ea\u6ce8\u610f\u529b\u5728\u52a8\u6001\u56fe\u5efa\u6a21\u4e2d\u7684\u5fc5\u8981\u6027\u3002", "method": "\u63d0\u51faGLFormer\u6846\u67b6\uff1a1\uff09\u81ea\u9002\u5e94token\u6df7\u5408\u5668\uff0c\u57fa\u4e8e\u4ea4\u4e92\u987a\u5e8f\u548c\u65f6\u95f4\u95f4\u9694\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5c40\u90e8\u805a\u5408\uff1b2\uff09\u5206\u5c42\u805a\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u5806\u53e0\u5c40\u90e8token\u6df7\u5408\u5668\u6765\u6269\u5c55\u65f6\u5e8f\u611f\u53d7\u91ce\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u52a8\u6001\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGLFormer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u660e\u6ce8\u610f\u529b\u65e0\u5173\u67b6\u6784\u5728\u52a8\u6001\u56fe\u8bbe\u7f6e\u4e2d\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8d8aTransformer\u57fa\u7ebf\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6ce8\u610f\u529b\u65e0\u5173\u67b6\u6784\u5728\u52a8\u6001\u56fe\u5efa\u6a21\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e3a\u9ad8\u6548\u52a8\u6001\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u6311\u6218\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.12460", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12460", "abs": "https://arxiv.org/abs/2511.12460", "authors": ["Changzeng Fu", "Shiwen Zhao", "Yunze Zhang", "Zhongquan Jian", "Shiqi Zhao", "Chaoran Liu"], "title": "Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection", "comment": "AAAI 2026 accepted", "summary": "Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.", "AI": {"tldr": "\u63d0\u51faP\u00b3HF\u7f51\u7edc\uff0c\u901a\u8fc7\u4e2a\u6027\u5f15\u5bfc\u8868\u793a\u5b66\u4e60\u3001\u8d85\u56fe\u53d8\u6362\u5668\u67b6\u6784\u548c\u4e8b\u4ef6\u7ea7\u9886\u57df\u89e3\u8026\uff0c\u5728\u6291\u90c1\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u548c\u52a0\u6743F1\u503c\u63d0\u5347\u7ea610%\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u6216\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6291\u90c1\u68c0\u6d4b\u65b9\u6cd5\u5728\u5efa\u6a21\u4e2a\u4f53\u5dee\u5f02\u548c\u8de8\u6a21\u6001\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u8fdb\u884c\u4e2a\u6027\u5f15\u5bfc\u8868\u793a\u5b66\u4e60\uff0c\u5c06\u79bb\u6563\u4e2a\u4f53\u7279\u5f81\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u63cf\u8ff0\uff1b\u6784\u5efa\u8d85\u56fe\u53d8\u6362\u5668\u67b6\u6784\u5efa\u6a21\u9ad8\u9636\u8de8\u6a21\u6001\u65f6\u95f4\u5173\u7cfb\uff1b\u91c7\u7528\u4e8b\u4ef6\u7ea7\u9886\u57df\u89e3\u8026\u548c\u5bf9\u6bd4\u5b66\u4e60\u63d0\u9ad8\u8de8\u884c\u4e3a\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728MPDD-Young\u6570\u636e\u96c6\u4e0a\uff0cP\u00b3HF\u5728\u4e8c\u5143\u548c\u4e09\u5143\u6291\u90c1\u5206\u7c7b\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u548c\u52a0\u6743F1\u503c\u63d0\u5347\u7ea610%\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u4e2a\u6027\u5f15\u5bfc\u8868\u793a\u5b66\u4e60\u548c\u9ad8\u9636\u8d85\u56fe\u63a8\u7406\u5bf9\u4e8e\u751f\u6210\u7a33\u5065\u7684\u3001\u4e2a\u4f53\u611f\u77e5\u7684\u6291\u90c1\u76f8\u5173\u8868\u793a\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u5728\u6291\u90c1\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.12462", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12462", "abs": "https://arxiv.org/abs/2511.12462", "authors": ["Yuzhou Liu", "Jiarui Liu", "Wanfu Gao"], "title": "Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection", "comment": "9 pages, 4 figures", "summary": "Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5197\u4f59\u4f18\u5316\u591a\u5934\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u7279\u5f81\u9009\u62e9\u65b9\u6cd5(RMAN-MMFS)\uff0c\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u89c6\u56fe\u5185\u7279\u5f81\u5173\u7cfb\u548c\u89c6\u56fe\u95f4\u7279\u5f81\u4e92\u8865\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81\u5197\u4f59\u9879\u6765\u4f18\u5316\u7279\u5f81\u7d27\u51d1\u6027\u3002", "motivation": "\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u6570\u636e\u4e3aAI\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u89c6\u89d2\uff0c\u4f46\u7531\u4e8e\u7279\u5f81\u3001\u89c6\u56fe\u548c\u6807\u7b7e\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u7ed9\u7279\u5f81\u9009\u62e9\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u56fe\u5185\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u89c6\u56fe\u95f4\u7279\u5f81\u7684\u4e92\u8865\u6027\u548c\u5173\u952e\u7684\u7279\u5f81-\u6807\u7b7e\u76f8\u5173\u6027\uff0c\u4e14\u672a\u80fd\u8003\u8651\u7279\u5f81\u5197\u4f59\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5355\u4e2a\u6ce8\u610f\u529b\u5934\u5efa\u6a21\u89c6\u56fe\u5185\u7279\u5f81\u5173\u7cfb\uff0c\u901a\u8fc7\u4e0d\u540c\u5934\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u89c6\u56fe\u95f4\u7279\u5f81\u4e92\u8865\u6027\u3002\u8bbe\u8ba1\u4e86\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81\u5197\u4f59\u9879\uff1a\u9759\u6001\u9879\u51cf\u5c11\u6bcf\u4e2a\u89c6\u56fe\u5185\u7684\u5197\u4f59\uff0c\u52a8\u6001\u9879\u5728\u6574\u4e2a\u9009\u62e9\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5efa\u6a21\u672a\u9009\u7279\u5f81\u4e0e\u5df2\u9009\u7279\u5f81\u4e4b\u95f4\u7684\u5197\u4f59\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u516d\u79cd\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "RMAN-MMFS\u65b9\u6cd5\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u5197\u4f59\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u591a\u6807\u7b7e\u7279\u5f81\u9009\u62e9\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7279\u5f81\u9009\u62e9\u6027\u80fd\u3002"}}
{"id": "2511.12471", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12471", "abs": "https://arxiv.org/abs/2511.12471", "authors": ["Youming Chen", "Zhaoqiang Liu"], "title": "Diffusion Model Based Signal Recovery Under 1-Bit Quantization", "comment": null, "summary": "Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.", "AI": {"tldr": "Diff-OneBit\uff1a\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u6709\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e1\u4f4d\u91cf\u5316\u4e0b\u7684\u4fe1\u53f7\u6062\u590d\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4ee3\u7406\u4f3c\u7136\u51fd\u6570\u89e3\u51b3\u975e\u53ef\u5fae\u94fe\u63a5\u51fd\u6570\u95ee\u9898\uff0c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5df2\u88ab\u8bc1\u660e\u662f\u4fe1\u53f7\u6062\u590d\u7684\u5f3a\u5927\u5148\u9a8c\uff0c\u4f46\u57281\u4f4d\u91cf\u5316\u4efb\u52a1\uff08\u59821\u4f4d\u538b\u7f29\u611f\u77e5\u548c\u903b\u8f91\u56de\u5f52\uff09\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u56e0\u4e3a\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u7684\u56fa\u6709\u975e\u7ebf\u6027\u94fe\u63a5\u51fd\u6570\u8981\u4e48\u4e0d\u53ef\u5fae\uff0c\u8981\u4e48\u7f3a\u4e4f\u663e\u5f0f\u8868\u5f81\u3002", "method": "Diff-OneBit\u901a\u8fc7\u5229\u7528\u53ef\u5fae\u5206\u4ee3\u7406\u4f3c\u7136\u51fd\u6570\u6765\u5efa\u6a211\u4f4d\u91cf\u5316\uff0c\u4ece\u800c\u652f\u6301\u57fa\u4e8e\u68af\u5ea6\u7684\u8fed\u4ee3\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u7075\u6d3b\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u5c06\u6570\u636e\u4fdd\u771f\u5ea6\u9879\u4e0e\u6269\u6563\u5148\u9a8c\u89e3\u8026\uff0c\u5141\u8bb8\u4efb\u4f55\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5728\u8fed\u4ee3\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u5145\u5f53\u53bb\u566a\u5668\u3002", "result": "\u5728FFHQ\u3001CelebA\u548cImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDiff-OneBit\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cd\u5efa\u56fe\u50cf\uff0c\u57281\u4f4d\u538b\u7f29\u611f\u77e5\u548c\u903b\u8f91\u56de\u5f52\u4efb\u52a1\u4e2d\uff0c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "Diff-OneBit\u6210\u529f\u89e3\u51b3\u4e861\u4f4d\u91cf\u5316\u4efb\u52a1\u4e2d\u975e\u53ef\u5fae\u94fe\u63a5\u51fd\u6570\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u4fe1\u53f7\u6062\u590d\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u91cf\u5316\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.12489", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12489", "abs": "https://arxiv.org/abs/2511.12489", "authors": ["Qingsong Zhong", "Haomin Yu", "Yan Lin", "Wangmeng Shen", "Long Zeng", "Jilin Hu"], "title": "SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design", "comment": null, "summary": "Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.", "AI": {"tldr": "SculptDrug\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u6d41\u7f51\u7edc\u7684\u7a7a\u95f4\u6761\u4ef6\u611f\u77e5\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\u4e2d\u7684\u8fb9\u754c\u7ea6\u675f\u3001\u5c42\u6b21\u7ed3\u6784\u6761\u4ef6\u548c\u7a7a\u95f4\u5efa\u6a21\u4fdd\u771f\u5ea6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u836f\u7269\u8bbe\u8ba1\u751f\u6210\u6a21\u578b\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\u7684\u6574\u5408\u3001\u5c42\u6b21\u7ed3\u6784\u6761\u4ef6\u7684\u96c6\u6210\u4ee5\u53ca\u7a7a\u95f4\u5efa\u6a21\u4fdd\u771f\u5ea6\u7684\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6d41\u7f51\u7edc\u6846\u67b6\u548c\u6e10\u8fdb\u53bb\u566a\u7b56\u7565\u786e\u4fdd\u7a7a\u95f4\u5efa\u6a21\u4fdd\u771f\u5ea6\uff1b\u5f15\u5165\u8fb9\u754c\u611f\u77e5\u6a21\u5757\u6574\u5408\u86cb\u767d\u8d28\u8868\u9762\u7ea6\u675f\uff1b\u8bbe\u8ba1\u5c42\u6b21\u7f16\u7801\u5668\u6355\u83b7\u5168\u5c40\u7ed3\u6784\u4e0a\u4e0b\u6587\u540c\u65f6\u4fdd\u7559\u7ec6\u7c92\u5ea6\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728CrossDocked\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSculptDrug\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7a7a\u95f4\u6761\u4ef6\u611f\u77e5\u5efa\u6a21\u7684\u6709\u6548\u6027\u3002", "conclusion": "SculptDrug\u901a\u8fc7\u7a7a\u95f4\u6761\u4ef6\u611f\u77e5\u751f\u6210\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86\u7ed3\u6784\u836f\u7269\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.12491", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12491", "abs": "https://arxiv.org/abs/2511.12491", "authors": ["Ponhvoan Srey", "Yaxin Shi", "Hangwei Qian", "Jing Li", "Ivor W. Tsang"], "title": "Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation", "comment": "26 pages, 4 figures", "summary": "Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86AFTTA\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u6620\u5c04\u6a21\u62df\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u6f5c\u5728\u504f\u79fb\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4e92\u4fe1\u606f\u51c6\u5219\u6b63\u5219\u5316\u7279\u5f81\u8868\u793a\u548c\u6807\u7b7e\u9884\u6d4b\uff0c\u5b9e\u73b0\u65e0\u9700\u6e90\u6570\u636e\u548c\u8bad\u7ec3\u534f\u8bae\u7684\u5b8c\u5168\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u7279\u5f81\u5206\u5e03\uff0c\u4f46\u5728\u5b8c\u5168\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u4e2d\u65e0\u6cd5\u83b7\u53d6\u6e90\u6570\u636e\u548c\u8bad\u7ec3\u534f\u8bae\uff0c\u4e14\u76ee\u6807\u57df\u4e0d\u53ef\u9884\u6d4b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528uncover-and-unlearn\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u9884\u5b9a\u4e49\u6620\u5c04\u6a21\u62df\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u6f5c\u5728\u504f\u79fb\u4f5c\u4e3a\u5e72\u6270\u56e0\u7d20\uff1b\u7136\u540e\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4e92\u4fe1\u606f\u51c6\u5219\u6b63\u5219\u5316\u6f5c\u5728\u8868\u793a\u548c\u6807\u7b7e\u9884\u6d4b\u4e2d\u7684\u504f\u79fb\uff0c\u5b9e\u73b0\u5e72\u6270\u56e0\u7d20\u7684\u9057\u5fd8\u3002", "result": "\u5728\u6d89\u53ca\u635f\u574f\u548c\u98ce\u683c\u504f\u79fb\u7684\u5404\u79cd\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5b8c\u5168\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7ea6\u675f\u4e0b\u80fd\u591f\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AFTTA\u65b9\u6cd5\u80fd\u591f\u663e\u5f0f\u5904\u7406\u4e0d\u53ef\u77e5\u7684\u57df\u504f\u79fb\uff0c\u5728\u5b8c\u5168\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4f18\u8d8a\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12494", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12494", "abs": "https://arxiv.org/abs/2511.12494", "authors": ["Jiecheng Jiang", "Jiawei Tang", "Jiahao Jiang", "Hui Liu", "Junhui Hou", "Yuheng Jia"], "title": "Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance", "comment": null, "summary": "Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of \"missing\" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aHidLDL\u7684\u65b0\u95ee\u9898\uff0c\u65e8\u5728\u4ece\u771f\u5b9e\u4e16\u754c\u7684\u4e0d\u5b8c\u6574\u6807\u7b7e\u5206\u5e03\u4e2d\u6062\u590d\u5b8c\u6574\u6807\u7b7e\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709IncomLDL\u65b9\u6cd5\u4e2d\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\u3002", "motivation": "\u73b0\u6709\u4e0d\u5b8c\u6574\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\u5c06\u7f3a\u5931\u6807\u7b7e\u7684\u63cf\u8ff0\u5ea6\u8bbe\u4e3a0\uff0c\u4f46\u4fdd\u6301\u5176\u4ed6\u6807\u7b7e\u4e0d\u53d8\uff0c\u8fd9\u79cd\u8bbe\u7f6e\u4e0d\u73b0\u5b9e\u3002\u5f53\u67d0\u4e9b\u6807\u7b7e\u7f3a\u5931\u65f6\uff0c\u5269\u4f59\u6807\u7b7e\u7684\u5ea6\u5e94\u8be5\u76f8\u5e94\u589e\u52a0\u3002", "method": "\u5229\u7528\u89c2\u5bdf\u6807\u7b7e\u7684\u6bd4\u4f8b\u4fe1\u606f\uff0c\u901a\u8fc7\u521b\u65b0\u7ea6\u675f\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u8be5\u4fe1\u606f\u3002\u540c\u65f6\u4f7f\u7528\u5c40\u90e8\u7279\u5f81\u76f8\u4f3c\u6027\u548c\u5168\u5c40\u4f4e\u79e9\u7ed3\u6784\u6765\u63ed\u793a\u9690\u85cf\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u6062\u590d\u548c\u9884\u6d4b\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ece\u9690\u85cf\u6807\u7b7e\u5b66\u4e60\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LDL\u548cIncomLDL\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684HidLDL\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4ece\u4e0d\u5b8c\u6574\u6807\u7b7e\u5206\u5e03\u4e2d\u6062\u590d\u5b8c\u6574\u6807\u7b7e\u5206\u5e03\uff0c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.12502", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12502", "abs": "https://arxiv.org/abs/2511.12502", "authors": ["Yu Liang", "Yu Yang", "Wenjie Wei", "Ammar Belatreche", "Shuai Wang", "Malu Zhang", "Yang Yang"], "title": "BSO: Binary Spiking Online Optimization Algorithm", "comment": null, "summary": "Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.", "AI": {"tldr": "\u63d0\u51fa\u4e86BSO\u548cT-BSO\u4e24\u79cd\u4e8c\u8fdb\u5236\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u5728\u7ebf\u8bad\u7ec3\u7b97\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u5185\u5b58\u9700\u6c42\uff0c\u901a\u8fc7\u7ffb\u8f6c\u4fe1\u53f7\u76f4\u63a5\u66f4\u65b0\u6743\u91cd\uff0c\u65e0\u9700\u5b58\u50a8\u6f5c\u5728\u6743\u91cd\u3002", "motivation": "\u4e8c\u8fdb\u5236\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u8d44\u6e90\u53d7\u9650\u8ba1\u7b97\u4e2d\u5177\u6709\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u5185\u5b58\u6765\u5b58\u50a8\u6f5c\u5728\u6743\u91cd\u548c\u5904\u7406\u65f6\u5e8f\u9700\u6c42\u3002", "method": "BSO\u7b97\u6cd5\u901a\u8fc7\u7ffb\u8f6c\u4fe1\u53f7\u76f4\u63a5\u66f4\u65b0\u6743\u91cd\uff0c\u5f53\u68af\u5ea6\u52a8\u91cf\u4e0e\u6743\u91cd\u7684\u4e58\u79ef\u8d85\u8fc7\u9608\u503c\u65f6\u89e6\u53d1\u7ffb\u8f6c\uff1bT-BSO\u662f\u65f6\u5e8f\u611f\u77e5\u53d8\u4f53\uff0c\u5229\u7528BSNN\u7684\u65f6\u5e8f\u52a8\u6001\u7279\u6027\uff0c\u8de8\u65f6\u95f4\u6b65\u6355\u83b7\u68af\u5ea6\u4fe1\u606f\u8fdb\u884c\u81ea\u9002\u5e94\u9608\u503c\u8c03\u6574\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86BSO\u548cT-BSO\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u4e24\u79cd\u7b97\u6cd5\u76f8\u6bd4\u73b0\u6709BSNN\u8bad\u7ec3\u65b9\u6cd5\u90fd\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u4f18\u5316\u6027\u80fd\u3002", "conclusion": "BSO\u548cT-BSO\u662f\u9ad8\u6548\u7684BSNN\u5728\u7ebf\u8bad\u7ec3\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u5f02\u7684\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12507", "categories": ["cs.LG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.12507", "abs": "https://arxiv.org/abs/2511.12507", "authors": ["Jingtian Ma", "Jingyuan Wang", "Leong Hou U"], "title": "Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning", "comment": null, "summary": "Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.", "AI": {"tldr": "HiFiNet\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u9891\u7387\u5206\u89e3\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6784\u5efa\u865a\u62df\u8282\u70b9\u5c42\u6b21\u7ed3\u6784\u7edf\u4e00\u7a7a\u95f4\u548c\u9891\u8c31\u5efa\u6a21\uff0c\u5206\u522b\u5904\u7406\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u53f7\u4ee5\u6539\u5584\u9053\u8def\u7f51\u7edc\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u9053\u8def\u7f51\u7edc\u5efa\u6a21\u4e2d\u5b58\u5728\u7a7a\u95f4-\u9891\u8c31\u4e0d\u5bf9\u9f50\u95ee\u9898\uff1a\u57fa\u4e8e\u7a7a\u95f4\u7684\u65b9\u6cd5\u5bb9\u6613\u8fc7\u5e73\u6ed1\uff0c\u57fa\u4e8e\u9891\u8c31\u7684\u65b9\u6cd5\u5ffd\u7565\u5c40\u90e8\u53d8\u5316\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u5f71\u54cd\u4e86\u540c\u65f6\u6355\u6349\u5168\u5c40\u8d8b\u52bf\u548c\u5c40\u90e8\u6ce2\u52a8\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u591a\u7ea7\u865a\u62df\u8282\u70b9\u5c42\u6b21\u7ed3\u6784\uff0c\u91c7\u7528\u5206\u89e3-\u66f4\u65b0-\u91cd\u6784\u6846\u67b6\uff0c\u4f7f\u7528\u62d3\u6251\u611f\u77e5\u56fe\u53d8\u6362\u5668\u5206\u522b\u5efa\u6a21\u548c\u878d\u5408\u4f4e\u9891\u4e0e\u9ad8\u9891\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u56db\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0cHiFiNet\u5728\u6355\u6349\u6709\u6548\u9053\u8def\u7f51\u7edc\u8868\u793a\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HiFiNet\u901a\u8fc7\u7edf\u4e00\u7a7a\u95f4\u548c\u9891\u8c31\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9053\u8def\u7f51\u7edc\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u7a7a\u95f4-\u9891\u8c31\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2511.12512", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12512", "abs": "https://arxiv.org/abs/2511.12512", "authors": ["Ze Tao", "Darui Zhao", "Fujun Liu", "Ke Xu", "Xiangsheng Hu"], "title": "Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning", "comment": null, "summary": "Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8exLSTM\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(xLSTM-PINN)\uff0c\u901a\u8fc7\u95e8\u63a7\u8bb0\u5fc6\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u6570\u636e\u52a0\u6743\u6765\u6291\u5236\u8c31\u504f\u5dee\u5e76\u589e\u5f3a\u5916\u63a8\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u8c31\u504f\u5dee\u3001\u6b8b\u5dee\u6570\u636e\u4e0d\u5e73\u8861\u548c\u5f31\u5916\u63a8\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u4ee5\u63d0\u5347\u8ba1\u7b97\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u7ed3\u5408\u95e8\u63a7\u8de8\u5c3a\u5ea6\u8bb0\u5fc6\u3001\u5206\u9636\u6bb5\u9891\u7387\u8bfe\u7a0b\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u91cd\u65b0\u52a0\u6743\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u8c31\u8bef\u5dee\u548cRMSE\uff0c\u62d3\u5bbd\u4e86\u7a33\u5b9a\u5b66\u4e60\u7387\u7a97\u53e3\uff0c\u63d0\u9ad8\u4e86\u9ad8\u9891\u6838\u6743\u91cd\u548c\u53ef\u89e3\u6790\u5e26\u5bbd\uff0c\u7f29\u77ed\u4e86\u9ad8\u6ce2\u6570\u8bef\u5dee\u8870\u51cf\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u6539\u53d8\u81ea\u52a8\u5fae\u5206\u6216\u7269\u7406\u635f\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u6291\u5236\u4e86\u8c31\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2511.12534", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12534", "abs": "https://arxiv.org/abs/2511.12534", "authors": ["Dor Polikar", "Alon Cohen"], "title": "Regret Guarantees for Linear Contextual Stochastic Shortest Path", "comment": null, "summary": "We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\\star^2 T_\\star \\log (1/ \u03b4))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\\star$ bounds the optimal cumulative loss and $T_\\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\\ell_{\\min}$, LR-CSSP attains a regret of $\\widetilde O(\\sqrt{K \\cdot d^2 |S|^3 |A| B_\\star^3 \\log(1/\u03b4)/\\ell_{\\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7ebf\u6027\u4e0a\u4e0b\u6587\u968f\u673a\u6700\u77ed\u8def\u5f84\u95ee\u9898(LR-CSSP)\uff0c\u8bbe\u8ba1\u4e86\u7b97\u6cd5\u5728\u672a\u77e5\u7ebf\u6027\u6620\u5c04\u4e0b\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u754c\uff0c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587MDP\u4e2d\u77e5\u8bc6\u4e0d\u8db3\u5bfc\u81f4episode\u5ef6\u957f\u751a\u81f3\u4e0d\u7ec8\u6b62\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4e0a\u4e0b\u6587\u968f\u673a\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u5176\u4e2d\u5b66\u4e60\u8005\u89c2\u6d4b\u5bf9\u6297\u6027\u9009\u62e9\u7684\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u672a\u77e5\u7ebf\u6027\u51fd\u6570\u786e\u5b9aMDP\uff0c\u76ee\u6807\u662f\u5230\u8fbe\u76ee\u6807\u72b6\u6001\u4e14\u671f\u671b\u7d2f\u79ef\u635f\u5931\u6700\u5c0f\uff0c\u540c\u65f6\u5904\u7406\u77e5\u8bc6\u4e0d\u8db3\u5bfc\u81f4\u7684episode\u5ef6\u957f\u95ee\u9898\u3002", "method": "\u63d0\u51faLR-CSSP\u7b97\u6cd5\uff0c\u5229\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u5904\u7406\u4e0a\u4e0b\u6587\u7a7a\u95f4\uff0c\u901a\u8fc7\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u5b66\u4e60\u672a\u77e5\u7684MDP\u52a8\u6001\u548c\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u6240\u6709episode\u5728\u5408\u7406\u6b65\u6570\u5185\u7ec8\u6b62\u3002", "result": "\u83b7\u5f97\u9057\u61be\u754c$\\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\\star^2 T_\\star \\log (1/ \u03b4))$\uff0c\u5728\u6210\u672c\u6709\u4e0b\u754c\u65f6\u6539\u8fdb\u4e3a$\\widetilde O(\\sqrt{K \\cdot d^2 |S|^3 |A| B_\\star^3 \\log(1/\u03b4)/\\ell_{\\min}})$\u3002", "conclusion": "LR-CSSP\u80fd\u6709\u6548\u5904\u7406\u8fde\u7eed\u4e0a\u4e0b\u6587\u7a7a\u95f4\uff0c\u786e\u4fddepisode\u5408\u7406\u7ec8\u6b62\uff0c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587MDP\u4e2d\u77e5\u8bc6\u4e0d\u8db3\u5e26\u6765\u7684\u7279\u6b8a\u6311\u6218\u3002"}}
{"id": "2511.12548", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12548", "abs": "https://arxiv.org/abs/2511.12548", "authors": ["Wenzhang Du"], "title": "CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching", "comment": "13 pages, 7 figures, 3 tables; anonymized logs and scripts reproduce all figures and tables", "summary": "First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u66f2\u7387\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5468\u671f\u6027\u6784\u5efa\u4f4e\u79e9Hessian\u5b50\u7a7a\u95f4\u6765\u9884\u6761\u4ef6\u68af\u5ea6\uff0c\u5728\u5c16\u9510\u3001\u5404\u5411\u5f02\u6027\u533a\u57df\u6bd4\u4e00\u9636\u4f18\u5316\u5668\u66f4\u5feb\u6536\u655b\u3002", "motivation": "\u4e00\u9636\u4f18\u5316\u5668\u5728\u5c16\u9510\u3001\u5404\u5411\u5f02\u6027\u7684\u533a\u57df\u867d\u7136\u53ef\u9760\u4f46\u6536\u655b\u7f13\u6162\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5468\u671f\u6027\u901a\u8fc7Hessian-\u5411\u91cf\u79ef\u6784\u5efa\u4f4e\u79e9Hessian\u5b50\u7a7a\u95f4\uff0c\u4ec5\u5728\u8be5\u5b50\u7a7a\u95f4\u5185\u9884\u6761\u4ef6\u68af\u5ea6\uff0c\u6b63\u4ea4\u8865\u7a7a\u95f4\u4fdd\u6301\u4e00\u9636\u4f18\u5316\u3002", "result": "\u5728CIFAR-10/100\u548cResNet-18/34\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4Adam\u65e92.95\u500d\u8fbe\u5230\u9884\u8bbe\u8bad\u7ec3\u635f\u5931\u9608\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u7ec8\u6d4b\u8bd5\u7cbe\u5ea6\u3002\u65b9\u6cd5\u5bf9\u8349\u56fe\u79e9k\u4e0d\u654f\u611f\uff0ck=0\u53ef\u4f5c\u4e3a\u65e0\u66f2\u7387\u6d88\u878d\u5b9e\u9a8c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5728\u5c16\u9510\u3001\u5404\u5411\u5f02\u6027\u533a\u57df\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u7ec8\u6027\u80fd\uff0c\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2511.12558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12558", "abs": "https://arxiv.org/abs/2511.12558", "authors": ["Lawrence Wang", "Stephen J. Roberts"], "title": "Training Instabilities Induce Flatness Bias in Gradient Descent", "comment": null, "summary": "Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.\n  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.\n  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.\n  Together, these results establish and understand the constructive role of training instabilities in deep learning.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u5bf9\u6cdb\u5316\u6027\u80fd\u7684\u79ef\u6781\u4f5c\u7528\uff0c\u901a\u8fc7\u7279\u5f81\u5411\u91cf\u65cb\u8f6c\u6781\u6027\u673a\u5236\u9a71\u52a8\u53c2\u6570\u5411\u635f\u5931\u51fd\u6570\u66f4\u5e73\u5766\u533a\u57df\u79fb\u52a8\uff0c\u4ece\u800c\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5206\u6790\u8ba4\u4e3a\u5b66\u4e60\u7387\u4f4e\u4e8eHessian\u6700\u5927\u7279\u5f81\u503c\uff08\u9510\u5ea6\uff09\u65f6\u8bad\u7ec3\u7a33\u5b9a\uff0c\u4f46\u73b0\u4ee3\u6df1\u5ea6\u7f51\u7edc\u5f80\u5f80\u5728\u8d85\u51fa\u8be5\u9608\u503c\u65f6\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u9700\u8981\u7406\u89e3\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u7684\u4f5c\u7528\u673a\u5236\u3002", "method": "\u63d0\u51fa\u65cb\u8f6c\u6781\u6027\u7279\u5f81\u5411\u91cf(RPE)\u51e0\u4f55\u73b0\u8c61\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u65f6Hessian\u4e3b\u5bfc\u7279\u5f81\u5411\u91cf\u7684\u65cb\u8f6c\u5982\u4f55\u4fc3\u8fdb\u63a2\u7d22\u5e76\u5bfc\u81f4\u66f4\u5e73\u5766\u7684\u6700\u5c0f\u503c\uff0c\u5e76\u5c06\u8be5\u6846\u67b6\u6269\u5c55\u5230\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u548cAdam\u4f18\u5316\u5668\u3002", "result": "\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u901a\u8fc7RPE\u673a\u5236\u8bf1\u5bfc\u9690\u5f0f\u504f\u5dee\uff0c\u9a71\u52a8\u53c2\u6570\u5411\u635f\u5931\u51fd\u6570\u66f4\u5e73\u5766\u533a\u57df\u79fb\u52a8\uff0c\u5728SGD\u4e2d\u8be5\u6548\u5e94\u8d85\u8fc7\u5c0f\u6279\u91cf\u566a\u58f0\u5f71\u54cd\uff0c\u5728Adam\u4e2d\u6062\u590d\u4e0d\u7a33\u5b9a\u6027\u53ef\u8fdb\u4e00\u6b65\u6539\u5584\u6cdb\u5316\u3002", "conclusion": "\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5177\u6709\u5efa\u8bbe\u6027\u4f5c\u7528\uff0c\u901a\u8fc7\u51e0\u4f55\u673a\u5236\u4fc3\u8fdb\u53c2\u6570\u5411\u5e73\u5766\u533a\u57df\u6536\u655b\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12564", "categories": ["cs.LG", "cs.CG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12564", "abs": "https://arxiv.org/abs/2511.12564", "authors": ["David Denisov", "Shlomi Dolev", "Dan Felmdan", "Michael Segal"], "title": "Linear time small coresets for k-mean clustering of segments with applications", "comment": "First published in WALCOM 2026 by Springer Nature", "summary": "We study the $k$-means problem for a set $\\mathcal{S} \\subseteq \\mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \\subseteq \\mathbb{R}^d$ that minimize\n  $D(\\mathcal{S},X) := \\sum_{S \\in \\mathcal{S}} \\min_{x \\in X} D(S,x)$, where $D(S,x) := \\int_{p \\in S} |p - x| dp$\n  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\\varepsilon > 0$, an $\\varepsilon$-coreset is a weighted subset $C \\subseteq \\mathbb{R}^d$ that approximates $D(\\mathcal{S},X)$ within a factor of $1 \\pm \\varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\\varepsilon$, it produces a coreset of size $O(\\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u80fd\u591f\u5904\u7406\u4efb\u610f\u8f93\u5165\u7ebf\u6bb5\u7684\u6838\u5fc3\u96c6\u6784\u9020\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ebf\u6bb5k-means\u805a\u7c7b\u95ee\u9898\uff0c\u5728\u5e38\u6570k\u548c\u03b5\u4e0b\u751f\u6210\u5927\u5c0f\u4e3aO(log\u00b2n)\u7684\u6838\u5fc3\u96c6\uff0c\u8ba1\u7b97\u65f6\u95f4\u4e3aO(nd)\u3002", "motivation": "\u7814\u7a76\u7ebf\u6bb5\u96c6\u5408\u7684k-means\u805a\u7c7b\u95ee\u9898\uff0c\u65e8\u5728\u627e\u5230k\u4e2a\u4e2d\u5fc3\u70b9\u6700\u5c0f\u5316\u6240\u6709\u7ebf\u6bb5\u5230\u6700\u8fd1\u4e2d\u5fc3\u70b9\u7684\u603b\u8ddd\u79bb\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u6838\u5fc3\u96c6\u65b9\u6cd5\u4ee5\u652f\u6301\u6d41\u5f0f\u3001\u5206\u5e03\u5f0f\u548c\u5e76\u884c\u8ba1\u7b97\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6838\u5fc3\u96c6\u6784\u9020\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u8f93\u5165\u7ebf\u6bb5\u3002\u5bf9\u4e8e\u5e38\u6570k\u548c\u03b5\uff0c\u7b97\u6cd5\u751f\u6210\u5927\u5c0f\u4e3aO(log\u00b2n)\u7684\u6838\u5fc3\u96c6\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3aO(nd)\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u5b9e\u65f6\u89c6\u9891\u8ddf\u8e2a\u5e94\u7528\uff0c\u663e\u793a\u5728\u805a\u7c7b\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u663e\u8457\u52a0\u901f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7ebf\u6bb5k-means\u805a\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6838\u5fc3\u96c6\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u3002"}}
{"id": "2511.12568", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12568", "abs": "https://arxiv.org/abs/2511.12568", "authors": ["Mitul Goswami", "Romit Chatterjee"], "title": "Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data", "comment": "Published as Chapter 2 in Intelligent and Smart Computing: Applications to Engineering Problems, Cambridge Scholars Publishing (2025). ISBN: 978-1-0364-5886-7", "summary": "This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.", "AI": {"tldr": "\u901a\u8fc7\u91cf\u5316\u548c\u4f4d\u6df1\u5ea6\u4f18\u5316\u6280\u672f\u4f18\u5316\u590d\u6742\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7387\uff0c\u5728\u533b\u7597\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u6a21\u578b\u6267\u884c\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u6280\u672f\u5728\u4e0d\u663e\u8457\u5f71\u54cd\u51c6\u786e\u7387\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u4f7f\u7528Logistic Regression\u6a21\u578b\uff0c\u5728\u533b\u7597\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u91cf\u5316\u548c\u4f4d\u6df1\u5ea6\u4f18\u5316\u7b56\u7565\uff0c\u5c06\u8f93\u5165\u6570\u636e\u4ecefloat64\u964d\u7ea7\u5230float32\u548cint32\u3002", "result": "\u65f6\u95f4\u590d\u6742\u6027\u663e\u8457\u964d\u4f4e\uff0c\u4f18\u5316\u540e\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\uff0c\u5c55\u793a\u4e86\u5148\u8fdb\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u4f18\u5316\u6280\u672f\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u4e00\u7ec4\u53c2\u6570\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u60c5\u51b5\u8fdb\u884c\u8c03\u6574\u3002"}}
{"id": "2511.12581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12581", "abs": "https://arxiv.org/abs/2511.12581", "authors": ["Kai Ma", "Zhen Wang", "Hongquan He", "Qi Xu", "Tinghuan Chen", "Hao Geng"], "title": "LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction", "comment": null, "summary": "Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7f51\u8868\u53d8\u6362\u5668\uff08LNT\uff09\u5904\u7406SPICE\u6587\u4ef6\uff0c\u5c06\u7f51\u8868\u62d3\u6251\u8868\u793a\u4e3a3D\u70b9\u4e91\uff0c\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\u6570\u767e\u4e07\u8282\u70b9\u7684\u7f51\u8868\uff0c\u7528\u4e8e\u9759\u6001\u7535\u538b\u964d\u9884\u6d4b\u3002", "motivation": "\u9759\u6001IR\u538b\u964d\u5206\u6790\u5728\u82af\u7247\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\u4f46\u8017\u65f6\uff0c\u53ef\u80fd\u9700\u8981\u6570\u5c0f\u65f6\uff0c\u4e14\u89e3\u51b3IR\u538b\u964d\u8fdd\u89c4\u9700\u8981\u8fed\u4ee3\u5206\u6790\uff0c\u8ba1\u7b97\u8d1f\u62c5\u91cd\u3002\u5feb\u901f\u51c6\u786e\u7684IR\u538b\u964d\u9884\u6d4b\u5bf9\u51cf\u5c11\u82af\u7247\u8bbe\u8ba1\u603b\u65f6\u95f4\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7f51\u8868\u53d8\u6362\u5668\u5904\u7406SPICE\u6587\u4ef6\uff0c\u5c06\u7f51\u8868\u62d3\u6251\u8868\u793a\u4e3a3D\u70b9\u4e91\u8868\u793a\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u6570\u5341\u4e07\u5230\u6570\u767e\u4e07\u8282\u70b9\u7684\u7f51\u8868\u3002\u6240\u6709\u7c7b\u578b\u6570\u636e\uff08\u7f51\u8868\u6587\u4ef6\u548c\u56fe\u50cf\u6570\u636e\uff09\u90fd\u88ab\u7f16\u7801\u4e3a\u6f5c\u5728\u7a7a\u95f4\u7279\u5f81\u5e76\u8f93\u5165\u6a21\u578b\u8fdb\u884c\u9759\u6001\u7535\u538b\u964d\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728ICCAD 2023\u7ade\u8d5b\u83b7\u80dc\u56e2\u961f\u548c\u6700\u5148\u8fdb\u7b97\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73F1\u5206\u6570\u548c\u6700\u4f4eMAE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u96c6\u6210\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u4e92\u8865\u9884\u6d4b\uff0c\u5728\u9759\u6001\u7535\u538b\u964d\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.12601", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12601", "abs": "https://arxiv.org/abs/2511.12601", "authors": ["Odysseas Boufalis", "Jorge Carrasco-Pollo", "Joshua Rosenthal", "Eduardo Terres-Caballero", "Alejandro Garc\u00eda-Castellanos"], "title": "Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization", "comment": null, "summary": "Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.", "AI": {"tldr": "ScaleGMNs\u901a\u8fc7\u6784\u5efa\u5bf9\u6392\u5217\u548c\u53c2\u6570\u7f29\u653e\u53d8\u6362\u7b49\u53d8\u7684\u67b6\u6784\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u7684\u5185\u5728\u5bf9\u79f0\u6027\uff0c\u5c06\u76f8\u4f3c\u7f51\u7edc\u6620\u5c04\u5230\u540c\u4e00\u635f\u5931\u76c6\u5730\uff0c\u5b9e\u73b0\u6a21\u578b\u5408\u5e76\u548c\u5e73\u6ed1\u7ebf\u6027\u63d2\u503c\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u5b58\u5728\u5185\u5728\u5bf9\u79f0\u6027\uff0c\u4ea7\u751f\u591a\u4e2a\u7b49\u6548\u6700\u5c0f\u503c\u3002\u5148\u524d\u5de5\u4f5c\u4ec5\u5904\u7406\u6392\u5217\u5bf9\u79f0\u6027\uff0c\u672c\u6587\u6269\u5c55\u8be5\u65b9\u6cd5\uff0c\u540c\u65f6\u7eb3\u5165\u7f29\u653e\u5bf9\u79f0\u6027\u3002", "method": "\u4f7f\u7528ScaleGMNs\u4f5c\u4e3a\u4e0d\u53d8\u7f16\u7801\u5668\u7684\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u65e0\u9700\u663e\u5f0f\u89e3\u51b3\u5206\u914d\u95ee\u9898\u5373\u53ef\u5bf9\u9f50INRs\u548cCNNs\u5728\u6392\u5217\u548c\u7f29\u653e\u5bf9\u79f0\u6027\u4e0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5c06\u76f8\u4f3c\u7f51\u7edc\u81ea\u7136\u6536\u655b\u5230\u540c\u4e00\u76c6\u5730\uff0c\u5b9e\u73b0\u6a21\u578b\u5408\u5e76\u548c\u5e73\u6ed1\u7ebf\u6027\u63d2\u503c\uff0c\u907f\u514d\u9ad8\u635f\u5931\u533a\u57df\u3002", "conclusion": "ScaleGMNs\u901a\u8fc7\u5229\u7528\u6392\u5217\u548c\u7f29\u653e\u5bf9\u79f0\u6027\uff0c\u6709\u6548\u4fc3\u8fdb\u6a21\u578b\u5408\u5e76\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u516c\u5f00\u3002"}}
{"id": "2511.12628", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12628", "abs": "https://arxiv.org/abs/2511.12628", "authors": ["Ke Hu", "Liyao Xiang", "Peng Tang", "Weidong Qiu"], "title": "FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions", "comment": "coference", "summary": "Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.", "AI": {"tldr": "FedTopo\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u5f15\u5bfc\u7684\u5757\u7b5b\u9009\u548c\u62d3\u6251\u5d4c\u5165\u6765\u89e3\u51b3\u975eIID\u6570\u636e\u4e0b\u7684\u8868\u793a\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u6536\u655b\u901f\u5ea6\u548c\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u5728\u5f02\u6784\uff08\u975eIID\uff09\u5ba2\u6237\u7aef\u6570\u636e\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u7279\u5f81\u8868\u793a\u53d1\u6563\uff0c\u50cf\u7d20\u6216\u5757\u7ea7\u76ee\u6807\u65e0\u6cd5\u6355\u6349\u5bf9\u9ad8\u7ef4\u89c6\u89c9\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u7684\u5168\u5c40\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u63d0\u51faFedTopo\u6846\u67b6\uff0c\u5305\u542b\u62d3\u6251\u5f15\u5bfc\u5757\u7b5b\u9009\uff08TGBS\uff09\u81ea\u52a8\u9009\u62e9\u6700\u5177\u62d3\u6251\u4fe1\u606f\u7684\u5757\uff0c\u62d3\u6251\u5d4c\u5165\uff08TE\uff09\u91cf\u5316\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u62d3\u6251\u4fe1\u606f\uff0c\u4ee5\u53ca\u62d3\u6251\u5bf9\u9f50\u635f\u5931\uff08TAL\uff09\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5ba2\u6237\u7aef\u4e0e\u5168\u5c40\u6a21\u578b\u7684\u62d3\u6251\u4e00\u81f4\u6027\u3002", "result": "\u5728Fashion-MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u7684\u56db\u79cd\u975eIID\u5206\u533a\u5b9e\u9a8c\u8868\u660e\uff0cFedTopo\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u52a0\u901f\u4e86\u6536\u655b\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u3002", "conclusion": "FedTopo\u901a\u8fc7\u5229\u7528\u62d3\u6251\u4fe1\u606f\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u5bfc\u81f4\u7684\u8868\u793a\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.12644", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12644", "abs": "https://arxiv.org/abs/2511.12644", "authors": ["Sascha Lange", "Roland Hafner", "Martin Riedmiller"], "title": "NFQ2.0: The CartPole Benchmark Revisited", "comment": null, "summary": "This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e8620\u5e74\u524d\u7684\u795e\u7ecf\u62df\u5408Q\u8fed\u4ee3(NFQ)\u7b97\u6cd5\uff0c\u63d0\u51fa\u73b0\u4ee3\u5316\u53d8\u4f53NFQ2.0\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc6\u522b\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56\u548c\u8d85\u53c2\u6570\uff0c\u63d0\u5347\u5728\u5de5\u4e1a\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u53ef\u91cd\u590d\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "NFQ\u4f5c\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5148\u9a71\u65b9\u6cd5\uff0c\u867d\u7136\u5728\u771f\u5b9e\u4e16\u754c\u63a7\u5236\u95ee\u9898\u4e0a\u53d6\u5f97\u521d\u6b65\u6210\u529f\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8c03\u53c2\u4e14\u96be\u4ee5\u590d\u73b0\u3002\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u5b66\u4e60\u8fc7\u7a0b\u7684\u53ef\u91cd\u590d\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faNFQ2.0\u73b0\u4ee3\u5316\u53d8\u4f53\uff0c\u5e94\u7528\u4e8eCartPole\u4efb\u52a1\uff0c\u4f7f\u7528\u6807\u51c6\u5de5\u4e1a\u7ec4\u4ef6\u6784\u5efa\u771f\u5b9e\u7cfb\u7edf\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5206\u6790\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56\u548c\u8d85\u53c2\u6570\u3002", "result": "NFQ2.0\u5728\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u539f\u59cb\u7248\u672c\uff0c\u7814\u7a76\u8bc6\u522b\u51fa\u589e\u5f3a\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u4ece\u4e1a\u8005\u66f4\u597d\u5730\u590d\u73b0\u548c\u6539\u8fdb\u7ed3\u679c\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u66f4\u6709\u6548\u5730\u5e94\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u3002"}}
{"id": "2511.12663", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12663", "abs": "https://arxiv.org/abs/2511.12663", "authors": ["Chen Gu", "Yingying Sun", "Yifan She", "Donghui Hu"], "title": "FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning", "comment": null, "summary": "Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.", "AI": {"tldr": "FLClear\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u8f6c\u7f6e\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u65e0\u78b0\u649e\u6c34\u5370\u805a\u5408\u3001\u589e\u5f3a\u6c34\u5370\u5b89\u5168\u6027\u4ee5\u53ca\u53ef\u89c6\u5316\u6240\u6709\u6743\u9a8c\u8bc1\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u4e2d\u592e\u670d\u52a1\u5668\u53ef\u80fd\u6076\u610f\u64cd\u7eb5\u5168\u5c40\u6a21\u578b\u4ee5\u62b9\u9664\u5ba2\u6237\u7aef\u8d21\u732e\u6216\u865a\u5047\u58f0\u79f0\u6240\u6709\u6743\uff0c\u4fb5\u72af\u5ba2\u6237\u7aef\u7684\u77e5\u8bc6\u4ea7\u6743\u3002\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5b58\u5728\u6c34\u5370\u78b0\u649e\u3001\u5b89\u5168\u6027\u4e0d\u8db3\u548c\u9a8c\u8bc1\u673a\u5236\u4e0d\u76f4\u89c2\u7b49\u95ee\u9898\u3002", "method": "\u5f15\u5165\u8f6c\u7f6e\u6a21\u578b\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u8054\u5408\u4f18\u5316\uff0c\u96c6\u6210\u6c34\u5370\u548c\u4e3b\u4efb\u52a1\u76ee\u6807\u3002\u9a8c\u8bc1\u65f6\u4ece\u8f6c\u7f6e\u6a21\u578b\u91cd\u6784\u6c34\u5370\uff0c\u901a\u8fc7\u89c6\u89c9\u68c0\u67e5\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u805a\u5408\u65b9\u6848\u548c\u653b\u51fb\u573a\u666f\u4e0b\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cFLClear\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u5b66\u4e60\u6c34\u5370\u65b9\u6cd5\u3002", "conclusion": "FLClear\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u6c34\u5370\u805a\u5408\u3001\u589e\u5f3a\u5b89\u5168\u6027\u548c\u76f4\u89c2\u9a8c\u8bc1\u3002"}}
{"id": "2511.12682", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.12682", "abs": "https://arxiv.org/abs/2511.12682", "authors": ["Amirpasha Hedayat", "Karthik Duraisamy"], "title": "Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction", "comment": "13 pages, 7 figures, Preprint", "summary": "Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u77ed\u671f\u5929\u6c14\u9884\u62a5\u7684\u9ad8\u6548\u964d\u9636\u5efa\u6a21\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8eResNet\u7684\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548c\u5757\u6ce8\u610f\u529b\u6a21\u5757\u6765\u964d\u7ef4\uff0c\u5728\u5ef6\u8fdf\u5d4c\u5165\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7ebf\u6027\u7b97\u5b50\u6765\u6355\u83b7\u52a8\u6001\u3002", "motivation": "\u5929\u6c14\u9884\u62a5\u662f\u4e00\u4e2a\u590d\u6742\u7684\u9ad8\u7ef4\u975e\u7ebf\u6027\u6df7\u6c8c\u7cfb\u7edf\u9884\u6d4b\u95ee\u9898\uff0c\u73b0\u6709AI\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u672c\u5de5\u4f5c\u4f18\u5148\u8003\u8651\u6548\u7387\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7cbe\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eResNet\u7684\u5377\u79ef\u81ea\u7f16\u7801\u5668\uff0c\u589e\u5f3a\u5757\u6ce8\u610f\u529b\u6a21\u5757\u6765\u964d\u7ef4\u5929\u6c14\u6570\u636e\uff0c\u5728\u5ef6\u8fdf\u5d4c\u5165\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7ebf\u6027\u7b97\u5b50\u6765\u9ad8\u6548\u6355\u83b7\u52a8\u6001\u3002", "result": "\u4f7f\u7528ERA5\u518d\u5206\u6790\u6570\u636e\u96c6\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u6570\u636e\u671f\u95f4\u5185\u80fd\u6709\u6548\u9884\u6d4b\u5929\u6c14\u6a21\u5f0f\uff0c\u4f46\u5728\u6cdb\u5316\u5230\u672a\u6765\u72b6\u6001\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u8d85\u51fa\u8bad\u7ec3\u7a97\u53e3\u540e\u9884\u6d4b\u7cbe\u5ea6\u4e0b\u964d\u3002", "conclusion": "\u5929\u6c14\u7cfb\u7edf\u5728\u9002\u5f53\u6784\u5efa\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u53ef\u901a\u8fc7\u7ebf\u6027\u64cd\u4f5c\u6709\u6548\u6355\u83b7\u5f3a\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u6295\u5f71\u8bef\u5dee\u800c\u975e\u63a8\u7406\u8bef\u5dee\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u4e3a\u6df7\u6c8c\u7cfb\u7edf\u964d\u9636\u5efa\u6a21\u63d0\u4f9b\u91cd\u8981\u89c1\u89e3\uff0c\u5efa\u8bae\u5c06\u9ad8\u6548\u964d\u9636\u6a21\u578b\u4e0e\u66f4\u590d\u6742AI\u67b6\u6784\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\u3002"}}
{"id": "2511.12706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12706", "abs": "https://arxiv.org/abs/2511.12706", "authors": ["Daniel Furelos-Blanco", "Charles Pert", "Frederik Kelbel", "Alex F. Spies", "Alessandra Russo", "Michael Dennis"], "title": "Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs", "comment": "Extended version of paper accepted for publication at the 40th AAAI Conference on Artificial Intelligence (AAAI)", "summary": "Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.", "AI": {"tldr": "ATLAS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u548c\u5173\u5361\u8054\u5408\u8bfe\u7a0b\uff0c\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u4efb\u52a1\u548c\u5173\u5361\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u7684\u6311\u6218\u3002", "motivation": "\u8bad\u7ec3\u901a\u7528\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u9075\u5faa\u590d\u6742\u6307\u4ee4\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u6311\u6218\u3002\u968f\u673a\u91c7\u6837\u4efb\u52a1-\u5173\u5361\u5bf9\u901a\u5e38\u4f1a\u4ea7\u751f\u4e0d\u53ef\u89e3\u7684\u7ec4\u5408\uff0c\u56e0\u6b64\u9700\u8981\u534f\u540c\u8bbe\u8ba1\u4efb\u52a1\u548c\u5173\u5361\u3002", "method": "\u57fa\u4e8e\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1(UED)\uff0cATLAS\u81ea\u52a8\u751f\u6210\u53ef\u89e3\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1-\u5173\u5361\u5bf9\u7528\u4e8e\u7b56\u7565\u8bad\u7ec3\uff0c\u5229\u7528\u4efb\u52a1\u548c\u5173\u5361\u7ed3\u6784\u7684\u7a81\u53d8\u6765\u52a0\u901f\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eATLAS\u663e\u8457\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u53ef\u89e3\u5bf9\u91c7\u6837\u6982\u7387\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u3002\u5229\u7528\u4efb\u52a1\u548c\u5173\u5361\u7ed3\u6784\u7684\u7a81\u53d8\u80fd\u52a0\u901f\u6536\u655b\u5230\u9ad8\u6027\u80fd\u7b56\u7565\u3002", "conclusion": "ATLAS\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u548c\u5173\u5361\u7684\u8054\u5408\u8bfe\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e2d\u6307\u4ee4\u8ddf\u968f\u7684\u6311\u6218\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.12709", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12709", "abs": "https://arxiv.org/abs/2511.12709", "authors": ["Sangwoo Seo", "Hyunsung Kim", "Jiwan Kim", "Chanyoung Park"], "title": "Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations", "comment": "Preprint", "summary": "Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.", "AI": {"tldr": "\u63d0\u51faAdaMeshNet\u6846\u67b6\uff0c\u5728\u57fa\u4e8e\u7f51\u683c\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u91cd\u8fde\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8ba1\u7b97\u91cd\u8fde\u5ef6\u8fdf\u5206\u6570\u6765\u52a8\u6001\u9009\u62e9\u6d88\u606f\u4f20\u9012\u5c42\u8fdb\u884c\u91cd\u8fde\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u77ac\u65f6\u4ea4\u4e92\u548c\u5ffd\u7565\u8ddd\u79bb\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7f51\u683c\u7ec6\u5316\u6280\u672f\u4f1a\u5bfc\u81f4\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8fc7\u5ea6\u6324\u538b\u95ee\u9898\uff0c\u963b\u788d\u957f\u7a0b\u7269\u7406\u4ea4\u4e92\u7684\u6355\u6349\u3002\u73b0\u6709\u56fe\u91cd\u8fde\u65b9\u6cd5\u5728\u5e94\u7528\u524d\u5b8c\u6210\u6240\u6709\u91cd\u8fde\u64cd\u4f5c\uff0c\u5047\u8bbe\u8282\u70b9\u95f4\u77ac\u65f6\u4ea4\u4e92\u4e14\u5ffd\u7565\u7c92\u5b50\u95f4\u8ddd\u79bb\u4fe1\u606f\uff0c\u8fd9\u5728\u7269\u7406\u4e0a\u662f\u4e0d\u73b0\u5b9e\u7684\u3002", "method": "\u63d0\u51faAdaMeshNet\u6846\u67b6\uff0c\u5728\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u91cd\u8fde\u8fc7\u7a0b\u3002\u8ba1\u7b97\u57fa\u4e8e\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u548c\u901f\u5ea6\u5dee\u7684\u91cd\u8fde\u5ef6\u8fdf\u5206\u6570\uff0c\u52a8\u6001\u9009\u62e9\u6d88\u606f\u4f20\u9012\u5c42\u8fdb\u884c\u91cd\u8fde\uff0c\u5b9e\u73b0\u7f51\u683c\u56fe\u7684\u81ea\u9002\u5e94\u91cd\u8fde\u3002", "result": "\u5728\u57fa\u4e8e\u7f51\u683c\u7684\u6d41\u4f53\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cAdaMeshNet\u4f18\u4e8e\u4f20\u7edf\u91cd\u8fde\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5efa\u6a21\u7269\u7406\u4ea4\u4e92\u7684\u5e8f\u5217\u6027\u8d28\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002", "conclusion": "AdaMeshNet\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u8fde\u8fc7\u7a0b\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u7269\u7406\u4e0d\u73b0\u5b9e\u6027\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u7f51\u683c\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6d41\u4f53\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2511.12713", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12713", "abs": "https://arxiv.org/abs/2511.12713", "authors": ["Pedro Il\u00eddio", "Felipe Kenji Nakano", "Alireza Gharahighehi", "Robbe D'hondt", "Ricardo Cerri", "Celine Vens"], "title": "Oxytrees: Model Trees for Bipartite Learning", "comment": "7 pages, 6 figures, AAAI Conference on Artificial Intelligence 2026", "summary": "Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.", "AI": {"tldr": "\u63d0\u51faOxytrees\uff1a\u57fa\u4e8e\u4ee3\u7406\u7684\u53cc\u805a\u7c7b\u6a21\u578b\u6811\uff0c\u7528\u4e8e\u4e8c\u5206\u5b66\u4e60\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6\u800c\u4e0d\u635f\u5931\u9884\u6d4b\u6027\u80fd", "motivation": "\u5f53\u524d\u4e8c\u5206\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f80\u5f80\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u5176\u4ed6\u95ee\u9898\u6216\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898", "method": "\u901a\u8fc7\u5c06\u4ea4\u4e92\u77e9\u9635\u538b\u7f29\u4e3a\u884c\u548c\u5217\u4ee3\u7406\u77e9\u9635\u6765\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff1b\u63d0\u51fa\u65b0\u7684\u53f6\u5b50\u5206\u914d\u7b97\u6cd5\u52a0\u901f\u9884\u6d4b\uff1b\u5728\u53f6\u5b50\u4e2d\u4f7f\u7528\u57fa\u4e8eKronecker\u79ef\u6838\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u751f\u6210\u66f4\u6d45\u7684\u6811", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u53cc\u805a\u7c7b\u68ee\u6797\uff0c\u8bad\u7ec3\u65f6\u95f4\u63d0\u5347\u9ad8\u8fbe30\u500d\uff0c\u5728\u5927\u591a\u6570\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u8868\u73b0\u7ade\u4e89\u6027\u6216\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u5f52\u7eb3\u8bbe\u7f6e\u4e2d", "conclusion": "Oxytrees\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86Python API\u4ee5\u652f\u6301\u8be5\u9886\u57df\u7684\u53ef\u91cd\u590d\u7814\u7a76"}}
{"id": "2511.12722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12722", "abs": "https://arxiv.org/abs/2511.12722", "authors": ["Nakshatra Gupta", "Sumanth Prabhu", "Supratik Chakraborty", "R Venkatesh"], "title": "On Robustness of Linear Classifiers to Targeted Data Poisoning", "comment": null, "summary": "Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6d4b\u91cf\u6570\u636e\u96c6\u5bf9\u6807\u7b7e\u6270\u52a8\u4e2d\u6bd2\u653b\u51fb\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u9c81\u68d2\u6027\u7684\u4e0a\u4e0b\u754c\u6765\u8bc4\u4f30\u6a21\u578b\u5b89\u5168\u6027\u3002", "motivation": "\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u4f1a\u7834\u574f\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\uff0c\u624b\u52a8\u68c0\u6d4b\u4e2d\u6bd2\u6837\u672c\u5728\u5927\u578b\u8bad\u7ec3\u96c6\u4e2d\u5f88\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u8bc4\u4f30\u6570\u636e\u96c6\u5bf9\u4e2d\u6bd2\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5728\u53ea\u80fd\u6270\u52a8\u8bad\u7ec3\u6570\u636e\u6807\u7b7e\u7684\u5a01\u80c1\u6a21\u578b\u4e0b\uff0c\u63d0\u51fa\u8ba1\u7b97\u9c81\u68d2\u6027\u4e0a\u4e0b\u754c\u7684\u6280\u672f\uff0c\u5373\u4f7f\u5bfb\u627e\u7cbe\u786e\u9c81\u68d2\u6027\u662fNP\u5b8c\u5168\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u8ba1\u7b97\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7684\u9c81\u68d2\u6027\u8fb9\u754c\uff0c\u8d85\u51fa\u8fb9\u754c\u7684\u6295\u6bd2\u4f1a\u663e\u8457\u5f71\u54cd\u6d4b\u8bd5\u70b9\u5206\u7c7b\uff0c\u4e14\u5728\u73b0\u6709\u6280\u672f\u5931\u8d25\u7684\u8bb8\u591a\u60c5\u51b5\u4e0b\u4ecd\u80fd\u8ba1\u7b97\u8fb9\u754c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6280\u672f\u80fd\u6709\u6548\u8bc4\u4f30\u6570\u636e\u96c6\u5bf9\u6807\u7b7e\u6270\u52a8\u4e2d\u6bd2\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2511.12723", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12723", "abs": "https://arxiv.org/abs/2511.12723", "authors": ["Gennaro Vessio"], "title": "LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks", "comment": null, "summary": "Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.", "AI": {"tldr": "LAYA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8f93\u51fa\u5c42\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u805a\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u95f4\u5c42\u7684\u8868\u793a\uff0c\u800c\u4e0d\u662f\u4ec5\u4f7f\u7528\u6700\u540e\u4e00\u5c42\u8868\u793a\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4ec5\u4f9d\u8d56\u6700\u540e\u4e00\u5c42\u9690\u85cf\u8868\u793a\u8fdb\u884c\u9884\u6d4b\uff0c\u4f46\u4e2d\u95f4\u5c42\u5305\u542b\u4ece\u4f4e\u5c42\u6a21\u5f0f\u5230\u9ad8\u5c42\u62bd\u8c61\u7684\u4e30\u5bcc\u4e92\u8865\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5f80\u5f80\u88ab\u4e22\u5f03\u3002", "method": "\u63d0\u51faLAYA\uff08Layer-wise Attention Aggregator\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u8f93\u5165\u6761\u4ef6\u5316\u7684\u6ce8\u610f\u529b\u6743\u91cd\u6765\u52a8\u6001\u805a\u5408\u5404\u5c42\u7279\u5f81\u8868\u793a\uff0c\u5f62\u6210\u67b6\u6784\u65e0\u5173\u7684\u9884\u6d4b\u673a\u5236\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAYA\u59cb\u7ec8\u5339\u914d\u6216\u4f18\u4e8e\u6807\u51c6\u8f93\u51fa\u5c42\uff0c\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u5347\u7ea61\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5c42\u5f52\u56e0\u5206\u6570\u3002", "conclusion": "LAYA\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u76f4\u63a5\u4ece\u6a21\u578b\u8ba1\u7b97\u4e2d\u4ea7\u751f\u7684\u53ef\u89e3\u91ca\u6027\u4fe1\u53f7\uff0c\u65e0\u9700\u5916\u90e8\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u3002"}}
{"id": "2511.12725", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12725", "abs": "https://arxiv.org/abs/2511.12725", "authors": ["William Ward Armstrong"], "title": "Convolutional Model Trees", "comment": "9 pages. No figures. This paper gives an algorithm for creating a continuously differentiable approximation from sample data from the same type of function(in theory) using a forest of model trees (like CART trees with linear functions instead of constants)", "summary": "A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u5efa\u6a21\u578b\u6811\u68ee\u6797\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u964d\u91c7\u6837\u56fe\u50cf\u3001\u786e\u5b9a\u6811\u7684\u8d85\u5e73\u9762\u3001\u5e94\u7528\u5377\u79ef\u5904\u7406\u8bad\u7ec3\u56fe\u50cf\u7684\u5c0f\u53d8\u5f62\uff0c\u4ee5\u53ca\u521b\u5efa\u6a21\u578b\u6811\u68ee\u6797\u6765\u63d0\u9ad8\u7cbe\u5ea6\u548c\u5e73\u6ed1\u62df\u5408\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u56fe\u50cf\u51fd\u6570\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u8981\u5904\u7406\u56fe\u50cf\u7684\u5c0f\u53d8\u5f62\uff08\u5982\u65cb\u8f6c\u548c\u89c6\u89d2\u53d8\u5316\uff09\uff0c\u5e76\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5e73\u6ed1\u7684\u62df\u5408\u3002", "method": "\u901a\u8fc7\u964d\u91c7\u6837\u56fe\u50cf\u3001\u786e\u5b9a\u8d85\u5e73\u9762\u3001\u5e94\u7528\u5377\u79ef\u5904\u7406\u5c0f\u53d8\u5f62\u3001\u521b\u5efa\u6a21\u578b\u6811\u68ee\u6797\uff0c\u5e76\u5229\u7528\u50cf\u7d20\u3001\u8d85\u5e73\u9762\u7cfb\u6570\u548c\u53f6\u51fd\u6570\u7cfb\u6570\u4e4b\u95f4\u76841\u5bf91\u5bf9\u5e94\u5173\u7cfb\u6765\u5904\u7406\u66f4\u5927\u7684\u53d8\u5f62\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u56fe\u50cf\u7684\u5c0f\u53d8\u5f62\u548c\u66f4\u5927\u7684\u53d8\u5f62\uff08\u5982\u4efb\u610f\u65cb\u8f6c\u548c\u89c6\u89d2\u53d8\u5316\uff09\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u65b9\u6cd5\u5e73\u6ed1\u68ee\u6797\u8f93\u51fa\u4ee5\u4ea7\u751f\u8fde\u7eed\u53ef\u5fae\u7684\u8fd1\u4f3c\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bad\u7ec3\u8fc7\u7a0b\u88ab\u8bc1\u660e\u662f\u6536\u655b\u7684\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u56fe\u50cf\u53d8\u5f62\u5e76\u5b9e\u73b0\u5e73\u6ed1\u3001\u9ad8\u7cbe\u5ea6\u7684\u51fd\u6570\u62df\u5408\u3002"}}
{"id": "2511.12742", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12742", "abs": "https://arxiv.org/abs/2511.12742", "authors": ["Zhongteng Cai", "Yaxuan Wang", "Yang Liu", "Xueru Zhang"], "title": "Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering", "comment": "Accepted by AAAI-26", "summary": "As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop\" that can lead to training instability or \\textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \\textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6f5c\u5728\u7a7a\u95f4\u8fc7\u6ee4\uff08LSF\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u6ee4\u6df7\u5408\u6570\u636e\u96c6\u4e2d\u4e0d\u592a\u771f\u5b9e\u7684\u5408\u6210\u6570\u636e\u6765\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u65e0\u9700\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u6216\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u968f\u7740\u5408\u6210\u6570\u636e\u5728\u4e92\u8054\u7f51\u4e0a\u7684\u6269\u6563\uff0c\u5b83\u4eec\u7ecf\u5e38\u88ab\u91cd\u590d\u7528\u4e8e\u8bad\u7ec3\u65b0\u4e00\u4ee3\u751f\u6210\u6a21\u578b\uff0c\u5f62\u6210\"\u81ea\u6211\u6d88\u8017\u5faa\u73af\"\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u6a21\u578b\u5d29\u6e83\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3002", "method": "\u57fa\u4e8e\u5bf9\u81ea\u6d88\u8017\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u52a8\u6001\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u63d0\u51fa\u6f5c\u5728\u7a7a\u95f4\u8fc7\u6ee4\uff08LSF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u6ee4\u6389\u4e0d\u592a\u771f\u5b9e\u7684\u5408\u6210\u6570\u636e\u6765\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLSF\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u3002", "conclusion": "LSF\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u6216\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u6709\u6548\u65b9\u6cd5\u6765\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2511.12745", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12745", "abs": "https://arxiv.org/abs/2511.12745", "authors": ["Vivek Chawla", "Boris Slautin", "Utkarsh Pratiush", "Dayakar Penumadu", "Sergei Kalinin"], "title": "DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes", "comment": "33 pages, 10 main figures, 7 additional in SI", "summary": "Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.", "AI": {"tldr": "DIVIDE\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u673a\u5236\u7279\u5b9a\u7684\u6df1\u5ea6\u7f16\u7801\u5668\u548c\u7ed3\u6784\u5316\u9ad8\u65af\u8fc7\u7a0b\u6765\u89e3\u8026\u79d1\u5b66\u6570\u636e\u96c6\u4e2d\u591a\u4e2a\u72ec\u7acb\u673a\u5236\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u673a\u5236\u611f\u77e5\u9884\u6d4b\u3002", "motivation": "\u79d1\u5b66\u6570\u636e\u96c6\u901a\u5e38\u6765\u81ea\u591a\u4e2a\u72ec\u7acb\u673a\u5236\uff08\u5982\u7a7a\u95f4\u3001\u5206\u7c7b\u6216\u7ed3\u6784\u6548\u5e94\uff09\uff0c\u8fd9\u4e9b\u673a\u5236\u7684\u5171\u540c\u5f71\u54cd\u63a9\u76d6\u4e86\u5404\u81ea\u7684\u8d21\u732e\uff0c\u9700\u8981\u89e3\u8026\u8fd9\u4e9b\u5f71\u54cd\u4ee5\u83b7\u5f97\u66f4\u6e05\u6670\u7684\u4e2a\u4f53\u8d21\u732e\u7406\u89e3\u3002", "method": "\u6574\u5408\u673a\u5236\u7279\u5b9a\u7684\u6df1\u5ea6\u7f16\u7801\u5668\u4e0e\u7ed3\u6784\u5316\u9ad8\u65af\u8fc7\u7a0b\u5728\u8054\u5408\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u7f16\u7801\u5668\u5206\u79bb\u4e0d\u540c\u673a\u5236\uff0c\u9ad8\u65af\u8fc7\u7a0b\u6355\u6349\u5176\u7ec4\u5408\u6548\u5e94\u5e76\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\uff0c\u652f\u6301\u7ed3\u6784\u5316\u5148\u9a8c\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u3001FerroSIM\u94c1\u7535\u6a21\u5f0f\u6a21\u62df\u548c\u5b9e\u9a8cPFM\u78c1\u6ede\u56de\u7ebf\u4e0a\uff0cDIVIDE\u6210\u529f\u5206\u79bb\u673a\u5236\uff0c\u91cd\u73b0\u52a0\u6027\u548c\u7f29\u653e\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u5728\u566a\u58f0\u4e0b\u4fdd\u6301\u7a33\u5065\u3002", "conclusion": "DIVIDE\u80fd\u591f\u6709\u6548\u89e3\u8026\u79d1\u5b66\u6570\u636e\u96c6\u4e2d\u7684\u72ec\u7acb\u673a\u5236\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u9884\u6d4b\u548c\u9ad8\u6548\u4e3b\u52a8\u5b66\u4e60\uff0c\u53ef\u81ea\u7136\u6269\u5c55\u5230\u591a\u529f\ufffd\ufffd\u6570\u636e\u96c6\u3002"}}
{"id": "2511.12751", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12751", "abs": "https://arxiv.org/abs/2511.12751", "authors": ["Timur Anvar", "Jeffrey Chen", "Yuyan Wang", "Rohan Chandra"], "title": "Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving", "comment": null, "summary": "Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5c0f\u578b\u672c\u5730\u90e8\u7f72LLM\uff08<14B\u53c2\u6570\uff09\u901a\u8fc7\u5956\u52b1\u5851\u5f62\u800c\u975e\u76f4\u63a5\u63a7\u5236\u6765\u652f\u6301\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\u3002\u6bd4\u8f83\u4e86\u7eafRL\u3001\u7eafLLM\u548c\u6df7\u5408\u65b9\u6cd5\uff0c\u53d1\u73b0\u6df7\u5408\u65b9\u6cd5\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f46LLM\u5f71\u54cd\u7684\u65b9\u6cd5\u5b58\u5728\u7cfb\u7edf\u6027\u4fdd\u5b88\u504f\u5dee\u3002", "motivation": "RL\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u8bed\u4e49\u548c\u793e\u4f1a\u60c5\u5883\uff1b\u7eafLLM\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u4e0d\u7a33\u5b9a\u4e14\u4f9d\u8d56\u6602\u8d35API\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c0f\u578b\u672c\u5730LLM\u901a\u8fc7\u5956\u52b1\u5851\u5f62\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\u4e09\u79cd\u65b9\u6cd5\uff1a\u7eafRL\u3001\u7eafLLM\u3001\u6df7\u5408\u65b9\u6cd5\uff08LLM\u5728\u8bad\u7ec3\u671f\u95f4\u901a\u8fc7\u8bc4\u5206\u72b6\u6001-\u52a8\u4f5c\u8f6c\u6362\u6765\u589e\u5f3aRL\u5956\u52b1\uff0c\u6d4b\u8bd5\u65f6\u4f7f\u7528\u6807\u51c6RL\u7b56\u7565\u6267\u884c\uff09\u3002", "result": "\u7eafRL\u6210\u529f\u738773-89%\uff0c\u6548\u7387\u5408\u7406\uff1b\u7eafLLM\u6210\u529f\u7387\u53ef\u8fbe94%\u4f46\u901f\u5ea6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff1b\u6df7\u5408\u65b9\u6cd5\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u3002LLM\u5f71\u54cd\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u4fdd\u5b88\u504f\u5dee\u548c\u663e\u8457\u7684\u6a21\u578b\u4f9d\u8d56\u6027\u53d8\u5f02\u3002", "conclusion": "\u5f53\u524d\u5c0f\u578bLLM\u5728\u5b89\u5168\u5173\u952e\u63a7\u5236\u4efb\u52a1\u4e2d\u5b58\u5728\u91cd\u8981\u5c40\u9650\u6027\uff0c\u5c3d\u7ba1\u80fd\u63d0\u9ad8\u6210\u529f\u7387\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6548\u7387\u4e0b\u964d\u548c\u4fdd\u5b88\u884c\u4e3a\uff0c\u51f8\u663e\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8c28\u614e\u4f7f\u7528LLM\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.12764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12764", "abs": "https://arxiv.org/abs/2511.12764", "authors": ["Hao Wei", "Aleksandra Franz", "Bjoern List", "Nils Thuerey"], "title": "INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers", "comment": "Accepted at NeurIPS 2025. 35 pages, 10 figures", "summary": "When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\\(\\mathrm{INC}\\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \\(\\mathrm{INC}\\) reduces the error amplification on the order of \\(\u0394t^{-1} + L\\), where \\(\u0394t\\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \\(\\mathrm{INC}\\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\\(R^2\\)) by up to 158.7\\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC", "AI": {"tldr": "\u63d0\u51fa\u95f4\u63a5\u795e\u7ecf\u6821\u6b63\u5668(INC)\uff0c\u901a\u8fc7\u5c06\u5b66\u4e60\u6821\u6b63\u96c6\u6210\u5230\u63a7\u5236\u65b9\u7a0b\u800c\u975e\u76f4\u63a5\u72b6\u6001\u66f4\u65b0\uff0c\u51cf\u5c11\u81ea\u56de\u5f52\u8bef\u5dee\uff0c\u5728\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684PDE\u4eff\u771f\u3002", "motivation": "\u4f20\u7edf\u6df7\u5408\u6c42\u89e3\u5668\u5c06\u5b66\u4e60\u6821\u6b63\u76f4\u63a5\u5e94\u7528\u4e8e\u6c42\u89e3\u5668\u8f93\u51fa\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u81ea\u56de\u5f52\u8bef\u5dee\uff0c\u7279\u522b\u662f\u5728\u6df7\u6c8c\u72b6\u6001\u4e0b\uff0c\u6270\u52a8\u4f1a\u7d2f\u79ef\u653e\u5927\u3002", "method": "INC\u65b9\u6cd5\u5c06\u5b66\u4e60\u6821\u6b63\u96c6\u6210\u5230\u63a7\u5236\u65b9\u7a0b\u4e2d\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u66f4\u65b0\u72b6\u6001\uff0c\u4ece\u800c\u5c06\u8bef\u5dee\u653e\u5927\u964d\u4f4e\u5230\u0394t\u207b\u00b9 + L\u7684\u91cf\u7ea7\u3002", "result": "\u57281D\u6df7\u6c8c\u7cfb\u7edf\u52303D\u6e4d\u6d41\u7b49\u591a\u79cd\u6d4b\u8bd5\u4e2d\uff0cINC\u5c06\u957f\u671f\u8f68\u8ff9\u6027\u80fd(R\u00b2)\u63d0\u5347\u9ad8\u8fbe158.7%\uff0c\u7a33\u5b9a\u4e86\u6fc0\u8fdb\u7c97\u5316\u4e0b\u7684\u7206\u70b8\u95ee\u9898\uff0c\u5728\u590d\u67423D\u6e4d\u6d41\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "conclusion": "INC\u5b9e\u73b0\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684PDE\u4eff\u771f\uff0c\u5177\u6709\u5f62\u5f0f\u5316\u8bef\u5dee\u51cf\u5c11\uff0c\u4e3a\u5177\u6709\u53ef\u9760\u7269\u7406\u4fdd\u8bc1\u7684\u66f4\u5feb\u79d1\u5b66\u548c\u5de5\u7a0b\u4eff\u771f\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.12770", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.12770", "abs": "https://arxiv.org/abs/2511.12770", "authors": ["Zhenyu Lei", "Patrick Soga", "Yaochen Zhu", "Yinhan He", "Yushun Dong", "Jundong Li"], "title": "MolEdit: Knowledge Editing for Multimodal Molecule Language Models", "comment": null, "summary": "Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.", "AI": {"tldr": "MolEdit\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u5b50\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u77e5\u8bc6\u9002\u914d\u5668\u548c\u4e13\u4e1a\u77e5\u8bc6\u611f\u77e5\u7f16\u8f91\u5207\u6362\u5668\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7684\u5206\u5b50\u77e5\u8bc6\u4fee\u6539\u540c\u65f6\u4fdd\u62a4\u65e0\u5173\u77e5\u8bc6\u3002", "motivation": "\u5206\u5b50\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u56e0\u8bad\u7ec3\u6570\u636e\u8fc7\u65f6\u6216\u6076\u610f\u7be1\u6539\u800c\u7f16\u7801\u548c\u4f20\u64ad\u9519\u8bef\u77e5\u8bc6\uff0c\u5f71\u54cd\u4e0b\u6e38\u53d1\u73b0\u6d41\u7a0b\u3002\u76ee\u524d\u77e5\u8bc6\u7f16\u8f91\u5728\u901a\u7528AI\u9886\u57df\u5df2\u6709\u63a2\u7d22\uff0c\u4f46\u5728\u5206\u5b50\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4ecd\u5c5e\u7a7a\u767d\uff0c\u9762\u4e34\u5206\u5b50\u77e5\u8bc6\u591a\u9762\u6027\u548c\u76f8\u4e92\u4f9d\u8d56\u6027\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51faMolEdit\u6846\u67b6\uff0c\u5305\u542b\u591a\u4e13\u5bb6\u77e5\u8bc6\u9002\u914d\u5668\uff08\u5c06\u7f16\u8f91\u8def\u7531\u5230\u4e0d\u540c\u5206\u5b50\u65b9\u9762\u7684\u4e13\u4e1a\u4e13\u5bb6\uff09\u548c\u4e13\u4e1a\u77e5\u8bc6\u611f\u77e5\u7f16\u8f91\u5207\u6362\u5668\uff08\u4ec5\u5728\u8f93\u5165\u4e0e\u5b58\u50a8\u7f16\u8f91\u9ad8\u5ea6\u5339\u914d\u65f6\u6fc0\u6d3b\u9002\u914d\u5668\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u6d41\u884c\u7684\u5206\u5b50\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u4e0a\uff0cMolEdit\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u4e0a\u63d0\u534718.8%\uff0c\u5728\u5c40\u90e8\u6027\u4e0a\u63d0\u534712.0%\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3002", "conclusion": "MolEdit\u662f\u9996\u4e2a\u9488\u5bf9\u5206\u5b50\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u4fee\u6539\u76ee\u6807\u77e5\u8bc6\u540c\u65f6\u4fdd\u62a4\u65e0\u5173\u5206\u5b50\u77e5\u8bc6\uff0c\u4e3a\u5206\u5b50\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2511.12779", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12779", "abs": "https://arxiv.org/abs/2511.12779", "authors": ["Zhenshuo Zhang", "Minxuan Duan", "Youran Ye", "Hongyang R. Zhang"], "title": "Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation", "comment": "17 pages. To appear in AAAI'26", "summary": "We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \\ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\\%$ on average, while delivering up to $26\\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.", "AI": {"tldr": "\u63d0\u51faPolicyGradEx\u7b97\u6cd5\uff0c\u901a\u8fc7\u5143\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5c06\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u9ad8\u6548\u805a\u7c7b\u6210\u5c11\u91cf\u76f8\u5173\u7ec4\uff0c\u5b9e\u73b016%\u6027\u80fd\u63d0\u5347\u548c26\u500d\u52a0\u901f\u3002", "motivation": "\u5728\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5f53\u76ee\u6807\u6570\u91cfn\u589e\u957f\u65f6\uff0c\u4e3a\u6240\u6709\u76ee\u6807\u5b66\u4e60\u5355\u4e00\u7b56\u7565\u662f\u6b21\u4f18\u7684\u3002\u9700\u8981\u5c06\u76f8\u5173\u76ee\u6807\u5206\u7ec4\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u5143\u8bad\u7ec3\u5b66\u4e60\u6240\u6709\u76ee\u6807\u7684\u5143\u7b56\u7565\uff1b2\uff09\u5fae\u8c03\u9002\u5e94\u968f\u673a\u91c7\u6837\u5b50\u96c6\uff0c\u5229\u7528\u7b56\u7565\u7f51\u7edc\u7684\u4e00\u9636\u8fd1\u4f3c\u7279\u6027\u4f30\u8ba1\u4efb\u52a1\u4eb2\u548c\u5ea6\u77e9\u9635\uff0c\u7136\u540e\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5728\u673a\u5668\u4eba\u63a7\u5236\u548cMeta-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534716%\uff0c\u901f\u5ea6\u63d0\u5347\u8fbe26\u500d\u3002\u57fa\u4e8e\u635f\u5931\u7684\u805a\u7c7b\u6bd4\u968f\u673a\u5206\u7ec4\u548c\u68af\u5ea6\u76f8\u4f3c\u6027\u5206\u7ec4\u63d0\u534719%\u3002", "conclusion": "PolicyGradEx\u80fd\u6709\u6548\u4f30\u8ba1\u4efb\u52a1\u4eb2\u548c\u5ea6\u5e76\u805a\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u3002\u901a\u8fc7Hessian\u8ff9\u5206\u6790\u9a8c\u8bc1\u4e86\u6cdb\u5316\u8bef\u5dee\u7684\u975e\u5e73\u51e1\u5ea6\u91cf\u3002"}}
{"id": "2511.12791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12791", "abs": "https://arxiv.org/abs/2511.12791", "authors": ["Dahao Tang", "Nan Yang", "Yanli Li", "Zhiyu Zhu", "Zhibo Jin", "Dong Yuan"], "title": "Optimal Look-back Horizon for Time Series Forecasting in Federated Learning", "comment": "Accepted by AAAI-26 as Oral Presentation", "summary": "Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u90a6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u81ea\u9002\u5e94\u56de\u6eaf\u7a97\u53e3\u9009\u62e9\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5728\u7a7a\u95f4\u516c\u5f0f\u5316\u89e3\u51b3\u6570\u636e\u5206\u6563\u3001\u5f02\u6784\u548c\u975e\u72ec\u7acb\u5206\u5e03\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5206\u6563\u3001\u5f02\u6784\u4e14\u975e\u72ec\u7acb\u5206\u5e03\uff0c\u9009\u62e9\u5408\u9002\u7684\u56de\u6eaf\u7a97\u53e3\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u96c6\u4e2d\u5f0f\u548c\u72ec\u7acb\u5206\u5e03\u8bbe\u7f6e\u3002", "method": "\u5f15\u5165\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u6355\u6349\u5ba2\u6237\u7aef\u6570\u636e\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u5b9a\u4e49\u5c06\u65f6\u95f4\u5e8f\u5217\u7a97\u53e3\u6620\u5c04\u5230\u5185\u5728\u8868\u793a\u7a7a\u95f4\u7684\u53d8\u6362\uff0c\u63a8\u5bfc\u9884\u6d4b\u635f\u5931\u7684\u5206\u89e3\u4e3a\u8d1d\u53f6\u65af\u9879\u548c\u8fd1\u4f3c\u9879\u3002", "result": "\u5206\u6790\u8868\u660e\u589e\u52a0\u56de\u6eaf\u7a97\u53e3\u53ef\u6539\u5584\u786e\u5b9a\u6027\u6a21\u5f0f\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u4f46\u4e5f\u4f1a\u56e0\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\u548c\u6837\u672c\u6548\u7387\u964d\u4f4e\u800c\u589e\u52a0\u8fd1\u4f3c\u8bef\u5dee\u3002", "conclusion": "\u5f53\u4e0d\u53ef\u7ea6\u635f\u5931\u5f00\u59cb\u9971\u548c\u800c\u8fd1\u4f3c\u635f\u5931\u6301\u7eed\u4e0a\u5347\u65f6\uff0c\u603b\u9884\u6d4b\u635f\u5931\u5728\u6700\u5c0f\u56de\u6eaf\u7a97\u53e3\u5904\u8fbe\u5230\u6700\u5c0f\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u81ea\u9002\u5e94\u7a97\u53e3\u9009\u62e9\u63d0\u4f9b\u4e86\u4e25\u683c\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.12797", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.12797", "abs": "https://arxiv.org/abs/2511.12797", "authors": ["Nathan Breslow", "Aayush Mishra", "Mahler Revsine", "Michael C. Schatz", "Anqi Liu", "Daniel Khashabi"], "title": "Genomic Next-Token Predictors are In-Context Learners", "comment": null, "summary": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?\n  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u8bc1\u660e\u57fa\u56e0\u7ec4\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u6d4b\u8bad\u7ec3\u80fd\u591f\u81ea\u7136\u6d8c\u73b0\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u8868\u660eICL\u662f\u8de8\u6a21\u6001\u7684\u901a\u7528\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u9650\u4e8e\u8bed\u8a00\u9886\u57df\u3002", "motivation": "\u63a2\u7d22\u4e0a\u4e0b\u6587\u5b66\u4e60\u662f\u5426\u80fd\u5728\u975e\u8bed\u8a00\u5e8f\u5217\u9886\u57df\uff08\u5982\u57fa\u56e0\u7ec4\u5e8f\u5217\uff09\u4e2d\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u6d4b\u8bad\u7ec3\u81ea\u7136\u6d8c\u73b0\uff0c\u6311\u6218ICL\u4ec5\u6e90\u4e8e\u4eba\u7c7b\u8bed\u8a00\u7edf\u8ba1\u7279\u6027\u7684\u89c2\u70b9\u3002", "method": "\u5f00\u53d1\u53d7\u63a7\u5b9e\u9a8c\u6846\u67b6\uff0c\u5728\u8bed\u8a00\u548c\u57fa\u56e0\u7ec4\u5f62\u5f0f\u4e2d\u5b9e\u4f8b\u5316\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\uff0c\u76f4\u63a5\u6bd4\u8f83\u57fa\u56e0\u7ec4\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u7684ICL\u8868\u73b0\u3002\u4f7f\u7528Evo2\u57fa\u56e0\u7ec4\u6a21\u578b\u8fdb\u884c\u5927\u89c4\u6a21\u6838\u82f7\u9178\u9884\u6d4b\u8bad\u7ec3\u3002", "result": "\u57fa\u56e0\u7ec4\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7c7b\u4f3c\uff0c\u968f\u7740\u4e0a\u4e0b\u6587\u6f14\u793a\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8868\u73b0\u51fa\u5bf9\u6570\u7ebf\u6027\u7684\u6a21\u5f0f\u5f52\u7eb3\u589e\u76ca\u3002\u8fd9\u662f\u57fa\u56e0\u7ec4\u5e8f\u5217\u4e2d\u9996\u6b21\u89c2\u5bdf\u5230\u7684\u81ea\u7136\u6d8c\u73b0ICL\u8bc1\u636e\u3002", "conclusion": "ICL\u662f\u5927\u89c4\u6a21\u9884\u6d4b\u5efa\u6a21\u5728\u4e30\u5bcc\u6570\u636e\u4e0a\u7684\u7ed3\u679c\uff0c\u5177\u6709\u8de8\u6a21\u6001\u7684\u666e\u9002\u6027\uff0c\u4e3a\u7406\u89e3\u5143\u5b66\u4e60\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u3001\u6a21\u6001\u65e0\u5173\u7684\u89c6\u89d2\u3002"}}
{"id": "2511.12804", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12804", "abs": "https://arxiv.org/abs/2511.12804", "authors": ["Ali Falahati", "Mohammad Mohammadi Amiri", "Kate Larson", "Lukasz Golab"], "title": "The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation", "comment": null, "summary": "In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u4e3a\u81ea\u6d88\u8d39\u751f\u6210\u6a21\u578b\u7684\u9012\u5f52\u518d\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u57fa\u4e8eBradley-Terry\u6a21\u578b\u7684\u504f\u597d\u5bf9\u9f50\u673a\u5236\u5728\u957f\u671f\u6f14\u5316\u4e2d\u7684\u4e09\u79cd\u6536\u655b\u6a21\u5f0f\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u591a\u6837\u6027\u3001\u5bf9\u79f0\u5f71\u54cd\u548c\u521d\u59cb\u5316\u72ec\u7acb\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u3002", "motivation": "\u968f\u7740\u81ea\u6d88\u8d39\u751f\u6210\u6a21\u578b\u5728\u81ea\u8eab\u8f93\u51fa\u4e0a\u8bad\u7ec3\uff0c\u7528\u6237\u504f\u597d\u5bf9\u9f50\u53d8\u6210\u4e86\u9012\u5f52\u8fc7\u7a0b\u800c\u975e\u4e00\u6b21\u6027\u4efb\u52a1\u3002\u9700\u8981\u5efa\u7acb\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u5206\u6790\u8fd9\u79cd\u9012\u5f52\u518d\u8bad\u7ec3\u5bf9\u957f\u671f\u5bf9\u9f50\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u57fa\u4e8eBradley-Terry\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u7b5b\u9009\u673a\u5236\uff0c\u5c06\u5bf9\u9f50\u5efa\u6a21\u4e3a\u6a21\u578b\u6240\u6709\u8005\uff08\u7b5b\u9009\u5b66\u4e60\u5185\u5bb9\uff09\u548c\u516c\u5171\u7528\u6237\uff08\u51b3\u5b9a\u5171\u4eab\u5185\u5bb9\uff09\u4e24\u4e2a\u9635\u8425\u7684\u4e92\u52a8\u8fc7\u7a0b\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e09\u79cd\u7ed3\u6784\u6536\u655b\u673a\u5236\uff1a\u5171\u8bc6\u5d29\u6e83\u3001\u5171\u4eab\u6700\u4f18\u59a5\u534f\u548c\u4e0d\u5bf9\u79f0\u7cbe\u70bc\u3002\u8bc1\u660e\u4e86\u9012\u5f52BT\u7b5b\u9009\u673a\u5236\u65e0\u6cd5\u540c\u65f6\u4fdd\u6301\u591a\u6837\u6027\u3001\u786e\u4fdd\u5bf9\u79f0\u5f71\u54cd\u548c\u6d88\u9664\u521d\u59cb\u5316\u4f9d\u8d56\u7684\u57fa\u672c\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u3002", "conclusion": "\u5bf9\u9f50\u4e0d\u662f\u9759\u6001\u76ee\u6807\u800c\u662f\u6f14\u5316\u5747\u8861\uff0c\u65e2\u53d7\u6743\u529b\u4e0d\u5bf9\u79f0\u6027\u5f71\u54cd\uff0c\u4e5f\u53d7\u8def\u5f84\u4f9d\u8d56\u5f71\u54cd\u3002\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u751f\u6210\u6a21\u578b\u9012\u5f52\u8bad\u7ec3\u7684\u793e\u4f1a\u52a8\u6001\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.12808", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.12808", "abs": "https://arxiv.org/abs/2511.12808", "authors": ["Omar Adalat", "Francesco Belardinelli"], "title": "Expressive Temporal Specifications for Reward Monitoring", "comment": null, "summary": "Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\\text{LTL}_f[\\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.", "AI": {"tldr": "\u4f7f\u7528\u5b9a\u91cf\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTL_f[F])\u5408\u6210\u5956\u52b1\u76d1\u63a7\u5668\uff0c\u4e3a\u53ef\u89c2\u6d4b\u72b6\u6001\u8f68\u8ff9\u751f\u6210\u5bc6\u96c6\u5956\u52b1\u6d41\uff0c\u89e3\u51b3\u957f\u65f6\u51b3\u7b56\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898", "motivation": "\u6307\u5b9a\u4fe1\u606f\u4e30\u5bcc\u4e14\u5bc6\u96c6\u7684\u5956\u52b1\u51fd\u6570\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u76f4\u63a5\u5f71\u54cd\u4ee3\u7406\u8bad\u7ec3\u6548\u7387\u3002\u5f53\u524d\u6587\u732e\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u5e03\u5c14\u8bed\u4e49\u5728\u957f\u65f6\u51b3\u7b56\u4e2d\u4f1a\u4ea7\u751f\u7a00\u758f\u5956\u52b1\u95ee\u9898", "method": "\u5229\u7528\u5b9a\u91cf\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5728\u6709\u9650\u8f68\u8ff9\u4e0a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5408\u6210\u5956\u52b1\u76d1\u63a7\u5668\uff0c\u57fa\u4e8e\u8fd0\u884c\u65f6\u53ef\u89c2\u6d4b\u72b6\u6001\u8f68\u8ff9\u751f\u6210\u5bc6\u96c6\u5956\u52b1\u6d41\u3002\u8be5\u6846\u67b6\u4e0e\u7b97\u6cd5\u65e0\u5173\uff0c\u4ec5\u4f9d\u8d56\u72b6\u6001\u6807\u8bb0\u51fd\u6570\uff0c\u81ea\u7136\u652f\u6301\u975e\u9a6c\u5c14\u53ef\u592b\u5c5e\u6027\u7684\u6307\u5b9a", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b9a\u91cf\u76d1\u63a7\u5668\u5728\u6700\u5927\u5316\u4efb\u52a1\u5b8c\u6210\u5ea6\u7684\u5b9a\u91cf\u6d4b\u91cf\u548c\u51cf\u5c11\u6536\u655b\u65f6\u95f4\u65b9\u9762\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5e03\u5c14\u76d1\u63a7\u5668\uff0c\u5177\u4f53\u8868\u73b0\u53d6\u51b3\u4e8e\u73af\u5883", "conclusion": "\u5b9a\u91cf\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5bc6\u96c6\u5956\u52b1\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5f15\u5bfc\u4ee3\u7406\u884c\u4e3a\uff0c\u89e3\u51b3\u957f\u65f6\u51b3\u7b56\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898"}}
{"id": "2511.12817", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12817", "abs": "https://arxiv.org/abs/2511.12817", "authors": ["Shasha Zhou", "Mingyu Huang", "Jack Cole", "Charles Britton", "Ming Yin", "Jan Wolber", "Ke Li"], "title": "Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs", "comment": "Accepted as a conference paper at AAAI'26", "summary": "The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.", "AI": {"tldr": "FAITH\u6846\u67b6\u5229\u7528\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u8bc4\u4f30LLM\u751f\u6210\u54cd\u5e94\u7684\u771f\u5b9e\u6027\uff0c\u65e0\u9700\u53c2\u8003\u7b54\u6848\uff0c\u901a\u8fc7\u5206\u89e3\u58f0\u660e\u3001\u94fe\u63a5\u77e5\u8bc6\u56fe\u8c31\u548c\u57fa\u4e8e\u8bc1\u636e\u8def\u5f84\u8bc4\u5206\uff0c\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u4e0e\u4e34\u5e8a\u533b\u751f\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72LLM\u9700\u8981\u4e25\u683c\u7684\u9a8c\u8bc1\uff0c\u4e86\u89e3\u6f5c\u5728\u5371\u5bb3\uff0c\u7814\u7a76\u4f7f\u7528\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u8bc4\u4f30LLM\u54cd\u5e94\u771f\u5b9e\u6027\u7684\u53ef\u9760\u6027\u548c\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faFAITH\u6846\u67b6\uff1a\u5206\u89e3\u54cd\u5e94\u4e3a\u539f\u5b50\u58f0\u660e\uff0c\u94fe\u63a5\u5230\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u57fa\u4e8e\u8bc1\u636e\u8def\u5f84\u8fdb\u884c\u8bc4\u5206\uff0c\u65e0\u9700\u53c2\u8003\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8bc4\u4f30\u4e0e\u4e34\u5e8a\u533b\u751f\u5224\u65ad\u76f8\u5173\u6027\u663e\u8457\u66f4\u9ad8\uff0c\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u80fd\u529b\u7684LLM\uff0c\u5bf9\u6587\u672c\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8bc4\u5206\u53ef\u89e3\u91ca\u6027\u6709\u52a9\u4e8e\u7406\u89e3LLM\u5c40\u9650\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u9650\u5236\uff0c\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u662f\u533b\u7597\u9886\u57df\u81ea\u52a8\u771f\u5b9e\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2511.12828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12828", "abs": "https://arxiv.org/abs/2511.12828", "authors": ["Mohammad Marufur Rahman", "Guanchu Wang", "Kaixiong Zhou", "Minghan Chen", "Fan Yang"], "title": "Catastrophic Forgetting in Kolmogorov-Arnold Networks", "comment": "14 pages, 5 figures, accepted in the main technical track of AAAI 2026", "summary": "Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86KANs\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u5206\u6790\u9057\u5fd8\u4e0e\u6fc0\u6d3b\u652f\u6301\u91cd\u53e0\u548c\u5185\u5728\u6570\u636e\u7ef4\u5ea6\u7684\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86KAN-LoRA\u9002\u914d\u5668\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5fae\u8c03\u3002", "motivation": "KANs\u88ab\u8ba4\u4e3a\u901a\u8fc7\u5c40\u90e8\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u5177\u6709\u5185\u5728\u7684\u6297\u9057\u5fd8\u80fd\u529b\uff0c\u4f46\u5176\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u884c\u4e3a\u548c\u5c40\u9650\u6027\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\u5206\u6790\u9057\u5fd8\u4e0e\u6fc0\u6d3b\u652f\u6301\u91cd\u53e0\u548c\u5185\u5728\u6570\u636e\u7ef4\u5ea6\u7684\u5173\u7cfb\uff0c\u5728\u5408\u6210\u548c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5e76\u63d0\u51fa\u4e86KAN-LoRA\u9002\u914d\u5668\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5fae\u8c03\u3002", "result": "KANs\u5728\u4f4e\u7ef4\u7b97\u6cd5\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\uff0c\u4f46\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u8a00\u5efa\u6a21\u7b49\u9ad8\u7ef4\u9886\u57df\u4ecd\u7136\u5bb9\u6613\u53d1\u751f\u9057\u5fd8\u3002KAN-LoRA\u5728\u77e5\u8bc6\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "KANs\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5177\u6709\u7279\u5b9a\u4f18\u52bf\uff0c\u4f46\u5728\u9ad8\u7ef4\u9886\u57df\u4ecd\u9762\u4e34\u9057\u5fd8\u6311\u6218\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2511.12829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12829", "abs": "https://arxiv.org/abs/2511.12829", "authors": ["Michael Chen", "Raghav Kansal", "Abhijith Gandrakota", "Zichun Hao", "Jennifer Ngadiuba", "Maria Spiropulu"], "title": "An Evaluation of Representation Learning Methods in Particle Physics Foundation Models", "comment": null, "summary": "We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u7c92\u5b50\u7269\u7406\u5b66\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u76ee\u6807\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e0b\u6bd4\u8f83\u4e86\u5bf9\u6bd4\u5b66\u4e60\u3001\u63a9\u7801\u7c92\u5b50\u5efa\u6a21\u548c\u751f\u6210\u91cd\u5efa\u7b49\u4e0d\u540c\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u7684\u76d1\u7763\u67b6\u6784\u6539\u8fdb\u3002", "motivation": "\u4e3a\u7c92\u5b50\u7269\u7406\u5b66\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u548c\u900f\u660e\u6bd4\u8f83\uff0c\u9694\u79bb\u5b66\u4e60\u76ee\u6807\u7684\u8d21\u732e\uff0c\u7a81\u51fa\u5404\u81ea\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u7684\u57fa\u4e8etransformer\u7684\u7c92\u5b50\u4e91\u7f16\u7801\u5668\uff0c\u91c7\u7528\u6807\u51c6\u5316\u9884\u5904\u7406\u3001\u5339\u914d\u91c7\u6837\u548c\u4e00\u81f4\u8bc4\u4f30\u534f\u8bae\uff0c\u5728\u55b7\u6ce8\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u5bf9\u6bd4\u5b66\u4e60\uff08\u76d1\u7763\u548c\u81ea\u76d1\u7763\uff09\u3001\u63a9\u7801\u7c92\u5b50\u5efa\u6a21\u548c\u751f\u6210\u91cd\u5efa\u76ee\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u76d1\u7763\u67b6\u6784\u4fee\u6539\uff0c\u5728\u57fa\u51c6\u8bc4\u4f30\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7c92\u5b50\u7269\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u53c2\u8003\u70b9\uff0c\u4f7f\u6574\u4e2a\u793e\u533a\u80fd\u591f\u5b9e\u73b0\u66f4\u900f\u660e\u548c\u7a33\u5065\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.12838", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12838", "abs": "https://arxiv.org/abs/2511.12838", "authors": ["Rongqin Chen", "Fan Mo", "Pak Lon Ip", "Shenghui Zhang", "Dan Wu", "Ye Li", "Leong Hou U"], "title": "Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency", "comment": "Accepted by AAAI 2026", "summary": "Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \\textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \\emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \\emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.", "AI": {"tldr": "Co-Sparsify\u662f\u4e00\u4e2a\u8fde\u63a5\u611f\u77e5\u7684\u7a00\u758f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c062\u8282\u70b9\u6d88\u606f\u4f20\u9012\u9650\u5236\u5728\u8fde\u901a\u7ec4\u4ef6\u4e2d\uff0c3\u8282\u70b9\u4ea4\u4e92\u9650\u5236\u5728\u53cc\u8fde\u901a\u7ec4\u4ef6\u4e2d\uff0c\u6d88\u9664\u53ef\u8bc1\u660e\u5197\u4f59\u7684\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u76842-FWL\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e2-FWL\u6d4b\u8bd5\u7684\u9ad8\u9636\u56fe\u795e\u7ecf\u7f51\u7edc(HOGNNs)\u5177\u6709\u4f18\u8d8a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u8fbeO(n\u00b3)\u3002\u73b0\u6709\u7684\u6548\u7387\u65b9\u6cd5\u901a\u5e38\u4ee5\u964d\u4f4e\u8868\u8fbe\u80fd\u529b\u4e3a\u4ee3\u4ef7\u6765\u7f13\u89e3\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "Co-Sparsify\u6846\u67b6\u7684\u5173\u952e\u6d1e\u5bdf\u662f\uff1a3\u8282\u70b9\u4ea4\u4e92\u4ec5\u5728\u53cc\u8fde\u901a\u7ec4\u4ef6\uff08\u6bcf\u4e2a\u8282\u70b9\u5bf9\u90fd\u5728\u73af\u4e0a\u7684\u6700\u5927\u5b50\u56fe\uff09\u4e2d\u5177\u6709\u8868\u8fbe\u5fc5\u8981\u6027\u3002\u5728\u8fd9\u4e9b\u7ec4\u4ef6\u4e4b\u5916\uff0c\u7ed3\u6784\u5173\u7cfb\u53ef\u4ee5\u901a\u8fc72\u8282\u70b9\u6d88\u606f\u4f20\u9012\u6216\u5168\u5c40\u8bfb\u53d6\u5b8c\u5168\u6355\u83b7\u3002", "result": "\u5728PPGN\u4e0a\uff0cCo-Sparsify\u5728\u5408\u6210\u5b50\u7ed3\u6784\u8ba1\u6570\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u51c6\u786e\u7387\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\uff08ZINC\u3001QM9\uff09\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u9ad8\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u5e76\u4e0d\u76f8\u4e92\u6392\u65a5\uff1a\u57fa\u4e8e\u539f\u7406\u7684\u3001\u62d3\u6251\u5f15\u5bfc\u7684\u7a00\u758f\u5316\u80fd\u591f\u5b9e\u73b0\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u5f3a\u5927\u4e14\u9ad8\u6548\u7684GNNs\u3002"}}
{"id": "2511.12846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12846", "abs": "https://arxiv.org/abs/2511.12846", "authors": ["Zelin Zhu", "Yancheng Huang", "Kai Yang"], "title": "RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees", "comment": null, "summary": "Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.", "AI": {"tldr": "RoS-Guard\u662f\u4e00\u79cd\u9488\u5bf9\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7ebf\u6027\u7cfb\u7edf\u7684\u9c81\u68d2\u6700\u4f18\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u5c55\u5f00\u5b9e\u73b0GPU\u52a0\u901f\uff0c\u63d0\u4f9b\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\u5e76\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u7cbe\u786e\u7684\u7cfb\u7edf\u77e5\u8bc6\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u4e0d\u5207\u5b9e\u9645\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u901a\u8fc7\u7d27\u5bc6\u677e\u5f1b\u548c\u91cd\u6784OCD\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u795e\u7ecf\u5c55\u5f00\u6280\u672f\u5b9e\u73b0GPU\u52a0\u901f\u7684\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RoS-Guard\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "conclusion": "RoS-Guard\u4e3a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u6700\u4f18\u7684\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\u548c\u9ad8\u6548\u8ba1\u7b97\u80fd\u529b\u3002"}}
{"id": "2511.12865", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12865", "abs": "https://arxiv.org/abs/2511.12865", "authors": ["Wei Xu", "Fan Yang", "Qinyuan Cui", "Zhi Chen"], "title": "An approach of deep reinforcement learning for maximizing the net present value of stochastic projects", "comment": null, "summary": "This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u968f\u673a\u6d3b\u52a8\u6301\u7eed\u65f6\u95f4\u548c\u73b0\u91d1\u6d41\u91cf\u7684\u9879\u76ee\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u53cc\u6df1\u5ea6Q\u7f51\u7edc\uff08DDQN\uff09\u65b9\u6cd5\u6700\u5927\u5316\u671f\u671b\u51c0\u73b0\u503c\uff0c\u5728\u590d\u6742\u73af\u5883\u4e0b\u4f18\u4e8e\u4f20\u7edf\u7b56\u7565\u3002", "motivation": "\u5728\u5177\u6709\u968f\u673a\u6d3b\u52a8\u6301\u7eed\u65f6\u95f4\u548c\u79bb\u6563\u60c5\u666f\u4e0b\u73b0\u91d1\u6d41\u91cf\u7684\u9879\u76ee\u4e2d\uff0c\u9700\u8981\u6ee1\u8db3\u4f18\u5148\u7ea6\u675f\u6761\u4ef6\uff0c\u76ee\u6807\u662f\u901a\u8fc7\u52a0\u901f\u73b0\u91d1\u6d41\u5165\u548c\u63a8\u8fdf\u73b0\u91d1\u6d41\u51fa\u6765\u6700\u5927\u5316\u671f\u671b\u51c0\u73b0\u503c\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u63d0\u51fa\u53cc\u6df1\u5ea6Q\u7f51\u7edc\uff08DDQN\uff09\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u7f51\u7edc\u67b6\u6784\u548c\u76ee\u6807\u7f51\u7edc\u6765\u7f13\u89e3\u52a8\u4f5c\u503c\u9ad8\u4f30\u95ee\u9898\u5e76\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u6bd4\u8f83\u5b9e\u9a8c\u663e\u793aDDQN\u5728\u5927\u578b\u6216\u9ad8\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edf\u521a\u6027\u7b56\u7565\u548c\u52a8\u6001\u7b56\u7565\uff0c\u5177\u6709\u66f4\u597d\u7684\u8ba1\u7b97\u80fd\u529b\u3001\u7b56\u7565\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u53cc\u7f51\u7edc\u67b6\u6784\u6709\u6548\u7f13\u89e3\u52a8\u4f5c\u503c\u9ad8\u4f30\uff0c\u76ee\u6807\u7f51\u7edc\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6536\u655b\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "DDQN\u4e0d\u4ec5\u5728\u590d\u6742\u9879\u76ee\u4f18\u5316\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u671f\u671b\u51c0\u73b0\u503c\uff0c\u800c\u4e14\u4e3a\u7a33\u5b9a\u6709\u6548\u7684\u7b56\u7565\u5b9e\u65bd\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\u3002"}}
{"id": "2511.12869", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12869", "abs": "https://arxiv.org/abs/2511.12869", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Zeeshan Memon", "Muhammad Ibtsaam Qadir", "Sagnik Bhattacharya", "Hassan Rizwan", "Abhiram R. Gorle", "Maahe Zehra Kazmi", "Ayesha Mohsin", "Muhammad Usman Rafique", "Zihao He", "Pulkit Mehta", "Muhammad Ali Jamshed", "John M. Cioffi"], "title": "On the Fundamental Limits of LLMs at Scale", "comment": "Submitted to TMLR 2025", "summary": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ece\u8ba1\u7b97\u7406\u8bba\u3001\u4fe1\u606f\u8bba\u548c\u7edf\u8ba1\u5b66\u4e60\u89d2\u5ea6\u5f62\u5f0f\u5316\u5206\u6790\u4e86LLM\u6269\u5c55\u7684\u4e94\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u63a8\u7406\u9000\u5316\u3001\u68c0\u7d22\u8106\u5f31\u6027\u548c\u591a\u6a21\u6001\u4e0d\u5bf9\u9f50\uff0c\u5e76\u6307\u51fa\u4e86\u8fd9\u4e9b\u9650\u5236\u7684\u7406\u8bba\u4e0a\u9650\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LLM\u6269\u5c55\u9650\u5236\u7684\u63cf\u8ff0\u591a\u505c\u7559\u5728\u7ecf\u9a8c\u5c42\u9762\uff0c\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u73b0\u8c61\u4e0e\u8ba1\u7b97\u3001\u4fe1\u606f\u548c\u5b66\u4e60\u7684\u57fa\u672c\u9650\u5236\u76f8\u8fde\u63a5\u7684\u7406\u8bba\u7efc\u5408\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aLLM\u6269\u5c55\u63d0\u4f9b\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u91c7\u7528\u8bc1\u660e\u9a71\u52a8\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u8ba1\u7b97\u7406\u8bba\uff08\u4e0d\u53ef\u8ba1\u7b97\u6027\u3001\u5bf9\u89d2\u7ebf\u8bba\u8bc1\uff09\u3001\u4fe1\u606f\u8bba\uff08\u6709\u9650\u63cf\u8ff0\u957f\u5ea6\u3001\u6837\u672c\u590d\u6742\u5ea6\uff09\u548c\u51e0\u4f55\u8ba1\u7b97\u6548\u5e94\u5206\u6790\uff0c\u7cfb\u7edf\u5f62\u5f0f\u5316LLM\u6269\u5c55\u7684\u7406\u8bba\u4e0a\u9650\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4efb\u4f55\u53ef\u8ba1\u7b97\u679a\u4e3e\u6a21\u578b\u65cf\uff0c\u5bf9\u89d2\u7ebf\u8bba\u8bc1\u4fdd\u8bc1\u5b58\u5728\u67d0\u4e9b\u8f93\u5165\u5fc5\u7136\u5931\u8d25\uff1b\u4e0d\u53ef\u5224\u5b9a\u67e5\u8be2\u5bfc\u81f4\u65e0\u9650\u5931\u8d25\u96c6\uff1b\u4fe1\u606f\u7ea6\u675f\u9650\u5236\u4e86\u53ef\u8fbe\u5230\u7684\u51c6\u786e\u5ea6\uff1b\u4e0a\u4e0b\u6587\u538b\u7f29\u5b58\u5728\u51e0\u4f55\u548c\u8ba1\u7b97\u6548\u5e94\uff1b\u57fa\u4e8e\u4f3c\u7136\u7684\u8bad\u7ec3\u504f\u5411\u6a21\u5f0f\u5b8c\u6210\u800c\u975e\u63a8\u7406\u3002", "conclusion": "LLM\u6269\u5c55\u5728\u67d0\u4e9b\u9886\u57df\u6709\u6548\uff0c\u4f46\u5728\u67d0\u4e9b\u9886\u57df\u4f1a\u9971\u548c\u751a\u81f3\u65e0\u6cd5\u8fdb\u5c55\u3002\u63d0\u51fa\u4e86\u6709\u754c\u9884\u8a00\u68c0\u7d22\u3001\u4f4d\u7f6e\u8bfe\u7a0b\u5b66\u4e60\u548c\u7a00\u758f/\u5206\u5c42\u6ce8\u610f\u529b\u7b49\u5b9e\u9645\u7f13\u89e3\u8def\u5f84\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdbLLM\u6269\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2511.12890", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12890", "abs": "https://arxiv.org/abs/2511.12890", "authors": ["Arth Sojitra", "Omer San"], "title": "Method of Manufactured Learning for Solver-free Training of Neural Operators", "comment": null, "summary": "Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5236\u9020\u5b66\u4e60\u65b9\u6cd5(MML)\uff0c\u4e00\u79cd\u4e0d\u4f9d\u8d56\u6570\u503c\u6c42\u89e3\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u6790\u6784\u9020\u7269\u7406\u4e00\u81f4\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u795e\u7ecf\u7b97\u5b50\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6602\u8d35\u6570\u503c\u6a21\u62df\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u7531\u6570\u503c\u6c42\u89e3\u5668\u751f\u6210\u7684\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u5e76\u7ea6\u675f\u4e86\u5bf9\u7269\u7406\u7cfb\u7edf\u7684\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e\u5236\u9020\u89e3\u65b9\u6cd5\uff0cMML\u901a\u8fc7\u4ece\u53d7\u63a7\u89e3\u6790\u7a7a\u95f4\u91c7\u6837\u5e73\u6ed1\u5019\u9009\u89e3\uff0c\u5e76\u901a\u8fc7\u76f4\u63a5\u5e94\u7528\u63a7\u5236\u5fae\u5206\u7b97\u5b50\u63a8\u5bfc\u76f8\u5e94\u7684\u5f3a\u5236\u573a\u6765\u751f\u6210\u6570\u636e\u96c6\u3002\u5728\u63a8\u7406\u65f6\u5c06\u5f3a\u5236\u9879\u8bbe\u4e3a\u96f6\u4ee5\u6062\u590d\u539f\u59cb\u63a7\u5236\u65b9\u7a0b\u3002", "result": "\u5728\u70ed\u4f20\u5bfc\u3001\u5e73\u6d41\u3001Burgers\u548c\u6269\u6563-\u53cd\u5e94\u65b9\u7a0b\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMML\u5b9e\u73b0\u4e86\u9ad8\u5149\u8c31\u7cbe\u5ea6\u3001\u4f4e\u6b8b\u5dee\u8bef\u5dee\u548c\u5bf9\u672a\u89c1\u6761\u4ef6\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MML\u901a\u8fc7\u5c06\u6570\u636e\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89e3\u6790\u5408\u6210\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u6c42\u89e3\u5668\u65e0\u5173\u7684\u9014\u5f84\u6765\u6784\u5efa\u7269\u7406\u57fa\u7840\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u6570\u503c\u6a21\u62df\u6216\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002"}}
{"id": "2511.12898", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12898", "abs": "https://arxiv.org/abs/2511.12898", "authors": ["Zhiqi Li", "Yuchen Sun", "Greg Turk", "Bo Zhu"], "title": "Functional Mean Flow in Hilbert Space", "comment": "29 pages, 13 figures", "summary": "We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.", "AI": {"tldr": "Functional Mean Flow (FMF) \u662f\u4e00\u79cd\u5728\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u7684\u4e00\u6b65\u751f\u6210\u6a21\u578b\uff0c\u5c06 Mean Flow \u6846\u67b6\u6269\u5c55\u5230\u51fd\u6570\u57df\uff0c\u63d0\u4f9b\u4e86\u51fd\u6570\u6d41\u5339\u914d\u7684\u7406\u8bba\u516c\u5f0f\u548c\u9ad8\u6548\u8bad\u7ec3\u91c7\u6837\u7684\u5b9e\u9645\u5b9e\u73b0\u3002", "motivation": "\u5c06\u4e00\u6b65 Mean Flow \u6846\u67b6\u6269\u5c55\u5230\u51fd\u6570\u57df\uff0c\u4e3a\u51fd\u6570\u6570\u636e\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u51fd\u6570\u6d41\u5339\u914d\u7684\u7406\u8bba\u516c\u5f0f\uff0c\u5f15\u5165\u4e86 $x_1$-\u9884\u6d4b\u53d8\u4f53\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u91c7\u6837\u7684\u5b9e\u9645\u5b9e\u73b0\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u4e00\u6b65\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u3001\u56fe\u50cf\u3001\u504f\u5fae\u5206\u65b9\u7a0b\u548c3D\u51e0\u4f55\u7b49\u591a\u79cd\u51fd\u6570\u6570\u636e\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "FMF \u662f\u4e00\u4e2a\u5728\u51fd\u6570\u57df\u4e2d\u6709\u6548\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u5177\u6709\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2511.12903", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12903", "abs": "https://arxiv.org/abs/2511.12903", "authors": ["Bo Hu", "Jose C. Principe"], "title": "Contrastive Entropy Bounds for Density and Conditional Density Decomposition", "comment": null, "summary": "This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.\n  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.\n  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u8d1d\u53f6\u65af\u9ad8\u65af\u89c6\u89d2\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u6210\u672c\u8fbe\u5230\u6982\u7387\u8fb9\u754c\uff0c\u5b66\u4e60\u8fd1\u4f3c\u4f7f\u8fb9\u754c\u7d27\u81f4\u4e14\u6210\u672c\u6700\u4f18\u7684\u5bc6\u5ea6\u6a21\u578b\uff08\u901a\u5e38\u662f\u9ad8\u65af\u6df7\u5408\u5bc6\u5ea6\uff09\u3002\u63d0\u51fa\u4e86\u57fa\u4e8eHilbert\u7a7a\u95f4\u548c\u5206\u89e3\u7684\u65b9\u6cd5\u6765\u5904\u7406\u591a\u8f93\u51fa\u7f51\u7edc\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u52a8\u7f16\u7801\u5668\u76ee\u6807\u7b49\u4ef7\u4e8e\u6700\u5927\u5316\u9ad8\u65af\u7b97\u5b50\u7684\u8ff9\uff0c\u800c\u6838\u8303\u6570\u53ef\u7528\u4e8e\u8bad\u7ec3MDNs\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u8d1d\u53f6\u65af\u9ad8\u65af\u89c6\u89d2\u7406\u89e3\u4f18\u5316\u8fc7\u7a0b\u5982\u4f55\u8fbe\u5230\u6982\u7387\u8fb9\u754c\uff0c\u5e76\u63a2\u7d22\u5728\u4e0d\u9700\u8981\u4e00\u5bf9\u4e00\u5bf9\u5e94\u5173\u7cfb\u65f6\u5982\u4f55\u6700\u5927\u5316\u6574\u4f53\u79e9\u800c\u975e\u8ff9\u3002", "method": "\u4f7f\u7528Hilbert\u7a7a\u95f4\u548c\u5206\u89e3\u65b9\u6cd5\u5904\u7406\u591a\u8f93\u51fa\u7f51\u7edc\u4ea7\u751f\u591a\u4e2a\u4e2d\u5fc3\u5b9a\u4e49\u9ad8\u65af\u6df7\u5408\u7684\u60c5\u51b5\u3002\u63d0\u51fa\u4e86\u7f16\u7801\u5668-\u6df7\u5408-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5176\u4e2d\u89e3\u7801\u5668\u662f\u591a\u8f93\u51fa\u7684\uff0c\u6bcf\u4e2a\u6837\u672c\u4ea7\u751f\u591a\u4e2a\u4e2d\u5fc3\u3002\u5229\u7528\u9ad8\u65af\u7b97\u5b50\u7684\u8ff9\u8bad\u7ec3\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u6838\u8303\u6570\u4f5c\u4e3a\u6563\u5ea6\u8bad\u7ec3MDNs\u3002", "result": "\u53d1\u73b0\u81ea\u52a8\u7f16\u7801\u5668\u7684\u76ee\u6807\u7b49\u4ef7\u4e8e\u6700\u5927\u5316\u9ad8\u65af\u7b97\u5b50\u7684\u8ff9\uff08\u7279\u5f81\u503c\u4e4b\u548c\uff09\uff0c\u800c\u6838\u8303\u6570\uff08\u5947\u5f02\u503c\u4e4b\u548c\uff09\u53ef\u7528\u4e8e\u6700\u5927\u5316\u6574\u4f53\u79e9\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u589e\u52a0\u6837\u672c\u591a\u6837\u6027\uff0c\u9632\u6b62\u7f51\u7edc\u4ea7\u751f\u76f8\u540c\u5e38\u6570\u7684\u5e73\u51e1\u89e3\u3002", "conclusion": "\u4ece\u8d1d\u53f6\u65af\u9ad8\u65af\u89c6\u89d2\u4e3a\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u9ad8\u65af\u7b97\u5b50\u7684\u8ff9\u548c\u6838\u8303\u6570\u5206\u522b\u9002\u7528\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u548cMDNs\u7684\u8bad\u7ec3\uff0c\u63d0\u51fa\u7684\u67b6\u6784\u80fd\u591f\u7d27\u81f4\u6982\u7387\u8fb9\u754c\u5e76\u5b9a\u91cf\u5206\u6790\u3002"}}
{"id": "2511.12905", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12905", "abs": "https://arxiv.org/abs/2511.12905", "authors": ["Tania-Amanda Fredrick Eneye", "Ashlesha Malla", "Pawan Paudel"], "title": "LinkedIn Profile Characteristics and Professional Success Indicators", "comment": null, "summary": "This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u67906.2\u4e07\u4e2aLinkedIn\u533f\u540d\u6863\u6848\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u804c\u4e1a\u6210\u529f\u6307\u6807\uff08\u664b\u5347\u3001\u7c89\u4e1d\u6570\u3001\u804c\u4e1a\u53d1\u5c55\u901f\u5ea6\uff09\uff0c\u53d1\u73b0\u664b\u5347\u53ef\u9ad8\u5ea6\u9884\u6d4b\uff0c\u7c89\u4e1d\u589e\u957f\u66f4\u590d\u6742\u3002", "motivation": "\u63a2\u7d22LinkedIn\u6863\u6848\u7279\u5f81\u4e0e\u804c\u4e1a\u6210\u529f\u7684\u5173\u7cfb\uff0c\u4e3a\u4e13\u4e1a\u4eba\u58eb\u4f18\u5316LinkedIn\u5c55\u793a\u548c\u804c\u4e1a\u7b56\u7565\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u5206\u6790\u8d85\u8fc76.2\u4e07\u4e2a\u533f\u540dLinkedIn\u6863\u6848\u6570\u636e\uff0c\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u8bc6\u522b\u5f71\u54cd\u804c\u4e1a\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002", "result": "\u664b\u5347\u53ef\u9ad8\u5ea6\u9884\u6d4b\uff0c\u4f46\u7c89\u4e1d\u589e\u957f\u8868\u73b0\u51fa\u66f4\u590d\u6742\u7684\u6a21\u5f0f\uff0c\u8bc6\u522b\u51fa\u9a71\u52a8\u804c\u4e1a\u6210\u529f\u7684\u6700\u5177\u5f71\u54cd\u529b\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u4e86\u4f18\u5316LinkedIn\u5b58\u5728\u548c\u804c\u4e1a\u7b56\u7565\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u804c\u4e1a\u6210\u529f\u6307\u6807\u7684\u9884\u6d4b\u96be\u5ea6\u5dee\u5f02\u3002"}}
{"id": "2511.12934", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12934", "abs": "https://arxiv.org/abs/2511.12934", "authors": ["Zhi Kou", "Xiang-Rong Sheng", "Shuguang Han", "Zhishan Zhao", "Yueyao Cheng", "Han Zhu", "Jian Xu", "Bo Zheng"], "title": "AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking", "comment": null, "summary": "In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.", "AI": {"tldr": "\u63d0\u51fa\u5f02\u6b65\u63a8\u7406\u6846\u67b6(AIF)\uff0c\u5c06\u9884\u6392\u5e8f\u6a21\u578b\u7684\u7528\u6237\u4fa7\u548c\u7269\u54c1\u4fa7\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u9884\u6d4b\u89e3\u8026\uff0c\u901a\u8fc7\u5e76\u884c\u548c\u8fd1\u7ebf\u8ba1\u7b97\u51cf\u5c11\u5197\u4f59\u548c\u5ef6\u8fdf\uff0c\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u9884\u6392\u5e8f\u6a21\u578b\u91c7\u7528\u987a\u5e8f\u6267\u884c\u6846\u67b6\uff0c\u5b58\u5728\u76f8\u540c\u7528\u6237/\u7269\u54c1\u7684\u5197\u4f59\u8ba1\u7b97\u548c\u4e25\u683c\u987a\u5e8f\u64cd\u4f5c\u5bfc\u81f4\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bb9\u91cf\u548c\u7cfb\u7edf\u6548\u7387\u3002", "method": "AIF\u6846\u67b6\u5c06\u4ea4\u4e92\u65e0\u5173\u7ec4\u4ef6(\u5355\u7528\u6237\u6216\u7269\u54c1\u5185\u64cd\u4f5c)\u4e0e\u5b9e\u65f6\u9884\u6d4b\u89e3\u8026\uff0c\u7528\u6237\u4fa7\u8ba1\u7b97\u4e0e\u68c0\u7d22\u9636\u6bb5\u5e76\u884c\u6267\u884c\uff0c\u7269\u54c1\u4fa7\u8ba1\u7b97\u4ee5\u8fd1\u7ebf\u65b9\u5f0f\u5b8c\u6210\uff0c\u4ea4\u4e92\u76f8\u5173\u7ec4\u4ef6\u91c7\u7528\u8fd1\u4f3c\u65b9\u6cd5\u5728\u7ebf\u9884\u6d4b\u3002", "result": "AIF\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u5e76\u964d\u4f4e\u5ef6\u8fdf\uff0c\u91ca\u653e\u8d44\u6e90\u663e\u8457\u6539\u5584\u4ea4\u4e92\u65e0\u5173\u7ec4\u4ef6\u7684\u7279\u5f81\u96c6\u548c\u6a21\u578b\u67b6\u6784\uff0c\u5728\u6dd8\u5b9d\u5c55\u793a\u5e7f\u544a\u7cfb\u7edf\u4e2d\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "\u901a\u8fc7\u6846\u67b6\u4e0e\u6a21\u578b\u7684\u534f\u540c\u8bbe\u8ba1\uff0cAIF\u5728\u672a\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u548c\u5ef6\u8fdf\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.12945", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12945", "abs": "https://arxiv.org/abs/2511.12945", "authors": ["Yujie Li", "Zezhi Shao", "Chengqing Yu", "Yisong Fu", "Tao Sun", "Yongjun Xu", "Fei Wang"], "title": "APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift", "comment": null, "summary": "Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.", "AI": {"tldr": "\u63d0\u51fa\u4e86Affine Prototype Timestamp (APT)\u6a21\u5757\uff0c\u901a\u8fc7\u65f6\u95f4\u6233\u6761\u4ef6\u539f\u578b\u5b66\u4e60\u52a8\u6001\u751f\u6210\u4eff\u5c04\u53c2\u6570\uff0c\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u5c40\u90e8\u7edf\u8ba1\u5f52\u4e00\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u5206\u5e03\u504f\u79fb\uff1bRevIN\u7b49\u65b9\u6cd5\u5728\u5904\u7406\u7f3a\u5931\u503c\u3001\u566a\u58f0\u89c2\u6d4b\u548c\u65e0\u6548\u901a\u9053\u4eff\u5c04\u53d8\u6362\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u53ef\u63d2\u62d4\u6a21\u5757APT\uff0c\u901a\u8fc7\u65f6\u95f4\u6233\u6761\u4ef6\u539f\u578b\u5b66\u4e60\u6ce8\u5165\u5168\u5c40\u5206\u5e03\u7279\u5f81\uff0c\u52a8\u6001\u751f\u6210\u4eff\u5c04\u53c2\u6570\u6765\u8c03\u5236\u8f93\u5165\u8f93\u51fa\u5e8f\u5217\uff0c\u4f7f\u4e3b\u5e72\u7f51\u7edc\u80fd\u4ece\u81ea\u76d1\u7763\u7684\u5206\u5e03\u611f\u77e5\u805a\u7c7b\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u79cd\u4e3b\u5e72-\u5f52\u4e00\u5316\u7ec4\u5408\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAPT\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "APT\u662f\u4e00\u4e2a\u517c\u5bb9\u4efb\u610f\u9884\u6d4b\u4e3b\u5e72\u548c\u5f52\u4e00\u5316\u7b56\u7565\u7684\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u5728\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u80fd\u6709\u6548\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.12951", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12951", "abs": "https://arxiv.org/abs/2511.12951", "authors": ["Ziling Fan", "Ruijia Liang", "Yiwen Hu"], "title": "A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series", "comment": null, "summary": "Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eFEDformer\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u98ce\u9669\u9884\u6d4b\uff0c\u901a\u8fc7\u9891\u57df\u5206\u6790\u548c\u4fe1\u53f7\u5206\u89e3\u63d0\u5347\u5bf9\u91d1\u878d\u6ce2\u52a8\u6027\u7684\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u5177\u6709\u9ad8\u5ea6\u6ce2\u52a8\u6027\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6355\u6349\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u548c\u590d\u6742\u5468\u671f\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u9891\u7387\u589e\u5f3a\u5206\u89e3Transformer\uff08FEDformer\uff09\u3001\u57fa\u4e8e\u6b8b\u5dee\u7684\u5f02\u5e38\u68c0\u6d4b\u5668\u548c\u98ce\u9669\u9884\u6d4b\u5934\uff0c\u5728\u65f6\u57df\u548c\u9891\u57df\u540c\u65f6\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff0c\u5c06\u4fe1\u53f7\u5206\u89e3\u4e3a\u8d8b\u52bf\u548c\u5b63\u8282\u5206\u91cf\u3002", "result": "\u5728S&P 500\u3001\u7eb3\u65af\u8fbe\u514b\u7efc\u5408\u6307\u6570\u548c\u5e03\u4f26\u7279\u539f\u6cb9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\uff0cRMSE\u964d\u4f4e15.7%\uff0c\u5f02\u5e38\u68c0\u6d4bF1\u5206\u6570\u63d0\u534711.5%\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u91d1\u878d\u6ce2\u52a8\u6027\uff0c\u4e3a\u5e02\u573a\u5d29\u76d8\u9884\u6d4b\u548c\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2511.12955", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12955", "abs": "https://arxiv.org/abs/2511.12955", "authors": ["Onur Vural", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series", "comment": "This work has been accepted at the 2025 IEEE International Conference on Big Data (IEEE BigData 2025) on October 23, 2025", "summary": "Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684GCTAF\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8de8\u6ce8\u610f\u529b\u5168\u5c40\u4ee4\u724c\u6765\u589e\u5f3a\u957f\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u7528\u4e8e\u89e3\u51b3\u592a\u9633\u8000\u6591\u9884\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u592a\u9633\u8000\u6591\u9884\u6d4b\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u8000\u6591\u4e8b\u4ef6\u56fa\u6709\u7684\u4e0d\u5e73\u8861\u6027\u2014\u2014\u5f3a\u70c8\u8000\u6591\u76f8\u5bf9\u7a00\u5c11\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u7684\u5b66\u4e60\u3002\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ec5\u4f9d\u8d56\u65f6\u95f4\u5e8f\u5217\u5185\u7684\u5c40\u90e8\u4ea4\u4e92\uff0c\u96be\u4ee5\u6355\u6349\u5bf9\u8000\u6591\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u7684\u5168\u5c40\u663e\u8457\u65f6\u95f4\u6a21\u5f0f\u3002", "method": "GCTAF\u67b6\u6784\u5728\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u57fa\u7840\u4e0a\u6ce8\u5165\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u8de8\u6ce8\u610f\u529b\u5168\u5c40\u4ee4\u724c\uff0c\u8fd9\u4e9b\u4ee4\u724c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u4e0e\u8f93\u5165\u5e8f\u5217\u4ea4\u4e92\uff0c\u6c47\u603b\u6574\u4e2a\u5e8f\u5217\u4e2d\u7684\u663e\u8457\u65f6\u95f4\u6a21\u5f0f\uff0c\u7136\u540e\u878d\u5408\u56de\u65f6\u95f4\u8868\u793a\u4e2d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u5bf9\u8000\u6591\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u7684\u5168\u5c40\u663e\u8457\u3001\u975e\u8fde\u7eed\u65f6\u95f4\u70b9\u3002", "result": "\u5728\u57fa\u51c6\u592a\u9633\u8000\u6591\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cGCTAF\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5f3a\u70c8\u8000\u6591\u5e76\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6539\u8fdb\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5728\u592a\u9633\u8000\u6591\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u9ad8\u6f5c\u529b\u3002", "conclusion": "GCTAF\u4f5c\u4e3a\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u65f6\u95f4\u6c47\u603b\u5668\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u6355\u6349\u4e0e\u8000\u6591\u76f8\u5173\u7684\u5224\u522b\u6027\u52a8\u6001\u7684\u80fd\u529b\uff0c\u4e3a\u592a\u9633\u8000\u6591\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.12979", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.12979", "abs": "https://arxiv.org/abs/2511.12979", "authors": ["Zhengchao Wang", "Yitao Hu", "Jianing Ye", "Zhuxuan Chang", "Jiazheng Yu", "Youpeng Deng", "Keqiu Li"], "title": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.", "AI": {"tldr": "RAGPulse\u662f\u4e00\u4e2a\u5f00\u6e90RAG\u5de5\u4f5c\u8d1f\u8f7d\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u6536\u96c6\u81ea\u670d\u52a1\u8d85\u8fc74\u4e07\u5e08\u751f\u7684\u5927\u5b66\u95ee\u7b54\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u771f\u5b9eRAG\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u663e\u8457\u65f6\u95f4\u5c40\u90e8\u6027\u548c\u9ad8\u5ea6\u504f\u659c\u7684\u70ed\u95e8\u6587\u6863\u8bbf\u95ee\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u7528LLM\u63a8\u7406\u8ddf\u8e2a\u65e0\u6cd5\u6355\u6349RAG\u7279\u6709\u7684\u52a8\u6001\u7279\u6027\uff08\u5982\u77e5\u8bc6\u4f9d\u8d56\uff09\uff0c\u5bfc\u81f4\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u4ece2024\u5e744\u6708\u8d77\u8fd0\u884c\u7684\u5927\u5b66\u95ee\u7b54\u7cfb\u7edf\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u91c7\u7528\u9690\u79c1\u4fdd\u62a4\u7684\u57fa\u4e8e\u54c8\u5e0c\u7684\u6570\u636e\u683c\u5f0f\uff0c\u5e76\u8fdb\u884c\u6df1\u5165\u7684\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\u771f\u5b9eRAG\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u663e\u8457\u65f6\u95f4\u5c40\u90e8\u6027\u548c\u9ad8\u5ea6\u504f\u659c\u7684\u70ed\u95e8\u6587\u6863\u8bbf\u95ee\u6a21\u5f0f\u3002", "conclusion": "RAGPulse\u4e3a\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u548c\u9a8c\u8bc1RAG\u7cfb\u7edf\u4f18\u5316\u7b56\u7565\uff08\u5982\u5185\u5bb9\u611f\u77e5\u6279\u5904\u7406\u548c\u68c0\u7d22\u7f13\u5b58\uff09\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u57fa\u7840\uff0c\u6700\u7ec8\u63d0\u5347RAG\u670d\u52a1\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.13010", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13010", "abs": "https://arxiv.org/abs/2511.13010", "authors": ["Jeongwhan Choi", "Seungjun Park", "Sumin Park", "Sung-Bae Cho", "Noseong Park"], "title": "Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs", "comment": "Accepted in AAAI 2026 for Oral Representation. This is the extended version including the appendix", "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5f62\u8282\u70b9\u7684\u65b0\u6982\u5ff5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u5408\u5b50\u56fe\u7ea7\u7279\u5f81\u8868\u793a\u6765\u589e\u5f3aMPNN\u7684\u957f\u7a0b\u4f9d\u8d56\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387", "motivation": "GNN\u5728\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u56feTransformer\u867d\u7136\u80fd\u5904\u7406\u957f\u7a0b\u4ea4\u4e92\u4f46\u5ffd\u7565\u4e86MPNN\u7684\u5c40\u90e8\u6027\u548c\u6548\u7387\u4f18\u52bf", "method": "\u57fa\u4e8e\u771f\u5b9e\u7f51\u7edc\u4e2d\u7684\u5206\u5f62\u7ed3\u6784\uff0c\u5f15\u5165\u5206\u5f62\u8282\u70b9\u4e0e\u539f\u59cb\u8282\u70b9\u5171\u5b58\uff0c\u901a\u8fc7\u56fe\u5212\u5206\u81ea\u7136\u8bf1\u5bfc\u5206\u5f62\u7ed3\u6784\uff0c\u81ea\u9002\u5e94\u805a\u5408\u5b50\u56fe\u7ea7\u7279\u5f81\u8868\u793a", "result": "\u5206\u5f62\u8282\u70b9\u7f13\u89e3\u4e86\u8fc7\u5ea6\u538b\u7f29\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u76f4\u63a5\u6377\u5f84\u8fde\u63a5\uff0c\u4f7f\u5b50\u56fe\u7ea7\u8868\u793a\u80fd\u591f\u957f\u7a0b\u4f20\u64ad\uff0c\u63d0\u9ad8\u4e86MPNN\u7684\u8868\u8fbe\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301MPNN\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u56feTransformer\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u6709\u6548\u6539\u5584\u4e86MPNN\u7684\u957f\u7a0b\u4f9d\u8d56\u95ee\u9898"}}
{"id": "2511.12985", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12985", "abs": "https://arxiv.org/abs/2511.12985", "authors": ["Minsoo Jo", "Dongyoon Yang", "Taesup Kim"], "title": "Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks", "comment": "Accepted by AAAI 2026", "summary": "Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \\textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2\u51e0\u4f55\u7684\u65b0\u578b\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\uff0c\u5728\u5207\u7ebf\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u68af\u5ea6\u5e76\u5206\u89e3\u4e3a\u5f84\u5411\u548c\u89d2\u5ea6\u5206\u91cf\uff0c\u4ec5\u4ece\u89d2\u5ea6\u65b9\u5411\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5982FGSM\u548cPGD\u5728\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u4e2d\u7814\u7a76\u8f83\u591a\uff0c\u4f46\u53cc\u66f2\u7f51\u7edc\u7684\u53d1\u5c55\u9700\u8981\u5728\u975e\u6b27\u51e0\u4f55\u4e2d\u91cd\u65b0\u8bc4\u4f30\u653b\u51fb\u7b56\u7565\u3002\u4f20\u7edf\u65b9\u6cd5\u4e0d\u8003\u8651\u53cc\u66f2\u7ed3\u6784\u53ef\u80fd\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u653b\u51fb\u3002", "method": "\u5728\u53cc\u66f2\u7a7a\u95f4\u7684\u5207\u7ebf\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u68af\u5ea6\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u5f84\u5411\uff08\u6df1\u5ea6\uff09\u5206\u91cf\u548c\u89d2\u5ea6\uff08\u8bed\u4e49\uff09\u5206\u91cf\uff0c\u4ec5\u4f7f\u7528\u89d2\u5ea6\u65b9\u5411\u7684\u6270\u52a8\u6765\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u805a\u7126\u4e8e\u53cc\u66f2\u51e0\u4f55\u4e2d\u7f16\u7801\u7684\u8bed\u4e49\u654f\u611f\u65b9\u5411\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u548c\u7f51\u7edc\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u653b\u51fb\u65b9\u6cd5\u6bd4\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u83b7\u5f97\u66f4\u9ad8\u7684\u6b3a\u9a97\u7387\uff0c\u540c\u65f6\u4ea7\u751f\u5177\u6709\u9ad8\u5f71\u54cd\u529b\u7684\u6270\u52a8\uff0c\u66f4\u6df1\u5165\u5730\u63ed\u793a\u4e86\u53cc\u66f2\u5d4c\u5165\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5728\u5f2f\u66f2\u8868\u793a\u7a7a\u95f4\u4e2d\u51e0\u4f55\u611f\u77e5\u5bf9\u6297\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u653b\u51fb\u5c42\u6b21\u5d4c\u5165\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2511.13023", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13023", "abs": "https://arxiv.org/abs/2511.13023", "authors": ["Jiacheng Wang", "Yejun Zeng", "Jinyang Guo", "Yuqing Ma", "Aishan Liu", "Xianglong Liu"], "title": "SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment", "comment": null, "summary": "Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.", "AI": {"tldr": "SLMQuant\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30LLM\u538b\u7f29\u6280\u672f\u5728SLMs\u4e0a\u5e94\u7528\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86SLMs\u4e0eLLMs\u5728\u91cf\u5316\u654f\u611f\u6027\u4e0a\u7684\u6839\u672c\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9SLMs\u7684\u538b\u7f29\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5c3d\u7ba1\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\u4f5c\u4e3a\u8d44\u6e90\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u6a21\u578b\u538b\u7f29\u65b9\u9762\u5b58\u5728\u672a\u89e3\u51b3\u7684\u6548\u7387\u5dee\u8ddd\u3002\u91cf\u5316\u5bf9LLMs\u6709\u6548\uff0c\u4f46\u5bf9SLMs\u7684\u9002\u7528\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8de8\u591a\u79cd\u67b6\u6784\u548c\u4efb\u52a1\u7684\u7efc\u5408\u591a\u8f68\u8bc4\u4f30\uff0c\u5206\u6790\u6700\u5148\u8fdb\u7684\u91cf\u5316\u65b9\u6cd5\u5728SLMs\u4e0a\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u5f71\u54cdSLM\u91cf\u5316\u7684\u5173\u952e\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0SLMs\u4e0eLLMs\u5728\u91cf\u5316\u654f\u611f\u6027\u4e0a\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u76f4\u63a5\u79fb\u690dLLM\u4f18\u5316\u6280\u672f\u4f1a\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\uff0c\u56e0\u4e3aSLMs\u5177\u6709\u72ec\u7279\u7684\u67b6\u6784\u7279\u5f81\u548c\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "SLMQuant\u4e3a\u5728\u8fb9\u7f18\u5e94\u7528\u4e2d\u63a8\u8fdb\u9ad8\u6548SLM\u90e8\u7f72\u5efa\u7acb\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u5e76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u90e8\u7f72\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2511.13035", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13035", "abs": "https://arxiv.org/abs/2511.13035", "authors": ["Zeyuan Wang", "Da Li", "Yulin Chen", "Ye Shi", "Liang Bai", "Tianyuan Yu", "Yanwei Fu"], "title": "One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow", "comment": "Accepted in AAAI 2026 Poster", "summary": "We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e00\u6b65\u751f\u6210\u7b56\u7565\uff0c\u901a\u8fc7MeanFlow\u7684\u6b8b\u5dee\u91cd\u6784\u5b9e\u73b0\u566a\u58f0\u5230\u52a8\u4f5c\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u517c\u5bb9Q\u5b66\u4e60\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u548c\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u4e00\u6b65\u9ad8\u65af\u7b56\u7565\u63a8\u7406\u5feb\u4f46\u96be\u4ee5\u6355\u6349\u590d\u6742\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u800c\u57fa\u4e8e\u6d41\u7684\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u4f46\u901a\u5e38\u9700\u8981\u84b8\u998f\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u9ad8\u6548\u63a8\u7406\u53c8\u80fd\u8868\u8fbe\u591a\u6a21\u6001\u5206\u5e03\u7684\u5355\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91cd\u6784MeanFlow\uff0c\u5c06\u901f\u5ea6\u573a\u548c\u566a\u58f0\u5230\u52a8\u4f5c\u53d8\u6362\u96c6\u6210\u5230\u5355\u4e00\u7b56\u7565\u7f51\u7edc\u4e2d\uff0c\u63d0\u51fa\u6709\u6548\u7684\u6b8b\u5dee\u91cd\u6784\u5f62\u5f0f\uff0c\u652f\u6301\u76f4\u63a5\u566a\u58f0\u5230\u52a8\u4f5c\u751f\u6210\uff0c\u5b9e\u73b0\u5355\u9636\u6bb5Q\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728OGBench\u548cD4RL\u57fa\u51c6\u768473\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u79bb\u7ebf\u548c\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5747\u53d6\u5f97\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u4e00\u6b65\u9ad8\u6548\u63a8\u7406\u3001\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u5efa\u6a21\u80fd\u529b\u548c\u5355\u9636\u6bb5\u7a33\u5b9a\u8bad\u7ec3\u7684\u4f18\u52bf\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13016", "abs": "https://arxiv.org/abs/2511.13016", "authors": ["Subramanyam Sahoo"], "title": "The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training", "comment": "Paper accepted to the 2nd Workshop on Aligning Reinforcement Learning Experimentalists and Theorists (ARLET 2025) at NeurIPS; the paper consists of 14 pages (including the appendix) and contains 3 figures", "summary": "Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u7814\u7a76\u786c\u5956\u52b1\u3001\u8fde\u7eed\u5956\u52b1\u548c\u6df7\u5408\u5956\u52b1\u7ed3\u6784\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6df7\u5408\u5956\u52b1\u8c03\u5ea6\u5668\u5e73\u8861\u63a2\u7d22\u548c\u7a33\u5b9a\u6027", "motivation": "\u5956\u52b1\u8bbe\u8ba1\u662fRLHF\u548c\u5bf9\u9f50\u7814\u7a76\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u5956\u52b1\u7ed3\u6784\u5bf9LLM\u5fae\u8c03\u7684\u5f71\u54cd", "method": "\u4f7f\u7528Qwen3-4B\u6a21\u578b\u548cLoRA\u5fae\u8c03\u5728GSM8K\u6570\u636e\u96c6\u4e0a\uff0c\u8bc4\u4f30\u5305\u542b\u6b63\u786e\u6027\u3001\u56f0\u60d1\u5ea6\u3001\u63a8\u7406\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u7684\u5956\u52b1\u516c\u5f0f\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u6df7\u5408\u5956\u52b1\u8c03\u5ea6\u5668", "result": "\u6df7\u5408\u5956\u52b1\u7ed3\u6784\u76f8\u6bd4\u7eaf\u786c\u5956\u52b1\u6216\u8fde\u7eed\u5956\u52b1\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027", "conclusion": "\u901a\u8fc7\u81ea\u9002\u5e94\u5956\u52b1\u5efa\u6a21\u4e3a\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6df7\u5408\u5956\u52b1\u7ed3\u6784\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18"}}
{"id": "2511.13052", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13052", "abs": "https://arxiv.org/abs/2511.13052", "authors": ["Yunhun Nam", "Jaehyung Kim", "Jongheon Jeong"], "title": "Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting", "comment": "17 pages; AAAI 2026; Code is available at https://github.com/yunpal/LfU", "summary": "Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to \"undesirable\" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.", "AI": {"tldr": "\u63d0\u51faLfU\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316SFT\u8fc7\u7a0b\u6765\u7f13\u89e3\u8bed\u8a00\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u4e0b\u5fae\u8c03\u65f6\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5229\u7528\u4e0d\u826f\u6a21\u578b\u66f4\u65b0\u7684\u8868\u793a\u5bf9\u9f50\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfSFT\u5728\u6709\u9650\u6570\u636e\u4e0b\u5bb9\u6613\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u8fc7\u62df\u5408\uff0c\u4f7f\u5176\u4f9d\u8d56\u865a\u5047\u6a21\u5f0f\u6216\u635f\u5bb3\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u901a\u7528\u80fd\u529b\u3002", "method": "\u63d0\u51faLfU\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u6b63\u5219\u5316\u76f4\u63a5\u5bf9\u9f50\u6a21\u578b\u5185\u90e8\u8868\u793a\u4e0e\u7ecf\u8fc7\u4e0d\u826f\u66f4\u65b0\u540e\u7684\u8868\u793a\uff0c\u5229\u7528\u8868\u793a\u7ea7\u6570\u636e\u589e\u5f3a\u6765\u63d0\u5347\u6cdb\u5316\u3002", "result": "\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u6bd4\u4f20\u7edfSFT\u5e73\u5747\u63d0\u534716.8%\uff0c\u4e14\u5bf9\u63d0\u793a\u53d8\u5316\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u8f93\u51fa\u6027\u80fd\u6807\u51c6\u5dee\u964d\u4f4e92.1%\u3002", "conclusion": "LfU\u4f5c\u4e3a\u6709\u6548\u5148\u9a8c\uff0c\u5728\u63d0\u5347\u9002\u5e94\u6027\u7684\u540c\u65f6\u80fd\u4fdd\u6301\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2511.13018", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13018", "abs": "https://arxiv.org/abs/2511.13018", "authors": ["Sairam S", "Sara Girdhar", "Shivam Soni"], "title": "The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference", "comment": "15 pages, 4 figures", "summary": "The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic \"representation bottleneck\": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent \"nuisance bottleneck,\" linking it to GNN over-squashing via a targeted \"Hub-Periphery Trade-off\" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical \"final-stage bottleneck.\"", "AI": {"tldr": "R-Learner\u5728\u56fe\u4e0a\u5e94\u7528\u65f6\u5b58\u5728\u4e25\u91cd\u7684\"\u8868\u793a\u74f6\u9888\"\u95ee\u9898\uff0c\u56fe\u65e0\u5173\u7684\u6700\u7ec8\u9636\u6bb5\u6a21\u578b\u4f1a\u5bfc\u81f4\u5b8c\u5168\u5931\u8d25\uff0c\u800c\u7aef\u5230\u7aef\u7684Graph R-Learner\u80fd\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "R-Learner\u5728\u5904\u7406\u7f51\u7edc\u6570\u636e\u65f6\u9762\u4e34\u6838\u5fc3\u5047\u8bbe\u6311\u6218\uff0c\u5176\u6700\u7ec8\u9636\u6bb5\u6a21\u578b\u9700\u8981\u9002\u5e94\u56fe\u4f9d\u8d56\u7684\u56e0\u679c\u5f02\u8d28\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u7cfb\u7edf\u5206\u6790R-Learner\u5728\u56fe\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u7aef\u5230\u7aef\u7684Graph R-Learner\uff0c\u5e76\u8fdb\u884c\"Hub-Periphery Trade-off\"\u5206\u6790\u6765\u63ed\u793aGNN\u8fc7\u538b\u7f29\u95ee\u9898\u3002", "result": "\u7edf\u8ba1\u663e\u8457\u8bc1\u660e(p<0.001)\u56fe\u65e0\u5173\u6700\u7ec8\u9636\u6bb5\u7684R-Learner\u5b8c\u5168\u5931\u8d25(MSE>4.0)\uff0c\u800cGraph R-Learner\u663e\u8457\u4f18\u4e8e\u975eDML GNN T-Learner\u57fa\u7ebf\u3002\u53d1\u73b0\u62d3\u6251\u4f9d\u8d56\u7684\"\u5e72\u6270\u74f6\u9888\"\u4e0eGNN\u8fc7\u538b\u7f29\u76f8\u5173\u3002", "conclusion": "R-Learner\u6027\u80fd\u4e3b\u8981\u53d7\u6700\u7ec8\u9636\u6bb5CATE\u4f30\u8ba1\u5668\u7684\u5f52\u7eb3\u504f\u5dee\u9a71\u52a8\uff0c\u800c\u975e\u5e72\u6270\u6a21\u578b\u9009\u62e9\u3002\u63ed\u793a\u4e86\u5173\u952e\u7684\"\u6700\u7ec8\u9636\u6bb5\u74f6\u9888\"\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u57fa\u51c6\u3002"}}
{"id": "2511.13060", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13060", "abs": "https://arxiv.org/abs/2511.13060", "authors": ["Duo Yi"], "title": "Latency and Ordering Effects in Online Decisions", "comment": null, "summary": "Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_\u03a6$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \\ge L_{\\mathrm{ideal}} + g_1(\u03bb) + g_2(\\varepsilon_\\star) + g_{12}(\u03bb,\\varepsilon_\\star) - D_{\\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\\mathrm{ncx}}\\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5ef6\u8fdf\u53cd\u9988\u548c\u987a\u5e8f\u654f\u611f\u52a8\u6001\u7684\u5728\u7ebf\u51b3\u7b56\u7cfb\u7edf\u7684\u57fa\u51c6\u635f\u5931\u4e0b\u754c\u5206\u6790\u6846\u67b6\uff0c\u5c06\u5ef6\u8fdf\u3001\u987a\u5e8f\u654f\u611f\u6027\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u91cf\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u60e9\u7f5a\u9879\u3002", "motivation": "\u5728\u7ebf\u51b3\u7b56\u7cfb\u7edf\u7ecf\u5e38\u9762\u4e34\u5ef6\u8fdf\u53cd\u9988\u548c\u987a\u5e8f\u654f\u611f\u52a8\u6001\u7684\u6311\u6218\uff0c\u5176\u4e2d\u52a8\u4f5c\u4f1a\u5f71\u54cd\u89c2\u5bdf\u7ed3\u679c\u7684\u5230\u8fbe\u987a\u5e8f\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u590d\u6742\u6548\u5e94\u3002", "method": "\u4f7f\u7528Bregman\u6563\u5ea6\u4f5c\u4e3a\u635f\u5931\u57fa\u51c6\uff0c\u8bc1\u660e\u8d85\u989d\u57fa\u51c6\u635f\u5931\u5b58\u5728\u7ed3\u6784\u5316\u4e0b\u754c\uff0c\u5305\u542b\u5ef6\u8fdf\u60e9\u7f5a\u3001\u987a\u5e8f\u654f\u611f\u6027\u60e9\u7f5a\u3001\u51e0\u4f55\u4ea4\u4e92\u60e9\u7f5a\u548c\u975e\u51f8\u6027\u60e9\u7f5a\u3002\u6269\u5c55\u5230\u8fd1\u6b63\u5219\u548c\u5f31\u51f8\u8bbe\u7f6e\uff0c\u5e76\u63d0\u4f9b\u901a\u8fc72\u00d72\u968f\u673a\u5b9e\u9a8c\u548c\u6d41\u5f0f\u8bca\u65ad\u4f30\u8ba1\u8fd9\u4e9b\u9879\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c06\u5f02\u6784\u5ef6\u8fdf\u3001\u975e\u4ea4\u6362\u6027\u548c\u5b9e\u73b0\u5dee\u8ddd\u6548\u5e94\u6253\u5305\u6210\u5355\u4e2a\u53ef\u89e3\u91ca\u7684\u4e0b\u754c\u9648\u8ff0\uff0c\u53ef\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u548c\u8c03\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u7ebf\u51b3\u7b56\u7cfb\u7edf\u5728\u5ef6\u8fdf\u53cd\u9988\u548c\u987a\u5e8f\u654f\u611f\u52a8\u6001\u4e0b\u7684\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u51f8\u4f18\u5316\u65b9\u6cd5\u7684\u9650\u5236\u3002"}}
{"id": "2511.13022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13022", "abs": "https://arxiv.org/abs/2511.13022", "authors": ["Eshani Patel", "Yisong Yue", "Geeling Chau"], "title": "Learning Time-Scale Invariant Population-Level Neural Representations", "comment": "10 pages, 5 figures, NeurIPS 2025 Foundation Models for the Brain and Body", "summary": "General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65f6\u95f4\u5c3a\u5ea6\u589e\u5f3a\u9884\u8bad\u7ec3(TSAP)\u65b9\u6cd5\uff0c\u89e3\u51b3\u795e\u7ecf\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5bf9\u9884\u5904\u7406\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u5339\u914d\u7684\u654f\u611f\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u9884\u5904\u7406\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u5339\u914d\u65f6\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u5c3a\u5ea6\u7684\u4e0d\u53d8\u6027\u8868\u793a\u3002", "method": "\u5f15\u5165\u65f6\u95f4\u5c3a\u5ea6\u589e\u5f3a\u9884\u8bad\u7ec3(TSAP)\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u52a0\u5165\u65f6\u95f4\u5c3a\u5ea6\u589e\u5f3a\u6765\u6784\u5efa\u5bf9\u65f6\u95f4\u5c3a\u5ea6\u7684\u4e0d\u53d8\u6027\u8868\u793a\u3002", "result": "TSAP\u65b9\u6cd5\u5728\u4e0d\u540c\u89e3\u7801\u4efb\u52a1\u4e2d\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u5bf9\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u6784\u5efa\u4e86\u4e0d\u53d8\u6027\u3002", "conclusion": "\u5904\u7406\u9884\u5904\u7406\u591a\u6837\u6027\u662f\u6784\u5efa\u53ef\u6cdb\u5316\u795e\u7ecf\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\uff0cTSAP\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13061", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13061", "abs": "https://arxiv.org/abs/2511.13061", "authors": ["Vladim\u00edr Macko", "Vladim\u00edr Bo\u017ea"], "title": "MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity", "comment": "8 pages + 7 pages appendix, 11 figures, Code available at https://github.com/vlejd/macko_spmv", "summary": "Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.", "AI": {"tldr": "MACKO-SpMV\u662f\u4e00\u79cdGPU\u4f18\u5316\u7684\u7a00\u758f\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u683c\u5f0f\u548c\u5185\u6838\uff0c\u4e13\u95e8\u4e3a\u5904\u7406\u5927\u8bed\u8a00\u6a21\u578b\u4e2d30-90%\u975e\u7ed3\u6784\u5316\u7a00\u758f\u5ea6\u800c\u8bbe\u8ba1\uff0c\u572850%\u7a00\u758f\u5ea6\u4e0b\u5b9e\u73b01.5\u500d\u5185\u5b58\u51cf\u5c11\u548c1.2-1.5\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709SpMV\u65b9\u6cd5\u5728\u5904\u7406\u5927\u8bed\u8a00\u6a21\u578b\u526a\u679d\u540e\u5e38\u89c1\u7684\u4f4e\u4e14\u975e\u7ed3\u6784\u5316\u7a00\u758f\u5ea6\uff0830-90%\uff09\u65f6\u6027\u80fd\u4e0d\u4f73\uff0c\u5bfc\u81f4\u975e\u7ed3\u6784\u5316\u526a\u679d\u53ea\u80fd\u63d0\u4f9b\u6709\u9650\u7684\u5185\u5b58\u51cf\u5c11\u548c\u52a0\u901f\u6548\u679c\u3002", "method": "\u63d0\u51faMACKO-SpMV\uff0c\u4e00\u79cd\u4e0eGPU\u6267\u884c\u6a21\u578b\u517c\u5bb9\u7684GPU\u4f18\u5316\u683c\u5f0f\u548c\u5185\u6838\u534f\u540c\u8bbe\u8ba1\uff0c\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\uff0c\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u5355\u5143\u6216\u683c\u5f0f\u7279\u5b9a\u7684\u9884\u8ba1\u7b97\u3002", "result": "\u572850%\u7a00\u758f\u5ea6\u4e0b\uff0cMACKO\u9996\u6b21\u5b9e\u73b0\u663e\u8457\u7684\u5185\u5b58\u51cf\u5c11\uff081.5\u500d\uff09\u548c\u52a0\u901f\uff081.2-1.5\u500d\uff09\uff0c\u76f8\u6bd4\u5176\u4ed6SpMV\u57fa\u7ebf\uff1acuSPARSE\u5feb2.8-13.0\u500d\uff0cSputnik\u5feb1.9-2.6\u500d\uff0cDASP\u5feb2.2-2.5\u500d\u3002\u5e94\u7528\u4e8eLlama2-7B\u6a21\u578b\u65f6\uff0c\u5728fp16\u7cbe\u5ea6\u4e0b\u5b9e\u73b01.5\u500d\u5185\u5b58\u51cf\u5c11\u548c1.5\u500d\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "MACKO\u4f7f\u5f97\u572850%\u7a00\u758f\u5ea6\u4e0b\u7684\u975e\u7ed3\u6784\u5316\u526a\u679d\u5728\u5b9e\u9645\u5927\u8bed\u8a00\u6a21\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u53d8\u5f97\u5408\u7406\u53ef\u884c\u3002"}}
{"id": "2511.13062", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13062", "abs": "https://arxiv.org/abs/2511.13062", "authors": ["Mohit Meena", "Yash Punjabi", "Abhishek A", "Vishal Sharma", "Mahesh Chandran"], "title": "Self-Adaptive Graph Mixture of Models", "comment": "17 pages, 5 figures", "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.", "AI": {"tldr": "\u63d0\u51faSAGMM\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u548c\u7ec4\u5408\u591a\u79cdGNN\u67b6\u6784\u6765\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9009\u62e9\u96be\u9898\uff0c\u5728\u591a\u4e2a\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524dGNN\u6027\u80fd\u589e\u957f\u8d8b\u4e8e\u5e73\u7f13\uff0c\u590d\u6742\u6a21\u578b\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff0c\u4e14\u4e3a\u7279\u5b9a\u56fe\u4efb\u52a1\u9009\u62e9\u5408\u9002\u6a21\u578b\u5b58\u5728\u56f0\u96be\u3002", "method": "SAGMM\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5229\u7528\u67b6\u6784\u591a\u6837\u6027\u548c\u62d3\u6251\u611f\u77e5\u6ce8\u610f\u529b\u95e8\u63a7\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u81ea\u9002\u5e94\u5206\u914d\u4e13\u5bb6\u6a21\u578b\uff0c\u5e76\u5305\u542b\u526a\u679d\u673a\u5236\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u572816\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSAGMM\u5728\u8282\u70b9\u5206\u7c7b\u3001\u56fe\u5206\u7c7b\u3001\u56de\u5f52\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6216\u5339\u914d\u9886\u5148\u7684GNN\u57fa\u7ebf\u548c\u73b0\u6709\u6df7\u5408\u65b9\u6cd5\u3002", "conclusion": "SAGMM\u4e3a\u73b0\u5b9e\u4e16\u754c\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u52a8\u9009\u62e9\u6700\u9002\u5408\u7684\u6a21\u578b\u7ec4\u5408\u3002"}}
{"id": "2511.13116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13116", "abs": "https://arxiv.org/abs/2511.13116", "authors": ["Qipeng Song", "Nan Yang", "Ziqi Xu", "Yue Li", "Wei Shao", "Feng Xia"], "title": "Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning", "comment": null, "summary": "Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.", "AI": {"tldr": "GFOES\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u9057\u5fd8\u7684\u6846\u67b6\uff0c\u5728\u4ec5\u6709\u5c11\u91cf\u4fdd\u7559\u6570\u636e\u4e14\u65e0\u6cd5\u8bbf\u95ee\u9057\u5fd8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u751f\u6210\u6700\u4f18\u64e6\u9664\u6837\u672c\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u6765\u5b9e\u73b0\u6709\u6548\u7684\u7c7b\u522b\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8bbf\u95ee\u5b8c\u6574\u7684\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u6587\u9488\u5bf9\u66f4\u73b0\u5b9e\u4f46\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\uff1afew-shot zero-glance\u8bbe\u7f6e\uff0c\u5373\u53ea\u6709\u5c11\u91cf\u4fdd\u7559\u6570\u636e\u53ef\u7528\u4e14\u9057\u5fd8\u6570\u636e\u5b8c\u5168\u65e0\u6cd5\u8bbf\u95ee\u3002", "method": "\u63d0\u51faGFOES\u6846\u67b6\uff0c\u5305\u542b\u751f\u6210\u53cd\u9988\u7f51\u7edc(GFN)\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u8fc7\u7a0b\u3002GFN\u5408\u6210\u6700\u4f18\u64e6\u9664\u6837\u672c(OES)\uff0c\u8fd9\u4e9b\u6837\u672c\u5728\u76ee\u6807\u7c7b\u522b\u4e0a\u4ea7\u751f\u9ad8\u635f\u5931\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9057\u5fd8\u7c7b\u522b\u7279\u5b9a\u77e5\u8bc6\u800c\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u9057\u5fd8\u6570\u636e\u3002\u4e24\u9636\u6bb5\u5fae\u8c03\u5305\u62ec\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u6fc0\u8fdb\u9057\u5fd8\uff0c\u7b2c\u4e8c\u9636\u6bb5\u6062\u590d\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGFOES\u5728logit\u548c\u8868\u793a\u5c42\u9762\u90fd\u80fd\u5b9e\u73b0\u6709\u6548\u9057\u5fd8\uff0c\u540c\u65f6\u4ec5\u4f7f\u75285%\u7684\u539f\u59cb\u6570\u636e\u5c31\u80fd\u4fdd\u6301\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "GFOES\u4e3a\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13044", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13044", "abs": "https://arxiv.org/abs/2511.13044", "authors": ["Rosario Napoli", "Giovanni Lonia", "Antonio Celesti", "Massimo Villari", "Maria Fazio"], "title": "Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data", "comment": "Accepted at the 14th International Joint Conference on Knowledge Graphs (IJCKG) 2025", "summary": "Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.", "AI": {"tldr": "Bi-View\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408Node2Vec\u548cGraphSAGE\u4e24\u79cd\u56fe\u5d4c\u5165\u6280\u672f\u6765\u589e\u5f3a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u8282\u70b9\u7279\u5f81\u7684\u4fe1\u606f\u5185\u5bb9\uff0c\u751f\u6210\u6539\u8fdb\u7684\u56fe\u5d4c\u5165\uff0c\u4ece\u800c\u63d0\u5347\u56fe\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u989d\u5916\u7684\u5408\u6210\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u624d\u80fd\u8868\u73b0\u826f\u597d\uff0c\u9650\u5236\u4e86\u5728\u7a00\u758f\u6216\u4e0d\u5b8c\u6574\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002\u77e5\u8bc6\u56fe\u8c31\u7531\u4e8e\u5176\u8bed\u4e49\u6027\u8d28\u53ef\u80fd\u9690\u85cf\u5927\u91cf\u4fe1\u606f\uff0c\u73b0\u6709\u56fe\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u77e5\u8bc6\u56fe\u8c31\u65f6\u9762\u4e34\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u7684\u56fe\u5d4c\u5165\u6280\u672f\uff1aNode2Vec\uff08\u901a\u8fc7\u65e0\u76d1\u7763\u968f\u673a\u6e38\u8d70\u6355\u83b7\u7ed3\u6784\u6a21\u5f0f\uff09\u548cGraphSAGE\uff08\u4ee5\u76d1\u7763\u65b9\u5f0f\u805a\u5408\u90bb\u57df\u4fe1\u606f\uff09\u3002\u9996\u5148\u8ba1\u7b97Node2Vec\u5d4c\u5165\u8868\u793a\u56fe\u62d3\u6251\uff0c\u7136\u540e\u7528\u57fa\u4e8e\u4e2d\u5fc3\u6027\u7684\u6307\u6807\u4e30\u5bcc\u8282\u70b9\u7279\u5f81\u4f5c\u4e3aGraphSAGE\u8f93\u5165\uff0c\u6700\u540e\u901a\u8fc7\u878d\u5408\u5c42\u7ed3\u5408\u539f\u59cbNode2Vec\u5d4c\u5165\u548cGraphSAGE\u5f71\u54cd\u7684\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u521d\u59cb\u7279\u5f81\u8f83\u5dee\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u66f4\u51c6\u786e\u548c\u7cbe\u786e\u7684\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u56fe\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "Bi-View\u65b9\u6cd5\u80fd\u591f\u6355\u83b7\u56fe\u7684\u62d3\u6251\u548c\u8bed\u4e49\u7279\u6027\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u4f46\u672a\u660e\u786e\u8868\u793a\u7684\u4fe1\u606f\u7279\u5f81\uff0c\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u5408\u6210\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u56fe\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.13133", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13133", "abs": "https://arxiv.org/abs/2511.13133", "authors": ["Shudong Wang", "Xinfei Wang", "Chenhao Zhang", "Shanchen Pang", "Haiyuan Gui", "Wenhao Ji", "Xiaojian Liao"], "title": "Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning", "comment": null, "summary": "Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.\n  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.", "AI": {"tldr": "SoCo-DT\u662f\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u91cd\u8981\u6027\u7684\u8f6f\u51b2\u7a81\u89e3\u51b3\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a9\u7801\u503c\u548c\u81ea\u9002\u5e94\u7a00\u758f\u5ea6\u7b56\u7565\u6765\u7f13\u89e3\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u63a9\u7801\u7684\u65b9\u6cd5\u4f7f\u7528\u7c97\u7c92\u5ea6\u4e8c\u5143\u63a9\u7801\u4f1a\u8fc7\u5ea6\u6291\u5236\u5173\u952e\u51b2\u7a81\u53c2\u6570\uff0c\u963b\u788d\u4efb\u52a1\u95f4\u77e5\u8bc6\u5171\u4eab\uff0c\u4e14\u91c7\u7528\u56fa\u5b9a\u7a00\u758f\u5ea6\u7b56\u7565\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7684\u51b2\u7a81\u7a0b\u5ea6\u5dee\u5f02\u3002", "method": "\u5229\u7528Fisher\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u63a9\u7801\u503c\uff0c\u4fdd\u7559\u91cd\u8981\u53c2\u6570\u540c\u65f6\u6291\u5236\u51b2\u7a81\u53c2\u6570\uff1b\u5f15\u5165\u57fa\u4e8e\u56db\u5206\u4f4d\u8ddd\u7684\u52a8\u6001\u7a00\u758f\u5ea6\u8c03\u6574\u7b56\u7565\uff0c\u6784\u5efa\u4efb\u52a1\u7279\u5b9a\u7684\u9608\u503c\u65b9\u6848\uff1b\u91c7\u7528\u975e\u5bf9\u79f0\u4f59\u5f26\u9000\u706b\u8c03\u5ea6\u6301\u7eed\u66f4\u65b0\u9608\u503c\u3002", "result": "\u5728Meta-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSoCo-DT\u5728MT50\u4e0a\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u53477.6%\uff0c\u5728\u6b21\u4f18\u6570\u636e\u96c6\u4e0a\u63d0\u534710.5%\u3002", "conclusion": "SoCo-DT\u80fd\u6709\u6548\u7f13\u89e3\u68af\u5ea6\u51b2\u7a81\uff0c\u63d0\u9ad8\u591a\u4efb\u52a1\u6574\u4f53\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13198", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13198", "abs": "https://arxiv.org/abs/2511.13198", "authors": ["Zhixin Ou", "Peng Liang", "Jianchen Han", "Baihui Liu", "Linbo Qiao"], "title": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer", "comment": null, "summary": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.", "AI": {"tldr": "ParaDySe\u662f\u4e00\u4e2a\u7528\u4e8e\u52a8\u6001\u5e8f\u5217\u7684\u81ea\u9002\u5e94\u5e76\u884c\u7b56\u7565\u5207\u6362\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u77ed\u5e8f\u5217\u901a\u4fe1\u5e76\u884c\u5316\u53d6\u6d88\u548c\u957f\u5e8f\u5217\u5185\u5b58\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u6846\u67b6\u5bf9\u52a8\u6001\u957f\u5ea6\u5e8f\u5217\u91c7\u7528\u9884\u5b9a\u4e49\u7684\u9759\u6001\u5e76\u884c\u7b56\u7565\uff0c\u5bfc\u81f4\u77ed\u5e8f\u5217\u51fa\u73b0\u901a\u4fe1-\u5e76\u884c\u5316\u53d6\u6d88\u95ee\u9898\uff0c\u957f\u5e8f\u5217\u51fa\u73b0\u5185\u5b58\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u5b9e\u73b0\u5e76\u884c\u7b56\u7565\u7684\u6a21\u5757\u5316\u51fd\u6570\u5e93\uff0c\u6784\u5efa\u5e8f\u5217\u611f\u77e5\u7684\u5185\u5b58\u548c\u65f6\u95f4\u6210\u672c\u6a21\u578b\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u7b97\u6cd5\u4e3a\u52a8\u6001\u5e8f\u5217\u9009\u62e9\u6700\u4f18\u5c42\u95f4\u7b56\u7565\u3002", "result": "\u5728\u5e8f\u5217\u957f\u5ea6\u8fbe624K\u7684\u6570\u636e\u96c6\u4e0a\uff0cParaDySe\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u4e0d\u8db3\u548c\u901a\u4fe1\u5e76\u884c\u5316\u53d6\u6d88\u74f6\u9888\u3002", "conclusion": "ParaDySe\u901a\u8fc7\u7cfb\u7edf\u96c6\u6210\u957f\u5e8f\u5217\u4f18\u5316\u4e0e\u73b0\u6709\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7b56\u7565\u7684\u65e0\u7f1d\u70ed\u5207\u6362\u3002"}}
{"id": "2511.13053", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.13053", "abs": "https://arxiv.org/abs/2511.13053", "authors": ["Akira Tamamori"], "title": "Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks", "comment": "4 pages, 3 figures", "summary": "Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.", "AI": {"tldr": "\u901a\u8fc7\u51e0\u4f55\u5206\u6790Hopfield\u7f51\u7edc\u7684\u80fd\u91cf\u666f\u89c2\uff0c\u53d1\u73b0\u5b58\u5728\"\u4f18\u5316\u810a\"\u73b0\u8c61\uff0c\u5728\u9ad8\u8d1f\u8f7d\u548c\u5168\u5c40\u6838\u6761\u4ef6\u4e0b\u7f51\u7edc\u80fd\u6700\u5927\u5316\u5438\u5f15\u5b50\u7a33\u5b9a\u6027\uff0c\u63ed\u793a\u4e86\u76f4\u63a5\u9a71\u52a8\u529b\u4e0e\u53cd\u9988\u529b\u4e4b\u95f4\u7684\u53cd\u76f8\u5173\u6027\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u57fa\u4e8e\u6838\u7684\u5b66\u4e60\u65b9\u6cd5\u5982\u4f55\u663e\u8457\u63d0\u5347Hopfield\u7f51\u7edc\u5b58\u50a8\u5bb9\u91cf\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u8fd9\u4e00\u589e\u5f3a\u80cc\u540e\u7684\u7269\u7406\u539f\u7406\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165\"\u5cf0\u9876\u9510\u5ea6\"\u5ea6\u91cf\u6765\u91cf\u5316\u5438\u5f15\u5b50\u5c40\u90e8\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u7cfb\u7edf\u6539\u53d8\u6838\u5bbd\u5ea6\u548c\u5b58\u50a8\u8d1f\u8f7d\uff0c\u5206\u6790\u80fd\u91cf\u666f\u89c2\u7684\u51e0\u4f55\u7279\u6027\uff0c\u5e76\u5bf9\u666f\u89c2\u68af\u5ea6\u8fdb\u884c\u7406\u8bba\u5206\u89e3\u3002", "result": "\u53d1\u73b0\u4e86\u4e30\u5bcc\u7684\u5438\u5f15\u5b50\u5f62\u72b6\u76f8\u56fe\uff0c\u6838\u5fc3\u53d1\u73b0\u662f\"\u4f18\u5316\u810a\"\u7684\u51fa\u73b0\uff0c\u5728\u8be5\u533a\u57df\u76f4\u63a5\u9a71\u52a8\u529b\uff08\u7531\u9ad8\u5b58\u50a8\u8d1f\u8f7d\u653e\u5927\uff09\u4e3b\u5bfc\u4e86\u5bf9\u6297\u7684\u96c6\u4f53\u53cd\u9988\u529b\u3002", "conclusion": "\u7f51\u7edc\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5229\u7528\u6a21\u5f0f\u95f4\u76f8\u4e92\u4f5c\u7528\u4f5c\u4e3a\u534f\u4f5c\u53cd\u9988\u63a7\u5236\u7cfb\u7edf\uff0c\u5851\u9020\u51fa\u9c81\u68d2\u7684\u80fd\u91cf\u666f\u89c2\uff0c\u8fd9\u4e3a\u9ad8\u5bb9\u91cf\u8054\u60f3\u8bb0\u5fc6\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7269\u7406\u56fe\u50cf\u548c\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2511.13223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13223", "abs": "https://arxiv.org/abs/2511.13223", "authors": ["Yuxiang Zhang", "Zhengxu Yu", "Weihang Pan", "Zhongming Jin", "Qiang Fu", "Deng Cai", "Binbin Lin", "Jieping Ye"], "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs", "comment": "Accepted to NeurIPS 2025", "summary": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.", "AI": {"tldr": "TokenSqueeze\u662f\u4e00\u79cd\u65b0\u7684Long2Short\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6df1\u5ea6\u548c\u5206\u5e03\u5bf9\u9f50\u7684\u8bed\u8a00\u7cbe\u70bc\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u63a8\u7406LLMs\u7684token\u4f7f\u7528\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406LLMs\u751f\u6210\u957f\u94fe\u5f0f\u601d\u7ef4\u75d5\u8ff9\u5bfc\u81f4token\u4f7f\u7528\u91cf\u589e\u52a0\uff0c\u9020\u6210\u66f4\u9ad8\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\uff0c\u9700\u8981\u5e73\u8861\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u3002", "method": "1. \u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6df1\u5ea6\u4e0e\u95ee\u9898\u590d\u6742\u5ea6\u5339\u914d\u7684\u6837\u672c\uff1b2. \u5206\u5e03\u5bf9\u9f50\u7684\u8bed\u8a00\u7cbe\u70bc\u65b9\u6cd5\uff0c\u5728\u4e0d\u6539\u53d8\u63a8\u7406\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u8bed\u8a00\u8868\u8fbe\u3002", "result": "DeepSeek-R1-Distill-Qwen-7B\u4f7f\u7528\u8be5\u65b9\u6cd5\u5728MATH500\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8650%\u7684\u5e73\u5747token\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "TokenSqueeze\u4ec5\u5229\u7528\u6a21\u578b\u81ea\u751f\u6210\u6570\u636e\uff0c\u65e0\u9700\u4f9d\u8d56\u624b\u52a8\u6574\u7406\u7684\u77ed\u7b54\u6848\u6570\u636e\u96c6\uff0c\u5c31\u80fd\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f\u7684\u63a8\u7406\u3002"}}
{"id": "2511.13243", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13243", "abs": "https://arxiv.org/abs/2511.13243", "authors": ["Xiaoqi Han", "Ru Li", "Ran Yi", "Hongye Tan", "Zhuomin Liang", "V\u00edctor Guti\u00e9rrez-Basulto", "Jeff Z. Pan"], "title": "Uncovering and Mitigating Transient Blindness in Multimodal Model Editing", "comment": "Accepted at AAAI'26", "summary": "Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u4f4e\u76f8\u4f3c\u6027\u6216\u968f\u673a\u8f93\u5165\u800c\u5938\u5927\u6210\u529f\u7387\u7684\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u77ac\u6001\u76f2\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4f4d\u7f6e\u611f\u77e5\u5bf9\u6297\u635f\u5931\u6765\u5e73\u8861\u8de8\u6a21\u6001\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u6a21\u578b\u7f16\u8f91\u7684\u591a\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5938\u5927\u6210\u529f\u7387\u7684\u95ee\u9898\uff0c\u5b83\u4eec\u4f9d\u8d56\u4f4e\u76f8\u4f3c\u6027\u6216\u968f\u673a\u8f93\u5165\u6765\u63a9\u76d6\u8fc7\u62df\u5408\u73b0\u8c61\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u51c6\u786e\u8861\u91cf\u7f16\u8f91\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u968f\u673a\u56fe\u50cf\u4f4d\u7f6e\u6027\u3001\u65e0\u56fe\u50cf\u4f4d\u7f6e\u6027\u3001\u4e00\u81f4\u56fe\u50cf\u4f4d\u7f6e\u6027\uff09\u7684\u5168\u9762\u4f4d\u7f6e\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e03\u79cd\u4e0d\u540c\u7684\u6570\u636e\u7c7b\u578b\u5b9e\u73b0\u8be6\u7ec6\u5206\u6790\uff1b\u5f15\u5165De-VQA\u52a8\u6001\u8bc4\u4f30\u65b9\u6cd5\uff1b\u63d0\u51fa\u4f4d\u7f6e\u611f\u77e5\u5bf9\u6297\u635f\u5931\u6765\u5e73\u8861\u8de8\u6a21\u6001\u8868\u793a\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e73\u5747\u51cf\u5c1117%\u7684\u77ac\u6001\u76f2\u73b0\u8c61\u5e76\u6539\u5584\u4f4d\u7f6e\u6027\uff1b\u4ee4\u724c\u5206\u6790\u663e\u793a\u7f16\u8f91\u5bf9\u6587\u672c\u4ee4\u724c\u7684\u5f71\u54cd\u4e0d\u6210\u6bd4\u4f8b\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f4d\u7f6e\u6027\u8bc4\u4f30\u6846\u67b6\u548c\u4f4d\u7f6e\u611f\u77e5\u5bf9\u6297\u635f\u5931\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u4e2d\u7684\u77ac\u6001\u76f2\u95ee\u9898\uff0c\u663e\u8457\u6539\u5584\u4e86\u7f16\u8f91\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13244", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13244", "abs": "https://arxiv.org/abs/2511.13244", "authors": ["Nadav Bojan Sellam", "Meital Bojan", "Paul Schanda", "Alex Bronstein"], "title": "Seek and You Shall Fold", "comment": null, "summary": "Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u975e\u53ef\u5fae\u5206\u6307\u5bfc\u6846\u67b6\uff0c\u5c06\u8fde\u7eed\u6269\u6563\u751f\u6210\u6a21\u578b\u4e0e\u4efb\u610f\u9ed1\u76d2\u76ee\u6807\u8026\u5408\uff0c\u7528\u4e8e\u5728\u86cb\u767d\u8d28\u751f\u6210\u6a21\u578b\u4e2d\u6574\u5408\u5b9e\u9a8c\u6570\u636e\uff0c\u7279\u522b\u662f\u6838\u78c1\u5171\u632f\u5316\u5b66\u4f4d\u79fb\u6570\u636e\u3002", "motivation": "\u51c6\u786e\u86cb\u767d\u8d28\u7ed3\u6784\u5bf9\u7406\u89e3\u751f\u7269\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c06\u5b9e\u9a8c\u6570\u636e\u6574\u5408\u5230\u86cb\u767d\u8d28\u751f\u6210\u6a21\u578b\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u6838\u78c1\u5171\u632f\u5316\u5b66\u4f4d\u79fb\u7b49\u975e\u53ef\u5fae\u5206\u9884\u6d4b\u5668\u96be\u4ee5\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u6761\u4ef6\u91c7\u6837\u517c\u5bb9\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u975e\u53ef\u5fae\u5206\u6307\u5bfc\u6846\u67b6\uff0c\u5c06\u8fde\u7eed\u6269\u6563\u751f\u6210\u6a21\u578b\u4e0e\u4efb\u610f\u9ed1\u76d2\u76ee\u6807\u901a\u8fc7\u5b9a\u5236\u9057\u4f20\u7b97\u6cd5\u8026\u5408\uff0c\u652f\u6301\u8ddd\u79bb\u7ea6\u675f\u3001\u6838\u5965\u5f17\u8c6a\u6cfd\u6548\u5e94\u7ea6\u675f\u548c\u5316\u5b66\u4f4d\u79fb\u7b49\u591a\u79cd\u6a21\u5f0f\u3002", "result": "\u8bc1\u660e\u4e86\u5316\u5b66\u4f4d\u79fb\u6307\u5bfc\u7ed3\u6784\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9884\u6d4b\u5668\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5c55\u793a\u4e86\u6574\u5408\u591a\u6837\u5316\u5b9e\u9a8c\u4fe1\u53f7\u7684\u901a\u7528\u7b56\u7565\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5b9e\u73b0\u8d85\u8d8a\u53ef\u5fae\u5206\u9650\u5236\u7684\u81ea\u52a8\u5316\u3001\u6570\u636e\u6761\u4ef6\u5316\u86cb\u767d\u8d28\u5efa\u6a21\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.13078", "categories": ["cs.LG", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.13078", "abs": "https://arxiv.org/abs/2511.13078", "authors": ["Liuyi Jin", "Pasan Gunawardena", "Amran Haroon", "Runzhi Wang", "Sangwoo Lee", "Radu Stoleru", "Michael Middleton", "Zepeng Huo", "Jeeeun Kim", "Jason Moats"], "title": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning", "comment": null, "summary": "Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.", "AI": {"tldr": "EMSGlass\u662f\u4e00\u4e2a\u57fa\u4e8eEMSNet\u591a\u6a21\u6001\u591a\u4efb\u52a1\u6a21\u578b\u548cEMSServe\u4f4e\u5ef6\u8fdf\u670d\u52a1\u6846\u67b6\u7684\u667a\u80fd\u773c\u955c\u7cfb\u7edf\uff0c\u4e13\u95e8\u4e3a\u6025\u6551\u533b\u7597\u670d\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u3001\u751f\u547d\u4f53\u5f81\u548c\u573a\u666f\u56fe\u50cf\u6765\u63d0\u5347\u6025\u6551\u4eba\u5458\u7684\u5b9e\u65f6\u6001\u52bf\u611f\u77e5\u548c\u51b3\u7b56\u6548\u7387\u3002", "motivation": "\u6025\u6551\u533b\u7597\u6280\u672f\u4eba\u5458\u5728\u9ad8\u538b\u73af\u5883\u4e0b\u5de5\u4f5c\uff0c\u9700\u8981\u5728\u91cd\u8ba4\u77e5\u8d1f\u8377\u4e0b\u5feb\u901f\u505a\u51fa\u5173\u952e\u51b3\u7b56\u3002\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u6a21\u6001\u4fe1\u606f\u6574\u5408\u548c\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86EMSNet\u591a\u6a21\u6001\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u6574\u5408\u6587\u672c\u3001\u751f\u547d\u4f53\u5f81\u548c\u573a\u666f\u56fe\u50cf\uff0c\u540c\u65f6\u652f\u63015\u4e2a\u5173\u952eEMS\u4efb\u52a1\uff1b\u6784\u5efa\u4e86EMSServe\u4f4e\u5ef6\u8fdf\u670d\u52a1\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u6001\u611f\u77e5\u6a21\u578b\u5206\u5272\u548c\u7279\u5f81\u7f13\u5b58\u673a\u5236\u3002", "result": "EMSNet\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5355\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\uff1bEMSServe\u6bd4\u76f4\u63a5PyTorch\u591a\u6a21\u6001\u63a8\u7406\u5feb1.9-11.7\u500d\uff1b\u7528\u6237\u7814\u7a76\u663e\u793aEMSGlass\u80fd\u663e\u8457\u63d0\u5347\u5b9e\u65f6\u6001\u52bf\u611f\u77e5\u3001\u51b3\u7b56\u901f\u5ea6\u548c\u64cd\u4f5c\u6548\u7387\u3002", "conclusion": "EMSGlass\u6210\u529f\u5c06\u591a\u6a21\u6001\u667a\u80fd\u4e0e\u771f\u5b9e\u4e16\u754c\u6025\u6551\u54cd\u5e94\u5de5\u4f5c\u6d41\u7a0b\u76f8\u7ed3\u5408\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u8d4b\u80fd\u7684EMS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2511.13274", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13274", "abs": "https://arxiv.org/abs/2511.13274", "authors": ["Taras Sereda", "Tom St. John", "Burak Bartan", "Natalie Serrino", "Sachin Katti", "Zain Asgar"], "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators", "comment": "Under review at MLSys 2026", "summary": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.\n  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.", "AI": {"tldr": "KForge\u662f\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684GPU\u5185\u6838\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u4e24\u4e2a\u534f\u4f5c\u7684LLM\u4ee3\u7406\uff1a\u751f\u6210\u4ee3\u7406\u901a\u8fc7\u7f16\u8bd1\u548c\u6b63\u786e\u6027\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u7a0b\u5e8f\uff0c\u6027\u80fd\u5206\u6790\u4ee3\u7406\u901a\u8fc7\u5206\u6790\u6027\u80fd\u6570\u636e\u6307\u5bfc\u4f18\u5316\u3002\u8be5\u67b6\u6784\u53ea\u9700\u5355\u6b21\u793a\u4f8b\u5373\u53ef\u9488\u5bf9\u65b0\u5e73\u53f0\uff0c\u652f\u6301\u8de8\u5e73\u53f0\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u5728NVIDIA CUDA\u548cApple Metal\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "GPU\u5185\u6838\u5bf9\u673a\u5668\u5b66\u4e60\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4e0d\u540c\u52a0\u901f\u5668\u4e0a\u4f18\u5316\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u786c\u4ef6\u5e73\u53f0\uff0c\u9700\u8981\u4e00\u79cd\u5e73\u53f0\u65e0\u5173\u7684\u4f18\u5316\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u4e24\u4e2a\u534f\u4f5c\u7684LLM\u4ee3\u7406\uff1a\u751f\u6210\u4ee3\u7406\u8d1f\u8d23\u7a0b\u5e8f\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u6027\u80fd\u5206\u6790\u4ee3\u7406\u89e3\u91ca\u6027\u80fd\u5206\u6790\u6570\u636e\u5e76\u63d0\u4f9b\u4f18\u5316\u5efa\u8bae\u3002\u901a\u8fc7\u529f\u80fd\u6027\u548c\u4f18\u5316\u4f20\u9012\u8fdb\u884c\u534f\u4f5c\uff0c\u652f\u6301\u4ece\u7a0b\u5e8f\u5316API\u5230GUI\u5de5\u5177\u7684\u5404\u79cd\u6027\u80fd\u5206\u6790\u6570\u636e\u3002", "result": "\u5b9e\u73b0\u4e86\u8de8\u5e73\u53f0\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e00\u4e2a\u67b6\u6784\u7684\u53c2\u8003\u5b9e\u73b0\u80fd\u663e\u8457\u63d0\u5347\u5176\u4ed6\u786c\u4ef6\u76ee\u6807\u7684\u751f\u6210\u8d28\u91cf\u3002\u5728NVIDIA CUDA\u548cApple Metal\u7b49\u4e0d\u540c\u5e76\u884c\u8ba1\u7b97\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u7684\u7a0b\u5e8f\u5408\u6210\u80fd\u529b\u3002", "conclusion": "KForge\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684GPU\u5185\u6838\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u534f\u4f5c\u4ee3\u7406\u67b6\u6784\u5b9e\u73b0\u4e86\u8de8\u786c\u4ef6\u5e73\u53f0\u7684\u7a0b\u5e8f\u4f18\u5316\uff0c\u53ea\u9700\u5355\u6b21\u793a\u4f8b\u5373\u53ef\u9002\u5e94\u65b0\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2511.13082", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13082", "abs": "https://arxiv.org/abs/2511.13082", "authors": ["Kyunghyun Lee", "Yong-Min Shin", "Minwoo Shin", "Jihun Kim", "Sunghwan Lim", "Won-Yong Shin", "Kyungho Yoon"], "title": "Real-time prediction of breast cancer sites using deformation-aware graph neural network", "comment": null, "summary": "Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u65f6\u53d8\u5f62\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u4e73\u817a\u764c\u6d3b\u68c0\u4e2d\u51c6\u786e\u9884\u6d4b\u80bf\u7624\u4f4d\u7f6e\u53d8\u5f62\uff0c\u5b9e\u73b0\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u548c4000\u500d\u8ba1\u7b97\u52a0\u901f\u3002", "motivation": "\u89e3\u51b3\u95f4\u63a5MRI\u5f15\u5bfc\u6d3b\u68c0\u4e2d\u5b9e\u65f6\u53d8\u5f62\u4e73\u623f\u6a21\u578b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u957f\u3001\u6210\u672c\u9ad8\u7684\u9650\u5236\u3002", "method": "\u7ed3\u5408\u4e2a\u4f53\u7279\u5f02\u6027\u6709\u9650\u5143\u6a21\u578b\u548cGNN\uff0c\u5229\u7528MRI\u56fe\u50cf\u7ed3\u6784\u4fe1\u606f\u5904\u7406\u8868\u9762\u4f4d\u79fb\u548c\u8ddd\u79bb\u56fe\u6570\u636e\uff0c\u9884\u6d4b\u7ec4\u7ec7\u6574\u4f53\u4f4d\u79fb\u3002", "result": "\u5728\u5e7b\u5f71\u548c\u771f\u5b9e\u60a3\u8005\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0c\u764c\u75c7\u8282\u70b9\u4f4d\u79fb\u8bef\u5dee\u5c0f\u4e8e0.2mm\uff0c\u7a7a\u95f4\u91cd\u53e0DSC\u8fbe0.977\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u4f20\u7edfFE\u6a21\u62df\u5feb4000\u500d\u3002", "conclusion": "\u8be5\u53d8\u5f62\u611f\u77e5GNN\u6a21\u578b\u4e3a\u4e73\u817a\u764c\u6d3b\u68c0\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u80bf\u7624\u4f4d\u79fb\u9884\u6d4b\u65b9\u6848\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u8bca\u65ad\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2511.13322", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13322", "abs": "https://arxiv.org/abs/2511.13322", "authors": ["Senne Deproost", "Dennis Steckelmacher", "Ann Now\u00e9"], "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning", "comment": "Accepted for BNAIC/BeNeLearn 2025", "summary": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVoronoi\u5206\u533a\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u72b6\u6001\u7a7a\u95f4\u5212\u5206\u4e3a\u591a\u4e2a\u533a\u57df\uff0c\u5728\u6bcf\u4e2a\u533a\u57df\u5185\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u7ebf\u6027\u6a21\u578b\u6765\u8fd1\u4f3c\u590d\u6742\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u4ece\u800c\u5b9e\u73b0\u77e5\u8bc6\u84b8\u998f\u5e76\u63d0\u9ad8\u900f\u660e\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u867d\u7136\u6027\u80fd\u4f18\u79c0\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u8fd9\u7ed9\u6ee1\u8db3\u76d1\u7ba1\u8981\u6c42\u548c\u5efa\u7acb\u4fe1\u4efb\u5e26\u6765\u6311\u6218\u3002\u9700\u8981\u5c06\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8f6c\u79fb\u5230\u4eba\u7c7b\u53ef\u8bfb\u7684\u6a21\u578b\u4e2d\uff0c\u4f46\u5355\u4e00\u7b80\u5316\u6a21\u578b\u5728\u52a8\u6001\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5728\u7075\u6d3b\u6027\u548c\u590d\u6742\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u4f7f\u7528Voronoi\u5206\u533a\u65b9\u6cd5\u5c06\u72b6\u6001\u7a7a\u95f4\u5212\u5206\u4e3a\u591a\u4e2a\u533a\u57df\uff0c\u5728\u6bcf\u4e2a\u533a\u57df\u5185\u8bad\u7ec3\u4e13\u95e8\u7684\u7ebf\u6027\u6a21\u578b\u6765\u8fd1\u4f3c\u539f\u59cb\u9ed1\u76d2\u63a7\u5236\u5668\u7684\u884c\u4e3a\u3002\u8be5\u65b9\u6cd5\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u5404\u79cd\u63a7\u5236\u5668\u3002", "result": "\u5728\u7f51\u683c\u4e16\u754c\u73af\u5883\u548c\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8fd9\u79cd\u5c40\u90e8\u4e13\u4e1a\u5316\u7ebf\u6027\u6a21\u578b\u7684\u84b8\u998f\u65b9\u6cd5\u4ea7\u751f\u7684\u7b56\u7565\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e14\u6027\u80fd\u4e0e\u539f\u59cb\u9ed1\u76d2\u7b56\u7565\u76f8\u5f53\u751a\u81f3\u7565\u6709\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eVoronoi\u5206\u533a\u7684\u5c40\u90e8\u4e13\u4e1a\u5316\u7ebf\u6027\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e73\u8861\u6a21\u578b\u590d\u6742\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u7565\u5fae\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2511.13351", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13351", "abs": "https://arxiv.org/abs/2511.13351", "authors": ["Xinlan Wu", "Bin Zhu", "Feng Han", "Pengkun Jiao", "Jingjing Chen"], "title": "Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning", "comment": null, "summary": "Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u98df\u7269\u5b66\u4e60\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ccLoRA\u67b6\u6784\u548c\u8d28\u91cf\u589e\u5f3a\u4f2a\u56de\u653e\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u98df\u7269\u5206\u6790\u5927\u6a21\u578b\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9700\u8981\u6602\u8d35\u7684\u4ece\u5934\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u53ccLoRA\u67b6\u6784\uff1a\u4e13\u7528LoRA\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u5e76\u4fdd\u6301\u4e0e\u5148\u524d\u4efb\u52a1\u5b50\u7a7a\u95f4\u7684\u6b63\u4ea4\u6027\uff0c\u534f\u4f5cLoRA\u901a\u8fc7\u4f2a\u56de\u653e\u6574\u5408\u8de8\u4efb\u52a1\u5171\u4eab\u77e5\u8bc6\u3002\u8d28\u91cf\u589e\u5f3a\u4f2a\u56de\u653e\u5229\u7528\u81ea\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u51cf\u5c11\u751f\u6210\u6837\u672c\u4e2d\u7684\u5e7b\u89c9\u3002", "result": "\u5728Uni-Food\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u8f7b\u9057\u5fd8\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u9996\u4e2a\u9488\u5bf9\u590d\u6742\u98df\u7269\u4efb\u52a1\u7684\u6709\u6548\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u98df\u7269\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u8425\u517b\u548c\u6162\u6027\u75c5\u9884\u9632\u7b49\u5065\u5eb7\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13124", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.13124", "abs": "https://arxiv.org/abs/2511.13124", "authors": ["Changxi Chi", "Yufei Huang", "Jun Xia", "Jiangbin Zheng", "Yunfan Liu", "Zelin Zang", "Stan Z. Li"], "title": "Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schr\u00f6dinger Bridges", "comment": null, "summary": "Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schr\u00f6dinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u859b\u5b9a\u8c14\u6865\u8fd1\u4f3c\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f4\u63a5\u5bf9\u9f50\u63a7\u5236\u7ec4\u548c\u6270\u52a8\u7ec4\u7684\u5355\u7ec6\u80de\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u5355\u7ec6\u80de\u6270\u52a8\u6570\u636e\u4e0d\u6210\u5bf9\u7684\u95ee\u9898\uff0c\u5e76\u5728\u9057\u4f20\u548c\u836f\u7269\u6270\u52a8\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5355\u7ec6\u80de\u6270\u52a8\u7ed3\u679c\u9884\u6d4b\u5bf9\u57fa\u56e0\u529f\u80fd\u5206\u6790\u548c\u836f\u7269\u5019\u9009\u9009\u62e9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6d4b\u5e8f\u7684\u7834\u574f\u6027\uff0c\u5355\u7ec6\u80de\u6570\u636e\u901a\u5e38\u4e0d\u6210\u5bf9\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u6761\u4ef6\u6216\u4f9d\u8d56\u5148\u9a8c\u7a7a\u95f4\u8fdb\u884c\u95f4\u63a5\u5206\u5e03\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u7cbe\u786e\u7684\u6270\u52a8\u5efa\u6a21\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5c0f\u6279\u91cf\u6700\u4f18\u4f20\u8f93\u7684\u914d\u5bf9\u6765\u8fd1\u4f3c\u859b\u5b9a\u8c14\u6865\uff0c\u907f\u514d\u53cc\u5411\u63a8\u7406\u548c\u53cd\u5411\u8fc7\u7a0b\u5b9a\u4e49\u7684\u4e0d\u9002\u5b9a\u6027\uff0c\u540c\u65f6\u5efa\u6a21\u79bb\u6563\u57fa\u56e0\u6fc0\u6d3b\u72b6\u6001\u548c\u8fde\u7eed\u8868\u8fbe\u5206\u5e03\u3002", "result": "\u5728\u516c\u5171\u9057\u4f20\u548c\u836f\u7269\u6270\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u5f02\u8d28\u6027\u5355\u7ec6\u80de\u54cd\u5e94\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u5bf9\u9f50\u63a7\u5236\u7ec4\u548c\u6270\u52a8\u7ec4\u5206\u5e03\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u5355\u7ec6\u80de\u6270\u52a8\u5e76\u6355\u6349\u7ec6\u80de\u5f02\u8d28\u6027\uff0c\u4e3a\u5355\u7ec6\u80de\u6270\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13373", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13373", "abs": "https://arxiv.org/abs/2511.13373", "authors": ["Prakrit Timilsina", "Anuj Nepal", "Rajan Kadel", "Robin Doss"], "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs", "comment": null, "summary": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u53c2\u6570\u7a7a\u95f4\u878d\u5408\u6280\u672f\u5e94\u7528\u4e8e\u4e24\u4e2a\u57fa\u4e8eMistral-7B\u7684\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5bf9\u4e8e\u67b6\u6784\u517c\u5bb9\u7684\u6a21\u578b\uff0c\u7b80\u5355\u7684\u5e73\u5747\u65b9\u6cd5\u5728\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5206\u5e03\u5f0f\u533b\u7597AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u5f0f\u533b\u7597\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff1a\u6574\u5408\u8de8\u673a\u6784\u4e13\u4e1a\u77e5\u8bc6\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3001\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3001\u9632\u6b62\u6a21\u578b\u66f4\u65b0\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u65b9\u6cd5\uff0c\u7ed3\u5408\u9009\u62e9\u6027\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u6ce8\u610f\u529b\u5c42\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u52a0\u6743\u63d2\u503c\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86Task Arithmetic\u3001\u7ebf\u6027\u5e73\u5747\u3001DARE-TIES\u3001DELLA\u3001Breadcrumbs\u548c\u5206\u5c42\u65b9\u6cd5\u7b49\u516d\u79cd\u53c2\u6570\u878d\u5408\u6280\u672f\u3002", "result": "\u67b6\u6784\u517c\u5bb9\u7684\u6a21\u578b\u4ece\u7b80\u5355\u5e73\u5747\u65b9\u6cd5\u4e2d\u83b7\u76ca\u663e\u8457\uff0cTask Arithmetic\u5728MedQA\u4e0a\u8fbe\u523045.80%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u590d\u6742\u7684\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "\u5bf9\u4e8e\u67b6\u6784\u517c\u5bb9\u7684\u6a21\u578b\uff0c\u7b80\u5355\u5e73\u5747\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u77e5\u8bc6\u6574\u5408\u57fa\u7ebf\uff0c\u4e3a\u53ef\u6269\u5c55\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.13144", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13144", "abs": "https://arxiv.org/abs/2511.13144", "authors": ["Jiacheng Cheng", "Xu Zhang", "Guanghui Qiu", "Yifang Zhang", "Yinchuan Li", "Kaiyuan Feng"], "title": "Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching", "comment": "Accepted in AAAI 2026", "summary": "Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.", "AI": {"tldr": "\u63d0\u51fapFed1BS\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u4f4d\u968f\u673a\u8349\u56fe\u5b9e\u73b0\u6781\u7aef\u901a\u4fe1\u538b\u7f29\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u548c\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u53cc\u5411\u901a\u4fe1\u5f00\u9500\u548c\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u5728\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u3002", "method": "\u4f7f\u7528\u4e00\u4f4d\u968f\u673a\u8349\u56fe\u8fdb\u884c\u901a\u4fe1\u538b\u7f29\uff0c\u5f15\u5165\u57fa\u4e8e\u7b26\u53f7\u7684\u6b63\u5219\u5316\u5668\u6307\u5bfc\u672c\u5730\u6a21\u578b\u4e0e\u5168\u5c40\u5171\u8bc6\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u672c\u5730\u6570\u636e\u7279\u5f81\uff0c\u91c7\u7528\u5feb\u901fHadamard\u53d8\u6362\u63d0\u9ad8\u6295\u5f71\u6548\u7387\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u7b97\u6cd5\u6536\u655b\u5230\u5168\u5c40\u52bf\u51fd\u6570\u7684\u5e73\u7a33\u90bb\u57df\uff0c\u6570\u503c\u6a21\u62df\u663e\u793apFed1BS\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u5728\u6027\u80fd\u4e0a\u4e0e\u5148\u8fdb\u7684\u901a\u4fe1\u9ad8\u6548\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u76f8\u5f53\u3002", "conclusion": "pFed1BS\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u4fe1\u538b\u7f29\u4e0e\u4e2a\u6027\u5316\u5b66\u4e60\u7684\u5e73\u8861\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u548c\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13391", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13391", "abs": "https://arxiv.org/abs/2511.13391", "authors": ["Chengdong Ma", "Th\u00e9o Tao Zhaowei", "Pengyu Li", "Minghao Liu", "Haojun Chen", "Zihao Mao", "Yuan Cheng", "Yuan Qi", "Yaodong Yang"], "title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning", "comment": null, "summary": "Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.", "AI": {"tldr": "PackingStar\u4f7f\u7528\u535a\u5f08\u8bba\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u9ad8\u7ef4\u63a5\u543b\u6570\u95ee\u9898\uff0c\u572825-31\u7ef4\u4e2d\u8d85\u8d8a\u6240\u6709\u5df2\u77e5\u8bb0\u5f55\uff0c\u53d1\u73b0\u4e866000\u591a\u4e2a\u65b0\u7ed3\u6784\u3002", "motivation": "\u63a5\u543b\u6570\u95ee\u9898\u662f\u51e0\u4f55\u5b66\u3001\u6570\u8bba\u548c\u4fe1\u606f\u8bba\u7684\u57fa\u7840\u6311\u6218\uff0c\u9ad8\u7ef4\u51e0\u4f55\u7684\u4e0d\u89c4\u5219\u6027\u548c\u6307\u6570\u7ea7\u7ec4\u5408\u590d\u6742\u6027\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u53cc\u73a9\u5bb6\u77e9\u9635\u5b8c\u6210\u535a\u5f08\uff0c\u4e00\u4e2a\u73a9\u5bb6\u586b\u5145\u77e9\u9635\u6761\u76ee\uff0c\u53e6\u4e00\u4e2a\u4fee\u6b63\u6b21\u4f18\u6761\u76ee\uff0c\u5408\u4f5c\u6700\u5927\u5316\u77e9\u9635\u5927\u5c0f\uff0c\u5bf9\u5e94\u63a5\u543b\u6570\u3002", "result": "\u572825-31\u7ef4\u4e2d\u8d85\u8d8a\u6240\u6709\u4eba\u7c7b\u5df2\u77e5\u8bb0\u5f55\uff0c25\u7ef4\u914d\u7f6e\u51e0\u4f55\u5bf9\u5e94Leech\u683c\u70b9\uff0c13\u7ef4\u9996\u6b21\u7a81\u78341971\u5e74\u7684\u6709\u7406\u7ed3\u6784\u9650\u5236\uff0c\u53d1\u73b06000\u591a\u4e2a\u65b0\u7ed3\u6784\u3002", "conclusion": "AI\u80fd\u591f\u63a2\u7d22\u8d85\u8d8a\u4eba\u7c7b\u76f4\u89c9\u7684\u9ad8\u7ef4\u7a7a\u95f4\uff0c\u4e3a\u63a5\u543b\u6570\u95ee\u9898\u548c\u66f4\u5e7f\u6cdb\u7684\u51e0\u4f55\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.13147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13147", "abs": "https://arxiv.org/abs/2511.13147", "authors": ["Shaoyuan Chen", "Zhixuan Chen", "Dawei Yang", "Zhihang Yuan", "Qiang Wu"], "title": "OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs", "comment": null, "summary": "Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.", "AI": {"tldr": "OTARo\u662f\u4e00\u79cd\u65b0\u9896\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5141\u8bb8\u8bbe\u5907\u7aefLLM\u7075\u6d3b\u5207\u6362\u91cf\u5316\u7cbe\u5ea6\uff0c\u901a\u8fc7\u4e00\u6b21\u5fae\u8c03\u5b9e\u73b0\u591a\u6bd4\u7279\u5bbd\u5ea6\u7684\u6027\u80fd\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u7ed3\u6784\u9650\u5236\uff0c\u65e0\u6cd5\u652f\u6301\u8bbe\u5907\u7aef\u5728\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u4e0b\u7075\u6d3b\u5207\u6362\u7cbe\u5ea6\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u7406\u89e3\u4efb\u52a1\u548c\u751f\u6210\u4efb\u52a1\u5bf9\u91cf\u5316\u7cbe\u5ea6\u7684\u5bb9\u5fcd\u5ea6\u4e0d\u540c\u3002", "method": "\u63d0\u51fa\u5171\u4eab\u6307\u6570\u6d6e\u70b9\u6570(SEFP)\u91cf\u5316\u673a\u5236\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u5c3e\u6570\u622a\u65ad\u5b9e\u73b0\u4e0d\u540c\u6bd4\u7279\u5bbd\u5ea6\uff1b\u91c7\u7528\u5229\u7528-\u63a2\u7d22\u6bd4\u7279\u5bbd\u5ea6\u8def\u5f84\u641c\u7d22(BPS)\u548c\u4f4e\u7cbe\u5ea6\u5f02\u6b65\u7d2f\u79ef(LAA)\u7b56\u7565\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u5728LLaMA3.2-1B\u3001LLaMA3-8B\u7b49\u6d41\u884cLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOTARo\u5728\u6240\u6709\u7cbe\u5ea6\u4e0b\u90fd\u80fd\u5b9e\u73b0\u4e00\u81f4\u5f3a\u5927\u4e14\u9c81\u68d2\u7684\u6027\u80fd\u3002", "conclusion": "OTARo\u6210\u529f\u89e3\u51b3\u4e86\u8bbe\u5907\u7aefLLM\u91cf\u5316\u7cbe\u5ea6\u7075\u6d3b\u5207\u6362\u7684\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13414", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13414", "abs": "https://arxiv.org/abs/2511.13414", "authors": ["Hanwen Hu", "Zimo Wen", "Shiyou Qian", "Jian Co"], "title": "PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation", "comment": null, "summary": "Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.", "AI": {"tldr": "PAST\u7f51\u7edc\u901a\u8fc7\u4e3b-\u8f85\u52a9\u65f6\u7a7a\u6a21\u5f0f\u5904\u7406\u4ea4\u901a\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\uff0c\u572827\u79cd\u7f3a\u5931\u6570\u636e\u6761\u4ef6\u4e0b\u4f18\u4e8e7\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0cRMSE\u63d0\u5347\u8fbe26.2%\uff0cMAE\u63d0\u5347\u8fbe31.6%\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u968f\u673a\u7f3a\u5931\u4f4d\u7f6e\uff0c\u65e0\u6cd5\u5b66\u4e60\u957f\u671f\u548c\u5927\u89c4\u6a21\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fd9\u5728\u5e7f\u6cdb\u7f3a\u5931\u6761\u4ef6\u4e0b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faPAST\u7f51\u7edc\uff0c\u5305\u542b\u56fe\u96c6\u6210\u6a21\u5757(GIM)\u548c\u4ea4\u53c9\u95e8\u63a7\u6a21\u5757(CGM)\u3002GIM\u901a\u8fc7\u52a8\u6001\u56fe\u548c\u533a\u95f4\u611f\u77e5dropout\u6355\u83b7\u4e3b\u8981\u6a21\u5f0f\uff0cCGM\u901a\u8fc7\u53cc\u5411\u95e8\u63a7\u63d0\u53d6\u8f85\u52a9\u6a21\u5f0f\u3002\u4e24\u4e2a\u6a21\u5757\u901a\u8fc7\u5171\u4eab\u9690\u85cf\u5411\u91cf\u4ea4\u4e92\uff0c\u5728\u96c6\u6210\u81ea\u76d1\u7763\u6846\u67b6\u4e0b\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u768427\u79cd\u7f3a\u5931\u6570\u636e\u6761\u4ef6\u4e0b\uff0cPAST\u7684\u63d2\u8865\u7cbe\u5ea6\u4f18\u4e8e7\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0cRMSE\u63d0\u5347\u8fbe26.2%\uff0cMAE\u63d0\u5347\u8fbe31.6%\u3002", "conclusion": "PAST\u901a\u8fc7\u533a\u5206\u4e3b\u8981\u548c\u8f85\u52a9\u6a21\u5f0f\uff0c\u6709\u6548\u5904\u7406\u5404\u79cd\u7f3a\u5931\u6570\u636e\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u6027\u80fd\u3002"}}
{"id": "2511.13174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13174", "abs": "https://arxiv.org/abs/2511.13174", "authors": ["Ella J. Schmidtobreick", "Daniel Arnstr\u00f6m", "Paul H\u00e4usner", "Jens Sj\u00f6lund"], "title": "Warm-starting active-set solvers using graph neural networks", "comment": "Under review, 15 pages, 8 figures", "summary": "Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.", "AI": {"tldr": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u4e2d\u7684\u6709\u6548\u96c6\uff0c\u4ee5\u70ed\u542f\u52a8\u6c42\u89e3\u5668\uff0c\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u5e76\u63d0\u9ad8\u5b9e\u65f6\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u4e8c\u6b21\u89c4\u5212\u6c42\u89e3\u5668\u5728\u5b9e\u65f6\u63a7\u5236\u548c\u4f18\u5316\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5728\u65f6\u95f4\u5173\u952e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u52a0\u901f\u6c42\u89e3\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u5b66\u4e60\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5c06\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u8868\u793a\u4e3a\u4e8c\u5206\u56fe\uff0c\u5b66\u4e60\u9884\u6d4b\u6700\u4f18\u6709\u6548\u96c6\u6765\u70ed\u542f\u52a8DAQP\u6c42\u89e3\u5668\u3002", "result": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u4e0b\u90fd\u80fd\u51cf\u5c11\u6c42\u89e3\u5668\u8fed\u4ee3\u6b21\u6570\uff0c\u6027\u80fd\u4e0e\u591a\u5c42\u611f\u77e5\u5668\u57fa\u7ebf\u76f8\u5f53\uff0c\u4e14\u80fd\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7ef4\u5ea6\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u5b66\u4e60\u5728\u52a0\u901f\u5b9e\u65f6\u5e94\u7528\uff08\u5982\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff09\u4e2d\u7684\u4f18\u5316\u8fc7\u7a0b\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2511.13444", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13444", "abs": "https://arxiv.org/abs/2511.13444", "authors": ["Zhipeng Ma", "Bo N\u00f8rregaard J\u00f8rgensen", "Zheng Grace Ma"], "title": "Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes", "comment": null, "summary": "Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5377\u79ef\u805a\u7c7b\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u53d1\u73b0\u5de5\u4e1a\u64cd\u4f5c\u6a21\u5f0f\uff0c\u901a\u8fc7\u7070\u5ea6\u77e9\u9635\u8f6c\u6362\u3001\u4e24\u9636\u6bb5\u805a\u7c7b\u7b56\u7565\u548c\u590d\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u5728\u7194\u7089\u64cd\u4f5c\u6570\u636e\u4e2d\u8bc6\u522b\u51fa7\u79cd\u53ef\u89e3\u91ca\u7684\u64cd\u4f5c\u6a21\u5f0f\u3002", "motivation": "\u5de5\u4e1a\u8fc7\u7a0b\u76d1\u63a7\u4e2d\u4f20\u611f\u5668\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7f3a\u4e4f\u6807\u7b7e\u3001\u9ad8\u53d8\u5f02\u6027\u4e14\u5b58\u5728\u64cd\u4f5c\u566a\u58f0\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6a21\u5f0f\u3002\u73b0\u6709\u805a\u7c7b\u6280\u672f\u8981\u4e48\u4f9d\u8d56\u56fa\u5b9a\u8ddd\u79bb\u5ea6\u91cf\uff0c\u8981\u4e48\u4f7f\u7528\u9759\u6001\u6570\u636e\u8bbe\u8ba1\u7684\u6df1\u5ea6\u6a21\u578b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u3001\u975e\u7ed3\u6784\u5316\u7684\u5de5\u4e1a\u5e8f\u5217\u3002", "method": "\u5c06\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u901a\u8fc7\u91cd\u53e0\u6ed1\u52a8\u7a97\u53e3\u8f6c\u6362\u4e3a\u7070\u5ea6\u77e9\u9635\u8868\u793a\uff0c\u4f7f\u7528\u6df1\u5ea6\u5377\u79ef\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff1b\u96c6\u6210\u8f6f\u786c\u805a\u7c7b\u8f93\u51fa\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u8fdb\u884c\u4f18\u5316\uff1b\u4f7f\u7528\u65b0\u5f00\u53d1\u7684\u590d\u5408\u8bc4\u5206S_eva\uff08\u7ed3\u5408\u6807\u51c6\u5316\u8f6e\u5ed3\u7cfb\u6570\u3001Calinski-Harabasz\u548cDavies-Bouldin\u6307\u6570\uff09\u5ba2\u89c2\u8bc4\u4f30\u805a\u7c7b\u6027\u80fd\u3002", "result": "\u5728\u5317\u6b27\u94f8\u9020\u5382\u76843900\u591a\u4e2a\u7194\u7089\u7194\u5316\u64cd\u4f5c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8bc6\u522b\u51fa7\u79cd\u53ef\u89e3\u91ca\u7684\u64cd\u4f5c\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u80fd\u8017\u3001\u70ed\u52a8\u529b\u5b66\u548c\u751f\u4ea7\u6301\u7eed\u65f6\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002\u76f8\u6bd4\u7ecf\u5178\u548c\u6df1\u5ea6\u805a\u7c7b\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6574\u4f53\u6027\u80fd\u3001\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9886\u57df\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u5e8f\u5217\u4e0d\u89c4\u5219\u6027\u3001\u6a21\u5f0f\u91cd\u53e0\u548c\u5ea6\u91cf\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u8bca\u65ad\u548c\u80fd\u6e90\u4f18\u5316\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13178", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13178", "abs": "https://arxiv.org/abs/2511.13178", "authors": ["Mingxuan Tian", "Haochen Mu", "Donghong Ding", "Mengjiao Li", "Yuhan Ding", "Jianping Zhao"], "title": "Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach", "comment": null, "summary": "With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7b97\u5b50(PINO)\u65b9\u6cd5\uff0c\u7528\u4e8e\u91d1\u5c5e\u589e\u6750\u5236\u9020\u4e2d\u5b9e\u65f6\u9884\u6d4b\u672a\u676515\u79d2\u7684z\u548cy\u65b9\u5411\u53d8\u5f62\u573a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u503c\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\u7684\u95ee\u9898\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u548c\u667a\u80fd\u5236\u9020\u7cfb\u7edf\u7684\u53d1\u5c55\u8feb\u5207\u9700\u8981\u5b9e\u65f6\u53d8\u5f62\u573a\u9884\u6d4b\u6765\u63a7\u5236\u91d1\u5c5e\u589e\u6750\u5236\u9020\u7f3a\u9677\uff0c\u4f46\u4f20\u7edf\u6570\u503c\u6a21\u62df\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8fd0\u884c\u65f6\u95f4\u957f\uff0c\u800c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u63d0\u53d6\u957f\u65f6\u57df\u9884\u6d4b\u7684\u65f6\u7a7a\u7279\u5f81\u4e14\u65e0\u6cd5\u89e3\u8026\u70ed\u529b\u573a\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc-\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(PIDeepONet-RNN)\uff0c\u4f7f\u7528\u4e3b\u5e72\u548c\u5206\u652f\u7f51\u7edc\u5206\u522b\u5904\u7406\u6e29\u5ea6\u5386\u53f2\u548c\u7f16\u7801\u53d8\u5f62\u573a\uff0c\u5b9e\u73b0\u70ed\u529b\u54cd\u5e94\u7684\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u70ed\u4f20\u5bfc\u65b9\u7a0b\u4f5c\u4e3a\u8f6f\u7ea6\u675f\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u6a21\u578b\u5728\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6709\u9650\u5143\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6d4b\u8bd5\uff0c\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8bef\u5dee\u7d2f\u79ef\u548c\u65f6\u95f4\u6548\u7387\u3002z\u548cy\u65b9\u5411\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a0.9733mm\u548c0.2049mm\uff0c\u7194\u6c60\u533a\u57df\u8bef\u5dee\u8f83\u9ad8\u4f46\u5173\u952e\u6c89\u79ef\u533a\u57df\u68af\u5ea6\u8303\u6570\u8f83\u4f4e\u3002", "conclusion": "PINO\u4ee3\u7406\u6a21\u578b\u5728\u5b9e\u65f6\u957f\u65f6\u57df\u7269\u7406\u573a\u9884\u6d4b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u7f3a\u9677\u63a7\u5236\uff0c\u5176\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u7684\u57fa\u7840\u51fd\u6570\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7840\u3002"}}
{"id": "2511.13457", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13457", "abs": "https://arxiv.org/abs/2511.13457", "authors": ["Bin Liu", "Qinghao Zhao", "Yuxi Zhou", "Zhejun Sun", "Kaijie Lei", "Deyun Zhang", "Shijia Geng", "Shenda Hong"], "title": "Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure", "comment": "19 pages, 5 figures", "summary": "Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u80ba\u6d3b\u91cf\u65f6\u95f4\u5e8f\u5217\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\uff0c\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b\u53f3\u5fc3\u8870\u7aed\uff0c\u5728UK Biobank\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u7279\u522b\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u4e9a\u7ec4\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u53f3\u5fc3\u8870\u7aed\u662f\u4e00\u79cd\u4e0e\u9ad8\u53d1\u75c5\u7387\u548c\u6b7b\u4ea1\u7387\u76f8\u5173\u7684\u75be\u75c5\uff0c\u80ba\u75c5\u5e38\u5bfc\u81f4\u53f3\u5fc3\u5ba4\u8d1f\u8377\u589e\u52a0\u5f15\u53d1\u53f3\u5fc3\u8870\u7aed\u3002\u4ece\u80ba\u5fc3\u75c5\u60a3\u8005\u4e2d\u65e9\u671f\u7b5b\u67e5\u51fa\u53ef\u80fd\u53d1\u5c55\u4e3a\u53f3\u5fc3\u8870\u7aed\u7684\u60a3\u8005\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u610f\u4e49\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ece\u6570\u636e\u589e\u5f3a\u7684\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u5b66\u4e60\u80ba\u6d3b\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u7a33\u5065\u4f4e\u7ef4\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u8be5\u8868\u793a\u4e0e\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\u878d\u5408\uff0c\u8f93\u5165CatBoost\u5206\u7c7b\u5668\u8fdb\u884c\u53f3\u5fc3\u8870\u7aed\u9884\u6d4b\u3002", "result": "\u5728UK Biobank\u768426,617\u540d\u4e2a\u4f53\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578bAUROC\u8fbe\u52300.7501\uff1b\u5728\u9ad8\u98ce\u9669\u4e9a\u7ec4\u4e2d\u8868\u73b0\u66f4\u4f73\uff1a\u6162\u6027\u80be\u75c5\u60a3\u8005\u6d4b\u8bd5\u96c6AUROC\u4e3a0.8194\uff0c\u74e3\u819c\u6027\u5fc3\u810f\u75c5\u60a3\u8005\u4e3a0.8413\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u80ba\u6d3b\u91cf\u65f6\u95f4\u5e8f\u5217\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\uff0c\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u663e\u793a\u51fa\u65e9\u671f\u68c0\u6d4b\u53f3\u5fc3\u8870\u7aed\u7684\u826f\u597d\u6f5c\u529b\u3002"}}
{"id": "2511.13185", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13185", "abs": "https://arxiv.org/abs/2511.13185", "authors": ["Aishwarya Venkataramanan", "Sai Karthikeya Vemuri", "Adithya Ashok Chalain Valapil", "Joachim Denzler"], "title": "Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction", "comment": "EurIPS DiffSys workshop 2025", "summary": "Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.", "AI": {"tldr": "\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e86\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u5728CARS\u5230\u62c9\u66fc\u4fe1\u53f7\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bc1\u660e\u5c06\u7269\u7406\u77e5\u8bc6\u7ea6\u675f\u878d\u5165\u6a21\u578b\u53ef\u4ee5\u6539\u5584\u6821\u51c6\uff0c\u4e3a\u66f4\u53ef\u9760\u7684CARS\u6570\u636e\u5206\u6790\u63d0\u4f9b\u8def\u5f84\u3002", "motivation": "CARS\u5149\u8c31\u5b66\u5728\u533b\u5b66\u3001\u6750\u6599\u79d1\u5b66\u548c\u5316\u5b66\u5206\u6790\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d7\u5230\u975e\u5171\u632f\u80cc\u666f\u5e72\u6270\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u91cd\u5efa\u62c9\u66fc\u5149\u8c31\uff0c\u4f46\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e86\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u5728CARS\u5230\u62c9\u66fc\u4fe1\u53f7\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c06\u7269\u7406\u77e5\u8bc6\u7ea6\u675f\uff08\u5982Kramers-Kronig\u5173\u7cfb\u548c\u5149\u6ed1\u6027\u7ea6\u675f\uff09\u4ee5\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u7684\u5f62\u5f0f\u6574\u5408\u5230\u6a21\u578b\u4e2d\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u7269\u7406\u77e5\u8bc6\u7ea6\u675f\u6574\u5408\u5230\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6a21\u578b\u4e2d\u80fd\u591f\u6539\u5584\u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u66f4\u53ef\u9760\u7684CARS\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u79d1\u5b66\u548c\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2511.13463", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13463", "abs": "https://arxiv.org/abs/2511.13463", "authors": ["Hussein Rajabu", "Lijun Qian", "Xishuang Dong"], "title": "Multi-task GINN-LP for Multi-target Symbolic Regression", "comment": null, "summary": "In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u4efb\u52a1\u56de\u5f52GINN-LP\uff08MTRGINN-LP\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u76ee\u6807\u7b26\u53f7\u56de\u5f52\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408GINN-LP\u4e0e\u591a\u4efb\u52a1\u6df1\u5ea6\u5b66\u4e60\uff0c\u89e3\u51b3\u4f20\u7edf\u7b26\u53f7\u56de\u5f52\u5728\u73b0\u5b9e\u591a\u8f93\u51fa\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7b26\u53f7\u56de\u5f52\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u5927\u591a\u6570\u65b9\u6cd5\u5728\u5177\u6709\u660e\u786e\u5173\u7cfb\u7684\u79d1\u5b66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\uff1b\u4e3b\u8981\u9488\u5bf9\u5355\u8f93\u51fa\u56de\u5f52\uff0c\u800c\u8bb8\u591a\u73b0\u5b9e\u95ee\u9898\u6d89\u53ca\u5177\u6709\u76f8\u4e92\u4f9d\u8d56\u53d8\u91cf\u7684\u591a\u76ee\u6807\u8f93\u51fa\u3002", "method": "\u901a\u8fc7\u96c6\u6210GINN-LP\u4e0e\u591a\u4efb\u52a1\u6df1\u5ea6\u5b66\u4e60\uff0c\u6a21\u578b\u7ed3\u5408\u4e86\u5305\u542b\u591a\u4e2a\u5e42\u9879\u903c\u8fd1\u5668\u5757\u7684\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u8f93\u51fa\u5c42\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u6355\u83b7\u76ee\u6807\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u80fd\u6e90\u6548\u7387\u9884\u6d4b\u548c\u53ef\u6301\u7eed\u519c\u4e1a\u7b49\u5b9e\u9645\u591a\u76ee\u6807\u5e94\u7528\u4e0a\u9a8c\u8bc1\u4e86\u591a\u4efb\u52a1GINN-LP\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5177\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u6027\u80fd\u548c\u9ad8\u5ea6\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u5c06\u7b26\u53f7\u56de\u5f52\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u591a\u8f93\u51fa\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2511.13234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13234", "abs": "https://arxiv.org/abs/2511.13234", "authors": ["Boris Kriuk"], "title": "MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing", "comment": "8 pages, 5 figures", "summary": "Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (\u03c3=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.", "AI": {"tldr": "MorphBoost\u662f\u4e00\u79cd\u65b0\u578b\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7ec4\u7ec7\u6811\u7ed3\u6784\u52a8\u6001\u8c03\u6574\u5206\u88c2\u884c\u4e3a\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u6f14\u5316\u5206\u88c2\u51fd\u6570\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6\u63d0\u5347\u7b97\u6cd5\u4f7f\u7528\u9759\u6001\u6811\u7ed3\u6784\u548c\u56fa\u5b9a\u5206\u88c2\u6807\u51c6\uff0c\u65e0\u6cd5\u9002\u5e94\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u68af\u5ea6\u5206\u5e03\u7684\u53d8\u5316\u548c\u4e0d\u540c\u5b66\u4e60\u9636\u6bb5\u7684\u95ee\u9898\u7279\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u5206\u88c2\u51fd\u6570\uff0c\u57fa\u4e8e\u7d2f\u79ef\u68af\u5ea6\u7edf\u8ba1\u548c\u8fed\u4ee3\u76f8\u5173\u5b66\u4e60\u538b\u529b\u6f14\u5316\uff1b\u5305\u62ec\u5f62\u6001\u53d8\u5316\u5206\u88c2\u51c6\u5219\u3001\u81ea\u52a8\u95ee\u9898\u6307\u7eb9\u8bc6\u522b\u3001\u5411\u91cf\u5316\u6811\u9884\u6d4b\u3001\u4ea4\u4e92\u611f\u77e5\u7279\u5f81\u91cd\u8981\u6027\u548c\u5feb\u901f\u6a21\u5f0f\u4f18\u5316\u3002", "result": "\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cMorphBoost\u5e73\u5747\u6027\u80fd\u4f18\u4e8eXGBoost 0.84%\uff0c\u83b7\u5f974/10\u6570\u636e\u96c6\u80dc\u5229\uff0840%\u80dc\u7387\uff09\u548c6/30\u524d\u4e09\u540d\uff0820%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f4e\u65b9\u5dee\uff08\u03c3=0.0948\uff09\u548c\u6700\u9ad8\u6700\u5c0f\u51c6\u786e\u7387\u3002", "conclusion": "MorphBoost\u901a\u8fc7\u52a8\u6001\u81ea\u7ec4\u7ec7\u6811\u7ed3\u6784\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u5c55\u73b0\u4e86\u68af\u5ea6\u63d0\u5347\u7b97\u6cd5\u7684\u65b0\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2511.13240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13240", "abs": "https://arxiv.org/abs/2511.13240", "authors": ["Arka Pal", "Teo Kitanovski", "Arthur Liang", "Akilesh Potti", "Micah Goldblum"], "title": "Incoherent Beliefs & Inconsistent Actions in Large Language Models", "comment": null, "summary": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.", "AI": {"tldr": "LLMs\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u4fe1\u5ff5\u66f4\u65b0\u4e0d\u4e00\u81f4\u548c\u884c\u52a8\u4e0e\u4fe1\u5ff5\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u9759\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u4e5f\u5b58\u5728\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0e\u9759\u6001\u6570\u636e\u96c6\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u987a\u5e8f\u4ea4\u4e92\u3001\u8fde\u8d2f\u66f4\u65b0\u4fe1\u5ff5\u5e76\u57fa\u4e8e\u4fe1\u5ff5\u505a\u51fa\u9002\u5f53\u51b3\u7b56\u3002\u9884\u6d4bLLMs\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\u5f88\u91cd\u8981\uff0c\u4f46\u4ece\u9759\u6001\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u786e\u5b9a\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u68c0\u9a8cLLMs\u7684\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a\u8fde\u8d2f\u66f4\u65b0\u4fe1\u5ff5\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u884c\u52a8\u4e0e\u4fe1\u5ff5\u4e00\u81f4\u6027\u7684\u7a0b\u5ea6\u3002\u5305\u62ec\u76f4\u63a5\u5f15\u51fa\u540e\u9a8c\u4e0e\u6b63\u786e\u66f4\u65b0\u5148\u9a8c\u7684\u6bd4\u8f83\uff0c\u4ee5\u53ca\u5728\u535a\u5f69\u5e02\u573a\u4e2d\u884c\u52a8\u4e0e\u4fe1\u5ff5\u7684\u4e00\u81f4\u6027\u5206\u6790\u3002", "result": "\u53d1\u73b0LLMs\u5728\u4fe1\u5ff5\u66f4\u65b0\u4e0a\u5b58\u5728\u9ad8\u8fbe30%\u7684\u5e73\u5747\u4e0d\u4e00\u81f4\u6027\uff1b\u5728\u884c\u52a8\u4e0a\u4e5f\u7ecf\u5e38\u4e0e\u6301\u6709\u7684\u4fe1\u5ff5\u4e0d\u4e00\u81f4\uff1b\u5373\u4f7f\u5728\u9ad8\u51c6\u786e\u7387\u6216\u826f\u597d\u6821\u51c6\u7684\u5f3a\u6a21\u578b\u4e2d\uff0c\u8fd9\u4e9b\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5728\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u9884\u6d4bLLM\u884c\u4e3a\u7684\u56f0\u96be\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u52a8\u6001\u4ea4\u4e92\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002"}}
{"id": "2511.13625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13625", "abs": "https://arxiv.org/abs/2511.13625", "authors": ["Kaichi Irie", "Shuhei Watanabe", "Masaki Onishi"], "title": "Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization", "comment": "Accepted to 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)", "summary": "Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u62df\u725b\u987f\u66f4\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u7a0b\u5728\u6279\u91cf\u8c03\u7528\u91c7\u96c6\u51fd\u6570\u7684\u540c\u65f6\u4fdd\u6301\u4e0e\u987a\u5e8f\u591a\u8d77\u70b9\u4f18\u5316\u76f8\u540c\u7684\u7406\u8bba\u6536\u655b\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u91c7\u96c6\u51fd\u6570\u4f18\u5316\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u91c7\u96c6\u51fd\u6570\u4f18\u5316\u7684\u4e3b\u8981\u8ba1\u7b97\u74f6\u9888\u5728\u4e8e\u591a\u8d77\u70b9\u4f18\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u5982BoTorch\u901a\u8fc7\u6279\u91cf\u5904\u7406\u91c7\u96c6\u51fd\u6570\u6765\u52a0\u901f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u62df\u725b\u987f\u6cd5\u7684\u9006Hessian\u8fd1\u4f3c\u4e2d\u5b58\u5728\u975e\u5bf9\u89d2\u8fd1\u4f3c\u8bef\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u6536\u655b\u901f\u5ea6\u53d8\u6162\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u534f\u7a0b\u89e3\u8026\u62df\u725b\u987f\u66f4\u65b0\uff0c\u5728\u6279\u91cf\u8c03\u7528\u91c7\u96c6\u51fd\u6570\u7684\u540c\u65f6\u4fdd\u6301\u72ec\u7acb\u7684\u62df\u725b\u987f\u66f4\u65b0\u8fc7\u7a0b\uff0c\u907f\u514d\u4e86\u9006Hessian\u7684\u975e\u5bf9\u89d2\u8fd1\u4f3c\u8bef\u5dee\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4e0e\u987a\u5e8f\u591a\u8d77\u70b9\u4f18\u5316\u76f8\u540c\u7684\u7406\u8bba\u6536\u655b\u6027\uff0c\u800c\u4e14\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u5927\u5e45\u51cf\u5c11\u4e86\u5b9e\u9645\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u62df\u725b\u987f\u66f4\u65b0\u548c\u6279\u91cf\u91c7\u96c6\u51fd\u6570\u8c03\u7528\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u91c7\u96c6\u51fd\u6570\u4f18\u5316\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6536\u655b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\u3002"}}
{"id": "2511.13640", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13640", "abs": "https://arxiv.org/abs/2511.13640", "authors": ["Haohui Wang", "Jingyuan Qi", "Jianpeng Chen", "Jun Wu", "Lifu Huang", "Lecheng Zheng", "Kevin Choi", "Balaji Veeramani", "Edward Bowen", "Alison Hu", "Tyler Cody", "Dawei Zhou"], "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures", "comment": null, "summary": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u5bf9LLM\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u4e86\u4e09\u79cd\u7f29\u653e\u884c\u4e3a\u9636\u6bb5\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u6df7\u5408\u6570\u636e\u7684\u6cdb\u5316\u8fb9\u754c\u7406\u8bba\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u3002", "motivation": "\u5408\u6210\u6570\u636e\u867d\u7136\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u4f46\u4f1a\u5f15\u5165\u7cfb\u7edf\u6027\u5206\u5e03\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u957f\u5c3e\u77e5\u8bc6\u65b9\u9762\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u8fd9\u5bf9\u6df7\u5408\u6570\u636e\u96c6\u7684\u6548\u7528\u8bc4\u4f30\u63d0\u51fa\u4e86\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u8bc6\u522b\u4e86\u4e09\u79cd\u7f29\u653e\u884c\u4e3a\u9636\u6bb5\uff0c\u63a8\u5bfc\u4e86\u9488\u5bf9\u771f\u5b9e-\u5408\u6210\u6df7\u5408\u6570\u636e\u7684LLM\u6cdb\u5316\u8fb9\u754c\u7406\u8bba\uff0c\u5e76\u57fa\u4e8e\u7406\u8bba\u53d1\u73b0\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u60c5\u611f\u5206\u7c7b\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u590d\u6742\u63a8\u7406\u56db\u4e2a\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u4f30\u503c\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u8bc4\u4f30\u6df7\u5408\u771f\u5b9e-\u5408\u6210\u6570\u636e\u96c6\u7684\u6548\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u5730\u5229\u7528\u5408\u6210\u6570\u636e\u8fdb\u884cLLM\u8bad\u7ec3\u3002"}}
{"id": "2511.13653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13653", "abs": "https://arxiv.org/abs/2511.13653", "authors": ["Leo Gao", "Achyuta Rajaram", "Jacob Coxon", "Soham V. Govande", "Bowen Baker", "Dan Mossing"], "title": "Weight-sparse transformers have interpretable circuits", "comment": null, "summary": "Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.", "AI": {"tldr": "\u901a\u8fc7\u8bad\u7ec3\u6743\u91cd\u7a00\u758f\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5bfb\u627e\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7535\u8def\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u6743\u8861\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u89e3\u91ca\u73b0\u6709\u5bc6\u96c6\u6a21\u578b\u3002", "motivation": "\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5bfb\u627e\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7535\u8def\u662f\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u9886\u57df\u7684\u6838\u5fc3\u76ee\u6807\uff0c\u5f53\u524d\u9700\u8981\u66f4\u6e05\u6670\u3001\u66f4\u6613\u7406\u89e3\u7684\u795e\u7ecf\u7f51\u7edc\u8fde\u63a5\u7ed3\u6784\u3002", "method": "\u8bad\u7ec3\u6743\u91cd\u7a00\u758f\u6a21\u578b\uff0c\u7ea6\u675f\u5927\u591a\u6570\u6743\u91cd\u4e3a\u96f6\uff0c\u4f7f\u6bcf\u4e2a\u795e\u7ecf\u5143\u53ea\u6709\u5c11\u91cf\u8fde\u63a5\uff1b\u901a\u8fc7\u526a\u679d\u6765\u9694\u79bb\u7279\u5b9a\u4efb\u52a1\u76f8\u5173\u7684\u7535\u8def\u90e8\u5206\u3002", "result": "\u83b7\u5f97\u7684\u7535\u8def\u5305\u542b\u5bf9\u5e94\u81ea\u7136\u6982\u5ff5\u7684\u795e\u7ecf\u5143\u548c\u6b8b\u5dee\u901a\u9053\uff0c\u5177\u6709\u5c11\u91cf\u76f4\u63a5\u53ef\u89e3\u91ca\u7684\u8fde\u63a5\uff1b\u7a00\u758f\u5316\u5728\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u53ef\u6539\u5584\u8fd9\u4e00\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u524d\u6240\u672a\u6709\u7684\u53ef\u7406\u89e3\u7535\u8def\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u9a8c\u8bc1\uff0c\u4f46\u5c06\u7a00\u758f\u6a21\u578b\u6269\u5c55\u5230\u5343\u4e07\u7ea7\u53c2\u6570\u4ee5\u4e0a\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002"}}
{"id": "2511.13250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13250", "abs": "https://arxiv.org/abs/2511.13250", "authors": ["Aleksandar Stankovi\u0107", "Dejan Lisica"], "title": "Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs", "comment": "8 pages, 3 figures, 5 tables. Code and artifacts: https://github.com/SV25-22/ECHO-Proteins", "summary": "We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.", "AI": {"tldr": "\u4e3aogbn-proteins\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8fb9\u7f18\u611f\u77e5\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u8fb9\u7f18\u7279\u5f81\u805a\u5408\u548c\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u53d1\u73b0\u57fa\u4e8esum\u7684\u8fb9\u7f18\u5230\u8282\u70b9\u7279\u5f81\u805a\u5408\u6548\u679c\u6700\u597d\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u6807\u51c6\u5316\u65b9\u6cd5\u548c\u540e\u5904\u7406\u6280\u672f\u3002", "motivation": "\u4e3aogbn-proteins\u6570\u636e\u96c6\u5efa\u7acb\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7814\u7a76\u8fb9\u7f18\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u56fe\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\uff0c\u7279\u522b\u662f\u8fb9\u7f18\u8bc1\u636e\u805a\u5408\u548c\u6d88\u606f\u4f20\u9012\u4e2d\u7684\u7cfb\u7edf\u9009\u62e9\u95ee\u9898\u3002", "method": "\u4f7f\u7528PyTorch Geometric\u5b9e\u73b0GraphSAGE\uff0c\u6bd4\u8f83\u4e0d\u540c\u8fb9\u7f18\u7279\u5f81\u805a\u5408\u65b9\u6cd5(sum/mean/max)\uff0c\u7814\u7a76LayerNorm\u3001BatchNorm\u548c\u6761\u4ef6LayerNorm\u7b49\u6807\u51c6\u5316\u6280\u672f\uff0c\u5e76\u5e94\u7528\u540e\u5904\u7406\u6280\u672f\u5982\u6e29\u5ea6\u7f29\u653e\u548c\u6807\u7b7e\u76f8\u5173\u6027\u5e73\u6ed1\u3002", "result": "sum\u805a\u5408\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff1bBatchNorm\u83b7\u5f97\u6700\u9ad8AUC\uff1b\u6761\u4ef6LayerNorm\u5728AUC\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u66f4\u597d\u7684\u9608\u503cF1\uff1b\u540e\u5904\u7406\u6280\u672f\u663e\u8457\u6539\u5584\u4e86micro-F1\u548c\u6821\u51c6\u8bef\u5dee\uff0c\u4e14\u5bf9AUC\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "\u4e3aogbn-proteins\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8fb9\u7f18\u7279\u5f81\u805a\u5408\u65b9\u5f0f\u3001\u6807\u51c6\u5316\u6280\u672f\u548c\u540e\u5904\u7406\u5bf9\u6027\u80fd\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u5e76\u53d1\u5e03\u4e86\u6807\u51c6\u5316\u7684\u5b9e\u9a8c\u4ee3\u7801\u548c\u811a\u672c\u3002"}}
{"id": "2511.13685", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13685", "abs": "https://arxiv.org/abs/2511.13685", "authors": ["Disha Varshney", "Samarth Garg", "Sarthak Tyagi", "Deeksha Varshney", "Nayan Deep", "Asif Ekbal"], "title": "Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers", "comment": "40 pages", "summary": "In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSSRGNet\u6a21\u578b\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u86cb\u767d\u8d28\u6b8b\u57fa\u56fe\u548c\u5e8f\u5217\u8fde\u63a5\u6765\u9884\u6d4b\u86cb\u767d\u8d28\u4e8c\u7ea7\u7ed3\u6784\uff0c\u5728f1\u5206\u6570\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u672a\u6807\u8bb0\u7684\u6c28\u57fa\u9178\u5e8f\u5217\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u53ef\u83b7\u5f97\u7684\u86cb\u767d\u8d283D\u7ed3\u6784\u6570\u636e\uff0c\u800c3D\u7ed3\u6784\u5bf9\u86cb\u767d\u8d28\u529f\u80fd\u5177\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u6c28\u57fa\u9178\u5e8f\u5217\uff0c\u91c7\u7528GCN\u548cR-GCN\u7b49\u6d88\u606f\u4f20\u9012\u673a\u5236\u6355\u83b7\u86cb\u767d\u8d28\u7ed3\u6784\u7684\u51e0\u4f55\u7279\u5f81\uff0c\u901a\u8fc7\u5806\u53e0\u5377\u79ef\u5c42\u5b66\u4e60\u7a7a\u95f4\u56fe\u7684\u7ec4\u5408\u4fe1\u606f\u3002", "result": "\u5728NetSurfP-2.0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSSRGNet\u6a21\u578b\u57283\u6001\u548c8\u6001\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u7684f1\u5206\u6570\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5229\u7528\u86cb\u767d\u8d28\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u86cb\u767d\u8d28\u4e8c\u7ea7\u7ed3\u6784\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.13702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13702", "abs": "https://arxiv.org/abs/2511.13702", "authors": ["Luyao Niu", "Nuoxian Huang"], "title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification", "comment": null, "summary": "Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.", "AI": {"tldr": "\u63d0\u51fa\u4e86ST-ProC\u6846\u67b6\uff0c\u4e00\u79cd\u56fe\u539f\u578b\u591a\u76ee\u6807\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3GPS\u8f68\u8ff9\u51fa\u884c\u6a21\u5f0f\u8bc6\u522b\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u534721.5%\u3002", "motivation": "GPS\u8f68\u8ff9\u51fa\u884c\u6a21\u5f0f\u8bc6\u522b\u9762\u4e34\u6807\u6ce8\u6210\u672c\u9ad8\u5bfc\u81f4\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u786e\u8ba4\u504f\u5dee\u4e14\u5ffd\u7565\u6570\u636e\u6d41\u5f62\u7ed3\u6784\u3002", "method": "\u7ed3\u5408\u56fe\u6b63\u5219\u5316\u3001\u539f\u578b\u951a\u5b9a\u548c\u8fb9\u754c\u611f\u77e5\u4f2a\u6807\u7b7e\u7b56\u7565\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u8f85\u4ee5\u5bf9\u6bd4\u5b66\u4e60\u548c\u5e08\u751f\u4e00\u81f4\u6027\u635f\u5931\u7684\u652f\u6301\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7a00\u758f\u6807\u7b7e\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u76f8\u6bd4FixMatch\u7b49\u6700\u5148\u8fdb\u65b9\u6cd5\u6027\u80fd\u63d0\u534721.5%\u3002", "conclusion": "ST-ProC\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u6570\u636e\u6d41\u5f62\u7ed3\u6784\u548c\u566a\u58f0\u6291\u5236\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u51fa\u884c\u6a21\u5f0f\u8bc6\u522b\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2511.13338", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13338", "abs": "https://arxiv.org/abs/2511.13338", "authors": ["Yunze Leng", "Rohan Ghosh", "Mehul Motani"], "title": "Tab-PET: Graph-Based Positional Encodings for Tabular Transformers", "comment": null, "summary": "Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u4f4d\u7f6e\u7f16\u7801(PEs)\u53ef\u4ee5\u63d0\u5347\u8868\u683c\u6570\u636e\u4e0aTransformer\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u901a\u8fc7\u964d\u4f4e\u7279\u5f81\u6709\u6548\u79e9\u6765\u7b80\u5316\u4efb\u52a1\u3002\u4f5c\u8005\u63d0\u51fa\u4e86Tab-PET\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u5173\u8054\u548c\u56e0\u679c\u7684\u56fe\u7ed3\u6784\u6765\u4f30\u8ba1PEs\uff0c\u572850\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u8868\u683c\u6570\u636e\u7f3a\u4e4f\u7ed3\u6784\u6027\u7ebf\u7d22\uff0c\u4f20\u7edfTransformer\u6a21\u578b\u5728\u8868\u683c\u6570\u636e\u4e0a\u901a\u5e38\u4e0d\u4f7f\u7528\u4f4d\u7f6e\u7f16\u7801\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u5148\u9a8c\u7ed3\u6784\u4fe1\u606f\u3002\u4f46\u4f5c\u8005\u53d1\u73b0\u7ed3\u6784\u7ebf\u7d22\u7279\u522b\u662f\u4f4d\u7f6e\u7f16\u7801\u53ef\u4ee5\u6539\u5584\u8868\u683cTransformer\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51faTab-PET\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u4f30\u8ba1\u4f4d\u7f6e\u7f16\u7801\u3002\u63a2\u7d22\u4e24\u79cd\u56fe\u4f30\u8ba1\u8303\u5f0f\uff1a\u57fa\u4e8e\u5173\u8054\u7684\u548c\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u3002\u5c06\u56fe\u7ed3\u6784\u4fe1\u606f\u878d\u5165Transformer\u7684\u5d4c\u5165\u8868\u793a\u4e2d\u3002", "result": "\u572850\u4e2a\u5206\u7c7b\u548c\u56de\u5f52\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u56fe\u63a8\u5bfc\u7684\u4f4d\u7f6e\u7f16\u7801\u663e\u8457\u63d0\u5347\u4e863T\u6a21\u578b\u7684\u6027\u80fd\u3002\u57fa\u4e8e\u5173\u8054\u7684\u56fe\u6bd4\u56e0\u679c\u9a71\u52a8\u7684\u56fe\u83b7\u5f97\u66f4\u7a33\u5b9a\u548c\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u4f4d\u7f6e\u7f16\u7801\u5728\u8868\u683cTransformer\u4e2d\u5177\u6709\u610f\u5916\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u53ef\u4ee5\u901a\u8fc7\u964d\u4f4e\u7279\u5f81\u6709\u6548\u79e9\u6765\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u4f4d\u7f6e\u7f16\u7801\u662f\u63d0\u5347\u8868\u683cTransformer\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.13712", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13712", "abs": "https://arxiv.org/abs/2511.13712", "authors": ["Kiana Vu", "\u0130smet Sel\u00e7uk \u00d6zer", "Phung Lai", "Zheng Wu", "Thilanka Munasinghe", "Jennifer Wei"], "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness", "comment": null, "summary": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53ef\u89e3\u91caAI\u5728\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u91ce\u706b\u9884\u6d4b\u4e3a\u4f8b\uff0c\u4f7f\u7528SHAP\u65b9\u6cd5\u63ed\u793a\u6a21\u578b\u51b3\u7b56\u8def\u5f84\u548c\u6f5c\u5728\u504f\u89c1\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u6781\u7aef\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u4e25\u91cd\u6027\uff0c\u9700\u8981\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u64cd\u4f5c\u7684\u9884\u6d4b\u3002\u867d\u7136AI\u6a21\u578b\u5728\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u9650\u5236\u4e86\u5728\u5b9e\u9645\u51b3\u7b56\u4e2d\u7684\u91c7\u7528\uff0c\u5f71\u54cd\u4e86\u4fe1\u4efb\u5ea6\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "method": "\u4f7f\u7528\u91ce\u706b\u9884\u6d4b\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u8bc4\u4f30\u5404\u79cdAI\u6a21\u578b\uff0c\u5e76\u91c7\u7528SHAP\u65b9\u6cd5\u6765\u63ed\u793a\u5173\u952e\u7279\u5f81\u3001\u51b3\u7b56\u8def\u5f84\u548c\u6a21\u578b\u884c\u4e3a\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\u3002\u63d0\u4f9b\u652f\u6301\u6027\u53ef\u89c6\u5316\u6765\u589e\u5f3aXAI\u8f93\u51fa\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5206\u6790\u8868\u660eXAI\u4e0d\u4ec5\u6f84\u6e05\u4e86\u6a21\u578b\u63a8\u7406\uff0c\u8fd8\u652f\u6301\u9886\u57df\u4e13\u5bb6\u548c\u54cd\u5e94\u56e2\u961f\u7684\u5173\u952e\u51b3\u7b56\u3002\u53ef\u89c6\u5316\u589e\u5f3a\u4e86\u7279\u5f81\u91cd\u8981\u6027\u548c\u65f6\u7a7a\u6a21\u5f0f\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0AI\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u6027\uff0c\u8fd8\u9700\u8981\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u8fd9\u5bf9\u4e8e\u707e\u5bb3\u51c6\u5907\u3001\u98ce\u9669\u7f13\u89e3\u548c\u6c14\u5019\u97e7\u6027\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.13339", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13339", "abs": "https://arxiv.org/abs/2511.13339", "authors": ["Han Meng", "Gang Mei", "Hong Tian", "Nengxiong Xu", "Jianbing Peng"], "title": "Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model", "comment": null, "summary": "Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8868\u683c\u57fa\u7840\u6a21\u578b\u7684\u7b80\u5355\u800c\u7a33\u5065\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7edf\u8ba1\u51c6\u786e\u7684\u5ca9\u4f53\u4e0d\u8fde\u7eed\u9762\u751f\u6210\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e13\u95e8\u4e3a\u5c0f\u6570\u636e\u8bbe\u8ba1\u7684\u57fa\u5ea7\u6a21\u578b\u7684\u5f3a\u5927\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u5728\u6709\u9650\u7684\u6d4b\u91cf\u4e0d\u8fde\u7eed\u9762\u4e2d\u6709\u6548\u6355\u6349\u6f5c\u5728\u7684\u590d\u6742\u5206\u5e03\u6a21\u5f0f\u3002", "motivation": "\u5ca9\u4f53\u4e0d\u8fde\u7eed\u9762\u5bf9\u5ca9\u4f53\u529b\u5b66\u884c\u4e3a\u548c\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5185\u90e8\u5206\u5e03\u5927\u591a\u4e0d\u53ef\u89c2\u6d4b\uff0c\u901a\u5e38\u901a\u8fc7\u8868\u9762\u66b4\u9732\u7684\u4e0d\u8fde\u7eed\u9762\u4f7f\u7528\u751f\u6210\u9884\u6d4b\u65b9\u6cd5\u8fdb\u884c\u63a8\u65ad\u3002\u7136\u800c\uff0c\u8868\u9762\u66b4\u9732\u89c2\u6d4b\u672c\u8d28\u4e0a\u7a00\u758f\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u6355\u6349\u6f5c\u5728\u7684\u590d\u6742\u5206\u5e03\u6a21\u5f0f\uff0c\u8981\u4e48\u5728\u6570\u636e\u7a00\u758f\u6761\u4ef6\u4e0b\u7f3a\u4e4f\u7a33\u5065\u6027\u3002", "method": "\u5229\u7528\u4e13\u95e8\u4e3a\u5c0f\u6570\u636e\u8bbe\u8ba1\u7684\u8868\u683c\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5176\u5f3a\u5927\u7684\u6837\u672c\u5b66\u4e60\u80fd\u529b\u6765\u6355\u6349\u6709\u9650\u6d4b\u91cf\u4e0d\u8fde\u7eed\u9762\u4e2d\u7684\u590d\u6742\u5206\u5e03\u6a21\u5f0f\u3002", "result": "\u5728\u5341\u4e2a\u5177\u6709\u4e0d\u540c\u89c4\u6a21\u548c\u5206\u5e03\u6a21\u5f0f\u7684\u6570\u636e\u96c6\u4e0a\u7684\u6bd4\u8f83\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u548c\u6df1\u5ea6\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5ca9\u4f53\u7ed3\u6784\u7684\u5b9a\u91cf\u8868\u5f81\uff0c\u652f\u6301\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u6570\u636e\u9a71\u52a8\u5ca9\u571f\u5de5\u7a0b\u8bbe\u8ba1\u3002"}}
{"id": "2511.13419", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.13419", "abs": "https://arxiv.org/abs/2511.13419", "authors": ["Shaheen Mohammed Saleh Ahmed", "Hakan Hakan Guneyli"], "title": "MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction", "comment": null, "summary": "Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data", "AI": {"tldr": "\u63d0\u51faMMWSTM-ADRAN+\u53cc\u6d41\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u5929\u6c14\u72b6\u6001\u8f6c\u6362\u6a21\u578b\u548c\u5f02\u5e38\u9a71\u52a8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u6781\u7aef\u6c14\u6e29\u4e8b\u4ef6\u9884\u6d4b", "motivation": "\u51c6\u786e\u9884\u6d4b\u6781\u7aef\u6c14\u6e29\u4e8b\u4ef6\u5bf9\u6c14\u5019\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u77ed\u7a0b\u9884\u6d4b\u4e2d\u4ecd\u9762\u4e34\u6311\u6218", "method": "\u53cc\u6d41\u67b6\u6784\uff1aMMWSTM\u6d41\u4f7f\u7528BiLSTM\u548c\u53ef\u5b66\u4e60\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u6355\u6349\u5929\u6c14\u72b6\u6001\u53d8\u5316\uff1bADRAN\u6d41\u4f7f\u7528BiGRU\u3001\u591a\u5934\u81ea\u6ce8\u610f\u529b\u548c\u5f02\u5e38\u653e\u5927\u5c42\u589e\u5f3a\u5bf9\u4f4e\u6982\u7387\u4fe1\u53f7\u7684\u654f\u611f\u6027\uff1b\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u878d\u5408\u95e8\u81ea\u9002\u5e94\u786e\u5b9a\u5404\u6d41\u8d21\u732e\uff1b\u4f7f\u7528ExtremeWeatherLoss\u51fd\u6570\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u589e\u5f3a\u4f18\u5316", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u8026\u5408\u5929\u6c14\u72b6\u6001\u52a8\u6001\u5efa\u6a21\u548c\u5f02\u5e38\u4fe1\u53f7\u589e\u5f3a\u673a\u5236\uff0c\u4e3a\u6781\u7aef\u6c14\u6e29\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.13453", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.13453", "abs": "https://arxiv.org/abs/2511.13453", "authors": ["Iulius Gherasim", "Carlos Garc\u00eda S\u00e1nchez"], "title": "Hardware optimization on Android for inference of AI models", "comment": "8 pages", "summary": "The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.", "AI": {"tldr": "\u7814\u7a76\u5728Android\u7cfb\u7edf\u4e0aAI\u6a21\u578b\u7684\u6700\u4f18\u6267\u884c\u914d\u7f6e\uff0c\u91cd\u70b9\u5173\u6ce8\u76ee\u6807\u68c0\u6d4b(YOLO)\u548c\u56fe\u50cf\u5206\u7c7b(ResNet)\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e0d\u540c\u91cf\u5316\u65b9\u6848\u548c\u8bbe\u5907\u52a0\u901f\u5668(GPU/NPU)\u7684\u4f7f\u7528\u6548\u679c\u3002", "motivation": "\u79fb\u52a8\u8ba1\u7b97\u4e2dAI\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u6700\u5c0f\u5316\u5ef6\u8fdf\u548c\u9ad8\u54cd\u5e94\u6027\uff0c\u4f46\u9762\u4e34\u5b9e\u65f6\u7ea6\u675f\u548c\u5f02\u6784\u786c\u4ef6\u67b6\u6784\u7684\u6311\u6218\uff0c\u9700\u8981\u627e\u5230\u6700\u4f18\u6267\u884c\u7b56\u7565\u3002", "method": "\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u91cf\u5316\u65b9\u6848\uff0c\u5229\u7528\u8bbe\u5907\u52a0\u901f\u5668(GPU\u548cNPU)\uff0c\u5728Android\u7cfb\u7edf\u4e0a\u5bf9YOLO\u76ee\u6807\u68c0\u6d4b\u548cResNet\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u8fdb\u884c\u914d\u7f6e\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u786e\u5b9a\u5728\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931\u548c\u6700\u5927\u63a8\u7406\u52a0\u901f\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u7684\u7ec4\u5408\u914d\u7f6e\u3002", "conclusion": "\u627e\u5230\u4e86\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72AI\u6a21\u578b\u65f6\uff0c\u91cf\u5316\u65b9\u6848\u4e0e\u786c\u4ef6\u52a0\u901f\u5668\u7684\u6700\u4f73\u7ec4\u5408\u914d\u7f6e\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u7cbe\u5ea6\u7684\u6700\u4f18\u6743\u8861\u3002"}}
{"id": "2511.13469", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13469", "abs": "https://arxiv.org/abs/2511.13469", "authors": ["Shiyuan Luo", "Chonghao Qiu", "Runlong Yu", "Yiqun Xie", "Xiaowei Jia"], "title": "GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction", "comment": null, "summary": "Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.", "AI": {"tldr": "\u63d0\u51fa\u4e86GREAT\u6846\u67b6\uff0c\u901a\u8fc7\u8f85\u52a9\u53d8\u6362\u589e\u5f3a\u73af\u5883\u5efa\u6a21\u6570\u636e\uff0c\u89e3\u51b3\u672a\u76d1\u6d4b\u533a\u57df\u9884\u6d4b\u95ee\u9898\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73af\u5883\u5efa\u6a21\u9762\u4e34\u672a\u76d1\u6d4b\u533a\u57df\u9884\u6d4b\u6311\u6218\uff0c\u7531\u4e8e\u89c2\u6d4b\u6570\u636e\u6709\u9650\u4e14\u5730\u7406\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u52a0\u4e0a\u7a7a\u95f4\u5f02\u8d28\u6027\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u865a\u5047\u6a21\u5f0f\u3002\u9700\u8981\u4fdd\u6301\u7269\u7406\u5173\u7cfb\u4e0d\u53d8\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "GREAT\u6846\u67b6\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u591a\u5c42\u7684\u53d8\u6362\u51fd\u6570\uff0c\u589e\u5f3a\u539f\u59cb\u73af\u5883\u7279\u5f81\u548c\u65f6\u95f4\u5f71\u54cd\uff0c\u901a\u8fc7\u53cc\u5c42\u8bad\u7ec3\u8fc7\u7a0b\u7ea6\u675f\u589e\u5f3a\u6570\u636e\u4fdd\u7559\u6e90\u6570\u636e\u5173\u952e\u6a21\u5f0f\u3002", "result": "\u5728\u7f8e\u56fd\u4e1c\u90e8\u516d\u4e2a\u751f\u6001\u591a\u6837\u5316\u6d41\u57df\u7684\u6cb3\u6d41\u6e29\u5ea6\u9884\u6d4b\u5b9e\u9a8c\u4e2d\uff0cGREAT\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u73af\u5883\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5168\u9762\u76d1\u6d4b\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2511.13497", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13497", "abs": "https://arxiv.org/abs/2511.13497", "authors": ["Liudmila A. Zhukas", "Vivian Ni Zhang", "Qiang Miao", "Qingfeng Wang", "Marko Cetina", "Jungsang Kim", "Lawrence Carin", "Christopher Monroe"], "title": "Quantum Machine Learning via Contrastive Training", "comment": "7 figures, 20 pages total", "summary": "Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u8868\u793a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u53ef\u7f16\u7a0b\u79bb\u5b50\u9631\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u7f16\u7801\u56fe\u50cf\u4e3a\u91cf\u5b50\u6001\u5e76\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u6027\u589e\u52a0\u65f6\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u53ef\u7f16\u7a0b\u79bb\u5b50\u9631\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b0\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b66\u4e60\u4e0d\u53d8\u6027\uff0c\u7136\u540e\u8fdb\u884c\u5fae\u8c03\u5206\u7c7b\u3002", "result": "\u76f8\u6bd4\u968f\u673a\u521d\u59cb\u5316\u6a21\u578b\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u7684\u8fd0\u884c\u95f4\u53d8\u5f02\u6027\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u91cf\u5b50\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u6761\u6807\u6ce8\u6548\u7387\u9ad8\u7684\u8def\u5f84\uff0c\u5bf9\u91cf\u5b50\u539f\u751f\u6570\u636e\u96c6\u5177\u6709\u76f4\u63a5\u76f8\u5173\u6027\uff0c\u5e76\u4e3a\u5904\u7406\u66f4\u5927\u89c4\u6a21\u7ecf\u5178\u8f93\u5165\u63d0\u4f9b\u4e86\u6e05\u6670\u8def\u5f84\u3002"}}
{"id": "2511.13514", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.13514", "abs": "https://arxiv.org/abs/2511.13514", "authors": ["Pragatheeswaran Vipulananthan", "Kamal Premaratne", "Dilip Sarkar", "Manohar N. Murthi"], "title": "A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data", "comment": "IEEE International Conference on Knowledge Graph (ICKG), 378-387, 2024", "summary": "Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{\u00f6}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box\" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u7269\u7406\u5b66\u7684\u767d\u76d2\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u6838\u5747\u503c\u5d4c\u5165\u6620\u5c04\u5230\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u6784\u5efa\u5f20\u91cf\u7f51\u7edc\u542f\u53d1\u76841D\u81ea\u65cb\u94fe\u54c8\u5bc6\u987f\u91cf\uff0c\u5e76\u6c42\u89e3\u859b\u5b9a\u8c14\u65b9\u7a0b\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u9ed1\u76d2\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6982\u7387\u767d\u76d2\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u6838\u5747\u503c\u5d4c\u5165\u6620\u5c04\u5230\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u6784\u5efa\u5f20\u91cf\u7f51\u7edc\u542f\u53d1\u76841D\u81ea\u65cb\u94fe\u54c8\u5bc6\u987f\u91cf\uff0c\u6c42\u89e3\u859b\u5b9a\u8c14\u65b9\u7a0b\u5e76\u5e94\u7528\u5fae\u6270\u7406\u8bba\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u53d8\u5316\u70b9\u68c0\u6d4b\u548c\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u767d\u76d2\u6a21\u578b\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4e3a\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u91cf\u5b50\u7269\u7406\u542f\u53d1\u7684\u767d\u76d2\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u767d\u76d2\u6a21\u578b\u3002"}}
{"id": "2511.13527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13527", "abs": "https://arxiv.org/abs/2511.13527", "authors": ["Ihab Asaad", "Maha Shadaydeh", "Joachim Denzler"], "title": "Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images", "comment": "Accepted at EurIPS 2025 Workshop: Unifying Perspectives on Learning Biases (UPLB)", "summary": "Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u5757\u72b6\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\uff0c\u53d1\u73b0\u80bf\u7624\u68c0\u6d4b\u4e2d\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\uff08\u80bf\u7624\u5757\u7ec4\u7ec7\u533a\u57df\u66f4\u5927\uff09\uff0c\u63d0\u51fa\u4f7f\u7528GERNE\u53bb\u504f\u65b9\u6cd5\u63d0\u5347\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u7ea67%\u3002", "motivation": "\u5757\u72b6\u591a\u6807\u7b7e\u5206\u7c7b\u80fd\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3001\u7b80\u5316\u8bad\u7ec3\uff0c\u4f46\u5728\u80bf\u7624\u68c0\u6d4b\u4e2d\u53d1\u73b0\u865a\u5047\u76f8\u5173\u6027\uff1a\u80bf\u7624\u5757\u901a\u5e38\u5305\u542b\u66f4\u5927\u7ec4\u7ec7\u533a\u57df\uff0c\u800c\u975e\u80bf\u7624\u5757\u591a\u4e3a\u80cc\u666f\u3002\u8fd9\u79cd\u76f8\u5173\u6027\u4f1a\u5bfc\u81f4\u6a21\u578b\u9884\u6d4b\u504f\u5dee\u3002", "method": "\u91c7\u7528GERNE\u53bb\u504f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u9002\u5e94\u6027\u5730\u6700\u5927\u5316\u6700\u5dee\u7ec4\u51c6\u786e\u7387\uff08WGA\uff09\uff0c\u7528\u4e8e\u7f13\u89e3\u5757\u7ec4\u6210\u4e0e\u6807\u7b7e\u4e4b\u95f4\u7684\u865a\u5047\u76f8\u5173\u6027\u5f71\u54cd\u3002", "result": "\u76f8\u6bd4\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\uff0cGERNE\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0d\u540c\u9608\u503c\u4e0b\u5c06\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u63d0\u5347\u4e86\u7ea67%\uff0c\u663e\u8457\u6539\u5584\u4e86\u5173\u952e\u5c11\u6570\u60c5\u51b5\uff08\u5c0f\u7ec4\u7ec7\u80bf\u7624\u5757\u548c\u5927\u7ec4\u7ec7\u975e\u80bf\u7624\u5757\uff09\u7684\u6027\u80fd\u3002", "conclusion": "\u5757\u72b6\u5206\u7c7b\u95ee\u9898\u4e2d\u9700\u8981\u8003\u8651\u865a\u5047\u76f8\u5173\u6027\u611f\u77e5\u5b66\u4e60\uff0c\u53bb\u504f\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u5173\u952e\u5c11\u6570\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\uff0c\u786e\u4fdd\u66f4\u53ef\u9760\u7684\u80bf\u7624\u68c0\u6d4b\u3002"}}
{"id": "2511.13541", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13541", "abs": "https://arxiv.org/abs/2511.13541", "authors": ["Yue Hou", "Ruomei Liu", "Yingke Su", "Junran Wu", "Ke Xu"], "title": "Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries", "comment": "Accepted by AAAI 2026 (The 40th Annual AAAI Conference on Artificial Intelligence)", "summary": "A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.", "AI": {"tldr": "BaCa\u662f\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u56fe\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u52a8\u6001\u5b57\u5178\u548c\u8fb9\u754c\u611f\u77e5\u5206\u6570\u6821\u51c6\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u53ea\u4f18\u5316\u5206\u5e03\u5185\u7279\u5f81\uff0c\u96be\u4ee5\u51c6\u786e\u8868\u793a\u5206\u5e03\u8fb9\u754c\uff0c\u4e14\u56fe\u6570\u636e\u7684\u6f5c\u5728\u591a\u56e0\u7d20\u7ed3\u6784\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u56fe\u8bba\u4f30\u8ba1\u548c\u6df7\u5408\u7b56\u7565\u751f\u6210\u8fb9\u754c\u611f\u77e5\u62d3\u6251\uff0c\u6784\u5efa\u53cc\u52a8\u6001\u5b57\u5178\u6355\u83b7\u6f5c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8868\u793a\uff0c\u8fdb\u884c\u8fb9\u754c\u611f\u77e5\u5206\u6570\u6821\u51c6\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBaCa\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "BaCa\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u6821\u51c6\u6709\u6548\u63d0\u5347\u4e86\u56fe\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u6216\u4f7f\u7528\u8f85\u52a9\u5f02\u5e38\u6570\u636e\u96c6\u3002"}}
{"id": "2511.13561", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13561", "abs": "https://arxiv.org/abs/2511.13561", "authors": ["Shihao Dong", "Yue Liu", "Xiaotong Zhou", "Yuhui Zheng", "Huiying Xu", "Xinzhong Zhu"], "title": "RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise", "comment": null, "summary": "Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.", "AI": {"tldr": "\u63d0\u51faRAC-DMVC\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9760\u6027\u56fe\u5f15\u5bfc\u591a\u6e90\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3\u591a\u89c6\u56fe\u805a\u7c7b\u4e2d\u7684\u7f3a\u5931\u566a\u58f0\u548c\u89c2\u6d4b\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u589e\u5f3a\u591a\u89c6\u56fe\u805a\u7c7b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u89e3\u51b3\u66f4\u5177\u6311\u6218\u6027\u7684\u591a\u6e90\u566a\u58f0\u95ee\u9898\uff0c\u5305\u62ec\u7f3a\u5931\u566a\u58f0\u548c\u89c2\u6d4b\u566a\u58f0\u3002", "method": "\u6784\u5efa\u53ef\u9760\u6027\u56fe\u6307\u5bfc\u9c81\u68d2\u8868\u793a\u5b66\u4e60\uff1a1) \u4ea4\u53c9\u89c6\u56fe\u91cd\u5efa\u5904\u7406\u89c2\u6d4b\u566a\u58f0\uff1b2) \u53ef\u9760\u6027\u611f\u77e5\u566a\u58f0\u5bf9\u6bd4\u5b66\u4e60\u7f13\u89e3\u566a\u58f0\u8868\u793a\u5e26\u6765\u7684\u6b63\u8d1f\u5bf9\u9009\u62e9\u504f\u5dee\uff1b3) \u53cc\u91cd\u6ce8\u610f\u529b\u63d2\u8865\u5904\u7406\u7f3a\u5931\u566a\u58f0\uff1b4) \u81ea\u76d1\u7763\u805a\u7c7b\u84b8\u998f\u6a21\u5757\u4f18\u5316\u8868\u793a\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAC-DMVC\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u566a\u58f0\u6bd4\u4f8b\u4e0b\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "RAC-DMVC\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u591a\u6e90\u566a\u58f0\u73af\u5883\u4e0b\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u95ee\u9898\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.13637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13637", "abs": "https://arxiv.org/abs/2511.13637", "authors": ["Ana Durica", "John Booth", "Ivana Drobnjak"], "title": "Towards Multimodal Representation Learning in Paediatric Kidney Disease", "comment": "4 pages, 3 figures. EurIPS 2025 Multimodal Representation Learning for Healthcare (MMRL4H) workshop paper", "summary": "Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.", "AI": {"tldr": "\u4f7f\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\uff0c\u901a\u8fc7\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u513f\u7ae5\u672a\u676530\u5929\u5185\u662f\u5426\u4f1a\u51fa\u73b0\u5f02\u5e38\u8840\u6e05\u808c\u9150\u503c\u3002", "motivation": "\u513f\u79d1\u80be\u810f\u75be\u75c5\u8868\u73b0\u548c\u8fdb\u5c55\u5dee\u5f02\u5927\uff0c\u9700\u8981\u6301\u7eed\u76d1\u6d4b\u80be\u529f\u80fd\u3002", "method": "\u6574\u5408\u7eb5\u5411\u5b9e\u9a8c\u5ba4\u5e8f\u5217\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\u7684\u65f6\u95f4\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u521d\u6b65\u8bc1\u660e\u7b80\u5355\u7684\u65f6\u95f4\u8868\u793a\u53ef\u4ee5\u6355\u6349\u5e38\u89c4\u513f\u79d1\u6570\u636e\u4e2d\u7684\u6709\u7528\u6a21\u5f0f\u3002", "conclusion": "\u4e3a\u672a\u6765\u4f7f\u7528\u989d\u5916\u4e34\u5e8a\u4fe1\u53f7\u548c\u66f4\u8be6\u7ec6\u80be\u810f\u7ed3\u679c\u7684\u591a\u6a21\u6001\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.13645", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13645", "abs": "https://arxiv.org/abs/2511.13645", "authors": ["Aleksandar Stankovi\u0107"], "title": "FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs", "comment": "15 pages. Code and reproducibility scripts: https://github.com/SV25-22/FuseSampleAgg", "summary": "We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.", "AI": {"tldr": "FuseSampleAgg\u662f\u4e00\u4e2aCUDA\u7b97\u5b50\uff0c\u5c06GraphSAGE\u7684\u90bb\u5c45\u91c7\u6837\u548c\u5747\u503c\u805a\u5408\u878d\u5408\u4e3a\u5355\u6b21\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u4f20\u7edfGraphSAGE\u5b9e\u73b0\u9700\u8981\u591a\u6b21\u5185\u6838\u542f\u52a8\u548c\u4e2d\u95f4\u7ed3\u679c\u5b58\u50a8\uff0c\u5bfc\u81f4\u5185\u5b58\u6d41\u91cf\u5927\u3001\u6027\u80fd\u5f00\u9500\u9ad8\u3002", "method": "\u901a\u8fc7\u878d\u5408\u90bb\u5c45\u91c7\u6837\u548c\u5747\u503c\u805a\u5408\u4e3a\u5355\u6b21\u64cd\u4f5c\uff0c\u6d88\u9664\u5757\u7269\u5316\u548c\u989d\u5916\u5185\u6838\u542f\u52a8\uff0c\u901a\u8fc7\u7d22\u5f15\u91cd\u653e\u4fdd\u6301GraphSAGE\u5747\u503c\u8bed\u4e49\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u9ad851\u500d\u901f\u5ea6\u63d0\u5347\u548c100\u500d\u5185\u5b58\u51cf\u5c11\uff0c\u7b97\u5b50\u5177\u6709\u786e\u5b9a\u6027\u4e14\u4e0ePyTorch\u4f18\u5316\u5668\u517c\u5bb9\u3002", "conclusion": "FuseSampleAgg\u901a\u8fc7\u64cd\u4f5c\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e86GraphSAGE\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.13654", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13654", "abs": "https://arxiv.org/abs/2511.13654", "authors": ["Pascal Zimmer", "Ghassan Karame"], "title": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning", "comment": "To appear in the Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) 2026", "summary": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u4f18\u5316\u8d85\u53c2\u6570\uff08\u5b66\u4e60\u7387\u3001\u6743\u91cd\u8870\u51cf\u3001\u52a8\u91cf\u3001\u6279\u5927\u5c0f\uff09\u5bf9\u8fc1\u79fb\u653b\u51fb\u548c\u67e5\u8be2\u653b\u51fb\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u4e0d\u540c\u653b\u51fb\u7c7b\u578b\u4e0b\u5b66\u4e60\u7387\u8c03\u6574\u65b9\u5411\u76f8\u53cd\uff0c\u5e76\u63a2\u7d22\u4e86\u540c\u65f6\u589e\u5f3a\u4e24\u79cd\u653b\u51fb\u9c81\u68d2\u6027\u7684\u8d85\u53c2\u6570\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u7814\u7a76\u4f18\u5316\u8d85\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u96c6\u4e2d\u8bad\u7ec3\u3001\u96c6\u6210\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u7b49\u591a\u79cd\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e0b\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4e0d\u540c\u4f18\u5316\u8d85\u53c2\u6570\u5bf9\u8fc1\u79fb\u653b\u51fb\u548c\u67e5\u8be2\u653b\u51fb\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5b66\u4e60\u7387\u8c03\u6574\u5bf9\u4e24\u79cd\u653b\u51fb\u7c7b\u578b\u4ea7\u751f\u76f8\u53cd\u6548\u679c\uff1a\u964d\u4f4e\u5b66\u4e60\u7387\u53ef\u63d0\u5347\u8fc1\u79fb\u653b\u51fb\u9c81\u68d2\u6027\u8fbe64%\uff0c\u800c\u63d0\u9ad8\u5b66\u4e60\u7387\u53ef\u589e\u5f3a\u67e5\u8be2\u653b\u51fb\u9c81\u68d2\u6027\u8fbe28%\u3002\u5206\u5e03\u5f0f\u6a21\u578b\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u80fd\u6700\u6709\u6548\u5730\u540c\u65f6\u7f13\u89e3\u4e24\u79cd\u653b\u51fb\u3002", "conclusion": "\u4f18\u5316\u8d85\u53c2\u6570\u662f\u63d0\u5347\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u578b\u5728\u8d85\u53c2\u6570\u8c03\u4f18\u4e0b\u80fd\u5b9e\u73b0\u6700\u4f73\u7684\u53cc\u91cd\u653b\u51fb\u9632\u62a4\u6548\u679c\u3002"}}
{"id": "2511.13680", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13680", "abs": "https://arxiv.org/abs/2511.13680", "authors": ["Leopoldo Agorio", "Juan Cervi\u00f1o", "Miguel Calvo-Fullana", "Alejandro Ribeiro", "Juan Andr\u00e9s Bazerque"], "title": "Cross-Learning from Scarce Data via Multi-Task Constrained Optimization", "comment": "13 pages, 11 figures", "summary": "A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \\emph{cross-learning} framework to overcome data scarcity by jointly estimating \\emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.", "AI": {"tldr": "\u63d0\u51fa\u591a\u4efb\u52a1\u4ea4\u53c9\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u591a\u4e2a\u76f8\u5173\u4efb\u52a1\u7684\u786e\u5b9a\u6027\u53c2\u6570\u6765\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u6570\u636e\u4e30\u5bcc\u7684\u4efb\u52a1\u5411\u6570\u636e\u7a00\u7f3a\u4efb\u52a1\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u5f53\u76d1\u7763\u6570\u636e\u6709\u9650\u65f6\uff0c\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6848\u4f8b\u3002\u9700\u8981\u514b\u670d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u53c2\u6570\u63a8\u65ad\u5bf9\u6709\u9650\u6570\u636e\u654f\u611f\u7684\u5173\u952e\u573a\u666f\u4e2d\u3002", "method": "\u5c06\u8054\u5408\u4f30\u8ba1\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7ea6\u675f\u63a7\u5236\u4e0d\u540c\u6a21\u578b\u53c2\u6570\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5141\u8bb8\u53c2\u6570\u8de8\u4efb\u52a1\u5dee\u5f02\u4f46\u7ed3\u5408\u591a\u4e2a\u6570\u636e\u6e90\u7684\u4fe1\u606f\u3002", "result": "\u5728\u7406\u8bba\u6846\u67b6\u4e2d\u63d0\u4f9b\u9ad8\u65af\u6570\u636e\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u4f20\u67d3\u75c5\u4f20\u64ad\u7b49\u771f\u5b9e\u6570\u636e\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u4ea4\u53c9\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4ea4\u53c9\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u4ece\u6570\u636e\u4e30\u5bcc\u7684\u4efb\u52a1\u5411\u6570\u636e\u7a00\u7f3a\u4efb\u52a1\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ea7\u751f\u66f4\u51c6\u786e\u53ef\u9760\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u4e3a\u6709\u9650\u6570\u636e\u4e0b\u7684\u53c2\u6570\u63a8\u65ad\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13701", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13701", "abs": "https://arxiv.org/abs/2511.13701", "authors": ["Gianluigi Pillonetto", "Alberto Giaretta", "Mauro Bisiacco"], "title": "Learning stochasticity: a nonparametric framework for intrinsic noise estimation", "comment": null, "summary": "Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.", "AI": {"tldr": "Trine\u662f\u4e00\u4e2a\u975e\u53c2\u6570\u3001\u57fa\u4e8e\u6838\u7684\u4e09\u9636\u6bb5\u56de\u5f52\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63a8\u65ad\u72b6\u6001\u4f9d\u8d56\u7684\u5185\u5728\u566a\u58f0\uff0c\u7279\u522b\u9002\u7528\u4e8e\u751f\u7269\u548c\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u3002", "motivation": "\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u548c\u968f\u673a\u6548\u5e94\u7684\u4e0d\u5b8c\u5168\u77e5\u8bc6\u4f7f\u5f97\u81ea\u4e0b\u800c\u4e0a\u7684\u5efa\u6a21\u65b9\u6cd5\u5e38\u5e38\u65e0\u6548\uff0c\u7279\u522b\u662f\u5728\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u548c\u4fe1\u53f7\u901a\u8def\u7b49\u590d\u6742\u7cfb\u7edf\u4e2d\uff0c\u5185\u5728\u566a\u58f0\u7684\u5efa\u6a21\u5bf9\u4e8e\u7406\u89e3\u7cfb\u7edf\u52a8\u6001\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002", "method": "Trine\u91c7\u7528\u4e09\u9636\u6bb5\u7b97\u6cd5\uff0c\u7ed3\u5408\u53ef\u89e3\u6790\u6c42\u89e3\u7684\u5b50\u95ee\u9898\u548c\u7ed3\u6784\u5316\u6838\u67b6\u6784\uff0c\u80fd\u591f\u540c\u65f6\u6355\u6349\u7a81\u53d1\u7684\u566a\u58f0\u9a71\u52a8\u6ce2\u52a8\u548c\u5e73\u6ed1\u7684\u72b6\u6001\u4f9d\u8d56\u65b9\u5dee\u53d8\u5316\u3002", "result": "\u5728\u751f\u7269\u548c\u751f\u6001\u7cfb\u7edf\u7684\u9a8c\u8bc1\u4e2d\uff0cTrine\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u53c2\u6570\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u63ed\u793a\u9690\u85cf\u52a8\u6001\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u7406\u60f3\u89c2\u6d4b\u8005\u76f8\u5ab2\u7f8e\u3002", "conclusion": "Trine\u6846\u67b6\u4e3a\u7406\u89e3\u5185\u5728\u566a\u58f0\u5982\u4f55\u5f71\u54cd\u590d\u6742\u7cfb\u7edf\u884c\u4e3a\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7f3a\u4e4f\u5f3a\u5148\u9a8c\u77e5\u8bc6\u7684\u53c2\u6570\u6a21\u578b\u96be\u4ee5\u5904\u7406\u7684\u60c5\u51b5\u3002"}}
{"id": "2511.13705", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.13705", "abs": "https://arxiv.org/abs/2511.13705", "authors": ["Alaa Mezghiche"], "title": "Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering", "comment": "16 pages", "summary": "Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI \"Gene Expression Cancer RNA-Seq\" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.", "AI": {"tldr": "\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u548c\u805a\u7c7b\u5206\u6790\u5728KIRC\u764c\u75c7\u4e2d\u53d1\u73b0\u4e86\u4e00\u4e2a\u7f55\u89c1\u4f46\u7a33\u5b9a\u7684\u5206\u5b50\u4e9a\u578b\uff0c\u800c\u6cdb\u764c\u5206\u6790\u4e3b\u8981\u53d7\u7ec4\u7ec7\u6765\u6e90\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u9ad8\u7ef4RNA-seq\u6570\u636e\u4e2d\u7684\u7f55\u89c1\u4f46\u53ef\u91cd\u590d\u7684\u57fa\u56e0\u7ec4\u4e9a\u578b\uff0c\u8d85\u8d8a\u6807\u51c6\u6807\u7b7e\u7684\u5206\u7c7b\u3002", "method": "\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u8868\u793a\u4e0e\u805a\u7c7b\u548c\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u5728KIRC\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u9ad8\u53d8\u57fa\u56e0\uff0c\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\uff0c\u8fd0\u884ck-means\u805a\u7c7b\uff0c\u5e76\u901a\u8fc7\u7a33\u5b9a\u6027\u6807\u51c6\u8bc6\u522b\u7f55\u89c1\u4e9a\u578b\u3002", "result": "\u5728KIRC\u4e2d\u53d1\u73b0\u4e86\u4e00\u4e2a\u7f55\u89c1\u4e9a\u578bC0\uff08\u53606.85%\u60a3\u8005\uff09\uff0c\u5177\u6709\u9ad8\u5ea6\u7a33\u5b9a\u6027\uff08Jaccard = 0.787\uff09\uff0c\u800c\u6cdb\u764c\u5206\u6790\u4e3b\u8981\u53cd\u6620\u7ec4\u7ec7\u6765\u6e90\u5dee\u5f02\u3002", "conclusion": "\u6cdb\u764c\u805a\u7c7b\u53d7\u7ec4\u7ec7\u6765\u6e90\u4e3b\u5bfc\uff0c\u800c\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u764c\u75c7\u5185\u5206\u6790\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u7f55\u89c1\u3001\u53ef\u91cd\u590d\u7684\u5206\u5b50\u4e9a\u578b\u3002"}}
