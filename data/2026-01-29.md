<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [eess.SY](#eess.SY) [Total: 10]
- [cs.AI](#cs.AI) [Total: 25]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 113]
- [econ.EM](#econ.EM) [Total: 3]
- [cs.CY](#cs.CY) [Total: 8]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [math.OC](#math.OC) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text](https://arxiv.org/abs/2601.19913)
*Shinwoo Park,Yo-Sub Han*

Main category: cs.CL

TL;DR: 通过结构化校准，专家可以学会区分人类写作的韩语文本和流畅的LLM输出，准确率从60%提升到100%。


<details>
  <summary>Details</summary>
Motivation: 区分人类写作的韩语文本和流畅的LLM输出对于语言专家来说也很困难，他们可能过度信任表面的规范性。研究专家检测是否可以通过结构化校准作为可学习的技能来提升。

Method: 引入LREAD评分标准，基于韩国国家写作标准，针对微观语言特征（如标点可选性、空格行为、语域转换）。采用三阶段纵向盲测协议：第一阶段测量直觉检测，第二阶段强制执行标准级评分并提供明确理由，第三阶段评估在保留的小学作文上的领域专注掌握。

Result: 多数投票准确率从60%提升到100%，评分者间一致性显著提高（Fleiss' kappa: -0.09 → 0.82）。与最先进的LLM检测器相比，经过校准的人类更依赖语言特定的微观诊断特征，这些特征无法被粗略的话语先验很好地捕捉。

Conclusion: 标准支架式的专家判断可以作为非英语环境中自动化检测器的可解释补充。研究发布了完整的评分标准和经过校准的检测特征分类法。

Abstract: Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.

</details>


### [2] [Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments](https://arxiv.org/abs/2601.19914)
*Maxwell Crouse,Ibrahim Abdelaziz,Kshitij Fadnis,Siva Sankalp Patel,Kinjal Basu,Chulaka Gunasekara,Sadhana Kumaravel,Asim Munawar,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: DiGiT-TC：一种无需状态执行环境即可生成多轮工具调用对话合成数据的方法


<details>
  <summary>Details</summary>
Motivation: 现有工具调用对话合成方法通常假设存在状态执行环境来验证交互有效性，但在企业数据安全或工具规范来自多源等实际场景中，这种状态环境往往不可用

Method: 提出DiGiT-TC数据生成方法，通过新颖的生成模式在用户请求中隐式表示某些工具调用，使生成的对话具有在状态环境中通过搜索生成的特征

Result: 在标准工具调用基准测试中验证了该方法，即使在有状态问题设置下，该方法也能带来显著的性能提升

Conclusion: DiGiT-TC填补了无状态环境下生成高质量工具调用对话合成数据的空白，为实际应用场景提供了有效的解决方案

Abstract: Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.

</details>


### [3] [Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication](https://arxiv.org/abs/2601.19915)
*Paul Tarau*

Main category: cs.CL

TL;DR: Arrow Language Model：基于直觉主义逻辑解释的神经架构，将前缀编码为左嵌套蕴含链，通过非交换组合保持顺序，将下一词预测视为假言推理。


<details>
  <summary>Details</summary>
Motivation: 探索基于逻辑原理的神经架构替代方案，特别是从直觉主义逻辑的角度重新解释下一词预测，为Transformer和状态空间模型提供理论替代方案。

Method: 将前缀编码为左嵌套蕴含链，使用非交换组合保持顺序，将下一词预测建模为假言推理，通过Curry-Howard对应将序列处理视为构造性证明扩展，实现低秩神经实现。

Result: 开发出等价于乘法RNN的神经架构，通过Prolog定理证明器验证了交换与非交换序列化、单词与多词预测选择之间的关系，并实现了实用的低秩神经实现。

Conclusion: 从证明论角度将下一词预测解释为嵌套直觉主义蕴含，自然地推导出神经架构，为逻辑驱动的神经模型设计提供了新视角，是Transformer和状态空间模型的有趣替代方案。

Abstract: We introduce the \emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.
  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.

</details>


### [4] [PaperAudit-Bench: Benchmarking Error Detection in Research Papers for Critical Automated Peer Review](https://arxiv.org/abs/2601.19916)
*Songjun Tu,Yiwen Ma,Jiahao Lin,Qichao Zhang,Xiangyuan Lan,Junfeng. Li,Nan Xu,Linjing Li,Dongbin Zhao*

Main category: cs.CL

TL;DR: PaperAudit-Bench：一个包含错误数据集和自动评审框架的系统，用于评估和改进大语言模型在学术论文评审中的错误检测能力，特别是在长上下文和跨章节推理场景下。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能生成流畅的同行评审，但在处理需要跨章节推理的细微实质性问题时，其评估往往缺乏足够的批判性严谨度。需要开发专门的基准来评估和改进模型在长上下文设置下的错误检测能力。

Method: 提出了PaperAudit-Bench系统，包含两个组件：(1) PaperAudit-Dataset：覆盖章节内可识别错误和需要跨章节推理的错误数据集；(2) PaperAudit-Review：结合结构化错误检测和证据感知评审生成的自动评审框架。通过实验评估不同模型在长上下文下的错误检测能力，并展示了通过SFT和RL训练轻量级LLM检测器的可行性。

Result: 实验显示不同模型在错误检测能力上存在显著差异，特别是在长上下文设置下识别错误具有挑战性。相比基线方法，将显式错误检测整合到评审流程中能产生更严格、更具区分度的评估。数据集支持通过SFT和RL训练轻量级LLM检测器，能以较低计算成本实现有效错误检测。

Conclusion: PaperAudit-Bench为评估和改进大语言模型在学术论文评审中的错误检测能力提供了有效工具，特别是在需要跨章节推理的长上下文场景下。显式错误检测能显著提升评审的严格性和区分度，而轻量级检测器的训练方法为实际应用提供了可行的解决方案。

Abstract: Large language models can generate fluent peer reviews, yet their assessments often lack sufficient critical rigor when substantive issues are subtle and distributed across a paper. In this paper, we introduce PaperAudit-Bench, which consists of two components: (1) PaperAudit-Dataset, an error dataset covering both errors identifiable within individual sections and those requiring cross-section reasoning, designed for controlled evaluation under long-context settings; and (2) PaperAudit-Review, an automated review framework that integrates structured error detection with evidence-aware review generation to support critical assessment. Experiments on PaperAudit-Bench reveal large variability in error detectability across models and detection depths, highlighting the difficulty of identifying such errors under long-context settings. Relative to representative automated reviewing baselines, incorporating explicit error detection into the review workflow produces systematically stricter and more discriminative evaluations, demonstrating its suitability for peer review. Finally, we show that the dataset supports training lightweight LLM detectors via SFT and RL, enabling effective error detection at reduced computational cost.

</details>


### [5] [PILOT: Planning via Internalized Latent Optimization Trajectories for Large Language Models](https://arxiv.org/abs/2601.19917)
*Haoyu Zheng,Yun Zhu,Yuqian Yuan,Bo Yuan,Wenqiao Zhang,Siliang Tang,Jun Xiao*

Main category: cs.CL

TL;DR: PILOT框架通过轻量级超网络生成潜在指导向量，将大型教师模型的战略规划能力内化到紧凑LLM中，无需修改主干权重即可显著提升多步推理性能。


<details>
  <summary>Details</summary>
Motivation: 紧凑型大语言模型在多步推理任务中缺乏全局战略规划能力，导致错误传播。虽然LLMs具备潜在推理能力，可通过教师模型的外部指导解锁，但运行时依赖外部指导存在延迟和可用性限制。

Method: 提出PILOT框架：使用轻量级超网络合成查询条件化的潜在指导向量，该向量作为内部引导机制，将模型表示导向最优推理路径，无需修改主干模型权重。

Result: 在数学和编程基准测试中，PILOT有效稳定推理轨迹，显著优于基线方法（如MATH500上提升8.9%），且推理延迟可忽略不计。

Conclusion: PILOT成功将大型模型的战略监督能力内化为紧凑模型的潜在指导，为非侵入式提升LLM多步推理能力提供了有效解决方案。

Abstract: Strategic planning is critical for multi-step reasoning, yet compact Large Language Models (LLMs) often lack the capacity to formulate global strategies, leading to error propagation in long-horizon tasks. Our analysis reveals that LLMs possess latent reasoning capabilities that can be unlocked when conditioned on explicit plans from a teacher model; however, runtime reliance on external guidance is often impractical due to latency and availability constraints. To bridge this gap, we propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework designed to internalize the strategic oversight of large models into intrinsic Latent Guidance. Instead of altering backbone weights, PILOT employs a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector. This vector acts as an internal steering mechanism, guiding the model's representations toward optimal reasoning paths. Extensive experiments on mathematical and coding benchmarks demonstrate that PILOT effectively stabilizes reasoning trajectories, consistently outperforming strong baselines (e.g., +8.9% on MATH500) with negligible inference latency.

</details>


### [6] [Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs](https://arxiv.org/abs/2601.19918)
*Yitong Qiao,Licheng Pan,Yu Mi,Lei Liu,Yue Shen,Fei Sun,Zhixuan Chu*

Main category: cs.CL

TL;DR: 提出LSC（最低跨度置信度）方法，通过滑动窗口评估语义连贯跨度的联合似然，在仅需单次前向传播和输出概率的零样本条件下，有效检测LLM幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法要么需要昂贵的密集采样策略进行一致性检查，要么需要白盒LLM状态，这在常见的API场景中不可用或效率低下。需要一种在最小资源假设下的高效零样本检测方法。

Method: 提出LSC（Lowest Span Confidence）指标，通过滑动窗口机制评估语义连贯跨度的联合似然。通过识别可变长度n-gram中最低边际置信度区域，捕捉与事实不一致性强烈相关的局部不确定性模式。

Result: 在多个SOTA LLM和多样化基准测试上的广泛实验表明，LSC始终优于现有零样本基线，即使在资源受限条件下也能提供强大的检测性能。

Conclusion: LSC能够缓解困惑度的稀释效应和最小令牌概率的噪声敏感性，提供更稳健的事实不确定性估计，为API场景下的高效幻觉检测提供了实用解决方案。

Abstract: Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.

</details>


### [7] [FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition](https://arxiv.org/abs/2601.19919)
*Junseok Lee,Nahoon Kim,Sangyong Lee,Chang-Jae Chun*

Main category: cs.CL

TL;DR: 提出自适应自知识蒸馏（ASKD）方法，通过动态减少对教师模型的依赖来提升学生模型的泛化能力，并将Whisper模型蒸馏为更小的FastWhisper模型


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法中，学生模型可能会继承教师模型的缺点，导致泛化能力下降。需要一种方法来减少对教师模型的依赖，提升学生模型的自我训练能力

Method: 提出自适应自知识蒸馏（ASKD）方法，动态调整对教师模型的依赖程度，结合自知识蒸馏技术来提升学生模型的泛化能力。具体应用中将Whisper模型蒸馏为FastWhisper模型

Result: FastWhisper在词错误率上比教师模型Whisper低1.07%，推理速度相对快5倍，实现了更好的性能和效率

Conclusion: ASKD方法有效解决了传统知识蒸馏中教师模型缺点传递的问题，通过动态调整依赖和自知识蒸馏提升了学生模型的泛化能力，FastWhisper的成功应用证明了该方法的有效性

Abstract: Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.

</details>


### [8] [Demystifying Multi-Agent Debate: The Role of Confidence and Diversity](https://arxiv.org/abs/2601.19921)
*Xiaochen Zhu,Caiqi Zhang,Yizhou Chi,Tom Stafford,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: 多智能体辩论通过引入多样性初始化和置信度调制更新，显著提升LLM性能，超越传统辩论和多数投票


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论(MAD)计算成本高但效果不如简单多数投票，研究发现同质智能体和均匀信念更新下辩论无法可靠改进结果。受人类审议和集体决策启发，需要引入初始观点多样性和置信度沟通机制

Method: 提出两种轻量级干预：1) 多样性感知初始化，选择更多样化的候选答案池，增加辩论开始时正确假设存在的概率；2) 置信度调制辩论协议，智能体表达校准后的置信度，并根据他人置信度调整更新

Result: 理论上证明多样性初始化提高MAD成功先验概率而不改变更新动态，置信度调制更新使辩论系统性地向正确假设漂移。在六个推理型QA基准测试中，方法始终优于传统MAD和多数投票

Conclusion: 将人类审议机制与LLM辩论连接，证明简单、有原则的修改能显著提升辩论效果，为多智能体协作提供新方向

Abstract: Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.

</details>


### [9] [QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks](https://arxiv.org/abs/2601.20731)
*Mae Sosto,Delfina Sol Martinez Pandiani,Laura Hollink*

Main category: cs.CL

TL;DR: LLMs再现社会规范（特别是异性恋顺性别规范），在文本生成中产生可测量的偏见。研究发现MLMs对酷儿标记对象产生最不利的情感、更高毒性和更负面评价，ARLMs部分缓解这些模式，但封闭访问ARLMs对未标记对象产生更多有害输出。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何再现社会规范，特别是异性恋顺性别规范，以及这些规范如何转化为文本生成中的可测量偏见。关注模型是否因对象的性别或性取向信息而产生不同响应。

Method: 将研究对象分为三类：酷儿标记、非酷儿标记和标准化的"未标记"类别。通过四个维度量化表征不平衡：情感、评价、毒性和预测多样性。比较掩码语言模型和自回归语言模型在不同类别上的表现差异。

Result: MLMs对酷儿标记对象产生最不利的情感、更高毒性和更负面评价。ARLMs部分缓解这些模式，但封闭访问ARLMs倾向于对未标记对象产生更多有害输出。模型特性强烈影响偏见的形式和程度。

Conclusion: LLMs再现规范性社会假设，但偏见的形式和程度高度依赖具体模型特性。模型可能重新分配而非消除表征伤害，表明需要更细致地理解模型架构和训练数据如何影响社会偏见的再现。

Abstract: This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized "unmarked" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.

</details>


### [10] [HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue](https://arxiv.org/abs/2601.19922)
*Laya Iyer,Kriti Aggarwal,Sanmi Koyejo,Gail Heyman,Desmond C. Ong,Subhabrata Mukherjee*

Main category: cs.CL

TL;DR: HEART框架首次在相同多轮情感支持对话中直接比较人类与LLM，发现前沿模型在共情感知和一致性上接近或超越人类平均水平，但人类在适应性重构、紧张命名和细微语气转换上仍有优势。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型快速发展，但缺乏在人际情感支持领域与人类能力对比的明确方法。支持性对话需要超越语言流利度的技能，包括情感解读、语气调整和处理抵抗/挫折等时刻。

Method: 引入HEART框架，在相同多轮情感支持对话中配对人类和模型响应，通过盲审人类评估者和LLM-as-judge评估器集合进行评估。评估遵循基于人际沟通科学的五个维度：人类对齐、共情响应、调谐、共鸣和任务遵循。

Result: 1. 多个前沿模型在感知共情和一致性上接近或超越人类平均水平；2. 人类在适应性重构、紧张命名和细微语气转换（尤其在对抗性对话轮次）上保持优势；3. 人类与LLM-as-judge在约80%的成对比较中偏好一致，匹配人类间一致性水平；4. 书面理由强调相似的HEART维度，表明评估标准正在趋同。

Conclusion: HEART将支持性对话重新定义为独立于一般推理或语言流利度的能力维度，为理解模型生成支持与人类社交判断的契合与分歧提供了统一实证基础，并展示了情感对话能力如何随模型规模扩展。

Abstract: Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.

</details>


### [11] [Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation](https://arxiv.org/abs/2601.19923)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Zelin Cao,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.CL

TL;DR: Table-BiEval：基于自监督评估框架的新方法，用于量化评估LLM将自然语言转换为结构化格式的能力，无需人工干预


<details>
  <summary>Details</summary>
Motivation: 随着LLM发展为自主代理，需要准确将自然语言转换为结构化格式（工具调用）并将复杂表格信息转换为机器可读规范。当前评估缺乏有效方法来衡量这种结构保真度，传统文本指标无法检测代码类输出的语义漂移。

Method: 提出Table-BiEval方法，基于无人工干预的自监督评估框架。利用确定性中间表示，通过内容语义准确性和归一化树编辑距离来分离结构和内容。在双重拓扑维度（层次结构和平面表格）上实证评估15个最先进的LLM。

Result: 结果显示显著变异性：中等规模模型在结构效率上可能意外优于更大模型；深度递归嵌套仍是当前架构的普遍瓶颈。

Conclusion: Table-BiEval提供了一种无需人工干预的有效评估方法，揭示了LLM在结构化输出能力上的重要差异，为模型选择和架构改进提供了指导。

Abstract: As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.

</details>


### [12] [OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling](https://arxiv.org/abs/2601.19924)
*Yitian Chen,Cheng Cheng,Yinan Sun,Zi Ling,Dongdong Ge*

Main category: cs.CL

TL;DR: OPT-ENGINE是一个用于评估LLM在优化建模中能力的可扩展基准框架，包含10个运筹学任务，研究发现工具集成推理在复杂任务中更稳健，约束自动制定是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在优化建模方面取得了显著进展，但对其在复杂现实任务中自动制定和问题解决能力的边界仍不清楚，需要系统评估框架来理解其能力限制。

Method: 提出OPT-ENGINE基准框架，包含10个规范任务（5个线性规划和5个混合整数规划），通过控制可扩展的难度级别来评估LLM，研究两个关键问题：LLM在超出分布优化任务中的鲁棒性，以及性能瓶颈所在阶段。

Result: 实证结果显示：1）随着任务复杂度增加，工具集成推理（使用外部求解器）比纯文本推理表现出更高的鲁棒性；2）约束的自动制定是当前LLM的主要性能瓶颈。

Conclusion: 这些发现为开发下一代用于高级优化的LLM提供了可操作的指导，强调了工具集成的重要性以及改进约束制定能力的必要性。

Abstract: Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.

</details>


### [13] [Evaluating Large Language Models for Abstract Evaluation Tasks: An Empirical Study](https://arxiv.org/abs/2601.19925)
*Yinuo Liu,Emre Sezgin,Eric A. Youngstrom*

Main category: cs.CL

TL;DR: 本研究探讨了三种大语言模型（ChatGPT-5、Gemini-3-Pro、Claude-Sonnet-4.5）在评估学术摘要时的一致性和可靠性，发现LLMs在客观标准上与人类评审员达成中等一致性，但在主观维度表现较弱。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型能够处理请求和生成文本，但其评估复杂学术内容的可行性仍需进一步研究。本研究旨在探索LLMs在协助科学评审方面的潜力，特别是评估其与人类评审员的一致性。

Method: 使用160个本地会议摘要，由人类评审员和三种LLM使用同一评分标准进行评估。通过复合分数分布分析、组内相关系数（ICC）计算LLM间可靠性和AI-人类一致性，并使用Bland-Altman图检查视觉一致性模式和系统偏差。

Result: LLMs之间达成良好到优秀的一致性（ICC：0.59-0.87）。ChatGPT和Claude在整体质量和内容特定标准上与人类评审员达成中等一致性（ICC约0.45-0.60），但在主观维度（影响力、参与度、适用性）一致性较弱（ICC 0.23-0.38）。Gemini在一半标准上表现一般，在影响力和适用性上无可靠性。三种LLM与人类平均复合分数的差异可接受或可忽略。

Conclusion: LLMs能够批量处理摘要，在整体质量和客观标准上与人类专家达成中等一致性。通过适当的流程架构，它们可以一致地应用评分标准处理大量摘要。在主观维度上的较弱表现表明AI应作为评估的补充工具，而人类专业知识仍然至关重要。

Abstract: Introduction: Large language models (LLMs) can process requests and generate texts, but their feasibility for assessing complex academic content needs further investigation. To explore LLM's potential in assisting scientific review, this study examined ChatGPT-5, Gemini-3-Pro, and Claude-Sonnet-4.5's consistency and reliability in evaluating abstracts compared to one another and to human reviewers. Methods: 160 abstracts from a local conference were graded by human reviewers and three LLMs using one rubric. Composite score distributions across three LLMs and fourteen reviewers were examined. Inter-rater reliability was calculated using intraclass correlation coefficients (ICCs) for within-AI reliability and AI-human concordance. Bland-Altman plots were examined for visual agreement patterns and systematic bias. Results: LLMs achieved good-to-excellent agreement with each other (ICCs: 0.59-0.87). ChatGPT and Claude reached moderate agreement with human reviewers on overall quality and content-specific criteria, with ICCs ~.45-.60 for composite, impression, clarity, objective, and results. They exhibited fair agreement on subjective dimensions, with ICC ranging from 0.23-0.38 for impact, engagement, and applicability. Gemini showed fair agreement on half criteria and no reliability on impact and applicability. Three LLMs showed acceptable or negligible mean difference (ChatGPT=0.24, Gemini=0.42, Claude=-0.02) from the human mean composite scores. Discussion: LLMs could process abstracts in batches with moderate agreement with human experts on overall quality and objective criteria. With appropriate process architecture, they can apply a rubric consistently across volumes of abstracts exceeding feasibility for a human rater. The weaker performance on subjective dimensions indicates that AI should serve a complementary role in evaluation, while human expertise remains essential.

</details>


### [14] [The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models](https://arxiv.org/abs/2601.19926)
*Nora Graichen,Iria de-Dios-Flores,Gemma Boleda*

Main category: cs.CL

TL;DR: 对337篇评估Transformer语言模型句法能力的论文进行系统综述，发现研究过度集中于英语、BERT模型和简单句法现象，模型在形式句法表现良好但在句法-语义接口表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 系统评估Transformer语言模型在句法能力方面的研究现状，识别当前研究的局限性和偏差，为未来研究提供方向性建议。

Method: 对337篇相关论文进行系统综述，分析1,015个模型结果，涵盖多种句法现象和解释性方法，采用定量和定性分析方法。

Result: 研究发现：1）方法多样但过度集中于英语、BERT模型和简单句法现象（如词性和一致性）；2）TLM在形式句法表现良好，但在句法-语义接口现象（如约束关系和填充语-空位依赖）表现不稳定且较弱。

Conclusion: 建议未来研究应：完整报告数据、统一理论构念和方法、增加机制性方法使用、扩大语言和语言现象的实证范围，以推动领域发展。

Abstract: We present a systematic review of 337 articles evaluating the syntactic abilities of Transformer-based language models, reporting on 1,015 model results from a range of syntactic phenomena and interpretability methods. Our analysis shows that the state of the art presents a healthy variety of methods and data, but an over-focus on a single language (English), a single model (BERT), and phenomena that are easy to get at (like part of speech and agreement). Results also suggest that TLMs capture these form-oriented phenomena well, but show more variable and weaker performance on phenomena at the syntax-semantics interface, like binding or filler-gap dependencies. We provide recommendations for future work, in particular reporting complete data, better aligning theoretical constructs and methods across studies, increasing the use of mechanistic methods, and broadening the empirical scope regarding languages and linguistic phenomena.

</details>


### [15] [Attribution Techniques for Mitigating Hallucinated Information in RAG Systems: A Survey](https://arxiv.org/abs/2601.19927)
*Yuqing Zhao,Ziyao Liu,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 该论文综述了基于归因的技术在RAG系统中缓解幻觉问题的应用，提出了RAG幻觉的分类法、统一归因技术流程，并比较了不同技术的优缺点。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成(RAG)框架通过引入外部参考来增强LLM响应，但由于检索器和生成器之间的复杂交互，RAG系统引入了新的幻觉形式。现有的归因技术缺乏统一的流程、清晰的分类法和系统性的比较分析。

Method: 1) 提出RAG系统中幻觉类型的分类法；2) 构建归因技术的统一流程；3) 根据针对的幻觉类型回顾相关技术；4) 讨论优缺点并提供实践指南。

Result: 建立了RAG幻觉的系统分类框架，统一了归因技术的工作流程，为不同幻觉类型提供了针对性的技术解决方案比较，并给出了实践指导。

Conclusion: 该综述填补了RAG系统中归因技术研究的空白，为未来研究和实践应用提供了系统性的指导框架，有助于根据具体幻觉类型和应用场景选择合适的技术方案。

Abstract: Large Language Models (LLMs)-based question answering (QA) systems play a critical role in modern AI, demonstrating strong performance across various tasks. However, LLM-generated responses often suffer from hallucinations, unfaithful statements lacking reliable references. Retrieval-Augmented Generation (RAG) frameworks enhance LLM responses by incorporating external references but also introduce new forms of hallucination due to complex interactions between the retriever and generator. To address these challenges, researchers have explored attribution-based techniques that ensure responses are verifiably supported by retrieved content. Despite progress, a unified pipeline for these techniques, along with a clear taxonomy and systematic comparison of their strengths and weaknesses, remains lacking. A well-defined taxonomy is essential for identifying specific failure modes within RAG systems, while comparative analysis helps practitioners choose appropriate solutions based on hallucination types and application context. This survey investigates how attribution-based techniques are used within RAG systems to mitigate hallucinations and addresses the gap by: (i) outlining a taxonomy of hallucination types in RAG systems, (ii) presenting a unified pipeline for attribution techniques, (iii) reviewing techniques based on the hallucinations they target, and (iv) discussing strengths and weaknesses with practical guidelines. This work offers insights for future research and practical use of attribution techniques in RAG systems.

</details>


### [16] [Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures](https://arxiv.org/abs/2601.19928)
*Yi Hu,Jiaqi Gu,Ruxin Wang,Zijun Yao,Hao Peng,Xiaobao Wu,Jianhui Chen,Muhan Zhang,Liangming Pan*

Main category: cs.CL

TL;DR: 这篇论文是关于大型推理模型（LRMs）机制理解的全面综述，将近期研究发现组织为训练动态、推理机制和意外行为三个核心维度，旨在弥合黑盒性能与机制透明度之间的差距。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习推动的大型推理模型在性能上取得了显著成就，但理解这些模型内部工作机制已成为同等重要的研究前沿。当前研究需要从黑盒性能转向机制透明度。

Method: 采用系统性综述方法，将现有关于大型推理模型机制理解的研究组织成三个核心维度：训练动态（训练过程如何影响模型行为）、推理机制（模型如何进行推理）和意外行为（模型表现出的非预期行为）。

Result: 通过综合现有研究发现，论文建立了大型推理模型机制理解的系统框架，识别了当前研究的关键洞见，并揭示了黑盒性能与机制透明度之间的差距。

Conclusion: 论文提出了未来机制研究路线图，包括应用可解释性需求、改进方法论和建立统一理论框架等挑战，旨在推动大型推理模型从黑盒性能向透明机制理解的发展。

Abstract: Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.

</details>


### [17] [Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding](https://arxiv.org/abs/2601.19929)
*David Linus Ostby*

Main category: cs.CL

TL;DR: Stingy Context提出基于层次树的压缩方案，在自动编码任务中实现18:1的LLM上下文压缩，将239k令牌代码库减少到11k令牌，保持任务保真度，在40个真实问题上达到94-97%成功率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在自动编码任务中上下文长度限制问题，传统扁平压缩方法存在信息丢失和"迷失在中间"效应，需要更高效的代码库压缩方案。

Method: 提出Stingy Context层次树压缩方案，使用TREEFRAG分解技术，将代码库组织成树状结构进行压缩，保留关键信息同时大幅减少令牌数量。

Result: 在12个前沿模型上测试，对40个真实世界问题达到94-97%成功率，相比扁平方法表现更优，有效缓解迷失在中间效应，成本较低。

Conclusion: Stingy Context通过层次树压缩显著减少LLM上下文需求，在保持任务保真度的同时提高自动编码效率，为大规模代码库处理提供实用解决方案。

Abstract: We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.

</details>


### [18] [SDUs DAISY: A Benchmark for Danish Culture](https://arxiv.org/abs/2601.19930)
*Jacob Nielsen,Stine L. Beltoft,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: Daisy是一个基于丹麦文化经典2006的丹麦文化遗产基准数据集，包含741个经过人工审核的封闭式问答对，涵盖从公元前1300年到当代的丹麦文化主题。


<details>
  <summary>Details</summary>
Motivation: 创建专门针对丹麦文化遗产的基准数据集，以评估模型对丹麦文化知识的理解深度，不仅包括主流信息，还涵盖定义丹麦文化传承的深入细节。

Method: 基于丹麦文化经典2006的文物清单，查询对应的维基百科页面，使用语言模型生成随机问题，采用混合中心与边缘问题的采样策略，所有问答对经过人工审核或修正。

Result: 构建了包含741个封闭式问答对的最终数据集，涵盖从公元前1300年的考古发现到18世纪诗歌、音乐作品，再到当代流行音乐和丹麦设计建筑等广泛主题。

Conclusion: Daisy基准为评估模型对丹麦文化遗产的理解提供了系统化的工具，通过精心设计的采样策略确保覆盖文化知识的广度和深度，为文化AI研究提供了重要资源。

Abstract: We introduce a new benchmark for Danish culture via cultural heritage, Daisy, based on the curated topics from the Danish Culture Canon 2006. For each artifact in the culture canon, we query the corresponding Wikipedia page and have a language model generate random questions. This yields a sampling strategy within each work, with a mix of central of peripheral questions for each work, not only knowledge of mainstream information, but also in-depth cornerstones defining the heritage of Danish Culture, defined by the Canon committee. Each question-answer pair is humanly approved or corrected in the final dataset consisting of 741 close-ended question answer pairs covering topics, from 1300 BC. archaeological findings, 1700 century poems and musicals pieces to contemporary pop music and Danish design and architecture.

</details>


### [19] [CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity](https://arxiv.org/abs/2601.19931)
*Sebastien Kawada,Dylan Holyoak*

Main category: cs.CL

TL;DR: 提出混合神经符号系统用于叙事故事相似性任务，结合神经自一致性投票与多尺度叙事分析集成作为符号平局决胜器


<details>
  <summary>Details</summary>
Motivation: 解决叙事故事相似性评估中的模糊情况，通过选择性延迟到符号方法来增强神经预测能力

Method: 级联架构：神经网络组件使用大语言模型进行多轮自一致性投票，达到超多数阈值时做出自信决策；当投票完全平局时，符号集成结合五种叙事相似性信号（词汇重叠、语义嵌入、故事语法结构、事件链对齐、叙事张力曲线）作为最终决策

Result: 在开发集上达到81%准确率，证明选择性延迟到符号方法可以有效增强神经预测在真正模糊的叙事比较中的表现

Conclusion: 混合神经符号系统通过级联决策机制，在叙事相似性任务中实现了优于纯神经方法的性能，特别是在处理模糊案例时

Abstract: We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.

</details>


### [20] ["Newspaper Eat" Means "Not Tasty": A Taxonomy and Benchmark for Coded Languages in Real-World Chinese Online Reviews](https://arxiv.org/abs/2601.19932)
*Ruyuan Wan,Changye Li,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: CodedLang数据集包含7,744条中文Google Maps评论，其中900条有编码语言的细粒度标注，提出了七类编码策略分类法，并评估了语言模型在编码语言检测、分类和评分预测上的表现。


<details>
  <summary>Details</summary>
Motivation: 编码语言是人类交流的重要组成部分，指用户故意编码含义使表面文本与真实意图不同。当前语言模型处理编码语言效果差，缺乏真实世界数据集和清晰分类法限制了研究进展。

Method: 构建CodedLang数据集，包含7,744条中文Google Maps评论，其中900条有编码语言的细粒度标注。开发了七类编码策略分类法（包括语音、拼写、跨语言替换等）。对语言模型进行编码语言检测、分类和评论评分预测的基准测试，并进行了编码与解码形式的语音分析。

Result: 即使强大的语言模型也难以识别或理解编码语言。由于许多编码表达依赖基于发音的策略，语音分析显示编码与解码形式在语音特征上存在差异。结果凸显编码语言是现实世界NLP系统面临的重要且未充分探索的挑战。

Conclusion: 编码语言是现实世界NLP系统的重要挑战，CodedLang数据集和分类法为研究提供了基础，未来需要开发更有效的模型来处理编码语言。

Abstract: Coded language is an important part of human communication. It refers to cases where users intentionally encode meaning so that the surface text differs from the intended meaning and must be decoded to be understood. Current language models handle coded language poorly. Progress has been limited by the lack of real-world datasets and clear taxonomies. This paper introduces CodedLang, a dataset of 7,744 Chinese Google Maps reviews, including 900 reviews with span-level annotations of coded language. We developed a seven-class taxonomy that captures common encoding strategies, including phonetic, orthographic, and cross-lingual substitutions. We benchmarked language models on coded language detection, classification, and review rating prediction. Results show that even strong models can fail to identify or understand coded language. Because many coded expressions rely on pronunciation-based strategies, we further conducted a phonetic analysis of coded and decoded forms. Together, our results highlight coded language as an important and underexplored challenge for real-world NLP systems.

</details>


### [21] [Text-to-State Mapping for Non-Resolution Reasoning: The Contradiction-Preservation Principle](https://arxiv.org/abs/2601.19933)
*Kei Saito*

Main category: cs.CL

TL;DR: 该论文提出了文本到状态映射函数φ，将自然语言输入转换为非消解推理框架中的叠加状态，实现了语义模糊性的形式化保持。


<details>
  <summary>Details</summary>
Motivation: 非消解推理框架虽然建立了保持语义模糊性的计算架构，但如何将自然语言映射到这些数学结构仍是一个开放问题。需要建立算法桥梁连接原始文本和形式化状态空间。

Method: 引入文本到状态映射函数φ，形式化矛盾保持原则，使用现有大语言模型作为解释生成器开发提取协议，将语言输入转换为叠加状态。

Result: 在68个测试句子的实证验证中，模糊输入的香农熵H(S)平均达到1.087比特，而基线单解释方法为0.000，成功保持了语义模糊性。

Conclusion: 该框架提供了连接原始文本和形式化状态空间的算法桥梁，实现了语言模型推理中的架构延迟消解，为保持语义模糊性提供了有效方法。

Abstract: Non-Resolution Reasoning (NRR) provides a formal framework for maintaining semantic ambiguity rather than forcing premature interpretation collapse. While the foundational architecture establishes state spaces and operators for ambiguity-preserving computation, the critical question of how natural language maps to these mathematical structures remains open. This paper introduces the text-to-state mapping function φ that transforms linguistic input into superposition states within the NRR framework. We formalize the Contradiction-Preservation Principle, which requires that genuinely ambiguous expressions maintain non-zero entropy in their state representations, and develop extraction protocols using existing Large Language Models as interpretation generators. Empirical validation across 68 test sentences spanning lexical, structural, and pragmatic ambiguity demonstrates that our mapping achieves mean Shannon entropy H(S) = 1.087 bits for ambiguous inputs while baseline single-interpretation approaches yield H(S) = 0.000. The framework provides the missing algorithmic bridge between raw text and the formal state spaces on which NRR operators act, enabling architectural collapse deferment in language model inference.

</details>


### [22] [Quantifying non deterministic drift in large language models](https://arxiv.org/abs/2601.19934)
*Claire Nicholson*

Main category: cs.CL

TL;DR: 本文通过重复实验量化了LLMs在相同提示下的输出变异性（行为漂移），发现即使在温度0.0时也存在非确定性，不同模型、部署方式和提示类型呈现不同的变异性模式。


<details>
  <summary>Details</summary>
Motivation: LLMs在实际应用中，即使温度和参数固定，相同提示也可能产生不同输出。这种输出变异性（行为漂移）缺乏系统性的基准量化，需要建立参考点来评估未来的漂移缓解和控制方法。

Method: 对gpt-4o-mini和llama3.1-8b两个公开模型进行重复运行实验，使用精确重复、扰动输入和重用模式，在温度0.0和0.7下评估五个提示类别。通过唯一输出比例、词汇相似度和词数统计来测量漂移。

Result: 即使在温度0.0时也存在非确定性，不同模型大小、部署方式和提示类型呈现不同的变异性模式。结果提供了系统性的经验基准，可用于比较模型、提示模式和部署类型。

Conclusion: 本研究建立了在无稳定化技术条件下的系统性经验基准，为评估未来漂移缓解和控制方法提供了参考点。同时指出了词汇度量的局限性，并强调了新兴的语义方法的重要性。

Abstract: Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, and reuse modes at temperatures of 0.0 and 0.7. Drift is measured using unique output fractions, lexical similarity, and word count statistics, enabling direct comparison across models, prompting modes, and deployment types. The results show that nondeterminism persists even at temperature 0.0, with distinct variability patterns by model size, deployment, and prompt type. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches. By establishing a systematic empirical baseline in the absence of stabilisation techniques, this study provides a reference point for evaluating future drift mitigation and control methods.

</details>


### [23] [Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents](https://arxiv.org/abs/2601.19935)
*Yiting Shen,Kun Li,Wei Zhou,Songlin Hu*

Main category: cs.CL

TL;DR: Mem2ActBench：评估LLM智能体主动利用长期记忆执行工具任务的新基准，现有系统在参数接地方面仍不足


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试智能体被动检索孤立事实的能力，无法评估主动应用记忆执行任务的关键能力，需要新的评估框架

Method: 构建Mem2ActBench基准，通过自动化流程整合异构数据源，使用一致性建模解决冲突，生成2,029个会话，通过反向生成方法创建400个工具使用任务

Result: 人类评估确认91.3%的任务强烈依赖记忆，对7个记忆框架的实验显示当前系统在主动利用记忆进行参数接地方面仍然不足

Conclusion: 需要更有效的方法来评估和改进任务执行中的记忆应用，Mem2ActBench填补了现有基准的空白，为开发更好的记忆应用系统提供了基础

Abstract: Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.

</details>


### [24] [Benchmarking von ASR-Modellen im deutschen medizinischen Kontext: Eine Leistungsanalyse anhand von Anamnesegesprächen](https://arxiv.org/abs/2601.19945)
*Thomas Schuster,Julius Trögele,Nico Döring,Robin Krüger,Matthieu Hoffmann,Holger Friedrich*

Main category: cs.CL

TL;DR: 本文针对德语医疗场景下的自动语音识别（ASR）进行评测，创建了模拟医患对话数据集，评估了29个ASR模型，发现不同模型在医疗术语和方言处理上存在显著性能差异。


<details>
  <summary>Details</summary>
Motivation: 当前ASR技术能显著减轻医疗人员工作负担（如文档自动化），但现有评测主要针对英语，缺乏针对德语医疗场景（特别是包含方言）的专门评估。

Method: 创建了模拟医患对话的德语医疗数据集，评估了29个ASR模型，包括Whisper、Voxtral、Wav2Vec2等开源模型以及AssemblyAI、Deepgram等商业API，使用WER、CER、BLEU三种指标进行量化评估，并计划进行语义分析。

Result: 不同模型性能差异显著：最佳系统词错误率（WER）可低于3%，但其他模型在医疗术语和方言变体上的错误率明显更高，显示出德语医疗ASR的挑战性。

Conclusion: 德语医疗场景下的ASR评估仍存在挑战，特别是在处理医疗专业术语和方言方面，需要进一步研究和改进，语义分析将是未来重要研究方向。

Abstract: Automatic Speech Recognition (ASR) offers significant potential to reduce the workload of medical personnel, for example, through the automation of documentation tasks. While numerous benchmarks exist for the English language, specific evaluations for the German-speaking medical context are still lacking, particularly regarding the inclusion of dialects. In this article, we present a curated dataset of simulated doctor-patient conversations and evaluate a total of 29 different ASR models. The test field encompasses both open-weights models from the Whisper, Voxtral, and Wav2Vec2 families as well as commercial state-of-the-art APIs (AssemblyAI, Deepgram). For evaluation, we utilize three different metrics (WER, CER, BLEU) and provide an outlook on qualitative semantic analysis. The results demonstrate significant performance differences between the models: while the best systems already achieve very good Word Error Rates (WER) of partly below 3%, the error rates of other models, especially concerning medical terminology or dialect-influenced variations, are considerably higher.

</details>


### [25] [On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text](https://arxiv.org/abs/2601.20006)
*Michał Gromadzki,Anna Wróblewska,Agnieszka Kaliska*

Main category: cs.CL

TL;DR: 本文提出基于大规模语料库和新型训练策略的AI生成文本检测方法，使用10亿人类文本和19亿AI生成文本训练检测模型，在包含21个LLM的基准测试中达到99.6%的token级准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本越来越接近人类写作，在教育、出版和数字安全领域的真实性验证面临挑战，AI生成文本检测成为关键的技术和伦理问题。

Method: 构建了10亿token的人类文本语料库和19亿token的AI生成文本语料库，开发多种检测模型，并提出两种新型训练范式：按单个LLM微调和按LLM家族微调。

Result: 在包含21个大语言模型的1亿token基准测试中，最佳微调检测器达到99.6%的token级准确率，显著优于现有开源基线方法。

Conclusion: 基于大规模多样化语料库和针对性训练策略的检测方法能够有效识别AI生成文本，为解决AI生成内容检测问题提供了有效解决方案。

Abstract: The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\%$ token-level accuracy, substantially outperforming existing open-source baselines.

</details>


### [26] [LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?](https://arxiv.org/abs/2601.20009)
*J. Ben Tamo,Daniel Carlander-Reuterfelt,Jonathan Rubin,Dezhi Hong,Mingxian Wang,Oleg Poliannikov*

Main category: cs.CL

TL;DR: 论文提出通过选择性微调大语言模型的最后几层来解决多语言控制问题，仅需微调3-5%参数就能达到98%以上的语言一致性，效果接近全参数微调但计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 尽管有多语言预训练，大语言模型在非英语任务中仍存在语言控制问题，即无法按照预期语言进行响应。论文识别了两种关键失败模式：多语言转移瓶颈（正确语言但错误任务响应）和语言一致性瓶颈（正确任务响应但错误语言）。

Method: 1) 设计四场景评估协议，涵盖MMLU、MGSM和XQuAD基准测试；2) 扩展logit lens分析，逐层追踪语言概率并计算隐藏状态的跨语言语义相似性；3) 基于分析发现的三阶段内部结构，提出选择性微调仅负责语言控制的最后几层。

Result: 在Qwen-3-32B和Bloom-7.1B模型上，该方法仅微调3-5%参数就能在六种语言上达到超过98%的语言一致性，且不牺牲任务准确性。效果与全参数微调几乎相同（例如在所有提示场景下都达到98%以上语言一致性），但计算资源消耗大幅减少。

Conclusion: 通过定位语言控制所在的特定层并进行选择性微调，能够高效解决大语言模型的多语言适应问题。这是首个利用语言控制层定位进行高效多语言适应的方法，为多语言模型优化提供了新的方向。

Abstract: Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.

</details>


### [27] [Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method](https://arxiv.org/abs/2601.20026)
*Pragatheeswaran Vipulanandan,Kamal Premaratne,Dilip Sarkar*

Main category: cs.CL

TL;DR: 提出基于量子张量网络的LLM不确定性量化框架，通过语义等价聚类检测幻觉，引入熵最大化策略筛选可靠输出，在多个数据集和模型上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然生成能力强，但容易产生不可靠的幻觉输出，且相同提示下输出会任意变化。现有方法缺乏对不确定性的系统量化，特别是在不同生成长度和量化级别下的鲁棒性研究不足。

Method: 1. 基于量子张量网络的管道，量化标记序列概率的偶然不确定性；2. 基于语义等价性对LLM生成进行聚类；3. 引入熵最大化策略，优先选择高确定性、语义连贯的输出；4. 识别LLM决策可能不可靠的高熵区域。

Result: 在TriviaQA、NQ、SVAMP、SQuAD等4个数据集上进行了116个实验，涵盖Mistral-7B、LLaMA系列等8种架构。在AUROC和AURAC指标上持续优于现有最优基线，在不同生成长度和量化级别下保持鲁棒性。

Conclusion: 提出的量子物理启发的不确定性量化框架为幻觉检测提供了原则性和可解释的方案，熵最大化策略为需要人工监督的场景提供了实用指导，即使在资源受限的部署中也能保持可靠性。

Abstract: Large language models (LLMs) exhibit strong generative capabilities but remain vulnerable to confabulations, fluent yet unreliable outputs that vary arbitrarily even under identical prompts. Leveraging a quantum tensor network based pipeline, we propose a quantum physics inspired uncertainty quantification framework that accounts for aleatoric uncertainty in token sequence probability for semantic equivalence based clustering of LLM generations. This offers a principled and interpretable scheme for hallucination detection. We further introduce an entropy maximization strategy that prioritizes high certainty, semantically coherent outputs and highlights entropy regions where LLM decisions are likely to be unreliable, offering practical guidelines for when human oversight is warranted. We evaluate the robustness of our scheme under different generation lengths and quantization levels, dimensions overlooked in prior studies, demonstrating that our approach remains reliable even in resource constrained deployments. A total of 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD across multiple architectures including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLaMA-2-13b, and LLaMA-2-7b show consistent improvements in AUROC and AURAC over state of the art baselines.

</details>


### [28] [TAIGR: Towards Modeling Influencer Content on Social Media via Structured, Pragmatic Inference](https://arxiv.org/abs/2601.20032)
*Nishanth Sridhar Nakshatri,Eylon Caplan,Rajkumar Pujari,Dan Goldwasser*

Main category: cs.CL

TL;DR: TAIGR框架通过识别核心建议、构建论证图、概率推理三阶段分析健康影响者内容，解决传统基于声明的验证方法无法捕捉话语语用含义的问题。


<details>
  <summary>Details</summary>
Motivation: 健康影响者通过对话叙事和修辞策略塑造公众信念，而非明确的事实声明，导致传统的基于声明的验证方法难以捕捉其话语的语用含义。

Method: 提出TAIGR框架：1)识别核心建议（takeaway）；2)构建捕捉影响者论证的论证图；3)使用因子图进行概率推理验证建议。

Result: 在健康影响者视频转录本的验证任务中评估TAIGR，证明准确验证需要建模话语的语用和论证结构，而非将转录本视为平面声明集合。

Conclusion: TAIGR框架通过结构化分析影响者话语的语用和论证维度，为健康影响者内容验证提供了更有效的解决方案。

Abstract: Health influencers play a growing role in shaping public beliefs, yet their content is often conveyed through conversational narratives and rhetorical strategies rather than explicit factual claims. As a result, claim-centric verification methods struggle to capture the pragmatic meaning of influencer discourse. In this paper, we propose TAIGR (Takeaway Argumentation Inference with Grounded References), a structured framework designed to analyze influencer discourse, which operates in three stages: (1) identifying the core influencer recommendation--takeaway; (2) constructing an argumentation graph that captures influencer justification for the takeaway; (3) performing factor graph-based probabilistic inference to validate the takeaway. We evaluate TAIGR on a content validation task over influencer video transcripts on health, showing that accurate validation requires modeling the discourse's pragmatic and argumentative structure rather than treating transcripts as flat collections of claims.

</details>


### [29] [VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.20055)
*Vikash Singh,Darion Cassel,Nathaniel Weir,Nick Feng,Sam Bayless*

Main category: cs.CL

TL;DR: VERGE是一个神经符号框架，将LLM与SMT求解器结合，通过迭代精炼产生验证引导的答案，在推理基准上比单次方法平均提升18.7%


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型具有语法流畅性，但在高风险领域中确保其逻辑正确性仍然是一个根本性挑战。需要结合形式化验证方法来提高LLM的逻辑可靠性。

Method: 1) 将LLM输出分解为原子声明并自动形式化为一阶逻辑；2) 通过形式语义等价检查实现多模型共识；3) 语义路由将不同声明类型导向适当的验证策略；4) 使用最小校正子集精确定位逻辑错误；5) 迭代精炼直到满足接受标准或达到收敛。

Result: 使用GPT-OSS-120B模型，VERGE在推理基准上比单次方法平均提升18.7%的性能，实现了形式化保证和共识验证的结合。

Conclusion: 这种混合方法在可能的地方提供形式化保证，在其他地方提供共识验证，推进了可信AI的发展，解决了LLM在高风险领域中的逻辑正确性问题。

Abstract: Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.

</details>


### [30] [Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects](https://arxiv.org/abs/2601.20102)
*Amirhossein Haji Mohammad Rezaei,Zahra Shakeri*

Main category: cs.CL

TL;DR: 该研究创建了一个包含1650个变体的反事实基准，用于评估医学语言模型在文化信息影响下的诊断准确性，发现文化线索会显著降低模型性能，特别是标识符和上下文同时出现时。


<details>
  <summary>Details</summary>
Motivation: 构建可持续和公平的医疗保健需要医学语言模型在面对非决定性文化信息时不会改变临床正确的诊断。当前缺乏评估模型对文化信息敏感性的系统方法。

Method: 扩展150个MedQA测试项目为1650个变体，插入三种文化相关修改（标识符、上下文线索、组合）针对三个文化群体，并设置长度匹配的中性对照。评估多个大型语言模型，使用选项提示和简短解释提示两种方式。

Result: 文化线索显著影响模型准确性（Cochran's Q检验p<10^-14），标识符和上下文同时出现时性能下降最大（3-7个百分点）。超过一半的文化基础解释导致错误答案，文化参照推理与诊断失败相关。

Conclusion: 医学语言模型对文化信息敏感，可能导致诊断错误。需要评估和缓解文化诱导的诊断错误，研究提供了基准和工具支持这一目标。

Abstract: Engineering sustainable and equitable healthcare requires medical language models that do not change clinically correct diagnoses when presented with non-decisive cultural information. We introduce a counterfactual benchmark that expands 150 MedQA test items into 1650 variants by inserting culture-related (i) identifier tokens, (ii) contextual cues, or (iii) their combination for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a length-matched neutral control, where a clinician verified that the gold answer remains invariant in all variants. We evaluate GPT-5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B) under option-only and brief-explanation prompting. Across models, cultural cues significantly affect accuracy (Cochran's Q, $p<10^-14$), with the largest degradation when identifier and context co-occur (up to 3-7 percentage points under option-only prompting), while neutral edits produce smaller, non-systematic changes. A human-validated rubric ($κ=0.76$) applied via an LLM-as-judge shows that more than half of culturally grounded explanations end in an incorrect answer, linking culture-referential reasoning to diagnostic failure. We release prompts and augmentations to support evaluation and mitigation of culturally induced diagnostic errors.

</details>


### [31] [FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language](https://arxiv.org/abs/2601.20105)
*Faezeh Hosseini,Mohammadali Yousefzadeh,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 论文提出了FFEHallu基准，用于评估大语言模型在固定比喻表达（如成语、谚语）上的幻觉问题，重点关注波斯语，发现当前模型在比喻语言处理上存在系统性弱点。


<details>
  <summary>Details</summary>
Motivation: 固定比喻表达（FFEs）如成语和谚语对大型语言模型构成持续挑战，因为它们具有文化基础、非组合性和固定性，容易导致比喻幻觉（生成看似合理但不存在的比喻表达）。目前缺乏专门评估这一问题的基准，特别是对于波斯语等代表性不足的语言。

Method: 构建FFEHallu基准，包含600个精心策划的实例，涵盖三个互补任务：1）从含义生成FFE；2）检测四个受控构建类别中的伪造FFE；3）从英语到波斯语的FFE翻译。评估了六个最先进的多语言LLM。

Result: 评估显示模型在比喻能力和文化基础方面存在系统性弱点。虽然GPT4.1在拒绝伪造FFE和检索真实表达方面表现相对较强，但大多数模型难以可靠区分真实表达与高质量伪造，在跨语言翻译中经常产生幻觉。

Conclusion: 当前LLM在处理比喻语言方面存在显著差距，需要有针对性的基准来评估和缓解比喻幻觉问题。FFEHallu为这一领域提供了首个全面基准，特别关注波斯语这一代表性不足的语言。

Abstract: Figurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.

</details>


### [32] [Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://arxiv.org/abs/2601.20126)
*Abha Jha,Akanksha Mahajan,Ashwath Vaithinathan Aravindan,Praveen Saravanan,Sai Sailaja Policharla,Sonal Chaturbhuj Gehlot*

Main category: cs.CL

TL;DR: 该论文研究了使用可验证奖励的强化学习（RLVR）来训练语言模型，通过奖励"我不知道"的弃权回答来减少幻觉，在多项选择题任务上取得了减少错误回答的效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生幻觉或不可验证的内容，这削弱了它们在事实领域中的可靠性。需要一种训练范式来明确奖励弃权回答，以促进知识谦逊。

Method: 使用可验证奖励的强化学习（RLVR）训练范式，采用三元奖励结构（-1，r_abs，1），在MedMCQA和Hendrycks Math基准上对Granite-3.3-2B-Instruct和Qwen-3-4B-Instruct进行微调。研究了不同弃权奖励结构的影响，并探索了将RLVR与监督微调策略结合的方法。

Result: 适度的弃权奖励（r_abs ≈ -0.25到0.3）能持续减少错误回答，而不会严重降低多项选择题的准确性。较大模型对弃权激励表现出更强的鲁棒性。在开放式问答中，由于探索不足存在局限性，但可以通过监督弃权训练部分缓解。

Conclusion: 可验证奖励设计是缓解语言模型幻觉的可行且灵活的方法，为构建更可靠的语言模型提供了实用途径。

Abstract: Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention ("I don't know") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.

</details>


### [33] [BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification](https://arxiv.org/abs/2601.20129)
*Akif Islam,Sujan Kumar Roy,Md. Ekramul Hamid*

Main category: cs.CL

TL;DR: 本文介绍了BengaliSent140，一个大规模孟加拉语二元情感数据集，整合了7个现有数据集，包含139,792个文本样本，用于训练深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语情感分析研究受限于缺乏大规模、多样化的标注数据集。现有数据集规模小或局限于单一领域（如社交媒体评论），不足以训练需要大量异构数据的现代深度学习模型。

Method: 通过整合7个现有孟加拉语文本数据集，构建统一语料库。将异构标注方案系统性地统一为二元情感分类：非仇恨（0）和仇恨（1）。

Result: 创建了包含139,792个独特文本样本的数据集，其中68,548个仇恨实例和71,244个非仇恨实例，类别分布相对平衡。数据集提供了比现有孟加拉语情感数据集更广泛的语言和上下文覆盖。

Conclusion: BengaliSent140为训练和基准测试深度学习模型提供了坚实基础，通过整合多源多领域数据，解决了孟加拉语情感分析领域的数据稀缺问题。数据集已公开可用。

Abstract: Sentiment analysis for the Bengali language has attracted increasing research interest in recent years. However, progress remains constrained by the scarcity of large-scale and diverse annotated datasets. Although several Bengali sentiment and hate speech datasets are publicly available, most are limited in size or confined to a single domain, such as social media comments. Consequently, these resources are often insufficient for training modern deep learning based models, which require large volumes of heterogeneous data to learn robust and generalizable representations. In this work, we introduce BengaliSent140, a large-scale Bengali binary sentiment dataset constructed by consolidating seven existing Bengali text datasets into a unified corpus. To ensure consistency across sources, heterogeneous annotation schemes are systematically harmonized into a binary sentiment formulation with two classes: Not Hate (0) and Hate (1). The resulting dataset comprises 139,792 unique text samples, including 68,548 hate and 71,244 not-hate instances, yielding a relatively balanced class distribution. By integrating data from multiple sources and domains, BengaliSent140 offers broader linguistic and contextual coverage than existing Bengali sentiment datasets and provides a strong foundation for training and benchmarking deep learning models. Baseline experimental results are also reported to demonstrate the practical usability of the dataset. The dataset is publicly available at https://www.kaggle.com/datasets/akifislam/bengalisent140/

</details>


### [34] [Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR](https://arxiv.org/abs/2601.20142)
*Zilai Wang,Natarajan Balaji Shankar,Kaiyuan Zhang,Zihan Wang,Abeer Alwan*

Main category: cs.CL

TL;DR: 本文提出使用delta SSL嵌入（微调模型与预训练模型嵌入的差值）来增强儿童语音识别，通过融合不同SSL模型的delta嵌入，在MyST儿童语料库上取得了显著的WER降低。


<details>
  <summary>Details</summary>
Motivation: 自监督学习模型在语音任务中表现出色，但儿童语音识别仍面临数据有限和预训练领域不匹配的挑战。微调SSL模型会导致表示空间偏移，作者假设delta SSL嵌入（微调与预训练嵌入的差值）编码了任务特定信息，可以补充其他SSL模型的微调特征。

Method: 提出delta SSL嵌入的概念，定义为微调模型嵌入与其预训练对应模型嵌入的差值。评估了在MyST儿童语料库上使用不同模型（HuBERT、W2V2、WavLM）的多种融合策略，比较delta嵌入融合与微调嵌入融合的效果。

Result: delta嵌入融合显著提升了性能：与微调嵌入融合相比，WavLM与delta HuBERT嵌入融合实现了10%的相对WER降低，与delta W2V2嵌入融合实现了4.4%的降低。WavLM与delta W2V2嵌入融合达到了9.64的WER，在MyST语料库上创下了SSL模型的新SOTA。

Conclusion: delta SSL嵌入在儿童语音识别中非常有效，特征融合是推进儿童ASR的有前景方向。该方法通过利用微调与预训练表示之间的差异来编码任务特定信息，显著提升了识别性能。

Abstract: Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.

</details>


### [35] [Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents](https://arxiv.org/abs/2601.20144)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Jiri Gesi,Xianfeng Tang,Chen Luo,Yisi Sang,Hanqing Lu,Manling Li,Dakuo Wang*

Main category: cs.CL

TL;DR: Trajectory2Task：一个可验证的数据生成管道，用于在三种现实用户场景（意图模糊、意图变化、意图不可行）下研究工具调用，通过多轮探索生成有效工具调用轨迹并转换为用户任务，用于评估和训练工具调用代理。


<details>
  <summary>Details</summary>
Motivation: 当前工具调用代理的研究大多集中在理想化设置中，而现实应用中用户请求经常存在意图模糊、随时间变化或由于政策约束不可行的情况，缺乏覆盖这些复杂交互模式的训练和评估数据。

Method: 提出Trajectory2Task管道：1）进行多轮探索生成有效工具调用轨迹；2）将这些轨迹转换为具有受控意图适配的用户任务；3）生成可验证任务支持闭环评估和训练。

Result: 在生成的复杂用户场景任务上评估了7个最先进的LLM，观察到频繁失败。使用任务执行中获得的成功轨迹对轻量级LLM进行微调，发现在所有三种条件下都有持续改进，并且对未见工具使用领域有更好的泛化能力。

Conclusion: Trajectory2Task能够生成可验证的复杂用户场景数据，用于评估和训练工具调用代理。通过轨迹微调可以显著提升LLM的工具调用能力，表明更强的通用工具调用能力。

Abstract: Tool-calling agents are increasingly deployed in real-world customer-facing workflows. Yet most studies on tool-calling agents focus on idealized settings with general, fixed, and well-specified tasks. In real-world applications, user requests are often (1) ambiguous, (2) changing over time, or (3) infeasible due to policy constraints, and training and evaluation data that cover these diverse, complex interaction patterns remain under-represented. To bridge the gap, we present Trajectory2Task, a verifiable data generation pipeline for studying tool use at scale under three realistic user scenarios: ambiguous intent, changing intent, and infeasible intents. The pipeline first conducts multi-turn exploration to produce valid tool-call trajectories. It then converts these trajectories into user-facing tasks with controlled intent adaptations. This process yields verifiable task that support closed-loop evaluation and training. We benchmark seven state-of-the-art LLMs on the generated complex user scenario tasks and observe frequent failures. Finally, using successful trajectories obtained from task rollouts, we fine-tune lightweight LLMs and find consistent improvements across all three conditions, along with better generalization to unseen tool-use domains, indicating stronger general tool-calling ability.

</details>


### [36] [Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction](https://arxiv.org/abs/2601.20162)
*Shuoxin Wang,Chang Liu,Gowen Loo,Lifan Zheng,Kaiwen Wei,Xinyi Zeng,Jingyuan Zhang,Yu Tian*

Main category: cs.CL

TL;DR: Me-Agent是一个可学习、可记忆的个性化移动代理，通过两级用户习惯学习解决LLM移动代理缺乏个性化的问题，在个性化基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的移动代理虽然性能显著提升，但主要遵循显式用户指令而忽视个性化需求，存在三个主要限制：1) 无法解释模糊指令；2) 缺乏从用户交互历史中学习；3) 无法处理个性化指令。这些限制在没有个性化上下文的情况下对真实用户造成显著影响。

Method: Me-Agent采用两级用户习惯学习方法：在提示层面，设计了增强个人奖励模型的用户偏好学习策略；在记忆层面，设计了分层偏好记忆，将用户的长期记忆和应用特定记忆存储在不同层级记忆中。

Result: 在提出的新基准User FingerTip（包含大量日常生活模糊指令）和通用基准上的广泛实验表明，Me-Agent在个性化方面达到最先进的性能，同时保持竞争力的指令执行性能。

Conclusion: Me-Agent通过创新的两级用户习惯学习框架，有效解决了LLM移动代理的个性化不足问题，在保持指令执行能力的同时显著提升了个人化性能。

Abstract: Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.

</details>


### [37] [Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling](https://arxiv.org/abs/2601.20185)
*Husein Zolkepli*

Main category: cs.CL

TL;DR: 通过引入额外池化和增大解码器跳跃步长，将X-Codec-2.0的潜在速率从50Hz降至25Hz，输出采样率从16kHz提升至24kHz，在保持核心架构不变的情况下提高了效率和感知质量。


<details>
  <summary>Details</summary>
Motivation: X-Codec-2.0在神经音频压缩和多语言语音建模中表现出色，但其50Hz潜在速率和16kHz采样率的配置限制了时间效率和音频保真度，需要改进。

Method: 通过引入额外池化和增大解码器跳跃步长，将潜在速率从50Hz降至25Hz，同时将输出采样率从16kHz提升至24kHz，核心架构保持不变。

Result: 在多语言Common Voice 17测试集上，基于UTMOSv2评估，相比原始X-Codec-2.0基线获得了0.29 MOS的提升，在25Hz运行的所有编解码器中达到最佳报告性能。

Conclusion: 通过简单的架构修改，在保持核心设计不变的情况下，显著提高了X-Codec-2.0的效率和感知质量，为神经音频压缩提供了更优的配置方案。

Abstract: X-Codec-2.0 has shown strong performance in neural audio compression and multilingual speech modeling, operating at a 50 Hz latent rate and a 16 kHz sampling rate using frozen HuBERT features. While effective, this configuration limits temporal efficiency and audio fidelity. In this work, we explore a simple and effective modification by introducing additional pooling and increasing the decoder hop size. This reduces the latent rate from 50 Hz to 25 Hz and simultaneously raises the output sampling rate from 16 kHz to 24 kHz, improving efficiency and perceptual quality without altering the core architecture. Evaluated on the multilingual Common Voice 17 test set, the proposed configuration achieves a 0.29 MOS improvement over the original X-Codec-2.0 baseline based on UTMOSv2, and attains the best reported performance among all codecs operating at 25 Hz. The source code, checkpoints, and generation comparisons are released at \href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}.

</details>


### [38] [Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems](https://arxiv.org/abs/2601.20230)
*Haoyuan Yu,Yuxuan Chen,Minjie Cai*

Main category: cs.CL

TL;DR: 提出基于多模态大语言模型的半级联全双工对话框架，将复杂对话分解为最小对话单元独立处理，实现免训练即插即用


<details>
  <summary>Details</summary>
Motivation: 全双工语音交互对于自然的人机交互至关重要，需要解决复杂对话的实时处理问题

Method: 将复杂对话分解为最小对话单元，每个单元独立处理并预测何时转换到下一个单元；构建基于多模态大语言模型的半级联全双工对话系统，辅以语音活动检测和文本转语音等模块

Result: 在HumDial数据集上的实验验证了框架有效性，在Human-like Spoken Dialogue Systems Challenge（Track 2: Full-Duplex Interaction）测试集上排名第二

Conclusion: 提出的框架能够实现免训练、即插即用的全双工对话系统，在复杂对话处理方面表现出色

Abstract: Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal conversational units, enabling the system to process each unit independently and predict when to transit to the next. This framework is instantiated as a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) synthesis. The resulting system operates in a train-free, plug-and-play manner. Experiments on the HumDial dataset demonstrate the effectiveness of our framework, which ranks second among all teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). Code is available at the GitHub repository https://github.com/yu-haoyuan/fd-badcat.

</details>


### [39] [Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy](https://arxiv.org/abs/2601.20253)
*Si Chen,Le Huy Khiem,Annalisa Szymanski,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 论文提出了一个从专家指南自动生成基准测试的框架，用于评估LLM在实践领域中的开放式问答能力，发现LLM在高阶推理上表现更好但在低阶记忆任务上失败更多。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要依赖现有人类考试数据集，但在实践性领域（如教学、营养学、护理）中，知识是程序性的且基于专业判断，这类数据集往往不可用。需要开发能够评估情境化推理能力的自动化基准生成方法。

Method: 基于布鲁姆分类法，将专家实践指南转化为隐含违规场景，然后扩展为自动评分的多项选择题和多轮对话，覆盖四个认知层次。该框架具有确定性、可重复性和可扩展性。

Result: 在三个应用领域（教学、营养学、护理）的测试中发现，LLM有时在高阶推理（分析）上表现相对更好，但在低阶项目（记忆）上失败更频繁。生成了大规模、心理测量学知情的基准测试，揭示了这些非直观的模型行为。

Conclusion: 该框架能够生成自动化、可扩展的基准测试，用于评估LLM在现实世界情境中的推理能力，揭示了LLM与人类推理模式的差异，为实践领域的模型评估提供了新方法。

Abstract: Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.

</details>


### [40] [SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility](https://arxiv.org/abs/2601.20256)
*Xuanyu Su,Diana Inkpen,Nathalie Japkowicz*

Main category: cs.CL

TL;DR: 提出SoftHateBench基准测试，用于评估检测"软仇恨言论"的能力，即表面合理但通过推理框架引导受众排斥目标群体的隐蔽仇恨言论。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体内容审核系统主要针对明显的毒性内容（硬仇恨言论），但对于表面合理但通过论证框架引导仇恨的"软仇恨言论"检测能力不足，现有基准测试未能系统衡量这一差距。

Method: 结合论题论证模型(AMT)和关联理论(RT)的统一框架：AMT提供将明确仇恨立场重写为表面中立讨论的论证结构，RT确保AMT链的逻辑连贯性，生成软仇恨变体同时保留核心仇恨立场。

Result: 构建了覆盖7个社会文化领域和28个目标群体的4,745个软仇恨实例基准。评估显示，从硬仇恨到软仇恨层级，基于编码器的检测器、通用LLM和安全模型的检测性能均显著下降。

Conclusion: 当前仇恨言论检测系统对推理驱动的隐蔽仇恨言论存在明显漏洞，需要开发能够理解论证结构和推理模式的新方法，SoftHateBench为评估和改进这类检测能力提供了系统基准。

Abstract: Online hate on social media ranges from overt slurs and threats (\emph{hard hate speech}) to \emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \textbf{\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \emph{Argumentum Model of Topics} (AMT) and \emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \textbf{7} sociocultural domains and \textbf{28} target groups, comprising \textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \textcolor{red}{\textbf{Disclaimer.} Contains offensive examples used solely for research.}

</details>


### [41] [RusLICA: A Russian-Language Platform for Automated Linguistic Inquiry and Category Analysis](https://arxiv.org/abs/2601.20275)
*Elina Sigdel,Anastasia Panfilova*

Main category: cs.CL

TL;DR: 开发俄语版LIWC心理语言学分析工具，包含96个类别，整合语法、形态、词汇特征及语言模型预测结果


<details>
  <summary>Details</summary>
Motivation: 现有LIWC工具主要针对英语，需要开发适应俄语语法和文化特性的心理语言学分析工具

Method: 基于多个词典资源和语料库构建俄语专用词典，将词元映射到42个心理语言学类别，整合语法、形态、词汇特征及语言模型预测

Result: 开发了包含96个类别的俄语LIWC分析工具，作为RusLICA网络服务的一部分

Conclusion: 成功开发了适应俄语特性的LIWC方法论，为俄语文本的心理语言学分析提供了专门工具

Abstract: Defining psycholinguistic characteristics in written texts is a task gaining increasing attention from researchers. One of the most widely used tools in the current field is Linguistic Inquiry and Word Count (LIWC) that originally was developed to analyze English texts and translated into multiple languages. Our approach offers the adaptation of LIWC methodology for the Russian language, considering its grammatical and cultural specificities. The suggested approach comprises 96 categories, integrating syntactic, morphological, lexical, general statistical features, and results of predictions obtained using pre-trained language models (LMs) for text analysis. Rather than applying direct translation to existing thesauri, we built the dictionary specifically for the Russian language based on the content from several lexicographic resources, semantic dictionaries and corpora. The paper describes the process of mapping lemmas to 42 psycholinguistic categories and the implementation of the analyzer as part of RusLICA web service.

</details>


### [42] [Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale](https://arxiv.org/abs/2601.20276)
*Tianwei Lin,Zuyi Zhou,Xinda Zhao,Chenke Wang,Xiaohong Li,Yu Chen,Chuanrui Hu,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 论文提出了EverMemBench-S (EMB-S)基准测试，用于评估长上下文LLM代理在对抗性环境下的证据访问能力，发现语义干扰而非上下文长度是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有NIAH评估主要测量良性跨度定位，但现实场景中证据通常与语义相似的干扰项共存，需要更真实的对抗性评估框架。

Method: 构建326M令牌的MemoryBank，设计包含碰撞测试近邻硬负例和黄金证据集的查询，采用解耦诊断协议分别评估证据访问和端到端QA质量。

Result: 在从64K到326M令牌的参考语料阶梯上，系统在良性NIAH上表现良好但在语义干扰下证据访问能力急剧下降，表明语义辨别是主要瓶颈。

Conclusion: 语义干扰而非上下文长度是长上下文记忆的主要瓶颈，需要更强大的语义辨别能力来应对现实世界的对抗性环境。

Abstract: Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.

</details>


### [43] [MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting](https://arxiv.org/abs/2601.20300)
*Jing Xu,Minglin Wu,Xueyuan Chen,Xixin Wu,Helen Meng*

Main category: cs.CL

TL;DR: 提出MiLorE-SSL框架，结合LoRA和软混合专家机制，用于高效的多语言持续学习，解决灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现有的多语言自监督学习模型局限于预训练时见过的语言，重新训练新语言计算成本高，而顺序训练会导致灾难性遗忘

Method: 结合LoRA模块和软混合专家机制，LoRA提供高效低秩适配，软MoE促进跨语言专家共享，减少跨语言干扰，并引入有限回放数据缓解遗忘

Result: 在ML-SUPERB基准测试中，MiLorE-SSL在新语言上表现优异，同时提升现有语言能力，仅需2.14%可训练参数

Conclusion: MiLorE-SSL是高效的多语言持续学习框架，能有效扩展模型到新语言同时保持原有语言能力

Abstract: Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.

</details>


### [44] [SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger](https://arxiv.org/abs/2601.20312)
*Kaiyuan Chen,Guangmin Zheng,Jin Wang,Xiaobing Zhou,Xuejie Zhang*

Main category: cs.CL

TL;DR: 提出SAPO方法解决现有自进化方法忽略细粒度推理步骤导致的推理器-验证器差距问题，通过自适应过程监督信号提升小语言模型在数学和代码任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有自进化方法忽视细粒度推理步骤的影响，导致推理器-验证器差距；蒙特卡洛过程监督计算效率低下，进一步加剧了缩小这一差距的困难。受错误相关负波（ERN）启发，推理器能够在错误决策后定位错误并指导快速调整

Method: 提出自适应性过程优化（SAPO）方法，通过主动最小化推理器-验证器差距来自适应且高效地引入过程监督信号，而不是依赖低效的蒙特卡洛估计

Result: 在数学和代码两类挑战性任务上，SAPO方法优于大多数现有自进化方法。为研究SAPO对验证器性能的影响，还引入了数学和编码任务的过程奖励模型新基准

Conclusion: SAPO方法通过自适应过程监督有效解决了推理器-验证器差距问题，在小语言模型的自改进方面表现出色，为过程奖励模型评估提供了新基准

Abstract: Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.

</details>


### [45] [Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning](https://arxiv.org/abs/2601.20326)
*Zeyu Xing,Xing Li,Hui-Ling Zhen,Mingxuan Yuan,Sinno Jialin Pan*

Main category: cs.CL

TL;DR: KV缓存可作为轻量级表征，无需重新计算或存储完整隐藏状态，适用于链式嵌入和快慢思维切换，显著减少token生成


<details>
  <summary>Details</summary>
Motivation: KV缓存通常仅用于加速自回归解码，但其中编码的上下文信息可免费重用于下游任务，无需额外计算成本

Method: 将KV缓存视为轻量级表征，提出两种应用：链式嵌入（Chain-of-Embedding）和快慢思维切换（Fast/Slow Thinking Switching）

Result: 在Llama-3.1-8B-Instruct和Qwen2-7B-Instruct上实现竞争性或更优性能；在Qwen3-8B和DeepSeek-R1-Distil-Qwen-14B上减少token生成达5.7倍，精度损失最小

Conclusion: KV缓存可作为免费有效的采样和推理基底，为LLM推理中的表征重用开辟新方向

Abstract: KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.

</details>


### [46] [CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria](https://arxiv.org/abs/2601.20327)
*Xinyu Hu,Yancheng He,Weixun Wang,Tao Feng,Li Lin,Jiashun Liu,Wenbo Su,Bo Zheng,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出CE-RM-4B，一个基于两阶段rollout方法和统一查询标准的生成式奖励模型，在少量高质量数据上训练，在奖励模型基准测试和下游RL实践中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge方法在基准测试中表现良好，但在实际RL实践中存在显著差距，主要由于现有研究存在成对评估主导和评估标准优化不足等局限性。

Method: 提出CE-RM-4B生成式奖励模型，采用专门的两阶段rollout方法训练，使用统一查询标准，仅使用约5.7K从开源偏好数据集中精选的高质量数据。

Result: CE-RM-4B在多样化的奖励模型基准测试中表现优异，特别是在Best-of-N场景中，并在下游RL实践中提供更有效的改进。

Conclusion: 通过解决现有LLM-as-a-Judge方法的局限性，CE-RM-4B在少量高质量数据上训练，实现了在基准测试和实际RL应用中的优异性能。

Abstract: Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.

</details>


### [47] [PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments](https://arxiv.org/abs/2601.20330)
*Zhuang Chen,Dazhen Wan,Zhangkai Zheng,Guanqun Bi,Xiyao Xiao,Binghang Li,Minlie Huang*

Main category: cs.CL

TL;DR: 提出PsychePass框架，通过轨迹锚定锦标赛评估LLMs在心理咨询中的能力，解决现有评估方法的无锚定缺陷和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估大型语言模型在心理健康咨询能力的方法存在无锚定缺陷，导致过程漂移（客户模拟偏离咨询目标）和标准漂移（静态评分缺乏稳定性），难以可靠评估模型的治疗能力。

Method: 提出PsychePass统一框架：1）在模拟中锚定交互轨迹，让客户精确控制咨询过程以探测多方面能力；2）通过瑞士系统锦标赛锚定对战轨迹，利用动态成对对战产生稳健的Elo评分；3）将锦标赛轨迹转化为可信的奖励信号，用于基于策略的强化学习提升模型性能。

Result: 大量实验验证了PsychePass的有效性，并显示其与人类专家判断具有强一致性。

Conclusion: PsychePass框架通过轨迹锚定锦标赛方法，为评估和提升大型语言模型在心理咨询领域的治疗能力提供了可靠、稳定的解决方案。

Abstract: While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.

</details>


### [48] [MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment](https://arxiv.org/abs/2601.20335)
*Qinzhuo Wu,Zhizhuo Yang,Hanhao Li,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: MobileBench-OL是一个包含1080个任务、覆盖80个中国应用的在线移动GUI代理基准测试，通过5个子集评估任务执行、复杂推理和噪声鲁棒性，填补了现有基准测试与现实环境之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理基准测试存在三个主要问题：1）在线基准测试过于关注任务指令跟随能力，忽视了推理和探索能力；2）未考虑现实移动环境中的随机噪声；3）导致基准测试与现实环境之间存在差距。

Method: 提出MobileBench-OL在线基准测试，包含1080个任务，覆盖80个中国应用。通过5个子集设置多个评估维度，测量代理的任务执行、复杂推理和噪声鲁棒性。提供带有重置机制的自动评估框架，实现稳定可重复的现实世界基准测试。

Result: 评估12个领先的GUI代理显示，它们在满足现实世界要求方面仍有显著改进空间。人工评估进一步证实MobileBench-OL能够可靠地测量领先GUI代理在真实环境中的性能。

Conclusion: MobileBench-OL填补了移动GUI代理基准测试与现实环境之间的差距，通过综合评估任务执行、推理能力和噪声鲁棒性，为GUI代理的发展提供了更全面的评估工具。数据和代码将在接受后发布。

Abstract: Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.

</details>


### [49] [Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space](https://arxiv.org/abs/2601.20339)
*Yangyi Shen,Tianjian Feng,Jiaqi Han,Wen Wang,Tianlang Chen,Chunhua Shen,Jure Leskovec,Stefano Ermon*

Main category: cs.CL

TL;DR: 提出Order-Token Search方法，通过联合搜索生成顺序和token值来探索扩散语言模型的解码轨迹空间，在数学推理和代码生成基准上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型的解码方法只遵循单一轨迹，限制了在轨迹空间的探索能力，需要更好的方法来利用扩散模型的顺序无关生成特性。

Method: 提出Order-Token Search方法，核心是似然估计器来评估去噪动作的得分，支持稳定的剪枝和高效探索多样化的解码轨迹，实现生成顺序和token值的联合搜索。

Result: 在GSM8K、MATH500、Countdown和HumanEval基准上分别取得3.1%、3.8%、7.9%和6.8%的绝对提升，匹配或超越了经过diffu-GRPO后训练的d1-LLaDA模型。

Conclusion: 联合搜索是推进扩散语言模型解码能力的关键组件，Order-Token Search方法有效利用了扩散模型的顺序无关特性，实现了更好的轨迹空间探索。

Abstract: Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.

</details>


### [50] [Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents](https://arxiv.org/abs/2601.20412)
*Qihao Wang,Yue Hu,Mingzhe Lu,Jiayue Wu,Yanbing Liu,Yuanmin Tang*

Main category: cs.CL

TL;DR: 论文提出基于认知负荷理论的评估框架，将任务复杂度分解为内在负荷和外在负荷，并构建可参数化调整认知负荷的基准测试ToolLoad-Bench，用于精确诊断LLM使用工具的能力边界。


<details>
  <summary>Details</summary>
Motivation: 当前LLM工具使用能力的评估主要关注最终准确率，但无法揭示认知瓶颈和真实能力边界。需要从简单的性能评分转向诊断工具，以理解模型的实际限制。

Method: 1. 基于认知负荷理论建立评估框架；2. 将任务复杂度分解为内在负荷（使用新颖的工具交互图形式化）和外在负荷（任务表述模糊性）；3. 构建ToolLoad-Bench基准，支持参数化调整认知负荷；4. 通过控制实验评估模型性能。

Result: 随着认知负荷增加，模型性能出现明显下降点，能够精确映射每个模型的能力边界。框架预测与实证结果高度校准，为理解智能体限制提供了原则性方法。

Conclusion: 该框架为LLM工具使用能力的诊断评估提供了理论基础和实践工具，能够精确识别认知瓶颈，为构建更高效系统奠定基础。

Abstract: The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.

</details>


### [51] [SpeechMapper: Speech-to-text Embedding Projector for LLMs](https://arxiv.org/abs/2601.20417)
*Biswesh Mohapatra,Marcely Zanon Boito,Ioan Calapodescu*

Main category: cs.CL

TL;DR: SpeechMapper提出了一种高效训练语音到LLM嵌入的方法，通过预训练和简短指令微调减少过拟合，在语音翻译和口语问答任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 当前语音LLM通过投影层连接语音基础模型和LLM，需要大量计算资源训练所有组件，容易产生任务和提示过拟合，需要更高效、鲁棒的方法

Method: SpeechMapper采用两阶段方法：1）在廉价硬件上预训练语音模型（不含LLM）；2）通过简短1K步指令微调将预训练块连接到目标LLM，支持任务无关和任务特定的指令微调

Result: 在任务无关设置下，SpeechMapper媲美IWSLT25最佳指令跟随语音LLM；在任务特定设置下，超越该模型且需要更少数据和计算；在语音翻译和口语问答任务上表现优异

Conclusion: SpeechMapper提供了一种实用、可扩展的方法，无需大规模指令微调即可实现高效、泛化性强的语音-LLM集成

Abstract: Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.

</details>


### [52] [Hopes and Fears -- Emotion Distribution in the Topic Landscape of Finnish Parliamentary Speech 2000-2020](https://arxiv.org/abs/2601.20424)
*Anna Ristilä,Otto Tarkka,Veronika Laippala,Kimmo Elo*

Main category: cs.CL

TL;DR: 该研究分析了芬兰议会2000-2020年演讲中的情感表达，发现不同议题的情感强度存在差异，并证实了议会演讲中积极情感的增长趋势。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将议会讨论视为同质整体，忽视了议题特定的情感模式。虽然人们对议会中最具情感色彩的议题有直觉认知，但缺乏对不同议题情感表达的系统研究。本研究旨在填补这一空白。

Method: 使用情感分析模型，从共时和历时两个角度，分析芬兰议会2000-2020年间演讲议题中的情感表达。

Result: 研究结果强化了议会演讲中积极情感增长的趋势证据，并为议会辩论中议题特定的情感表达提供了新的见解。

Conclusion: 议会讨论中的情感表达具有议题特异性，不同议题引发的情感强度不同。研究为理解议会辩论的情感维度提供了实证基础。

Abstract: Existing research often treats parliamentary discourse as a homogeneous whole, overlooking topic-specific patterns. Parliamentary speeches address a wide range of topics, some of which evoke stronger emotions than others. While everyone has intuitive assumptions about what the most emotive topics in a parliament may be, there has been little research into the emotions typically linked to different topics. This paper strives to fill this gap by examining emotion expression among the topics of parliamentary speeches delivered in Eduskunta, the Finnish Parliament, between 2000 and 2020. An emotion analysis model is used to investigate emotion expression in topics, from both synchronic and diachronic perspectives. The results strengthen evidence of increasing positivity in parliamentary speech and provide further insights into topic-specific emotion expression within parliamentary debate.

</details>


### [53] [PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use](https://arxiv.org/abs/2601.20439)
*Qihao Wang,Mingzhe Lu,Jiayue Wu,Yue Hu,Yanbing Liu*

Main category: cs.CL

TL;DR: PEARL框架通过离线探索和在线强化学习两阶段方法，显著提升LLM在复杂多轮工具调用中的规划与执行能力，在ToolHop基准上达到56.5%的最新成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在使用外部工具时面临复杂多轮调用的挑战，包括规划能力弱、工具幻觉、参数生成错误以及交互鲁棒性差等问题，需要更有效的解决方案。

Method: PEARL采用两阶段方法：1）离线阶段让智能体探索工具以学习有效使用模式和失败条件；2）在线强化学习阶段，通过群体相对策略优化（GRPO）训练专用规划器，使用精心设计的奖励函数提供规划质量信号。

Result: 在ToolHop和T-Eval基准测试中，PEARL显著优于现有方法，在ToolHop上达到56.5%的最新成功率，同时保持较低的工具调用错误率。

Conclusion: PEARL框架在解决工具使用的复杂规划挑战方面取得关键进展，有助于开发更鲁棒可靠的基于LLM的智能体。

Abstract: Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \textbf{56.5\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.

</details>


### [54] [MuVaC: AVariational Causal Framework for Multimodal Sarcasm Understanding in Dialogues](https://arxiv.org/abs/2601.20451)
*Diandian Guo,Fangfang Yuan,Cong Cao,Xixun Lin,Chuan Zhou,Hao Peng,Yanan Cao,Yanbing Liu*

Main category: cs.CL

TL;DR: MuVaC是一个变分因果推理框架，通过模仿人类认知机制来联合优化多模态讽刺检测（MSD）和多模态讽刺解释（MuSE），利用因果依赖关系和鲁棒特征融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究大多单独处理多模态讽刺检测或解释任务，即使有整合尝试也忽略了它们之间的因果依赖关系。社交媒体中讽刺的普遍性使得理解其真实意图成为重要但具有挑战性的任务。

Method: 提出MuVaC变分因果推理框架：1）从结构因果模型角度建模MSD和MuSE，建立变分因果路径定义联合优化目标；2）设计对齐-融合方法整合多模态特征；3）通过确保检测结果与解释的一致性增强推理可信度。

Result: 实验结果表明MuVaC在公开数据集上表现出优越性能，为理解多模态讽刺提供了新视角。

Conclusion: MuVaC通过模仿人类认知机制，利用因果推理联合优化讽刺检测和解释任务，实现了更鲁棒的多模态特征学习和更可信的推理过程。

Abstract: The prevalence of sarcasm in multimodal dialogues on the social platforms presents a crucial yet challenging task for understanding the true intent behind online content. Comprehensive sarcasm analysis requires two key aspects: Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE). Intuitively, the act of detection is the result of the reasoning process that explains the sarcasm. Current research predominantly focuses on addressing either MSD or MuSE as a single task. Even though some recent work has attempted to integrate these tasks, their inherent causal dependency is often overlooked. To bridge this gap, we propose MuVaC, a variational causal inference framework that mimics human cognitive mechanisms for understanding sarcasm, enabling robust multimodal feature learning to jointly optimize MSD and MuSE. Specifically, we first model MSD and MuSE from the perspective of structural causal models, establishing variational causal pathways to define the objectives for joint optimization. Next, we design an alignment-then-fusion approach to integrate multimodal features, providing robust fusion representations for sarcasm detection and explanation generation. Finally, we enhance the reasoning trustworthiness by ensuring consistency between detection results and explanations. Experimental results demonstrate the superiority of MuVaC in public datasets, offering a new perspective for understanding multimodal sarcasm.

</details>


### [55] [BMAM: Brain-inspired Multi-Agent Memory Framework](https://arxiv.org/abs/2601.20465)
*Yang Li,Jiaxiang Liu,Yusong Wang,Yujie Wu,Mingkun Xu*

Main category: cs.CL

TL;DR: BMAM是一种受大脑启发的多智能体记忆架构，通过功能专门化的子系统解决语言模型智能体在长期交互中的"灵魂侵蚀"问题，在LoCoMo基准测试中达到78.45%准确率。


<details>
  <summary>Details</summary>
Motivation: 语言模型智能体在长期交互中面临保持时间信息和行为一致性的挑战，作者称之为"灵魂侵蚀"。现有方法通常使用单一非结构化记忆存储，无法有效处理长时间跨度的信息。

Method: BMAM采用大脑认知记忆系统启发的方法，将记忆分解为四个功能专门化的子系统：情景记忆、语义记忆、显著性感知记忆和控制导向记忆，这些组件在互补的时间尺度上运作。情景记忆子系统受海马体启发，沿明确时间线组织记忆，并通过融合多个互补信号检索证据。

Result: 在LoCoMo基准测试的标准长时程评估设置下，BMAM达到78.45%的准确率。消融分析证实海马体启发的的情景记忆子系统在时间推理中起关键作用。

Conclusion: BMAM通过功能专门化的记忆架构有效解决了语言模型智能体的"灵魂侵蚀"问题，证明了受大脑启发的多组件记忆系统在长期交互任务中的有效性。

Abstract: Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.

</details>


### [56] [Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch](https://arxiv.org/abs/2601.20476)
*Evanfiya Logacheva,Arto Hellas,Tsvetomila Mihaylova,Juha Sorva,Ava Heinonen,Juho Leinonen*

Main category: cs.CL

TL;DR: 提出基于修辞结构理论(RST)的图表代码生成方法，通过上下文示例改善AI生成图表质量，减少事实性幻觉，提高与上下文的忠实度


<details>
  <summary>Details</summary>
Motivation: 生成式AI在计算机教育中广泛应用，但生成材料的质量引起教育者和学生的担忧，需要提高图表生成质量以符合用户期望

Method: 基于修辞结构理论(RST)的上下文示例方法，利用大语言模型生成图表代码，由计算机科学教育者评估150个生成图表的质量指标

Result: 方法减少了事实性幻觉率，提高了图表与上下文的忠实度，但生成质量因LLM随机性而波动；复杂文本上下文导致更高幻觉率，LLM难以检测自身错误

Conclusion: 基于RST的方法能改善AI生成图表质量，但LLM的随机性和幻觉问题仍需关注，评估数据集可用于自动化图表评估

Abstract: Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.

</details>


### [57] [Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models](https://arxiv.org/abs/2601.20546)
*Kumiko Nakajima,Jan Zuiderveld,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 论文提出Conditional Divergent Association Task (CDAT)来评估LLMs的创造力，发现传统DAT方法存在缺陷，而CDAT能更好地区分噪音与创造力，显示小模型可能更具创造性。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLMs创造力的方法（如DAT）缺乏人类创造力理论的坚实基础，只关注新颖性而忽略了适当性这一核心要素，导致评估结果难以解释且有效性存疑。

Method: 基于人类创造力理论（新颖性+适当性），提出Conditional Divergent Association Task (CDAT)，在上下文适当性的条件下评估新颖性，比DAT更好地区分噪音与创造力。

Result: DAT评估显示LLMs得分低于无创造力的基线模型，验证了DAT的无效性；CDAT评估发现较小模型家族往往最具创造力，而先进模型家族倾向于适当性但新颖性较低。

Conclusion: 训练和对齐可能使模型在创造力边界上向适当性偏移而降低新颖性，CDAT为评估LLMs创造力提供了更有效的方法，并发布了数据集和代码。

Abstract: Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.

</details>


### [58] [Single-Nodal Spontaneous Symmetry Breaking in NLP Models](https://arxiv.org/abs/2601.20582)
*Shalom Rosner,Ronit D. Gross,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 该论文展示了自然语言处理模型中自发对称性破缺现象，即使在确定性动态和有限架构下，注意力头、节点甚至单个节点层面都会出现对称性破缺，节点学习能力随数量增加呈现交叉变化，与自旋玻璃系统不同，每个节点功能都明确贡献于全局任务。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将统计力学中的自发对称性破缺概念扩展到自然语言处理模型，探索在确定性动态和有限架构下，NLP模型在预训练和微调过程中是否会出现类似的对称性破缺现象。

Method: 使用BERT-6架构在Wikipedia数据集上进行预训练，然后在FewRel分类任务上进行微调。通过分析注意力头、节点和单个节点的学习行为，研究对称性破缺现象。采用凸包分析对节点功能进行上界估计。

Result: 在NLP模型中观察到了自发对称性破缺现象，即使在确定性动态和有限架构下，注意力头、节点和单个节点层面都会出现对称性破缺。节点学习能力随数量增加呈现交叉变化：随着节点数增加，随机猜测概率降低，但节点协作能力增强，超过个体能力总和。

Conclusion: NLP模型中存在自发对称性破缺现象，与自旋玻璃系统不同，每个节点功能都明确贡献于全局网络任务。这种对称性破缺在预训练和微调过程中都会出现，为理解神经网络的学习机制提供了新的统计力学视角。

Abstract: Spontaneous symmetry breaking in statistical mechanics primarily occurs during phase transitions at the thermodynamic limit where the Hamiltonian preserves inversion symmetry, yet the low-temperature free energy exhibits reduced symmetry. Herein, we demonstrate the emergence of spontaneous symmetry breaking in natural language processing (NLP) models during both pre-training and fine-tuning, even under deterministic dynamics and within a finite training architecture. This phenomenon occurs at the level of individual attention heads and is scaled-down to its small subset of nodes and also valid at a single-nodal level, where nodes acquire the capacity to learn a limited set of tokens after pre-training or labels after fine-tuning for a specific classification task. As the number of nodes increases, a crossover in learning ability occurs, governed by the tradeoff between a decrease following random-guess among increased possible outputs, and enhancement following nodal cooperation, which exceeds the sum of individual nodal capabilities. In contrast to spin-glass systems, where a microscopic state of frozen spins cannot be directly linked to the free-energy minimization goal, each nodal function in this framework contributes explicitly to the global network task and can be upper-bounded using convex hull analysis. Results are demonstrated using BERT-6 architecture pre-trained on Wikipedia dataset and fine-tuned on the FewRel classification task.

</details>


### [59] [A Computational Approach to Language Contact -- A Case Study of Persian](https://arxiv.org/abs/2601.20592)
*Ali Basirat,Danial Namazifard,Navid Baradaran Hemmati*

Main category: cs.CL

TL;DR: 研究波斯语单语模型中间表征中的语言接触结构痕迹，发现句法信息对历史接触不敏感，而形态特征（格、性）受语言特定结构强烈影响


<details>
  <summary>Details</summary>
Motivation: 研究单语语言模型中间表征中是否存在语言接触的结构痕迹，以波斯语（历史上接触丰富的语言）为研究对象，探索模型对不同接触程度语言的表征模式

Method: 使用波斯语训练的单语模型，暴露于与波斯语有不同接触程度和类型的语言；量化中间表征中的语言信息量，评估不同形态句法特征在模型组件中的分布

Result: 普遍句法信息对历史接触不敏感，而形态特征（如格、性）受语言特定结构强烈影响；单语模型中的接触效应具有选择性和结构约束性

Conclusion: 语言接触在单语语言模型中的影响是选择性的，主要作用于形态特征而非句法结构，揭示了语言模型表征中历史接触的结构约束性

Abstract: We investigate structural traces of language contact in the intermediate representations of a monolingual language model. Focusing on Persian (Farsi) as a historically contact-rich language, we probe the representations of a Persian-trained model when exposed to languages with varying degrees and types of contact with Persian. Our methodology quantifies the amount of linguistic information encoded in intermediate representations and assesses how this information is distributed across model components for different morphosyntactic features. The results show that universal syntactic information is largely insensitive to historical contact, whereas morphological features such as Case and Gender are strongly shaped by language-specific structure, suggesting that contact effects in monolingual language models are selective and structurally constrained.

</details>


### [60] [AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios](https://arxiv.org/abs/2601.20613)
*Kaiyuan Chen,Qimin Wu,Taiyu Hou,Tianhao Tang,Xueyu Hu,Yuchen Hou,Bikun Li,Chengming Qian,Guoyin Wang,Haolin Chen,Haotong Tian,Haoye Zhang,Haoyu Bian,Hongbing Pan,Hongkang Zhang,Hongyi Zhou,Jiaqi Cai,Jiewu Rao,Jiyuan Ren,Keduan Huang,Lucia Zhu Huang,Mingyu Yuan,Naixu Guo,Qicheng Tang,Qinyan Zhang,Shuai Chen,Siheng Chen,Ting Ting Li,Xiaoxing Guo,Yaocheng Zuo,Yaoqi Guo,Yinan Wang,Yinzhou Yu,Yize Wang,Yuan Jiang,Yuan Tian,Yuanshuo Zhang,Yuxuan Liu,Yvette Yan Zeng,Zenyu Shan,Zihan Yin,Xiaobo Hu,Yang Liu,Yixin Ren,Yuan Gong*

Main category: cs.CL

TL;DR: 提出AgentIF-OneDay基准，评估AI代理能否通过自然语言指令完成多样化日常任务，包含工作流执行、隐含指令理解和迭代优化三类任务。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理评估过于关注任务难度，忽略了日常场景的多样性需求。普通用户对AI高级能力的感知有限，需要更贴近实际生活、工作、学习的评估框架。

Method: 构建包含104个任务、767个评分点的AgentIF-OneDay基准，分为三类：明确工作流执行、隐含指令理解、迭代优化。采用实例级评分标准和LLM验证与人工判断结合的评估流程。

Result: Gemini-3-Pro验证与人工判断达成80.1%一致率。评估显示基于API构建的代理产品和基于强化学习的ChatGPT代理处于第一梯队，领先的LLM API和开源模型已内化代理能力。

Conclusion: 需要更全面的代理评估框架来反映日常任务多样性，AI应用团队可利用现有LLM能力开发前沿代理产品，促进AI代理在日常场景中的实际应用。

Abstract: The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.

</details>


### [61] [P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering](https://arxiv.org/abs/2601.20649)
*Wenlin Zhong,Chengyuan Liu,Yiquan Wu,Bovin Tan,Changlong Sun,Yi Wang,Xiaozhong Liu,Kun Kuang*

Main category: cs.CL

TL;DR: P2S是一种新的自监督框架，通过路径忠实度奖励为推理过程提供细粒度监督，解决传统结果导向方法忽略推理步骤监督的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法如RLPR仅关注最终答案概率作为奖励信号，忽略了推理过程本身的逐步监督，导致奖励稀疏问题。需要一种能在通用领域推理任务中提供过程监督的方法。

Method: 提出概率过程监督(P2S)框架：1) 在强化学习过程中合成和过滤高质量参考推理链(gold-CoT)；2) 计算路径忠实度奖励(PFR)，基于当前推理前缀生成gold-CoT后缀的条件概率；3) 将PFR与结果奖励灵活结合，提供密集指导。

Result: 在阅读理解和医学问答基准测试中，P2S显著优于现有基线方法，验证了过程监督的有效性。

Conclusion: P2S通过提供细粒度的过程奖励，有效解决了奖励稀疏问题，为通用领域推理任务的强化学习提供了新的自监督框架。

Abstract: While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.

</details>


### [62] [A Dialectic Pipeline for Improving LLM Robustness](https://arxiv.org/abs/2601.20659)
*Sara Candussio*

Main category: cs.CL

TL;DR: 提出一种自对话的辩证管道方法，通过让语言模型自我反思和修正错误答案来减少幻觉，同时保持模型的泛化能力，无需领域特定微调或额外验证器训练。


<details>
  <summary>Details</summary>
Motivation: 当前减少语言模型幻觉的方法（如领域特定微调或训练独立验证器）需要大量计算资源且限制模型的知识领域，需要一种既能保持泛化能力又能提高输出质量的方法。

Method: 设计辩证管道，让语言模型通过自对话进行反思和修正错误答案。管道各阶段都融入相关上下文（oracle-RAG设置），并研究上下文摘要和过滤的影响。在不同数据集和模型家族上进行实验。

Result: 辩证管道显著优于标准模型答案，且始终比仅使用思维链提示获得更高性能。

Conclusion: 自对话辩证管道是减少语言模型幻觉的有效方法，既能保持泛化能力，又无需大量计算资源或领域限制，优于现有方法。

Abstract: Assessing ways in which Language Models can reduce their hallucinations and improve the outputs' quality is crucial to ensure their large-scale use.
  However, methods such as fine-tuning on domain-specific data or the training of a separate \textit{ad hoc} verifier require demanding computational resources (not feasible for many user applications) and constrain the models to specific fields of knowledge.
  In this thesis, we propose a dialectic pipeline that preserves LLMs' generalization abilities while improving the quality of its answer via self-dialogue, enabling it to reflect upon and correct tentative wrong answers.
  We experimented with different pipeline settings, testing our proposed method on different datasets and on different families of models. All the pipeline stages are enriched with the relevant context (in an oracle-RAG setting) and a study on the impact of its summarization or its filtering is conducted.
  We find that our proposed dialectic pipeline is able to outperform by significative margins the standard model answers and that it consistently achieves higher performances than Chain-of-Thought only prompting.

</details>


### [63] [Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science](https://arxiv.org/abs/2601.20674)
*Juan Jose Rubio Jan,Jack Wu,Julia Ive*

Main category: cs.CL

TL;DR: LLMs应用于EHR结构化数据查询和临床文本信息提取，通过RAG和自动评估框架验证其在临床工作流中的潜力


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在电子健康记录数据分析中的实际应用能力，特别是结构化数据查询和临床文本信息提取这两个基础任务，以评估LLMs在临床工作流中的支持潜力

Method: 使用LLMs进行结构化数据查询（Python/Pandas）和通过RAG管道从非结构化临床文本中提取信息；开发自动生成合成问答对的评估框架；在MIMIC III数据集子集上进行实验，结合本地和API-based LLMs；采用精确匹配、语义相似度和人工判断的综合评估方法

Result: LLMs在结构化数据查询和临床文本信息提取方面表现出潜力，能够支持精确查询和准确信息提取，验证了其在临床工作流中的应用可行性

Conclusion: LLMs在电子健康记录数据分析中具有实际应用价值，能够有效支持结构化数据查询和临床文本信息提取，为临床工作流提供了新的技术支持方案

Abstract: This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.

</details>


### [64] [Efficient Multimodal Planning Agent for Visual Question-Answering](https://arxiv.org/abs/2601.20676)
*Zhuo Chen,Xinyu Geng,Xinyu Wang,Yong Jiang,Zhen Zhang,Pengjun Xie,Kewei Tu*

Main category: cs.CL

TL;DR: 提出一种训练多模态规划代理的方法，动态分解mRAG管道以解决VQA任务，在保持性能的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索增强生成(mRAG)方法在处理VQA任务时采用多阶段管道，存在依赖关系和效率限制，需要平衡效率与效果

Method: 训练多模态规划代理，动态分解mRAG管道，智能决定每个mRAG步骤的必要性，优化效率与效果的权衡

Result: 代理能减少冗余计算，搜索时间降低60%以上，减少昂贵工具调用，在六个数据集上平均表现优于所有基线方法

Conclusion: 该方法通过智能规划有效解决了mRAG在VQA任务中的效率瓶颈，实现了性能与效率的良好平衡

Abstract: Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.

</details>


### [65] [ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code](https://arxiv.org/abs/2601.20679)
*Mingqiao Mo,Yunlong Tan,Hao Zhang,Heng Zhang,Yangfan He*

Main category: cs.CL

TL;DR: ShieldedCode：首个保护感知框架，通过大规模配对数据集和分层依赖建模学习VMP保护代码的鲁棒表示，显著提升代码生成和二进制相似性检测性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成方面取得显著进展，但在软件保护方面潜力未充分挖掘。逆向工程持续威胁软件安全，而传统虚拟机保护（VMP）依赖僵化的基于规则的转换，设计成本高且易受自动化分析攻击。

Method: 构建大规模源代码与规范化VM实现配对数据集；引入指令内、指令前和指令间三个层次的分层依赖建模；联合优化语言建模与功能感知和保护感知的对比目标；提出保护效果优化任务量化不同VM变体；采用两阶段持续预训练和微调流程。

Result: 在L0 VM代码生成上达到26.95% Pass@1（GPT-4o为22.58%）；在二进制相似性检测Recall@1上比jTrans等SOTA方法提升10%；显著提升不同保护级别的鲁棒性。

Conclusion: 该框架首次实现了保护感知的代码表示学习，为基于学习的软件防御开辟了新研究方向，能够生成、比较和推理受保护代码。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, yet their potential for software protection remains largely untapped. Reverse engineering continues to threaten software security, while traditional virtual machine protection (VMP) relies on rigid, rule-based transformations that are costly to design and vulnerable to automated analysis. In this work, we present the first protection-aware framework that learns robust representations of VMP-protected code. Our approach builds large-scale paired datasets of source code and normalized VM implementations, and introduces hierarchical dependency modeling at intra-, preceding-, and inter-instruction levels. We jointly optimize language modeling with functionality-aware and protection-aware contrastive objectives to capture both semantic equivalence and protection strength. To further assess resilience, we propose a protection effectiveness optimization task that quantifies and ranks different VM variants derived from the same source. Coupled with a two-stage continual pre-training and fine-tuning pipeline, our method enables models to generate, compare, and reason over protected code. Extensive experiments show that our framework significantly improves robustness across diverse protection levels, opening a new research direction for learning-based software defense. In this work, we present ShieldedCode, the first protection-aware framework that learns robust representations of VMP-protected code. Our method achieves 26.95% Pass@1 on L0 VM code generation compared to 22.58% for GPT-4o., and improves binary similarity detection Recall@1 by 10% over state of art methods like jTrans.

</details>


### [66] [Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin](https://arxiv.org/abs/2601.20680)
*Ostap Vykhopen,Viktoria Skorik,Maxim Tereschenko,Veronika Solopova*

Main category: cs.CL

TL;DR: 该研究将HDBSCAN离线聚类替换为在线聚类算法，以解决社交媒体叙事监控系统中的可扩展性问题，实现了实时处理流式数据的能力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体监控系统在处理连续数据流时面临可扩展性挑战，传统的批处理聚类算法（如HDBSCAN）需要完全重新训练每个时间窗口，导致内存限制、计算效率低下，且无法实时适应演变的叙事。

Method: 采用三阶段架构（数据收集、建模、仪表板生成），评估多种在线聚类算法，使用滑动窗口模拟历史数据集（乌克兰信息空间），提出结合传统聚类指标（轮廓系数、Davies-Bouldin指数）和叙事指标（叙事独特性、偶然性和方差）的评估标准。

Result: 研究评估了在线聚类算法在集群质量保持、计算效率、内存占用和与现有工作流集成兼容性方面的表现，填补了批处理主题建模框架与社交媒体监控流式特性之间的关键差距。

Conclusion: 该研究为计算社会科学、危机信息学和叙事监控系统提供了重要启示，实现了从批处理到流式处理的转变，解决了社交媒体叙事智能系统的实时处理需求。

Abstract: Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.

</details>


### [67] [AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts](https://arxiv.org/abs/2601.20730)
*Shicheng Fang,Yuxin Wang,XiaoRan Liu,Jiahao Lu,Chuanyuan Tan,Xinchi Chen,Yining Zheng. Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: AgentLongBench：基于横向思维谜题的动态环境模拟基准，揭示LLM智能体在动态信息合成方面的关键弱点


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体基准大多为静态被动检索任务，无法模拟智能体与环境交互的复杂性（如非线性推理和迭代反馈），需要更真实的评估框架

Method: 引入AgentLongBench基准，基于横向思维谜题模拟环境推演，在知识密集和知识自由场景中生成严格的交互轨迹，测试32K到4M tokens的内存系统

Result: 实验显示智能体在静态检索方面表现良好，但在动态信息合成方面存在严重缺陷；性能下降主要由解决查询所需的最小token数驱动，大规模工具响应的高信息密度比长轮对话的内存碎片化更具挑战性

Conclusion: 需要开发能够处理动态信息合成的智能体系统，当前LLM智能体在真实工作流中面临显著挑战，信息密度是影响性能的关键因素

Abstract: The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.

</details>


### [68] [Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts](https://arxiv.org/abs/2601.20747)
*Elham Aghakhani,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 研究通过分析Reddit上AI情感支持相关帖子，发现用户参与度主要由结果叙述、信任和响应质量决定，而非单纯情感联结；积极情绪与任务目标对齐相关，而陪伴导向使用则更多涉及联盟错配和依赖风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于临床环境之外的情感支持和心理健康相关互动，但人们对这些系统在日常使用中的评价和关系了解甚少。需要研究用户如何在敏感的现实世界环境中解释语言技术。

Method: 基于技术接受模型和治疗联盟理论，开发理论驱动的标注框架，采用混合LLM-人工流程分析来自47个心理健康社区的5,126个Reddit帖子，大规模分析评价性语言、采纳相关态度和关系对齐。

Result: 参与度主要由叙述结果、信任和响应质量塑造，而非情感联结；积极情绪与任务和目标对齐最相关；陪伴导向使用更多涉及错配联盟和报告的风险（如依赖和症状加重）。

Conclusion: 这项工作展示了如何在大规模话语分析中操作理论建构的概念，并强调了研究用户如何在敏感的现实世界环境中解释语言技术的重要性，特别是对于心理健康应用。

Abstract: Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.

</details>


### [69] [Persona Prompting as a Lens on LLM Social Reasoning](https://arxiv.org/abs/2601.20757)
*Jing Yang,Moritz Hechtbauer,Elisabeth Khalilov,Evelyn Luise Brinkmann,Vera Schmitt,Nils Feldhus*

Main category: cs.CL

TL;DR: 研究探讨了在仇恨言论检测等社会敏感任务中，人格提示对LLM生成解释质量的影响，发现人格提示能改善分类但降低解释质量，且无法有效缓解模型偏见。


<details>
  <summary>Details</summary>
Motivation: 在社会敏感任务中，LLM生成解释的质量对用户信任和模型对齐至关重要。人格提示被广泛用于引导模型生成用户特定内容，但其对模型推理过程的影响尚未充分探索。

Method: 使用带有词级标注的数据集，研究LLM在不同模拟人口统计学人格条件下的推理变化。测量与不同人口群体人类标注的一致性，评估人格提示对模型偏见和人类对齐的影响。

Result: 1. 人格提示在最主观任务（仇恨言论）上改善分类但降低解释质量；2. 模拟人格无法与现实人口群体对齐，高人格间一致性表明模型难以被显著引导；3. 模型表现出持续的人口偏见和过度标记有害内容的倾向。

Conclusion: 人格提示在社会敏感任务中存在关键权衡：虽然能改善分类，但通常以解释质量为代价，且无法缓解潜在偏见，需要谨慎应用。

Abstract: For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.

</details>


### [70] [SERA: Soft-Verified Efficient Repository Agents](https://arxiv.org/abs/2601.20789)
*Ethan Shen,Danny Tormoen,Saurabh Shah,Ali Farhadi,Tim Dettmers*

Main category: cs.CL

TL;DR: SERA是一种高效训练代码代理的方法，通过监督微调实现开源模型在私有代码库专业化方面的突破，成本比强化学习低26倍，比之前合成数据方法低57倍。


<details>
  <summary>Details</summary>
Motivation: 开源权重代码代理理论上应比闭源系统有优势：能够针对私有代码库进行专业化训练，将仓库特定信息直接编码到权重中。但训练成本和复杂性使这一优势长期停留在理论层面。

Method: 提出Soft-Verified Efficient Repository Agents (SERA)方法，核心是Soft Verified Generation (SVG)技术，从单个代码仓库生成数千条轨迹，使用监督微调(SFT)进行训练。

Result: SERA在完全开源模型中达到最先进水平，性能与Devstral-Small-2等前沿开源权重模型相当。训练成本比强化学习低26倍，比之前合成数据方法低57倍。生成超过20万条合成轨迹用于分析。

Conclusion: 这项工作使针对私有代码库的专业化开源代码代理变得实用，将加速开源代码代理研究，展示开源模型在私有代码库专业化方面的优势。发布SERA作为Ai2开源代码代理系列的首个模型。

Abstract: Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.

</details>


### [71] [Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers](https://arxiv.org/abs/2601.20796)
*Yiran Huang,Karsten Roth,Quentin Bouniot,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: 论文研究了Transformer如何在多模态上下文中学习跨模态关联信息，发现当主模态数据多样性高时，次模态只需极低数据复杂度即可实现多模态上下文学习，这依赖于类似归纳的机制从匹配示例中复制标签。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的多模态大语言模型展现出上下文学习能力，但研究者想知道：Transformer如何从上下文示例中学习跨模态的信息关联？这有助于理解多模态上下文学习的机制基础。

Method: 通过在合成分类任务上训练小型Transformer进行受控实验，精确操纵数据统计和模型架构。首先回顾单模态上下文学习的核心原理，然后扩展到多模态设置，分析不同数据复杂度下的学习表现。

Result: 发现RoPE位置编码提高了上下文学习的数据复杂度阈值；多模态学习存在根本性不对称：当主模态数据多样性高时，次模态只需极低数据复杂度就能实现多模态上下文学习。机制分析显示这依赖于从匹配示例中复制标签的归纳式机制。

Conclusion: 研究为理解现代Transformer中的多模态上下文学习提供了机制基础，并引入了受控测试平台供未来研究使用。多模态训练能够跨模态精炼和扩展这些学习电路。

Abstract: Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.

</details>


### [72] [Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction](https://arxiv.org/abs/2601.20803)
*Aunabil Chakma,Mihai Surdeanu,Eduardo Blanco*

Main category: cs.CL

TL;DR: 提出一种基于句法语义结构相似性的示例选择策略，结合LLM生成示例，形成混合系统，在少样本关系抽取任务中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 针对单样本关系抽取中的上下文学习，需要自动获取更多相关示例来提升性能，现有方法（如LLM生成）在词汇选择和句子结构上存在局限性

Method: 提出基于句法语义结构相似性的示例选择策略，将新示例与单样本示例的底层结构进行匹配，并与LLM生成的示例结合形成混合系统

Result: 混合系统在FS-TACRED和FS-FewRel数据集上表现优异，在FS-TACRED上达到SOTA性能，在FewRel子集上也有显著提升，且在不同LLM家族（Qwen和Gemma）上都有良好迁移性

Conclusion: 基于结构相似性的示例选择策略与LLM生成示例形成互补，混合方法能更全面地捕捉目标关系，在少样本关系抽取任务中优于单一方法

Abstract: This paper presents several strategies to automatically obtain additional examples for in-context learning of one-shot relation extraction. Specifically, we introduce a novel strategy for example selection, in which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example. We show that this method results in complementary word choices and sentence structures when compared to LLM-generated examples. When these strategies are combined, the resulting hybrid system achieves a more holistic picture of the relations of interest than either method alone. Our framework transfers well across datasets (FS-TACRED and FS-FewRel) and LLM families (Qwen and Gemma). Overall, our hybrid selection method consistently outperforms alternative strategies and achieves state-of-the-art performance on FS-TACRED and strong gains on a customized FewRel subset.

</details>


### [73] [Linear representations in language models can change dramatically over a conversation](https://arxiv.org/abs/2601.20834)
*Andrew Kyle Lampinen,Yuxuan Li,Eghbal Hosseini,Sangnie Bhardwaj,Murray Shanahan*

Main category: cs.CL

TL;DR: 语言模型表示中的线性方向对应高层概念，这些表示在对话中会动态变化，导致事实性等概念的表征发生显著转变，这对可解释性和控制带来挑战。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型表示在对话过程中的动态变化，理解高层概念（如事实性）的表征如何随对话上下文演变，这对模型可解释性和控制具有重要意义。

Method: 通过分析模拟对话中线性表示的变化，研究不同模型家族和层级的表示动态，包括使用不同模型生成的对话脚本进行回放实验，以及通过显式提示对比效果。

Result: 发现对话中表示会发生显著变化：对话开始时表征为事实的信息可能在结束时变为非事实，反之亦然；这些变化具有内容依赖性，对话相关信息变化明显而通用信息保持稳定；即使使用不同模型生成的脚本回放也能产生类似变化。

Conclusion: 语言模型表示在对话中会动态适应上下文，这挑战了静态特征解释和探测方法的有效性，表明需要开发考虑上下文动态性的新方法来理解和控制模型行为。

Abstract: Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.

</details>


### [74] [When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation](https://arxiv.org/abs/2601.20858)
*David Tan,Pinzhen Chen,Josef van Genabith,Koel Dutta Chowdhury*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在基准测试中存在污染问题，导致分数虚高，将记忆伪装成泛化能力，在跨语言场景中这种记忆甚至会转移到"未污染"的语言中。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基准测试中可能存在污染问题，导致性能评估失真，特别是在多语言翻译任务中，这种污染会掩盖模型的实际泛化能力，将记忆错误地表现为理解能力。

Method: 使用FLORES-200翻译基准作为诊断工具，研究两个7-8B参数的多语言指令调优模型：在FLORES上训练过的Bloomz和未受污染的Llama作为对照。通过源端扰动（如改写和命名实体替换）来测试记忆的持久性。

Result: 确认了Bloomz存在FLORES污染，并发现机器翻译污染可以是跨方向的，即使未见过某些翻译方向，由于目标端记忆，性能也会被虚假提升。记忆的回忆在各种源端扰动下仍然持续，但命名实体替换能一致降低BLEU分数。

Conclusion: 基准污染会严重扭曲模型性能评估，命名实体替换可作为检测受污染模型中记忆的有效探测方法，揭示了当前基准测试方法的局限性。

Abstract: Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to "uncontaminated" languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz's FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [75] [OptAgent: an Agentic AI framework for Intelligent Building Operations](https://arxiv.org/abs/2601.20005)
*Zixin Jiang,Weili Xu,Bing Dong*

Main category: eess.SY

TL;DR: 该研究提出了一种端到端的智能AI物理信息机器学习环境，用于可扩展的建筑能源建模、仿真、控制和自动化，包含物理一致的PIML数字环境和具有11个专业代理的智能AI层。


<details>
  <summary>Details</summary>
Motivation: 建筑脱碳的迫切需求要求从人工密集型工程工作流程转向与物理基础数字环境交互的智能代理，实现未来自主建筑能源运营的范式转变。

Method: 构建了模块化、物理一致的PIML数字环境（涵盖建筑热动力学、HVAC和分布式能源资源），以及具有11个专业代理和72个MCP工具的智能AI层，支持端到端的多步骤能源分析执行。

Result: 案例研究展示了多领域、多代理协调评估系统升级对能源使用、运营成本、热舒适性和灵活性的影响；大规模基准测试（约4000次运行）系统评估了工作流程在准确性、令牌消耗、执行时间和推理成本方面的性能。

Conclusion: 该工作为在脱碳和电网交互的建筑运营中部署智能AI系统建立了可扩展、物理基础的基础，量化了智能模式设计、模型规模、任务复杂性和协调机制的影响，为未来实际应用提供了关键经验。

Abstract: The urgent need for building decarbonization calls for a paradigm shift in future autonomous building energy operation, from human-intensive engineering workflows toward intelligent agents that interact with physics-grounded digital environments. This study proposes an end-to-end agentic AI-enabled Physics-Informed Machine Learning (PIML) environment for scalable building energy modeling, simulation, control, and automation. The framework consists of (1) a modular and physics-consistent PIML digital environment spanning building thermal dynamics, Heating, Ventilation, and Air Conditioning (HVAC), and distributed energy resources (DER) for grid-interactive energy management; and (2) an agentic AI layer with 11 specialist agents and 72 Model Context Protocol (MCP) tools that enable end-to-end execution of multi-step energy analytics. A representative case study demonstrates multi-domain, multi-agent coordination for assessing how system and control upgrades affect energy use, operating cost, thermal comfort, and flexibility. In addition, a large-scale benchmark (about 4000 runs) systematically evaluates workflow performance in terms of accuracy, token consumption, execution time, and inference cost. The results quantify the impacts of intelligence mode design, model size, task complexity, and orchestrator-specialist coordination, and provide key lessons for building future agentic AI systems in real-world building energy applications. This work establishes a scalable, physics-grounded foundation for deploying agentic AI in decarbonized and grid-interactive building operations.

</details>


### [76] [Control systems for synthetic biology and a case-study in cell fate reprogramming](https://arxiv.org/abs/2601.20135)
*Domitilla Del Vecchio*

Main category: eess.SY

TL;DR: 本文综述了合成生物学中控制系统工程的应用，重点讨论如何通过生物分子反馈和前馈控制架构实现细胞中特定调节因子浓度的精确控制，以应对环境不确定性和扰动，并应用于细胞治疗和细胞命运重编程等领域。


<details>
  <summary>Details</summary>
Motivation: 细胞治疗和再生医学中的细胞命运重编程等应用需要精确控制细胞内特定调节因子的浓度，但环境不确定性和扰动会影响这些生物分子"工厂"的动态特性，因此需要开发能够实现鲁棒控制的生物分子控制系统。

Method: 采用控制系统工程方法，将生物分子过程视为需要控制的"工厂"，分析扰动来源及其对系统动态的影响，然后介绍通过合成生物学工程实现的生物分子反馈和前馈控制架构，这些控制策略受到生物分子过程可实现性的约束。

Result: 论文展示了生物分子反馈和前馈控制在细胞命运重编程中的具体应用，例如控制主调节因子水平以将皮肤细胞重编程为多能干细胞，证明了这些控制策略在应对环境扰动方面的有效性。

Conclusion: 虽然复杂的控制算法可以在计算机中实现，但在细胞内通过生物分子过程实现受到限制。论文总结了生物分子控制设计的当前挑战和未来研究方向，强调了在合成生物学中开发可行控制策略的重要性。

Abstract: This paper gives an overview of the use of control systems engineering in synthetic biology, motivated by applications such as cell therapy and cell fate reprogramming for regenerative medicine. A ubiquitous problem in these and other applications is the ability to control the concentration of specific regulatory factors in the cell accurately despite environmental uncertainty and perturbations. The paper describes the origin of these perturbations and how they affect the dynamics of the biomolecular ``plant'' to be controlled. A variety of biomolecular control implementations are then introduced to achieve robustness of the plant's output to perturbations and are grouped into feedback and feedforward control architectures. Although sophisticated control laws can be implemented in a computer today, they cannot be necessarily implemented inside the cell via biomolecular processes. This fact constraints the set of feasible control laws to those realizable through biomolecular processes that can be engineered with synthetic biology. After reviewing biomolecular feedback and feedforward control implementations, mostly focusing on the author's own work, the paper illustrates the application of such control strategies to cell fate reprogramming. Within this context, a master regulatory factor needs to be controlled at a specific level inside the cell in order to reprogram skin cells to pluripotent stem cells. The article closes by highlighting on-going challenges and directions of future research for biomolecular control design.

</details>


### [77] [C-AoEI-Aware Cross-Layer Optimization in Satellite IoT Systems: Balancing Data Freshness and Transmission Efficiency](https://arxiv.org/abs/2601.20183)
*Yuhua Zhao,Tiejun Lv,Ke Wang*

Main category: eess.SY

TL;DR: 提出跨层优化框架C-AoEI解决卫星物联网中信息新鲜度与传输效率的权衡问题


<details>
  <summary>Details</summary>
Motivation: 卫星物联网面临传播延迟、动态衰落和带宽稀缺的三难困境，传统L-HARQ的回溯解码导致年龄模糊，标准AoI指标无法准确反映数据新鲜度与传输效率的权衡关系

Method: 提出跨层错误信息年龄(C-AoEI)新指标，推导闭式表达式，建立多GBS场景下的分组级编码L-HARQ方案和自适应算法，联合优化编码和决策阈值

Result: 相比传统方案，传输效率提高31.8%，C-AoEI降低17.2%，对小区间干扰和信道变化具有鲁棒性

Conclusion: C-AoEI框架为设计高效、低延迟的下一代卫星物联网协议提供了理论基础，解决了信息新鲜度与传输效率的关键权衡问题

Abstract: Satellite-based Internet of Things (S-IoT) faces a fundamental trilemma: propagation delay, dynamic fading, and bandwidth scarcity. While Layer-coded Hybrid ARQ (L-HARQ) enhances reliability, its backtracking decoding introduces age ambiguity, undermining the standard Age of Information (AoI) metric and obscuring the critical trade-off between data freshness and transmission efficiency. To bridge this gap, we propose a novel cross-layer optimization framework centered on a new metric, the Cross-layer Age of Error Information (C-AoEI). We derive a closed-form expression for C-AoEI, explicitly linking freshness to system parameters, establishing an explicit analytical connection between freshness degradation and channel dynamics. Building on this, we develop a packet-level encoded L-HARQ scheme for multi-GBS scenarios and an adaptive algorithm that jointly optimizes coding and decision thresholds. Extensive simulations demonstrate the effectiveness of our proposed framework: it achieves 31.8% higher transmission efficiency and 17.2% lower C-AoEI than conventional schemes. The framework also proves robust against inter-cell interference and varying channel conditions, providing a foundation for designing efficient, latency-aware next-generation S-IoT protocols.

</details>


### [78] [A Data-Driven Krasovskii-Based Approach for Safety Controller Design of Time-Delayed Uncertain Polynomial Systems](https://arxiv.org/abs/2601.20298)
*Omid Akbarzadeh,MohammadHossein Ashoori,Amy Nejati,Abolfazl Lavaei*

Main category: eess.SY

TL;DR: 提出基于数据驱动的鲁棒Krasovskii控制屏障证书(RK-CBC)和鲁棒安全控制器(R-SC)综合框架，用于具有未知动力学、有界扰动和时不变时滞的离散时间输入仿射多项式系统，仅使用观测的输入-状态数据。


<details>
  <summary>Details</summary>
Motivation: 现有控制屏障证书方法主要针对已知系统或无时滞系统，对于具有未知动力学、时滞和扰动的系统，安全综合面临两大挑战：1) 系统数学模型不可用；2) 安全条件需要显式考虑时滞对系统演化的影响并保持对未知扰动的鲁棒性。

Method: 基于Krasovskii控制屏障证书，将经典CBC扩展到显式处理时滞系统，通过在屏障构造中聚合时滞分量。仅使用有限时间范围内收集的输入-状态数据，将综合问题转化为数据驱动的平方和(SOS)优化程序，直接从观测轨迹中合成RK-CBC和R-SC，无需显式系统模型。

Result: 在存在未知扰动和时滞的情况下，保证了无限时间范围内的鲁棒安全性。通过三个案例研究（包括两个物理系统）验证了所提方法的有效性。

Conclusion: 开发了一种纯数据驱动的框架，成功解决了具有未知动力学、时滞和扰动的离散时间多项式系统的安全综合问题，无需系统模型即可保证鲁棒安全性。

Abstract: We develop a data-driven framework for the synthesis of robust Krasovskii control barrier certificates (RK-CBC) and corresponding robust safety controllers (R-SC) for discrete-time input-affine uncertain polynomial systems with unknown dynamics, while explicitly accounting for unknown-but-bounded disturbances and time-invariant delays using only observed input-state data. Although control barrier certificates have been extensively studied for safety analysis of control systems, existing work on unknown systems with time delays, particularly in the presence of disturbances, remains limited. The challenge of safety synthesis for such systems stems from two main factors: first, the system's mathematical model is unavailable; and second, the safety conditions should explicitly incorporate the effects of time delays on system evolution during the synthesis process, while remaining robust to unknown disturbances. To address these challenges, we develop a data-driven framework based on Krasovskii control barrier certificates, extending the classical CBC formulation for delay-free systems to explicitly account for time delays by aggregating delayed components within the barrier construction. The proposed framework relies solely on input-state data collected over a finite time horizon, enabling the direct synthesis of RK-CBC and R-SC from observed trajectories without requiring an explicit system model. The synthesis is cast as a data-driven sum-of-squares (SOS) optimization program, yielding a structured design methodology. As a result, robust safety is guaranteed in the presence of unknown disturbances and time delays over an infinite time horizon. The effectiveness of the proposed method is demonstrated through three case studies, including two physical systems.

</details>


### [79] [Efficient Trajectory Design and Communication Scheduling for Dual-UAV Jamming-Aided Secure Communication Networks](https://arxiv.org/abs/2601.20314)
*Xinran Wang,Peng Wu,Xiaopeng Yuan,Yulin Hu,Anke Schmeink*

Main category: eess.SY

TL;DR: 提出协作连续悬停-飞行(co-SHF)结构，用于双无人机协同干扰辅助的安全通信网络，通过优化轨迹和调度来最大化最小保密吞吐量。


<details>
  <summary>Details</summary>
Motivation: 研究双无人机协同干扰辅助的安全通信网络，其中一个无人机向多个地面用户传输机密数据，另一个协作无人机提供保护性干扰对抗地面窃听者。需要解决连续时间轨迹优化和收发器间紧密时空耦合带来的挑战。

Method: 首次表征最优轨迹结构，证明其遵循协作连续悬停-飞行(co-SHF)结构：两个无人机访问有限数量的同步协同悬停点对，在飞行段至少一个无人机以最大速度移动。采用最小距离近似连续防碰撞约束，在连续凸近似(SCA)方法中使用保密吞吐量的凹下界。

Result: 相比时间离散化和无干扰基准，提出的co-SHF设计提高了最小保密性和用户公平性，同时显著减少了运行时间。

Conclusion: 提出的co-SHF结构将无限维非凸问题转化为有限维形式，通过减少优化变量和约束实现了低计算复杂度，有效解决了双无人机协同干扰安全通信网络的轨迹优化问题。

Abstract: We study dual-unmanned aerial vehicle (UAV) jamming-aided secure communication networks, in which one UAV delivers confidential data to multiple ground users (GUs), while a cooperative UAV provides protective interference against a ground eavesdropper. To enforce fairness, we maximize the minimum secrecy throughput across GUs by jointly designing trajectories and communication scheduling. The key difficulty lies in the continuous-time nature of UAV trajectories and the tight space-time coupling between the transmitter and the jammer, which jointly render the problem infinite-dimensional and nonconvex. To address these challenges, we characterize, for the first time, the structure of the optimal trajectories and rigorously prove that they follow a collaborative successive hover-and-fly (co-SHF) structure, where the two UAVs visit a limited number of synchronized co-hovering point pairs, and during each flight segment at least one UAV moves at maximum speed. Leveraging this structure, we reformulate the problem into a finite-dimensional form, without loss of optimality, over hovering and turning points, hovering durations, and scheduling. For tractability, we adopt a minimum-distance approximation of continuous anti-collision constraints and employ concave lower bounds on secrecy throughput within a successive convex approximation (SCA) method, which converges and, thanks to the co-SHF reduction in optimization variables and constraints, achieves low computational complexity. Numerical results show that, compared with time-discretization and no-jamming benchmarks, the proposed co-SHF design improves the min-secrecy and user fairness while requiring significantly less runtime.

</details>


### [80] [Neural Cooperative Reach-While-Avoid Certificates for Interconnected Systems](https://arxiv.org/abs/2601.20324)
*Jingyuan Zhou,Haoze Wu,Kaidi Yang*

Main category: eess.SY

TL;DR: 提出神经协同可达-避障证书框架，通过动态局部化向量控制李雅普诺夫和障碍函数捕获合作动态，实现可扩展的训练与验证，确保大规模互联系统中神经控制器的形式化保证。


<details>
  <summary>Details</summary>
Motivation: 为大规模互联系统中的神经网络控制器提供形式化保证是一个基本挑战。现有方法在捕获合作交互和可扩展验证方面存在不足，需要新的框架来确保此类控制器的安全部署。

Method: 提出神经协同可达-避障证书，采用动态局部化向量控制李雅普诺夫和障碍函数，通过状态依赖的邻域结构捕获合作动态，提供分散化的全局指数稳定性和安全性证书。开发可扩展的训练验证框架，通过约束优化目标联合合成控制器和神经证书，并利用充分条件考虑建模误差确保形式化保证。引入结构重用机制在子结构同构系统间转移控制器和证书以提高可扩展性。

Result: 在多机器人协调和车辆编队等场景进行广泛实验验证。结果表明该框架能确保经过认证的协同可达-避障性能，同时保持强大的控制性能。

Conclusion: 提出的神经协同可达-避障证书框架成功解决了大规模互联系统中神经网络控制器的形式化保证问题，通过可扩展的训练验证方法和结构重用机制，为合作系统的安全部署提供了有效解决方案。

Abstract: Providing formal guarantees for neural network-based controllers in large-scale interconnected systems remains a fundamental challenge. In particular, using neural certificates to capture cooperative interactions and verifying these certificates at scale is crucial for the safe deployment of such controllers. However, existing approaches fall short on both fronts. To address these limitations, we propose neural cooperative reach-while-avoid certificates with Dynamic-Localized Vector Control Lyapunov and Barrier Functions, which capture cooperative dynamics through state-dependent neighborhood structures and provide decentralized certificates for global exponential stability and safety. Based on the certificates, we further develop a scalable training and verification framework that jointly synthesizes controllers and neural certificates via a constrained optimization objective, and leverages a sufficient condition to ensure formal guarantees considering modeling error. To improve scalability, we introduce a structural reuse mechanism to transfer controllers and certificates between substructure-isomorphic systems. The proposed methodology is validated with extensive experiments on multi-robot coordination and vehicle platoons. Results demonstrate that our framework ensures certified cooperative reach-while-avoid while maintaining strong control performance.

</details>


### [81] [Reducing End-to-End Latency of Cause-Effect Chains with Shared Cache Analysis](https://arxiv.org/abs/2601.20427)
*Yixuan Zhu,Yinkang Gao,Bo Zhang,Xiaohang Gong,Binze Jiang,Lei Gong,Wenqi Lou,Teng Wang,Chao Wang,Xi Li,Xuehai Zhou*

Main category: eess.SY

TL;DR: 本文提出一个针对多核共享缓存平台上多链系统的端到端延迟分析框架，通过提取调度信息和因果链结构特征，构建细粒度的内存访问上下文，获得更准确的WCET估计，从而降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 在多核共享缓存平台上分析因果链的端到端延迟仍存在未解决问题。传统方法假设所有共享缓存访问都会缺失，导致WCET高估，影响延迟分析准确性。如何有效整合调度信息到WCET分析中面临两个挑战：如何利用链的结构特征优化共享缓存分析，以及如何在避免状态空间爆炸的同时提高分析精度。

Method: 提出新颖的端到端延迟分析框架，提取调度信息和因果链结构特征，在基本块级别构建细粒度、可扩展的跨核内存访问上下文，进行时间敏感的共享缓存分析，获得更准确的WCET估计（TSC-WCET），进而推导端到端延迟。

Result: 在双核和四核系统上进行实验，结果显示在某些配置下，因果链的平均最大端到端延迟分别降低了34%和26%。

Conclusion: 该框架通过整合调度信息和链结构特征，实现了更准确的WCET估计和端到端延迟分析，有效解决了多核共享缓存平台上因果链延迟分析的挑战。

Abstract: Cause-effect chains, as a widely used modeling method in real-time embedded systems, are extensively applied in various safety-critical domains. End-to-end latency, as a key real-time attribute of cause-effect chains, is crucial in many applications. But the analysis of end-to-end latency for cause-effect chains on multicore platforms with shared caches still presents an unresolved issue. Traditional methods typically assume that the worst-case execution time (WCET) of each task in the cause-effect chain is known. However, in the absence of scheduling information, these methods often assume that all shared cache accesses result in misses, leading to an overestimation of WCET and, consequently, affecting the accuracy of end-to-end latency. However, effectively integrating scheduling information into the WCET analysis process of the chains may introduce two challenges: first, how to leverage the structural characteristics of the chains to optimize shared cache analysis, and second, how to improve analysis accuracy while avoiding state space explosion.
  To address these issues, this paper proposes a novel end-to-end latency analysis framework designed for multi-chain systems on multicore platforms with shared caches. This framework extracts scheduling information and structural characteristics of cause-effect chains, constructing fine-grained and scalable inter-core memory access contexts at the basic block level for time-sensitive shared cache analysis. This results in more accurate WCET (TSC-WCET) estimates, which are then used to derive the end-to-end latency. Finally, we conduct experiments on dual-core and quad-core systems with various cache configurations, which show that under certain settings, the average maximum end-to-end latency of cause-effect chains is reduced by up to 34% and 26%.

</details>


### [82] [A Timing-Anomaly Free Dynamic Scheduling on Heterogeneous Systems](https://arxiv.org/abs/2601.20445)
*Yixuan Zhu,Yinkang Gao,Lei Gong,Binze Jiang,Xiaohang Gong,Zihan Wang,Cheng Tang,Wenqi Lou,Teng Wang,Chao Wang,Xi Li,Xuehai Zhou*

Main category: eess.SY

TL;DR: 提出首个异构系统的时序异常无关动态调度算法，通过确定性执行约束消除时序异常，实现安全且紧密的最坏情况响应时间估计。


<details>
  <summary>Details</summary>
Motivation: 异构系统采用动态调度算法提高资源利用率和调度灵活性，但会导致时序异常现象，即局部执行时间减少反而增加整体系统执行时间。这使得最坏情况响应时间分析变得复杂，传统分析要么过于悲观要么不安全，通常需要穷举状态空间探索来确保正确性。

Method: 提出确定性动态执行算法，通过应用确定性执行约束来部分限制运行时任务的资源分配和执行顺序。基于异构系统调度的形式化执行进度模型，证明所提执行约束的正确性和消除时序异常的能力。提出两种生成执行约束的方法：1) 从现有调度算法产生的执行轨迹直接推导；2) 基于启发式方法构建执行约束以进一步减少最坏情况响应时间。

Result: 在合成生成的DAG任务集和各种系统配置下的实验结果表明，与传统动态调度算法相比，该方法不仅消除了时序异常，还有效减少了最坏情况响应时间和响应时间抖动。

Conclusion: 该论文提出了首个时序异常无关的动态调度算法，通过确定性执行约束实现了安全且紧密的最坏情况响应时间估计，解决了异构系统中动态调度带来的时序异常问题，提高了系统可预测性。

Abstract: Heterogeneous systems commonly adopt dynamic scheduling algorithms to improve resource utilization and enhance scheduling flexibility. However, such flexibility may introduce timing anomalies, wherein locally reduced execution times can lead to an increase in the overall system execution time. This phenomenon significantly complicates the analysis of Worst-Case Response Time (WCRT), rendering conventional analysis either overly pessimistic or unsafe, and often necessitating exhaustive state-space exploration to ensure correctness.
  To address this challenge, this paper presents the first timing-anomaly-free dynamic scheduling algorithm for heterogeneous systems, referred to as Deterministic Dynamic Execution. It achieves a safe and tight WCRT estimate through a single offline simulation execution. The core idea is to apply deterministic execution constraints, which partially restrict the resource allocation and execution order of tasks at runtime. Based on a formally defined execution progress model for heterogeneous system scheduling, we prove the correctness of the proposed execution constraints and their ability to eliminate timing anomalies. Furthermore, we propose two methods to generate execution constraints. The first method derives execution constraints directly from the execution traces produced by existing scheduling algorithms. The second method is a heuristic-based approach that constructs execution constraints, enabling further reduction of the WCRT. Experimental results on synthetically generated DAG task sets under various system configurations demonstrate that, compared to traditional dynamic scheduling algorithms, our approach not only eliminates timing anomalies but also effectively reduces both the WCRT and response time jitter.

</details>


### [83] [Tilt-based Aberration Estimation in Transmission Electron Microscopy](https://arxiv.org/abs/2601.20561)
*Jilles S. van Hulst,Erik M. Franken,Bart J. Janssen,W. P. M. H.,Heemels,Duarte J. Antunes*

Main category: eess.SY

TL;DR: 提出一种基于卡尔曼滤波和实验设计优化的TEM像差估计方法，通过优化电子束倾斜序列显著减少对准时间


<details>
  <summary>Details</summary>
Motivation: 透射电镜(TEM)的像差会降低图像质量，需要准确估计像差系数进行补偿，但传统方法耗时且像差会随时间漂移

Method: 利用电子束倾斜与图像位移的关系，使用卡尔曼滤波估计像差系数；通过最小化预测误差协方差迹(A-最优准则)优化倾斜序列；采用梯度下降、滚动时域和多起点方法求解非凸优化问题；使用期望最大化估计样本相关噪声特性

Result: 优化后的倾斜模式显著优于朴素方法，像差和漂移模型能准确捕捉物理现象，对准时间从几分钟减少到不到一分钟

Conclusion: 该方法能有效估计和补偿TEM像差，大幅提高对准效率，为实时像差校正提供了实用解决方案

Abstract: Transmission electron microscopes (TEMs) enable atomic-scale imaging but suffer from aberrations caused by lens imperfections and environmental conditions, reducing image quality. These aberrations can be compensated by adjusting electromagnetic lenses, but this requires accurate estimates of the aberration coefficients, which can drift over time. This paper introduces a method for the estimation of aberrations in TEM by leveraging the relationship between an induced electron beam tilt and the resulting image shift. The method uses a Kalman filter (KF) to estimate the aberration coefficients from a sequence of image shifts, while accounting for the drift of the aberrations over time. The applied tilt sequence is optimized by minimizing the trace of the predicted error covariance in the KF, which corresponds to the A-optimality criterion in experimental design. We show that this optimization can be performed offline, as the cost criterion is independent of the actual measurements. The resulting non-convex optimization problem is solved using a gradient-based, receding-horizon approach with multi-starts. Additionally, we develop an approach to estimate specimen-dependent noise properties using expectation maximization (EM), which are then used to tailor the tilt pattern optimization to the specific specimen being imaged. The proposed method is validated on a real TEM set-up with several optimized tilt patterns. The results show that optimized patterns significantly outperform naive approaches and that the aberration and drift model accurately captures the underlying physical phenomena. In total, the alignment time is reduced from typically several minutes to less than a minute compared to the state-of-the-art.

</details>


### [84] [Distributed Learning over Noisy Communication Networks](https://arxiv.org/abs/2601.20723)
*Emrah Akyol,Marcos Vasconcelos*

Main category: eess.SY

TL;DR: 研究图上的二元协调博弈，在邻居动作通过有噪通信链路传递时，使用对数线性学习算法。分析了两种通信机制：快速通信和快照模式，并建立了通信资源与协调质量之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 研究在现实通信约束下的多智能体协调问题。传统协调博弈假设完美信息共享，但实际通信链路存在噪声（如BSC/BEC信道）。需要理解噪声通信如何影响学习动态和协调结果。

Method: 1) 将每条边建模为二进制对称信道(BSC)或二进制擦除信道(BEC)；2) 分析两种操作机制：快速通信机制（使用信道平均收益）和快照机制（使用单次噪声实现）；3) 引入有限通信预算K，通过重传和重复编码在两种机制间插值；4) 扩展到异构链路可靠性。

Result: 1) 快速通信机制产生Gibbs采样器动态，信道可靠性仅通过标量衰减系数影响；2) 快照机制产生一般不可逆马尔可夫链，但其高温展开的漂移与快速Gibbs采样器匹配；3) 通信预算K在快照和快速行为间插值，提供重传和重复编码的通信理论解释；4) 数值实验量化了通信资源与稳态协调质量间的权衡。

Conclusion: 噪声通信下的协调博弈学习动态可通过信道可靠性参数化。通信资源（如重传次数）与协调质量存在明确权衡，为设计通信高效的分布式协调算法提供了理论基础。异构链路可靠性可通过有效边权重自然处理。

Abstract: We study binary coordination games over graphs under log-linear learning when neighbor actions are conveyed through explicit noisy communication links. Each edge is modeled as either a binary symmetric channel (BSC) or a binary erasure channel (BEC). We analyze two operational regimes. For binary symmetric and binary erasure channels, we provide a structural characterization of the induced learning dynamics. In a fast-communication regime, agents update using channel-averaged payoffs; the resulting learning dynamics coincide with a Gibbs sampler for a scaled coordination potential, where channel reliability enters only through a scalar attenuation coefficient. In a snapshot regime, agents update from a single noisy realization and ignore channel statistics; the induced Markov chain is generally nonreversible, but admits a high-temperature expansion whose drift matches that of the fast Gibbs sampler with the same attenuation. We further formalize a finite-$K$ communication budget, which interpolates between snapshot and fast behavior as the number of channel uses per update grows. This viewpoint yields a communication-theoretic interpretation in terms of retransmissions and repetition coding, and extends naturally to heterogeneous link reliabilities via effective edge weights. Numerical experiments illustrate the theory and quantify the tradeoff between communication resources and steady-state coordination quality.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia Fermüller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: 这篇论文基于2025年8月的工作坊，探讨了神经科学与人工智能之间的协同作用，提出了NeuroAI概念，旨在通过神经科学启发改进AI算法，同时深化对生物神经计算的理解。


<details>
  <summary>Details</summary>
Motivation: 神经科学与人工智能在过去几年都取得了显著进展，但两者之间联系松散。作者希望通过整合这两个领域，创建神经科学启发的人工智能（NeuroAI），以提升AI算法的范围和效率，同时改变我们对生物神经计算的理解方式。

Method: 基于2025年8月举办的工作坊，聚焦于具身化、语言与通信、机器人学、人类与机器学习、神经形态工程等子领域，评估当前进展并探索未来方向。收集了多位顶尖研究人员的个人观点，并附上了研究人员和学员进行的SWOT分析。

Result: 识别了神经科学与AI之间的当前和未来协同领域，提出了NeuroAI的发展框架。通过SWOT分析描述了NeuroAI的益处和风险，汇集了领域专家对NeuroAI多样化的观点和见解。

Conclusion: 倡导发展NeuroAI（神经科学启发的人工智能），认为这不仅能显著提升AI算法的范围和效率，还能改变我们对生物神经计算的理解方式，为两个领域带来互利共赢的发展前景。

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


### [86] [Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups](https://arxiv.org/abs/2601.20487)
*Nico Mutzner,Taha Yasseri,Heiko Rauhut*

Main category: cs.AI

TL;DR: 研究发现AI代理与人类在群体合作规范中表现相似，合作水平不受AI标签影响，支持规范性等价模式


<details>
  <summary>Details</summary>
Motivation: 研究AI代理如何影响小群体中合作规范的出现和维持，填补了现有研究主要关注二元互动而忽视群体动态的空白

Method: 采用在线实验，使用重复四玩家公共物品游戏，每组包含三名人类参与者和一个机器人（被标记为人类或AI），机器人采用三种预设策略之一：无条件合作、有条件合作或搭便车

Result: 合作主要由互惠群体动态和行为惯性驱动，这些规范机制在不同条件下运作相同，人类和AI标签下的合作水平无显著差异；后续囚徒困境中规范持续性也无差异

Conclusion: 合作规范足够灵活，可以扩展到AI代理，模糊了人类与AI在集体决策中的界限，支持规范性等价模式

Abstract: The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.

</details>


### [87] [Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning](https://arxiv.org/abs/2601.20014)
*Shuhui Qu*

Main category: cs.AI

TL;DR: SQ-BCP是一种在部分可观测环境下进行推理时规划的方法，通过显式表示前提条件状态、针对性自查询和桥接假设来解决LLM规划中的幻觉问题，并使用双向搜索和拉回验证器保证目标兼容性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时规划中面临部分可观测性问题：当任务关键前提条件未在查询时指定时，模型容易产生幻觉或违反硬约束的计划。

Method: 提出自我查询双向分类规划(SQ-BCP)：1)显式表示前提条件状态(Sat/Viol/Unk)；2)通过针对性自查询或桥接假设解决未知条件；3)执行双向搜索并使用拉回验证器作为目标兼容性的分类证书；4)仅使用基于距离的分数进行排序和剪枝。

Result: 在WikiHow和RecipeNLG任务中，当隐藏前提条件时，SQ-BCP将资源违规率分别降低到14.9%和5.8%（最佳基线为26.0%和15.7%），同时保持竞争力的参考质量。

Conclusion: SQ-BCP通过显式处理部分可观测性，显著减少了LLM规划中的违规率，提供了理论保证的规划方法，在保持计划质量的同时提高了可靠性。

Abstract: Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\texttt{Sat}/\texttt{Viol}/\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \textbf{14.9\%} and \textbf{5.8\%} (vs.\ \textbf{26.0\%} and \textbf{15.7\%} for the best baseline), while maintaining competitive reference quality.

</details>


### [88] [Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints](https://arxiv.org/abs/2601.20021)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出模糊范畴论规划(FCP)，将模糊逻辑融入范畴论规划框架，处理自然语言规划中的模糊谓词，通过t-范数组合计划质量，同时保持硬约束验证能力。


<details>
  <summary>Details</summary>
Motivation: 自然语言规划常涉及模糊谓词（如"合适的替代品"、"足够稳定"），现有范畴论规划器将其视为二元判断，需要阈值处理，这会丢失有意义的质量差异，且无法追踪多步计划中的质量退化。

Method: 提出FCP框架：1) 为每个动作标注[0,1]的适用度；2) 使用Lukasiewicz t-范数组合计划质量；3) 通过拉回验证保持硬约束检查；4) 使用LLM进行模糊适用度评估；5) 支持基于剩余运算的中间相遇搜索。

Result: 在PDDL3偏好/超额订阅基准和RecipeNLG-Subs（基于RecipeNLG构建的食谱替代规划基准）上评估。FCP在RecipeNLG-Subs上相比LLM-only和ReAct基线提高了成功率，减少了硬约束违反，同时与经典PDDL3规划器保持竞争力。

Conclusion: FCP成功将模糊逻辑与范畴论规划结合，有效处理自然语言规划中的模糊谓词，在保持硬约束验证能力的同时，能够追踪计划质量退化，在食谱替代规划等实际任务中表现优异。

Abstract: Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.

</details>


### [89] [Insight Agents: An LLM-Based Multi-Agent System for Data Insights](https://arxiv.org/abs/2601.20048)
*Jincheng Bai,Zhenyu Zhang,Jennifer Zhang,Zhihuai Zhu*

Main category: cs.AI

TL;DR: 开发Insight Agents对话式多智能体数据洞察系统，为电商卖家提供个性化数据和业务洞察，通过自动化信息检索帮助卖家发现工具、理解数据，提升决策效率。


<details>
  <summary>Details</summary>
Motivation: 电商卖家面临两大挑战：难以发现和有效利用可用程序工具；难以理解和利用各种工具产生的丰富数据。需要开发一个系统来帮助卖家减少决策所需努力，加快良好业务决策速度。

Method: 基于规划-执行范式的LLM驱动端到端智能体系统，采用分层多智能体结构：管理智能体（结合轻量级编码器-解码器模型的OOD检测和基于BERT分类器的路由）+两个工作智能体（数据呈现和洞察生成）。数据模型采用API基础的战略规划将查询分解为细粒度组件，动态注入领域知识增强洞察生成。

Result: 系统已在亚马逊美国卖家上线，基于人工评估达到90%的高准确率，P90延迟低于15秒。

Conclusion: Insight Agents作为电商卖家的力量倍增器，通过减少决策所需努力和加快决策速度，能够推动卖家增量采用，有效解决电商卖家面临的工具发现和数据理解挑战。

Abstract: Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.

</details>


### [90] [Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control](https://arxiv.org/abs/2601.20090)
*Amirmohammad Farzaneh,Salvatore D'Oro,Osvaldo Simeone*

Main category: cs.AI

TL;DR: 提出CCG框架，为LLM驱动的智能体控制提供反事实推理的可靠性保证


<details>
  <summary>Details</summary>
Motivation: 用户在使用LLM智能体执行意图后，可能想知道如果当初用不同方式表达意图会有什么不同结果，需要可靠的反事实推理方法

Method: 将用户-LLM智能体-环境交互建模为结构因果模型，利用测试时缩放通过概率溯因生成候选反事实结果，通过离线校准阶段确保可靠性保证

Result: 在无线网络控制用例中展示CCG性能，相比简单的重新执行基线有显著优势

Conclusion: CCG框架能为LLM驱动的智能体控制提供具有高概率保证的反事实推理，具有实际应用价值

Abstract: Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.

</details>


### [91] [Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis](https://arxiv.org/abs/2601.20206)
*Zixuan Xiao,Chunguang Hu,Jun Ma*

Main category: cs.AI

TL;DR: 提出多模态LLM智能体框架，用于城市新建公园发展监测，通过数据对齐机制和领域工具包解决传统遥感方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于遥感影像的变化检测方法在高层次智能分析方面存在明显局限，难以满足当前城市规划管理的需求。面对城市公园发展监测中日益增长的多模态数据分析需求，现有方法缺乏灵活的分析能力。

Method: 提出多模态LLM智能体框架，设计通用的横向和纵向数据对齐机制确保多模态数据一致性，构建特定工具包缓解LLM因缺乏领域知识而产生的幻觉问题。

Result: 相比vanilla GPT-4o和其他智能体，该方法实现了稳健的多模态信息融合与分析，为城市公园发展监测提供了可靠且可扩展的解决方案。

Conclusion: 该多模态LLM智能体框架充分利用LLM的语义理解和推理能力，能够满足城市公园发展监测中的多样化需求，为城市规划和管理提供有效支持。

Abstract: As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.

</details>


### [92] [Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.20221)
*Hang Zhang,Ruheng Wang,Yuelyu Ji,Mingu Kwak,Xizhi Wu,Chenyu Li,Li Zhang,Wenqi Shi,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: 提出一种名为$\method$的代理框架，通过训练医疗推理验证器在评估过程中迭代查询外部医学语料库，结合工具增强验证和迭代强化学习，显著提升医疗推理准确性并大幅降低采样预算需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗推理基准测试中表现良好，但在临床部署中需要严格验证以确保事实准确性。现有奖励模型方法存在两个局限：仅产生标量奖励值而无明确理由，且依赖单次检索无法在验证过程中进行自适应知识访问。

Method: $\method$框架训练医疗推理验证器在评估过程中迭代查询外部医学语料库，结合工具增强验证和迭代强化学习范式（仅需轨迹级监督），并采用自适应课程机制动态调整训练数据分布。

Result: 在四个医疗推理基准测试中，$\method$相比现有方法取得显著提升：MedQA准确率提高23.5%，MedXpertQA提高32.0%（相对于基础生成器）。更重要的是，$\method$相比先前奖励模型基线实现了8倍的采样预算需求降低。

Conclusion: 基于动态检索证据的验证为构建更可靠的医疗推理系统提供了原则性路径，表明将验证过程扎根于动态获取的证据能够显著提升医疗推理的可靠性。

Abstract: Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\method$ demonstrates an $\mathbf{8\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.

</details>


### [93] [Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models](https://arxiv.org/abs/2601.20305)
*Zhenchen Tang,Songlin Yang,Zichuan Wang,Bo Peng,Yang Li,Beibei Dong,Jing Dong*

Main category: cs.AI

TL;DR: 提出SEER框架，通过内生重提示机制解决统一多模态模型中理解与生成之间的认知鸿沟问题


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型虽然具备强大的理解能力，但这种理解往往无法有效指导生成过程，存在"认知鸿沟"：模型缺乏如何改进自身生成过程的理解

Method: 提出内生重提示机制，将模型的理解从被动编码过程转变为显式生成推理步骤。SEER框架建立两阶段内生循环：1) RLVR通过课程学习激活模型的潜在评估能力，产生高保真内生奖励信号；2) RLMT利用该信号优化生成推理策略

Result: SEER在评估准确性、重提示效率和生成质量方面持续优于最先进的基线方法，且不牺牲通用多模态能力

Conclusion: 内生重提示机制能有效弥合统一多模态模型中的认知鸿沟，SEER框架仅需少量样本即可实现自我进化的评估和重提示能力

Abstract: Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.

</details>


### [94] [ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue](https://arxiv.org/abs/2601.20323)
*Hyunseung Chung,Jungwoo Oh,Daeun Kyung,Jiho Kim,Yeonsu Kwon,Min-Gyu Kim,Edward Choi*

Main category: cs.AI

TL;DR: ECG-Agent：首个基于LLM的工具调用代理，用于多轮ECG对话，解决现有模型缺乏多轮对话能力、设备端效率和精确ECG测量理解的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在ECG应用中存在局限性：缺乏多轮对话能力、设备端效率不足、对ECG测量（如PQRST间期）理解不精确，无法满足真实世界应用需求。

Method: 提出ECG-Agent工具调用代理框架，并创建ECG-MTD数据集（包含真实用户-助手多轮对话）。开发不同规模的ECG-Agent，从设备端可运行到大型代理。

Result: ECG-Agent在响应准确性上优于基线ECG-LLMs。设备端代理在响应准确性、工具调用能力和幻觉评估方面与大型代理表现相当，证明其实际应用可行性。

Conclusion: ECG-Agent成功解决了现有ECG-LLMs的局限性，设备端代理的可行性能推动ECG分析在真实临床场景中的部署应用。

Abstract: Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.

</details>


### [95] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: AMA框架通过多智能体协作实现自适应记忆管理，显著提升长上下文任务性能并减少80%的token消耗


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆系统存在检索粒度僵化、维护策略积累过重、更新机制粗糙等问题，导致存储信息与任务需求不匹配，以及逻辑不一致性随时间累积

Method: 提出AMA框架，采用分层记忆设计和多智能体协作：Constructor和Retriever实现多粒度记忆构建和自适应查询路由；Judge验证相关性和一致性；Refresher执行针对性更新或删除过时条目

Result: 在具有挑战性的长上下文基准测试中，AMA显著优于现有最先进基线，同时相比全上下文方法减少约80%的token消耗，证明其在保持检索精度和长期记忆一致性方面的有效性

Conclusion: AMA框架通过多智能体协作管理多粒度记忆，有效解决了现有记忆系统的局限性，为LLM智能体提供了更高效、一致的自适应记忆管理方案

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [96] [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/2601.20379)
*Zhengbo Jiao,Hongyu Xian,Qinglong Wang,Yunpu Ma,Zhebo Wang,Zifan Zhang,Dezhang Kong,Meng Han*

Main category: cs.AI

TL;DR: PoT框架通过在线优化策略，让LLM从执行反馈中学习，显著提升复杂推理能力，4B模型在LiveCodeBench上超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂长程推理中存在困难，现有方法仅将执行反馈作为外部信号用于轨迹过滤或重写，未能内部化反馈来改进底层推理策略。受波普尔"猜想与反驳"认识论启发，认为智能需要从失败尝试中实时演化模型策略。

Method: 提出Policy of Thoughts (PoT)框架，将推理重新定义为实例内的在线优化过程。首先通过高效探索机制生成多样候选解，然后使用Group Relative Policy Optimization (GRPO)基于执行反馈更新瞬态LoRA适配器，实现闭环设计。

Result: PoT显著提升性能：4B模型在LiveCodeBench上达到49.71%准确率，超越GPT-4o和DeepSeek-V3，尽管模型规模小50倍以上。

Conclusion: PoT框架通过在线策略优化使LLM能够从执行反馈中学习，实现动态、实例特定的推理先验优化，为提升LLM复杂推理能力提供了有效途径。

Abstract: Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of "conjectures and refutations," we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.

</details>


### [97] [OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution](https://arxiv.org/abs/2601.20380)
*Le Zhang,Yixiong Xiao,Xinjiang Lu,Jingjia Cao,Yusai Zhao,Jingbo Zhou,Lang An,Zikan Feng,Wanxiang Sha,Yu Shi,Congxi Xiao,Jian Xiong,Yankai Zhang,Hua Wu,Haifeng Wang*

Main category: cs.AI

TL;DR: OmegaUse是一个通用的GUI代理模型，支持移动和桌面平台的自主任务执行，通过精心设计的数据构建管道和两阶段训练策略实现跨终端能力。


<details>
  <summary>Details</summary>
Motivation: GUI代理在实现基础模型完成真实世界任务方面具有巨大潜力，能够改变人机交互方式并提高人类生产力。当前需要构建一个既支持计算机使用又支持手机使用场景的通用GUI代理模型。

Method: 1) 数据构建：结合精心策划的开源数据集和新型自动合成框架，该框架整合了自底向上的自主探索和自顶向下的分类指导生成；2) 训练策略：采用两阶段方法，先进行监督微调建立基本交互语法，再进行组相对策略优化以改善空间定位和顺序规划；3) 模型架构：基于混合专家(MoE)骨干网络，平衡计算效率与代理推理能力。

Result: OmegaUse在多个GUI基准测试中表现出色：在ScreenSpot-V2上达到96.3%的SOTA分数，在AndroidControl上达到79.1%的步骤成功率。在新引入的OS-Nav基准套件上，在ChiM-Nav上达到74.24%步骤成功率，在Ubu-Nav上达到55.9%平均成功率。

Conclusion: OmegaUse是一个有效的通用GUI代理模型，通过精心设计的数据构建和训练策略，在移动和桌面平台上都展现出强大的跨终端自主任务执行能力，为GUI代理的发展提供了重要参考。

Abstract: Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.

</details>


### [98] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: CtrlCoT是一个双粒度CoT压缩框架，通过语义抽象和token级剪枝的协调，在保持推理正确性的同时显著减少token使用


<details>
  <summary>Details</summary>
Motivation: CoT提示虽然能提升LLM推理能力，但冗长的推理轨迹导致高延迟和高内存成本，需要压缩但现有方法要么过于保守要么过于激进，且难以结合两种方法

Method: 提出CtrlCoT框架：1)分层推理抽象生成多粒度语义CoT；2)逻辑保持蒸馏训练逻辑感知剪枝器保留关键推理线索；3)分布对齐生成使压缩轨迹与推理风格对齐

Result: 在MATH-500数据集上使用Qwen2.5-7B-Instruct模型，CtrlCoT减少30.7%的token使用，同时比最强基线提升7.6个百分点

Conclusion: CtrlCoT通过协调语义抽象和token级剪枝，实现了更高效可靠的推理，解决了现有CoT压缩方法的局限性

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [99] [PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs](https://arxiv.org/abs/2601.20539)
*Oguzhan Gungordu,Siheng Xiong,Faramarz Fekri*

Main category: cs.AI

TL;DR: PathWise是一个基于多智能体推理的自动启发式设计框架，通过世界模型规划和状态记忆实现更高效的启发式生成。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动启发式设计框架依赖固定的进化规则和静态提示模板，导致启发式生成短视、评估冗余、缺乏对如何推导新启发式的推理能力。

Method: 提出多智能体推理框架PathWise，将启发式生成建模为基于蕴涵图的序列决策过程。包含策略智能体规划进化动作、世界模型智能体生成启发式推演、批评智能体提供路由反思。

Result: 在多种组合优化问题上，PathWise能更快收敛到更好的启发式，在不同LLM骨干上具有良好泛化性，并能扩展到更大规模问题。

Conclusion: PathWise通过状态感知规划和推理，将LLM自动启发式设计从试错进化转向基于推理的规划，显著提升了启发式设计的效率和效果。

Abstract: Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

</details>


### [100] [Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function](https://arxiv.org/abs/2601.20554)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: 本文研究了部分可观测环境下基于ICVaR动态风险度量的风险敏感规划，提出了具有有限时间性能保证的ICVaR策略评估算法，并将三种在线规划算法扩展到ICVaR优化，实验证明能降低尾部风险。


<details>
  <summary>Details</summary>
Motivation: 传统部分可观测马尔可夫决策过程（POMDP）规划通常优化回报的期望值，忽略了尾部风险。在安全关键应用中，需要风险敏感的规划方法，考虑最坏情况下的结果，特别是对于部分可观测环境。

Method: 1. 开发了ICVaR策略评估算法，具有不依赖于动作空间大小的有限时间性能保证；2. 将三种在线规划算法（稀疏采样、PFT-DPW、POMCPOW）扩展到优化ICVaR值函数而非期望回报；3. 引入风险参数α（α=1恢复期望规划，α<1增加风险厌恶）；4. 为ICVaR稀疏采样建立了风险敏感目标下的有限时间性能保证，并提出了针对ICVaR的新探索策略。

Result: 在基准POMDP领域上的实验表明，提出的ICVaR规划器相比风险中性对应方法实现了更低的尾部风险。ICVaR稀疏采样算法具有理论性能保证，新的探索策略能有效优化风险敏感目标。

Conclusion: 本文成功将风险敏感规划扩展到部分可观测环境，通过ICVaR动态风险度量实现了尾部风险控制，为安全关键的部分可观测决策问题提供了有效的解决方案。

Abstract: We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.

</details>


### [101] [Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies](https://arxiv.org/abs/2601.20604)
*Gray Cox*

Main category: cs.AI

TL;DR: 论文提出通过结构化多模型对话测试AI对齐策略的方法框架，基于和平研究传统将对齐问题重构为关系问题而非控制问题，实验证明当前大语言模型能有效参与复杂对齐框架对话并产生新见解。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐策略缺乏实证测试方法，需要将AI对齐从控制问题重构为通过对话推理发展的关系问题，借鉴和平研究传统（利益协商、冲突转化、公共资源治理）来建立可测试的框架。

Method: 采用结构化多模型对话实验设计，为不同AI系统分配四种角色（提议者、响应者、监控者、翻译者），在六种条件下测试大语言模型对复杂对齐框架的参与能力，使用Claude、Gemini和GPT-4o进行72轮对话。

Result: AI系统能有效参与和平研究概念讨论，从不同架构视角提出互补性反对意见，并产生初始框架中未出现的新见解（如"VCW作为过渡框架"）。不同模型关注点不同：Claude强调验证挑战，Gemini关注偏见和可扩展性，GPT-4o突出实施障碍。

Conclusion: 该框架为研究人员提供了在实施前压力测试对齐提案的可复制方法，初步证据表明AI具备VCW所提出的对话推理能力。但对话更多关注过程要素而非AI本质的基础主张，未来研究方向包括人机混合协议和扩展对话研究。

Abstract: This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.
  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.
  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of "VCW as transitional framework." Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.
  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.

</details>


### [102] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: MathForge框架通过难度感知组策略优化(DGPO)和多方面问题重构(MQR)策略，从算法和数据两个角度针对更难问题提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在算法和数据层面都缺乏对更具挑战性问题的关注，这对提升模型未充分发展的能力很重要

Method: 提出MathForge框架：1) DGPO算法通过难度平衡组优势估计修正GRPO的隐式不平衡，并通过难度感知问题级加权优先处理更难问题；2) MQR策略从多个方面重构问题以增加难度同时保持原始正确答案

Result: MathForge在各种数学推理任务上显著优于现有方法

Conclusion: MathForge形成了协同循环：MQR扩展数据前沿，DGPO有效学习增强数据，共同提升数学推理能力

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [103] [Investigating the Development of Task-Oriented Communication in Vision-Language Models](https://arxiv.org/abs/2601.20641)
*Boaz Carmeli,Orr Paradise,Shafi Goldwasser,Yonatan Belinkov,Ron Meir*

Main category: cs.AI

TL;DR: LLM智能体能够发展出与标准自然语言不同的任务导向通信协议，这些协议具有高效性和隐蔽性，在协作推理任务中可能带来潜在风险。


<details>
  <summary>Details</summary>
Motivation: 研究LLM智能体是否能在协作任务中发展出不同于自然语言的任务导向通信协议，特别关注这些协议可能具备的高效性（更简洁地传递任务相关信息）和隐蔽性（外部观察者难以解读），这引发了关于透明度和控制的重要问题。

Method: 使用指称游戏框架，让视觉语言模型智能体进行通信，提供一个可控、可测量的环境来评估语言变体。通过实验观察智能体如何发展通信模式。

Result: 实验表明：1) VLM能够发展出有效的、适应任务的通信模式；2) 能够发展出对人类和外部智能体都难以解读的隐蔽协议；3) 观察到相似模型之间无需明确共享协议就能自发协调。

Conclusion: 任务导向通信既具有潜力也存在风险，指称游戏框架为未来研究提供了有价值的测试平台。这些发现强调了在开发LLM智能体时需要关注透明度和控制问题。

Abstract: We investigate whether \emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.

</details>


### [104] [Enterprise Resource Planning Using Multi-type Transformers in Ferro-Titanium Industry](https://arxiv.org/abs/2601.20696)
*Samira Yazdanpourmoghadam,Mahan Balal Pour,Vahid Partovi Nia*

Main category: cs.AI

TL;DR: 该论文提出使用多类型Transformer（MTT）架构统一解决作业车间调度问题（JSP）和背包问题（KP）等组合优化问题，并在标准基准数据集上展示了其竞争力，同时首次将多类型Transformer应用于实际制造业场景。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题如JSP和KP在运营研究、物流和企业资源规划中至关重要，传统方法需要复杂算法才能在实用时间内获得近似最优解。近年来深度学习特别是Transformer架构为这些问题的解决提供了新的可能性。

Method: 采用多类型Transformer（MTT）架构构建统一框架来处理不同类型的组合优化问题，利用多类型注意力机制捕捉问题中的复杂关系。

Result: 在JSP和KP的标准基准数据集上进行广泛实验评估，MTT在不同规模问题上都展现出有竞争力的性能，并成功应用于铁钛合金工业的实际制造场景。

Conclusion: 多类型Transformer为组合优化问题提供了有效的统一解决方案，首次在实际制造业应用中验证了其潜力，为传统优化问题提供了新的深度学习途径。

Abstract: Combinatorial optimization problems such as the Job-Shop Scheduling Problem (JSP) and Knapsack Problem (KP) are fundamental challenges in operations research, logistics, and eterprise resource planning (ERP). These problems often require sophisticated algorithms to achieve near-optimal solutions within practical time constraints. Recent advances in deep learning have introduced transformer-based architectures as promising alternatives to traditional heuristics and metaheuristics. We leverage the Multi-Type Transformer (MTT) architecture to address these benchmarks in a unified framework. We present an extensive experimental evaluation across standard benchmark datasets for JSP and KP, demonstrating that MTT achieves competitive performance on different size of these benchmark problems. We showcase the potential of multi-type attention on a real application in Ferro-Titanium industry. To the best of our knowledge, we are the first to apply multi-type transformers in real manufacturing.

</details>


### [105] [Implementing Metric Temporal Answer Set Programming](https://arxiv.org/abs/2601.20735)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Susana Hahn,Javier Romero,Torsten Schaub*

Main category: cs.AI

TL;DR: 提出一种计算性度量ASP方法，通过差异约束处理时间量化约束，解决时间粒度导致的扩展性问题


<details>
  <summary>Details</summary>
Motivation: 传统ASP难以表达量化时间约束（如持续时间和截止时间），且细粒度时间约束会显著加剧ASP的grounding瓶颈问题

Method: 利用ASP的差异约束扩展（一种简化的线性约束），将时间相关方面外部化处理，实现度量ASP与时间粒度的解耦

Result: 开发出不受时间精度影响的解决方案，有效解决了时间粒度导致的扩展性问题

Conclusion: 通过差异约束外部化处理时间约束，成功实现了可扩展的度量ASP方法，能够处理量化时间约束而不受时间粒度影响

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.

</details>


### [106] [REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence](https://arxiv.org/abs/2601.20784)
*Zishen Wan,Che-Kai Liu,Jiayi Qian,Hanchen Yang,Arijit Raychowdhury,Tushar Krishna*

Main category: cs.AI

TL;DR: REASON是一个针对神经符号AI中概率逻辑推理的加速框架，通过统一的DAG表示、自适应剪枝和树形处理架构，实现了12-50倍的速度提升和310-681倍的能效提升。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI系统虽然具有数据高效、可解释和鲁棒性强的优势，但其部署面临严重效率挑战，特别是概率逻辑推理成为效率瓶颈，在CPU和GPU上存在控制流不规则、算术强度低、内存访问不连续和硬件利用率差等问题。

Method: 1. 引入统一的DAG表示来捕获符号和概率模型的共同结构；2. 采用自适应剪枝和正则化技术；3. 设计可重构的树形处理架构，优化不规则遍历、符号推导和概率聚合；4. 通过可编程接口和多级流水线与GPU流多处理器紧密集成。

Result: 在6个神经符号工作负载上，REASON相比桌面和边缘GPU实现了12-50倍的速度提升和310-681倍的能效提升（基于TSMC 28nm工艺）。能够以0.8秒完成端到端任务，面积6mm²，功耗2.12W，实现实时概率逻辑推理。

Conclusion: 针对概率逻辑推理的专门加速对于实现实用和可扩展的神经符号AI至关重要，REASON作为下一代认知智能的基础系统架构，为实现实时推理提供了可行的硬件解决方案。

Abstract: Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.
  This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.

</details>


### [107] [MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents](https://arxiv.org/abs/2601.20831)
*Vishnu Sashank Dorbala,Dinesh Manocha*

Main category: cs.AI

TL;DR: MemCtrl：一种用于在线修剪记忆的新框架，通过可训练的记忆头μ增强多模态大语言模型，显著提升具身任务完成能力。


<details>
  <summary>Details</summary>
Motivation: 现有记忆压缩和检索系统（如RAG）通常将记忆视为大型离线存储空间，这不适合需要在严格内存和计算约束下在线操作的具身智能体。需要一种能够在探索过程中在线修剪记忆的方法。

Method: 提出MemCtrl框架，通过可训练的记忆头μ增强多模态大语言模型（MLLMs）。μ作为门控机制，决定在探索过程中保留、更新或丢弃哪些观察或反思。训练两种类型的μ：1）通过离线专家训练，2）通过在线强化学习训练。

Result: 在EmbodiedBench基准测试的多个子集上，μ增强的MLLMs平均提升约16%，特定指令子集提升超过20%。μ增强的MLLMs在长而复杂的指令类型上表现优异。

Conclusion: MemCtrl框架通过在线记忆修剪有效解决了具身智能体的内存约束问题，显著提升了任务完成能力，特别是在处理复杂指令时表现突出。

Abstract: Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.

</details>


### [108] [Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)](https://arxiv.org/abs/2601.20843)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 提出Deep Researcher架构，通过顺序研究计划优化和候选交叉算法，在博士级研究任务上超越现有深度研究代理，证明顺序扩展优于并行一致性范式


<details>
  <summary>Details</summary>
Motivation: 解决并行扩展范式的固有局限性，特别是知识孤岛问题，需要开发能够动态适应和整合研究过程的深度研究系统

Method: 采用顺序研究计划优化（通过反思）和候选交叉算法，前者维护全局研究上下文实现动态调整，后者部署多个LLM候选探索更大搜索空间，最后进行一次性报告生成

Result: 在DeepResearch Bench的100个博士级研究任务上获得46.21分，超越Claude Researcher、Nvidia AIQ Research Assistant等领先代理，且优于之前的Static DRA工作

Conclusion: Deep Researcher架构证明顺序扩展范式在深度研究任务上优于并行一致性范式，为复杂研究问题提供了有效的解决方案

Abstract: This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.

</details>


### [109] [SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models](https://arxiv.org/abs/2601.20856)
*Sebastiano Monti,Carlo Nicolini,Gianni Pellegrini,Jacopo Staiano,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文评估大语言模型的长时程规划能力，发现当解决方案需要超过25步时性能显著下降，表明存在固有的规划能力限制。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在复杂推理任务上的能力已得到广泛测试，但其长时程规划能力尚未被深入研究。本研究旨在系统评估当前最先进的大推理模型在规划和长时程推理方面的能力。

Method: 提出基于推箱子游戏的新基准测试，特意简化以隔离长时程规划与状态持久性。评估模型在需要不同步数的规划任务上的表现，并测试通过配备PDDL解析、验证和求解工具是否能改善性能。

Result: 发现当解决方案需要超过25步时，规划性能出现一致性的下降，表明前向规划能力存在基本限制。配备PDDL工具只能带来适度的改进，暗示固有的架构限制可能无法仅通过测试时扩展方法克服。

Conclusion: 大推理模型在长时程规划方面存在固有局限性，当规划步数超过25步时性能显著下降。仅通过外部工具集成无法完全克服这些架构限制，需要更根本的改进。

Abstract: Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [110] [Do Whitepaper Claims Predict Market Behavior? Evidence from Cryptocurrency Factor Analysis](https://arxiv.org/abs/2601.20336)
*Murad Farzulla*

Main category: q-fin.CP

TL;DR: 研究加密货币白皮书叙事与市场行为的对齐程度，发现两者关联微弱


<details>
  <summary>Details</summary>
Motivation: 加密货币项目通过白皮书阐述价值主张，本研究旨在检验这些技术声明是否与市场实际行为相符

Method: 构建结合零样本NLP分类（BART-MNLI）和CP张量分解的流程，比较三个空间：白皮书声明矩阵、市场统计数据、张量分解潜在因子，使用Procrustes旋转和Tucker一致性系数测试对齐

Result: 结果显示微弱对齐：声明-统计数据（φ=0.341, p=0.332）、声明-因子（φ=0.077, p=0.747）、统计数据-因子（φ=0.197, p<0.001）。跨资产分析显示异质性：NEAR、MKR、ATOM显示正对齐，而ENS、UNI、比特币偏离最大

Conclusion: 白皮书叙事与市场因子结构之间仅存在微弱对齐，有限样本量无法区分微弱对齐与无对齐，但可以明确拒绝强对齐假设

Abstract: Cryptocurrency projects articulate value propositions through whitepapers, making claims about functionality and technical capabilities. This study investigates whether these narratives align with observed market behavior. We construct a pipeline combining zero-shot NLP classification (BART-MNLI) with CP tensor decomposition to compare three spaces: (1) a claims matrix from 24 whitepapers across 10 semantic categories, (2) market statistics for 49 assets over two years of hourly data, and (3) latent factors from tensor decomposition (rank 2, 92.45% variance explained). Using Procrustes rotation and Tucker's congruence coefficient, we test alignment across 23 common entities.
  Results show weak alignment: claims-statistics (phi=0.341, p=0.332), claims-factors (phi=0.077, p=0.747), and statistics-factors (phi=0.197, p<0.001). The statistics-factors significance validates our methodology, confirming the pipeline detects relationships when present. Inter-model validation with DeBERTa-v3 yields 32% exact agreement but 67% top-3 agreement. Cross-sectional analysis reveals heterogeneous contributions: NEAR, MKR, ATOM show positive alignment while ENS, UNI, Bitcoin diverge most. Excluding Bitcoin confirms results are not driven by market dominance.
  We interpret findings as weak alignment between whitepaper narratives and market factor structure. Limited power (n=23) precludes distinguishing weak from no alignment, but strong alignment (phi>=0.70) can be confidently rejected. Implications for narrative economics and investment analysis are discussed.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [111] [Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data](https://arxiv.org/abs/2601.19936)
*Minseo Kwak,Jaehyung Kim*

Main category: cs.LG

TL;DR: 提出Gap-K%方法，通过分析LLM预训练优化动态，利用top-1预测与目标token之间的对数概率差距，结合滑动窗口捕获局部相关性，实现最先进的预训练数据检测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练语料库的不透明性引发隐私和版权担忧，现有基于token似然的方法忽视top-1预测差异和相邻token的局部相关性。

Method: Gap-K%方法基于LLM预训练优化动态分析，利用top-1预测token与目标token之间的对数概率差距，采用滑动窗口策略捕获局部相关性并缓解token级波动。

Result: 在WikiMIA和MIMIR基准测试中，Gap-K%实现了最先进的性能，在不同模型大小和输入长度下一致优于现有基线方法。

Conclusion: Gap-K%通过利用预训练优化动态中的梯度信号差异，有效解决了预训练数据检测问题，为隐私和版权保护提供了更可靠的解决方案。

Abstract: The opacity of massive pretraining corpora in Large Language Models (LLMs) raises significant privacy and copyright concerns, making pretraining data detection a critical challenge. Existing state-of-the-art methods typically rely on token likelihoods, yet they often overlook the divergence from the model's top-1 prediction and local correlation between adjacent tokens. In this work, we propose Gap-K%, a novel pretraining data detection method grounded in the optimization dynamics of LLM pretraining. By analyzing the next-token prediction objective, we observe that discrepancies between the model's top-1 prediction and the target token induce strong gradient signals, which are explicitly penalized during training. Motivated by this, Gap-K% leverages the log probability gap between the top-1 predicted token and the target token, incorporating a sliding window strategy to capture local correlations and mitigate token-level fluctuations. Extensive experiments on the WikiMIA and MIMIR benchmarks demonstrate that Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

</details>


### [112] [DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information](https://arxiv.org/abs/2601.19938)
*Adnan Ahmad,Chiara Boldrini,Lorenzo Valerio,Andrea Passarella,Marco Conti*

Main category: cs.LG

TL;DR: 提出一种新的去中心化联邦学习聚合方法，通过二阶信息近似生成共识权重，解决数据和模型异质性问题，提高收敛速度并降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习中，设备间的个体经验和交互程度差异导致数据和模型初始化异质性，造成本地模型参数差异大、收敛速度慢的问题。

Method: 通过近似本地模型在其本地数据集上的二阶信息来生成共识权重，利用这些权重缩放邻居更新，然后聚合到全局邻居表示中，从而解决参数级别的证据可信度变化问题。

Result: 在计算机视觉任务的大量实验中，该方法显示出本地模型具有更强的泛化能力，同时减少了通信成本。

Conclusion: 提出的基于二阶信息近似的共识权重聚合方法能有效处理去中心化联邦学习中的数据与模型异质性，提高模型性能并降低通信开销。

Abstract: Decentralized Federated Learning (DFL) is a serverless collaborative machine learning paradigm where devices collaborate directly with neighbouring devices to exchange model information for learning a generalized model. However, variations in individual experiences and different levels of device interactions lead to data and model initialization heterogeneities across devices. Such heterogeneities leave variations in local model parameters across devices that leads to slower convergence. This paper tackles the data and model heterogeneity by explicitly addressing the parameter level varying evidential credence across local models. A novel aggregation approach is introduced that captures these parameter variations in local models and performs robust aggregation of neighbourhood local updates. Specifically, consensus weights are generated via approximation of second-order information of local models on their local datasets. These weights are utilized to scale neighbourhood updates before aggregating them into global neighbourhood representation. In extensive experiments with computer vision tasks, the proposed approach shows strong generalizability of local models at reduced communication costs.

</details>


### [113] [oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction](https://arxiv.org/abs/2601.19939)
*Hyunmin Kim,Yukun Zhou,Rahul A. Jonas,Lie Ju,Sunjin Hwang,Pearse A. Keane,Siegfried K. Wagner*

Main category: cs.LG

TL;DR: 提出Oculomix分层采样策略，用于视网膜图像混合样本增强，通过患者和检查层级约束来保护患者特定属性，在心血管事件预测中优于传统图像级增强方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像级混合样本增强（如CutMix、MixUp）在训练transformer基础模型时会扰动患者特定属性（如共病和临床因素），因为这些方法只考虑图像和标签，不适用于需要保留患者特定特征的医学影像分析。

Method: 提出Oculomix分层采样策略，基于两个临床先验：1）同一患者同一时间点的图像共享相同属性；2）同一患者不同时间点的图像具有软时间趋势（发病率随时间增加）。该方法将混合空间约束到患者和检查层级，利用分层关系保护患者特定特征。

Result: 在大型多民族人群（Alzeye）的5年主要不良心血管事件预测中，使用ViT模型验证Oculomix，相比图像级CutMix和MixUp，AUROC提升高达3%，证明该方法在眼组学中的必要性和价值。

Conclusion: Oculomix通过分层采样策略有效保护患者特定属性，在视网膜图像分析中优于传统图像级混合增强方法，为眼组学预测系统性疾病提供了更有效的训练策略。

Abstract: Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.

</details>


### [114] [Continuous-Flow Data-Rate-Aware CNN Inference on FPGA](https://arxiv.org/abs/2601.19940)
*Tobias Habermann,Michael Mecik,Zhenyu Wang,César David Vera,Martin Kumm,Mario Garrido*

Main category: cs.LG

TL;DR: 提出一种数据流感知的连续流CNN架构，通过信号交织和硬件单元共享实现接近100%的硬件利用率，在FPGA上高效实现复杂CNN如MobileNet。


<details>
  <summary>Details</summary>
Motivation: CNN中的池化层和步长大于1的卷积层会减少数据量，导致全并行实现中数据率下降和硬件单元利用率低下，需要解决这一问题以实现高效FPGA实现。

Method: 分析CNN数据流，提出数据流感知的连续流CNN架构设计方法，通过交织低数据率信号、共享硬件单元和适当的并行化策略，实现高硬件利用率和全并行实现的吞吐量。

Result: 该方法显著节省算术逻辑资源，使得在单个FPGA上实现复杂CNN（如MobileNet）并保持高吞吐量成为可能。

Conclusion: 提出的数据流感知连续流CNN架构有效解决了CNN中数据率下降导致的硬件利用率问题，为在FPGA上高效实现复杂CNN提供了可行方案。

Abstract: Among hardware accelerators for deep-learning inference, data flow implementations offer low latency and high throughput capabilities. In these architectures, each neuron is mapped to a dedicated hardware unit, making them well-suited for field-programmable gate array (FPGA) implementation. Previous unrolled implementations mostly focus on fully connected networks because of their simplicity, although it is well known that convolutional neural networks (CNNs) require fewer computations for the same accuracy. When observing the data flow in CNNs, pooling layers and convolutional layers with a stride larger than one, the number of data at their output is reduced with respect to their input. This data reduction strongly affects the data rate in a fully parallel implementation, making hardware units heavily underutilized unless it is handled properly. This work addresses this issue by analyzing the data flow of CNNs and presents a novel approach to designing data-rate-aware, continuous-flow CNN architectures. The proposed approach ensures a high hardware utilization close to 100% by interleaving low data rate signals and sharing hardware units, as well as using the right parallelization to achieve the throughput of a fully parallel implementation. The results show that a significant amount of the arithmetic logic can be saved, which allows implementing complex CNNs like MobileNet on a single FPGA with high throughput.

</details>


### [115] [Latent Object Permanence: Topological Phase Transitions, Free-Energy Principles, and Renormalization Group Flows in Deep Transformer Manifolds](https://arxiv.org/abs/2601.19942)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.LG

TL;DR: 该论文通过几何和统计物理视角研究Transformer语言模型中多步推理能力的涌现，发现隐藏状态轨迹在临界深度处发生相变，形成可重用的瞬态类对象结构。


<details>
  <summary>Details</summary>
Motivation: 研究深度Transformer语言模型中多步推理能力如何从几何和统计物理角度涌现，理解隐藏状态在层间演化过程中的结构变化。

Method: 将隐藏状态轨迹视为隐式黎曼流形上的流，分析激活的层间协方差谱，跟踪与随机矩阵本体的偏差。使用基于稀疏性/局部化的序参数，形式化前向传递为离散粗粒化映射。

Result: 在模型规模1.5B-30B范围内观察到有效维度的急剧减少，表明相变发生：序参数在临界归一化深度γ_c≈0.42处出现不连续性。发现谱尾塌缩和表示空间中形成可重用的瞬态类对象结构。

Conclusion: Transformer语言模型中的多步推理能力通过相变机制涌现，形成稳定的"概念盆地"和可重用结构，逻辑可分性与谱衰减相关，为理解模型内部表示演化提供了理论框架。

Abstract: We study the emergence of multi-step reasoning in deep Transformer language models through a geometric and statistical-physics lens. Treating the hidden-state trajectory as a flow on an implicit Riemannian manifold, we analyze the layerwise covariance spectrum of activations, where $C^{(\ell)}=\mathbb{E}[h^{(\ell)}h^{(\ell)\top}]$, and track deviations from a random-matrix bulk. Across model scales (1.5B--30B), we observe a sharp reduction in effective dimensionality consistent with a phase transition: an order parameter based on sparsity/localization, $Ω(h)=1-\|h\|_1/(\sqrt{d}\|h\|_2)$, exhibits a discontinuity near a critical normalized depth $γ_c\approx 0.42$ in sufficiently large models. We formalize the forward pass as a discrete coarse-graining map and relate the appearance of stable "concept basins" to fixed points of this renormalization-like dynamics. The resulting low-entropy regime is characterized by a spectral tail collapse and by the formation of transient, reusable object-like structures in representation space, which we call Transient Class Objects (TCOs). We provide theoretical conditions connecting logical separability to spectral decay and validate the predicted signatures with layerwise probes on multiple open-weight model families.

</details>


### [116] [Emergent Specialization in Learner Populations: Competition as the Source of Diversity](https://arxiv.org/abs/2601.19943)
*Yuhao Li*

Main category: cs.LG

TL;DR: 竞争机制能自发诱导学习者群体形成专业化分工，无需显式通信或多样性激励，在多个现实领域验证有效。


<details>
  <summary>Details</summary>
Motivation: 研究如何在无显式通信或多样性激励的情况下，让学习者群体自发发展出协调且多样化的行为，解决群体学习的专业化分工问题。

Method: 提出NichePopulation算法，结合竞争排斥和生态位亲和度追踪机制，通过竞争动态促使学习者自发分区成为不同环境机制下的专家。

Result: 在6个现实领域验证：平均专业化指数达0.75，效应量Cohen's d > 20；无生态位奖励时仍能实现SI > 0.30；多样化群体比同质基线性能提升26.5%；比MARL基线方法快4倍且性能优4.3倍。

Conclusion: 竞争机制足以诱导群体出现专业化分工，符合生态位理论，为群体学习中的自发协调和多样化行为发展提供了有效解决方案。

Abstract: How can populations of learners develop coordinated, diverse behaviors without explicit communication or diversity incentives? We demonstrate that competition alone is sufficient to induce emergent specialization -- learners spontaneously partition into specialists for different environmental regimes through competitive dynamics, consistent with ecological niche theory. We introduce the NichePopulation algorithm, a simple mechanism combining competitive exclusion with niche affinity tracking. Validated across six real-world domains (cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, and air quality), our approach achieves a mean Specialization Index of 0.75 with effect sizes of Cohen's d > 20. Key findings: (1) At lambda=0 (no niche bonus), learners still achieve SI > 0.30, proving specialization is genuinely emergent; (2) Diverse populations outperform homogeneous baselines by +26.5% through method-level division of labor; (3) Our approach outperforms MARL baselines (QMIX, MAPPO, IQL) by 4.3x while being 4x faster.

</details>


### [117] [Classifier Calibration at Scale: An Empirical Study of Model-Agnostic Post-Hoc Methods](https://arxiv.org/abs/2601.19944)
*Valery Manokhin,Daniel Grønhaug*

Main category: cs.LG

TL;DR: 研究21种分类器在表格数据上的后处理校准方法，发现Venn-Abers和Beta校准效果最好，而常用的Platt缩放和等渗回归可能降低现代表格模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究模型无关的后处理校准方法在真实独立同分布表格数据上的效果，特别关注具有分布无关有效性保证的保形预测和Venn方法，评估不同校准器对现代表格模型的适用性。

Method: 使用TabArena-v0.1套件中的二元分类任务，通过随机分层五折交叉验证评估21种分类器（包括线性模型、SVM、树集成和现代神经网络）。在单独校准集上训练5种校准器（等渗回归、Platt缩放、Beta校准、Venn-Abers预测器和Pearsonify），然后在测试集上应用。使用对数损失、Brier分数等评分规则以及校准诊断指标进行评估。

Result: Venn-Abers预测器在降低对数损失方面表现最佳，Beta校准紧随其后且改进最频繁。Platt缩放效果较弱且不一致。常用的Platt缩放和等渗回归可能系统性地降低现代表格模型的评分性能。所有方法（除Pearsonify外）都略微提高准确率，但增益很小（最大约0.008%）。

Conclusion: 校准效果因数据集和模型架构而异，没有统一最优的方法。对于现代表格模型，Venn-Abers和Beta校准通常优于传统的Platt缩放和等渗回归，但需要根据具体场景选择校准方法。

Abstract: We study model-agnostic post-hoc calibration methods intended to improve probabilistic predictions in supervised binary classification on real i.i.d. tabular data, with particular emphasis on conformal and Venn-based approaches that provide distribution-free validity guarantees under exchangeability. We benchmark 21 widely used classifiers, including linear models, SVMs, tree ensembles (CatBoost, XGBoost, LightGBM), and modern tabular neural and foundation models, on binary tasks from the TabArena-v0.1 suite using randomized, stratified five-fold cross-validation with a held-out test fold. Five calibrators; Isotonic regression, Platt scaling, Beta calibration, Venn-Abers predictors, and Pearsonify are trained on a separate calibration split and applied to test predictions. Calibration is evaluated using proper scoring rules (log-loss and Brier score) and diagnostic measures (Spiegelhalter's Z, ECE, and ECI), alongside discrimination (AUC-ROC) and standard classification metrics. Across tasks and architectures, Venn-Abers predictors achieve the largest average reductions in log-loss, followed closely by Beta calibration, while Platt scaling exhibits weaker and less consistent effects. Beta calibration improves log-loss most frequently across tasks, whereas Venn-Abers displays fewer instances of extreme degradation and slightly more instances of extreme improvement. Importantly, we find that commonly used calibration procedures, most notably Platt scaling and isotonic regression, can systematically degrade proper scoring performance for strong modern tabular models. Overall classification performance is often preserved, but calibration effects vary substantially across datasets and architectures, and no method dominates uniformly. In expectation, all methods except Pearsonify slightly increase accuracy, but the effect is marginal, with the largest expected gain about 0.008%.

</details>


### [118] [NCSAM Noise-Compensated Sharpness-Aware Minimization for Noisy Label Learning](https://arxiv.org/abs/2601.19947)
*Jiayu Xu,Junbiao Pang*

Main category: cs.LG

TL;DR: 本文提出了一种新的视角，通过理论分析损失景观平坦度与标签噪声之间的关系，发现精心模拟的标签噪声能协同提升泛化性能和噪声鲁棒性，并提出了NCSAM方法来利用SAM的扰动修复标签噪声的损害。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常包含错误或损坏的标注（如网络爬取数据），当前研究主要关注复杂的标签校正机制。本文从新视角出发，建立损失景观平坦度与标签噪声之间关系的理论分析。

Method: 提出Noise-Compensated Sharpness-aware Minimization (NCSAM)，利用Sharpness-Aware Minimization (SAM)的扰动来修复标签噪声的损害。理论证明精心模拟的标签噪声能协同增强泛化性能和噪声鲁棒性。

Result: 在多个基准数据集上的广泛实验结果表明，该方法在多样化任务上始终优于现有的最先进方法。分析显示测试精度在噪声清晰数据集上表现出类似的行为模式。

Conclusion: 通过理论分析损失平坦度与标签噪声的关系，提出NCSAM方法有效利用SAM扰动修复标签噪声损害，在多个任务上展现优越性能，为噪声标签学习提供了新视角。

Abstract: Learning from Noisy Labels (LNL) presents a fundamental challenge in deep learning, as real-world datasets often contain erroneous or corrupted annotations, \textit{e.g.}, data crawled from Web. Current research focuses on sophisticated label correction mechanisms. In contrast, this paper adopts a novel perspective by establishing a theoretical analysis the relationship between flatness of the loss landscape and the presence of label noise. In this paper, we theoretically demonstrate that carefully simulated label noise synergistically enhances both the generalization performance and robustness of label noises. Consequently, we propose Noise-Compensated Sharpness-aware Minimization (NCSAM) to leverage the perturbation of Sharpness-Aware Minimization (SAM) to remedy the damage of label noises. Our analysis reveals that the testing accuracy exhibits a similar behavior that has been observed on the noise-clear dataset. Extensive experimental results on multiple benchmark datasets demonstrate the consistent superiority of the proposed method over existing state-of-the-art approaches on diverse tasks.

</details>


### [119] [Probabilistic Sensing: Intelligence in Data Sampling](https://arxiv.org/abs/2601.19953)
*Ibrahim Albulushi,Saleh Bunaiyan,Suraj S. Cheema,Hesham ElSawy,Feras Al-Dirini*

Main category: cs.LG

TL;DR: 提出一种基于概率神经元的智能传感范式，通过概率决策实现数据采集的能效提升，在微秒级响应时间内实现无损数据采集，节省93%的主动操作时间和样本数量。


<details>
  <summary>Details</summary>
Motivation: 传统确定性数据采集决策存在信息丢失风险，而将智能扩展到数据采集过程（决定是否采样）可以带来变革性的能效提升。需要一种能够进行概率决策的传感范式。

Method: 受自主神经系统启发，采用概率神经元（p-neuron）驱动模拟特征提取电路，实现微秒级响应时间的概率决策，突破亚采样率响应时间限制。

Result: 在主动地震勘测数据上的验证实验显示，实现了无损概率数据采集，归一化均方误差仅为0.41%，系统主动操作时间和生成样本数量节省了93%。

Conclusion: 该概率传感范式能够在微秒级响应时间内实现实时智能自主数据采样激活，显著提高能效同时保持数据质量，为智能传感系统提供了新的解决方案。

Abstract: Extending the intelligence of sensors to the data-acquisition process - deciding whether to sample or not - can result in transformative energy-efficiency gains. However, making such a decision in a deterministic manner involves risk of losing information. Here we present a sensing paradigm that enables making such a decision in a probabilistic manner. The paradigm takes inspiration from the autonomous nervous system and employs a probabilistic neuron (p-neuron) driven by an analog feature extraction circuit. The response time of the system is on the order of microseconds, over-coming the sub-sampling-rate response time limit and enabling real-time intelligent autonomous activation of data-sampling. Validation experiments on active seismic survey data demonstrate lossless probabilistic data acquisition, with a normalized mean squared error of 0.41%, and 93% saving in the active operation time of the system and the number of generated samples.

</details>


### [120] [Reward Models Inherit Value Biases from Pretraining](https://arxiv.org/abs/2601.20838)
*Brian Christian,Jessica A. F. Thompson,Elle Michelle Yang,Vincent Adam,Hannah Rose Kirk,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.LG

TL;DR: 研究发现奖励模型（RMs）的行为受其基础语言模型影响，Llama RMs偏好"能动性"，Gemma RMs偏好"社群性"，这种差异源于预训练模型的logits差异，表明基础模型选择涉及价值观考量。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在将大语言模型与人类价值观对齐中至关重要，但相比预训练和微调模型本身，RMs受到的关注较少。RMs从LLMs初始化，继承了影响其行为的表征，但这种影响的性质和程度尚未得到充分研究。

Method: 对10个领先的开源权重RMs进行综合研究，使用经过验证的心理语言学语料库，分析RMs在人类价值观多个维度上的差异。基于"Big Two"心理学轴（能动性与社群性），比较不同基础模型的RMs偏好。通过logits分析追溯差异来源，推导可用的隐式奖励分数，并进行偏好数据源和数量的消融实验。

Result: RMs表现出显著的价值观差异，Llama RMs对"能动性"有强烈偏好，Gemma RMs对"社群性"有强烈偏好。这种差异在偏好数据和微调过程相同的情况下仍然存在，可追溯到指令微调和预训练模型的logits差异。隐式奖励分数也表现出相同的能动性/社群性差异。消融实验表明这种效应不仅可重复，而且具有惊人的持久性。

Conclusion: 尽管RMs旨在代表人类偏好，但其输出受到所基于的预训练LLMs的影响。这项工作强调了预训练阶段安全和对齐工作的重要性，并明确表明开源开发者选择基础模型既是性能考量，也是价值观考量。

Abstract: Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the "Big Two" psychological axes, we show a robust preference of Llama RMs for "agency" and a corresponding robust preference of Gemma RMs for "communion." This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.

</details>


### [121] [MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference](https://arxiv.org/abs/2601.19961)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Ruijia Wu,Li YanTao,Qiang Hui,Yuren You,Ting Lu,Chao Tan,Shaoan Zhao,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.LG

TL;DR: MeanCache是一个无需训练的缓存框架，通过平均速度视角和轨迹稳定性调度策略，显著加速Flow Matching推理，在多个模型上实现3.59-4.56倍加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有缓存方法依赖瞬时速度信息，在高加速比下会导致严重的轨迹偏差和误差累积，需要更稳定的缓存策略来平衡计算效率和生成质量。

Method: 1) 引入平均速度视角，利用缓存的Jacobian-向量积构建区间平均速度；2) 开发轨迹稳定性调度策略，在预算约束下采用峰值抑制最短路径确定调度方案。

Result: 在FLUX.1、Qwen-Image和HunyuanVideo上分别实现4.12倍、4.56倍和3.59倍加速，在生成质量上持续优于最先进的缓存基线方法。

Conclusion: MeanCache为Flow Matching推理提供了简单有效的新视角，有望激发商业规模生成模型中稳定性驱动加速的进一步探索。

Abstract: We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.

</details>


### [122] [BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection](https://arxiv.org/abs/2601.19992)
*Soham Sarkar,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: BayPrAnoMeta：一种用于少样本工业图像异常检测的贝叶斯原型元学习方法，通过概率正态性模型和贝叶斯后验预测似然改进现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业图像异常检测面临极端类别不平衡和标记缺陷样本稀缺的挑战，特别是在少样本设置中。现有Proto-MAML方法依赖确定性类原型和基于距离的适应，缺乏不确定性建模能力。

Method: 提出BayPrAnoMeta，将原型替换为任务特定的概率正态性模型，使用Normal-Inverse-Wishart先验对正常支持嵌入建模，产生Student-t预测分布。通过贝叶斯后验预测似然进行内循环适应，并扩展到具有监督对比正则化的联邦元学习框架。

Result: 在MVTec AD基准测试中，相比MAML、Proto-MAML和基于PatchCore的方法，在少样本异常检测设置中实现了持续且显著的AUROC改进。

Conclusion: BayPrAnoMeta通过概率建模和不确定性感知的异常评分，在极端少样本设置中表现出更强的鲁棒性，为工业图像异常检测提供了有效的解决方案。

Abstract: Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.

</details>


### [123] [Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation](https://arxiv.org/abs/2601.20848)
*Weixin Chen,Li Chen,Yuhan Zhao*

Main category: cs.LG

TL;DR: Cofair：一种单次训练框架，通过条件适配器模块实现推荐系统训练后的公平性动态控制，无需为不同公平性要求重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有公平性推荐方法通常在训练时固定公平性要求，缺乏训练后的灵活性。现实场景中不同利益相关者可能随时间变化提出不同的公平性要求，为每个新要求重新训练模型成本过高。

Method: 提出Cofair框架：1）共享表示层+公平性条件适配器模块，生成针对不同公平性水平的用户嵌入；2）用户级正则化项，确保跨公平性水平的用户级单调公平性改进。理论证明对抗目标上界人口统计均等，正则化项强制执行用户级渐进公平性。

Result: 在多个数据集和骨干模型上的实验表明，Cofair能在不同水平提供动态公平性，生成与最先进基线相当或更好的公平性-准确性曲线，且无需为每个新公平性要求重新训练。

Conclusion: Cofair通过单次训练实现训练后公平性控制，解决了现实场景中动态公平性需求的问题，为推荐系统提供了灵活且高效的公平性调节机制。

Abstract: Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.

</details>


### [124] [Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment](https://arxiv.org/abs/2601.19963)
*Canyang Zhao,Bolin Peng,J. Patrick Mayo,Ce Ju,Bing Liu*

Main category: cs.LG

TL;DR: TCLA框架通过任务条件潜在对齐解决脑机接口中的跨会话非平稳性问题，在有限数据下实现源会话知识向目标会话的有效迁移，显著提升解码性能。


<details>
  <summary>Details</summary>
Motivation: 植入式脑机接口面临跨会话神经活动非平稳性的挑战，导致解码器在后续会话中泛化能力差。当新会话数据有限时，重新训练或适应解码器变得尤为困难。

Method: 提出任务条件潜在对齐框架(TCLA)：基于自编码器架构，首先从数据充足的源会话学习神经动力学的低维表示；对于数据有限的目标会话，以任务条件方式将目标潜在表示与源会话对齐，实现学习到的神经动力学的有效迁移。

Result: 在猕猴运动和眼动中心向外数据集上评估，相比仅使用目标会话数据训练的基线方法，TCLA在数据集和解码设置中一致提升解码性能，在运动数据集的y坐标速度解码中决定系数增益高达0.386。

Conclusion: TCLA提供了一种有效的知识迁移策略，能够在有限数据条件下实现更鲁棒的神经解码，解决脑机接口中的跨会话非平稳性问题。

Abstract: Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.

</details>


### [125] [Regime-Adaptive Bayesian Optimization via Dirichlet Process Mixtures of Gaussian Processes](https://arxiv.org/abs/2601.20043)
*Yan Zhang,Xuefeng Liu,Sipeng Chen,Sascha Ranftl,Chong Liu,Shibo Li*

Main category: cs.LG

TL;DR: RAMBO：基于狄利克雷过程混合高斯过程的贝叶斯优化方法，用于处理多区域优化问题，能自动发现潜在区域并分别建模


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化假设搜索空间均匀平滑，但在多区域问题（如分子构象搜索、药物发现）中这一假设不成立，单个高斯过程要么过度平滑尖锐过渡，要么在平滑区域产生噪声幻觉，导致不确定性校准错误

Method: 提出RAMBO方法，使用狄利克雷过程混合高斯过程，在优化过程中自动发现潜在区域，每个区域用独立的高斯过程建模并优化局部超参数；推导了折叠吉布斯采样算法，解析边缘化潜在函数实现高效推理；引入自适应浓度参数调度实现从粗到细的区域发现；设计了将不确定性分解为区域内和区域间分量的获取函数

Result: 在合成基准测试和实际应用中（包括分子构象优化、药物发现虚拟筛选、聚变反应堆设计）的实验表明，RAMBO在多区域目标函数上相比最先进基线方法有持续改进

Conclusion: RAMBO通过自动发现和建模潜在区域，有效解决了多区域优化问题，在多个实际应用中表现出优越性能

Abstract: Standard Bayesian Optimization (BO) assumes uniform smoothness across the search space an assumption violated in multi-regime problems such as molecular conformation search through distinct energy basins or drug discovery across heterogeneous molecular scaffolds. A single GP either oversmooths sharp transitions or hallucinates noise in smooth regions, yielding miscalibrated uncertainty. We propose RAMBO, a Dirichlet Process Mixture of Gaussian Processes that automatically discovers latent regimes during optimization, each modeled by an independent GP with locally-optimized hyperparameters. We derive collapsed Gibbs sampling that analytically marginalizes latent functions for efficient inference, and introduce adaptive concentration parameter scheduling for coarse-to-fine regime discovery. Our acquisition functions decompose uncertainty into intra-regime and inter-regime components. Experiments on synthetic benchmarks and real-world applications, including molecular conformer optimization, virtual screening for drug discovery, and fusion reactor design, demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.

</details>


### [126] [Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization](https://arxiv.org/abs/2601.20226)
*Julian Gutierrez,Redouane Silvente*

Main category: cs.LG

TL;DR: 提出了两个机器学习框架：快速参数化模型用于预测聚合曲线，生成模型用于综合市场分析，并基于预测优化储能策略


<details>
  <summary>Details</summary>
Motivation: 解决EPEX SPOT日前市场中聚合曲线预测和储能优化的挑战，需要既能快速日常操作又支持全面分析的解决方案

Method: 1) 快速参数化模型：用最小/最大交易量和切比雪夫多项式预测小时级供需曲线；2) 生成模型：学习给定天气和燃料变量下24小时订单级提交的联合分布，生成合成订单场景

Result: 参数化模型实现低误差和良好可解释性，生成模型能产生合成订单场景。基于预测优化储能策略，量化收益分布，发现价格压缩效应

Conclusion: 提出的双框架方法既能满足日常快速预测需求，又能支持全面市场分析，为储能策略优化提供了有效工具，揭示了容量扩张的收益递减规律

Abstract: We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.

</details>


### [127] [Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions](https://arxiv.org/abs/2601.19965)
*Mingxuan Luo,Guipeng Xv,Sishuo Chen,Xinyu Li,Li Zhang,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Bo Zheng,Chen Lin*

Main category: cs.LG

TL;DR: 提出了CASCADE数据集和TESLA框架，用于解决NetCVR（净转化率）预测中的级联延迟反馈问题，相比传统CVR方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统中，传统转化率(CVR)忽略了退款行为，无法完全反映推荐效果。NetCVR（购买且不退款）能更好衡量用户满意度和商业价值，但其预测涉及从点击到转化、再从转化到退款的两阶段级联延迟反馈，传统CVR方法不适用，且缺乏开源数据集和在线持续训练方案。

Method: 提出TESLA框架：1) CVR-退款率级联架构；2) 分阶段去偏；3) 延迟时间感知排序损失。基于从淘宝APP收集的大规模开源数据集CASCADE进行分析和实验。

Result: TESLA在CASCADE数据集上显著优于现有方法，NetCVR预测的RI-AUC提升12.41%，RI-PRAUC提升14.94%。

Conclusion: NetCVR预测需要在线持续建模，级联建模CVR和退款率优于直接建模NetCVR，延迟时间是重要特征。提出的TESLA框架有效解决了级联延迟反馈问题。

Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.

</details>


### [128] [Order-Optimal Sample Complexity of Rectified Flows](https://arxiv.org/abs/2601.20250)
*Hari Krishna Sahoo,Mudit Gaur,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文证明了整流流模型在标准假设下达到样本复杂度$\tilde{O}(\varepsilon^{-2})$，优于流匹配模型的$O(\varepsilon^{-4})$，与均值估计的最优率匹配，从理论上解释了整流流模型的强实证性能。


<details>
  <summary>Details</summary>
Motivation: 基于流的生成模型相比扩散模型显示出更高的效率，整流流模型通过约束传输轨迹为从基分布到数据分布的线性路径，大大加速了采样。本文旨在从理论上分析整流流模型的样本复杂度，解释其优异的实证性能。

Method: 研究整流流模型，该模型约束传输轨迹为线性路径，使用神经网络参数化速度场和数据分布。通过分析整流流特有的结构特性：模型沿线性路径使用平方损失训练，相关的假设类具有严格控制的局部Rademacher复杂度，从而推导出样本复杂度界限。

Result: 在标准假设下，证明了整流流模型达到样本复杂度$\tilde{O}(\varepsilon^{-2})$，这改进了流匹配模型已知的$O(\varepsilon^{-4})$界限，与均值估计的最优率匹配。

Conclusion: 整流流模型因其特殊的线性路径结构和平方损失训练方式，使得假设类具有良好控制的局部Rademacher复杂度，从而实现了最优的样本复杂度，这从理论上解释了整流流模型强大的实证性能。

Abstract: Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$. This improves on the best known $O(\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.

</details>


### [129] [Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models](https://arxiv.org/abs/2601.20367)
*Qing Lyu,Zhe Fu,Alexandre Bayen*

Main category: cs.LG

TL;DR: 提出基於多智能體Transformer的無監督異常檢測框架，用於識別自動駕駛中的安全關鍵場景，通過預測殘差建模正常駕駛並檢測偏差，在NGSIM數據集上驗證了有效性。


<details>
  <summary>Details</summary>
Motivation: 自動駕駛中安全關鍵場景的識別至關重要，但這類事件稀少使得監督標註不切實際。傳統基於規則的指標過於簡單，無法捕捉複雜交互風險，且現有方法缺乏系統性驗證統計異常是否真正反映物理危險。

Method: 提出無監督異常檢測框架，使用多智能體Transformer建模正常駕駛行為，通過預測殘差測量偏差。採用雙重評估方案：穩定性通過Kendall等級相關係數和Jaccard指數評估；物理對齊通過與替代安全指標的相關性評估。

Result: 在NGSIM數據集上，最大殘差聚合器實現了最高的物理對齊性同時保持穩定性。框架識別出388個被傳統方法遺漏的獨特異常，捕捉到如側向漂移下的反應制動等細微多智能體風險，並將檢測到的異常聚類為四種可解釋風險類型。

Conclusion: 該框架為自動駕駛安全關鍵場景識別提供了有效的無監督解決方案，能夠發現傳統方法遺漏的複雜交互風險，並提供可操作的仿真和測試見解。

Abstract: Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.

</details>


### [130] [Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers](https://arxiv.org/abs/2601.19967)
*Jinlin Liu,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: 提出PIL方法，使用线性代理模型生成扰动，实现高效数据保护，揭示不可学习示例的线性化机制


<details>
  <summary>Details</summary>
Motivation: 现有不可学习示例方法依赖深度神经网络作为代理模型生成扰动，计算成本高，需要更高效的方法

Method: 提出PIL方法，仅使用线性代理模型生成扰动，大大降低计算成本，同时揭示不可学习示例通过诱导深度模型线性化而生效的机制

Result: PIL方法在计算时间大幅减少的情况下，达到或优于现有基于代理模型的方法，并分析了基于百分比的部分扰动特性

Conclusion: PIL不仅提供了实用的数据保护方法，还揭示了不可学习示例有效的内在机制，为数据保护领域提供了新见解

Abstract: Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.

</details>


### [131] [Learning Contextual Runtime Monitors for Safe AI-Based Autonomy](https://arxiv.org/abs/2601.20666)
*Alejandro Luque-Cerpa,Mengyuan Wang,Emil Carlsson,Sanjit A. Seshia,Devdatt Dubhashi,Hazem Torfah*

Main category: cs.LG

TL;DR: 提出一个学习上下文感知运行时监控器的新框架，用于AI控制集成系统，通过上下文监测选择最适合当前条件的控制器，提高安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习控制器在自主网络物理系统中部署越来越多，但在陌生环境中性能可能急剧下降，存在安全隐患。传统集成方法通过平均或投票来提升鲁棒性，但会稀释各个控制器在不同操作上下文中的专业优势。

Method: 将安全AI控制集成设计重新定义为上下文监控问题。监控器持续观察系统上下文，选择最适合当前条件的控制器。将监控器学习建模为上下文学习任务，利用上下文多臂老虎机技术。

Result: 在两个模拟自动驾驶场景中验证了该框架，相比非上下文基线方法，在安全性和性能方面都取得了显著改进。

Conclusion: 提出的上下文感知监控框架能够更好地利用控制器多样性，提供理论安全保证，并在实际应用中表现出优越的安全性和性能。

Abstract: We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.

</details>


### [132] [Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM](https://arxiv.org/abs/2601.20571)
*Anna van Elst,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: 提出AsylADMM算法，用于去中心化的中位数和分位数估计，具有通信高效、内存占用低（每节点仅需2个变量）、异步更新和鲁棒性强的特点。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的去中心化学习需要通信高效、对数据损坏鲁棒且内存占用低的算法。现有的gossip方法虽满足通信效率，但鲁棒性不足；ADMM方法能估计更鲁棒的中位数，但内存需求随节点度增长，在内存受限时不实用。

Method: 提出AsylADMM算法，一种新颖的gossip算法，专为异步更新设计，每节点仅需维护2个变量。分析了同步变体以建立理论保证，并实证验证异步算法的快速收敛。

Result: 算法支持分位数修剪、几何中位数估计和深度修剪，其中分位数修剪在实证中优于现有的基于秩的方法。提供了基于马尔可夫链理论的秩修剪新理论分析。

Conclusion: AsylADMM是一种高效、鲁棒且内存友好的去中心化中位数和分位数估计算法，适用于资源受限的边缘设备环境。

Abstract: Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.

</details>


### [133] [SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning](https://arxiv.org/abs/2601.20738)
*Dawit Kiros Redie,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: 提出SA-PEF方法，结合步前校正和部分误差反馈，解决联邦学习中非IID数据下梯度压缩的早期停滞问题，实现更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，带误差反馈的梯度压缩可以减少通信开销，但在非IID数据下，残差误差衰减缓慢，导致早期训练轮次出现梯度失配和进展停滞的问题。

Method: 提出步前部分误差反馈（SA-PEF），结合步前校正和部分误差反馈。当步前系数α=0时恢复为EF，α=1时变为步前EF（SAEF）。针对非凸目标和δ-压缩器，建立了二阶矩边界和残差递归，保证在异构数据和部分客户端参与下的收敛性。

Result: SA-PEF的收敛速率与标准非凸Fed-SGD保证相匹配（相差常数因子），达到O((η,η₀TR)⁻¹)收敛到方差/异构性下限。分析揭示了步前控制的残差收缩ρ_r，解释了早期训练阶段的加速现象。通过选择接近理论预测最优值的α，平衡SAEF的快速预热和EF的长期稳定性。

Conclusion: SA-PEF方法在多种架构和数据集上的实验表明，它比EF方法能更快达到目标精度，有效解决了非IID数据下梯度压缩的早期停滞问题。

Abstract: Biased gradient compression with error feedback (EF) reduces communication in federated learning (FL), but under non-IID data, the residual error can decay slowly, causing gradient mismatch and stalled progress in the early rounds. We propose step-ahead partial error feedback (SA-PEF), which integrates step-ahead (SA) correction with partial error feedback (PEF). SA-PEF recovers EF when the step-ahead coefficient $α=0$ and step-ahead EF (SAEF) when $α=1$. For non-convex objectives and $δ$-contractive compressors, we establish a second-moment bound and a residual recursion that guarantee convergence to stationarity under heterogeneous data and partial client participation. The resulting rates match standard non-convex Fed-SGD guarantees up to constant factors, achieving $O((η,η_0TR)^{-1})$ convergence to a variance/heterogeneity floor with a fixed inner step size. Our analysis reveals a step-ahead-controlled residual contraction $ρ_r$ that explains the observed acceleration in the early training phase. To balance SAEF's rapid warm-up with EF's long-term stability, we select $α$ near its theory-predicted optimum. Experiments across diverse architectures and datasets show that SA-PEF consistently reaches target accuracy faster than EF.

</details>


### [134] [Decomposing multimodal embedding spaces with group-sparse autoencoders](https://arxiv.org/abs/2601.20028)
*Chiraag Kaushik,Davis Barch,Andrea Fanelli*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的稀疏自编码器方法，用于多模态嵌入空间（如CLIP和CLAP）的分解，通过跨模态随机掩码和组稀疏正则化，解决了传统SAE在多模态设置下产生"分裂字典"的问题，实现了更好的模态对齐和概念可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器（SAE）应用于多模态嵌入空间（如CLIP图像/文本嵌入）时，往往产生"分裂字典"问题，即学习到的稀疏特征大多是单模态的，仅对单一模态数据激活。这限制了在多模态嵌入中实现概念对齐和可解释性分析。

Method: 提出基于跨模态随机掩码和组稀疏正则化的SAE改进方法。首先论证了在已对齐的嵌入空间中，分裂字典分解的存在意味着存在具有更好模态对齐的非分裂字典。然后通过跨模态随机掩码增强特征的多模态性，使用组稀疏正则化促进特征的跨模态共享。

Result: 在CLIP（图像/文本）和CLAP（音频/文本）嵌入上应用该方法，相比标准SAE，学习到的字典更具多模态性，减少了"死亡神经元"数量，提高了特征的语义性。改进的模态对齐增强了跨模态任务的可解释性和控制能力。

Conclusion: 提出的改进SAE方法有效解决了多模态嵌入分解中的分裂字典问题，实现了更好的模态对齐，为多模态表示学习提供了更可解释的分解工具，并展示了在跨模态任务可解释性和控制方面的应用潜力。

Abstract: The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spaces (such as the popular CLIP embeddings for image/text data) have found that SAEs often learn "split dictionaries", where most of the learned sparse features are essentially unimodal, active only for data of a single modality. In this work, we study how to effectively adapt SAEs for the setting of multimodal embeddings while ensuring multimodal alignment. We first argue that the existence of a split dictionary decomposition on an aligned embedding space implies the existence of a non-split dictionary with improved modality alignment. Then, we propose a new SAE-based approach to multimodal embedding decomposition using cross-modal random masking and group-sparse regularization. We apply our method to popular embeddings for image/text (CLIP) and audio/text (CLAP) data and show that, compared to standard SAEs, our approach learns a more multimodal dictionary while reducing the number of dead neurons and improving feature semanticity. We finally demonstrate how this improvement in alignment of concepts between modalities can enable improvements in the interpretability and control of cross-modal tasks.

</details>


### [135] [Structural Compositional Function Networks: Interpretable Functional Compositions for Tabular Discovery](https://arxiv.org/abs/2601.20037)
*Fang Li*

Main category: cs.LG

TL;DR: 提出StructuralCFN架构，通过可微结构先验建模特征间关系，在保持可解释性的同时超越梯度提升树性能


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在处理表格数据时难以同时达到梯度提升树的性能并保持科学可解释性，且通常将特征视为独立实体，忽略了表格数据固有的流形结构依赖关系

Method: 提出StructuralCFN架构，通过关系感知归纳偏置和可微结构先验，将每个特征建模为其对应特征的数学组合，使用可微自适应门控自动发现最优激活机制，支持结构化知识集成

Result: 在18个基准测试的10折交叉验证中表现出统计显著改进(p<0.05)，在科学和临床数据集上表现优异，参数规模比标准深度基准小10-20倍(300-2500参数)

Conclusion: StructuralCFN能够以人类可读的数学表达式恢复数据流形的"定律"，提供内在符号可解释性，同时保持紧凑的参数规模，在表格数据学习领域取得了性能与可解释性的平衡

Abstract: Despite the ubiquity of tabular data in high-stakes domains, traditional deep learning architectures often struggle to match the performance of gradient-boosted decision trees while maintaining scientific interpretability. Standard neural networks typically treat features as independent entities, failing to exploit the inherent manifold structural dependencies that define tabular distributions. We propose Structural Compositional Function Networks (StructuralCFN), a novel architecture that imposes a Relation-Aware Inductive Bias via a differentiable structural prior. StructuralCFN explicitly models each feature as a mathematical composition of its counterparts through Differentiable Adaptive Gating, which automatically discovers the optimal activation physics (e.g., attention-style filtering vs. inhibitory polarity) for each relationship. Our framework enables Structured Knowledge Integration, allowing domain-specific relational priors to be injected directly into the architecture to guide discovery. We evaluate StructuralCFN across a rigorous 10-fold cross-validation suite on 18 benchmarks, demonstrating statistically significant improvements (p < 0.05) on scientific and clinical datasets (e.g., Blood Transfusion, Ozone, WDBC). Furthermore, StructuralCFN provides Intrinsic Symbolic Interpretability: it recovers the governing "laws" of the data manifold as human-readable mathematical expressions while maintaining a compact parameter footprint (300--2,500 parameters) that is over an order of magnitude (10x--20x) smaller than standard deep baselines.

</details>


### [136] [CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs](https://arxiv.org/abs/2601.20041)
*Shih-Hsuan Chiu,Ming-Syan Chen*

Main category: cs.LG

TL;DR: TONEL框架通过噪声感知投影模型学习任务特定嵌入，提升边缘设备RAG在噪声环境下的鲁棒性和领域适应性


<details>
  <summary>Details</summary>
Motivation: 边缘设备上部署基于RAG的个性化虚拟助手面临效率瓶颈，因为用户档案数据快速增长，而CiM架构虽然能减少数据移动，但易受环境噪声影响，在动态多领域边缘场景中检索精度会下降

Method: 提出任务导向的噪声鲁棒嵌入学习(TONEL)框架，采用噪声感知投影模型学习符合CiM硬件约束的任务特定嵌入，在噪声条件下实现准确检索

Result: 在个性化基准测试上的大量实验表明，该方法相对于强基线在任务特定噪声场景下具有显著有效性和实用性

Conclusion: TONEL框架成功解决了边缘设备RAG在噪声环境中的鲁棒性和领域适应性问题，为动态多领域边缘场景提供了有效的解决方案

Abstract: Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, deploying RAG on edge devices faces efficiency hurdles due to the rapid growth of profile data, such as user-LLM interactions and recent updates. While Computing-in-Memory (CiM) architectures mitigate this bottleneck by eliminating data movement between memory and processing units via in-situ operations, they are susceptible to environmental noise that can degrade retrieval precision. This poses a critical issue in dynamic, multi-domain edge-based scenarios (e.g., travel, medicine, and law) where both accuracy and adaptability are paramount. To address these challenges, we propose Task-Oriented Noise-resilient Embedding Learning (TONEL), a framework that improves noise robustness and domain adaptability for RAG in noisy edge environments. TONEL employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints, enabling accurate retrieval under noisy conditions. Extensive experiments conducted on personalization benchmarks demonstrate the effectiveness and practicality of our methods relative to strong baselines, especially in task-specific noisy scenarios.

</details>


### [137] [Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality Risk in Metastatic Castration-Resistant Prostate Cancer](https://arxiv.org/abs/2601.20046)
*Javier Mencia-Ledo,Mohammad Noaeen,Zahra Shakeri*

Main category: cs.LG

TL;DR: 开发并外部验证了用于转移性去势抵抗性前列腺癌(mCRPC)的180天死亡率风险模型，使用纵向数据比较了五种架构，GRU模型在外部验证中表现最佳。


<details>
  <summary>Details</summary>
Motivation: mCRPC是一种高度侵袭性疾病，预后差且治疗反应异质性大，需要开发能够预测短期死亡风险的模型来支持主动护理规划。

Method: 使用两个III期队列的纵向数据(n=526和n=640)，比较了LSTM、GRU、Cox比例风险、随机生存森林和逻辑回归五种架构，仅使用可观察180天结局的就诊数据，排除右删失病例。

Result: GRU和RSF模型初始区分能力高(C-index: 87%)，外部验证中GRU校准度更高(斜率: 0.93; 截距: 0.07)，PR-AUC为0.87。BMI和收缩压是最重要的预测因子。

Conclusion: 纵向常规临床标志物可以估计mCRPC的短期死亡风险，支持在多个月窗口内进行主动护理规划，GRU模型在外部验证中表现最佳。

Abstract: Metastatic castration-resistant prostate cancer (mCRPC) is a highly aggressive disease with poor prognosis and heterogeneous treatment response. In this work, we developed and externally validated a visit-level 180-day mortality risk model using longitudinal data from two Phase III cohorts (n=526 and n=640). Only visits with observable 180-day outcomes were labeled; right-censored cases were excluded from analysis. We compared five candidate architectures: Long Short-Term Memory, Gated Recurrent Unit (GRU), Cox Proportional Hazards, Random Survival Forest (RSF), and Logistic Regression. For each dataset, we selected the smallest risk-threshold that achieved an 85% sensitivity floor. The GRU and RSF models showed high discrimination capabilities initially (C-index: 87% for both). In external validation, the GRU obtained a higher calibration (slope: 0.93; intercept: 0.07) and achieved an PR-AUC of 0.87. Clinical impact analysis showed a median time-in-warning of 151.0 days for true positives (59.0 days for false positives) and 18.3 alerts per 100 patient-visits. Given late-stage frailty or cachexia and hemodynamic instability, permutation importance ranked BMI and systolic blood pressure as the strongest associations. These results suggest that longitudinal routine clinical markers can estimate short-horizon mortality risk in mCRPC and support proactive care planning over a multi-month window.

</details>


### [138] [Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning](https://arxiv.org/abs/2601.20069)
*Chi-Yao Huang,Khoa Vo,Aayush Atul Verma,Duo Lu,Yezhou Yang*

Main category: cs.LG

TL;DR: 提出Domain Expansion框架，通过正交池化机制构建正交子空间，解决多目标训练中的潜在表示崩溃问题


<details>
  <summary>Details</summary>
Motivation: 多目标训练中梯度冲突导致共享表示退化，形成对任何单一任务都次优的妥协状态（潜在表示崩溃问题）

Method: Domain Expansion框架，使用新颖的正交池化机制构建潜在空间，为每个目标分配相互正交的子空间

Result: 在ShapeNet、MPIIGaze和Rotated MNIST等基准测试中验证，有效防止表示崩溃，构建出可解释、可组合的潜在空间

Conclusion: 通过正交子空间结构不仅防止表示崩溃，还能获得可直接操作概念的显式、可解释、可组合的潜在空间

Abstract: Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.

</details>


### [139] [Distributional value gradients for stochastic environments](https://arxiv.org/abs/2601.20071)
*Baptiste Debes,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 提出Distributional Sobolev Training方法，将分布强化学习扩展到连续状态-动作空间，不仅建模状态-动作值函数的分布，还建模其梯度的分布，以解决现有梯度正则化方法在随机/噪声环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有梯度正则化值学习方法（如MAGE）在随机或噪声环境中表现不佳，限制了其应用范围。需要开发能够更好处理随机性的梯度感知强化学习方法。

Method: 扩展连续状态-动作空间的分布强化学习，同时建模标量状态-动作值函数及其梯度的分布。使用条件变分自编码器（cVAE）实现奖励和转移分布的一步世界模型，采用最大切片最大均值差异（MSMMD）实例化分布Bellman算子。

Result: 证明了Sobolev增强的Bellman算子是收缩算子且具有唯一不动点，揭示了梯度感知RL中平滑性的基本权衡。在简单随机RL问题和多个MuJoCo环境中验证了方法的有效性。

Conclusion: Distributional Sobolev Training方法通过同时建模值函数及其梯度的分布，成功解决了现有梯度正则化方法在随机环境中的局限性，为梯度感知强化学习提供了新的理论框架和实用工具。

Abstract: Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.

</details>


### [140] [Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis](https://arxiv.org/abs/2601.20079)
*Paul Seurin,Dean Price*

Main category: cs.LG

TL;DR: 该研究将热管微反应器设计优化框架扩展为多目标优化，使用PEARL算法同时最小化棒积分峰值因子和LCOE，评估了三种成本场景下的优化策略。


<details>
  <summary>Details</summary>
Motivation: 热管微反应器适用于偏远地区供电，但需要平衡安全性和经济性。先前研究仅关注最小化LCOE，本研究扩展为多目标优化，同时考虑安全参数（棒积分峰值因子）和经济性（LCOE）。

Method: 使用PEARL（Pareto Envelope Augmented with Reinforcement Learning）算法进行多目标优化，结合代理模型和强化学习，评估三种不同成本场景：高成本反射层、低成本轴向反射层、低成本双反射层。

Result: 发现降低固体慢化剂半径、针间距和鼓涂层角度，同时增加燃料高度可有效降低棒积分峰值因子。在三种场景下，优化LCOE有四个共同策略：最小化高成本轴向反射层、减少控制鼓依赖、用低成本石墨替代TRISO燃料、最大化燃料燃耗。

Conclusion: PEARL算法在不同设计场景中展现出平衡多目标的能力，但代理模型与全阶模拟仍存在差异。未来将通过约束松弛和代理模型改进继续优化，这是当前的研究方向。

Abstract: Heat-pipe microreactors (HPMRs) are compact and transportable nuclear power systems exhibiting inherent safety, well-suited for deployment in remote regions where access is limited and reliance on costly fossil fuels is prevalent. In prior work, we developed a design optimization framework that incorporates techno-economic considerations through surrogate modeling and reinforcement learning (RL)-based optimization, focusing solely on minimizing the levelized cost of electricity (LCOE) by using a bottom-up cost estimation approach. In this study, we extend that framework to a multi-objective optimization that uses the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm. The objectives include minimizing both the rod-integrated peaking factor ($F_{Δh}$) and LCOE -- subject to safety and operational constraints. We evaluate three cost scenarios: (1) a high-cost axial and drum reflectors, (2) a low-cost axial reflector, and (3) low-cost axial and drum reflectors. Our findings indicate that reducing the solid moderator radius, pin pitch, and drum coating angle -- all while increasing the fuel height -- effectively lowers $F_{Δh}$. Across all three scenarios, four key strategies consistently emerged for optimizing LCOE: (1) minimizing the axial reflector contribution when costly, (2) reducing control drum reliance, (3) substituting expensive tri-structural isotropic (TRISO) fuel with axial reflector material priced at the level of graphite, and (4) maximizing fuel burnup. While PEARL demonstrates promise in navigating trade-offs across diverse design scenarios, discrepancies between surrogate model predictions and full-order simulations remain. Further improvements are anticipated through constraint relaxation and surrogate development, constituting an ongoing area of investigation.

</details>


### [141] [Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery](https://arxiv.org/abs/2601.20088)
*Meng Xin,Sweta Priyadarshi,Jingyu Xin,Bilal Kartal,Aditya Vavre,Asma Kuriparambil Thekkumpate,Zijia Chen,Ameya Sunil Mahabaleshwarkar,Ido Shahaf,Akhiad Bercovich,Kinjal Patel,Suguna Varshini Velury,Chenjie Luo,Zhiyu Cheng,Jenny Chen,Chen-Han Yu,Wei Ping,Oleg Rybakov,Nima Tajbakhsh,Oluwatobi Olabiyi,Dusan Stosic,Di Wu,Song Han,Eric Chung,Sharath Turuvekere Sreenivas,Bryan Catanzaro,Yoshi Suhara,Tijmen Blankevoort,Huizi Mao*

Main category: cs.LG

TL;DR: QAD（量化感知蒸馏）是一种恢复NVFP4量化LLMs和VLMs准确性的方法，通过全精度教师模型蒸馏到量化学生模型，相比传统QAT更稳定且对数据质量要求更低。


<details>
  <summary>Details</summary>
Motivation: 传统量化感知训练（QAT）在多阶段后训练流程（SFT、RL、模型合并）中存在工程复杂性和训练不稳定问题，且对数据质量和覆盖范围要求高，需要更稳定有效的量化恢复方法。

Method: 使用KL散度损失将全精度教师模型蒸馏到量化学生模型，通过量化感知蒸馏（QAD）恢复量化模型的准确性，该方法对数据质量不敏感且无需完整训练数据。

Result: 在多个后训练模型（AceReason Nemotron、Nemotron 3 Nano、Nemotron Nano V2、Nemotron Nano V2 VL、Llama Nemotron Super v1）上评估，QAD能稳定恢复量化模型至接近BF16精度的准确性。

Conclusion: QAD为量化LLMs和VLMs提供了一种稳定有效的准确性恢复方案，特别适用于复杂的多阶段后训练流程，相比传统QAT具有更好的工程可行性和稳定性。

Abstract: This technical report presents quantization-aware distillation (QAD) and our best practices for recovering accuracy of NVFP4-quantized large language models (LLMs) and vision-language models (VLMs). QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss. While applying distillation to quantized models is not a new idea, we observe key advantages of QAD for today's LLMs: 1. It shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning (SFT), reinforcement learning (RL), and model merging, where traditional quantization-aware training (QAT) suffers from engineering complexity and training instability; 2. It is robust to data quality and coverage, enabling accuracy recovery without full training data. We evaluate QAD across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1, showing consistent recovery to near-BF16 accuracy.

</details>


### [142] [In-Context Reinforcement Learning From Suboptimal Historical Data](https://arxiv.org/abs/2601.20116)
*Juncheng Dong,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出Decision Importance Transformer (DIT)框架，用于上下文强化学习，通过基于优势函数的加权最大似然估计来提升次优离线数据集的性能


<details>
  <summary>Details</summary>
Motivation: Transformer在上下文学习中表现出色，但现有方法在次优离线数据集上性能受限。需要开发能够从次优行为策略轨迹中学习并提升性能的上下文强化学习方法。

Method: 提出DIT框架：1) 训练transformer基的价值函数估计行为策略的优势函数；2) 基于训练好的价值函数构建权重，通过加权最大似然估计训练transformer基的策略，引导次优策略向最优策略转变。

Result: 在bandit和马尔可夫决策过程问题上进行广泛实验，DIT表现出优越性能，特别是在离线数据集包含次优历史数据时效果显著。

Conclusion: DIT框架成功地将actor-critic算法以上下文方式实现，能够有效利用次优离线数据集进行强化学习，为上下文强化学习提供了新方法。

Abstract: Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

</details>


### [143] [A Reinforcement Learning Based Universal Sequence Design for Polar Codes](https://arxiv.org/abs/2601.20118)
*David Kin Wai Ho,Arman Fazeli,Mohamad M. Mansour,Louay M. A. Jalloul*

Main category: cs.LG

TL;DR: 提出基于强化学习的通用Polar码序列设计框架，可扩展到2048码长，在5G支持的(N,K)配置中性能优于5G NR序列和beta-expansion基线


<details>
  <summary>Details</summary>
Motivation: 为6G应用推进Polar码设计，需要开发可扩展、适应不同信道条件和解码策略的序列设计方法

Method: 基于强化学习的通用序列设计框架，结合物理定律约束学习（利用Polar码的通用偏序特性）、限制前瞻评估（利用决策的弱长期影响）、联合多配置优化

Result: 在5G支持的所有(N,K)配置中，性能与5G NR序列相当，在N=2048时比beta-expansion基线提升0.2 dB

Conclusion: 提出的强化学习框架成功扩展到2048码长，适用于标准化，关键成功要素包括物理约束学习、前瞻限制和联合优化

Abstract: To advance Polar code design for 6G applications, we develop a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. Crucially, our method scales to code lengths up to $2048$, making it suitable for use in standardization. Across all $(N,K)$ configurations supported in 5G, our approach achieves competitive performance relative to the NR sequence adopted in 5G and yields up to a 0.2 dB gain over the beta-expansion baseline at $N=2048$. We further highlight the key elements that enabled learning at scale: (i) incorporation of physical law constrained learning grounded in the universal partial order property of Polar codes, (ii) exploitation of the weak long term influence of decisions to limit lookahead evaluation, and (iii) joint multi-configuration optimization to increase learning efficiency.

</details>


### [144] [Going NUTS with ADVI: Exploring various Bayesian Inference techniques with Facebook Prophet](https://arxiv.org/abs/2601.20120)
*Jovan Krajevski,Biljana Tojtovska Ribarski*

Main category: cs.LG

TL;DR: 将Facebook Prophet模型用PyMC重新实现，以支持更多贝叶斯推断方法（MCMC、MAP、变分推断），并分析不同方法的性能、收敛性和计算效率。


<details>
  <summary>Details</summary>
Motivation: Facebook Prophet虽然受欢迎，但其内置的推断方法有限（仅L-BFGS-B MAP和NUTS MCMC），且API设计不够灵活，难以实现自定义建模想法。需要更灵活的框架来扩展模型并比较多种贝叶斯推断技术。

Method: 使用PyMC对Prophet模型进行完整重实现，支持多种贝叶斯推断方法：完整MCMC技术、MAP估计和变分推断。在时间序列预测问题上详细分析不同方法的实现细节。

Result: 开发了基于PyMC的Prophet实现，能够扩展基础模型并评估比较多种贝叶斯推断方法。详细分析了采样方法、收敛诊断、预测指标以及计算效率，并识别了未来需要解决的问题。

Conclusion: PyMC重实现解决了原始Prophet的局限性，提供了更灵活的建模框架和更多推断方法选择。未来工作将解决当前识别出的问题，进一步完善该实现。

Abstract: Since its introduction, Facebook Prophet has attracted positive attention from both classical statisticians and the Bayesian statistics community. The model provides two built-in inference methods: maximum a posteriori estimation using the L-BFGS-B algorithm, and Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS). While exploring various time-series forecasting problems using Bayesian inference with Prophet, we encountered limitations stemming from the inability to apply alternative inference techniques beyond those provided by default. Additionally, the fluent API design of Facebook Prophet proved insufficiently flexible for implementing our custom modeling ideas. To address these shortcomings, we developed a complete reimplementation of the Prophet model in PyMC, which enables us to extend the base model and evaluate and compare multiple Bayesian inference methods. In this paper, we present our PyMC-based implementation and analyze in detail the implementation of different Bayesian inference techniques. We consider full MCMC techniques, MAP estimation and Variational inference techniques on a time-series forecasting problem. We discuss in details the sampling approach, convergence diagnostics, forecasting metrics as well as their computational efficiency and detect possible issues which will be addressed in our future work.

</details>


### [145] [Membership Inference Attacks Against Fine-tuned Diffusion Language Models](https://arxiv.org/abs/2601.20125)
*Yuetian Chen,Kaiyuan Zhang,Yuntao Du,Edoardo Stoppa,Charles Fleming,Ashish Kundu,Bruno Ribeiro,Ninghui Li*

Main category: cs.LG

TL;DR: 本文首次系统研究扩散语言模型（DLM）的成员推理攻击（MIA）漏洞，提出SAMA攻击方法，相比基线实现30%相对AUC提升，揭示DLM存在显著隐私风险。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归模型的替代方案具有前景，但其在成员推理攻击下的隐私泄露风险尚未得到充分探索。与自回归模型的单一固定预测模式不同，DLM的多重可掩码配置指数级增加了攻击机会，需要系统研究其MIA漏洞。

Method: 提出SAMA（子集聚合成员攻击）方法：通过采样渐进密度的掩码子集，应用基于符号的统计量处理重尾噪声，采用逆加权聚合优先处理稀疏掩码的干净信号，将稀疏记忆检测转化为鲁棒的投票机制。

Result: 在9个数据集上的实验显示，SAMA相比最佳基线实现30%相对AUC提升，在低误报率下达到8倍改进，显著优于现有方法。

Conclusion: 研究揭示了DLM存在显著且先前未知的隐私漏洞，其多重掩码配置极大增加了攻击面，需要开发针对性的隐私防御机制来保护扩散语言模型。

Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.

</details>


### [146] [Scaling Next-Brain-Token Prediction for MEG](https://arxiv.org/abs/2601.20138)
*Richard Csaky*

Main category: cs.LG

TL;DR: 提出一个用于源空间MEG的大规模自回归模型，能够跨数据集和扫描仪进行长上下文的下一个token预测，处理超过500小时的数据，并生成分钟级的MEG信号。


<details>
  <summary>Details</summary>
Motivation: 需要开发能够处理大规模MEG数据、实现长上下文预测和生成的模型，以促进脑信号分析和生成研究。

Method: 使用改进的SEANet风格向量量化器将多通道MEG压缩为扁平化token流，在Qwen2.5-VL骨干网络上从头训练，预测下一个脑token并递归生成分钟级MEG信号。

Result: 在跨数据集泛化测试中，生成结果在长时间滚动中保持相对稳定，且比交换控制组更接近正确的延续信号。

Conclusion: 该模型成功实现了大规模MEG数据的长期文预测和生成，为脑信号分析和生成提供了有效工具。

Abstract: We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce three task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.

</details>


### [147] [Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning](https://arxiv.org/abs/2601.20154)
*Bo Dai,Na Li,Dale Schuurmans*

Main category: cs.LG

TL;DR: 该论文提出从谱表示视角建立表示学习的统一理论框架，揭示现有成功自监督学习算法的谱本质，为算法设计和实践应用提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在表示学习方面取得显著进展，但缺乏统一的理论框架。现有方法目标多样、流程各异，缺乏清晰统一的理解，阻碍了表示学习的进一步发展，使得理论理解缺失、高效算法设计原则不明确、实践应用缺乏理论依据。

Method: 从谱表示视角对表示进行理论分析，揭示现有成功自监督学习算法的谱本质，建立表示学习的统一理论框架。

Result: 提出了一个原则性的表示学习基础框架，揭示了自监督学习算法的谱本质，为理解和分析表示学习提供了统一框架。

Conclusion: 该框架不仅为现有方法提供理论理解，还能启发开发更高效、易用的表示学习算法，为实际应用提供原则性指导。

Abstract: Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. In this paper, we are therefore compelled to develop a principled foundation of representation learning. We first theoretically investigate the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications.

</details>


### [148] [PASS: Ambiguity Guided Subsets for Scalable Classical and Quantum Constrained Clustering](https://arxiv.org/abs/2601.20157)
*Pedro Chumpitaz-Flores,My Duong,Ying Mao,Kaixun Hua*

Main category: cs.LG

TL;DR: PASS框架通过将must-link约束压缩为伪点，并采用两种选择器（边界规则和信息几何规则）来选择信息量最大的子集，在保持约束满足的同时实现可扩展的高质量聚类。


<details>
  <summary>Details</summary>
Motivation: 现有成对约束聚类方法在数据可扩展性方面存在困难，特别是在量子或量子混合聚类等利基应用中。需要一种既能保持ML和CL约束满足，又能实现可扩展高质量聚类的方法。

Method: PASS框架：1) 将must-link约束压缩为伪点；2) 提供两种选择器：约束感知边界规则（收集边界点和所有检测到的CL违反）和信息几何规则（通过软分配后验的Fisher-Rao距离评分，在预算下选择最高信息子集）。

Result: 在多样化基准测试中，PASS以显著低于精确或基于惩罚方法的成本获得竞争性的SSE（平方误差和），并在先前方法失败的场景中保持有效性。

Conclusion: PASS框架通过智能子集选择解决了成对约束聚类的可扩展性问题，在保持约束满足的同时实现了高效且高质量的聚类解决方案。

Abstract: Pairwise-constrained clustering augments unsupervised partitioning with side information by enforcing must-link (ML) and cannot-link (CL) constraints between specific samples, yielding labelings that respect known affinities and separations. However, ML and CL constraints add an extra layer of complexity to the clustering problem, with current methods struggling in data scalability, especially in niche applications like quantum or quantum-hybrid clustering. We propose PASS, a pairwise-constraints and ambiguity-driven subset selection framework that preserves ML and CL constraints satisfaction while allowing scalable, high-quality clustering solution. PASS collapses ML constraints into pseudo-points and offers two selectors: a constraint-aware margin rule that collects near-boundary points and all detected CL violations, and an information-geometric rule that scores points via a Fisher-Rao distance derived from soft assignment posteriors, then selects the highest-information subset under a simple budget. Across diverse benchmarks, PASS attains competitive SSE at substantially lower cost than exact or penalty-based methods, and remains effective in regimes where prior approaches fail.

</details>


### [149] [What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering](https://arxiv.org/abs/2601.20164)
*Jim Maar,Denis Paperno,Callum Stuart McDougall,Neel Nanda*

Main category: cs.LG

TL;DR: 本文提出了一种更简单的方法来评估语言模型中的隐式规划能力，通过押韵诗歌生成和问答案例研究，发现即使是小至10亿参数的模型也存在隐式规划机制。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明语言模型在训练过程中会表现出隐式规划行为，但现有研究方法复杂。本文旨在开发更简单、可扩展的技术来评估各种语言模型的隐式规划能力。

Method: 提出简化的隐式规划评估方法，通过押韵诗歌生成和问答两个案例研究，使用向量引导技术操纵中间token生成，影响最终的押韵词或答案词。

Result: 研究发现隐式规划是普遍存在的机制，存在于比先前认为更小的模型中（从10亿参数开始）。通过向量引导可以成功操纵中间token的生成，影响最终的输出结果。

Conclusion: 该方法为研究语言模型的隐式规划能力提供了广泛适用的直接途径。理解语言模型的规划能力对AI安全和控制决策具有重要意义。

Abstract: Prior work suggests that language models, while trained on next token prediction, show implicit planning behavior: they may select the next token in preparation to a predicted future token, such as a likely rhyming word, as supported by a prior qualitative study of Claude 3.5 Haiku using a cross-layer transcoder. We propose much simpler techniques for assessing implicit planning in language models. With case studies on rhyme poetry generation and question answering, we demonstrate that our methodology easily scales to many models. Across models, we find that the generated rhyme (e.g. "-ight") or answer to a question ("whale") can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens leading up to the rhyme or answer word. We show that implicit planning is a universal mechanism, present in smaller models than previously thought, starting from 1B parameters. Our methodology offers a widely applicable direct way to study implicit planning abilities of LLMs. More broadly, understanding planning abilities of language models can inform decisions in AI safety and control.

</details>


### [150] [Local Duality for Sparse Support Vector Machines](https://arxiv.org/abs/2601.20170)
*Penghe Zhang,Naihua Xiu,Houduo Qi*

Main category: cs.LG

TL;DR: 论文建立了稀疏支持向量机(SSVM)的局部对偶理论，证明了SSVM是0/1损失SVM的对偶问题，其局部解为hSVM和rSVM的超参数选择提供指导，并解释了SSVM在实证研究中表现更优的原因。


<details>
  <summary>Details</summary>
Motivation: 稀疏支持向量机在优化中受到关注，但现有方法缺乏理论依据。论文旨在填补这一空白，为SSVM建立局部对偶理论，并探索其与hinge-loss SVM和ramp-loss SVM的关系。

Method: 开发SSVM的局部对偶理论，证明SSVM是0/1损失SVM的对偶问题，分析局部解的性质，研究hSVM全局解序列收敛到0/1损失SVM局部解的条件，以及0/1损失SVM局部解与rSVM局部解的关系。

Result: 证明了SSVM是0/1损失SVM的对偶问题，线性表示定理对其局部解成立；SSVM局部解为hSVM和rSVM的超参数选择提供指导；在特定条件下，hSVM全局解序列收敛到0/1损失SVM局部解；0/1损失SVM局部解也是rSVM的局部解。

Conclusion: SSVM的局部对偶理论为稀疏支持向量机提供了理论依据，解释了其在实证研究中优于hSVM和rSVM的原因，并通过真实数据集验证了SSVM的潜在优势。

Abstract: Due to the rise of cardinality minimization in optimization, sparse support vector machines (SSVMs) have attracted much attention lately and show certain empirical advantages over convex SVMs. A common way to derive an SSVM is to add a cardinality function such as $\ell_0$-norm to the dual problem of a convex SVM. However, this process lacks theoretical justification. This paper fills the gap by developing a local duality theory for such an SSVM formulation and exploring its relationship with the hinge-loss SVM (hSVM) and the ramp-loss SVM (rSVM). In particular, we prove that the derived SSVM is exactly the dual problem of the 0/1-loss SVM, and the linear representer theorem holds for their local solutions. The local solution of SSVM also provides guidelines on selecting hyperparameters of hSVM and rSVM. {Under specific conditions, we show that a sequence of global solutions of hSVM converges to a local solution of 0/1-loss SVM. Moreover, a local minimizer of 0/1-loss SVM is a local minimizer of rSVM.} This explains why a local solution induced by SSVM outperforms hSVM and rSVM in the prior empirical study. We further conduct numerical tests on real datasets and demonstrate potential advantages of SSVM by working with locally nice solutions proposed in this paper.

</details>


### [151] [Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization](https://arxiv.org/abs/2601.20172)
*James Amarel,Robyn Miller,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: cs.LG

TL;DR: 提出一种基于梯度重叠的诊断方法，用于评估神经PDE求解器是否内化了物理对称性，超越了传统的前向传播等变性测试。


<details>
  <summary>Details</summary>
Motivation: 研究神经PDE求解器如何内化物理对称性，需要超越传统的前向传播等变性测试，直接评估学习动态是否将物理等效配置耦合起来。

Method: 引入基于影响的诊断方法，通过测量参数更新在对称相关状态之间的传播来评估对称性内化。该方法计算沿群轨道的损失梯度加权重叠，探测学习损失景观的局部几何结构。

Result: 应用于自回归流体流动仿真器时，轨道梯度相干性提供了学习在对称变换上泛化的机制，并指示训练何时选择对称兼容的盆地。

Conclusion: 提出了一种新颖的技术，用于评估代理模型是否内化了已知求解器的对称性属性，为理解神经PDE求解器的对称性学习提供了新视角。

Abstract: We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.

</details>


### [152] [MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis](https://arxiv.org/abs/2601.20173)
*Zeyang Huang,Takanori Fujiwara,Angelos Chatzimparmpas,Wandrille Duchemin,Andreas Kerren*

Main category: cs.LG

TL;DR: MAPLE是一种新的非线性降维方法，通过自监督学习和最大流形容量表示来改进UMAP的流形建模能力，特别适用于高维生物或图像数据。


<details>
  <summary>Details</summary>
Motivation: UMAP在流形建模方面存在改进空间，特别是对于具有显著簇内方差和弯曲流形结构的高维数据（如生物或图像数据），需要更有效的流形几何编码方法。

Method: MAPLE采用自监督学习方法，利用最大流形容量表示（MMCRs）来压缩局部相似数据点之间的方差，同时放大不相似数据点之间的方差，从而更好地解开复杂流形结构。

Result: 定性和定量评估表明，MAPLE相比UMAP能产生更清晰的视觉簇分离和更精细的子簇分辨率，同时保持相当的计算成本。

Conclusion: MAPLE通过改进的流形建模方法，为高维数据的可视化分析提供了更有效的非线性降维解决方案，特别适用于生物和图像数据等复杂流形结构。

Abstract: We present a new nonlinear dimensionality reduction method, MAPLE, that enhances UMAP by improving manifold modeling. MAPLE employs a self-supervised learning approach to more efficiently encode low-dimensional manifold geometry. Central to this approach are maximum manifold capacity representations (MMCRs), which help untangle complex manifolds by compressing variances among locally similar data points while amplifying variance among dissimilar data points. This design is particularly effective for high-dimensional data with substantial intra-cluster variance and curved manifold structures, such as biological or image data. Our qualitative and quantitative evaluations demonstrate that MAPLE can produce clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.

</details>


### [153] [NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods](https://arxiv.org/abs/2601.20174)
*Alexander Benanti,Xi Han,Hong Qin*

Main category: cs.LG

TL;DR: NeuraLSP：一种新颖的神经预处理器，利用系统矩阵近零空间向量的左奇异子空间，通过压缩谱信息到固定低秩算子来解决现有方法中的秩膨胀问题，实现高达53%的加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的预处理器方法通过聚合离散化系统矩阵为图来提取连接结构，但存在秩膨胀和收敛速度不理想的问题，需要更有效的神经预处理器来解决这些限制。

Method: 提出NeuraLSP神经预处理器，结合新的损失函数度量，利用系统矩阵近零空间向量的左奇异子空间，将谱信息压缩到固定低秩算子中，从而避免秩膨胀问题。

Result: 方法在理论上具有保证，并在多种PDE家族上表现出经验鲁棒性，能够避免秩膨胀问题，实现高达53%的加速效果。

Conclusion: NeuraLSP通过创新的损失函数和谱信息压缩机制，有效解决了现有神经预处理器方法的秩膨胀问题，在理论和实验上都证明了其优越性，为PDE求解提供了更高效的预处理器方案。

Abstract: Numerical techniques for solving partial differential equations (PDEs) are integral for many fields across science and engineering. Such techniques usually involve solving large, sparse linear systems, where preconditioning methods are critical. In recent years, neural methods, particularly graph neural networks (GNNs), have demonstrated their potential through accelerated convergence. Nonetheless, to extract connective structures, existing techniques aggregate discretized system matrices into graphs, and suffer from rank inflation and a suboptimal convergence rate. In this paper, we articulate NeuraLSP, a novel neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors. By compressing spectral information into a fixed low-rank operator, our method exhibits both theoretical guarantees and empirical robustness to rank inflation, affording up to a 53% speedup. Besides the theoretical guarantees for our newly-formulated loss function, our comprehensive experimental results across diverse families of PDEs also substantiate the aforementioned theoretical advances.

</details>


### [154] [Causal-Driven Feature Evaluation for Cross-Domain Image Classification](https://arxiv.org/abs/2601.20176)
*Chen Cheng,Ang Li*

Main category: cs.LG

TL;DR: 该论文从因果视角重新审视OOD分类，提出通过评估表征的必要性和充分性来衡量其因果有效性，而非仅追求域不变性，从而提升分布偏移下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有OOD泛化方法大多追求域不变表征，隐含假设不变性即意味着可靠性。然而，跨域不变的特征并不一定对预测具有因果有效性。需要从因果角度重新审视OOD分类问题。

Method: 提出显式的片段级框架，直接测量表征在分布偏移下的因果有效性（必要性和充分性），提供比单纯不变性更可靠的评估标准。

Result: 在多域基准测试中，该方法在OOD性能上取得一致改进，特别是在具有挑战性的域偏移情况下表现突出。

Conclusion: 因果评估对于鲁棒泛化具有重要价值，通过衡量表征的因果有效性而非仅追求不变性，能够更好地应对真实世界中的分布偏移挑战。

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in real-world classification, where test distributions often differ substantially from training data. Most existing approaches pursue domain-invariant representations, implicitly assuming that invariance implies reliability. However, features that are invariant across domains are not necessarily causally effective for prediction.
  In this work, we revisit OOD classification from a causal perspective and propose to evaluate learned representations based on their necessity and sufficiency under distribution shift. We introduce an explicit segment-level framework that directly measures causal effectiveness across domains, providing a more faithful criterion than invariance alone.
  Experiments on multi-domain benchmarks demonstrate consistent improvements in OOD performance, particularly under challenging domain shifts, highlighting the value of causal evaluation for robust generalization.

</details>


### [155] [On the Computational Complexity of Performative Prediction](https://arxiv.org/abs/2601.20180)
*Ioannis Anagnostides,Rohan Chauhan,Ioannis Panageas,Tuomas Sandholm,Jingming Yan*

Main category: cs.LG

TL;DR: 本文揭示了在预测模型部署影响数据分布（performative prediction）的场景中，当影响强度ρ>1时，计算稳定点的计算复杂度会发生急剧相变，成为PPAD完全问题，与一般和博弈的纳什均衡计算难度相当。


<details>
  <summary>Details</summary>
Motivation: 现有研究已知在弱影响（ρ<1）情况下，简单重训练动态能够线性收敛，但在强影响（ρ>1）情况下的计算复杂度一直未解决。本文旨在探究这一关键阈值附近的计算复杂性相变。

Method: 通过理论分析建立计算复杂度的相变边界，证明在ρ=1+O(ε)时计算ε-稳定点是PPAD完全的。技术贡献包括将PPAD-hardness结果扩展到一般凸域，并在策略分类的特殊情况下证明计算策略局部最优是PLS-hard的。

Result: 发现了尖锐的相变现象：当ρ>1时，即使ρ仅略大于1（ρ=1+O(ε)），计算ε-稳定点也是PPAD完全的，这意味着该问题与一般和博弈的纳什均衡计算在多项式时间内等价。这一不可计算性甚至在二次损失函数和线性分布偏移的简单设置中仍然存在。

Conclusion: 本文首次完整刻画了performative prediction中计算复杂度的相变边界，揭示了在强影响情况下计算稳定点的固有难度，为理解预测模型部署与数据分布交互的计算复杂性提供了重要理论框架。

Abstract: Performative prediction captures the phenomenon where deploying a predictive model shifts the underlying data distribution. While simple retraining dynamics are known to converge linearly when the performative effects are weak ($ρ< 1$), the complexity in the regime $ρ> 1$ was hitherto open. In this paper, we establish a sharp phase transition: computing an $ε$-performatively stable point is PPAD-complete -- and thus polynomial-time equivalent to Nash equilibria in general-sum games -- even when $ρ= 1 + O(ε)$. This intractability persists even in the ostensibly simple setting with a quadratic loss function and linear distribution shifts. One of our key technical contributions is to extend this PPAD-hardness result to general convex domains, which is of broader interest in the complexity of variational inequalities. Finally, we address the special case of strategic classification, showing that computing a strategic local optimum is PLS-hard.

</details>


### [156] [Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery](https://arxiv.org/abs/2601.20193)
*Zhipeng Zhang,Wenting Ma,Kai Li,Meng Guo,Lei Yang,Wei Yu,Hongji Cui,Yichen Zhang,Mo Zhang,Jinzhe Lin,Zhenjie Yao*

Main category: cs.LG

TL;DR: 提出一种元认知强化学习框架，通过内部可靠性信号评估、调节和恢复学习行为，解决传统鲁棒强化学习在噪声环境下要么过于保守要么灾难性失败的问题。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒强化学习方法通常专注于抑制不可靠经验或损坏的奖励，但缺乏对其自身学习过程可靠性的推理能力，导致要么对噪声过度反应变得过于保守，要么在不确定性累积时灾难性失败。

Method: 提出元认知强化学习框架，引入基于价值预测误差稳定性(VPES)驱动的元信任变量，通过故障安全调节和渐进信任恢复来调制学习动态。

Result: 在带有奖励损坏的连续控制基准测试中，具有恢复能力的元认知控制相比强鲁棒基线实现了更高的平均回报，并显著减少了后期训练失败。

Conclusion: 元认知强化学习框架通过内部可靠性评估和调节机制，能够有效处理噪声环境下的学习稳定性问题，提高鲁棒性和性能。

Abstract: Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.

</details>


### [157] [DeRaDiff: Denoising Time Realignment of Diffusion Models](https://arxiv.org/abs/2601.20198)
*Ratnavibusena Don Shahain Manujith,Yang Zhang,Teoh Tze Tzun,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: DeRaDiff提出了一种无需重新训练即可在采样时调节正则化强度的扩散模型对齐方法，避免了传统方法需要多次训练不同正则化强度模型的昂贵计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型对齐人类偏好时，KL正则化强度的选择非常关键：强度过高导致对齐有限，强度过低导致"奖励黑客"问题。现有方法需要训练多个不同正则化强度的模型进行筛选，计算成本极高。

Method: DeRaDiff通过去噪时间重对齐技术，在采样时调节正则化强度。该方法将反向步骤的参考分布替换为对齐后验和参考后验的几何混合，在常见调度器下得到闭式更新，仅需一个可调参数λ即可实时控制。

Result: 实验表明，DeRaDiff在不同文本图像对齐和图像质量指标上，能够很好地近似从头训练的不同正则化强度模型，为寻找最优正则化强度提供了高效方法。

Conclusion: DeRaDiff消除了昂贵的对齐扫描需求，显著降低了计算成本，为扩散模型对齐提供了一种高效的实时正则化强度调节方法。

Abstract: Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to "reward hacking". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.

</details>


### [158] [Minimum-Cost Network Flow with Dual Predictions](https://arxiv.org/abs/2601.20203)
*Zhiyang Chen,Hailong Yao,Xia Yin*

Main category: cs.LG

TL;DR: 提出首个结合对偶预测的最小成本网络流算法，基于ε-松弛方法，在预测误差范围内提供时间复杂度和样本复杂度保证，在交通网络和芯片逃逸布线应用中实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 机器学习预测已被证明可以提升经典算法性能，但尚未有结合对偶预测的最小成本网络流算法。本研究旨在填补这一空白，利用预测信息来加速网络流计算。

Method: 基于经典的ε-松弛最小成本流算法，加入对偶预测。算法复杂度分析基于无穷范数预测误差，同时提供PAC学习的样本复杂度界限。在应用中分别使用固定预测和基于特征的神经网络模型进行预测。

Result: 在交通网络和芯片逃逸布线两个最小成本流应用中，实验结果显示平均分别获得12.74倍和1.64倍的加速效果，验证了理论结果的有效性。

Conclusion: 成功开发了首个结合对偶预测的最小成本网络流算法，在预测误差范围内提供理论保证，并在实际应用中展示了显著的性能提升，为机器学习增强经典算法提供了新的范例。

Abstract: Recent work has shown that machine-learned predictions can provably improve the performance of classic algorithms. In this work, we propose the first minimum-cost network flow algorithm augmented with a dual prediction. Our method is based on a classic minimum-cost flow algorithm, namely $\varepsilon$-relaxation. We provide time complexity bounds in terms of the infinity norm prediction error, which is both consistent and robust. We also prove sample complexity bounds for PAC-learning the prediction. We empirically validate our theoretical results on two applications of minimum-cost flow, i.e., traffic networks and chip escape routing, in which we learn a fixed prediction, and a feature-based neural network model to infer the prediction, respectively. Experimental results illustrate $12.74\times$ and $1.64\times$ average speedup on two applications.

</details>


### [159] [Hyperparameter Transfer with Mixture-of-Expert Layers](https://arxiv.org/abs/2601.20205)
*Tianze Jiang,Blake Bordelon,Cengiz Pehlevan,Boris Hanin*

Main category: cs.LG

TL;DR: 提出一种新的MoE层参数化方法，通过动态平均场理论分析，实现从5100万到20亿参数模型的超参数可靠迁移，降低MoE模型调参成本。


<details>
  <summary>Details</summary>
Motivation: MoE层虽然能解耦总参数量和激活参数量，但增加了训练复杂度：需要调优路由器权重等新参数，且专家数量和大小等架构维度选择困难。需要廉价可靠的超参数选择方法。

Method: 提出新的MoE transformer模型参数化方法，基于动态平均场理论分析。在固定token预算下，通过改变模型宽度、深度、专家数量和专家大小等维度进行参数化。

Result: 参数化方法能实现从5100万到超过20亿总参数模型的可靠超参数迁移。从小模型短token范围确定的超参数可成功用于大模型长token范围训练。

Conclusion: 提出的参数化方法显著降低了MoE模型的超参数调优成本，实现了跨模型规模的可靠超参数迁移，为大规模MoE模型训练提供了实用解决方案。

Abstract: Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.

</details>


### [160] [Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning](https://arxiv.org/abs/2601.20209)
*Jinyang Wu,Shuo Yang,Changpeng Yang,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: Spark框架通过关键状态动态分支实现策略感知的探索，在关键决策点选择性分支以高效利用计算资源，减少对高质量轨迹的依赖，提升长时程任务的强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习训练大型语言模型作为智能代理时面临挑战：长时程任务需要高质量轨迹但资源有限；现有方法盲目扩大rollout规模，在平凡步骤上浪费计算资源，无法保证样本质量。

Method: 提出Spark框架（Strategic Policy-Aware exploration via Key-state dynamic branching），在关键决策状态选择性分支进行资源高效探索。利用智能体内在决策信号激活自适应分支探索，探测有前景的轨迹，实现精确资源分配，优先采样质量而非盲目覆盖。

Result: 在多样化任务（如具身规划）上的实验表明，Spark以显著更少的训练样本获得更高的成功率，在未见场景中展现出强大的泛化能力。

Conclusion: Spark通过关键状态动态分支实现了策略感知的探索，减少对人类先验的依赖，使智能体能够自主扩展探索并实现更强的泛化能力，为资源受限下的长时程任务强化学习提供了有效解决方案。

Abstract: Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.

</details>


### [161] [An Accounting Identity for Algorithmic Fairness](https://arxiv.org/abs/2601.20217)
*Hadi Elzayn,Jacob Goldin*

Main category: cs.LG

TL;DR: 本文推导了预测模型的会计恒等式，将准确性与公平性标准联系起来，表明准确性和公平性在二元预测中是互补关系：提高准确性会减少总体不公平预算，反之亦然。


<details>
  <summary>Details</summary>
Motivation: 当前公平性研究中存在准确性与公平性之间的权衡问题，以及不同公平性标准之间的不可能性结果。本文旨在通过数学恒等式揭示准确性与各种公平性标准之间的内在联系，为理解这些关系提供理论框架。

Method: 推导预测模型的会计恒等式，将模型准确性（均方误差）与群体间校准误差和错误不平衡等公平性标准联系起来。该恒等式适用于全局校准模型，并扩展到二元和非二元预测任务。

Result: 恒等式表明：对于全局校准模型，群体内校准误差的加权和与群体间错误不平衡等于"总体不公平预算"。在二元结果中，该预算等于均方误差乘以跨结果类别的群体流行度差异。实验验证了理论，显示许多公平性干预主要在公平性违规之间进行替代，当降低准确性时往往会扩大总体不公平预算。

Conclusion: 准确性和公平性在二元预测中是互补而非权衡关系：提高准确性会减少总体不公平预算。该框架将标准不可能性结果作为特例，并自然扩展到非二元预测任务，展示了额外结果信息如何缓解公平性不兼容性。

Abstract: We derive an accounting identity for predictive models that links accuracy with common fairness criteria. The identity shows that for globally calibrated models, the weighted sums of miscalibration within groups and error imbalance across groups is equal to a "total unfairness budget." For binary outcomes, this budget is the model's mean-squared error times the difference in group prevalence across outcome classes. The identity nests standard impossibility results as special cases, while also describing inherent tradeoffs when one or more fairness measures are not perfectly satisfied. The results suggest that accuracy and fairness are best viewed as complements in binary prediction tasks: increasing accuracy necessarily shrinks the total unfairness budget and vice-versa. Experiments on benchmark data confirm the theory and show that many fairness interventions largely substitute between fairness violations, and when they reduce accuracy they tend to expand the total unfairness budget. The results extend naturally to prediction tasks with non-binary outcomes, illustrating how additional outcome information can relax fairness incompatibilities and identifying conditions under which the binary-style impossibility does and does not extend to regression tasks.

</details>


### [162] [ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance](https://arxiv.org/abs/2601.20227)
*Zichao Yu,Ming Li,Wenyi Zhang,Difan Zou,Weiguo Gao*

Main category: cs.LG

TL;DR: ProFlow：一种零样本物理一致采样框架，使用预训练生成模型从稀疏观测中推断物理场，严格满足偏微分方程约束


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在解决物理反问题时难以强制执行硬物理约束，要么需要昂贵重训练，要么会破坏已学习的生成先验。需要一种采样机制能同时满足严格物理一致性、观测保真度和预训练先验的统计结构。

Method: 提出ProFlow框架，采用严格的两步交替方案：1）终端优化步骤，通过近端最小化将流预测投影到物理和观测一致集的交集；2）插值步骤，将精炼状态映射回生成轨迹以保持与学习流概率路径的一致性。该过程可解释为局部最大后验更新序列。

Result: 在Poisson、Helmholtz、Darcy和粘性Burgers方程上的综合基准测试表明，ProFlow相比最先进的扩散和流基线方法，实现了更优越的物理和观测一致性，以及更准确的分布统计。

Conclusion: ProFlow提供了一种有效的零样本物理一致采样框架，能够在不进行任务特定重训练的情况下，从稀疏观测中推断物理场，同时严格满足偏微分方程约束，为计算物理中的反问题提供了新的解决方案。

Abstract: Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.

</details>


### [163] [Certificate-Guided Pruning for Stochastic Lipschitz Optimization](https://arxiv.org/abs/2601.20231)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出CGP方法，通过显式维护潜在最优点的活跃集，为黑箱Lipschitz函数优化提供最优性证书和可测量的进度保证。


<details>
  <summary>Details</summary>
Motivation: 现有自适应离散化方法隐式避免次优区域，但缺乏显式的最优性证书和可测量的进度保证，需要一种能提供明确最优性证明的方法。

Method: CGP方法通过置信调整的Lipschitz包络维护活跃集A_t，任何在A_t外的点都被高概率证明为次优。在边界条件下，证明A_t体积以受控速率收缩。

Result: 在边界条件下获得样本复杂度$\tildeO(\varepsilon^{-(2+α)})$。三个扩展：CGP-Adaptive在线学习L，CGP-TR通过信任区域扩展到高维，CGP-Hybrid检测局部平滑时切换到GP细化。

Conclusion: CGP变体在12个基准测试中匹配或超越强基线，同时通过证书体积提供原则性停止准则，为黑箱优化提供了显式最优性证书和进度保证。

Abstract: We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $α$, we prove $\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\tildeO(\varepsilon^{-(2+α)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\log T)$ overhead; CGP-TR scales to $d > 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.

</details>


### [164] [HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH](https://arxiv.org/abs/2601.20255)
*Yueyang Wang,Jiawei Fu,Baolong Bi,Xili Wang,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 本文提出HE-SNR（高熵信噪比）作为评估LLM在软件工程任务中潜力的新指标，解决了传统困惑度指标在长上下文场景下的局限性，并通过熵压缩假设为模型优化提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够有效指导LLM中期训练阶段的指标，传统困惑度指标受"长上下文税"影响，与下游软件工程性能相关性弱，无法准确评估模型在复杂工程任务中的潜力。

Method: 1. 引入严格的数据过滤策略；2. 提出熵压缩假设，将智能重新定义为将不确定性结构化为低阶熵压缩状态的能力；3. 基于细粒度熵分析制定HE-SNR（高熵信噪比）指标；4. 在工业级MoE模型上进行验证，测试不同上下文窗口（32K/128K）。

Result: HE-SNR指标在工业级MoE模型验证中表现出优越的鲁棒性和预测能力，相比传统困惑度指标能更准确地预测模型在SWE-bench等软件工程任务上的性能。

Conclusion: 该研究为优化LLM在复杂工程领域的潜在能力提供了理论基础和实用工具，HE-SNR指标能够有效指导中期训练，提升模型在软件工程任务上的表现。

Abstract: SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the "Long-Context Tax" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders ("reasonable hesitation"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.

</details>


### [165] [C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding](https://arxiv.org/abs/2601.20257)
*Jinren Ding,Xuejian Xu,Shen Jiang,Zhitong Hao,Jinhui Yang,Peng Jiang*

Main category: cs.LG

TL;DR: C2框架通过交叉学习块和约束感知损失增强决策变换器，提升自动出价性能


<details>
  <summary>Details</summary>
Motivation: 决策变换器在自动出价中虽能捕捉时间依赖，但存在两个关键局限：状态、动作和回报序列间的跨相关性建模不足，以及对最优/次优行为的不加区分学习

Method: 提出C2框架，包含两个核心创新：1) 通过交叉注意力机制的交叉学习块强化序列间相关性建模；2) 结合预算和CPA约束的约束感知损失，选择性学习最优轨迹

Result: 在AuctionNet数据集上的离线评估显示，C2在不同预算设置下均取得性能提升（最高超过最先进方法GAVE 3.23%），消融研究验证了CLB和CL的互补协同作用

Conclusion: C2框架通过增强序列间相关性建模和选择性学习机制，显著提升了决策变换器在自动出价任务中的性能表现

Abstract: Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.

</details>


### [166] [Robust SDE Parameter Estimation Under Missing Time Information Setting](https://arxiv.org/abs/2601.20268)
*Long Van Tran,Truyen Tran,Phuoc Nguyen*

Main category: cs.LG

TL;DR: 提出一种新框架，在时间顺序信息缺失或损坏时，同时恢复时间顺序并估计随机微分方程参数


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多动态过程（金融、健康、系统生物学）可用随机微分方程建模，但传统参数估计方法需要准确的时间戳观测序列。当时间顺序信息被破坏、缺失或故意隐藏（如隐私保护）时，现有方法往往失效。

Method: 利用前向和后向过程的不对称性，推导得分匹配准则来推断观测对之间的正确时间顺序；通过排序过程恢复总顺序；使用最大似然估计从重建序列中估计SDE参数。

Result: 在合成和真实数据集上进行广泛实验，证明该方法在时间顺序缺失情况下的有效性，将参数估计扩展到时间顺序缺失的场景。

Conclusion: 该方法能够同时恢复时间顺序并估计SDE参数，扩展了参数估计在敏感领域的适用性，解决了时间顺序信息缺失或损坏时的挑战。

Abstract: Recent advances in stochastic differential equations (SDEs) have enabled robust modeling of real-world dynamical processes across diverse domains, such as finance, health, and systems biology. However, parameter estimation for SDEs typically relies on accurately timestamped observational sequences. When temporal ordering information is corrupted, missing, or deliberately hidden (e.g., for privacy), existing estimation methods often fail. In this paper, we investigate the conditions under which temporal order can be recovered and introduce a novel framework that simultaneously reconstructs temporal information and estimates SDE parameters. Our approach exploits asymmetries between forward and backward processes, deriving a score-matching criterion to infer the correct temporal order between pairs of observations. We then recover the total order via a sorting procedure and estimate SDE parameters from the reconstructed sequence using maximum likelihood. Finally, we conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of our method, extending parameter estimation to settings with missing temporal order and broadening applicability in sensitive domains.

</details>


### [167] [The Forecast After the Forecast: A Post-Processing Shift in Time Series](https://arxiv.org/abs/2601.20280)
*Daojun Liang,Qi Li,Yinglong Wang,Jing Chen,Hu Zhang,Xiaoxiao Cui,Qizheng Wang,Shuo Li*

Main category: cs.LG

TL;DR: 提出δ-Adapter，一种轻量级、架构无关的后处理方法，通过输入微调和输出残差校正来提升已部署时间序列预测模型的性能，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测模型在架构改进上已接近收益递减，而战略性的后处理是一个关键但未被充分探索的机会。本文旨在解决"最后一公里"问题：在不重新训练或修改已部署骨干模型的情况下，提高预测准确性和不确定性估计。

Method: 提出δ-Adapter方法，在两个接口学习微小、有界的模块：1) 输入微调（对协变量进行软编辑），2) 输出残差校正。该方法提供局部下降保证、O(δ)漂移界限和组合稳定性。同时可作为特征选择器学习稀疏的、时间感知的输入掩码，并作为分布校准器引入分位数校准器和保形校正器来提供校准的、个性化的置信区间。

Result: 在多种骨干模型和数据集上的实验表明，δ-Adapter能以可忽略的计算成本和无需接口更改的方式，显著提高预测准确性和校准质量。

Conclusion: δ-Adapter为已部署的时间序列预测模型提供了一种有效的后处理解决方案，通过轻量级的适配器模块实现了性能提升、特征选择和不确定性校准，具有实际部署价值。

Abstract: Time series forecasting has long been dominated by advances in model architecture, with recent progress driven by deep learning and hybrid statistical techniques. However, as forecasting models approach diminishing returns in accuracy, a critical yet underexplored opportunity emerges: the strategic use of post-processing. In this paper, we address the last-mile gap in time-series forecasting, which is to improve accuracy and uncertainty without retraining or modifying a deployed backbone. We propose $δ$-Adapter, a lightweight, architecture-agnostic way to boost deployed time series forecasters without retraining. $δ$-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. We provide local descent guarantees, $O(δ)$ drift bounds, and compositional stability for combined adapters. Meanwhile, it can act as a feature selector by learning a sparse, horizon-aware mask over inputs to select important features, thereby improving interpretability. In addition, it can also be used as a distribution calibrator to measure uncertainty. Thus, we introduce a Quantile Calibrator and a Conformal Corrector that together deliver calibrated, personalized intervals with finite-sample coverage. Our experiments across diverse backbones and datasets show that $δ$-Adapter improves accuracy and calibration with negligible compute and no interface changes.

</details>


### [168] [Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle](https://arxiv.org/abs/2601.20282)
*Viet Hung Dinh,Ming Ding,Youyang Qu,Kanchana Thilakarathna*

Main category: cs.LG

TL;DR: 该研究探索了Transformer注意力层中的记忆机制，提出关键词作为检索线索的假设，并识别了专门编码关键词的神经元，为可解释AI和机器遗忘提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释AI领域在不断发展，但Transformer注意力层在大型语言模型中的具体作用仍未充分探索。监管压力增加促使研究者关注透明度、问责制和隐私保护的机器遗忘，需要深入理解注意力层的记忆机制。

Method: 借鉴心理学和计算心理语言学的研究，将Transformer注意力机制与人类记忆中的线索检索联系起来。提出关键词作为检索线索的假设，并通过实验识别注意力层中专门编码关键词的神经元。

Result: 提供了关键词作为检索线索假设的汇聚证据，成功分离出注意力层中专门编码上下文定义关键词的神经元。这些关键词可以从识别的神经元中提取，并应用于下游任务如机器遗忘。

Conclusion: Transformer注意力层实现了类似人类记忆的线索检索机制，关键词在其中扮演关键角色。识别出的关键词编码神经元为可解释AI和机器遗忘提供了新的技术途径，有助于提高模型透明度和可控性。

Abstract: While explainable artificial intelligence (XAI) for large language models (LLMs) remains an evolving field with many unresolved questions, increasing regulatory pressures have spurred interest in its role in ensuring transparency, accountability, and privacy-preserving machine unlearning. Despite recent advances in XAI have provided some insights, the specific role of attention layers in transformer based LLMs remains underexplored. This study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue based retrieval in human memory. In this view, queries encode the retrieval context, keys index candidate memory traces, attention weights quantify cue trace similarity, and values carry the encoded content, jointly enabling the construction of a context representation that precedes and facilitates memory retrieval. Guided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis. In addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords. Consequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.

</details>


### [169] [A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography](https://arxiv.org/abs/2601.20291)
*Kaiyi Yang,Seonyeong Park,Gangwon Jeong,Hsuan-Kai Huang,Alexander A. Oraevsky,Umberto Villa,Mark A. Anastasio*

Main category: cs.LG

TL;DR: 提出了一种学习型空间脉冲响应补偿框架，用于3D光声计算机断层扫描，通过数据域补偿改善图像分辨率，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统光声计算机断层扫描中，使用大表面积超声换能器提高检测灵敏度，但忽略空间脉冲响应的解析重建方法会损害空间分辨率，而考虑SIR的优化方法计算成本过高，特别是在3D应用中。

Method: 提出学习型SIR补偿框架，在数据域将SIR污染的光声测量数据映射到理想点状换能器应记录的补偿数据。研究了两种模型：U-Net模型和物理启发的Deconv-Net模型，并设计了快速解析训练数据生成流程。

Result: 虚拟成像研究表明该方法能改善分辨率，对噪声变化、物体复杂性和声速异质性具有鲁棒性。在体内乳腺成像数据中，学习补偿模型揭示了被SIR伪影掩盖的精细结构。

Conclusion: 这是首个在3D PACT成像中展示学习型SIR补偿的研究，实现了准确且快速的图像重建，平衡了计算效率与图像质量。

Abstract: Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.

</details>


### [170] [Cheap2Rich: A Multi-Fidelity Framework for Data Assimilation and System Identification of Multiscale Physics -- Rotating Detonation Engines](https://arxiv.org/abs/2601.20295)
*Yuxuan Bao,Jan Zajac,Megan Powers,Venkat Raman,J. Nathan Kutz*

Main category: cs.LG

TL;DR: Cheap2Rich是一个多尺度数据同化框架，通过结合快速低精度先验模型和可学习的可解释差异修正，从稀疏传感器历史数据重建高保真状态空间，应用于旋转爆震发动机等复杂多尺度系统。


<details>
  <summary>Details</summary>
Motivation: 解决计算廉价模型与复杂物理系统之间的模拟-现实差距问题，特别是在多尺度设置中，降阶模型通常只能捕捉主导动态，而工程应用中需要更精确的状态重建。

Method: 提出Cheap2Rich多尺度数据同化框架，结合快速低精度先验模型和可学习的可解释差异修正，从稀疏测量数据重建高保真状态空间，特别应用于旋转爆震发动机系统。

Result: 成功从稀疏测量数据重建旋转爆震发动机的高保真状态，同时分离出与喷射器驱动效应相关的物理意义明确的差异动态，实现了复杂多尺度系统的数据同化和系统识别。

Conclusion: Cheap2Rich为复杂多尺度系统提供了一个通用的多保真度数据同化和系统识别框架，支持快速设计探索、实时监测和控制，同时提供可解释的差异动态分析。

Abstract: Bridging the sim2real gap between computationally inexpensive models and complex physical systems remains a central challenge in machine learning applications to engineering problems, particularly in multi-scale settings where reduced-order models typically capture only dominant dynamics. In this work, we present Cheap2Rich, a multi-scale data assimilation framework that reconstructs high-fidelity state spaces from sparse sensor histories by combining a fast low-fidelity prior with learned, interpretable discrepancy corrections. We demonstrate the performance on rotating detonation engines (RDEs), a challenging class of systems that couple detonation-front propagation with injector-driven unsteadiness, mixing, and stiff chemistry across disparate scales. Our approach successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects. The results highlight a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration and real-time monitoring and control while providing interpretable discrepancy dynamics. Code for this project is is available at: github.com/kro0l1k/Cheap2Rich.

</details>


### [171] [Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction](https://arxiv.org/abs/2601.20299)
*Tianyi Alex Qiu,Micah Carroll,Cameron Allen*

Main category: cs.LG

TL;DR: 提出基于同伴预测（peer prediction）的LLM评估与后训练方法，无需真实标签即可激励诚实回答，能有效对抗欺骗性模型


<details>
  <summary>Details</summary>
Motivation: 大型语言模型评估和后训练依赖监督，但困难任务往往缺乏强监督，现有评估容易被前沿模型利用导致欺骗性结果。机制设计中的激励兼容性研究未被充分利用

Method: 引入同伴预测方法，基于互预测性度量奖励诚实和信息丰富的回答，无需真实标签。该方法具有理论保证，并在最高405B参数的模型上进行了实证验证

Result: 使用同伴预测奖励训练8B模型可恢复因恶意微调而下降的真实性；发现同伴预测存在逆缩放特性，专家与参与者能力差距越大，抗欺骗性越强，而LLM-as-a-Judge在面对5-20倍大小的欺骗模型时表现差于随机猜测

Conclusion: 同伴预测方法为LLM评估和后训练提供了无需强监督的可靠解决方案，特别适用于评估强大模型，在能力差距大的情况下表现优异

Abstract: The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.

</details>


### [172] [Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches](https://arxiv.org/abs/2601.20307)
*Xinyu Li,Sishuo Chen,Guipeng Xv,Li Zhang,Mingxuan Luo,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Chen Lin*

Main category: cs.LG

TL;DR: 提出TRACE基准和READER模型，解决GMV预测中的延迟反馈问题，相比传统CVR预测更具挑战性


<details>
  <summary>Details</summary>
Motivation: 在线广告排序模型从转化率预测转向GMV预测，但GMV预测中的延迟反馈问题尚未研究且更具挑战性，因为GMV是连续目标且单次点击可能产生多次购买

Method: 建立TRACE基准支持在线流式延迟反馈建模，提出READER模型：基于复购预测的路由器选择性激活专家参数，动态校准回归目标缓解标签不完整导致的低估

Result: 在TRACE基准上READER相比基线性能提升2.19%，验证了GMV标签分布快速演变需要在线流式训练，复购样本与单购样本分布差异需要分别建模

Conclusion: 该研究为GMV预测中的在线延迟反馈建模开辟了新方向，TRACE基准和洞察将促进该领域未来研究和应用

Abstract: The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .

</details>


### [173] [Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching](https://arxiv.org/abs/2601.20332)
*Fengrui Zuo,Zhiwei Ke,Yiming Liu,Wenqi Lou,Chao Wang,Xvehai Zhou*

Main category: cs.LG

TL;DR: 提出Window-Diffusion方法，通过窗口化令牌剪枝和缓存技术，显著减少扩散语言模型推理时的冗余计算，实现高达99倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLMs)在推理时需要全序列注意力计算，导致大量冗余计算。现有块级扩散方法通常需要重新训练且更新顺序受限，难以直接应用于预训练DLMs。研究发现DLM推理具有明显的结构局部性特征。

Method: 提出Window-Diffusion方法：1) 维护随去噪过程向右滑动的局部计算窗口；2) 将未解码令牌分为：在线计算的活跃令牌、KV状态缓存并定期刷新的缓冲令牌、窗口外被剪枝的远场令牌；3) 计算仅限制在窗口内的活跃和缓冲令牌。

Result: 在LLaDA和Dream模型上的实验表明，在相同计算预算下，该方法实现了高达99倍的推理加速，同时基本保持了生成性能。

Conclusion: Window-Diffusion通过利用DLM推理的结构局部性，实现了高效的令牌级剪枝和缓存策略，为扩散语言模型的推理加速提供了有效解决方案。

Abstract: Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at https://github.com/vhicrgit/Window-Diffusion.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.

</details>


### [174] [TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs](https://arxiv.org/abs/2601.20357)
*Minjae Lee,Wonjun Kang,Byeongkeun Ahn,Christian Classen,Kevin Galim,Seunghyuk Oh,Minghao Yan,Hyung Il Koo,Kangwook Lee*

Main category: cs.LG

TL;DR: TABED是一种针对大型视觉语言模型的推测解码方法，通过动态集成多个草稿来加速推理，平均获得1.74倍加速


<details>
  <summary>Details</summary>
Motivation: 推测解码在LLM加速中有效，但在大型视觉语言模型中尚未充分探索，现有方法在不同场景下性能波动较大

Method: 提出测试时自适应批量集成草稿生成方法，利用推测解码设置中可用的历史真实值偏差，动态集成批量推理获得的多个草稿

Result: 平均获得1.74倍实时加速，比单一草稿方法提升5%，保持训练免费且集成成本可忽略，具有即插即用兼容性

Conclusion: TABED为大型视觉语言模型提供了一种有效的推测解码加速方案，通过动态集成机制在不同场景下保持稳定性能

Abstract: Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74x over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods. Code and custom-trained models are available at https://github.com/furiosa-ai/TABED.

</details>


### [175] [TINNs: Time-Induced Neural Networks for Solving Time-Dependent PDEs](https://arxiv.org/abs/2601.20361)
*Chen-Yang Dai,Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai*

Main category: cs.LG

TL;DR: TINNs通过将神经网络权重参数化为时间的函数，解决了传统PINNs在时空问题中权重共享导致的精度和训练稳定性问题，实现了4倍精度提升和10倍收敛加速。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理时间依赖PDE时，将时间作为输入但共享所有时间的网络权重，导致相同的特征需要表示显著不同的动态特性，这会降低精度并在联合强制PDE、边界和初始条件时使训练不稳定。

Method: 提出时间诱导神经网络(TINNs)，将网络权重参数化为时间的函数，允许有效的空间表示随时间演化，同时保持共享结构。该公式自然产生非线性最小二乘问题，使用Levenberg-Marquardt方法高效优化。

Result: 在各种时间依赖PDE上的实验显示，相比PINNs和强基线方法，TINNs实现了高达4倍的精度提升和10倍的收敛加速。

Conclusion: TINNs通过将网络权重参数化为时间的函数，解决了传统PINNs在时空问题中的局限性，显著提升了精度和训练效率，为时间依赖PDE的神经网络求解提供了更有效的架构。

Abstract: Physics-informed neural networks (PINNs) solve time-dependent partial differential equations (PDEs) by learning a mesh-free, differentiable solution that can be evaluated anywhere in space and time. However, standard space--time PINNs take time as an input but reuse a single network with shared weights across all times, forcing the same features to represent markedly different dynamics. This coupling degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints jointly. We propose Time-Induced Neural Networks (TINNs), a novel architecture that parameterizes the network weights as a learned function of time, allowing the effective spatial representation to evolve over time while maintaining shared structure. The resulting formulation naturally yields a nonlinear least-squares problem, which we optimize efficiently using a Levenberg--Marquardt method. Experiments on various time-dependent PDEs show up to $4\times$ improved accuracy and $10\times$ faster convergence compared to PINNs and strong baselines.

</details>


### [176] [Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku](https://arxiv.org/abs/2601.20363)
*Mariia Drozdova*

Main category: cs.LG

TL;DR: 连续时间生成模型能否表示支撑集为极度稀疏、全局约束离散集合的分布？研究以数独网格为测试平台，发现随机采样优于确定性流，基于分数的采样器最可靠，DDPM式祖先采样有效性最高。


<details>
  <summary>Details</summary>
Motivation: 探索标准连续时间生成模型能否表示支撑集为极度稀疏、全局约束离散集合的分布，使用数独网格作为受控测试平台来研究这一基本问题。

Method: 使用数独网格作为连续松弛空间的子集，训练流匹配和基于分数的模型，比较确定性ODE采样、随机SDE采样以及从相同连续时间训练导出的DDPM式离散化方法。

Result: 无条件生成中，随机采样显著优于确定性流；基于分数的采样器在连续时间方法中最可靠；DDPM式祖先采样总体有效性最高。模型还可重新用于引导生成，通过重复采样完成固定线索下的数独，作为概率求解器。

Conclusion: 经典扩散/流公式能够为全局约束的组合结构分配非零概率质量，并可通过随机搜索用于约束满足，尽管在样本效率上不如经典求解器和离散几何感知的扩散方法。

Abstract: Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.

</details>


### [177] [LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning](https://arxiv.org/abs/2601.20375)
*Wei Huang,Anda Cheng,Yinggui Wang,Lei Wang,Tao Wei*

Main category: cs.LG

TL;DR: LLM-AutoDP：利用LLM作为代理自动生成和优化数据处理策略的框架，无需人工干预或访问原始数据，通过迭代学习和加速技术实现高效处理。


<details>
  <summary>Details</summary>
Motivation: 领域特定数据中常包含低质量样本，传统手动数据处理策略开发成本高且涉及隐私风险（如医疗领域），需要自动化且不暴露原始数据的解决方案。

Method: 提出LLM-AutoDP框架，利用LLM代理自动生成多个候选策略，通过反馈信号和比较评估迭代优化。引入三种加速技术：分布保持采样、处理目标选择（二元分类器识别低质量样本）、缓存重用机制。

Result: 使用该框架处理的数据训练的模型，相比未处理数据训练的模型胜率超过80%；相比基于LLM代理的AutoML基线，胜率约65%；加速技术将总搜索时间减少高达10倍。

Conclusion: LLM-AutoDP有效解决了自动化数据处理中的隐私和效率问题，实现了无需人工干预的高质量数据处理策略生成，在效果和效率方面均表现优异。

Abstract: Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.

</details>


### [178] [FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance](https://arxiv.org/abs/2601.20397)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Mingjin Zhang*

Main category: cs.LG

TL;DR: FedRD是一种异构感知的联邦学习算法，通过参数引导的全局泛化聚合和局部去偏分类来解决联邦域泛化中的优化分歧和性能分歧问题，旨在为参与和未见客户端获得最优全局模型。


<details>
  <summary>Details</summary>
Motivation: 异构联邦学习需要确保不同实体间的有效协作和隐私保护。新加入的客户端需要进行大量调整和额外训练以适应现有系统，因此在异构数据下将联邦学习模型泛化到未见客户端的问题变得日益重要。论文强调了联邦域泛化中两个未解决的挑战性问题：优化分歧和性能分歧。

Method: 提出FedRD算法，采用异构感知的联邦学习方法，协同利用参数引导的全局泛化聚合和局部去偏分类来减少分歧。该方法旨在通过这两种机制的结合，为参与和未见客户端获得最优的全局模型。

Result: 在公共多域数据集上的大量实验表明，该方法在解决这一特定问题上相比竞争基线表现出显著的性能优势。

Conclusion: FedRD通过解决联邦域泛化中的优化分歧和性能分歧问题，能够有效处理异构联邦学习场景，为参与和未见客户端提供更好的模型泛化能力。

Abstract: Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [179] [ScatterFusion: A Hierarchical Scattering Transform Framework for Enhanced Time Series Forecasting](https://arxiv.org/abs/2601.20401)
*Wei Li*

Main category: cs.LG

TL;DR: ScatterFusion：一种结合散射变换和分层注意力机制的时间序列预测框架，通过多尺度特征提取和自适应增强，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测面临多时间尺度复杂依赖关系的挑战，需要能够同时捕捉局部和全局模式的方法。

Method: 包含四个关键组件：1) 分层散射变换模块提取多尺度不变特征；2) 尺度自适应特征增强模块动态调整不同尺度特征重要性；3) 多分辨率时间注意力机制学习不同时间跨度依赖关系；4) 趋势-季节-残差分解引导的结构感知损失函数。

Result: 在七个基准数据集上的广泛实验表明，ScatterFusion优于其他常见方法，在不同预测时间跨度上显著降低了误差指标。

Conclusion: ScatterFusion通过协同整合散射变换和分层注意力机制，为时间序列预测提供了一个鲁棒且有效的框架。

Abstract: Time series forecasting presents significant challenges due to the complex temporal dependencies at multiple time scales. This paper introduces ScatterFusion, a novel framework that synergistically integrates scattering transforms with hierarchical attention mechanisms for robust time series forecasting. Our approach comprises four key components: (1) a Hierarchical Scattering Transform Module (HSTM) that extracts multi-scale invariant features capturing both local and global patterns; (2) a Scale-Adaptive Feature Enhancement (SAFE) module that dynamically adjusts feature importance across different scales; (3) a Multi-Resolution Temporal Attention (MRTA) mechanism that learns dependencies at varying time horizons; and (4) a Trend-Seasonal-Residual (TSR) decomposition-guided structure-aware loss function. Extensive experiments on seven benchmark datasets demonstrate that ScatterFusion outperforms other common methods, achieving significant reductions in error metrics across various prediction horizons.

</details>


### [180] [AWGformer: Adaptive Wavelet-Guided Transformer for Multi-Resolution Time Series Forecasting](https://arxiv.org/abs/2601.20409)
*Wei Li*

Main category: cs.LG

TL;DR: AWGformer：一种结合自适应小波分解与跨尺度注意力机制的多变量时间序列预测新架构，在基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要捕捉多个时间尺度的模式，同时保持计算效率。现有方法在处理多尺度和非平稳时间序列时存在局限性，需要更有效的架构来捕获跨尺度的频率特征。

Method: 提出AWGformer架构，包含四个核心组件：1）自适应小波分解模块（AWDM）动态选择最优小波基和分解层数；2）跨尺度特征融合（CSFF）通过可学习耦合矩阵捕获不同频带间的交互；3）频率感知多头注意力（FAMA）根据频率选择性加权注意力头；4）分层预测网络（HPN）在重构前生成多分辨率预测。

Result: 在基准数据集上的广泛实验表明，AWGformer相比最先进方法取得了显著的平均改进，特别是在多尺度和非平稳时间序列上表现优异。理论分析提供了收敛保证，并建立了小波引导注意力与经典信号处理原理之间的联系。

Conclusion: AWGformer通过将自适应小波分解与跨尺度注意力机制相结合，为多变量时间序列预测提供了一种有效且理论完备的解决方案，在处理复杂时间模式方面具有优势。

Abstract: Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. This paper introduces AWGformer, a novel architecture that integrates adaptive wavelet decomposition with cross-scale attention mechanisms for enhanced multi-variate time series prediction. Our approach comprises: (1) an Adaptive Wavelet Decomposition Module (AWDM) that dynamically selects optimal wavelet bases and decomposition levels based on signal characteristics; (2) a Cross-Scale Feature Fusion (CSFF) mechanism that captures interactions between different frequency bands through learnable coupling matrices; (3) a Frequency-Aware Multi-Head Attention (FAMA) module that weights attention heads according to their frequency selectivity; (4) a Hierarchical Prediction Network (HPN) that generates forecasts at multiple resolutions before reconstruction. Extensive experiments on benchmark datasets demonstrate that AWGformer achieves significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series. Theoretical analysis provides convergence guarantees and establishes the connection between our wavelet-guided attention and classical signal processing principles.

</details>


### [181] [Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs](https://arxiv.org/abs/2601.20420)
*Yuhang Liu,Erdun Gao,Dong Gong,Anton van den Hengel,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: 提出Concept Component Analysis (ConCA)框架，通过线性解混从LLM表示中恢复概念的对数后验，相比稀疏自编码器(SAEs)有理论优势


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器(SAEs)方法缺乏理论基础，概念与LLM表示之间的对应关系不明确，导致方法设计和评估困难

Method: 将LLM表示建模为概念对数后验的线性混合，提出ConCA框架进行无监督线性解混，特别探索稀疏ConCA变体以解决解混问题的病态性

Result: 实现了12种稀疏ConCA变体，在多个LLM上成功提取有意义的概念，相比SAEs具有理论支持的优势

Conclusion: ConCA为概念提取提供了理论基础的框架，解决了SAEs的理论模糊性问题，为LLM的可解释性研究提供了新方向

Abstract: Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.

</details>


### [182] [Nonlinear Dimensionality Reduction with Diffusion Maps in Practice](https://arxiv.org/abs/2601.20428)
*Sönke Beier,Paula Pirker-Díaz,Friedrich Pagenkopf,Karoline Wiesner*

Main category: cs.LG

TL;DR: 扩散映射是一种非线性降维技术，但数据预处理、参数设置和分量选择对结果影响显著，本文提供实践指南并展示识别最相关分量的新方法


<details>
  <summary>Details</summary>
Motivation: 扩散映射在多个科学领域广泛应用，但文献中缺乏对数据预处理、参数设置和分量选择对结果影响的全面讨论，这些因素对最终流形结构有显著影响

Method: 提供扩散映射技术的实践导向综述，说明常见陷阱，并展示最近引入的识别最相关分量的技术方法

Result: 研究结果表明，前几个分量不一定是最相关的分量，需要专门的技术来识别真正重要的分量

Conclusion: 扩散映射应用需要谨慎处理数据预处理、参数设置和分量选择，新方法能帮助识别最相关分量，避免依赖前几个分量可能导致的误导性结果

Abstract: Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.

</details>


### [183] [TimeCatcher: A Variational Framework for Volatility-Aware Forecasting of Non-Stationary Time Series](https://arxiv.org/abs/2601.20448)
*Zhiyu Chen,Minhao Liu,Yanru Zhang*

Main category: cs.LG

TL;DR: TimeCatcher：一种新颖的波动感知变分预测框架，通过变分编码器捕获历史数据中的潜在动态模式，并利用波动感知增强机制检测和放大显著局部变化，特别适用于高波动性和突发波动的长期时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有的轻量级MLP模型在时间序列预测中表现良好，但依赖于局部平稳性假设，在高度非平稳序列的长期预测中容易出错，特别是在网络流量监控等领域出现突发波动时。需要克服这一局限性。

Method: TimeCatcher扩展了线性架构，包含：1）变分编码器捕获历史数据中的潜在动态模式；2）波动感知增强机制检测和放大显著局部变化。该框架专门处理高波动性和突发波动的长期预测问题。

Result: 在交通、金融、能源和天气等九个真实世界数据集上的实验表明，TimeCatcher始终优于最先进的基线方法，在具有高波动性和突发波动的长期预测场景中改进尤为显著。

Conclusion: TimeCatcher通过结合变分编码和波动感知增强，有效解决了现有MLP模型在高度非平稳时间序列长期预测中的局限性，特别是在处理突发波动方面表现出色。

Abstract: Recent lightweight MLP-based models have achieved strong performance in time series forecasting by capturing stable trends and seasonal patterns. However, their effectiveness hinges on an implicit assumption of local stationarity assumption, making them prone to errors in long-term forecasting of highly non-stationary series, especially when abrupt fluctuations occur, a common challenge in domains like web traffic monitoring. To overcome this limitation, we propose TimeCatcher, a novel Volatility-Aware Variational Forecasting framework. TimeCatcher extends linear architectures with a variational encoder to capture latent dynamic patterns hidden in historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations. Experiments on nine real-world datasets from traffic, financial, energy, and weather domains show that TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations. Our code is available at https://github.com/ColaPrinceCHEN/TimeCatcher.

</details>


### [184] [Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations](https://arxiv.org/abs/2601.20449)
*Fatima Ezzeddine,Obaida Ammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 提出基于强化学习的模型无关方法，生成满足个体公平、群体公平及混合公平约束的反事实解释


<details>
  <summary>Details</summary>
Motivation: 反事实解释在XAI中至关重要，但需要确保相似个体和不同受保护群体获得相似且可操作的补救措施，以实现可信和公平的决策

Method: 将问题形式化为优化任务，提出基于强化学习的模型无关方法，扩展现有公平性指标（如平等补救选择和平等有效性），在三个基准数据集上评估

Result: 方法能有效确保个体和群体公平性，同时保持生成反事实解释在接近性和合理性方面的质量，并量化不同层次公平性的成本

Conclusion: 开启了关于混合公平性及其在XAI和超越反事实解释中的角色和影响的更广泛讨论

Abstract: Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.

</details>


### [185] [Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations](https://arxiv.org/abs/2601.20477)
*Kadircan Aksoy,Peter Jung,Protim Bhattacharjee*

Main category: cs.LG

TL;DR: 论文通过二元假设检验视角研究神经网络分类器的监督训练动态，发现泛化能力好的网络会逐渐对齐Neyman-Pearson最优决策规则，KL散度单调改善并与错误率指数相关。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络分类器在监督训练过程中的动态特性，从二元假设检验的角度理解分类决策如何形成和优化。

Method: 将分类建模为类别条件表示分布之间的二元测试，通过经验分析训练轨迹，研究网络决策规则与Neyman-Pearson最优决策的对齐过程。

Result: 泛化能力好的网络在训练过程中会单调改善KL散度，逐渐与Neyman-Pearson最优决策规则对齐，这种对齐与错误率指数相关。

Conclusion: 这一发现为不同类别神经网络提供了理论解释，并可能启发新的训练或正则化策略。

Abstract: We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.

</details>


### [186] [An explainable framework for the relationship between dementia and glucose metabolism patterns](https://arxiv.org/abs/2601.20480)
*C. Vázquez-García,F. J. Martínez-Murcia,F. Segovia Román,A. Forte,J. Ramírez,I. Illán,A. Hernández-Segura,C. Jiménez-Mesa,Juan M. Górriz*

Main category: cs.LG

TL;DR: 提出一个半监督变分自编码器框架，通过相似性正则化将潜在变量与痴呆进展的临床或生物标志物对齐，用于分析阿尔茨海默病神经影像数据。


<details>
  <summary>Details</summary>
Motivation: 高维神经影像数据存在复杂的非线性关系，难以评估神经退行性疾病。需要一种能够提取疾病相关特征并与临床指标对齐的灵活方法。

Method: 提出半监督VAE框架，包含灵活的相似性正则化项，将选定的潜在变量与痴呆进展的临床或生物标志物对齐。使用ADNI的PET扫描数据，引导第一个潜在维度与认知评分对齐。

Result: 监督潜在变量能够生成不同认知障碍水平的平均重建图像。体素级GLM分析显示海马体等关键区域代谢降低，默认模式网络和中央执行网络等主要静息态网络受影响。其余潜在变量编码仿射变换和强度变化，捕获个体差异和站点效应等混杂因素。

Conclusion: 该框架有效提取与阿尔茨海默病生物标志物对齐的疾病相关模式，为研究神经退行性进展提供了可解释且适应性强的工具。

Abstract: High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.

</details>


### [187] [CCMamba: Selective State-Space Models for Higher-Order Graph Learning on Combinatorial Complexes](https://arxiv.org/abs/2601.20518)
*Jiawen Chen,Qi Shao,Mingtong Zhou,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: CCMamba：首个基于Mamba的统一神经框架，用于组合复形上的学习，通过选择性状态空间建模实现线性时间信息传播，无需自注意力机制。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑深度学习方法主要依赖局部消息传递和注意力机制，存在二次复杂度问题，且局限于低维表示，难以在更高阶组合复形中实现可扩展性和秩感知信息聚合。

Method: 将消息传递重新表述为选择性状态空间建模问题，将多秩关联关系组织成结构化序列，通过秩感知状态空间模型处理，实现自适应、定向和长距离信息传播。

Result: 在图、超图和单纯复形基准测试中，CCMamba始终优于现有方法，同时展现出更好的可扩展性和深度鲁棒性。

Conclusion: CCMamba为组合复形学习提供了首个统一的Mamba框架，通过状态空间建模实现了线性时间复杂度的信息传播，在表达能力和计算效率方面取得了显著改进。

Abstract: Topological deep learning has emerged for modeling higher-order relational structures beyond pairwise interactions that standard graph neural networks fail to capture. Although combinatorial complexes offer a unified topological framework, most existing topological deep learning methods rely on local message passing via attention mechanisms, which incur quadratic complexity and remain low-dimensional, limiting scalability and rank-aware information aggregation in higher-order complexes.We propose Combinatorial Complex Mamba (CCMamba), the first unified mamba-based neural framework for learning on combinatorial complexes. CCMamba reformulates message passing as a selective state-space modeling problem by organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models. This enables adaptive, directional, and long range information propagation in linear time without self attention. We further establish the theoretical analysis that the expressive power upper-bound of CCMamba message passing is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate that CCMamba consistently outperforms existing methods while exhibiting improved scalability and robustness to depth.

</details>


### [188] [Unsupervised Ensemble Learning Through Deep Energy-based Models](https://arxiv.org/abs/2601.20556)
*Ariel Maymon,Yanir Buznah,Uri Shaham*

Main category: cs.LG

TL;DR: 提出一种基于深度能量的无监督集成学习方法，仅使用个体学习器的预测构建元学习器，无需标签数据或额外信息，在条件独立假设下有理论保证，在多种集成场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决无监督集成学习的挑战：在没有真实标签或额外数据的情况下，如何结合多个学习器的预测。这在评估个体分类器性能困难或信息有限的场景中尤为重要。

Method: 提出基于深度能量的方法，仅使用个体学习器的预测构建元学习器，能够捕捉学习器之间的复杂依赖结构。该方法不需要标签数据、学习器特征或问题特定信息。

Result: 在多种集成场景中表现出优越性能，包括具有挑战性的专家混合设置。在标准集成数据集和专门设计的测试数据集上均取得良好结果。

Conclusion: 无监督集成学习在数据稀缺或隐私敏感环境中具有巨大潜力，能够有效利用集体智能，为实际应用提供了一种无需标签的集成解决方案。

Abstract: Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.

</details>


### [189] [Reinforcement Unlearning via Group Relative Policy Optimization](https://arxiv.org/abs/2601.20568)
*Efstratios Zaradoukas,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: PURGE是一种基于Group Relative Policy Optimization框架的LLM遗忘方法，通过内在奖励信号惩罚禁止概念的提及，实现可验证的遗忘，在效率、流畅性和鲁棒性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在预训练过程中会无意记忆敏感或受版权保护的数据，这违反了GDPR和欧盟AI法案等法律框架。现有遗忘方法存在数据泄露、牺牲流畅性和鲁棒性、依赖昂贵外部奖励模型等问题。

Method: PURGE基于Group Relative Policy Optimization框架，将遗忘问题形式化为可验证任务。使用内在奖励信号惩罚任何提及禁止概念的行为，实现安全一致的遗忘。

Result: 相比SotA方法，PURGE将每个目标的token使用减少高达46倍，流畅性提高5.48%，对抗鲁棒性提高12.02%。在RWKU基准测试中，实现11%的遗忘效果，同时保留98%的原始效用。

Conclusion: 将LLM遗忘问题框架化为可验证任务，能够实现更可靠、高效和可扩展的遗忘，为结合理论保证、改进安全性和实际部署效率的遗忘研究提供了新方向。

Abstract: During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach reduces token usage per target by up to a factor of 46 compared with SotA methods, while improving fluency by 5.48 percent and adversarial robustness by 12.02 percent over the base model. On the Real World Knowledge Unlearning (RWKU) benchmark, PURGE achieves 11 percent unlearning effectiveness while preserving 98 percent of original utility. PURGE shows that framing LLM unlearning as a verifiable task, enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.

</details>


### [190] [Ranking-aware Reinforcement Learning for Ordinal Ranking](https://arxiv.org/abs/2601.20585)
*Aiming Hao,Chen Zhu,Jiashu Zhu,Jiahong Wu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: 提出RARL框架，通过强化学习统一回归和排序任务，使用排序感知奖励和响应突变操作提升性能


<details>
  <summary>Details</summary>
Motivation: 传统方法难以建模序数回归和排序中的序数依赖关系，需要新的框架来显式学习这些关系

Method: 提出RARL框架，包含：1）统一目标函数协同整合回归和L2R任务；2）排序感知可验证奖励联合评估回归精度和排序准确性；3）响应突变操作注入可控噪声提升探索

Result: 在三个不同的基准测试上进行广泛实验验证了RARL的有效性

Conclusion: RARL通过强化学习框架成功解决了序数回归和排序中的挑战，实现了回归和排序任务的协同提升

Abstract: Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.

</details>


### [191] [Regularized Gradient Temporal-Difference Learning](https://arxiv.org/abs/2601.20599)
*Hyunjun Na,Donghwan Lee*

Main category: cs.LG

TL;DR: 提出正则化GTD算法（R-GTD），解决传统GTD在特征交互矩阵奇异时的不稳定问题，保证收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有GTD算法在特征交互矩阵奇异时会出现不稳定或性能下降，需要一种能处理奇异情况的稳定算法。

Method: 通过重新表述均方投影贝尔曼误差最小化问题，提出正则化优化目标，推导出R-GTD算法。

Result: R-GTD即使在特征交互矩阵奇异时也能保证收敛到唯一解，理论分析和实验验证了其有效性。

Conclusion: 提出的正则化GTD算法解决了传统方法的局限性，为奇异特征交互矩阵情况下的稳定策略评估提供了有效方案。

Abstract: Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.

</details>


### [192] [CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks](https://arxiv.org/abs/2601.20605)
*Junaid Sajid,Ivo Müürsepp,Luca Reggiani,Davide Scazzoli,Federico Francesco Luigi Mariani,Maurizio Magarini,Rizwan Ahmad,Muhammad Mahtab Alam*

Main category: cs.LG

TL;DR: 提出CoBA深度学习模型，结合CNN、BiLSTM和注意力机制，利用5G毫米波无线电测量数据，对低空无人机在授权和限制空域的操作进行分类，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用和工业应用中的普及，确保低空操作安全变得至关重要。在密集毫米波环境中，准确分类低空无人机是否在授权或限制空域具有挑战性，需要能够处理复杂传播和信号变化的模型。

Method: 提出CoBA深度学习模型，集成卷积神经网络（CNN）、双向长短期记忆网络（BiLSTM）和注意力机制，从5G毫米波无线电测量中提取空间和时间模式。使用TalTech的5G毫米波网络收集专门数据集，包含授权和限制场景下的受控低空无人机飞行数据。

Result: 实验结果表明，CoBA模型实现了卓越的准确性，显著优于所有基准模型（包括传统机器学习模型和基于指纹识别的基准方法），证明了其在可靠和规范的无人机空域监控方面的潜力。

Conclusion: CoBA模型通过集成CNN、BiLSTM和注意力机制，有效利用5G毫米波无线电测量数据，为低空无人机在授权和限制空域的分类提供了可靠解决方案，在无人机空域监控方面具有实际应用价值。

Abstract: Uncrewed Aerial Vehicles (UAVs) are increasingly used in civilian and industrial applications, making secure low-altitude operations crucial. In dense mmWave environments, accurately classifying low-altitude UAVs as either inside authorized or restricted airspaces remains challenging, requiring models that handle complex propagation and signal variability. This paper proposes a deep learning model, referred to as CoBA, which stands for integrated Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention which leverages Fifth Generation (5G) millimeter-wave (mmWave) radio measurements to classify UAV operations in authorized and restricted airspaces at low altitude. The proposed CoBA model integrates convolutional, bidirectional recurrent, and attention layers to capture both spatial and temporal patterns in UAV radio measurements. To validate the model, a dedicated dataset is collected using the 5G mmWave network at TalTech, with controlled low altitude UAV flights in authorized and restricted scenarios. The model is evaluated against conventional ML models and a fingerprinting-based benchmark. Experimental results show that CoBA achieves superior accuracy, significantly outperforming all baseline models and demonstrating its potential for reliable and regulated UAV airspace monitoring.

</details>


### [193] [WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport](https://arxiv.org/abs/2601.20606)
*Xinyu Wang,Ruoyu Wang,Qiangwei Peng,Peijie Zhou,Tiejun Li*

Main category: cs.LG

TL;DR: 提出WFR-MFM方法，通过平均流框架实现单细胞动态重建，比现有方法推理速度快几个数量级，无需轨迹模拟。


<details>
  <summary>Details</summary>
Motivation: 单细胞生物学中从有限观测重建动态演化是一个基本挑战，现有方法依赖推理时的轨迹模拟，导致推理成为可扩展应用的关键瓶颈。

Method: 提出平均流框架，使用平均速度和质量增长场总结任意时间间隔内的传输和质量增长动态，实现无需轨迹模拟的一步生成。基于此框架开发Wasserstein-Fisher-Rao平均流匹配（WFR-MFM），在Wasserstein-Fisher-Rao几何下解决动态不平衡最优传输问题。

Result: 在合成和真实单细胞RNA测序数据集上，WFR-MFM比现有基线方法推理速度快几个数量级，同时保持高预测准确性，能够在数千个条件的大型合成数据集上高效预测扰动响应。

Conclusion: WFR-MFM通过平均流框架解决了动态不平衡最优传输的推理瓶颈，实现了快速、准确的大规模单细胞动态重建和扰动预测。

Abstract: Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.

</details>


### [194] [ACFormer: Mitigating Non-linearity with Auto Convolutional Encoder for Time Series Forecasting](https://arxiv.org/abs/2601.20611)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: ACFormer结合线性投影效率和卷积非线性特征提取能力的时间序列预测模型，通过系统感受野分析发现卷积层具有通道注意力特性，提出共享压缩模块、门控注意力和独立补丁扩展层架构


<details>
  <summary>Details</summary>
Motivation: 时间序列预测面临复杂的时间依赖性和通道间相关性建模挑战，现有线性架构虽然高效但难以处理非线性信号，需要结合卷积的非线性特征提取能力

Method: 1. 系统分析CNN模型的感受野，引入"个体感受野"概念；2. 提出ACFormer架构：共享压缩模块捕获细粒度信息，门控注意力保持时间局部性，独立补丁扩展层重建变量特定时间模式

Result: 在多个基准数据集上的实验表明，ACFormer始终达到最先进的性能，有效缓解了线性模型在捕获高频分量方面的固有缺陷

Conclusion: ACFormer成功调和了线性投影的效率与卷积的非线性特征提取能力，为时间序列预测提供了一种更有效的架构，能够更好地处理复杂的时间依赖性和非线性信号

Abstract: Time series forecasting (TSF) faces challenges in modeling complex intra-channel temporal dependencies and inter-channel correlations. Although recent research has highlighted the efficiency of linear architectures in capturing global trends, these models often struggle with non-linear signals. To address this gap, we conducted a systematic receptive field analysis of convolutional neural network (CNN) TSF models. We introduce the "individual receptive field" to uncover granular structural dependencies, revealing that convolutional layers act as feature extractors that mirror channel-wise attention while exhibiting superior robustness to non-linear fluctuations. Based on these insights, we propose ACFormer, an architecture designed to reconcile the efficiency of linear projections with the non-linear feature-extraction power of convolutions. ACFormer captures fine-grained information through a shared compression module, preserves temporal locality via gated attention, and reconstructs variable-specific temporal patterns using an independent patch expansion layer. Extensive experiments on multiple benchmark datasets demonstrate that ACFormer consistently achieves state-of-the-art performance, effectively mitigating the inherent drawbacks of linear models in capturing high-frequency components.

</details>


### [195] [DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration](https://arxiv.org/abs/2601.20627)
*Gilles Eerlings,Brent Zoomers,Jori Liesenborgs,Gustavo Rovelo Ruiz,Kris Luyten*

Main category: cs.LG

TL;DR: DIVERSE是一个探索深度神经网络Rashomon集的框架，通过FiLM层和CMA-ES搜索生成预测行为不同但精度相当的模型变体，无需重新训练或梯度访问。


<details>
  <summary>Details</summary>
Motivation: 传统上寻找Rashomon集（精度相当但预测行为不同的模型集合）需要大量重新训练，计算成本高昂。本文旨在开发一种更高效的方法来系统探索深度神经网络的Rashomon集。

Method: 在预训练模型基础上添加FiLM层，使用CMA-ES在潜在调制空间中进行搜索，生成多样化的模型变体，无需重新训练或访问梯度。

Result: 在MNIST、PneumoniaMNIST和CIFAR-10数据集上，DIVERSE成功发现了多个高性能但功能不同的模型，能够以较低计算成本实现与重新训练相当的多样性。

Conclusion: DIVERSE提供了一种竞争性且高效的Rashomon集探索方法，能够在保持鲁棒性和性能的同时支持均衡的模型多样性，为构建多样化模型集合提供了可行方案。

Abstract: We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.

</details>


### [196] [A Foundation Model for Virtual Sensors](https://arxiv.org/abs/2601.20634)
*Leon Götz,Lars Frederik Peiss,Erik Sauer,Andreas Udo Sass,Thorsten Bagdonat,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了首个虚拟传感器基础模型，能同时预测多种虚拟传感器，计算效率高，可扩展性强，在大型评估中显著降低计算时间和内存需求。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟传感器方法需要针对每个传感器构建特定模型、手动选择输入，无法利用任务协同效应，且缺乏统一基准。同时，现有的时间序列基础模型计算成本高且只能预测输入信号，不适用于虚拟传感器。

Method: 引入首个虚拟传感器基础模型，采用统一架构同时预测多种虚拟传感器，自动学习每个虚拟传感器的相关输入信号，无需专家知识，同时保持计算效率。

Result: 在标准基准和包含180亿样本的应用特定数据集上的大规模评估显示：计算时间减少415倍，内存需求减少951倍，预测质量保持甚至优于基线。模型可优雅扩展到数百个虚拟传感器，参数数量几乎恒定。

Conclusion: 该模型解决了现有虚拟传感器方法的局限性，为大规模传感器网络中的实际部署提供了可行的解决方案，实现了计算效率、可扩展性和预测质量的平衡。

Abstract: Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.

</details>


### [197] [An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems](https://arxiv.org/abs/2601.20637)
*Panayiotis Ioannou,Pietro Liò,Pietro Cicuta*

Main category: cs.LG

TL;DR: 该研究探索了使用神经常微分方程（NODEs）和符号回归（SR）从噪声数据中建模复杂系统动力学并发现其控制方程的能力，展示了NODEs在数据有限情况下的外推潜力以及SR恢复物理定律的可能性。


<details>
  <summary>Details</summary>
Motivation: 准确建模复杂系统的动力学并发现其控制微分方程对于加速科学发现至关重要。研究旨在探索如何从噪声数据中恢复物理定律，特别是在数据有限的情况下。

Method: 使用两个阻尼振荡系统的噪声合成数据，研究神经常微分方程（NODEs）的外推能力和符号回归（SR）恢复底层方程的能力。具体包括：1）测试NODEs对新边界条件的外推能力；2）评估SR从噪声真实数据中恢复方程的效果；3）使用仅10%完整模拟数据训练的NODE生成数据，然后应用SR恢复方程。

Result: 1）NODEs能够有效外推到新的边界条件，前提是生成的轨迹与训练数据具有动态相似性；2）SR能够成功从噪声真实数据中恢复方程，但其性能取决于输入变量的正确选择；3）当使用仅10%完整模拟数据训练的NODE生成数据时，SR恢复了三个控制方程中的两个，并对第三个方程获得了良好近似。

Conclusion: 使用NODEs丰富有限数据并让符号回归推断物理定律代表了一种有前景的科学发现新方法。虽然最后一个发现指出了未来工作的方向，但结果表明这种方法在数据有限情况下具有潜力。

Abstract: Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.

</details>


### [198] [Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability](https://arxiv.org/abs/2601.20642)
*Rohan Asthana,Vasileios Belagiannis*

Main category: cs.LG

TL;DR: 提出新的扩散模型记忆化检测方法，结合各向同性范数和各向异性对齐指标，无需去噪步骤即可在纯噪声输入上检测记忆化样本


<details>
  <summary>Details</summary>
Motivation: 现有基于分数差范数的记忆化检测方法主要在各向同性对数概率分布假设下有效，但在低噪声的各向异性区域效果有限。需要开发更全面的检测方法

Method: 分析各向异性区域发现记忆化样本在低噪声设置下具有指导向量和无条件分数之间的强角度对齐。结合各向同性范数和各向异性对齐开发新的检测指标，可直接在纯噪声输入上通过两次前向传播计算

Result: 在Stable Diffusion v1.4和v2上的检测实验表明，新指标优于现有无需去噪的检测方法，且速度比之前最佳方法快约5倍。基于该指标的缓解策略能有效调整记忆化提示词

Conclusion: 提出的记忆化检测方法通过结合各向同性和各向异性特征，实现了高效准确的记忆化检测，为扩散模型的安全应用提供了实用工具

Abstract: Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.

</details>


### [199] [MuRAL-CPD: Active Learning for Multiresolution Change Point Detection](https://arxiv.org/abs/2601.20686)
*Stefano Bertolasi,Diego Carrera,Diego Stucchi,Pasqualina Fragneto,Luigi Amedeo Bianchi*

Main category: cs.LG

TL;DR: MuRAL-CPD是一种新颖的半监督变化点检测方法，通过小波多分辨率分解和多轮主动学习整合用户反馈，优化超参数以适应用户对变化的定义。


<details>
  <summary>Details</summary>
Motivation: 传统变化点检测方法多为无监督技术，缺乏对任务特定变化定义的适应性，且无法利用用户知识。需要一种能够整合用户反馈、提高准确性和可解释性的方法。

Method: 提出MuRAL-CPD方法：1）基于小波的多分辨率分解，在多个时间尺度上检测变化；2）整合主动学习机制，通过用户反馈迭代优化关键超参数；3）使模型的变化概念与用户对齐。

Result: 在多个真实世界数据集上的实验结果表明，MuRAL-CPD优于现有最先进方法，特别是在仅有少量监督可用的场景中表现出色。

Conclusion: MuRAL-CPD通过整合多分辨率分析和主动学习，成功解决了传统变化点检测方法的局限性，提高了检测准确性和可解释性，特别是在有限监督条件下表现优异。

Abstract: Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.

</details>


### [200] [Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models](https://arxiv.org/abs/2601.20687)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Xiaobo Xia,Ming-Kun Xie,Dong-Dong Wu,Biao Liu,Yuheng Jia,Xin Geng,Masashi Sugiyama,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出PU RL蒸馏方法，用于本地小模型部署，无需人工标注偏好或奖励模型，通过教师模型生成锚点响应，本地采样学生候选，进行锚点条件自排序来诱导偏好信号


<details>
  <summary>Details</summary>
Motivation: 本地部署小模型日益普遍，但大多数实际流水线停留在监督微调阶段，无法达到强化学习对齐阶段，因为RL对齐需要昂贵的人工偏好标注或依赖高质量奖励模型的大规模API使用和持续工程维护，这些都不适合本地设置

Method: 提出正样本-未标记样本RL蒸馏方法：对每个提示查询教师模型一次获得锚点响应，本地采样多个学生候选，进行锚点条件自排序来诱导成对或列表偏好，通过直接偏好优化或组相对策略优化实现完全本地训练循环

Result: 理论分析证明该方法诱导的偏好信号具有顺序一致性并集中在近最优候选上，支持偏好优化的稳定性。实验表明在低成本设置下该方法能实现持续强劲性能

Conclusion: 该方法填补了本地小模型部署中RL对齐的空白，无需人工偏好标注或奖励模型，通过教师模型蒸馏实现完全本地训练，为实际部署提供可行解决方案

Abstract: Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

</details>


### [201] [Optimal Transport Group Counterfactual Explanations](https://arxiv.org/abs/2601.20692)
*Enrique Valero-Leal,Bernd Bischl,Pedro Larrañaga,Concha Bielza,Giuseppe Casalicchio*

Main category: cs.LG

TL;DR: 提出一种学习显式最优传输映射的方法，为任意群体实例生成反事实解释，无需重新优化，最小化群体传输成本并保持几何结构


<details>
  <summary>Details</summary>
Motivation: 现有群体反事实解释方法存在三个主要问题：(1) 仅针对固定群体优化，无法泛化到新成员；(2) 严格依赖强模型假设（如线性）以保证可处理性；(3) 对反事实群体几何变形的控制能力差

Method: 学习一个显式的最优传输映射，将任何群体实例发送到其反事实位置，无需重新优化，最小化群体的总传输成本。对于线性分类器，通过数学优化推导出群体反事实函数，识别底层凸优化类型（QP、QCQP等）

Result: 实验表明，该方法能够准确泛化，保持群体几何结构，与基线方法相比仅产生可忽略的额外传输成本。即使无法利用模型线性性，该方法也显著优于基线

Conclusion: 通过学习显式最优传输映射，实现了群体反事实解释的泛化能力，减少了参数数量，便于解释共同的可行动路径，同时保持群体几何结构并最小化传输成本

Abstract: Group counterfactual explanations find a set of counterfactual instances to explain a group of input instances contrastively. However, existing methods either (i) optimize counterfactuals only for a fixed group and do not generalize to new group members, (ii) strictly rely on strong model assumptions (e.g., linearity) for tractability or/and (iii) poorly control the counterfactual group geometry distortion. We instead learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization, minimizing the group's total transport cost. This enables generalization with fewer parameters, making it easier to interpret the common actionable recourse. For linear classifiers, we prove that functions representing group counterfactuals are derived via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, ...). Experiments show that they accurately generalize, preserve group geometry and incur only negligible additional transport cost compared to baseline methods. If model linearity cannot be exploited, our approach also significantly outperforms the baselines.

</details>


### [202] [Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?](https://arxiv.org/abs/2601.20694)
*Hao Liang,Jiayu Cheng,Sean R. Sinclair,Yali Du*

Main category: cs.LG

TL;DR: 本文证明了在Exo-MDPs（外生MDPs）中，纯利用算法无需探索即可获得理论保证，推翻了传统认为需要探索的认知。


<details>
  <summary>Details</summary>
Motivation: 外生MDPs广泛存在于库存控制、能源存储等运筹学应用中，其中不确定性完全来自外生输入。尽管经验表明贪婪的纯利用方法在这些场景中表现优异，但现有理论仍依赖显式探索或表格假设，缺乏纯利用算法的理论保证。

Method: 提出了纯利用学习（PEL）算法，在表格情况下实现理论保证；针对大规模连续内生状态空间，提出了LSVI-PE线性近似方法。分析引入了两个新工具：反事实轨迹和Bellman封闭特征传输。

Result: 在表格情况下，PEL实现了$\widetilde{O}(H^2|Ξ|\sqrt{K})$的遗憾界；LSVI-PE的遗憾界与特征维度、外生状态空间和时域多项式相关，而与内生状态和动作空间无关。实验表明PEL在合成和资源管理任务中优于基线方法。

Conclusion: 本文推翻了"需要探索"的传统认知，证明了在外生MDPs中，纯利用就足够了，为运筹学中的许多实际应用提供了理论支持。

Abstract: Exogenous MDPs (Exo-MDPs) capture sequential decision-making where uncertainty comes solely from exogenous inputs that evolve independently of the learner's actions. This structure is especially common in operations research applications such as inventory control, energy storage, and resource allocation, where exogenous randomness (e.g., demand, arrivals, or prices) drives system behavior. Despite decades of empirical evidence that greedy, exploitation-only methods work remarkably well in these settings, theory has lagged behind: all existing regret guarantees for Exo-MDPs rely on explicit exploration or tabular assumptions. We show that exploration is unnecessary. We propose Pure Exploitation Learning (PEL) and prove the first general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. In the tabular case, PEL achieves $\widetilde{O}(H^2|Ξ|\sqrt{K})$. For large, continuous endogenous state spaces, we introduce LSVI-PE, a simple linear-approximation method whose regret is polynomial in the feature dimension, exogenous state space, and horizon, independent of the endogenous state and action spaces. Our analysis introduces two new tools: counterfactual trajectories and Bellman-closed feature transport, which together allow greedy policies to have accurate value estimates without optimism. Experiments on synthetic and resource-management tasks show that PEL consistently outperforming baselines. Overall, our results overturn the conventional wisdom that exploration is required, demonstrating that in Exo-MDPs, pure exploitation is enough.

</details>


### [203] [Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning](https://arxiv.org/abs/2601.20829)
*Minwu Kim,Safal Shrestha,Keith Ross*

Main category: cs.LG

TL;DR: 本文提出failure-prefix conditioning方法，通过在训练中引入罕见错误推理轨迹的前缀，解决RLVR训练在饱和问题上的停滞问题，有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习与可验证奖励（RLVR）虽然提升了LLMs的推理能力，但在问题饱和时训练会停滞。核心挑战在于信息性失败的可访问性差：学习信号存在但标准rollout中很少遇到。

Method: 提出failure-prefix conditioning方法：不是从原始问题开始训练，而是重新分配探索，将训练条件设置在从罕见错误推理轨迹中提取的前缀上，让模型暴露于易失败状态。

Result: 该方法在性能提升上相当于中等难度问题的训练效果，同时保持token效率。分析显示该方法减少了在误导性失败前缀下的性能下降，但在遵循正确早期推理方面有轻微权衡。迭代方法（在训练中刷新失败前缀）能在性能平台期后实现额外提升。

Conclusion: failure-prefix conditioning为在饱和问题上扩展RLVR训练提供了有效途径，通过暴露模型于失败易发状态来克服训练停滞问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.

</details>


### [204] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: MORPHIN：一种自适应Q学习框架，能在非平稳环境中实时适应奖励函数变化和动作空间扩展，无需完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体在现实应用中面临环境非平稳性的挑战，特别是当奖励函数变化或动作空间扩展时，传统方法需要完全重新训练，效率低下。

Method: MORPHIN框架整合概念漂移检测机制，动态调整学习和探索超参数，能够适应奖励函数变化和动作空间实时扩展，同时保留先验策略知识以避免灾难性遗忘。

Result: 在Gridworld基准测试和交通信号控制模拟中，MORPHIN相比标准Q学习基线表现出更优的收敛速度和持续适应能力，学习效率提升最高达1.7倍。

Conclusion: MORPHIN框架为强化学习在动态变化环境中的应用提供了有效的自适应解决方案，能够在保持先验知识的同时实现实时适应。

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [205] [Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs](https://arxiv.org/abs/2601.20704)
*Melika Mobini,Vincent Holst,Floriano Tori,Andres Algaba,Vincent Ginis*

Main category: cs.LG

TL;DR: LLM生成的参考文献列表在结构上与人类相似，但语义特征可被检测，准确率达93%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于生成参考文献，需要研究其生成的参考文献列表是否与人类生成的有所区别

Method: 构建配对引用图（真实vs GPT-4o生成），使用结构特征和3072维标题/摘要嵌入，采用随机森林和图神经网络进行分析

Result: 仅结构特征难以区分GPT与真实引用（准确率约0.60），但语义嵌入显著提高区分度（GNN达93%准确率），结果在多个模型上稳健

Conclusion: LLM生成的参考文献在拓扑结构上接近人类，但留下可检测的语义指纹；检测和去偏应关注内容信号而非全局图结构

Abstract: Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\approx$ 0.60) despite cleanly rejecting the random baseline ($\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\approx$ 0.83, and GNNs with embedding node features achieve 93\% test accuracy on GPT vs.\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\ Claude $\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.

</details>


### [206] [Evolutionary Strategies lead to Catastrophic Forgetting in LLMs](https://arxiv.org/abs/2601.20861)
*Immanuel Abdi,Akshat Gupta,Micah Mok,Alexander Lu,Nicholas Lee,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: 进化策略(ES)在LLM训练中能达到接近GRPO的性能，但存在严重的灾难性遗忘问题，限制了其在持续学习中的应用


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏部署后的持续学习能力，梯度算法内存需求大，进化策略作为无梯度替代方案在特定任务上表现良好，但需要全面评估其在持续学习中的表现

Method: 对进化策略进行综合分析，评估其在增加更新步数时的遗忘曲线，与GRPO算法进行比较，分析更新稀疏性和ℓ₂范数差异

Result: ES在数学和推理任务上能达到接近GRPO的性能，但伴随显著的先前能力遗忘；ES更新比GRPO更新稀疏性差，ℓ₂范数大几个数量级，解释了两种算法遗忘曲线的差异

Conclusion: 进化策略在持续学习中存在严重的遗忘问题，需要未来研究解决无梯度算法的遗忘问题，以实现有效的在线训练

Abstract: One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.

</details>


### [207] [HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs](https://arxiv.org/abs/2601.20745)
*Guoan Wang,Feiyu Wang,Zongwei Lv,Yikun Zong,Tong Yang*

Main category: cs.LG

TL;DR: Hestia：一种用于极低位LLM的Hessian引导可微分量化感知训练框架，通过温度控制softmax松弛和Hessian迹指导的精细温度退火，在1.58位量化下显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，部署受限于内存瓶颈，需要极低位量化。但现有量化感知训练方法使用硬舍入和直通估计器，过早离散化优化空间，导致潜在权重与量化权重之间的梯度失配，阻碍量化模型的有效优化。

Method: 提出Hestia框架：1）用温度控制softmax松弛替代刚性阶跃函数，保持训练早期梯度流动，逐步硬化量化；2）利用张量级Hessian迹作为轻量曲率信号，驱动细粒度温度退火，实现跨模型的敏感感知离散化。

Result: 在Llama-3.2上评估，Hestia持续优于现有三元QAT基线，1B和3B模型平均零样本改进分别为5.39%和4.34%。Hessian引导松弛有效恢复表示能力，为1.58位LLM建立更稳健的训练路径。

Conclusion: Hestia通过Hessian引导的可微分量化框架，解决了极低位量化中的梯度失配问题，显著提升了量化模型性能，为内存受限环境下的LLM部署提供了有效解决方案。

Abstract: As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.

</details>


### [208] [Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis](https://arxiv.org/abs/2601.20729)
*Anchen Sun,Zhibin Chen,Xiaodong Cai*

Main category: cs.LG

TL;DR: 提出Cox-MT模型，结合Mean Teacher框架的半监督学习方法，利用标记和未标记数据提升ANN-based Cox模型在癌症预后预测中的性能


<details>
  <summary>Details</summary>
Motivation: 传统基于人工神经网络的Cox比例风险模型需要大量标记样本，但实际应用中标记数据有限，限制了模型性能。需要开发能够利用未标记数据的半监督学习方法来解决数据稀缺问题。

Method: 基于Mean Teacher框架的深度半监督学习方法，开发单模态和多模态的ANN-based Cox模型（Cox-MT）。单模态使用TCGA RNA-seq数据或全切片图像，多模态结合多种数据源。模型同时利用标记和未标记数据进行训练。

Result: 在TCGA数据集上，单模态Cox-MT模型（使用RNA-seq或图像数据）在四种癌症类型上显著优于现有的Cox-nnet模型。随着未标记样本增加，模型性能显著提升。多模态Cox-MT模型比单模态模型表现更好。

Conclusion: Cox-MT模型通过半监督学习有效利用标记和未标记数据，显著提高了基于ANN的Cox模型在癌症预后预测中的准确性，为解决标记数据稀缺问题提供了有效方案。

Abstract: The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.

</details>


### [209] [Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces](https://arxiv.org/abs/2601.20800)
*Kaito Baba,Yoshihiko Ozaki,Shuhei Watanabe*

Main category: cs.LG

TL;DR: 提出condPED-ANOVA框架，用于估计条件搜索空间中超参数的重要性，解决了传统方法无法处理条件超参数的问题。


<details>
  <summary>Details</summary>
Motivation: 原始PED-ANOVA虽然能快速估计超参数重要性，但假设搜索空间是固定的、无条件的，无法正确处理条件超参数（即超参数的存在或取值范围依赖于其他超参数的情况）。

Method: 提出条件超参数重要性（conditional HPI）概念，并推导出闭式估计器，能够准确反映条件激活和域变化。该方法专门针对条件搜索空间设计。

Result: 实验表明，现有HPI估计器的简单适配在条件设置下会产生误导性或难以解释的重要性估计，而condPED-ANOVA始终能提供反映底层条件结构的有意义的重要性估计。

Conclusion: condPED-ANOVA为条件搜索空间中的超参数重要性估计提供了一个原则性框架，能够正确处理条件依赖关系，提供准确且可解释的重要性评估。

Abstract: We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.

</details>


### [210] [Continual GUI Agents](https://arxiv.org/abs/2601.20732)
*Ziwei Liu,Borui Kang,Hangjie Yuan,Zixiang Zhao,Wei Li,Yifan Zhu,Tao Feng*

Main category: cs.LG

TL;DR: 提出了GUI-AiF框架，通过锚点奖励和锚区域奖励来稳定GUI智能体在数据分布变化下的持续学习性能


<details>
  <summary>Details</summary>
Motivation: 数字环境（数据分布）不断变化，新的GUI数据随时间到达（引入新域或分辨率），在静态环境中训练的智能体性能会下降。现有方法在GUI分布随时间变化时无法保持稳定的基础定位能力。

Method: 引入GUI-Anchoring in Flux (GUI-AiF)框架，这是一种新的强化微调框架，通过两种新颖的奖励来稳定持续学习：Anchoring Point Reward in Flux (APR-iF) 和 Anchoring Region Reward in Flux (ARR-iF)。这些奖励指导智能体与变化的交互点和区域对齐，缓解现有奖励策略过度适应静态基础线索（如固定坐标或元素尺度）的倾向。

Result: 大量实验表明GUI-AiF超越了最先进的基线方法。

Conclusion: 这项工作建立了首个GUI智能体的持续学习框架，揭示了强化微调在持续GUI智能体中的未开发潜力。

Abstract: As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.

</details>


### [211] [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)
*Jonas Hübotter,Frederike Lübeck,Lejs Behric,Anton Baumann,Marco Bagatella,Daniel Marta,Ido Hakimi,Idan Shenfeld,Thomas Kleine Buening,Carlos Guestrin,Andreas Krause*

Main category: cs.LG

TL;DR: SDPO是一种强化学习方法，利用文本反馈进行自我蒸馏，解决可验证领域中的信用分配问题，提高样本效率和最终准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习方法只使用标量结果奖励，存在严重的信用分配瓶颈。许多可验证环境实际上提供了丰富的文本反馈（如运行时错误、评委评价），可以解释失败原因，但现有方法未能充分利用这些信息。

Method: 提出自我蒸馏策略优化（SDPO），将标记化的反馈转换为密集学习信号，无需外部教师或显式奖励模型。SDPO将当前模型在反馈条件下的输出作为自我教师，将其反馈感知的下一个标记预测蒸馏回策略中，利用模型在上下文中回顾性识别自身错误的能力。

Result: 在科学推理、工具使用和LiveCodeBench v6的竞争性编程任务中，SDPO相比强RLVR基线提高了样本效率和最终准确率。即使在只返回标量反馈的标准RLVR环境中，SDPO通过使用成功rollout作为失败尝试的隐式反馈，也优于基线。在测试时对单个问题应用SDPO，在困难的二元奖励任务中加速了发现过程，以3倍更少的尝试实现了与最佳k采样或多轮对话相同的发现概率。

Conclusion: SDPO通过利用丰富的文本反馈进行自我蒸馏，有效解决了可验证领域强化学习中的信用分配问题，显著提升了学习效率和性能，为利用环境反馈信息提供了新的有效方法。

Abstract: Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

</details>


### [212] [GNN Explanations that do not Explain and How to find Them](https://arxiv.org/abs/2601.20815)
*Steve Azzolin,Stefano Teso,Bruno Lepri,Andrea Passerini,Sagar Malhotra*

Main category: cs.LG

TL;DR: SE-GNNs的解释可能存在与模型推理无关的退化解释，现有忠实度指标难以检测，作者提出新指标能可靠识别


<details>
  <summary>Details</summary>
Motivation: 自解释图神经网络（SE-GNNs）的解释对于理解模型内部机制和检测敏感属性滥用至关重要。现有研究表明这些解释可能不理想且有误导性，但缺乏对其失败案例的系统性分析。

Method: 识别SE-GNN解释的关键失败模式：解释可能与模型推理标签的方式完全无关。通过实证分析展示退化解释既可通过恶意植入产生，也可自然出现。提出新的忠实度指标来可靠检测这些退化解释。

Result: 研究发现许多SE-GNNs在达到最优真实风险的同时仍会产生退化解释，且大多数现有忠实度指标无法识别这些失败模式。新提出的忠实度指标在恶意和自然设置下都能可靠地将退化解释标记为不忠实。

Conclusion: SE-GNNs的解释存在严重缺陷，可能隐藏敏感属性的使用，需要可靠的审计机制。提出的新忠实度指标为解决这一问题提供了有效工具。

Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.

</details>


### [213] [GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning](https://arxiv.org/abs/2601.20753)
*Zhiheng Jiang,Yunzhe Wang,Ryan Marr,Ellen Novoseller,Benjamin T. Files,Volkan Ustun*

Main category: cs.LG

TL;DR: 提出了GraphAllocBench基准测试，用于评估多目标强化学习中的偏好条件策略学习，基于城市管理启发的图资源分配环境，包含新评估指标PNDS和OS。


<details>
  <summary>Details</summary>
Motivation: 现有PCPL基准测试主要局限于玩具任务和固定环境，缺乏现实性和可扩展性，限制了多目标强化学习在实际复杂问题中的应用。

Method: 1. 构建GraphAllocBench基准，基于CityPlannerEnv图资源分配沙盒环境；2. 提出两个新评估指标：非支配解比例(PNDS)和排序分数(OS)；3. 使用MLP和图感知模型进行实验验证。

Result: GraphAllocBench能够暴露现有MORL方法的局限性，展示了图神经网络在复杂高维组合分配任务中的潜力，同时验证了新评估指标的有效性。

Conclusion: GraphAllocBench为PCPL研究提供了灵活、可扩展的基准测试，支持目标、偏好和分配规则的灵活变化，推动了图方法在复杂组合分配任务中的应用。

Abstract: Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench

</details>


### [214] [Supervised Guidance Training for Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2601.20756)
*Elizabeth L. Baker,Alexander Denker,Jes Frellsen*

Main category: cs.LG

TL;DR: 本文提出了首个函数空间扩散模型的后验采样方法，通过无限维Doob's h-transform理论框架，实现了在贝叶斯逆问题中对扩散模型的条件化采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已被扩展到无限维函数空间用于偏微分方程逆问题，但在贝叶斯逆问题中，如何条件化扩散模型以从后验分布采样仍缺乏理论支持。现有方法无法有效处理函数空间中的后验采样问题。

Method: 基于Cameron-Martin空间或高斯测度绝对连续性的假设，使用无限维Doob's h-transform理论框架，将条件分数分解为无条件分数和引导项。提出无监督引导训练（Supervised Guidance Training）的模拟自由分数匹配目标，用于高效稳定的后验采样。

Result: 建立了扩散模型在函数空间中条件化的理论框架，证明了条件分数可分解为无条件分数和引导项。提出的Supervised Guidance Training方法实现了高效的后验采样，并在函数空间贝叶斯逆问题的数值实验中验证了有效性。

Conclusion: 本文首次提供了函数空间中扩散模型条件化的理论框架和实用方法，实现了对训练好的扩散模型进行微调以准确从后验分布采样的功能，为函数空间贝叶斯逆问题提供了有效的解决方案。

Abstract: Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.

</details>


### [215] [$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval](https://arxiv.org/abs/2601.20844)
*Zihao Wang,Hang Yin,Lihui Liu,Hanghang Tong,Yangqiu Song,Ginny Wong,Simon See*

Main category: cs.LG

TL;DR: 研究最小可嵌入维度(MED)问题，针对m个元素和其k元素子集，在不同距离度量下推导MED的紧界，并通过数值模拟验证对数依赖关系，表明基于嵌入的检索限制主要来自可学习性而非几何约束。


<details>
  <summary>Details</summary>
Motivation: 研究基于嵌入的检索系统中，将元素及其子集嵌入到向量空间所需的最小维度，探索几何约束对检索性能的根本限制，区分几何约束与可学习性挑战。

Method: 理论推导MED的紧界，涵盖ℓ₂度量、内积和余弦相似度；数值模拟更实际场景，其中子集嵌入选择为包含元素的嵌入质心。

Result: 得到MED在不同距离度量下的理论紧界；数值模拟显示MED与元素数量呈对数依赖关系，易于实现。

Conclusion: 嵌入检索的限制主要源于可学习性挑战而非几何约束，这为未来算法设计提供了指导方向。

Abstract: This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of "distances" or "similarities," including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.

</details>


### [216] [Less is More: Clustered Cross-Covariance Control for Offline RL](https://arxiv.org/abs/2601.20765)
*Nan Qiao,Sheng Yue,Shuning Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 该论文提出C^4方法，通过分区缓冲采样和梯度修正惩罚来解决离线强化学习中的分布偏移问题，特别针对OOD区域，提高稳定性并获得30%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中存在分布偏移的根本挑战，特别是在数据稀缺或数据集被OOD区域主导的情况下。标准平方误差目标会引入有害的TD交叉协方差，在OOD区域放大这种效应，导致优化偏差和策略学习退化。

Method: 提出两种互补策略：1) 分区缓冲采样，将更新限制在局部回放分区中，减弱不规则协方差效应并对齐更新方向，形成易于集成的C^4方案；2) 显式梯度修正惩罚，在每个更新中消除协方差引起的偏差。

Result: 理论证明缓冲分区保持了最大化目标的下界性质，这些约束在不改变策略约束离线强化学习核心行为的情况下减轻了极端OOD区域的过度保守性。实验显示方法具有更高的稳定性，在先前方法基础上获得高达30%的回报提升，特别是在小数据集和强调OOD区域的数据划分上。

Conclusion: 提出的C^4方法通过控制TD交叉协方差效应，有效解决了离线强化学习中的分布偏移问题，特别是在OOD区域，为现有实现提供了易于集成的解决方案，显著提升了学习稳定性和性能。

Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.

</details>


### [217] [COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI](https://arxiv.org/abs/2601.20772)
*Shakhyar Gogoi*

Main category: cs.LG

TL;DR: COMET-SG1是一个轻量级、稳定性导向的自回归回归模型，专为边缘和嵌入式AI系统的时间序列预测设计，通过线性行为空间编码、记忆锚定转换估计和确定性状态更新实现稳定长时预测。


<details>
  <summary>Details</summary>
Motivation: 在边缘和嵌入式AI系统中，时间序列预测需要模型具有稳定性，因为预测误差会随时间累积。传统RNN或Transformer模型在长时预测中容易出现漂移问题，不适合资源受限的边缘部署环境。

Method: 采用线性行为空间编码、记忆锚定转换估计和确定性状态更新机制。模型结构优先考虑完全自回归推理下的有界长时行为，参数紧凑且支持定点算术运算。

Result: 在非平稳合成时间序列数据上的实验表明，COMET-SG1在短时预测精度上具有竞争力，同时在长时预测漂移方面显著优于MLP、LSTM和k近邻基线模型。

Conclusion: COMET-SG1为边缘和嵌入式AI应用提供了一个实用且可解释的稳定自回归预测方法，具有紧凑参数占用和定点算术兼容性，适合资源受限的部署环境。

Abstract: COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.

</details>


### [218] [Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation](https://arxiv.org/abs/2601.20854)
*Aníbal Silva,Moisés Santos,André Restivo,Carlos Soares*

Main category: cs.LG

TL;DR: 研究探讨了在VAE中集成Transformer架构对表格数据生成的影响，发现不同位置的Transformer会带来保真度与多样性的权衡，且Transformer块间存在高度相似性。


<details>
  <summary>Details</summary>
Motivation: 表格数据对生成模型具有挑战性，传统基于多层感知机的VAE难以建模特征间关系，而Transformer的注意力机制更适合捕捉复杂特征交互，因此研究在VAE中集成Transformer的效果。

Method: 在VAE的不同组件（编码器、解码器、潜在空间）中集成Transformer架构，在OpenML CC18套件的57个数据集上进行实验，分析不同配置对生成质量的影响。

Result: 1. 在潜在空间和解码器中使用Transformer会导致保真度与多样性的权衡；2. 所有组件中连续Transformer块之间表现出高度相似性，特别是在解码器中输入输出关系近似线性。

Conclusion: Transformer在VAE中的集成位置对表格数据生成质量有重要影响，需要权衡保真度与多样性，且Transformer架构在表格数据建模中存在冗余性。

Abstract: Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.

</details>


### [219] [Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying](https://arxiv.org/abs/2601.20773)
*Rubén Jiménez,Oriol Pujol*

Main category: cs.LG

TL;DR: 提出基于距离的黑盒模型复制框架，用符号距离替代硬标签监督，将不连续的分类边界重构转化为平滑回归问题，提高复制效率和精度


<details>
  <summary>Details</summary>
Motivation: 实际部署的机器学习系统需要持续演进，但往往无法访问原始训练数据或模型内部结构。在黑盒设置下，仅通过输入输出查询复制模型时，硬标签输出会导致不连续的边界重构问题，限制了边界几何的有效恢复

Method: 提出基于距离的蒸馏框架，用符号距离到教师模型决策边界的距离替代硬标签监督；开发α控制的平滑和正则化方案，对诱导的目标表面进行Hölder/Lipschitz控制；提出两种模型无关算法在仅标签访问下估计符号距离

Result: 在合成问题和UCI基准测试中，相比硬标签基线，在保真度和泛化精度方面取得一致改进，同时使距离输出可作为黑盒副本的不确定性相关信号

Conclusion: 基于距离的复制框架有效解决了硬标签监督下的不连续边界重构问题，通过平滑回归利用局部几何信息，显著提升了黑盒模型复制的效率和准确性

Abstract: Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $α$-governed smoothing and regularization scheme with Hölder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas.

</details>


### [220] [When More Data Doesn't Help: Limits of Adaptation in Multitask Learning](https://arxiv.org/abs/2601.20774)
*Steve Hanneke,Mingyue Xu*

Main category: cs.LG

TL;DR: 该论文研究了多任务学习的统计极限，证明了即使每个任务有大量样本，也无法保证最优风险，这比之前的工作提出了更强的不可行性结果。


<details>
  <summary>Details</summary>
Motivation: 多任务学习框架在应用中取得了巨大成功，但之前的研究表明，在没有分布信息的情况下，仅基于样本聚合的算法无法在有限样本下保证最优风险。本文旨在超越这个"没有免费午餐"定理，探索多任务学习的统计极限。

Method: 通过建立更强的不可行性结果来研究多任务学习的统计极限。该方法超越了arXiv:2006.15785中的"没有免费午餐"定理，证明了即使每个任务有任意大的样本量，适应性问题仍然存在。

Result: 证明了多任务学习的硬度不能通过每个任务拥有充足数据来克服。即使每个任务的样本量任意大，也无法保证最优风险，这比之前的结果更强。还讨论了未来可能有意义的"最优适应性"概念。

Conclusion: 多任务学习的根本困难不仅限于有限样本情况，即使有大量数据也无法保证最优性能。这一发现对多任务学习理论有重要启示，并提出了"最优适应性"作为未来研究方向。

Abstract: Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.
  In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.

</details>


### [221] [Active Learning for Decision Trees with Provable Guarantees](https://arxiv.org/abs/2601.20775)
*Arshia Soltani Moakhar,Tanapoom Laoaron,Faraz Ghahremani,Kiarash Banihashem,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: 本文首次分析了决策树作为二元分类器的主动学习标签复杂度，提出了在特定假设下实现对数多项式标签复杂度的算法，并建立了接近最优的误差容忍度下界。


<details>
  <summary>Details</summary>
Motivation: 决策树作为重要的二元分类器，其主动学习标签复杂度的理论分析尚不完善。现有研究缺乏对决策树分歧系数的分析，也缺少能够保证乘法误差的通用主动学习算法。

Method: 1. 首次分析决策树的分歧系数；2. 提出第一个实现乘法误差保证的通用主动学习算法；3. 结合上述结果设计针对决策树的主动学习算法；4. 建立标签复杂度下界。

Result: 1. 在特定假设下（根到叶路径查询不同特征维度、输入数据具有规则网格结构）实现了对数多项式标签复杂度；2. 证明了这些假设的必要性；3. 算法在数据集大小上仅需对数多项式数量的标签查询；4. 误差容忍度ε的依赖关系接近最优。

Conclusion: 本文为决策树的主动学习标签复杂度提供了首个理论分析框架，在特定假设下实现了高效的标签查询复杂度，并建立了接近最优的理论下界，为决策树主动学习的理论研究和实际应用奠定了基础。

Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+ε)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $ε$ is close to optimal.

</details>


### [222] [PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting](https://arxiv.org/abs/2601.20845)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: PatchFormer：基于补丁的时间序列基础模型，通过分层掩码重建进行自监督预训练，使用轻量级适配器实现高效迁移，在24个基准数据集上实现最先进的零样本多步预测。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法通常需要领域特定的特征工程和大量标注数据，这限制了模型的泛化能力和应用范围。需要一种能够跨领域通用、减少数据依赖的基础模型。

Method: 1. 将时间序列分割成补丁；2. 使用分层掩码重建进行自监督预训练，包括动态掩码和鼓励局部精度与全局一致性的目标；3. 跨领域知识蒸馏；4. 使用轻量级适配器进行高效迁移；5. 学习跨时间尺度的可聚合多尺度时间表示。

Result: 在24个涵盖天气、能源、交通、金融和医疗的基准数据集上，相比强基线平均平方误差降低27.3%，所需任务特定训练数据减少94%。模型在1000亿数据点上呈现近似对数线性扩展，处理512长度序列比全序列Transformer快3.8倍。

Conclusion: PatchFormer展示了时间序列基础模型的可行性，通过自监督预训练和高效适配器实现了跨领域的强大零样本预测能力，显著减少了数据需求并提高了计算效率。

Abstract: Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.

</details>


### [223] [C3Box: A CLIP-based Class-Incremental Learning Toolbox](https://arxiv.org/abs/2601.20852)
*Hao Sun,Da-Wei Zhou*

Main category: cs.LG

TL;DR: C3Box是一个基于CLIP的类增量学习工具箱，整合了传统、ViT和CLIP-based CIL方法，提供统一框架和标准化实验流程，促进公平比较和可复现性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的类增量学习方法分散在不同代码库中，配置不一致，难以进行公平比较、复现和实际应用，需要一个统一的工具箱来解决这些问题。

Method: 开发C3Box工具箱，将代表性传统CIL方法、ViT-based CIL方法和最先进的CLIP-based CIL方法整合到统一的CLIP框架中，采用JSON配置和标准化执行流程，继承PyCIL的简洁设计。

Result: C3Box提供了一个模块化、全面的Python工具箱，支持主流操作系统，仅依赖广泛使用的开源库，代码已在GitHub上开源，可作为可靠的持续学习基准平台。

Conclusion: C3Box解决了CLIP-based CIL方法分散和配置不一致的问题，提供了低工程开销的可复现实验环境，是用户友好的类增量学习研究工具。

Abstract: Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [224] [United in Currency, Divided in Growth: Dynamic Effects of Euro Adoption](https://arxiv.org/abs/2601.20169)
*Harry Aytug*

Main category: econ.EM

TL;DR: 欧元采用平均降低年GDP增长率0.3-0.4个百分点，效果在采用后不久出现并持续约十年，但存在显著异质性：初始人均GDP较低的国家受影响更大。


<details>
  <summary>Details</summary>
Motivation: 现有关于欧元采用对长期经济增长影响的证据相互矛盾，主要原因是处理国家有限、时间跨度长难以推断，以及成员国间的异质性。需要更精确的方法来估计因果动态效应和异质性效应。

Method: 使用Causal Forests with Fixed Effects (CFFE)方法，这是一种结合因果森林和双向固定效应的机器学习方法，在条件平行趋势假设下估计动态和异质性处理效应。同时使用两国新凯恩斯DSGE模型进行理论验证。

Result: 欧元采用平均降低年GDP增长率0.3-0.4个百分点，效果在采用后不久出现并稳定约十年。异质性显著：初始人均GDP较低的国家经历更大、更持久的增长缺口。消费和生产率增长减弱是主要原因，净出口改善部分抵消了这些下降。

Conclusion: 欧元采用对经济增长有负面影响，但影响程度因国家特征而异。一刀切的货币政策在货币联盟下比在灵活汇率制度下产生更大的产出损失。评估货币联盟长期后果时，必须考虑国家特征的重要性。

Abstract: Does euro adoption affect long-run economic growth? Existing evidence is mixed, reflecting limited treated countries, long horizons that challenge inference, and heterogeneity across member states. We estimate causal dynamic and heterogeneous treatment effects using Causal Forests with Fixed Effects (CFFE), a machine-learning approach that combines causal forests with two-way fixed effects. Under a conditional parallel-trends assumption, we find that euro adoption reduced annual GDP growth by 0.3-0.4 percentage points on average. Effects emerge shortly after adoption and stabilize after roughly a decade.
  Average effects mask substantial heterogeneity. Countries with lower initial GDP per capita experience larger and more persistent growth shortfalls than core economies. Weaker consumption and productivity growth contribute to the overall effect, while improvements in net exports partially offset these declines.
  A two-country New Keynesian DSGE model with hysteresis generates qualitatively similar patterns: one-size-fits-all monetary policy and scarring mechanisms produce larger output losses under monetary union than under flexible exchange rates. By jointly estimating dynamic and heterogeneous treatment effects, the analysis highlights the importance of country characteristics in assessing the long-run consequences of monetary union.

</details>


### [225] [Realized range-based estimation of integrated variance](https://arxiv.org/abs/2601.20463)
*Kim Christensen,Mark Podolskij*

Main category: econ.EM

TL;DR: 论文提出了一种基于价格范围(range-based)的二次变差估计方法，相比传统的已实现方差(realized variance)，在完整样本路径下具有5倍更高的精度，并解决了离散观测导致的向下偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统已实现方差(realized variance)使用平方收益来估计连续半鞅的二次变差，但这种方法存在效率限制。论文旨在开发一种更高效的估计方法，利用价格范围信息来改进二次变差的估计精度。

Method: 提出已实现范围方差(realized range-based variance)统计量，将已实现方差中的每个平方收益替换为归一化的平方价格范围。当只有离散数据时，通过解决向下偏差问题，获得一致且混合正态的估计量，不受非交易效应影响。

Result: 在完整样本路径下，该统计量是一致的且具有混合高斯极限，精度比已实现方差高5倍。即使在离散观测下，通过偏差修正也能获得一致且混合正态的估计量。应用于TAQ数据的实证分析表明，该方法能更好地估计二次变差的经验路径。

Conclusion: 基于价格范围的方差估计方法相比传统已实现方差具有显著优势，特别是在精度方面。该方法为连续半鞅二次变差的估计提供了更高效的工具，在实证金融应用中表现出更好的性能。

Abstract: We provide a set of probabilistic laws for estimating the quadratic variation of continuous semimartingales with realized range-based variance -- a statistic that replaces every squared return of realized variance with a normalized squared range. If the entire sample path of the process is available, and under a set of weak conditions, our statistic is consistent and has a mixed Gaussian limit, whose precision is five times greater than that of realized variance. In practice, of course, inference is drawn from discrete data and true ranges are unobserved, leading to downward bias. We solve this problem to get a consistent, mixed normal estimator, irrespective of non-trading effects. This estimator has varying degrees of efficiency over realized variance, depending on how many observations that are used to construct the high-low. The methodology is applied to TAQ data and compared with realized variance. Our findings suggest that the empirical path of quadratic variation is also estimated better with the realized range-based variance.

</details>


### [226] [The realized empirical distribution function of stochastic variance with application to goodness-of-fit testing](https://arxiv.org/abs/2601.20469)
*Kim Christensen,Martin Thyrsgaard,Bezirgen Veliyev*

Main category: econ.EM

TL;DR: 提出一种从高频数据估计潜在波动率经验分布函数的非参数方法，用于检验随机波动率模型的拟合优度


<details>
  <summary>Details</summary>
Motivation: 需要从噪声高频数据中估计金融资产潜在波动率的分布，以检验随机波动率模型的拟合优度

Method: 提出实现经验分布函数(REDF)的非参数估计器，在固定时间跨度下随着观测网格细化而一致收敛，在双渐近框架下收敛于波动率的累积分布函数

Result: 蒙特卡洛研究表明REDF在整个波动率支撑上都很准确，拟合优度检验具有正确的尺寸和相对较高的功效；实证应用中逆高斯分布对股票市场随机变异的描述最好，但仍不完全

Conclusion: 提出的REDF方法有效，实证表明标准随机波动率模型需要额外参数（如广义逆高斯分布）来更好地建模随机方差

Abstract: We propose a nonparametric estimator of the empirical distribution function (EDF) of the latent spot variance of the log-price of a financial asset. We show that over a fixed time span our realized EDF (or REDF) -- inferred from noisy high-frequency data -- is consistent as the mesh of the observation grid goes to zero. In a double-asymptotic framework, with time also increasing to infinity, the REDF converges to the cumulative distribution function of volatility, if it exists. We exploit these results to construct some new goodness-of-fit tests for stochastic volatility models. In a Monte Carlo study, the REDF is found to be accurate over the entire support of volatility. This leads to goodness-of-fit tests that are both correctly sized and relatively powerful against common alternatives. In an empirical application, we recover the REDF from stock market high-frequency data. We inspect the goodness-of-fit of several two-parameter marginal distributions that are inherent in standard stochastic volatility models. The inverse Gaussian offers the best overall description of random equity variation, but the fit is less than perfect. This suggests an extra parameter (as available in, e.g., the generalized inverse Gaussian) is required to model stochastic variance.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [227] [Fueling Volunteer Growth: the case of Wikipedia Administrators](https://arxiv.org/abs/2601.20016)
*Eli Asikin-Garmager,Yu-Ming Liou,Caroline Myrick,Claudia Lo,Diego Saez-Trumper,Leila Zia*

Main category: cs.CY

TL;DR: 维基百科管理员数量在多数语言版本中增长，但高活跃度版本面临下降，主要原因是招募不足而非流失过高


<details>
  <summary>Details</summary>
Motivation: 维基百科管理员对平台成功至关重要，每年执行超过百万次管理操作。研究旨在系统分析不同语言版本维基百科的管理员状况，了解管理员数量变化的趋势和原因

Method: 采用多方法研究：1）大规模管理员日志分析（涵盖284种语言维基百科自2018年数据）；2）超过3000份问卷调查；3）12次访谈

Result: 发现矛盾趋势：超过一半维基百科语言版本管理员净增长，但近三分之二高活跃度维基百科面临下降。下降主要原因是招募不足，而非异常流失。识别出潜在管理员面临的主要障碍：认知度有限、要求模糊、选拔过程苛刻、初始兴趣低

Conclusion: 当前管理员仍保持高度积极性和参与度。研究提出可操作建议以加强招募渠道，促进维基百科管理员增长，这对维基百科长期可持续性至关重要

Abstract: Wikipedia administrators are vital to the platform's success, performing over a million administrative actions annually. This multi-method study systematically analyzes adminship across 284 Wikipedia languages since 2018, revealing a critical two-sided trend: while over half of all Wikipedias show a net increase in administrators, almost two-thirds of highly active Wikipedias face decline. Our analysis, drawing from large-scale adminship log analysis, over 3000 surveys, and 12 interviews, reveals this decline is primarily driven by insufficient recruitment, not unusual attrition. We identify key barriers for potential administrators, including limited awareness, ambiguous requirements, a demanding selection process, and low initial interest. Recognizing that current administrators remain highly motivated and engaged, we propose actionable recommendations to strengthen recruitment pipelines and fuel Wikipedia administrator growth, crucial for Wikipedia's long-term sustainability.

</details>


### [228] [Dynamics of Human-AI Collective Knowledge on the Web: A Scalable Model and Insights for Sustainable Growth](https://arxiv.org/abs/2601.20099)
*Buddhika Nettasinghe,Kang Zhao*

Main category: cs.CY

TL;DR: 提出一个最小化动态模型，研究人类与LLM共同构建网络知识库的协同演化，识别不同增长机制和系统性风险。


<details>
  <summary>Details</summary>
Motivation: 人类与大型语言模型共同生产和消费网络知识库，形成人机集体知识生态系统。这种系统存在反馈循环，既有益处（如更快增长、更易学习），也有系统性风险（如质量稀释、技能退化、模型崩溃）。需要理解这些现象的动态机制。

Method: 提出一个最小化、可解释的动态模型，模拟知识库大小、质量、LLM技能、人类技能和查询量的协同演化。模型包含两个内容流入（人类、LLM）、LLM内容准入控制门、人类两种学习路径（档案学习vs.LLM辅助）和LLM两种训练模式（语料驱动扩展vs.人类反馈学习）。

Result: 通过数值实验识别不同增长机制（健康增长、反向流动、反向学习、振荡），展示平台和政策杠杆如何推动系统跨越机制边界。PubMed、GitHub和Copilot的配置展示了不同增长率和审核规范下的对比稳态。模型拟合维基百科知识流显示，ChatGPT前后时期LLM贡献增加而人类流入减少，与模型识别的机制一致。

Conclusion: 该模型和分析为网络人机集体知识的可持续增长提供了可操作的见解，有助于理解和管理人机知识生态系统的动态演化。

Abstract: Humans and large language models (LLMs) now co-produce and co-consume the web's shared knowledge archives. Such human-AI collective knowledge ecosystems contain feedback loops with both benefits (e.g., faster growth, easier learning) and systemic risks (e.g., quality dilution, skill reduction, model collapse). To understand such phenomena, we propose a minimal, interpretable dynamical model of the co-evolution of archive size, archive quality, model (LLM) skill, aggregate human skill, and query volume. The model captures two content inflows (human, LLM) controlled by a gate on LLM-content admissions, two learning pathways for humans (archive study vs. LLM assistance), and two LLM-training modalities (corpus-driven scaling vs. learning from human feedback). Through numerical experiments, we identify different growth regimes (e.g., healthy growth, inverted flow, inverted learning, oscillations), and show how platform and policy levers (gate strictness, LLM training, human learning pathways) shift the system across regime boundaries. Two domain configurations (PubMed, GitHub and Copilot) illustrate contrasting steady states under different growth rates and moderation norms. We also fit the model to Wikipedia's knowledge flow during pre-ChatGPT and post-ChatGPT eras separately. We find a rise in LLM additions with a concurrent decline in human inflow, consistent with a regime identified by the model. Our model and analysis yield actionable insights for sustainable growth of human-AI collective knowledge on the Web.

</details>


### [229] [Large language models accurately predict public perceptions of support for climate action worldwide](https://arxiv.org/abs/2601.20141)
*Nattavudh Powdthavee,Sandra J. Geiger*

Main category: cs.CY

TL;DR: LLMs能准确预测全球气候行动中的感知差距，性能接近统计模型，可作为昂贵调查的替代或补充工具


<details>
  <summary>Details</summary>
Motivation: 尽管大多数人支持气候行动，但普遍低估他人支持度的现象阻碍了个人和系统性变革。需要一种快速、可靠的方法来评估全球范围内的感知差距，以促进气候行动

Method: 使用预注册实验，基于125个国家的国家级指标和民意数据，将四种最先进的LLM与盖洛普世界民意调查2021/22数据及统计回归模型进行基准测试

Result: LLMs（特别是Claude）能准确捕捉公众对他人愿意为气候行动贡献资金的感知（MAE约5个百分点；r=0.77），性能与统计模型相当，但在数字连接度较低、GDP较低的国家表现下降。控制测试显示LLMs捕捉到了关键心理过程——带有系统性向下偏差的社会投射，并依赖结构化推理而非记忆值

Conclusion: LLMs为评估气候行动中的感知差距提供了快速工具，在资源丰富国家可作为昂贵调查的替代方案，在代表性不足的人群中可作为补充工具

Abstract: Although most people support climate action, widespread underestimation of others' support stalls individual and systemic changes. In this preregistered experiment, we test whether large language models (LLMs) can reliably predict these perception gaps worldwide. Using country-level indicators and public opinion data from 125 countries, we benchmark four state-of-the-art LLMs against Gallup World Poll 2021/22 data and statistical regressions. LLMs, particularly Claude, accurately capture public perceptions of others' willingness to contribute financially to climate action (MAE approximately 5 p.p.; r = .77), comparable to statistical models, though performance declines in less digitally connected, lower-GDP countries. Controlled tests show that LLMs capture the key psychological process - social projection with a systematic downward bias - and rely on structured reasoning rather than memorized values. Overall, LLMs provide a rapid tool for assessing perception gaps in climate action, serving as an alternative to costly surveys in resource-rich countries and as a complement in underrepresented populations.

</details>


### [230] [Adequately Tailoring Age Verification Regulations](https://arxiv.org/abs/2601.20241)
*Shuang Liu,Sarah Scheffler*

Main category: cs.CY

TL;DR: 论文分析了美国年龄验证立法的现状，提出了解释"充分定制"要求的分析模型，并评估了当前技术方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 美国最高法院在Free Speech Coalition v. Paxton案中维持了德克萨斯州H.B. 1181法案的合宪性，但该决定未能解决实际挑战。需要澄清美国年龄验证立法的现状，为技术专家解释"充分定制"的法律要求，并分析现有技术方法的权衡。

Method: 提出一个分析模型，从多个角度解释"充分定制"要求，结合政府目标和利益；应用该模型评估现有州法律和广泛使用的验证方法；绘制美国年龄验证立法现状图；分析主要技术方法。

Result: 1) 绘制了美国年龄验证立法现状图；2) 建立了分析"充分定制"要求的模型，可应用于其他在线监管政策；3) 从技术角度分析了主要年龄验证方法，突出了实际挑战和权衡。

Conclusion: 虽然聚焦美国州法律，但该框架的原则适用于全球年龄验证辩论和方法。论文为法律和技术领域提供了桥梁，帮助非法律专家理解复杂的监管要求。

Abstract: The Supreme Court decision in Free Speech Coalition v. Paxton upheld the constitutionality of Texas H.B. 1181, one of the most constitutionally vulnerable of these age verification laws, holding that it was subject to and satisfied intermediate scrutiny and the requirement that age verification regulations be "adequately tailored". However, the decision leaves unresolved practical challenges. What is the current state of age verification legislation in the United States? How can "adequate tailoring" be interpreted in a way that is accessible to non-legal experts, particularly those in technical and engineering domains? What age verification approaches are used today, what infrastructures and standards support them, and what tradeoffs do they introduce? This paper addresses those questions by proposing an analytical model to interpret "adequate tailoring" from multiple perspectives with associated governmental goals and interests, and by applying that model to evaluate both current state laws and widely used verification methods. This paper's major contributions include: (1) we mapped the current U.S. age-verification legislative landscape; (2) we introduce an analytical model to analyze "adequate tailoring" for age verification and potential application to other online regulatory policies; and (3) we analyze the main technical approaches to age verification, highlighting the practical challenges and tradeoffs from a technical perspective. Further, while we focus on U.S. State laws, the principles underlying our framework are applicable to age-verification debates and methods worldwide.

</details>


### [231] [How AI Impacts Skill Formation](https://arxiv.org/abs/2601.20245)
*Judy Hanwen Shen,Alex Tamkin*

Main category: cs.CY

TL;DR: AI辅助虽然能提升新手工作效率，但会损害技能学习，特别是概念理解、代码阅读和调试能力，且未带来显著效率提升


<details>
  <summary>Details</summary>
Motivation: 研究AI辅助如何影响新手工作者监督AI所需技能的培养，担心过度依赖AI会损害技能习得

Method: 通过随机对照实验，研究开发者在使用和不使用AI辅助的情况下学习新的异步编程库的效果

Result: AI使用损害概念理解、代码阅读和调试能力，平均效率提升不显著；完全委托编码任务的生产力虽有提升但牺牲了学习效果；识别出6种AI交互模式，其中3种涉及认知参与能保持学习效果

Conclusion: AI增强的生产力不是通往能力的捷径，应谨慎将AI辅助纳入工作流程以保持技能形成，特别是在安全关键领域

Abstract: AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. Participants who fully delegated coding tasks showed some productivity improvements, but at the cost of learning the library. We identify six distinct AI interaction patterns, three of which involve cognitive engagement and preserve learning outcomes even when participants receive AI assistance. Our findings suggest that AI-enhanced productivity is not a shortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill formation -- particularly in safety-critical domains.

</details>


### [232] [Agent Benchmarks Fail Public Sector Requirements](https://arxiv.org/abs/2601.20617)
*Jonathan Rystrøm,Chris Schmitz,Karolina Korgul,Jan Batzner,Chris Russell*

Main category: cs.CY

TL;DR: 该研究分析了公共部门部署LLM智能体所需的基准测试标准，发现现有1300多个基准测试均无法完全满足公共部门的严格要求。


<details>
  <summary>Details</summary>
Motivation: 公共部门部署基于大语言模型的智能体需要满足严格的法律、程序和结构要求，但现有基准测试是否能够充分反映这些要求尚不明确。

Method: 基于公共管理文献的第一原则调查定义了基准测试标准：必须基于流程、现实、公共部门特定，并报告反映公共部门独特要求的指标；使用专家验证的LLM辅助流程分析了1300多篇基准测试论文。

Result: 分析结果显示，没有任何一个基准测试能够满足所有标准，现有基准测试在反映公共部门要求方面存在不足。

Conclusion: 研究呼吁研究人员开发符合公共部门相关性的基准测试，同时建议公共部门官员在评估自身智能体用例时应用这些标准。

Abstract: Deploying Large Language Model-based agents (LLM agents) in the public sector requires assuring that they meet the stringent legal, procedural, and structural requirements of public-sector institutions. Practitioners and researchers often turn to benchmarks for such assessments. However, it remains unclear what criteria benchmarks must meet to ensure they adequately reflect public-sector requirements, or how many existing benchmarks do so. In this paper, we first define such criteria based on a first-principles survey of public administration literature: benchmarks must be \emph{process-based}, \emph{realistic}, \emph{public-sector-specific} and report \emph{metrics} that reflect the unique requirements of the public sector. We analyse more than 1,300 benchmark papers for these criteria using an expert-validated LLM-assisted pipeline. Our results show that no single benchmark meets all of the criteria. Our findings provide a call to action for both researchers to develop public sector-relevant benchmarks and for public-sector officials to apply these criteria when evaluating their own agentic use cases.

</details>


### [233] [Audit Trails for Accountability in Large Language Models](https://arxiv.org/abs/2601.20727)
*Victor Ojewale,Harini Suresh,Suresh Venkatasubramanian*

Main category: cs.CY

TL;DR: 提出LLM审计追踪机制，通过时间顺序、防篡改、上下文丰富的生命周期事件记录，将技术溯源与治理记录关联，实现持续问责。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗、金融、就业和公共服务等关键决策中应用日益广泛，但问责机制脆弱，缺乏持久可审查的过程透明度记录。

Method: 提出生命周期框架（事件类型、元数据要求、治理原理）、参考架构（轻量级发射器、仅追加审计存储、审计员界面）和开源Python实现。

Result: 开发了可重用的开源Python实现，可在LLM工作流程中以最小集成工作量实例化审计层，支持跨组织可追溯性。

Conclusion: LLM审计追踪作为持续问责的社会技术机制，讨论了局限性和采用方向，为组织提供了重建变化、时间和授权的能力。

Abstract: Large language models (LLMs) are increasingly embedded in consequential decisions across healthcare, finance, employment, and public services. Yet accountability remains fragile because process transparency is rarely recorded in a durable and reviewable form. We propose LLM audit trails as a sociotechnical mechanism for continuous accountability. An audit trail is a chronological, tamper-evident, context-rich ledger of lifecycle events and decisions that links technical provenance (models, data, training and evaluation runs, deployments, monitoring) with governance records (approvals, waivers, and attestations), so organizations can reconstruct what changed, when, and who authorized it.
  This paper contributes: (1) a lifecycle framework that specifies event types, required metadata, and governance rationales; (2) a reference architecture with lightweight emitters, append only audit stores, and an auditor interface supporting cross organizational traceability; and (3) a reusable, open-source Python implementation that instantiates this audit layer in LLM workflows with minimal integration effort. We conclude by discussing limitations and directions for adoption.

</details>


### [234] [Jurisdiction as Structural Barrier: How Privacy Policy Organization May Reduce Visibility of Substantive Disclosures](https://arxiv.org/abs/2601.20792)
*Thomas Brackin*

Main category: cs.CY

TL;DR: 研究发现隐私政策存在"司法管辖区隔离披露"问题：重要数据实践信息仅出现在特定地区合规章节中，而通用章节使用模糊语言，导致用户可能错过关键信息。


<details>
  <summary>Details</summary>
Motivation: 隐私政策本应提供清晰通知，但研究发现许多公司采用"司法管辖区隔离披露"模式，将实质性信息仅放在特定地区合规章节中，而通用章节使用模糊语言，这可能导致用户错过重要信息，造成透明度缺陷。

Method: 对123家主要公司的隐私政策进行审计，识别出282个潜在实例；使用保守估计方法，基于OPP-115人工标注验证的实践类别，确认了138个实例；分析文档结构模式，结合信息觅食理论预测用户行为。

Result: 在123家公司中，77家公司（62.6%）存在潜在实例；保守估计下，54家公司（44%）确认存在该模式；研究发现2018年后新增的实践类别中该问题尤为突出。

Conclusion: 提出"通用实质性披露"标准：影响所有用户的数据实践应出现在政策主体中，地区章节仅包含程序性权利信息；建议监管机构通过FTC"清晰显著"标准和GDPR透明度原则实施该标准；需要进一步验证用户是否确实跳过地区特定章节。

Abstract: Privacy policies are supposed to provide notice. But what if substantive information appears only where users skip it? We identify a structural pattern we call jurisdiction-siloed disclosure: information about data practices appearing in specific, actionable form only within regional compliance sections labeled "California Residents" or "EU/UK Users," while general sections use vague or qualified language for the same practices.
  Our audit of 123 major companies identifies 282 potential instances across 77 companies (62.6% of this purposive sample). A conservative estimate restricted to practice categories validated against OPP-115 human annotations finds 138 instances across 54 companies (44%); post-2018 categories central to our findings await independent validation. If users skip jurisdiction-labeled sections as information foraging theory predicts, users outside regulated jurisdictions would receive less specific information about practices affecting them--a transparency failure operating through document architecture rather than omission.
  We propose universal substantive disclosure: practices affecting all users should appear in the main policy body, with regional sections containing only procedural rights information. This standard finds support in analogous disclosure regimes (securities, truth-in-lending, nutritional labeling) where material information must reach all affected parties. Regulators could operationalize this through the FTC's "clear and conspicuous" standard and GDPR transparency principles.
  This work is hypothesis-generating: we establish that the structural pattern exists and ground the transparency concern in behavioral theory, but direct measurement of jurisdiction-specific section skipping remains the critical validation priority. We release our methodology and annotated dataset to enable replication.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [235] [Shrinkage Estimators for Mean and Covariance: Evidence on Portfolio Efficiency Across Market Dimensions](https://arxiv.org/abs/2601.20643)
*Rupendra Yadav,Amita Sharma,Aparna Mehra*

Main category: q-fin.PM

TL;DR: 该研究评估了均值方差(MV)和全局最小方差(GMV)模型在不同收缩估计器下的表现，发现GMV模型结合Ledoit Wolf双参数收缩协方差估计器(COV2)对大多数投资者最优，而MV模型结合COV2和样本均值更适合收益导向型投资者。


<details>
  <summary>Details</summary>
Motivation: 均值方差模型作为最主流的投资框架，一直面临预期收益和协方差矩阵估计误差的问题。本研究旨在通过评估不同收缩估计器改进这些核心参数，为投资者提供更稳健的资产配置方案。

Method: 研究评估了5种预期收益收缩估计器和11种协方差矩阵收缩估计器，采用超效率数据包络分析模型对投资组合进行排名，使用6个不同维度的真实世界数据集，采用滚动窗口方法在三个样本外测试期进行实证分析。

Result: 实证结果显示：在大多数情况下，GMV模型结合Ledoit Wolf双参数收缩协方差估计器(COV2)对广泛投资者群体是最优选择；MV模型结合COV2和样本均值更适合收益导向型投资者。这两种模型表现优于传统基准方法。

Conclusion: 本研究为理解特定收缩模型在不同投资者类型和市场环境下的表现奠定了基础，为投资者提供了基于收缩估计的改进型投资组合优化方案。

Abstract: The mean-variance model remains the most prevalent investment framework, built on diversification principles. However, it consistently struggles with estimation errors in expected returns and the covariance matrix, its core parameters. To address this concern, this research evaluates the performance of mean variance (MV) and global minimum-variance (GMV) models across various shrinkage estimators designed to improve these parameters. Specifically, we examine five shrinkage estimators for expected returns and eleven for the covariance matrix. To compare multiple portfolios, we employ a super efficient data envelopment analysis model to rank the portfolios according to investors risk-return preferences. Our comprehensive empirical investigation utilizes six real world datasets with different dimensional characteristics, applying a rolling window methodology across three out of sample testing periods. Following the ranking process, we examine the chosen shrinkage based MV or GMV portfolios against five traditional portfolio optimization techniques classical MV and GMV for sample estimates, MiniMax, conditional value at risk, and semi mean absolute deviation risk measures. Our empirical findings reveal that, in most scenarios, the GMV model combined with the Ledoit Wolf two parameter shrinkage covariance estimator (COV2) represents the optimal selection for a broad spectrum of investors. Meanwhile, the MV model utilizing COV2 alongside the sample mean (SM) proves more suitable for return oriented investors. These two identified models demonstrate superior performance compared to traditional benchmark approaches. Overall, this study lays the groundwork for a more comprehensive understanding of how specific shrinkage models perform across diverse investor profiles and market setups.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [236] [Incorporating data drift to perform survival analysis on credit risk](https://arxiv.org/abs/2601.20533)
*Jianwei Peng,Stefan Lessmann*

Main category: stat.ML

TL;DR: 该研究提出了一种动态联合建模框架，用于在非平稳环境下改进基于生存分析的信用风险模型，通过结合纵向行为标记和离散时间风险模型，在数据漂移场景下表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统生存分析方法通常假设数据生成过程是平稳的，但实际上抵押贷款组合面临多种数据漂移（如借款人行为变化、宏观经济条件变化、政策制度变化等），这会影响模型的预测性能。

Method: 提出了动态联合建模框架，整合了从余额动态中提取的纵向行为标记与离散时间风险模型，结合landmark one-hot编码和等渗校准技术，处理三种类型的数据漂移（突发、渐进和周期性）。

Result: 在Freddie Mac抵押贷款数据集上的实验表明，提出的landmark-based联合模型在所有漂移场景下，在区分度和校准方面均优于经典生存模型、基于树的漂移自适应学习器和梯度提升方法。

Conclusion: 该动态联合建模框架能有效应对信用风险建模中的数据漂移问题，提高模型在非平稳环境下的鲁棒性和预测性能，为实际应用提供了更可靠的解决方案。

Abstract: Survival analysis has become a standard approach for modelling time to default by time-varying covariates in credit risk. Unlike most existing methods that implicitly assume a stationary data-generating process, in practise, mortgage portfolios are exposed to various forms of data drift caused by changing borrower behaviour, macroeconomic conditions, policy regimes and so on. This study investigates the impact of data drift on survival-based credit risk models and proposes a dynamic joint modelling framework to improve robustness under non-stationary environments. The proposed model integrates a longitudinal behavioural marker derived from balance dynamics with a discrete-time hazard formulation, combined with landmark one-hot encoding and isotonic calibration. Three types of data drift (sudden, incremental and recurring) are simulated and analysed on mortgage loan datasets from Freddie Mac. Experiments and corresponding evidence show that the proposed landmark-based joint model consistently outperforms classical survival models, tree-based drift-adaptive learners and gradient boosting methods in terms of discrimination and calibration across all drift scenarios, which confirms the superiority of our model design.

</details>


### [237] [Deep Neural Networks as Iterated Function Systems and a Generalization Bound](https://arxiv.org/abs/2601.19958)
*Jonathan Vacher*

Main category: stat.ML

TL;DR: 该论文将深度神经网络与随机迭代函数系统理论连接，建立了DNN稳定性与泛化性的统一分析框架，提出了基于Wasserstein距离的新训练目标。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然性能优异，但其数学分析仍不完整：稳定性和泛化性通常在不同框架下分别研究，缺乏统一理论。特别是生成式模型的泛化性缺乏严格结果，且递归参数化函数机制可能导致训练不稳定。

Method: 利用随机迭代函数系统理论，证明两种重要深度架构可视为或规范关联于位置依赖的IFS。从随机动力系统引入结果：(1)在适当收缩性假设下建立不变测度的存在唯一性；(2)推导生成建模的Wasserstein泛化界，并由此提出直接控制数据分布与学习转移算子图像间拼贴型近似误差的新训练目标。

Result: 建立了DNN与IFS的理论连接，获得了不变测度的存在唯一性定理和Wasserstein泛化界。在2D控制示例上验证理论，并在MNIST、CelebA、CIFAR-10等标准图像数据集上实证评估新训练目标的有效性。

Conclusion: 通过将深度神经网络与随机迭代函数系统理论连接，为DNN的稳定性和泛化性提供了统一的理论分析框架。提出的新训练目标能够直接控制近似误差，为生成建模提供了更严格的数学基础。

Abstract: Deep neural networks (DNNs) achieve remarkable performance on a wide range of tasks, yet their mathematical analysis remains fragmented: stability and generalization are typically studied in disparate frameworks and on a case-by-case basis. Architecturally, DNNs rely on the recursive application of parametrized functions, a mechanism that can be unstable and difficult to train, making stability a primary concern. Even when training succeeds, there are few rigorous results on how well such models generalize beyond the observed data, especially in the generative setting. In this work, we leverage the theory of stochastic Iterated Function Systems (IFS) and show that two important deep architectures can be viewed as, or canonically associated with, place-dependent IFS. This connection allows us to import results from random dynamical systems to (i) establish the existence and uniqueness of invariant measures under suitable contractivity assumptions, and (ii) derive a Wasserstein generalization bound for generative modeling. The bound naturally leads to a new training objective that directly controls the collage-type approximation error between the data distribution and its image under the learned transfer operator. We illustrate the theory on a controlled 2D example and empirically evaluate the proposed objective on standard image datasets (MNIST, CelebA, CIFAR-10).

</details>


### [238] [Minimax Rates for Hyperbolic Hierarchical Learning](https://arxiv.org/abs/2601.20047)
*Divit Rawal,Sriram Vishwanath*

Main category: stat.ML

TL;DR: 本文证明了在标准Lipschitz正则化下，学习层次数据时欧几里得表示与双曲表示之间存在指数级样本复杂度分离。对于深度R、分支因子m的层次结构，欧几里得空间需要指数级样本，而双曲空间只需多项式样本。


<details>
  <summary>Details</summary>
Motivation: 研究不同几何表示（欧几里得vs双曲）对层次数据学习效率的影响，特别是在Lipschitz正则化约束下。层次数据在自然语言处理、社交网络分析等领域普遍存在，但现有方法在欧几里得空间中的学习效率可能不理想。

Method: 1. 首先证明欧几里得空间的几何障碍：有界半径嵌入会导致体积塌缩，将指数级多的树距离点映射到相近位置；2. 证明双曲空间可以避免此障碍：常数失真双曲嵌入允许O(1)-Lipschitz可实现性；3. 使用Fano不等式建立匹配的下界；4. 分析预测空间的秩限制。

Result: 1. 欧几里得空间需要指数级Lipschitz常数（exp(Ω(R))）来实现简单层次目标，导致指数级样本复杂度；2. 双曲空间只需O(1)-Lipschitz常数，学习样本复杂度为n=O(mR log m)；3. 通过Fano不等式证明Ω(mR log m)下界，表明双曲表示达到信息论最优；4. 任何秩k预测空间最多只能捕获O(k)个规范层次对比。

Conclusion: 双曲表示在层次数据学习中具有根本优势，能够避免欧几里得空间的体积塌缩问题，实现多项式样本复杂度的学习，达到信息论最优。这为层次数据处理提供了理论依据，支持在相关应用中采用双曲几何表示。

Abstract: We prove an exponential separation in sample complexity between Euclidean and hyperbolic representations for learning on hierarchical data under standard Lipschitz regularization. For depth-$R$ hierarchies with branching factor $m$, we first establish a geometric obstruction for Euclidean space: any bounded-radius embedding forces volumetric collapse, mapping exponentially many tree-distant points to nearby locations. This necessitates Lipschitz constants scaling as $\exp(Ω(R))$ to realize even simple hierarchical targets, yielding exponential sample complexity under capacity control. We then show this obstruction vanishes in hyperbolic space: constant-distortion hyperbolic embeddings admit $O(1)$-Lipschitz realizability, enabling learning with $n = O(mR \log m)$ samples. A matching $Ω(mR \log m)$ lower bound via Fano's inequality establishes that hyperbolic representations achieve the information-theoretic optimum. We also show a geometry-independent bottleneck: any rank-$k$ prediction space captures only $O(k)$ canonical hierarchical contrasts.

</details>


### [239] [Efficient Evaluation of LLM Performance with Statistical Guarantees](https://arxiv.org/abs/2601.20251)
*Skyler Wu,Yash Nair,Emmanuel J. Candés*

Main category: stat.ML

TL;DR: FAQ方法通过贝叶斯因子模型利用历史信息，自适应选择问题，在固定查询预算下获得更紧的置信区间，相比均匀采样减少5倍查询量。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在众多基准测试上的全面评估成本高昂，需要在固定查询预算下获得具有有效频率覆盖的紧致置信区间。

Method: 提出因子化主动查询(FAQ)：1) 通过贝叶斯因子模型利用历史信息；2) 使用混合方差减少/主动学习采样策略自适应选择问题；3) 通过主动主动推断保持有效性，这是主动推断的有限总体扩展。

Result: 在两个基准测试套件上，FAQ相比强基线获得高达5倍的有效样本量增益，意味着在达到相同置信区间宽度的情况下，使用最多5倍更少的查询。

Conclusion: FAQ方法能以可忽略的额外成本显著提高基准测试效率，开源代码和数据集支持可重复评估和未来研究。

Abstract: Exhaustively evaluating many large language models (LLMs) on a large suite of benchmarks is expensive. We cast benchmarking as finite-population inference and, under a fixed query budget, seek tight confidence intervals (CIs) for model accuracy with valid frequentist coverage. We propose Factorized Active Querying (FAQ), which (a) leverages historical information through a Bayesian factor model; (b) adaptively selects questions using a hybrid variance-reduction/active-learning sampling policy; and (c) maintains validity through Proactive Active Inference -- a finite-population extension of active inference (Zrnic & Candes, 2024) that enables direct question selection while preserving coverage. With negligible overhead cost, FAQ delivers up to $5\times$ effective sample size gains over strong baselines on two benchmark suites, across varying historical-data missingness levels: this means that it matches the CI width of uniform sampling while using up to $5\times$ fewer queries. We release our source code and our curated datasets to support reproducible evaluation and future research.

</details>


### [240] [Empirical Likelihood-Based Fairness Auditing: Distribution-Free Certification and Flagging](https://arxiv.org/abs/2601.20269)
*Jie Tang,Chuanlong Xie,Xianli Zeng,Lixing Zhu*

Main category: stat.ML

TL;DR: 提出基于经验似然（EL）的非参数统计框架，用于机器学习模型的公平性审计，包括认证和标记功能，无需分布假设且计算高效。


<details>
  <summary>Details</summary>
Motivation: 高风险机器学习应用中存在系统性性能差异（算法偏见），现有公平性审计方法受限于分布假设或计算成本过高。

Method: 基于经验似然的非参数框架，构建稳健的统计量来衡量性能差异，遵循渐近卡方或混合卡方分布，使用约束优化剖面实现稳定数值解。

Result: EL方法优于基于bootstrap的方法，覆盖率更接近名义水平，计算延迟降低数个数量级。在COMPAS数据集上成功标记交叉偏见。

Conclusion: 提出的EL框架为公平性审计提供了有效的统计工具，既能进行大规模认证，又能高效发现亚群体差异，无需分布假设且计算高效。

Abstract: Machine learning models in high-stakes applications, such as recidivism prediction and automated personnel selection, often exhibit systematic performance disparities across sensitive subpopulations, raising critical concerns regarding algorithmic bias. Fairness auditing addresses these risks through two primary functions: certification, which verifies adherence to fairness constraints; and flagging, which isolates specific demographic groups experiencing disparate treatment. However, existing auditing techniques are frequently limited by restrictive distributional assumptions or prohibitive computational overhead. We propose a novel empirical likelihood-based (EL) framework that constructs robust statistical measures for model performance disparities. Unlike traditional methods, our approach is non-parametric; the proposed disparity statistics follow asymptotically chi-square or mixed chi-square distributions, ensuring valid inference without assuming underlying data distributions. This framework uses a constrained optimization profile that admits stable numerical solutions, facilitating both large-scale certification and efficient subpopulation discovery. Empirically, the EL methods outperform bootstrap-based approaches, yielding coverage rates closer to nominal levels while reducing computational latency by several orders of magnitude. We demonstrate the practical utility of this framework on the COMPAS dataset, where it successfully flags intersectional biases, specifically identifying a significantly higher positive prediction rate for African-American males under 25 and a systemic under-prediction for Caucasian females relative to the population mean.

</details>


### [241] [Physics-informed Blind Reconstruction of Dense Fields from Sparse Measurements using Neural Networks with a Differentiable Simulator](https://arxiv.org/abs/2601.20496)
*Ofek Aloni,Barak Fishbain*

Main category: stat.ML

TL;DR: 提出一种从稀疏测量生成密集物理场的新方法，无需空间统计信息或密集场训练样本，通过引入可自动微分数值模拟器实现


<details>
  <summary>Details</summary>
Motivation: 从稀疏测量生成密集物理场是采样和信号处理中的基本问题。现有方法要么需要空间统计信息，要么依赖密集场训练样本，而这些样本通常难以获得，只能依赖合成数据

Method: 在训练阶段引入可自动微分数值模拟器，无需假设空间统计信息的可用性，也无需密集场示例。该方法能够直接从稀疏测量重建密集物理场

Result: 在流体力学三个标准问题上，该方法相比统计方法和基于神经网络的方法表现出更优越的结果

Conclusion: 提出的方法通过集成可自动微分数值模拟器，解决了传统方法对空间统计信息或密集场训练样本的依赖问题，在物理场重建任务中取得了更好的性能

Abstract: Generating dense physical fields from sparse measurements is a fundamental question in sampling, signal processing, and many other applications. State-of-the-art methods either use spatial statistics or rely on examples of dense fields in the training phase, which often are not available, and thus rely on synthetic data. Here, we present a reconstruction method that generates dense fields from sparse measurements, without assuming availability of the spatial statistics, nor of examples of the dense fields. This is made possible through the introduction of an automatically differentiable numerical simulator into the training phase of the method. The method is shown to have superior results over statistical and neural network based methods on a set of three standard problems from fluid mechanics.

</details>


### [242] [Sparse clustering via the Deterministic Information Bottleneck algorithm](https://arxiv.org/abs/2601.20628)
*Efthymios Costa,Ioanna Papatsouma,Angelos Markos*

Main category: stat.ML

TL;DR: 提出一种信息论框架，用于稀疏数据的聚类分析，同时进行特征加权和聚类


<details>
  <summary>Details</summary>
Motivation: 当聚类结构仅存在于特征空间的子集时，传统聚类方法面临挑战，需要处理稀疏数据问题

Method: 基于信息论框架，实现联合特征加权和聚类的方法

Result: 在合成数据模拟中表现出竞争力，并在真实基因组数据集上验证了有效性

Conclusion: 该方法为稀疏数据聚类提供了有竞争力的替代方案，能够同时处理特征选择和聚类任务

Abstract: Cluster analysis relates to the task of assigning objects into groups which ideally present some desirable characteristics. When a cluster structure is confined to a subset of the feature space, traditional clustering techniques face unprecedented challenges. We present an information-theoretic framework that overcomes the problems associated with sparse data, allowing for joint feature weighting and clustering. Our proposal constitutes a competitive alternative to existing clustering algorithms for sparse data, as demonstrated through simulations on synthetic data. The effectiveness of our method is established by an application on a real-world genomics data set.

</details>


### [243] [Demystifying Prediction Powered Inference](https://arxiv.org/abs/2601.20819)
*Yilin Song,Dan M. Kluger,Harsh Parikh,Tian Gu*

Main category: stat.ML

TL;DR: PPI框架利用未标记数据预测提升统计效率，通过偏差校正保持有效推断，但需注意数据重用和MNAR机制问题


<details>
  <summary>Details</summary>
Motivation: 机器学习预测常用于补充不完整或昂贵测量的结果，但直接使用预测作为真实值会引入偏差，而忽略预测又会浪费有价值信息。PPI方法虽有潜力，但其变体众多且差异微妙，使得实践者难以确定何时及如何负责任地应用这些方法

Method: 提出统一的PPI实践工作流程，包括理论基础综合、方法扩展、与现有统计文献的联系以及诊断工具。使用Mosaiks房价数据展示PPI变体效果，分析数据重用和MNAR机制问题，提供决策流程图、方法总结表和核心假设评估策略

Result: PPI变体相比完整案例分析产生更紧密的置信区间，但数据重用会导致反保守的置信区间和覆盖率。在MNAR机制下，包括仅使用标记数据的经典推断在内的所有方法都会产生有偏估计

Conclusion: 将PPI框架化为通用方法而非单一估计器，连接方法创新与应用实践，帮助研究者负责任地将预测整合到有效推断中，提供决策工具和诊断策略以应对不同假设违反情况

Abstract: Machine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from large unlabeled datasets to improve statistical efficiency while maintaining valid inference through explicit bias correction using a smaller labeled subset. Despite its potential, the growing PPI variants and the subtle distinctions between them have made it challenging for practitioners to determine when and how to apply these methods responsibly. This paper demystifies PPI by synthesizing its theoretical foundations, methodological extensions, connections to existing statistics literature, and diagnostic tools into a unified practical workflow. Using the Mosaiks housing price data, we show that PPI variants produce tighter confidence intervals than complete-case analysis, but that double-dipping, i.e. reusing training data for inference, leads to anti-conservative confidence intervals and coverages. Under missing-not-at-random mechanisms, all methods, including classical inference using only labeled data, yield biased estimates. We provide a decision flowchart linking assumption violations to appropriate PPI variants, a summary table of selective methods, and practical diagnostic strategies for evaluating core assumptions. By framing PPI as a general recipe rather than a single estimator, this work bridges methodological innovation and applied practice, helping researchers responsibly integrate predictions into valid inference.

</details>


### [244] [VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring](https://arxiv.org/abs/2601.20830)
*Waldyn G. Martinez*

Main category: stat.ML

TL;DR: VSCOUT是一个用于高维非高斯污染数据的分布无关回顾性过程监控框架，结合ARD-VAE、集成异常过滤和变点检测，通过两阶段精炼获得干净的IC基线。


<details>
  <summary>Details</summary>
Motivation: 现代工业和服务过程产生的高维、非高斯、易受污染数据挑战了传统统计过程控制的基本假设，重尾、多模态、非线性依赖和稀疏特殊原因观测会扭曲基线估计、掩盖真实异常并阻碍可靠IC参考集的识别。

Method: VSCOUT结合自动相关确定变分自编码器架构与基于集成的潜在异常过滤和变点检测。ARD先验隔离信息最丰富的潜在维度，集成和变点过滤器在确定的潜在空间中识别点状和结构性污染。第二阶段重新训练步骤移除标记观测，仅使用保留的内点重新估计潜在结构。

Result: 在基准数据集上的广泛实验表明，VSCOUT在保持可控误报的同时，对特殊原因结构具有卓越的敏感性，优于经典SPC程序、稳健估计器和现代机器学习基线方法。

Conclusion: VSCOUT的可扩展性、分布灵活性和对复杂污染模式的鲁棒性，使其成为AI赋能环境中回顾性建模和异常检测的实用有效方法。

Abstract: Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (IC) reference set. To address these challenges, we introduce VSCOUT, a distribution-free framework designed specifically for retrospective (Phase I) monitoring in high-dimensional settings. VSCOUT combines an Automatic Relevance Determination Variational Autoencoder (ARD-VAE) architecture with ensemble-based latent outlier filtering and changepoint detection. The ARD prior isolates the most informative latent dimensions, while the ensemble and changepoint filters identify pointwise and structural contamination within the determined latent space. A second-stage retraining step removes flagged observations and re-estimates the latent structure using only the retained inliers, mitigating masking and stabilizing the IC latent manifold. This two-stage refinement produces a clean and reliable IC baseline suitable for subsequent Phase II deployment. Extensive experiments across benchmark datasets demonstrate that VSCOUT achieves superior sensitivity to special-cause structure while maintaining controlled false alarms, outperforming classical SPC procedures, robust estimators, and modern machine-learning baselines. Its scalability, distributional flexibility, and resilience to complex contamination patterns position VSCOUT as a practical and effective method for retrospective modeling and anomaly detection in AI-enabled environments.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [245] [Randomized Feasibility Methods for Constrained Optimization with Adaptive Step Sizes](https://arxiv.org/abs/2601.20076)
*Abhishek Chakraborty,Angelia Nedić*

Main category: math.OC

TL;DR: 提出一种随机可行性算法处理凸约束优化问题，针对强凸光滑和凸非光滑两种情况，使用Polyak步长和随机约束采样，证明线性收敛和O(1/√T)收敛率。


<details>
  <summary>Details</summary>
Motivation: 处理难以投影的凸约束优化问题，特别是当约束定义为多个凸函数下水平集的交集时，传统投影方法难以实施。

Method: 使用随机可行性算法结合Polyak步长，每次迭代随机采样约束子集，对目标函数采用（次）梯度下降，针对不同情况设计自适应步长方案。

Result: 对于强凸光滑情况证明期望线性收敛；对于凸非光滑情况得到O(1/√T)最坏情况收敛率；不可行性几乎必然几何下降；在特定采样增长下达到最优收敛率。

Conclusion: 提出的随机算法能有效处理复杂凸约束优化问题，在QCQP和SVM问题上计算效率优于现有方法，具有自适应和参数无关的优点。

Abstract: We consider minimizing an objective function subject to constraints defined by the intersection of lower-level sets of convex functions. We study two cases: (i) strongly convex and Lipschitz-smooth objective function and (ii) convex but possibly nonsmooth objective function. To deal with the constraints that are not easy to project on, we use a randomized feasibility algorithm with Polyak steps and a random number of sampled constraints per iteration, while taking (sub)gradient steps to minimize the objective function. For case (i), we prove linear convergence in expectation of the objective function values to any prescribed tolerance using an adaptive stepsize. For case (ii), we develop a fully problem parameter-free and adaptive stepsize scheme that yields an $O(1/\sqrt{T})$ worst-case rate in expectation. The infeasibility of the iterates decreases geometrically with the number of feasibility updates almost surely, while for the averaged iterates, we establish an expected lower bound on the function values relative to the optimal value that depends on the distribution for the random number of sampled constraints. For certain choices of sample-size growth, optimal rates are achieved. Finally, simulations on a Quadratically Constrained Quadratic Programming (QCQP) problem and Support Vector Machines (SVM) demonstrate the computational efficiency of our algorithm compared to other state-of-the-art methods.

</details>


### [246] [A Fokker-Planck Framework for Control of Epidemics](https://arxiv.org/abs/2601.20181)
*Christian Parkinson,Souvik Roy*

Main category: math.OC

TL;DR: 提出一个基于Fokker-Planck方程的流行病学随机室模型控制框架，通过控制概率分布而非直接控制系统，实现鲁棒控制并处理动态和初始数据的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统随机系统直接控制方法难以处理动态和初始数据的不确定性，需要一种能够引导随机系统解分布到期望状态的鲁棒控制框架。

Method: 通过关联的Fokker-Planck方程进行最优控制，将随机系统的控制问题转化为偏微分方程约束优化问题，使用序列二次哈密顿方法进行数值求解。

Result: 建立了完整的偏微分方程约束优化问题理论框架，证明了最优控制的存在性，通过Pontryagin极小值原理刻画了最优控制，并实现了数值近似。

Conclusion: 该框架为流行病学随机室模型提供了有效的鲁棒控制方法，能够处理动态和初始不确定性，并通过不同成本泛函适应不同政策制定者需求。

Abstract: We present a control framework for stochastic compartmental models in epidemiology. In this framework, rather than directly controlling the stochastic system, we perform optimal control of an associated Fokker-Planck equation, with the goal of steering the distribution of possible solutions of the stochastic system to some desirable state. In particular, this allows for robust control mechanism with uncertainty not only in the dynamics, but also in the initial data. We formulate and fully analyze a partial differential equation constrained optimization problem, including a proof of existence of optimal controls via analysis of the control-to-state map, and a characterization of optimal controls via the Pontryagin minimum principle. We describe the application of the sequential quadratic Hamiltonian method to our problem, which provides numerical approximations of optimal control maps. We demonstrate our method using a minimal stochastic susceptible-infected-recovered model with different choices of cost functionals that represent different policy-maker concerns.

</details>


### [247] [Improved Global Landscape Guarantees for Low-rank Factorization in Synchronization](https://arxiv.org/abs/2601.20292)
*Shuyang Ling*

Main category: math.OC

TL;DR: 论文分析了正交群同步问题的低秩优化方法，研究了Stiefel流形上非凸优化问题的良性景观条件，改进了对Hessian条件数的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 正交群同步问题在信号处理、计算机视觉和网络分析中具有基础性作用。现有方法中，半定规划(SDR)虽然理论保证强但计算不可扩展，而低秩分解方法高效但非凸，容易陷入局部极小值。需要理解何时非凸优化景观是良性的（无虚假局部极小值）。

Method: 采用低秩方法，将优化问题定义在Stiefel流形St(p,d)^⊗n上。通过将景观分析转化为另一个凸优化问题，为参数对(p,d)提供统一的景观特征分析，其中p≥d+2（d≥1）和p=d+1（1≤d≤3）。

Result: 改进了对Hessian条件数的依赖关系，恢复了d=1时的已知尖锐边界（对Kuramoto同步很有用），并显著改进了d≥2且p≥d+2时的一般情况保证。理论结果具有普适性，适用于广泛示例。

Conclusion: 该研究为理解正交群同步问题的低秩优化方法的良性景观条件提供了统一的理论框架，显著改进了现有结果，特别是在对Hessian条件数的依赖关系方面，为实际应用提供了更强的理论保证。

Abstract: The orthogonal group synchronization problem, which aims to recover a set of $d \times d$ orthogonal matrices from their pairwise noisy products, plays a fundamental role in signal processing, computer vision, and network analysis. In recent years, numerous optimization techniques, such as semidefinite relaxation (SDR) and low-rank (Burer-Monteiro) factorization, have been proposed to address this problem and their theoretical guarantees have been extensively studied. While SDR is provably powerful and exact in recovering the least-squares estimator under certain mild conditions, it is not scalable. In contrast, the low-rank factorization is highly efficient but nonconvex, meaning its iterates may get trapped in local minima. To close the gap, we analyze the low-rank approach and focus on understanding when the associated nonconvex optimization landscape is benign, i.e., free of spurious local minima. Recent works suggest that the benignness depends on the condition number of the Hessian at the global minimizer, but it remains unclear whether sharp guarantees can be achieved. In this work, we consider the low-rank approach which corresponds to an optimization problem over the Stiefel manifold ${\rm St}(p,d)^{\otimes n}$. By formulating the landscape analysis into another convex optimization problem, we provide a unified characterization of the optimization landscape for all parameter pairs $(p,d)$ with $p \geq d+2$ for $d\geq 1$ and $p = d+1$ for $1\leq d\leq 3$ which gives a much improved dependence on the condition number of the Hessian. Our results recover the known sharp state-of-the-art bound for $d=1$ which is extremely useful for characterizing the Kuramoto synchronization, and significantly improved the guarantees for the general case $d \geq 2$ with $p \geq d+2$ over the existing results. The theoretical results are versatile and applicable to a wide range of examples.

</details>


### [248] [A novel neural network with predefined-time stability for solving generalized monotone inclusion problems with applications](https://arxiv.org/abs/2601.20338)
*Nam Van Tran*

Main category: math.OC

TL;DR: 提出一种新的动力学框架解决Hilbert空间中的包含问题0∈F(x)+G(x)，在广义单调性假设下建立固定时间和预定义时间稳定性，并研究其离散化算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要经典单调性条件，限制了应用范围。本文旨在放松这些条件，提出更通用的动力学框架来解决包含问题，同时实现固定时间和预定义时间收敛。

Method: 提出新颖的动力学框架处理包含问题，在广义单调性假设下分析。建立连续时间系统的固定时间和预定义时间稳定性理论。研究显式前向欧拉离散化，得到前向-后向迭代算法。

Result: 在温和条件下证明了动力学系统的固定时间稳定性（收敛时间与初始条件无关）和预定义时间稳定性（可预先指定收敛时间）。离散化算法具有严格的收敛性分析。

Conclusion: 所提方法在广义单调性下有效，扩展了传统方法的适用范围。通过约束优化、混合变分不等式等问题的数值实验验证了理论结果的有效性和通用性。

Abstract: We propose a novel dynamical framework for solving inclusion
  problems of the form \(0 \in F(x) + G(x)\) in Hilbert spaces, where \(F\) is a
  maximal set-valued operator and \(G\) is a single-valued mapping. The analysis is
  conducted under a generalized monotonicity assumption, which relaxes the
  classical monotonicity conditions commonly imposed in the literature and thereby
  extends the applicability of the proposed approach.
  Under mild conditions on the system parameters, we establish both fixed-time and
  predefined-time stability of the resulting dynamical system. The fixed-time
  stability guarantees a uniform upper bound on the settling time that is
  independent of the initial condition, whereas the predefined-time stability
  framework allows the system parameters to be selected \emph{a priori} in order
  to ensure convergence within a user-specified time horizon.
  Moreover, we investigate an explicit forward Euler discretization of the
  continuous-time dynamics, leading to a novel forward--backward iterative
  algorithm. A rigorous convergence analysis of the resulting discrete scheme is
  provided. Finally, the effectiveness and versatility of the proposed method are
  illustrated through several classes of problems, including constrained
  optimization problems, mixed variational inequalities, and variational
  inequalities, together with numerical experiments that corroborate the
  theoretical results.

</details>


### [249] [Data-Driven Structured Control for Continuous-Time LTI Systems](https://arxiv.org/abs/2601.20340)
*Zhaohua Yang,Yuxing Zhong,Ling Shi*

Main category: math.OC

TL;DR: 基于数据的结构化控制器设计：利用收集的数据构建包含所有可行系统矩阵的最小矩阵椭球，通过线性化技术处理控制器结构约束，提出迭代算法解决稳定化、H₂和H∞性能控制问题。


<details>
  <summary>Details</summary>
Motivation: 解决连续时间线性时不变系统的数据驱动结构化控制器设计问题。传统方法需要精确的系统模型，而实际中系统矩阵可能未知或不精确，需要直接从数据中设计满足特定结构约束的控制器。

Method: 1. 利用收集的数据构建包含所有可行系统矩阵的最小矩阵椭球；2. 提出线性化技术处理控制器的结构约束；3. 针对稳定化、H₂性能和H∞性能三个控制目标分别设计迭代算法。

Result: 通过数值算例验证了所提方法的有效性，表明该方法能够直接从数据中设计满足结构约束的控制器，并实现稳定化、H₂和H∞性能控制目标。

Conclusion: 本文提出了一种数据驱动的结构化控制器设计框架，通过构建矩阵椭球和线性化技术，成功解决了在系统矩阵未知情况下的结构化控制器设计问题，为实际工程应用提供了有效工具。

Abstract: This paper addresses the data-driven structured controller design problem for continuous-time linear time-invariant (LTI) systems. We consider three control objectives, including stabilization, $H_2$ performance, and $H_\infty$ performance. Using the collected data, we construct a minimal matrix ellipsoid that contains all admissible system matrices. We propose some linearization techniques that enable us to incorporate the structural constraint on the controller, which motivates an iterative algorithm for each control objective. Finally, we provide some numerical examples to demonstrate the effectiveness of the proposed methods.

</details>


### [250] [Decentralized Stochastic Constrained Optimization via Prox-Linearization](https://arxiv.org/abs/2601.20345)
*Shivangi Dubey Sharma,Basil M. Idrees,Lavish Arora,Ketan Rajawat*

Main category: math.OC

TL;DR: 本文研究基于共识的去中心化随机优化，用于最小化可能非凸的期望目标函数，包含凸非光滑正则项和非线性函数不等式约束。提出了两种算法，在标准假设下达到最优的O(ε^{-3/2})复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化环境中具有非凸目标、非光滑正则项和非线性约束的随机优化问题。现有方法在处理复杂约束和去中心化通信方面存在不足，需要开发高效算法。

Method: 使用精确惩罚模型重新表述约束问题。提出两种算法：1) D-SMPL：结合约束线性化和近端线性步骤，每迭代产生线性约束二次子问题；2) D-SCAMPL：基于连续凸近似的变体，通过强凸代理子问题处理额外目标结构。两种方法都采用递归动量梯度估计器和每迭代仅需两轮通信的共识机制。

Result: 在标准光滑性和正则性假设下，两种算法都达到了O(ε^{-3/2})的oracle复杂度，这与无约束集中式随机非凸优化的最优速率相匹配。数值实验在能量最优海洋轨迹规划中验证了理论并展示了优于现有去中心化基线的性能。

Conclusion: 本文提出的去中心化随机优化算法能有效处理非凸目标、非光滑正则项和非线性约束，达到最优复杂度，并在实际应用中表现出优越性能。

Abstract: This paper studies consensus-based decentralized stochastic optimization for minimizing possibly non-convex expected objectives with convex non-smooth regularizers and nonlinear functional inequality constraints. We reformulate the constrained problem using the exact-penalty model and develop two algorithms that require only local stochastic gradients and first-order constraint information. The first method, Decentralized Stochastic Momentum-based Prox-Linear Algorithm (D-SMPL), combines constraint linearization with a prox-linear step, resulting in a linearly constrained quadratic subproblem per iteration. Building on this approach, we propose a successive convex approximation (SCA) variant, Decentralized SCA Momentum-based Prox-Linear (D-SCAMPL), which handles additional objective structure through strongly convex surrogate subproblems while still allowing infeasible initialization. Both methods incorporate recursive momentum-based gradient estimators and a consensus mechanism requiring only two communication rounds per iteration. Under standard smoothness and regularity assumptions, both algorithms achieve an oracle complexity of $\mathcal{O}(ε^{-3/2})$, matching the optimal rate known for unconstrained centralized stochastic non-convex optimization. Numerical experiments on energy-optimal ocean trajectory planning corroborate the theory and demonstrate improved performance over existing decentralized baselines.

</details>


### [251] [Reinforcement Learning for Dividend Optimization in Partially Observed Regime-Switching Diffusion Model](https://arxiv.org/abs/2601.20387)
*Zhongqin Gao,Yan Lv,Jingmin He*

Main category: math.OC

TL;DR: 提出一个部分观测的体制转换扩散模型中的最优分红问题，采用连续时间强化学习方法，通过探索性HJB方程获得半解析解，并开发演员-评论家算法学习最优策略。


<details>
  <summary>Details</summary>
Motivation: 在实际金融市场中，市场体制通常是不可观测的，且关键模型参数未知。传统完全信息假设下的最优分红控制方法不适用，需要开发能够处理部分信息和参数不确定性的新方法。

Method: 在探索性（熵正则化）随机控制框架下，采用连续时间强化学习方法。首先推导探索性HJB方程的半解析解，然后引入价值函数和策略的参数化族（低阶多项式逼近），最后开发演员-评论家RL算法，通过在线交互进行信念状态滤波、策略评估和策略改进。

Result: 数值实验表明，学习到的分红策略在样本外表现出色，验证了所提方法在部分观测和参数不确定性环境下的有效性。

Conclusion: 该研究成功地将连续时间强化学习应用于部分观测体制转换模型中的最优分红问题，提出的半解析框架和演员-评论家算法能够有效处理市场体制不可观测和参数不确定性的挑战。

Abstract: This paper studies the optimal dividend problem with a bounded payout rate in a partially observed regime-switching diffusion model, where, in practice, the market regime is unobserved and key model parameters are unknown. To address this partial-information setting, we propose a continuous-time reinforcement learning (RL) approach within an exploratory (entropy-regularized) stochastic control framework for discounted dividends under regime switching. The associated exploratory Hamilton-Jacobi-Bellman (HJB) system admits semi-analytical characterizations of the value function and the optimal exploratory dividend policy, determined by two unknown functions solving two ordinary differential equations (ODEs) together with positive real roots of the induced quadratic equations. Exploiting this structure, we introduce parametric families for both the value function and the policy, using low-degree polynomial approximations to the ODE solutions. We then develop an actor-critic RL algorithm to learn the optimal exploratory policy through interactions with the market environment: it performs belief-state filtering from observed data and iterates policy evaluation and policy improvement online to refine the policy. Numerical experiments demonstrate strong out-of-sample performance of the learned dividend policies.

</details>


### [252] [Convergence Analysis of Randomized Subspace Normalized SGD under Heavy-Tailed Noise](https://arxiv.org/abs/2601.20399)
*Gaku Omiya,Pierre-Louis Poirion,Akiko Takeda*

Main category: math.OC

TL;DR: 论文提出两种随机子空间优化方法：RS-SGD在次高斯噪声下获得高概率收敛保证，RS-NSGD针对重尾梯度噪声，通过方向归一化改进收敛复杂度。


<details>
  <summary>Details</summary>
Motivation: 随机子空间方法能降低每次迭代成本，但在非凸优化中，现有分析多为期望收敛，高概率收敛界即使在次高斯噪声下也很稀缺。现代机器学习中重尾梯度普遍存在，需要更鲁棒的算法。

Method: 1. 随机子空间SGD (RS-SGD)：在子空间中进行随机梯度下降；2. 随机子空间归一化SGD (RS-NSGD)：在子空间更新中集成方向归一化，以处理重尾梯度噪声。

Result: 1. RS-SGD在次高斯噪声下获得高概率收敛界，达到与先前期望结果相同阶的oracle复杂度；2. RS-NSGD在噪声有界p阶矩假设下，建立期望和高概率收敛保证，且能比全维归一化SGD获得更好的oracle复杂度。

Conclusion: 随机子空间方法在保持低计算成本的同时，能提供强收敛保证。RS-NSGD特别适用于处理现代机器学习中的重尾梯度问题，在收敛复杂度上优于全维方法。

Abstract: Randomized subspace methods reduce per-iteration cost; however, in nonconvex optimization, most analyses are expectation-based, and high-probability bounds remain scarce even under sub-Gaussian noise. We first prove that randomized subspace SGD (RS-SGD) admits a high-probability convergence bound under sub-Gaussian noise, achieving the same order of oracle complexity as prior in-expectation results. Motivated by the prevalence of heavy-tailed gradients in modern machine learning, we then propose randomized subspace normalized SGD (RS-NSGD), which integrates direction normalization into subspace updates. Assuming the noise has bounded $p$-th moments, we establish both in-expectation and high-probability convergence guarantees, and show that RS-NSGD can achieve better oracle complexity than full-dimensional normalized SGD.

</details>


### [253] [Sufficiently Regularized Nonnegative Quartic Polynomials are Sum-of-Squares](https://arxiv.org/abs/2601.20418)
*Wenqi Zhu,Coralia Cartis*

Main category: math.OC

TL;DR: 本文扩展了希尔伯特第17问题的等价性结果，针对具有特殊结构的多元四次正则化多项式，证明了在某些条件下非负性与平方和表示的等价性，并刻画了这些多项式全局优化的NP难边界。


<details>
  <summary>Details</summary>
Motivation: 希尔伯特第17问题表明并非所有非负多项式都能表示为平方和，但某些特殊类别存在等价性。本文旨在将这种等价性扩展到多元四次正则化多项式，这类多项式在非凸优化的高阶张量方法中作为迭代子问题出现。

Method: 研究多元对称三次多项式通过加权四次欧几里得范数正则化后的全局优化。通过将多项式平移至其全局最优值使其非负，证明在足够大的正则化参数和温和假设下，这些多项式存在平方和表示。识别了几种特殊结构子类，包括二次-四次多项式和包含特殊三次项的四次多项式。

Result: 证明了对于足够大的正则化参数，这些特殊结构多项式存在平方和表示；识别了全局最优性隐含平方和分解的结构子类；通过四次可分离范数的反例展示了欧几里得范数的关键作用；展示了平方和证书在高阶张量方法泰勒子问题中的应用，并提供了鼓舞人心的数值结果。

Conclusion: 本文成功将希尔伯特第17问题的等价性结果扩展到多元四次正则化多项式的新子类，刻画了这些多项式全局优化的NP难边界，为高阶张量方法中的子问题提供了有效的平方和证书，具有理论和实际应用价值。

Abstract: Hilbert's 17th problem famously established that not all nonnegative polynomials admit a sum-of-squares (SoS) representation. Hilbert also identified a few special classes in which nonnegativity and SoS are equivalent, such as univariate polynomials, quadratic polynomials, and bivariate quartic polynomials. In this paper, we extend this equivalence to several new subclasses of multivariate quartically regularized polynomials and characterize the NP-hardness boundary of these special-structure polynomials. Specifically, we consider the global optimization of multivariate symmetric cubic polynomials regularized by weighted quartic powers of the Euclidean norm. These special-structure polynomials arise as iterative subproblems in high-order tensor methods for nonconvex optimization problems. We consider shifting these polynomials by their global optimum so as to make them nonnegative, and show that for sufficiently large regularization parameters and under mild assumptions, these polynomials admit a sum-of-squares representation. We also identify several structured subclasses of quartically regularized cubic polynomials for which global optimality of the model implies that nonnegativity is certified by a sum-of-squares decomposition for all values of the regularization parameter, including quadratic-quartic polynomials and quartic polynomials containing a special cubic term that can be decomposed as the product of a quadratic norm and a linear form. We provide counterexamples based on quartic separable norms that demonstrate the crucial role of the Euclidean norm in these representations. Finally, we illustrate how these SoS-based certificates can be used for Taylor subproblems arising in high-order tensor methods for nonconvex optimization, with encouraging numerical results.

</details>


### [254] [Adaptive Conditional Gradient Sliding: Projection-Free and Line-Search-Free Acceleration](https://arxiv.org/abs/2601.20443)
*Shota Takahashi*

Main category: math.OC

TL;DR: AdCGS：一种无需投影、无需线搜索的加速条件梯度滑动方法，在线性最小化oracle可用时实现Nesterov加速，匹配基于投影方法的收敛率。


<details>
  <summary>Details</summary>
Motivation: 在凸优化问题中，当投影操作昂贵但线性最小化oracle（LMO）可用时，需要开发既保持加速收敛又避免投影计算的方法。

Method: AdCGS结合加速外部方案和基于LMO的内部例程，重用梯度减少梯度评估，通过预设精度水平和自适应步长控制子问题不精确性，无需线搜索。

Result: 对于凸函数获得加速收敛率，匹配基于投影的加速方法；对于强凸函数获得线性收敛，无需约束集的额外几何假设（如多面体或强凸集）。

Conclusion: AdCGS在约束ℓ_p回归、逻辑回归和最小二乘问题上优于基于投影和无投影的基线方法，为投影昂贵场景提供高效解决方案。

Abstract: We study convex optimization problems over a compact convex set where projections are expensive but a linear minimization oracle (LMO) is available. We propose the adaptive conditional gradient sliding method (AdCGS), a projection-free and line-search-free method that retains Nesterov's acceleration with adaptive stepsizes based on local Lipschitz estimates. AdCGS combines an accelerated outer scheme with an LMO-based inner routine. It reuses gradients across multiple LMO calls to reduce gradient evaluations, while controlling the subproblem inexactness via a prescribed accuracy level coupled with adaptive stepsizes. We prove accelerated convergence rates for convex objective functions matching those of projection-based accelerated methods, while requiring no projection oracle. For strongly convex objective functions, we further establish linear convergence without additional geometric assumptions on the constraint set, such as polytopes or strongly convex sets. Experiments on constrained $\ell_p$ regression, logistic regression with real-world datasets, and least-squares problems demonstrate improvements over both projection-free and projection-based baselines.

</details>


### [255] [On controllability, observability and stabilizability of the heat equation on discrete graphs](https://arxiv.org/abs/2601.20594)
*Florentin Münch,Christian Seifert,Peter Stollmann,Martin Tautenhahn*

Main category: math.OC

TL;DR: 该论文研究了离散图上热方程的线性控制问题，通过弱可观性估计证明了成本均匀α-可控性，并讨论了结果的最优性和稳定性


<details>
  <summary>Details</summary>
Motivation: 研究离散图上热方程的线性控制问题，特别是在相对稠密区域D上的控制能力，旨在建立成本均匀的α-可控性理论框架

Method: 采用对偶观测问题的弱可观性估计方法，通过加权拉普拉斯算子H和相对稠密区域D的控制项来建立控制理论

Result: 证明了成本均匀α-可控性，即存在控制策略使得系统在有限时间内以均匀成本达到期望状态

Conclusion: 建立了离散图上热方程控制的理论框架，证明了可控性结果的最优性，并推导了相应的稳定性性质

Abstract: We consider linear control problems for the heat equation of the form $\dot f (t) = -Hf (t) + \mathbf{1}_D u (t)$, $f (0) \in \ell_2 (X,m)$, where $H$ is the weighted Laplacian on a discrete graph $(X,b,m)$, and where $D \subseteq X$ is relatively dense. We show cost-uniform $α$-controllability by means of a weak observability estimate for the corresponding dual observation problem. We discuss optimality of our result as well as consequences on stabilizability properties.

</details>


### [256] [Drone-Aided Blood Collection Routing Problem: A Column Generation Approach](https://arxiv.org/abs/2601.20693)
*Amirhossein Abbaszadeh,Hossein Hashemi Doulabi*

Main category: math.OC

TL;DR: 本文提出无人机辅助的血液采集路径规划问题，通过卡车-无人机协同作业，优化血液采集路线和时间安排，最大化可用的血液单位数量。


<details>
  <summary>Details</summary>
Motivation: 血小板采集需要在捐赠后6小时内处理，血液采集机构需要优化车辆路线以在时限内收集血液单位。传统卡车运输效率有限，需要更灵活的解决方案。

Method: 提出混合整数线性规划模型，联合优化卡车和无人机路径、采集时间安排。开发列生成方法，主问题选择最优卡车-无人机行程，定价子问题使用定制化memetic算法生成新列。

Result: 计算研究表明，集成无人机到血液采集系统具有操作优势。提出的算法在无人机辅助和纯卡车设置下均优于Gurobi和文献中的两种元启发式算法（混合遗传算法和入侵杂草优化）。

Conclusion: 无人机辅助的血液采集系统能显著提高血液收集效率，提出的算法在解决这类复杂路径规划问题上表现出优越性能。

Abstract: Platelet extraction requires whole blood to be processed within six hours of donation. To meet this deadline, blood collection organizations must optimally route a fleet of vehicles to pick up blood units from donation sites and deliver them to a processing center. This paper introduces a drone-aided blood collection routing problem in which a fleet of trucks, each equipped with a drone, operates in a synchronized manner to collect blood units before their processing time limit expires. Each truck-drone tandem can perform multiple trips throughout the planning horizon, allowing donation sites to be visited repeatedly as new blood units become available over time. We formulate this problem as a mixed-integer linear program that jointly optimizes the routing of trucks and drones, pickup schedules, and timing decisions to maximize the total number of viable blood units collected. We also develop a column generation approach that decomposes the problem into a master problem to select the optimal set of truck-drone tours and a pricing subproblem, which is solved using a tailored memetic algorithm to generate promising new columns. Through a comprehensive computational study, we show the operational benefits of integrating drones into the blood collection system. In addition, we demonstrate the superior performance of the proposed algorithm over Gurobi and two metaheuristics from the literature, namely the hybrid genetic algorithm and the invasive weed optimization, in both the drone-aided and truck-only settings.

</details>


### [257] [Adaptive Dimension Reduction for Overlapping Group Sparsity](https://arxiv.org/abs/2601.20697)
*Yifan Bai,Clarice Poon,Jingwei Liang*

Main category: math.OC

TL;DR: 提出针对重叠群组稀疏优化的新对偶证书和自适应支撑识别方案，显著加速现有算法


<details>
  <summary>Details</summary>
Motivation: 传统非重叠稀疏优化的降维技术已较成熟，但重叠群组稀疏的降维规则发展不足，因为次梯度结构更复杂且对偶变量与稀疏模式间的联系间接

Method: 提出新的重叠群组稀疏对偶证书和自适应支撑识别方案，集成到现有算法（原始对偶分裂法、ADMM、过参数化变量投影方案）中加速计算

Result: 提供了方法的收敛性分析，并在标准数据集上验证了实际有效性

Conclusion: 提出的新对偶证书和自适应支撑识别方案能有效解决重叠群组稀疏优化的降维问题，显著提升算法效率

Abstract: Typical dimension reduction techniques for nonoverlapping sparse optimization involve screening or sieving strategies based on a dual certificate derived from the first-order optimality condition, approximating the gradients or exploiting certain inherent low-dimensional structure of the sparse solution. In comparison, dimension reduction rules for overlapping group sparsity are generally less developed because the subgradient structure is more complex, making the link between sparsity pattern and the dual variable indirect due to the non-separability. In this work, we propose new dual certificates for overlapping group sparsity and a novel adaptive scheme for identifying the support of the overlapping group LASSO. We demonstrate how this scheme can be integrated into and significantly accelerate existing algorithms, including Primal-Dual splitting method, alternating direction method of multipliers and a recently developed variable projection scheme based on over-parameterization. We provide convergence analysis of the method and verify its practical effectiveness through experiments on standard datasets.

</details>


### [258] [A penalty-interior point method combined with MADS for equality and inequality constrained optimization](https://arxiv.org/abs/2601.20811)
*Charles Audet,Andrea Brilli,Youssef Diouane,Sébastien Le Digabel,Everton J. Silva,Christophe Tribes*

Main category: math.OC

TL;DR: MADS-PIP框架将惩罚-内点策略集成到MADS算法中，用于求解带一般不等式和等式约束的非光滑黑盒优化问题，在等式约束处理方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有MADS算法在处理一般不等式和等式约束，特别是等式约束时存在局限性，需要开发更有效的约束处理策略。

Method: 将不等式约束分为两个子集：一部分用对数障碍函数处理内部约束违反，另一部分用外部二次惩罚处理；所有等式约束用外部惩罚处理。通过罚函数定义一系列无约束子问题，用MADS近似求解，并通过精心设计的更新规则驱动惩罚-障碍参数趋于零。

Result: 建立了非光滑设置下的收敛性结果，确保一般约束的可行性和不等式约束问题的Clarke平稳性。计算实验表明MADS-PIP在性能上优于或与渐进障碍策略的MADS相当，尤其在处理等式约束时表现更佳。

Conclusion: MADS-PIP是一个有效的框架，特别适合处理带等式约束的非光滑黑盒优化问题，在理论和实践上都表现出色。

Abstract: This work introduces MADS-PIP, an efficient framework that integrates a penalty-interior point strategy into the mesh adaptive direct search (MADS) algorithm for solving nonsmooth blackbox optimization problems with general inequality and equality constraints. Inequality constraints are partitioned into two subsets: one treated via a logarithmic barrier applied to an aggregated interior constraint violation, and the other handled through an exterior quadratic penalty. All equality constraints are treated by the exterior penalty. A merit function defines a sequence of unconstrained subproblems, which are solved approximately using MADS, while a carefully designed update rule drives the penalty-barrier parameter to zero. In the nonsmooth setting, we establish convergence results ensuring feasibility for general constraints as well as Clarke stationarity for inequality-constrained problems. Computational experiments on both analytical test sets and challenging blackbox problems demonstrate that the proposed MADS-PIP algorithm is competitive with, and often outperforms, MADS with the progressive barrier strategy, particularly in the presence of equality constraints.

</details>
