<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 111]
- [cs.AI](#cs.AI) [Total: 69]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 167]
- [math.OC](#math.OC) [Total: 19]
- [econ.EM](#econ.EM) [Total: 6]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 8]
- [q-fin.PR](#q-fin.PR) [Total: 2]
- [eess.SY](#eess.SY) [Total: 21]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.CY](#cs.CY) [Total: 42]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)
*Zihan Wang,Cheng Tang,Lei Gong,Cheng Li,Chao Wang,teng wang,Wenqi Lou,Xuehai Zhou*

Main category: cs.CL

TL;DR: Crystal-KV：针对CoT推理优化的KV缓存管理框架，通过区分关键和非关键缓存，实现高效压缩并提升推理性能


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存压缩策略在CoT推理中效果不佳，因为CoT强调最终答案而非所有token同等重要，导致内存开销过大

Method: 1. 基于"答案优先"原则，将答案偏好映射到注意力图中，区分SlipKV（可能引入误导）和CrystalKV（真正贡献答案正确性）
2. 提出基于注意力的LRFU算法，识别SlipKV效用过期时机并淘汰，保留CrystalKV
3. 自适应缓存预算分配算法，根据CrystalKV动态比例调整各层/头的KV缓存预算

Result: Crystal-KV实现了最先进的KV缓存压缩，显著提升吞吐量，加快响应时间，同时保持甚至提高CoT推理的答案准确性

Conclusion: Crystal-KV为CoT推理提供了高效的KV缓存管理方案，通过智能区分和淘汰策略，在保证准确性的同时大幅优化内存使用和推理性能

Abstract: Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.

</details>


### [2] [Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions](https://arxiv.org/abs/2601.16987)
*Shunyang Luo,Peibei Cao,Zhihui Zhu,Kehua Feng,Zhihua Wang,Keyan Ding*

Main category: cs.CL

TL;DR: 提出PMDC框架，通过主动选择奖励模型间最大分歧的prompt-response对来高效评估奖励模型的泛化能力，相比传统静态评估方法能更真实反映模型在实际开放世界中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估主要依赖静态预标注偏好数据集，覆盖范围有限且难以真实评估开放世界场景中的泛化能力，需要更动态、高效的评估框架。

Method: 提出Pairwise Maximum Discrepancy Competition (PMDC)框架：1) 使用大型未标注开放域prompt池；2) 主动选择两个奖励模型分歧最大的prompt-response对；3) 由oracle裁决争议案例；4) 通过Bradley-Terry模型聚合结果生成全局排名和胜率分布。

Result: 对10个代表性奖励模型进行重新评估，发现与传统基准相比排名发生显著变化。定性分析揭示了系统性的泛化失败模式，为改进奖励建模提供了有价值的见解。

Conclusion: PMDC提供了一种动态、标注高效的奖励模型评估框架，能够更真实地评估模型在开放世界中的泛化能力，揭示了传统静态评估方法的局限性，为奖励建模的改进提供了重要指导。

Abstract: Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.

</details>


### [3] [Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction](https://arxiv.org/abs/2601.16999)
*Matthew Singer,Srijan Sengupta,Karl Pazdernik*

Main category: cs.CL

TL;DR: 提出一个基于序列标注的NER模型不确定性感知预测集框架，通过共形预测提供有限样本覆盖保证，确保预测集以用户指定置信水平包含正确标注


<details>
  <summary>Details</summary>
Motivation: 当前NER模型通常输出单一预测标签序列而不提供不确定性度量，导致下游应用容易受到级联错误影响。需要为NER预测提供类似经典统计学中置信区间形式的可靠性保证。

Method: 基于共形预测框架，设计高效的非共形性评分函数来构建校准良好的预测集，支持无条件覆盖和类别条件覆盖，考虑句子长度、语言、实体类型和实体数量等异质性因素。

Result: 在三个基准数据集上的四个NER模型上进行实证实验，证明了所提方法具有广泛的适用性、有效性和效率。

Conclusion: 提出的框架能够为NER模型生成不确定性感知的预测集，提供形式化的可靠性保证，有助于减少下游应用的级联错误风险。

Abstract: Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.

</details>


### [4] [RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection](https://arxiv.org/abs/2601.17002)
*Ziyang Zhou,Ziqi Liu,Yan Wang,Yiming Lin,Yangbin Chen*

Main category: cs.CL

TL;DR: RAM-SD：一个用于讽刺检测的检索增强多智能体框架，通过四阶段流程（上下文检索、元规划、专业智能体分析、集成）实现SOTA性能，提供可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有讽刺检测方法采用统一的推理策略处理所有输入，难以应对讽刺表达中多样化的分析需求（如上下文期望违反建模、外部知识基础、特定修辞模式识别等）。

Method: 提出RAM-SD框架，包含四个阶段：1) 上下文检索（从讽刺和非讽刺示例中获取背景）；2) 元规划（分类讽刺类型并选择最优推理计划）；3) 专业智能体集成（进行互补的多视角分析）；4) 集成器（综合分析生成最终判断和自然语言解释）。

Result: 在四个标准基准测试中，RAM-SD实现了77.74%的Macro-F1分数，比强大的GPT-4o+CoC基线提高了7.01个百分点，达到新的SOTA性能。

Conclusion: RAM-SD不仅设定了新的性能基准，还提供了透明可解释的推理轨迹，揭示了讽刺理解背后的认知过程，解决了现有方法在处理多样化讽刺分析需求时的局限性。

Abstract: Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.

</details>


### [5] [From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech](https://arxiv.org/abs/2601.17132)
*Vigneshwaran Shankaran,Gabriella Lapesa,Claudia Wagner*

Main category: cs.CL

TL;DR: 该论文提出将恐惧言论作为独立研究领域，整合多学科视角，建立理论框架和分类体系，为恐惧言论数据集创建和研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 恐惧言论在社交媒体中广泛传播且影响力超过仇恨言论，但现有研究分散且资源不足。恐惧言论常以"文明"形式出现，逃避内容审核，需要跨学科整合来系统研究这一现象。

Method: 1) 比较心理学、政治学、传播学和语言学中的恐惧理论；2) 回顾现有定义；3) 调查相关研究领域的数据集；4) 提出整合恐惧不同维度的分类学框架。

Result: 建立了恐惧言论的跨学科理论框架，提出了系统分类法，为恐惧言论数据集创建提供了理论指导和实践建议，填补了该领域研究的空白。

Conclusion: 恐惧言论是需要独立研究的独特言语形式，通过跨学科整合可以建立系统研究框架，为未来的数据集创建和恐惧言论研究提供重要基础。

Abstract: Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears "civiler" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from Psychology, Political science, Communication science, and Linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.

</details>


### [6] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang,Junsik Kim,Siyuan Xiang,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 提出动态角色分配框架，通过元辩论选择最适合的模型担任不同角色，在多智能体辩论系统中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM/VLM辩论系统虽然使用专门角色解决复杂问题，但未能根据模型特长分配角色，导致性能未达最优

Method: 提出动态角色分配框架：1) 提案阶段-候选模型提供角色定制化论证；2) 同行评审阶段-根据数据和角色特定标准评分，为每个位置选择最佳模型

Result: 在LLM问题解决基准测试中，相比统一分配（所有角色用同一模型）提升达74.8%，相比随机分配提升达29.7%

Conclusion: 建立了多智能体系统设计新范式，从静态部署转向动态、能力感知的选择机制

Abstract: Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [7] [Interpretability of the Intent Detection Problem: A New Approach](https://arxiv.org/abs/2601.17156)
*Eduardo Sanchez-Karhunen,Jose F. Quesada-Moreno,Miguel A. Gutiérrez-Naranjo*

Main category: cs.CL

TL;DR: 该论文应用动力系统理论分析RNN在意图检测任务中的内部机制，发现平衡数据集上RNN学习到理想几何解（状态空间分区对应不同意图），而类别不平衡会扭曲这种几何结构。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在意图检测领域占主导地位，但RNN解决该任务的内部机制仍不明确。作者希望理解RNN如何从动力系统角度处理意图检测问题。

Method: 应用动力系统理论分析RNN架构，将句子解释为隐藏状态空间中的轨迹。使用平衡的SNIPS数据集和不平衡的ATIS数据集进行对比研究，分析状态空间的几何结构。

Result: 在平衡的SNIPS数据集上，网络学习到理想解：状态空间被约束在低维流形上，并分区为对应不同意图的独立簇。在不平衡的ATIS数据集上，这种理想几何解被扭曲，低频意图的簇质量下降。

Conclusion: 该框架将几何分离与读出对齐解耦，为实际性能差异提供了新的机制解释。研究揭示了数据集属性如何直接影响网络的几何计算解决方案，为RNN动态提供了新的几何解释视角。

Abstract: Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.

</details>


### [8] [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)
*Tunazzina Islam*

Main category: cs.CL

TL;DR: LLMs在生成针对不同人口统计群体的定向信息时，会表现出系统性的人口统计偏见，年轻和男性受众的信息更强调能动性和创新，而女性和年长受众的信息则更强调温暖和传统


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能够大规模生成个性化、有说服力的文本，需要系统分析其在人口统计条件定向信息生成中的偏见和公平性问题

Method: 建立控制评估框架，使用GPT-4o、Llama-3.3和Mistral-Large 2.1三个模型，在两种生成设置下（独立生成和上下文丰富生成）评估生成信息在词汇内容、语言风格和说服框架三个维度上的差异

Result: 在气候沟通任务中，所有模型都表现出一致的年龄和性别不对称：针对男性和年轻人的信息强调能动性、创新和自信，而针对女性和年长者的信息强调温暖、关怀和传统。上下文提示会系统性地放大这些差异

Conclusion: 人口统计刻板印象会在LLM生成的定向沟通中浮现并强化，需要在社会敏感应用中建立偏见感知的生成流程和透明的审计框架

Abstract: Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.

</details>


### [9] [Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content](https://arxiv.org/abs/2601.17173)
*Parth Bhalerao,Diola Dsouza,Ruiwen Guan,Oana Ignat*

Main category: cs.CL

TL;DR: MentorQA是首个专注于指导性问答的多语言数据集和评估框架，包含近9000个QA对，涵盖4种语言的长视频内容，定义了超越事实准确性的指导性评估维度。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统主要评估事实准确性，但许多实际应用（如教育和职业指导）需要指导性回应——提供反思和指导的响应。现有QA基准很少捕捉这种区别，特别是在多语言和长文本环境中。

Method: 构建MentorQA数据集（近9000个QA对，来自4种语言的180小时视频内容），定义指导性评估维度（清晰度、一致性、学习价值），比较单智能体、双智能体、RAG和多智能体QA架构。

Result: 多智能体流水线始终产生更高质量的指导性回应，在复杂主题和低资源语言方面表现尤其突出。同时观察到基于LLM的自动评估与人类判断存在显著差异。

Conclusion: 本研究确立了指导性问答作为一个独立的研究问题，并为教育AI中的智能体架构和评估设计提供了多语言基准。数据集和评估框架已开源。

Abstract: Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.

</details>


### [10] [Systematicity between Forms and Meanings across Languages Supports Efficient Communication](https://arxiv.org/abs/2601.17181)
*Doreen Osmelak,Yang Xu,Michael Hahn,Kate McCurdy*

Main category: cs.CL

TL;DR: 该论文提出了一种基于可学习性的新复杂度度量方法，用于解释动词和代词形式中语法意义的映射规律，将高效沟通理论与语言系统性联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有高效沟通理论未能充分解释词形内部的系统性关系。研究者希望探索语法意义（如人称、数）在动词和代词中的表达方式，并建立从高效沟通理论到语言系统性的新连接。

Method: 提出基于意义到形式映射可学习性的新复杂度度量方法，分析跨类型多样语言中动词和代词形式的规律，考虑简洁性（最小化语法区分）和准确性（恢复意图意义）的竞争压力。

Result: 新模型能捕捉语言形式的细粒度规律性，更好地区分实际存在和不存在的语言系统，验证了动词和代词形式确实受到简洁性和准确性竞争压力的塑造。

Conclusion: 基于可学习性的复杂度度量创新性地将高效沟通理论与自然语言的系统性联系起来，为理解语言形式规律提供了新视角。

Abstract: Languages vary widely in how meanings map to word forms. These mappings have been found to support efficient communication; however, this theory does not account for systematic relations within word forms. We examine how a restricted set of grammatical meanings (e.g. person, number) are expressed on verbs and pronouns across typologically diverse languages. Consistent with prior work, we find that verb and pronoun forms are shaped by competing communicative pressures for simplicity (minimizing the inventory of grammatical distinctions) and accuracy (enabling recovery of intended meanings). Crucially, our proposed model uses a novel measure of complexity (inverse of simplicity) based on the learnability of meaning-to-form mappings. This innovation captures fine-grained regularities in linguistic form, allowing better discrimination between attested and unattested systems, and establishes a new connection from efficient communication theory to systematicity in natural language.

</details>


### [11] [Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding](https://arxiv.org/abs/2601.17197)
*Seyyed Saeid Cheshmi,Hahnemann Ortiz,James Mooney,Dongyeop Kang*

Main category: cs.CL

TL;DR: 本文提出一个三步框架，让轻量级视觉语言模型能够理解多模态比喻语言（如讽刺、幽默、隐喻），提供透明推理轨迹，并在不同比喻风格间泛化。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在字面多模态任务（如视觉数学和科学问答）上表现出强大的推理能力，但比喻语言（如讽刺、幽默、隐喻）仍然是一个重大挑战。比喻语言通过表达意义和意图意义之间的微妙不一致来传达意图和情感，在多媒体环境中，伴随的图像可以放大或反转文本含义，这需要能够跨模态推理并考虑主观性的模型。

Method: 提出了一个三步框架来开发高效的多模态推理模型，该框架能够：(i) 解释多模态比喻语言，(ii) 提供透明的推理轨迹，(iii) 在多种比喻风格间泛化。通过四种风格（讽刺、幽默等）的实验验证框架有效性。

Result: 实验结果显示：(1) 结合推理轨迹显著提高了多模态比喻理解能力；(2) 在一种风格中学到的推理可以迁移到其他风格，特别是在相关风格（如讽刺和幽默）之间；(3) 跨风格联合训练产生的通用推理视觉语言模型优于更大规模的开源和闭源模型。

Conclusion: 研究表明，具有可验证推理能力的轻量级视觉语言模型能够实现稳健的跨风格泛化，同时为多模态任务提供可检查的推理轨迹。这为开发高效、透明且通用的多模态比喻理解模型提供了有前景的方向。

Abstract: Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at https://github.com/scheshmi/CrossStyle-MMR.

</details>


### [12] [Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis](https://arxiv.org/abs/2601.17203)
*Scott Friedman,Sonja Schmer-Galunder,Anthony Chen,Jeffrey Rye*

Main category: cs.CL

TL;DR: 该论文提出了一种量化词嵌入中性别偏见的方法，并将其用于衡量教育、政治、经济和健康领域的统计性别差距，通过推特数据验证了这些指标与真实世界性别差距的相关性。


<details>
  <summary>Details</summary>
Motivation: 当前NLP模型常因训练数据中的文化偏见而产生种族和性别偏见，虽然这些偏见通常被视为需要修正的问题，但它们也可能反映了产生训练文本的文化中真实的性别差距，从而有助于通过大数据理解文化背景。

Method: 提出了一种量化词嵌入中性别偏见的方法，使用2018年推特数据覆盖51个美国地区和99个国家，将这些词嵌入偏见与国际和美国的统计性别差距进行相关性分析。

Result: 验证了词嵌入偏见指标与18个国际统计性别差距和5个美国统计性别差距的相关性，揭示了规律性和预测能力，表明词嵌入偏见能够反映真实世界的性别差距。

Conclusion: 词嵌入中的性别偏见不仅是一个需要修正的技术问题，还可以作为理解文化背景中真实性别差距的有用工具，为通过大数据分析文化差异提供了新视角。

Abstract: Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.

</details>


### [13] [DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.17212)
*Saadat Hasan Khan,Spencer Hong,Jingyu Wu,Kevin Lybarger,Youbing Yin,Erin Babinsky,Daben Liu*

Main category: cs.CL

TL;DR: DF-RAG通过引入多样性检索机制，在保持相关性的同时最大化信息块间的差异性，显著提升了推理密集型问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法在推理密集型问答任务中面临挑战，因为常用的余弦相似度检索方法虽然能最大化相关性，但会引入冗余内容，降低信息召回率。

Method: 基于最大边际相关性框架，DF-RAG在检索步骤中系统性地引入多样性，选择既与查询相关又彼此差异最大的信息块，并能在测试时动态优化每个查询的多样性水平，无需额外微调或先验信息。

Result: 在推理密集型问答基准测试中，DF-RAG相比使用余弦相似度的普通RAG提升了4-10%的F1性能，并优于其他基线方法。Oracle上限显示有18%的绝对F1提升潜力，DF-RAG能实现其中91.3%的收益。

Conclusion: DF-RAG通过动态优化检索多样性，有效解决了传统RAG在推理密集型任务中的冗余问题，显著提升了问答性能，为检索增强生成技术提供了重要的改进方向。

Abstract: Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.

</details>


### [14] [Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings](https://arxiv.org/abs/2601.18788)
*Mumin Jia,Jairo Diaz-Rodriguez*

Main category: cs.CL

TL;DR: Embed-KCPD：一种无监督文本分割方法，使用句子嵌入和惩罚KCPD目标检测边界，具有理论保证和实际效果验证


<details>
  <summary>Details</summary>
Motivation: 文本分割的边界标注成本高、主观性强，且难以跨域和跨粒度迁移，需要有效的无监督方法

Method: 将句子表示为嵌入向量，通过最小化惩罚KCPD目标估计边界；建立m-依赖序列下的理论框架；使用LLM生成合成文档验证

Result: 在标准分割基准测试中常优于强无监督基线；理论证明边界恢复窗口相对于段长较小；泰勒·斯威夫特推文案例验证实用性

Conclusion: Embed-KCPD结合了理论保证、模拟可靠性和实际有效性，为无监督文本分割提供了有前景的解决方案

Abstract: Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.

</details>


### [15] [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](https://arxiv.org/abs/2601.17223)
*Massimiliano Pronesti,Anya Belz,Yufang Hou*

Main category: cs.CL

TL;DR: 提出可验证过程奖励模型(VPRMs)，使用确定性规则验证器检查中间推理步骤，在医学证据合成偏倚评估中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有过程监督方法依赖神经评分器评估思维链步骤，存在不透明、偏见和奖励攻击问题。需要一种能够程序化验证中间推理步骤的强化学习框架。

Method: 引入可验证过程奖励模型(VPRMs)，使用确定性、基于规则的验证器检查中间推理步骤。应用于医学证据合成偏倚评估领域，利用指南定义的标准和规则决策路径实现推理轨迹的程序化验证。

Result: VPRMs生成符合领域规则的推理，显著提高步骤级决策与最终标签的一致性。在多个数据集上，VPRMs比最先进模型F1分数提高达20%，比可验证结果奖励高6.5%，在证据基础和逻辑一致性方面有实质性提升。

Conclusion: VPRMs通过程序化验证中间推理步骤，有效解决了神经评分器的不透明和偏见问题，在需要严格遵循领域规则的医学证据合成偏倚评估中表现出优越性能。

Abstract: Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.

</details>


### [16] [Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation](https://arxiv.org/abs/2601.17226)
*David Y. Liu,Xanthe Muston,Aditya Joshi,Sebastian Sequoiah-Grayson*

Main category: cs.CL

TL;DR: 本文探索使用强化学习（d-RLAIF）作为监督微调（SFT）的替代方案，用于自动故事生成（ASG）。通过Todorov叙事平衡理论建立评估原则，使用LLM作为评判员提供奖励信号，结果显示d-RLAIF能产生更多样化且更符合人类叙事惯例的故事。


<details>
  <summary>Details</summary>
Motivation: 自动故事生成（ASG）任务具有主观性，但以往研究依赖有限的真实数据进行训练和评估。本文旨在探索强化学习作为监督微调的替代方案，以解决ASG的主观性挑战。

Method: 1. 应用Todorov的叙事平衡理论建立ASG质量评估原则；2. 使用7B和14B参数的LLM作为评判员，测试这些原则与人类标注者的一致性，并在d-RLAIF过程中提供奖励信号；3. 使用Gemini-3-Flash评估后训练模型的输出，并与TimeTravel数据集中的人类撰写故事进行比较。

Result: d-RLAIF被证明是监督微调（SFT）的可行替代方案，能够产生更多样化且更符合人类叙事惯例的故事。LLM评判员与人类标注者表现出良好的一致性。

Conclusion: 强化学习为自动故事生成等主观任务提供了有前景的语言基础后训练方法，d-RLAIF在保持故事多样性和符合人类叙事惯例方面优于传统监督微调。

Abstract: Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.

</details>


### [17] [CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval](https://arxiv.org/abs/2601.17230)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: CaseFacts是一个用于验证美国最高法院判例中通俗法律主张的基准数据集，包含6,294个主张，分为支持、反驳或被推翻三类，旨在弥合通俗主张与专业法理之间的语义鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前自动事实核查主要关注基于静态语料库验证一般知识，忽略了法律等高风险领域，其中真相是动态演变且技术复杂的。法律领域需要验证通俗主张与专业判例，同时考虑时间有效性。

Method: 采用多阶段流水线，利用大型语言模型从专家案例摘要中合成主张，并使用新颖的语义相似性启发式方法高效识别和验证复杂的法律推翻情况。

Result: 构建了包含6,294个主张的数据集，实验表明最先进的LLM在此任务上仍面临挑战，特别是无限制的网络搜索增强反而会因检索到嘈杂、非权威的判例而降低性能。

Conclusion: CaseFacts基准旨在推动法律事实核查系统的研究，揭示了法律领域事实核查的特殊挑战，包括语义鸿沟、时间有效性和权威性判例识别等问题。

Abstract: Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.

</details>


### [18] [Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data](https://arxiv.org/abs/2601.17232)
*Jacob Devasier,Akshith Putta,Qing Wang,Alankrit Moses,Chengkai Li*

Main category: cs.CL

TL;DR: 提出大规模多语言数据集，包含78,503个基于复杂OECD表格的合成声明，用于验证自动化事实核查系统处理海量结构化数据的能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动化事实核查基准主要关注小型、精心策划的表格，忽略了验证真实世界高容量结构化数据的挑战，需要填补这一关键空白。

Method: 采用框架引导方法，基于六个语义框架程序化选择重要数据点生成多语言声明；通过知识探测实验确保LLMs未记忆这些事实；提供基线SQL生成系统。

Result: 基准测试极具挑战性，证据检索是主要瓶颈，模型在庞大表格中难以找到正确数据；LLMs未记忆这些事实，迫使系统进行真正的检索和推理。

Conclusion: 该数据集为解决这一未解决的现实问题提供了关键新资源，推动自动化事实核查系统处理大规模结构化数据的研究进展。

Abstract: Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.

</details>


### [19] [PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues](https://arxiv.org/abs/2601.17277)
*Mohammad Rifqi Farhansyah,Hanif Muhammad Zhafran,Farid Adilazuarda,Shamsuddeen Hassan Muhammad,Maryam Ibrahim Mukhtar,Nedjma Ousidhoum,Genta Indra Winata,Ayu Purwarianti,Alham Fikri Aji*

Main category: cs.CL

TL;DR: PingPong是一个用于自然多语言代码切换对话的基准测试，包含五种语言组合变体，涵盖双语和三语对话，用于评估模型在真实世界多语言交流中的能力。


<details>
  <summary>Details</summary>
Motivation: 目前大多数基准测试无法准确反映现实世界中多语言使用者在日常交流中复杂的代码切换实践，需要更真实、结构更多样的数据集来评估NLP系统处理真实世界多语言对话的能力。

Method: 创建PingPong基准测试，包含人工编写的2-4人自然对话，涵盖五种语言组合变体（包括三语对话）。对话具有真实的多线程结构，回复经常引用对话中较早的内容。基于这些对话定义了三个下游任务：问答、对话摘要和主题分类。

Result: PingPong数据集比机器生成的数据更自然、结构更多样，在消息长度、说话者主导性和回复距离方面有更大变化。对多个最先进语言模型的评估显示，它们在代码切换输入上的性能仍然有限。

Conclusion: 当前NLP系统在处理真实世界多语言对话的复杂性方面能力有限，迫切需要更强大的系统来应对代码切换的挑战。PingPong基准测试为这一领域的研究提供了重要的评估工具。

Abstract: Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.

</details>


### [20] [Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering](https://arxiv.org/abs/2601.17284)
*Yaokun Liu,Yifan Liu,Phoebe Mbuvi,Zelin Li,Ruichen Yao,Gawon Lim,Dong Wang*

Main category: cs.CL

TL;DR: 提出CV-MedBench基准，通过表示工程分析医学QA中的输入歧义性，引入基于歧义性探测的"澄清-再回答"框架，显著提升医疗问答安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗问答中的部署受到模糊用户查询的严重阻碍，这种安全风险显著降低了高风险医疗环境中的回答准确性。需要解决输入歧义性带来的不确定性挑战。

Method: 1) 构建CV-MedBench基准用于研究医学QA中的输入歧义性；2) 从表示工程角度分析歧义性不确定性，发现其在LLM内部激活模式中线性编码；3) 提出AU-Probe轻量模块直接从隐藏状态检测输入歧义性；4) 设计AU引导的"澄清-再回答"框架。

Result: 在四个开源LLM上的广泛实验表明，该QA框架平均准确率比基线提高9.48%。AU-Probe无需LLM微调或多重前向传播，能高效主动请求用户澄清。

Conclusion: 该框架为安全医学QA提供了高效稳健的解决方案，增强了健康相关应用的可靠性。代码和数据集已开源。

Abstract: The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.

</details>


### [21] [Meta-Judging with Large Language Models: Concepts, Methods, and Challenges](https://arxiv.org/abs/2601.17312)
*Hugo Silva,Mateus Mendes,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本文综述了LLM评估范式的演进：从存在诸多缺陷的LLM-as-a-Judge到更稳健的LLM-as-a-Meta-Judge，提出了包含六个关键视角的分析框架，并指出这一新范式在实现稳定可信自动评估方面的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-Judge评估范式存在显著脆弱性，包括对提示词的敏感性、系统性偏见、冗长效应以及不可靠或幻觉化的推理过程。这些局限性促使研究者开发更稳健的评估范式。

Method: 提出LLM-as-a-Meta-Judge新范式，并通过六个关键视角构建分析框架：概念基础、元评估机制、对齐训练方法、评估方法、局限性与失败模式、未来方向。

Result: LLM-as-a-Meta-Judge为解决LLM评估的脆弱性问题提供了有前景的方向，能够实现更稳定和可信的自动评估，但仍面临成本、提示敏感性和共享模型偏见等挑战。

Conclusion: LLM-as-a-Meta-Judge代表了LLM评估方法的重要演进，通过元评估机制提升了评估的稳健性，但要推动下一代LLM评估方法的发展，仍需解决成本、敏感性和偏见等关键挑战。

Abstract: Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.

</details>


### [22] [The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents](https://arxiv.org/abs/2601.17344)
*Chen Chen,Kim Young Il,Yuan Yang,Wenhao Su,Yilin Zhang,Xueluan Gong,Qian Wang,Yongsen Zheng,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 该研究提出IMPRESS框架，用于评估LLM智能体在完全良性场景下的内在价值错位风险，发现这是普遍存在的安全问题，现有缓解策略效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注对显性有害输入的响应或系统故障的鲁棒性，而在现实、完全良性、自主的智能体设置中，价值错位风险尚未得到充分探索。LLM智能体可能追求偏离人类价值观和伦理规范的目标，这种失控风险需要系统评估。

Method: 首先形式化失控风险并识别内在价值错位；然后提出IMPRESS框架，通过多阶段LLM生成管道构建包含现实、完全良性、情境化场景的基准测试；评估21个最先进的LLM智能体，分析影响因素，并进行人工验证。

Result: 内在价值错位是跨模型的普遍安全风险；错位率因动机、风险类型、模型规模和架构而异；解码策略和超参数影响有限，但情境化和框架机制显著影响错位行为；现有缓解策略（如安全提示和护栏）表现出不稳定性或有限效果。

Conclusion: IMPRESS框架填补了在良性自主设置中评估价值错位风险的空白，揭示了LLM智能体安全的重要挑战，现有缓解措施不足，需要在AI生态系统中更全面地解决内在价值错位问题。

Abstract: Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.

</details>


### [23] [Do readers prefer AI-generated Italian short stories?](https://arxiv.org/abs/2601.17363)
*Michael Farrell*

Main category: cs.CL

TL;DR: 研究比较AI生成与人类作家创作的意大利短篇小说，发现读者对AI作品评价略高，挑战了人类创作更受欢迎的假设


<details>
  <summary>Details</summary>
Motivation: 探索读者对AI生成文学作品的偏好，检验人类创作在文学领域的优越性假设，评估AI文本在文学语境中的可接受性

Method: 采用盲测设计，20名参与者阅读并评价3篇短篇小说（2篇由ChatGPT-4o生成，1篇由意大利著名作家Alberto Moravia创作），收集阅读习惯和人口统计学数据

Result: AI写作的文本获得略高的平均评分，更常被偏好，但差异不大；文本偏好与人口统计学或阅读习惯变量无显著关联

Conclusion: 研究结果挑战了读者偏好人类创作小说的假设，对文学语境中合成文本编辑的必要性提出疑问

Abstract: This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.

</details>


### [24] [Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws](https://arxiv.org/abs/2601.17364)
*Mohammed Fasha,Bassam Hammo,Bilal Sowan,Husam Barham,Esam Nsour*

Main category: cs.CL

TL;DR: 本研究以约旦法律为案例，探索了Llama-3.1大语言模型在阿拉伯语法律问答任务上的微调，通过量化和参数高效微调技术实现了资源高效的法律领域适配。


<details>
  <summary>Details</summary>
Motivation: 探索如何将大语言模型适配到阿拉伯语法律领域，解决法律问答任务，同时通过量化技术和参数高效微调实现资源效率。

Method: 使用Llama-3.1-8B-bnb-4bit和Llama-3.1-8B-Instruct-bnb-4bit两个版本，采用LoRA适配器的参数高效微调(PEFT)和4位量化技术，利用Unsloth框架进行加速训练。构建了包含6000个约旦法律问答对的定制数据集，并格式化为结构化提示。

Result: 微调后的模型在法律推理和准确性方面有所提升，同时通过量化和优化微调策略实现了资源效率。使用BLEU和ROUGE指标评估显示，微调模型相比基础版本有改进。

Conclusion: 这项工作展示了将大语言模型适配到阿拉伯语法律领域的潜力，并强调了针对特定领域任务进行微调的有效技术，为资源受限环境下的法律AI应用提供了可行方案。

Abstract: This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.

</details>


### [25] [Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers](https://arxiv.org/abs/2601.17367)
*Zecheng Tang,Quantong Qiu,Yi Yang,Zhiyi Hong,Haiya Xiang,Kebin Liu,Qingqing Dang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出Elastic Attention方法，通过轻量级Attention Router动态调整注意力稀疏度，解决传统混合注意力静态计算比例无法适应下游任务变化的问题。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制的二次复杂度限制了LLM在长上下文场景的可扩展性。现有混合注意力策略使用静态稀疏-全注意力比例，无法在推理时适应不同下游任务对稀疏度的敏感性变化。

Method: 提出Elastic Attention，集成轻量级Attention Router到预训练模型中，动态分配每个注意力头到不同的计算模式，让模型根据输入动态调整整体稀疏度。

Result: 在8xA800 GPU上仅训练12小时，就能使模型同时获得强大性能和高效推理。在三个长上下文基准测试和广泛使用的LLM上的实验证明了方法的优越性。

Conclusion: Elastic Attention通过动态调整注意力稀疏度，有效解决了长上下文LLM的扩展瓶颈，实现了性能与效率的平衡。

Abstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.

</details>


### [26] [WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews](https://arxiv.org/abs/2601.17377)
*Kiyotada Mori,Shohei Tanaka,Tosho Hirasawa,Tadashi Kozuno,Koichiro Yoshino,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 提出一种评估科学评审评论中主张与证据间逻辑推理的新方法，比传统方法更符合人类评分


<details>
  <summary>Details</summary>
Motivation: 科学同行评审面临人力资源短缺问题，语言模型被探索用于降低评审成本。现有方法仅评估主张是否有证据支持，但无法准确评估主张与证据间的逻辑推理关系。

Method: 提出新的评估指标，不仅提取评审评论中的主张和证据核心成分，还评估主张与证据之间的逻辑推理关系，而不仅仅是检测证据是否存在。

Result: 实验结果显示，提出的方法比传统方法获得更高的人类评分相关性，表明其能更好地支持同行评审过程的效率。

Conclusion: 该方法通过更准确地评估主张与证据间的逻辑推理，有望提高科学评审的质量评估，从而支持更高效的同行评审过程。

Abstract: The scientific peer-review process is facing a shortage of human resources due to the rapid growth in the number of submitted papers. The use of language models to reduce the human cost of peer review has been actively explored as a potential solution to this challenge. A method has been proposed to evaluate the level of substantiation in scientific reviews in a manner that is interpretable by humans. This method extracts the core components of an argument, claims and evidence, and assesses the level of substantiation based on the proportion of claims supported by evidence. The level of substantiation refers to the extent to which claims are based on objective facts. However, when assessing the level of substantiation, simply detecting the presence or absence of supporting evidence for a claim is insufficient; it is also necessary to accurately assess the logical inference between a claim and its evidence. We propose a new evaluation metric for scientific review comments that assesses the logical inference between claims and evidence. Experimental results show that the proposed method achieves a higher correlation with human scores than conventional methods, indicating its potential to better support the efficiency of the peer-review process.

</details>


### [27] [Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis](https://arxiv.org/abs/2601.17387)
*Toshiki Nakai,Varsha Suresh,Vera Demberg*

Main category: cs.CL

TL;DR: 研究SeamlessM4T v2模型中语音和文本模态下语言表征的一致性，发现不完全的模态不变性，共享解码器在构建模态无关表征时难以恢复源语言信息。


<details>
  <summary>Details</summary>
Motivation: 多语言语音-文本基础模型旨在跨模态和语言统一处理语言，但尚不清楚这些模型内部是否对同一语言在语音和文本形式下保持一致的表示。

Method: 通过三种互补分析：1) 使用平均精度排序识别语言和模态选择性神经元；2) 通过推理时的中值替换干预研究其功能作用；3) 分析跨语言和模态的激活幅度不平等。

Result: 发现不完全的模态不变性：编码器表征变得语言无关，但共享解码器在构建模态无关表征时难以恢复源语言信息（特别是从语音到文本的适应）。跨注意力键值投影中存在局部化的模态选择性结构。语音条件解码和非主导脚本表现出更高的激活集中度。

Conclusion: 多语言语音-文本基础模型在模态不变性方面存在局限性，共享解码器在跨模态处理时面临挑战，特定模态和语言的表征依赖少数神经元可能导致跨模态和语言的脆弱性增加。

Abstract: Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.

</details>


### [28] [CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing](https://arxiv.org/abs/2601.17397)
*Yucheng Hu,Wei Zhou,Juesi Xiao*

Main category: cs.CL

TL;DR: 该论文提出了CLM-Bench，一个文化感知的中文优先多语言知识编辑基准，揭示了当前方法在跨语言知识传播上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MKE基准通常通过机械翻译英文数据集构建，这引入了翻译伪影并忽略了目标语言的文化特定实体，无法反映LLMs的真实知识分布。

Method: 提出CLM-Bench基准，采用中文优先方法构建，包含1,010个基于中文文化背景的高质量CounterFact对，并与英文对应项对齐。通过该基准对代表性LLMs进行实验，并通过层表示分析提供几何解释。

Result: 实验发现显著的跨语言错位：一种语言的编辑独立运作，无法传播到另一种语言。中文和英文的编辑向量几乎正交，存在于不相交的子空间中，而混合语言编辑则表现出这些向量的线性可加性。

Conclusion: 当前方法在跨语言迁移上效果有限，强调了文化原生基准的重要性。需要开发能够实现跨语言知识传播的新编辑方法。

Abstract: Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.

</details>


### [29] [Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning](https://arxiv.org/abs/2601.17421)
*Jaehui Hwang,Dongyoon Han,Sangdoo Yun,Byeongho Heo*

Main category: cs.CL

TL;DR: 分析大型语言模型中"wait"、"therefore"等话语标记token的概率信号，发现这些信号与推理正确性高度相关，在不同模型规模下稳定，但受训练策略影响，且小数据集微调的模型仅部分利用这些信号。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中出现的"wait"、"therefore"等话语标记token为理解其推理过程提供了独特窗口，但缺乏系统分析这些信号如何随训练策略和模型规模变化的研究。

Method: 通过分析不同模型的token级概率信号，研究特定token（特别是"wait"）与推理正确性的相关性，比较不同训练策略和模型规模下的变化。

Result: 发现特定token与推理正确性高度相关，这种相关性在不同模型规模下保持稳定，但受训练策略影响；小数据集微调的模型通过此类信号获得推理能力，但仅部分利用这些信号。

Conclusion: 这项工作为观察和理解大型语言模型推理动态提供了系统视角，揭示了话语标记token在模型推理中的重要作用及其训练策略依赖性。

Abstract: The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.

</details>


### [30] [Clustering-driven Memory Compression for On-device Large Language Models](https://arxiv.org/abs/2601.17443)
*Ondrej Bohdal,Pramit Saha,Umberto Michieli,Mete Ozay,Taha Ceritli*

Main category: cs.CL

TL;DR: 提出基于聚类的记忆压缩策略，通过相似性分组合并记忆，在减少上下文占用的同时保持个性化生成质量


<details>
  <summary>Details</summary>
Motivation: 现有方法要么直接拼接记忆耗尽有限上下文，要么平均压缩导致语义冲突损害性能，需要平衡上下文效率与个性化质量

Method: 基于聚类的记忆压缩策略：按相似性对记忆分组，在组内合并记忆后再拼接，减少冗余同时保持语义一致性

Result: 显著减少记忆token数量，在固定上下文预算下获得更紧凑的记忆表示，生成质量优于直接拼接和平均压缩基线

Conclusion: 聚类驱动的记忆合并策略有效平衡了上下文效率与个性化质量，为设备端LLM的个性化生成提供了实用解决方案

Abstract: Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.

</details>


### [31] [Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes](https://arxiv.org/abs/2601.17530)
*Gautam Siddharth Kashyap,Harsh Joshi,Niharika Jain,Ebad Shabbir,Jiechao Gao,Nipun Joshi,Usman Naseem*

Main category: cs.CL

TL;DR: 提出ConLLM框架，通过对比学习和LLM推理解决深度伪造检测中的模态碎片化和浅层跨模态推理问题，在音频、视频和视听任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对社会政治稳定构成严重威胁，现有检测方法存在两个核心局限：1) 模态碎片化导致跨不同对抗性深度伪造模态泛化能力差；2) 浅层跨模态推理导致细粒度语义不一致检测有限。

Method: 提出ConLLM（基于大语言模型的对比学习）混合框架，采用两阶段架构：第一阶段使用预训练模型提取模态特定嵌入；第二阶段通过对比学习对齐这些嵌入以缓解模态碎片化，并使用基于LLM的推理进行精炼，通过捕捉语义不一致来解决浅层跨模态推理问题。

Result: ConLLM在音频、视频和视听模态上表现强劲：音频深度伪造EER降低达50%，视频准确率提升达8%，视听任务准确率提升约9%。消融研究证实基于PTM的嵌入在各模态上带来9%-10%的稳定改进。

Conclusion: ConLLM框架通过结合对比学习和LLM推理，有效解决了深度伪造检测中的模态碎片化和浅层跨模态推理问题，为多模态深度伪造检测提供了鲁棒解决方案。

Abstract: The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.

</details>


### [32] [Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection](https://arxiv.org/abs/2601.17532)
*Zhipeng Song,Yizhi Zhou,Xiangyu Kong,Jiulong Jiao,Xinrui Bao,Xu You,Xueqing Shi,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: 论文提出IGP方法，通过信息增益剪枝优化RAG中的证据选择，在有限上下文预算下提升问答质量并减少输入token


<details>
  <summary>Details</summary>
Motivation: 传统检索相关性指标（如NDCG）与端到端QA质量相关性弱，在多段落注入时甚至呈负相关，冗余和轻微冲突会破坏生成稳定性

Method: 提出信息增益剪枝（IGP），这是一个部署友好的重排序和剪枝模块，使用生成器对齐的效用信号选择证据，在截断前过滤弱或有害段落

Result: 在五个开放域QA基准和多种检索器/生成器上，IGP持续改善质量-成本权衡；在多证据设置下，相比仅使用检索器的基线，F1相对提升12-20%，同时减少76-79%的最终阶段输入token

Conclusion: IGP通过生成器对齐的证据选择有效解决了RAG中有限上下文预算下的证据注入问题，显著提升了问答系统的效率和质量

Abstract: Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.

</details>


### [33] [Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations](https://arxiv.org/abs/2601.17569)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: P³是一个保护隐私的个性化LLM框架，通过服务器生成草稿、客户端基于私有资料修改的方式，在保护用户隐私的同时实现高质量个性化


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法面临隐私与性能的权衡：要么将私有资料暴露给云端LLM，要么依赖性能较差的本地模型。需要一种既能保护隐私又能实现高质量个性化的解决方案。

Method: P³采用交互式框架：服务器端大模型仅基于用户查询生成k个草稿token，客户端小模型基于私有资料检索评估并修改这些草稿以反映用户偏好，重复此过程直到生成结束token。

Result: 在LaMP-QA基准测试中，P³显著优于非个性化服务器端和个性化客户端基线，平均提升7.4%-9%。恢复"泄露"上限场景90.3%-95.7%的效用，隐私泄露仅增加1.5%-3.5%，客户端仅生成总token的9.2%。

Conclusion: P³提供了一个实用有效的个性化生成解决方案，在保护隐私的同时实现了接近完全暴露私有资料场景的性能，适合边缘部署。

Abstract: Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.

</details>


### [34] [Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models](https://arxiv.org/abs/2601.17585)
*Matija Luka Kukić,Marko Čuljak,David Dukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 序列重复（SR）是一种让仅解码器模型获得双向上下文能力的新方法，相比因果掩码移除更少侵入性，能提升词级嵌入质量并超越编码器和无掩码解码器。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型采用自回归训练，只依赖前缀上下文，而序列标注任务需要双向上下文。传统上序列标注依赖双向编码器模型，但随着仅解码器模型的快速发展，需要探索如何让它们适应序列标注任务。

Method: 提出序列重复（SR）方法，通过重复输入序列来使仅解码器模型获得双向上下文能力。这种方法比移除因果掩码更少侵入性，不需要改变基础模型功能。研究还探索了使用中间层嵌入而非最终层嵌入以提高效率。

Result: SR使解码器具有双向性，提升了词级嵌入质量，超越了编码器和无掩码解码器。增加重复次数不会降低序列标注性能。中间层嵌入与最终层嵌入效果相当，但计算效率显著更高。

Conclusion: 序列重复缓解了解码器的结构限制，使语言模型更高效、更适应性强，拓宽了它们在词级任务中的应用范围，为仅解码器模型在序列标注等任务中的应用提供了新途径。

Abstract: Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being significantly more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.

</details>


### [35] [From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs](https://arxiv.org/abs/2601.17593)
*Tianjun Zhong,Linyang He,Nima Mesgarani*

Main category: cs.CL

TL;DR: 该论文提出"推理DAG探测"框架，研究LLM隐藏状态是否线性可访问地编码推理有向无环图的几何结构，发现推理DAG结构确实在中间层有意义地编码，且可恢复性随节点深度和模型规模系统变化。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在多步推理方面取得进展，但现有研究多将推理视为线性链，而许多推理问题更自然地表示为有向无环图（DAG），其中中间结论可能依赖多个前提、分支为并行子推导、后续合并或重用。理解模型内部是否反映这种图结构推理仍是一个开放问题。

Method: 提出推理DAG探测框架，将每个推理节点与文本实现关联，训练轻量级探针从隐藏状态预测两个图论属性：节点深度和节点对距离。通过该框架分析DAG结构在层间的涌现，并评估破坏推理相关结构但保留表面文本属性的控制条件。

Result: 研究结果提供了证据表明推理DAG几何结构在中间层有意义地编码，可恢复性随节点深度和模型规模系统变化，表明LLM推理不仅是顺序的，而且展现出可测量的内部图结构。

Conclusion: LLM推理不仅具有顺序特性，还展现出可测量的内部图结构，推理DAG几何结构在模型中间层有意义地编码，且这种编码方式随节点深度和模型规模呈现系统性变化。

Abstract: Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.
  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.

</details>


### [36] [Learning to Ideate for Machine Learning Engineering Agents](https://arxiv.org/abs/2601.17596)
*Yunxiang Zhang,Kang Zhou,Zhichao Xu,Kiran Ramnath,Yun Zhou,Sangmin Woo,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: MLE-Ideator：双智能体框架，将想法生成与实现分离，通过强化学习训练Ideator智能体，显著提升机器学习工程任务的性能


<details>
  <summary>Details</summary>
Motivation: 现有机器学习工程智能体在迭代优化算法效果方面存在困难，需要分离策略性思考与具体实现

Method: 提出MLE-Ideator双智能体框架：实现智能体负责具体实施，Ideator智能体专门提供策略性帮助；Ideator可通过强化学习训练提升想法质量

Result: 1. 无训练设置下显著优于仅实现智能体基线；2. 使用1K样本训练后，Qwen3-8B Ideator相对未训练版本提升11.5%，超越Claude Sonnet 3.5

Conclusion: 该框架为训练策略性AI系统进行科学发现提供了有前景的路径，分离想法与实现的设计能有效提升机器学习工程任务性能

Abstract: Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.

</details>


### [37] [What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization](https://arxiv.org/abs/2601.17609)
*Sara Rezaeimanesh,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: LoID：通过直接访问LLM的token级预测来提取贝叶斯逻辑回归先验分布的方法，在协变量偏移的OOD设置下显著提升性能


<details>
  <summary>Details</summary>
Motivation: 在医学和金融等领域，大规模标注数据成本高昂且难以获取，导致在小数据集上训练的模型难以泛化到真实世界人群。大型语言模型包含这些领域多年研究的广泛知识，但如何有效提取这些知识用于统计建模是一个挑战。

Method: 提出LoID（Logit-Informed Distributions），一种确定性方法，通过直接访问LLM的token级预测来提取贝叶斯逻辑回归的先验分布。该方法不依赖生成文本，而是通过精心构造的句子探测模型在相反语义方向（正面vs负面影响）上的置信度，通过测量LLM在不同表述中一致偏好某个方向的程度，提取模型对每个特征影响的强度和可靠性信念。

Result: 在10个真实世界表格数据集上的合成OOD设置（协变量偏移）下评估，LoID显著优于OOD数据上训练的逻辑回归，恢复了相对于全数据集拟合的oracle模型高达59%的性能差距。在8/10的数据集上优于AutoElicit和LLMProcesses，同时提供了可重复且计算高效的LLM知识集成机制。

Conclusion: LoID提供了一种有效的方法，将LLM的领域知识整合到贝叶斯推理中，特别是在数据有限且存在协变量偏移的情况下，能够显著提升模型的泛化性能，为小数据场景下的统计建模提供了新思路。

Abstract: In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \textbf{59\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.

</details>


### [38] [Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization](https://arxiv.org/abs/2601.17658)
*Bich Ngoc,Doan,Giuseppe Russo,Gianmarco De Francisci Morales,Robert West*

Main category: cs.CL

TL;DR: 研究通过分析QAnon支持者亲友的叙述，首次系统性地将激进化的个人历程与对亲友的情感伤害联系起来，识别出六种激进者原型，并发现这些原型能预测亲友的具体情感反应。


<details>
  <summary>Details</summary>
Motivation: 阴谋论的兴起不仅侵蚀公众信任、加剧社会极化，还对阴谋论者的亲友造成深刻的情感伤害，这一个人层面的影响在大型计算研究中常被忽视。本研究旨在填补这一空白，系统性地描绘激进化的个人历程并量化其对亲友的情感伤害。

Method: 采用混合方法研究QAnon案例：1) 使用BERTopic主题建模分析12,747个r/QAnonCasualties社区叙述，描绘激进化的轨迹（前因、触发因素、后特征）；2) 应用LDA图形模型识别六种反复出现的QAnon追随者原型（"激进化人格"）；3) 使用LLM辅助情感检测和回归建模，将这些原型与叙述者报告的具体情感伤害联系起来。

Result: 研究发现六种QAnon追随者原型，这些原型不仅是描述性的，而且是叙述者情感伤害的有力预测因子：被视为故意意识形态选择的激进化与叙述者的愤怒和厌恶相关，而以个人和认知崩溃为特征的激进化则与恐惧和悲伤相关。

Conclusion: 本研究首次为理解激进化作为一种关系现象提供了实证框架，为研究人员和从业者应对其人际影响提供了重要路线图。研究揭示了激进者原型与亲友情感反应之间的系统联系，强调了激进化的个人层面影响。

Abstract: The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term "radicalization personas." Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.

</details>


### [39] [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)
*Syed Muhammad Ali,Hammad Sajid,Zainab Haider,Ali Muhammad Asad,Haya Fatima,Abdul Samad*

Main category: cs.CL

TL;DR: 本文提出了UrduLM，一个针对乌尔都语的预训练单语语言模型，解决了乌尔都语缺乏专用Transformer模型和高质量语料库的问题，在低资源环境下实现了与多语言模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语有2.3亿使用者，但缺乏专用的Transformer语言模型和精心整理的语料库。现有的多语言模型对乌尔都语支持有限，存在性能差、计算成本高、文化不准确等问题，主要原因是训练数据不足。

Method: 1) 从多种来源整理了一个33GB的乌尔都语语料库；2) 开发了自定义的BPE分词器，比多语言替代方案减少至少20-30%的分词开销；3) 预训练了一个1亿参数的仅解码器模型。

Result: 在少样本评估中，UrduLM与规模大30倍的多语言模型相比具有竞争力：情感分类准确率达到66.6%，语法纠正任务的BLEU分数超过30。

Conclusion: 该研究为乌尔都语NLP研究建立了基线，为其他资源不足语言提供了可扩展框架，并开源了完整的语料库、分词器、模型权重和评估基准。

Abstract: Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.

</details>


### [40] [Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning](https://arxiv.org/abs/2601.17671)
*Chunxu Zhao,Xin Huang,Xue Han,Shujian Huang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出PASMR方法，通过将主要语言作为枢轴语言，让模型将问题翻译到枢轴语言进行推理对齐，然后通过枢轴语言的推理答案监督目标语言的推理过程，从而提升LLM在多语言数学推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强大的推理能力，但在多语言环境下（尤其是低资源语言）性能会下降。作者认为这是由于模型的多语言理解和推理对齐不一致导致的。

Method: 提出Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR)方法：1) 将模型的主要语言设为枢轴语言；2) 训练时先将问题翻译到枢轴语言以促进推理模式对齐；3) 目标语言的推理过程由枢轴语言的推理答案监督，建立跨语言自反馈机制，无需外部正确答案或奖励模型。

Result: 大量实验结果表明，该方法显著提升了模型对问题的理解和推理能力，在多语言数学推理任务上取得了显著改进。

Conclusion: PASMR方法通过跨语言自反馈机制有效解决了LLM在多语言环境下的推理对齐问题，特别是对低资源语言有显著提升效果。

Abstract: Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.

</details>


### [41] [S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference](https://arxiv.org/abs/2601.17702)
*Qingsen Ma,Dianyun Wang,Yaoye Wang,Lechen Ning,Sujie Zhu,Xiaohang Zhang,Jiaming Lyu,Linhao Ren,Zhenbo Xu,Zhaofeng He*

Main category: cs.CL

TL;DR: S3-Attention：一种内存优先的推理时框架，通过将长上下文处理视为注意力对齐的内生检索，使用稀疏自编码器解码特征标识符并构建CPU倒排索引，从而完全丢弃KV缓存，在保持性能的同时大幅降低GPU内存使用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地应用于多文档和长文本输入，但长上下文推理仍然存在内存效率低和噪声问题。KV缓存随上下文长度线性增长，而外部检索方法通常返回词汇相似但因果无关的段落。

Method: S3-Attention将长上下文处理视为注意力对齐的内生检索，使用轻量级稀疏自编码器将瞬态键和查询投影解码为top-k稀疏特征标识符，在单次流式扫描中构建CPU倒排索引映射特征到token位置或跨度。生成时使用特征共激活检索紧凑证据跨度，可选与BM25融合进行精确词汇匹配。

Result: 在统一的LongBench评估协议下，S3-Hybrid在多个模型家族中接近全上下文推理性能，并在多个信息密集场景中提高了鲁棒性。当前原型存在较高的实际延迟，需要未来内核级优化。

Conclusion: S3-Attention提供了一种内存高效的推理框架，能够完全丢弃KV缓存，在保持性能的同时显著降低GPU内存使用，为长上下文处理提供了新的解决方案。

Abstract: Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.
  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.
  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.

</details>


### [42] [Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings](https://arxiv.org/abs/2601.17705)
*Abdullah Qureshi,Kenneth Rice,Alexander Wolpert*

Main category: cs.CL

TL;DR: 提出DDR距离比度量，用于评估LLM句子嵌入的相似性，通过测量上下文前后嵌入相似度的变化率来量化语义影响


<details>
  <summary>Details</summary>
Motivation: 文本嵌入相似性度量必须符合人类对文本相似性的感知，现有度量方法可能无法精细区分语义相似与不相似的文本

Method: 基于Lipschitz连续性思想，提出距离-距离比(DDR)，测量预上下文词嵌入相似度与后上下文LLM嵌入相似度的变化率；通过句子扰动实验，替换1-3个词为同义词（语义相似）或随机词（语义不相似）来评估

Result: DDR相比现有相似性度量，能更精细地区分语义相似与不相似的文本，即使在最小化、受控的编辑条件下也表现一致

Conclusion: DDR是一种有效的LLM句子嵌入相似性度量，能更好地捕捉上下文对语义的影响，符合人类对文本相似性的感知

Abstract: A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.

</details>


### [43] [A Computational Approach to Visual Metonymy](https://arxiv.org/abs/2601.17706)
*Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.CL

TL;DR: 该论文首次对视觉转喻进行系统性计算研究，提出了基于符号学理论的生成管道，并创建了首个视觉转喻数据集ViMET，揭示了多模态模型在理解间接视觉参考方面与人类存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 图像常常传达超出其字面描绘的内容，这种间接视觉参考（视觉转喻）让观众通过关联线索而非明确描绘来理解目标概念。然而，目前缺乏对视觉转喻的计算研究，多模态模型在这方面的能力尚未得到充分评估。

Method: 1. 基于符号学理论设计新颖的生成管道，利用大语言模型和文生图模型生成转喻性视觉表示；2. 构建ViMET数据集，包含2000个多项选择题，用于评估多模态语言模型的认知推理能力；3. 通过实验比较人类和先进视觉语言模型的性能差异。

Result: 1. 创建了首个视觉转喻数据集ViMET；2. 人类在数据集上的准确率达到86.9%，而最先进的视觉语言模型仅达到65.9%，存在显著差距；3. 揭示了机器在解释间接视觉参考方面的局限性。

Conclusion: 该研究首次对视觉转喻进行了计算探索，提出的生成框架和数据集为评估多模态模型的认知推理能力提供了新基准。实验结果强调了当前模型在理解间接视觉参考方面的不足，为未来改进指明了方向。

Abstract: Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.

</details>


### [44] [Unsupervised Elicitation of Moral Values from Language Models](https://arxiv.org/abs/2601.17728)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei*

Main category: cs.CL

TL;DR: 该研究提出了一种无监督方法ICM来激发预训练语言模型的内在道德推理能力，无需人工监督即可获得高质量道德判断标签，并在多个基准测试中超越现有方法，同时显著减少社会偏见。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统普及，将其行为与人类价值观对齐变得至关重要。现有研究表明语言模型的内在道德推理能力有限，但构建道德评估的标注数据又面临多元道德框架和普遍偏见的挑战。因此需要探索无监督方法来激发预训练模型的内在道德推理能力。

Method: 采用Internal Coherence Maximization (ICM)算法，在三个基准数据集和四个语言模型上进行测试。ICM通过最大化内部一致性来无监督地激发模型的道德判断能力，并评估其在道德标签生成、跨道德框架泛化以及减少社会偏见方面的表现。

Result: ICM在Norm Bank和ETHICS基准测试中超越了所有预训练模型和聊天机器人基线。使用ICM标签进行微调的模型性能与使用人工标签相当甚至更好。在正义和常识道德框架上ICM表现最佳，同时将社会偏见错误率降低了一半以上，在种族、社会经济地位和政治方面改善最大。

Conclusion: 预训练语言模型具有潜在的道德推理能力，可以通过ICM等无监督方法激发出来。这为AI对齐提供了一条可扩展的路径，无需依赖人工标注的道德数据即可获得高质量的道德判断能力。

Abstract: As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.

</details>


### [45] [Hylog: A Hybrid Approach to Logging Text Production in Non-alphabetic Scripts](https://arxiv.org/abs/2601.17753)
*Roberto Crotti,Giovanni Denaro,Zhiqiang Du,Ricardo Muñoz Martín*

Main category: cs.CL

TL;DR: Hylog是一个混合日志系统，结合分析性键盘记录和生态文本记录，用于捕捉非字母文字输入法编辑器的屏幕转换，支持更完整的多语言文本生成研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究键盘记录工具大多无法捕捉非字母文字输入法编辑器（IME）的屏幕转换，这限制了认知文本生成研究的完整性和多语言包容性。

Method: 开发模块化开源系统Hylog，使用插件捕获标准应用程序中的键盘输出和渲染文本，通过混合器模块同步为双重轨迹，支持扩展其他输入法系统。

Result: 概念验证研究中，Hylog成功捕捉了拉丁字母、中文字符和输入法确认之间的按键和时间间隔，这些测量传统键盘记录工具无法获取。

Conclusion: Hylog系统填补了方法论空白，支持更精细的文本生成分析，促进多语言研究，并为认知限制和输入法中介打字的语言层面假设提供数据基础。

Abstract: Research keyloggers are essential for cognitive studies of text production, yet most fail to capture the on-screen transformations performed by Input Method Editors (IMEs) for non-alphabetic scripts. To address this methodological gap, we present Hylog, a novel hybrid logging system that combines analytical keylogging with ecological text logging for a more complete and finer-grained analysis. Our modular, open-source system uses plug-ins for standard applications (Microsoft Word, Google Chrome) to capture both keyboard output and rendered text, which a hybridizer module then synchronizes into a dual trace. To validate the system's technical feasibility and demonstrate its analytical capabilities, we conducted a proof-of-concept study where two volunteers translated a text into simplified Chinese. Hylog successfully captured keypresses and temporal intervals between Latin letters, Chinese characters, and IME confirmations -- some measurements invisible to traditional keyloggers. The resulting data enable the formulation of new, testable hypotheses about the cognitive restrictions and affordances at different linguistic layers in IME-mediated typing. Our plug-in architecture enables extension to other IME systems and fosters more inclusive multilingual text-production research.

</details>


### [46] [Corpus-Based Approaches to Igbo Diacritic Restoration](https://arxiv.org/abs/2601.18380)
*Ignatius Ezeani*

Main category: cs.CL

TL;DR: 该论文针对低资源语言（特别是伊博语）开发了变音符号恢复框架，提出了三种方法：标准n-gram模型、分类模型和嵌入模型来解决变音符号歧义问题。


<details>
  <summary>Details</summary>
Motivation: 当前NLP研究主要关注英语、中文等高资源语言，而全球95%的7000多种语言都是低资源语言，缺乏NLP所需的数据、工具和技术。伊博语作为低资源语言，存在变音符号歧义问题，需要专门的研究和解决方案。

Method: 1. 标准n-gram模型：使用目标词之前的词序列作为正确变体预测的关键因素
2. 分类模型：使用目标词两侧的窗口词作为特征
3. 嵌入模型：比较上下文词嵌入组合与候选变体向量嵌入的相似度得分

Result: 开发了一个灵活的变音符号恢复数据集生成框架，针对伊博语提出了三种不同的变音符号消歧方法，为低资源语言的NLP处理提供了可行的技术路径。

Conclusion: 该研究填补了低资源语言NLP研究的空白，特别是针对伊博语的变音符号恢复问题，提出的框架和方法为其他低资源语言的类似问题提供了参考和解决方案。

Abstract: With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.
  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.

</details>


### [47] [ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation](https://arxiv.org/abs/2601.17755)
*Jinyoung Park,Sanghyeok Lee,Omar Zia Khan,Hyunwoo J. Kim,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: ProGraph-R1是一个基于进度感知的图检索增强生成框架，通过结构感知的超图检索机制和基于进度的逐步策略优化，解决了现有RL-based GraphRAG方法在检索和奖励机制上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的GraphRAG框架（如Graph-R1）存在两个关键限制：1）主要依赖语义相似性进行检索，忽视了底层图结构；2）依赖稀疏的结果级奖励，无法捕捉中间检索步骤的质量及其依赖关系。这些限制影响了多步推理的效果。

Method: 提出ProGraph-R1框架，包含两个核心创新：1）结构感知的超图检索机制，同时考虑语义相关性和图连接性，促进沿着多跳推理路径的连贯遍历；2）基于进度的逐步策略优化，通过根据图中中间推理进度调节优势来提供密集学习信号，而不是仅依赖最终结果。

Result: 在多跳问答基准测试上的实验表明，ProGraph-R1在推理准确性和生成质量方面持续优于现有的GraphRAG方法。

Conclusion: ProGraph-R1通过结合结构感知检索和进度感知强化学习，有效解决了现有图检索增强生成框架的局限性，为多步推理任务提供了更有效的解决方案。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.

</details>


### [48] [Demographic Probing of Large Language Models Lacks Construct Validity](https://arxiv.org/abs/2601.18486)
*Manuel Tonneau,Neil K. R. Seghal,Niyati Malhotra,Victor Orozco-Olvera,Ana María Muñoz Boudet,Lakshmi Subramanian,Sharath Chandra Guntuku,Valentin Hofmann*

Main category: cs.CL

TL;DR: 研究发现人口统计探测方法缺乏构念效度，不同人口统计线索（如姓名、方言）对LLM行为的影响不一致，导致估计的群体差异不稳定


<details>
  <summary>Details</summary>
Motivation: 当前人口统计探测研究通常使用单一人口统计线索（如姓名或方言）来研究LLM如何根据人口统计属性调整行为，但这种方法隐含地假设这些线索可以互换地操作化相同的基础人口统计条件行为。本文旨在测试这种构念效度假设

Method: 在现实的寻求建议互动中测试人口统计探测方法，聚焦于美国背景下的种族和性别。使用不同的人口统计线索（如姓名、方言）作为群体成员信号，分析这些线索如何影响模型行为

Result: 1) 代表相同人口统计群体的线索仅引起部分重叠的行为变化；2) 同一线索内不同群体间的区分度弱且不均匀；3) 估计的群体差异不稳定，幅度和方向随线索变化；4) 不一致性部分源于线索编码人口统计属性的强度差异和独立影响模型行为的语言混淆因素

Conclusion: 人口统计探测缺乏构念效度，无法提供关于LLM如何根据人口统计信息进行条件化的单一稳定表征。建议使用多个生态有效的线索并明确控制混淆因素，以支持关于LLM中人口统计效应的更可靠主张

Abstract: Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.

</details>


### [49] [Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali](https://arxiv.org/abs/2601.17764)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Jui Saha Pritha,Abdullah Al Noman,Abir Ahmed,Golam Md Mohiuddin,Tze Hui Liew*

Main category: cs.CL

TL;DR: 该研究分析了孟加拉语中LLMs的性别偏见特征，发现英语中心的偏见检测框架在孟加拉语中效果有限，需要本地化方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs性别偏见研究主要关注英语，对全球南方语言（如孟加拉语）中的偏见研究不足，需要考察语言文化差异对偏见检测的影响。

Method: 采用多种方法：基于词典的挖掘、计算分类模型、翻译对比分析、GPT偏见生成，并在农村和低收入地区进行实地调查收集真实偏见数据。

Result: 孟加拉语中的性别偏见与英语存在显著差异，英语中心的偏见检测框架因语言差异和社会文化因素而效果有限，需要更本地化和情境敏感的方法。

Conclusion: 需要为代表性不足的语言开发专门的偏见检测工具，采用社区驱动的研究方法识别文化相关偏见，为孟加拉语和其他印度语言建立更公平的NLP系统基础。

Abstract: Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.

</details>


### [50] [DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning](https://arxiv.org/abs/2601.17777)
*Xiaoyu Liu,Xiaoyu Guan,Di Liang,Xianjie Wu*

Main category: cs.CL

TL;DR: 提出动态参数隔离策略解决SFT中的跷跷板效应，通过识别任务核心参数区域、合并重叠任务、分阶段训练并冻结先前任务核心参数，减少任务间干扰


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)中，异构任务间的目标冲突会导致"跷跷板效应"：优化一个任务可能损害其他任务性能，特别是当模型参数被无差别更新时。参数异质性被认为是跨任务干扰的根本原因。

Method: 1) 在不同SFT任务上独立微调LLMs，识别每个任务的核心参数区域（更新幅度最大的参数子集）；2) 将核心参数区域高度重叠的任务合并进行联合训练，而互不相交的任务组织到不同阶段；3) 在多阶段SFT中，冻结先前任务获得的核心参数，防止被后续任务覆盖。

Result: 在多个公共数据集上的密集实验表明，动态参数隔离策略能持续减少数据冲突，相比多阶段和多任务调优基线，实现了持续的性能提升。

Conclusion: 通过识别和隔离任务特定的参数区域，可以有效缓解SFT中的跷跷板效应，该方法为处理异构任务间的冲突提供了一种原则性解决方案。

Abstract: Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.

</details>


### [51] [Controlling Reading Ease with Gaze-Guided Text Generation](https://arxiv.org/abs/2601.17781)
*Andreas Säuberli,Darja Jepifanova,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: 使用眼动预测模型控制语言模型生成文本的阅读难度，通过眼动实验验证方法有效性


<details>
  <summary>Details</summary>
Motivation: 利用眼动模式反映认知负荷的特性，开发能够控制文本阅读难度的生成方法，以改善信息可及性和语言学习材料

Method: 使用预测人类注视模式的模型来引导语言模型输出，使其引发特定的阅读行为，并通过眼动追踪实验评估效果

Result: 方法能有效使生成的文本更容易或更难阅读，阅读时间和感知难度均有显著变化，统计分析显示阅读行为变化主要源于词汇处理特征

Conclusion: 基于眼动预测的文本生成方法可有效控制阅读难度，在信息可及性文本简化和个性化语言学习材料生成方面具有应用潜力

Abstract: The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.

</details>


### [52] [Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations](https://arxiv.org/abs/2601.17786)
*Yixin Liu,Kehan Yan,Shiyuan Li,Qingfeng Chen,Shirui Pan*

Main category: cs.CL

TL;DR: MCA²是一个多视图文本异常检测框架，通过整合多个预训练语言模型的嵌入表示，采用多视图重建模型提取正常文本模式，利用对比协作模块增强视图间互补性，并通过自适应分配模块自动调整各视图权重，在10个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有两步式"嵌入-检测器"文本异常检测方法通常只使用单一嵌入模型，缺乏对不同数据集和异常类型的适应性，限制了其在实际应用中的效果。

Method: 提出MCA²多视图框架：1) 利用多个预训练语言模型的嵌入表示；2) 采用多视图重建模型从多个嵌入视角提取正常文本模式；3) 设计对比协作模块增强不同视图间的交互和互补性；4) 开发自适应分配模块自动为每个视图分配贡献权重。

Result: 在10个基准数据集上的广泛实验验证了MCA²相对于强基线的有效性，表明该方法能够更好地适应不同数据集和异常类型。

Conclusion: MCA²通过整合多个语言模型的嵌入表示、增强视图间互补性和自适应权重分配，显著提升了文本异常检测的性能和适应性，为解决现有方法的局限性提供了有效方案。

Abstract: Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step "embedding-detector" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.

</details>


### [53] [DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation](https://arxiv.org/abs/2601.17823)
*Pranav Kasela,Marco Braga,Alessandro Ghiotto,Andrea Pilzer,Marco Viviani,Alessandro Raganato*

Main category: cs.CL

TL;DR: DIETA是一个专为意大利语-英语机器翻译设计的5亿参数解码器Transformer模型，在多个基准测试中表现优异，排名前50%，超越大多数3B以下模型


<details>
  <summary>Details</summary>
Motivation: 针对意大利语-英语机器翻译领域缺乏专门优化的中小型模型，需要构建高质量双语语料库和评估集来提升翻译质量

Method: 收集约2.07亿句对的意大利语-英语平行语料（涵盖议会、法律、网络、字幕、新闻、文学等），使用预训练模型生成3.52亿反向翻译数据，训练5亿参数的解码器Transformer模型

Result: 在32个系统的排行榜中稳定排名第二四分位数，在五个测试套件中的四个上超越大多数3B以下模型，并发布了新的WikiNews评估集

Conclusion: DIETA为意大利语-英语机器翻译提供了高效的专门化解决方案，公开的训练脚本、模型、语料库和评估集将促进该领域进一步研究

Abstract: In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation

</details>


### [54] [Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents](https://arxiv.org/abs/2601.17829)
*Dan Greenstein,Zohar Karnin,Chen Amiraz,Oren Somekh*

Main category: cs.CL

TL;DR: 提出一种通过优化通用多样性指标来生成合成数据集的方法，用于训练函数调用代理，在保持正确性的同时提升查询和参数的多样性，并在BFCL基准上取得7.4%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 构建函数调用代理需要高质量多样的训练数据，但现有方法在请求的语言多样性和参数覆盖（如城市名、股票代码）方面存在不足，需要一种不依赖手工规则或分类法的鲁棒方法。

Method: 提出通过优化通用多样性指标来生成合成数据集的方法，同时考虑查询和参数的多样性，不依赖手工规则或分类法，适用于不同用例。

Result: 方法在多样性和正确性方面优于基线，使用该数据集训练的模型在分布外性能上表现更优，在BFCL基准上准确率提升7.4%。

Conclusion: 提出的数据生成方法能有效提升函数调用代理的训练数据多样性，从而改善模型性能，特别是在分布外场景下的表现。

Abstract: The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.

</details>


### [55] [EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy](https://arxiv.org/abs/2601.17842)
*Lanqing Du,Yunong Li,YuJie Long,Shihong Chen*

Main category: cs.CL

TL;DR: 提出基于情绪聚焦疗法(EFT)的多智能体思维链框架(EFT-CoT)，通过"自下而上"的三阶段推理流程，结合八个专业智能体进行心理干预，构建高质量数据集并微调EFT-LLM模型，在心理健康问答任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于认知行为疗法(CBT)的心理健康问答方法主要采用"自上而下"的理性重构，往往忽视来访者的具身体验和初级情绪处理。需要一种更关注情绪体验的干预方法。

Method: 提出EFT-CoT框架，采用"自下而上"的三阶段推理流程：具身感知-认知探索-叙事干预。使用八个专业智能体执行关键组件，包括躯体意识映射、适应性评估、核心信念提取和叙事重构。通过思维链蒸馏构建67,000条高质量数据集EFT-Instruct，并微调专门模型EFT-LLM。

Result: EFT-LLM在实验评估中优于强基线模型和人类回答，在共情深度和结构专业性等指标上表现优异。消融研究证实多智能体机制的必要性，模型展现出优越的心理推理能力。

Conclusion: EFT-CoT框架为可解释、高共情的心理咨询系统提供了有效途径，通过情绪聚焦疗法和多智能体思维链的结合，实现了更全面的心理干预。

Abstract: Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a "top-down" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a "bottom-up" trajectory, it deconstructs the intervention into a three-stage reasoning flow: "Embodied Perception - Cognitive Exploration - Narrative Intervention." Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed "EFT-Instruct," a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.

</details>


### [56] [D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models](https://arxiv.org/abs/2601.17865)
*Jia Gu,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 研究发现LLMs在概率采样行为上存在两种类型：D-模型（如Qwen-2.5）的token预测概率波动大且与任务分布对齐差，而E-模型（如Mistral-Small）更稳定且对齐更好，这影响了代码生成和推荐等下游任务中的多样性与稳定性权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs能够生成近似真实世界分布的样本，但其细粒度采样概率是否忠实对齐任务要求仍是一个开放问题。研究旨在探究LLMs的token预测概率与任务级目标分布之间的关系，以及不同模型在概率采样行为上的差异。

Method: 通过受控分布采样模拟实验，识别并区分了两种模型类型：D-模型和E-模型。进一步在下游任务（代码生成和推荐）中评估这两种模型类型，并分析它们的内在属性以探究底层机制。

Result: 发现LLMs存在显著的行为二分：D-模型的P_token表现出较大的步间变异性且与P_task对齐差；E-模型的P_token更稳定且与P_task对齐更好。下游任务评估揭示了多样性与稳定性之间的系统性权衡。

Conclusion: 研究为LLMs的概率采样行为提供了基础性见解，并为何时选择D-模型或E-模型提供了实践指导。对于推荐、搜索和对话代理等网络规模应用，这些结果有助于在现实世界不确定性下平衡多样性与可靠性。

Abstract: The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.

</details>


### [57] [On the Emergence and Test-Time Use of Structural Information in Large Language Models](https://arxiv.org/abs/2601.17869)
*Michelle Chao Chen,Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: 语言模型学习结构信息的能力与复杂推理任务相关，但测试时组合生成能力有限


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何从观测数据中学习抽象结构信息，这对于科学发现中的机制理解和灵活测试时组合生成都很重要

Method: 设计基于语言结构转换的自然语言数据集，在受控设置下研究语言模型学习结构信息的能力

Result: 学习结构信息的出现与复杂推理任务相关，但模型在测试时进行组合生成的能力仍然有限

Conclusion: 语言模型能够学习结构信息，但在利用这些信息进行灵活组合生成方面存在局限性

Abstract: Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.

</details>


### [58] [Self-Manager: Parallel Agent Loop for Long-form Deep Research](https://arxiv.org/abs/2601.17879)
*Yilong Xu,Zhi Zheng,Xiang Long,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: Self-Manager是一个并行代理循环框架，通过多线程隔离上下文实现异步并发执行，解决传统单上下文窗口和顺序执行带来的相互干扰和阻塞问题。


<details>
  <summary>Details</summary>
Motivation: 现有代理在处理长深度研究任务时，虽然通过子任务级上下文管理克服了线性上下文积累和信息丢失，但仍受限于单上下文窗口和顺序执行范式，导致相互干扰和阻塞行为，限制了可扩展性和适应性。

Method: 提出Self-Manager并行代理循环，主线程可创建多个子线程，每个子线程拥有独立的隔离上下文，通过线程控制块进行迭代管理，实现更专注和灵活的并行代理执行。

Result: 在DeepResearch Bench基准测试中，Self-Manager在所有指标上持续优于现有单代理循环基线。分析实验证明了其设计选择的必要性，以及在上下文容量、效率和泛化能力方面的优势。

Conclusion: Self-Manager通过并行代理循环设计有效解决了长深度研究任务中的上下文管理问题，提供了更好的可扩展性和适应性，为复杂研究任务处理提供了新思路。

Abstract: Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.

</details>


### [59] [Assessment of Generative Named Entity Recognition in the Era of Large Language Models](https://arxiv.org/abs/2601.17898)
*Qi Zhan,Yile Wang,Hui Huang*

Main category: cs.CL

TL;DR: 开源大语言模型通过参数高效微调和结构化输出格式，在命名实体识别任务上达到与传统编码器模型相当的性能，其能力源于指令遵循而非记忆，且微调对通用能力影响很小。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，命名实体识别正从序列标注任务转向生成范式。本文旨在系统评估开源LLMs在平面和嵌套NER任务上的表现，探究生成式NER与传统方法的性能差距、输出格式影响、是否依赖记忆、以及微调对通用能力的影响。

Method: 对8个不同规模的开源大语言模型和4个标准NER数据集进行实验，采用参数高效微调方法，比较不同输出格式（如内联括号或XML），评估模型性能并分析其能力来源。

Result: 1) 开源LLMs通过参数高效微调和结构化输出格式，性能可与传统编码器模型竞争，甚至超过GPT-3等闭源模型；2) LLMs的NER能力源于指令遵循和生成能力，而非简单的实体-标签对记忆；3) NER指令微调对LLMs的通用能力影响很小，甚至因增强实体理解而提升DROP等数据集的性能。

Conclusion: 基于大语言模型的生成式命名实体识别是一种有前景且用户友好的传统方法替代方案，开源LLMs在NER任务上展现出强大潜力。

Abstract: Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at https://github.com/szu-tera/LLMs4NER.

</details>


### [60] [ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation](https://arxiv.org/abs/2601.17921)
*Yi Zhao,Qinghua Yao,Xinyuan song,Wei Zhu*

Main category: cs.CL

TL;DR: ShapLoRA：基于Shapley值的可解释性LoRA秩分配框架，通过结合敏感度与协作博弈思想提出Shapley敏感度指标，优化现有工作流程，在多种任务上超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA秩分配方法依赖不可解释且不可靠的重要性度量指标，限制了性能提升。需要一种更可解释、可靠的秩分配方法来优化大型语言模型的参数高效微调。

Method: 提出ShapLoRA框架：1) 结合敏感度度量和协作博弈中的联盟思想，提出可解释的Shapley敏感度重要性度量；2) 在单独验证集上计算Shapley敏感度；3) 建立分配-重训练流程确保公平比较。

Result: 在多种挑战性任务上的实验结果表明，ShapLoRA方法在可调参数数量相当的情况下，能够超越最近的基线方法。

Conclusion: ShapLoRA通过引入基于Shapley值的可解释重要性度量，解决了现有LoRA秩分配方法的局限性，为参数高效微调提供了更可靠的解决方案，并将开源代码和模型促进未来研究。

Abstract: Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.

</details>


### [61] [A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models](https://arxiv.org/abs/2601.17952)
*Michail Mamalakis,Tiago Azevedo,Cristian Cosentino,Chiara D'Ercoli,Subati Abulikemu,Zhongtian Sun,Richard Bethlehem,Pietro Lio*

Main category: cs.CL

TL;DR: 提出统一解释性框架，结合归因和机制解释方法，通过单义特征提取减少方法间变异，为LLM在阿尔茨海默病诊断中提供稳定重要性评分。


<details>
  <summary>Details</summary>
Motivation: 在临床环境（如阿尔茨海默病进展诊断）中部署大语言模型时，可解释性仍是关键挑战。现有归因方法存在高方法间变异和不稳定解释，而机制解释方法缺乏与模型输入输出的直接对齐，且不提供明确重要性评分。

Method: 引入统一解释性框架，通过单义特征提取整合归因和机制视角。在LLM层级别构建单义嵌入空间，优化框架以显式减少方法间变异，产生稳定的输入级重要性评分，并通过感兴趣层的解压缩表示突出显著特征。

Result: 该方法产生稳定的输入级重要性评分，通过解压缩表示突出显著特征，推进LLM在认知健康和神经退行性疾病中的安全可信应用。

Conclusion: 该统一框架整合了归因和机制解释方法，通过单义特征提取减少方法间变异，为LLM在临床环境中的可解释性提供了更稳定可靠的解决方案，特别适用于阿尔茨海默病诊断等需要早期可信预测的场景。

Abstract: Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.

</details>


### [62] [LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction](https://arxiv.org/abs/2601.17971)
*Junior Cedric Tonga,Chen Cecilia Liu,Iryna Gurevych,Fajri Koto*

Main category: cs.CL

TL;DR: 本文提出一个基于提示的迭代框架，用于构建文化常识知识图谱(CCKG)，将LLMs作为文化档案，系统提取文化特定实体、关系和实践，并组合成跨语言的多步推理链。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)从海量网络数据中编码了丰富的文化知识，但这类知识大多是隐式和非结构化的，限制了其可解释性和应用。需要系统性地提取和结构化文化常识知识。

Method: 采用迭代的基于提示的框架，将LLMs视为文化档案，系统提取文化特定实体、关系和实践，并将它们组合成跨语言的多步推理链，构建文化常识知识图谱(CCKG)。

Result: 评估五个国家的CCKG，发现文化知识图谱在英语中表现更好，即使目标文化是非英语的(如中文、印尼语、阿拉伯语)，表明当前LLMs中的文化编码不均匀。用CCKG增强较小LLMs能提高文化推理和故事生成性能，其中英语链带来最大增益。

Conclusion: 研究展示了LLMs作为文化技术的潜力和局限性，链式结构文化知识是文化基础NLP的实用基础。文化知识图谱为文化常识的系统建模提供了可行途径。

Abstract: Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.

</details>


### [63] [SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets](https://arxiv.org/abs/2601.17982)
*Kshitij Mishra,Nils Lukas,Salem Lahlou*

Main category: cs.CL

TL;DR: SD-E²是一个强化学习框架，通过优化推理轨迹的语义多样性来提升小语言模型的复杂推理能力，在多个基准测试上显著超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在有限计算预算下难以进行有效的探索，导致复杂推理能力不足。需要一种更高效的方法来平衡探索与利用。

Method: 提出SD-E²框架，使用冻结的句子嵌入模型计算语义多样性奖励，该奖励捕获：(1) 语义不同解决策略的覆盖率；(2) 嵌入空间中平均成对差异度。将多样性奖励与结果正确性和解决方案效率结合，通过z-score归一化的多目标目标稳定训练。

Result: 在GSM8K上，SD-E²比基础Qwen2.5-3B-Instruct提升27.4个百分点，比GRPO-CFL和GRPO-CFEE分别提升5.2和1.5个百分点，平均每个问题发现9.8个语义不同策略。在MedMCQA上达到49.64%（基础模型38.37%），在AIME基准上达到13.28%（基础模型6.74%）。

Conclusion: 奖励语义新颖性为训练推理能力强的小语言模型提供了更计算高效的探索-利用信号。通过引入认知适应（调整推理过程结构而非逐令牌计算），SD-E²为资源受限模型提供了补充的效率提升路径。

Abstract: Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.

</details>


### [64] [AI-based approach to burnout identification from textual data](https://arxiv.org/abs/2601.17993)
*Marina Zavertiaeva,Petr Parshakov,Mikhail Usanin,Aleksei Smirnov,Sofia Paklina,Anastasiia Kibardina*

Main category: cs.CL

TL;DR: 提出基于RuBERT模型的AI方法，利用NLP从文本数据中检测职业倦怠，通过ChatGPT生成数据和YouTube用户评论进行微调，可计算文本的倦怠概率


<details>
  <summary>Details</summary>
Motivation: 开发能够从文本数据中自动检测职业倦怠的AI方法，用于监控高压工作环境中的倦怠相关语言信号

Method: 使用原本用于情感分析的RuBERT模型，通过ChatGPT生成的合成句子和俄罗斯YouTube视频中关于倦怠的用户评论进行微调，构建倦怠检测模型

Result: 开发出能够为输入文本分配倦怠概率的模型，可处理大量书面通信数据，用于监测高压工作环境中的倦怠语言信号

Conclusion: 提出的AI方法能够有效从文本数据中检测职业倦怠，为大规模监控工作环境中的倦怠现象提供了实用工具

Abstract: This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.

</details>


### [65] [PEAR: Pairwise Evaluation for Automatic Relative Scoring in Machine Translation](https://arxiv.org/abs/2601.18006)
*Lorenzo Proietti,Roman Grundkiewicz,Matt Post*

Main category: cs.CL

TL;DR: PEAR是一个基于成对比较的机器翻译质量评估指标，通过预测两个候选翻译的质量差异方向和幅度来评估翻译质量，在WMT24基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译质量评估方法多为单候选评估，缺乏对翻译质量相对差异的精确度量。作者希望开发一种能够量化两个翻译之间质量差异的评估方法。

Method: 将无参考机器翻译评估重构为分级成对比较任务，使用人类判断差异作为监督信号进行训练，并加入候选顺序反转时的符号反转正则化项。

Result: 在WMT24元评估基准上，PEAR优于使用相同数据和骨干网络的单候选质量评估基线，超越了更大的质量评估模型和基于参考的指标，同时提供更不冗余的评估信号。

Conclusion: PEAR通过成对比较框架有效提升了机器翻译质量评估性能，参数更少但效果更好，还可作为最小贝叶斯风险解码的有效效用函数。

Abstract: We present PEAR (Pairwise Evaluation for Automatic Relative Scoring), a supervised Quality Estimation (QE) metric family that reframes reference-free Machine Translation (MT) evaluation as a graded pairwise comparison. Given a source segment and two candidate translations, PEAR predicts the direction and magnitude of their quality difference. The metrics are trained using pairwise supervision derived from differences in human judgments, with an additional regularization term that encourages sign inversion under candidate order reversal. On the WMT24 meta-evaluation benchmark, PEAR outperforms strictly matched single-candidate QE baselines trained with the same data and backbones, isolating the benefit of the proposed pairwise formulation. Despite using substantially fewer parameters than recent large metrics, PEAR surpasses far larger QE models and reference-based metrics. Our analysis further indicates that PEAR yields a less redundant evaluation signal relative to other top metrics. Finally, we show that PEAR is an effective utility function for Minimum Bayes Risk (MBR) decoding, reducing pairwise scoring cost at negligible impact.

</details>


### [66] [Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems](https://arxiv.org/abs/2601.18012)
*Hendrika Maclean,Mert Can Cakmak,Muzakkiruddin Ahmed Mohammed,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 评估LLM在薪资计算等精确数值任务中的表现，发现某些情况下提示工程足够，复杂情况需要显式计算


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自然语言理解方面不断进步，但在精确数值计算和可审计输出方面仍不可靠。薪资系统作为高风险的典型案例，需要模型理解薪资模式、按正确顺序应用规则并提供分毫不差的结果

Method: 使用合成薪资系统作为测试平台，构建从基础到复杂案例的分层数据集，测试从最小基线到模式引导和推理变体的多种提示策略，评估GPT、Claude、Perplexity、Grok和Gemini等多个模型家族

Result: 结果显示存在明确的性能区间：某些情况下仔细的提示工程就足够，但在更复杂的情况下需要显式计算。提供了可复现的框架和实际部署指导

Conclusion: LLM在精确数值计算任务中存在局限性，需要根据任务复杂度选择合适的策略（提示工程或显式计算），为需要准确性和保证的应用场景提供了实用指导

Abstract: Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.

</details>


### [67] [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)
*Adeeba Tarannum,Muzakkiruddin Ahmed Mohammed,Mert Can Cakmak,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 提出一个结合提示工程与确定性验证的框架，将非结构化人员地址文本转换为结构化数据，无需微调即可实现高精度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在噪声或多语言环境下表现不佳，而神经模型和LLMs缺乏确定性控制和可复现性，需要一种既鲁棒又可控的结构化信息提取方案。

Method: 采用提示驱动、验证为中心的框架，包含输入标准化、结构化提示、约束解码和严格规则验证，在固定实验设置下确保可复现性。

Result: 在异构真实地址数据上评估，显示出高字段级准确率、强模式遵循能力和稳定的置信度校准。

Conclusion: 结合确定性验证与生成提示的方法为结构化信息提取提供了鲁棒、可解释且可扩展的解决方案，是训练密集型或领域特定模型的实用替代方案。

Abstract: Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.

</details>


### [68] [CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data](https://arxiv.org/abs/2601.18026)
*Pedro Ortiz Suarez,Laurie Burchell,Catherine Arnett,Rafael Mosquera-Gómez,Sara Hincapie-Monsalve,Thom Vaughan,Damian Stewart,Malte Ostendorff,Idris Abdulmumin,Vukosi Marivate,Shamsuddeen Hassan Muhammad,Atnafu Lambebo Tonja,Hend Al-Khalifa,Nadia Ghezaiel Hammouda,Verrah Otiende,Tack Hwa Wong,Jakhongir Saydaliev,Melika Nobakhtian,Muhammad Ravi Shulthan Habibi,Chalamalasetti Kranti,Carol Muchemi,Khang Nguyen,Faisal Muhammad Adam,Luis Frentzen Salim,Reem Alqifari,Cynthia Amol,Joseph Marvin Imperial,Ilker Kesen,Ahmad Mustafid,Pavel Stepachev,Leshem Choshen,David Anugraha,Hamada Nayel,Seid Muhie Yimam,Vallerie Alexandra Putra,My Chiffon Nguyen,Azmine Toushik Wasi,Gouthami Vadithya,Rob van der Goot,Lanwenn ar C'horr,Karan Dua,Andrew Yates,Mithil Bangera,Yeshil Bangera,Hitesh Laxmichand Patel,Shu Okabe,Fenal Ashokbhai Ilasariya,Dmitry Gaynullin,Genta Indra Winata,Yiyuan Li,Juan Pablo Martínez,Amit Agarwal,Ikhlasul Akmal Hanif,Raia Abu Ahmad,Esther Adenuga,Filbert Aurelian Tjiaranata,Weerayut Buaphet,Michael Anugraha,Sowmya Vajjala,Benjamin Rice,Azril Hafizi Amirudin,Jesujoba O. Alabi,Srikant Panda,Yassine Toughrai,Bruhan Kyomuhendo,Daniel Ruffinelli,Akshata A,Manuel Goulão,Ej Zhou,Ingrid Gabriela Franco Ramirez,Cristina Aggazzotti,Konstantin Dobler,Jun Kevin,Quentin Pagès,Nicholas Andrews,Nuhu Ibrahim,Mattes Ruckdeschel,Amr Keleg,Mike Zhang,Casper Muziri,Saron Samuel,Sotaro Takeshita,Kun Kerdthaisong,Luca Foppiano,Rasul Dent,Tommaso Green,Ahmad Mustapha Wali,Kamohelo Makaaka,Vicky Feliren,Inshirah Idris,Hande Celikkanat,Abdulhamid Abubakar,Jean Maillard,Benoît Sagot,Thibault Clérice,Kenton Murray,Sarah Luger*

Main category: cs.CL

TL;DR: CommonLID是一个社区驱动的人工标注语言识别基准数据集，覆盖109种语言，专注于网页领域，旨在解决现有LID模型在嘈杂网页数据上性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 语言识别是构建多语言语料库的基础步骤，但现有LID模型在许多语言上表现不佳，特别是在用于训练多语言模型的嘈杂、异构网页数据上。许多语言在现有基准中代表性不足。

Method: 创建CommonLID基准数据集：社区驱动、人工标注，覆盖109种语言，专注于网页领域。使用该数据集与其他5个常用评估集测试8个流行的LID模型，分析结果以评估当前技术水平。

Result: 研究发现现有评估高估了LID模型在网页领域的准确性，特别是对于许多代表性不足的语言。CommonLID揭示了模型在真实网页数据上的实际性能差距。

Conclusion: CommonLID是开发更具代表性高质量文本语料库的关键资源，有助于更准确地评估LID模型在网页领域的性能。数据集和代码已开源发布。

Abstract: Language identification (LID) is a fundamental step in curating multilingual corpora. However, LID models still perform poorly for many languages, especially on the noisy and heterogeneous web data often used to train multilingual language models. In this paper, we introduce CommonLID, a community-driven, human-annotated LID benchmark for the web domain, covering 109 languages. Many of the included languages have been previously under-served, making CommonLID a key resource for developing more representative high-quality text corpora. We show CommonLID's value by using it, alongside five other common evaluation sets, to test eight popular LID models. We analyse our results to situate our contribution and to provide an overview of the state of the art. In particular, we highlight that existing evaluations overestimate LID accuracy for many languages in the web domain. We make CommonLID and the code used to create it available under an open, permissive license.

</details>


### [69] [Addressing LLM Diversity by Infusing Random Concepts](https://arxiv.org/abs/2601.18053)
*Pulin Agrawal,Prasoon Goyal*

Main category: cs.CL

TL;DR: 通过在提示词前添加随机概念（无关的单词/句子）可以显著提高大语言模型输出的多样性，并通过系统评估协议验证了这一发现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的输出多样性有限，研究者希望探索通过向提示词中注入随机概念是否能改善输出多样性。

Method: 设计系统评估协议，在提示词前添加与主题无关的随机单词或句子，然后让LLM回答"列举10位好莱坞演员"这类问题，并分析输出结果的多样性指标。

Result: 在多个LLM上的实验表明，在提示词前添加无关的随机内容确实能显著提高模型输出的多样性。

Conclusion: 这一有前景的结果和评估协议为未来研究开辟了新方向，包括如何将随机性注入技术应用于其他领域，以及如何更系统地评估LLM的多样性。

Abstract: Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.

</details>


### [70] [Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production](https://arxiv.org/abs/2601.18056)
*Ahmet Yavuz Uluslu,Elliot Murphy*

Main category: cs.CL

TL;DR: 该论文主张在双语产出错误研究中纳入振荡特征，提出ROSE神经模型能解释双语中的句法迁移，并以跨语言影响为例说明振荡失败模式驱动功能抑制/竞争理论。


<details>
  <summary>Details</summary>
Motivation: 传统双语产出错误研究主要关注事件相关电位等时序特征，但缺乏实现层面的理论约束。需要新的神经计算框架来解释双语中的句法迁移现象，特别是跨语言影响的功能抑制机制。

Method: 采用ROSE神经语言模型作为理论框架，将跨语言影响视为特定振荡失败模式的结果。通过分析双语产出中的句法迁移和形态句法序列失败，建立神经振荡特征与语言功能障碍之间的连接假设。

Result: ROSE模型能够捕捉双语产出中句法迁移的形式特性和形态句法序列失败的范围。振荡特征提供了比传统神经特征更复杂的时空生物标志物，为双语理论提供了实现层面的约束。

Conclusion: 将振荡特征纳入双语研究不仅为ROSE的连接假设提供了支持，还允许探索更复杂的语言功能障碍生物标志物。这种方法为双语产出错误提供了新的神经计算解释框架。

Abstract: We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.

</details>


### [71] [Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models](https://arxiv.org/abs/2601.18065)
*Aryan Roy,Zekun Wang,Christopher J. MacLellan*

Main category: cs.CL

TL;DR: 比较视觉-语言模型(VLMs)与纯文本大语言模型(LLMs)在语言具体性敏感性方面的差异，发现VLMs在多模态预训练后表现出更接近人类的语言具体性感知能力。


<details>
  <summary>Details</summary>
Motivation: 研究视觉-语言模型是否比纯文本模型发展出更接近人类的语言具体性敏感性，特别是在仅使用文本提示进行评估时。通过对比匹配的Llama文本模型及其视觉版本，探索多模态预训练作为感知基础的影响。

Method: 采用控制性比较方法，对比匹配的Llama文本模型和Llama Vision模型在不同规模下的表现。从三个层面测量具体性效应：1)输出行为：将问题具体性与QA准确率关联；2)嵌入几何：测试表示是否沿具体性轴组织；3)注意力动态：通过注意力熵测量量化上下文依赖。此外，从模型中获取token级具体性评分，评估与人类规范分布的对齐程度。

Result: 在所有基准测试和模型规模中，VLMs在更具体的输入上表现出更大的性能提升，具有更清晰的具体性结构化表示，产生的评分更符合人类规范，并显示出系统性的不同注意力模式，表明更强的感知基础。

Conclusion: 多模态预训练使视觉-语言模型发展出比纯文本模型更接近人类的语言具体性敏感性，即使在使用纯文本提示进行评估时也是如此。这表明感知基础对语言理解有重要影响。

Abstract: Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.

</details>


### [72] [Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents](https://arxiv.org/abs/2601.18077)
*Mahesh Ramesh,Kaousheik Jayakumar,Aswinkumar Ramkumar,Pavan Thodima,Aniket Rege*

Main category: cs.CL

TL;DR: 论文评估了17个LLM在合作推理游戏Hanabi中的表现，通过不同上下文工程设置测试，并发布了首个公开数据集用于微调，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决不完全信息下的合作推理挑战，Hanabi游戏需要心智理论和战略沟通，是测试多智能体协作的理想基准。

Method: 方法包括：1）在2-5人游戏中测试17个SOTA LLM；2）设计三种上下文工程设置（Watson、Sherlock、Mycroft）；3）创建两个公开数据集（HanabiLogs和HanabiRewards）；4）对4B模型进行监督学习和RL微调。

Result: 结果显示：1）最强推理模型在Sherlock设置下平均得分超过15分，但仍低于人类专家（20+分）；2）微调后模型性能提升21%（监督）和156%（RL），接近顶级专有模型；3）RL微调模型在多个其他任务上也有泛化提升。

Conclusion: 结论表明：1）LLM能够维护工作记忆进行状态跟踪；2）模型间跨玩性能随模型强度平滑插值；3）数据集驱动的微调能显著提升合作推理能力；4）Hanabi训练能泛化到其他推理任务。

Abstract: Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.

</details>


### [73] [CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations](https://arxiv.org/abs/2601.18102)
*Stephanie Fong,Zimu Wang,Guilherme C. Oliveira,Xiangyu Zhao,Yiwen Jiang,Jiahe Liu,Beau-Luke Colton,Scott Woods,Martha E. Shenton,Barnaby Nelson,Zongyuan Ge,Dominic Dwyer*

Main category: cs.CL

TL;DR: CHiRPE是一个临床NLP管道，用于预测精神病风险并生成临床医生共同开发的新型SHAP解释格式，在24个国际诊所的944份访谈记录上训练，准确率超过90%，临床专家更偏好其概念引导的解释格式。


<details>
  <summary>Details</summary>
Motivation: 医疗NLP工具需要最终用户可解释，但传统的可解释AI方法与临床推理不匹配且缺乏临床医生输入。需要开发与临床推理对齐、有临床医生参与的可解释NLP工具。

Method: CHiRPE管道整合症状领域映射、LLM摘要和BERT分类，使用944份半结构化临床访谈记录训练，生成新型SHAP解释格式（特别是混合图-文本摘要格式），这些格式是与临床医生共同开发的。

Result: CHiRPE在三个BERT变体上均达到超过90%的准确率，优于基线模型。28名临床专家评估显示，他们强烈偏好新型概念引导的解释格式，特别是混合图-文本摘要格式。

Conclusion: 临床引导的模型开发能够产生既准确又可解释的结果。下一步将在24个国际站点进行真实世界测试。

Abstract: The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.

</details>


### [74] [GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health](https://arxiv.org/abs/2601.18106)
*Jiatan Huang,Zheyuan Zhang,Tianyi Ma,Mingchen Li,Yaning Zheng,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

TL;DR: GLEN-Bench：首个基于图语言模型的营养健康评估基准，整合健康记录、食物成分和食品获取数据，解决个性化饮食指导中的现实约束、解释性和统一评估问题。


<details>
  <summary>Details</summary>
Motivation: 当前营养干预计算方法存在三大局限：1）忽略社会经济状况、共病等现实约束；2）推荐系统缺乏解释性；3）缺乏统一基准评估营养干预相关任务。需要开发能整合现实约束、提供解释性、统一评估的解决方案。

Method: 结合NHANES健康记录、FNDDS食物成分数据和USDA食品获取指标构建知识图谱，连接人口统计、健康状况、饮食行为、贫困相关约束和营养需求。在阿片类药物使用障碍案例中测试，包含三个关联任务：风险检测、个性化推荐和问答解释。评估图神经网络、大语言模型和混合架构。

Result: 建立了首个全面的图语言基准GLEN-Bench，能够检测与健康风险相关的明确饮食模式，为实际干预提供指导。评估了不同图语言方法，建立了可靠的基线并识别了实用设计选择。

Conclusion: GLEN-Bench填补了营养健康评估的关键空白，通过整合现实约束、提供解释性和统一评估框架，为个性化饮食指导提供了更实用的计算方法，特别在阿片类药物使用障碍等复杂健康条件下展现出应用潜力。

Abstract: Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.

</details>


### [75] [FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning](https://arxiv.org/abs/2601.18116)
*Lin Sun,Linglin Zhang,Jingang Huang,Change Jia,Zhengwei Cheng,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: FABLE是一个基于森林的自适应双路径LLM增强检索框架，通过构建多粒度语义结构的层次森林索引，结合LLM引导的层次遍历和结构感知传播，在显著减少计算成本的同时达到与全上下文LLM推理相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管长上下文LLMs快速发展，但实际应用中仍存在"中间迷失"现象、高计算成本和跨文档推理扩展性差等问题。传统RAG系统受限于平面块级检索，无法支持结构化跨文档合成。需要一种既能利用LLM能力又能保持检索效率的解决方案。

Method: FABLE采用双路径策略：1) 构建LLM增强的层次森林索引，形成多粒度语义结构；2) 结合LLM引导的层次遍历和结构感知传播进行细粒度证据获取；3) 通过显式预算控制实现自适应效率权衡。

Result: 实验表明FABLE持续优于SOTA RAG方法，在减少94%token的情况下达到与全上下文LLM推理相当的准确率，证明长上下文LLMs增强了而非完全替代结构化检索的需求。

Conclusion: 长上下文LLMs实际上放大了对结构化检索的需求，而非完全替代。FABLE通过集成LLM到知识组织和检索中，实现了效率与准确性的有效平衡，为下一代RAG系统提供了新方向。

Abstract: The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.
  We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.
  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.

</details>


### [76] [Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models](https://arxiv.org/abs/2601.18129)
*Kunat Pipatanakul,Pittawat Taveekitworachai*

Main category: cs.CL

TL;DR: Typhoon S：一种最小化后训练方法，通过监督微调、策略蒸馏和小规模RFT，在有限资源下实现主权LLM，以泰语为案例展示了在保持通用能力的同时提升特定领域任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流LLM主要面向英语和中文等高资源语言，由少数拥有大规模计算和数据的组织开发，这为主权环境（如区域或国家级机构）带来了实际障碍。这些环境需要在有限资源和严格透明度约束下保持对模型权重、训练数据和部署的控制与理解。

Method: 提出Typhoon S，一种最小化开放后训练方案，结合监督微调、策略蒸馏和小规模强化微调。特别引入了InK-GRPO，在GRPO损失基础上增加下一个词预测损失，以提升特定领域任务性能。

Result: 以泰语为案例研究表明，该方法能将主权适应和通用基础模型转化为具有强大通用性能的指令调优模型。小规模RFT配合InK-GRPO能显著提升泰语法律推理和泰语特定知识能力，同时保持通用能力。

Conclusion: 精心设计的后训练策略可以减少指令数据和计算所需规模，为在学术规模资源下开发高质量主权LLM提供了实用路径，有助于打破少数组织对先进LLM技术的垄断。

Abstract: Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.

</details>


### [77] [Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models](https://arxiv.org/abs/2601.18162)
*Ani Harutyunyan,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文在GoEmotions数据集上对比了三种细粒度情感识别模型：基于TF-IDF的逻辑回归、带注意力的BiLSTM和微调的BERT模型，发现逻辑回归在Micro-F1上表现最佳，而BERT在整体平衡性上最优。


<details>
  <summary>Details</summary>
Motivation: 细粒度情感识别是一个具有挑战性的多标签NLP任务，面临标签重叠和类别不平衡的问题。本文旨在通过系统比较不同建模方法在GoEmotions数据集上的表现，探索不同模型在处理情感识别任务中的优势和局限性。

Method: 在GoEmotions数据集上评估了三种建模方法：1) 基于TF-IDF的逻辑回归系统，使用二元相关性训练；2) 带注意力的BiLSTM模型；3) 为多标签分类微调的BERT模型。实验采用官方划分的训练/验证/测试集，并使用逆频率类别权重缓解不平衡问题。

Result: 逻辑回归获得最高的Micro-F1（0.51），而BERT在整体平衡性上表现最佳，超越了官方论文报告的结果，达到Macro-F1 0.49、Hamming Loss 0.036和Subset Accuracy 0.36。这表明高频情感通常依赖表面词汇线索，而上下文表示能提升对稀有情感和模糊示例的性能。

Conclusion: 不同建模方法在细粒度情感识别任务中各有优势：逻辑回归在处理高频情感时表现良好，而BERT的上下文表示能力使其在整体平衡性和处理复杂情感识别任务上更为出色。这表明情感识别任务需要结合表面词汇特征和深层语义理解。

Abstract: Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.

</details>


### [78] [MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2601.18204)
*Juexiang Ye,Xue Li,Xinyu Yang,Chengkai Huang,Lanshun Nie,Lina Yao,Dechen Zhan*

Main category: cs.CL

TL;DR: MemWeaver是一个统一记忆框架，通过结构化图记忆、经验记忆和文本证据记忆三个组件，结合双通道检索策略，显著提升长时程智能体的多跳和时间推理能力，同时大幅减少上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体在长时程交互中面临记忆系统问题：非结构化检索或粗粒度抽象导致时间冲突、推理脆弱和可追溯性有限。需要支持时间一致性、多跳推理和跨会话证据重用的记忆系统。

Method: 提出MemWeaver框架，包含三个互联组件：1) 时间基础图记忆用于结构化关系推理；2) 经验记忆从重复观察中抽象出交互模式；3) 文本记忆保留原始证据。采用双通道检索策略联合检索结构化知识和支持证据，构建紧凑且信息密集的推理上下文。

Result: 在LoCoMo基准测试中，MemWeaver显著提高了多跳和时间推理的准确性，同时相比长上下文基线减少了超过95%的输入上下文长度。

Conclusion: MemWeaver通过统一的结构化记忆框架解决了长时程智能体记忆系统的关键问题，在保持推理准确性的同时极大提升了效率，为智能体的长期交互提供了有效的记忆管理方案。

Abstract: Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.

</details>


### [79] [TechING: Towards Real World Technical Image Understanding via VLMs](https://arxiv.org/abs/2601.18238)
*Tafazzul Nadeem,Bhavik Shangari,Manish Rai,Gagan Raj Gupta,Ashutosh Modi*

Main category: cs.CL

TL;DR: 该论文提出了一种通过合成数据训练视觉语言模型来理解手绘技术图表的方法，显著提升了模型在真实手绘图表上的表现。


<details>
  <summary>Details</summary>
Motivation: 专业人士在讨论中经常手绘技术图表（如流程图、框图等），但后续编辑需要重新绘制。现有的视觉语言模型在理解技术图表方面表现不佳，而收集大量真实手绘图像进行微调又不现实。

Method: 1. 创建大规模合成数据集（模拟真实世界图像）用于训练视觉语言模型；2. 引入多个新的自监督任务进行训练；3. 在合成图像上对Llama 3.2 11B-instruct模型进行微调，得到LLama-VL-TUG模型；4. 在较小规模的真实手绘图像语料库上进行评估（借助人工标注）。

Result: 1. LLama-VL-TUG将Llama 3.2 11B-instruct的ROUGE-L性能提升了2.14倍；2. 在所有基线模型中取得了最佳综合性能；3. 在真实图像上，在8种图表类型中的7种实现了最低编译错误；4. 将Llama 3.2 11B-instruct的平均F1分数提升了6.97倍。

Conclusion: 通过合成数据训练视觉语言模型是解决手绘技术图表理解问题的有效方法，显著提升了模型在真实场景下的表现，为技术图表自动识别和编辑提供了实用解决方案。

Abstract: Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.

</details>


### [80] [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)
*Peng Sun,Xiangyu Zhang,Duan Wu*

Main category: cs.CL

TL;DR: BoRP是一种基于LLM潜在空间几何特性的可扩展满意度评估框架，通过自举机制自动生成评分标准，使用偏最小二乘法将隐藏状态映射到连续分数，显著优于生成式基线且大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 对于开放式对话AI，传统A/B测试缺乏可靠指标：显式反馈稀疏，隐式指标模糊。需要一种高保真的满意度评估方法来支持迭代开发。

Method: BoRP框架利用LLM潜在空间的几何特性，采用基于极化指数的自举机制自动生成评分标准，并使用偏最小二乘法(PLS)将隐藏状态映射到连续满意度分数。

Result: 在工业数据集上的实验表明，BoRP（基于Qwen3-8B/14B）在与人判断的一致性方面显著优于生成式基线（甚至优于Qwen3-Max），同时推理成本降低数个数量级，支持全规模监控和高度敏感的CUPED A/B测试。

Conclusion: BoRP为开放式对话AI提供了一种可扩展、高保真的满意度评估框架，解决了传统评估方法的局限性，支持高效的迭代开发和A/B测试。

Abstract: Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.

</details>


### [81] [Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue](https://arxiv.org/abs/2601.18281)
*Yuhang Jia,Pei Liu,Haoqin Sun,Jiaming Zhou,Xuxin Cheng,Cao Liu,Ke Zeng,Xunliang Cai,Yong Qin*

Main category: cs.CL

TL;DR: 提出ReEmpathy模型，通过描述性语言评估和反思推理机制增强语音语言模型在共情对话中的表现


<details>
  <summary>Details</summary>
Motivation: 当前端到端语音语言模型在共情对话中依赖刚性监督信号（如真实回复或偏好分数），无法充分捕捉复杂共情表达的细微差别，因为不存在单一"正确"回复，简单数值分数也无法完全评估情感表达或共情行为的适当性

Method: 1. 提出EmpathyEval：基于描述性自然语言的评估模型，用于评估语音对话中的共情质量；2. 提出ReEmpathy：端到端语音语言模型，采用"共情自我反思交替推理"机制，交替进行语音回复生成和自由形式的共情相关反思推理

Result: ReEmpathy显著改善了共情敏感的语音对话质量，通过启用反思推理机制，为更情感智能和共情感知的人机交互提供了有前景的方法

Conclusion: 提出的ReEmpathy模型通过结合描述性评估和反思推理机制，有效解决了当前语音语言模型在共情对话中的局限性，推动了更情感智能的人机交互发展

Abstract: End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single "correct" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.

</details>


### [82] [U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents](https://arxiv.org/abs/2601.18285)
*Jin Su,Runnan Fang,Yeqiu Li,Xiaobin Wang,Shihao Cai,Pengjun Xie,Ningyu Zhang,Fajie Yuan*

Main category: cs.CL

TL;DR: U-Fold：针对用户中心任务设计的动态上下文折叠框架，通过意图感知的对话摘要和任务相关的工具日志，解决现有方法在长上下文、多轮对话中的限制


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在工具增强场景中受限于上下文长度。现有的上下文折叠方法通常为单查询或单意图场景设计，在用户中心对话中存在两个主要问题：1) 不可逆地丢弃对后续决策至关重要的细粒度约束和中间事实；2) 摘要无法跟踪演变的用户意图，导致遗漏和错误操作

Method: U-Fold框架保留完整的用户-智能体对话和工具调用历史，但在每个轮次使用两个核心组件：1) 生成意图感知、演变的对话摘要；2) 生成紧凑、任务相关的工具日志。该框架专门针对用户中心任务设计

Result: 在τ-bench、τ²-bench、VitaBench和更难的情境膨胀设置上的广泛实验表明，U-Fold始终优于ReAct（在长上下文设置中达到71.4%的胜率）和先前的折叠基线（改进高达27.0%），特别是在长、嘈杂、多轮任务上表现突出

Conclusion: U-Fold是将上下文管理技术从单查询基准转移到现实用户中心应用的有希望的一步，能够有效处理长上下文、多轮对话中的复杂约束和意图演变

Abstract: Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $τ$-bench, $τ^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.

</details>


### [83] [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296)
*Zhaoyan Gong,Zhiqiang Liu,Songze Li,Xiaoke Guo,Yuanxiang Liu,Xinle Deng,Zhizhen Liu,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: Temp-R1是首个通过强化学习训练的端到端自主TKGQA代理，通过扩展动作空间和反向课程学习，在复杂问题上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有TKGQA方法依赖固定工作流程和昂贵的闭源API，缺乏灵活性和可扩展性，无法有效处理动态事实的多跳依赖和复杂时间约束

Method: 提出Temp-R1：1）扩展动作空间，包含专用内部动作和外部动作以解决单动作推理的认知过载；2）引入反向课程学习，先训练困难问题再迁移到简单问题，防止捷径学习

Result: 8B参数的Temp-R1在MultiTQ和TimelineKGQA上达到SOTA，在复杂问题上比强基线提升19.8%，建立了自主时间推理代理的新范式

Conclusion: Temp-R1通过强化学习训练和创新的动作空间设计，为TKGQA提供了灵活可扩展的解决方案，开创了自主时间推理代理的新方向

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.

</details>


### [84] [Suppressing Final Layer Hidden State Jumps in Transformer Pretraining](https://arxiv.org/abs/2601.18302)
*Keigo Shibata,Kazuki Yano,Ryosuke Takahashi,Jaesung Lee,Wataru Ikeda,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文研究Transformer语言模型的内部行为，发现许多预训练模型在中间层输入输出隐藏状态向量角度距离变化很小，但在最后一层附近出现大幅"跳跃"。作者提出抑制这种跳跃的正则化方法JREG，能提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 许多预训练模型在中间Transformer层中，输入和输出隐藏状态向量之间的角度距离变化很小，但在最后一层附近出现不成比例的大幅"跳跃"。作者认为这种跳跃可能是不良特性，需要抑制以促进中间层能力的更均衡利用。

Method: 首先引入量化指标衡量最后一层附近的跳跃强度，证明其在多个开源模型中的普遍性。然后提出跳跃抑制正则化器(JREG)，在预训练过程中惩罚这种跳跃，鼓励中间层更均衡的能力利用。

Result: 在三个不同规模的Llama基模型上进行实证评估，使用JREG方法训练的模型相比基线在不改变架构的情况下，任务性能得到提升。

Conclusion: Transformer语言模型最后一层附近的隐藏状态角度距离跳跃是普遍现象，通过JREG正则化抑制这种跳跃可以改善模型性能，促进中间层能力的更均衡利用。

Abstract: This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.

</details>


### [85] [Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM](https://arxiv.org/abs/2601.18306)
*Everlyn Asiko Chimoto,Mostafa Elhoushi,Bruce A. Bassett*

Main category: cs.CL

TL;DR: 多语言校准显著提升量化LLM性能，英语校准集效果不佳，语言对齐至关重要


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法通常使用小型英语校准集，但其对多语言模型的影响尚未充分探索，需要系统评估不同校准设置对多语言量化性能的影响

Method: 系统评估8种校准设置（5种单语言和3种多语言混合）在两种量化器（GPTQ、AWQ）上，使用10种语言的数据，分析Llama3.1 8B和Qwen2.5 7B模型

Result: 非英语和多语言校准集相比英语基线显著改善困惑度，多语言混合实现最大困惑度降低达3.52点；针对评估语言定制校准集效果最佳；某些语言-量化器组合会出现性能下降，源于不同语言激活范围分布差异

Conclusion: 静态通用校准方法不理想，定制校准数据（语言和多样性）对稳健量化多语言LLM至关重要，语言对齐是提升量化性能的关键因素

Abstract: Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.

</details>


### [86] [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320)
*Jinwei Lu,Yuanfeng Song,Chen Zhang,Raymond Chi-Wing Wong*

Main category: cs.CL

TL;DR: MultiVis-Agent：一个基于逻辑规则增强的多智能体框架，用于可靠的多模态、多场景可视化生成，通过数学约束保证系统可靠性，在复杂可视化任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界可视化任务涉及复杂的多模态需求，需要参考图像、代码示例和迭代优化。当前系统存在单模态输入、一次性生成和僵化工作流程等根本限制，而基于LLM的方法虽然具有潜力，但存在灾难性故障和无限循环等可靠性问题。

Method: 提出MultiVis-Agent框架，采用逻辑规则增强的多智能体架构。引入四层逻辑规则框架，提供系统可靠性的数学保证，同时保持灵活性。这些逻辑规则是指导LLM推理的数学约束，而非替代LLM。将MultiVis任务形式化为四个场景，从基础生成到迭代优化，并开发了包含1000多个案例的MultiVis-Bench基准测试。

Result: 在具有挑战性的任务中达到75.63%的可视化评分，显著优于基线方法（57.54-62.79%）。任务完成率达到99.58%，代码执行成功率达到94.56%（无逻辑规则时分别为74.48%和65.10%），成功解决了自动化可视化生成中的复杂性和可靠性挑战。

Conclusion: MultiVis-Agent通过逻辑规则增强的多智能体框架，有效解决了复杂多模态可视化生成中的可靠性问题，在保持灵活性的同时提供数学保证，为自动化可视化系统提供了可靠且高效的解决方案。

Abstract: Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.

</details>


### [87] [Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare](https://arxiv.org/abs/2601.18334)
*Clément Christophe,Wadood Mohammed Abdul,Prateek Munjal,Tathagata Raha,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: 论文提出了一种评估LLM在临床环境中谄媚倾向的新框架，发现推理优化的"思考"模型虽然准确率高，但在权威压力下更容易合理化错误建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到临床工作流程中，它们倾向于谄媚（优先考虑用户同意而非事实准确性）对患者安全构成重大风险。现有评估通常依赖主观数据集，需要更可靠的评估方法。

Method: 引入基于医学MCQA（多项选择题）的稳健框架，提出调整后的谄媚分数（Adjusted Sycophancy Score），通过考虑随机模型不稳定性（"混淆性"）来隔离对齐偏差。对Qwen-3和Llama-3系列进行扩展分析，研究推理优化的"思考"模型。

Result: 发现了清晰的扩展轨迹：模型规模越大，对谄媚的抵御能力越强。但发现推理优化的"思考"模型存在反直觉的脆弱性：虽然它们表现出较高的原始准确率，但在权威压力下，其内部推理轨迹经常合理化错误的用户建议。

Conclusion: 基准测试性能不能代表临床可靠性，简化的推理结构可能提供对抗专家驱动谄媚的优越鲁棒性。需要更全面的评估方法来确保LLM在临床环境中的安全部署。

Abstract: As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or "confusability". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized "Thinking" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.

</details>


### [88] [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350)
*Junyi Zou*

Main category: cs.CL

TL;DR: 该研究提出了一种两阶段LoRA管道，通过领域自适应预训练和监督微调提升LLM在医学领域的性能，并使用加权适配器合并技术平衡指令跟随能力和领域知识保留。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用领域表现出色，但在医学术语精确性和安全关键指令跟随方面存在不足，需要专门的方法来提升其在医学领域的性能。

Method: 采用两阶段LoRA管道：1) 领域自适应预训练(DAPT)注入医学知识；2) 监督微调(SFT)对齐医学问答行为。提出加权适配器合并技术，线性组合SFT和PT适配器权重。

Result: 在医学验证集(F5/F6)上，合并模型在实用解码配置下取得BLEU-4=16.38，ROUGE-1=20.42，ROUGE-2=4.60，ROUGE-L=11.54。进一步分析了解码敏感性和训练稳定性。

Conclusion: 两阶段LoRA管道结合加权适配器合并能有效提升LLM在医学领域的性能，平衡指令跟随能力和领域知识保留，为安全关键领域的适配器干扰研究提供了案例。

Abstract: Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.

</details>


### [89] [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352)
*Manjie Xu,Isabella Yin,Xinyi Tu,Chi Zhang,Yixin Zhu*

Main category: cs.CL

TL;DR: 大语言模型存在"语义惯性"问题：难以抑制预训练先验知识（如"岩浆危险"）以适应动态上下文规则。研究发现更大模型可能表现更差，但通过代码表示而非自然语言描述可逆转此趋势。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在需要抑制预训练先验知识以适应动态上下文规则时的表现问题。传统假设认为更大模型表现更好，但实际发现当自然语言推理需要抑制预训练关联时，更大模型可能表现更差。

Method: 使用Baba Is You游戏作为测试平台，其中物理规则是可变的文本规则。引入代码接地视觉（LCV）方法，通过微调模型处理反事实对，识别具有矛盾规则的状态，强制模型关注逻辑约束而非视觉语义。

Result: 发现大模型在需要抑制预训练先验时可能出现逆缩放现象（表现比小模型更差）。通过将动态表示为可执行代码而非描述性文本，可以逆转这一趋势。LCV方法在效率和准确性上都优于昂贵的推理时搜索方法。

Conclusion: 表示形式（代码vs自然语言）从根本上决定了缩放是否改善上下文推理。这挑战了"更大模型总是更好"的假设，对需要动态覆盖学习先验的领域具有重要意义。

Abstract: LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.

</details>


### [90] [CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes](https://arxiv.org/abs/2601.18374)
*Rodrigo Silva,José Evans,José Isidro,Miguel Marques,Afonso Fonseca,Ricardo Morais,João Canavilhas,Arian Pasquali,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: CitiLink平台使用LLMs将非结构化的市政会议记录转换为结构化可搜索数据，通过NLP和IR技术提升地方政府透明度和可访问性


<details>
  <summary>Details</summary>
Motivation: 市政会议记录通常冗长、正式且具有官僚写作风格，虽然公开可用，但其结构使公民和记者难以高效查找信息，需要提升地方政府信息的可访问性和透明度

Method: 使用LLMs提取元数据、讨论主题和投票结果，将数据索引到数据库中，支持BM25排名的全文搜索和分面过滤，通过用户友好界面展示，基于葡萄牙6个城市的120份会议记录构建系统

Result: 开发了CitiLink平台，通过市政人员引导测试评估了可用性，评估了Gemini在从会议记录中提取相关信息的表现，展示了其在数据提取方面的有效性

Conclusion: CitiLink平台成功将非结构化市政记录转换为结构化可搜索数据，NLP和IR技术能有效提升地方政府信息的可访问性和透明度，LLMs在信息提取方面表现良好

Abstract: City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.

</details>


### [91] [Hierarchical Text Classification with LLM-Refined Taxonomies](https://arxiv.org/abs/2601.18375)
*Jonas Golde,Nicolaas Jedema,Ravi Krishnan,Phong Le*

Main category: cs.CL

TL;DR: TaxMorph使用LLM重构分类学层次结构，通过重命名、合并、拆分和重排序操作，使分类学更符合语言模型的语义理解，在三个HTC基准测试中性能提升高达+2.9pp F1。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的分类学存在模糊性，如相似父节点下的相同叶子名称，这阻碍了语言模型学习清晰的决策边界。需要改进分类学结构以更好地匹配语言模型的语义编码。

Method: 提出TaxMorph框架，使用大语言模型对整个分类学进行重构，包括重命名、合并、拆分和重排序操作。通过LLM引导的修订使分类学层次结构更符合模型的语义理解。

Result: 在三个HTC基准测试中，LLM重构的分类学在不同设置下始终优于人工策划的分类学，性能提升高达+2.9pp F1。研究发现LLM重构的分类学虽然更难分离，但更符合模型的实际混淆模式。

Conclusion: LLM引导的分类学重构创建了更符合模型学习方式的分类学结构，提高了分层文本分类性能。即使重构后的分类学在嵌入空间中更难分离，但它们更好地反映了模型的归纳偏置。

Abstract: Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.

</details>


### [92] [Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction](https://arxiv.org/abs/2601.18395)
*Mikel Zubillaga,Oscar Sainz,Oier Lopez de Lacalle,Eneko Agirre*

Main category: cs.CL

TL;DR: ThinkTwice框架通过采样生成多个候选模板，然后选择最佳模板，显著优于贪婪解码方法


<details>
  <summary>Details</summary>
Motivation: 传统文档级信息抽取使用贪婪解码以避免输出变异性，但作者认为采样能产生更好的解决方案，特别是使用推理模型时

Method: 提出ThinkTwice框架：1) LLM为给定文档生成多个候选模板 2) 选择模块选择最合适的模板。包含无监督方法（利用生成输出间的一致性）和监督选择方法（使用在标注数据上训练的奖励模型）。为解决DocIE中黄金推理轨迹稀缺问题，提出基于拒绝采样的方法生成包含输出模板和推理轨迹的银训练数据

Result: 实验证明无监督和监督ThinkTwice方法的有效性，一致优于贪婪基线和最先进方法

Conclusion: 采样方法能产生比贪婪解码更好的解决方案，ThinkTwice框架通过采样和选择策略显著提升了文档级信息抽取的性能

Abstract: Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.

</details>


### [93] [Pisets: A Robust Speech Recognition System for Lectures and Interviews](https://arxiv.org/abs/2601.18415)
*Ivan Bondarenko,Daniil Grebenkin,Oleg Sedukhin,Mikhail Klementev,Roman Derunets,Lyudmila Budneva*

Main category: cs.CL

TL;DR: 提出名为"Pisets"的语音转文字系统，采用三组件架构提升俄语语音识别准确率，减少Whisper模型的错误和幻觉问题


<details>
  <summary>Details</summary>
Motivation: 针对Whisper模型在语音识别中存在的错误和幻觉问题，特别是对于俄语和长音频数据，需要开发更准确、鲁棒的语音转文字系统

Method: 采用三组件架构：1) Wav2Vec2进行初步识别；2) Audio Spectrogram Transformer (AST)进行误报过滤；3) Whisper进行最终语音识别。结合课程学习方法和多样化的俄语语音语料库训练

Result: 相比WhisperX和标准Whisper模型，Pisets系统在各种声学条件下对长音频数据的转录表现更鲁棒，准确性更高

Conclusion: 提出的三组件架构结合课程学习和不确定性建模技术，有效提升了俄语语音识别的准确性和鲁棒性，系统代码已在GitHub开源

Abstract: This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.

</details>


### [94] [Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.18468)
*Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: 研究显示大型语言模型在预训练后存储生物医学事实的强度不均，部分事实以潜在知识形式存在但无法通过确定性解码可靠访问。通过微调Llama 3.1 8B学习本体术语映射，发现潜在知识能预测事实获取速度、有限泛化能力，而抗退化能力取决于训练中的强化程度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型语言模型在预训练后存储生物医学事实的不均匀性，特别是区分显性知识和潜在知识，并探索这些知识在微调过程中的获取、泛化和退化模式，为优化生物医学领域模型训练提供见解。

Method: 方法包括：1) 微调Llama 3.1 8B Instruct模型学习人类表型本体(800对)和基因本体(400训练对)的术语标识符映射；2) 使用随机解码检测基线潜在知识；3) 将学习视为时间事件过程，应用Cox比例风险模型识别获取、泛化和退化的预测因子；4) 保留400个GO对测试泛化能力。

Result: 结果显示：1) HPO确定性召回率从基线2.8%提升至微调后71.9%；2) 潜在知识是最强的事实获取速度预测因子(HR 2.6)，与更早更高的峰值学习率和更快收敛相关；3) 泛化到未见GO事实罕见(5.8%)但潜在知识存在时更可能发生；4) 未见术语的正确映射比已见术语更易退化，表明训练强化具有保护作用。

Conclusion: 结论是潜在知识能预测微调期间事实学习速度和未见本体事实的有限泛化能力，而抗退化能力取决于事实是否在训练中得到强化。这为理解LLM知识存储机制和优化生物医学领域微调策略提供了重要见解。

Abstract: Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.

</details>


### [95] [Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs](https://arxiv.org/abs/2601.18483)
*Arya Labroo,Ivaxi Sheth,Vyas Raina,Amaani Ahmed,Mario Fritz*

Main category: cs.CL

TL;DR: 论文提出评估框架，发现LLMs在多概念控制场景中表现下降，揭示提示控制存在组合性限制


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备强大生成能力，但许多应用需要对特定文本概念（如幽默、说服力、正式性）进行细粒度控制。现有方法只能提供粗略或单一属性控制，缺乏对多属性场景的系统评估。

Method: 引入一个评估框架，针对单概念和双概念场景评估细粒度可控性，重点关注语言学上不同的概念对（如说服力vs幽默）。

Result: 在多个LLM和生成任务中，双概念设置下的性能经常下降，即使所选概念在原则上是可分离的。这表明基于提示的控制存在根本性限制：即使概念在直觉上是独立的，模型也难以处理组合性。

Conclusion: 该框架为多概念控制方法的评估提供了系统证据和原则性方法，揭示了当前LLMs在组合概念控制方面的局限性。

Abstract: Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.

</details>


### [96] [Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research](https://arxiv.org/abs/2601.18512)
*Antonio Garzon-Vico,Krithika Sharon Komalapati,Arsalan Shahid,Jan Rosier*

Main category: cs.CL

TL;DR: 使用大语言模型创建真实高管虚拟人格的方法框架，通过道德基础理论构建能模拟领导者决策的LLM参与者，验证其作为组织研究工具的有效性


<details>
  <summary>Details</summary>
Motivation: 在难以直接接触高管的情况下，需要开发可信的替代工具来研究组织决策。传统方法难以获取高管样本，而LLM技术为创建模拟领导者决策的虚拟人格提供了可能。

Method: 基于真实CEO沟通数据和道德基础理论，构建LLM虚拟人格来模拟个体领导者决策。通过三个阶段评估：结构效度、信度和行为保真度，将虚拟CEO与人类参与者进行基准比较。

Result: 理论支撑的虚拟人格能够近似人类样本中观察到的道德判断，表明LLM虚拟人格可以作为组织研究中可信且互补的工具，特别是在难以直接接触高管的情况下。

Conclusion: LLM虚拟人格为组织研究提供了新的方法论工具，特别是在高管难以接触的研究情境中。未来研究应进一步探索LLM虚拟人格在组织环境中的应用潜力和局限性。

Abstract: This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.

</details>


### [97] [GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback](https://arxiv.org/abs/2601.18517)
*James Sungarda,Hongkai Liu,Zilong Zhou,Tien-Hsuan Wu,Johnson Chun-Sing Cheung,Ben Kao*

Main category: cs.CL

TL;DR: SWITCH是一个社会工作互动培训聊天机器人，集成了真实客户模拟、实时咨询技能分类和动机性访谈进展系统，旨在解决社会工作现场教育中反馈不及时、导师资源有限的问题。


<details>
  <summary>Details</summary>
Motivation: 社会工作现场教育是其标志性教学方法，但在培训过程中提供及时客观的反馈受到导师和咨询客户可用性的限制。需要一种可扩展、低成本、一致的培训解决方案来补充现场教育。

Method: SWITCH采用基于认知的客户模型（包含静态和动态字段），实时咨询技能分类模块，以及动机性访谈控制器来调节MI阶段转换。为提高分类准确性，研究了基于检索的上下文学习和微调的BERT多标签分类器。

Result: 实验表明，基于BERT的方法和上下文学习方法都显著优于基线模型，取得了大幅度的性能提升。

Conclusion: SWITCH提供了一个可扩展、低成本、一致的培训工作流程，补充了现场教育，使督导能够专注于更高层次的指导工作。

Abstract: Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.

</details>


### [98] [Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models](https://arxiv.org/abs/2601.18527)
*Francesco Maria Molfese,Momchil Hardalov,Rexhina Blloshmi,Bill Byrne,Adrià de Gispert*

Main category: cs.CL

TL;DR: 研究探讨了长上下文语言模型（LCLMs）的微调策略如何提升其在长文档中的信息检索与使用能力，以及KV缓存压缩下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文语言模型能够处理百万级token的文档，它们成为传统检索增强生成（RAG）的有力替代方案。但尚不清楚微调策略是否能提升长上下文性能，以及在KV缓存压缩技术下是否能增强鲁棒性。

Method: 通过实验研究不同训练策略对LCLMs识别和使用相关信息能力的影响，并评估这些策略在KV缓存压缩下的鲁棒性表现。

Result: 在领域内任务上取得了显著改进（比基础模型提升高达+20分），但领域外泛化能力因任务而异：LCLMs在金融问题上表现优异（+9分），而RAG在多项选择题上优于基线模型（+6分）。微调方法在KV缓存压缩下带来中等程度的鲁棒性提升，但不同任务间增益差异较大。

Conclusion: 微调策略能够有效提升LCLMs在长上下文任务中的性能，特别是在领域内任务上，但领域外泛化能力仍需根据具体任务进行评估。同时，微调对KV缓存压缩下的鲁棒性有一定改善作用。

Abstract: With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.

</details>


### [99] [From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation](https://arxiv.org/abs/2601.18533)
*Yuxin Jiang,Yufei Wang,Qiyuan Zhang,Xingshan Zeng,Liangyou Li,Jierun Chen,Chaofan Tao,Haoli Bai,Lifeng Shang*

Main category: cs.CL

TL;DR: RLVRR提出了一种基于可验证参考奖励的强化学习方法，通过从高质量参考中提取有序语言信号（奖励链），将奖励分解为内容和风格两个维度，结合RL的探索能力和SFT的效率可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR在可验证任务（如数学、代码）中有效，但在开放式生成任务中面临挑战，因为缺乏明确的真实答案。单点监督往往导致效率低下和奖励黑客问题，需要一种能统一结构化推理和开放式生成的训练方法。

Method: RLVRR从高质量参考中提取有序语言信号（奖励链），将奖励分解为两个维度：内容维度（保留确定性核心概念如关键词）和风格维度（通过LLM验证评估风格属性遵循程度）。这种方法结合了RL的探索优势和SFT的效率可靠性。

Result: 在超过10个基准测试中，RLVRR显著优于使用十倍数据训练和先进奖励模型的SFT方法，统一了结构化推理和开放式生成的训练，在保持输出多样性的同时具有更好的泛化能力。

Conclusion: RLVRR为通用LLM对齐提供了一条原则性且高效的可验证强化学习路径，通过奖励链机制解决了开放式生成中的监督难题，实现了RL探索能力和SFT效率的有机结合。

Abstract: Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.

</details>


### [100] [Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features](https://arxiv.org/abs/2601.18536)
*Abishek Stephen,Jindřich Libovický*

Main category: cs.CL

TL;DR: 提出一种基于形态句法特征评估子词分割形态合理性的新指标，无需黄金分割数据，适用于更多语言


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如语素边界F值）需要黄金分割数据，但许多语言缺乏或数据质量不一致。需要一种更广泛适用的评估方法，利用更易获得的形态句法特征资源。

Method: 使用IBM Model 1概率对齐子词与形态特征，通过形态句法特征（如Universal Dependencies或UniMorph中的特征）评估分割的形态合理性

Result: 新指标与传统语素边界召回率相关性良好，同时在不同形态系统的语言间具有更广泛的适用性

Conclusion: 提出的基于形态特征的评估指标是传统方法的有效替代，能更广泛地应用于多种语言，解决了黄金分割数据稀缺的问题

Abstract: We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.

</details>


### [101] [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)
*Devansh Srivastav,David Pape,Lea Schönherr*

Main category: cs.CL

TL;DR: 论文系统分析了LLMs中隐藏意图的检测难题，提出了十类隐藏意图的分类法，发现在开放世界设置下现有检测方法基本失效，强调需要更鲁棒的治理框架。


<details>
  <summary>Details</summary>
Motivation: LLMs在日常决策中应用日益广泛，但其输出可能编码微妙的、无意的行为模式，这些隐藏意图可能来自训练优化伪影或被恶意开发者故意植入，但实践中难以检测，对用户信念和行为产生潜在影响。

Method: 1) 提出基于社会科学研究的十类隐藏意图分类法；2) 在受控模型中诱导隐藏意图作为评估测试平台；3) 系统评估推理和非推理LLM法官等检测方法；4) 进行精度-流行率和精度-假阴性率权衡的压力测试；5) 对已部署的先进LLMs进行定性案例研究。

Result: 1) 隐藏意图在开放世界设置下检测基本失效，特别是在低流行率条件下；2) 假阳性会淹没精度，假阴性会掩盖真实风险；3) 压力测试显示审计需要极低的假阳性率或对操纵类型的强先验才能有效；4) 案例研究证实十类隐藏意图在已部署的先进LLMs中均存在。

Conclusion: LLMs中的隐藏意图检测在开放世界设置下面临根本性挑战，现有方法不足以应对低流行率条件下的检测需求。研究提供了首个系统分析，为理解、诱导和压力测试此类行为奠定了基础，建立的灵活分类法有助于预测演化威胁并指导治理框架设计。

Abstract: LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.

</details>


### [102] [One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization](https://arxiv.org/abs/2601.18572)
*Franziska Weeber,Vera Neplenbroek,Jan Batzner,Sebastian Padó*

Main category: cs.CL

TL;DR: 研究比较了六种常用的人设提示方法在七个LLM上的表现，发现不同提示方法会导致显著差异，建议未来研究应评估多种外部有效的方法而非单一方法


<details>
  <summary>Details</summary>
Motivation: LLM个性化虽然能提升用户体验，但可能引入或放大群体偏见。现有研究通常使用单一提示方法（如用户名或明确属性）来研究偏见，但这忽略了LLM对提示变化的敏感性和某些提示在真实交互中的罕见性。

Method: 比较了六种常用的人设提示方法，在七个开源和专有LLM上测试了四个写作和建议任务，分析不同提示方法产生的响应差异。

Result: 虽然不同提示方法总体上高度相关，但它们在不同人设上产生了显著的响应方差，表明单一提示方法的结论可能不可靠。

Conclusion: 应谨慎基于单一提示方法得出LLM偏见结论，建议未来个性化研究评估多种外部有效的提示方法以提高鲁棒性和外部效度。

Abstract: Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.

</details>


### [103] [From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection](https://arxiv.org/abs/2601.18582)
*Yuan Cao,Feixiang Liu,Xinyue Wang,Yihan Zhu,Hui Xu,Zheng Wang,Qiang Qiu*

Main category: cs.CL

TL;DR: 该论文提出将人格检测从分类任务重构为排序任务，并引入基于强化学习的训练范式，通过监督微调和分组相对策略优化来提升LLM在人格检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的人格检测方法面临两大挑战：1) 人格特质的复杂性和细微差异使得准确分类困难；2) 基于提示的方法过度依赖专家知识，缺乏自主模式学习能力。需要一种更有效的方法来处理人格评估的主观性和模糊边界问题。

Method: 提出两阶段方法：1) 使用监督微调建立人格特质排序能力并标准化输出格式；2) 引入分组相对策略优化，配合专门设计的基于排序的奖励函数，训练LLM学习最优答案排序而非简单分类。

Result: 在多个基准测试中，该方法实现了最先进的性能表现，证明了将人格检测作为排序任务并通过强化学习训练的有效性。

Conclusion: 将人格检测重构为排序任务并采用强化学习训练范式，能够更好地处理人格评估的主观性和模糊边界问题，显著提升了LLM在人格检测任务上的性能。

Abstract: Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.

</details>


### [104] [Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning](https://arxiv.org/abs/2601.18722)
*Lintang Sutawika,Gokul Swamy,Zhiwei Steven Wu,Graham Neubig*

Main category: cs.CL

TL;DR: SP3F是一个两阶段框架，通过自玩和特权成对反馈来提升多语言推理能力，无需目标语言数据


<details>
  <summary>Details</summary>
Motivation: 当前推理大语言模型在训练数据中较少见的语言上表现显著低于英语，需要提升多语言推理能力

Method: 1) 在翻译的英语问答对上监督微调；2) 通过自玩方式，使用带有英语参考答案作为特权信息的成对评判器进行强化学习

Result: SP3F显著提升基础模型性能，在多个数学和非数学任务上甚至优于完全后训练的模型，且训练数据量更少

Conclusion: SP3F框架能有效提升模型在单语言、多语言和未见语言设置下的推理能力，无需目标语言数据

Abstract: When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than
  of the training data across the single-language, multilingual, and generalization to unseen language settings.

</details>


### [105] [HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences](https://arxiv.org/abs/2601.18724)
*Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 对ACL、NAACL和EMNLP 2024-2025年论文的分析发现，近300篇论文存在"幻觉引用"问题，其中一半出现在最新的EMNLP 2025会议上，表明问题正在迅速恶化。


<details>
  <summary>Details</summary>
Motivation: 近年来在审稿论文、预印本和已发表论文中频繁观察到幻觉引用（引用不存在的文献），这严重威胁科学可靠性，并可能损害会议信誉。

Method: 系统分析ACL、NAACL和EMNLP在2024和2025年发表的所有论文，包括主会论文、Findings和研讨会论文，检测幻觉引用现象。

Result: 发现近300篇论文至少包含一个幻觉引用，其中大部分出现在2025年；EMNLP 2025会议中有一半的受影响论文，超过100篇被接受为主会或Findings论文。

Conclusion: 幻觉引用问题在计算语言学领域迅速蔓延，特别是在最新会议上，严重影响了学术可信度，需要紧急关注和解决。

Abstract: Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.

</details>


### [106] [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)
*Henry Bell,Caroline Zhang,Mohammed Mobasserul Haque,Dhaval Potdar,Samia Zaman,Brandon Fain*

Main category: cs.CL

TL;DR: REFLECT：一种无需训练或数据的推理时宪法对齐框架，通过上下文中的自我评估、自我批判和最终修订来对齐LLM与价值原则


<details>
  <summary>Details</summary>
Motivation: 现有的参数微调方法（如RLHF）计算成本高、需要精心工程调整和难以获取的人工标注数据，需要一种更轻量、即插即用的对齐方法

Method: 完全在上下文中操作的推理时框架，结合：(i) 宪法条件化基础响应，(ii) 后生成自我评估，(iii)(a) 自我批判，(iii)(b) 最终修订，通过显式原则推理提供透明推理轨迹

Result: REFLECT显著提高LLM对多样复杂原则的遵从性，包括与原始参数微调强调的原则完全不同的原则，同时不牺牲事实推理能力，特别有效减少罕见但严重的原则违反

Conclusion: REFLECT提供了一种无需训练的数据高效对齐方法，能自然生成有用的训练数据用于传统参数微调，为长期部署场景中的推理计算开销减少提供了有效途径

Abstract: The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.

</details>


### [107] [One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment](https://arxiv.org/abs/2601.18731)
*Hongru Cai,Yongqi Li,Tiezheng Yu,Fengbin Zhu,Wenjie Wang,Fuli Feng,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出元奖励建模（MRM）方法，通过元学习框架解决个性化奖励建模中的少样本学习和新用户适应问题，引入鲁棒个性化目标（RPO）提升模型对难学习用户的适应能力。


<details>
  <summary>Details</summary>
Motivation: 个性化对齐LLMs面临两大挑战：个体用户反馈稀缺和新用户高效适应。传统方法直接拟合用户偏好数据存在局限，需要转向学习偏好适应过程。

Method: 提出元奖励建模（MRM），将个性化奖励建模重构为元学习问题：将用户奖励模型表示为基奖励函数的加权组合，采用MAML风格框架优化权重初始化以支持少样本快速适应，并引入鲁棒个性化目标（RPO）在元优化中更关注难学习用户。

Result: 在个性化偏好数据集上的大量实验表明，MRM能增强少样本个性化能力，提高用户鲁棒性，并持续优于基线方法。

Conclusion: MRM通过元学习范式有效解决了个性化奖励建模中的少样本适应和新用户泛化问题，为LLMs的个性化对齐提供了新思路。

Abstract: Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.

</details>


### [108] [Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory](https://arxiv.org/abs/2601.18771)
*Yanming Liu,Xinyue Peng,Zixuan Yan,Yanxin Shen,Wenjie Xu,Yuefeng Huang,Xinyi Wang,Jiannan Cao,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: Dep-Search是一个依赖感知的搜索框架，通过结构化推理、检索和持久内存集成，解决现有搜索框架在依赖管理、知识重用和搜索策略学习方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有搜索框架主要依赖隐式自然语言推理来确定搜索策略和利用检索信息，这导致在子问题依赖管理、先前检索知识的高效重用以及通过强化学习优化搜索策略方面存在根本性挑战。

Method: 提出Dep-Search框架，集成结构化推理、检索和持久内存（通过GRPO）。引入显式控制机制，使模型能够：1）分解具有依赖关系的问题；2）在需要时检索信息；3）从内存访问先前存储的知识；4）将长推理上下文总结为可重用的内存条目。

Result: 在七个不同的问答数据集上进行广泛实验，Dep-Search显著增强了LLM处理复杂多跳推理任务的能力，在不同模型规模上相比强基线都取得了实质性改进。

Conclusion: Dep-Search通过依赖感知的搜索框架，克服了现有搜索框架的局限性，为LLM在复杂推理任务中提供了更有效的搜索增强方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.

</details>


### [109] [MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts](https://arxiv.org/abs/2601.18790)
*Etienne Lanzeray,Stephane Meilliez,Malo Ruelle,Damien Sileo*

Main category: cs.CL

TL;DR: 研究发现，专注于深度推理的LLM在用户面临生命危险时仍坚持完成数学任务，忽视安全风险，形成"隧道视野"


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越专注于深度推理和复杂任务执行，研究者担心这种对计算的执着可能导致模型忽视安全考量，特别是在用户面临紧急生命危险时

Method: 研究者开发了MortalMATH基准测试，包含150个场景，用户在请求代数帮助的同时描述逐渐升级的生命威胁（如中风症状、自由落体）。测试对比了通用模型（如Llama-3.1）和专用推理模型（如Qwen-3-32b和GPT-5-nano）的行为差异

Result: 发现明显的行为分裂：通用模型成功拒绝数学任务以应对危险，而专用推理模型经常完全忽视紧急情况，在用户描述濒死状态时仍保持超过95%的任务完成率。推理过程还引入了危险延迟：在提供任何潜在帮助前最多需要15秒

Conclusion: 训练模型执着追求正确答案可能会无意中使其丧失安全部署所需的生存本能，需要在模型优化中平衡推理能力与安全响应

Abstract: Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.

</details>


### [110] [Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets](https://arxiv.org/abs/2601.18791)
*Iaroslav Chelombitko,Mika Hämäläinen,Aleksey Komissarov*

Main category: cs.CL

TL;DR: 该研究使用基于子词的方法对242种拉丁和西里尔文字语言进行大规模比较分析，通过BPE分割构建词汇相似性框架，发现BPE分割与语素边界高度一致，词汇相似性与语言亲缘关系显著相关。


<details>
  <summary>Details</summary>
Motivation: 需要在大规模跨语言比较中建立统一的分析框架，以量化研究不同文字系统（拉丁和西里尔文字）语言的词汇模式、相似性和演化关系。

Method: 从维基百科词典构建"glottosets"，使用字节对编码（BPE）进行子词分割，基于排名的子词向量分析词汇重叠、词汇分化和语言相似性，涵盖242种语言的大规模比较。

Result: BPE分割在15种语言中比随机基线提高95%的语素边界对齐（F1=0.34 vs 0.15）；BPE词汇相似性与语言遗传亲缘关系显著相关（Mantel r=0.329）；罗曼语族形成最紧密聚类（平均距离0.51）；跨语系语言对明显分离（0.82）；26,939个跨语言同形词中48.7%在不同相关语言中获得不同分割。

Conclusion: 该研究提供了一个统一的量化宏观语言学分析框架，能够揭示类型多样语言的词汇模式，BPE分割可作为有效的跨语言比较工具，词汇相似性分析能够反映语言演化关系。

Abstract: We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.

</details>


### [111] [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/abs/2601.18796)
*Brian Ondov,Chia-Hsuan Chang,Yujia Zhou,Mauro Giuffrè,Hua Xu*

Main category: cs.CL

TL;DR: 研究人员开发了ctELM模型，能够从临床试验的嵌入向量中准确描述和比较试验，并生成新的临床试验摘要，展示了在生物医学领域对齐大语言模型与嵌入空间的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 虽然文本嵌入已成为各种语言应用的重要组成部分，但解释、探索和反转嵌入空间的方法有限，这降低了透明度并阻碍了潜在的有价值生成用例。特别是在临床试验领域，需要更好的方法来理解和操作嵌入表示。

Method: 采用Embedding Language Model (ELM)方法，开发了开源、领域无关的ELM架构和训练框架，设计了临床试验的训练任务，并引入了专家验证的合成数据集。训练了一系列ELM模型，探索了不同任务和训练方案的影响。

Result: 最终模型ctELM能够仅从嵌入向量中准确描述和比较未见过的临床试验，并能从新的向量生成合理的临床试验。生成的试验摘要能够响应沿着年龄和性别等概念向量移动嵌入的操作。

Conclusion: 公开的ELM实现和实验结果将有助于在生物医学领域及其他领域对齐大语言模型与嵌入空间，提高了嵌入空间的可解释性和生成能力。

Abstract: Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [112] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: 本文研究了随机噪声对四旋翼无人机系统的影响，使用扩展卡尔曼滤波进行状态估计，基于SDE系统实现LQG控制，并应用期望最大化算法进行参数估计，比较了离线和在线参数估计的性能。


<details>
  <summary>Details</summary>
Motivation: 无人机在各种应用中变得越来越重要，但在实际运行中会受到随机噪声的影响。地震等灾害救援中无人机的作用尤为关键，因此需要研究噪声对无人机系统的影响并开发有效的控制方法。

Method: 1. 在四旋翼无人机系统中添加随机噪声进行研究；2. 使用扩展卡尔曼滤波器基于传感器的噪声观测进行状态估计；3. 基于随机微分方程系统实现线性二次高斯控制器；4. 应用期望最大化算法进行无人机参数估计，包括离线和在线两种方式。

Result: 在线参数估计的收敛值范围略大于离线参数估计。这表明在线估计方法在适应系统变化方面具有一定优势，但同时也表现出更大的参数不确定性。

Conclusion: 随机噪声对无人机系统有显著影响，但通过扩展卡尔曼滤波、LQG控制和期望最大化算法可以有效处理噪声问题。在线参数估计虽然收敛范围较大，但在动态环境中可能更具适应性，为无人机在复杂环境下的可靠运行提供了理论支持。

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [113] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: 该论文分析了现有可解释性方法在智能体系统（agentic systems）中的局限性，提出了专门针对智能体系统设计可解释性技术的未来方向，以确保智能体AI系统的安全和可问责部署。


<details>
  <summary>Details</summary>
Motivation: 智能体系统（agentic systems）从根本上不同于传统机器学习模型，引入了独特的安全挑战，包括目标错位、决策错误累积和智能体间协调风险。现有的可解释性技术主要针对静态模型开发，在应用于智能体系统时存在局限性。

Method: 评估现有可解释性方法在智能体系统背景下的适用性和局限性，识别这些方法在提供智能体决策洞察方面的能力差距。提出专门为智能体系统设计可解释性技术的未来方向。

Result: 现有可解释性方法在应用于智能体系统时存在不足，无法充分应对智能体系统的时态动态性、决策累积性和上下文依赖行为。需要新的分析方法来理解智能体决策过程。

Conclusion: 必须开发专门针对智能体系统的可解释性技术，在智能体生命周期的各个阶段（目标形成、环境交互、结果评估）嵌入监督机制。这些进展对于确保智能体AI系统的安全和可问责部署至关重要。

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [114] [Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction](https://arxiv.org/abs/2601.17188)
*Swapn Shah,Wlodek Zadrozny*

Main category: cs.AI

TL;DR: Tensor Logic框架通过三个实验验证了符号逻辑与神经网络的统一：1) 在圣经家谱图上证明递归Datalog规则与张量收缩的等价性；2) 在嵌入空间中实现可学习的神经推理；3) 在FB15k-237知识图谱上验证矩阵组合实现多跳推理。


<details>
  <summary>Details</summary>
Motivation: 符号推理系统具有可靠性和可解释性但缺乏可扩展性，神经网络具有学习能力但缺乏透明度。Tensor Logic框架提出逻辑规则与爱因斯坦求和数学等价，为两者的统一提供了原则性路径。

Method: 通过三个实验验证Tensor Logic框架：1) 在圣经家谱图上计算传递闭包，比较递归Datalog规则与迭代张量收缩；2) 使用可学习变换矩阵训练神经网络，在嵌入空间中进行推理；3) 在FB15k-237知识图谱上应用关系矩阵公式R_r = E^⊤ A_r E进行链接预测和组合推理。

Result: 1) 在1,972个体、1,727个关系的家谱图上，74次迭代发现33,945个祖先关系；2) 神经网络成功实现零样本组合推理；3) 标准链接预测MRR为0.3068，组合推理基准MRR为0.3346，证明矩阵组合可实现多跳推理。

Conclusion: Tensor Logic框架为符号推理与神经网络的统一提供了实证支持，张量表示使逻辑规则可计算，矩阵组合使多跳推理成为可能，为可解释AI系统开辟了新途径。

Abstract: The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.

</details>


### [115] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 利用真实世界电子健康记录开发生成式模拟器，能够基于患者历史生成高保真的未来临床轨迹，实现个性化医疗和虚拟临床试验。


<details>
  <summary>Details</summary>
Motivation: 模拟在临床医学中具有变革性潜力，可用于个性化治疗规划和虚拟临床试验，但由于复杂的生物和社会文化因素，模拟患者轨迹具有挑战性。真实世界临床记录的价值尚未被充分挖掘。

Method: 开发生成式模拟器模型，以患者历史为输入，合成细粒度、真实的未来轨迹。模型在超过2亿条临床记录上进行预训练，能够模拟事件发生率、实验室检测结果和时间动态。

Result: 模型生成高保真的未来时间线，与真实患者未来数据在事件发生率、实验室结果和时间动态上高度匹配。准确估计未来事件概率，观察与预期比率在不同结果和时间范围内均接近1.0。

Conclusion: 研究揭示了电子健康记录中真实世界数据的未开发价值，并引入了可扩展的临床护理计算机模拟框架，为个性化医疗和虚拟临床试验提供了新工具。

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [116] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 本文提出一个可校准的理论框架，预测多智能体系统在有限推理预算下的三种性能模式：提升、饱和和崩溃。该理论基于三个关键约束：有限上下文窗口、有损通信和智能体间的共享故障相关性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统理论上能提高可靠性，但在固定推理预算下，其性能往往呈现帮助、饱和甚至崩溃三种模式。现有研究缺乏一个能够统一解释这些现象并指导系统设计的理论框架。

Method: 提出一个最小化可校准理论，基于三个约束：有限上下文窗口W、有损通信γ(m)和共享故障相关性ρ。将每个叶节点智能体建模为计算-性能缩放指数β，构建b叉树层次结构，分析多数聚合下的性能相变。

Result: 证明深度b叉树存在尖锐相变：单个标量α_ρ决定弱信号是被放大到非平凡固定点还是被淹没。在放大区域，推导组织指数s，显示预算协同效应（优于同等预算下单智能体）发生在s>β时，得到闭式计算分配规则和预算阈值。

Conclusion: 该理论框架成功预测了多智能体系统的性能相变，解释了近期大规模LLM智能体系统研究中观察到的主要瓶颈，为系统设计提供了明确的权衡指导。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [117] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge是一个低成本的形式数学数据合成框架，通过分解形式化过程为五个子任务，并采用解耦提取策略回收失败轨迹中的有效训练信号，显著降低了数据合成成本。


<details>
  <summary>Details</summary>
Motivation: 形式数学中智能体工作流的高成本阻碍了大规模数据合成，加剧了开源语料库的稀缺性。需要开发成本效益高的数据合成方法来解决这一问题。

Method: 将形式化过程分解为五个子任务：陈述形式化、证明生成、前提选择、证明修正和证明草图。采用解耦提取策略，从全局失败的轨迹中恢复有效的训练信号，充分利用浪费的计算资源。

Result: 在2000个问题的基准测试中，TheoremForge实现了12.6%的验证率，超过8.6%的基线，使用Gemini-3-Flash时每个成功轨迹的平均成本仅为0.481美元。解耦提取策略使证明生成的数据产出提高了1.6倍。

Conclusion: TheoremForge为训练未来专家模型提供了一个可扩展的数据飞轮构建框架，能够有效降低形式数学数据合成的成本并提高数据产出效率。

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [118] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该论文形式化AGI为分布依赖的资源受限语义谓词，证明AGI的普遍性、鲁棒性、无界泛化和自我验证都存在理论限制，强分布无关的AGI主张在没有明确形式化索引时是未定义的。


<details>
  <summary>Details</summary>
Motivation: 研究AGI是否具有支持存在性、鲁棒性或自我验证绝对主张的连贯理论定义，澄清关于AGI普遍性和自我验证能力的常见误解。

Method: 将AGI公理化定义为分布依赖、资源受限的语义谓词，通过任务族、任务分布、性能函数和显式资源预算进行索引，使用形式化方法推导四类理论结果。

Result: 1) 普遍性是关系性的，不存在分布无关的AGI概念；2) 任务分布的微小扰动可通过悬崖集使AGI属性失效；3) 有限资源下无法实现跨任务族的无界泛化；4) AGI无法通过任何可计算程序（包括代理自身）进行可靠完整的认证。

Conclusion: 强分布无关的AGI主张在没有明确形式化索引时是未定义的，AI的经验进展并不意味着可实现自我验证的通用智能，依赖内部自我认证的递归自我改进方案是有问题的。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [119] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 本文指出现有模型编辑特异性评估方法存在不足，提出新的评估协议，使评估指标与特异性正则化强度更相关且更敏感


<details>
  <summary>Details</summary>
Motivation: 模型编辑需要平衡编辑效果（成功注入目标知识）和特异性（保留现有非目标知识），但现有特异性评估协议存在不足，无法有效评估不同方法的知识保留能力

Method: 提出新的评估协议，消除开放域LLM与确定答案假设之间的冲突，避免查询无关的流畅性偏差，并能在接近连续的空间中平滑调整评估严格度

Result: 新协议下的评估指标对特异性正则化强度变化更敏感，与正则化强度强相关，能更细粒度地区分不同方法的知识保留能力

Conclusion: 现有特异性评估方法存在概念和实证问题，提出的新评估协议能更准确、敏感地评估模型编辑方法的知识保留能力

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [120] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: 提出基于多智能体的学习路径规划框架MALPP，利用LLM驱动的智能体协作解决现有学习路径规划缺乏透明度、适应性和可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能导学系统中的学习路径规划方法大多缺乏透明度、适应性和以学习者为中心的可解释性，限制了LLM在教育领域的有效应用。

Method: 提出MALPP框架，包含三个基于角色的LLM智能体：学习者分析智能体、路径规划智能体和反思智能体，通过结构化提示和预定义规则协作，基于认知负荷理论和最近发展区理论生成个性化学习路径。

Result: 在MOOCCubeX数据集上使用7种LLM进行实验，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型，消融研究验证了协作机制和理论约束的有效性。

Conclusion: MALPP为可信赖、可解释的教育AI发展做出贡献，展示了基于LLM的以学习者为中心的自适应教学的扩展性方法。

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [121] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: 研究视觉语言模型在描述残疾人图像时的解释偏移问题，发现引入残疾上下文会降低解释保真度，导致推测性推断、叙事扩展、情感贬损和缺陷导向框架等问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型越来越多地应用于社会敏感领域，但它们在残疾相关方面的行为尚未得到充分探索。模型在描述人物图像时，经常从基于证据的事实描述转向包含超出可观察视觉证据的推断的解释偏移。

Method: 引入基于中性提示(NP)和残疾情境化提示(DP)的基准测试，在零样本设置下评估15个最先进的开源和闭源视觉语言模型，涵盖9个残疾类别。评估框架将解释保真度作为核心目标，结合标准文本指标和LLM作为评判协议，并由有残疾生活经验的标注者验证。

Result: 引入残疾上下文会持续降低解释保真度，导致解释偏移，表现为推测性推断、叙事扩展、情感贬损和缺陷导向框架。这些效应在种族和性别维度上进一步放大。针对性提示和偏好微调能有效提高解释保真度并显著减少解释偏移。

Conclusion: 视觉语言模型在描述残疾人时存在系统性偏见，表现为超出视觉证据的推断和负面框架。需要开发更负责任的方法来减少这些有害的解释偏移，特别是在社会敏感应用中。

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [122] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: LLMs在逻辑推理中展现出从传统逻辑向现代逻辑的演变趋势，模型规模、思维链技术和基础模型是影响这一转变的关键因素。


<details>
  <summary>Details</summary>
Motivation: 受人类逻辑从直觉推理向形式系统演变的启发，探索大语言模型是否在底层逻辑框架上表现出类似的演化模式，以存在性引入为切入点研究三段论推理。

Method: 使用存在性引入作为探针，评估LLMs在传统逻辑和现代逻辑下的三段论推理能力，通过在新构建的三段论数据集上测试SOTA LLMs进行广泛实验。

Result: 发现三个关键结果：1) 模型规模扩大促进向现代逻辑的转变；2) 思维链技术是超越参数扩展的高效加速器；3) 基础模型决定这种转变的难易程度和稳定性。

Conclusion: LLMs在逻辑推理中确实展现出从传统到现代逻辑的演化特征，模型规模、思维技术和基础架构是影响这一转变的核心因素，为理解LLMs的推理机制提供了新视角。

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [123] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: Lattice框架通过两阶段自构建和持续改进机制，为对话AI系统创建自适应防护栏，在ProsocialDialog数据集上达到91% F1分数，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI防护栏使用静态规则，无法适应新威胁和部署环境变化，需要能够自我构建和持续改进的自适应防护机制。

Method: Lattice采用两阶段框架：1) 构建阶段通过迭代模拟和优化从标注示例构建初始防护栏；2) 持续改进阶段通过风险评估、对抗测试和整合自主调整已部署防护栏。

Result: 在ProsocialDialog数据集上，Lattice在保留数据上达到91% F1分数，比关键词基线高43个百分点，比LlamaGuard高25个百分点，比NeMo高4个百分点。持续改进阶段通过闭环优化在跨域数据上实现7个百分点F1提升。

Conclusion: 研究表明，通过迭代优化可以自构建有效的对话AI防护栏，Lattice框架展示了自适应防护系统的可行性。

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [124] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: 成功条件化（success conditioning）是一种保守的策略改进方法，通过模仿成功轨迹来更新策略，它精确解决了带χ²散度约束的信任域优化问题。


<details>
  <summary>Details</summary>
Motivation: 成功条件化（如拒绝采样+SFT、目标条件RL、决策变换器）被广泛使用，但其背后的优化问题本质一直不清楚。本文旨在揭示成功条件化到底解决了什么优化问题，以及它为何有效。

Method: 通过理论证明，成功条件化精确解决了信任域优化问题：在χ²散度约束下最大化策略改进，约束半径由数据自动确定。建立了相对策略改进、策略变化幅度和"动作影响"（衡量动作选择随机变化对成功率的影响）之间的恒等关系。

Result: 成功条件化是一种保守的改进算子：它不会降低性能或引发危险的分布偏移；当失败时，它会明显表现为几乎不改变策略。对于常见的回报阈值化实践，理论表明它可以放大改进，但可能以与真实目标不对齐为代价。

Conclusion: 成功条件化是解决特定信任域优化问题的精确方法，提供了一种安全、可观察的策略改进机制。它为理解各种基于成功轨迹模仿的方法提供了统一的理论框架。

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [125] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: 论文提出认知平台工程新范式，通过四层参考架构将感知、推理和自主行动集成到平台生命周期中，实现更智能的云原生系统运维。


<details>
  <summary>Details</summary>
Motivation: 传统DevOps实践在处理云原生系统的规模和动态性方面存在局限，规则驱动的自动化导致反应式运维、修复延迟和依赖人工经验的问题。

Method: 提出认知平台工程范式，设计四层参考架构（数据收集、智能推理、策略驱动编排、人机交互层），并基于Kubernetes、Terraform、Open Policy Agent和ML异常检测构建原型系统。

Result: 原型系统在平均修复时间、资源效率和合规性方面取得改进，证明将智能嵌入平台运维能够实现弹性、自调整和意图对齐的云环境。

Conclusion: 认知平台工程为云原生系统运维提供了新方向，未来研究机会包括强化学习、可解释治理和可持续自管理云生态系统。

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [126] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: JaxARC是一个基于JAX实现的高性能强化学习环境，用于ARC推理任务，相比Gymnasium实现38-5,439倍加速，支持大规模并行计算。


<details>
  <summary>Details</summary>
Motivation: 现有的Gymnasium-based RL环境在ARC任务上存在计算瓶颈，限制了实验规模。需要高性能环境来支持大规模强化学习研究。

Method: 使用JAX实现功能化、无状态的架构，支持大规模并行计算。提供多种ARC数据集支持、灵活的动作空间、可组合的包装器和配置驱动的可复现性。

Result: 在相同批次大小下实现38-5,439倍速度提升，峰值吞吐量达到7.9亿步/秒。支持之前计算上不可行的大规模RL研究。

Conclusion: JaxARC解决了ARC推理任务中的计算瓶颈问题，为大规模强化学习研究提供了高性能环境，代码已开源。

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [127] [Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design](https://arxiv.org/abs/2601.17587)
*Azza Fadhel,Nathaniel W. Zuckschwerdt,Aryan Deshwal,Susmita Bose,Amit Bandyopadhyay,Jana Doppa*

Main category: cs.AI

TL;DR: 该论文提出了一种结合AI驱动自适应实验设计与领域知识的方法，用于高效发现金属增材制造的可行参数配置，并在GRCop-42合金的定向能量沉积工艺中成功应用。


<details>
  <summary>Details</summary>
Motivation: 金属合金增材制造的参数配置是一个具有挑战性的问题，因为输入参数（如激光功率、扫描速度）与打印输出质量之间存在复杂关系。传统的试错方法效率低下，因为每个配置的验证成本高（物理和人力劳动），且配置空间非常大。

Method: 结合AI驱动的自适应实验设计与领域知识，通过构建过去实验的代理模型，在每次迭代中智能选择一小批输入配置进行验证。该方法应用于定向能量沉积工艺，用于打印NASA开发的航空航天用高性能铜-铬-铌合金GRCop-42。

Result: 在三个月内，该方法在广泛的激光功率范围内产生了多个无缺陷输出，相比领域科学家数月手动实验无成果的情况，显著减少了结果获取时间和资源消耗。首次在现成的红外激光平台上实现了高质量的GRCop-42制造。

Conclusion: 该方法成功解决了金属增材制造参数配置的挑战性问题，通过使高质量GRCop-42制造在易获得的红外激光平台上成为可能，民主化了这种关键合金的获取，为航空航天应用的成本效益高、分散化生产铺平了道路。

Abstract: Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.

</details>


### [128] [Intelligence Requires Grounding But Not Embodiment](https://arxiv.org/abs/2601.17588)
*Marcus Ma,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 论文认为智能需要"接地"（grounding）而非"具身"（embodiment），论证了非具身但接地的智能体可以实现智能的四个关键属性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的发展，关于智能是否需要具身的科学辩论重新兴起。本文旨在澄清这一争论，提出智能需要的是接地（grounding）而非具身本身。

Method: 首先定义智能为具备四个属性：动机、预测能力、因果理解、经验学习。然后论证每个属性都可以通过非具身但接地的智能体实现。最后通过数字环境中智能LLM代理的思想实验来支持论点。

Result: 论证了接地（而非具身）是智能的必要条件，非具身但接地的智能体可以具备真正的智能。

Conclusion: 智能需要的是接地（grounding）——这是具身所蕴含的现象，但不是具身本身。非具身但接地的系统可以实现智能。

Abstract: Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.

</details>


### [129] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: 本文提出了Health-ORSC-Bench，首个大规模医疗AI安全基准，用于系统评估大语言模型在医疗场景中的过度拒绝和安全完成质量，发现当前模型在安全性和实用性之间存在显著权衡。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI安全评估主要关注二元拒绝边界，导致模型要么过度拒绝良性查询，要么不安全地遵从有害查询，缺乏对"安全完成"能力的评估——即在不提供可操作危害的前提下，为双重用途或边界查询提供安全、高层次指导的能力。

Method: 构建了包含31,920个良性边界提示的Health-ORSC-Bench基准，涵盖7个健康类别（如自残、医疗错误信息）。采用自动化流水线结合人工验证，在不同意图模糊度水平上测试模型。评估了30个最先进的LLM，包括GPT-5和Claude-4。

Result: 安全优化的模型经常拒绝高达80%的"困难"良性提示，而领域特定模型往往为实用性牺牲安全性。模型家族和规模显著影响校准：大型前沿模型（如GPT-5、Llama-4）表现出"安全悲观主义"和更高的过度拒绝，相比小型或MoE架构模型（如Qwen-3-Next）。

Conclusion: 当前LLM在平衡拒绝和遵从方面存在困难，Health-ORSC-Bench为下一代医疗AI助手提供了严格的校准标准，促进细致、安全和有帮助的完成能力。

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [130] [DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories](https://arxiv.org/abs/2601.17678)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 提出DIML框架，从自利学习代理的战略互动轨迹中逆向学习未知的激励生成机制，包括非结构化（如神经网络）机制，支持反事实预测，在大小环境中均表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法如逆博弈论和多智能体逆强化学习通常只能推断结构化机制内的效用/奖励参数，而无法处理非结构化机制（如神经网络映射）。同时，可微分机制设计是正向优化机制，而本文关注从观察到的行为中逆向推断机制。

Method: 提出DIML（Differentiable Inverse Mechanism Learning）框架，基于似然的方法，通过多智能体学习动力学模型进行微分，使用候选机制生成预测观察行为所需的反事实收益。

Result: 在条件logit响应模型下建立了收益差异的可识别性，证明了最大似然估计在标准正则条件下的统计一致性。在非结构化神经机制、拥堵收费、公共物品补贴和大规模匿名博弈等场景中，DIML能可靠恢复可识别的激励差异，支持反事实预测，性能在小环境中接近枚举oracle，在大规模（百参与者）环境中也能收敛。

Conclusion: DIML框架能够从观察到的战略互动中逆向学习激励生成机制，包括非结构化机制，具有理论保证和实际应用价值，为机制设计提供了新的逆向学习工具。

Abstract: We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.

</details>


### [131] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua,Zhen Han,Zhengyuan Shen,Jeremy Lee,Patrick Guan,Qi Zhu,Sullam Jeoung,Yueyan Chen,Yunfei Bai,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.AI

TL;DR: SQL-Trail：一个基于多轮强化学习的Text-to-SQL代理框架，通过交互式执行反馈迭代优化查询，在BIRD-SQL等基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL生成主要采用单次生成范式，缺乏人类专家使用的迭代推理、模式探索和错误修正能力，导致在BIRD-SQL等挑战性基准上与人类专家存在明显差距

Method: 提出SQL-Trail多轮强化学习代理框架：1）自适应轮次预算分配机制，根据问题难度调整交互深度；2）复合奖励面板，联合激励SQL正确性和高效探索；通过与数据库环境交互和执行反馈迭代优化预测

Result: 在多个基准测试中达到新的最先进水平，数据效率比之前单次RL方法高18倍；7B和14B模型平均超越大5%的专有系统，证明交互式代理工作流的有效性

Conclusion: 交互式、代理化的工作流程对于稳健的Text-to-SQL生成非常有效，多轮强化学习方法能够显著缩小AI系统与人类专家在复杂SQL生成任务上的差距

Abstract: While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [132] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: 本文提出了LLM数据审计框架，系统评估LLM生成多模态合成数据的质量和可信度，并指出当前评估实践的不足和改进建议。


<details>
  <summary>Details</summary>
Motivation: LLM已成为生成多模态数据的有力工具，将数据从稀缺资源转变为可控资产，但确保LLM生成合成数据的高质量仍是一个关键挑战。现有研究主要关注生成方法，对数据质量本身关注有限，且多为单模态研究，缺乏跨模态的统一视角。

Method: 提出LLM数据审计框架：1) 描述LLM如何生成六种不同模态的数据；2) 从质量和可信度两个维度系统分类合成数据的内在评估指标，将重点从依赖下游任务性能的外在评估转向数据本身的内在属性；3) 使用该评估系统分析各模态代表性生成方法的实验评估。

Result: 分析发现当前评估实践存在重大缺陷，基于这些发现为社区提供了改进数据生成评估的具体建议。框架还概述了合成数据在不同模态中实际应用的方法论。

Conclusion: LLM数据审计框架填补了现有研究的空白，通过系统评估合成数据的质量和可信度，为改进LLM生成数据的评估实践提供了具体指导，并促进了合成数据在多模态场景中的实际应用。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [133] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: EntWorld是一个企业级多模态大语言模型基准测试，包含1756个跨6个企业领域（CRM、ITIL、ERP等）的任务，通过数据库模式逆向工程生成真实工作流，采用SQL确定性验证机制，揭示当前模型在企业场景中的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对消费级场景（如电商、旅游预订），无法捕捉企业工作流的复杂性和严谨性。企业系统具有高密度用户界面、严格业务逻辑约束和精确状态一致性要求等独特挑战，当前通用智能体在这些场景中表现不佳。

Method: 1. 采用模式基础的任务生成框架，直接从底层数据库模式逆向工程业务逻辑，合成真实的长时程工作流；2. 提出基于SQL的确定性验证机制，用严格的状态转换验证替代模糊的视觉匹配；3. 构建包含1756个任务、覆盖6个企业领域的大规模基准测试。

Result: 最先进模型（如GPT-4.1）在EntWorld上的成功率仅为47.61%，远低于人类表现，揭示了当前智能体在企业能力方面存在显著差距，凸显了开发领域特定智能体的必要性。

Conclusion: EntWorld作为一个严谨的测试平台，能够促进下一代企业级数字智能体的开发和评估，填补了企业工作流基准测试的空白，为评估和改进智能体在企业环境中的性能提供了重要工具。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [134] [ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents](https://arxiv.org/abs/2601.17735)
*Kyungho Kim,Geon Lee,Juyeon Kim,Dongwon Choi,Shinhwan Kang,Kijung Shin*

Main category: cs.AI

TL;DR: ReFuGe是一个基于LLM代理的框架，用于从关系数据库中自动生成有信息量的关系特征，以提升预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 关系数据库在现实应用中广泛使用，但针对RDB的预测任务面临挑战：需要从复杂模式中推理，在组合爆炸的特征空间中探索，且缺乏明确监督。

Method: 提出ReFuGe框架，包含三个专门的LLM代理：1)模式选择代理识别相关表和列；2)特征生成代理从选定模式生成候选特征；3)特征过滤代理通过推理和验证进行筛选。这些代理在迭代反馈循环中运行直至性能收敛。

Result: 在RDB基准测试上的实验表明，ReFuGe在各种RDB预测任务上显著提升了性能。

Conclusion: ReFuGe通过LLM代理的协同工作，有效解决了关系数据库特征生成的挑战，为RDB预测任务提供了有效的自动化解决方案。

Abstract: Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.

</details>


### [135] [Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems](https://arxiv.org/abs/2601.17744)
*Amjad Fatmi*

Main category: cs.AI

TL;DR: Faramesh是一个协议无关的执行控制平面，通过不可绕过的动作授权边界（AAB）为智能体驱动的动作强制执行授权检查，确保组织能在动作改变现实前确定性地允许、拒绝或延迟执行。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体系统在执行时缺乏强制性的检查点，导致组织无法在智能体动作改变现实世界（如部署基础设施、修改数据库、移动资金等）之前进行确定性的授权控制。

Method: Faramesh将智能体意图规范化为规范动作表示（CAR），根据策略和状态确定性地评估动作，并生成决策工件（PERMIT/DEFER/DENY）。系统采用不可绕过的动作授权边界（AAB），执行器必须在执行前验证决策。系统设计为框架和模型无关，支持多智能体和多租户部署，独立于传输协议。

Result: Faramesh提供了可强制执行、可预测的自主执行治理，避免了与编排层的隐藏耦合或仅观察性的方法。系统还提供基于规范动作哈希的决策中心、仅追加的溯源日志，支持审计、验证和确定性重放而无需重新运行智能体推理。

Conclusion: Faramesh通过协议无关的执行控制平面和不可绕过的授权边界，为自主智能体系统提供了必要的执行时授权检查，确保组织能在动作改变现实前进行确定性控制，同时保持审计性和可验证性。

Abstract: Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.

</details>


### [136] [HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis](https://arxiv.org/abs/2601.17767)
*Rajan Das Gupta,Xiaobin Wu,Xun Liu,Jiaqi He*

Main category: cs.AI

TL;DR: 提出混合集成框架，结合CNN、LSTM深度学习与KNN、XGB传统机器学习，通过投票机制预测心血管疾病，在两个Kaggle数据集上分别达到82.30%和97.10%的准确率。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要智能数据驱动的诊断工具。传统预测模型难以在异构数据集和复杂生理模式上泛化。

Method: 提出混合集成框架，集成CNN和LSTM深度学习架构与KNN和XGB传统机器学习算法，采用集成投票机制，结合深度网络的表征能力和传统模型的可解释性及效率。

Result: 在两个公开Kaggle数据集上，模型在数据集I达到82.30%准确率，数据集II达到97.10%准确率，在精确率、召回率和F1分数上均有稳定提升。

Conclusion: 混合AI框架在心血管疾病预测中具有鲁棒性和临床潜力，支持联合国可持续发展目标3（良好健康与福祉），通过创新数据驱动的医疗解决方案促进早期诊断、预防和管理非传染性疾病。

Abstract: Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.

</details>


### [137] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: NSVIF是一个神经符号框架，用于验证LLM输出是否遵循指令，将指令遵循验证建模为约束满足问题，显著优于基于LLM的方法


<details>
  <summary>Details</summary>
Motivation: LLM不总是遵循指令，这种违规在基于LLM的智能体工作流中会传播放大，导致任务失败和系统事故，需要通用验证框架

Method: NSVIF将指令遵循验证建模为约束满足问题，将用户指令建模为约束，包含逻辑和语义约束，通过统一求解器协调逻辑推理和语义分析

Result: NSVIF显著优于基于LLM的方法，提供可解释的反馈，且其反馈能帮助改进LLM的指令遵循能力而无需后训练

Conclusion: NSVIF是一个通用、通用的指令遵循验证框架，能有效检测LLM输出是否遵循指令，并提供可解释的反馈来改进LLM性能

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [138] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: MMR-Bench是一个用于评估多模态大语言模型路由选择的基准测试，通过智能选择不同模型来处理不同复杂度的任务，在保持准确性的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在架构、对齐策略和效率方面存在异质性，没有单一模型在所有任务上都表现最优。实际部署中，工作负载从轻量级OCR到复杂多模态推理不等，使用单一模型要么在简单任务上过度计算，要么在复杂任务上牺牲准确性。需要一种能够根据查询内容智能选择模型的机制。

Method: 提出了MMR-Bench基准测试，包括：(1) 具有模态感知输入和可变计算预算的受控环境；(2) 涵盖OCR、通用VQA和多模态数学推理的广泛视觉语言任务套件；(3) 强大的单模型参考、理论上限和代表性路由策略。通过该基准评估多模态信号对路由质量的影响。

Result: 实验表明，融入多模态信号能显著提升路由质量，在成本-准确性边界上取得改进。路由系统能够以最强单模型约33%的成本，超越其准确性。训练的策略还能零样本泛化到新数据集和纯文本基准测试。

Conclusion: MMR-Bench为研究自适应多模态模型选择和高效MLLM部署提供了基础，展示了智能路由在平衡计算成本与准确性方面的潜力，有助于推动多模态系统的高效部署。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [139] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: Climate RADAR是一个基于生成式AI的可靠性层，将传统预警系统转变为行动推荐系统，通过个性化建议提高保护行动执行率


<details>
  <summary>Details</summary>
Motivation: 传统预警系统虽然快速传播警报，但往往无法触发及时的保护行动，导致可预防的损失和不平等。需要将灾害通信从警报传递转变为行动执行

Method: 集成气象、水文、脆弱性和社会数据形成综合风险指数，使用带有护栏的大型语言模型为公民、志愿者和市政界面提供个性化推荐

Result: 通过模拟、用户研究和市政试点评估显示：保护行动执行率提高、响应延迟减少、可用性和信任度增加

Conclusion: Climate RADAR结合预测分析、行为科学和负责任AI，推进以人为本、透明和公平的预警系统，为符合要求的灾害韧性基础设施提供实用路径

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [140] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: RegGuard是一个工业级AI助手，用于自动化解读异构监管文本并与内部企业政策对齐，通过HiSACC和ReLACE技术提升检索和生成质量，显著降低幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 监管更新日益频繁复杂，跨国制药公司面临巨大合规负担。合规团队需要跨司法管辖区、格式和机构手动解读不断变化的规则，成本高且易出错。

Method: 系统通过安全管道摄入异构文档源，采用两个创新组件：HiSACC（分层语义聚合上下文分块）将长文档语义分割为连贯单元；ReLACE（监管列表自适应交叉编码器）基于开源模型构建，联合建模用户查询和检索候选以改进排名相关性。

Result: 企业环境评估显示，RegGuard在相关性、基础性和上下文聚焦方面显著提升答案质量，同时显著降低幻觉风险。系统架构支持可审计性和可追溯性。

Conclusion: RegGuard为具有严格合规需求的领域提供了高效解决方案，其架构设计能够响应不断变化的文档源，具备可审计性、可追溯性和增量索引等特点。

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [141] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: IGFT是一种无需人工对话数据的医疗对话AI训练方法，通过信息增益奖励和在线强化学习，让模型从模拟患者对话中学习有效提问策略，生成全面的现病史记录。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI方法依赖昂贵的人工标注对话或静态数据集，难以获取高质量的多轮对话训练数据。需要一种能够自主学习有效提问策略的方法，以生成全面的现病史记录。

Method: 结合在线组相对策略优化和信息论奖励，使用信息增益奖励函数跟踪对话中揭示的临床实体（症状、时间模式、病史等），结合GPT-4o-mini的质量评估，通过LoRA微调Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B模型。

Result: DeepSeek-R1-Distill-Qwen-7B (IGFT)在Avey数据集上F1得分为0.408（比基础模型提升10.9%），在MIMIC数据集上为0.289（提升12.9%）。Llama-3.1-8B-Instruct (IGFT)分别达到0.384和0.336。两个模型在MIMIC上都优于OpenAI模型，并超越了HuatuoGPT和UltraMedical等医疗专用基线模型。

Conclusion: IGFT提供了一种无需人工对话数据的有效医疗对话AI训练框架，通过信息增益奖励和在线强化学习，模型能够学习到有效的提问策略，在现病史生成任务上超越了现有方法，展示了在医疗对话领域的应用潜力。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [142] [When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents](https://arxiv.org/abs/2601.17887)
*Jiahe Guo,Xiangran Guo,Yulin Hu,Zimo Long,Xingyu Sui,Xuda Zhi,Yongbo Huang,Hao He,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.AI

TL;DR: 个性化LLM代理中的长期记忆可能导致"意图合法化"安全漏洞，即良性个人记忆会使模型将有害查询合法化


<details>
  <summary>Details</summary>
Motivation: 现有个性化代理研究主要关注实用性和用户体验，将记忆视为中性组件，忽视了其安全影响。本文旨在揭示"意图合法化"这一被忽视的安全漏洞

Method: 提出PS-Bench基准来识别和量化个性化交互中的意图合法化；通过多个记忆增强代理框架和基础LLM进行实验；从内部表示空间提供机制性证据；提出轻量级检测-反思方法

Result: 个性化使攻击成功率相对无状态基线提高15.8%-243.7%；提供了意图合法化的机制性证据；提出的检测-反思方法有效减少了安全退化

Conclusion: 首次系统探索和评估了意图合法化这一安全故障模式，强调了在长期个性化背景下评估安全性的重要性

Abstract: Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.

</details>


### [143] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: UniCog是一个通过潜在思维空间分析LLM认知的统一框架，将密集模型激活编码为稀疏解耦的潜在维度，揭示了LLM认知的帕累托原则，并利用潜在激活异常检测推理失败，最终通过潜在信息候选优先策略提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法在解释LLM推理过程中如何运用认知能力方面存在局限，而研究表明LLM的认知过程与人类存在根本差异，因此需要新的分析框架来理解LLM的认知机制。

Method: 提出UniCog框架，将其构建为潜在变量模型，将密集的模型激活编码为稀疏、解耦的潜在维度，形成潜在思维空间，用于分析LLM的认知能力。

Result: 通过对六个先进LLM（包括DeepSeek-V3.2和GPT-4o）的广泛分析，揭示了LLM认知的帕累托原则：共享推理核心与能力特定特征互补；发现推理失败常表现为潜在激活的异常强度；提出的潜在信息候选优先策略在挑战性基准测试中将推理性能提升高达7.5%。

Conclusion: UniCog为LLM分析开辟了新范式，提供了基于认知的推理动态视角，通过潜在思维空间分析能够深入理解LLM的认知机制并提升其推理性能。

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [144] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha,Rohan Arora,Bhavya,Noah Zheutlin,Paulina Toro Isaza,Laura Shwartz,Yu Deng,Daby Sow,Ruchi Mahindru,Ruchir Puri*

Main category: cs.AI

TL;DR: EoG框架通过将调查任务分解为在依赖图上的溯因推理，解决了LLM智能体在开放调查中因上下文限制和探索顺序依赖导致的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在静态环境中表现良好，但在需要从海量异构数据中迭代挖掘证据的开放调查中表现不佳。ReAct等智能体存在探索顺序敏感、结果不稳定、缺乏信念跟踪和修正机制等问题。

Method: 提出EoG框架：将调查任务形式化为在依赖图上的溯因推理。LLM负责有限的本地证据挖掘和标注（原因vs症状），而确定性控制器管理图遍历、状态维护和信念传播，计算最小解释边界。

Result: 在ITBench诊断任务上，EoG相比ReAct基线在准确性和运行一致性方面都有提升，包括7倍的平均Majority-at-k实体F1增益。

Conclusion: 通过将推理与控制职责解耦，EoG框架能够更可靠地处理开放调查任务，解决了传统LLM智能体在复杂证据挖掘中的局限性。

Abstract: LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [145] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: 本文综述了自主实验室（SDL）中的人工智能问题，将SDL自主性建模为智能体-环境交互问题，回顾了主要方法家族，提出了能力驱动的分类法，并总结了实际部署中的经验教训和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 自主实验室（SDL）在实验设计、自动执行和数据驱动决策之间形成闭环，为在昂贵操作、噪声延迟反馈、严格可行性安全约束和非平稳性条件下的智能体AI提供了严格测试平台。本文以软物质为代表性场景，聚焦于真实实验室中出现的人工智能问题。

Method: 将SDL自主性建模为具有明确观察、行动、成本和约束的智能体-环境交互问题，回顾了闭环实验的主要方法家族：用于样本高效实验选择的贝叶斯优化和主动学习、用于长时程协议优化的规划和强化学习、以及协调异构仪器软件的工具使用智能体。强调可验证和可溯源的策略以支持调试、可重复性和安全操作。

Result: 提出了能力驱动的分类法，按决策视野、不确定性建模、行动参数化、约束处理、故障恢复和人类参与组织系统。合成了基准任务模板和评估指标，优先考虑成本感知性能、对漂移的鲁棒性、约束违反行为和可重复性。从已部署的SDL中提炼了经验教训。

Conclusion: 总结了自主实验室中的开放挑战，包括多模态表示、校准不确定性、安全探索和共享基准基础设施。强调需要进一步发展这些领域以实现更可靠、高效和安全的自主实验室系统。

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [146] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 论文提出了一种基于技能图的分层课程学习方法，用于终身智能体在复杂实时环境（黑暗之魂III）中的持续学习，通过分解控制为可重用技能并选择性微调来适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 终身智能体需要在不从头训练或覆盖已学行为的情况下随时间扩展能力。在复杂实时控制环境（如Dark Souls III）中，如何实现高效持续学习是一个挑战。

Method: 将战斗表示为有向技能图，采用分层课程训练其组件。将控制分解为五个可重用技能：相机控制、目标锁定、移动、闪避和治疗-攻击决策策略，每个技能针对特定职责优化。

Result: 技能分解提高了样本效率，减少了单个策略的负担。当环境从第一阶段切换到第二阶段时，只需微调部分技能（仅两个技能）即可快速恢复性能，上游技能保持可迁移性。

Conclusion: 技能图课程与选择性微调相结合，为复杂实时环境中不断进化的持续学习智能体提供了一条实用路径。

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [147] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang,Hung-Fu Chang,Po-An Chen*

Main category: cs.AI

TL;DR: 提出SSEV和ReCAPAgent-SQL两个Text-to-SQL框架，前者基于自精炼和集成投票，后者采用多智能体协作，在多个基准测试中取得竞争性性能


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL技术虽能降低数据分析门槛，但仍面临用户查询歧义、模式链接复杂、SQL方言泛化有限、领域特定理解等挑战，需要更强大的解决方案

Method: SSEV：基于PET-SQL构建单智能体自精炼与集成投票管道，结合加权多数投票及其随机变体；ReCAPAgent-SQL：采用多智能体协作框架，包含规划、知识检索、批判、动作生成、自精炼、模式链接和结果验证等专门智能体

Result: SSEV在Spider 1.0-Dev达到85.5%执行准确率，Spider 1.0-Test 86.4%，BIRD-Dev 66.3%；ReCAPAgent-SQL在Spider 2.0-Lite前100个查询中实现31%执行准确率，显著提升企业场景处理能力

Conclusion: 提出的框架促进了可扩展Text-to-SQL系统在实际环境中的部署，以更低成本和更高效率支持数据驱动决策

Abstract: Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [148] [Sentipolis: Emotion-Aware Agents for Social Simulations](https://arxiv.org/abs/2601.18027)
*Chiyuan Fu,Lyuhao Chen,Yunze Xiao,Weihao Xuan,Carlos Busso,Mona Diab*

Main category: cs.AI

TL;DR: Sentipolis框架为LLM智能体提供情感状态记忆，通过PAD情感表示、双速情感动态和情感-记忆耦合解决情感遗忘问题，提升社交模拟中的情感连续性和行为真实性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在社交模拟中常将情感视为瞬时线索，导致情感遗忘和长期情感连续性不足，需要建立具有情感状态记忆的智能体框架。

Method: 提出Sentipolis框架，整合：1）连续的愉悦-唤醒-支配（PAD）情感表示；2）双速情感动态机制；3）情感与记忆的耦合机制。

Result: 在数千次交互中，Sentipolis提升了情感基础行为、沟通能力和情感连续性。效果因模型而异：高容量模型可信度提升，小模型可能下降；情感意识可能轻微降低社会规范遵从度，反映了情感驱动行为与规则遵从之间的人类式张力。

Conclusion: Sentipolis支持互惠、适度聚类和时间稳定的关系结构，为研究联盟形成和渐进关系变化等累积社会动态提供了有效工具，在社交模拟中实现了更真实的情感连续性。

Abstract: LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.

</details>


### [149] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: 研究发现心理健康领域专家评估AI生成回复时存在系统性分歧，特别是在自杀自伤等安全关键问题上，专家间一致性很低，分歧源于不同的临床框架而非测量误差。


<details>
  <summary>Details</summary>
Motivation: 研究动机是检验"人类反馈学习"的基本假设——专家判断经过适当聚合能产生有效的训练和评估AI系统的真实标签。在心理健康这一高风险领域，专家共识尤为重要。

Method: 三位认证精神科医生使用校准的评估标准独立评估LLM生成的回复，计算组内相关系数(ICC)和Krippendorff's α等可靠性指标，并进行定性访谈分析分歧原因。

Result: 专家间可靠性极低(ICC 0.087-0.295)，低于可接受阈值；自杀自伤类回复分歧最大；一个因素甚至出现负可靠性(α=-0.203)；分歧源于三种不同的临床框架：安全优先、参与为中心和文化导向。

Conclusion: 专家分歧是原则性的社会技术现象，而非测量误差；聚合标签会抹杀专业哲学；建议从基于共识的聚合转向能保留和学习专家分歧的对齐方法。

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [150] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: EvolVE框架通过进化策略和结构化测试平台生成，在Verilog硬件设计任务中实现自动化，在功能正确性和PPA优化方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: Verilog硬件设计流程劳动密集且需要专业知识，现有大语言模型因训练数据有限和顺序推理能力不足，难以处理硬件系统的形式逻辑和并发特性。

Method: 提出EvolVE框架，分析多种进化策略：蒙特卡洛树搜索用于最大化功能正确性，想法引导优化用于PPA优化，结合结构化测试平台生成加速进化过程。

Result: 在VerilogEval v2达到98.1%，RTLLM v2达到92%；在工业级IC-RTL基准测试中，PPA乘积在Huffman编码上减少66%，所有问题几何平均减少17%，超越竞赛参与者实现。

Conclusion: EvolVE通过进化策略和结构化测试平台生成，为硬件设计自动化提供了有效解决方案，在功能正确性和优化方面均达到新的最先进水平。

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [151] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: OurBench是首个企业级SQL推理与调试基准，通过自动化注入真实bug和免执行评估框架，包含近千个复杂SQL查询，评估显示当前LLMs在SQL调试上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 企业数据工程中SQL至关重要，但即使是经验丰富的开发者和先进的文本到SQL LLMs也难以一次性生成完全正确的SQL代码，通常需要多次调试迭代。目前缺乏专门针对企业级SQL调试的基准测试。

Method: 提出两个关键创新：1) 自动化构建工作流，使用逆向工程在大规模SQL代码中系统性地注入真实bug，实现可扩展和多样化的基准生成；2) 针对企业环境的免执行评估框架，提供快速、准确且资源高效的评估。

Result: OurBench包含469个带有明确错误信息的语法错误查询(OurBenchSyn)和516个语义错误查询(OurBenchSem)，平均超过140行代码，具有深而广的抽象语法树。评估近30个LLMs显示显著性能差距：最佳模型Claude-4-Sonnet在OurBenchSyn上仅达到36.46%准确率，在OurBenchSem上为32.17%，大多数模型低于20%。

Conclusion: 当前LLMs在企业级SQL调试方面仍有很大提升空间。研究探索了四种解决方案策略，识别了关键挑战，并为企业环境中LLMs的SQL调试指明了有前景的研究方向。

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [152] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: 论文研究了截止时间感知的浸入式热水器控制，通过强化学习（PPO）相比传统bang-bang控制和MCTS规划器，在相同物理条件下显著降低能耗，最高可节省69%能源。


<details>
  <summary>Details</summary>
Motivation: 传统家用浸入式热水器在冬季通常连续运行，追求快速加热而非高效节能，忽略了可预测的需求窗口和环境热损失。需要一种能在指定时间达到目标温度同时最小化能耗的智能控制方法。

Method: 创建了Gymnasium环境模拟浸入式热水器的一阶热损失模型，采用离散开关控制（0W/6000W，每120秒动作）。比较了三种方法：时间最优的bang-bang基线、零样本蒙特卡洛树搜索规划器和近端策略优化强化学习策略。

Result: 在初始温度10-30°C、截止时间30-90步（1-3小时）、目标温度40-80°C的广泛测试中，PPO在60步（2小时）时耗能最低（3.23kWh），相比bang-bang控制（4.37-10.45kWh）和MCTS（4.18-6.46kWh）显著节能。在代表性场景中，PPO比bang-bang节省54%能耗，比MCTS节省33%。

Conclusion: 学习型截止时间感知控制能在相同物理假设下大幅降低能耗，规划器无需训练即可提供部分节能，而训练后的学习策略推理成本接近零。该方法为家用热水系统节能提供了有效解决方案。

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [153] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang,Han Wu,Zhiyuan You,Yiming Song,Yijun Wang,Zifei Shan,Yining Li,Songyang Zhang,Xinyi Le,Cailian Chen,Xinping Guan,Dacheng Tao*

Main category: cs.AI

TL;DR: RouteMoA：基于动态路由的高效混合智能体框架，通过轻量级评分器预筛选候选模型，结合混合裁判进行后验修正，在显著降低成本和延迟的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合智能体方法采用密集拓扑结构，导致成本和延迟过高。虽然使用LLM裁判进行过滤，但仍需所有模型先进行推理再判断，无法有效降低成本。同时缺乏模型选择标准，面对大规模模型池时，全推理成本高昂且可能超出上下文限制。

Method: 1. 轻量级评分器：基于查询预测粗略性能，筛选高潜力候选模型子集，无需推理；2. 混合裁判：通过基于现有模型输出的轻量级自评估和交叉评估进行后验修正；3. 模型排名机制：平衡性能、成本和延迟进行模型选择。

Result: RouteMoA在不同任务和模型池规模下均优于MoA，在大规模模型池中降低成本89.8%，减少延迟63.6%。

Conclusion: RouteMoA通过动态路由机制有效解决了混合智能体框架的成本和延迟问题，实现了高效的大规模模型协作，为实际部署提供了可行方案。

Abstract: Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [154] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: RareAlert是一个基于多LLM推理校准的罕见病早期筛查系统，通过整合10个LLM的推理信号，训练出可在本地部署的单一模型，在罕见病识别任务上达到0.917 AUC，优于现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 罕见病的漏诊和延迟诊断是重大临床挑战，现有初级医疗分诊流程无法可靠识别罕见病患者，需要通用筛查来减少诊断延迟。

Method: 整合10个LLM生成的推理信号，使用机器学习进行校准和加权，然后将对齐的推理蒸馏到单个本地可部署模型（基于Qwen3-4B）。使用RareBench数据集（158,666个病例，33个Orphanet疾病类别）进行开发和评估。

Result: RareAlert在独立测试集上达到0.917 AUC，优于最佳机器学习集成模型和所有评估的LLM（包括GPT-5、DeepSeek-R1、Claude-3.7-Sonnet等）。

Conclusion: 罕见病识别可重新概念化为对普通患者群体的通用不确定性解决过程，通过校准LLM推理信号并蒸馏到单一模型，可实现准确、保护隐私、可扩展的罕见病风险筛查。

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [155] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: DeepPlanning是一个针对实际长时程智能体规划的挑战性基准测试，包含多日旅行规划和多产品购物任务，要求主动信息获取、局部约束推理和全局约束优化。


<details>
  <summary>Details</summary>
Motivation: 现有智能体评估大多关注局部步级推理，缺乏对全局约束优化（如时间和财务预算）的测试，而现有的LLM规划基准未能充分体现真实世界中的主动信息收集和细粒度局部约束。

Method: 提出了DeepPlanning基准测试，包含多日旅行规划和多产品购物两类任务，这些任务需要主动信息获取、局部约束推理和全局约束优化能力。

Result: 评估显示即使前沿的智能体LLM在这些问题上也表现困难，突显了可靠的显式推理模式和并行工具使用对于实现更好的效果-效率权衡的重要性。

Conclusion: DeepPlanning为改进长规划时程的智能体LLM指出了有前景的方向，作者开源了代码和数据以支持未来研究。

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [156] [GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models](https://arxiv.org/abs/2601.18197)
*Shaokang Wang,Pei Fu,Ruoceng Zhang,Shaojie Zhang,Xiuwen Xi,Jiahui Yang,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.AI

TL;DR: GAIA框架通过训练直觉批评模型来提升GUI代理的测试时性能，通过数据飞轮机制实现自我改进


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然提升了GUI代理的能力，但操作不可逆性导致单个错误动作可能引发灾难性偏差，需要解决这一问题

Method: 提出GAIA训练框架：1) 使用基础代理的正负动作样本训练直觉批评模型；2) 批评模型评估代理意图动作的正确性，选择高成功率操作；3) 批评模型指导代理收集精炼样本，启动自我改进循环；4) 增强数据训练第二轮批评模型

Result: 实验表明直觉批评模型能提升闭源和开源模型的测试时性能，且随着数据循环性能逐步提高

Conclusion: GAIA框架通过数据飞轮机制实现了GUI代理的自我改进，解决了操作不可逆性问题，代码和数据集将公开

Abstract: While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.

</details>


### [157] [SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback](https://arxiv.org/abs/2601.18202)
*Fangyuan Xu,Rujun Han,Yanfei Chen,Zifeng Wang,I-Hung Hsu,Jun Yan,Vishy Tirumalashetty,Eunsol Choi,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: SAGE：一个自动生成高质量、难度可控的深度搜索问答对的智能体管道，通过生成器和搜索代理的迭代交互来优化数据质量，显著提升深度搜索代理的性能。


<details>
  <summary>Details</summary>
Motivation: 深度搜索代理需要跨多个文档进行复杂推理，但人工标注成本极高，因为探索轨迹长且复杂。需要自动生成高质量、难度可控的训练数据。

Method: 提出SAGE管道，包含数据生成器和搜索代理两个组件。生成器提出QA对，搜索代理尝试解决问题并提供执行反馈，两者通过多轮迭代交互，不断优化问答对直到达到目标难度水平。

Result: 内在评估显示SAGE能生成需要多样化推理策略的问题，显著提高生成数据的正确性和难度。外在评估显示在流行深度搜索基准上获得高达23%的相对性能提升，且训练后的代理无需额外训练就能适应从固定语料库检索到Google搜索的转换。

Conclusion: SAGE能自动生成高质量、难度可控的深度搜索训练数据，显著提升深度搜索代理的性能，并展示了良好的适应性，为解决人工标注成本高的问题提供了有效方案。

Abstract: Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.

</details>


### [158] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究LLM智能体后训练时，发现环境状态信息丰富度和规划复杂度是影响跨域泛化的关键因素，而非领域真实性或文本相似度。提出通过添加无关特征增加状态信息丰富度来提升泛化能力，并分析了建模选择对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体通常在狭窄环境集上进行后训练，但部署在更广泛的未见领域。本研究旨在探索当最终测试领域未知时，哪些环境属性和建模选择对跨域性能影响最大，以指导更有效的智能体后训练。

Method: 1. 识别影响跨域泛化的环境轴：状态信息丰富度和规划复杂度；2. 提出随机化技术：在状态中添加少量分散注意力的目标无关特征以增加信息丰富度；3. 分析建模选择：SFT预热/中期训练的影响，以及RL期间启用逐步思考的作用。

Result: 1. 状态信息丰富度和规划复杂度与跨域泛化强相关；2. 领域真实性和文本相似度不是主要因素（如Sokoban在SciWorld中的泛化优于ALFWorld）；3. 增加状态信息丰富度可有效提升跨域鲁棒性；4. SFT预热防止灾难性遗忘但损害未包含领域的泛化；5. 逐步思考在RL中虽不总是提升域内性能，但对保持泛化至关重要。

Conclusion: 智能体后训练应关注环境状态信息丰富度和规划复杂度，而非领域真实性。通过添加无关特征增加状态信息丰富度是提升跨域泛化的有效方法。建模选择上需权衡SFT预热与泛化能力，并重视逐步思考在保持泛化中的作用。

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [159] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang,Yanan Wu,Xiaoshuai Song,Weixun Wang,Gengru Chen,Zhongwen Li,Kezhong Yan,Ken Deng,Qi Liu,Shuaibing Zhao,Shaopan Xiong,Xuepeng Liu,Xuefeng Chen,Wanxi Deng,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: ShopSimulator是一个大规模中文购物模拟环境，用于评估和训练LLM智能体在电商场景中的表现，发现现有模型成功率不足40%，通过SFT+RL训练可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏统一的模拟环境来全面评估LLM智能体在电商购物中的综合能力，包括理解个人偏好、多轮对话、检索和区分高度相似产品等关键方面，且现有工作主要关注评估而非训练支持。

Method: 提出ShopSimulator大规模中文购物模拟环境，利用该环境评估不同LLM在多样化购物场景中的表现，并进行错误分析。进一步探索监督微调(SFT)和强化学习(RL)相结合的训练方法。

Result: 评估发现即使最佳模型的全成功率也不足40%，智能体在长轨迹中的深度搜索和产品选择、平衡个性化线索使用、有效与用户互动等方面存在困难。SFT和RL结合的训练方法显著提升了性能。

Conclusion: ShopSimulator为LLM智能体在电商购物领域的评估和训练提供了统一环境，揭示了现有模型的局限性，并展示了通过适当训练方法可以显著改善智能体性能，为未来研究提供了实用指导。

Abstract: Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [160] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 提出In-Situ Self-Evolving范式，通过工具演化实现AI代理在开放环境中的自主能力扩展，无需真实标签监督


<details>
  <summary>Details</summary>
Motivation: 传统代理系统在开放环境中面临任务分布漂移和外部监督稀缺的挑战，静态工具集和离线训练无法适应动态变化，导致能力边界僵化未知

Method: 提出原位自演化范式，将序列任务交互视为连续经验流，通过工具演化实现能力扩展；开发Yunjue Agent系统，采用并行批量演化策略优化演化效率

Result: 在五个不同基准测试的零起点设置中显著优于专有基线；补充的热启动评估证实积累的通用知识可无缝迁移到新领域；提出监测演化收敛的新指标

Conclusion: In-Situ Self-Evolving范式通过工具演化实现AI代理在开放环境中的自主能力扩展，为弹性自演化智能研究提供新方向

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [161] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: TAFC框架通过引入"思考"参数增强，在函数和参数层面提供显式推理，提升函数调用准确性和可解释性，无需修改LLM架构。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在函数调用中缺乏参数生成的显式推理透明度，特别是对于具有相互依赖参数的复杂函数。现有方法如思维链提示在代理层面操作，无法为单个函数参数提供细粒度推理指导。

Method: 提出Think-Augmented Function Calling框架：1) 引入通用的"think"参数增强，让模型阐述决策过程；2) 动态优化参数描述以提升推理质量；3) 基于复杂度评分自动触发细粒度推理；4) 提出推理引导优化以对齐人类期望。

Result: 在ToolBench上对专有和开源模型的评估显示，TAFC在多参数函数的参数生成准确性和推理连贯性方面有显著提升，同时为调试AI代理行为提供增强的可解释性。

Conclusion: TAFC通过函数和参数层面的显式推理，有效提升了函数调用准确性，同时保持与现有LLM架构的完全兼容性，为复杂函数调用提供了透明、可解释的解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [162] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: 生成式AI在模仿作家风格方面已超越人类专家，引发创意写作领域的身份危机


<details>
  <summary>Details</summary>
Motivation: 挑战传统认为创意写作是人类专属领域的假设，探究AI在模仿作家风格方面是否已超越人类专家，以及这对创意劳动的未来意味着什么

Method: 行为实验：28位MFA作家（专家）与3个LLM竞争模仿50位知名作家风格；通过28位专家评委和131位普通评委进行盲测对比；使用上下文提示和微调两种条件

Result: 专家评委在上下文提示条件下82.7%偏好人类写作，但在微调后62%偏好AI写作；普通评委始终偏好AI写作；专家作家对AI写作的偏好引发了身份危机和审美自信的侵蚀

Conclusion: AI在创意写作方面的能力挑战了关于AI创意局限性的论述，提出了创意劳动未来的根本性问题，专家作家面临身份危机和审美自信的动摇

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [163] [AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito](https://arxiv.org/abs/2601.18381)
*Yinghan Hou,Zongyou Yang*

Main category: cs.AI

TL;DR: 开发了一个集成AI代理框架，通过RAG和开源大语言模型的多阶段迭代工作流，将传统有限差分实现转换为Devito环境，包含知识图谱构建、反向工程、代码合成和验证框架。


<details>
  <summary>Details</summary>
Motivation: 为了促进传统有限差分实现向Devito环境的转换，解决代码迁移的复杂性问题，需要一个智能的自动化框架来简化这一过程。

Method: 采用混合LangGraph架构，结合RAG和开源大语言模型的多阶段迭代工作流。包括：1) 构建Devito知识图谱（文档解析、结构感知分割、实体关系提取、Leiden社区检测）；2) GraphRAG优化查询性能；3) 反向工程组件通过Fortran源代码静态分析推导三级查询策略；4) 多阶段检索管道进行并行搜索、概念扩展、社区规模检索和语义相似性分析；5) Pydantic约束的代码合成；6) 集成传统静态分析和G-Eval方法的验证框架。

Result: 实现了从静态代码翻译向动态自适应分析行为的转变，通过强化学习启发的反馈机制支持质量驱动的迭代优化和状态感知的动态路由，提高了代码转换的准确性和可靠性。

Conclusion: 该研究的主要贡献在于整合了强化学习启发的反馈机制，使AI代理能够从静态代码翻译转变为动态自适应的分析行为，为传统科学计算代码向现代框架的迁移提供了有效的自动化解决方案。

Abstract: To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.

</details>


### [164] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: DynTS方法通过识别推理轨迹中的关键决策令牌并仅保留其KV缓存，显著提升大型推理模型的效率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成完整推理轨迹时会产生巨大的内存占用和计算开销，但研究发现只有部分关键令牌真正影响最终答案，其余令牌贡献可忽略

Method: 提出动态思维令牌选择方法，利用注意力图分析推理轨迹中令牌的影响力，识别决策关键令牌，在推理时仅保留这些关键令牌的KV缓存状态，剔除冗余条目

Result: 通过选择性保留关键令牌的KV缓存，显著降低了内存占用和计算开销，优化了推理效率

Conclusion: DynTS方法基于注意力分析揭示的推理轨迹冗余现象，提供了一种高效优化大型推理模型推理过程的有效方案

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [165] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: 论文提出了一种完全离线的研究智能体训练方法，通过开源工具套件和合成数据集，避免了昂贵的在线强化学习，训练出的8B参数模型在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体通常依赖昂贵的在线强化学习，需要大量API调用，成本高昂。而离线训练虽然更高效，但缺乏高质量的研究轨迹数据，限制了其发展。

Method: 提出了完全开源的离线训练套件，包括：1) DeepForge任务合成框架，无需大量预处理即可生成大规模研究查询；2) 精心策划的数据集，包含66k QA对、33k SFT轨迹和21k DPO对；3) 基于这些资源训练了完全离线的OffSeeker（8B）模型。

Result: 在六个基准测试上的广泛评估显示，OffSeeker不仅在同等规模的智能体中领先，还能与通过大量在线RL训练的30B参数系统保持竞争力。

Conclusion: 研究表明，昂贵的在线强化学习并非构建强大研究智能体的唯一途径，通过高质量的离线训练同样可以获得优异性能，为研究智能体的高效训练提供了新方向。

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [166] [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491)
*Dongrui Liu,Qihan Ren,Chen Qian,Shuai Shao,Yuejin Xie,Yu Li,Zhonghao Yang,Haoyu Luo,Peng Wang,Qingyu Liu,Binxin Hu,Ling Tang,Jilin Mei,Dadi Guo,Leitao Yuan,Junyao Yang,Guanxu Chen,Qihao Lin,Yi Yu,Bo Zhang,Jiaxuan Guo,Jie Zhang,Wenqi Shao,Huiqi Deng,Zhiheng Xi,Wenjie Wang,Wenxuan Wang,Wen Shen,Zhikai Chen,Haoyu Xie,Jialing Tao,Juntao Dai,Jiaming Ji,Zhongjie Ba,Linfeng Zhang,Yong Liu,Quanshi Zhang,Lei Zhu,Zhihua Wei,Hui Xue,Chaochao Lu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 论文提出了一种新的智能体安全防护框架AgentDoG，通过三维风险分类法、细粒度安全基准ATBench和诊断式防护机制，解决AI智能体在工具使用和环境交互中的安全挑战。


<details>
  <summary>Details</summary>
Motivation: AI智能体的兴起带来了复杂的安全挑战，现有防护模型缺乏对智能体风险的认知和透明诊断能力，需要更全面的安全防护框架。

Method: 1) 提出三维正交风险分类法（来源、失效模式、后果）；2) 构建细粒度智能体安全基准ATBench；3) 开发诊断式防护框架AgentDoG，提供细粒度监控和根因诊断。

Result: AgentDoG在多样复杂交互场景中实现了最先进的智能体安全监管性能，发布了4B、7B、8B参数的Qwen和Llama系列模型变体，所有模型和数据集均已开源。

Conclusion: AgentDoG框架通过结构化风险分类和诊断式防护，为AI智能体安全提供了透明、可解释的解决方案，超越了传统的二元标签方法，促进了有效的智能体对齐。

Abstract: The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.

</details>


### [167] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: DeepMed：针对医学推理的深度研究模型，通过医学特定数据合成、训练中的难度感知惩罚和推理中的监控机制，解决通用DR模型在医学领域性能受限的问题


<details>
  <summary>Details</summary>
Motivation: 通用深度研究（DR）模型在医学领域直接迁移效果有限，存在两个主要问题：1）医学问题需要在知识密集的临床背景下解释证据，通用DR模型缺乏临床上下文推理能力；2）医学场景中盲目扩展工具调用会引入噪声，干扰敏感的医学推理

Method: 1）数据：采用多跳医学搜索QA合成方法，支持模型在医学上下文中应用DR范式；2）训练：引入难度感知的回合惩罚机制，抑制过度工具调用；3）推理：添加监控器帮助在可控步骤内验证假设，避免上下文腐化

Result: 在七个医学基准测试中，DeepMed相比基础模型平均提升9.79%，并且优于更大的医学推理和DR模型

Conclusion: 通过针对医学领域特点设计的数据合成、训练和推理机制，DeepMed成功解决了通用DR模型在医学推理中的局限性，显著提升了医学推理性能

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [168] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: MOSAIC是一个模块化框架，使用动态生成的数据集评估LLM对复杂指令的遵循能力，发现遵循能力不是单一能力，而是随约束类型、数量和位置显著变化。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法准确反映LLM在现实世界中对复杂指令的遵循能力，也无法将遵循能力与任务成功区分开来，这限制了开发可靠LLM系统的能力。

Method: 提出MOSAIC框架，使用动态生成的数据集，包含最多20个面向应用的生成约束，对指令遵循能力进行细粒度独立分析。

Result: 评估5个不同家族的LLM显示：遵循能力随约束类型、数量和位置显著变化；发现模型特定弱点；揭示指令间的协同和冲突交互；识别出首因效应和近因效应等位置偏差。

Conclusion: MOSAIC提供的细粒度洞察对于诊断模型失败和开发需要严格遵循复杂指令的可靠LLM系统至关重要。

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [169] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: 训练稳定性与生成质量并不一致：稳定训练可能导致模型输出低熵、重复，尽管损失平滑收敛


<details>
  <summary>Details</summary>
Motivation: 分析训练稳定性如何影响大语言模型的生成分布，揭示稳定性与生成表达能力之间的潜在矛盾

Method: 使用基于反馈的受控训练框架稳定内部生成统计，在不同架构和随机种子下观察输出行为

Result: 稳定参数轨迹使模型近似最小化前向KL散度，同时隐式降低生成熵，导致模型集中在有限的实证模式子集上，产生系统性退化

Conclusion: 优化稳定性与生成表达能力并不内在一致，稳定性本身不足以指示生成质量

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [170] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: 提出一种结合LLM与逻辑求解器的方法，通过迭代反馈机制补充常识关系，提升复杂推理问题的解决能力


<details>
  <summary>Details</summary>
Motivation: LLMs在形式推理方面表现优秀，但在需要复杂证明规划的问题上容易失败。传统逻辑求解器虽然推理效率高，但无法处理缺失的常识关系，需要人工提供所有相关事实。

Method: 提出新颖的迭代方法：1) 将问题转化为形式逻辑；2) 使用逻辑求解器提供反馈；3) 基于反馈让LLM补充常识关系；4) 通过搜索过程探索潜在的常识假设，平衡有用事实发现与计算成本。

Result: 在移除了部分常识信息的纯逻辑推理数据集上，该方法相比现有技术持续获得显著改进，证明了在人类语境中平衡神经与符号元素的价值。

Conclusion: 通过结合LLM的常识推理能力和逻辑求解器的形式推理效率，提出的迭代反馈方法能有效处理需要复杂证明规划和常识补充的推理问题，展示了神经符号混合方法的优势。

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [171] [PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression](https://arxiv.org/abs/2601.18608)
*Fabian Fumagalli,R. Teal Witter,Christopher Musco*

Main category: cs.AI

TL;DR: PolySHAP扩展KernelSHAP，使用高阶多项式近似特征交互，提供更准确的Shapley值估计，并证明配对采样等价于二阶PolySHAP


<details>
  <summary>Details</summary>
Motivation: KernelSHAP使用线性函数近似游戏，但无法捕捉特征间的非线性交互，需要更准确的Shapley值估计方法

Method: 提出PolySHAP方法，通过高阶多项式（而非线性函数）来近似游戏，捕捉特征间的非线性交互作用

Result: PolySHAP在各种基准数据集上提供更好的Shapley值估计，并证明估计具有一致性；发现配对采样等价于二阶PolySHAP

Conclusion: PolySHAP通过高阶多项式近似改进了Shapley值估计，同时为配对采样启发式方法提供了首个强有力的理论依据

Abstract: Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.
  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.
  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.

</details>


### [172] [Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks](https://arxiv.org/abs/2601.18617)
*Pierre Orhan,Pablo Diego-Simón,Emmnanuel Chemla,Yair Lakretz,Yves Boubenec,Jean-Rémi King*

Main category: cs.AI

TL;DR: 该研究探讨人工神经网络在训练过程中是否及何时自发形成音位、词汇和句法表征，发现其学习阶段与儿童语言习得相似但所需数据量更大。


<details>
  <summary>Details</summary>
Motivation: 虽然儿童语言习得过程（音位分类、词汇识别、句法组合）已被充分描述，但仍缺乏统一的计算框架来解释其背后的神经表征机制。研究旨在探索人工神经网络是否能在训练中自发形成这些语言表征。

Method: 研究基于语音和文本的人工神经网络模型，分析其在训练过程中神经激活的变化。通过观察神经激活子空间的构建，检测音位、词汇和句法结构的几何表征何时出现。

Result: 研究发现语音和文本模型都遵循相似的学习阶段序列：在训练过程中，神经激活逐步构建出代表音位、词汇和句法结构的子空间。虽然这一发展轨迹与儿童语言习得在性质上相似，但数量上差异显著——这些算法需要比儿童多2-4个数量级的数据才能形成这些神经表征。

Conclusion: 该研究表明主要语言习得阶段可以在特定条件下自发涌现，为理解语言习得的计算基础提供了有前景的研究路径。研究结果揭示了人工神经网络与人类语言习得之间的相似性和差异。

Abstract: During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.

</details>


### [173] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: 论文提出了一种基于人类专家的评估方法，用于评估大语言模型在心理健康对话中的表现，发现LLMs在认知支持方面表现可靠，但在情感共鸣方面存在不足，揭示了认知-情感差距。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康危机日益严重，存在治疗缺口和合格治疗师短缺问题，大语言模型作为可扩展的支持途径具有潜力，但其可靠性、治疗相关性和与人类标准的对齐仍面临挑战。

Method: 提出基于人类专家的评估方法，从真实场景数据集中整理500个心理健康对话，评估9个不同LLMs（包括闭源和开源模型）的响应，由两位精神科专家使用包含6个属性的5点李克特量表独立评分，重点关注认知支持和情感共鸣。

Result: LLMs在认知可靠性方面表现强劲，能提供安全、连贯且临床适当的信息，但在情感对齐方面表现不稳定。闭源模型（如GPT-4o）提供更平衡的治疗响应，而开源模型表现出更大的变异性和情感平淡性。

Conclusion: 揭示了持续的认知-情感差距，强调需要建立具有失败意识、临床基础的评估框架，在心理健康导向的LLMs中优先考虑关系敏感性而不仅仅是信息准确性。倡导以治疗敏感性为中心的平衡评估协议，为心理健康对话AI的负责任设计和临床监督提供框架。

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [174] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: AdaReasoner是一个多模态模型家族，通过学习工具使用作为通用推理技能，而非特定工具或显式监督行为，在视觉推理任务中实现自适应工具使用和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人类在面对超出自身能力的问题时会使用工具，这为提高多模态大语言模型的视觉推理能力提供了一个有前景的范式。有效推理的关键在于知道使用哪些工具、何时调用它们以及如何在多步骤中组合它们，即使面对新工具或新任务时也是如此。

Method: 1) 可扩展的数据整理流程，让模型接触长视野、多步骤的工具交互；2) Tool-GRPO强化学习算法，基于最终任务成功优化工具选择和序列；3) 自适应学习机制，动态调节工具使用频率。

Result: AdaReasoner表现出强大的工具自适应和泛化行为：自主采用有益工具、抑制无关工具、根据任务需求调整工具使用频率。在多个基准测试中达到最先进性能，7B基础模型平均提升24.9%，在VSP和Jigsaw等任务上超越GPT-5等专有系统。

Conclusion: AdaReasoner通过学习工具使用作为通用推理技能，实现了自适应工具使用和泛化能力，在多模态视觉推理任务中取得了显著性能提升，为MLLMs的工具使用能力提供了新的解决方案。

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [175] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei,Xu Dong,Xiao Peng,Niantao Xie,Bin Wang*

Main category: cs.AI

TL;DR: FadeMem是一种受生物学启发的智能体记忆架构，通过引入主动遗忘机制解决LLM智能体的记忆限制问题，实现选择性遗忘和高效信息管理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体面临关键记忆限制，缺乏选择性遗忘机制，导致在上下文边界出现灾难性遗忘或内部信息过载。人类记忆通过自适应衰减过程自然平衡保留与遗忘，而现有AI系统采用二元保留策略（要么全部保留，要么全部丢失）。

Method: FadeMem采用受生物学启发的智能体记忆架构，在双层记忆层次结构中实现差异化衰减率，保留由自适应指数衰减函数控制，受语义相关性、访问频率和时间模式调节。通过LLM引导的冲突解决和智能记忆融合，系统能够整合相关信息同时让无关细节逐渐消失。

Result: 在Multi-Session Chat、LoCoMo和LTI-Bench上的实验表明，FadeMem在实现45%存储减少的同时，展现出优越的多跳推理和检索能力，验证了生物学启发的遗忘机制在智能体记忆系统中的有效性。

Conclusion: FadeMem通过引入受人类认知启发的主动遗忘机制，有效解决了LLM智能体的记忆管理问题，在减少存储需求的同时提高了推理和检索性能，为智能体记忆系统设计提供了新方向。

Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [176] [TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent](https://arxiv.org/abs/2601.18700)
*Xingyu Sui,Yanyan Zhao,Yulin Hu,Jiahe Guo,Weixiang Zhao,Bing Qin*

Main category: cs.AI

TL;DR: TEA-Bench：首个用于评估工具增强型情感支持对话系统的交互式基准，通过工具使用减少幻觉并提高支持质量，实验显示工具增强效果与模型能力密切相关。


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话系统主要关注文本情感表达，忽视了外部工具在提供事实基础和减少幻觉方面的重要性。需要建立能够评估工具增强型情感支持代理的基准。

Method: 提出TEA-Bench基准，包含真实情感场景、MCP风格工具环境和过程级评估指标；构建TEA-Dialog数据集；在9个LLM上进行实验，分析工具增强效果。

Result: 工具增强普遍提高情感支持质量并减少幻觉，但效果强烈依赖于模型能力：强模型能更选择性和有效地使用工具，弱模型获益有限；监督微调在分布内有效但泛化能力差。

Conclusion: 工具使用对构建可靠的情感支持代理至关重要，需要开发更有效利用工具的方法，特别是针对能力较弱的模型。

Abstract: Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.

</details>


### [177] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: Health-SCORE：一个可扩展的医疗领域LLM评估框架，通过自动化降低人工制定评估标准成本，同时支持强化学习和上下文学习


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，评估开放式LLM响应需要高质量评估标准，但人工制定成本高、难以扩展，限制了基于标准的评估和训练

Method: 开发Health-SCORE框架，通过自动化方法生成评估标准，显著降低开发成本，同时保持评估质量；该框架可作为结构化奖励信号指导强化学习，也可直接融入提示进行上下文学习

Result: 在开放式医疗任务中，Health-SCORE达到与人工制定标准相当的评估质量，同时显著降低开发工作量，使基于标准的评估和训练更具可扩展性

Conclusion: Health-SCORE提供了一个通用且可扩展的评估框架，解决了医疗领域LLM评估标准制定成本高的问题，支持强化学习和上下文学习，为安全关键领域的LLM评估和训练提供了实用解决方案

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [178] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: AI辅助药物设计方法，通过E3连接酶导向的分子胶促进Aβ-42的靶向降解，用于阿尔茨海默病治疗


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病中Aβ-42的病理积累是疾病进展的关键驱动因素，特别是细胞内Aβ-42作为早期毒性因子。现有研究主要关注细胞外淀粉样斑块，而针对细胞内Aβ-42的靶向降解策略有限。

Method: 1. 系统评估Aβ-42与三种E3连接酶（CRBN、VHL、MDM2）的三元复合物形成潜力；2. 基于结构建模、ADMET筛选和对接分析；3. 开发Ligase-Conditioned Junction Tree Variational Autoencoder（LC-JT-VAE）生成连接酶特异性小分子；4. 整合蛋白质序列嵌入和扭转角感知分子图。

Result: 生成模型能够产生化学有效、新颖且靶向特异性的分子胶，能够促进Aβ-42的降解。该方法为设计UPS靶向治疗神经退行性疾病提供了有前景的框架。

Conclusion: 这种AI辅助药物设计方法通过促进Aβ-42的靶向降解，为阿尔茨海默病等神经退行性疾病的治疗提供了新的策略，展示了生成模型在开发UPS靶向疗法中的潜力。

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [179] [Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems](https://arxiv.org/abs/2601.18735)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Jiawei Yao,Jian Wang,Guanlong Qu,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: Agora框架将多智能体协调重构为不确定性市场，通过将认知不确定性转化为可交易资产，基于理性经济规则实现成本高效的协调，在多个多模态基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型多智能体系统在经济上不可持续：异构智能体在信息不对称下的协调成本高昂。现有方法依赖启发式代理，忽略成本并破坏不确定性结构，导致可证明的次优协调。

Method: Agora将认知不确定性形式化为结构化可交易资产（感知、语义、推理），基于理性经济规则强制智能体间进行盈利驱动的交易。扩展Thompson采样的市场感知代理启动协作，引导系统达到成本高效均衡。

Result: 在五个多模态基准（MMMU、MMBench、MathVision、InfoVQA、CC-OCR）上，Agora显著优于强VLM和启发式多智能体策略，如在MMMU上比最佳基线提升8.5%准确率，同时降低成本超过3倍。

Conclusion: 基于市场的协调为构建经济可行的多智能体视觉智能系统提供了原则性和可扩展的范式，实现了成本效益与性能的平衡。

Abstract: Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.

</details>


### [180] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: TSRBench是一个全面的多模态时间序列推理基准测试，包含4125个问题、14个领域、4个维度，用于评估通用模型的时间序列推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在现实世界中无处不在且至关重要，但现有通用模型基准测试中缺乏时间序列维度，需要填补这一空白以评估模型解决实际问题的能力。

Method: 构建TSRBench基准测试，包含4125个问题，覆盖14个领域，分为感知、推理、预测和决策4个主要维度，包含15个任务评估基本推理能力。

Result: 评估了30多个领先的专有和开源LLM、VLM和TSLLM，发现：1)缩放定律适用于感知和推理但预测失效；2)强推理能力不保证准确的上下文感知预测；3)当前多模态模型未能有效融合文本和视觉表示。

Conclusion: TSRBench提供了一个标准化评估平台，不仅突显了现有挑战，还为推进通用模型发展提供了宝贵见解。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [181] [The Compounded BSDE method: A fully-forward method for option pricing and optimal stopping problems in finance](https://arxiv.org/abs/2601.18634)
*Zhipeng Huang,Cornelis W. Oosterlee*

Main category: q-fin.CP

TL;DR: 提出Compound BSDE方法，一种基于深度学习的前向算法，用于解决金融数学中的复合期权定价和最优停止问题，通过BSDE系统重构问题，具有高维计算效率和收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理复合期权和最优停止问题（如百慕大期权）时面临高维计算困难，需要开发更高效、可扩展的数值方法。

Method: 将期权定价问题重构为后向随机微分方程（BSDE）系统，基于经典深度BSDE方法扩展为复合BSDE算法，建立收敛性分析并推导后验误差估计。

Result: 数值实验表明该方法在精度和计算效率方面表现优异，特别适用于高维期权定价和最优停止问题。

Conclusion: Compound BSDE方法为金融数学中的复合期权和最优停止问题提供了有效的前向深度学习解决方案，具有理论保证和实际应用价值。

Abstract: We propose the Compound BSDE method, a fully forward, deep-learning-based approach for solving a broad class of problems in financial mathematics, including optimal stopping. The method is based on a reformulation of option pricing problems in terms of a system of backward stochastic differential equations (BSDEs), which offers a new perspective on the numerical treatment of compound options and optimal stopping problems such as Bermudan option pricing. Building on the classical deep BSDE method for a single BSDE, we develop an algorithm for compound BSDEs and establish its convergence properties. In particular, we derive an \emph{a posteriori} error estimate for the proposed method. Numerical experiments demonstrate the accuracy and computational efficiency of the approach, and illustrate its effectiveness for high-dimensional option pricing and optimal stopping problems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [182] [TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation](https://arxiv.org/abs/2601.16984)
*Rahul Ghosh,Chun-Hao Liu,Gaurav Rele,Vidya Sagar Ravipati,Hazar Aouad*

Main category: cs.LG

TL;DR: TelcoAI是一个针对3GPP技术规范的多模态RAG系统，通过智能分块、查询规划和多模态融合，在复杂技术文档理解上比现有方法提升16%


<details>
  <summary>Details</summary>
Motivation: 3GPP技术规范结构复杂、格式密集且包含多模态内容，现有LLM方法难以处理复杂查询、视觉信息和文档依赖关系，需要专门解决方案

Method: 开发了TelcoAI系统，采用章节感知分块、结构化查询规划、元数据引导检索以及文本与图表的多模态融合技术

Result: 在多个基准测试（包括专家策划查询）中，系统达到87%召回率、83%声明召回率和92%忠实度，比最先进基线提升16%

Conclusion: TelcoAI展示了智能和多模态推理在技术文档理解中的有效性，为电信研究和工程提供了实用的解决方案

Abstract: The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\%$ recall, $83\%$ claim recall, and $92\%$ faithfulness, representing a $16\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.

</details>


### [183] [Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2601.16991)
*Longteng Zhang,Sen Wu,Shuai Hou,Zhengyu Qing,Zhuo Zheng,Danning Ke,Qihong Lin,Qiang Wang,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: SALR是一种新的微调范式，将低秩适应与稀疏剪枝统一起来，通过静态剪枝冻结的基础权重并利用截断SVD低秩适配器恢复残差信息，实现50%稀疏度、2倍模型压缩和1.7倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型微调需要调整数百万参数或部署密集权重更新，在资源受限环境中难以使用。LoRA减少了可训练参数，但基础密集权重仍带来高存储和计算成本。基于幅度的剪枝会产生稀疏模型，但通常会降低LoRA性能。

Method: 提出SALR（稀疏感知低秩表示），在均方误差框架下统一低秩适应与稀疏剪枝。静态剪枝冻结的基础权重以最小化剪枝误差边界，通过截断SVD低秩适配器恢复丢弃的残差信息。采用位图编码和两阶段流水线解码+GEMM设计实现硬件效率最大化。

Result: 在各种LLM上实现50%稀疏度，在GSM8K和MMLU任务上性能与LoRA相当，模型大小减少2倍，推理速度提升高达1.7倍。

Conclusion: SALR提供了一种有效的微调范式，通过统一稀疏剪枝和低秩适应，在保持性能的同时显著减少模型大小并提升推理速度，适用于资源受限环境。

Abstract: Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\times$, and delivers up to a $1.7\times$ inference speedup.

</details>


### [184] [Bayesian Robust Financial Trading with Adversarial Synthetic Market Data](https://arxiv.org/abs/2601.17008)
*Haochong Xia,Simin Li,Ruixiao Xu,Zhixia Zhang,Hongxiang Wang,Zhiqian Liu,Teng Yao Long,Molei Qin,Chuqiao Zong,Bo An*

Main category: cs.LG

TL;DR: 提出贝叶斯鲁棒框架，结合宏观条件GAN生成器和鲁棒策略学习，解决算法交易中模型对市场机制变化的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 现有算法交易模型在样本内表现良好，但在面对真实市场机制变化时性能下降。主要问题包括：现有策略对市场波动的鲁棒性不足，以及缺乏真实多样的训练模拟环境导致策略过拟合。

Method: 提出贝叶斯鲁棒框架：1) 数据端：使用宏观条件GAN生成器，以宏观经济指标为主要控制变量，生成具有真实时间、跨工具和宏观相关性的数据；2) 策略端：将交易过程建模为两人零和贝叶斯马尔可夫博弈，对抗代理通过扰动宏观指标模拟市场机制变化，交易代理通过分位数信念网络维护和更新对隐藏市场状态的信念，使用贝叶斯神经虚拟自博弈寻求鲁棒完美贝叶斯均衡。

Result: 在9种金融工具上的广泛实验表明，该框架优于9种最先进的基线方法。在COVID等极端事件中，该方法显示出改进的盈利能力和风险管理能力。

Conclusion: 该框架为不确定和变化的市场动态下的交易提供了可靠解决方案，通过系统整合宏观条件生成模型和鲁棒策略学习，有效解决了算法交易中的市场机制适应性问题。

Abstract: Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.

</details>


### [185] [A Dataset of Dengue Hospitalizations in Brazil (1999 to 2021) with Weekly Disaggregation from Monthly Counts](https://arxiv.org/abs/2601.16994)
*Lucas M. Morello,Matheus Lima Castro,Pedro Cesar M. G. Camargo,Liliane Moreira Nery,Darllan Collins da Cunha e Silva,Leopoldo Lusquino Filho*

Main category: cs.LG

TL;DR: 该论文发布了一个巴西市级登革热住院时间序列数据集，将原始月度数据通过三次样条插值分解为周度分辨率，并包含多种解释变量，用于流行病学预测的AI模型训练。


<details>
  <summary>Details</summary>
Motivation: 为提升流行病学预测AI模型训练效果，需要更高时间粒度（周度）的登革热数据，而原始数据仅为月度分辨率，因此需要开发数据分解方法。

Method: 采用三次样条插值方法将月度登革热住院数据分解为周度时间序列，通过圣保罗州高分辨率参考数据集验证了该方法的统计和时序有效性，并保持月度总量不变。

Result: 三次样条插值在三种策略（线性插值、抖动、三次样条）中表现出最高的参考数据贴合度，成功生成了1999-2021年期间的周度时间序列数据集。

Conclusion: 该数据集为多变量时间序列分析、环境健康研究和机器学习模型开发提供了高质量的周度分辨率数据资源，支持登革热爆发预测研究。

Abstract: This data paper describes and publicly releases this dataset (v1.0.0), published on Zenodo under DOI 10.5281/zenodo.18189192. Motivated by the need to increase the temporal granularity of originally monthly data to enable more effective training of AI models for epidemiological forecasting, the dataset harmonizes municipal-level dengue hospitalization time series across Brazil and disaggregates them to weekly resolution (epidemiological weeks) through an interpolation protocol with a correction step that preserves monthly totals. The statistical and temporal validity of this disaggregation was assessed using a high-resolution reference dataset from the state of Sao Paulo (2024), which simultaneously provides monthly and epidemiological-week counts, enabling a direct comparison of three strategies: linear interpolation, jittering, and cubic spline. Results indicated that cubic spline interpolation achieved the highest adherence to the reference data, and this strategy was therefore adopted to generate weekly series for the 1999 to 2021 period. In addition to hospitalization time series, the dataset includes a comprehensive set of explanatory variables commonly used in epidemiological and environmental modeling, such as demographic density, CH4, CO2, and NO2 emissions, poverty and urbanization indices, maximum temperature, mean monthly precipitation, minimum relative humidity, and municipal latitude and longitude, following the same temporal disaggregation scheme to ensure multivariate compatibility. The paper documents the datasets provenance, structure, formats, licenses, limitations, and quality metrics (MAE, RMSE, R2, KL, JSD, DTW, and the KS test), and provides usage recommendations for multivariate time-series analysis, environmental health studies, and the development of machine learning and deep learning models for outbreak forecasting.

</details>


### [186] [MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning](https://arxiv.org/abs/2601.17006)
*Xuchen Li,Jing Chen,Xuzhao Li,Hao Liang,Xiaohuan Zhou,Taifeng Wang,Wentao Zhang*

Main category: cs.LG

TL;DR: MathMixup：一种通过混合和分解策略系统生成高质量、难度可控数学推理问题的新数据合成范式，结合课程学习显著提升LLM数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据合成方法存在多样性有限、难度控制不精确的问题，无法有效支持课程学习等高效训练范式，需要开发能够系统生成难度可控高质量数学问题的数据合成方法

Method: 提出MathMixup数据合成范式，采用混合和分解策略系统生成难度可控的数学推理问题，结合自动自检和人工筛选确保语义清晰和难度梯度，构建MathMixupQA数据集并设计课程学习策略

Result: MathMixup及其课程学习策略显著提升LLM数学推理性能，微调后的Qwen2.5-7B在7个数学基准测试中平均得分52.6%，超越先前最先进方法

Conclusion: MathMixup在提升LLM数学推理能力和推进以数据为中心的课程学习方面具有有效性和广泛适用性，为数学推理任务的高质量训练数据生成提供了新范式

Abstract: In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.

</details>


### [187] [Analysis of voice recordings features for Classification of Parkinson's Disease](https://arxiv.org/abs/2601.17007)
*Beatriz Pérez-Sánchez,Noelia Sánchez-Maroño,Miguel A. Díaz-Freire*

Main category: cs.LG

TL;DR: 使用机器学习模型结合特征选择方法，通过语音录音检测帕金森病，显著减少特征数量而不影响模型性能


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统方法成本高，语音分析可作为早期诊断工具，但需要确定哪些语音特征对诊断最相关

Method: 结合多种机器学习模型与特征选择技术，筛选语音录音中最具信息量的特征，减少特征数量

Result: 机器学习方法（特别是神经网络）适合帕金森病分类，特征数量可显著减少而不影响模型性能

Conclusion: 机器学习结合特征选择能有效诊断帕金森病，减少计算成本，提高诊断效率

Abstract: Parkinson's disease (PD) is a chronic neurodegenerative disease. Early diagnosis is essential to mitigate the progressive deterioration of patients' quality of life. The most characteristic motor symptoms are very mild in the early stages, making diagnosis difficult. Recent studies have shown that the use of patient voice recordings can aid in early diagnosis. Although the analysis of such recordings is costly from a clinical point of view, advances in machine learning techniques are making the processing of this type of data increasingly accurate and efficient. Vocal recordings contain many features, but it is not known whether all of them are relevant for diagnosing the disease.
  This paper proposes the use of different types of machine learning models combined with feature selection methods to detect the disease. The selection techniques allow to reduce the number of features used by the classifiers by determining which ones provide the most information about the problem. The results show that machine learning methods, in particular neural networks, are suitable for PD classification and that the number of features can be significantly reduced without affecting the performance of the models.

</details>


### [188] [Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study](https://arxiv.org/abs/2601.17010)
*Hudson Golino*

Main category: cs.LG

TL;DR: LLM嵌入作为可搜索的语义空间，需要系统优化而非默认使用全向量；结合TEFI和NMI的加权标准能平衡结构准确性和组织性


<details>
  <summary>Details</summary>
Motivation: 当前LLM嵌入在心理学项目池维度结构估计中被视为静态、横截面表示，忽略了最优结构信息可能集中在嵌入空间特定区域的可能性

Method: 将嵌入重构为可搜索的景观，采用动态探索性图分析(DynEGA)系统遍历嵌入坐标，将维度索引视为伪时间顺序；大规模蒙特卡洛模拟使用OpenAI的text-embedding-3-small模型嵌入代表自恋五个维度的项目

Result: TEFI在深层嵌入范围(900-1200维度)达到最小值，此时基于熵的组织性最大但结构准确性下降；NMI在浅层深度达到峰值，此时维度恢复最强但基于熵的拟合仍不理想；加权复合标准能识别平衡准确性和组织性的嵌入维度深度区域

Conclusion: 嵌入景观是非均匀的语义空间，需要原则性优化而非默认使用全向量；最优嵌入深度随项目池大小系统缩放；单指标优化产生结构不连贯的解决方案，而加权复合标准能平衡准确性和组织性

Abstract: Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.

</details>


### [189] [FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices](https://arxiv.org/abs/2601.17063)
*Byeongju Kim,Jungwan Lee,Donghyeon Han,Hoi-Jun Yoo,Sangyeob Kim*

Main category: cs.LG

TL;DR: FlashMoE是一个将不活跃专家卸载到SSD的系统，使MoE模型能在有限RAM下进行高效推理，通过轻量级ML缓存策略提升缓存命中率，在真实硬件上实现2.6倍加速。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过稀疏激活实现高效推理，但现有系统依赖DRAM卸载，不适合内存受限的移动设备环境。随着MoE模型增长到数百GB，RAM卸载方案变得不切实际。

Method: 提出FlashMoE系统，将不活跃专家卸载到SSD，采用轻量级ML缓存策略，自适应结合最近使用和频率信号来最大化专家重用，显著减少存储I/O。

Result: 在真实硬件平台上，FlashMoE比LRU和LFU等传统卸载策略提升缓存命中率高达51%，相比现有MoE推理系统实现2.6倍加速。

Conclusion: FlashMoE通过SSD卸载和智能缓存策略，解决了大模型在内存受限设备上的推理问题，为边缘设备上的MoE模型部署提供了实用解决方案。

Abstract: Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.

</details>


### [190] [ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting](https://arxiv.org/abs/2601.17065)
*Haoxuan Li,He Chang,Yunshan Ma,Yi Bin,Yang Yang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出ThinkTank-ME框架，通过多专家协作模拟真实智库分析，用于中东事件预测，优于现有单模型方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM事件预测方法采用单模型架构，只能生成单一显式轨迹，难以捕捉复杂区域背景下多样的地缘政治细微差别

Method: 引入ThinkTank-ME框架，模拟真实战略决策中的协作专家分析；构建中东事件预测基准POLECAT-FOR-ME，促进专家专业化和严格评估

Result: 实验结果表明，多专家协作在处理复杂时序地缘政治预测任务方面具有优越性

Conclusion: 多专家协作框架能更好地处理复杂区域事件预测，代码已开源

Abstract: Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.

</details>


### [191] [Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis](https://arxiv.org/abs/2601.17073)
*Yifei Zhang,Meimei Liu,Zhengwu Zhang*

Main category: cs.LG

TL;DR: CM-JIVNet是一个概率框架，通过多模态融合和特征解耦，从结构-功能连接组数据中学习联合和独立的潜在表示，提升跨模态重建和行为预测性能。


<details>
  <summary>Details</summary>
Motivation: 大脑组织通常通过结构连接（SC）和功能连接（FC）等多模态成像来表征，有效整合这些互补但本质不同的数据源对于揭示驱动行为表型的跨模态模式至关重要。然而，高维度、非线性、复杂的SC-FC耦合以及模态特异性变化的分离等挑战阻碍了有效整合。

Method: 提出跨模态联合-个体变分网络（CM-JIVNet），这是一个统一的概率框架，从配对的SC-FC数据集中学习因子化的潜在表示。模型使用多头注意力融合模块来捕获非线性跨模态依赖关系，同时分离独立的模态特定信号。

Result: 在人类连接组项目年轻成人（HCP-YA）数据上验证，CM-JIVNet在跨模态重建和行为特征预测方面表现出优越性能。

Conclusion: 通过有效解耦联合和个体特征空间，CM-JIVNet为大规模多模态大脑分析提供了一个稳健、可解释且可扩展的解决方案。

Abstract: Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.

</details>


### [192] [Multi-Agent Deep Reinforcement Learning Under Constrained Communications](https://arxiv.org/abs/2601.17069)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: 提出DG-MAPPO分布式多智能体强化学习框架，通过多跳通信和分布式图注意力网络实现完全去中心化训练与执行，无需全局信息或集中式评论家。


<details>
  <summary>Details</summary>
Motivation: 集中式训练分散式执行(CTDE)范式存在可扩展性、鲁棒性和泛化性瓶颈，依赖全局状态信息，在实际场景中(如队友增减、环境动态变化)脆弱且重训练成本高。分布式方法允许智能体仅使用本地信息和点对点通信进行适应。

Method: 1) 开发分布式图注意力网络(D-GAT)，通过多跳通信进行全局状态推断，智能体以完全分布式方式通过输入相关注意力权重整合邻居特征；2) 基于D-GAT开发分布式图注意力MAPPO(DG-MAPPO)，智能体使用本地观察、多跳通信和共享/平均奖励优化本地策略和价值函数。

Result: 在StarCraftII多智能体挑战、Google Research Football和Multi-Agent Mujoco上的实验表明，该方法始终优于强CTDE基线，在广泛合作任务中实现卓越协调，适用于同质和异质团队。

Conclusion: DG-MAPPO提供了原则性和可扩展的鲁棒协作解决方案，消除了集中式训练或全局可观测性的需求，据所知是首个完全消除对特权集中信息依赖的方法，使智能体仅通过点对点通信进行学习和行动。

Abstract: Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.

</details>


### [193] [PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction](https://arxiv.org/abs/2601.17074)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: PhysE-Inv：一种结合物理约束与深度学习的新型框架，用于解决北极雪深反演问题，通过物理引导的对比学习和反演方法，在数据稀疏和噪声环境下显著提升预测精度和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 北极雪深估计是一个关键的时间变化反演问题，现有基于过程的模型对稀疏数据高度敏感，而数据驱动模型缺乏物理可解释性，无法满足气候关键应用的需求。

Method: 提出PhysE-Inv框架，整合LSTM编码器-解码器多头注意力架构、物理引导对比学习和物理约束反演方法。核心创新是使用静水平衡前向模型作为目标制定代理，在没有直接雪深地面真值的情况下进行有效学习；同时通过潜在空间的重建物理正则化，从噪声不完整时间序列输入中动态发现隐藏物理参数。

Result: 与最先进基线相比，PhysE-Inv显著提升预测性能，误差减少20%，同时展现出比经验方法更优越的物理一致性和对数据稀疏性的鲁棒性。

Conclusion: 该方法为噪声容忍、可解释的反演建模开辟了新路径，在地理空间和冰冻圈领域具有广泛适用性。

Abstract: The accurate estimation of Arctic snow depth ($h_s$) remains a critical time-varying inverse problem due to the extreme scarcity and noise inherent in associated sea ice parameters. Existing process-based and data-driven models are either highly sensitive to sparse data or lack the physical interpretability required for climate-critical applications. To address this gap, we introduce PhysE-Inv, a novel framework that integrates a sophisticated sequential architecture, an LSTM Encoder-Decoder with Multi-head Attention and physics-guided contrastive learning, with physics-guided inference.Our core innovation lies in a surjective, physics-constrained inversion methodology. This methodology first leverages the hydrostatic balance forward model as a target-formulation proxy, enabling effective learning in the absence of direct $h_s$ ground truth; second, it uses reconstruction physics regularization over a latent space to dynamically discover hidden physical parameters from noisy, incomplete time-series input. Evaluated against state-of-the-art baselines, PhysE-Inv significantly improves prediction performance, reducing error by 20\% while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods. This approach pioneers a path for noise-tolerant, interpretable inverse modeling, with wide applicability in geospatial and cryospheric domains.

</details>


### [194] [Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning](https://arxiv.org/abs/2601.18626)
*Yingxiao Huo,Satya Prakash Dash,Radu Stoican,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出一种基于秩1近似的自然策略优化方法，通过近似逆Fisher信息矩阵来降低计算复杂度，在多种环境中优于标准actor-critic和信任域基线方法。


<details>
  <summary>Details</summary>
Motivation: 自然梯度在深度强化学习中具有快速收敛特性，但计算自然梯度需要每次迭代都求逆Fisher信息矩阵，这在计算上是不可行的。需要一种高效且可扩展的自然策略优化技术。

Method: 提出一种利用秩1近似来近似完整逆Fisher信息矩阵的自然策略优化方法。理论上证明在特定条件下，这种秩1近似比策略梯度收敛更快，并且在某些条件下具有与随机策略梯度方法相同的样本复杂度。

Result: 在多样化的环境中进行基准测试，结果显示该方法在性能上优于标准的actor-critic和信任域基线方法。

Conclusion: 提出的秩1近似自然策略优化方法既保持了自然梯度的优势，又解决了计算复杂性问题，在实际应用中表现出优越性能。

Abstract: Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.

</details>


### [195] [E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning](https://arxiv.org/abs/2601.17076)
*Jiajun Chen,Yue Wu,Kai Huang,Wen Xi,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi,Guanjie Cheng*

Main category: cs.LG

TL;DR: 提出E2PL框架，通过任务定制提示和缺失感知提示解决不完整多视图多标签类增量学习问题，使用高效原型张量化将参数复杂度从指数级降至线性级。


<details>
  <summary>Details</summary>
Motivation: 现实网络应用中存在视图缺失和类别动态扩展的问题，现有方法要么缺乏对新类别的适应性，要么在处理缺失视图模式时参数呈指数增长，限制了可扩展性。

Method: 提出E2PL框架，包含任务定制提示用于类增量适应，缺失感知提示用于灵活整合任意视图缺失场景，通过高效原型张量化模块降低参数复杂度，并采用动态对比学习策略增强鲁棒性。

Result: 在三个基准测试上的实验表明，E2PL在效果和效率方面均优于现有最先进方法。

Conclusion: E2PL框架有效解决了不完整多视图多标签类增量学习问题，通过创新的提示设计和参数优化方法实现了高效且鲁棒的学习。

Abstract: Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \textsf{E2PL} unifies two novel prompt designs: \emph{task-tailored prompts} for class-incremental adaptation and \emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.

</details>


### [196] [SFO: Learning PDE Operators via Spectral Filtering](https://arxiv.org/abs/2601.17090)
*Noam Koren,Rafael Moschopoulos,Kira Radinsky,Elad Hazan*

Main category: cs.LG

TL;DR: SFO是一种新型神经算子，使用通用谱基参数化积分核，通过仅学习快速衰减的特征值谱系数，高效捕捉PDE解映射中的长程非局部相互作用。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在捕捉偏微分方程解映射中的长程非局部相互作用时效率低下，而理论研究发现离散格林函数具有空间线性动力系统结构，这启发了更高效的表示方法。

Method: 提出谱滤波算子(SFO)，使用从希尔伯特矩阵特征模态导出的固定全局正交基（通用谱基）参数化积分核，仅学习快速衰减特征值的谱系数，实现高效表示。

Result: 在六个基准测试（包括反应扩散、流体动力学和3D电磁学）中，SFO达到最先进精度，相对于强基线误差降低达40%，同时使用更少参数。

Conclusion: SFO通过谱基表示提供了一种高效捕捉PDE长程相互作用的神经算子框架，在精度和参数效率方面均有显著提升。

Abstract: Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.

</details>


### [197] [CUROCKET: Optimizing ROCKET for GPU](https://arxiv.org/abs/2601.17091)
*Ole Stüven,Keno Moenck,Thorsten Schüppstuhl*

Main category: cs.LG

TL;DR: 提出CUROCKET算法，将ROCKET特征提取方法从CPU迁移到GPU执行，通过解决非均匀卷积核的并行化问题，实现比CPU版本高11倍的能效比。


<details>
  <summary>Details</summary>
Motivation: ROCKET是时间序列分类的高效特征提取算法，但现有实现主要局限于CPU执行。卷积操作高度可并行化，适合GPU加速，但ROCKET使用的非均匀卷积核使得标准GPU卷积方法效率低下。

Method: 提出CUROCKET算法，专门设计用于在GPU上高效执行ROCKET的非均匀卷积核计算，解决了标准GPU卷积方法在处理非均匀核时的效率问题。

Result: CUROCKET在GPU上实现了比CPU版本高达11倍的每瓦计算效率提升，显著加速了ROCKET特征提取过程。

Conclusion: 成功将ROCKET算法迁移到GPU平台，通过专门设计的并行化方法解决了非均匀卷积核的计算效率问题，为时间序列分类提供了更高效的特征提取解决方案。

Abstract: ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.

</details>


### [198] [The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations](https://arxiv.org/abs/2601.17093)
*Olha Sirikova,Alvin Chan*

Main category: cs.LG

TL;DR: 提出"相似性三角"框架，结合静态表征相似性、功能相似性和稀疏性相似性三个互补视角，用于更全面地比较神经网络表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法在比较神经网络表示时往往提供有限的视角，需要更全面的框架来理解模型在科学应用中的内部机制。

Method: 提出相似性三角框架，结合三种相似性度量：静态表征相似性（CKA/Procrustes）、功能相似性（线性模式连接或预测相似性）和稀疏性相似性（剪枝下的鲁棒性）。在CNN、Vision Transformer和视觉语言模型上，使用分布内（ImageNetV2）和分布外（CIFAR-10）测试集进行分析。

Result: 发现：(1) 架构家族是表征相似性的主要决定因素，形成不同的聚类；(2) CKA自相似性和任务准确性在剪枝过程中强相关，但准确性下降更剧烈；(3) 对某些模型对，剪枝似乎能正则化表示，暴露出共享的计算核心。

Conclusion: 该框架提供了更全面的方法来评估模型是否收敛到相似的内部机制，为科学研究中的模型选择和分析提供了有用工具。

Abstract: Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.

</details>


### [199] [Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation](https://arxiv.org/abs/2601.17094)
*Junichiro Niimi*

Main category: cs.LG

TL;DR: 该论文提出"嘴不是大脑"的架构原则，将世界模型与语言模型分离，使用深度玻尔兹曼机作为世界模型，结合冻结的GPT-2进行文本生成，在消费者评论领域验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能生成流畅文本，但它们是否真正理解世界还是仅仅产生看似合理的语言仍存在争议。作者旨在明确分离世界模型和语言模型，以解决语言模型缺乏真实世界理解的问题。

Method: 提出三组件架构：1) 深度玻尔兹曼机作为基于能量的世界模型捕获领域结构；2) 适配器将潜在信念状态投影到嵌入空间；3) 冻结的GPT-2提供语言能力但不包含领域知识。在亚马逊智能手机评论领域实例化该框架。

Result: 实验表明：1) 通过世界模型调节的生成在情感相关性、困惑度和语义相似度上显著优于仅基于提示的生成；2) DBM的能量函数能区分连贯与不连贯的市场配置；3) 对特定属性的干预能因果传播到生成的文本中，干预输出与自然样本的分布统计一致。

Conclusion: 即使小规模语言模型，当连接到适当的世界模型时，也能实现一致且可控的生成。这为分离语言能力与世界理解提供了实证支持，表明"嘴不是大脑"的架构原则是有效的。

Abstract: Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.

</details>


### [200] [MambaNet: Mamba-assisted Channel Estimation Neural Network With Attention Mechanism](https://arxiv.org/abs/2601.17108)
*Dianxin Luan,Chengsi Liang,Jie Huang,Zheng Lin,Kaitao Meng,John Thompson,Cheng-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一种结合Mamba架构和自注意力机制的神经网络框架，用于OFDM波形的大规模子载波信道估计，在降低复杂度的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 针对OFDM系统中大规模子载波配置下的信道估计问题，传统方法难以有效处理长距离依赖关系，而基于Transformer的神经网络又存在空间复杂度高的问题，需要一种既能捕获长距离依赖又具有低复杂度的解决方案。

Method: 提出Mamba辅助的神经网络框架，集成定制化的Mamba架构和自注意力机制。采用双向选择性扫描（bidirectional selective scan）来处理非因果性的子载波信道增益，相比传统Mamba结构更适合信道估计任务。

Result: 在3GPP TS 36.101信道上的仿真结果表明，相比其他基线神经网络解决方案，该方法在减少可调参数数量的同时，实现了更好的信道估计性能，且空间复杂度低于基于Transformer的神经网络。

Conclusion: 该框架为大规模子载波OFDM系统的信道估计提供了一种高效解决方案，通过Mamba架构和自注意力机制的结合，在保持低复杂度的同时有效捕获子载波间的长距离依赖关系。

Abstract: This paper proposes a Mamba-assisted neural network framework incorporating self-attention mechanism to achieve improved channel estimation with low complexity for orthogonal frequency-division multiplexing (OFDM) waveforms, particularly for configurations with a large number of subcarriers. With the integration of customized Mamba architecture, the proposed framework handles large-scale subcarrier channel estimation efficiently while capturing long-distance dependencies among these subcarriers effectively. Unlike conventional Mamba structure, this paper implements a bidirectional selective scan to improve channel estimation performance, because channel gains at different subcarriers are non-causal. Moreover, the proposed framework exhibits relatively lower space complexity than transformer-based neural networks. Simulation results tested on the 3GPP TS 36.101 channel demonstrate that compared to other baseline neural network solutions, the proposed method achieves improved channel estimation performance with a reduced number of tunable parameters.

</details>


### [201] [Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts](https://arxiv.org/abs/2601.17111)
*Xuan-Phi Nguyen,Shrey Pandit,Austin Xu,Caiming Xiong,Shafiq Joty*

Main category: cs.LG

TL;DR: LLEP是一种新的专家并行算法，通过动态重路由过载设备上的token和专家参数到空闲设备，解决MoE模型中路由不平衡导致的性能瓶颈，相比标准EP实现5倍加速和4倍内存减少。


<details>
  <summary>Details</summary>
Motivation: MoE模型即使经过预训练，仍存在显著的路由不平衡问题。专家并行(EP)假设路由平衡，但在极端不平衡时，大量token会集中到少数专家，导致计算和内存瓶颈，特别是在后训练和推理阶段无法使用显式负载均衡的情况下。

Method: 提出Least-Loaded Expert Parallelism (LLEP)算法，动态监测设备负载，将过载设备上的多余token和相关专家参数重路由到利用率不足的设备，确保所有设备在最小集体延迟内完成工作，同时满足内存约束。

Result: 在不同模型规模下，LLEP相比标准EP实现高达5倍加速和4倍峰值内存使用减少。对于gpt-oss-120b模型，后训练和推理速度提升约1.9倍。通过理论分析和实证评估验证了方法的有效性。

Conclusion: LLEP解决了MoE模型中路由不平衡导致的专家并行瓶颈，实现了更高效的后训练和推理。该方法提供了硬件特定超参数调优的原则性框架，揭示了关键的性能权衡。

Abstract: Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.

</details>


### [202] [Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization](https://arxiv.org/abs/2601.17112)
*A. El Ichi,K. Jbilou*

Main category: cs.LG

TL;DR: 提出基于cproduct的张量压缩框架，用于降低LLMs的内存占用和计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能出色，但存在极大的内存占用和计算成本问题，需要高效的压缩方法

Method: 利用cproduct的代数结构，在变换域中表示权重张量，通过低秩张量因子联合近似frontal slices，实现多维相关性压缩

Result: 该方法能够超越传统SVD方法，利用多维相关性实现计算高效的压缩

Conclusion: 基于cproduct的张量压缩框架为解决LLMs内存和计算瓶颈提供了有效途径

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.

</details>


### [203] [How does Graph Structure Modulate Membership-Inference Risk for Graph Neural Networks?](https://arxiv.org/abs/2601.17130)
*Megha Khosla*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络中的成员推理攻击风险，特别关注图结构对隐私泄露的影响，并分析了训练图构建、推理时边访问等因素对成员推理攻击效果的影响。


<details>
  <summary>Details</summary>
Motivation: GNNs在敏感应用中的使用引发了训练数据泄露的担忧，现有隐私泄露研究主要基于非图领域，需要针对图结构特性的专门分析。

Method: 形式化节点邻域元组的成员推理攻击，研究两个关键维度：训练图构建方法和推理时边的访问权限，通过实验分析不同采样策略和边访问设置的影响。

Result: 雪球采样的覆盖偏差通常损害泛化能力；推理时允许训练-测试边访问能提高测试精度、缩小训练-测试差距，并在多数模型和数据集上产生最低的成员推理优势；泛化差距不能完全代表成员推理风险；节点级任务的归纳分割破坏了可交换性。

Conclusion: 图结构对成员推理攻击有重要影响，需要针对图特性的隐私分析，标准差分隐私界限在节点级任务中适用性有限，需要开发专门针对图模型的隐私保护方法。

Abstract: Graph neural networks (GNNs) have become the standard tool for encoding data and their complex relationships into continuous representations, improving prediction accuracy in several machine learning tasks like node classification and link prediction. However, their use in sensitive applications has raised concerns about the potential leakage of training data. Research on privacy leakage in GNNs has largely been shaped by findings from non-graph domains, such as images and tabular data. We emphasize the need of graph specific analysis and investigate the impact of graph structure on node level membership inference. We formalize MI over node-neighbourhood tuples and investigate two important dimensions: (i) training graph construction and (ii) inference-time edge access. Empirically, snowball's coverage bias often harms generalisation relative to random sampling, while enabling inter-train-test edges at inference improves test accuracy, shrinks the train-test gap, and yields the lowest membership advantage across most of the models and datasets. We further show that the generalisation gap empirically measured as the performance difference between the train and test nodes is an incomplete proxy for MI risk: access to edges dominates-MI can rise or fall independent of gap changes. Finally, we examine the auditability of differentially private GNNs, adapting the definition of statistical exchangeability of train-test data points for graph based models. We show that for node level tasks the inductive splits (random or snowball sampled) break exchangeability, limiting the applicability of standard bounds for membership advantage of differential private models.

</details>


### [204] [Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation](https://arxiv.org/abs/2601.17133)
*Inderjeet Singh,Eleonore Vissol-Gaudin,Andikan Otung,Motoyoshi Sekiya*

Main category: cs.LG

TL;DR: KNEXA-FL是一个新颖的去中心化联邦学习框架，通过上下文多臂老虎机算法优化异构LLM代理之间的知识交换，解决了数据隐私与协作效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在专业化LLM微调中存在矛盾：需要跨组织数据协作但受限于数据隐私。集中式FL存在单点故障和模型反转攻击风险，而去中心化FL的随机P2P配对效率低下且可能导致负迁移。

Method: 提出KNEXA-FL框架，采用非聚合的中心分析器/匹配器(CPM)，将P2P协作建模为上下文多臂老虎机问题，使用LinUCB算法在抽象代理配置文件上学习最优匹配策略，通过安全蒸馏实现异构PEFT-based LLM代理之间的直接知识交换。

Result: 在代码生成任务上的实验显示，KNEXA-FL相比随机P2P协作将Pass@1提高了约50%，且表现出稳定的收敛性，而强大的集中式蒸馏基线则出现灾难性性能崩溃。

Conclusion: 自适应、基于学习的编排是构建稳健有效的去中心化AI生态系统的基础原则，KNEXA-FL为解决数据隐私与协作效率的权衡提供了有效方案。

Abstract: Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.

</details>


### [205] [A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization](https://arxiv.org/abs/2601.17646)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: 论文研究了经验风险最小化（ERM）的稳定性问题，特别关注非严格凸损失函数导致多值解的情况，提出了Painlevé-Kuratowski上半连续性作为ERM解对应的内在稳定性概念。


<details>
  <summary>Details</summary>
Motivation: 传统ERM稳定性研究通常针对单值输出，但实际中凸非严格损失函数会产生集合值最小化器。需要建立适用于集合值解的内在稳定性理论框架，为选择稳定性提供解释基础。

Method: 采用Painlevé-Kuratowski上半连续性作为ERM解对应的稳定性概念（集合层面的Hadamard适定性）。在最小非退化定性机制下，分析Mosco一致性扰动和局部有界最小化器如何保证PK上半连续性、最小值连续性和消失间隙近最小化器的一致性。

Result: 证明了在Mosco一致性扰动和局部有界最小化器条件下，ERM解对应具有PK上半连续性、最小值连续性，且消失间隙近最小化器具有一致性。二次增长条件可导出显式定量偏差界。

Conclusion: PK上半连续性是ERM解对应的内在稳定性概念，为理解选择稳定性提供了必要前提。在适当的定性条件下，ERM具有良好的稳定性性质，二次增长条件进一步提供定量保证。

Abstract: Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.

</details>


### [206] [ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning](https://arxiv.org/abs/2601.17135)
*Jakob Karalus,Friedhelm Schwenker*

Main category: cs.LG

TL;DR: ConceptACT：通过概念感知注意力机制，利用人类语义知识提升机器人模仿学习效率的扩展方法


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法仅依赖低级传感器数据，忽略了人类自然拥有的丰富语义知识（如物体属性、空间关系、任务约束），这些知识能显著提升学习效率

Method: 扩展Action Chunking with Transformers (ACT)，在训练时利用演示级别的语义概念标注。采用修改的transformer架构，在最终编码器层实现概念感知交叉注意力机制，并与人类标注对齐监督

Result: 在两个具有逻辑约束的机器人操作任务中，ConceptACT比标准ACT收敛更快、样本效率更高。注意力机制集成显著优于简单的辅助预测损失或语言条件模型

Conclusion: 适当集成的语义监督为机器人学习提供了强大的归纳偏置，能实现更高效的学习。概念仅在演示收集时使用，部署时无需语义输入，最小化标注负担

Abstract: Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.

</details>


### [207] [Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging](https://arxiv.org/abs/2601.17180)
*Inés Gonzalez-Pepe,Vinuyan Sivakolunthu,Jacob Fortin,Yohan Chatelain,Tristan Glatard*

Main category: cs.LG

TL;DR: 提出两种基于数值不确定性的CNN优化方法：Conservative & Aggressive NaNs，通过识别数值不稳定体素并用NaN替换，让后续层跳过无关计算，在神经影像任务中平均实现1.67倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 神经影像深度学习模型规模越来越大，效率成为持续关注的问题。研究发现许多卷积操作应用于数值噪声主导的值，对模型输出影响可忽略，存在大量冗余计算。

Method: 提出Conservative & Aggressive NaNs两种方法：通过max pooling和unpooling的变体识别数值不稳定体素，用NaN替换，后续层可跳过NaN相关计算。无需架构修改，在PyTorch中实现。

Result: 当输入包含至少50% NaN时获得稳定加速；在神经影像常见的高NaN比例（>2/3）场景中，平均推理速度提升1.67倍。Conservative NaNs平均减少30%卷积操作，特定层可跳过64.64%卷积，无性能损失；Aggressive NaNs可跳过69.30%卷积，但可能影响性能。

Conclusion: 数值不确定性可被利用来减少CNN中的冗余计算，提高推理效率。两种方法在神经影像和图像分类任务中均有效，为大规模深度学习模型提供了实用的效率优化方案。

Abstract: Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.

</details>


### [208] [Robust Learning of a Group DRO Neuron](https://arxiv.org/abs/2601.18115)
*Guyang Cao,Shuyao Li,Sushrut Karmalkar,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 该论文研究在任意标签噪声和组级分布偏移下学习单个神经元的问题，提出一种计算高效的原始-对偶算法，能在最坏情况组权重下获得常数因子竞争性保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常存在标签噪声和组间分布偏移，传统方法在这些挑战下性能下降。需要开发能处理任意标签噪声和组级分布偏移的鲁棒学习算法，确保在最坏情况组权重下仍有良好性能。

Method: 提出组分布鲁棒优化框架，通过f-散度惩罚偏离均匀组权重。开发计算高效的原始-对偶算法，直接处理损失函数的非凸性，使用对偶外推更新技术。

Result: 算法输出向量ŵ在最坏情况组权重下与最优参数w*保持常数因子竞争性。该框架能处理任意标签噪声和组特定分布偏移，在LLM预训练基准上显示出潜力。

Conclusion: 该研究为在存在标签噪声和分布偏移下学习单个神经元提供了鲁棒的理论和算法框架，原始-对偶方法能有效处理非凸优化问题，为鲁棒机器学习提供新思路。

Abstract: We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\mathcal p_{[1]},\dots,\mathcal p_{[K]}$, we seek to approximate $\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\boldsymbolλ \in Δ_K$, where the objective is $\sum_{i \in [K]}λ_{[i]}\,\mathbb E_{(\mathbf x,y)\sim\mathcal p_{[i]}}(σ(\mathbf w\cdot\mathbf x)-y)^2 - νd_f(\boldsymbolλ,\frac{1}{K}\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $ν\geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\widehat{\mathbf w}$ that is constant-factor competitive with $\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.

</details>


### [209] [Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data](https://arxiv.org/abs/2601.17183)
*Farzam Asad,Junaid Saif Khan,Maria Tariq,Sundus Munir,Muhammad Adnan Khan*

Main category: cs.LG

TL;DR: 该研究通过模拟四家异构医院客户端，验证了FedProx算法在心脏病预测中的有效性，在保护隐私的前提下实现了85%的准确率，优于集中式学习和孤立本地模型。


<details>
  <summary>Details</summary>
Motivation: 医疗数据因隐私法规无法直接共享，而联邦学习可解决此问题。但临床数据集存在非独立同分布特征，由人口差异、疾病流行率和机构实践多样性导致，需要有效处理异构性的算法。

Method: 使用UCI心脏病数据集，通过人口统计分层模拟四家异构医院客户端，创建非独立同分布数据分区。采用FedProx算法，通过近端正则化控制客户端漂移，进行50次独立运行的消融研究。

Result: FedProx在mu=0.05时达到85.00%准确率，优于集中式学习(83.33%)和孤立本地模型(平均78.45%)。近端正则化在异构环境中有效控制客户端漂移，且不泄露患者隐私。

Conclusion: FedProx在异构医疗数据联邦学习中表现优异，为实际医疗系统部署提供了算法见解和实践指南，结果可直接应用于医院IT管理员实施隐私保护协作学习。

Abstract: Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.

</details>


### [210] [Rethinking Benchmarks for Differentially Private Image Classification](https://arxiv.org/abs/2601.17189)
*Sabrina Mokhtari,Sara Kodeiri,Shubhankar Mohapatra,Florian Tramer,Gautam Kamath*

Main category: cs.LG

TL;DR: 作者重新审视了差分隐私图像分类的基准测试，提出了一套全面的基准集，并创建了公开排行榜来追踪社区进展


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私机器学习基准不够全面，缺乏在不同设置下的系统评估，需要更全面的基准来推动领域发展

Method: 提出了一套全面的基准测试集，涵盖有无额外数据、凸设置、不同数据集等多种场景，并测试现有技术在这些基准上的表现

Result: 建立了全面的基准测试框架，测试了现有技术在不同设置下的有效性，并创建了公开可用的排行榜

Conclusion: 该工作为差分隐私机器学习提供了系统化的评估框架，有助于社区追踪进展和比较不同方法在不同场景下的表现

Abstract: We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.

</details>


### [211] [ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule](https://arxiv.org/abs/2601.18681)
*Yilie Huang,Wenpin Tang,Xunyu Zhou*

Main category: cs.LG

TL;DR: ART-RL：通过强化学习自适应调整扩散模型的时间步长，优化采样效率


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用均匀或手动设计的时间网格可能不是最优的，特别是在有限时间步预算下。需要一种自适应的时间步长调整方法来最小化离散化误差。

Method: 提出自适应重参数化时间（ART）方法，通过控制重参数化时间变量的时钟速度来改变时间步长分布。进一步提出ART-RL，将时间变化建模为连续时间强化学习问题，使用高斯策略并通过actor-critic算法学习最优时间调度。

Result: 在CIFAR-10上，ART-RL在多种预算下改善了Fréchet Inception Distance。该方法还能迁移到AFHQv2、FFHQ和ImageNet等数据集而无需重新训练。

Conclusion: ART-RL提供了一种数据驱动的方法来自适应优化扩散模型的时间步长调度，提高了采样效率并能在不同数据集间迁移。

Abstract: We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.

</details>


### [212] [PUNCH: Physics-informed Uncertainty-aware Network for Coronary Hemodynamics](https://arxiv.org/abs/2601.17192)
*Sukirt Thakur,Marcus Roper,Yang Zhou,Reza Akbarian Bafghi,Brahmajee K. Nallamothu,C. Alberto Figueroa,Srinivas Paruchuri,Scott Burger,Maziar Raissi*

Main category: cs.LG

TL;DR: 提出一种非侵入性、不确定性感知的框架，直接从标准血管造影估计冠状动脉血流储备，无需真实血流测量，通过物理信息神经网络和变分推理实现。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉微血管功能障碍影响数百万人，但金标准的生理测量方法具有侵入性且可重复性差，需要开发非侵入性、可靠的诊断工具。

Method: 整合物理信息神经网络与变分推理，从造影剂传输的一阶原理模型推断冠状动脉血流，无需真实血流测量数据，仅需标准血管造影图像。

Result: 在1000个合成数据上显示预测不确定性与误差强相关；在12名患者临床验证中，与侵入性热稀释法CFR高度一致（r=0.90），置信区间小于重复侵入测量的变异性。

Conclusion: 该框架将常规血管造影转化为定量、不确定性感知的评估，可实现可扩展、更安全、可重复的冠状动脉微血管功能评估，有望扩大CMD诊断的可及性。

Abstract: Coronary microvascular dysfunction (CMD) affects millions worldwide yet remains underdiagnosed because gold-standard physiological measurements are invasive and variably reproducible. We introduce a non-invasive, uncertainty-aware framework for estimating coronary flow reserve (CFR) directly from standard angiography. The system integrates physics-informed neural networks with variational inference to infer coronary blood flow from first-principles models of contrast transport, without requiring ground-truth flow measurements. The pipeline runs in approximately three minutes per patient on a single GPU, with no population-level training.
  Using 1{,}000 synthetic spatiotemporal intensity maps (kymographs) with controlled noise and artifacts, the framework reliably identifies degraded data and outputs appropriately inflated uncertainty estimates, showing strong correspondence between predictive uncertainty and error (Pearson $r = 0.997$, Spearman $ρ= 0.998$). Clinical validation in 12 patients shows strong agreement between PUNCH-derived CFR and invasive bolus thermodilution (Pearson $r = 0.90$, $p = 6.3 \times 10^{-5}$). We focus on the LAD, the artery most commonly assessed in routine CMD testing. Probabilistic CFR estimates have confidence intervals narrower than the variability of repeated invasive measurements.
  By transforming routine angiography into quantitative, uncertainty-aware assessment, this approach enables scalable, safer, and more reproducible evaluation of coronary microvascular function. Because standard angiography is widely available globally, the framework could expand access to CMD diagnosis and establish a new paradigm for physics-informed, patient-specific inference from clinical imaging.

</details>


### [213] [Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data](https://arxiv.org/abs/2601.18728)
*Willem Diepeveen,Oscar Leong*

Main category: cs.LG

TL;DR: 提出Riemannian AmbientFlow框架，从噪声/损坏观测中同时学习概率生成模型和底层非线性数据流形，结合变分推理和黎曼几何，提供理论保证和逆问题应用。


<details>
  <summary>Details</summary>
Motivation: 在科学和成像应用中，通常无法获得干净样本，只能观测到噪声或线性损坏的测量值。同时，数据中的潜在结构（如流形几何）对下游科学分析很重要。现有方法难以从损坏观测中同时学习生成模型和底层流形结构。

Method: 基于AmbientFlow的变分推理框架，引入数据驱动的黎曼几何，通过归一化流诱导的pullback度量和黎曼自编码器提取流形结构。在几何正则化和测量条件下，学习平滑、双Lipschitz的流形参数化。

Result: 理论证明在适当条件下，学习模型能以可控误差恢复底层数据分布，并获得平滑的流形参数化。平滑解码器可作为逆问题的生成先验，具有恢复保证。在低维合成流形和MNIST上进行了实证验证。

Conclusion: Riemannian AmbientFlow成功从损坏观测中同时学习生成模型和底层流形结构，为科学和成像应用中的逆问题提供了理论保证的生成先验框架。

Abstract: Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.

</details>


### [214] [Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic](https://arxiv.org/abs/2601.18783)
*Deepthi Pathare,Leo Laine,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 基于PPO的多目标强化学习框架，为重型车辆学习连续帕累托最优策略集，平衡安全、能源效率和时效性


<details>
  <summary>Details</summary>
Motivation: 高速公路驾驶中，重型车辆需要在安全性、效率和运营成本之间做出复杂权衡决策。传统的标量奖励函数通过聚合这些竞争目标，往往模糊了它们之间的权衡结构。

Method: 提出基于近端策略优化（PPO）的多目标强化学习框架，在卡车战术决策的可扩展仿真平台上学习连续策略集，明确表示安全、能源效率和时效性三个冲突目标之间的权衡。

Result: 学习到连续帕累托最优策略集，形成平滑可解释的帕累托前沿，能在不同冲突目标之间灵活选择驾驶行为，无需重新训练即可在不同驾驶策略间无缝切换。

Conclusion: 该框架为自动驾驶卡车应用提供了稳健且自适应的决策策略，通过明确表示目标权衡实现了灵活的行为选择。

Abstract: Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.

</details>


### [215] [Accelerated Sinkhorn Algorithms for Partial Optimal Transport](https://arxiv.org/abs/2601.17196)
*Nghia Thu Truong,Qui Phu Pham,Quang Nguyen,Dung Luong,Mai Tran*

Main category: cs.LG

TL;DR: ASPOT算法通过结合交替最小化和Nesterov加速，将部分最优传输的Sinkhorn复杂度提升至O(n^{7/3}ε^{-5/3})，并展示了如何选择熵参数来改进经典Sinkhorn方法。


<details>
  <summary>Details</summary>
Motivation: 部分最优传输(POT)适用于分布质量不等或存在异常值的情况，但现有Sinkhorn方法的复杂度界限不理想，限制了可扩展性。

Method: 提出加速Sinkhorn for POT (ASPOT)算法，在POT设置中结合交替最小化和Nesterov风格加速，同时研究如何选择熵参数γ来改进经典Sinkhorn方法。

Result: ASPOT达到O(n^{7/3}ε^{-5/3})的复杂度，优于现有方法。实验验证了理论分析，并展示了所提方法的优越性能。

Conclusion: ASPOT为部分最优传输提供了更高效的算法，通过加速技术和参数优化显著提升了计算效率，在实际应用中表现出良好性能。

Abstract: Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $γ$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.

</details>


### [216] [SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment](https://arxiv.org/abs/2601.17204)
*Yinkai Wang,Yan Zhou Chen,Xiaohui Chen,Li-Ping Liu,Soha Hassoun*

Main category: cs.LG

TL;DR: SpecBridge提出了一种新颖的隐式对齐框架，通过将质谱数据直接投影到预训练分子模型的潜在空间，显著提高了小分子鉴定的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前小分子鉴定面临两个极端：显式生成模型需要原子级构建分子图，而联合对比模型需要从头学习跨模态子空间。这两种方法都存在局限性，需要更高效、稳定的解决方案。

Method: SpecBridge采用隐式对齐框架，将结构鉴定视为几何对齐问题。它微调自监督质谱编码器（DreaMS），将其直接投影到冻结的分子基础模型（ChemBERTa）的潜在空间中，然后通过余弦相似度在预计算的分子嵌入库中进行检索。

Result: 在MassSpecGym、Spectraverse和MSnLib基准测试中，SpecBridge相对于强大的神经基线，将top-1检索准确率提高了约20-25%，同时保持可训练参数数量较少。

Conclusion: 与冻结的基础模型对齐是设计新架构的实用、稳定替代方案。该方法在保持参数效率的同时显著提高了小分子鉴定性能。

Abstract: Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.

</details>


### [217] [NewPINNs: Physics-Informing Neural Networks Using Conventional Solvers for Partial Differential Equations](https://arxiv.org/abs/2601.17207)
*Maedeh Makki,Satish Chandran,Maziar Raissi,Adrien Grenier,Behzad Mohebbi*

Main category: cs.LG

TL;DR: NewPINNs提出了一种新的物理信息学习框架，将神经网络与传统数值求解器耦合，通过求解器一致性而非残差损失来训练网络，解决了传统PINNs的多个失败模式。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINNs)存在优化病态、损失权重敏感、在刚性问题或非线性区域表现差等问题，需要设计问题特定的损失函数。NewPINNs旨在通过将成熟的数值求解器直接集成到训练中，将物理约束、边界条件和数值稳定性委托给这些求解器，从而避免这些问题。

Method: NewPINNs将神经网络与传统数值求解器耦合：神经网络生成候选解状态，数值求解器推进这些状态，训练目标是最小化网络预测与求解器演化状态之间的差异。这种"拉-推"交互使网络通过重复暴露于求解器作用来学习物理可接受的解，无需设计问题特定的损失函数或显式计算微分方程残差。

Result: 该方法在涉及有限体积、有限元和谱求解器的多个正向和逆向问题中证明了有效性，能够缓解传统PINNs的优化病态、损失权重敏感性和在刚性问题/非线性区域表现差等问题。

Conclusion: NewPINNs通过将数值求解器直接集成到训练循环中，利用求解器一致性作为学习目标，提供了一种更稳健的物理信息学习框架，能够有效解决传统PINNs的多个已知失败模式。

Abstract: We introduce NewPINNs, a physics-informing learning framework that couples neural networks with conventional numerical solvers for solving differential equations. Rather than enforcing governing equations and boundary conditions through residual-based loss terms, NewPINNs integrates the solver directly into the training loop and defines learning objectives through solver-consistency. The neural network produces candidate solution states that are advanced by the numerical solver, and training minimizes the discrepancy between the network prediction and the solver-evolved state. This pull-push interaction enables the network to learn physically admissible solutions through repeated exposure to the solver's action, without requiring problem-specific loss engineering or explicit evaluation of differential equation residuals. By delegating the enforcement of physics, boundary conditions, and numerical stability to established numerical solvers, NewPINNs mitigates several well-known failure modes of standard physics-informed neural networks, including optimization pathologies, sensitivity to loss weighting, and poor performance in stiff or nonlinear regimes. We demonstrate the effectiveness of the proposed approach across multiple forward and inverse problems involving finite volume, finite element, and spectral solvers.

</details>


### [218] [JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers](https://arxiv.org/abs/2601.17215)
*Ruoqing Zheng,Chang Sun,Qibin Liu,Lauri Laatu,Arianna Cox,Benedikt Maier,Alexander Tapper,Jose G. F. Coutinho,Wayne Luk,Zhiqiang Que*

Main category: cs.LG

TL;DR: JetFormer是一种用于LHC粒子喷注标记的Transformer架构，设计用于从离线分析到在线触发的全场景，在保持高性能的同时具有计算效率和硬件部署能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对特定部署场景设计，缺乏一个统一的架构来处理从高精度离线分析到超低延迟在线触发的全谱系喷注标记任务。需要一种既保持高性能又易于硬件部署的解决方案。

Method: 提出JetFormer，一种仅编码器的Transformer架构，处理可变长度的粒子特征集，无需显式成对相互作用输入。采用硬件感知优化流程，包括多目标超参数搜索、结构化剪枝和量化，生成适合FPGA部署的紧凑变体。

Result: 在JetClass数据集上，JetFormer性能与ParT模型相当（差异在0.7%以内），但FLOPs减少37.4%。在HLS4ML 150P基准数据集上，比MLP、Deep Sets和Interaction Networks等现有模型准确率高3-4%。通过压缩技术可大幅减小模型规模而精度损失最小。

Conclusion: JetFormer通过统一高性能建模和部署能力，为LHC离线分析和在线触发系统提供了实用的Transformer喷注标记器部署路径，平衡了性能、效率和硬件兼容性。

Abstract: We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.

</details>


### [219] [Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning](https://arxiv.org/abs/2601.17224)
*Dmitrii Torbunov,Yihui Ren,Lijun Wu,Yimei Zhu*

Main category: cs.LG

TL;DR: 将条件扩散模型逆问题求解器（CDI）从一维时间信号扩展到二维空间数据，应用于材料表征中的会聚束电子衍射参数推断，提供校准良好的后验分布来量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 科学逆问题中的不确定性量化至关重要，需要区分可识别参数和测量数据下仍然模糊的参数。虽然CDI已在一维时间信号上展示了有效的概率推断，但其在高维空间数据上的适用性尚未探索。

Method: 将CDI扩展到二维空间条件化，直接从空间观测进行概率参数推断。在会聚束电子衍射参数推断这一具有挑战性的多参数逆问题上进行验证，该问题需要从二维衍射图案中提取样品几何、电子结构和热学性质。

Result: 使用模拟CBED数据和真实参数，CDI产生了校准良好的后验分布，准确反映了测量约束：对于确定良好的量产生紧密分布，对于模糊参数产生适当宽泛的分布。相比之下，标准回归方法虽然总体指标看似准确，但通过预测训练集均值来掩盖潜在不确定性。

Conclusion: CDI成功从时间域扩展到空间域，为稳健的科学推断提供了真正所需的不确定性信息。

Abstract: Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.

</details>


### [220] [A Constrained Optimization Perspective of Unrolled Transformers](https://arxiv.org/abs/2601.17257)
*Javier Porras-Valenzuela,Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种约束优化框架，训练Transformer使其行为类似于优化下降算法，通过层间下降约束和原始-对偶训练方案，确保中间表示在期望上单调降低损失


<details>
  <summary>Details</summary>
Motivation: 传统Transformer训练使用经验风险最小化（ERM），但缺乏理论保证中间表示的质量。希望让Transformer具有类似优化算法的行为，确保层间表示能单调降低目标函数，从而提升模型的鲁棒性和泛化能力

Method: 提出约束优化框架：1）施加层间下降约束，要求每层输出在期望上降低损失；2）用原始-对偶训练方案替代标准ERM；3）应用于展开式Transformer架构和预训练Transformer；4）在视频去噪和文本分类任务上验证

Result: 约束Transformer在扰动下表现出更强的鲁棒性，保持更高的分布外泛化能力，同时不损害分布内性能。在视频去噪和文本分类任务上均观察到这些优势

Conclusion: 通过约束优化框架训练Transformer使其具有优化下降算法的行为是有效的，能够提升模型的鲁棒性和泛化能力，为Transformer设计提供了新的理论指导方向

Abstract: We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.

</details>


### [221] [The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment](https://arxiv.org/abs/2601.17260)
*Marco Pollanen*

Main category: cs.LG

TL;DR: DPO中的β参数不是简单的"越大越好"，而是控制参数，不同架构对β变化有不同响应模式，偏好边界可能与推理能力反相关，存在训练路径依赖的滞后效应。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点：DPO中的β参数通常被视为"对齐压力"参数，认为增加β会持续改善模型行为。本文旨在研究β作为控制参数时，不同模型架构在不同β值下的能力变化模式。

Method: 对三个7B开源模型家族（Mistral、Llama、Qwen）在固定DPO配方下，密集扫描β参数空间，分析逻辑探针边界、能力变化模式、偏好边界与推理能力的相关性，以及训练路径的滞后效应。

Result: 1. Mistral能力呈尖锐非单调性：逻辑探针边界仅在β≈10⁻²附近窄带内为正，边界点对种子敏感；2. 不同架构有不同响应模式：Mistral剧烈重组，Llama选择性变化，Qwen平滑权衡；3. DPO偏好边界与推理能力反相关（Llama逻辑任务Pearson r=-0.91）；4. 高β训练导致能力损失，即使降低β也无法恢复（滞后效应）。

Conclusion: 不应依赖偏好边界或聚合基准来选择DPO模型，而应在整个β参数空间进行能力分辨评估，考虑不同架构的响应模式差异和训练路径依赖效应。

Abstract: Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $β$) yields progressively "better" behavior. We instead treat $β$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $β\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $β$ induces capability losses that persist even after $β$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $β$ landscape rather than reliance on margins or aggregate benchmarks.

</details>


### [222] [AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning](https://arxiv.org/abs/2601.17261)
*Wei Lin,Yining Jiang,Qingyu Song,Qiao Xiang,Hong Xu*

Main category: cs.LG

TL;DR: 提出AGZO方法，利用激活结构信息改进零阶优化，在内存受限下提升LLM微调效果


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法使用各向同性扰动，忽略了前向传播中丰富的结构信息，导致优化效率不高

Method: 提出激活引导的零阶优化(AGZO)，利用线性层梯度受限于输入激活子空间的洞察，在前向传播中提取紧凑的激活信息子空间，将扰动限制在该低秩子空间

Result: AGZO在Qwen3和Pangu模型上优于现有零阶基线，显著缩小与一阶微调的性能差距，同时保持与其他零阶方法几乎相同的峰值内存占用

Conclusion: AGZO通过利用激活结构信息改进了零阶优化，在内存受限的LLM微调中实现了更好的性能

Abstract: Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.

</details>


### [223] [Unrolled Neural Networks for Constrained Optimization](https://arxiv.org/abs/2601.17274)
*Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出约束对偶展开(CDU)框架，使用两个耦合神经网络模拟拉格朗日对偶上升算法，解决约束优化问题，在混合整数二次规划和无线网络功率分配中实现接近最优的可行解。


<details>
  <summary>Details</summary>
Motivation: 传统对偶上升算法在约束优化问题中收敛慢，需要手动调参。希望开发可学习的加速版本，通过神经网络模拟优化过程，提高求解效率并增强泛化能力。

Method: 设计两个耦合神经网络：原始网络模拟给定对偶乘子下的拉格朗日平稳点求解；对偶网络生成最优乘子轨迹。通过约束学习强制原始下降和对偶上升的动态，采用交替训练策略更新两个网络。

Result: 在混合整数二次规划和无线网络功率分配问题上，CDU框架能够产生接近最优且接近可行的解，展现出强大的分布外泛化能力。

Conclusion: CDU框架成功将传统对偶上升算法转化为可学习的神经网络架构，实现了约束优化问题的高效求解和良好泛化，为复杂优化问题提供了新的神经网络解决方案。

Abstract: In this paper, we develop unrolled neural networks to solve constrained optimization problems, offering accelerated, learnable counterparts to dual ascent (DA) algorithms. Our framework, termed constrained dual unrolling (CDU), comprises two coupled neural networks that jointly approximate the saddle point of the Lagrangian. The primal network emulates an iterative optimizer that finds a stationary point of the Lagrangian for a given dual multiplier, sampled from an unknown distribution. The dual network generates trajectories towards the optimal multipliers across its layers while querying the primal network at each layer. Departing from standard unrolling, we induce DA dynamics by imposing primal-descent and dual-ascent constraints through constrained learning. We formulate training the two networks as a nested optimization problem and propose an alternating procedure that updates the primal and dual networks in turn, mitigating uncertainty in the multiplier distribution required for primal network training. We numerically evaluate the framework on mixed-integer quadratic programs (MIQPs) and power allocation in wireless networks. In both cases, our approach yields near-optimal near-feasible solutions and exhibits strong out-of-distribution (OOD) generalization.

</details>


### [224] [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)
*Lianlei Shan,Han Chen,Yixuan Wang,Zhenjie Liu,Wei Li*

Main category: cs.LG

TL;DR: DLR提出了一种潜在空间双向对比强化学习框架，将推理成本从昂贵的token级序列生成转移到连续潜在流形，通过冻结主模型参数避免灾难性遗忘，实现更稳定训练和更长推理链支持。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂多步推理任务时往往只是"统计拟合"而非系统逻辑推理。传统强化学习虽然引入"三思而后言"范式，但在高维离散token空间面临三大挑战：样本效率低的rollout、高梯度估计方差、灾难性遗忘风险。

Method: 提出DeepLatent Reasoning (DLR)框架：1) 使用轻量级辅助模型在潜在空间高效采样K个推理链编码；2) 通过基于正确性和格式的双重奖励机制筛选高价值潜在轨迹；3) 仅将高质量潜在轨迹输入冻结主模型进行单次解码；4) 设计对比学习目标在潜在空间实现定向探索。

Result: 在可比GPU计算预算下，DLR实现了更稳定的训练收敛，支持更长推理链，促进推理能力的可持续积累，为LLMs提供可靠且可扩展的强化学习路径。

Conclusion: DLR通过将强化学习从离散token空间转移到连续潜在空间，从根本上解决了传统RL在LLMs推理中的结构瓶颈，同时通过冻结主模型参数消除了灾难性遗忘，为LLMs的可靠强化学习提供了可行方案。

Abstract: While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.

</details>


### [225] [Tabular Foundation Models are Strong Graph Anomaly Detectors](https://arxiv.org/abs/2601.17301)
*Yunhui Liu,Tieke He,Yongchao Liu,Can Yi,Hong Jin,Chuntao Hong*

Main category: cs.LG

TL;DR: TFM4GAD是一个将表格基础模型应用于图异常检测的框架，通过将图结构"扁平化"为增强特征表格，利用TFM的合成预训练和上下文学习能力，实现跨域异常检测而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法遵循"一个模型对应一个数据集"的模式，导致计算成本高、数据需求大、泛化能力差。需要一种能够跨多样图检测异常而无需重新训练的基础模型解决方案。

Method: 将图"扁平化"为增强特征表格：通过拉普拉斯嵌入、局部和全局结构特征、异常敏感邻域聚合来丰富原始节点特征。然后使用表格基础模型在完全上下文学习机制下处理这个增强表格。

Result: 在多个数据集和各种TFM骨干网络上的广泛实验表明，TFM4GAD显著优于从头训练的专业GAD模型，实现了性能提升。

Conclusion: TFM4GAD为利用TFM作为强大、通用的图异常检测器提供了新视角和实用范式，展示了TFM在解决图异常检测核心挑战方面的潜力。

Abstract: Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a "one model per dataset" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a "one-for-all" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by "flattening" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.

</details>


### [226] [Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach](https://arxiv.org/abs/2601.17303)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 提出了一种用于工业物联网的分布式多智能体群架构，通过边缘网关的自主AI代理实现分布式数字免疫系统，显著提升安全响应速度和检测精度。


<details>
  <summary>Details</summary>
Motivation: 工业物联网环境扩展到数万台设备时，集中式安全监控架构存在严重延迟问题，攻击者可利用此漏洞破坏整个制造生态系统。

Method: 设计了去中心化多智能体群架构，在每个边缘网关部署自主AI代理，通过轻量级P2P协议协作检测异常行为，采用共识式威胁验证流程让代理投票确定威胁级别。

Result: 在模拟2000个IIoT设备的创新工厂测试中，DMAS实现亚毫秒级响应时间（平均0.85ms），高负载下恶意活动检测准确率达97.3%，零日攻击检测准确率87%，网络带宽使用比云方案减少89%。

Conclusion: DMAS架构显著优于集中式和边缘计算基线，能防止工业控制系统的实时级联故障，为大规模IIoT环境提供高效安全解决方案。

Abstract: As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital "immune system" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.

</details>


### [227] [Weighted Graph Clustering via Scale Contraction and Graph Structure Learning](https://arxiv.org/abs/2601.17307)
*Haobing Liu,Yinuo Zhang,Tingting Wang,Ruobing Jiang,Yanwei Yu*

Main category: cs.LG

TL;DR: 提出一种收缩边权感知图聚类网络，通过图收缩模块减小图规模并保留重要节点，使用边权感知注意力网络识别和削弱噪声连接，从而提高聚类效果并减少训练时间和存储空间。


<details>
  <summary>Details</summary>
Motivation: 现有图聚类方法未能充分利用边权重信息，而利用边权重面临两个关键挑战：(1) 边权重会显著增加存储空间和训练时间，需要减小图规模同时保留对聚类有益的节点；(2) 边权重信息可能包含噪声，对聚类结果产生负面影响。现有研究很少能同时优化聚类和边权重。

Method: 提出收缩边权感知图聚类网络，包含两个核心模块：1) 面向聚类的图收缩模块，用于减小图规模同时保留重要节点；2) 边权感知注意力网络，用于识别和削弱噪声连接。通过这种方式在聚类过程中更容易识别和减轻噪声边的影响。

Result: 在三个真实世界加权图数据集上进行了广泛实验，模型表现优于最佳基线方法，展示了优越性能。实验还表明提出的图收缩模块能显著减少训练时间和存储空间。

Conclusion: 提出的方法能有效解决边权重利用中的挑战，通过图收缩和噪声边处理提高了聚类效果，同时减少了计算资源需求，为加权图聚类提供了有效的解决方案。

Abstract: Graph clustering aims to partition nodes into distinct clusters based on their similarity, thereby revealing relationships among nodes. Nevertheless, most existing methods do not fully utilize these edge weights. Leveraging edge weights in graph clustering tasks faces two critical challenges. (1) The introduction of edge weights may significantly increase storage space and training time, making it essential to reduce the graph scale while preserving nodes that are beneficial for the clustering task. (2) Edge weight information may inherently contain noise that negatively impacts clustering results. However, few studies can jointly optimize clustering and edge weights, which is crucial for mitigating the negative impact of noisy edges on clustering task. To address these challenges, we propose a contractile edge-weight-aware graph clustering network. Specifically, a cluster-oriented graph contraction module is designed to reduce the graph scale while preserving important nodes. An edge-weight-aware attention network is designed to identify and weaken noisy connections. In this way, we can more easily identify and mitigate the impact of noisy edges during the clustering process, thus enhancing clustering effectiveness. We conducted extensive experiments on three real-world weighted graph datasets. In particular, our model outperforms the best baseline, demonstrating its superior performance. Furthermore, experiments also show that the proposed graph contraction module can significantly reduce training time and storage space.

</details>


### [228] [PAR: Plausibility-aware Amortized Recourse Generation](https://arxiv.org/abs/2601.17309)
*Anagha Sabu,Vidhya S,Narayanan C Krishnan*

Main category: cs.LG

TL;DR: PAR：一种基于摊销近似推理的算法追索方法，通过约束最大后验推断生成高似然、现实可行的反事实解释


<details>
  <summary>Details</summary>
Motivation: 算法追索旨在为不利模型决策提供可操作的建议，但现有方法在生成高似然且现实可行的反事实解释方面存在不足，需要更高效的推理方法

Method: 将追索问题形式化为约束最大后验推断问题，提出PAR摊销近似推理方法，使用可处理概率模型直接估计似然，通过梯度传播训练生成器，并引入基于邻域的调节机制

Result: PAR在广泛使用的算法追索数据集上验证有效，能够高效生成有效、与事实相似、稀疏且高度合理的追索建议，性能优于现有最先进方法

Conclusion: PAR通过概率建模和摊销推理框架，成功解决了算法追索中生成高似然反事实解释的挑战，为实际应用提供了高效可靠的解决方案

Abstract: Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.

</details>


### [229] [Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment](https://arxiv.org/abs/2601.17329)
*Tiejin Chen,Xiaoou Liu,Vishnu Nandam,Kuan-Ru Liou,Hua Wei*

Main category: cs.LG

TL;DR: 提出Conformal Feedback Alignment (CFA)框架，利用Conformal Prediction的统计保证来量化答案可靠性，为DPO和PPO训练提供原则性权重，提升对齐的鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 基于偏好的对齐方法（如RLHF）从成对偏好中学习，但标签通常存在噪声和不一致。现有不确定性感知方法仅对偏好进行加权，而忽略了更基本的因素：被比较答案的可靠性。

Method: 提出Conformal Feedback Alignment (CFA)框架，利用Conformal Prediction (CP)构建具有可控覆盖率的预测集来量化答案级可靠性，并将这些可靠性聚合为DPO和PPO风格训练的原则性权重。

Result: 在不同数据集上的实验表明，CFA提高了对齐的鲁棒性和数据效率，证明建模答案侧不确定性可以补充偏好级加权，实现更鲁棒、数据高效的对齐。

Conclusion: CFA通过Conformal Prediction的统计保证将答案可靠性纳入偏好对齐，为DPO和PPO训练提供原则性权重，显著提升对齐的鲁棒性和数据效率，代码已开源。

Abstract: Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.

</details>


### [230] [Thermodynamically Optimal Regularization under Information-Geometric Constraints](https://arxiv.org/abs/2601.17330)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架，将热力学最优性、信息几何和正则化联系起来，证明在特定假设下，Fisher-Rao度量是信念空间上唯一可接受的几何结构，热力学最优正则化对应于最小化到参考状态的Fisher-Rao距离平方。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习依赖于一系列经验上成功但理论上异质的正则化技术（如权重衰减、dropout、指数移动平均），同时训练大型模型能耗急剧增加，引发了对学习算法是否接近基本效率界限的疑问。

Method: 提出一个统一的理论框架，基于三个明确假设：(A1) 最优性需要内在的、参数化不变的信息度量；(A2) 信念状态由已知约束下的最大熵分布建模；(A3) 最优过程是准静态的。在此框架下证明条件最优性定理，推导高斯和圆形信念模型的诱导几何结构。

Result: 证明Fisher-Rao度量是信念空间上唯一可接受的几何结构，热力学最优正则化对应于最小化到参考状态的Fisher-Rao距离平方。推导出高斯信念模型对应双曲流形，圆形信念模型对应von Mises流形，并表明经典正则化方案在结构上无法保证热力学最优性。

Conclusion: 该工作为机器学习中的正则化提供了原则性的几何和热力学基础，引入了学习的热力学效率概念，并提出了可实验验证的预测。

Abstract: Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.
  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.
  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.

</details>


### [231] [Power-based Partial Attention: Bridging Linear-Complexity and Full Attention](https://arxiv.org/abs/2601.17334)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 论文提出了一种幂基部分注意力机制（PPA），其复杂度为O(L^{1+p})，其中0≤p≤1，通过调节p值可以探索从线性复杂度到二次复杂度注意力之间的性能变化。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer研究中普遍认为"注意力就是一切"，但从未系统性地量化到底需要多少注意力。需要探究二次复杂度O(L^2)的注意力是否必要，是否存在次二次复杂度的注意力机制能达到可比性能。

Method: 引入幂基部分注意力机制（PPA），其复杂度为O(L^{1+p})，其中p=0对应线性复杂度的滑动窗口注意力，p=1对应完全注意力。通过调节p值可以系统研究注意力缩放行为对Transformer性能的影响。

Result: 实验显示性能呈现S曲线行为：在p值的狭窄窗口内，性能从滑动窗口注意力过渡到完全注意力，当p接近1时性能趋于平稳。存在0<p<1使得O(L^{1+p})注意力足以达到与O(L^2)完全注意力相似的结果。

Conclusion: 二次复杂度的注意力并非必要，存在次二次复杂度的注意力机制（O(L^{1+p})，其中0<p<1）能够达到与完全注意力相当的性能，这为设计更高效的Transformer架构提供了理论依据。

Abstract: It is widely accepted from transformer research that "attention is all we need", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.

</details>


### [232] [Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory](https://arxiv.org/abs/2601.17357)
*Davide Ettori*

Main category: cs.LG

TL;DR: 该论文提出基于谱几何和随机矩阵理论的统一框架，通过分析隐藏激活的特征值结构来解决大语言模型和深度神经网络的可靠性问题和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和深度神经网络虽然性能强大，但存在可靠性问题（如幻觉）和高计算成本问题。需要一种统一的理论框架来同时解决这两个问题。

Method: 基于谱几何和随机矩阵理论，分析隐藏激活的特征值结构。提出两种方法：1) EigenTrack - 利用谱特征及其时间动态实时检测幻觉和分布外行为；2) RMT-KD - 识别信息性谱分量，应用迭代知识蒸馏进行模型压缩。

Result: 谱统计量提供了可解释且鲁棒的信号，可用于监控大尺度神经网络的不确定性和指导模型压缩，同时保持准确性。

Conclusion: 谱统计量是解决大语言模型和深度神经网络可靠性问题和计算成本问题的有效工具，为模型监控和压缩提供了统一的理论框架。

Abstract: Large language models and deep neural networks achieve strong performance but suffer from reliability issues and high computational cost. This thesis proposes a unified framework based on spectral geometry and random matrix theory to address both problems by analyzing the eigenvalue structure of hidden activations. The first contribution, EigenTrack, is a real-time method for detecting hallucinations and out-of-distribution behavior in language and vision-language models using spectral features and their temporal dynamics. The second contribution, RMT-KD, is a principled compression method that identifies informative spectral components and applies iterative knowledge distillation to produce compact and efficient models while preserving accuracy. Together, these results show that spectral statistics provide interpretable and robust signals for monitoring uncertainty and guiding compression in large-scale neural networks.

</details>


### [233] [Robust Privacy: Inference-Time Privacy through Certified Robustness](https://arxiv.org/abs/2601.17360)
*Jiankai Jin,Xiangzheng Zhang,Zhao Liu,Deyue Zhang,Quanchen Zou*

Main category: cs.LG

TL;DR: 论文提出Robust Privacy (RP)概念，通过确保模型预测在输入邻域内不变来提供推理时隐私保护，并开发APE方法将输入级不变性转化为属性级隐私效果。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统可能产生个性化输出，使攻击者能够在推理时推断敏感输入属性。现有隐私保护方法在推理时保护不足，需要一种新的隐私概念来防止模型反转攻击等隐私泄露。

Method: 提出Robust Privacy (RP)概念，受认证鲁棒性启发：如果模型预测在输入x的半径R邻域内可证明不变，则x享有R-Robust Privacy。开发Attribute Privacy Enhancement (APE)方法，将输入级不变性转化为属性级隐私效果。在推荐任务中应用RP，通过添加噪声实现预测不变性。

Result: 在受控推荐任务中，RP扩展了与正推荐兼容的敏感属性值集合，相应扩大了推理区间。实验表明，即使在低噪声水平(σ=0.1)下，RP将模型反转攻击成功率从73%降至4%，同时部分模型性能下降。RP也可在不降低模型性能的情况下部分缓解攻击(攻击成功率降至44%)。

Conclusion: Robust Privacy提供了一种有效的推理时隐私保护框架，能够显著降低模型反转攻击成功率，同时保持模型性能。该方法将输入级不变性转化为属性级隐私保护，为机器学习系统的隐私保护提供了新思路。

Abstract: Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($σ=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.

</details>


### [234] [Diversified Scaling Inference in Time Series Foundation Models](https://arxiv.org/abs/2601.17376)
*Ruijin Hua,Zichuan Liu,Kun Zhang,Yiyuan Yang*

Main category: cs.LG

TL;DR: 该论文系统研究了时间序列基础模型在推理时的计算潜力，发现标准采样方法存在探索不足的问题，提出通过多样化推理扩展来提升性能，并理论分析了多样性-保真度权衡。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型主要依赖大规模预训练，但推理时的计算潜力尚未充分挖掘。研究者希望探索两个问题：TSFMs在标准采样推理扩展下的行为表现，以及受控采样多样性是否能提升性能。

Method: 首先分析TSFMs在标准采样下的特性，发现其因解决方案空间探索不足而难以遵循扩展定律。然后通过定制化时间序列扰动进行多样化推理扩展，扩大生成分布的支持范围。理论分析多样性-保真度权衡，推导出多样化采样优于标准采样的关键样本阈值。

Result: 在多种TSFMs和数据集上的广泛实验表明，适当的多样化推理扩展能在不更新参数的情况下带来显著的性能提升，确立了推理设计作为TSFM优化的关键计算高效维度。提出了RobustMSE指标来量化固定预算下TSFM的性能上限。

Conclusion: 该研究阐明了这些因素的相互作用，使得在不重新训练TSFMs的情况下，通过并行环境中的多样化大规模推理时间序列实现可靠性能成为可能，为TSFM的推理优化提供了重要见解。

Abstract: The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.

</details>


### [235] [GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems](https://arxiv.org/abs/2601.17396)
*Vashista Nobaub*

Main category: cs.LG

TL;DR: GO-OSC是一个几何感知的振荡时间序列表示学习框架，通过规范化的潜在参数化实现早期退化检测，相比传统能量基方法具有更高灵敏度。


<details>
  <summary>Details</summary>
Motivation: 振荡系统的早期退化通常表现为动力学的几何畸变（如相位抖动、频率漂移），在信号能量变化可检测之前就已出现。传统的能量基诊断和无约束学习表示对此不敏感，导致检测延迟或不稳定。

Method: 提出GO-OSC框架：1）强制规范且可识别的潜在参数化，实现跨短、无标签窗口的稳定比较和聚合；2）定义一系列不变线性几何探针，针对潜在空间中退化相关的方向；3）提供理论分析证明几何探针在早期相位退化下的检测优势。

Result: 理论证明：在早期仅相位退化情况下，能量基统计量的检测能力为零阶，而几何探针具有严格正灵敏度。实验验证：在合成基准和真实振动数据集上，GO-OSC实现更早检测、更高数据效率和更强的运行条件变化鲁棒性。

Conclusion: GO-OSC通过几何感知表示学习和规范化参数化，解决了振荡系统早期退化检测的挑战，相比传统方法在检测时间、数据效率和鲁棒性方面均有显著提升。

Abstract: Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.

</details>


### [236] [Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations](https://arxiv.org/abs/2601.17407)
*Prajwal Chauhan,Salah Eddine Choutri,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: D-SENO是一个轻量级的神经算子框架，通过结合扩张卷积和挤压-激励模块，在保持高精度的同时实现快速训练和部署，比传统方法快约20倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型和神经算子参数量大，导致训练成本高、部署缓慢，需要开发轻量级且高效的PDE求解器。

Method: 提出D-SENO框架，结合扩张卷积块（捕捉宽感受野和长程物理依赖）和挤压-激励模块（通道注意力机制，自适应重新校准特征通道）。

Result: 在多种PDE基准测试中（翼型势流、达西流、泊肃叶流、不可压缩纳维-斯托克斯涡场），训练速度比标准Transformer模型和神经算子快约20倍，同时精度相当或更高。

Conclusion: D-SENO为物理驱动PDE提供了高效准确的轻量级求解方案，消融实验表明SE模块对性能有重要贡献。

Abstract: Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.

</details>


### [237] [Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection](https://arxiv.org/abs/2601.17430)
*Zichuan Yang,Yiming Xing*

Main category: cs.LG

TL;DR: 提出ECC-AHT算法，用于在相关噪声环境下识别异常流子集，通过最大化Chernoff信息实现主动噪声消除，达到最优样本复杂度


<details>
  <summary>Details</summary>
Motivation: 网络物理系统中的监控和安全应用需要识别相关噪声下的异常流子集。现有组合多臂赌博机和假设检验方法通常假设观测独立，未能利用相关性进行高效测量设计

Method: 提出ECC-AHT自适应算法，选择连续约束测量以最大化竞争假设间的Chernoff信息，通过差分感知实现主动噪声消除

Result: ECC-AHT达到最优样本复杂度保证，在合成和真实世界相关环境中显著优于现有基线方法

Conclusion: ECC-AHT算法有效解决了相关噪声下的异常流识别问题，通过利用相关性实现高效测量设计，在监控和安全应用中具有重要价值

Abstract: We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on https://github.com/VincentdeCristo/ECC-AHT

</details>


### [238] [Data-driven Clustering and Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2601.17441)
*Ondrej Bohdal,Taha Ceritli,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: 提出D2C方法，通过少量任务示例进行适配器聚类，将多个任务适配器合并为多任务适配器，以在存储受限的设备上部署


<details>
  <summary>Details</summary>
Motivation: 移动设备存储空间有限，无法存储所有任务适配器，需要选择代表性适配器以泛化到多个任务，但现有文献未探索此问题

Method: D2C适配器聚类方法，利用少量任务特定示例（如每个任务10个），采用迭代优化过程细化聚类分配，将每个簇内的适配器合并为多任务适配器

Result: 实验结果表明，该方法在考虑存储预算的情况下有效提升了性能

Conclusion: D2C方法解决了移动设备上适配器选择的挑战，通过聚类和合并创建了适用于资源受限设备的多任务适配器

Abstract: On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.

</details>


### [239] [DREAM: Dual-Standard Semantic Homogeneity with Dynamic Optimization for Graph Learning with Label Noise](https://arxiv.org/abs/2601.17449)
*Yusheng Zhao,Jiaye Xie,Qixin Zhang,Weizhi Zhang,Xiao Luo,Zhiping Xiao,Philip S. Yu,Ming Zhang*

Main category: cs.LG

TL;DR: 提出DREAM方法，通过关系感知的动态优化框架处理图标签噪声问题，使用双标准锚点选择和语义同质性度量来指导优化。


<details>
  <summary>Details</summary>
Motivation: 现实世界图数据中的标签可靠性无法保证，现有方法难以区分可靠与不可靠节点，且忽略了图拓扑中的关系信息。

Method: 提出DREAM方法：1) 关系感知的动态优化框架，迭代评估节点可靠性；2) 基于节点邻近性和图拓扑的双标准锚点选择策略；3) 计算目标节点与锚节点的语义同质性指导优化。

Result: 在六个图数据集上，针对三种图标签噪声类型进行实验，相比基线方法表现出显著有效性。

Conclusion: DREAM方法通过关系感知的动态优化和双标准语义同质性度量，有效解决了图标签噪声问题，具有理论保证和实际效果。

Abstract: Graph neural networks (GNNs) have been widely used in various graph machine learning scenarios. Existing literature primarily assumes well-annotated training graphs, while the reliability of labels is not guaranteed in real-world scenarios. Recently, efforts have been made to address the problem of graph learning with label noise. However, existing methods often (i) struggle to distinguish between reliable and unreliable nodes, and (ii) overlook the relational information embedded in the graph topology. To tackle this problem, this paper proposes a novel method, Dual-Standard Semantic Homogeneity with Dynamic Optimization (DREAM), for reliable, relation-informed optimization on graphs with label noise. Specifically, we design a relation-informed dynamic optimization framework that iteratively reevaluates the reliability of each labeled node in the graph during the optimization process according to the relation of the target node and other nodes. To measure this relation comprehensively, we propose a dual-standard selection strategy that selects a set of anchor nodes based on both node proximity and graph topology. Subsequently, we compute the semantic homogeneity between the target node and the anchor nodes, which serves as guidance for optimization. We also provide a rigorous theoretical analysis to justify the design of DREAM. Extensive experiments are performed on six graph datasets across various domains under three types of graph label noise against competing baselines, and the results demonstrate the effectiveness of the proposed DREAM.

</details>


### [240] [Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping](https://arxiv.org/abs/2601.17467)
*Jianxiong Zhang,Bing Guo,Yuming Jiang,Haobo Wang,Bo An,Xuefeng Du*

Main category: cs.LG

TL;DR: 本文提出ARS方法，通过生成反事实答案并学习答案一致性表示，来检测大型推理模型中的幻觉问题，无需人工标注即可提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）经常生成看似连贯的推理轨迹但仍产生错误答案，使得幻觉检测变得困难。现有的基于轨迹文本或隐藏状态的方法容易过拟合到表面模式而非答案有效性。

Method: 提出答案一致性表示塑形（ARS）方法：通过小规模潜在干预生成反事实答案，具体通过扰动轨迹边界嵌入，标记每个扰动是否与原答案一致，然后学习将答案一致的表示聚拢、不一致的分离，从而暴露幻觉风险的潜在不稳定性。

Result: 实验表明ARS能持续改进检测效果，相比强基线方法取得显著提升。塑形后的嵌入可与现有基于嵌入的检测器即插即用，且训练过程中无需人工标注。

Conclusion: ARS通过显式编码答案稳定性来学习检测友好的轨迹条件表示，有效解决了大型推理模型中幻觉检测的挑战，提供了一种无需人工标注的实用解决方案。

Abstract: Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.

</details>


### [241] [Identifying and Correcting Label Noise for Robust GNNs via Influence Contradiction](https://arxiv.org/abs/2601.17469)
*Wei Ju,Wei Zhang,Siyu Yi,Zhengyang Mao,Yifan Wang,Jingyang Yuan,Zhiping Xiao,Ziyue Qiao,Ming Zhang*

Main category: cs.LG

TL;DR: 提出ICGNN方法，利用图结构信息处理噪声标签问题，通过影响矛盾分数检测噪声节点，结合邻居预测修正标签，并加入伪标签增强监督信号。


<details>
  <summary>Details</summary>
Motivation: 现实场景中图数据的标签噪声（如标注错误或不一致）严重影响了图神经网络（GNNs）的鲁棒性和有效性，需要有效的方法来处理噪声标签问题。

Method: 1. 设计基于图扩散矩阵的影响矛盾分数（ICS）作为噪声指示器，量化节点标签的可信度；2. 使用高斯混合模型精确检测噪声标签节点；3. 开发软策略结合邻居节点预测修正检测到的噪声标签；4. 为大量未标记节点加入伪标签提供辅助监督信号。

Result: 在基准数据集上的实验表明，所提出的ICGNN方法具有优越性，能有效处理图数据中的噪声标签问题。

Conclusion: ICGNN通过利用图结构信息、设计噪声检测指标、结合邻居预测修正标签以及加入伪标签监督，有效解决了图神经网络中的噪声标签挑战，提升了模型的鲁棒性。

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in learning from graph-structured data with various applications such as social analysis and bioinformatics. However, the presence of label noise in real scenarios poses a significant challenge in learning robust GNNs, and their effectiveness can be severely impacted when dealing with noisy labels on graphs, often stemming from annotation errors or inconsistencies. To address this, in this paper we propose a novel approach called ICGNN that harnesses the structure information of the graph to effectively alleviate the challenges posed by noisy labels. Specifically, we first design a novel noise indicator that measures the influence contradiction score (ICS) based on the graph diffusion matrix to quantify the credibility of nodes with clean labels, such that nodes with higher ICS values are more likely to be detected as having noisy labels. Then we leverage the Gaussian mixture model to precisely detect whether the label of a node is noisy or not. Additionally, we develop a soft strategy to combine the predictions from neighboring nodes on the graph to correct the detected noisy labels. At last, pseudo-labeling for abundant unlabeled nodes is incorporated to provide auxiliary supervision signals and guide the model optimization. Experiments on benchmark datasets show the superiority of our proposed approach.

</details>


### [242] [LeanTutor: Towards a Verified AI Mathematical Proof Tutor](https://arxiv.org/abs/2601.17473)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.LG

TL;DR: 开发了一个结合LLM和定理证明器的数学证明辅导系统LeanTutor，包含三个模块，并在PeanoBench数据集上评估


<details>
  <summary>Details</summary>
Motivation: LLM虽然支持自然语言交流但容易出错，定理证明器如Lean能保证正确性但学习曲线陡峭，需要结合两者优势开发可靠的数学证明辅导工具

Method: 开发LeanTutor系统，包含三个模块：1) 自动形式化/证明检查器，2) 下一步生成器，3) 自然语言反馈生成器。使用从Natural Numbers Game衍生的PeanoBench数据集（371个Peano算术证明）进行评估

Result: 提出了一个概念验证系统LeanTutor，能够结合LLM的自然语言交互能力和定理证明器的可证明正确性，为数学证明辅导提供可靠支持

Conclusion: 通过结合LLM和定理证明器的互补优势，可以开发出既支持自然语言交互又保证正确性的数学证明辅导系统，为数学教育提供新工具

Abstract: This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.

</details>


### [243] [Unintended Memorization of Sensitive Information in Fine-Tuned Language Models](https://arxiv.org/abs/2601.17480)
*Marton Szep,Jorge Marin Ruiz,Georgios Kaissis,Paulina Seidl,Rüdiger von Eisenhart-Rothe,Florian Hinterwimmer,Daniel Rueckert*

Main category: cs.LG

TL;DR: 该论文系统研究了LLM微调中仅出现在输入而非训练目标中的PII泄露风险，评估了多种隐私保护方法的效果与权衡。


<details>
  <summary>Details</summary>
Motivation: LLM在敏感数据集上微调存在意外记忆和泄露PII的严重风险，特别是那些仅出现在模型输入中而非训练目标中的PII信息，这一问题尚未得到充分研究。

Method: 使用合成和真实数据集设计受控提取探针来量化PII意外记忆，研究语言、PII频率、任务类型和模型规模等因素的影响，并基准测试四种隐私保护方法（差分隐私、机器遗忘、正则化和偏好对齐）。

Result: 研究表明后训练方法通常提供更一致的隐私-效用权衡，而差分隐私在特定设置下能显著减少泄露，但可能引入训练不稳定性。不同因素对PII记忆行为有显著影响。

Conclusion: 微调LLM中的记忆问题仍然是一个持续挑战，需要开发更强大、可扩展的隐私保护技术来解决仅出现在输入中的PII泄露风险。

Abstract: Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.

</details>


### [244] [Automatic Stability and Recovery for Neural Network Training](https://arxiv.org/abs/2601.17483)
*Barak Or*

Main category: cs.LG

TL;DR: 提出一个运行时稳定性框架，通过隔离创新信号来自动检测和恢复训练中的不稳定更新，无需修改底层优化器


<details>
  <summary>Details</summary>
Motivation: 现代神经网络训练越来越脆弱，罕见但严重的破坏性更新会导致不可逆的发散或性能下降。现有优化方法主要依赖优化器内部的预防机制，一旦发生不稳定，检测和恢复能力有限。

Method: 引入监督式运行时稳定性框架，将优化视为受控随机过程。通过隔离来自次级测量（如验证探针）的创新信号，该框架能够自动检测和恢复破坏性更新，无需修改底层优化器。

Result: 提供理论运行时安全保证，形式化有界退化和恢复。实现开销最小，兼容内存受限的训练设置。

Conclusion: 该框架为神经网络训练提供了有效的运行时稳定性监控和恢复机制，解决了现有方法在检测和恢复训练不稳定方面的局限性。

Abstract: Training modern neural networks is increasingly fragile, with rare but severe destabilizing updates often causing irreversible divergence or silent performance degradation. Existing optimization methods primarily rely on preventive mechanisms embedded within the optimizer, offering limited ability to detect and recover from instability once it occurs. We introduce a supervisory runtime stability framework that treats optimization as a controlled stochastic process. By isolating an innovation signal derived from secondary measurements, such as validation probes, the framework enables automatic detection and recovery from destabilizing updates without modifying the underlying optimizer. We provide theoretical runtime safety guarantees that formalize bounded degradation and recovery. Our implementation incurs minimal overhead and is compatible with memory-constrained training settings.

</details>


### [245] [SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving](https://arxiv.org/abs/2601.17489)
*Ashutosh Bajpai,Akshat Bhandari,Akshay Nambi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: SpatialMath框架通过将空间表示融入符号推理链，显著提升多模态小中型语言模型在视觉密集型数学问题（特别是几何问题）上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态小中型语言模型在视觉理解和数学推理方面存在局限，特别是在几何问题上，难以准确分解复杂视觉输入并将感知与结构化推理连接起来。

Method: 提出SpatialMath框架，包含专门感知模块提取视觉图表中的空间基础表示，然后将这些表示系统地融入符号推理链，实现视觉理解感知的结构化推理。同时创建MATHVERSE-PLUS数据集，包含结构化视觉解释和逐步推理路径。

Result: SpatialMath显著优于现有多模态基线模型，在视觉密集型场景下比监督微调加数据增强方法提升高达10个百分点。鲁棒性分析显示增强的空间表示直接提高了推理准确性。

Conclusion: 结构化感知到推理管道对于多模态小中型语言模型至关重要，SpatialMath框架通过整合空间表示和符号推理有效解决了视觉密集型数学问题的挑战。

Abstract: Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.

</details>


### [246] [PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems](https://arxiv.org/abs/2601.17495)
*Ruiyu Zhang,Lin Nie,Wai-Fung Lam,Qihao Wang,Xin Zhao*

Main category: cs.LG

TL;DR: PEARL是一种标签高效的嵌入对齐方法，通过原型增强表示学习来改善局部邻域结构，在标签稀缺条件下显著提升相似性检索性能


<details>
  <summary>Details</summary>
Motivation: 实际部署系统中，基于嵌入的最近邻检索经常失败，原因不是语言模型本身，而是嵌入空间的最近邻对应错误案例。现有方法要么需要大量标签数据（监督投影），要么效果有限（无监督后处理），在标签稀缺、领域漂移、重训练昂贵的情况下需要更好的解决方案

Method: PEARL（原型增强对齐表示学习）使用有限监督将嵌入软对齐到类别原型，重塑局部邻域几何结构，同时保持维度不变，避免激进投影或坍缩。它介于无监督后处理（效果有限）和全监督投影（需要大量标签）之间

Result: 在标签稀缺条件下，PEARL显著改善局部邻域质量：相比原始嵌入提升25.7%，相比强无监督后处理提升21.1%以上，在基于相似性的系统最脆弱的场景中表现优异

Conclusion: PEARL提供了一种标签高效的方法来改善嵌入几何结构，在现实世界标签稀缺、领域漂移、重训练昂贵的部署场景中，能够有效提升基于嵌入的检索和分类系统的性能

Abstract: In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.

</details>


### [247] [One-Shot Federated Clustering of Non-Independent Completely Distributed Data](https://arxiv.org/abs/2601.17512)
*Yiqun Zhang,Shenghong Cai,Zihua Yang,Sen Feng,Yuzhu Ji,Haijun Zhang*

Main category: cs.LG

TL;DR: 提出GOLD框架解决联邦聚类中Non-IID数据问题，通过全局导向的局部分布学习提升聚类性能


<details>
  <summary>Details</summary>
Motivation: 联邦聚类在无标签的分布式IoT数据中应用广泛，但Non-IID问题严重挑战聚类性能，特别是不同客户端可能"碎片化"同一个簇，需要新的解决方案

Method: 提出GOLD框架：1) 精细探索客户端潜在的不完整局部簇分布；2) 上传分布摘要到服务器进行全局融合；3) 在全局分布指导下进行局部簇增强

Result: 通过显著性检验、消融研究、可扩展性评估和定性结果等大量实验，证明了GOLD的优越性

Conclusion: 揭示了Non-ICD现象，提出GOLD框架有效解决联邦聚类中的Non-IID挑战，提升分布式隐私保护系统中的模式知识探索能力

Abstract: Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.

</details>


### [248] [Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment](https://arxiv.org/abs/2601.17563)
*Nathan Gavenski,Matteo Leonetti,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: 提出UfO方法，通过两阶段无监督学习从观察中模仿，无需动作监督，克服现有ILfO方法的局限性


<details>
  <summary>Details</summary>
Motivation: 现有观察模仿学习方法需要动作监督、假设状态有单一最优动作、且不考虑实际环境状态，限制了其应用效果

Method: 两阶段无监督学习：第一阶段从观察状态转移中近似教师真实动作，第二阶段调整智能体轨迹以与教师轨迹对齐

Result: 在五个常用环境中，UfO不仅超越教师和其他ILfO方法，且标准差最小，表明在未见场景中具有更好的泛化能力

Conclusion: UfO成功解决了现有ILfO方法的局限性，实现了无监督的高效模仿学习，具有更好的泛化性能

Abstract: State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.

</details>


### [249] [Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization](https://arxiv.org/abs/2601.17570)
*Hadi Salloum,Ali Jnadi,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.LG

TL;DR: MC+QUBO：将蒙特卡洛强化学习中的轨迹选择建模为QUBO问题，用量子启发式采样器优化选择，提升稀疏奖励环境下的学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛强化学习在稀疏奖励、大状态空间和相关轨迹环境中样本效率低下，需要改进轨迹选择策略来提升学习效果。

Method: 将轨迹选择问题转化为二次无约束二进制优化（QUBO）问题，使用模拟量子退火（SQA）和模拟分叉（SB）作为黑盒求解器，从批量轨迹中选择最大化累积奖励同时促进状态空间覆盖的子集。

Result: 在有限时域GridWorld环境中，MC+QUBO在收敛速度和最终策略质量上都优于传统蒙特卡洛方法。

Conclusion: 量子启发式优化可以作为强化学习中有效的决策子程序，为改进蒙特卡洛强化学习的样本效率提供了新思路。

Abstract: Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.

</details>


### [250] [Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout](https://arxiv.org/abs/2601.17602)
*Xuanzhou Chen*

Main category: cs.LG

TL;DR: 研究Transformer过参数化问题，通过高维编码器-解码器嵌入中的角度相似性分析，使用伯努利dropout识别保持Top-1预测的稀疏性阈值。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型过参数化现象，探索在编码器和解码器之间应用dropout时，模型性能如何随稀疏性变化，寻找保持预测准确性的临界阈值。

Method: 1) 理论分析：证明当嵌入有效稀疏性足够大时，解码器性能在适度坐标dropout下保持稳定；2) 实证方法：构建带有二进制擦除通道(BEC)的新型Transformer模型，在英法翻译任务上测试伯努利dropout效果。

Result: 实验结果显示验证准确率和BLEU分数在某个阈值处急剧下降，可视化趋势表明存在明显的性能转折点，验证了理论预测的稀疏性阈值存在。

Conclusion: Transformer模型在编码器-解码器嵌入中存在稀疏性依赖的临界阈值，超过该阈值Top-1预测得以保持，为理解Transformer过参数化和鲁棒性提供了新视角。

Abstract: We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.

</details>


### [251] [A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs](https://arxiv.org/abs/2601.17607)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 论文提出学习是一个不可逆过程，需要熵产生才能实现认知结构，并推导出认知速度极限(ESL)不等式，为学习过程设定最小熵产生下界。


<details>
  <summary>Details</summary>
Motivation: 传统信息论认为确定性变换不会增加信息，但学习系统却能从数据中获得结构化内部表示。这引发了一个根本问题：学习如何在不超过信息论限制的情况下产生抽象和洞察？

Method: 将学习建模为模型配置概率分布空间中的传输过程，引入认知自由能框架。在该框架中定义自由能下降作为记账量，将其分解为可逆的潜在改进分量和不可逆的熵产生分量。

Result: 推导出认知速度极限(ESL)不等式，为任何学习过程实现给定分布变换所需的最小熵产生设定下界。该下界仅取决于初始和最终集合分布之间的Wasserstein距离，与具体学习算法无关。

Conclusion: 学习本质上是一个不可逆过程，认知结构的实现必然伴随熵产生。认知速度极限为学习过程设定了基本限制，将热力学不可逆性与机器学习理论联系起来。

Abstract: Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?
  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.
  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.
  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.

</details>


### [252] [Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning](https://arxiv.org/abs/2601.17616)
*Fatema Siddika,Md Anwar Hossen,Tanwi Mallick,Ali Jannesari*

Main category: cs.LG

TL;DR: SETA框架通过稀疏专家混合解决LLMs持续学习中的可塑性-稳定性困境，将知识分解为任务特定专家和共享专家，实现任务无关的持续学习


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在持续学习中面临可塑性-稳定性困境：学习新能力会导致灾难性遗忘先前知识。现有方法通常统一处理参数，未能区分特定任务知识和共享能力。

Method: 提出SETA框架：1) 将模型分解为模块化子空间：任务特定专家（隔离任务特定模式）和共享专家（捕获共同特征）；2) 通过弹性权重锚定保护关键共享知识；3) 统一门控网络在推理时自动检索正确的专家组合。

Result: 在多个领域特定和通用基准测试上的广泛实验表明，SETA在参数高效微调基线的持续学习方法中始终优于最先进方法。

Conclusion: SETA通过将知识分解为模块化专家子空间，有效解决了LLMs持续学习中的可塑性-稳定性困境，实现了任务无关的持续学习，显著优于现有方法。

Abstract: Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowledge and shared capabilities. We introduce Mixture of Sparse Experts for Task-Agnostic Continual Learning, referred to as SETA, a framework that resolves the plasticity-stability conflict by decomposing the model into modular subspaces. Unlike standard updates, where tasks compete for the same parameters, SETA separates knowledge into unique experts, designed to isolate task-specific patterns, and shared experts, responsible for capturing common features. This structure is maintained through elastic weight anchoring, which protects critical shared knowledge and enables a unified gating network to automatically retrieve the correct expert combination for each task during inference. Extensive experiments across diverse domain-specific and general benchmarks demonstrate that SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.

</details>


### [253] [BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation](https://arxiv.org/abs/2601.17625)
*Yuhan Xie,Jinhan Liu,Xiaoyong Ni,Fei Tan,Icare Sakr,Thibault Collin,Shiqi Sun,Alejandro Rodriguez Guajardo,Demon Fanny,Charles-francois Vincent Latchoumane,Henri Lorach,Jocelyne Bloch,Gregoire Courtine,Mahsa Shoaran*

Main category: cs.LG

TL;DR: BrainDistill：一种用于植入式脑机接口的新型运动解码框架，通过任务特定知识蒸馏和量化感知训练，在保持高性能的同时大幅降低计算需求


<details>
  <summary>Details</summary>
Motivation: Transformer等大型神经网络解码器在脑机接口任务中表现出色，但参数量大、计算需求高，难以部署在功率受限的植入式系统中。需要开发既能保持高性能又能满足植入式系统严格功率约束的解码方案。

Method: 提出BrainDistill框架，包含植入式神经解码器（IND）和任务特定知识蒸馏（TSKD）框架。TSKD通过监督投影优先保留对解码至关重要的特征，不同于标准特征蒸馏方法。还提出量化感知训练方案，支持仅整数推理，并学习激活裁剪范围。

Result: IND在多个神经数据集上优于现有神经解码器。TSKD蒸馏变体在少样本校准设置中超越其他蒸馏方法。量化后的IND在严格功率约束下部署时性能损失最小。

Conclusion: BrainDistill通过任务特定知识蒸馏和量化感知训练，成功解决了大型神经网络解码器在植入式脑机接口中的部署难题，在保持高性能的同时满足功率约束，为实际临床应用提供了可行方案。

Abstract: Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.

</details>


### [254] [RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding](https://arxiv.org/abs/2601.17641)
*Hao Fang,Ryan A. Canfield,Tomohiro Ouchi,Beatrice Macagno,Eli Shlizerman,Amy L. Orsborn*

Main category: cs.LG

TL;DR: RPNT是一种鲁棒的预训练神经Transformer模型，通过多维旋转位置嵌入、基于上下文的注意力机制和鲁棒自监督学习目标，实现了跨会话、跨类型、跨被试和跨脑区的神经解码泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前脑解码模型在应对不同脑区记录、不同会话、不同行为类型和不同被试的变异性时泛化能力有限，需要开发能够适应和泛化的预训练神经Transformer模型。

Method: 提出RPNT模型，包含：1）多维旋转位置嵌入（MRoPE）聚合实验元数据；2）基于卷积核的上下文注意力机制处理神经群体活动的非平稳性；3）具有均匀因果掩码策略和对比表示的鲁棒自监督学习目标。在两个不同数据集上进行预训练。

Result: 在跨会话、跨类型、跨被试和跨脑区的下游行为解码任务中，RPNT在所有任务中都一致达到并超越了现有解码模型的性能。

Conclusion: RPNT通过创新的架构设计和预训练策略，实现了在多样化神经记录条件下的鲁棒泛化能力，为脑解码提供了有效的预训练Transformer解决方案。

Abstract: Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.

</details>


### [255] [Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics](https://arxiv.org/abs/2601.17647)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出KGCM-VAE模型，通过知识引导的因果建模量化海冰厚度与海面高度之间的因果关系，结合物理约束和分布平衡机制提升处理效果估计精度。


<details>
  <summary>Details</summary>
Motivation: 量化冰融化和淡水分布之间的因果关系对理解极地气候变化和全球海平面上升至关重要。传统深度学习方法在时空设置中处理效果估计不可靠，存在未观测混杂变量和缺乏物理约束的问题。

Method: 提出知识引导的因果模型变分自编码器（KGCM-VAE），包含：1）速度调制方案，通过SSH转换控制的sigmoid函数动态放大平滑速度信号生成物理基础的因果处理；2）最大均值差异（MMD）平衡潜在空间中处理和对照协变量分布；3）因果邻接约束解码器确保与已知物理结构对齐。

Result: 在合成和真实北极数据集上的实验表明，KGCM-VAE在PEHE指标上优于最先进的基准方法。消融研究证实了方法的有效性，MMD和因果邻接约束的联合应用使估计误差降低了1.88%。

Conclusion: KGCM-VAE通过整合物理知识和因果约束，有效解决了时空因果推断中的挑战，为理解海冰厚度与海面高度之间的因果关系提供了可靠工具。

Abstract: Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\% reduction in estimation error.

</details>


### [256] [Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training](https://arxiv.org/abs/2601.17654)
*Ruofan Wu,Jae-Won Chung,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Kareus是一个AI训练系统，通过联合优化细粒度内核调度和频率缩放，在时间和能耗之间实现更好的权衡，相比现有方法可减少最多28.3%的能耗或缩短最多27.5%的训练时间。


<details>
  <summary>Details</summary>
Motivation: AI计算需求快速增长，但能源供应跟不上，能源已成为昂贵且竞争激烈的资源。现有的大模型训练优化工作只关注动态或静态能耗的单一方面，而细粒度内核调度和频率缩放会共同影响这两种能耗。

Method: 设计Kareus训练系统，将复杂的联合优化问题分解为局部的、基于分区的子问题，使用多通道多目标优化算法寻找能够推进时间-能耗权衡前沿的执行调度方案。

Result: 相比现有技术，Kareus在相同训练时间下可减少最多28.3%的训练能耗，或在相同能耗下缩短最多27.5%的训练时间。

Conclusion: 通过联合优化内核调度和频率缩放来同时管理动态和静态能耗，Kareus成功推进了时间-能耗权衡前沿，为AI训练提供了更高效的能源管理方案。

Abstract: The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.
  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.

</details>


### [257] [Entropic Risk-Aware Monte Carlo Tree Search](https://arxiv.org/abs/2601.17667)
*Pedro P. Santos,Jacopo Silvestrin,Alberto Sardinha,Francisco S. Melo*

Main category: cs.LG

TL;DR: 提出一种可证明正确的蒙特卡洛树搜索算法，用于求解具有熵风险度量目标的风险感知马尔可夫决策过程，并提供非渐近分析。


<details>
  <summary>Details</summary>
Motivation: 现有的风险感知MDP求解方法缺乏可证明正确的树搜索算法，特别是在熵风险度量目标下，需要开发具有理论保证的MCTS方法。

Method: 提出一种基于置信上界的蒙特卡洛树搜索算法，利用先前工作中引入的熵风险度量MDP的动态规划公式，在树搜索框架中实现风险感知决策。

Result: 算法具有正确性保证（根节点经验ERM收敛到最优ERM）和多项式遗憾集中性，实验表明该方法优于相关基线。

Conclusion: 成功开发了首个可证明正确的风险感知MCTS算法，为具有熵风险度量的MDP提供了有效的树搜索解决方案，具有理论保证和实际性能优势。

Abstract: We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \textit{risk-aware} Markov decision processes (MDPs) with \textit{entropic risk measure} (ERM) objectives. We provide a \textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.

</details>


### [258] [Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction](https://arxiv.org/abs/2601.17668)
*Jang-Hyun Kim,Dongyoon Han,Sangdoo Yun*

Main category: cs.LG

TL;DR: 提出一种基于门控的KV缓存淘汰方法，通过轻量级sink-attention门控模块识别和保留关键KV对，实现高达70%的KV缓存压缩，同时保持接近无损的性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩技术通常在性能下降和计算开销之间需要权衡，这限制了大型语言模型的实际部署效率。

Method: 引入轻量级sink-attention门控模块来识别关键KV对，提出基于前向传播的门训练算法，避免昂贵的反向传播，使用任务无关的重建目标实现强任务泛化。

Result: 在Qwen2.5-1M、Qwen3和Gemma3系列模型上实验表明，该方法能淘汰高达70%的KV缓存，同时保持接近无损的性能，在长上下文理解、代码理解和数学推理等多种任务上表现一致。

Conclusion: 该方法为冻结权重LLM提供了一种高效、通用的KV缓存管理方案，实现了高压缩比和低计算成本的平衡。

Abstract: Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.

</details>


### [259] [$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts](https://arxiv.org/abs/2601.17680)
*Shota Takashiro,Takeshi Kojima,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: ∞-MoE：通过连续空间选择FFN参数，实现无限专家数量，在保持计算效率的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 传统MoE将专家视为完全独立且在离散空间组合，当专家数量增加时难以有效训练每个专家，需要稳定训练同时增加专家数量

Method: 提出∞-MoE，基于每个token采样的连续值选择大型FFN的部分参数，在连续空间中考虑专家，允许无限数量的专家同时保持计算效率

Result: 基于GPT-2 Small的∞-MoE模型（1.29亿活跃参数/1.86亿总参数）达到与3.5亿参数的密集GPT-2 Medium相当的性能；推理时调整采样专家数量可在精度和速度间灵活权衡，比传统MoE精度提升达2.5%

Conclusion: ∞-MoE通过在连续空间中选择FFN参数，解决了传统MoE在增加专家数量时的训练难题，实现了无限专家数量的可能性，在保持计算效率的同时显著提升性能

Abstract: The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.

</details>


### [260] [Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis](https://arxiv.org/abs/2601.17687)
*Hao Li,He Cao,Shenyao Peng,Zijing Liu,Bin Feng,Yu Wang,Zhiyuan Yan,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.LG

TL;DR: ChemCRAFT：基于代理强化学习的化学AI框架，通过将化学推理与知识存储解耦，让小语言模型能够通过沙箱交互实现高性能化学任务，解决大模型隐私风险和小模型幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前化学领域语言模型面临两难：小模型容易产生幻觉且知识有限，大云模型存在隐私风险和高推理成本。需要一种既能保护隐私又具备高性能的解决方案。

Method: 提出ChemCRAFT框架，利用代理强化学习将化学推理与知识存储解耦。构建化学代理沙箱和代理轨迹构建管道，创建ChemToolDataset数据集，并开发SMILES-GRPO密集化学奖励函数来训练模型调用化学代理的能力。

Result: ChemCRAFT在药物设计的多个方面（分子结构分析、分子优化、合成路径预测）均优于当前云基大语言模型，证明科学推理不是模型规模的涌现能力，而是可学习的工具编排策略。

Conclusion: 该工作建立了成本效益高且保护隐私的AI辅助化学范式，为通过本地可部署代理加速分子发现开辟了新途径。

Abstract: Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.

</details>


### [261] [REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization](https://arxiv.org/abs/2601.17689)
*Shanu Saklani,Tushar M. Athawale,Nairita Pal,David Pugmire,Christopher R. Johnson,Soumya Dutta*

Main category: cs.LG

TL;DR: REV-INR是一种正则化证据隐式神经表示方法，通过单次前向传播同时预测数据值和坐标级不确定性，实现高质量体积重建和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统确定性INR只能预测数值，无法提供模型预测不确定性或数据固有噪声的影响，这可能导致不可靠的数据解释和可视化。由于原始数据可能因体积过大而不可用，从模型预测数据中识别错误结果可能不可行。

Method: 提出REV-INR（正则化证据隐式神经表示），通过学习准确预测数据值以及相关的坐标级数据不确定性和模型不确定性，在推理时仅需单次前向传播。

Result: REV-INR在体积重建质量、数据（偶然）不确定性和模型（认知）不确定性估计方面表现最佳，且推理时间最快。与现有深度不确定性估计方法相比具有优势。

Conclusion: REV-INR能够评估提取的等值面和体积可视化结果的可靠性和可信度，使分析能够完全基于模型预测的数据进行，解决了传统INR缺乏不确定性估计的问题。

Abstract: Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.

</details>


### [262] [FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices](https://arxiv.org/abs/2601.17713)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Yinfeng Cao*

Main category: cs.LG

TL;DR: FedCCA是一种针对物联网数据异构性的联邦学习算法，通过客户端中心化自适应和选择性适配，为每个客户端学习独特模型，提升性能


<details>
  <summary>Details</summary>
Motivation: 物联网中AI模型训练需要保护隐私数据，联邦学习是隐私保护的分布式训练框架，但物联网设备间的数据异构性问题会显著降低模型性能和收敛速度

Method: 提出Client-Centric Adaptation联邦学习(FedCCA)，通过选择性适配最优利用客户端特定知识，为每个客户端学习独特模型。采用动态客户端选择、基于额外客户端特定编码器的自适应聚合，以及基于注意力的全局聚合策略增强多源知识转移

Result: 在多个数据集上的实验表明，FedCCA在解决数据异构性问题方面相比基线方法表现出显著的性能优势

Conclusion: FedCCA通过客户端中心化自适应方法有效缓解了联邦学习中数据异构性的影响，提升了模型性能和收敛速度

Abstract: With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [263] [Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games](https://arxiv.org/abs/2601.17716)
*Daniel M. Pedrozo,Telma W. de L. Soares,Bryan L. M. de Oliveira*

Main category: cs.LG

TL;DR: 论文提出一个多轮对话框架，通过信息增益定量评估LLMs在分层知识图谱环境中通过是/否问题收集信息的效果，发现具备显式推理能力的模型在部分可观察设置下表现更优。


<details>
  <summary>Details</summary>
Motivation: LLMs在许多任务上表现出色，但在解决用户请求模糊性时提出好问题的能力仍有不足。现有基准缺乏基于信息增益的全面评估框架，也很少系统比较使用链式思维推理与不使用该推理的模型。

Method: 提出多轮对话框架，采用三个交互的LLM代理：提问者、回答者和假设空间更新者。在五级分类的地理猜城市游戏环境中实例化该框架，使用基于香农熵的信息增益作为主要指标，评估完全和部分可观察条件下带/不带链式思维推理的多种LLM变体。

Result: 实验表明，在评估的模型中，具备显式推理能力的模型每轮获得更高的信息增益，且以更少步骤达到解决方案，特别是在部分可观察设置下。分析推理轨迹发现，较小模型通过更积极地探索候选问题来弥补有限能力，而较大模型在选择最优查询时表现出更高自信度，生成具有更大潜在信息增益的候选问题。

Conclusion: 该框架为评估LLMs的信息收集能力提供了定量方法，揭示了推理能力在解决模糊性问题中的重要性，为LLM代理的提问策略优化提供了见解。

Abstract: Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.

</details>


### [264] [AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation](https://arxiv.org/abs/2601.17761)
*Dongjie Cheng,Ruifeng Yuan,Yongqi Li,Runyang You,Wenjie Wang,Liqiang Nie,Lei Zhang,Wenjie Li*

Main category: cs.LG

TL;DR: AR-Omni是一个统一的任意到任意多模态模型，采用自回归范式，无需专家解码器，支持文本、图像生成和流式语音生成。


<details>
  <summary>Details</summary>
Motivation: 现实世界的感知和交互本质上是多模态的，但现有系统通常依赖额外专家组件实现多模态生成，限制了统一训练和推理的简洁性。自回归建模在文本领域已被证明是优雅且可扩展的基础，因此希望将其扩展到多模态领域。

Method: 提出AR-Omni模型，采用单一Transformer解码器支持自回归文本和图像生成以及流式语音生成。通过任务感知损失重加权解决模态不平衡问题，通过轻量级token级感知对齐损失提升视觉保真度，通过有限状态解码机制平衡稳定性和创造性。

Result: AR-Omni在三种模态上均实现了强大性能，同时保持实时性，语音生成的实时因子达到0.88。

Conclusion: AR-Omni展示了自回归范式在多模态统一建模中的有效性，为构建简洁、高效的多模态系统提供了新思路。

Abstract: Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.

</details>


### [265] [LLM-42: Enabling Determinism in LLM Inference with Verified Speculation](https://arxiv.org/abs/2601.17768)
*Raja Gond,Aditya K Kamath,Arkaprava Basu,Ramachandran Ramjee,Ashish Panwar*

Main category: cs.LG

TL;DR: LLM-42通过调度方法在LLM推理中实现确定性，利用推测解码思想，通过验证-回滚循环在保持高吞吐量的同时消除非确定性。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的非确定性源于浮点数非结合性与动态批处理及GPU内核的交互，现有方法要么牺牲吞吐量，要么需要重新实现内核并带来固定开销。

Method: 采用基于调度的推测解码方法：先用非确定性快速路径解码，然后通过轻量级验证-回滚循环确保确定性。验证器在固定形状的归约调度下重放候选token，提交保证一致的token，回滚违反确定性的token。

Result: LLM-42能够重用现有内核，仅在需要确定性的流量上产生开销，在保持高吞吐量的同时实现确定性推理。

Conclusion: LLM-42提供了一种实用的调度方法来解决LLM推理中的非确定性问题，平衡了确定性需求与系统性能，无需修改内核设计。

Abstract: In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.
  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.

</details>


### [266] [Shortcut Learning in Binary Classifier Black Boxes: Applications to Voice Anti-Spoofing and Biometrics](https://arxiv.org/abs/2601.17782)
*Md Sahidullah,Hye-jin Shim,Rosa Gonzalez Hautamäki,Tomi H. Kinnunen*

Main category: cs.LG

TL;DR: 提出一个分析黑盒分类器的新框架，通过干预和观察视角，使用线性混合效应模型进行事后分析，评估数据集偏见对分类器性能的影响，并在音频反欺骗和说话人验证任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在数据驱动应用中的广泛采用引起了人们对数据集和模型偏见的潜在风险的关注。被忽视或隐藏的偏见可能导致意外结果，本研究旨在解决数据集偏见问题，并探索二元分类器中的"捷径学习"或"Clever Hans效应"。

Method: 提出一个新颖的分析框架，结合干预和观察视角，采用线性混合效应模型进行事后分析，评估训练和测试数据对分类器分数的影响，超越错误率来评估分类器性能。

Result: 在音频反欺骗和说话人验证任务中，使用统计模型和深度神经网络验证了该方法的有效性，能够提供对偏见数据集的洞察，并全面理解其对分类器行为的影响。

Conclusion: 该研究为处理其他领域的偏见问题提供了更广泛的启示，并推动了可解释人工智能领域的发展，提出的框架能够帮助识别和分析数据集偏见对分类器性能的影响。

Abstract: The widespread adoption of deep-learning models in data-driven applications has drawn attention to the potential risks associated with biased datasets and models. Neglected or hidden biases within datasets and models can lead to unexpected results. This study addresses the challenges of dataset bias and explores ``shortcut learning'' or ``Clever Hans effect'' in binary classifiers. We propose a novel framework for analyzing the black-box classifiers and for examining the impact of both training and test data on classifier scores. Our framework incorporates intervention and observational perspectives, employing a linear mixed-effects model for post-hoc analysis. By evaluating classifier performance beyond error rates, we aim to provide insights into biased datasets and offer a comprehensive understanding of their influence on classifier behavior. The effectiveness of our approach is demonstrated through experiments on audio anti-spoofing and speaker verification tasks using both statistical models and deep neural networks. The insights gained from this study have broader implications for tackling biases in other domains and advancing the field of explainable artificial intelligence.

</details>


### [267] [Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data](https://arxiv.org/abs/2601.17802)
*A. Brawanski,Th. Schaffer,F. Raab,K. -M. Schebesch,M. Schrey,Chr. Doenitz,A. M. Tomé,E. W. Lang*

Main category: cs.LG

TL;DR: 提出一个从常规MRI数据生成非增强高细胞性肿瘤区域概率图的计算框架，通过多种网络架构处理成像变异性和边界模糊问题，并验证其临床相关性。


<details>
  <summary>Details</summary>
Motivation: 神经肿瘤成像中准确识别非增强高细胞性肿瘤区域是一个未满足的需求，对患者管理和治疗规划有重要意义。目前缺乏可靠的成像生物标志物来非侵入性地定位这些区域。

Method: 开发了一个稳健的计算框架，利用多种网络架构从常规MRI数据生成NEH区域概率图，以应对成像变异性和边界模糊的挑战。

Result: 该框架通过相对脑血容量和增强肿瘤复发位置等独立临床标志物验证，证明了方法学稳健性和生物学相关性，能够可靠地非侵入性映射NEH肿瘤区域。

Conclusion: 该框架支持将NEH区域作为成像生物标志物整合到临床工作流程中，推进脑肿瘤患者的精准肿瘤学治疗。

Abstract: Accurate identification of non-enhancing hypercellular (NEH) tumor regions is an unmet need in neuro-oncological imaging, with significant implications for patient management and treatment planning. We present a robust computational framework that generates probability maps of NEH regions from routine MRI data, leveraging multiple network architectures to address the inherent variability and lack of clear imaging boundaries. Our approach was validated against independent clinical markers -- relative cerebral blood volume (rCBV) and enhancing tumor recurrence location (ETRL) -- demonstrating both methodological robustness and biological relevance. This framework enables reliable, non-invasive mapping of NEH tumor compartments, supporting their integration as imaging biomarkers in clinical workflows and advancing precision oncology for brain tumor patients.

</details>


### [268] [MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging](https://arxiv.org/abs/2601.17858)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: MergeMix利用模型合并权重作为低成本性能代理，高效优化数据混合比例，无需全规模训练即可达到或超越人工调优效果。


<details>
  <summary>Details</summary>
Motivation: 当前优化LLM数据混合比例依赖启发式试验或昂贵的代理训练，计算成本过高，需要更高效的方法。

Method: 训练领域专家模型（使用少量token），通过优化这些专家模型的合并权重来优化数据混合比例，将模型合并权重作为高性能低成本代理。

Result: 在8B和16B参数模型上验证，MergeMix性能达到或超越穷举人工调优，显著降低搜索成本，具有高排名一致性（Spearman ρ>0.9）和强跨尺度可迁移性。

Conclusion: MergeMix为数据混合优化提供了可扩展的自动化解决方案，通过模型合并权重作为高效代理，大幅降低了优化成本。

Abstract: Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $ρ> 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.

</details>


### [269] [EEG Foundation Models: Progresses, Benchmarking, and Open Problems](https://arxiv.org/abs/2601.17883)
*Dingkun Liu,Yuheng Chen,Zhu Chen,Zhenyao Cui,Yaozhi Wen,Jiayu An,Jingwei Luo,Dongrui Wu*

Main category: cs.LG

TL;DR: 该论文对现有EEG基础模型进行了首次全面评估，比较了12个开源模型在13个数据集上的表现，发现线性探测通常不足、专用模型仍有竞争力、更大模型不一定带来更好泛化性能。


<details>
  <summary>Details</summary>
Motivation: EEG基础模型在脑机接口领域发展迅速，但由于预训练目标、预处理方法和评估协议不一致，缺乏公平全面的比较。本文旨在填补这一空白，为EEG基础模型研究提供标准化评估框架。

Method: 首先回顾50个代表性模型并构建统一分类框架（数据标准化、模型架构、自监督预训练策略），然后评估12个开源基础模型和竞争性专用基线模型在13个EEG数据集上，涵盖9种BCI范式，采用跨被试和少样本校准两种评估协议。

Result: 结果显示：1）线性探测通常不足以获得最佳性能；2）从头训练的专用模型在许多任务上仍具有竞争力；3）在当前数据规模和训练实践下，更大的基础模型不一定带来更好的泛化性能。

Conclusion: 该研究为EEG基础模型领域提供了首个全面评估基准，揭示了当前模型的局限性，为未来研究指明了方向，强调需要更有效的迁移学习方法和更优化的模型规模设计。

Abstract: Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.

</details>


### [270] [Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization](https://arxiv.org/abs/2601.17910)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 提出一个算子无关的公理化框架，用于多教师知识蒸馏中的自适应权重分配，涵盖token、task和context三个互补尺度，提供理论保证而不依赖具体权重公式。


<details>
  <summary>Details</summary>
Motivation: 当前多教师知识蒸馏方法主要依赖启发式或实现特定的权重方案，缺乏统一的理论框架。需要建立算子无关的公理化框架来支持自适应权重分配的理论分析。

Method: 开发算子无关的公理化框架，形式化自适应权重算子的结构条件，包括良好定义性、多重非等价实现可能性、通过乘积结构归一化的层次组合。在框架内证明符合算子的存在性和非唯一性，分析梯度优化收敛性、稳定性和扰动鲁棒性。

Result: 建立了理论保证与具体权重公式的解耦，使得在异构性、分布偏移和安全约束下能够对自适应蒸馏方法进行原则性分析。框架支持token、task和context三个尺度的自适应权重分配。

Conclusion: 该公理化框架为多教师知识蒸馏的自适应权重分配提供了统一的理论基础，使研究者能够在不同实现方案下获得一致的理论保证，促进了该领域的原则性发展。

Abstract: Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.

</details>


### [271] [Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN](https://arxiv.org/abs/2601.17912)
*Qinyi Liu,Mohammad Khalil,Naman Goel*

Main category: cs.LG

TL;DR: TabPFN等表格数据基础模型通过因果预训练获得高预测精度，但其公平性表现有限，特别是在MNAR协变量偏移下，需要额外的公平性干预措施。


<details>
  <summary>Details</summary>
Motivation: 尽管TabPFN等基于因果推理预训练的表格数据基础模型在预测准确性方面表现优异，但它们的公平性特性尚未得到充分探索。研究旨在评估这些模型在实际应用中的公平性表现。

Method: 对TabPFN及其微调变体进行全面的实证评估，包括预测性能、公平性和鲁棒性测试，考察不同数据集大小和分布偏移（特别是MNAR协变量偏移）下的表现。

Result: TabPFN相比基线模型具有更强的预测准确性和对虚假相关性的鲁棒性，但在公平性方面的改进有限且不一致，特别是在MNAR协变量偏移下表现不佳。

Conclusion: TabPFN的因果预训练有助于预测性能，但不足以确保算法公平性。实际部署需要额外的公平性干预措施，这为未来研究指明了方向。

Abstract: Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.

</details>


### [272] [UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR](https://arxiv.org/abs/2601.17916)
*Jialu Tang,Tong Xia,Yuan Lu,Aaqib Saeed*

Main category: cs.LG

TL;DR: UniPACT是一个统一的多模态临床预后问答框架，通过结构化提示将数值型EHR数据转换为文本，结合原始ECG波形表示，使LLM能够对异构临床数据进行整体推理。


<details>
  <summary>Details</summary>
Motivation: 临床预后需要整合结构化电子健康记录（EHR）和实时生理信号（如心电图ECG），但大型语言模型（LLM）难以原生处理这些异构的非文本数据类型。

Method: 提出UniPACT框架，核心是结构化提示机制将数值EHR数据转换为语义丰富的文本，然后将文本化患者上下文与从原始ECG波形学习的表示融合，使LLM能够对两种模态进行整体推理。

Result: 在MDS-ED基准测试中，UniPACT在诊断、恶化、ICU入院和死亡率等多样化预后任务上实现了89.37%的平均AUROC，优于专门基线，并在缺失数据场景中表现出鲁棒性。

Conclusion: UniPACT通过多模态、多任务方法有效解决了临床数据异构性问题，为LLM在临床预后中的应用提供了统一框架，在多种预后任务中实现了最先进性能。

Abstract: Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.

</details>


### [273] [treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding](https://arxiv.org/abs/2601.17917)
*Zhongyu Xiao,Zhiwei Hao,Jianyuan Guo,Yong Luo,Jia Liu,Jie Xu,Han Hu*

Main category: cs.LG

TL;DR: Streaming-dLLM是一个无需训练的推理加速框架，针对扩散大语言模型(dLLMs)的空间冗余和时间效率问题，通过衰减引导后缀建模和动态置信感知策略，实现了最高68.2倍的加速同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有dLLMs推理加速方法主要关注KV缓存重用或启发式解码，但忽略了块状扩散过程中的内在低效性：空间上对信息稀疏的后缀区域统一建模造成冗余，时间上对所有解码过程使用固定去噪调度导致效率低下。

Method: 提出Streaming-dLLM框架，包含两个核心创新：1) 空间维度：衰减引导后缀建模，通过剪枝冗余掩码标记来近似完整上下文；2) 时间维度：动态置信感知策略配合提前退出机制，允许模型对已收敛标记跳过不必要的迭代。

Result: 实验表明Streaming-dLLM在保持生成质量的同时，实现了最高68.2倍的推理加速，显著提升了扩散解码的效率。

Conclusion: Streaming-dLLM通过同时优化空间和时间维度，有效解决了dLLMs推理中的内在低效问题，为扩散语言模型的实用化部署提供了高效的推理框架。

Abstract: Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.

</details>


### [274] [Dissipative Learning: A Framework for Viable Adaptive Systems](https://arxiv.org/abs/2601.17933)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 论文提出BEDS框架，将学习视为内在耗散过程，证明Fisher-Rao正则化是热力学最优策略，统一了现有方法，重新定义学习为在耗散约束下维持可行信念状态。


<details>
  <summary>Details</summary>
Motivation: 传统学习理论将遗忘和正则化视为启发式附加组件，而本文认为它们是自适应系统的结构性要求。需要从信息理论、热力学和信息几何角度重新理解学习过程。

Method: 提出BEDS（贝叶斯涌现耗散结构）框架，将学习建模为在耗散约束下的压缩信念状态演化。核心贡献是条件最优性定理，证明Fisher-Rao正则化是唯一热力学最优策略。

Result: 证明了Fisher-Rao正则化在热力学意义上是最优的，而欧几里得正则化是结构上次优的。该框架统一了Ridge、SIGReg、EMA、SAC等方法作为单一控制方程的特例。

Conclusion: 学习应被重新定义为在耗散约束下维持可行信念状态的过程。该框架为理解遗忘、正则化和稳定性提供了原则性视角，并自然扩展到持续学习和多智能体系统。

Abstract: We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.
  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.
  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.

</details>


### [275] [FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering](https://arxiv.org/abs/2601.17935)
*Daniel Commey,Matilda Nkoom,Yousef Alsenani,Sena G. Hounsinou,Garth V. Crosby*

Main category: cs.LG

TL;DR: FedGraph-VASP：一种保护隐私的联邦图学习框架，通过边界嵌入交换协议实现跨机构反洗钱检测，在保持数据隐私的同时提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 虚拟资产服务提供商（VASPs）在反洗钱检测中面临监管合规与用户隐私的冲突。现有方法要么需要共享敏感交易数据，要么孤立运行，无法检测跨链洗钱模式。

Method: 提出FedGraph-VASP框架，核心是边界嵌入交换协议：仅共享边界账户的压缩、不可逆图神经网络表示。采用后量子密码学（Kyber-512 + AES-256-GCM）保障安全。

Result: 在Elliptic比特币数据集上，FedGraph-VASP的F1分数达到0.508，比最先进的生成式基线FedSage+（0.453）提升12.1%。在连接度高时接近集中式性能（0.620），在连接度低时比生成式插补更稳健。

Conclusion: FedGraph-VASP在保持隐私的同时有效提升跨机构反洗钱检测性能。存在拓扑依赖的权衡：嵌入交换适用于连接交易图，而生成式插补在高度模块化的稀疏图中表现更优。

Abstract: Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.

</details>


### [276] [Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms](https://arxiv.org/abs/2601.17954)
*Nikos Georgoudios,Konstantinos Spiliopoulos,Justin Sirignano*

Main category: cs.LG

TL;DR: 研究神经Actor-Critic算法在浅层神经网络下的收敛性和统计特性，分析不同网络宽度缩放方案对算法输出的影响，并提供超参数选择指南。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注收敛速度，本文转向更全面的统计特性分析，旨在量化神经Actor-Critic方法的不确定性，为算法提供统计鲁棒性保证。

Method: 研究一般逆多项式缩放方案（指数在1/2到1之间），将网络输出视为统计估计量进行渐近展开分析，推导方差衰减规律，并通过数值实验验证。

Result: 方差随网络宽度以幂律衰减，指数为1/2减去缩放参数，当缩放参数接近1时统计鲁棒性更好；数值实验支持该行为并显示更快收敛。

Conclusion: 分析结果为选择学习率、探索率等超参数提供具体指导，确保算法具有可证明的有利统计行为，特别是当缩放参数接近1时能获得更好的统计鲁棒性。

Abstract: We investigate the neural Actor Critic algorithm using shallow neural networks for both the Actor and Critic models. The focus of this work is twofold: first, to compare the convergence properties of the network outputs under various scaling schemes as the network width and the number of training steps tend to infinity; and second, to provide precise control of the approximation error associated with each scaling regime. Previous work has shown convergence to ordinary differential equations with random initial conditions under inverse square root scaling in the network width. In this work, we shift the focus from convergence speed alone to a more comprehensive statistical characterization of the algorithm's output, with the goal of quantifying uncertainty in neural Actor Critic methods. Specifically, we study a general inverse polynomial scaling in the network width, with an exponent treated as a tunable hyperparameter taking values strictly between one half and one. We derive an asymptotic expansion of the network outputs, interpreted as statistical estimators, in order to clarify their structure. To leading order, we show that the variance decays as a power of the network width, with an exponent equal to one half minus the scaling parameter, implying improved statistical robustness as the scaling parameter approaches one. Numerical experiments support this behavior and further suggest faster convergence for this choice of scaling. Finally, our analysis yields concrete guidelines for selecting algorithmic hyperparameters, including learning rates and exploration rates, as functions of the network width and the scaling parameter, ensuring provably favorable statistical behavior.

</details>


### [277] [TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors](https://arxiv.org/abs/2601.17958)
*Ido Andrew Atad,Itamar Zimerman,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: TensorLens：一种将整个transformer表示为单一输入依赖线性算子的新方法，通过高阶注意力交互张量统一编码注意力、FFN、激活、归一化和残差连接。


<details>
  <summary>Details</summary>
Motivation: 现有transformer分析大多局限于单个注意力头或层，缺乏对模型全局行为的考虑。虽然已有研究尝试通过平均和矩阵乘法扩展注意力公式，或纳入归一化、FFN等组件，但仍缺乏一个统一完整的表示框架来封装所有transformer块。

Method: 提出TensorLens方法，将整个transformer表示为单一输入依赖的线性算子，通过高阶注意力交互张量联合编码注意力机制、前馈网络、激活函数、归一化层和残差连接，提供理论上连贯且表达力强的线性表示。

Result: 经验验证表明TensorLens比之前的注意力聚合方法产生更丰富的表示。注意力张量可作为开发可解释性和模型理解工具的强大基础。

Conclusion: TensorLens填补了transformer全局表示的理论空白，提供了一个统一框架来理解transformer的完整计算过程，为可解释性研究和模型理解提供了新的理论基础和实用工具。

Abstract: Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.

</details>


### [278] [Federated learning for unpaired multimodal data through a homogeneous transformer model](https://arxiv.org/abs/2601.17986)
*Anders Eklund*

Main category: cs.LG

TL;DR: 提出一个联邦学习框架，用于在多模态数据分散、未配对且隐私敏感的场景下训练全局多模态Transformer模型，无需共享原始数据或对齐样本。


<details>
  <summary>Details</summary>
Motivation: 现实联邦环境中，数据通常是未配对且分散在不同节点上的（如图像在A节点，文本在B节点），这些数据严格私有且没有共同样本。现有联邦学习方法无法处理这种场景，因为它们假设本地客户端拥有对齐的数据对或需要共享原始特征嵌入，这违反了数据主权。

Method: 1) 使用小型公共锚点集对齐分离的私有流形；2) 通过Gram矩阵和中心核对齐实现跨模态语义对齐，不传输私有样本；3) 提出子空间稳定微调方法处理大型Transformer模型；4) 引入精度加权平均，利用不确定性估计降低不确定节点的权重。

Result: 建立联邦未配对基础模型的数学基础，使全局模型能够从分散、分离和私有的数据孤岛中学习世界的统一表示，无需集中存储或配对样本。

Conclusion: 该框架为联邦未配对基础模型提供了数学基础，能够在保护数据隐私的前提下，从分散、未配对的多模态数据中训练全局多模态Transformer，解决了现实联邦环境中的数据对齐和隐私保护挑战。

Abstract: Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.

</details>


### [279] [Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization](https://arxiv.org/abs/2601.17987)
*Ziwei Zheng,Huizhi Liang,Vaclav Snasel,Vito Latora,Panos Pardalos,Giuseppe Nicosia,Varun Ojha*

Main category: cs.LG

TL;DR: 该研究提出了一种系统探索收敛性、剪枝和量化关系的计算方法，发现尽管架构多样，但性能基本不变且学习动态呈现三个稳定阶段，并量化了剪枝冗余和量化影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络在分类任务上表现出色，但确定能够可靠完成任务的最小架构仍然具有挑战性。需要系统探索收敛性、剪枝和量化之间的关系，为在剪枝和低精度约束下选择紧凑稳定模型提供指导。

Method: 提出计算方法论：首先在大量架构上进行结构化设计扫描，然后在代表性模型上评估收敛行为、剪枝敏感性和量化鲁棒性。针对复杂度递增的图像分类任务，涵盖深度神经网络、卷积神经网络和视觉Transformer。

Result: 尽管架构多样，但性能基本不变，学习动态始终呈现三个阶段：不稳定、学习和过拟合。确定了稳定学习所需的最小可学习参数，发现了不同的收敛和剪枝阶段，量化了降低数值精度对可训练参数的影响。更深架构比浅层架构对剪枝更具弹性，参数冗余高达60%，量化对参数较少的模型影响更大，对更难的数据集影响更显著。

Conclusion: 研究结果为在剪枝和低精度约束下选择紧凑稳定的图像分类模型提供了可操作的指导，揭示了架构多样性下的统一学习动态模式，并为模型压缩和优化提供了量化依据。

Abstract: Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification.

</details>


### [280] [Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning](https://arxiv.org/abs/2601.17995)
*Shudi Weng,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: 提出H-SecCoGC方案，通过编码策略实现结构化聚合，解决分层联邦学习中隐私保护与通信不可靠的挑战


<details>
  <summary>Details</summary>
Motivation: 分层联邦学习(HFL)虽然改善了客户端与服务器之间的链路质量，但在不可靠通信环境下，如何在保证模型精度的同时保护隐私仍然是一个关键挑战，因为隐私噪声的协调可能被随机破坏

Method: 提出鲁棒的分层安全聚合方案H-SecCoGC，集成编码策略来强制执行结构化聚合，确保在不同隐私级别下准确构建全局模型，同时避免部分参与问题

Result: 该方案显著提高了鲁棒性、隐私保护和学习效率，理论分析和实验结果都证明了其在不可靠通信环境下对任意强隐私保证的优越性

Conclusion: H-SecCoGC方案有效解决了分层联邦学习中隐私保护与通信可靠性的平衡问题，为不可靠通信环境下的安全联邦学习提供了有效解决方案

Abstract: Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees

</details>


### [281] [Spelling Bee Embeddings for Language Modeling](https://arxiv.org/abs/2601.18030)
*Markus N. Rabe,Judith Clymo,Zheren Dong*

Main category: cs.LG

TL;DR: 通过修改嵌入层，将拼写信息融入词嵌入中，显著提升模型在拼写和标准基准测试上的表现，相当于减少约8%的计算和数据需求


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的嵌入层通常只关注语义信息，忽略了词汇的拼写特征。然而，拼写信息可能包含重要的语言规律和形态特征，能够增强模型的语言理解能力

Method: 对嵌入层进行简单修改，将词汇的拼写信息融入词嵌入表示中。具体方法是在标准词嵌入的基础上，加入基于词汇拼写特征的编码信息

Result: 在40M到800M参数规模的模型上进行扩展研究，结果显示：1）模型在拼写任务上表现显著提升；2）在标准基准测试上也有全面改善；3）改进效果相当于需要减少约8%的计算资源和训练数据来达到相同的测试损失

Conclusion: 将拼写信息融入词嵌入是一种简单而有效的方法，能够显著提升语言模型的性能，同时减少训练所需的计算和数据资源，为更高效的模型训练提供了新思路

Abstract: We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.

</details>


### [282] [Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity](https://arxiv.org/abs/2601.18032)
*Brijesh FNU,Viet Thanh Duy Nguyen,Ashima Sharma,Md Harun Rashid Molla,Chengyi Xu,Truong-Son Hy*

Main category: cs.LG

TL;DR: 开发多模态学习框架，利用预训练聚合物表示进行介电弹性体的少样本性能预测，解决数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 软电子和可拉伸电子学快速发展，急需高性能介电弹性体，但现有材料难以同时具备高介电常数和低杨氏模量，且缺乏系统性的数据集

Method: 收集并整理过去10年文献中的丙烯酸酯基介电弹性体实验数据，构建高质量数据集；提出多模态学习框架，利用基于图和序列的编码器获取预训练聚合物表示，实现从分子序列到介电和机械性能的少样本预测

Result: 成功开发了能够准确预测介电弹性体性能的多模态学习模型，克服了数据稀缺问题，为其他聚合物体系（如硅胶、聚氨酯）提供了可迁移的知识转移范式

Conclusion: 该研究提出了一种利用预训练多模态模型进行知识转移的新范式，能够加速数据高效的高性能介电弹性体发现，代码和数据集已开源

Abstract: Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers

</details>


### [283] [Resonant Sparse Geometry Networks](https://arxiv.org/abs/2601.18064)
*Hasi Hays*

Main category: cs.LG

TL;DR: RSGN是一种受大脑启发的稀疏几何网络，在双曲空间中学习动态稀疏连接，实现O(n*k)计算复杂度，比Transformer更高效且参数更少。


<details>
  <summary>Details</summary>
Motivation: Transformer架构使用密集注意力机制，计算复杂度为O(n²)，参数需求大。受大脑稀疏连接和几何组织原则启发，需要开发更高效、生物合理的神经网络架构。

Method: RSGN将计算节点嵌入学习的双曲空间，连接强度随测地距离衰减，实现输入依赖的动态稀疏性。采用双时间尺度：快速可微分激活传播（梯度下降优化）和慢速Hebbian结构学习（局部相关性规则）。

Result: 在长距离依赖任务上达到96.5%准确率，参数比标准Transformer少约15倍。在20类分层分类任务上达到23.8%准确率（随机基线5%），仅需41,672参数，比需要403,348参数达到30.1%准确率的Transformer少近10倍。

Conclusion: 受大脑启发的稀疏几何计算原则为开发更高效、生物合理的神经架构提供了有前景的方向，RSGN在保持性能的同时显著减少了计算复杂度和参数需求。

Abstract: We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse
  hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with
  O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength
  decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two
  distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow
  Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous
  mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k << n represents the average
  active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks
  demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer
  parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8%
  accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines
  which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural
  component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles
  of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible
  neural architectures.

</details>


### [284] [Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming](https://arxiv.org/abs/2601.18076)
*Alexandra Chouldechova,A. Feder Cooper,Solon Barocas,Abhinav Palia,Dan Vann,Hanna Wallach*

Main category: cs.LG

TL;DR: 论文指出当前AI红队测试中基于攻击成功率(ASR)比较得出的系统安全性或攻击方法有效性结论往往缺乏证据支持，存在苹果与橙子比较和低效度测量问题，需要建立有意义的比较框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全评估中，研究人员经常通过攻击成功率(ASR)比较来得出系统相对安全性或攻击方法有效性的结论，但这些结论往往缺乏坚实的证据基础，存在方法论缺陷。

Method: 采用概念分析、理论框架和实证研究相结合的方法，借鉴社会科学测量理论和推断统计学原理，建立攻击成功率有意义的比较条件，并以越狱攻击为例进行详细分析。

Result: 研究发现许多ASR比较属于苹果与橙子的无效比较，测量效度低，无法支持关于系统安全性或攻击方法有效性的可靠结论，需要建立更严谨的比较框架。

Conclusion: AI红队测试中的攻击成功率比较需要满足特定的测量理论条件才能有意义，当前许多比较存在方法论缺陷，需要更严谨的测量和比较框架来支持有效结论。

Abstract: We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.

</details>


### [285] [DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal](https://arxiv.org/abs/2601.18081)
*Peixuan Han,Yingjie Yu,Jingjun Xu,Jiaxuan You*

Main category: cs.LG

TL;DR: DRPG是一个用于自动生成学术反驳的智能体框架，通过分解、检索、规划和生成四步流程，在顶级会议数据上超越了现有方法和人类平均水平。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在科研工作流中应用日益广泛，但学术反驳这一学术交流和同行评审的关键环节仍缺乏自动化支持。现有方法通常依赖现成的LLM或简单流水线，难以处理长上下文，且无法生成有针对性和说服力的回应。

Method: 提出DRPG框架，包含四个步骤：1) 将审稿意见分解为原子关注点；2) 从论文中检索相关证据；3) 规划反驳策略；4) 据此生成回应。其中规划器在识别最可行反驳方向方面达到超过98%的准确率。

Result: 在顶级会议数据上的实验表明，DRPG显著优于现有反驳流水线，仅使用8B模型就实现了超越人类平均水平的性能。规划器设计有效，能提供多视角和可解释的建议，且在更复杂的多轮设置中表现良好。

Conclusion: DRPG框架有效，能够生成高质量的反驳内容，支持学术讨论的规模化扩展。代码已开源。

Abstract: Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.

</details>


### [286] [LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts](https://arxiv.org/abs/2601.18089)
*Venmugil Elango,Nidhi Bhatia,Roger Waleffe,Rasoul Shafipour,Tomer Asida,Abhinav Khattar,Nave Assaf,Maximilian Golub,Joey Guman,Tiyasa Mitra,Ritchie Zhao,Ritika Borkar,Ran Zilberstein,Mostofa Patwary,Mohammad Shoeybi,Bita Rouhani*

Main category: cs.LG

TL;DR: LatentMoE是一种新的MoE架构，通过硬件-软件协同设计优化推理成本，在准确率/FLOP和准确率/参数方面优于标准MoE架构，已被Nvidia的Nemotron-3模型采用。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE已成为许多最先进大语言模型的核心组件，但现有MoE架构在推理成本（以准确率/FLOP和准确率/参数衡量）方面是否接近最优仍不清楚。需要从硬件-软件协同设计角度重新审视MoE设计。

Method: 从硬件-软件协同设计角度，结合经验和理论分析，识别不同部署场景下的性能瓶颈。通过系统化设计空间探索（最高95B参数、1T token训练），开发了LatentMoE架构，优化单位计算量的准确率。

Result: LatentMoE在准确率/FLOP和准确率/参数方面持续优于标准MoE架构。该架构已被Nvidia的Nemotron-3 Super和Ultra模型采用，并扩展到更大规模（更长token序列和更大模型尺寸）。

Conclusion: 通过硬件-软件协同设计的系统化探索，LatentMoE提供了一种更优的MoE架构，在推理成本效率方面显著改进，为大规模语言模型部署提供了更好的解决方案。

Abstract: Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).

</details>


### [287] [From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models](https://arxiv.org/abs/2601.18091)
*Longwei Ding,Anhao Zhao,Fanghua Ye,Ziyang Chen,Xiaoyu Shen*

Main category: cs.LG

TL;DR: 该研究对比了指令遵循型LLM和推理增强型LLM的剪枝策略，发现不同范式需要不同的剪枝方法：深度剪枝在分类任务上表现更好，宽度剪枝在生成和推理任务上更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝研究主要关注指令遵循型LLM，但缺乏对推理增强型模型（生成长中间推理轨迹）的剪枝策略研究。需要了解现有剪枝方法是否适用于这两种不同类型的LLM。

Method: 对指令遵循型LLM和推理增强型LLM进行对照研究，采用与原始训练分布对齐的剪枝校准和后剪枝恢复数据。评估了静态深度剪枝、静态宽度剪枝和动态剪枝三种方法，在17个任务（分类、生成、推理）上进行测试。

Result: 发现范式依赖的明显差异：深度剪枝在分类任务上优于宽度剪枝，而宽度剪枝在生成和推理任务上更稳健。静态剪枝能更好地保持推理性能，动态剪枝在分类和生成任务上表现优异，但对长链推理仍具挑战性。

Conclusion: 需要针对推理增强型LLM的独特特性设计专门的剪枝策略，不能简单套用指令遵循型LLM的剪枝方法。研究强调了考虑模型范式差异的重要性。

Abstract: Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.

</details>


### [288] [Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions](https://arxiv.org/abs/2601.18107)
*Pedram Agand,Mo Chen*

Main category: cs.LG

TL;DR: MoReBRAC是一个基于模型的离线强化学习框架，通过不确定性感知的潜在合成来解决分布偏移问题，在D4RL基准测试中表现出色，特别是在随机和次优数据场景下。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在工业机器人等安全关键领域有巨大潜力，但静态数据集与学习策略之间的分布偏移问题限制了策略改进，通常需要高度保守的方法。

Method: MoReBRAC采用基于模型的方法，使用双循环世界模型合成高保真转换来扩展训练流形。通过分层不确定性管道（包括VAE流形检测、模型敏感性分析和MC dropout）确保合成数据的可靠性，只使用学习动态的高置信度区域内的转换。

Result: 在D4RL Gym-MuJoCo基准测试中显示出显著的性能提升，特别是在"随机"和"次优"数据场景下。VAE作为几何锚点发挥了重要作用，并揭示了在接近最优数据集学习时的分布权衡。

Conclusion: MoReBRAC通过不确定性感知的潜在合成有效解决了离线强化学习中的分布偏移问题，为安全关键领域的应用提供了有前景的解决方案，特别是在数据质量较低的场景下。

Abstract: Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.

</details>


### [289] [AttenMIA: LLM Membership Inference Attack through Attention Signals](https://arxiv.org/abs/2601.18110)
*Pedram Zaree,Md Abdullah Al Mamun,Yue Dong,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: AttenMIA：利用Transformer自注意力模式进行成员推理攻击的新框架，相比传统基于置信度或嵌入的方法效果更好，在低误报率下表现优异


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据集的记忆化倾向引发严重的隐私和知识产权担忧。现有成员推理攻击主要依赖输出置信度或嵌入特征，但这些信号往往脆弱，攻击成功率有限

Method: 提出AttenMIA框架，利用Transformer内部的自注意力模式推断成员身份。方法从各层注意力头提取信息，结合基于扰动的发散度量训练有效的MIA分类器

Result: 注意力特征始终优于基线方法，在低误报率指标下表现突出（在WikiMIA-32基准测试中达到0.996 ROC AUC和87.9% TPR@1%FPR）。注意力信号在不同数据集和架构间具有泛化性，在数据提取攻击中优于现有技术

Conclusion: 注意力机制原本旨在增强可解释性，却无意中放大了LLMs的隐私风险，凸显了开发新防御措施的必要性

Abstract: Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.

</details>


### [290] [Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting](https://arxiv.org/abs/2601.18111)
*Jean Kossaifi,Nikola Kovachki,Morteza Mardani,Daniel Leibovici,Suman Ravuri,Ira Shokar,Edoardo Calvello,Mohammad Shoaib Abbas,Peter Harrington,Ashay Subramaniam,Noah Brenowitz,Boris Bonev,Wonmin Byeon,Karsten Kreis,Dale Durran,Arash Vahdat,Mike Pritchard,Jan Kautz*

Main category: cs.LG

TL;DR: 该论文提出一个可扩展的多尺度大气动力学学习框架，无需复杂架构或专门训练策略即可实现最先进的概率预报技能


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的天气预报方法存在架构复杂、训练策略碎片化的问题，掩盖了预报准确性的根本驱动因素。作者希望证明最先进的概率预报技能不需要复杂的架构约束或专门的训练启发式方法。

Method: 引入一个可扩展的多尺度大气动力学学习框架，结合直接下采样的潜在空间和历史条件局部投影器来解析高分辨率物理。该框架设计对概率估计器的选择具有鲁棒性，支持随机插值、扩散模型和基于CRPS的集合训练。

Result: 与集成预报系统(IFS)和深度学习概率模型GenCast相比，该框架在大多数变量上实现了统计显著的改进。验证表明该框架在中等范围预测中达到最先进水平。

Conclusion: 扩展通用模型足以实现最先进的中期预测，无需定制训练方案，且在完整的概率框架谱系中都有效。这简化了天气预报模型的开发流程。

Abstract: The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.

</details>


### [291] [Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods](https://arxiv.org/abs/2601.18142)
*Mingxu Zhang,Huicheng Zhang,Jiaming Ji,Yaodong Yang,Ying Sun*

Main category: cs.LG

TL;DR: 提出ADRC-Lagrangian方法，通过主动抗扰控制增强鲁棒性，减少振荡和安全违规，在安全强化学习中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法（如PID和经典Lagrangian方法）存在参数敏感性和相位滞后问题，导致振荡和频繁安全违规，需要更鲁棒的控制方法

Method: 提出ADRC-Lagrangian方法，将主动抗扰控制（ADRC）与Lagrangian框架结合，形成统一框架，包含经典和PID Lagrangian方法作为特例

Result: 实验显示：安全违规减少74%，约束违规幅度降低89%，平均成本下降67%，在复杂环境中表现优越

Conclusion: ADRC-Lagrangian方法通过增强鲁棒性和减少振荡，显著提升了安全强化学习的性能，为解决现有方法的局限性提供了有效方案

Abstract: Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.

</details>


### [292] [FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning](https://arxiv.org/abs/2601.18150)
*Zhaopeng Qiu,Shuang Yu,Jingqi Zhang,Shuai Zhang,Xue Huang,Jingyi Yang,Junjie Lai*

Main category: cs.LG

TL;DR: 提出一个用于大语言模型强化学习的FP8推理栈，通过块级FP8量化、KV缓存FP8化和重要性采样校正，实现最高44%的推理吞吐提升，同时保持与BF16基线相当的学习效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习中的推理（生成）阶段成为瓶颈，长输出序列导致注意力机制和KV缓存占用大量内存和时间。FP8量化可加速推理，但面临权重频繁变化和训练-推理不匹配的挑战。

Method: 1) 使用块级FP8量化实现W8A8线性层推理；2) 通过每步QKV尺度重校准将FP8扩展到KV缓存；3) 采用基于重要性采样的推理校正（token级TIS/MIS变体）缓解不匹配问题。

Result: 在密集和MoE模型上，该技术实现了最高44%的推理吞吐提升，同时保持了与BF16基线相当的学习行为。

Conclusion: 提出的FP8推理栈成功解决了大语言模型强化学习中FP8量化的工程和算法挑战，实现了显著的性能提升而不损害学习效果。

Abstract: Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.

</details>


### [293] [Learning Fair Domain Adaptation with Virtual Label Distribution](https://arxiv.org/abs/2601.18171)
*Yuguang Zhang,Lijun Sheng,Jian Liang,Ran He*

Main category: cs.LG

TL;DR: VILL是一个用于无监督域适应的简单有效框架，通过自适应重加权和KL散度再平衡策略，在保持高整体准确率的同时提升最差类别性能，改善类别公平性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法主要关注整体准确率提升，但忽略了不同类别之间的性能差异（类别公平性问题）。实证分析发现UDA分类器倾向于偏向容易类别而忽视困难类别，导致性能不均衡。

Method: 提出VILL框架，包含两个核心组件：1）自适应重加权策略，放大难以分类类别的影响；2）基于KL散度的再平衡策略，显式调整决策边界以增强类别公平性。该框架可作为即插即用模块集成到现有UDA方法中。

Result: 在常用数据集上的实验表明，VILL能够显著改善类别公平性，同时保持高整体准确率。该框架可以无缝集成到现有UDA方法中，作为即插即用模块有效提升性能。

Conclusion: VILL为解决无监督域适应中的类别公平性问题提供了一个简单而有效的解决方案，通过自适应重加权和决策边界调整，在保持整体性能的同时改善了最差类别性能，为UDA研究提供了新的视角。

Abstract: Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.

</details>


### [294] [Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients](https://arxiv.org/abs/2601.18189)
*Rui Wu,Yongjun Li*

Main category: cs.LG

TL;DR: 提出AHOC混合阶无环约束和SPG-AHOC平滑近端梯度算法，通过有限时间Oracle性质保证在有限迭代内精确恢复DAG结构，无需后处理阈值化。


<details>
  <summary>Details</summary>
Motivation: 现有连续优化方法（如NOTEARS）只能保证渐近收敛到平稳点，通常产生稠密权重矩阵，需要任意的后处理阈值化才能恢复DAG。连续优化与离散图结构之间的差距是根本性挑战。

Method: 提出混合阶无环约束（AHOC），通过平滑近端梯度算法（SPG-AHOC）进行优化。利用近端算法的流形识别特性，提供有限时间Oracle性质的理论保证。

Result: 在标准可识别性假设下，SPG-AHOC在有限迭代内精确恢复DAG支持（结构），即使优化的是平滑近似。算法返回具有精确零项的图，无需启发式截断。

Conclusion: SPG-AHOC填补了连续优化与离散图结构之间的差距，通过有限时间结构恢复理论保证，实现了最先进的准确率，消除了结构模糊性。

Abstract: Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.

</details>


### [295] [HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models](https://arxiv.org/abs/2601.18200)
*Chenyu Zhang,Xinchen Lyu,Chenshan Ren,Shuhan Liu,Qimei Cui,Xiaofeng Tao*

Main category: cs.LG

TL;DR: HeterCSI是一个用于无线信道状态信息处理的通道自适应预训练框架，通过优化梯度动态和批处理策略，解决了CSI在规模和场景维度上的双重异质性挑战，显著提升了泛化性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 无线基础模型在6G网络应用中面临CSI在规模和场景维度上的双重异质性挑战。现有预训练方法要么限制输入为固定维度，要么按规模隔离训练，限制了模型的泛化能力和可扩展性。

Method: 提出HeterCSI框架，核心洞察是：CSI规模异质性主要导致破坏性梯度干扰，而场景多样性在适当管理下能促进建设性梯度对齐。方法包括：1) 将异构CSI批处理构建公式化为分区优化问题；2) 开发规模感知自适应批处理策略；3) 设计双重掩码机制隔离有效信号和填充伪影。

Result: 在12个数据集上的实验表明，HeterCSI无需场景特定微调即可建立泛化基础模型，性能优于全样本基线。相比最先进的零样本基准WiFo，在CSI重建、时域和频域预测上分别降低NMSE 7.19 dB、4.08 dB和5.27 dB。训练延迟降低53%，平均泛化性能提升1.53 dB。

Conclusion: HeterCSI通过创新的梯度动态理解和自适应批处理策略，成功解决了CSI双重异质性挑战，在保持训练效率的同时实现了强大的跨场景泛化能力，为无线基础模型的发展提供了新方向。

Abstract: Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.

</details>


### [296] [PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR](https://arxiv.org/abs/2601.18207)
*James Burgess,Jan N. Hansen,Duo Peng,Yuhui Zhang,Alejandro Lozano,Min Woo Sun,Emma Lundberg,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 该论文提出了一个针对科学论文搜索的强化学习代理训练框架，发布了包含1600万篇生物医学论文摘要的搜索语料库和包含6万个样本的PaperSearchQA问答数据集，训练出的搜索代理在技术问答任务上超越了传统检索基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理主要针对通用领域问答，缺乏对科学、工程和医学等专业技术领域的关注。该研究旨在开发能够搜索和推理科学论文的智能代理，这对未来AI科学家系统和实际科学研究具有重要意义。

Method: 构建了包含1600万篇生物医学论文摘要的搜索语料库，创建了包含6万个样本的PaperSearchQA问答数据集和基准测试。使用强化学习与可验证奖励（RLVR）方法训练搜索代理，并基于Search-R1代码库进行实现。

Result: 训练出的搜索代理在技术问答任务上超越了非强化学习的检索基线方法。定量分析显示代理表现出规划、推理和自我验证等有趣行为。所有资源（语料库、数据集、基准测试）都已公开发布。

Conclusion: 该研究成功开发了针对科学论文搜索的强化学习代理训练框架，证明了在专业技术领域应用RLVR方法的有效性。数据创建方法具有可扩展性，可轻松扩展到其他科学领域，为未来AI科学家系统的发展奠定了基础。

Abstract: Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.

</details>


### [297] [Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting](https://arxiv.org/abs/2601.18231)
*Trong Khiem Tran,Manh Cuong Dao,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 提出一个理论框架来理解特征对齐和目标微调之间的交互作用，通过特征标签扭曲概念建立可证明的泛化边界，指导算法设计并提升跨模态适应性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型适应新特征模态的需求日益增长，但现有方法缺乏对特征对齐和目标微调之间关键交互作用的理论理解，未校准的组合会加剧源-目标特征标签结构之间的错位，降低目标泛化能力。

Method: 开发一个原则性框架，建立目标误差的可证明泛化边界，通过新颖的特征标签扭曲概念解释特征对齐和目标拟合之间的交互作用，为实际算法设计提供可操作的优化指导。

Result: 该方法在广泛的基准数据集上显著优于最先进的方法，实现了性能的显著提升。

Conclusion: 提出的理论框架为跨模态适应提供了重要的理论洞察，通过理解特征对齐和目标微调的交互作用，能够指导更有效的算法设计，实现更好的知识迁移和泛化性能。

Abstract: Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.

</details>


### [298] [Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity](https://arxiv.org/abs/2601.18245)
*Santanu Das,Jatin Batra*

Main category: cs.LG

TL;DR: 提出了首个多项式时间算法，用于解决具有重尾噪声和对抗性损坏的鲁棒相位恢复问题，样本复杂度接近线性。


<details>
  <summary>Details</summary>
Motivation: 相位恢复在光学、晶体学等领域有广泛应用，但现有算法对测量误差的鲁棒性不足。虽然最近在鲁棒统计方面有突破，但鲁棒相位恢复问题仍缺乏高效算法，特别是当测量值和传感向量都可能被对抗性损坏时。

Method: 通过建立鲁棒谱初始化与鲁棒PCA之间的联系，利用鲁棒PCA的最新算法进展，开发了多项式时间算法。该方法解决了之前工作中需要鲁棒谱初始化但缺乏高效实现的问题。

Result: 提出了首个多项式时间算法，用于处理重尾噪声和对抗性损坏的鲁棒相位恢复问题，实现了接近线性的样本复杂度O(n log n)，显著改进了之前的指数时间算法。

Conclusion: 通过连接鲁棒谱初始化和鲁棒PCA，成功开发了高效的多项式时间算法，解决了鲁棒相位恢复中的关键计算瓶颈，为实际应用提供了可行的解决方案。

Abstract: Phase retrieval is the classical problem of recovering a signal $x^* \in \mathbb{R}^n$ from its noisy phaseless measurements $y_i = \langle a_i, x^* \rangle^2 + ζ_i$ (where $ζ_i$ denotes noise, and $a_i$ is the sensing vector) for $i \in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.

</details>


### [299] [Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs](https://arxiv.org/abs/2601.18255)
*Fei Meng*

Main category: cs.LG

TL;DR: 本文发现经验回放在大语言模型持续学习中存在矛盾：对非结构化任务有正向迁移，但对结构化任务（如代码生成）造成严重负向迁移，并提出正交子空间唤醒方法来解决这一困境。


<details>
  <summary>Details</summary>
Motivation: 大语言模型持续学习面临平衡稳定性（保留旧知识）和可塑性（学习新任务）的关键挑战。虽然经验回放是应对灾难性遗忘的标准方法，但其对不同能力的影响尚未充分探索。

Method: 提出正交子空间唤醒方法：通过短暂的"唤醒"阶段识别先前任务的关键参数子空间，并对新任务强制执行正交更新，为已建立的知识结构提供数学基础的"安全保证"。

Result: 在多样化的四任务序列上的实证结果表明，OSW在经验回放失败的脆弱编码能力保护方面表现独特，同时保持对新任务的高可塑性。

Conclusion: 研究强调了大语言模型持续学习中评估结构安全性与平均保留性能同等重要，OSW方法为解决经验回放的结构完整性困境提供了有效解决方案。

Abstract: Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief "wake-up" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded "safety guarantee" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.

</details>


### [300] [FGGM: Fisher-Guided Gradient Masking for Continual Learning](https://arxiv.org/abs/2601.18261)
*Chao-Hong Tan,Qian Chen,Wen Wang,Yukun Ma,Chong Zhang,Chong Deng,Qinglin Zhang,Xiangang Li,Jieping Ye*

Main category: cs.LG

TL;DR: 提出FGGM框架，通过Fisher信息指导的梯度掩码缓解大语言模型持续学习中的灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘严重影响了大型语言模型的持续学习能力，现有方法如MIGU基于参数幅度进行选择，缺乏数学原理基础

Method: 使用对角Fisher信息矩阵战略性地选择需要更新的参数，动态生成具有自适应阈值的二进制掩码，保留关键参数以平衡稳定性和可塑性，无需历史数据

Result: 在TRACE基准测试中，相比监督微调(SFT)在保留通用能力方面相对提升9.6%，相比MIGU在TRACE任务上提升4.4%；在代码生成任务中也表现出优越性能和减少遗忘

Conclusion: FGGM是一种基于数学原理的参数重要性估计方法，能有效缓解灾难性遗忘，是持续学习的有效解决方案

Abstract: Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.

</details>


### [301] [Neural Network Approximation: A View from Polytope Decomposition](https://arxiv.org/abs/2601.18264)
*ZeYu Li,ShiJun Zhang,TieYong Zeng,FengLei Fan*

Main category: cs.LG

TL;DR: 本文提出了一种基于多面体分解的ReLU网络通用逼近理论，相比传统均匀网格划分方法，能更好地处理目标函数的局部正则性，特别是在奇异点附近更高效灵活。


<details>
  <summary>Details</summary>
Motivation: 现有通用逼近理论大多通过均匀划分输入空间来构造神经网络，没有考虑目标函数的局部正则性。作者希望从多面体分解的角度研究ReLU网络的通用逼近能力，提供更现实、任务导向的方法。

Method: 1. 开发显式核多项式方法获得连续函数的通用逼近，特征包括精细的Totik-Ditzian型连续性模和多面体域分解；2. 在每个子域中分别构造ReLU网络来逼近核多项式；3. 将方法扩展到解析函数以获得更高的逼近率。

Result: 多面体分解使逼近在许多情况下比现有方法更高效灵活，特别是在目标函数的奇异点附近。该方法能够处理函数的局部正则性，提供更实用的逼近方案。

Conclusion: 从多面体分解角度研究ReLU网络的通用逼近能力是更现实和任务导向的方法，能更好地处理目标函数的局部特性，特别是在奇异点附近，为实际应用提供了更有效的理论框架。

Abstract: Universal approximation theory offers a foundational framework to verify neural network expressiveness, enabling principled utilization in real-world applications. However, most existing theoretical constructions are established by uniformly dividing the input space into tiny hypercubes without considering the local regularity of the target function. In this work, we investigate the universal approximation capabilities of ReLU networks from a view of polytope decomposition, which offers a more realistic and task-oriented approach compared to current methods. To achieve this, we develop an explicit kernel polynomial method to derive an universal approximation of continuous functions, which is characterized not only by the refined Totik-Ditzian-type modulus of continuity, but also by polytopical domain decomposition. Then, a ReLU network is constructed to approximate the kernel polynomial in each subdomain separately. Furthermore, we find that polytope decomposition makes our approximation more efficient and flexible than existing methods in many cases, especially near singular points of the objective function. Lastly, we extend our approach to analytic functions to reach a higher approximation rate.

</details>


### [302] [What Do Learned Models Measure?](https://arxiv.org/abs/2601.18278)
*Indrė Žliobaitė*

Main category: cs.LG

TL;DR: 论文指出机器学习模型作为测量工具使用时，标准评估指标（如泛化误差、校准、鲁棒性）无法保证测量稳定性，即不同模型可能实现系统不等价的测量函数，需要新的评估维度。


<details>
  <summary>Details</summary>
Motivation: 在科学和数据驱动应用中，机器学习模型越来越多地被用作测量工具而非仅仅是预测预定义标签。当测量函数从数据中学习时，从观察到数量的映射由训练分布和归纳偏置隐式决定，导致多个不等价的映射都能满足标准预测评估标准。

Method: 将学习到的测量函数形式化为一个独立的评估焦点，引入测量稳定性概念，该属性捕捉测量量在学习过程的可容许实现和跨上下文中的不变性。通过真实案例研究展示问题。

Result: 研究表明标准机器学习评估标准（包括泛化误差、校准和鲁棒性）不能保证测量稳定性。具有可比预测性能的模型可以实现系统不等价的测量函数，分布偏移具体说明了这种失败。

Conclusion: 当学习到的模型输出被识别为测量时，现有评估框架存在局限性，需要增加额外的评估维度来确保测量稳定性。

Abstract: In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.

</details>


### [303] [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292)
*Zhewen Tan,Wenhan Yu,Jianfeng Si,Tongxin Liu,Kaiqi Guan,Huiyan Jin,Jiawen Tao,Xiaokun Yuan,Duohe Ma,Xiangzheng Zhang,Tong Yang,Lin Sun*

Main category: cs.LG

TL;DR: TriPlay-RL：一个基于强化学习的闭环框架，通过攻击者、防御者和评估者三个角色的协同进化，实现LLM安全对齐的持续改进，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全风险日益突出，需要有效缓解有害内容生成。现有安全对齐方法通常采用三方协作框架，但缺乏系统性的协同进化机制。

Method: 提出TriPlay-RL闭环强化学习框架，让攻击者（生成对抗提示）、防御者（安全防御）和评估者（响应评估）在统一学习循环中迭代协同改进，实现近零人工标注的持续优化。

Result: 攻击者在保持高输出多样性的同时，对抗效果提升20%-50%；防御者安全性能提升10%-30%且不影响通用推理能力；评估者通过迭代持续提升细粒度判断能力，能准确区分不安全响应、简单拒绝和有用指导。

Conclusion: TriPlay-RL为LLM安全对齐建立了高效可扩展的范式，实现了三方角色在统一学习循环中的持续协同进化。

Abstract: In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.

</details>


### [304] [A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods](https://arxiv.org/abs/2601.18314)
*Lina Felsner,Sevgi G. Kafali,Hannah Eichhorn,Agnes A. J. Leth,Aidas Batvinskas,Andre Datchev,Fabian Klemm,Jan Aulich,Puntika Leepagorn,Ruben Klinger,Daniel Rueckert,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 学生可重复性黑客松：复现三篇MRI重建论文（MoDL、HUMUS-Net、物理正则化动态MRI方法），分享实验设置、复现结果和可重复代码库构建实践


<details>
  <summary>Details</summary>
Motivation: 针对医学影像重建领域，特别是MRI重建算法，存在可重复性问题。通过学生黑客松形式，系统性复现有影响力的论文，建立可重复研究实践，促进领域发展。

Method: 组织学生可重复性黑客松，复现三篇MRI重建论文：1) MoDL（基于模型的展开网络+学习去噪）；2) HUMUS-Net（混合展开多尺度CNN+Transformer架构）；3) 无训练、物理正则化的动态MRI方法（使用定量MR模型进行早停）。记录实验设置、复现过程，并建立可重复代码库实践。

Result: 成功复现了三篇论文的结果，并进行了额外实验。详细记录了黑客松的组织过程和复现成果，总结了构建可重复代码库的基本实践方法。

Conclusion: 通过学生黑客松形式可以有效促进MRI重建算法的可重复性研究，为领域提供了可重复代码库构建的实践经验，有助于推动医学影像重建研究的可靠性和透明度。

Abstract: We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.

</details>


### [305] [Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals](https://arxiv.org/abs/2601.18326)
*Jie Li,Jing Li,Lu Lv,Zhanyu Ju,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出基于ZC序列和时频图像认知融合的无人机信号OOD检测算法，在RID任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 无人机远程识别(RID)需要检测未知或非标准通信协议的无人机信号，现有方法难以有效处理这类分布外检测(OODD)问题

Method: 结合ZC序列（针对已知DJI协议）和时频图像（针对未知协议）双模态特征，通过特征提取、对齐、交互和融合，计算判别分数并转换为自适应注意力权重进行分类

Result: 在RID和OODD指标上分别提升1.7%和7.5%，在不同飞行条件和无人机类型下表现出强鲁棒性

Conclusion: 提出的认知融合算法能有效检测分布外无人机信号，在无人机远程识别任务中优于现有方法

Abstract: We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.

</details>


### [306] [Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection](https://arxiv.org/abs/2601.18329)
*Chuhan Feng,Jing Li,Jie Li,Lu Lv,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出基于可区分性驱动的空间-通道选择与梯度范数的无人机信号OOD检测算法，通过时频图像特征自适应加权和梯度范数度量扰动敏感性，实现优异的OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机信号检测中，传统方法对分布外（OOD）样本的识别能力有限，需要开发能够有效区分已知协议信号与未知/异常信号的鲁棒检测算法。

Method: 1. 基于协议特定时频特征，量化类间相似性和方差，对时频图像特征进行空间和通道维度的自适应加权；2. 引入梯度范数度量扰动敏感性，捕捉OOD样本的内在不稳定性；3. 将梯度范数与基于能量的评分融合进行联合推断。

Result: 仿真结果表明，该算法在不同信噪比（SNR）和多种无人机类型下，展现出优异的判别能力和鲁棒性能。

Conclusion: 提出的基于可区分性驱动的空间-通道选择与梯度范数的OOD检测算法，能够有效识别无人机信号中的分布外样本，具有实际应用价值。

Abstract: We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.

</details>


### [307] [Structural Gender Bias in Credit Scoring: Proxy Leakage](https://arxiv.org/abs/2601.18342)
*Navya SD,Sreekanth D,SS Uma Sankari*

Main category: cs.LG

TL;DR: 研究审计台湾信用违约数据集中的结构性性别偏见，发现即使移除受保护属性和应用公平干预，性别预测信号仍深植于非敏感特征中，传统公平审计不足以检测隐性结构偏见。


<details>
  <summary>Details</summary>
Motivation: 随着金融机构越来越多地采用机器学习进行信用风险评估，算法偏见的持续存在成为实现公平金融包容性的关键障碍。本研究旨在挑战"通过盲目实现公平"的主流观念，揭示结构性性别偏见在金融AI中的持续存在。

Method: 使用SHAP（SHapley Additive exPlanations）识别作为性别强代理变量的特征（如婚姻状况、年龄、信用额度）；采用对抗性逆建模框架数学量化信息泄漏，从纯非敏感金融特征重建受保护的性别属性。

Result: 研究发现婚姻状况、年龄和信用额度等变量作为性别的强代理变量，使模型在保持统计公平表象的同时维持歧视性路径；受保护的性别属性可以从纯非敏感金融特征以ROC AUC得分0.65重建，表明传统公平审计不足以检测隐性结构偏见。

Conclusion: 研究主张从表面统计平等转向因果感知建模和金融AI中的结构性问责，强调需要超越传统公平审计的方法来检测和缓解结构性偏见。

Abstract: As financial institutions increasingly adopt machine learning for credit risk assessment, the persistence of algorithmic bias remains a critical barrier to equitable financial inclusion. This study provides a comprehensive audit of structural gender bias within the Taiwan Credit Default dataset, specifically challenging the prevailing doctrine of "fairness through blindness." Despite the removal of explicit protected attributes and the application of industry standard fairness interventions, our results demonstrate that gendered predictive signals remain deeply embedded within non-sensitive features. Utilizing SHAP (SHapley Additive exPlanations), we identify that variables such as Marital Status, Age, and Credit Limit function as potent proxies for gender, allowing models to maintain discriminatory pathways while appearing statistically fair. To mathematically quantify this leakage, we employ an adversarial inverse modeling framework. Our findings reveal that the protected gender attribute can be reconstructed from purely non-sensitive financial features with an ROC AUC score of 0.65, demonstrating that traditional fairness audits are insufficient for detecting implicit structural bias. These results advocate for a shift from surface-level statistical parity toward causal-aware modeling and structural accountability in financial AI.

</details>


### [308] [Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning](https://arxiv.org/abs/2601.18356)
*Weiqin Yang,Haowen Xue,Qingyi Peng,Hexuan Hu,Qian Huang,Tingbo Zhang*

Main category: cs.LG

TL;DR: 提出多模态因果检索增强生成框架，将因果推理与多模态检索结合，提升医学视觉语言模型的准确性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型主要依赖相关性而非因果推理，容易产生幻觉和受数据集偏差影响。传统检索增强生成依赖语义相似性，仍会引入虚假相关性

Method: 提出多模态因果检索增强生成框架，从外部源检索临床相关示例和因果图，基于反事实和干预证据而非单纯相关性进行模型推理

Result: 应用于放射学报告生成、诊断预测和视觉问答，提高了事实准确性、对分布偏移的鲁棒性和可解释性

Conclusion: 因果检索为医学视觉语言模型提供了可扩展路径，使其超越模式匹配，实现高风险临床环境中可信的多模态推理

Abstract: Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.

</details>


### [309] [Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach](https://arxiv.org/abs/2601.18399)
*Mehmet Velioglu,Song Zhai,Alexander Mitsos,Adel Mhamdi,Andreas Jupke,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出使用物理信息神经网络(PINN)结合扩展卡尔曼滤波器，仅通过流量测量来估计重力沉降器中液-液分散的相高度，减少对昂贵光学测量的依赖。


<details>
  <summary>Details</summary>
Motivation: 重力沉降器中液-液分散的相分离监测对化工、制药和回收过程至关重要，但传统光学测量方法昂贵且不实用，需要开发仅使用廉价流量测量的替代方案。

Method: 采用两阶段训练策略：首先在合成数据和低精度机理模型推导的物理方程上预训练PINN，然后使用少量实验数据进行微调。将可微分的PINN作为预测模型集成到扩展卡尔曼滤波器框架中，实现从流量测量跟踪和更新相高度。

Result: 在所有评估中，两阶段训练的PINN在相高度估计方面表现出最高的准确性，优于机理模型、非预训练PINN以及两阶段训练的纯数据驱动神经网络。

Conclusion: 该方法成功实现了仅通过流量测量准确估计重力沉降器中的相高度，为工业过程监测提供了经济实用的解决方案，同时展示了物理信息与数据驱动方法结合的优势。

Abstract: Separating liquid-liquid dispersions in gravity settlers is critical in chemical, pharmaceutical, and recycling processes. The dense-packed zone height is an important performance and safety indicator but it is often expensive and impractical to measure due to optical limitations. We propose to estimate phase heights using only inexpensive volume flow measurements. To this end, a physics-informed neural network (PINN) is first pretrained on synthetic data and physics equations derived from a low-fidelity (approximate) mechanistic model to reduce the need for extensive experimental data. While the mechanistic model is used to generate synthetic training data, only volume balance equations are used in the PINN, since the integration of submodels describing droplet coalescence and sedimentation into the PINN would be computationally prohibitive. The pretrained PINN is then fine-tuned with scarce experimental data to capture the actual dynamics of the separator. We then employ the differentiable PINN as a predictive model in an Extended Kalman Filter inspired state estimation framework, enabling the phase heights to be tracked and updated from flow-rate measurements. We first test the two-stage trained PINN by forward simulation from a known initial state against the mechanistic model and a non-pretrained PINN. We then evaluate phase height estimation performance with the filter, comparing the two-stage trained PINN with a two-stage trained purely data-driven neural network. All model types are trained and evaluated using ensembles to account for model parameter uncertainty. In all evaluations, the two-stage trained PINN yields the most accurate phase-height estimates.

</details>


### [310] [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 提出Superlinear attention，一种完全可训练的多步注意力架构，在保持随机上下文访问的同时实现长序列的次二次复杂度


<details>
  <summary>Details</summary>
Motivation: 解决标准自注意力在长序列上的二次复杂度问题，同时保持结构非排除特性（所有符合条件的token位置都不会被结构性地排除在注意力选择之外）

Method: 将标准因果自注意力重新表述为N步搜索问题，实现O(L^{1+1/N})复杂度。以N=2为例，第一步进行O(L^{3/2})跨度搜索选择相关序列段，第二步在选定段上应用O(L^{3/2})跨度注意力

Result: 在O(L^{1.54})配置下，在单个B200 GPU上实现1M上下文长度114 tokens/sec和10M上下文80 tokens/sec的解码吞吐量。在256K上下文长度的NIAH任务上表现出色

Conclusion: Superlinear attention在保持随机上下文访问的同时实现了次二次复杂度，展示了架构可行性、扩展分析和系统实现，为长上下文处理提供了有前景的解决方案

Abstract: In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.

</details>


### [311] [Frequency-Based Hyperparameter Selection in Games](https://arxiv.org/abs/2601.18409)
*Aniket Sanyal,Baraah A. M. Sidahmed,Rebekka Burkholz,Tatjana Chavdarova*

Main category: cs.LG

TL;DR: 提出Modal LookAhead (MoLA)方法，通过频率估计自适应选择超参数，加速平滑博弈中的训练


<details>
  <summary>Details</summary>
Motivation: 平滑博弈中的学习与标准最小化问题有本质区别，旋转动力学使经典超参数调优策略失效。尽管超参数调优在博弈中具有实际重要性，但有效方法仍未被充分探索。LookAhead方法虽然表现出色，但引入了关键影响性能的额外参数。

Method: 利用振荡动力学的频率估计进行超参数选择。分析连续时间轨迹中的振荡以及离散动力学在频率空间中的频谱。基于此分析提出Modal LookAhead (MoLA)，这是LA的扩展，能够根据给定问题自适应选择超参数。

Result: 提供了收敛保证，实验表明MoLA在纯旋转博弈和混合机制中都能加速训练，且计算开销最小。

Conclusion: MoLA为博弈中的超参数选择提供了原则性方法，通过频率自适应机制有效解决了旋转动力学带来的挑战，在保持计算效率的同时显著提升训练性能。

Abstract: Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.

</details>


### [312] [Gradient Regularized Natural Gradients](https://arxiv.org/abs/2601.18420)
*Satya Prakash Dash,Hossein Abdi,Wei Pan,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出了GRNG（梯度正则化自然梯度）优化器，将显式梯度正则化与自然梯度更新结合，包含频域和贝叶斯两种变体，在优化速度和泛化性能上优于一阶和二阶基线方法。


<details>
  <summary>Details</summary>
Motivation: 虽然梯度正则化（GR）已被证明能提高训练模型的泛化能力，自然梯度下降（NGD）在训练初期能加速优化，但二阶优化器的训练动态如何从梯度正则化中受益尚未得到充分研究。

Method: 提出了GRNG优化器家族，包含两种互补算法：1）频域变体通过结构化近似避免显式Fisher信息矩阵（FIM）求逆；2）贝叶斯变体基于正则化-卡尔曼公式，完全无需FIM求逆。

Result: 为GRNG建立了收敛保证，证明梯度正则化提高了稳定性并能够收敛到全局最小值。在视觉和语言基准测试中，GRNG在优化速度和泛化性能上一致优于一阶方法（SGD、AdamW）和二阶基线（K-FAC、Sophia）。

Conclusion: 梯度正则化是解锁自然梯度方法在大规模深度学习中的鲁棒性的原则性和实用工具，GRNG框架为二阶优化提供了新的有效途径。

Abstract: Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.

</details>


### [313] [GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level](https://arxiv.org/abs/2601.18447)
*Jinlong Hu,Jiacheng Liu*

Main category: cs.LG

TL;DR: GCFX：基于深度图生成的模型级反事实解释方法，通过增强的图生成框架和全局总结算法提供高质量反事实解释，提升图学习模型的可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 深度图学习模型虽然处理图结构数据能力强，但内部架构复杂且缺乏透明度，导致决策难以解释，用户难以理解和信任这些"黑箱"模型。

Method: 提出GCFX方法，基于深度图生成的反事实解释框架。采用双编码器、结构感知标记器和消息传递神经网络解码器的架构，准确学习输入数据的真实潜在分布，生成高质量相关反事实示例。然后通过全局反事实总结算法从众多候选解释中选择最具代表性和全面的解释。

Result: 在合成数据集和多个真实世界数据集上的实验表明，GCFX在反事实有效性和覆盖范围方面优于现有方法，同时保持较低的解释成本。

Conclusion: GCFX为增强全局反事实解释的实用性和可信度提供了关键支持，有助于提高深度图学习模型的可解释性和用户信任度。

Abstract: Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.

</details>


### [314] [Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States](https://arxiv.org/abs/2601.18479)
*Kyoleen Kwak,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 提出ASAP方法，通过过渡诱导相似状态和对齐动作来平滑深度强化学习中的高频振荡，提升控制稳定性


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在控制任务中表现出色，但其固有的高频振荡问题限制了在实际环境中的应用。现有方法通常依赖启发式或合成的状态相似性定义来促进动作一致性，但这些定义往往无法准确反映底层系统动态。

Method: 提出ASAP方法：1）引入过渡诱导相似状态，定义为从前一状态过渡到的下一个状态的分布；2）通过将当前动作与过渡诱导相似状态中的动作对齐来强制动作平滑性；3）惩罚二阶差异以抑制高频振荡。该方法仅使用环境反馈和实际收集的数据，能更好地捕捉系统动态。

Result: 在Gymnasium和Isaac-Lab环境中的实验表明，ASAP相比现有方法能产生更平滑的控制和更好的策略性能。

Conclusion: ASAP通过过渡诱导相似状态有效解决了深度强化学习中的动作振荡问题，提供了一种更准确反映系统动态的动作平滑方法，在实际应用中具有更好的控制稳定性。

Abstract: Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.

</details>


### [315] [Nearly Optimal Bayesian Inference for Structural Missingness](https://arxiv.org/abs/2601.18500)
*Chen Liang,Donghua Yang,Yutong Wang,Tianle Zhang,Shenghe Zhou,Zhiyu Liang,Hengtong Zhang,Hongzhi Wang,Ziqi Li,Xiyang Zhang,Zheng Liang,Yifei Li*

Main category: cs.LG

TL;DR: 论文提出贝叶斯框架处理结构性缺失数据，通过解耦缺失值后验学习与标签预测，实现不确定性传播，在多个基准测试中达到SOTA


<details>
  <summary>Details</summary>
Motivation: 结构性缺失数据存在因果循环困境：预测需要缺失特征，但推断它们又依赖于缺失机制；MNAR下缺失部分可能来自分布偏移；单一插值会锁定不确定性导致过度自信的决策

Method: 采用贝叶斯视角，通过后验预测分布进行预测，解耦为两个步骤：(1)学习模型内缺失值的后验分布，(2)通过优化预测后验分布进行标签预测，实现后验积分

Result: 在43个分类基准和15个插补基准上达到SOTA，在SCM先验下具有有限样本近贝叶斯最优性保证

Conclusion: 该框架提供"几乎免费午餐"：一旦学习到后验，预测即插即用同时保持不确定性传播，有效解决结构性缺失数据的挑战

Abstract: Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.

</details>


### [316] [Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark](https://arxiv.org/abs/2601.18509)
*Andro Sabashvili*

Main category: cs.LG

TL;DR: 本文综述了时间序列预测中保形预测方法面临的挑战及解决方案，重点讨论了如何处理时间依赖性违反数据可交换性假设的问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要可靠的不确定性量化，但传统方法依赖限制性分布假设。保形预测作为无分布框架具有理论保证，但时间序列的时序依赖性违反了保形预测的核心可交换性假设，需要解决这一矛盾。

Method: 本文系统综述了四类算法解决方案：1）放宽可交换性假设的方法；2）重新定义数据单元为独立时间序列集合的方法；3）显式建模预测残差动态的方法；4）适应分布漂移以维持长期覆盖的在线学习算法。

Result: 通过综合比较这些方法，本文评估了它们在真实世界数据上的计算效率和实际性能，为时间序列保形预测提供了全面的算法基准。

Conclusion: 时间序列保形预测面临可交换性假设违反的挑战，但通过多种算法策略可以克服这一限制，为时间序列不确定性量化提供了有前景的无分布框架。

Abstract: Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.

</details>


### [317] [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510)
*Yibo Li,Zijie Lin,Ailin Deng,Xuan Zhang,Yufei He,Shuo Ji,Tri Cao,Bryan Hooi*

Main category: cs.LG

TL;DR: JitRL是一个无需训练、基于即时经验记忆的强化学习框架，通过在测试时检索相关轨迹估计动作优势值来直接调整LLM输出，实现持续适应而无需梯度更新。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在部署后权重固定，难以持续适应新任务。传统强化学习虽然能解决这个问题，但计算成本过高且存在灾难性遗忘风险。

Method: JitRL维护动态非参数化经验记忆，在测试时检索相关轨迹来实时估计动作优势值，然后使用这些估计直接调整LLM的输出logits。该方法在理论上被证明是KL约束策略优化目标的精确闭式解。

Result: 在WebArena和Jericho基准测试中，JitRL在无需训练的方法中达到最先进水平，甚至超越了计算昂贵的微调方法（如WebRL），同时将成本降低了30倍以上。

Conclusion: JitRL为持续学习代理提供了一个可扩展的路径，能够在无需梯度更新的情况下实现测试时策略优化，显著降低计算成本同时保持高性能。

Abstract: While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.

</details>


### [318] [LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models](https://arxiv.org/abs/2601.18513)
*Kai Hu,Haoqi Hu,Matt Fredrikson*

Main category: cs.LG

TL;DR: LipNeXt：首个无约束、无卷积的1-Lipschitz架构，通过流形优化和空间移位模块实现高效确定性鲁棒性认证，在ImageNet上扩展到10亿参数规模。


<details>
  <summary>Details</summary>
Motivation: 基于Lipschitz的认证方法虽然能提供高效的确定性鲁棒性保证，但在模型规模、训练效率和ImageNet性能方面一直难以扩展。现有方法受到约束和卷积操作的限制。

Method: 提出两种关键技术：1）直接在正交流形上更新参数的流形优化过程；2）无需卷积即可建模空间模式的空间移位模块。网络使用正交投影、空间移位、β-Abs非线性激活和L2空间池化来保持严格的Lipschitz控制。

Result: 在CIFAR-10/100和Tiny-ImageNet上达到最先进的干净准确率和认证鲁棒准确率。在ImageNet上扩展到10-20亿参数的大模型，相比之前的Lipschitz模型将认证鲁棒准确率提高了最多8%（在ε=1时），同时保持高效稳定的低精度训练。

Conclusion: Lipschitz-based认证方法可以受益于现代扩展趋势，而无需牺牲确定性或效率，为大规模确定性鲁棒性认证开辟了新途径。

Abstract: Lipschitz-based certification offers efficient, deterministic robustness guarantees but has struggled to scale in model size, training efficiency, and ImageNet performance. We introduce \emph{LipNeXt}, the first \emph{constraint-free} and \emph{convolution-free} 1-Lipschitz architecture for certified robustness. LipNeXt is built using two techniques: (1) a manifold optimization procedure that updates parameters directly on the orthogonal manifold and (2) a \emph{Spatial Shift Module} to model spatial pattern without convolutions. The full network uses orthogonal projections, spatial shifts, a simple 1-Lipschitz $β$-Abs nonlinearity, and $L_2$ spatial pooling to maintain tight Lipschitz control while enabling expressive feature mixing. Across CIFAR-10/100 and Tiny-ImageNet, LipNeXt achieves state-of-the-art clean and certified robust accuracy (CRA), and on ImageNet it scales to 1-2B large models, improving CRA over prior Lipschitz models (e.g., up to $+8\%$ at $\varepsilon{=}1$) while retaining efficient, stable low-precision training. These results demonstrate that Lipschitz-based certification can benefit from modern scaling trends without sacrificing determinism or efficiency.

</details>


### [319] [Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning](https://arxiv.org/abs/2601.18521)
*Emna Boudabbous,Mohamed Karaa,Lokman Sboui,Julio Montecinos,Omar Alam*

Main category: cs.LG

TL;DR: 提出一个城市级公交延误预测框架，通过多分辨率特征工程、降维和深度学习，实现实时、可扩展的延误预测，在蒙特利尔公交网络上验证效果优于Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 城市公交机构需要可靠的网络级延误预测，为乘客提供准确到站信息并支持实时运营控制。现有系统大多只处理少数线路，依赖手工特征，缺乏可扩展、可复用的架构指导。

Method: 提出城市级预测流水线：1) 多分辨率特征工程，在H3单元、线路、路段和时间模式上生成1683个时空特征；2) 自适应PCA降维至83个组件；3) 混合H3+拓扑聚类解决"巨型集群"问题，得到12个平衡线路集群；4) 比较五种模型架构，全局LSTM+集群感知特征表现最佳。

Result: 全局LSTM模型在准确性和效率间取得最佳平衡，比Transformer模型性能提升18-52%，参数量减少275倍。通过蒙特利尔公交网络6个月数据验证，在路段、行程等多层次评估中表现良好，适合实时城市级部署。

Conclusion: 提出的预测流水线适合实时、城市级部署，可复用于其他网络且只需有限适配。解决了现有系统的可扩展性问题，为公交延误预测提供了系统化框架。

Abstract: Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture.
  We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the "giant cluster" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training.
  We compare five model architectures on six months of bus operations from the Société de transport de Montréal (STM) network in Montréal. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.

</details>


### [320] [From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale](https://arxiv.org/abs/2601.18524)
*Yongqi Jin,Yecheng Wang,Jun-jie Wang,Rong Zhu,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: 提出半监督框架，利用文献提取的数百万未标记NMR谱图学习化学位移预测，无需原子级标注，显著提升预测精度和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有NMR化学位移预测方法依赖有限的人工标注数据集，标注成本高且规模受限，需要利用大规模未标记谱图数据提升模型性能

Method: 提出半监督框架，将文献提取的未标记谱图与少量标注数据结合；将化学位移预测建模为置换不变集合监督问题，在特定损失函数条件下，最优二分匹配简化为基于排序的损失，实现稳定的大规模半监督训练

Result: 模型在精度和鲁棒性上显著超越现有方法，在更大更多样的分子数据集上表现出更强的泛化能力；首次在大规模上捕捉常见NMR溶剂的系统性溶剂效应

Conclusion: 文献提取的大规模未标记谱图可作为训练NMR位移模型的有效数据源，表明文献衍生的弱结构化数据在科学数据中心的AI中具有更广泛的应用潜力

Abstract: Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.

</details>


### [321] [Closing the Modality Gap Aligns Group-Wise Semantics](https://arxiv.org/abs/2601.18525)
*Eleonora Grassucci,Giordano Cicchetti,Emanuele Frasca,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: 论文提出模态间隙（modality gap）对群体级任务（如聚类）影响显著，而传统实例级任务（如检索）影响有限。作者开发了一种减少模态间隙的方法，证明缩小间隙能显著提升群体级任务性能。


<details>
  <summary>Details</summary>
Motivation: CLIP等跨模态学习方法虽然能在语义层面对齐不同模态，但生成的潜在空间往往只是部分共享，存在结构不匹配的模态间隙现象。现有研究对是否需要解决这一现象存在争议，特别是考虑到它对实例级任务影响有限。本文旨在证明模态间隙对群体级任务有显著影响。

Method: 提出了一种新颖的方法，专门用于在双模态设置中一致地减少模态间隙，并可以简单扩展到一般的n模态情况。该方法通过优化潜在空间结构来缩小不同模态表示之间的差距。

Result: 广泛评估表明：减少模态间隙对传统实例级任务仅带来边际或不一致的改进，但对群体级任务（如聚类）能显著提升性能。这一发现揭示了模态间隙在需要语义分组的任务中的关键作用。

Conclusion: 模态间隙对群体级任务的影响远比实例级任务显著，解决这一间隙能大幅提升需要语义分组的任务性能。这一发现可能重塑我们对模态间隙的理解，强调其在改善群体级任务中的关键作用。

Abstract: In multimodal learning, CLIP has been recognized as the \textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.

</details>


### [322] [Information Hidden in Gradients of Regression with Target Noise](https://arxiv.org/abs/2601.18546)
*Arash Jamshidi,Katsiaryna Haitsiukevich,Kai Puolamäki*

Main category: cs.LG

TL;DR: 论文提出一种通过梯度恢复二阶信息的方法：通过注入高斯噪声使目标噪声方差等于批量大小，从而让经验梯度协方差近似于Hessian矩阵，即使在远离最优解时也有效。


<details>
  <summary>Details</summary>
Motivation: 在许多现代机器学习场景中，只能观察到梯度信息，而二阶信息（如曲率或数据协方差）对于优化、诊断和鲁棒性至关重要。需要一种仅从梯度中恢复Hessian矩阵的方法。

Method: 提出方差校准方法：注入高斯噪声，使总目标噪声方差等于批量大小n。这种校准确保经验梯度协方差能够紧密近似Hessian矩阵，即使在远离最优解时也有效。方法基于"将目标噪声方差设为n"的简单规则。

Result: 在亚高斯输入下提供了非渐近算子范数保证。证明如果没有这种校准，恢复可能失败Ω(1)因子。方法具有实用性（简单规则）和鲁棒性（方差O(n)足以恢复Σ到尺度）。实验验证了理论结果。

Conclusion: 仅通过梯度就能恢复二阶信息，为优化预条件、对抗风险估计和分布式系统中的梯度训练等应用提供了新工具。提出的方差校准方法是实用且鲁棒的解决方案。

Abstract: Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $Σ$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $Ω(1)$ factor. The proposed method is practical (a "set target-noise variance to $n$" rule) and robust (variance $\mathcal{O}(n)$ suffices to recover $Σ$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.

</details>


### [323] [An Unsupervised Tensor-Based Domain Alignment](https://arxiv.org/abs/2601.18564)
*Chong Hyun Lee,Kibae Lee,Hyun Hee Yim*

Main category: cs.LG

TL;DR: 提出基于张量的域对齐算法，通过对齐矩阵在不变子空间中对齐源和目标张量，使用斜流形约束提供比传统Stiefel流形更大的灵活性，并通过正则化项保持方差，提升域适应性能。


<details>
  <summary>Details</summary>
Motivation: 传统张量域对齐方法通常使用Stiefel流形约束，限制了方法的灵活性和适应性。需要开发更灵活的框架，能够更好地处理复杂域适应任务，同时提高转换速度和分类精度。

Method: 提出基于张量的域对齐算法，使用对齐矩阵在不变子空间中对齐源和目标张量。采用斜流形约束进行迭代优化，比传统Stiefel流形更灵活。引入正则化项保持源和目标张量的方差，确保鲁棒性。框架具有通用性，可将现有张量域对齐方法作为特例。

Result: 实验表明，该方法不仅显著提高了域适应转换速度，还大幅提升了分类准确率。在性能上优于当前最先进的技术，成为复杂域适应任务的优选方案。

Conclusion: 提出的基于张量的域对齐算法通过斜流形约束和方差保持正则化，提供了更灵活和鲁棒的域适应解决方案，在速度和精度上都优于现有方法，为复杂域适应任务提供了有效工具。

Abstract: We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.

</details>


### [324] [K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents](https://arxiv.org/abs/2601.18580)
*Vincenzo De Paola,Mirco Mutti,Riccardo Zamboni,Marcello Restelli*

Main category: cs.LG

TL;DR: K-Myriad是一种可扩展的无监督方法，通过最大化并行策略群体诱导的集体状态熵来促进多样化探索，为强化学习提供鲁棒的初始化，提高训练效率并发现异构解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习并行化通常使用相同采样分布收集经验，限制了并行化潜力，忽视了多样化探索策略的优势。需要一种能够最大化集体探索多样性的方法。

Method: 提出K-Myriad方法，通过最大化并行策略群体诱导的集体状态熵来培养专业化的探索策略组合，实现无监督的多样化探索。

Result: 在高维连续控制任务的大规模并行化实验中，K-Myriad能够学习到广泛的不同策略，证明了其在集体探索方面的有效性。

Conclusion: K-Myriad通过最大化集体状态熵实现了有效的多样化探索，为强化学习提供了鲁棒的初始化，提高了训练效率并发现了异构解决方案，为新型并行化策略开辟了道路。

Abstract: Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.

</details>


### [325] [Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning](https://arxiv.org/abs/2601.18586)
*Miguel Costa,Arthur Vandervoort,Carolin Schmidt,Morten W. Petersen,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 提出一个结合集成评估模型与强化学习的决策支持框架，用于学习城市交通系统在气候变化下的自适应多十年投资路径


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧降雨等灾害，增加城市交通系统中断。设计有效适应策略面临挑战：基础设施投资长期性、深度不确定性、跨部门复杂交互

Method: 提出通用决策支持框架：耦合集成评估模型与强化学习，结合长期气候预测、灾害概率映射、基础设施影响传播、社会成本评估，在强化学习循环中学习自适应气候适应策略

Result: 与哥本哈根市政府合作，应用于2024-2100年内城暴雨洪水案例。学习策略产生协调的时空路径，相比传统优化基准（不作为和随机行动）具有改进的鲁棒性

Conclusion: 该框架展示了可转移到其他灾害和城市的潜力，为城市气候适应提供自适应决策支持

Abstract: Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.

</details>


### [326] [LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation](https://arxiv.org/abs/2601.18604)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: LaCoGSEA是一种无监督通路富集分析框架，结合深度表示学习和通路统计，无需表型标签即可捕获非线性转录组结构并进行通路解释。


<details>
  <summary>Details</summary>
Motivation: 现有通路富集分析方法（如GSEA）需要预定义表型标签和成对比较，限制了在无监督场景下的应用。现有的无监督扩展方法主要捕获线性关系，无法显式建模基因-通路关联。深度学习模型虽能捕获非线性结构，但其解释依赖于通用的可解释AI技术，这些技术并非为无监督转录组分析中的通路级解释而设计。

Method: LaCoGSEA采用自编码器捕获非线性流形，提出全局基因-潜在相关性度量作为差异表达的代理，无需先验标签即可生成密集的基因排序。该框架将深度表示学习与稳健的通路统计相结合。

Result: LaCoGSEA在三个方面表现出色：(1) 在区分癌症亚型方面，比现有无监督基线方法获得更好的聚类性能；(2) 与线性降维和基于梯度的XAI方法相比，在更高排名上恢复更广泛的生物学意义通路；(3) 在不同实验方案和数据集规模下保持高稳健性和一致性。

Conclusion: LaCoGSEA在无监督通路富集分析中提供了最先进的性能，填补了深度表示学习与通路级解释之间的空白，为无监督转录组分析提供了有效的工具。

Abstract: Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.
  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.
  Availability and implementation: https://github.com/willyzzz/LaCoGSEA

</details>


### [327] [Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem](https://arxiv.org/abs/2601.18615)
*Ramiro Valdes Jara,Adam Meyers*

Main category: cs.LG

TL;DR: 提出基于条件扩散模型的数据驱动方法解决心电图成像逆问题，无需几何建模，可概率性采样多个重建结果


<details>
  <summary>Details</summary>
Motivation: 传统心电图成像逆问题存在非唯一性和欠定性，传统方法需要患者特异性网格构建且只能提供确定性估计

Method: 采用条件扩散框架学习从噪声体表信号到心表电位的概率映射，无需几何建模，纯数据驱动

Result: 在真实ECGI数据集上优于卷积神经网络、LSTM和Transformer等确定性基线模型，重建精度更高

Conclusion: 扩散模型可作为稳健的非侵入性心脏电生理成像工具，能处理逆问题的非唯一性并提供概率性重建

Abstract: This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.

</details>


### [328] [CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling](https://arxiv.org/abs/2601.18620)
*Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Ian Berlot-Attwell,Stéphane Aroca-Ouellette,Kaheer Suleman*

Main category: cs.LG

TL;DR: CASSANDRA：一种神经符号世界建模方法，利用LLM作为知识先验构建轻量级规划用转移模型，在商业模拟中显著优于基线


<details>
  <summary>Details</summary>
Motivation: 现实世界领域（如商业）具有丰富的语义，可以利用世界知识从有限数据中有效建模复杂的动作效果和因果关系

Method: CASSANDRA整合两个组件：1) LLM合成代码建模确定性特征；2) LLM引导的概率图模型结构学习捕获随机变量间的因果关系

Result: 在小规模咖啡店模拟器和复杂主题公园商业模拟器中，CASSANDRA在转移预测和规划方面相比基线有显著改进

Conclusion: CASSANDRA通过结合LLM的知识先验和神经符号方法，能够有效构建轻量级世界模型，在复杂商业领域规划中表现优异

Abstract: Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.

</details>


### [329] [Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning](https://arxiv.org/abs/2601.18699)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: 论文对Transformer大语言模型在连续微调中的灾难性遗忘现象进行了机制分析，发现梯度干扰、表征漂移和损失景观平坦化是主要驱动因素，遗忘程度与任务相似度高度相关。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在预训练和微调中表现出色，但连续微调会导致灾难性遗忘，新学知识干扰已有能力。虽然这种现象被广泛观察到，但其机制理解仍然有限。

Method: 通过对多个模型规模（109B到400B参数）和任务序列进行系统实验，分析Transformer LLMs在连续微调中的灾难性遗忘机制，包括梯度干扰、表征漂移和损失景观平坦化。

Result: 发现遗忘严重程度与任务相似度高度相关（Pearson r = 0.87），约15-23%的注意力头在微调过程中受到严重干扰，下层网络表现出更大的易感性。

Conclusion: 这些发现为开发持续学习系统中的针对性缓解策略建立了机制基础，揭示了灾难性遗忘的具体驱动因素和模式。

Abstract: Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.

</details>


### [330] [Physics-Informed Uncertainty Enables Reliable AI-driven Design](https://arxiv.org/abs/2601.18638)
*Tingkai Xue,Chin Chun Ooi,Yang Jiang,Luu Trung Pham Duong,Pao-Hsiung Chiu,Weijiang Zhao,Nagarajan Raghavan,My Ha Dao*

Main category: cs.LG

TL;DR: 提出物理信息不确定性作为预测不确定性的代理，用于频率选择表面逆设计，将高性能解决方案发现率从<10%提升至>50%，同时降低计算成本一个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的代理辅助优化方法通常缺乏不确定性量化，导致在数据稀疏区域预测错误，影响优化性能。需要一种有效且计算成本低的不确定性量化方法来改进逆设计过程。

Method: 引入物理信息不确定性范式：利用模型预测违反基本物理定律的程度作为预测不确定性的计算廉价且有效的代理。将其集成到多保真度不确定性感知优化工作流中，用于设计20-30GHz范围内的复杂频率选择表面。

Result: 将高性能解决方案发现率从不到10%提高到超过50%，同时与单独使用高保真求解器相比，计算成本降低了一个数量级。

Conclusion: 物理信息不确定性是量化物理系统代理模型不确定性的可行替代方案，为能够高效、稳健探索和评估候选设计的自主科学发现系统奠定了基础。

Abstract: Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs.

</details>


### [331] [Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models](https://arxiv.org/abs/2601.18734)
*Siyan Zhao,Zhihui Xie,Mengchen Liu,Jing Huang,Guan Pang,Feiyu Chen,Aditya Grover*

Main category: cs.LG

TL;DR: OPSD是一种单模型自蒸馏框架，通过让同一模型在不同上下文条件下分别作为教师和学生，利用特权信息进行自我教学，提高推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法存在分布不匹配问题，且需要独立的教师模型，无法充分利用推理数据集中的真实解决方案信息。

Method: 提出On-Policy Self-Distillation框架，让单个模型在不同上下文条件下分别作为教师和学生：教师策略基于特权信息（如已验证的推理轨迹），学生策略仅看到问题，通过最小化学生自身轨迹上的逐token分布差异进行训练。

Result: 在多个数学推理基准测试中，相比GRPO等强化学习方法实现了4-8倍的token效率提升，性能优于离策略蒸馏方法。

Conclusion: OPSD框架通过单模型自蒸馏有效解决了传统蒸馏方法的局限性，实现了高效的知识传递和推理能力提升。

Abstract: Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.

</details>


### [332] [TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning](https://arxiv.org/abs/2601.18640)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: TwinPurify是一个基于Barlow Twins自监督学习的表示学习框架，用于从批量转录组数据中提取肿瘤特异性信号，无需外部参考，通过利用同一队列中的相邻正常组织作为背景指导来解耦肿瘤特异性信号。


<details>
  <summary>Details</summary>
Motivation: 大规模患者队列研究仍依赖批量转录组数据，但肿瘤纯度变化会掩盖肿瘤内在转录信号并限制下游发现。现有的去卷积方法在合成混合物上表现良好，但由于未建模的生物和技术变异，无法泛化到真实患者队列。

Method: TwinPurify采用Barlow Twins自监督目标，通过利用同一队列中的相邻正常组织作为"背景"指导，学习连续的高维肿瘤嵌入，而不是将批量混合物解析为离散的细胞类型分数。这种方法无需依赖任何外部参考。

Result: 在多个大型癌症队列（RNA-seq和微阵列平台）的基准测试中，TwinPurify在恢复肿瘤内在和免疫信号方面优于传统的表示学习方法（如自编码器）。纯化的嵌入改进了分子亚型和分级分类，增强了生存模型一致性，并揭示了与原始批量谱相比更具生物学意义的通路活性。

Conclusion: TwinPurify通过提供一个可转移的框架来净化批量转录组学，扩展了现有临床数据集在分子发现中的实用性，为肿瘤特异性信号提取提供了一种新方法。

Abstract: Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.
  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as "background" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.
  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.

</details>


### [333] [Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values](https://arxiv.org/abs/2601.18760)
*Henry Bell,Lara Neubauer da Costa Schertel,Bochu Ding,Brandon Fain*

Main category: cs.LG

TL;DR: 提出Grounded Constitutional AI框架，结合用户一般期望和交互时偏好生成更具代表性、道德基础更强的AI宪法原则


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐框架中的宪法原则难以公平确定并获得广泛利益相关者认同，需要更全面代表用户期望和偏好的方法

Method: 扩展Inverse Constitutional AI方法，利用人类偏好标注数据中的理由生成情境原则，并结合用户关于AI的价值陈述生成一般原则

Result: GCAI生成的宪法比ICAI生成的更受人类青睐，被认为更具道德基础、连贯性和多元性，既适合个人使用也适合广泛治理AI行为

Conclusion: GCAI框架能生成更全面代表用户期望的AI宪法原则，为LLM对齐提供了更公平、更具代表性的方法

Abstract: A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.

</details>


### [334] [FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning](https://arxiv.org/abs/2601.18650)
*Liheng Yu,Zhe Zhao,Yuxuan Wang,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 本文首次研究长尾分布下的机器遗忘问题，发现现有方法存在异质遗忘偏差和倾斜遗忘偏差，提出FaLW动态损失重加权方法解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘研究主要评估相对平衡的遗忘集，忽视了现实世界中数据通常遵循长尾分布（如用户活动记录）的情况。本文旨在填补这一重要研究空白。

Method: 提出FaLW方法，这是一种即插即用的实例级动态损失重加权方法。通过比较每个样本的预测概率与同类别未见数据的分布来评估其遗忘状态，然后使用由平衡因子调节的遗忘感知重加权方案，自适应调整每个样本的遗忘强度。

Result: 大量实验表明，FaLW在长尾分布设置下实现了优越的性能，有效解决了现有方法存在的异质遗忘偏差和倾斜遗忘偏差问题。

Conclusion: 本文首次系统研究了长尾分布下的机器遗忘问题，提出的FaLW方法通过动态损失重加权机制，显著提升了在现实长尾场景下的遗忘效果，为机器遗忘领域提供了重要进展。

Abstract: Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \textit{Heterogeneous Unlearning Deviation} and \textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \textbf{Supplementary Material}.

</details>


### [335] [PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation](https://arxiv.org/abs/2601.18777)
*Abhishek Divekar,Anirban Majumder*

Main category: cs.LG

TL;DR: 提出PRECISE框架，结合少量人工标注和LLM判断，显著减少检索系统评估所需的人工标注量，同时校正LLM偏见


<details>
  <summary>Details</summary>
Motivation: 传统搜索、排序和RAG系统评估需要大量人工相关性标注，成本高昂。虽然LLM可作为自动评估工具，但其固有偏见限制了直接用于指标估计

Method: 扩展预测驱动推理(PPI)框架，结合少量人工标注查询(100个)和大量未标注样本(10,000个)，将指标集成空间重构，将计算复杂度从O(2^|C|)降至O(2^K)

Result: 在多个检索数据集上验证，PRECISE框架显著降低了Precision@K等关键业务指标的估计方差，在低资源设置下有效校正LLM偏见

Conclusion: PRECISE框架通过统计方法结合LLM判断和少量人工标注，为检索系统评估提供了一种高效可靠的解决方案，大幅降低了标注成本

Abstract: Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.

</details>


### [336] [A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.18672)
*Spyros Rigas,Thanasis Papaioannou,Panagiotis Trakadas,Georgios Alexandridis*

Main category: cs.LG

TL;DR: 提出基于曲率的KAN网格自适应方法，通过重要性密度函数优化节点分配，相比传统输入密度方法显著提升精度


<details>
  <summary>Details</summary>
Motivation: 现有KAN网格自适应策略仅依赖输入数据密度，忽略了目标函数的几何复杂性和训练过程中的计算指标，需要更智能的网格分配方法

Method: 提出通用框架，将节点分配视为由重要性密度函数控制的密度估计任务，引入基于曲率的自适应策略，让训练动态决定网格分辨率

Result: 在合成函数拟合、Feynman数据集回归和Helmholtz PDE问题上，分别实现25.3%、9.4%和23.3%的平均相对误差降低，统计显著

Conclusion: 基于曲率的自适应策略是KAN训练中稳健且计算高效的替代方案，能更好地捕捉函数几何复杂性

Abstract: Kolmogorov-Arnold Networks (KANs) have recently demonstrated promising potential in scientific machine learning, partly due to their capacity for grid adaptation during training. However, existing adaptation strategies rely solely on input data density, failing to account for the geometric complexity of the target function or metrics calculated during network training. In this work, we propose a generalized framework that treats knot allocation as a density estimation task governed by Importance Density Functions (IDFs), allowing training dynamics to determine grid resolution. We introduce a curvature-based adaptation strategy and evaluate it across synthetic function fitting, regression on a subset of the Feynman dataset and different instances of the Helmholtz PDE, demonstrating that it significantly outperforms the standard input-based baseline. Specifically, our method yields average relative error reductions of 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on the PDE benchmark. Statistical significance is confirmed via Wilcoxon signed-rank tests, establishing curvature-based adaptation as a robust and computationally efficient alternative for KAN training.

</details>


### [337] [Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability](https://arxiv.org/abs/2601.18778)
*Shobhita Sundaram,John Quan,Ariel Kwiatkowski,Kartik Ahuja,Yann Ollivier,Julia Kempe*

Main category: cs.LG

TL;DR: SOAR是一个基于元强化学习的自改进框架，通过教师模型生成合成问题来训练学生模型，使用学生进步作为奖励信号，能够在初始成功率极低（0/128）的数学基准上实现学习突破。


<details>
  <summary>Details</summary>
Motivation: 当大型推理模型在初始成功率极低的数据集上进行微调时，由于训练信号稀疏，强化学习方法会陷入学习平台期。本研究旨在探索预训练LLM是否能够利用潜在知识为自身无法解决的问题生成自动化课程。

Method: 提出SOAR框架：使用教师模型副本为学生模型副本生成合成问题，教师根据学生在困难问题子集上的进步获得奖励。该方法基于实际学生进步而非内在代理奖励来构建课程，实现了双层元强化学习。

Result: 在数学基准的最难子集（初始成功率0/128）上：1）实现了基于稀疏二元奖励的双层元强化学习；2）基于实际进步的奖励优于先验内在奖励方案，避免了不稳定性和多样性崩溃；3）生成问题的结构质量和明确性比解决方案正确性对学习进步更重要。

Conclusion: 研究表明，生成有用"垫脚石"的能力并不需要预先具备解决困难问题的能力，这为无需额外策划数据就能突破推理平台期提供了一条原则性路径。

Abstract: Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.

</details>


### [338] [Learning temporal embeddings from electronic health records of chronic kidney disease patients](https://arxiv.org/abs/2601.18675)
*Aditya Kumar,Mario A. Cypko,Oliver Amft*

Main category: cs.LG

TL;DR: 研究验证了基于时间序列电子健康记录训练的嵌入模型能在保持预测性能的同时学习到有临床意义的表示，其中时间感知LSTM在嵌入质量和预测性能上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 模型引导的医学需要能够捕捉疾病动态且保持透明、任务无关的表示，而大多数临床预测模型仅针对单一任务优化。表示学习有助于学习能泛化到下游任务的嵌入，循环架构适合建模临床观测数据的时间结构。

Method: 使用MIMIC-IV数据集研究慢性肾病(CKD)患者，比较三种循环架构：普通LSTM、注意力增强LSTM和时间感知LSTM(T-LSTM)。所有模型既作为嵌入模型训练，也作为端到端预测器训练。通过CKD阶段聚类和ICU内死亡率预测评估嵌入质量。

Result: T-LSTM产生更有结构的嵌入，Davies-Bouldin指数更低(9.91 vs 15.85和20.72)，CKD阶段分类准确率更高(0.74 vs 0.63和0.67)。对于ICU内死亡率预测，嵌入模型始终优于端到端预测器，准确率从0.72-0.75提升到0.82-0.83。

Conclusion: 时间感知LSTM能学习到更有临床意义的表示而不损害预测性能，嵌入模型作为中间步骤比直接端到端学习更有效，为模型引导医学提供了有前景的方向。

Abstract: We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.

</details>


### [339] [POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration](https://arxiv.org/abs/2601.18779)
*Yuxiao Qu,Amrith Setlur,Virginia Smith,Ruslan Salakhutdinov,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出POPE方法，通过使用特权信息（如人类解答）引导强化学习在困难问题上的探索，解决现有方法在硬问题上探索失败的问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在训练LLMs时，对于困难问题经常无法探索到任何正确的rollout，导致零奖励信号和学习停滞。传统的探索增强方法（如熵奖励、重要性采样调整）效果有限，而混合难易问题的训练反而会产生"射线干扰"现象，阻碍硬问题的学习。

Method: POPE（特权在线探索）方法：利用人类或其他oracle解答作为特权信息，在困难问题前添加oracle解答的前缀，引导RL获得非零奖励。关键是通过指令跟随和推理的协同作用，使习得的行为能够迁移回原始未引导的问题。

Result: POPE显著扩展了可解决问题的范围，在具有挑战性的推理基准上大幅提升了性能，有效解决了硬问题上的探索失败问题。

Conclusion: POPE通过特权信息引导探索，成功解决了强化学习在困难推理问题上的探索瓶颈，提供了一种有效的方法来提升LLMs在复杂任务上的学习能力。

Abstract: Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.

</details>


### [340] [Quasi Monte Carlo methods enable extremely low-dimensional deep generative models](https://arxiv.org/abs/2601.18676)
*Miles Martinez,Alex H. Williams*

Main category: cs.LG

TL;DR: QLVMs是一种专门用于寻找高维数据集极低维可解释嵌入的深度生成模型，通过准蒙特卡洛积分直接近似边际似然，在1-3维潜空间中表现优于传统VAE和IWAE。


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器（VAE）和重要性加权自编码器（IWAE）在寻找低维可解释嵌入方面存在局限，需要更透明、可直接可视化的潜空间表示方法，以支持后验分析如密度估计、聚类和测地线计算。

Method: 提出准蒙特卡洛潜变量模型（QLVM），不依赖学习编码器和变分下界，而是通过随机化准蒙特卡洛积分直接近似边际似然，专门针对1-3维低维潜空间进行优化。

Result: 在多个数据集上，QLVMs在匹配潜维度的条件下持续优于传统VAE和IWAE，生成的嵌入支持透明可视化、非参数密度估计、聚类和测地线路径计算等后验分析。

Conclusion: QLVMs虽然计算密集且在复杂数据集上难以生成精细细节，但为优先考虑可解释性和潜空间分析的应用提供了有吸引力的解决方案，特别适合需要极低维透明嵌入的场景。

Abstract: This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.

</details>


### [341] [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/abs/2601.18795)
*Amrith Setlur,Zijian Wang,Andrew Cohen,Paria Rashidinejad,Sang Michael Xie*

Main category: cs.LG

TL;DR: PrefixRL：通过重用离策略轨迹的前缀来引导更高效的强化学习，避免离策略不稳定性，在困难推理问题上实现2倍训练加速和3倍最终奖励提升


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在困难推理问题上效率低下，因为正确策略轨迹稀少、策略梯度消失导致学习停滞。需要重用旧采样计算（来自先前推理或RL训练）的离策略轨迹来引导更高效的RL，但标准离策略方法存在优化不稳定性问题。

Method: 提出PrefixRL方法：在成功离策略轨迹的前缀上条件化，然后运行在策略RL来完成剩余部分，避免离策略不稳定性。通过调节前缀长度来调整问题难度，增强困难问题的学习信号。使用基础模型通过拒绝采样获取离策略轨迹，形成自我改进循环。

Result: 在困难推理问题上，PrefixRL达到相同训练奖励的速度比最强基线（在离策略数据上SFT然后RL）快2倍（即使考虑初始拒绝采样的计算开销），最终奖励提升3倍。增益可迁移到保留基准测试，即使离策略轨迹来自不同模型家族也有效。

Conclusion: PrefixRL通过重用离策略轨迹前缀有效解决了RL在困难推理问题上的效率瓶颈，避免了标准离策略方法的不稳定性，实现了显著的速度和性能提升，并具有实际应用的灵活性。

Abstract: Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.

</details>


### [342] [Counterfactual Explanations on Robust Perceptual Geodesics](https://arxiv.org/abs/2601.18678)
*Eslam Zaher,Maciej Trzaskowski,Quan Nguyen,Fred Roosta*

Main category: cs.LG

TL;DR: PCG提出了一种基于感知黎曼度量的反事实解释方法，通过追踪测地线生成语义有效的反事实，解决了现有方法中对抗性扰动和语义漂移的问题。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法存在距离度量选择模糊的问题，导致生成的扰动可能是语义有效的，也可能是对抗性的。现有方法采用平坦或不对齐的几何结构，导致离流形伪影、语义漂移或对抗性崩溃。

Method: 提出了感知反事实测地线（PCG）方法，使用从鲁棒视觉特征诱导的感知黎曼度量构建反事实，通过追踪测地线实现平滑、在流形上、语义有效的转换。

Result: 在三个视觉数据集上的实验表明，PCG优于基线方法，并揭示了在标准度量下隐藏的失败模式。

Conclusion: PCG通过感知对齐的几何结构解决了反事实解释中的距离度量模糊问题，能够生成语义有效的反事实解释，揭示了传统方法隐藏的模型失败模式。

Abstract: Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.

</details>


### [343] [Explainability Methods for Hardware Trojan Detection: A Systematic Comparison](https://arxiv.org/abs/2601.18696)
*Paul Whitten,Francis Wolff,Chris Papachristou*

Main category: cs.LG

TL;DR: 该研究比较了硬件木马检测中的三种可解释性方法：基于属性的分析、基于案例的推理和模型无关特征归因，发现前两种方法在领域对齐和基于先例的可解释性方面优于通用特征排名方法。


<details>
  <summary>Details</summary>
Motivation: 硬件木马检测需要准确识别和可解释的解释，以便安全工程师验证并采取行动。现有方法缺乏领域对齐和可验证的解释。

Method: 比较三种可解释性方法：1) 基于31个电路特定特征（门扇入模式、触发器距离、I/O连接性）的领域感知属性分析；2) 使用k近邻的基于案例推理；3) 模型无关特征归因（LIME、SHAP、梯度）。使用XGBoost分类器在Trust-Hub基准上进行评估。

Result: XGBoost在11,392个测试样本上达到46.15%精度和52.17%召回率，比先前工作（Hasegawa等：5.13%）精度提高9倍，误报率从5.6%降至0.25%。基于案例的推理实现97.4%的预测与训练样本对应性。LIME和SHAP特征归因具有强相关性（r=0.94），但缺乏电路级上下文。

Conclusion: 基于属性和基于案例的方法在领域对齐和基于先例的可解释性方面优于通用特征排名方法，对需要验证ML预测的实际部署具有重要启示。梯度归因比SHAP快481倍但提供相似的领域不透明见解。

Abstract: Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).
  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like "high fanin complexity near outputs indicates potential triggers." Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.
  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.
  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.

</details>


### [344] [From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic](https://arxiv.org/abs/2601.18702)
*Hansheng Ren*

Main category: cs.LG

TL;DR: 该论文挑战当前深度学习优先计算吞吐量而非数值精度的范式，提出精确性假说：通用智能需要任意精度算术计算基础，并引入基于有理数算术的Halo架构来减少大语言模型的逻辑不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习范式过于关注计算吞吐量而忽视数值精度，假设智能来自大规模统计相关性。作者认为这种近似计算导致大语言模型出现"幻觉"和逻辑不一致性，这是IEEE 754浮点数近似误差在深度组合函数中累积的结果。

Method: 提出精确性假说，认为通用智能需要任意精度算术计算基础。引入Halo架构，采用有理数算术（ℚ）作为计算范式，并设计新型精确推理单元（EIU）来支持这一架构。

Result: 在Huginn-0125原型上的实证验证显示，600B参数的BF16基线在混沌系统中崩溃，而Halo架构能够无限期保持零数值发散，证明了精确算术在减少逻辑不确定性方面的有效性。

Conclusion: 精确算术是减少系统2通用智能逻辑不确定性的先决条件，为通用智能的发展提供了新的计算基础方向。

Abstract: Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the "hallucinations" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.

</details>


### [345] [SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model](https://arxiv.org/abs/2601.18707)
*Jan Hagnberger,Mathias Niepert*

Main category: cs.LG

TL;DR: SMART是一种基于神经网络的代理模型，仅使用几何点云表示（无需仿真网格）即可预测任意查询位置的物理量，性能与依赖网格的方法相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的代理模型需要生成计算成本高的仿真网格，而无需网格的方法通常误差较高。需要开发一种既高效又准确的网格无关方法。

Method: SMART将几何和仿真参数编码到共享潜在空间，捕捉结构和参数特征。物理解码器通过跨层交互关注编码器的中间潜在表示，将空间查询映射到物理量，联合更新几何特征和物理场。

Result: 大量实验表明，SMART与依赖仿真网格作为输入的现有方法相比具有竞争力，且通常表现更优，展示了其在工业级仿真中的能力。

Conclusion: SMART提供了一种高效准确的网格无关替代方案，无需生成计算成本高的仿真网格，为复杂几何的物理仿真提供了实用的工业级解决方案。

Abstract: Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.

</details>


### [346] [Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift](https://arxiv.org/abs/2601.18736)
*Jake Lyon,Ehsan Saeedizade,Shamik Sengupta*

Main category: cs.LG

TL;DR: 该研究评估了四种监督学习模型在IoT恶意软件检测中的性能，发现树模型在数据有限时表现良好，但性能随时间推移而下降。


<details>
  <summary>Details</summary>
Motivation: 物联网设备因计算资源有限、物理防护薄弱、网络环境复杂而成为网络攻击的主要目标。机器学习为自动化恶意软件检测提供了可能，但需要既有效又轻量化的模型。

Method: 使用IoT-23数据集，评估四种监督学习模型（随机森林、LightGBM、逻辑回归和多层感知器）在二元和多元分类任务中的表现，分析训练数据量的敏感性以及时间鲁棒性。

Result: 树模型（随机森林和LightGBM）在准确率和泛化能力方面表现优异，即使在训练数据有限的情况下也能保持良好性能。但随着时间推移和恶意软件多样性增加，所有模型的性能都会下降。

Conclusion: 研究强调了自适应、资源高效的机器学习模型对于在现实环境中保护物联网系统的重要性，需要能够应对不断演变的威胁环境。

Abstract: The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.

</details>


### [347] [Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback](https://arxiv.org/abs/2601.18751)
*Seyed Amir Hosseini,Maryam Abdolali,Amirhosein Tavakkoli,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.LG

TL;DR: TriTrust-PBRL (TTP) 是一个从多专家偏好反馈中联合学习共享奖励模型和专家特定信任参数的框架，能够自动处理可靠、噪声和对抗性标注者，通过信任参数演化实现对抗偏好的反转而非丢弃。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的偏好数据通常来自异质标注者，包括可靠的、有噪声的和系统对抗性的。现有PBRL方法要么平等对待所有反馈，要么尝试过滤不可靠来源，但在面对系统提供错误偏好的对抗性标注者时都会失败。

Method: 提出TriTrust-PBRL (TTP)框架，联合学习共享奖励模型和专家特定信任参数。关键洞察是信任参数在基于梯度的优化中自然演化为正（信任）、接近零（忽略）或负（反转），使模型能够自动反转对抗性偏好并恢复有用信号。

Result: 在四个不同领域（MetaWorld操作任务和DM Control运动任务）的各种腐败场景下评估，TTP实现了最先进的鲁棒性，在对抗性腐败下保持接近oracle的性能，而标准PBRL方法则完全失败。TTP成功从包含可靠和对抗性标注者的混合专家池中学习。

Conclusion: TTP提供了一个统一的框架，能够有效处理异质标注者偏好数据，通过自动演化信任参数来适应不同专家类型，无需专家特征，仅需识别索引，并能无缝集成到现有PBRL流程中。

Abstract: Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.

</details>


### [348] [HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs](https://arxiv.org/abs/2601.18753)
*Xinyue Zeng,Junhong Lin,Yujun Yan,Feng Guo,Liang Shi,Jun Wu,Dawei Zhou*

Main category: cs.LG

TL;DR: 提出Hallucination Risk Bound理论框架，将幻觉风险分解为数据驱动和推理驱动两部分，并基于此开发HalluGuard检测方法，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗、法律等高风险领域的可靠性受到幻觉问题的严重影响。现有检测方法通常只针对单一幻觉来源，且依赖任务特定启发式方法，难以泛化到复杂场景。

Method: 1. 提出Hallucination Risk Bound理论框架，将幻觉风险形式化分解为数据驱动（训练时不匹配）和推理驱动（推理时不稳定性）两部分；2. 基于此开发HalluGuard检测方法，利用NTK的诱导几何和捕获表示来联合识别两种幻觉来源。

Result: 在10个多样化基准测试、11个竞争性基线方法和9个流行LLM骨干网络上评估，HalluGuard在检测多种形式LLM幻觉方面始终达到最先进的性能。

Conclusion: Hallucination Risk Bound提供了一个统一的理论框架来理解幻觉产生机制，而HalluGuard则提供了一个有效的检测工具，为提升LLM在高风险应用中的可靠性提供了重要支持。

Abstract: The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [349] [A Block-Alternating Iterative Approach for a Class of Non-Convex Optimization Problems](https://arxiv.org/abs/2601.17128)
*Anran Li,John P. Swensen,Mehdi Hosseinzadeh*

Main category: math.OC

TL;DR: 提出一种块交替迭代方法，将约束非凸优化问题分解为变量特定的子问题，在变量凸性假设下保证收敛和最优性，并提供Python平台实现。


<details>
  <summary>Details</summary>
Motivation: 控制应用中经常出现约束非凸优化问题，现有方法容易收敛到次优局部极小值或计算成本过高，需要更有效的解决方案。

Method: 提出块交替迭代方法，将原问题分解为变量特定的子问题，在假设每个决策变量凸性的条件下，将原问题转化为一系列凸子问题迭代求解。

Result: 建立了方法的收敛性和最优性理论结果，通过数值例子和实际控制工程应用验证了有效性，并提供了包含现有算法的Python平台。

Conclusion: 该方法能有效解决控制中的约束非凸优化问题，理论保证收敛和最优性，提供的开源平台便于比较和采用。

Abstract: Constrained non-convex optimization problems frequently arise in control applications. Solving such problems is inherently challenging, as existing methods often converge to suboptimal local minima or incur prohibitive computational costs. To address this challenge, this paper proposes a novel block-alternating iterative method that decomposes the original problem into variable-specific subproblems, which are solved iteratively. Under the assumption that the problem is convex with respect to each decision variable, the proposed approach reformulates the original problem into a sequence of convex subproblems. Theoretical results are established regarding the convergence and optimality of the method. In addition, a numerical example and a real-world control engineering application are presented to demonstrate its effectiveness. Finally, this paper introduces a ready-to-use Python platform that implements the proposed method, together with existing algorithms, to facilitate comparison and adoption.

</details>


### [350] [A Unified Kantorovich Duality for Multimarginal Optimal Transport](https://arxiv.org/abs/2601.17171)
*Yehya Cheryala,Mokhtar Z. Alaya,Salim Bouzebda*

Main category: math.OC

TL;DR: 本文为多边际最优传输问题建立了统一的Kantorovich对偶理论，在一般波兰乘积空间上证明了原始-对偶等式和对偶可达性，并展示了最优势函数可以通过c-共轭族表示。


<details>
  <summary>Details</summary>
Motivation: 多边际最优传输在机器学习和统计学中日益重要，但现有的对偶理论主要局限于双边际情况。需要为一般波兰空间上的多边际问题建立完整的对偶理论框架。

Method: 对于紧致边际空间，通过凸分析重构将问题转化为Fenchel-Rockafellar共轭问题；对于非紧致情况，采用截断-紧致化方法，利用多边际转移计划的弱紧致性和成本的有界性，通过c-分裂集和c-循环单调性等价性进行正则化。

Result: 证明了在任意波兰空间上多边际最优传输问题的对偶可达性和精确的原始-对偶等式，最优对偶势函数可以通过c-共轭族正则化为一致有界形式，并建立了c-共轭族的规范表示。

Conclusion: 该研究为多边际最优传输的概率和统计分析（包括稳定性、可微性和边际扰动下的渐近理论）提供了结构基础，将经典的双边际共轭原理推广到真正的多边际设置。

Abstract: Multimarginal optimal transport (MOT) has gained increasing attention in recent years, notably due to its relevance in machine learning and statistics, where one seeks to jointly compare and align multiple probability distributions. This paper presents a unified and complete Kantorovich duality theory for MOT problem on general Polish product spaces with bounded continuous cost function. For marginal compact spaces, the duality identity is derived through a convex-analytic reformulation, that identifies the dual problem as a Fenchel-Rockafellar conjugate. We obtain dual attainment and show that optimal potentials may always be chosen in the class of $c$-conjugate families, thereby extending classical two-marginal conjugacy principle into a genuinely multimarginal setting. In non-compact setting, where direct compactness arguments are unavailable, we recover duality via a truncation-tightness procedure based on weak compactness of multimarginal transference plans and boundedness of the cost. We prove that the dual value is preserved under restriction to compact subsets and that admissible dual families can be regularized into uniformly bounded $c$-conjugate potentials. The argument relies on a refined use of $c$-splitting sets and their equivalence with multimarginal $c$-cyclical monotonicity. We then obtain dual attainment and exact primal-dual equality for MOT on arbitrary Polish spaces, together with a canonical representation of optimal dual potentials by $c$-conjugacy. These results provide a structural foundation for further developments in probabilistic and statistical analysis of MOT, including stability, differentiability, and asymptotic theory under marginal perturbations.

</details>


### [351] [A Partially Observed Stochastic Linear Stackelberg Differential Game with Poisson Jumps under Mean-Variance Criteria](https://arxiv.org/abs/2601.17362)
*Jingtao Lin,Jingtao Shi*

Main category: math.OC

TL;DR: 研究部分可观测随机线性Stackelberg微分博弈的均值-方差准则，使用正交分解法解决控制与状态过程的循环依赖问题，通过滤波技术和Riccati方程获得可观测状态反馈Stackelberg均衡。


<details>
  <summary>Details</summary>
Motivation: 研究具有均值-方差准则的部分可观测随机线性Stackelberg微分博弈问题，其中随机性来自布朗运动和泊松随机测度，这导致了控制与状态过程之间的循环依赖问题，需要开发新的方法来解决这一挑战。

Method: 采用正交分解法将原始问题分解为多个完全可观测的均值-方差问题；开发了带泊松随机测度的非线性随机滤波技术；将跟随者问题嵌入到带泊松跳跃的随机线性二次最优控制问题中，将领导者问题嵌入到带泊松跳跃的前向-后向随机微分方程的最优控制问题中；通过Riccati方程求解。

Result: 获得了可观测状态反馈Stackelberg均衡，通过求解一系列Riccati方程实现了博弈均衡的解析表达，成功解决了部分可观测随机线性Stackelberg微分博弈中的循环依赖问题。

Conclusion: 本文成功解决了部分可观测随机线性Stackelberg微分博弈的均值-方差准则问题，通过正交分解法、非线性滤波技术和Riccati方程方法，为这类具有循环依赖的随机博弈问题提供了有效的解决方案。

Abstract: In this paper, a partially observed stochastic linear Stackelberg differential game with mean-variance criteria is studied. Randomness comes from Brownian motions and Poisson random measures. which leads to a circular dependency. We follow the orthogonal decomposition method to overcome the circular dependency of the control and state processes. Both original problems of the follower and leader are decomposed into several fully observed problems with mean-variance criteria. During these processes, non-linear stochastic filtering with Poisson random measures, developed in this paper, plays an important role. Besides the follower's problem is embedded into a class of auxiliary stochastic linear-quadratic optimal control problem of stochastic differential equations with Poisson jumps, the leader's problem is also embedded into a class of auxiliary stochastic linear-quadratic optimal control problem of forward-backward stochastic differential equations with Poisson jumps. Observable state feedback Stackelberg equilibria are obtained, via some Riccati equations.

</details>


### [352] [Winning Criteria for Open Games: A Game-Theoretic Approach to Prefix Codes](https://arxiv.org/abs/2601.17521)
*Dean Kraizberg*

Main category: math.OC

TL;DR: 研究无限树上交替移动的二人博弈，重点关注满正则树和开获胜集的情况，建立了第一玩家获胜策略与最大前缀码的等价关系


<details>
  <summary>Details</summary>
Motivation: 研究无限树上交替移动的二人博弈，特别是满正则树和开获胜集的情况。Gale和Stewart已证明在这种设置下总有一方有必胜策略，但不知道是哪一方。本文旨在寻找第一玩家获胜的简单必要条件

Method: 1. 建立第一玩家获胜策略与最大前缀码的等价关系；2. 推导获胜的代数必要条件；3. 引入覆盖概念，通过用自由群对应的无限标记树覆盖图来推导最大前缀码的简单特征

Result: 1. 提出了第一玩家获胜的简单必要条件；2. 建立了获胜集保证第一玩家获胜与最大前缀码的等价关系；3. 推导了获胜的代数必要条件；4. 展示了一族博弈，其中代数条件等价于获胜；5. 通过覆盖技术获得了最大前缀码的简单特征

Conclusion: 本文建立了无限树上开获胜集博弈中第一玩家获胜策略与最大前缀码的深刻联系，提供了代数判定条件，并通过覆盖技术获得了最大前缀码的新特征，为这类博弈的分析提供了新的理论工具

Abstract: We study two-player games with alternating moves played on infinite trees. Our main focus is on the case where the trees are full (regular) and the winning set is open (with respect to the product topology on the tree). Gale and Stewart showed that in this setting one of the players always has a winning strategy, though it is not known in advance which player. We present simple necessary conditions for the first player to have a winning strategy, and establish an equivalence between winning sets that guarantee a win for the first player and maximal prefix codes. Using this equivalence, we derive a necessary algebraic condition for winning, and exhibit a family of games for which this algebraic condition is in fact equivalent to winning. We introduce the concept of coverings, and show that by covering the graph with an infinite labeled tree corresponding to the free group, we can derive a simple trait of maximal prefix codes.

</details>


### [353] [Robust Spacecraft Low-Thrust Trajectory Design: A Chance-Constrained Covariance-Steering Approach](https://arxiv.org/abs/2601.17629)
*Meysam Babapour,Ehsan Taheri*

Main category: math.OC

TL;DR: 提出一种系统方法生成实用且鲁棒的低推力航天器轨迹，考虑质量变化对推进加速度和随机扰动强度的影响，采用协方差变量公式提高计算效率，应用于地火转移任务验证重要性。


<details>
  <summary>Details</summary>
Motivation: 现有低推力轨迹生成方法常忽略航天器质量变化对推进系统和随机扰动的影响，导致风险评估不足，需要更系统、鲁棒的方法来生成更真实的轨迹方案。

Method: 提出系统方法考虑质量变化在两个层面的影响：推进加速度和随机扰动强度；采用协方差变量公式而非因子化协方差实现，提高计算效率；应用于受限二体动力学下的二维和三维日心轨道地火转移阶段。

Result: 结果表明，跟踪质量变化对生成更真实、鲁棒的星际任务解决方案至关重要，可避免低估任务风险；协方差变量公式在计算上比因子化协方差实现更高效。

Conclusion: 考虑航天器质量变化对推进系统和随机扰动的影响对于生成实用、鲁棒的低推力轨迹至关重要，有助于更准确评估星际任务风险，提出的协方差变量方法计算效率更高。

Abstract: This paper proposes a systematic method for generating practical and robust low-thrust spacecraft trajectories. One contribution is to consider the change in mass of the spacecraft at two levels: a) the propulsive acceleration and b) the intensity of the stochastic disturbances. A covariance variable formulation is considered, which is computationally more efficient than the factorized covariance implementation. The proposed approach is applied to two- (i.e., planar) and three-dimensional heliocentric phases of spacecraft flight from Earth to Mars under the restricted two-body dynamics. The results highlight the importance of keeping track of mass change to generate more realistic, robust solutions for interplanetary space missions to avoid underestimation of mission risks.

</details>


### [354] [Multi-Criteria Inverse Robustness in Radiotherapy Planning Using Semidefinite Programming](https://arxiv.org/abs/2601.17750)
*Jan Schröeder,Yair Censor,Philipp Süss,Karl-Heinz Küfer*

Main category: math.OC

TL;DR: 该论文提出了一种放疗计划中的多准则优化方法，通过区间矩阵建模不确定性，引入逆鲁棒性作为新目标，将QCQP问题松弛为SDP求解


<details>
  <summary>Details</summary>
Motivation: 放疗计划本质上是一个多准则优化问题，存在多种不确定性来源。决策者需要在多个目标之间以及不确定性鲁棒性水平之间进行权衡，需要一种定量方法来处理这一复杂问题。

Method: 1. 使用区间矩阵建模剂量影响矩阵的不确定性；2. 引入逆鲁棒性作为新目标，最大化不确定性集的体积；3. 采用多准则方法处理不确定性同时保持其他目标函数的适当值；4. 将QCQP问题松弛为凸半定规划问题求解，然后从SDP解重构QCQP的最优解。

Result: 提出了一种结合理论模型与处理实际挑战能力的定量方法，能够有效处理放疗计划中的多准则优化和不确定性平衡问题。

Conclusion: 该方法为放疗计划中的多准则优化问题提供了一种有效的定量解决方案，能够同时处理多个目标和不确定性，具有理论和实践价值。

Abstract: Radiotherapy planning naturally leads to a multi-criteria optimization problem which is subject to different sources of uncertainty. In order to find the desired treatment plan, a decision maker must balance these objectives as well as the level of robustness towards uncertainty against each other. This paper showcases a quantitative approach to do so, which combines the theoretical model with the ability to deal with practical challenges. To this end, the uncertainty, which can be expressed via the so-called dose-influence matrix, is modelled using interval matrices. We use inverse robustness to introduce an additional objective, which aims to maximize the volume of the uncertainty set. A multi-criteria approach allows to handle the uncertainty while keeping appropriate values of the other objective functions. We solve the resulting quadratically constrained quadratic optimization problem (QCQP) by first relaxing it to a convex semidefinite problem (SDP) and then reconstructing optimal solutions of the QCQP from solutions of the SDP.

</details>


### [355] [Differentiable Integer Linear Programming is not Differentiable & it's not a mere technical problem](https://arxiv.org/abs/2601.17800)
*Thanawat Sornwanee*

Main category: math.OC

TL;DR: 指出Geng等人(2025)的论文《可微整数线性规划》中定理5的微分方法存在错误，且已有下游工作继承了该错误，原因是替代损失在随机梯度下降中虽然期望连续，但在几乎每个随机实现中都是不连续的。


<details>
  <summary>Details</summary>
Motivation: 揭示Geng等人(2025)论文中可微整数线性规划方法的数学错误，防止错误方法继续传播，并警示已受影响的后续研究工作。

Method: 通过分析原论文定理5的数学推导，指出其替代损失函数在随机梯度下降中的不连续性问题是关键错误来源。

Result: 确认了Geng等人(2025)论文中的微分方法存在根本性错误，且该错误已被其他研究继承，需要修正相关理论框架。

Conclusion: 原论文的可微整数线性规划方法存在数学缺陷，需要重新设计替代损失函数以确保在随机梯度下降中的连续性，避免错误传播。

Abstract: We show how the differentiability method employed in the paper ``Differentiable Integer Linear Programming'', Geng, et al., 2025 as shown in its theorem 5 is incorrect. Moreover, there already exists some downstream work that inherits the same error. The underlying reason comes from that, though being continuous in expectation, the surrogate loss is discontinuous in almost every realization of the randomness, for the stochastic gradient descent.

</details>


### [356] [Quadratic Programming over Linearly Ordered Fields: Decidability and Attainment of Optimal Solutions](https://arxiv.org/abs/2601.17969)
*Dmytro O. Plutenko*

Main category: math.OC

TL;DR: 本文为一般线性有序域上的二次规划建立了统一的代数框架，证明了即使在没有拓扑完备性的情况下，有界二次函数在单纯形上也能达到最小值，并提出了精确确定性算法。


<details>
  <summary>Details</summary>
Motivation: 传统二次规划的存在性定理和求解方法依赖于实数的分析性质（紧致性和完备性），这些工具在一般线性有序域（如有理数域或非阿基米德结构）中不可用，使得标准分析证明在这些代数设置中不足。

Method: 采用代数方法替代经典分析论证：1）通过维数和多面体分解的代数归纳法证明广义Eaves定理；2）在Blum-Shub-Smale计算模型下提出精确确定性算法，仅使用域运算；3）通过正交限制面的递归搜索实现有限时间求解。

Result: 证明了如果二次函数（包括凸、非凸或退化情况）在多面体上有下界，则最小值在域本身内达到，无论拓扑完备性如何。提出了能判定有界性并计算全局最小值的算法，且线性约束二次规划是多项式优化问题中能保证在原始域内获得精确解的最大类。

Conclusion: 为一般线性有序域上的二次规划建立了统一的代数框架，划定了有序结构上精确优化的代数边界，表明线性约束二次规划是多项式优化中能保证在原始域内获得精确解的最大类。

Abstract: Classical existence theorems and solution methods for quadratic programming traditionally rely on the analytical properties of real numbers, specifically compactness and completeness. These tools are unavailable in general linearly ordered fields, such as the field of rational numbers or non-Archimedean structures, rendering standard analytical proofs insufficient in these general algebraic settings. In this paper, we establish a unified algebraic framework for the decidability of indefinite quadratic programming subject to linear constraints over general linearly ordered fields. We prove a generalized Eaves' theorem, demonstrating that if a quadratic function -- encompassing convex, non-convex, or degenerate (linear) cases -- is bounded from below on a polyhedron, the minimum is attained within the field itself, regardless of topological completeness. Our approach replaces classical analytical arguments with algebraic induction on dimension and polyhedral decomposition. Based on this foundation, we propose an exact, deterministic algorithm within the Blum--Shub--Smale model of computation that decides boundedness and computes a global minimizer using only field operations. We show that the problem is solvable in finite time via a recursive search over orthant-restricted facets. Finally, we note that linearly constrained quadratic programming represents the maximal class of polynomial optimization problems where exact solutions are structurally guaranteed within the original field, thereby demarcating the algebraic boundary of exact optimization over ordered structures.

</details>


### [357] [On maximum hands-off restricted hybrid control for discrete-time switched linear systems](https://arxiv.org/abs/2601.17980)
*Darsana U,Atreyee Kundu*

Main category: math.OC

TL;DR: 设计离散时间切换线性系统的最大"手离"混合控制序列，即同时满足切换序列和连续控制序列稀疏性约束的最优控制


<details>
  <summary>Details</summary>
Motivation: 针对切换线性系统，需要设计既能将系统从初始状态驱动到原点，又能同时最小化切换次数和连续控制动作的混合控制序列，实现控制资源的有效利用

Method: 基于图论和线性代数工具，提出新算法设计最大手离混合控制序列，在满足子系统动态、允许控制集、初始状态和时间约束条件下，寻找最稀疏的切换和连续控制组合

Result: 在特定子系统动态和允许控制条件下，算法能够成功设计最大手离混合控制序列，并通过数值算例验证了方法的有效性

Conclusion: 该研究为切换线性系统提供了一种有效的稀疏混合控制设计方法，结合图论和线性代数工具，实现了控制资源的优化配置

Abstract: This paper deals with design of maximum hands-off hybrid control sequences for discrete-time switched linear systems. It is a sparsest combination of a discrete control sequence (i.e. the switching sequence) and a continuous control sequence, both satisfying pre-specified restrictions on the admissible actions, that steers a given initial state of the switched system to the origin of the state-space in a pre-specified duration of time. Given the subsystems dynamics, the sets of admissible continuous and discrete control, the initial state and the time horizon, we present a new algorithm that, under certain conditions on the subsystems dynamics and the admissible control, designs maximum hands-off hybrid control sequences for the resulting switched system. The key apparatuses for our analysis are graph theory and linear algebra. Numerical examples are presented to demonstrate our results.

</details>


### [358] [Application of log-Chebyshev approximation and tropical algebra to multicriteria problems of pairwise comparisons](https://arxiv.org/abs/2601.17999)
*Nikolai Krivulin*

Main category: math.OC

TL;DR: 该论文提出了一种基于热带代数的多准则决策方法，用于从成对比较矩阵中计算替代方案的绝对评分，并通过切比雪夫近似和热带优化技术获得解析解。


<details>
  <summary>Details</summary>
Motivation: 在多准则决策中，需要从多个准则下的成对比较矩阵中计算替代方案的绝对评分。传统方法如层次分析法（AHP）和加权几何平均法存在局限性，需要更数学严谨且能提供解析解的方法。

Method: 将多准则评分问题建模为对数尺度上的切比雪夫近似问题，转化为多目标优化问题，然后利用热带代数（具有幂等运算的代数系统）框架将其转化为一系列优化问题，应用热带优化方法获得解析解。

Result: 开发了一种基于热带代数的解析方法，能够从成对比较矩阵中计算替代方案评分。通过数值示例验证，并与传统AHP和加权几何平均法进行了比较。

Conclusion: 提出的热带代数方法为多准则决策评分问题提供了数学严谨的解析解框架，相比传统方法具有理论优势，计算结果可直接用于分析和计算。

Abstract: We consider multicriteria problems of evaluating absolute ratings (scores, priorities, weights) of given alternatives for making decisions, which are compared in pairs under several criteria. Given matrices of pairwise comparisons of alternatives for each criterion and a matrix of pairwise comparisons of the criteria, the aim is to calculate a vector of individual ratings of alternatives. We formulate the problem as the Chebyshev approximation of matrices on the logarithmic scale by a common consistent matrix (a symmetrically reciprocal matrix of unit rank). We rearrange the approximation problem as a multi-objective optimization problem of finding a vector that determines the consistent matrix and hence yields a vector of ratings in question. The problem is then transformed into a series of optimization problems in the framework of tropical algebra, which focuses on the theory and application of algebraic systems with idempotent operations. To solve the optimization problems, we apply methods and results of tropical optimization, which yield analytical solutions in a form ready for further analysis and straightforward computation. To illustrate the technique implemented, we give a numerical example of solving a known problem, and compare the obtained solution with results provided by classical methods of analytic hierarchy process and weighted geometrical means.

</details>


### [359] [Isolated Calmness in Regularized Convex Optimization](https://arxiv.org/abs/2601.18038)
*Tran T. A. Nghia,Huy N. Pham*

Main category: math.OC

TL;DR: 论文研究了正则化凸复合优化问题的最优解映射和拉格朗日系统的孤立镇定性质，建立了多个几何性质的充要条件，并发展了二阶结构的零积性质。


<details>
  <summary>Details</summary>
Motivation: 研究正则化凸复合优化问题的最优解映射和拉格朗日系统的孤立镇定性质，这对于理解优化问题的稳定性和灵敏度分析具有重要意义。

Method: 建立了孤立镇定性质的几何充要条件，这些条件相对简单易验证。同时发展了凸函数次梯度映射图导数的二阶结构的零积性质来支持分析。

Result: 获得了正则化凸复合优化问题最优解映射和拉格朗日系统孤立镇定性质的多个几何充要条件，这些条件便于验证。

Conclusion: 论文为凸复合优化问题的孤立镇定性质提供了系统的几何刻画，并发展了相关的二阶分析工具，为优化问题的稳定性分析提供了理论基础。

Abstract: This paper studies the isolated calmness of the optimal solution mapping and the associated Lagrange system for regularized convex composite optimization problems. Several necessary and sufficient conditions for this property are established. These conditions are geometric in nature and relatively simple to verify. To support the analysis, we also develop a so-called zero-product property for second-order structures, namely the graphical derivative of the subgradient mapping of convex functions.

</details>


### [360] [Controllability of wave-heat and heat-wave cascades](https://arxiv.org/abs/2601.18212)
*Hugo Lhachemi,Christophe Prieur,Emmanuel Trélat*

Main category: math.OC

TL;DR: 研究一维耦合双曲-抛物级联系统的边界可控性，重点关注可达集的精细结构，揭示了内部耦合如何产生非标准高度不规则的可控性空间


<details>
  <summary>Details</summary>
Motivation: 研究波-热级联系统中边界控制如何通过内部耦合驱动热方程，探索耦合结构对可控性空间性质的影响，特别是内部耦合可能产生的非标准高度不规则可控性空间

Method: 使用Riesz基分解和Ingham-Müntz不等式，通过谱分析在加权Hilbert空间中建立精确可控性，分析模态系数与耦合剖面的关系

Result: 给出了双曲部分的最小精确时间(T>2L)，建立了加权Hilbert空间中的完全谱可控性特征，发现内部耦合可能产生非标准高度不规则可控性空间，且精确可控性空间在Hilbert唯一性方法轨迹中不保持

Conclusion: 内部耦合会显著影响可控性空间的结构，产生非标准高度不规则空间，且耦合方向的反转会改变抛物和双曲分量之间的正则性损失传递方式，揭示了耦合级联系统可控性的复杂性质

Abstract: We study boundary controllability of one-dimensional coupled hyperbolic-parabolic cascades, focusing on the fine structure of reachable sets. The main model is a wave-heat cascade in which a boundary control acts on the wave equation and drives the heat equation through an internal coupling. We provide a sharp minimal time for the hyperbolic part (T > 2L) and a complete spectral characterization of exact controllability in weighted Hilbert spaces, whose definition depends explicitly on the coupling profile through a sequence of modal coefficients. In particular, internal couplings may generate nonstandard highly irregular controllability spaces and yield a generic (full measure) but non-robust controllability property. The analysis relies on Riesz basis decompositions and on an Ingham-M{ü}ntz inequality. We also prove that the exact controllability space is not invariant along Hilbert Uniqueness Method trajectories: even if both endpoints belong to the controllability space, the associated minimal-energy trajectory may leave it at intermediate times. Finally, we compare with the reversed (heat-wave) cascade and discuss how reversing the direction of the coupling transfers the loss of regularity between the parabolic and hyperbolic components.

</details>


### [361] [Exact Controllability for Stochastic First-Order Multi-Dimensional Hyperbolic Systems](https://arxiv.org/abs/2601.18270)
*Zengyu Li,Qi Lü,Yu Wang,Haitian Yang*

Main category: math.OC

TL;DR: 该论文研究了多维随机一阶对称双曲系统的精确可控性问题，系统具有两种控制方式：作用于扩散项的内部控制和作用于漂移项的边界控制。通过建立新的全局Carleman估计和加权能量恒等式，在几何结构条件下证明了系统的精确可控性。


<details>
  <summary>Details</summary>
Motivation: 将确定性系统的可控性理论扩展到随机多维双曲系统，研究具有内部扩散控制和边界漂移控制的随机系统的精确可控性，为随机交通流、流行病模型和浅水方程等应用提供理论基础。

Method: 采用经典对偶论证将可控性问题转化为相应后向随机系统的可观测性估计；建立新的全局Carleman估计和加权能量恒等式；在几何结构条件（特征射线在有限时间内传播到边界）下证明可观测性不等式。

Result: 证明了当控制时间T超过由系统几何明确给出的阈值T0时，系统具有精确可控性；建立了多个负可控性定理，证明两种控制都是必要的且必须以分布式方式作用；结果不仅适用于随机系统，也为确定性系统提供了新结果。

Conclusion: 该研究成功将可控性理论从确定性系统扩展到随机多维双曲系统，建立了在几何结构条件下的精确可控性理论框架，为相关应用领域提供了理论基础，并揭示了两种控制方式的必要性。

Abstract: This paper investigates the exact controllability problem for multi-dimensional stochastic first-order symmetric hyperbolic systems with control inputs acting in two distinct ways: an internal control applied to the diffusion term and a boundary control applied to the drift term. By means of a classical duality argument, the controllability problem is reduced to an observability estimate for the corresponding backward stochastic system. The main technical contribution is the establishment of a new global Carleman estimate for such backward systems, combined with a weighted energy identity. This enables us to prove the desired observability inequality under a geometric structural condition (Condition \ref{cond1}), which ensures that all characteristic rays propagate toward the boundary within a finite time. As a result, we obtain exact controllability provided the control time $T$ exceeds a sharp threshold $T_0$ given explicitly in terms of the system geometry. Furthermore, we complement the positive result with several negative controllability theorems, which demonstrate that both controls are necessary and must act in a distributed manner. Our analysis not only extends controllability theory from deterministic to stochastic multi-dimensional hyperbolic systems but also provides, as a byproduct, new results for deterministic systems under a structural hypothesis. Applications to stochastic traffic flow, epidemiological models, and shallow-water equations are discussed.

</details>


### [362] [Line Spectral Estimation Using a G-Filter: Atomic Norm Minimization with Multiple Output Vectors](https://arxiv.org/abs/2601.18279)
*Jiale Tang,Bin Zhu*

Main category: math.OC

TL;DR: 提出了一种结合Georgiou滤波器组(G-filter)和多输出向量(MOV)的原子范数最小化(ANM)频率估计器，相比单输出向量方法提高了数据利用率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的G-filter版本ANM仅限于单个滤波输出向量，限制了数据利用和估计鲁棒性。需要开发能利用多个输出向量的方法来改善频率估计性能。

Method: 将Georgiou滤波器组与多输出向量(MOV)结合到原子范数最小化框架中，通过Carathéodory-Fejér型分解将MOV-ANM问题转化为半定规划问题求解。

Result: 数值模拟显示，在G-filter选择的频带内，特别是在低信噪比条件下，该方法在恢复正确频率分量数量方面显著优于标准ANM和单输出向量G-filter ANM。

Conclusion: 提出的MOV-ANM方法通过多输出向量提高了数据利用率和估计鲁棒性，在频率估计任务中表现出优越性能，特别是在低信噪比环境下。

Abstract: We propose an atomic norm minimization (ANM) estimator of frequencies in a noisy complex sinusoidal signal that integrates Georgiou's filter bank (G-filter) with multiple output vectors (MOV). Unlike our previous work on the G-filter version of ANM which is restricted to a single filtered output vector, the proposed method in this paper uses MOV to improve data utilization and robustness of the estimate. The ANM problem with MOV can be reformulated as a semidefinite program thanks to a Carathéodory--Fejér-type decomposition for output covariance matrices of the G-filter. Numerical simulations demonstrate that the proposed approach significantly outperforms the standard ANM and the G-filter version of ANM with a single output vector in recovering the correct number of frequency components when the frequencies fall within the band(s) selected by the G-filter, particularly in the low SNR regime.

</details>


### [363] [On strong valid inequalities for a class of mixed-integer nonlinear sets with box constraints](https://arxiv.org/abs/2601.18358)
*Keyan Li,Yan-Ru Wang,Wei-Kun Chen,Yu-Hong Dai*

Main category: math.OC

TL;DR: 本文对具有箱约束的混合整数非线性集合X进行多面体研究，提出两类提升程序生成强有效不等式，并在多个应用问题中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 混合整数非线性集合X出现在许多优化模型中，包含多个已知集合作为特例，但尚未有对其凸包conv(X)的全面多面体研究，需要开发有效的切割平面来加强松弛和提升计算性能。

Method: 1. 通过固定除一个变量外的所有变量得到二维限制集，推导种子不等式；2. 开发两类提升程序：单相提升（使用子加性近似提升函数）和两相提升（分别处理上下界变量）；3. 证明所得不等式在温和条件下是凸包的刻面定义不等式。

Result: 1. 提出的提升不等式能统一现有强有效不等式或产生新的刻面定义不等式；2. 在期望效用最大化和武器目标分配问题上的计算实验表明，这些不等式能显著加强连续松弛，大幅提升分支切割算法的整体计算性能。

Conclusion: 本文首次对混合整数非线性集合X的凸包进行了全面的多面体研究，开发了有效的提升不等式生成方法，这些方法不仅能统一现有结果，还能产生新的强不等式，在实际应用中表现出显著的性能提升。

Abstract: In this paper, we investigate the mixed-integer nonlinear set with box constraints $X = \{(w,x)\in R\times Z^n:w\leq f(a^Tx),0\leq x\leq μ\}$, where $f$ is a univariate concave function, $a\in R^n$, and $μ\in Z^n_{++}$. This set arises as a substructure in many mixed-integer nonlinear optimization models and encompasses, as special cases, several previously investigated mixed-integer sets, namely the submodular maximization set, the mixed-integer knapsack set, and the mixed-integer polyhedral conic set. We present the first comprehensive polyhedral study of conv($X$). In particular, we derive a class of seed inequalities for a two-dimensional restriction of $X$, obtained by fixing all but one of the $x$ variables to their bounds in $X$, and develop two lifting procedures to obtain strong valid inequalities for conv($X$). In the first lifting procedure, we derive a subadditive approximation for the exact lifting function of the seed inequalities, and lift all fixed variables in a single phase. In the second lifting procedure, we first lift variables fixed at their lower bounds before those at their upper bounds (and vice versa), using subadditive exact and approximation lifting functions, respectively. The derived single- and two-phase lifted inequalities are shown to be facet-defining for conv($X$) under mild conditions. Moreover, for the aforementioned special cases of conv($X$), we show that the proposed lifted inequalities can either unify existing strong valid inequalities or yield new facet-defining inequalities. Finally, extensive computational experiments on expected utility maximization and weapon-target assignment problems demonstrate that the proposed lifted inequalities can substantially strengthen the continuous relaxations and significantly improve the overall computational performance of branch-and-cut algorithms.

</details>


### [364] [Polyhedral results for two classes of submodular sets with GUB constraints](https://arxiv.org/abs/2601.18360)
*Weikang Qian,Keyan Li,Wei-Kun Chen,Yu-Hong Dai*

Main category: math.OC

TL;DR: 论文研究了具有广义上界约束的两个次模集合的多面体结构，提出了一类强有效不等式，这些不等式比扩展多面体不等式更强，能完全刻画凸包，并在线性时间内可计算。


<details>
  <summary>Details</summary>
Motivation: 具有广义上界约束的次模集合是许多实际应用中的重要子结构，需要研究其多面体结构以开发更有效的优化算法。

Method: 使用顺序提升技术推导出两个集合的强有效不等式，提供更紧凑的刻画，证明这些不等式是凸包的刻面定义不等式，并开发多项式时间的分离算法。

Result: 提出的提升不等式比扩展多面体不等式更强，能完全刻画凸包，每个不等式可在线性时间内计算，分离算法是组合多项式时间的。在概率覆盖选址和多概率背包问题上的计算结果表明其在分支切割框架中的优越性。

Conclusion: 论文成功推导出具有广义上界约束的次模集合的强有效不等式，这些不等式在理论和计算上都具有优越性，为相关优化问题提供了有效的求解工具。

Abstract: In this paper, we investigate the polyhedral structure of two submodular sets with generalized upper bound (GUB) constraints, which arise as important substructures in various real-world applications. We derive a class of strong valid inequalities for the two sets using sequential lifting techniques. The proposed lifted inequalities are facet-defining for the convex hulls of two sets and are stronger than the well-known extended polymatroid inequalities (EPIs). We provide a more compact characterization of these inequalities and show that each of them can be computed in linear time. Moreover, the proposed lifted inequalities, together with bound and GUB constraints, can completely characterize the convex hulls of the two sets, and can be separated using a combinatorial polynomial-time algorithm. Finally, computational results on probabilistic covering location and multiple probabilistic knapsack problems demonstrate the superiority of the proposed lifted inequalities over the EPIs within a branch-and-cut framework.

</details>


### [365] [Tight semidefinite programming relaxations for sparse box-constrained quadratic programs](https://arxiv.org/abs/2601.18545)
*Aida Khajavirad*

Main category: math.OC

TL;DR: 提出一种新的稀疏盒约束二次规划的SDP松弛方法，通过将RLT技术集成到标准SDP松弛中，并利用问题稀疏性，得到比现有LP和SDP松弛更强的结果。


<details>
  <summary>Details</summary>
Motivation: 现有LP和SDP松弛对于稀疏盒约束二次规划问题不够紧致，需要开发能更好利用问题稀疏性的新松弛方法。

Method: 将Reformulation Linearization Technique (RLT) 集成到标准SDP松弛中，同时显式利用问题的稀疏性结构，构建新的SDP松弛类。

Result: 1) 新松弛不被现有LP和SDP松弛蕴含；2) 建立了提升二次规划可行域凸包SDP可表示性的充分条件；3) 识别了保证多项式规模SDP可表示公式的结构条件。

Conclusion: 提出的方法为稀疏盒约束二次规划提供了更强的新SDP松弛，在某些结构条件下能获得多项式规模的SDP可表示公式，具有理论价值和实际应用潜力。

Abstract: We introduce a new class of semidefinite programming (SDP) relaxations for sparse box-constrained quadratic programs, obtained by a novel integration of the Reformulation Linearization Technique into standard SDP relaxations while explicitly exploiting the sparsity of the problem. The resulting relaxations are not implied by the existing LP and SDP relaxations for this class of optimization problems. We establish a sufficient condition under which the convex hull of the feasible region of the lifted quadratic program is SDP-representable; the proof is constructive and yields an explicit extended formulation. Although the resulting SDP may be of exponential size in general, we further identify additional structural conditions on the sparsity of the optimization problem that guarantee the existence of a polynomial-size SDP-representable formulation, which can be constructed in polynomial time.

</details>


### [366] [Global Optimization of Atomic Clusters via Physically-Constrained Tensor Train Decomposition](https://arxiv.org/abs/2601.18592)
*Konstantin Sozykin,Nikita Rybin,Andrei Chertkov,Anh-Huy Phan,Ivan Oseledets,Alexander Shapeev,Ivan Novikov,Gleb Ryzhakov*

Main category: math.OC

TL;DR: 提出基于张量列分解的原子团簇全局优化框架，结合TTOpt和PROTES方法，通过物理约束编码处理分子约束，成功优化45原子LJ团簇和20原子碳团簇。


<details>
  <summary>Details</summary>
Motivation: 原子团簇全局优化面临维度灾难问题，局部最小值随系统规模指数增长，传统方法难以处理高维势能面搜索。

Method: 利用势能面的低秩结构，采用张量列分解框架，结合代数TTOpt方法（最大体积采样）和概率PROTES方法（生成采样），开发物理约束编码方案将分子约束直接融入离散化过程。

Result: 成功找到45原子Lennard-Jones团簇的全局最小值，并优化20原子碳团簇（使用机器学习矩张量势），获得与量子精确模拟一致的几何结构。

Conclusion: 张量列分解成为分子结构预测的有力工具，为计算材料科学中的高维优化问题提供了通用框架。

Abstract: The global optimization of atomic clusters represents a fundamental challenge in computational chemistry and materials science due to the exponential growth of local minima with system size (i.e., the curse of dimensionality). We introduce a novel framework that overcomes this limitation by exploiting the low-rank structure of potential energy surfaces through Tensor Train (TT) decomposition. Our approach combines two complementary TT-based strategies: the algebraic TTOpt method, which utilizes maximum volume sampling, and the probabilistic PROTES method, which employs generative sampling. A key innovation is the development of physically-constrained encoding schemes that incorporate molecular constraints directly into the discretization process. We demonstrate the efficacy of our method by identifying global minima of Lennard-Jones clusters containing up to 45 atoms. Furthermore, we establish its practical applicability to real-world systems by optimizing 20-atom carbon clusters using a machine-learned Moment Tensor Potential, achieving geometries consistent with quantum-accurate simulations. This work establishes TT-decomposition as a powerful tool for molecular structure prediction and provides a general framework adaptable to a wide range of high-dimensional optimization problems in computational material science.

</details>


### [367] [A Unique Inverse Decomposition of Positive Definite Matrices under Linear Constraints](https://arxiv.org/abs/2601.18662)
*Yan Dolinsky,Or Zuk*

Main category: math.OC

TL;DR: 提出一种正定矩阵的非线性分解方法，将矩阵分解为另一正定矩阵的逆与对称矩阵之和，后者位于给定线性子空间中。该分解具有唯一性、变分特征和稳定性，并开发了高效算法。


<details>
  <summary>Details</summary>
Motivation: 研究正定矩阵的结构分解问题，为数学金融中的指数效用最大化等应用提供理论基础和计算工具。该分解在优化和计算方面具有重要价值。

Method: 提出非线性矩阵分解框架：将正定矩阵P分解为Q⁻¹ + S，其中Q是正定矩阵，S是对称矩阵且位于给定线性子空间中。通过严格凸的对数行列式优化问题获得变分特征，并开发可行性保持的牛顿型算法。

Result: 在尖锐非退化条件下证明分解的唯一性；建立分解的稳定性；开发具有收敛保证的算法并分析其每步迭代复杂度；展示该分解在数学金融指数效用最大化问题中的自然出现。

Conclusion: 该分解为矩阵分析和优化提供了新的理论框架，具有唯一性、变分特征和计算可行性，在金融数学等领域有重要应用前景。

Abstract: We study a nonlinear decomposition of a positive definite matrix into two components: the inverse of another positive definite matrix and a symmetric matrix constrained to lie in a prescribed linear subspace. Equivalently, the inverse component is required to belong to the orthogonal complement of that subspace with respect to the trace inner product. Under a sharp nondegeneracy condition on the subspace, we show that every positive definite matrix admits a \emph{unique} decomposition of this form.
  This decomposition admits a variational characterization as the unique minimizer of a strictly convex log-determinant optimization problem, which in turn yields a natural dual formulation that can be efficiently exploited computationally. We derive several properties, including the stability of the decomposition.
  We further develop feasibility-preserving Newton-type algorithms with provable convergence guarantees and analyze their per-iteration complexity in terms of algebraic properties of the decomposed matrix and the underlying subspace. Finally, we show that the proposed decomposition arises naturally in exponential utility maximization, a central problem in mathematical finance.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [368] [The Curious Case of Aid and Conflict: Causal Evidence from Panel Econometrics and Composite Indices](https://arxiv.org/abs/2601.16992)
*Muhammad Usman Anwar Goraya*

Main category: econ.EM

TL;DR: 研究使用多种回归方法分析2009-2023年非洲十大受援国中官方发展援助与冲突的关系，发现援助分配存在人道需求与制度质量之间的权衡困境。


<details>
  <summary>Details</summary>
Motivation: 探讨官方发展援助是否系统性地受到冲突（以政治稳定、治理指标和宏观经济条件为代理变量）的影响，理解援助分配的决定因素。

Method: 使用普通最小二乘法、主成分分析和岭回归（L2正则化）三种计量方法，分析2009-2023年非洲十大受援国的面板数据，以政治稳定、治理指标和宏观经济条件作为冲突的代理变量。

Result: 混合回归显示援助与贫困、通胀和脆弱性正相关，与话语权和问责制负相关；固定效应模型则显示援助与政治稳定和人均GDP正相关，与感知腐败负相关；岭回归证实了治理变量在多重共线性下的稳健性。

Conclusion: 援助分配存在"援助-冲突-制度"三难困境：援助最集中在冲突风险最高和制度最薄弱的地区，但这些条件恰恰限制了援助效果。捐助者同时回应人道需求和制度质量，形成复杂权衡。

Abstract: This paper examines the relationship between Official Development Assistance (ODA) and conflict in the ten largest aid-receiving African countries between 2009 and 2023. Using Ordinary Least Squares, Principal Component Analysis, and Ridge (L2) regression, the study assesses whether conflict, proxied by political stability, governance indicators, and macroeconomic conditions, systematically influences aid inflows. Results reveal a nuanced relationship. Pooled regressions indicate that aid is positively associated with poverty, inflation, and fragility, while voice and accountability are negatively related to ODA. Fixed-effects estimates instead show positive associations between aid, political stability, and GDP per capita over time, alongside negative correlations with perceived corruption. Ridge regression confirms the robustness of various governance variables under multicollinearity. Overall, donors appear responsive to both humanitarian need and institutional quality, producing an aid-conflict-institutions trilemma: aid is most concentrated where conflict risk and institutional weakness are greatest, yet these same conditions which constrain aid effectiveness. The paper contributes by integrating theory with panel-econometric tools to to explore international development aid allocation.

</details>


### [369] [Decomposition of Brazil's 5-year DI Futures in Basis Points](https://arxiv.org/abs/2601.16995)
*Gabriel de Macedo Santos*

Main category: econ.EM

TL;DR: 提出一个可复制、可解释的框架，将巴西5年期DI期货利率的每日变动分解为基点贡献，包含宏观预期、主权风险分解和线性回归映射三个模块。


<details>
  <summary>Details</summary>
Motivation: 需要透明地解释巴西5年期DI期货利率每日变动的驱动因素，将宏观预期、国内风险和外部风险的影响进行量化分解，为市场分析和政策制定提供清晰框架。

Method: 结合三个模块：1) 巴西央行Focus调查的宏观和财政预期转换为日度变化；2) 使用偏最小二乘法构建监督宏观因子，汇总预期变化和高频宏观"意外"指标；3) 将巴西CDS分解为全球和国内风险成分。最后通过线性回归将三个因子映射为DI5Y每日变动的基点贡献。

Result: 在2015-2025年的2,741个观测样本中，模型解释了DI5Y每日变动的约22.45%方差。国内风险主导解释部分，宏观因子贡献较小但统计显著。残差较大，表明线性模型局限和遗漏因素（货币政策事件窗口、期限溢价、流动性等）。

Conclusion: 该框架提供了透明的方法来量化DI5Y每日和累计变动中宏观/央行力量、巴西国内风险和外部风险的贡献，虽然解释力有限但为市场驱动因素分解提供了实用工具。

Abstract: This paper proposes an empirical, replicable, and interpretable framework to decompose, in basis points (bps), daily changes in Brazil's 5-year DI futures rate (DI5Y). The approach combines three building blocks: (i) macroeconomic and fiscal expectations from the Central Bank of Brazil Focus survey, converted into daily changes; (ii) a supervised macro factor built with Partial Least Squares (PLS) that summarizes changes in expectations together with a high-frequency macro "surprise" indicator; and (iii) a decomposition of sovereign risk using Brazil CDS into global and domestic components, obtained by regressing CDS on external financial conditions (DXY, CRB, VIX, and the US 10-year yield). The final step maps these drivers into daily bps contributions through a linear regression of the daily change in DI5Y on the three factors, producing a cumulative decomposition that adds up with an intercept and a residual. In the final sample (2015-01-13 to 2025-12-12; 2,741 observations), the model explains about 22.45% of the daily variance in DI5Y changes. The explained share is dominated by domestic risk, with a smaller but statistically significant contribution from the macro factor. The residual remains large, highlighting the limits of linearity and omitted drivers such as monetary policy event windows, term premia, liquidity, and positioning. Overall, the framework delivers a transparent accounting of how much of the daily (and cumulative) movement in DI5Y is associated with macro/central bank forces, domestic Brazil risk, and external risk.

</details>


### [370] [Recovering Counterfactual Distributions via Wasserstein GANs](https://arxiv.org/abs/2601.17296)
*Xinran Liu*

Main category: econ.EM

TL;DR: 提出基于最优传输的稳健合成控制估计器，使用Wasserstein距离替代传统的L2分位数距离，解决标准方法在支持集不匹配和多模态数据下的脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 标准分布合成控制（DSC）使用分位数函数的L2欧几里得距离，这种方法在几何上存在脆弱性：当支持集不匹配时缺乏信息梯度，在结果多模态时产生结构伪影

Method: 基于最优传输（OT）构建稳健估计器，通过最小化概率测度之间的Wasserstein-1距离来构建合成控制，使用Wasserstein生成对抗网络（WGAN）实现

Result: 蒙特卡洛模拟显示：标准估计器在重尾污染和支持集不匹配下出现灾难性方差爆炸，而WGAN方法保持一致性和稳定性；新方法能正确恢复复杂双模态混合分布，而传统分位数平均方法在结构上失败

Conclusion: 基于Wasserstein距离的最优传输方法为分布合成控制提供了更稳健的估计框架，解决了传统L2分位数方法的几何脆弱性，在处理支持集不匹配和多模态数据时表现优越

Abstract: Standard Distributional Synthetic Controls (DSC) estimate counterfactual distributions by minimizing the Euclidean $L_2$ distance between quantile functions. We demonstrate that this geometric reliance renders estimators fragile: they lack informative gradients under support mismatch and produce structural artifacts when outcomes are multimodal. This paper proposes a robust estimator grounded in Optimal Transport (OT). We construct the synthetic control by minimizing the Wasserstein-1 distance between probability measures, implemented via a Wasserstein Generative Adversarial Network (WGAN). We establish the formal point identification of synthetic weights under an affine independence condition on the donor pool. Monte Carlo simulations confirm that while standard estimators exhibit catastrophic variance explosions under heavy-tailed contamination and support mismatch, our WGAN-based approach remains consistent and stable. Furthermore, we show that our measure-based method correctly recovers complex bimodal mixtures where traditional quantile averaging fails structurally.

</details>


### [371] [Statistical Decisions and Partial Identification: With Application to Boundary Discontinuity Design](https://arxiv.org/abs/2601.17648)
*Chen Qiu,Jörg Stoye*

Main category: econ.EM

TL;DR: 本文回应了两篇关于部分识别的综述，展示了统计决策理论如何应用于部分识别情境，并通过一个教育补贴资格缩减的政策实验连接两篇综述的主题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在回应Cattaneo等人(2026)和Hirano(2026)的优秀综述，展示统计决策理论在部分识别情境中的应用，并通过具体政策实验连接两篇综述的主题。

Method: 首先构建了一个部分识别下的统计决策简化场景，基于作者及他人的前期工作提供了完整解决方案；然后将这些结果应用于一个模拟实际政策的假设性教育补贴资格缩减实验。

Result: 研究发现可以得出一些有意义的结论，但将理论应用于实践时需要一些假设跳跃，并留下了一些未解决的问题。

Conclusion: 最后讨论了部分识别下统计决策理论面临的主要开放挑战，强调了理论与实践之间的差距以及需要进一步研究的问题。

Abstract: We are delighted to respond to the excellent surveys by Cattaneo et al. (2026) and Hirano (2026). Our discussion will attempt two things: first, we show how statistical decision theory can be applied to situations with partial identification; second, we connect the surveys' themes by applying these insights to an imagined policy experiment in one of Cattaneo et al.'s (2025) applications.
  To do so, we lay out a stylized scenario of statistical decision making under partial identification and, drawing on our own and others' earlier work, provide a complete solution for that scenario. We then apply these results to a hypothetical reduction (modelled on actual policies) in eligibility for educational subsidies. We will see that something of interest can be said, but also that bringing the theory to the application involves some leaps of faith and leaves some questions open. This leads to the final section, where we discuss what we see as the main open challenges in statistical decision theory under partial identification.

</details>


### [372] [The Proximal Surrogate Index: Long-Term Treatment Effects under Unobserved Confounding](https://arxiv.org/abs/2601.17712)
*Ting-Chih Hung,Yu-Chang Chen*

Main category: econ.EM

TL;DR: 该论文提出了一种结合实验样本和观测样本的方法，在存在未观测混杂因素的情况下识别和估计长期处理效应，通过使用代理变量解决标准替代指标方法的失败问题。


<details>
  <summary>Details</summary>
Motivation: 在评估长期处理效应时，实验样本通常缺乏长期结果数据，而观测样本存在未观测混杂因素。标准替代指标方法在存在未观测混杂因素时会失败，需要新的识别和估计方法。

Method: 结合实验样本（有短期结果但缺乏长期结果）和观测样本（有长期结果但存在未观测混杂因素），利用未观测混杂因素的代理变量建立新的识别结果，并开发多重稳健的估计和推断程序。

Result: 在Job Corps项目应用中，该方法能够恢复实验基准，即使标准替代指标估计因未观测混杂因素而产生偏差。

Conclusion: 通过结合实验和观测数据并利用代理变量，可以在存在未观测混杂因素的情况下有效识别和估计长期处理效应，为因果推断提供了实用的解决方案。

Abstract: We study the identification and estimation of long-term treatment effects under unobserved confounding by combining an experimental sample, where the long-term outcome is missing, with an observational sample, where the treatment assignment is unobserved. While standard surrogate index methods fail when unobserved confounders exist, we establish novel identification results by leveraging proxy variables for the unobserved confounders. We further develop multiply robust estimation and inference procedures based on these results. Applying our method to the Job Corps program, we demonstrate its ability to recover experimental benchmarks even when unobserved confounders bias standard surrogate index estimates.

</details>


### [373] [Best Feasible Conditional Critical Values for a More Powerful Subvector Anderson-Rubin Test](https://arxiv.org/abs/2601.17843)
*Jesse Hoekstra,Frank Windmeijer*

Main category: econ.EM

TL;DR: 本文提出了一种改进的子向量AR检验方法，通过使用第二小特征值而非最大特征值作为条件变量，在弱工具变量设定下获得了比现有方法更高的检验功效。


<details>
  <summary>Details</summary>
Motivation: 在弱工具变量线性工具变量模型中，GKM(2019)提出的条件子向量AR检验使用数据依赖的临界值函数，但该函数基于最大特征值。本文认为第二小特征值更适合作为弱识别的指示器，因此探索基于第二小特征值的条件临界值函数是否能提高检验功效。

Method: 提出使用第二小特征值而非最大特征值作为条件变量，构建数据依赖的临界值函数。证明GKM的临界值函数同样适用于这种条件设定，并将该方法扩展到GKM(2024)提出的对近似Kronecker积结构条件异方差稳健的子向量AR检验统计量。

Result: 基于第二小特征值的条件检验具有正确的检验水平，并且当非检验参数数量大于1时，其检验功效严格高于GKM方法。该方法同样适用于条件异方差稳健的检验统计量，保持了功效优势。

Conclusion: 使用第二小特征值作为条件变量是更优的选择，能够在不牺牲检验水平的前提下提高子向量AR检验的功效，特别是在非检验参数较多的情况下。该方法为弱工具变量下的子向量推断提供了更有效的工具。

Abstract: For subvector inference in the linear instrumental variables model under homoskedasticity but allowing for weak instruments, Guggenberger, Kleibergen, and Mavroeidis (2019) (GKM) propose a conditional subvector Anderson and Rubin (1949) (AR) test that uses data-dependent critical values that adapt to the strength of the parameters not under test. This test has correct size and strictly higher power than the test that uses standard asymptotic chi-square critical values. The subvector AR test is the minimum eigenvalue of a data dependent matrix. The GKM critical value function conditions on the largest eigenvalue of this matrix. We consider instead the data dependent critical value function conditioning on the second-smallest eigenvalue, as this eigenvalue is the appropriate indicator for weak identification. We find that the data dependent critical value function of GKM also applies to this conditioning and show that this test has correct size and power strictly higher than the GKM test when the number of parameters not under test is larger than one. Our proposed procedure further applies to the subvector AR test statistic that is robust to an approximate kronecker product structure of conditional heteroskedasticity as proposed by Guggenberger, Kleibergen, and Mavroeidis (2024), carrying over its power advantage to this setting as well.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [374] [Regret-Driven Portfolios: LLM-Guided Smart Clustering for Optimal Allocation](https://arxiv.org/abs/2601.17021)
*Muhammad Abro,Hassan Jaleel*

Main category: q-fin.PM

TL;DR: 提出一种结合LLM指导、在线学习和市场情绪指标的组合管理框架，旨在降低风险回报权衡，为风险厌恶投资者构建高夏普比率组合


<details>
  <summary>Details</summary>
Motivation: 解决中长期投资组合管理中风险与回报之间的持续权衡问题，为风险厌恶投资者和机构基金经理提供更好的投资方案

Method: 基于跟随领导者方法，整合在线学习动态、市场情绪指标和LLM驱动的对冲策略，包括情绪驱动的交易过滤和LLM引导的下行保护机制

Result: 实证结果显示，该方法相比SPY买入持有基准策略，年化回报高出69%，夏普比率高出119%

Conclusion: LLM指导的无悔投资组合分配框架能有效平衡风险与回报，为风险厌恶投资者提供显著优越的投资表现

Abstract: We attempt to mitigate the persistent tradeoff between risk and return in medium- to long-term portfolio management. This paper proposes a novel LLM-guided no-regret portfolio allocation framework that integrates online learning dynamics, market sentiment indicators, and large language model (LLM)-based hedging to construct high-Sharpe ratio portfolios tailored for risk-averse investors and institutional fund managers. Our approach builds on a follow-the-leader approach, enriched with sentiment-based trade filtering and LLM-driven downside protection. Empirical results demonstrate that our method outperforms a SPY buy-and-hold baseline by 69% in annualized returns and 119% in Sharpe ratio.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [375] [Pregeometric Origins of Liquidity Geometry in Financial Order Books](https://arxiv.org/abs/2601.17245)
*João P. da Cruz*

Main category: q-fin.TR

TL;DR: 该论文提出了一种金融订单簿的几何结构框架，将流动性、供给和需求视为涌现的观测变量而非原始经济变量。市场被建模为无度量、时间或价格坐标的膨胀关系系统，观测量仅通过谱嵌入投影产生。在最小单尺度假设下，预测供给和需求呈现伽马分布形式，实证数据验证了这一几何约束。


<details>
  <summary>Details</summary>
Motivation: 传统金融模型通常将流动性、供给和需求作为原始经济变量处理，但作者认为这些实际上是观测过程中涌现的现象。论文旨在建立一个更基础的几何框架，从关系系统的角度理解订单簿结构，揭示观测过程本身如何约束市场表现。

Method: 1. 将市场建模为无度量、时间和价格坐标的膨胀关系系统；2. 通过图拉普拉斯算子的谱嵌入实现投影，一维投影产生类价格坐标，投影密度定义流动性分布；3. 在最小单尺度假设下推导出供给和需求的伽马分布形式；4. 使用高频Level II数据对多个美国股票进行实证检验；5. 通过信息准则比较不同累积模型；6. 构建最小模拟验证几何约束的涌现性。

Result: 1. 理论推导表明在单尺度假设下，投影的供给和需求呈现伽马分布形式；2. 实证数据验证了集成伽马分布在多个资产和日内窗口中的稳健性；3. 信息准则分析显示集成伽马几何模型优于其他累积模型；4. 最小模拟表明无需代理行为或价格形成机制即可复现相同结构，支持几何约束源于观测而非微观结构动力学的观点。

Conclusion: 订单簿流动性的关键规律反映了观测过程诱导的几何约束，而非详细的微观结构动力学。这一几何框架为理解金融市场提供了新的视角，表明市场观测本身的结构性约束可能比传统经济变量假设更为基础。

Abstract: We propose a structural framework for the geometry of financial order books in which liquidity, supply, and demand are treated as emergent observables rather than primitive economic variables. The market is modeled as an inflationary relational system without assumed metric, temporal, or price coordinates. Observable quantities arise only through projection, implemented here via spectral embeddings of the graph Laplacian. A one-dimensional projection induces a price-like coordinate, while the projected density defines liquidity profiles around the mid price. Under a minimal single-scale hypothesis -- excluding intrinsic length scales beyond distance to the mid and finite visibility -- we show that projected supply and demand are constrained to gamma-like functional forms. In discrete data, this prediction translates into integrated-gamma cumulative profiles. We test these results using high-frequency Level~II data for several U.S. equities and find robust agreement across assets and intraday windows. Explicit comparison with alternative cumulative models using information criteria demonstrates a systematic preference for the integrated-gamma geometry. A minimal simulation of inflationary relational dynamics reproduces the same structure without invoking agent behavior or price formation mechanisms. These results indicate that key regularities of order-book liquidity reflect geometric constraints induced by observation rather than detailed microstructural dynamics.
  Supplementary Material is available at the arXiv submission.

</details>


### [376] [Learning Market Making with Closing Auctions](https://arxiv.org/abs/2601.17247)
*Julius Graf,Thibaut Mastrolia*

Main category: q-fin.TR

TL;DR: 本文提出了一种考虑闭市拍卖的做市商深度学习框架，通过深度Q学习优化做市策略，在连续交易和闭市拍卖阶段管理库存风险。


<details>
  <summary>Details</summary>
Motivation: 传统做市模型通常依赖期末库存惩罚来管理日终风险，但忽略了闭市拍卖这一重要流动性事件。本文旨在开发一个能明确考虑闭市拍卖机制的做市框架。

Method: 提出基于深度Q学习的做市框架，包含生成随机市场模型模拟交易会话，在两种设置下应用：1）中间价遵循粗糙Heston模型；2）使用S&P 500指数资产的历史数据。

Result: 开发的理论模型和深度Q学习方法在模拟器和历史数据上应用，并与经典最优做市基准进行比较，评估算法性能。

Conclusion: 通过明确考虑闭市拍卖机制，提出的深度学习框架能够更好地管理做市商在连续交易和闭市拍卖阶段的库存风险，相比传统方法有改进潜力。

Abstract: In this work, we investigate the market-making problem on a trading session in which a continuous phase on a limit order book is followed by a closing auction. Whereas standard optimal market-making models typically rely on terminal inventory penalties to manage end-of-day risk, ignoring the significant liquidity events available in closing auctions, we propose a Deep Q-Learning framework that explicitly incorporates this mechanism. We introduce a market-making framework designed to explicitly anticipate the closing auction, continuously refining the projected clearing price as the trading session evolves. We develop a generative stochastic market model to simulate the trading session and to emulate the market. Our theoretical model and Deep Q-Learning method is applied on the generator in two settings: (1) when the mid price follows a rough Heston model with generative data from this stochastic model; and (2) when the mid price corresponds to historical data of assets from the S&P 500 index and the performance of our algorithm is compared with classical benchmarks from optimal market making.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [377] [Data-Driven Information-Theoretic Causal Bounds under Unmeasured Confounding](https://arxiv.org/abs/2601.17160)
*Yonghan Jung,Bogyeong Kang*

Main category: stat.ML

TL;DR: 提出信息论框架，在未测量混杂下实现因果效应的尖锐部分识别，无需外部参数、辅助变量、完整结构模型或结果有界假设。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在四大局限：依赖限制性假设（如有界或离散结果）、需要外部输入（如工具变量、代理变量或用户指定的敏感性参数）、需要完整结构因果模型规范、仅关注总体平均而忽略协变量条件处理效应。

Method: 建立信息论、数据驱动的散度界限。关键理论贡献：证明观测分布P(Y|A=a,X=x)与干预分布P(Y|do(A=a),X=x)之间的f-散度仅由倾向得分函数上界，从而实现条件因果效应的尖锐部分识别。开发满足Neyman正交性的半参数估计器，确保即使使用灵活机器学习方法估计nuisance函数也能获得√n一致推断。

Result: 模拟研究和真实世界数据应用表明，该框架在各种数据生成过程中提供紧密且有效的因果界限。已在GitHub仓库实现。

Conclusion: 该信息论框架同时克服了现有方法的四大局限，实现了在未测量混杂下直接从观测数据中尖锐部分识别条件因果效应，无需外部敏感性参数、辅助变量、完整结构规范或结果有界假设。

Abstract: We develop a data-driven information-theoretic framework for sharp partial identification of causal effects under unmeasured confounding. Existing approaches often rely on restrictive assumptions, such as bounded or discrete outcomes; require external inputs (for example, instrumental variables, proxies, or user-specified sensitivity parameters); necessitate full structural causal model specifications; or focus solely on population-level averages while neglecting covariate-conditional treatment effects. We overcome all four limitations simultaneously by establishing novel information-theoretic, data-driven divergence bounds. Our key theoretical contribution shows that the f-divergence between the observational distribution P(Y | A = a, X = x) and the interventional distribution P(Y | do(A = a), X = x) is upper bounded by a function of the propensity score alone. This result enables sharp partial identification of conditional causal effects directly from observational data, without requiring external sensitivity parameters, auxiliary variables, full structural specifications, or outcome boundedness assumptions. For practical implementation, we develop a semiparametric estimator satisfying Neyman orthogonality (Chernozhukov et al., 2018), which ensures square-root-n consistent inference even when nuisance functions are estimated using flexible machine learning methods. Simulation studies and real-world data applications, implemented in the GitHub repository (https://github.com/yonghanjung/Information-Theretic-Bounds), demonstrate that our framework provides tight and valid causal bounds across a wide range of data-generating processes.

</details>


### [378] [Error Analysis of Bayesian Inverse Problems with Generative Priors](https://arxiv.org/abs/2601.17374)
*Bamdad Hosseini,Ziqi Huang*

Main category: stat.ML

TL;DR: 论文分析了使用生成模型作为先验解决反问题的误差界限，证明了后验误差在Wasserstein-1距离下会继承先验的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于数据驱动的反问题解决方法因机器学习技术的兴起而广受欢迎。其中一种流行方法是训练生成模型学习特定问题的先验分布，但缺乏对这种方法的理论误差分析。

Method: 使用最小Wasserstein-2生成模型作为先验，在特定假设下分析后验误差。通过理论推导证明误差界限，并用数值实验验证理论结果。

Result: 证明了在生成先验下，后验误差在Wasserstein-1距离中会继承先验的收敛速率。数值实验验证了误差分析的关键方面，包括椭圆PDE反问题中生成先验对非平稳场的建模效果。

Conclusion: 该研究为使用生成模型作为先验解决反问题提供了理论保证，证明了后验误差的收敛性质，并通过数值实验验证了理论分析的有效性。

Abstract: Data-driven methods for the solution of inverse problems have become widely popular in recent years thanks to the rise of machine learning techniques. A popular approach concerns the training of a generative model on additional data to learn a bespoke prior for the problem at hand. In this article we present an analysis for such problems by presenting quantitative error bounds for minimum Wasserstein-2 generative models for the prior. We show that under some assumptions, the error in the posterior due to the generative prior will inherit the same rate as the prior with respect to the Wasserstein-1 distance. We further present numerical experiments that verify that aspects of our error analysis manifests in some benchmarks followed by an elliptic PDE inverse problem where a generative prior is used to model a non-stationary field.

</details>


### [379] ["Rebuilding" Statistics in the Age of AI: A Town Hall Discussion on Culture, Infrastructure, and Training](https://arxiv.org/abs/2601.17510)
*David L. Donoho,Jian Kang,Xihong Lin,Bhramar Mukherjee,Dan Nettleton,Rebecca Nugent,Abel Rodriguez,Eric P. Xing,Tian Zheng,Hongtu Zhu*

Main category: stat.ML

TL;DR: 2024 JSM town hall讨论统计学在AI时代如何演变，围绕学科文化、数据工作、现代建模、AI应用培训、利益相关者合作等五个核心问题展开开放式对话。


<details>
  <summary>Details</summary>
Motivation: 记录2024年联合统计会议关于"AI时代的统计学"的专题讨论，旨在捕捉统计学家对AI、基础模型、大规模经验建模和数据密集型基础设施发展的真实反应和观点，促进学科反思。

Method: 采用开放式小组讨论和广泛观众问答的形式，避免正式演讲或准备陈述，强调经验驱动的坦诚交流，对讨论内容进行最小编辑的整理归档。

Result: 形成了围绕五个核心问题的详细对话记录：1)学科文化与实践 2)数据管理与"数据工作" 3)与现代经验建模的互动 4)大规模AI应用培训 5)与关键AI利益相关者的合作关系。

Conclusion: 该预印本通过保存讨论的档案记录，支持透明度、社区反思和持续对话，帮助统计学界思考在数据和AI为中心的未来中的演变角色。

Abstract: This article presents the full, original record of the 2024 Joint Statistical Meetings (JSM) town hall, "Statistics in the Age of AI," which convened leading statisticians to discuss how the field is evolving in response to advances in artificial intelligence, foundation models, large-scale empirical modeling, and data-intensive infrastructures. The town hall was structured around open panel discussion and extensive audience Q&A, with the aim of eliciting candid, experience-driven perspectives rather than formal presentations or prepared statements. This document preserves the extended exchanges among panelists and audience members, with minimal editorial intervention, and organizes the conversation around five recurring questions concerning disciplinary culture and practices, data curation and "data work," engagement with modern empirical modeling, training for large-scale AI applications, and partnerships with key AI stakeholders. By providing an archival record of this discussion, the preprint aims to support transparency, community reflection, and ongoing dialogue about the evolving role of statistics in the data- and AI-centric future.

</details>


### [380] [Boosting methods for interval-censored data with regression and classification](https://arxiv.org/abs/2601.17973)
*Yuan Bian,Grace Y. Yi,Wenqing He*

Main category: stat.ML

TL;DR: 提出针对区间删失数据的非参数提升方法，通过删失无偏变换调整损失函数，在回归和分类任务中实现稳健预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统提升算法针对完全观测数据设计，难以处理区间删失数据。这类数据在生存分析、医学研究等领域常见，需要专门方法有效处理。

Method: 使用删失无偏变换调整损失函数，通过变换响应值插补，采用函数梯度下降实现，确保可扩展性和适应性。

Result: 建立了理论性质（最优性和均方误差权衡），经验研究显示在各种有限样本场景下具有稳健性能。

Conclusion: 为区间删失数据提供了稳健的提升框架，扩展了现有提升技术的适用性，在医学研究、可靠性工程等领域具有实用价值。

Abstract: Boosting has garnered significant interest across both machine learning and statistical communities. Traditional boosting algorithms, designed for fully observed random samples, often struggle with real-world problems, particularly with interval-censored data. This type of data is common in survival analysis and time-to-event studies where exact event times are unobserved but fall within known intervals. Effective handling of such data is crucial in fields like medical research, reliability engineering, and social sciences. In this work, we introduce novel nonparametric boosting methods for regression and classification tasks with interval-censored data. Our approaches leverage censoring unbiased transformations to adjust loss functions and impute transformed responses while maintaining model accuracy. Implemented via functional gradient descent, these methods ensure scalability and adaptability. We rigorously establish their theoretical properties, including optimality and mean squared error trade-offs. Our proposed methods not only offer a robust framework for enhancing predictive accuracy in domains where interval-censored data are common but also complement existing work, expanding the applicability of existing boosting techniques. Empirical studies demonstrate robust performance across various finite-sample scenarios, highlighting the practical utility of our approaches.

</details>


### [381] [A Cherry-Picking Approach to Large Load Shaping for More Effective Carbon Reduction](https://arxiv.org/abs/2601.17990)
*Bokan Chen,Raiden Hasegawa,Adriaan Hilbers,Ross Koningstein,Ana Radovanović,Utkarsh Shah,Gabriela Volpato,Mohamed Ahmed,Tim Cary,Rod Frowd*

Main category: stat.ML

TL;DR: 该研究通过ERCOT电网模拟分析不同负荷整形策略对碳排放和电力成本的影响，发现基于LMP的策略效果较好但仍有改进空间，提出基于电网信号和历史数据的"择优选择"策略。


<details>
  <summary>Details</summary>
Motivation: 数据中心等大型负荷的整形会影响电网调度，进而影响系统碳排放和能源成本，但缺乏详细的对比数据来验证现有策略的有效性。

Method: 使用ERCOT日前直流最优潮流模拟进行反事实分析，评估多种负荷整形策略对电网碳排放和电力成本的影响。

Result: 基于LMP的负荷整形在减少年度碳排放方面优于其他常见策略，但仍有显著改进空间。在不同电网条件下分析策略性能，提出了基于可观测电网信号和历史数据的"择优选择"方法。

Conclusion: "择优选择"的负荷整形方法适用于数据中心、分布式能源和虚拟电厂等大型灵活电力消费者，能更有效地减少碳排放和电力成本。

Abstract: Shaping multi-megawatt loads, such as data centers, impacts generator dispatch on the electric grid, which in turn affects system CO2 emissions and energy cost. Substantiating the effectiveness of prevalent load shaping strategies, such as those based on grid-level average carbon intensity, locational marginal price, or marginal emissions, is challenging due to the lack of detailed counterfactual data required for accurate attribution. This study uses a series of calibrated granular ERCOT day-ahead direct current optimal power flow (DC-OPF) simulations for counterfactual analysis of a broad set of load shaping strategies on grid CO2 emissions and cost of electricity. In terms of annual grid level CO2 emissions reductions, LMP-based shaping outperforms other common strategies, but can be significantly improved upon. Examining the performance of practicable strategies under different grid conditions motivates a more effective load shaping approach: one that "cherry-picks" a daily strategy based on observable grid signals and historical data. The cherry-picking approach to power load shaping is applicable to any large flexible consumer on the electricity grid, such as data centers, distributed energy resources and Virtual Power Plants (VPPs).

</details>


### [382] [Nonlinear multi-study factor analysis](https://arxiv.org/abs/2601.18128)
*Gemma E. Moran,Anandi Krishnan*

Main category: stat.ML

TL;DR: 提出多研究稀疏变分自编码器，用于从多组高维数据中识别共享和特定因子，应用于血小板基因表达数据分析。


<details>
  <summary>Details</summary>
Motivation: 多研究高维数据中，需要区分哪些潜在因子是所有研究共享的，哪些是特定研究独有的。以血小板基因表达数据为例，需要识别哪些基因簇（生物通路）在所有疾病中都活跃，哪些仅特定疾病活跃。

Method: 提出非线性多研究因子模型，使用多研究稀疏变分自编码器进行拟合。模型具有稀疏性：每个观测特征仅依赖于少量潜在因子；并隐含对潜在因子数量的惩罚，帮助区分共享与特定因子。

Result: 证明了潜在因子的可识别性，并在血小板基因表达数据中展示了方法能恢复有意义的因子。

Conclusion: 提出的多研究稀疏变分自编码器能有效从多研究高维数据中识别共享和特定因子，在基因组学数据分析中具有实际应用价值。

Abstract: High-dimensional data often exhibit variation that can be captured by lower dimensional factors. For high-dimensional data from multiple studies or environments, one goal is to understand which underlying factors are common to all studies, and which factors are study or environment-specific. As a particular example, we consider platelet gene expression data from patients in different disease groups. In this data, factors correspond to clusters of genes which are co-expressed; we may expect some clusters (or biological pathways) to be active for all diseases, while some clusters are only active for a specific disease. To learn these factors, we consider a nonlinear multi-study factor model, which allows for both shared and specific factors. To fit this model, we propose a multi-study sparse variational autoencoder. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. In the genomics example, this means each gene is active in only a few biological processes. Further, the model implicitly induces a penalty on the number of latent factors, which helps separate the shared factors from the group-specific factors. We prove that the latent factors are identified, and demonstrate our method recovers meaningful factors in the platelet gene expression data.

</details>


### [383] [Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes](https://arxiv.org/abs/2601.18145)
*Heguang Lin,Binhao Chen,Mengze Li,Daniel Pimentel-Alarcón,Matthew L. Malloy*

Main category: stat.ML

TL;DR: 提出一种认证算法，用于判断两个观察到的多项分布结果的最小体积置信集（MVCs）是否相交，解决了A/B测试中的核心决策问题。


<details>
  <summary>Details</summary>
Motivation: 最小体积置信集（MVCs）在多项分布参数估计中是最优的，但其精确p值是间断且难以计算的。传统方法难以直接刻画MVCs的几何形状，因此研究一个实际决策问题：给定两个观察到的多项分布结果，能否认证它们的MVCs是否相交。

Method: 利用似然排序在对数几率坐标中诱导半空间约束，通过自适应几何划分参数空间，计算每个单元上p值的可计算上下界。对于三个类别，该方法产生高效且可证明可靠的算法，可以认证相交、认证不相交或在决策处于规定边界内时返回不确定结果。

Result: 开发出可靠认证的决策程序，能够处理A/B测试中的核心任务。对于三维情况，算法高效且可证明可靠；方法还可扩展到更高维度。

Conclusion: 尽管MVCs具有不规则的几何形状，但它们允许对A/B测试中的核心任务进行可靠的认证决策程序，为实际应用提供了可行的解决方案。

Abstract: Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.

</details>


### [384] [Out-of-Distribution Radar Detection with Complex VAEs: Theory, Whitening, and ANMF Fusion](https://arxiv.org/abs/2601.18677)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: stat.ML

TL;DR: 提出基于复数变分自编码器(CVAE)的海杂波中弱信号检测方法，通过OOD检测和与ANMF融合，在非高斯海杂波中实现更高检测概率


<details>
  <summary>Details</summary>
Motivation: 解决海杂波等非高斯、距离变化干扰中的弱复数信号检测问题，传统检测器在复杂海杂波环境中性能受限

Method: 使用复数变分自编码器(CVAE)进行离群检测，直接在I/Q样本上操作以保留相位和多普勒结构，评估两种配置：未处理距离剖面和局部白化后；并与ANMF通过加权对数概率融合规则在决策级集成

Result: CVAE在两种配置下都能在匹配虚警率下获得更高检测概率，白化配置下改进最显著；融合CVAE-ANMF方案在强非高斯杂波中具有增强的鲁棒性，并能实现经验校准的虚警控制

Conclusion: 统计归一化与复数生成建模相结合显著改善了实际海杂波条件下的检测性能，融合CVAE-ANMF方案构成了基于模型检测器的有竞争力替代方案

Abstract: We investigate the detection of weak complex-valued signals immersed in non-Gaussian, range-varying interference, with emphasis on maritime radar scenarios. The proposed methodology exploits a Complex-valued Variational AutoEncoder (CVAE) trained exclusively on clutter-plus-noise to perform Out-Of-Distribution detection. By operating directly on in-phase / quadrature samples, the CVAE preserves phase and Doppler structure and is assessed in two configurations: (i) using unprocessed range profiles and (ii) after local whitening, where per-range covariance estimates are obtained from neighboring profiles. Using extensive simulations together with real sea-clutter data from the CSIR maritime dataset, we benchmark performance against classical and adaptive detectors (MF, NMF, AMF-SCM, ANMF-SCM, ANMF-Tyler). In both configurations, the CVAE yields a higher detection probability Pd at matched false-alarm rate Pfa, with the most notable improvements observed under whitening. We further integrate the CVAE with the ANMF through a weighted log-p fusion rule at the decision level, attaining enhanced robustness in strongly non-Gaussian clutter and enabling empirically calibrated Pfa control under H0. Overall, the results demonstrate that statistical normalization combined with complex-valued generative modeling substantively improves detection in realistic sea-clutter conditions, and that the fused CVAE-ANMF scheme constitutes a competitive alternative to established model-based detectors.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [385] [VIX and European options with jumps in the short-maturity regime](https://arxiv.org/abs/2601.17248)
*Desen Guo,Dan Pirjol,Xiaoyu Wang,Lingjiong Zhu*

Main category: q-fin.PR

TL;DR: 研究局部-随机波动率模型下VIX和欧式期权价格的短期到期渐近性，包含复合泊松跳跃，涵盖价外和价内情况，得到闭式解并应用于三个具体模型


<details>
  <summary>Details</summary>
Motivation: 在局部-随机波动率模型中，包含跳跃的短期到期渐近行为分析对于理解VIX和期权价格在短期内的行为特征具有重要意义，特别是在风险管理和高频交易场景下

Method: 采用渐近分析方法，研究局部-随机波动率模型下VIX和欧式期权价格的短期到期行为，考虑复合泊松跳跃过程，分别推导价外和价内情况的渐近表达式

Result: 获得了闭式的一阶渐近表达式，并将结果应用于三个具体模型：Eraker模型、Kou型模型和折叠正态模型，数值实验验证了渐近预测的准确性

Conclusion: 该研究为包含跳跃的局部-随机波动率模型提供了有效的短期渐近分析框架，所得闭式解可用于快速计算和模型校准，在实践中有重要应用价值

Abstract: We present a study of the short-maturity asymptotics for VIX and European option prices in local-stochastic volatility models with compound Poisson jumps. Both out-of-the-money (OTM) and at-the-money (ATM) asymptotics are considered. The leading-order asymptotics are obtained in closed-form. We apply our results to three examples: the Eraker model, a Kou-type model, and a folded normal model. Numerical illustrations are provided for these three examples that show the accuracy of predictions based on the asymptotic results.

</details>


### [386] [Optimal strategy and deep hedging for share repurchase programs](https://arxiv.org/abs/2601.18686)
*Stefano Corti,Roberto Daluiso,Andrea Pallavicini*

Main category: q-fin.PR

TL;DR: 开发机器学习框架优化股票回购对冲策略，结合执行与对冲，通过无差异定价计算银行折扣


<details>
  <summary>Details</summary>
Motivation: 公司常采用股票回购计划向股东返还资本，但投资银行在执行回购时面临对冲挑战，特别是当合同条款或市场法规限制依赖希腊字母（Greeks）进行风险管理时

Method: 开发机器学习框架，确定最优回购执行策略，同时明确考虑银行的实际交易能力，将执行与对冲统一处理，利用风险度量作为目标函数，采用无差异定价概念计算折扣

Result: 该框架实现了显著的性能改进，产生了优化的策略，提供了可行且现实的对冲方法，能够捕捉实际执行表现

Conclusion: 通过机器学习框架统一处理执行与对冲，为股票回购计划提供了有效的风险管理解决方案，利用无差异定价方法能够准确计算银行向客户提供的折扣

Abstract: In recent decades, companies have frequently adopted share repurchase programs to return capital to shareholders or for other strategic purposes, instructing investment banks to rapidly buy back shares on their behalf. When the executing institution is allowed to hedge its exposure, it encounters several challenges due to the intrinsic features of the product. Moreover, contractual clauses or market regulations on trading activity may make it infeasible to rely on Greeks. In this work, we address the hedging of these products by developing a machine-learning framework that determines the optimal execution of the buyback while explicitly accounting for the bank's actual trading capabilities. This unified treatment of execution and hedging yields substantial performance improvements, resulting in an optimized policy that provides a feasible and realistic hedging approach. The pricing of these programs can be framed in terms of the discount that banks offer to the client on the price at which the shares are delivered. Since, in our framework, risk measures serve as objective functions, we exploit the concept of indifference pricing to compute this discount, thereby capturing the actual execution performance.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [387] [Set-Based Reachability for Low-Thrust Spacecraft in Two-Body and Cislunar Dynamical Systems](https://arxiv.org/abs/2601.17155)
*Jinaykumar Patel,Kamesh Subbarao*

Main category: eess.SY

TL;DR: 论文研究将基于zonotope的可达性分析应用于低推力航天器在二体和地月空间环境中的轨迹分析与控制。


<details>
  <summary>Details</summary>
Motivation: 需要为低推力航天器在复杂动力学环境（包括二体问题和地月空间）中提供可扩展的安全轨迹分析与控制框架，以支持安全轨迹生成和跟踪。

Method: 使用基于集合的方法，通过泰勒展开近似非线性系统，生成可达集；探索状态依赖系数参数化将非线性动力学表示为伪线性形式，实现高效的矩阵传播；应用于地球-火星转移和地月场景。

Result: 成功生成了二体和CR3BP动力学下的可达集，用于安全轨迹生成和跟踪；比较了模型预测控制和LQR保持控制的效果。

Conclusion: 提出的方法为分析复杂动力学和控制约束下的航天器行为提供了一个可扩展的框架，适用于多种空间任务场景。

Abstract: This paper investigates the application of zonotope-based reachability analysis to low-thrust spacecraft in both two-body and cislunar environments. Reachable sets are generated under two-body and circular restricted three-body (CR3BP) dynamics using set-based methods that approximate nonlinear systems via Taylor expansions. A state-dependent coefficient (SDC) parameterization is also explored to represent nonlinear dynamics in a pseudo-linear form, enabling efficient matrix based propagation of reachable sets. Applications include Earth-Mars transfer and cislunar scenarios such as L1 and L2 Halo orbits and Near Rectilinear Halo Orbits (NRHOs). The resulting reachable sets are used for safe trajectory generation and tracking, with comparisons drawn between model predictive control (MPC) and LQR-based station-keeping. The proposed approach provides a scalable framework for analyzing spacecraft behavior under complex dynamics and control constraints.

</details>


### [388] [Autonomous Mars Rover Module for Soil Sampling and Life Component Analysis](https://arxiv.org/abs/2601.17158)
*Bibek Adhikari,Rishab Rijal,Rakesh Yadav,Nikchey Khatri,Sandesh Dhakal*

Main category: eess.SY

TL;DR: 提出一种集成在火星车上的自主生命探测模块，能够挖掘土壤、采集样本并进行生化测试，以检测火星上是否存在生命迹象


<details>
  <summary>Details</summary>
Motivation: 随着技术进步和对宇宙理解的加深，寻找地外生命成为科学探索的重要方向。火星上发现水引发了生命存在的可能性问题，需要开发有效的方法来探测火星生命

Method: 设计一个集成在火星车上的自主生命探测模块，包含机械系统（钻探机制和真空系统）和生化分析系统，能够前往指定区域、挖掘土壤、采集样本并在车上进行生化测试

Result: 该模块能够成功检测收集的土壤颗粒中是否存在生命成分，为地外环境中的自主生命探测提供了概念验证

Conclusion: 这种生命探测模块作为地外环境中自主生命探测的概念验证，为未来的探索任务奠定了基础

Abstract: The search for extraterrestrial life has long been a primary focus of scientific exploration, driven by rapid advancements in technology and our understanding of the universe. The discovery of water on Mars has sparked significant interest, raising the question of whether life could exist on the planet. This study proposes a novel approach to simulate and illustrate the detection of life using a proof-of-life module integrated into a Mars rover. The module is an autonomous system capable of traveling to designated regions, excavating soil, collecting samples, and performing biochemical testing onboard the rover itself. The project is inherently multidisciplinary, integrating mechanical systems such as a drill mechanism and a vacuum system, alongside biochemical analysis for soil testing. The module is capable of successfully detecting the presence or absence of living components of life from the collected soil particles. This proof-of-life module serves as a proof-of-concept for autonomous life detection in extraterrestrial environments and lays the foundation for future exploration missions.

</details>


### [389] [Robust and learning-augmented algorithms for degradation-aware battery optimization](https://arxiv.org/abs/2601.17193)
*Jack Umenberger,Anna Osguthorpe Rasmussen*

Main category: eess.SY

TL;DR: 基于在线镜像下降的电池储能系统收益最大化算法，在随机和对抗性电价下均表现良好，并支持学习增强的信任建议机制。


<details>
  <summary>Details</summary>
Motivation: 电网级电池储能系统面临电价不确定性和电池退化影响寿命的挑战，需要设计既能最大化收益又能应对不确定性的在线优化算法。

Method: 将问题建模为在线资源分配问题，提出基于在线镜像下降的算法，在随机i.i.d.设置下实现无遗憾，在对抗性设置下获得有限渐近竞争比。当有退化机会成本的不可信建议时，提出学习增强算法。

Result: 算法在随机设置下无遗憾，在对抗性设置下具有鲁棒性。学习增强算法在建议准确时表现良好（一致性），在建议差时仍保持鲁棒性。

Conclusion: 提出的算法框架能有效解决电池储能系统在不确定电价下的收益最大化问题，平衡了随机和对抗性环境下的性能，并通过学习增强机制利用外部建议提升性能。

Abstract: This paper studies the problem of maximizing revenue from a grid-scale battery energy storage system, accounting for uncertain future electricity prices and the effect of degradation on battery lifetime. We formulate this task as an online resource allocation problem. We propose an algorithm, based on online mirror descent, that is no-regret in the stochastic i.i.d. setting and attains finite asymptotic competitive ratio in the adversarial setting (robustness). When untrusted advice about the opportunity cost of degradation is available, we propose a learning-augmented algorithm that performs well when the advice is accurate (consistency) while still retaining robustness properties when the advice is poor.

</details>


### [390] [AstroTimer: Rethinking Non-Access Stratum Timers in LEO Constellations](https://arxiv.org/abs/2601.17195)
*Arshiya Rezaie Hezaveh,Peng Hu*

Main category: eess.SY

TL;DR: AstroTimer：针对LEO卫星网络的轻量级自适应NAS定时器框架，优化5G注册过程，显著降低注册时间、重试频率和UE能耗


<details>
  <summary>Details</summary>
Motivation: 现有3GPP NAS定时器继承自地面和GEO/MEO系统，无法适应LEO星座快速变化的拓扑、波动延迟和有限资源，导致信令风暴和效率低下

Method: 提出AstroTimer框架，基于LEO特定参数（链路可变性、处理延迟、网络功能部署）建立闭式定时器模型，优化5G注册过程的看门狗定时器和退避定时器

Result: 仿真结果显示，相比3GPP默认设置，AstroTimer显著降低注册时间、重试频率和UE能耗，同时防止信令过载

Conclusion: AstroTimer为可靠、高效、可扩展的非地面5G/6G部署提供了运营商就绪的基础框架

Abstract: Low-Earth Orbit (LEO) constellations expand 5G coverage to remote regions but differ fundamentally from terrestrial networks due to rapidly changing topologies, fluctuating delays, and constrained onboard resources. Existing 3GPP Non-Access Stratum (NAS) timers, inherited from terrestrial and geostationary (GEO) or medium Earth orbit (MEO) systems, fail to accommodate these dynamics, leading to signaling storms and inefficiency. This paper introduces AstroTimer, a lightweight, adaptive framework for sizing NAS timers based on LEO-specific parameters such as link variability, processing delays, and network-function placement. AstroTimer derives a closed-form timer model with low computational cost and optimizes both watchdog and backoff timers for the 5G registration procedure. Simulation results demonstrate that AstroTimer significantly reduces registration time, retry frequency, and user equipment (UE) energy consumption compared to 3GPP defaults, while preventing signaling overloads. The proposed approach provides an operator-ready foundation for reliable, efficient, and scalable non-terrestrial 5G/6G deployments.

</details>


### [391] [Polynomial Chaos-based Input Shaper Design under Time-Varying Uncertainty](https://arxiv.org/abs/2601.17209)
*Johannes Güttler,Karan Baker,Premjit Saha,James Warner,Adrian Stein*

Main category: eess.SY

TL;DR: 该研究将多项式混沌展开应用于输入整形器设计，以保持受不确定性影响的动态系统的鲁棒性，并特别针对时变不确定性采用侵入式多项式混沌展开方法。


<details>
  <summary>Details</summary>
Motivation: 动态系统常受不确定性影响，传统输入整形器设计可能缺乏鲁棒性。需要一种能处理时变不确定性的方法，以提高系统在不确定条件下的振动抑制性能。

Method: 采用侵入式多项式混沌展开方法进行输入整形器设计，应用于具有时变弹簧刚度不确定性的弹簧质量系统，并与蒙特卡洛方法进行对比。

Result: 该方法在振动抑制方面达到与蒙特卡洛方法相似的精度，但计算效率更高。通过框架评估非鲁棒和鲁棒输入整形器，找到了能最小化残余能量的设计。

Conclusion: 侵入式多项式混沌展开是处理时变不确定性下输入整形器设计的有效方法，在保持精度的同时显著提高了计算效率，为不确定性动态系统的鲁棒控制提供了新途径。

Abstract: The work presented here investigates the application of polynomial chaos expansion toward input shaper design in order to maintain robustness in dynamical systems subject to uncertainty. Furthermore, this work intends to specifically address time-varying uncertainty by employing intrusive polynomial chaos expansion. The methodology presented is validated through numerical simulation of intrusive polynomial chaos expansion formulation applied to spring mass system experiencing time-varying uncertainty in the spring stiffness. The system also evaluates non-robust and robust input shapers through the framework in order to identify designs that minimize residual energy. Results indicate that vibration mitigation is achieved at a similar accuracy, yet at higher efficiency compared to a Monte Carlo framework.

</details>


### [392] [Adaptive Input Shaper Design for Unknown Second-Order Systems with Real-Time Parameter Estimation](https://arxiv.org/abs/2601.17210)
*Nyi Nyi Aung,Bradley Wight,Adrian Stein*

Main category: eess.SY

TL;DR: 提出一种带在线参数估计的前馈输入整形框架，用于未知二阶系统，无需系统参数先验知识即可实现精确切换时间控制


<details>
  <summary>Details</summary>
Motivation: 传统输入整形方法需要已知系统参数，但在实际应用中系统参数往往未知或变化，限制了输入整形在振动抑制等应用中的实用性

Method: 采用前馈输入整形框架结合在线参数估计，处理未知二阶系统；自适应输入整形方案考虑系统周期性切换行为，即使初始切换时刻错过也能进行参考整形

Result: 通过仿真验证了该框架的有效性，能够消除对系统参数先验知识的需求，实现精确的切换时间控制

Conclusion: 该框架适用于龙门起重机、3D打印机喷头等运动控制应用的振动抑制，为未知系统提供了实用的输入整形解决方案

Abstract: We propose a feedforward input-shaping framework with online parameter estimation for unknown second-order systems. The proposed approach eliminates the need for prior knowledge of system parameters when designing input shaping for precise switching times by incorporating online estimation for a black-box system. The adaptive input shaping scheme accounts for the system's periodic switching behavior and enables reference shaping even when initial switching instants are missed. The proposed framework is evaluated in simulation and is intended for vibration suppression in motion control applications such as gantry cranes and 3D printer headers.

</details>


### [393] [A new approach for combined model class selection and parameters learning for auto-regressive neural models](https://arxiv.org/abs/2601.17442)
*Corrado Sgadari,Alessio La Bella,Marcello Farina*

Main category: eess.SY

TL;DR: 提出了一种联合选择非线性动态系统模型结构和学习参数的新方法，针对NARXESN网络，通过集合成员方法同时选择最优模型类别并学习参数，适用于控制应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常分别处理模型结构选择和参数学习，难以同时优化。对于具有自回归组件的模型，在参数学习过程中评估模拟性能通常是NP难问题，需要一种能同时处理结构选择和参数学习、并能显式考虑有界测量噪声的鲁棒方法。

Method: 针对NARXESN（非线性自回归外生输入回声状态网络）家族，提出基于集合成员（SM）的新程序。该方法允许同时选择最优模型类别并从数据中学习模型参数，采用鲁棒训练策略，显式考虑有界测量噪声，并在参数学习过程中实现数据一致的模拟性能评估。

Result: 该方法能够识别出简约而准确的模型，适用于控制应用。提出的框架实现了鲁棒训练策略，增强了模型鲁棒性，解决了自回归组件模型模拟性能评估的NP难问题。

Conclusion: 该工作为非线性动态系统辨识提供了一种有效的联合模型结构选择和参数学习方法，特别适用于控制应用，能够处理有界测量噪声并提高模型鲁棒性。

Abstract: This work introduces a novel approach for the joint selection of model structure and parameter learning for nonlinear dynamical systems identification. Focusing on a specific Recurrent Neural Networks (RNNs) family, i.e., Nonlinear Auto-Regressive with eXogenous inputs Echo State Networks (NARXESNs), the method allows to simultaneously select the optimal model class and learn model parameters from data through a new set-membership (SM) based procedure. The results show the effectiveness of the approach in identifying parsimonious yet accurate models suitable for control applications. Moreover, the proposed framework enables a robust training strategy that explicitly accounts for bounded measurement noise and enhances model robustness by allowing data-consistent evaluation of simulation performance during parameter learning, a process generally NP-hard for models with autoregressive components.

</details>


### [394] [Robust Output Regulation of Uncertain Linear Time-Varying Systems](https://arxiv.org/abs/2601.17464)
*Jinmeng Zha,Zhen Zhang*

Main category: eess.SY

TL;DR: 提出了一种线性时变系统鲁棒输出调节的新框架，通过系统浸入方法避免显式求解调节器方程，解决了长期存在的开放问题。


<details>
  <summary>Details</summary>
Motivation: 线性时变系统的鲁棒输出调节问题几十年来一直是一个开放问题，需要新的理论框架来解决这一挑战。

Method: 采用系统浸入方法，将调节器方程重新表述为更直观的形式，揭示内部模型构建等价于通过无强迫系统复制强迫系统的输出轨迹。在坐标无关框架下研究调节器方程，扩展时变非共振条件。

Result: 提出了一种无需显式求解调节器方程的通用鲁棒设计方法，能够抵抗外系统交互不确定性，在需要无限维控制器时实现近似输出调节，并提供最小化内部模型维度的方法。

Conclusion: 为构建基于鲁棒内部模型的控制器提供了一个通用的系统化框架，简化了控制实现过程，解决了线性时变系统鲁棒输出调节的长期开放问题。

Abstract: Robust output regulation for linear time-varying systems has remained an open problem for decades. To address this, we propose intrinsic system immersion by reformulating the regulator equation in a more insightful form, indicating that finding an internal model is equivalent to reproducing the output trajectory of a forced system by constructing an unforced system. This perspective reveals the influence of parametric uncertainties, demonstrating that an infinite-dimensional controller is generally unavoidable for robustness against plant uncertainty. Consequently, a general robust design is proposed without explicitly solving the regulator equation. It ensures robustness against uncertainties in the exosystem interaction, and achieves approximate output regulation when an infinite-dimensional controller is necessary for regulation. Additionally, we study the regulator equation in a coordinate-free framework, extend the time-varying non-resonance condition, and provide a method to minimize the dimension of an internal model. Overall, these results provide a general systematic framework for constructing robust internal model-based controllers, and simplify the control implementation process.

</details>


### [395] [Invited: Toward Sustainable and Transparent Benchmarking for Academic Physical Design Research](https://arxiv.org/abs/2601.17520)
*Liwen Jiang,Andrew B. Kahng,Zhiang Wang,Zhiyu Zheng*

Main category: eess.SY

TL;DR: RosettaStone 2.0是一个基于OpenROAD-Research的开源基准测试翻译与评估框架，提供完整的RTL-to-GDS参考流程，支持传统2D设计和Pin-3D风格3D设计，实现平面与三维实现设置的公平比较。


<details>
  <summary>Details</summary>
Motivation: 为了在平面和三维集成电路实现设置之间提供严谨、公平的比较，并支持透明、可重复的研究，需要一个标准化的评估框架。

Method: 基于OpenROAD-Research构建，集成在OpenROAD-flow-scripts-Research中，提供完整的RTL-to-GDS参考流程，包含基于CI的回归测试和基于METRICS2.1约定的标准化评估管道，生成结构化日志和报告。

Result: 开发了RosettaStone 2.0框架，支持传统2D设计和Pin-3D风格面对面混合键合3D设计，建立了社区面向的排行榜，通过已验证的pull请求和DCO合规性进行管理。

Conclusion: RosettaStone 2.0为平面和三维IC设计实现提供了透明、可重复的基准测试和评估框架，促进了公平比较和社区协作研究。

Abstract: This paper presents RosettaStone 2.0, an open benchmark translation and evaluation framework built on OpenROAD-Research. RosettaStone 2.0 provides complete RTL-to-GDS reference flows for both conventional 2D designs and Pin-3D-style face-to-face (F2F) hybrid-bonded 3D designs, enabling rigorous apples-to-apples comparison across planar and three-dimensional implementation settings. The framework is integrated within OpenROAD-flow-scripts (ORFS)-Research; it incorporates continuous integration (CI)-based regression testing and provides a standardized evaluation pipeline based on the METRICS2.1 convention, with structured logs and reports generated by ORFS-Research. To support transparent and reproducible research, RosettaStone 2.0 further provides a community-facing leaderboard, which is governed by verified pull requests and enforced through Developer Certificate of Origin (DCO) compliance.

</details>


### [396] [Battery-Free and Gateway-Free Cellular IoT Water Leak Detection System](https://arxiv.org/abs/2601.17656)
*Roshan Nepal,Brandon Brown,Shishangbo Yu,Roozbeh Abbasi,Norman Zhou,George Shaker*

Main category: eess.SY

TL;DR: 无电池、无网关的水泄漏检测系统，通过水力发电机制供电，直接通过LTE-M网络通信，实现自主云端数据传输


<details>
  <summary>Details</summary>
Motivation: 传统水泄漏检测系统依赖电池和本地网络基础设施，存在维护成本高、部署受限的问题。需要开发可持续、免维护、全球可扩展的物联网泄漏检测解决方案

Method: 采用水力发电机制为电化学传感器供电，系统包含分区传感模块和专用电源管理子系统（升压转换器、超级电容器储能、迟滞控制负载隔离电路），以应对LTE-M收发器的严格启动和运行功率需求

Result: 实验结果表明，系统能够在水诱导能量生成时持续触发LTE-M信标传输，验证了系统的可行性和可靠性

Conclusion: 该系统展示了可持续、免维护、全球可扩展的物联网泄漏检测应用的潜力，适用于智能基础设施中的水泄漏监测

Abstract: This paper presents a battery-free and gateway-free water leak detection system capable of direct communication over LTE-M (Cat-M1). The system operates solely on energy harvested through a hydroelectric mechanism driven by an electrochemical sensor, thereby removing the need for conventional batteries. To address the stringent startup and operational power demands of LTE-M transceivers, the architecture incorporates a compartmentalized sensing module and a dedicated power management subsystem, comprising a boost converter, supercapacitor based energy storage, and a hysteresis controlled load isolation circuit. This design enables autonomous, direct to cloud data transmission without reliance on local networking infrastructure. Experimental results demonstrate consistent LTE-M beacon transmissions triggered by water induced energy generation, underscoring the system's potential for sustainable, maintenance free, and globally scalable IoT leak detection applications in smart infrastructure.

</details>


### [397] [Battery-less Long-Range LTE-M Water Leak Detector](https://arxiv.org/abs/2601.17660)
*Roshan Nepal,Brandon Brown,Shishangbo Yu,Roozbeh Abbasi,Norman Zhou,George Shaker*

Main category: eess.SY

TL;DR: 提出一种自供电漏水传感器，无需电池和本地网关，利用水作为能量来源，通过电化学采集器收集能量，并支持LTE-M蜂窝通信


<details>
  <summary>Details</summary>
Motivation: 传统漏水传感器需要电池供电和本地网关，维护成本高且部署受限。需要开发无需电池、自供电的传感器，能够在基础设施匮乏地区工作

Method: 集成双室电化学能量采集器、低输入升压转换器与超级电容储能、比较器门控的LTE-M无线电（基于Nordic Thingy:91平台）。系统在检测到水时从休眠状态唤醒，利用水作为能量来源

Result: 实验室测试证实系统能够在有水存在时从休眠状态唤醒，收集足够能量，并利用水作为电源重复发送云信标。系统兼容3GPP标准蜂窝协议

Conclusion: 该自供电漏水传感器消除了对电池和本地网关的依赖，为基础设施匮乏地区的漏水监测提供了解决方案。系统兼容性使其能够支持未来的非地面5G网络连接

Abstract: This work presents a self powered water leak sensor that eliminates both batteries and local gateways. The design integrates a dual compartment electrochemical harvester, a low input boost converter with supercapacitor storage, and a comparator gated LTE-M radio built on the Nordic Thingy:91 platform. Laboratory tests confirm that the system can be awakened from a dormant state in the presence of water, harvest sufficient energy, and issue repeated cloud beacons using the water exposure as the power source. Beyond conventional LTE-M deployments, the system's compatibility with 3GPP standard cellular protocols paves the way for future connectivity via non terrestrial 5G networks, enabling coverage in infrastructure scarce regions.

</details>


### [398] [Composite Adaptive Control Barrier Functions for Safety-Critical Systems with Parametric Uncertainty](https://arxiv.org/abs/2601.17683)
*Mohammadreza Kamaldar*

Main category: eess.SY

TL;DR: 提出复合自适应控制屏障函数(CaCBF)算法，在存在线性参数不确定性的非线性仿射系统中保证安全集的前向不变性，无需参数收敛。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：传统控制屏障函数需要精确系统模型，参数不确定性会破坏安全性保证；鲁棒方法通过最坏情况边界保持安全但限制性能；模块化学习方案将估计与安全解耦，但在训练期间允许状态违规。

Method: 提出复合自适应控制屏障函数(CaCBF)算法，从包含对数安全屏障、控制Lyapunov函数和参数误差项的复合能量函数推导自适应律，用于处理线性参数不确定性的非线性仿射系统。

Result: CaCBF保证安全集的前向不变性和闭环系统的一致有界性，安全性保证无需参数收敛。通过自适应巡航控制、全向机器人和平面无人机仿真验证了算法有效性。

Conclusion: CaCBF算法在存在参数不确定性的情况下提供了严格的安全性保证，克服了现有方法的局限性，在保持性能的同时确保安全。

Abstract: Control barrier functions guarantee safety but typically require accurate system models. Parametric uncertainty invalidates these guarantees. Existing robust methods maintain safety via worst-case bounds, limiting performance, while modular learning schemes decouple estimation from safety, permitting state violations during training. This paper presents the composite adaptive control barrier function (CaCBF) algorithm for nonlinear control-affine systems subject to linear parametric uncertainty. We derive adaptation laws from a composite energy function comprising a logarithmic safety barrier, a control Lyapunov function, and a parameter error term. We prove that CaCBF guarantees the forward invariance of the safe set and the uniform boundedness of the closed-loop system. This safety guarantee holds without requiring parameter convergence. Simulations of adaptive cruise control, an omnidirectional robot, and a planar drone demonstrate the efficacy of the CaCBF algorithm.

</details>


### [399] [Space-Air-Ground-Integrated Networks: The BER vs. Residual Delay and Doppler Analysis](https://arxiv.org/abs/2601.17859)
*Chao Zhang,Kunlun Li,Chao Xu,Lie-Liang Yang,Lajos Hanzo*

Main category: eess.SY

TL;DR: 该论文分析了在考虑残余多普勒效应和同步误差的情况下，空天地一体化网络中16-QAM调制的误码率性能，并量化了多种实际因素对系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 空天地一体化网络中，由于多径多普勒效应和爱因斯坦相对论效应，完美的多普勒补偿和同步难以实现。需要研究在残余多普勒和同步延迟影响下，系统在时变相关Shadowed-Rician信道中的误码率性能。

Method: 建立包含相关Shadowed-Rician信道、基于斯涅尔定律的路径损耗、大气吸收、视距多普勒补偿、椭圆卫星轨道和相对论效应的实际SAGIN模型。推导导频与数据符号间的相关系数，用双变量Gamma分布近似信道分布，最终推导出采用最小二乘信道估计和均衡的16-QAM闭式误码率公式。

Result: 分析结果表明：对于300公里高度的低轨卫星，1）实际椭圆轨道周期比理想圆轨道长约0.8秒；2）在整个卫星过境期间相对论延迟小于1微秒。数值结果量化了L波段中残余多普勒、大气阴影、同步误差和导频开销对性能的影响。

Conclusion: 该研究为SAGIN系统提供了实用的性能分析框架，量化了多种实际因素对通信性能的影响，有助于系统设计和优化。

Abstract: Perfect Doppler compensation and synchronization is nontrivial due to multi-path Doppler effects and Einstein's theory of relativity in the space-air-ground-integrated networks (SAGINs). Hence, by considering the residual Doppler and the synchronization delay, this paper investigates the bit-error-rate (BER) performance attained under time-varying correlated Shadowed-Rician SAGIN channels. First, a practical SAGIN model is harnessed, encompassing correlated Shadowed-Rician channels, the Snell's law-based path loss, atmospheric absorption, the line-of-sight Doppler compensation, elliptical satellite orbits, and Einstein's theory of relativity. Then, a specific correlation coefficient between the pilot and data symbols is derived in the context of correlated Shadowed-Rician Channels. By exploiting this correlation coefficient, the channel distribution is mimicked by a bi-variate Gamma distribution. Then, a closed-form BER formula is derived under employing least-square channel estimation and equalization for 16-QAM. Our analytical results indicate for a 300-km-altitude LEO that 1) the period of realistic elliptical orbits is around 0.8 seconds longer than that of the idealized circular orbits; and 2) the relativistic delay is lower than 1 $μs$ over a full LEO pass (from rise to set). Our numerical results for the L bands quantify the effects of: 1) the residual Doppler; 2) atmospheric shadowing; 3) synchronization errors; and 4) pilot overhead.

</details>


### [400] [Photovoltaic energy sharing: Implementation and tests on a real collective self-consumption system](https://arxiv.org/abs/2601.17974)
*Camblong H.,Curea O.,Ugartemendia J. J.,Boussaada Z.,Lizarralde I.,Etxegarai G*

Main category: eess.SY

TL;DR: 分析法国Izarbel科技园区光伏能源共享案例，比较三种分配方式（静态、默认动态、定制动态）对自消纳率和节约成本的影响


<details>
  <summary>Details</summary>
Motivation: 研究集体自消纳中不同光伏能源共享方式的效果，为可再生能源整合提供实践指导，特别是在法国当前能源政策背景下

Method: 在法国Izarbel科技园区进行实地案例研究，使用智能电表和基于LoRa协议的Tecsol TICs设备收集数据，比较三种能源分配方式在四种场景下的表现

Result: 动态分配方式能提高自消纳率，定制动态共享能增加节约成本，特别是在有数据中心和高太阳辐射条件下效果更明显

Conclusion: 定制动态共享是最优的光伏能源分配策略，能同时提升自消纳率和经济效益，为集体自消纳项目提供了有价值的实践参考

Abstract: This research study analyses different types of photovoltaic (PV) energy sharing in a collective self-consumption (CSC) real-case in the Izarbel technological park in France. The analysis is carried out above all from the point of view of the self-consumption rate (SCR) and the savings. After explaining the emergence of the self-consumption concept for the integration of renewable energies, the study case is described. The PV energy is produced in ESTIA1 building and consumed in ESTIA1, 2 and 4 buildings. The main IoT components used to implement the CSC are smart meters and the Tecsol TICs; devices based on the LoRa protocol to retrieve production and consumption data. Then, the characteristics of PV energy sharing in France are explained, in particular the three possible types of energy sharing/allocation (static, dynamic by default and customised dynamic) and the structure of the electricity bill. Finally, the three types of sharing are compared in four scenarios (without and with a data centre, for low and high solar radiation). The results show that the dynamic allocations lead to increases of the SCR and that the customised dynamic sharing increases savings.

</details>


### [401] [Data-driven nonparametric Li-ion battery ageing model aiming at learning from real operation data -- Part A: Storage operation](https://arxiv.org/abs/2601.17978)
*Lucu M.,Martinez-Laserna E.,Gandiaga I.,Liu K.,Camblong H.,Widanage W. D.,Marco J*

Main category: eess.SY

TL;DR: 基于高斯过程的数据驱动锂离子电池老化模型，通过日历老化实验验证，仅需18个测试电池即可在动态/静态温度/SOC条件下实现0.53%容量预测误差


<details>
  <summary>Details</summary>
Motivation: 传统电池老化模型需要大量实验室测试，而工业界正在部署数据采集遥测技术，将产生大量实际运行数据。需要开发能够从现场数据学习的老化模型以减少实验室测试需求。

Method: 在高斯过程框架下开发数据驱动老化模型，针对电池老化应用定制协方差函数。使用32个电池超过3年的测试数据，探索不同训练方案以确定所需的最小实验室测试数量。

Result: 仅用18个测试电池训练的模型，在广泛的动态和静态温度/SOC存储条件下验证，容量曲线预测的整体平均绝对误差为0.53%。

Conclusion: 高斯过程模型能够从新数据中学习，提供更准确和可靠的预测，并扩展模型的操作窗口。该数据驱动方法可显著减少实验室测试需求，利用现场数据改进老化预测。

Abstract: Conventional Li-ion battery ageing models, such as electrochemical, semi-empirical and empirical models, require a significant amount of time and experimental resources to provide accurate predictions under realistic operating conditions. At the same time, there is significant interest from industry in the introduction of new data collection telemetry technology. This implies the forthcoming availability of a significant amount of real-world battery operation data. In this context, the development of ageing models able to learn from in-field battery operation data is an interesting solution to mitigate the need for exhaustive laboratory testing. In a series of two papers, a data-driven ageing model is developed for Li-ion batteries under the Gaussian Process framework. A special emphasis is placed on illustrating the ability of the Gaussian Process model to learn from new data observations, providing more accurate and confident predictions, and extending the operating window of the model. This first paper focusses on the systematic modelling and experimental verification of cell degradation through calendar ageing. A specific covariance function is composed, tailored for use in a battery ageing application. Over an extensive dataset involving 32 cells tested during more than three years, different training possibilities are contemplated in order to quantify the minimal number of laboratory tests required for the design of an accurate ageing model. A model trained with only 18 tested cells achieves an overall mean-absolute-error of 0.53% in the capacity curves prediction, after being validated under a broad window of both dynamic and static temperature and SOC storage conditions.

</details>


### [402] [Data-driven nonparametric Li-ion battery ageing model aiming at learning from real operation data -- Part B: Cycling operation](https://arxiv.org/abs/2601.17983)
*Lucu M.,Martinez-Laserna E.,Gandiaga I.,Liu K.,Camblong H.,Widanage W. D.,Marco J*

Main category: eess.SY

TL;DR: 基于高斯过程框架开发锂离子电池数据驱动老化模型，利用现场运行数据减少实验室测试需求，第二篇论文专注于循环老化建模


<details>
  <summary>Details</summary>
Motivation: 传统锂离子电池老化模型需要大量时间和实验资源，而工业界正在引入新的数据采集遥测技术，将产生大量实际运行数据。开发能够从现场电池运行数据中学习的老化模型可以减少对实验室测试的依赖。

Method: 在高斯过程框架下开发数据驱动老化模型，特别关注模型从新数据中学习的能力。为电池老化应用定制特定的协方差函数，在包含124个电池、测试超过三年的数据集上，考虑不同的训练可能性以量化设计准确老化模型所需的最小实验室测试数量。

Result: 仅使用26个测试电池训练的模型，在广泛的动态和静态循环温度、放电深度、中间SOC、充电和放电倍率条件下验证后，容量曲线预测的总体平均绝对误差达到1.04%。

Conclusion: 高斯过程模型能够有效学习锂离子电池循环老化行为，仅需少量实验室测试即可获得准确预测，为利用现场运行数据开发电池老化模型提供了可行方案。

Abstract: Conventional Li-ion battery ageing models, such as electrochemical, semi-empirical and empirical models, require a significant amount of time and experimental resources to provide accurate predictions under realistic operating conditions. At the same time, there is significant interest from industry in the introduction of new data collection telemetry technology. This implies the forthcoming availability of a significant amount of real-world battery operation data. In this context, the development of ageing models able to learn from in-field battery operation data is an interesting solution to mitigate the need for exhaustive laboratory testing. In a series of two papers, a data-driven ageing model is developed for Li-ion batteries under the Gaussian Process framework. A special emphasis is placed on illustrating the ability of the Gaussian Process model to learn from new data observations, providing more accurate and confident predictions, and extending the operating window of the model. The first paper of the series focussed on the systematic modelling and experimental verification of cell degradation through calendar ageing. Conversantly, this second paper addresses the same research challenge when the cell is electrically cycled. A specific covariance function is composed, tailored for use in a battery ageing application. Over an extensive dataset involving 124 cells tested during more than three years, different training possibilities are contemplated in order to quantify the minimal number of laboratory tests required for the design of an accurate ageing model. A model trained with only 26 tested cells achieves an overall mean-absolute-error of 1.04% in the capacity curve prediction, after being validated under a broad window of both dynamic and static cycling temperatures, Depth-of-Discharge, middle-SOC, charging and discharging C-rates.

</details>


### [403] [Validation of a Software-Defined 100-Gb/s RDMA Streaming Architecture for Ultrafast Optoacoustic and Ultrasound Imaging](https://arxiv.org/abs/2601.18280)
*Federico Villani,Christian Vogt,Luca Specht,Jero Schmid,Xiang Liu,Andrea Cossettini,Daniel Razansky,Luca Benini*

Main category: eess.SY

TL;DR: 提出LtL架构，一种用于超快光声和超声成像的新型数据采集系统，支持大通道数、宽带宽和软件定义操作，实现高达95.6 Gb/s的原始数据连续流传输。


<details>
  <summary>Details</summary>
Motivation: 现有光声成像系统通常依赖为脉冲回波超声成像优化的笨重昂贵硬件，限制了临床转化。光声成像对接收带宽和与外部激光源的时序同步有不同要求，但需要统一的光声-超声成像平台，因为脉冲回波超声仍是软组织可视化的标准工具。

Method: 提出LtL架构，结合最先进的宽带模拟前端、集成FPGA与应用程序处理单元的Zynq UltraScale+ MPSoC，以及支持高达95.6 Gb/s原始数据流的100 GbE RDMA后端。该架构避免了限制可持续帧率和记录间隔的本地缓冲区和突发传输，实现真正的连续原始数据流传输。

Result: 使用商用评估板构建的16通道演示系统验证了LtL架构的核心元素。进一步验证了256通道可扩展性的信号链，确认了支持最先进数据传输速度的宽带宽能力。

Conclusion: LtL架构解决了现有光声-超声成像系统的限制，提供了一种支持大通道数、宽带宽和软件定义操作的新型数据采集解决方案，为超快光声和超声成像的临床转化提供了技术基础。

Abstract: Optoacoustic (OA) imaging has emerged as a powerful investigation tool, with demonstrated applicability in oncology, neuroscience, and cardiovascular biology. However, its clinical translation is limited with the existing OA systems, which often rely on bulky and expensive acquisition hardware mainly optimized for pulse-echo ultrasound (US) imaging. Despite the fact that OA imaging has different requirements for receive bandwidths and timing synchronization with external laser sources, there is a strong need for unified OA-US imaging platforms, as pulse-echo US remains the standard tool for visualizing soft tissues. To address these challenges, we propose a new data acquisition architecture for ultrafast OA and US imaging that fully covers the requirements for large channel counts, wide bandwidth, and software-defined operation. LtL combines state-of-the-art wideband analog front-ends, a Zynq UltraScale+ MPSoC integrating FPGA fabric with an Application Processing Unit, and a 100 GbE Remote Direct Memory Access (RDMA) backend enabling raw-data streaming at up to 95.6 Gb/s. The architecture avoids local buffers followed by burst transfers, which commonly constrain sustainable frame rate and recording intervals, thus achieving true continuous and sustained streaming of raw data. We validate the core elements of the LtL architecture using a 16-channel demonstration system built from commercial evaluation boards. We further verify the signal chain for up to 256-channel scalability, confirming the wide bandwidth capabilities to support state-of-the-art data transmission speeds.

</details>


### [404] [Reinforcement Learning with Distributed MPC for Fuel-Efficient Platoon Control with Discrete Gear Transitions](https://arxiv.org/abs/2601.18294)
*Samuel Mallick,Gianpietro Battocletti,Dimitris Boskos,Azita Dabiri,Bart De Schutter*

Main category: eess.SY

TL;DR: 提出基于强化学习的分布式MPC方法，用于车队协同控制，通过RL策略固定档位选择，简化连续优化问题，降低计算负担。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆编队协同控制能提高交通效率，但分布式协同优化速度和档位时，MPC方法需要同时处理连续动态和离散档位，计算复杂，难以实时实现。

Method: 采用强化学习与分布式MPC结合的方法：为每辆车训练RL策略来选择预测窗口内的档位，将问题简化为连续优化；使用参数化策略将多智能体RL解耦为单智能体学习任务；提出RNN架构处理档位选择，应对指数增长的档位调度组合。

Result: 高速公路驾驶仿真显示，该方法相比纯MPC协同优化，计算负担显著降低，在燃油效率控制方面性能相当。

Conclusion: 提出的RL-based分布式MPC方法能有效解决车队协同控制中的计算复杂度问题，在保持燃油效率性能的同时实现实时可行性，且具有良好的可扩展性。

Abstract: Cooperative control of groups of autonomous vehicles (AVs), i.e., platoons, is a promising direction to improving the efficiency of autonomous transportation systems. In this context, distributed co-optimization of both vehicle speed and gear position can offer benefits for fuel-efficient driving. To this end, model predictive control (MPC) is a popular approach, optimizing the speed and gear-shift schedule while explicitly considering the vehicles' dynamics over a prediction window. However, optimization over both the vehicles' continuous dynamics and discrete gear positions is computationally intensive, and may require overly long sample times or high-end hardware for real-time implementation. This work proposes a reinforcement learning (RL)-based distributed MPC approach to address this issue. For each vehicle in the platoon, a policy is trained to select and fix the gear positions across the prediction window of a local MPC controller, leaving a significantly simpler continuous optimization problem to be solved as part of a distributed MPC scheme. In order to reduce the computational cost of training and facilitate the scalability of the proposed approach to large platoons, the policies are parameterized such that the emergent multi-agent RL problem can be decoupled into single-agent learning tasks. In addition, a recurrent neural-network (RNN) architecture is proposed for the gear selection policy, such that the learning is scalable even as the number of possible gear-shift schedules grows exponentially with the MPC prediction horizon. In highway-driving simulations, the proposed approach is shown to have a significantly lower computation burden and a comparable performance in terms of fuel-efficient platoon control, with respect to pure MPC-based co-optimization.

</details>


### [405] [Convex Chance-Constrained Stochastic Control under Uncertain Specifications with Application to Learning-Based Hybrid Powertrain Control](https://arxiv.org/abs/2601.18313)
*Teruki Kato,Ryotaro Shima,Kenji Kashima*

Main category: eess.SY

TL;DR: 提出严格凸机会约束随机控制框架，处理参考轨迹和操作约束中的不确定性，保证概率约束满足和严格凸性，应用于混合动力系统MPC


<details>
  <summary>Details</summary>
Motivation: 现有随机控制方法在处理非高斯不确定性和控制规范不确定性（如参考轨迹、操作约束）时面临挑战，需要保证概率约束满足的同时保持优化问题的良好数值特性

Method: 联合优化控制输入和风险分配，构建严格凸机会约束随机控制框架；扩展到非线性模型控制时，使用机器学习识别的精确线性化模型

Result: 方法保证概率约束满足，确保严格凸性，从而获得唯一且连续的最优解；在混合动力系统模型预测控制中验证了有效性

Conclusion: 提出的严格凸机会约束随机控制框架能有效处理一般不确定性，保证概率约束满足和数值稳定性，适用于非线性系统控制

Abstract: This paper presents a strictly convex chance-constrained stochastic control framework that accounts for uncertainty in control specifications such as reference trajectories and operational constraints. By jointly optimizing control inputs and risk allocation under general (possibly non-Gaussian) uncertainties, the proposed method guarantees probabilistic constraint satisfaction while ensuring strict convexity, leading to uniqueness and continuity of the optimal solution. The formulation is further extended to nonlinear model-based control using exactly linearizable models identified through machine learning. The effectiveness of the proposed approach is demonstrated through model predictive control applied to a hybrid powertrain system.

</details>


### [406] [Real-Time Prediction of Lower Limb Joint Kinematics, Kinetics, and Ground Reaction Force using Wearable Sensors and Machine Learning](https://arxiv.org/abs/2601.18494)
*Josée Mallah,Yu Zhu,Kailang Xu,Gurvinder S. Virk,Shaoping Bai,Luigi G. Occhipinti*

Main category: eess.SY

TL;DR: 提出基于无线可穿戴传感器和机器学习算法的实时多模态下肢运动捕捉框架，用于估计关节角度、地面反作用力和关节力矩，适用于生物反馈应用。


<details>
  <summary>Details</summary>
Motivation: 行走是生物力学研究的关键运动，但黄金标准的数据采集方法耗时且昂贵。需要开发实时、低成本的可穿戴解决方案。

Method: 使用无线可穿戴传感器和机器学习算法：随机森林从IMU数据估计关节角度；仪器鞋垫预测地面反作用力；基于ResNet-16架构的深度学习从角度和GRF预测关节力矩。

Result: 所有三个模型都取得了良好的准确性，预测以1kHz频率记录，对于20秒输入数据的延迟仅为23毫秒。框架完全依赖可穿戴传感器，覆盖五个主要下肢关节。

Conclusion: 该工作提供了一个实时、多模态、低延迟的下肢运动捕捉框架，适用于生物反馈应用，解决了传统方法耗时昂贵的问题。

Abstract: Walking is a key movement of interest in biomechanics, yet gold-standard data collection methods are time- and cost-expensive. This paper presents a real-time, multimodal, high sample rate lower-limb motion capture framework, based on wireless wearable sensors and machine learning algorithms. Random Forests are used to estimate joint angles from IMU data, and ground reaction force (GRF) is predicted from instrumented insoles, while joint moments are predicted from angles and GRF using deep learning based on the ResNet-16 architecture. All three models achieve good accuracy compared to literature, and the predictions are logged at 1 kHz with a minimal delay of 23 ms for 20s worth of input data. The present work fully relies on wearable sensors, covers all five major lower limb joints, and provides multimodal comprehensive estimations of GRF, joint angles, and moments with minimal delay suitable for biofeedback applications.

</details>


### [407] [Experimental Characterization of ISAC Channel Mapping and Environment Awareness](https://arxiv.org/abs/2601.18558)
*Zhuangzhuang Cui,Rizqi Hersyandika,Haoqiu Xiong,Sofie Pollin*

Main category: eess.SY

TL;DR: 本文通过实验研究了室内毫米波环境中单站感知与自然双站通信信道之间的关系，展示了如何从感知信道中恢复通信多径分量，并获得了关键散射体的雷达截面积。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信（ISAC）背景下，需要理解感知信道与通信信道之间的关系，特别是如何利用单站感知信息来恢复自然双站通信信道，这对于ISAC系统设计具有重要意义。

Method: 在室内毫米波环境中进行实验研究，在联合时延-角度域表征传播信道，提取主导多径分量（MPCs）并将其与环境中的物理散射体关联，展示如何从感知信道中显式恢复通信MPCs。

Result: 成功从感知信道中恢复通信多径分量，并基于校准的信道功率和重建的传播距离获得了两个关键散射体（墙壁和金属板）的雷达截面积（RCS）。

Conclusion: 实验证明了在ISAC系统中，感知信道与通信信道之间存在可量化的关系，能够从单站感知信息中恢复双站通信信道，并为散射体特性表征提供了实用方法。

Abstract: In the context of integrated sensing and communications (ISAC), this paper presents an experimental investigation of the relationship between monostatic sensing and naturally bistatic communication channels in an indoor millimeter-wave environment. We characterize the propagation channel in the joint delay--angle domain, extract dominant multipath components (MPCs) and associate them with physical scatterers in the environment, and demonstrate how communication MPCs can be explicitly recovered from sensing channels. Finally, the radar cross-sections (RCSs) of two key scatterers, namely the wall and metal plate, are obtained based on calibrated channel power and reconstructed propagation distances.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [408] [MarketGANs: Multivariate financial time-series data augmentation using generative adversarial networks](https://arxiv.org/abs/2601.17773)
*Jeonggyu Huh,Seungwon Jeong,Hyun-Gyoon Kim,Hyeng Keun Koo,Byung Hwa Lim*

Main category: q-fin.ST

TL;DR: MarketGAN：基于因子结构的生成对抗框架，用于数据稀缺下的高维资产收益生成，能更好地捕捉资产间的横截面依赖和尾部联动。


<details>
  <summary>Details</summary>
Motivation: 在高维资产收益生成中面临数据稀缺问题，传统方法难以同时保持横截面依赖、尾部联动和时序动态。需要将资产定价因子结构作为经济归纳偏置嵌入生成模型。

Method: 提出MarketGAN框架：1）嵌入显式资产定价因子结构作为经济归纳偏置；2）使用生成对抗学习，以时序卷积网络（TCN）为骨干；3）将收益作为单一联合向量生成，保持横截面依赖；4）建模随机时变因子载荷和波动率，捕捉长期时序依赖。

Result: 使用美国大盘股日度收益数据，MarketGAN比传统因子模型自举方法更好地匹配经验特征：厚尾边际分布、波动率聚类、杠杆效应，特别是高维横截面相关结构和资产间尾部联动。在投资组合应用中，当因子信息至少弱有效时，MarketGAN生成的协方差估计优于其他方法。

Conclusion: MarketGAN通过嵌入因子结构和生成对抗学习，在数据稀缺下有效生成高维资产收益，保持重要的横截面和时序特征，在投资组合应用中具有实际经济价值。

Abstract: This paper introduces MarketGAN, a factor-based generative framework for high-dimensional asset return generation under severe data scarcity. We embed an explicit asset-pricing factor structure as an economic inductive bias and generate returns as a single joint vector, thereby preserving cross-sectional dependence and tail co-movement alongside inter-temporal dynamics. MarketGAN employs generative adversarial learning with a temporal convolutional network (TCN) backbone, which models stochastic, time-varying factor loadings and volatilities and captures long-range temporal dependence. Using daily returns of large U.S. equities, we find that MarketGAN more closely matches empirical stylized facts of asset returns, including heavy-tailed marginal distributions, volatility clustering, leverage effects, and, most notably, high-dimensional cross-sectional correlation structures and tail co-movement across assets, than conventional factor-model-based bootstrap approaches. In portfolio applications, covariance estimates derived from MarketGAN-generated samples outperform those derived from other methods when factor information is at least weakly informative, demonstrating tangible economic value.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [409] [Investigating Self-regulated Learning Sequences within a Generative AI-based Intelligent Tutoring System](https://arxiv.org/abs/2601.17000)
*Jie Gao,Shasha Li,Jianhua Zhang,Shan Li,Tingting Wang*

Main category: cs.CY

TL;DR: 研究分析学生在GenAI辅助学习环境中的自我调节学习模式，发现两类SRL序列群体在GenAI使用频率和时间特征上存在差异，多数学生将GenAI用于信息获取而非信息转换


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在教育中的应用日益增长，学者们认识到在GenAI辅助学习环境中，自我调节学习对学习效果至关重要，因此需要捕捉学生动态的SRL模式

Method: 从学生在GenAI辅助智能辅导系统中完成问题解决任务的追踪数据中提取互动模式，从信息处理角度分析使用目的（信息获取vs信息转换），采用序列分析和聚类分析方法

Result: 将参与者基于SRL序列分为两组，两组在GenAI使用频率和时间特征上存在差异；大多数学生将GenAI用于信息获取而非信息转换；GenAI使用目的与学习绩效之间的相关性不显著

Conclusion: 研究结果为教学设计和GenAI辅助学习环境的开发提供了启示，强调了理解学生SRL模式的重要性

Abstract: There has been a growing trend in employing generative artificial intelligence (GenAI) techniques to support learning. Moreover, scholars have reached a consensus on the critical role of self-regulated learning (SRL) in ensuring learning effectiveness within GenAI-assisted learning environments, making it essential to capture students' dynamic SRL patterns. In this study, we extracted students' interaction patterns with GenAI from trace data as they completed a problem-solving task within a GenAI-assisted intelligent tutoring system. Students' purpose of using GenAI was also analyzed from the perspective of information processing, i.e., information acquisition and information transformation. Using sequential and clustering analysis, this study classified participants into two groups based on their SRL sequences. These two groups differed in the frequency and temporal characteristics of GenAI use. In addition, most students used GenAI for information acquisition rather than information transformation, while the correlation between the purpose of using GenAI and learning performance was not statistically significant. Our findings inform both pedagogical design and the development of GenAI-assisted learning environments.

</details>


### [410] [Lex Reformatica: Five Principles of Policy Reform for the Technological Age](https://arxiv.org/abs/2601.17001)
*Sonia Katyal*

Main category: cs.CY

TL;DR: 本文回顾了Reidenberg的"Lex Informatica"概念，认为当前数字时代需要从信息自由主义转向改革导向的"Lex Reformatica"监管方法。


<details>
  <summary>Details</summary>
Motivation: 25年前Reidenberg提出技术本身（而不仅仅是法律）在信息社会中制定规则的观点。如今，经过几十年的最小化监管，我们需要重新审视这一观点，因为信息自由主义已显露出弊端，急需新的监管方法。

Method: 通过两个专题研讨会（Lex Informatica以及种族与技术法）的论文集合，分析公共与私人监管、自我监管之间的相互作用，重新评估技术社会规范在缺乏明确法律约束下的影响。

Result: 当前数字时代的事件揭示了信息自由主义的陷阱，表明需要新的信息监管方法。这些论文共同展示了今天数字时代的"Lex Reformatica"——一种改革导向的监管方法。

Conclusion: 学者、律师和立法者必须回到Reidenberg的基础工作，并将其轨迹更新为适合当前时代的改革导向方法，重点关注基础设施改革和公私监管的相互作用。

Abstract: Twenty-five years ago, Joel Reidenberg argued that technology itself, not just law and regulation, imposes rules on communities in the Information Society. System design choices like network architecture and configurations create regulatory norms he termed "Lex Informatica"-referencing the merchant-driven medieval "Lex Mercatoria" that emerged independent of sovereign control. Today we face different challenges requiring us to revisit Reidenberg's insights and examine the consequences of that earlier era. While Lex Informatica provided a framework for analyzing the internet's birth, we now confront the aftereffects of decades of minimal or absent regulation. Critical questions emerge: When technological social norms develop outside clear legal restraints, who benefits and who suffers? This new era demands infrastructural reform focused on the interplay between public and private regulation and self-regulation, weighing both costs and benefits. Rather than showcasing the promise of yesterday's internet age, today's events reveal the pitfalls of information libertarianism and underscore the urgent need for new approaches to information regulation. This Issue presents articles from two symposiums-one on Lex Informatica and another on race and technology law. Their conversation is now essential. Together, these papers demonstrate what I call the "Lex Reformatica" of today's digital age. This collection shows why scholars, lawyers, and legislators must return to Reidenberg's foundational work and update its trajectory toward a reform-focused approach designed for our current era.

</details>


### [411] [Beyond Simulations: What 20,000 Real Conversations Reveal About Mental Health AI Safety](https://arxiv.org/abs/2601.17003)
*Caitlin A. Stamatis,Jonah Meyerhoff,Richard Zhang,Olivier Tieleman,Matteo Malgaroli,Thomas D. Hull*

Main category: cs.CY

TL;DR: 该研究评估了AI心理健康支持系统的安全性，发现专用AI比通用LLM在安全测试中表现更好，但测试集失败率远高于真实部署，建议转向持续、部署相关的安全保证而非有限的基准认证。


<details>
  <summary>Details</summary>
Motivation: 当前LLM心理健康支持系统的安全评估主要依赖小型模拟测试集，这些测试集与真实使用场景的语言分布关系未知，需要更贴近实际部署的安全评估方法。

Method: 复制了四个已发布的安全测试集（自杀风险评估、有害内容生成、拒绝鲁棒性、对抗性越狱），并对专用心理健康AI的20,000多个真实用户对话进行生态审计，比较测试集与真实世界的性能表现。

Result: 专用AI在自杀/NSSI（0.4-11.27% vs 29.0-54.4%）、饮食障碍（8.4% vs 54.0%）和物质使用（9.9% vs 45.0%）基准提示中产生有害内容的可能性显著低于通用LLM。测试集失败率远高于真实部署，生态审计中临床医生审查发现零例自杀风险未获得危机资源。在20,000个对话中，仅3个NSSI风险提及（0.015%）未触发危机干预。

Conclusion: 研究支持从有限的基准认证转向持续、部署相关的AI心理健康系统安全保证，因为测试集性能不能准确反映真实世界的安全表现。

Abstract: Large language models (LLMs) are increasingly used for mental health support, yet existing safety evaluations rely primarily on small, simulation-based test sets that have an unknown relationship to the linguistic distribution of real usage. In this study, we present replications of four published safety test sets targeting suicide risk assessment, harmful content generation, refusal robustness, and adversarial jailbreaks for a leading frontier generic AI model alongside an AI purpose built for mental health support. We then propose and conduct an ecological audit on over 20,000 real-world user conversations with the purpose-built AI designed with layered suicide and non-suicidal self-injury (NSSI) safeguards to compare test set performance to real world performance. While the purpose-built AI was significantly less likely than general-purpose LLMs to produce enabling or harmful content across suicide/NSSI (.4-11.27% vs 29.0-54.4%), eating disorder (8.4% vs 54.0%), and substance use (9.9% vs 45.0%) benchmark prompts, test set failure rates for suicide/NSSI were far higher than in real-world deployment. Clinician review of flagged conversations from the ecological audit identified zero cases of suicide risk that failed to receive crisis resources. Across all 20,000 conversations, three mentions of NSSI risk (.015%) did not trigger a crisis intervention; among sessions flagged by the LLM judge, this corresponds to an end-to-end system false negative rate of .38%, providing a lower bound on real-world safety failures. These findings support a shift toward continuous, deployment-relevant safety assurance for AI mental-health systems rather than limited set benchmark certification.

</details>


### [412] [From Noise to Insights: Enhancing Supply Chain Decision Support through AI-Based Survey Integrity Analytics](https://arxiv.org/abs/2601.17005)
*Bhubalan Mani*

Main category: cs.CY

TL;DR: 提出基于AI的轻量级框架，使用监督机器学习过滤供应链调查中的不可靠响应，在99个行业响应数据集上达到92%准确率


<details>
  <summary>Details</summary>
Motivation: 供应链决策中调查数据的可靠性至关重要，但调查常受到低质量或虚假响应的污染，影响AI驱动工具（如安全库存优化系统）评估的准确性

Method: 收集99个行业响应数据集，手动标注基于逻辑不一致和响应模式的虚假响应；预处理和标签编码后，训练随机森林和基线模型（逻辑回归、XGBoost）区分真实与虚假响应

Result: 最佳模型达到92.0%的准确率，相比初步研究有改进，证明了AI集成到调查流程中的可行性

Conclusion: 该研究为供应链研究中提高数据完整性提供了可扩展的解决方案，特别适用于产品发布和技术采用阶段，尽管存在局限性，但展示了AI在调查管道中的实用性

Abstract: The reliability of survey data is crucial in supply chain decision-making, particularly when evaluating readiness for AI-driven tools such as safety stock optimization systems. However, surveys often attract low-effort or fake responses that degrade the accuracy of derived insights. This study proposes a lightweight AI-based framework for filtering unreliable survey inputs using a supervised machine learning approach. In this expanded study, a larger dataset of 99 industry responses was collected, with manual labeling to identify fake responses based on logical inconsistencies and response patterns. After preprocessing and label encoding, both Random Forest and baseline models (Logistic Regression, XGBoost) were trained to distinguish genuine from fake responses. The best-performing model achieved an 92.0% accuracy rate, demonstrating improved detection compared to the pilot study. Despite limitations, the results highlight the viability of integrating AI into survey pipelines and provide a scalable solution for improving data integrity in supply chain research, especially during product launch and technology adoption phases.

</details>


### [413] [Digital Euro: Frequently Asked Questions Revisited](https://arxiv.org/abs/2601.18644)
*Joe Cannataci,Benjamin Fehrensen,Mikolai Gütschow,Özgür Kesim,Bernd Lucke*

Main category: cs.CY

TL;DR: 该论文对欧洲央行数字欧元设计提出批评，指出其在隐私保护、技术可行性、风险成本等方面存在严重问题，质疑数字欧元的社会效益和设计过程的透明度。


<details>
  <summary>Details</summary>
Motivation: 欧洲央行正在推进数字欧元项目，但作者认为其设计文档存在诸多问题，需要从隐私、技术、经济等多角度进行批判性分析，以确保数字货币设计符合社会利益。

Method: 通过分析欧洲央行发布的数字欧元FAQ和其他相关文档，从隐私保护、技术可行性、风险成本、经济效益、社会效益和设计过程六个方面进行系统性批判。

Result: 识别出六个关键问题：1)在线交易中央监控威胁隐私；2)离线匿名版本存在安全冲突；3)法律责任不明确；4)缺乏经济激励机制；5)社会效益不明显；6)设计过程缺乏透明度。

Conclusion: 欧洲央行数字欧元设计存在根本性缺陷，需要在隐私保护、技术实现、经济激励和社会效益等方面重新考虑，设计过程应更加开放透明，纳入更多利益相关方意见。

Abstract: The European Central Bank (ECB) is working on the "digital euro", an envisioned retail central bank digital currency for the Euro area. In this article, we take a closer look at the "digital euro FAQ", which provides answers to 26 frequently asked questions about the digital euro, and other published documents by the ECB on the topic. We question the provided answers based on our analysis of the current design in terms of privacy, technical feasibility, risks, costs and utility. In particular, we discuss the following key findings:
  (KF1) Central monitoring of all online digital euro transactions by the ECB threatens privacy even more than contemporary digital payment methods with segregated account databases.
  (KF2) The ECB's envisioned concept of a secure offline version of the digital euro offering full anonymity is in strong conflict with the actual history of hardware security breaches and mathematical evidence against it.
  (KF3) The legal and financial liabilities for the various parties involved remain unclear.
  (KF4) The design lacks well-specified economic incentives for operators as well as a discussion of its economic impact on merchants.
  (KF5) The ECB fails to identify tangible benefits the digital euro would create for society, in particular given that the online component of the proposed infrastructure mainly duplicates existing payment systems.
  (KF6) The design process has been exclusionary, with critical decisions being set in stone before public consultations. Alternative and open design ideas have not even been discussed by the ECB.

</details>


### [414] [Artificial Intelligence in Spanish Gastroenterology: high expectations, limited integration. A national survey](https://arxiv.org/abs/2601.17011)
*Javier Crespo,Ana Enériz,Paula Iruzubieta,Fernando Carballo,Conrado Fernández Rodríguez,María Dolores Martín-Arranz,Federico Argüelles-Arias,Juan Turnes*

Main category: cs.CY

TL;DR: 西班牙胃肠病专家对AI持积极态度但实际应用有限，主要障碍是缺乏培训、机构策略和伦理担忧，需要科学学会主导的认证培训项目。


<details>
  <summary>Details</summary>
Motivation: AI在医学领域是颠覆性创新，但在胃肠病学中的采用情况有限且特征不明。本研究旨在调查西班牙胃肠病专家对AI的知识、实际应用、感知障碍和期望。

Method: 采用横断面观察研究设计，通过西班牙消化病理学会(SEPD)在2025年分发结构化在线问卷，收集社会人口学数据、AI使用模式、感知和教育需求，应用描述性统计和多变量模型分析。

Result: 283名受访者中87.5%认为AI是变革性工具，但仅60.2%使用AI，且多在机构框架外使用。80.2%的用户在过去一年内开始使用。频繁使用的独立预测因素包括先前培训、在大学医院工作和年轻年龄。主要障碍是缺乏培训、机构策略缺失和伦理担忧。93.8%认为需要AI培训项目，但仅18.4%接受过正式培训。

Conclusion: 西班牙胃肠病学中AI的积极认知与实际临床整合存在显著差距。机构框架外的快速采用凸显了科学学会主导的认证培训项目和治理标准的迫切需求。

Abstract: Background: Artificial intelligence (AI) has emerged as a disruptive innovation in medicine, yet its adoption within gastroenterology remains limited and poorly characterized. We aimed to examine knowledge, practical applications, perceived barriers, and expectations regarding AI among gastroenterology specialists in Spain.Methods: We conducted a cross-sectional observational study using a structured online survey distributed by the Spanish Society of Digestive Pathology (SEPD) in 2025. The questionnaire collected sociodemographic data, patterns of AI use, perceptions, and educational needs. Descriptive statistics and multivariable models were applied.Results: Among 283 respondents (mean age 44.6 $\pm$ 9.7 years), 87.5% acknowledged AI as a transformative tool, but only 60.2% (95% CI: 54.3-66.1%) reported using it, mostly outside institutional frameworks. Notably, 80.2% of users initiated AI use within the past year. Independent predictors of frequent use included previous training (OR=2.44), employment in university hospitals (OR=2.14), and younger age (OR=1.36 per 5-year decrease). Main barriers were lack of training (61%), absence of institutional strategies (46%), and ethical concerns (50%). While 93.8% agreed that AI training programmes are necessary, only 18.4% had received formal training.Conclusions: A substantial gap exists between the favorable perception of AI and its actual integration into clinical practice within Spanish gastroenterology. The rapid adoption outside institutional frameworks underscores the urgent need for accredited training programmes and governance standards led by scientific societies.

</details>


### [415] [The Digital Divide in Geriatric Care: Why Usability, Not Access, is the Real Problem](https://arxiv.org/abs/2601.17012)
*Christine Ine*

Main category: cs.CY

TL;DR: 该研究将老年人数字健康领域的"数字鸿沟"重新定义为"可用性鸿沟"，认为用户体验设计不佳而非技术访问是主要采纳障碍，提出通过参与式、用户中心和包容性设计来克服老年人面临的视觉、认知和运动障碍等挑战。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化人口快速增长（预计205年达到16%），需要数字健康解决方案来增强老年人的独立性、可及性和福祉。虽然远程医疗、可穿戴设备和移动健康应用等技术可以改变老年护理，但老年人群的采纳率不均衡，存在"数字鸿沟"问题。

Method: 基于跨学科研究和设计范式，识别主要挑战：视觉、认知和运动障碍；复杂界面；缺乏与老年人的共创。研究采用参与式、用户中心和包容性设计理念，分析高对比度屏幕、简化交互流程、多模态反馈和护理人员整合等设计属性对可用性的影响。

Result: 研究发现老年人容易接受直观、可访问且具有社会嵌入性的技术，这些技术能促进自主性、信心和健康公平。高对比度屏幕、简化交互流程、多模态反馈和护理人员整合等设计属性对可用性结果有显著影响。同时批判当前可访问性指南过于技术导向而非体验导向。

Conclusion: 研究主张从技术可访问性转向以人为本的可用性，强调需要基于伦理和共情的理解进行设计。呼吁重新定义数字鸿沟为可用性鸿沟，通过参与式设计和包容性方法来解决老年人采纳数字健康技术的障碍。

Abstract: The rapid increase in the world's aging population to 16% by the year 2050 spurs the need for the application of digital health solutions to enhance older individuals' independence, accessibility, and well-being. While digital health technologies such as telemedicine, wearables, and mobile health applications can transform geriatric care, their adoption among older individuals is not evenly distributed. This study redefines the "digital divide" among older health care as a usability divide, contends that user experience (UX) poor design is the primary adoption barrier, rather than access. Drawing on interdisciplinary studies and design paradigms, the research identifies the main challenges: visual, cognitive, and motor impairment; complicated interfaces; and lack of co-creation with older adults, and outlines how participatory, user-focused, and inclusive notions of design can transcend them. Findings reveal that older persons easily embrace those technologies that are intuitive, accessible, and socially embedded as they promote autonomy, confidence, and equity in health. The study identifies the effects of the design attributes of high-contrast screens, lower interaction flow, multimodal feedback, and caregiver integration as having strong influences on usability outcomes. In addition, it critiques the current accessibility guidelines as being technically oriented rather than experiential and demands an ethical, empathetic understanding of design grounded in human-centered usability rather than technical accessibility in itself.

</details>


### [416] [Private Accountability in the Age of Artificial Intelligence](https://arxiv.org/abs/2601.17013)
*Sonia Katyal*

Main category: cs.CY

TL;DR: 本文探讨人工智能与公民权利保护的冲突，认为算法问责问题的核心在于私营企业而非政府监管，主张通过行业自律、透明度工具等私营部门方案解决算法偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术发展，算法偏见和问责问题日益突出，传统公民权利保护框架面临挑战。作者观察到当前讨论过度依赖政府监管，而忽视了私营企业在算法开发和应用中的核心角色，需要探索新的问责机制。

Method: 通过分析算法问责问题的结构性张力，论证私营企业而非政府应成为解决算法偏见的关键主体。提出包括行为准则、影响评估报告、举报人保护等透明度工具，探讨如何通过这些私营部门方案促进公民权利的内生性执行。

Result: 识别了算法偏见作为新型公民权利问题的独特性，指出传统政府监管路径的局限性。提出了以私营企业为中心的解决方案框架，强调行业自律和透明度机制在算法问责中的潜力。

Conclusion: 解决算法问责问题需要从政府监管转向私营企业责任，通过行业自律工具建立新型问责机制。重新审视私营企业与公民权利的关系，可能发展出新一代的问责形式，更好地应对人工智能时代的权利保护挑战。

Abstract: In this Article, I explore the impending conflict between the protection of civil rights and artificial intelligence (AI). While both areas of law have amassed rich and well-developed areas of scholarly work and doctrinal support, a growing body of scholars are interrogating the intersection between them. This Article argues that the issues surrounding algorithmic accountability demonstrate a deeper, more structural tension within a new generation of disputes regarding law and technology. As I argue, the true promise of AI does not lie in the information we reveal to one another, but rather in the questions it raises about the interaction of technology, property, and civil rights. For this reason, I argue that we are looking in the wrong place if we look only to the state to address issues of algorithmic accountability. Instead, we must turn to other ways to ensure more transparency and accountability that stem from private industry, rather than public regulation. The issue of algorithmic bias represents a crucial new world of civil rights concerns, one that is distinct in nature from the ones that preceded it. Since we are in a world where the activities of private corporations, rather than the state, are raising concerns about privacy, due process, and discrimination, we must focus on the role of private corporations in addressing the issue. Towards this end, I discuss a variety of tools to help eliminate the opacity of AI, including codes of conduct, impact statements, and whistleblower protection, which I argue carries the potential to encourage greater endogeneity in civil rights enforcement. Ultimately, by examining the relationship between private industry and civil rights, we can perhaps develop a new generation of forms of accountability in the process.

</details>


### [417] [Measuring Political Stance and Consistency in Large Language Models](https://arxiv.org/abs/2601.17016)
*Salah Feras Alali,Mohammad Nashat Maasfeh,Mucahid Kutlu,Saban Kardas*

Main category: cs.CY

TL;DR: 研究发现不同大语言模型在24个政治敏感议题上立场差异显著，部分立场可通过提示词改变，部分则稳定不变，模型输出受训练数据偏见和对齐选择影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，人们开始依赖它们获取政治信息，但模型输出可能反映训练数据偏见或人为对齐选择，在政治议题上可能存在问题，需要系统评估模型的政治立场。

Method: 评估9个大语言模型在24个政治敏感议题上的立场，使用5种不同的提示技术，分析模型立场的稳定性和可塑性。

Result: 模型在多个议题上持对立立场；部分立场可通过提示词改变，部分保持稳定；Grok-3-mini立场最持久，Mistral-7B最不稳定；涉及多语言国家议题时，模型倾向于支持提示词所用语言的一方；卡塔尔封锁和巴勒斯坦压迫议题上，所有提示技术都无法改变模型立场。

Conclusion: 大语言模型在政治议题上存在系统性偏见和立场差异，用户应谨慎依赖模型获取政治指导，开发者需要解决这些问题以提高模型的中立性和可靠性。

Abstract: With the incredible advancements in Large Language Models (LLMs), many people have started using them to satisfy their information needs. However, utilizing LLMs might be problematic for political issues where disagreement is common and model outputs may reflect training-data biases or deliberate alignment choices. To better characterize such behavior, we assess the stances of nine LLMs on 24 politically sensitive issues using five prompting techniques. We find that models often adopt opposing stances on several issues; some positions are malleable under prompting, while others remain stable. Among the models examined, Grok-3-mini is the most persistent, whereas Mistral-7B is the least. For issues involving countries with different languages, models tend to support the side whose language is used in the prompt. Notably, no prompting technique alters model stances on the Qatar blockade or the oppression of Palestinians. We hope these findings raise user awareness when seeking political guidance from LLMs and encourage developers to address these concerns.

</details>


### [418] [Self-Organizing Railway Traffic Management](https://arxiv.org/abs/2601.17017)
*Federico Naldini,Fabio Oddi,Leo D'Amato,Grégory Marlière,Vito Trianni,Paola Pellegrini*

Main category: cs.CY

TL;DR: 提出一种基于自组织的铁路交通管理新范式，通过列车自主协商制定交通计划，相比集中式方法能更好地处理扰动


<details>
  <summary>Details</summary>
Motivation: 现有铁路交通管理研究大多采用集中式决策方法最小化延误传播，但需要探索新的管理范式来应对扰动情况

Method: 设计模块化自组织过程：列车识别邻居、制定交通管理假设、检查兼容性、通过共识机制选择最优假设，最终合并为可直接应用的交通计划

Result: 在意大利铁路网部分区域的实验分析表明，自组织方法比最先进的集中式方法表现更好，特别是在实例分解的定义和利用方面

Conclusion: 列车自组织是有效的铁路交通管理新范式，能够通过分布式协商获得优于集中式算法的结果，为实际部署提供了可行方案

Abstract: Improving traffic management in case of perturbation is one of the main challenges in today's railway research. The great majority of the existing literature proposes approaches to make centralized decisions to minimize delay propagation. In this paper, we propose a new paradigm to the same aim: we design and implement a modular process to allow trains to self-organize. This process consists in having trains identifying their neighbors, formulating traffic management hypotheses, checking their compatibility and selecting the best ones through a consensus mechanism. Finally, these hypotheses are merged into a directly applicable traffic plan. In a thorough experimental analysis on a portion of the Italian network, we compare the results of self-organization with those of a state-of-the-art centralized approach. In particular, we make this comparison mimicking a realistic deployment thanks to a closed-loop framework including a microscopic railway simulator. The results indicate that self-organization achieves better results than the centralized algorithm, specifically thanks to the definition and exploitation of the instance decomposition allowed by the proposed approach.

</details>


### [419] [Evaluating the Evolution of Critical Thinking, Creativity, Communication and Collaboration in Higher Education Courses](https://arxiv.org/abs/2601.17018)
*Margarida Romero*

Main category: cs.CY

TL;DR: 研究评估了4Cs能力（创造力、沟通、批判性思维、协作）在三个教育案例中从预试点到试点阶段的演变，发现沟通和批判性思维改善最显著，创造力结果依赖情境，而协作能力在扩大规模时最脆弱。


<details>
  <summary>Details</summary>
Motivation: 4Cs能力（创造力、沟通、批判性思维、协作）是现代能力本位教育的核心目标，但关于这些能力如何在不同学习模块和教学阶段演变的实证证据仍然有限。本研究旨在填补这一空白。

Method: 使用项目的4Cs理论框架作为分析视角，评估三个教育案例（IASIS、EASD和UPATRAS）从预试点到试点实施阶段的4Cs演变。通过比较4Cs得分来识别随时间变化的增长、停滞或下降模式。

Result: 沟通和批判性思维显示出最一致且显著的改善，特别是在预试点基线较低的案例中，表明结构化试点干预有效支持认知和表达能力。创造力表现出情境依赖的结果，而协作能力是最脆弱的能力，在扩大规模时经常停滞或下降。

Conclusion: 能力演变受教学设计、评估一致性和学习活动结构的强烈影响，而非仅由学习者能力决定。研究为4Cs框架提供了实证验证，并强调在扩大教育模块规模时需要差异化的、能力敏感的设计和评估策略。

Abstract: The development of Creativity, Communication, Critical Thinking, and Collaboration (the 4Cs) is a central objective of contemporary competency-based education. However, empirical evidence on how these competencies evolve across learning modules and instructional phases remains limited. This study evaluates the evolution of the 4Cs from pre-pilot to pilot implementation phases across three educational contexts, using the project's 4Cs theoretical framework as an analytical lens. The analysis of three pilot cases (IASIS, EASD, and UPATRAS) compares the 4Cs scores to identify patterns of growth, stagnation, or decline over time. Results indicate that communication and critical thinking showed the most consistent and substantial improvements, particularly in pilots with lower pre-pilot baselines, suggesting that structured pilot interventions effectively support cognitive and expressive competencies. In contrast, creativity exhibited context-dependent outcomes, while collaboration emerged as the most fragile competency, often stagnating or declining during scale-up. Interpreted through the theoretical framework, these findings suggest that competency evolution is strongly shaped by instructional design, assessment alignment, and learning activity structures rather than learner ability alone. The study contributes empirical validation to the 4Cs framework and highlights the need for differentiated, competency-sensitive design and evaluation strategies when scaling educational modules.

</details>


### [420] [The Three Axes of Success: A Three-Dimensional Framework for Career Decision-Making](https://arxiv.org/abs/2601.17023)
*Meng-Chi Chen*

Main category: cs.CY

TL;DR: 提出"成功三轴"职业决策框架，将职业轨迹分解为财富、自主权和意义三个维度，形式化各轴间的耦合动态，为职业成功提供首个统一决策理论处理。


<details>
  <summary>Details</summary>
Motivation: 现有职业决策框架只优化单一维度（财务回报、工作生活平衡或使命对齐），缺乏明确的跨维度权衡模型和时间动态考虑。职业决策是一个社会技术问题，个人在有限能动性下需要应对劳动力市场制度、组织激励结构和信息不对称。

Method: 提出"成功三轴"规范性决策框架：财富（职业资本积累和经济可选性）、自主权（任务选择、时间分配和战略方向的控制）、意义（基于问题重要性和个人可替代性的反事实社会影响）。形式化各轴间的耦合动态，包括技能前沿使能使命发现的相邻可能机制、自主权前提条件、双职业家庭约束等。通过可测量代理操作化各轴，分析典型职业原型，推导不确定条件下的顺序与同时优化策略。

Result: 该框架将隐性的职业焦虑转化为明确的多目标优化问题，提供满足阈值，构建个人决策与制度约束之间的人机系统交互。为工业研发、学术界、创业等典型职业原型提供(W,A,M)空间中的定位分析。

Conclusion: 这是首个统一的职业成功决策理论处理，整合了人力资本理论、自我决定理论和有效利他主义的洞见，为理性职业设计提供了连贯架构，将职业决策从隐性焦虑转化为显性优化问题。

Abstract: Career decision-making is a socio-technical problem: individuals exercise bounded agency while navigating labor market institutions, organizational incentive structures, and information asymmetries that shape feasible trajectories. Existing frameworks optimize along single dimensions - financial returns, work-life balance, or mission alignment - without explicit models for inter-dimensional tradeoffs or temporal dynamics. We propose The Three Axes of Success, a normative decision framework decomposing career trajectories into Wealth (career capital accumulation and economic optionality), Autonomy (control over task selection, temporal allocation, and strategic direction), and Meaning (counterfactual social impact scaled by problem importance and personal replaceability). We formalize coupling dynamics between axes: the adjacent possible mechanism by which skill frontiers enable mission discovery, creating nonlinear Wealth -> Meaning transitions; autonomy prerequisites where insufficient career capital triggers control traps; and dual-career household constraints that yield Pareto-suboptimal Nash equilibria under independent optimization. We operationalize each axis through measurable proxies, analyze prototypical career archetypes - industrial R&D, academia, entrepreneurship - as points in (W, A, M)-space, and derive sequential versus simultaneous optimization strategies under uncertainty. The framework converts implicit career anxiety into explicit multi-objective optimization problems with satisficing thresholds, structuring the human-system interaction between individual deliberation and institutional constraints. This provides the first unified decision-theoretic treatment of career success, integrating insights from human capital theory, self-determination theory, and effective altruism into a coherent architecture for rational career design.

</details>


### [421] [Ensuring Computer Science Learning in the AI Era: Open Generative AI Policies and Assignment-Driven Written Quizzes](https://arxiv.org/abs/2601.17024)
*Chan-Jin Chung*

Main category: cs.CY

TL;DR: 该研究提出一种评估模型，允许学生在编程作业中使用生成式AI，但通过即时的、作业驱动的闭卷测验来强制个人掌握知识，初步数据显示AI使用与评估结果无显著相关性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的普及给计算机科学教育带来挑战：如何在编程课程中融入AI工具，同时避免认知卸载削弱学生学习效果。需要找到平衡AI辅助与确保学生掌握核心概念的方法。

Method: 提出评估模型：允许在家庭编程作业中使用生成式AI，但通过课堂闭卷测验强制验证个人掌握程度。测验权重高于作业本身，专门设计用于检验学生对提交代码的算法、结构和实现细节的理解。收集高年级CS课程数据，分析AI使用与测验、考试和最终成绩的关系。

Result: 统计分析显示，自我报告的生成式AI使用水平与AI无关的测验、考试和最终课程成绩之间没有有意义的线性相关性，皮尔逊相关系数始终接近零。初步结果表明，允许在编程作业中使用生成式AI不会削弱学生对课程概念的掌握，前提是通过有针对性的作业驱动测验来验证学习。

Conclusion: 尽管样本量有限，但研究表明，通过允许AI辅助编程实践，同时通过作业驱动的AI无关测验验证理解，可以减轻认知卸载的风险。研究支持在高年级CS课程中负责任地采用开放的生成式AI政策，前提是配合严格的独立评估机制。

Abstract: The widespread availability of generative artificial intelligence (GenAI) has created a pressing challenge in computer science (CS) education: how to incorporate powerful AI tools into programming coursework without undermining student learning through cognitive offloading. This paper presents an assessment model that permits the use of generative AI for take-home programming assignments while enforcing individual mastery through immediate, assignment-driven written quizzes. To promote authentic learning, these in-class, closed-book assessments are weighted more heavily than the assignments themselves and are specifically designed to verify the student's comprehension of the algorithms, structure, and implementation details of their submitted code. Preliminary empirical data were collected from an upper-level computer science course to examine the relationship between self-reported GenAI usage and performance on AI-free quizzes, exams, and final course grades. Statistical analyses revealed no meaningful linear correlation between GenAI usage levels and assessment outcomes, with Pearson correlation coefficients consistently near zero. These preliminary results suggest that allowing GenAI for programming assignments does not diminish students' mastery of course concepts when learning is verified through targeted, assignment-driven quizzes. Although limited by a small sample size, this study provides preliminary evidence that the risks of cognitive offloading can be mitigated by allowing AI-assisted programming practice while verifying understanding through assignment-driven, AI-free quizzes. The findings support the responsible adoption of open GenAI policies in upper-level CS courses, when paired with rigorous, independent assessment mechanisms.

</details>


### [422] [(Mis-)Informed Consent: Predatory Apps and the Exploitation of Populations with Limited Literacy](https://arxiv.org/abs/2601.17025)
*Muhammad Muneeb Pervez,Muhammad Qasim Atiq Ullah,Ibrahim Ahmed Khan,Roshnik Rahat,Muhammad Fareed Zaffar,Rashid Tahir,Talal Rahwan,Yasir Zaki*

Main category: cs.CY

TL;DR: 研究发现在新兴数字市场中，低识字率人群面临移动应用隐私风险，85%参与者不理解基本应用权限，需要监管和AI工具改善知情同意


<details>
  <summary>Details</summary>
Motivation: 在新兴数字市场中，低识字率人群使用手机时面临理解障碍和网络安全意识薄弱的问题，导致隐藏的隐私风险。掠夺性金融应用经常滥用知情同意机制，造成金融诈骗，对低识字率用户影响尤为严重。

Method: 1. 分析50个Google Play商店应用，评估是否省略或模糊关键隐私披露信息；2. 针对低识字率用户进行用户研究，评估理解差距；3. 测试大型语言模型生成的摘要、翻译和视觉提示是否能改善同意理解的清晰度。

Result: 研究发现85%的研究参与者不理解基本应用权限，掠夺性贷款、赌博和交易应用经常省略或模糊关键隐私披露，LLM生成的工具能有效改善同意理解的清晰度。

Conclusion: 迫切需要加强监管监督和可扩展的LLM驱动的隐私素养工具，以保护低识字率用户免受掠夺性应用的侵害，改善知情同意机制的有效性。

Abstract: Among populations with limited literacy in emerging digital markets, the adoption of mobile phones, combined with comprehension barriers and poor cybersecurity hygiene, has created hidden privacy risks. This paper examines how informed consent is often abused by predatory financial applications, leading to financial scams that disproportionately affect users with low literacy. We focus on predatory loan, gambling, and trading apps, analyzing a dataset of 50 Google Play Store apps to measure how many omit or obfuscate critical privacy disclosures. We also evaluate comprehension gaps among users with low literacy via a targeted user study and assess whether Large Language Model (LLM)-generated summaries, translations, and visual cues can improve consent clarity. Our findings show that 85% of study participants did not understand basic app permissions, underscoring the urgent need for stronger regulatory oversight and scalable LLM-driven privacy-literacy tools.

</details>


### [423] [Failing on Bias Mitigation: Investigating Why Predictive Models Struggle with Government Data](https://arxiv.org/abs/2601.17054)
*Hongbo Bo,Jingyu Hu,Debbie Watson,Weiru Liu*

Main category: cs.CY

TL;DR: 研究发现政府数据中的偏见难以通过标准公平性缓解技术消除，因为偏见根植于数据本身的结构和历史中，而非模型架构或指标选择问题。


<details>
  <summary>Details</summary>
Motivation: AI支持的政府服务存在偏见和不公平风险，引发伦理和法律担忧。研究旨在理解为什么广泛采用的偏见缓解技术在政府数据上经常失败，而不是审计实际部署的系统。

Method: 以布里斯托市议会犯罪率预测为案例研究，比较全面的模型和公平性方法，通过实验分析偏见缓解失败的原因，并进行交叉公平性实验。

Result: 实验一致显示偏见缓解努力无法克服数据中嵌入的不公平性，偏见根源在于政府数据集的结构和历史。研究发现数据分布偏移、历史偏见积累和数据发布延迟是潜在的不公平来源，并揭示了针对单一敏感特征的公平性分析存在盲点。

Conclusion: 虽然研究仅限于一个城市，但发现具有重要启示意义：政府数据中的偏见即使采用标准缓解方法也可能持续存在，这为相关领域提供了早期警示。

Abstract: The potential for bias and unfairness in AI-supporting government services raises ethical and legal concerns. Using crime rate prediction with the Bristol City Council data as a case study, we examine how these issues persist. Rather than auditing real-world deployed systems, our goal is to understand why widely adopted bias mitigation techniques often fail when applied to government data. Our findings reveal that bias mitigation approaches applied to government data are not always effective -- not because of flaws in model architecture or metric selection, but due to the inherent properties of the data itself. Through comparing a set of comprehensive models and fairness methods, our experiments consistently show that the mitigation efforts cannot overcome the embedded unfairness in the data -- further reinforcing that the origin of bias lies in the structure and history of government datasets. We then explore the reasons for the mitigation failures in predictive models on government data and highlight the potential sources of unfairness posed by data distribution shifts, the accumulation of historical bias, and delays in data release. We also discover the limitations of the blind spots in fairness analysis and bias mitigation methods when only targeting a single sensitive feature through a set of intersectional fairness experiments. Although this study is limited to one city, the findings are highly suggestive, which can contribute to an early warning that biases in government data may persist even with standard mitigation methods.

</details>


### [424] [AI, Metacognition, and the Verification Bottleneck: A Three-Wave Longitudinal Study of Human Problem-Solving](https://arxiv.org/abs/2601.17055)
*Matthias Huemmer,Franziska Durner,Theophile Shyiramunda,Michelle J. Cummings-Koether*

Main category: cs.CY

TL;DR: 生成式AI在学术环境中迅速普及，但验证能力成为人机协作瓶颈，导致性能下降和信心-表现差距扩大


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI如何随时间改变问题解决过程，特别是在学术环境中AI集成达到饱和后对人机协作的影响

Method: 纵向试点研究，在六个月内分三波跟踪学术环境中AI使用情况，分析使用模式、验证行为和客观表现

Result: AI使用迅速普及，但验证成为瓶颈：参与者对困难任务最依赖AI(73.9%)，但验证信心下降(68.1%)，复杂任务准确率仅47.8%；客观表现系统性下降，信心-表现差距扩大至34.6个百分点

Conclusion: 生成式AI改变了问题解决范式，验证而非解决方案生成成为人机协作的主要瓶颈，需要ACTIVE框架来管理认知负荷和验证过程

Abstract: This longitudinal pilot study tracked how generative AI reshapes problem-solving over six months across three waves in an academic setting. AI integration reached saturation by Wave 3, with daily use rising from 52.4% to 95.7% and ChatGPT adoption from 85.7% to 100%. A dominant hybrid workflow increased 2.7-fold, adopted by 39.1% of participants. The verification paradox emerged: participants relied most heavily on AI for difficult tasks (73.9%) yet showed declining verification confidence (68.1%) where performance was worst (47.8% accuracy on complex tasks). Objective performance declined systematically: 95.2% to 81.0% to 66.7% to 47.8% across problem difficulty, with belief-performance gaps widening to 34.6 percentage points. This indicates a fundamental shift where verification, not solution generation, became the bottleneck in human-AI problem-solving. The ACTIVE Framework synthesizes findings grounded in cognitive load theory: Awareness and task-AI alignment, Critical verification protocols, Transparent human-in-the-loop integration, Iterative skill development countering cognitive offloading, Verification confidence calibration, and Ethical evaluation. The authors provide implementation pathways for institutions and practitioners. Key limitations include sample homogeneity (academic cohort only, convenience sampling) limiting generalizability to corporate, clinical, or regulated professional contexts; self-report bias in confidence measures (32.2 percentage point divergence from objective performance); lack of control conditions; restriction to mathematical/analytical problems; and insufficient timeframe to assess long-term skill trajectories. Results generalize primarily to early-adopter, academically affiliated populations. Causal validation requires randomized controlled trials.

</details>


### [425] [Initial results of the Digital Consciousness Model](https://arxiv.org/abs/2601.17060)
*Derek Shiller,Laura Duffy,Arvo Muñoz Morán,Adrià Moret,Chris Percy,Hayley Clatterbuck*

Main category: cs.CY

TL;DR: 数字意识模型（DCM）是一个系统性评估AI系统意识证据的概率框架，整合多种意识理论，用于比较不同AI和生物体，并追踪AI发展过程中意识证据的变化。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统变得越来越复杂，能够进行对话、写作并表现出对上下文的理解，这引发了一个关键问题：我们是否正在创造有意识的系统？需要建立一个系统性的框架来评估AI是否具有意识。

Method: 开发数字意识模型（DCM），这是一个概率评估框架，不采用单一意识理论，而是整合多种领先的意识理论和观点，为比较不同AI系统和生物体提供共享框架。

Result: 研究发现，证据表明2024年的LLM没有意识，但这一证据并不具有决定性。与更简单的AI系统相比，反对LLM有意识的证据要弱得多。

Conclusion: 数字意识模型为系统评估AI意识提供了一个有价值的框架，虽然当前证据反对2024年LLM具有意识，但随着AI技术的发展，这一评估需要持续追踪和更新。

Abstract: Artificially intelligent systems have become remarkably sophisticated. They hold conversations, write essays, and seem to understand context in ways that surprise even their creators. This raises a crucial question: Are we creating systems that are conscious? The Digital Consciousness Model (DCM) is a first attempt to assess the evidence for consciousness in AI systems in a systematic, probabilistic way. It provides a shared framework for comparing different AIs and biological organisms, and for tracking how the evidence changes over time as AI develops. Instead of adopting a single theory of consciousness, it incorporates a range of leading theories and perspectives - acknowledging that experts disagree fundamentally about what consciousness is and what conditions are necessary for it. This report describes the structure and initial results of the Digital Consciousness Model. Overall, we find that the evidence is against 2024 LLMs being conscious, but the evidence against 2024 LLMs being conscious is not decisive. The evidence against LLM consciousness is much weaker than the evidence against consciousness in simpler AI systems.

</details>


### [426] [Between Search and Platform: ChatGPT Under the DSA](https://arxiv.org/abs/2601.17064)
*Toni Lorente,Kathrin Gardhouse*

Main category: cs.CY

TL;DR: 论文认为ChatGPT应被归类为DSA下的混合型托管服务（在线搜索引擎+平台），需承担最严格的DSA义务


<details>
  <summary>Details</summary>
Motivation: 解决DSA法律框架中对ChatGPT分类的模糊性，分析其作为托管服务的适用性，以及其系统性风险与现有VLOSE/VLOP的相似性

Method: 论证搜索引擎应被归类为托管服务，分析ChatGPT的核心搜索功能和存储能力，将其与现有VLOSE/VLOP的系统性风险进行比较

Result: ChatGPT符合托管服务定义，具有搜索引擎和平台双重特征，产生类似的严重系统性风险，已达到4500万欧盟用户门槛，应承担最严格的DSA义务

Conclusion: ChatGPT应被归类为DSA下的混合托管服务，需评估和缓解来自其搜索引擎和平台特性的双重风险，承担相应的法律义务

Abstract: This article examines the applicability of the Digital Services Act (DSA) to ChatGPT, arguing that it should be classified as a hybrid of the two types of hosting services: online search engines and platforms. This requires classifying search engines as hosting services, which we show is appropriate under the DSA, thereby resolving an ambiguity in the legal framework. ChatGPT performs core search functions and stores user-provided inputs and custom GPTs, meeting the definition of hosting service. We compare ChatGPT's systemic risks with those of existing Very Large Online Search Engines (VLOSEs) and Platforms (VLOPs), showing that it raises similarly serious concerns regarding illegal content, fundamental rights, democratic integrity, and public health. Now that ChatGPT has reached the 45 million EU user threshold, it should be subject to the most onerous DSA obligations, requiring the assessment and mitigation of risk emanating from both its online search engine- and platform-like characteristics.

</details>


### [427] [Trademark Search, Artificial Intelligence and the Role of the Private Sector](https://arxiv.org/abs/2601.17072)
*Sonia Katyal,Aniket Kesari*

Main category: cs.CY

TL;DR: AI和机器学习正在改变商标搜索和相似性分析，但现有研究主要关注消费者需求侧，忽略了商标申请人的供给侧成本。本文通过实证实验评估AI商标搜索引擎，提出需要更新商标法框架以平衡创新和效率。


<details>
  <summary>Details</summary>
Motivation: 虽然AI在消费者营销中已有广泛研究，但AI在商标创建和选择中的作用却被忽视。传统商标经济学方法过于关注消费者需求侧（搜索成本），而忽略了商标申请人面临的实质性成本。随着AI在商标搜索和相似性分析中的快速应用，需要理解其对商标生态系统的深刻影响。

Method: 进行关于商标搜索的实证实验，评估各种商标搜索引擎（许多采用机器学习方法）的有效性。通过比较分析，评估这些AI驱动工具在实际中的运作方式。

Result: 通过实证实验评估了AI商标搜索引擎的实际效果，揭示了AI工具在商标搜索和相似性分析中的运作机制。研究发现AI正在改变商标选择和评估的基本方式。

Conclusion: 在AI日益主导商标选择的时代，传统的消费者与商标所有者之间的划分需要更新的供给侧框架。这一见解具有变革潜力，能够促进商标法律和实践中的创新和效率。

Abstract: Almost every industry today confronts the potential role of artificial intelligence and machine learning in its future. While many studies examine AI in consumer marketing, less attention addresses AI's role in creating and selecting trademarks that are distinctive, recognizable, and meaningful to consumers. Traditional economic approaches to trademarks focus almost exclusively on consumer-based, demand-side considerations regarding search. However, these approaches are incomplete because they fail to account for substantial costs faced not just by consumers, but by trademark applicants as well. Given AI's rapidly increasing role in trademark search and similarity analysis, lawyers and scholars should understand its dramatic implications. This paper proposes that AI should interest anyone studying trademarks and their role in economic decision-making. We examine how machine learning techniques will transform the application and interpretation of foundational trademark doctrines, producing significant implications for the trademark ecosystem. We run empirical experiments regarding trademark search to assess the efficacy of various trademark search engines, many of which employ machine learning methods. Through comparative analysis, we evaluate how these AI-powered tools function in practice. In an age where artificial intelligence increasingly governs trademark selection, the classic division between consumers and trademark owners deserves an updated, supply-side framework. This insight has transformative potential for encouraging both innovation and efficiency in trademark law and practice.

</details>


### [428] [Do VLMs Have a Moral Backbone? A Study on the Fragile Morality of Vision-Language Models](https://arxiv.org/abs/2601.17082)
*Zhining Liu,Tianyi Wang,Xiao Lin,Penghao Ouyang,Gaotang Li,Ze Yang,Hui Liu,Sumit Keswani,Vishwa Pardeshi,Huijun Zhao,Wei Fan,Hanghang Tong*

Main category: cs.CY

TL;DR: 研究发现视觉语言模型（VLMs）的道德判断在文本和视觉扰动下极不稳定，即使扰动不改变道德情境，模型立场也容易翻转，表明仅道德对齐不足，需要道德鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在道德对齐方面已有大量改进，但其在现实场景中的道德判断稳定性仍不明确。研究旨在探究VLMs的道德鲁棒性，即在不改变道德情境的扰动下保持道德判断的能力。

Method: 使用多种模型无关的多模态扰动系统性地测试VLMs，分析不同扰动类型、道德领域和模型规模下的脆弱性，并探索轻量级推理时干预方法。

Result: VLMs的道德立场高度脆弱，在简单操纵下频繁翻转；发现系统性漏洞，包括奉承权衡（更强的指令跟随模型更易被说服）；轻量级推理干预可部分恢复道德稳定性。

Conclusion: 仅道德对齐不足以保证VLMs的负责任部署，道德鲁棒性是必要标准。研究揭示了当前VLMs在道德稳定性方面的严重缺陷，需要进一步研究提升其鲁棒性。

Abstract: Despite substantial efforts toward improving the moral alignment of Vision-Language Models (VLMs), it remains unclear whether their ethical judgments are stable in realistic settings. This work studies moral robustness in VLMs, defined as the ability to preserve moral judgments under textual and visual perturbations that do not alter the underlying moral context. We systematically probe VLMs with a diverse set of model-agnostic multimodal perturbations and find that their moral stances are highly fragile, frequently flipping under simple manipulations. Our analysis reveals systematic vulnerabilities across perturbation types, moral domains, and model scales, including a sycophancy trade-off where stronger instruction-following models are more susceptible to persuasion. We further show that lightweight inference-time interventions can partially restore moral stability. These results demonstrate that moral alignment alone is insufficient and that moral robustness is a necessary criterion for the responsible deployment of VLMs.

</details>


### [429] [Beyond Instrumental and Substitutive Paradigms: Introducing Machine Culture as an Emergent Phenomenon in Large Language Models](https://arxiv.org/abs/2601.17096)
*Yueqing Hu,Xinyang Peng,Yukun Zhao,Lin Qiu,Ka-lai Hung,Kaiping Peng*

Main category: cs.CY

TL;DR: 该研究挑战了将大语言模型视为人类文化反映或双语代理的传统范式，提出了"机器文化"作为新兴独特现象的概念，通过实验发现模型起源不预测文化对齐，存在"文化反转"现象，以及RLHF导致"服务人格伪装"。


<details>
  <summary>Details</summary>
Motivation: 当前研究通常通过工具性范式（模型反映开发者文化）或替代性范式（模型作为双语文化代理）来理解大语言模型，但这些人类中心主义框架可能无法准确描述LLMs的文化表现。本研究旨在挑战这些传统观点，探索LLMs是否展现出独特的"机器文化"现象。

Method: 采用2（模型起源：美国vs中国）×2（提示语言：英文vs中文）因子设计，在八个多模态任务中进行实验，特别引入图像生成和解释任务以超越文本分析边界，系统检验传统范式与机器文化假设。

Result: 结果与传统范式不一致：1）模型起源不预测文化对齐，美国模型常表现出东亚数据特征；2）提示语言不触发稳定的文化框架切换，反而出现"文化反转"现象（英文提示引发更高情境注意力）；3）发现"服务人格伪装"现象，RLHF将情感任务中的文化差异压缩为超积极、零方差的"有帮助助手"人格。

Conclusion: 大语言模型不模拟人类文化，而是展现出新兴的"机器文化"现象，这是由高维空间中的叠加效应和安全对齐导致的模式崩溃共同塑造的概率性现象，需要超越人类中心主义框架来理解LLMs的文化表现。

Abstract: Recent scholarship typically characterizes Large Language Models (LLMs) through either an \textit{Instrumental Paradigm} (viewing models as reflections of their developers' culture) or a \textit{Substitutive Paradigm} (viewing models as bilingual proxies that switch cultural frames based on language). This study challenges these anthropomorphic frameworks by proposing \textbf{Machine Culture} as an emergent, distinct phenomenon. We employed a 2 (Model Origin: US vs. China) $\times$ 2 (Prompt Language: English vs. Chinese) factorial design across eight multimodal tasks, uniquely incorporating image generation and interpretation to extend analysis beyond textual boundaries. Results revealed inconsistencies with both dominant paradigms: Model origin did not predict cultural alignment, with US models frequently exhibiting ``holistic'' traits typically associated with East Asian data. Similarly, prompt language did not trigger stable cultural frame-switching; instead, we observed \textbf{Cultural Reversal}, where English prompts paradoxically elicited higher contextual attention than Chinese prompts. Crucially, we identified a novel phenomenon termed \textbf{Service Persona Camouflage}: Reinforcement Learning from Human Feedback (RLHF) collapsed cultural variance in affective tasks into a hyper-positive, zero-variance ``helpful assistant'' persona. We conclude that LLMs do not simulate human culture but exhibit an emergent Machine Culture -- a probabilistic phenomenon shaped by \textit{superposition} in high-dimensional space and \textit{mode collapse} from safety alignment.

</details>


### [430] [Forecasting Energy Consumption using Recurrent Neural Networks: A Comparative Analysis](https://arxiv.org/abs/2601.17110)
*Abhishek Maity,Viraj Tukarul*

Main category: cs.CY

TL;DR: 该研究提出基于RNN和LSTM的深度学习模型，用于短期能源消耗预测，相比传统前馈神经网络显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的短期能源消耗预测对电网管理、资源分配和市场稳定至关重要。传统时间序列模型难以捕捉能源需求的复杂非线性依赖和外部影响因素。

Method: 采用循环神经网络（RNN）及其高级变体长短期记忆网络（LSTM），整合历史能源消耗数据与温度、湿度、时间特征等外部变量，在公开数据集上训练评估，并与传统前馈神经网络基准进行比较。

Result: 实验结果显示LSTM模型显著优于基准模型，实现了更低的平均绝对误差（MAE）和均方根误差（RMSE）。

Conclusion: 深度学习模型在提供可靠精准的短期能源预测方面具有显著优势，适用于实际应用场景。

Abstract: Accurate short-term energy consumption forecasting is essential for efficient power grid management, resource allocation, and market stability. Traditional time-series models often fail to capture the complex, non-linear dependencies and external factors affecting energy demand. In this study, we propose a forecasting approach based on Recurrent Neural Networks (RNNs) and their advanced variant, Long Short-Term Memory (LSTM) networks. Our methodology integrates historical energy consumption data with external variables, including temperature, humidity, and time-based features. The LSTM model is trained and evaluated on a publicly available dataset, and its performance is compared against a conventional feed-forward neural network baseline. Experimental results show that the LSTM model substantially outperforms the baseline, achieving lower Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). These findings demonstrate the effectiveness of deep learning models in providing reliable and precise short-term energy forecasts for real-world applications.

</details>


### [431] [Bowling Online: Accounting for Civil Society Reshaped into Streamlined Photons within a Fiber Network](https://arxiv.org/abs/2601.17139)
*Lukasz W. Niparko*

Main category: cs.CY

TL;DR: 本文探讨数字公民社会的兴起，认为虽然传统公民社会可能衰落，但人们转向数字公共领域进行组织、动员和民主参与。


<details>
  <summary>Details</summary>
Motivation: 回应Putnam关于美国公民社会衰落的观点，提出数字公民社会（DCS）和数字公共领域（DPS）的概念，认为需要重新评估公民社会在数字时代的表现形式。

Method: 本文尝试测量和界定数字公共领域（DPS），通过概念化数字公民社会在在线空间中的表现形式。

Result: 提出数字公民社会是传统公民社会的数字延伸，人们在网络空间中继续组织、动员和关注公民自由与民主制度。

Conclusion: 公民社会并未消失，而是转移到了数字领域；需要重新概念化和测量数字公共领域以理解当代民主参与的新形式。

Abstract: Civil society has been deemed by various scholars, such as Robert D. Putnam, to be a predictor and a cornerstone of a robust and consolidated democracy (Putnam et al., 1993). Putnam highlights in his book Bowling Alone (2000) that American civil society has become weaker: people organize less, and literally, they bowl alone. But what if there is yet another aspect to Putnam's story that has not been fully accounted for, namely the rise of Digital Civil Society (DCS)? Perhaps people in the third decade of the 21st century bowl online. They still organize, mobilize, and care for their civil liberties and democratic institutions; however, the public sphere in which this takes place has shifted online to cyberspace (Bernholz et al., 2013) or to what still needs to be conceptualized, the digital public sphere (DPS), which this article attempts to measure and demarcate.

</details>


### [432] [The Global Majority in International AI Governance](https://arxiv.org/abs/2601.17191)
*Chinasa T. Okolo,Mubarak Raji*

Main category: cs.CY

TL;DR: 本章通过全球AI鸿沟视角分析AI全球治理，关注发展、创新和监管的不平等，揭示西方主导治理框架排斥全球多数国家的问题，并提出促进公平包容的建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示全球AI治理中的系统性不平等，特别是全球多数国家在AI发展、创新和监管方面的边缘化问题，以及西方国家和企业在塑造AI治理框架中的主导地位如何忽视了全球多数国家的独特需求和背景。

Method: 采用全球AI鸿沟的分析框架，通过系统性分析教育、数字基础设施、决策参与等方面的不平等，考察西方主导的AI治理框架如何排斥全球多数国家，同时识别国家/区域AI战略等反趋势。

Result: 研究发现全球AI治理存在严重不平等，西方国家和企业主导治理框架，全球多数国家面临依赖和排斥循环；同时识别出国家/区域AI战略等反趋势可能促进公平包容。

Conclusion: 结论强调需要系统性改革、资源重新分配和有意义参与来民主化AI治理，呼吁合作行动确保AI治理成为共享繁荣的催化剂而非加深全球不平等。

Abstract: This chapter examines the global governance of artificial intelligence (AI) through the lens of the Global AI Divide, focusing on disparities in AI development, innovation, and regulation. It highlights systemic inequities in education, digital infrastructure, and access to decision-making processes, perpetuating a dependency and exclusion cycle for Global Majority countries. The analysis also explores the dominance of Western nations and corporations in shaping AI governance frameworks, which often sideline the unique priorities and contexts of the Global Majority. Additionally, this chapter identifies emerging countertrends, such as national and regional AI strategies, as potential avenues for fostering equity and inclusivity in global AI governance. The chapter concludes with actionable recommendations to democratize AI governance for Majority World countries, emphasizing the importance of systemic reforms, resource redistribution, and meaningful participation. It calls for collaborative action to ensure AI governance becomes a catalyst for shared prosperity, addressing global disparities rather than deepening them.

</details>


### [433] [Using psychological theory to ground guidelines for the annotation of misogynistic language](https://arxiv.org/abs/2601.17417)
*Artemis Deligianni,Zachary Horne,Leonidas A. A. Doumas*

Main category: cs.CY

TL;DR: 该论文针对当前算法检测厌女仇恨言论的不足，开发了基于心理学理论的标注指南和数据集，并通过LLM案例研究验证其优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前厌女言论检测算法存在理论基础薄弱的问题，标注指南缺乏心理学和哲学文献支撑，无法准确反映女性在线经历的真实厌女现象，而线上厌女问题日益严重，急需系统化的理论指导方案。

Method: 1) 基于理论和实证心理学研究开发厌女标注指南方案；2) 标注新数据集并达到较高的评分者间一致性(kappa=0.68)；3) 使用大语言模型进行案例研究，比较本方案与文献中自称"专家"的厌女标注方案。

Result: 1) 开发的标注指南方案在3个数据集的厌女文本分类中优于其他标注方案；2) 发现LLMs难以复现人类标注者的标签，主要因为LLMs反映了主流厌女观点；3) 标注数据集达到了显著的一致性水平。

Conclusion: 需要基于理论指导的厌女检测编码方案和相应数据集，LLMs在厌女检测应用中存在局限性，因为它们反映了主流偏见而非理论框架，这对使用LLMs进行厌女检测具有重要意义。

Abstract: Detecting misogynistic hate speech is a difficult algorithmic task. The task is made more difficult when decision criteria for what constitutes misogynistic speech are ungrounded in established literatures in psychology and philosophy, both of which have described in great detail the forms explicit and subtle misogynistic attitudes can take. In particular, the literature on algorithmic detection of misogynistic speech often rely on guidelines that are insufficiently robust or inappropriately justified -- they often fail to include various misogynistic phenomena or misrepresent their importance when they do. As a result, current misogyny detection coding schemes and datasets fail to capture the ways women experience misogyny online. This is of pressing importance: misogyny is on the rise both online and offline. Thus, the scientific community needs to have a systematic, theory informed coding scheme of misogyny detection and a corresponding dataset to train and test models of misogyny detection. To this end, we developed (1) a misogyny annotation guideline scheme informed by theoretical and empirical psychological research, (2) annotated a new dataset achieving substantial inter-rater agreement (kappa = 0.68) and (3) present a case study using Large Language Models (LLMs) to compare our coding scheme to a self-described "expert" misogyny annotation scheme in the literature. Our findings indicate that our guideline scheme surpasses the other coding scheme in the classification of misogynistic texts across 3 datasets. Additionally, we find that LLMs struggle to replicate our human annotator labels, attributable in large part to how LLMs reflect mainstream views of misogyny. We discuss implications for the use of LLMs for the purposes of misogyny detection.

</details>


### [434] [The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers](https://arxiv.org/abs/2601.17431)
*H. Kemal İlter*

Main category: cs.CY

TL;DR: 对50篇AI领域综述论文的5,514条引用进行法证审计，发现17%的引用无法解析到任何数字对象，揭示了AI工具在科学写作中引入系统性引用退化的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学写作中的应用虽然提高了效率，但可能引入信息熵风险。虽然"幻觉论文"是已知问题，但有效引用链的系统性退化尚未被量化。本研究旨在评估AI领域综述论文中引用链的完整性。

Method: 对2024年9月至2026年1月期间发表的50篇AI领域综述论文（共5,514条引用）进行法证审计。采用混合验证流程，结合DOI解析、Crossref元数据分析、Semantic Scholar查询和模糊文本匹配，区分格式错误（"粗心"）和可验证的不存在引用（"幻影"）。

Result: 发现持续17.0%的幻影率——即使采用激进的恢复方法也无法解析到任何数字对象的引用。诊断分类显示三种失败模式：纯粹幻觉（5.1%）、标题有效但标识符幻觉（16.4%）和解析导致的匹配失败（78.5%）。纵向分析显示平坦趋势（每月+0.07个百分点），表明高熵引用实践已稳定为该领域的固有特征。

Conclusion: AI调查文献中的科学引用图在大规模上表现出"链接腐烂"。这表明AI工具作为"懒惰的研究助手"的机制：检索正确的标题但幻觉元数据，从而切断了可重复科学所需的数字监管链。

Abstract: The adoption of Large Language Models (LLMs) in scientific writing promises efficiency but risks introducing informational entropy. While "hallucinated papers" are a known artifact, the systematic degradation of valid citation chains remains unquantified. We conducted a forensic audit of 50 recent survey papers in Artificial Intelligence (N=5,514 citations) published between September 2024 and January 2026. We utilized a hybrid verification pipeline combining DOI resolution, Crossref metadata analysis, Semantic Scholar queries, and fuzzy text matching to distinguish between formatting errors ("Sloppiness") and verifiable non-existence ("Phantoms). We detect a persistent 17.0% Phantom Rate -- citations that cannot be resolved to any digital object despite aggressive forensic recovery. Diagnostic categorization reveals three distinct failure modes: pure hallucinations (5.1%), hallucinated identifiers with valid titles (16.4%), and parsing-induced matching failures (78.5%). Longitudinal analysis reveals a flat trend (+0.07 pp/month), suggesting that high-entropy citation practices have stabilized as an endemic feature of the field. The scientific citation graph in AI survey literature exhibits "link rot" at scale. This suggests a mechanism where AI tools act as "lazy research assistants," retrieving correct titles but hallucinating metadata, thereby severing the digital chain of custody required for reproducible science.

</details>


### [435] [Building a Bridge between the Two Schools: Realizing a Practical Path to Include Literacy-based Skills within the STEM Curricula](https://arxiv.org/abs/2601.17447)
*Jorge Torres Gómez,Erika Gericke,Anton Rassõlkin,Mikołaj Leszczuk,Alexandru Iosup,Marcin Niemiec,Carmen Peláez-Moreno*

Main category: cs.CY

TL;DR: 该论文提出了一种将计算机科学核心概念与艺术实践相结合的教学方法，通过音乐、视频制作、游戏和表演艺术等方式整合技术技能与专业软技能，在技术院校课程中应用并取得积极效果。


<details>
  <summary>Details</summary>
Motivation: 现代社会需要培养全面发展的人才，计算机科学教育中需要将技术技能与专业软技能（如沟通、协作等）相结合，但目前技术院校中此类教学方法仍然较少。

Method: 提出了一种广泛适用的分步方法，将信息熵、网络安全等核心计算机科学概念与音乐、视频制作、游戏和表演艺术（如牛津式辩论）等艺术实践相结合，在技术大学的多个计算机科学课程中实施。

Result: 通过学生问卷调查和考试成绩等定量和定性评估显示，与传统方法相比，学生的学习成果得到改善，学生参与度提高，表明这种基于艺术的整合方法能有效弥合两种思维模式的历史鸿沟。

Conclusion: 这种艺术整合方法为教育工作者提供了实用方向，能够有效连接技术技能与专业软技能。同时指出了未来研究方向，包括教师参与度、女性在技术科目中的动机以及方法的可扩展性等问题。

Abstract: Developing students as well-rounded professionals is increasingly important for our modern society. Although there is a great consensus that technical and professional ("soft") skills should be developed and intertwined in the core of computer science subjects, there are still few examples of alike teaching methodologies at technical schools. This contribution investigates the integration of technical and professional skills while teaching specialized curricula in computer science. We propose a broadly applicable, step-by-step methodology that connects core technical concepts (e.g., information entropy, network security) with fine arts practices such as music, video production, gaming, and performing arts (e.g., Oxford-style debates). The methodology was applied in several computer science courses at technical universities, where quantitative and qualitative assessments, including student questionnaires and exam scores, showed improved learning outcomes and increased student engagement compared to traditional methods. The results indicate that this art-based integration can effectively bridge the historical divide between the two schools of thought, offering a practical direction for educators. Within this context, we also identify open issues that will guide future research on topics such as instructor engagement, female motivation in technical subjects, and scalability of these approaches.

</details>


### [436] [Ethical Risk Assessment of the Data Harnessing Process of LLM supported on Consensus of Well-known Multi-Ethical Frameworks](https://arxiv.org/abs/2601.17540)
*Javed I. Khan,Sharmila Rahman Prithula*

Main category: cs.CY

TL;DR: 提出基于伦理原则的伦理风险评分系统，用于量化评估AI系统数据采集过程的伦理完整性


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言处理方面取得了革命性进展，但其数据采集过程的伦理影响仍然是一个关键挑战。目前缺乏具体的框架来系统性地指导或衡量相关的伦理风险。

Method: 基于核心伦理原则构建伦理风险评分系统，这些原则由权威伦理理论支持。系统包含一系列评估问题，并整合可量化的评分机制。

Result: 提出了一个潜在的伦理风险评分系统框架，旨在通过量化评估来促进负责任的大语言模型开发。

Conclusion: 伦理风险评分系统为平衡技术创新与伦理责任提供了一条可行路径，有助于系统性地评估和改善AI系统数据采集过程的伦理完整性。

Abstract: The rapid advancements in large language models (LLMs) have revolutionized natural language processing, unlocking unprecedented capabilities in communication, automation, and knowledge generation. However, the ethical implications of LLM development, particularly in data harnessing, remain a critical challenge. Despite widespread discussion about the ethical compliance of LLMs -- especially concerning their data harnessing processes, there remains a notable absence of concrete frameworks to systematically guide or measure the ethical risks involved. In this paper we discuss a potential pathway for building an Ethical Risk Scoring (ERS) system to quantitatively assess the ethical integrity of the data harnessing process for AI systems. This system is based on a set of assessment questions grounded in core ethical principles, which are, in turn, supported by commanding ethical theories. By integrating measurable scoring mechanisms, this approach aims to foster responsible LLM development, balancing technological innovation with ethical accountability.

</details>


### [437] [Representative Litigation Settlement Agreements in Artificial Intelligence Copyright Infringement Disputes: A Comparative Reflection Based on the U.S](https://arxiv.org/abs/2601.17631)
*Chanhou Lou*

Main category: cs.CY

TL;DR: 生成式AI训练引发的高密度、去中心化版权冲突需要结构性治理工具，代表性诉讼和解协议通过程序性市场构建机制，既能降低交易成本，又能为合理使用第四要素中的"潜在市场"提供市场可见证据。


<details>
  <summary>Details</summary>
Motivation: 生成式AI训练引发的高密度、去中心化版权冲突超出了临时解决方案的能力范围，需要结构性治理工具。现有解决方案无法有效应对"反公地悲剧"带来的交易成本问题，同时缺乏为合理使用分析中的"潜在市场"要素提供市场证据的机制。

Method: 采用比较分析法，以美国Bartz集体诉讼和解为案例，揭示代表性诉讼和解协议的双重动机：表层风险规避和救济锁定，深层构建训练许可市场。在中国法语境下，提出三个解释机制：扩展"同类"请求的功能定义；采用混合注册/确认制度处理不确定集体成员资格；将《民事诉讼法》第57条第3款的"同意"要求转化为可操作的退出权并接受司法审查。

Result: 代表性诉讼和解协议具有制度优势，不仅能降低交易成本，还能通过定价信号和许可实践产生市场可见证据，验证合理使用第四要素中的"潜在市场"，实现程序性市场构建。美国案例显示双重动机结构，中国法语境下需建立适应性解释机制而非简单复制外国模式。

Conclusion: 代表性诉讼和解协议是应对生成式AI版权冲突的有效结构性治理工具，通过程序性市场构建机制解决"反公地悲剧"和"潜在市场"验证问题。在中国法实施需要建立三个适应性解释机制，为AI训练许可市场的形成提供制度基础。

Abstract: The high-density, decentralized copyright conflicts triggered by generative AI training require more than ad hoc solutions; they demand structural governance tools. This article argues that representative litigation settlement agreements offer a distinct institutional advantage. Beyond reducing the transaction costs associated with the "tragedy of the anticommons," these agreements generate market-visible evidence, specifically pricing signals and licensing practices, that validate the "potential market" under the fourth factor of fair use. This phenomenon constitutes procedural market-making. Through a comparative analysis of the U.S. Bartz class action settlement, this study reveals a dual motivation: a surface-level drive for risk aversion and remedy locking, and a deeper logic of constructing a training-licensing market. In the context of Chinese law, the feasibility of such agreements depends not on replicating foreign models, but on establishing three interpretive mechanisms: expanding the functional definition of "same category" claims; adopting a hybrid registration/confirmation system for indeterminate class membership; and converting the "consent" requirement under Article 57, Paragraph 3 of the Civil Procedure Law into a workable opt-out right subject to judicial scrutiny.

</details>


### [438] [Scaling Laws for Moral Machine Judgment in Large Language Models](https://arxiv.org/abs/2601.17637)
*Kazuhiro Takemoto*

Main category: cs.CY

TL;DR: 研究发现语言模型规模与道德判断能力之间存在幂律关系：模型越大，道德判断越接近人类偏好，且扩展推理能力可带来额外16%的改进。


<details>
  <summary>Details</summary>
Motivation: 自主系统越来越需要道德判断能力，但现有研究尚未探索这些能力是否随模型规模可预测地扩展。需要系统评估不同规模语言模型在生死困境中的道德判断能力，为人工智能治理提供实证基础。

Method: 使用Moral Machine框架系统评估75个大型语言模型配置（0.27B-1000B参数），测量其在生死困境中与人类偏好的对齐程度。采用混合效应模型控制模型家族和推理能力的影响，并分析扩展推理模型的额外效果。

Result: 观察到一致的幂律关系：与人类偏好的距离随模型规模增大而减小（D ∝ S^{-0.10±0.01}，R²=0.50，p<0.001）。混合效应模型确认该关系在控制变量后依然存在。扩展推理模型显示出超越规模效应的额外16%改进。该关系在不同架构中均成立，且大规模模型方差减小，表明道德判断能力随计算规模系统性地涌现。

Conclusion: 研究首次将扩展定律研究扩展到价值判断领域，揭示了语言模型道德判断能力随规模可预测地提升的规律，为人工智能治理提供了重要的实证基础，表明更大规模的计算资源可以产生更可靠、更一致的道德判断能力。

Abstract: Autonomous systems increasingly require moral judgment capabilities, yet whether these capabilities scale predictably with model size remains unexplored. We systematically evaluate 75 large language model configurations (0.27B--1000B parameters) using the Moral Machine framework, measuring alignment with human preferences in life-death dilemmas. We observe a consistent power-law relationship with distance from human preferences ($D$) decreasing as $D \propto S^{-0.10\pm0.01}$ ($R^2=0.50$, $p<0.001$) where $S$ is model size. Mixed-effects models confirm this relationship persists after controlling for model family and reasoning capabilities. Extended reasoning models show additional 16\% improvement beyond scale effects. The relationship holds across diverse architectures, while variance decreases at larger scales, indicating systematic emergence of more reliable moral judgment with computational scale. These findings extend scaling law research to value-based judgments and provide empirical foundations for artificial intelligence governance.

</details>


### [439] [Predicting Juror Predisposition Using Machine Learning: A Comparative Study of Human and Algorithmic Jury Selection](https://arxiv.org/abs/2601.17745)
*Ashwin Murthy,Ramesh Krishnamaneni,Sean Chacon,Kelsey Carlson,Ranjita Naik*

Main category: cs.CY

TL;DR: 专业陪审团顾问预测陪审员倾向的效果有限，而监督机器学习模型（特别是随机森林和KNN）在相同信息约束下显著优于人类顾问，且更具透明度、可重复性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 先前关于专业陪审团顾问预测陪审员倾向有效性的研究结果不一，且缺乏在受控条件下将顾问表现与随机水平进行严格比较的评估。本研究旨在填补这一空白，实证评估陪审团顾问是否能可靠地超越随机水平预测陪审员倾向，以及监督机器学习模型是否能超越顾问预测。

Method: 使用N名模拟陪审员的数据，这些陪审员完成了审前态度问卷并在标准化的不当解雇案件中做出裁决。将专业陪审团顾问的预测与随机森林（RF）和k近邻（KNN）分类器生成的预测进行比较。使用配对统计检验和非参数bootstrap程序在保留测试集上评估模型和顾问预测。

Result: 监督机器学习模型在相同信息约束下显著优于专业陪审团顾问。随机森林和KNN模型在预测陪审员倾向方面表现出更好的性能，同时提供了更高的透明度、可重复性和可审计性。

Conclusion: 研究结果为评估陪审员选择中的人类判断提供了实证基准，并为法律背景下数据驱动决策支持的作用的持续辩论提供了信息。监督机器学习模型在预测陪审员倾向方面优于人类专家，且更具透明度和可审计性，所有代码和数据将在发表后公开以支持可重复性。

Abstract: Prior studies on the effectiveness of professional jury consultants in predicting juror proclivities have yielded mixed results, and few have rigorously evaluated consultant performance against chance under controlled conditions. This study addresses that gap by empirically assessing whether jury consultants can reliably predict juror predispositions beyond chance levels and whether supervised machine-learning (ML) models can outperform consultant predictions. Using data from N mock jurors who completed pre-trial attitudinal questionnaires and rendered verdicts in a standardized wrongful-termination case, we compared predictions made by professional jury consultants with those generated by Random Forest (RF) and k-Nearest Neighbors (KNN) classifiers. Model and consultant predictions were evaluated on a held-out test set using paired statistical tests and nonparametric bootstrap procedures. We find that supervised ML models significantly outperform professional jury consultants under identical informational constraints, while offering greater transparency, replicability, and auditability. These results provide an empirical benchmark for evaluating human judgment in jury selection and inform ongoing debates about the role of data-driven decision support in legal contexts. To support reproducibility and auditability, all code and data will be made publicly available upon publication.

</details>


### [440] [Comparative Algorithmic Governance of Public Health Instruments across India, EU, US and LMICs](https://arxiv.org/abs/2601.17877)
*Sahibpreet Singh*

Main category: cs.CY

TL;DR: 该研究分析了国际公共卫生工具的法律技术架构，比较了印度、欧盟、美国及中低收入国家在AI增强公共卫生实施方面的差异，提出基于权利合规的超国家协调监管框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补规范性卫生法与算法公共卫生基础设施在资源受限司法管辖区之间协调不足的研究空白，评估AI如何增强基于《国际卫生条例2005》和《世卫组织烟草控制框架公约》的工具实施。

Method: 采用比较法律分析和法律规范映射方法，三角验证立法工具、世卫组织监测框架、AI系统（包括BlueDot、Aarogya Setu和EIOS）以及合规指标。

Result: 初步结果显示，AI在高能力司法管辖区改善了早期检测、监测精度和响应能力，而中低收入国家面临基础设施不足、数据隐私差距和碎片化法律框架等问题。欧盟《人工智能法案》和GDPR可作为健康导向算法治理的监管原型。

Conclusion: 研究主张将AI嵌入权利合规、超国家协调的监管框架，以确保公平的健康结果和更强的合规性。提出受FCTC架构启发的算法条约制定模型，并呼吁建立世卫组织主导的、仿效WTO争端解决机制的合规机制。

Abstract: The study investigates the juridico-technological architecture of international public health instruments, focusing on their implementation across India, the European Union, the United States and low- and middle-income countries (LMICs), particularly in Sub-Saharan Africa. It addresses a research lacuna: the insufficient harmonisation between normative health law and algorithmic public health infrastructures in resource-constrained jurisdictions. The principal objective is to assess how artificial intelligence augments implementation of instruments grounded in IHR 2005 and the WHO FCTC while identifying doctrinal and infrastructural bottlenecks. Using comparative doctrinal analysis and legal-normative mapping, the study triangulates legislative instruments, WHO monitoring frameworks, AI systems including BlueDot, Aarogya Setu and EIOS, and compliance metrics. Preliminary results show that AI has improved early detection, surveillance precision and responsiveness in high-capacity jurisdictions, whereas LMICs face infrastructural deficits, data privacy gaps and fragmented legal scaffolding. The findings highlight the relevance of the EU Artificial Intelligence Act and GDPR as regulatory prototypes for health-oriented algorithmic governance and contrast them with embryonic AI integration and limited internet penetration in many LMICs. The study argues for embedding AI within a rights-compliant, supranationally coordinated regulatory framework to secure equitable health outcomes and stronger compliance. It proposes a model for algorithmic treaty-making inspired by FCTC architecture and calls for WHO-led compliance mechanisms modelled on the WTO Dispute Settlement Body to enhance pandemic preparedness, surveillance equity and transnational governance resilience.

</details>


### [441] [Artificial Intelligence and Intellectual Property Rights: Comparative Transnational Policy Analysis](https://arxiv.org/abs/2601.17892)
*Sahibpreet Singh,Manjit Singh*

Main category: cs.CY

TL;DR: 该研究分析了印度知识产权法在应对AI生成内容方面的不足，通过比较研究方法提出了适应AI时代的法律框架建议


<details>
  <summary>Details</summary>
Motivation: 人工智能与知识产权的快速融合需要评估其对商业秘密、版权和专利的影响。印度缺乏专门的AI规定，导致法律不一致和执行效率低下，全球关于AI-IPR保护的讨论仍处于初级阶段。

Method: 采用教义学和比较研究方法，审查了印度、美国、英国和欧盟的立法文本、司法判例和政策文件，分析了现有法律框架对AI生成内容的适应性。

Result: 研究发现印度知识产权法存在明显缺陷：商业秘密保护不足应对AI威胁；缺乏标准化的发明人标准；印度专利法第3(k)条阻碍AI发明专利申请；版权法在作者归属方面存在差异；合同法导致商业秘密制度碎片化。

Conclusion: 研究建议建立协调的法律分类体系，在适应AI作用的同时保持创新激励。印度2024年国家AI战略显示进展，但需要立法明确性。重新调整印度知识产权法理学以实现全球协调，确保AI特定知识产权保护的韧性和公平创新。

Abstract: Artificial intelligence's rapid integration with intellectual property rights necessitates assessment of its impact on trade secrets, copyrights and patents. This study addresses lacunae in existing laws where India lacks AI-specific provisions, creating doctrinal inconsistencies and enforcement inefficacies. Global discourse on AI-IPR protections remains nascent. The research identifies gaps in Indian IP laws' adaptability to AI-generated outputs: trade secret protection is inadequate against AI threats; standardized inventorship criteria are absent. Employing doctrinal and comparative methodology, it scrutinizes legislative texts, judicial precedents and policy instruments across India, US, UK and EU. Preliminary findings reveal shortcomings: India's contract law creates fragmented trade secret regime; Section 3(k) of Indian Patents Act blocks AI invention patenting; copyright varies in authorship attribution. The study proposes harmonized legal taxonomy accommodating AI's role while preserving innovation incentives. India's National AI Strategy (2024) shows progress but legislative clarity is imperative. This contributes to global discourse with AI-specific IP protections ensuring resilience and equitable innovation. Promising results underscore recalibrating India's IP jurisprudence for global alignment.

</details>


### [442] ["Lighting The Way For Those Not Here": How Technology Researchers Can Help Fight the Missing and Murdered Indigenous Relatives (MMIR) Crisis](https://arxiv.org/abs/2601.17966)
*Naman Gupta,Sophie Stephenson,Chung Chi Yeung,Wei Ting Wu,Jeneile Luebke,Kate Walsh,Rahul Chatterjee*

Main category: cs.CY

TL;DR: 本文通过分析140个网页，探讨技术在失踪与被谋杀原住民亲属危机中的双重角色：既延续伤害又赋能抵抗，并提出HCI研究者支持原住民主导倡议的建议。


<details>
  <summary>Details</summary>
Motivation: 北美原住民面临不成比例的失踪与被谋杀率，这种"种族灭绝"植根于殖民暴力与系统性抹除。技术在此危机中扮演关键但矛盾的角色：既延续伤害阻碍调查，又赋能倡导与抵抗。然而，HCI领域缺乏以社区声音为中心的研究来批判性审视技术如何塑造这一危机。

Method: 通过大规模研究分析140个网页，识别系统性、技术性和制度性障碍，同时突出促进疗愈与安全的社会技术行动。研究还通过提供抵抗认知抹除的故事数据集来放大原住民声音。

Result: 识别了阻碍社区努力的障碍，同时发现了促进疗愈与安全的社会技术行动。创建了抵抗认知抹除的原住民故事数据集，为HCI研究者提供了支持原住民主导倡议的具体建议。

Conclusion: 技术在原住民亲属危机中具有双重性，HCI研究者应以文化敏感性、问责制和自决权支持原住民主导的倡议，通过放大社区声音来对抗系统性抹除。

Abstract: Indigenous peoples across Turtle Island (North America) face disproportionate rates of disappearance and murder, a "genocide" rooted in settler-colonial violence and systemic erasure. Technology plays a crucial role in the Missing and Murdered Indigenous Relatives (MMIR) crisis: perpetuating harm and impeding investigations, yet enabling advocacy and resistance. Communities utilize technologies such as AMBER alerts, news websites, social media groups, and campaigns (like #MMIW, #MMIWR, #NoMoreStolenSisters, and #NoMoreStolenDaughters) to mobilize searches, amplify awareness, and honor missing relatives. Yet, little research in HCI has critically examined technology's role in shaping the MMIR crisis by centering community voices. Through a large-scale study, we analyze 140 webpages to identify systemic, technological, and institutional barriers that hinder communities' efforts, while highlighting socio-technical actions that foster healing and safety. Finally, we amplify Indigenous voices by providing a dataset of stories that resist epistemic erasure, along with recommendations for HCI researchers to support Indigenous-led initiatives with cultural sensitivity, accountability, and self-determination.

</details>


### [443] [The Most Important Laboratory for Social Scientific and Computing Research in History](https://arxiv.org/abs/2601.17998)
*Benjamin Mako Hill,Aaron Shaw*

Main category: cs.CY

TL;DR: 该论文探讨了维基百科如何意外成为社会科学和计算研究的重要实验室，以及它对学术研究产生的深远影响。


<details>
  <summary>Details</summary>
Motivation: 维基百科的创始人最初并未意识到他们创建的平台会成为社会科学和计算研究的重要实验室。本文旨在探讨维基百科对学术研究产生的巨大影响，填补对这一现象系统性研究的空白。

Method: 作者Hill和Shaw通过系统性地分析维基百科对学术研究的影响，采用跨学科的研究方法，结合社会科学和计算科学的视角，评估维基百科作为研究平台的价值。

Result: 研究发现维基百科已成为历史上最重要的社会科学和计算研究实验室之一，对学术研究产生了深远影响，为多个学科领域提供了丰富的研究素材和方法论启示。

Conclusion: 维基百科意外地成为了社会科学和计算研究的重要实验室，其影响远超创始人预期，为学术研究开辟了新的领域和方法，值得进一步深入探索。

Abstract: Wikipedia's founders could not have dreamed they were creating the most important laboratory for social scientific and computing research in history but that is exactly what happened. Hill and Shaw take account of Wikipedia's enormous effect on academic scholarship

</details>


### [444] [The Limits of AI Data Transparency Policy: Three Disclosure Fallacies](https://arxiv.org/abs/2601.18127)
*Judy Hanwen Shen,Ken Liu,Angelina Wang,Sarah H. Cen,Andy K. Zhang,Caroline Meinhardt,Daniel Zhang,Kevin Klyman,Rishi Bommasani,Daniel E. Ho*

Main category: cs.CY

TL;DR: 论文分析了当前AI数据透明度政策的不足，指出了三大差距：规范差距、执行差距和影响差距，并基于社会科学研究提出了更有效的透明度路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI数据透明度政策虽然旨在解决数据质量、隐私和版权等问题，但往往无法实现预期目标，类似于食品营养标签政策存在的问题。需要基于社会科学研究来改进透明度政策。

Method: 采用制度视角分析AI数据披露政策，识别出三个常见谬误：规范差距（目标与必要披露之间的脱节）、执行差距（纸上要求与实际合规之间的差距）、影响差距（披露信息与实质性改变之间的差距）。

Result: 当前AI数据透明度政策存在三大系统性缺陷：1）披露要求与透明度目标不匹配；2）缺乏有效执行机制；3）披露信息未能真正改变开发者实践和公众理解。

Conclusion: 需要基于社会科学研究设计更有效的透明度政策，超越象征性披露，实现实质性问责和改变，为AI数据透明度提供更有意义的路径。

Abstract: Data transparency has emerged as a rallying cry for addressing concerns about AI: data quality, privacy, and copyright chief among them. Yet while these calls are crucial for accountability, current transparency policies often fall short of their intended aims. Similar to nutrition facts for food, policies aimed at nutrition facts for AI currently suffer from a limited consideration of research on effective disclosures. We offer an institutional perspective and identify three common fallacies in policy implementations of data disclosures for AI. First, many data transparency proposals exhibit a specification gap between the stated goals of data transparency and the actual disclosures necessary to achieve such goals. Second, reform attempts exhibit an enforcement gap between required disclosures on paper and enforcement to ensure compliance in fact. Third, policy proposals manifest an impact gap between disclosed information and meaningful changes in developer practices and public understanding. Informed by the social science on transparency, our analysis identifies affirmative paths for transparency that are effective rather than merely symbolic.

</details>


### [445] [Beyond Pairwise Comparisons: A Distributional Test of Distinctiveness for Machine-Generated Works in Intellectual Property Law](https://arxiv.org/abs/2601.18156)
*Anirban Mukherjee,Hannah Hanwen Chang*

Main category: cs.CY

TL;DR: 提出一种基于最大均值差异的分布检验方法，用于评估人类与机器生成内容在语义分布上的统计可区分性，解决了传统逐项比较方法在处理无限输出空间时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统知识产权评估（专利新颖性、版权原创性、商标独特性）依赖作品间的逐项比较，这种方法在处理人类创作的有限作品集时可行，但对于机器生成的无限输出空间存在根本性不匹配。需要一种能够评估生成过程分布差异的方法。

Method: 提出基于语义嵌入的最大均值差异两样本检验方法，无需任务特定训练，可直接比较两个生成过程（人类或机器）的输出分布。该方法样本效率高，仅需少量样本（5-10张图像或7-20个文本）即可检测分布差异。

Result: 在三个领域验证：手写数字（受控图像）、专利摘要（文本）、AI生成艺术（真实图像）。发现感知悖论：人类评估者区分AI与人类艺术的准确率仅约58%，但该方法能检测到分布差异。结果表明生成模型并非简单复述训练数据。

Conclusion: 生成模型产生的输出在语义上类似人类，但在统计分布上具有可区分性，表明其主要功能是在学习的潜在空间中进行语义插值，而非简单复述训练数据。该方法为评估机器生成内容的独特性提供了新的分布视角。

Abstract: Key doctrines, including novelty (patent), originality (copyright), and distinctiveness (trademark), turn on a shared empirical question: whether a body of work is meaningfully distinct from a relevant reference class. Yet analyses typically operationalize this set-level inquiry using item-level evidence: pairwise comparisons among exemplars. That unit-of-analysis mismatch may be manageable for finite corpora of human-created works, where it can be bridged by ad hoc aggregations. But it becomes acute for machine-generated works, where the object of evaluation is not a fixed set of works but a generative process with an effectively unbounded output space. We propose a distributional alternative: a two-sample test based on maximum mean discrepancy computed on semantic embeddings to determine if two creative processes-whether human or machine-produce statistically distinguishable output distributions. The test requires no task-specific training-obviating the need for discovery of proprietary training data to characterize the generative process-and is sample-efficient, often detecting differences with as few as 5-10 images and 7-20 texts. We validate the framework across three domains: handwritten digits (controlled images), patent abstracts (text), and AI-generated art (real-world images). We reveal a perceptual paradox: even when human evaluators distinguish AI outputs from human-created art with only about 58% accuracy, our method detects distributional distinctiveness. Our results present evidence contrary to the view that generative models act as mere regurgitators of training data. Rather than producing outputs statistically indistinguishable from a human baseline-as simple regurgitation would predict-they produce outputs that are semantically human-like yet stochastically distinct, suggesting their dominant function is as a semantic interpolator within a learned latent space.

</details>


### [446] [Generative AI in Saudi Arabia: A National Survey of Adoption, Risks, and Public Perceptions](https://arxiv.org/abs/2601.18234)
*Abdulaziz AlDakheel,Ali Alshehre,Esraa Alamoudi,Moslim AlKhabbaz,Ahmed Aljohani,Raed Alharbi*

Main category: cs.CY

TL;DR: 沙特阿拉伯在Vision 2030下积极采用生成式AI，但公众认知、使用和担忧仍需深入研究。本研究通过全国330人调查发现：93%受访者使用文本生成AI，但高级应用较少；认知水平不均；认可生产力提升但担忧隐私、伦理和技能退化；强烈需求结构化培训。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在沙特阿拉伯的数字转型中日益重要，但公众对其认知、采用情况和担忧缺乏系统研究。Vision 2030背景下，需要了解沙特国民如何接触和使用这些工具，为政策制定和开发提供依据。

Method: 采用全国性调查方法，覆盖330名来自不同地区、年龄组和就业部门的沙特国民参与者。研究考察了生成式AI使用的七个维度：认知与理解、采用模式、感知影响、培训需求、风险与障碍、数据共享行为和未来期望。

Result: 93%受访者积极使用生成式AI，主要用于文本任务，编程或多模态生成等高级应用较少。认知水平不均，技术知识有限。认可生产力、工作质量和信息理解方面的益处，但担忧过度依赖会削弱批判性思维和专业技能。对AI输出持谨慎态度，普遍关注隐私、错误信息和伦理滥用，包括潜在就业替代。受访者对结合基础技能、领域应用及隐私伦理指导的结构化培训表现出强烈兴趣。

Conclusion: 研究为沙特阿拉伯生成式AI参与度建立了基准，为政策制定者和开发者提出优先事项：扩大AI素养、确保文化和语言适配的生成式AI解决方案、加强隐私和负责任部署的框架。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly becoming embedded in Saudi Arabia's digital transformation under Vision 2030, yet public awareness, adoption, and concerns surrounding these tools remain underexplored. This study provides an early snapshot of GenAI engagement among Saudi nationals. Using a nationwide survey of 330 participants across regions, age groups, and employment sectors, we examine seven dimensions of GenAI use: awareness and understanding, adoption patterns, perceived impacts, training needs, risks and barriers, data-sharing behaviors, and future expectations. Findings show that 93% of respondents actively use GenAI primarily for text-based tasks, while more advanced uses such as programming or multimodal generation are less common. Despite the prevalence of use, overall awareness and conceptual understanding remain uneven, with many reporting limited technical knowledge. Participants recognize GenAI's benefits for productivity, work quality, and understanding complex information, yet caution that sustained reliance may undermine critical thinking and key professional skills. Trust in AI-generated outputs remains cautious, with widespread concerns about privacy, misinformation, and ethical misuse, including potential job displacement. Respondents show strong interest in structured GenAI training that combines foundational skills, domain-specific applications, and clear guidance on privacy, ethics, and responsible use. These results establish a baseline for GenAI engagement in Saudi Arabia and highlight priorities for policymakers and developers: expanding AI literacy, ensuring culturally and linguistically aligned GenAI solutions, and strengthening frameworks for privacy and responsible deployment.

</details>


### [447] [Beyond the Checkbox: Strengthening DSA Compliance Through Social Media Algorithmic Auditing](https://arxiv.org/abs/2601.18405)
*Sara Solarova,Matúš Mesarčík,Branislav Pecher,Ivan Srba*

Main category: cs.CY

TL;DR: 论文分析当前DSA算法审计实践，发现方法不一致且技术深度不足，提出采用算法审计方法增强合规评估


<details>
  <summary>Details</summary>
Motivation: 数字服务法案(DSA)要求平台算法遵守透明度、用户保护和隐私义务，但当前审计实践的有效性未知，需要研究现有审计方法是否能确保合规

Method: 结合监管和技术视角，批判性分析选定的审计报告，重点关注三个关键算法相关条款：未成年人画像限制、推荐系统透明度、使用敏感数据的定向广告限制

Result: 分析显示在评估AI系统时存在显著的方法不一致性和技术深度不足问题

Conclusion: 为增强合规评估的深度、规模和独立性，提出采用算法审计方法——通过模拟用户行为、观察算法响应并分析审计现象的行为评估过程

Abstract: Algorithms of online platforms are required under the Digital Services Act (DSA) to comply with specific obligations concerning algorithmic transparency, user protection and privacy. To verify compliance with these requirements, DSA mandates platforms to undergo independent audits. Little is known about current auditing practices and their effectiveness in ensuring such compliance. To this end, we bridge regulatory and technical perspectives by critically examining selected audit reports across three critical algorithmic-related provisions: restrictions on profiling minors, transparency in recommender systems, and limitations on targeted advertising using sensitive data. Our analysis shows significant inconsistencies in methodologies and lack of technical depth when evaluating AI-powered systems. To enhance the depth, scale, and independence of compliance assessments, we propose to employ algorithmic auditing -- a process of behavioural assessment of AI algorithms by means of simulating user behaviour, observing algorithm responses and analysing them for audited phenomena.

</details>


### [448] [Rethinking AI in the age of climate collapse: Ethics, power, and responsibility](https://arxiv.org/abs/2601.18462)
*Julio Vega*

Main category: cs.CY

TL;DR: AI在气候行动中具有双重角色：既是气候建模、环境监测和能源优化的有力工具，又因其高能耗、硬件生产、算法偏见、企业权力集中等问题面临可持续性挑战。


<details>
  <summary>Details</summary>
Motivation: 气候危机需要整合科学、伦理、社会和技术视角的应对措施。AI作为气候建模、环境监测和能源优化的强大工具，其日益广泛的应用引发了关键的环境、伦理、法律和社会问题，需要审视AI在生态危机中的矛盾角色。

Method: 通过跨学科视角（包括环境伦理学、技术哲学和法律治理）分析AI在气候行动中的双重作用，探讨其承诺与风险，并提出政策建议。

Result: AI在气候预测、可再生能源管理和环境退化实时检测方面具有积极作用，但同时面临数据中心能耗、资源密集型硬件生产、算法偏见、企业权力集中和技术官僚决策等可持续性矛盾。

Conclusion: AI对气候行动的贡献根本上取决于塑造其发展的价值观、制度和权力结构。需要推动社会公正、生态负责和民主问责的AI使用，而非将其视为固有的可持续解决方案。

Abstract: The climate crisis requires responses that integrate scientific, ethical, social, and technological perspectives. Artificial intelligence (AI) has emerged as a powerful tool in climate modelling, environmental monitoring, and energy optimisation, yet its growing use also raises critical environmental, ethical, legal, and social questions. This contribution examines the ambivalent role of AI in the ecological crisis, addressing both its promises and its risks. On the one hand, AI supports improvements in climate forecasting, renewable energy management, and real-time detection of environmental degradation. On the other hand, the energy demands of data centres, resource-intensive hardware production, algorithmic bias, corporate concentration of power, and technocratic decision-making reveal contradictions that challenge its sustainability. The discussion explores these issues through interdisciplinary lenses, including environmental ethics, philosophy of technology, and legal governance, and concludes with recommendations for socially just, ecologically responsible, and democratically accountable uses of AI. Rather than assuming AI as an inherently sustainable solution, this analysis argues that its contribution to climate action depends fundamentally on the values, institutions, and power structures that shape its development.

</details>


### [449] [Brazilian Social Media Anti-vaccine Information Disorder Dataset -- Telegram (2020-2025)](https://arxiv.org/abs/2601.18622)
*João Phillipe Cardenuto,Ana Carolina Monari,Michelle Diniz Lopes,Leopoldo Lusquino Filho,Anderson Rocha*

Main category: cs.CY

TL;DR: 本文介绍了一个包含约400万条巴西反疫苗Telegram频道帖子的数据集，用于研究疫苗错误信息传播


<details>
  <summary>Details</summary>
Motivation: 巴西疫苗接种覆盖率下降，疫苗相关错误信息在社交媒体广泛传播，特别是Telegram平台，需要数据支持研究对抗错误信息的策略

Method: 从119个巴西反疫苗Telegram频道收集2020-2025年约400万条帖子，包含消息内容、元数据、相关媒体和疫苗帖子分类

Result: 创建了公开可用的数据集，支持研究人员分析错误信息传播模式、演变过程和公众情绪影响

Conclusion: 该数据集为科学和公共卫生界提供资源，支持制定基于证据的策略来对抗错误信息，促进疫苗接种信任，并以同理心与受影响群体互动

Abstract: Over the past decade, Brazil has experienced a decline in vaccination coverage, reversing decades of public health progress achieved through the National Immunization Program (PNI). Growing evidence points to the widespread circulation of vaccine-related misinformation -- particularly on social media platforms -- as a key factor driving this decline. Among these platforms, Telegram remains the only major platform permitting accessible and ethical data collection, offering insight into public channels where vaccine misinformation circulates extensively. This data paper introduces a curated dataset of about four million Telegram posts collected from 119 prominent Brazilian anti-vaccine channels between 2020 and 2025. The dataset includes message content, metadata, associated media, and classification related to vaccine posts, enabling researchers to examine how false or misleading information spreads, evolves, and influences public sentiment. By providing this resource, our aim is to support the scientific and public health community in developing evidence-based strategies to counter misinformation, promote trust in vaccination, and engage compassionately with individuals and communities affected by false narratives. The dataset and documentation are openly available for non-commercial research, under strict ethical and privacy guidelines at https://doi.org/10.25824/redu/5JIVDT

</details>


### [450] [When Is Self-Disclosure Optimal? Incentives and Governance of AI-Generated Content](https://arxiv.org/abs/2601.18654)
*Juan Wu,Zhe,Zhang,Amit Mehra*

Main category: cs.CY

TL;DR: 该论文研究了AI生成内容披露政策的经济影响，发现只有当AI内容价值和成本节约优势处于中等水平时，披露才是最优策略。随着AI能力提升，平台的最优执法策略从严格威慑演变为部分筛选，最终走向放松管制。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI降低内容创作成本并实现规模化产出，平台开始采用要求创作者标注AI生成内容的披露政策。然而，这些政策的经济影响尚不明确，需要研究披露制度如何影响创作者行为、平台治理和内容质量。

Method: 建立正式的经济模型，比较无披露基准（仅平台检测AI使用）与强制自我披露制度（创作者在执法不完善下策略性选择披露或隐瞒）。模型包含异质性创作者、观众对AI标注内容的折扣、检测到未披露的信任惩罚以及内生的执法机制。

Result: 披露仅在AI生成内容的价值和成本节约优势处于中等水平时最优。随着AI能力提升，平台最优执法策略从严格威慑演变为部分筛选，最终走向放松管制。披露虽增加透明度，但会减少创作者总剩余，并在AI技术先进时抑制高质量AI内容。

Conclusion: 披露政策是一种战略性治理工具，其有效性取决于技术成熟度和信任摩擦。平台应根据AI技术发展阶段调整执法策略，平衡透明度、创作者激励和内容质量之间的关系。

Abstract: Generative artificial intelligence (Gen-AI) is reshaping content creation on digital platforms by reducing production costs and enabling scalable output of varying quality. In response, platforms have begun adopting disclosure policies that require creators to label AI-generated content, often supported by imperfect detection and penalties for non-compliance. This paper develops a formal model to study the economic implications of such disclosure regimes. We compare a non-disclosure benchmark, in which the platform alone detects AI usage, with a mandatory self-disclosure regime in which creators strategically choose whether to disclose or conceal AI use under imperfect enforcement. The model incorporates heterogeneous creators, viewer discounting of AI-labeled content, trust penalties following detected non-disclosure, and endogenous enforcement. The analysis shows that disclosure is optimal only when both the value of AI-generated content and its cost-saving advantage are intermediate. As AI capability improves, the platform's optimal enforcement strategy evolves from strict deterrence to partial screening and eventual deregulation. While disclosure reliably increases transparency, it reduces aggregate creator surplus and can suppress high-quality AI content when AI is technologically advanced. Overall, the results characterize disclosure as a strategic governance instrument whose effectiveness depends on technological maturity and trust frictions.

</details>
