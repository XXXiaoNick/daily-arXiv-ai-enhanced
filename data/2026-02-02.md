<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 61]
- [stat.ML](#stat.ML) [Total: 16]
- [cs.AI](#cs.AI) [Total: 55]
- [math.OC](#math.OC) [Total: 19]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [eess.SY](#eess.SY) [Total: 7]
- [cs.LG](#cs.LG) [Total: 183]
- [econ.EM](#econ.EM) [Total: 2]
- [q-fin.GN](#q-fin.GN) [Total: 3]
- [cs.CY](#cs.CY) [Total: 11]
- [q-fin.RM](#q-fin.RM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 本文研究如何通过"醉酒语言"诱导LLMs产生安全漏洞，包括越狱攻击和隐私泄露，发现醉酒语言能有效降低LLMs的安全防护能力。


<details>
  <summary>Details</summary>
Motivation: 人类在酒精影响下容易产生不良行为和隐私泄露，本文研究醉酒语言作为驱动LLMs安全失效的机制，探索LLMs在醉酒语言诱导下的安全风险。

Method: 提出三种诱导醉酒语言的方法：基于角色的提示、因果微调、基于强化的后训练。在5个LLMs上评估，使用JailbreakBench和ConfAIde基准测试，结合人工评估和LLM评估器分析错误类别。

Result: 醉酒语言诱导的LLMs在JailbreakBench上表现出更高的越狱成功率（即使有防御措施），在ConfAIde上隐私泄露更严重。发现人类醉酒行为与LLMs醉酒语言诱导下的人性化特征存在对应关系。

Conclusion: 醉酒语言诱导方法简单高效，可能成为对抗LLM安全调优的手段，凸显了LLM安全的重大风险。需要关注醉酒语言对LLMs安全性的潜在威胁。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [2] [MrRoPE: Mixed-radix Rotary Position Embedding](https://arxiv.org/abs/2601.22181)
*Qingyuan Tian,Wenhong Zhu,Xiaoran Liu,Xiaofeng Wang,Rui Wang*

Main category: cs.CL

TL;DR: MrRoPE通过混合进制系统转换视角统一了各种RoPE扩展方法，提出了两种无需训练的扩展策略，在长序列任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前RoPE位置编码的扩展策略多样且缺乏统一理论基础，需要一种能够系统化理解和改进RoPE扩展能力的方法。

Method: 提出MrRoPE（混合进制RoPE），基于进制系统转换视角建立统一编码框架，将不同RoPE扩展方法视为不同的进制转换策略。在此基础上提出了两种无需训练的扩展方法：MrRoPE-Uni（均匀进制转换）和MrRoPE-Pro（渐进进制转换）。

Result: MrRoPE-Pro在128K上下文的"大海捞针"测试中保持超过85%的召回率，在Infinite-Bench检索和对话子集上的准确率是YaRN的两倍以上。理论分析证实MrRoPE-Pro有效提高了RoPE可达到的编码长度上限。

Conclusion: MrRoPE为RoPE扩展提供了统一的理论框架，提出的训练无关扩展方法在长序列任务上表现出色，验证了该理论和方法的可靠性与实用性。

Abstract: Rotary Position Embedding (RoPE)-extension refers to modifying or generalizing the Rotary Position Embedding scheme to handle longer sequences than those encountered during pre-training. However, current extension strategies are highly diverse and lack a unified theoretical foundation. In this paper, we propose MrRoPE (Mixed-radix RoPE), a generalized encoding formulation based on a radix system conversion perspective, which elegantly unifies various RoPE-extension approaches as distinct radix conversion strategies. Based on this theory, we introduce two training-free extensions, MrRoPE-Uni and MrRoPE-Pro, which leverage uniform and progressive radix conversion strategies, respectively, to achieve 'train short, test long' generalization. Without fine-tuning, MrRoPE-Pro sustains over 85% recall in the 128K-context Needle-in-a-Haystack test and achieves more than double YaRN's accuracy on Infinite-Bench retrieval and dialogue subsets. Theoretical analysis confirms that MrRoPE-Pro effectively raises the upper bound of RoPE's attainable encoding length, which further validates the reliability and utility of our theory and methodology.

</details>


### [3] [Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning](https://arxiv.org/abs/2601.22297)
*Chenxi Liu,Yanshuo Chen,Ruibo Chen,Tianyi Xiong,Tong Zheng,Heng Huang*

Main category: cs.CL

TL;DR: 提出SDRL训练框架，让单个LLM既能独立解决问题，又能从多智能体辩论中学习多样推理路径，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法通常训练LLM孤立解决问题，没有明确准备它们来综合和利用辩论中出现的不同推理路径。多智能体辩论在测试时能提升性能，但训练时缺乏相应机制。

Method: SDRL框架：1) 对同一提示采样多个候选解；2) 构建包含多样推理路径的辩论上下文；3) 基于此上下文生成第二轮响应；4) 联合优化初始响应和辩论条件响应，使模型既能作为独立求解器，又能作为辩论参与者。

Result: 在多个基础模型和推理基准测试中，SDRL既提升了整体MAD性能，又增强了单模型推理能力。

Conclusion: SDRL通过让单个LLM在训练时学习从多样推理轨迹中受益，有效结合了强化学习和多智能体辩论的优势，实现了更好的推理性能。

Abstract: The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.

</details>


### [4] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: MERMAID是一个基于记忆增强的多智能体真实性评估框架，通过紧密耦合检索与推理过程，实现动态证据获取和跨声明证据复用，在多个事实核查基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前真实性评估方法通常将证据检索视为静态、孤立的步骤，未能有效管理或跨声明复用检索到的证据，导致效率低下和冗余搜索。

Method: 提出MERMAID框架，整合智能体驱动的搜索、结构化知识表示和持久性记忆模块，采用Reason-Action风格的迭代过程，实现动态证据获取和跨声明证据复用。

Result: 在三个事实核查基准和两个声明验证数据集上使用多种LLM（GPT、LLaMA、Qwen系列）进行评估，结果显示MERMAID达到最先进性能，同时提高了搜索效率。

Conclusion: 通过协同检索、推理和记忆，MERMAID框架能够实现可靠的真实性评估，减少冗余搜索，提高验证效率和一致性。

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [5] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: LLMs在上下文学习中表现出两种不同的表征直线化模式：在连续预测任务中，随着上下文增加，神经轨迹变得更直且预测性能提升；在结构化预测任务中，直线化仅出现在有明确结构的阶段。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在上下文学习过程中是否会出现表征直线化现象，探索不同任务类型下表征变化的模式差异。

Method: 在Gemma 2模型上测量多种上下文任务中的表征直线化程度，分析不同任务类型（连续预测vs结构化预测）下神经轨迹的变化模式。

Result: 发现二分现象：连续预测任务中，上下文增加导致神经轨迹更直且预测改善；结构化预测任务中，直线化仅出现在有明确结构的阶段，其他阶段消失。

Conclusion: 上下文学习不是单一过程，LLMs像瑞士军刀一样根据任务结构动态选择策略，只有部分策略会产生表征直线化。

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [6] [Stability-Aware Prompt Optimization for Clinical Data Abstraction](https://arxiv.org/abs/2601.22373)
*Arinbjörn Kolbeinsson,Daniel Timbie,Sajjan Narsinghani,Sanjay Hariharan*

Main category: cs.CL

TL;DR: 该研究提出将临床LLM的提示敏感性和准确性作为联合优化目标，通过双目标提示优化循环同时提升准确性和稳定性，减少模型对提示措辞的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 临床抽象任务中使用的大型语言模型对提示措辞敏感，但现有研究通常将提示视为固定且孤立地研究不确定性。作者认为应将这两个问题联合处理，因为模型可能看似校准良好但对提示改写仍然脆弱。

Method: 在两个临床任务（MedAlign适用性/正确性和MS亚型抽象）上，使用多个开源和专有模型，通过翻转率测量提示敏感性，并将其与校准和选择性预测相关联。提出双目标提示优化循环，同时针对准确性和稳定性进行优化。

Result: 研究发现：1）更高的准确性不能保证提示稳定性；2）模型可能看似校准良好但对提示改写仍然脆弱；3）明确包含稳定性项的双目标优化能减少跨任务和模型的翻转率，有时以适度准确性为代价。

Conclusion: 提示敏感性应成为验证临床LLM系统的明确目标，双目标优化方法能有效提升模型对提示措辞的鲁棒性，建议在临床LLM系统验证中考虑这一维度。

Abstract: Large language models used for clinical abstraction are sensitive to prompt wording, yet most work treats prompts as fixed and studies uncertainty in isolation. We argue these should be treated jointly. Across two clinical tasks (MedAlign applicability/correctness and MS subtype abstraction) and multiple open and proprietary models, we measure prompt sensitivity via flip rates and relate it to calibration and selective prediction. We find that higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases. We propose a dual-objective prompt optimization loop that jointly targets accuracy and stability, showing that explicitly including a stability term reduces flip rates across tasks and models, sometimes at modest accuracy cost. Our results suggest prompt sensitivity should be an explicit objective when validating clinical LLM systems.

</details>


### [7] [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)
*Bailin Wang,Dan Friedman,Tao Lei,Chong Wang*

Main category: cs.CL

TL;DR: SPLA是一种稀疏+线性注意力框架，通过二阶泰勒展开选择相关块进行精确注意力计算，同时使用残差线性注意力压缩未选择块，避免IO开销，在长上下文任务中超越密集注意力模型。


<details>
  <summary>Details</summary>
Motivation: 现有块稀疏注意力方法存在选择保真度低和累积上下文丢失问题，因为它们完全丢弃未选择的块。需要一种既能保持效率又能保留重要上下文信息的方法。

Method: SPLA框架：1）使用二阶泰勒展开的度量准确选择相关块进行精确注意力计算；2）通过残差线性注意力模块将未选择块压缩为紧凑的循环状态；3）采用优化的基于减法的RLA公式，避免在推理时显式访问未选择块。

Result: SPLA在持续预训练中缩小了性能差距，在RULER等长上下文基准测试中超越了密集注意力模型，同时保持了具有竞争力的通用知识和推理能力。

Conclusion: SPLA通过结合精确的块选择和高效的未选择块压缩，解决了现有稀疏注意力方法的局限性，实现了长上下文建模中效率与性能的良好平衡。

Abstract: Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention (SPLA), a framework that utilizes a selection metric derived from second-order Taylor expansions to accurately identify relevant blocks for exact attention. Instead of discarding the remaining "long tail," SPLA compresses unselected blocks into a compact recurrent state via a residual linear attention (RLA) module. Crucially, to avoid IO overhead, we derive an optimized subtraction-based formulation for RLA -- calculating the residual as the difference between global and selected linear attention -- ensuring that unselected blocks are never explicitly accessed during inference. Our experiments demonstrate that SPLA closes the performance gap in continual pretraining, surpassing dense attention models on long-context benchmarks like RULER while maintaining competitive general knowledge and reasoning capabilities.

</details>


### [8] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: SP2DPO是一种改进的DPO方法，通过为每个偏好对分配特定的温度参数beta_i，而不是使用全局统一的beta，从而更好地处理异构偏好数据中的不同信号强度和噪声。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的偏好数据是异构的：混合了高信号的目标性失败（如安全性、事实性、指令违反）和低信号的主观性区别（如风格），同时还包含标签噪声。传统的DPO使用单一全局温度参数beta，将所有偏好对视为同等信息量，无法有效处理这种异质性。

Method: SP2DPO将DPO的全局温度替换为实例特定的调度参数beta_i，这些参数通过教师语言模型生成的语义差距标注（类别、幅度、置信度）离线预决定。在UltraFeedback偏好语料库（59,960对）上实例化此过程，构建可审计的beta_i工件，训练时保持标准DPO优化器，仅按对设置beta。

Result: 在四个开放权重的指令调优学生骨干模型（4B-8B）上，SP2DPO与调优的全局beta DPO基线竞争，并在两个骨干模型上提高了AlpacaEval 2.0长度控制胜率，同时避免了每个模型的beta搜索开销。

Conclusion: SP2DPO通过语义感知的每对温度调度，有效处理异构偏好数据，在保持训练效率的同时提升模型性能，避免了繁琐的超参数调优，为偏好优化提供了更精细的控制机制。

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [9] [Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading](https://arxiv.org/abs/2601.22386)
*Jamiu Adekunle Idowu,Ahmed Almasoud*

Main category: cs.CL

TL;DR: 多智能体LLM架构在识别弱作文方面表现更佳，而单智能体架构在中档作文评分上更好；两种架构对高质量作文都有困难；少量示例校准是性能提升的关键因素。


<details>
  <summary>Details</summary>
Motivation: 虽然自动作文评分系统越来越多地使用大语言模型，但不同架构选择如何影响其在各种作文质量水平上的表现尚不清楚，需要系统评估单智能体和多智能体LLM架构在作文评分中的效果。

Method: 使用ASAP 2.0语料库评估单智能体和多智能体LLM架构。多智能体系统将评分分解为三个专业智能体（内容、结构、语言），由主席智能体协调，实施包括否决规则和分数上限的评分标准对齐逻辑。在零样本和少样本条件下测试两种架构，使用GPT-5.1模型。

Result: 多智能体系统在识别弱作文方面显著更好，而单智能体系统在中档作文上表现更优。两种架构对高质量作文都有困难。少样本校准是系统性能的主导因素——每个分数级别仅提供两个示例就能将QWK提高约26%。

Conclusion: 架构选择应与具体部署优先级对齐：多智能体AI特别适合对有风险学生进行诊断性筛查，而单智能体模型为一般评估提供了经济高效的解决方案。

Abstract: Automated essay scoring (AES) systems increasingly rely on large language models, yet little is known about how architectural choices shape their performance across different essay quality levels. This paper evaluates single-agent and multi-agent LLM architectures for essay grading using the ASAP 2.0 corpus. Our multi-agent system decomposes grading into three specialist agents (Content, Structure, Language) coordinated by a Chairman Agent that implements rubric-aligned logic including veto rules and score capping. We test both architectures in zero-shot and few-shot conditions using GPT-5.1. Results show that the multi-agent system is significantly better at identifying weak essays while the single-agent system performs better on mid-range essays. Both architectures struggle with high-quality essays. Critically, few-shot calibration emerges as the dominant factor in system performance -- providing just two examples per score level improves QWK by approximately 26% for both architectures. These findings suggest architectural choice should align with specific deployment priorities, with multi-agent AI particularly suited for diagnostic screening of at-risk students, while single-agent models provide a cost-effective solution for general assessment.

</details>


### [10] [Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks](https://arxiv.org/abs/2601.22396)
*Candida M. Greco,Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 研究评估LLM生成的文化背景角色与世界价值观调查、文化地图和道德基础理论的匹配程度，发现这些合成角色能反映不同文化背景下的稳定差异和道德变化。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在模拟人类行为方面越来越有用，但这些合成角色在不同文化背景下是否准确反映世界和道德价值体系仍不确定。需要研究LLM生成的文化背景角色与现有框架的匹配程度。

Method: 基于世界价值观调查的可解释变量概念化并生成LLM角色，通过三个互补视角分析：1)在Inglehart-Welzel文化地图上的定位；2)与世界价值观调查的人口统计一致性；3)通过道德基础问卷分析道德特征，建立文化到道德的映射。

Result: 生成的角色在文化地图上显示出稳定差异，响应分布大致跟踪人类群体模式，道德特征分析揭示了不同文化配置下的道德变化。

Conclusion: 基于文化背景的角色生成和分析方法能够评估跨文化结构和道德变化，为LLM模拟人类文化行为提供了评估框架。

Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

</details>


### [11] [Bifocal Attention: Harmonizing Geometric and Spectral Positional Embeddings for Algorithmic Generalization](https://arxiv.org/abs/2601.22402)
*Kanishk Awadhiya*

Main category: cs.CL

TL;DR: 论文提出Bifocal Attention架构，通过分离几何位置编码和谱位置编码来解决RoPE在长距离递归推理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准RoPE存在"谱刚性"问题，其固定的几何衰减模式虽然适合局部句法连贯性，但无法捕捉递归逻辑和算法推理中的长距离周期性结构，导致模型在浅层推理链上训练后无法外推到更深递归步骤的"结构鸿沟"。

Method: 提出Bifocal Attention架构，将位置编码解耦为两种模态：几何眼（标准RoPE）用于精确的token级操作，谱眼（可学习谐波算子）用于跟踪长距离递归深度。同时提出Spectral Evolution训练协议，将位置频率初始化为静态几何参数，但允许通过梯度下降演化为针对任务特定算法拓扑优化的谐波基。

Result: 论文声称该方法能够解决标准RoPE在长距离递归推理中的局限性，但摘要中未提供具体实验结果数据。

Conclusion: Bifocal Attention通过分离几何和谱位置编码，结合Spectral Evolution训练协议，能够更好地处理算法推理中的长距离递归结构，弥补了标准RoPE在捕捉周期性模式方面的不足。

Abstract: Rotary Positional Embeddings (RoPE) have become the standard for Large Language Models (LLMs) due to their ability to encode relative positions through geometric rotation. However, we identify a significant limitation we term ''Spectral Rigidity'': standard RoPE utilizes a fixed geometric decay ($θ^{-i}$) optimized for local syntactic coherence, which fails to capture the long-range, periodic structures inherent in recursive logic and algorithmic reasoning. This results in a ''Structure Gap'', where models trained on shallow reasoning chains fail to extrapolate to deeper recursive steps. In this work, we introduce Bifocal Attention, an architectural paradigm that decouples positional encoding into two distinct modalities: Geometric Eyes (Standard RoPE) for precise token-level manipulation, and Spectral Eyes (Learnable Harmonic Operators) for tracking long-range recursive depth. We propose a novel training protocol, Spectral Evolution, which initializes positional frequencies as static geometric parameters but allows them to evolve via gradient descent into a harmonic basis optimized for the specific algorithmic topology of the task.

</details>


### [12] [Word-Centered Semantic Graphs for Interpretable Diachronic Sense Tracking](https://arxiv.org/abs/2601.22410)
*Imene Kolli,Kai-Robin Lange,Jonas Rieger,Carsten Jentsch*

Main category: cs.CL

TL;DR: 提出基于图的可解释框架，通过构建词中心语义网络来分析历时语料库中的语义演变，结合分布相似性和词汇可替换性，无需预定义义项库。


<details>
  <summary>Details</summary>
Motivation: 现有语义演变分析方法通常依赖预定义的义项库，缺乏透明度和灵活性。需要一种能够自然捕捉词义结构变化、无需人工标注义项的可解释框架。

Method: 为每个目标词和时间切片构建词中心语义网络：结合历时Skip-gram嵌入的分布相似性和时间特定掩码语言模型的词汇可替换性。通过聚类外围图识别义项相关结构，通过节点重叠对齐跨时间聚类，通过聚类组成和归一化聚类质量跟踪变化。

Result: 在《纽约时报杂志》文章语料库（1980-2017）的应用研究中显示：图连接性反映多义性动态，诱导的社区捕捉了对比轨迹：事件驱动的义项替换（trump）、语义稳定性与聚类过分割效应（god）、以及数字通信相关的渐进关联变化（post）。

Conclusion: 词中心语义图为探索义项演变提供了紧凑透明的表示方法，无需依赖预定义义项库，能够有效捕捉不同类型的语义变化轨迹。

Abstract: We propose an interpretable, graph-based framework for analyzing semantic shift in diachronic corpora. For each target word and time slice, we induce a word-centered semantic network that integrates distributional similarity from diachronic Skip-gram embeddings with lexical substitutability from time-specific masked language models. We identify sense-related structure by clustering the peripheral graph, align clusters across time via node overlap, and track change through cluster composition and normalized cluster mass. In an application study on a corpus of New York Times Magazine articles (1980 - 2017), we show that graph connectivity reflects polysemy dynamics and that the induced communities capture contrasting trajectories: event-driven sense replacement (trump), semantic stability with cluster over-segmentation effects (god), and gradual association shifts tied to digital communication (post). Overall, word-centered semantic graphs offer a compact and transparent representation for exploring sense evolution without relying on predefined sense inventories.

</details>


### [13] [Large Language Model Agents Are Not Always Faithful Self-Evolvers](https://arxiv.org/abs/2601.22436)
*Weixiang Zhao,Yingshuo Wang,Yichen Zhang,Yang Deng,Yanyan Zhao,Wanxiang Che,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 研究发现自进化LLM智能体在决策时对原始经验有因果依赖，但对浓缩经验经常忽视或误解，揭示了经验忠实性的不对称问题


<details>
  <summary>Details</summary>
Motivation: 自进化LLM智能体通过积累和重用过去经验不断改进，但尚不清楚它们是否真正依赖这些经验来指导行为。本文旨在系统研究自进化LLM智能体中的经验忠实性问题

Method: 使用受控因果干预方法，对原始和浓缩两种形式的经验进行处理，全面评估4个代表性框架、10个LLM骨干模型和9个环境

Result: 发现显著的不对称性：智能体始终依赖原始经验，但经常忽视或误解浓缩经验，即使这是唯一提供的经验。这种差距在单/多智能体配置和不同规模的骨干模型中持续存在

Conclusion: 经验忠实性差距源于三个因素：浓缩内容的语义限制、抑制经验的内在处理偏见，以及预训练先验已足够的任务机制。这些发现挑战了关于自进化方法的普遍假设，强调需要更忠实可靠的经验整合方法

Abstract: Self-evolving large language model (LLM) agents continually improve by accumulating and reusing past experience, yet it remains unclear whether they faithfully rely on that experience to guide their behavior. We present the first systematic investigation of experience faithfulness, the causal dependence of an agent's decisions on the experience it is given, in self-evolving LLM agents. Using controlled causal interventions on both raw and condensed forms of experience, we comprehensively evaluate four representative frameworks across 10 LLM backbones and 9 environments. Our analysis uncovers a striking asymmetry: while agents consistently depend on raw experience, they often disregard or misinterpret condensed experience, even when it is the only experience provided. This gap persists across single- and multi-agent configurations and across backbone scales. We trace its underlying causes to three factors: the semantic limitations of condensed content, internal processing biases that suppress experience, and task regimes where pretrained priors already suffice. These findings challenge prevailing assumptions about self-evolving methods and underscore the need for more faithful and reliable approaches to experience integration.

</details>


### [14] [Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss](https://arxiv.org/abs/2601.22439)
*Galim Turumtaev*

Main category: cs.CL

TL;DR: 提出阈值技术减少罕见token的边缘化影响，改善低资源语言在语言模型中的表示


<details>
  <summary>Details</summary>
Motivation: 神经语言模型在低资源语言上表现不佳，因为训练数据有限，这些语言的token在训练集中很罕见。罕见token在训练过程中受到不成比例的边缘化影响，阻碍了它们有效学习。

Method: 提出一种阈值技术，减少边缘化对罕见token的影响，使它们能从更有意义的对齐中受益。这是首次展示如何通过限制过度边缘化的有害影响来应用负采样改善罕见token的表示。

Result: 通过字符级语言模型的实验证明，该方法显著提高了低资源语言验证数据上的性能。

Conclusion: 该方法为增强低资源语言在语言模型中的表示提供了新途径，通过减少边缘化影响改善罕见token的学习效果。

Abstract: Neural language models often struggle with low-resource languages due to the limited availability of training data, making tokens from these languages rare in the training set. This paper addresses a specific challenge during training: rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively. We propose a thresholding technique that reduces the impact of this marginalization, allowing rare tokens to benefit from more meaningful alignment. Through experiments with a character-level language model, we demonstrate that this method significantly improves performance on low-resource language validation data. This work is the first to show how negative sampling can be applied to improve the representation of rare tokens by limiting the harmful influence of excessive marginalization, offering a new approach to enhancing language model performance for underrepresented languages.

</details>


### [15] [SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization](https://arxiv.org/abs/2601.22491)
*Jinyang Wu,Changpeng Yang,Yuhao Shen,Fangzhi Xu,Bolin Ni,Chonghua Liao,Yuchen Liu,Hongzhen Wang,Shuai Nie,Shuai Zhang,Haoran Luo,Jiaming Xu*

Main category: cs.CL

TL;DR: SSL（Sweet Spot Learning）是一种新颖的强化学习框架，通过渐进放大的分层奖励引导策略走向解空间的"甜点"区域，在视觉感知、规划、推理等任务上显著提升样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通常使用二元奖励，无法区分达到相同结果的不同轨迹的质量差异，忽略了解空间内的潜在多样性。受网球"甜点"概念的启发，需要一种能提供差异化指导的框架。

Method: SSL采用渐进放大的分层奖励机制：视觉感知任务通过距离分层建模奖励接近程度，复杂推理任务奖励向有前景解决方案的渐进进展。该方法保持最优解排序并增强梯度信噪比。

Result: 在GUI感知、短期/长期规划、复杂推理等12个基准测试中，SSL相比强基线取得一致改进，实现高达2.5倍的样本效率提升，并展现出有效的跨任务可迁移性。

Conclusion: SSL作为一种通用原则，能够训练出更强大、更鲁棒的智能体，为强化学习提供了新的差异化奖励指导范式。

Abstract: Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \textbf{S}weet \textbf{S}pot \textbf{L}earning (\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.

</details>


### [16] [Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic Tasks, Simulated Environments, and Rubric-Based Rewards](https://arxiv.org/abs/2601.22511)
*Yuan-Jay Lü,Chengyu Wang,Lei Shen,Jun Huang,Tong Xu*

Main category: cs.CL

TL;DR: SYNTHAGENT框架通过合成多样化的工具使用训练数据和模拟完整环境，解决了小模型在代理能力上的瓶颈，使小模型在多项任务上超越更大基线模型。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在代理能力上难以匹敌大模型，现有开源代理训练数据任务单一且易解决，真实世界API缺乏多样性且不稳定，限制了强化学习的进展。

Method: 使用强教师模型创建新颖任务和工具生态系统，将其改写为故意不完整的指令；LLM用户模拟器提供用户私有信息，模拟工具系统提供稳定工具响应；基于所需子目标、用户-代理交互和禁止行为构建任务级奖励机制。

Result: 在数学、搜索和工具使用等14个挑战性数据集上，使用合成数据训练的模型取得显著提升，小模型表现优于更大的基线模型。

Conclusion: SYNTHAGENT框架通过合成多样化的训练数据和模拟环境，有效解决了小模型代理能力训练的瓶颈，为开发更强大的小型代理模型提供了可行路径。

Abstract: Small LLMs often struggle to match the agentic capabilities of large, costly models. While reinforcement learning can help, progress has been limited by two structural bottlenecks: existing open-source agentic training data are narrow in task variety and easily solved; real-world APIs lack diversity and are unstable for large-scale reinforcement learning rollout processes. We address these challenges with SYNTHAGENT, a framework that jointly synthesizes diverse tool-use training data and simulates complete environments. Specifically, a strong teacher model creates novel tasks and tool ecosystems, then rewrites them into intentionally underspecified instructions. This compels agents to actively query users for missing details. When handling synthetic tasks, an LLM-based user simulator provides user-private information, while a mock tool system delivers stable tool responses. For rewards, task-level rubrics are constructed based on required subgoals, user-agent interactions, and forbidden behaviors. Across 14 challenging datasets in math, search, and tool use, models trained on our synthetic data achieve substantial gains, with small models outperforming larger baselines.

</details>


### [17] [One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean Geometry](https://arxiv.org/abs/2601.22521)
*Weisong Zhao,Tong Wang,Zichang Tan,Te Yang,Siran Peng,Haoyuan Zhang,Tianshuo Zhang,Haichao Shi,Meng Meng,Yang Yang,Xiangyu Zhu,Zhen Lei,Xiao-Yu Zhang,Xu Zhou*

Main category: cs.CL

TL;DR: 提出PMPO框架，通过幂均值几何参数p统一GRPO和GMPO，引入基于剪裁感知有效样本量的自适应p选择机制，在数学推理任务上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的强化学习方法（GRPO和GMPO）都依赖于固定的聚合几何结构，忽略了每个轨迹的演化和异质性特征，需要更灵活的框架来适应不同轨迹的特性。

Method: 提出幂均值策略优化（PMPO）框架，通过幂均值几何指数p参数化聚合几何结构；引入剪裁感知有效样本量（ESS）机制，建立轨迹剪裁分数到目标ESS的确定性映射，求解对应的p值，实现从激进算术均值到保守几何均值的动态过渡。

Result: 在多个数学推理基准测试中，PMPO优于包括GRPO和GMPO在内的强基线方法，证明了自适应聚合几何结构的有效性。

Conclusion: PMPO通过统一的幂均值框架和自适应p选择机制，解决了固定聚合几何结构的局限性，能够根据轨迹可靠性动态调整聚合策略，在强化学习中实现了更好的性能。

Abstract: Group-based reinforcement learning has evolved from the arithmetic mean of GRPO to the geometric mean of GMPO. While GMPO improves stability by constraining a conservative objective, it shares a fundamental limitation with GRPO: reliance on a fixed aggregation geometry that ignores the evolving and heterogeneous nature of each trajectory. In this work, we unify these approaches under Power-Mean Policy Optimization (PMPO), a generalized framework that parameterizes the aggregation geometry via the power-mean geometry exponent p. Within this framework, GRPO and GMPO are recovered as special cases. Theoretically, we demonstrate that adjusting p modulates the concentration of gradient updates, effectively reweighting tokens based on their advantage contribution. To determine p adaptively, we introduce a Clip-aware Effective Sample Size (ESS) mechanism. Specifically, we propose a deterministic rule that maps a trajectory clipping fraction to a target ESS. Then, we solve for the specific p to align the trajectory induced ESS with this target one. This allows PMPO to dynamically transition between the aggressive arithmetic mean for reliable trajectories and the conservative geometric mean for unstable ones. Experiments on multiple mathematical reasoning benchmarks demonstrate that PMPO outperforms strong baselines.

</details>


### [18] [$ρ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs](https://arxiv.org/abs/2601.22527)
*Jingyi Yang,Yuxian Jiang,Jing Shao*

Main category: cs.CL

TL;DR: 提出ρ-EOS方法，通过监测EOS令牌的隐式密度实现双向可变长度生成，解决掩码扩散大语言模型固定生成长度的限制。


<details>
  <summary>Details</summary>
Motivation: 当前掩码扩散大语言模型需要预定义固定生成长度，缺乏灵活性，必须在输出质量和计算效率之间权衡。需要一种能够自适应调整生成长度的方法。

Method: 通过研究去噪动态，发现EOS令牌的隐式密度可作为生成充分性的可靠信号。提出ρ-EOS策略，在统一去噪过程中持续估计隐式EOS密度：密度过高时触发MASK令牌收缩，密度不足时触发扩展，实现双向长度调整。

Result: 在数学和代码基准测试中，ρ-EOS在保持可比性能的同时，显著提高了推理效率和令牌利用率。

Conclusion: ρ-EOS是一种无需训练的单阶段策略，能够实现掩码扩散大语言模型的双向可变长度生成，解决了固定生成长度的根本限制，提高了模型的灵活性和效率。

Abstract: Beyond parallel generation and global context modeling, current masked diffusion large language models (dLLMs) suffer from a fundamental limitation: they require a predefined, fixed generation length, which lacks flexibility and forces an inevitable trade-off between output quality and computational efficiency. To address this, we study the denoising dynamics and find that the implicit density ($ρ$) of end-of-sequence ($\texttt{EOS}$) tokens serves as a reliable signal of generation sufficiency. In particular, the evolving implicit $\texttt{EOS}$ density during denoising reveals whether the current masked space is excessive or insufficient, thereby guiding the adjustment direction for generation length. Building on this insight, we propose $\textbf{$ρ$-$\texttt{EOS}$}$, a training-free, single-stage strategy that enables bidirectional variable-length generation for masked dLLMs. Unlike prior two-stage approaches--which require separate length adjustment and iterative mask insertion phases while supporting only unidirectional expansion--$\textbf{$ρ$-$\texttt{EOS}$}$ achieves bidirectional length adjustment within a unified denoising process by continuously estimating the implicit $\texttt{EOS}$ density: excessively high density triggers $\texttt{MASK}$ token contraction, while insufficient density induces expansion. Extensive experiments on mathematics and code benchmarks demonstrate that $\textbf{$ρ$-$\texttt{EOS}$}$ achieves comparable performance while substantially improving inference efficiency and token utilization.

</details>


### [19] [Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation](https://arxiv.org/abs/2601.22546)
*Shun Qian,Bingquan Liu,Chengjie Sun,Zhen Xu,Baoxun Wang*

Main category: cs.CL

TL;DR: 论文提出语言模型具有"全息特性"——在生成初期就捕获目标侧关键词，并基于此设计了HOLO插件来提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在上下文学习和思维链能力方面取得进展，但对其强大生成能力的具体特性研究较少。本文旨在深入探究LLMs的生成特性。

Method: 研究发现语言模型具有"全息特性"——在生成初期捕获目标侧关键词。基于此提出HOLO插件，在有限生成步数内提取关键词，并结合并行词汇约束文本生成方法补充句子。

Result: 在不同架构和规模的短文本生成场景中进行大量实验，结果显示HOLO在自动评估和人工评估指标上与基线方法表现相当，验证了全息特性的潜力。

Conclusion: 语言模型确实存在全息特性，HOLO插件能有效利用这一特性提升推理效率，为理解语言模型生成机制提供了新视角。

Abstract: The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to delve into the generation characteristics exhibited by LLMs. Through our investigation, we have discovered that language models tend to capture target-side keywords at the beginning of the generation process. We name this phenomenon the Holographic Characteristic of language models. For the purpose of exploring this characteristic and further improving the inference efficiency of language models, we propose a plugin called HOLO, which leverages the Holographic Characteristic to extract target-side keywords from language models within a limited number of generation steps and complements the sentence with a parallel lexically constrained text generation method. To verify the effectiveness of HOLO, we conduct massive experiments on language models of varying architectures and scales in the short-text generation scenario. The results demonstrate that HOLO achieves comparable performance to the baselines in terms of both automatic and human-like evaluation metrics and highlight the potential of the Holographic Characteristic.

</details>


### [20] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 该论文发现LLM评估中的自偏好偏差测量存在方法学混杂因素，提出评估者质量基线来分离信号与噪声，可减少89.6%的测量误差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为评估者时倾向于偏好自己的输出，这影响了自动化后训练和评估流程的完整性。但难以区分哪些评估偏差源于自恋效应，哪些源于一般实验混杂因素，导致自偏好偏差测量失真。

Method: 引入评估者质量基线，比较评估者错误投票给自己输出的概率与投票给其他模型错误输出的概率。通过37,448个查询评估，分离"简单"与"困难"问题的评估投票熵。

Result: 发现核心方法学混杂因素可将测量误差减少89.6%。应用基线后，仅51%的初始发现保持统计显著性。能够有效分离自偏好信号与困难问题上的噪声输出。

Conclusion: 提出的校正基线通过消除噪声数据，为未来自偏好研究提供了更可靠的测量方法。这项工作有助于系统性地分类和隔离评估者偏差效应。

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [21] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: SpanNorm是一种新的Transformer归一化方法，通过跨块残差连接和PostNorm风格计算，解决了PreNorm训练稳定但性能下降与PostNorm性能强但不稳定的两难问题。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中归一化层的位置选择存在根本性权衡：PreNorm架构确保训练稳定性但可能导致深度模型性能下降，而PostNorm架构提供强大性能但存在严重的训练不稳定性。需要一种方法能同时获得两者的优势。

Method: 提出SpanNorm技术，建立跨越整个Transformer块的干净残差连接以稳定信号传播，同时采用PostNorm风格的计算来归一化聚合输出以增强模型性能。结合有原则的缩放策略，保持网络中信号方差有界。

Result: 理论分析表明SpanNorm能防止PostNorm模型的梯度问题，并缓解PreNorm的表示崩溃。在密集和混合专家(MoE)场景中，SpanNorm始终优于标准归一化方案。

Conclusion: SpanNorm为更强大和稳定的Transformer架构铺平了道路，成功解决了PreNorm和PostNorm之间的根本性权衡问题。

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [22] [Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry](https://arxiv.org/abs/2601.22588)
*Zhuochun Li,Yong Zhang,Ming Li,Yuelyu Ji,Yiming Zeng,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao,Daqing He*

Main category: cs.CL

TL;DR: 提出Representation-as-a-Judge新范式，用小模型内部表征而非生成输出来评估，比LLM-as-a-Judge更高效可靠


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-Judge评估范式成本高、不透明、对提示敏感，需要探索更高效的评估方法

Method: 提出语义容量不对称假设，认为评估比生成需要更少语义容量；开发INSPECTOR框架，通过探测小模型内部表征预测评估分数

Result: 在GSM8K、MATH、GPQA等推理基准上，INSPECTOR显著优于基于提示的小模型，接近完整LLM评估器，同时更高效可靠

Conclusion: 评估任务不需要依赖大规模生成模型，可以利用小模型的潜在特征，Representation-as-a-Judge是解码无关的评估策略

Abstract: Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.

</details>


### [23] [Language Model Circuits Are Sparse in the Neuron Basis](https://arxiv.org/abs/2601.22594)
*Aryaman Arora,Zhengxuan Wu,Jacob Steinhardt,Sarah Schwettmann*

Main category: cs.CL

TL;DR: 研究发现MLP神经元与稀疏自编码器一样稀疏，可用于电路追踪，无需额外训练成本即可实现语言模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 神经网络的高层概念不一定与单个神经元对齐，语言模型可解释性研究通常使用稀疏自编码器来分解神经元基础。然而，并非所有基于神经元的表示都不可解释，本文旨在探索MLP神经元作为特征基础的潜力。

Method: 首次实证证明MLP神经元与稀疏自编码器一样稀疏，基于此开发了MLP神经元基础的端到端电路追踪流程，使用基于梯度的归因方法定位各种任务中的因果电路。

Result: 在标准主谓一致基准测试中，约100个MLP神经元组成的电路足以控制模型行为；在多跳城市→州→首都任务中，发现小神经元组编码特定潜在推理步骤，可通过引导改变模型输出。

Conclusion: 这项工作在不增加训练成本的情况下推进了语言模型的自动化可解释性，证明MLP神经元可以作为稀疏特征基础用于电路追踪。

Abstract: The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpretable units of model computation, for tasks such as \textit{circuit tracing}. However, not all neuron-based representations are uninterpretable. For the first time, we empirically show that \textbf{MLP neurons are as sparse a feature basis as SAEs}. We use this finding to develop an end-to-end pipeline for circuit tracing on the MLP neuron basis, which locates causal circuitry on a variety of tasks using gradient-based attribution. On a standard subject-verb agreement benchmark (Marks et al., 2025), a circuit of $\approx 10^2$ MLP neurons is enough to control model behaviour. On the multi-hop city $\to$ state $\to$ capital task from Lindsey et al., 2025, we find a circuit in which small sets of neurons encode specific latent reasoning steps (e.g.~`map city to its state'), and can be steered to change the model's output. This work thus advances automated interpretability of language models without additional training costs.

</details>


### [24] [Layer-wise Swapping for Generalizable Multilingual Safety](https://arxiv.org/abs/2601.22620)
*Hyunseo Shin,Wonseok Hwang*

Main category: cs.CL

TL;DR: 提出一种安全感知的层交换方法，将英语安全专家的安全对齐能力转移到低资源语言专家模型，无需额外训练，提升多语言安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在低资源语言上的安全风险仍然是一个关键挑战。现有的安全数据集主要面向英语，限制了多语言安全对齐的进展。低资源专家模型在各自指令数据集上微调后，往往比高资源模型表现出更高的不安全率。

Method: 提出安全感知的层交换方法，将安全对齐从英语安全专家转移到低资源语言专家。方法自适应地选择或混合模块基于其专业化程度，以增强转移能力。该方法在保持一般语言理解任务性能的同时，增强目标语言的安全性。

Result: 实验结果显示，该方法在MMMLU、BELEBELE和MGSM等通用基准测试中达到与语言专家相当的性能，同时在MultiJail安全基准测试中产生更对齐、危害更小的响应。

Conclusion: 提出的安全感知层交换方法能够有效将英语安全专家的安全对齐能力转移到低资源语言专家模型，在保持通用性能的同时显著提升多语言安全性，为解决低资源语言安全挑战提供了有效方案。

Abstract: Despite the rapid advancements of Large Language Models (LLMs), safety risks remain a critical challenge for low-resource languages. Existing safety datasets are predominantly English centric, limiting progress in multilingual safety alignment. As a result, low resource expert models, finetuned on their respective instruction datasets, tend to exhibit higher unsafety rates compared to their high resource counterparts. In this work, we propose a safety aware layer swapping method that transfers safety alignment from an English safety expert to low resource language experts without additional training. To further enhance transfer ability, our method adaptively selects or blends modules based on their degree of specialization. Our approach preserves performance on general language understanding tasks while enhancing safety in the target languages. Experimental results show that the proposed method achieves comparable performance to the language expert on general benchmarks such as MMMLU, BELEBELE, and MGSM, while producing more aligned and less harmful responses on the MultiJail safety benchmark.

</details>


### [25] [Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models](https://arxiv.org/abs/2601.22629)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Yiqiao Huang,Ivor Tsang,Yang You*

Main category: cs.CL

TL;DR: 提出TAPS方法，利用扩散语言模型的时间结构控制生成多样性，早期扰动促进语义分支，后期减少扰动保持流畅性


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型引入了显式时间维度，但如何利用这种结构来控制生成多样性以探索多个有效的语义或推理路径尚未得到充分探索

Method: 提出Time-Annealed Perturbation Sampling (TAPS)，一种无需训练推理策略，在扩散过程早期鼓励语义分支，后期逐步减少扰动以保持流畅性和指令遵循

Result: TAPS与非自回归和半自回归扩散骨干兼容，在创意写作和推理基准上一致提高输出多样性，同时不损害生成质量

Conclusion: 扩散语言模型存在时间分工现象，TAPS方法有效利用这一特性控制生成多样性，为探索多个有效语义路径提供了新思路

Abstract: Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, like diffusion models in image generation, exhibit a temporal division of labor: early denoising steps largely determine the global semantic structure, while later steps focus on local lexical refinement. Building on this insight, we propose Time-Annealed Perturbation Sampling (TAPS), a training-free inference strategy that encourages semantic branching early in the diffusion process while progressively reducing perturbations to preserve fluency and instruction adherence. TAPS is compatible with both non-autoregressive and semi-autoregressive Diffusion backbones, demonstrated on LLaDA and TraDo in our paper, and consistently improves output diversity across creative writing and reasoning benchmarks without compromising generation quality.

</details>


### [26] [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)
*Abhishek Tyagi,Yunuo Cen,Shrey Dhorajiya,Bharadwaj Veeravalli,Xuanyao Fong*

Main category: cs.CL

TL;DR: DART是一种轻量级、无需训练的运行时动态剪枝方法，通过监控注意力分布变化来推断上下文变化，动态更新神经元级掩码，在保持模型能力的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在显著的参数冗余，特别是在前馈网络中。现有剪枝方法存在两个主要限制：1）依赖数据集特定的校准，导致数据依赖性和计算开销；2）主要是静态方法，无法适应自回归生成过程中上下文变化时知识神经元的动态变化。

Method: DART（动态注意力引导的运行时追踪）是一种轻量级、无需训练的方法，通过监控注意力分数分布的变化来推断上下文变化，动态更新神经元级掩码以保留重要参数，实现基于上下文的实时剪枝。

Result: 在10个基准测试中，DART在LLAMA-3.1-8B模型70%FFN稀疏度下，比先前动态基线准确率提升高达14.5%。在摘要任务中，相比静态掩码剪枝，ROUGE-L分数提升高达3倍，性能接近原始密集模型。内存占用小于10MB（原模型16GB），FLOPs开销仅0.1%。

Conclusion: DART框架能有效适应不同语义上下文，在通用和领域特定任务中保持模型能力，同时显著减少计算资源需求，为LLM的高效部署提供了有效的动态剪枝解决方案。

Abstract: Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second, being predominantly static, they fail to account for the evolving subset of knowledge neurons in LLMs during autoregressive generation as the context evolves. To address this, we introduce DART, i.e., Dynamic Attention-Guided Runtime Tracing), a lightweight, training-free method that performs on-the-fly context-based pruning. DART monitors shifts in attention score distributions to infer context changes, dynamically updating neuron-level masks to retain salient parameters. Across ten benchmarks, DART outperforms prior dynamic baseline, achieving accuracy gains of up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity. Furthermore, DART achieves up to 3x better ROUGE-L scores with respect to static-masked pruning on summarization tasks, with its performance comparable to the original dense models. We conclusively demonstrate that the proposed framework effectively adapts to diverse semantic contexts, preserves model capabilities across both general and domain-specific tasks while running at less than 10MBs of memory for LLAMA-3.1-8B(16GBs) with 0.1% FLOPs overhead. The code is available at https://github.com/seeder-research/DART.

</details>


### [27] [NAG: A Unified Native Architecture for Encoder-free Text-Graph Modeling in Language Models](https://arxiv.org/abs/2601.22657)
*Haisong Gong,Zhibo Liu,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.CL

TL;DR: NAG提出了一种将图处理内化到语言模型原生架构中的统一框架，无需外部GNN编码器，通过改造自注意力机制和位置编码来同时处理文本语义和图结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图结构编码（使用GNN）和文本语义处理（使用LM）分离，导致概念上不连贯的交互范式，需要在抽象图标记和具体文本元素之间进行复杂的隐式对齐。

Method: NAG框架将图处理内化到语言模型的原生流形中：1）重新利用自注意力机制来强制执行拓扑依赖关系；2）重新校准位置ID以确保结构等价性。提出了两种实现：NAG-Zero（绝对保留基础模型的语言能力）和NAG-LoRA（增强结构适应）。

Result: 在多种图任务上的实验验证了NAG能够实现稳健的图理解，无需外部编码器的开销，为文本-图建模提供了更简单、更连贯的范式。

Conclusion: NAG挑战了外部编码器的必要性，提供了一个统一的框架，使语言模型能够利用其内在的语言能力同时理解节点/边内容和结构拓扑，为文本-图建模提供了更优的解决方案。

Abstract: Prevailing methods for integrating graphs into Language Models (LMs) typically rely on a segregated architecture: external Graph Neural Networks (GNNs) encode structural topology, while LMs process textual semantics. We argue this approach is suboptimal for text-graphs: it creates a conceptually disjointed interaction paradigm. By segregating structural encoding from semantic processing, these systems must perform a complex implicit alignment between abstract graph tokens and concrete textual elements. Challenging the necessity of external encoders, we propose NAG (Native Architecture for Graphs), a unified framework that internalizes graph processing within the LM's native manifold. Instead of bridging disparate embedding spaces, NAG repurposes the self-attention mechanism to enforce topological dependencies and recalibrates positional IDs to ensure structural equivalence. This allows the model to harness its intrinsic linguistic capability to simultaneously comprehend node and edge content alongside structural topology. We introduce two efficient implementations: NAG-Zero for absolute preservation of the base model's linguistic capabilities, and NAG-LoRA for enhanced structural adaptation. Experiments across diverse graph tasks validate that NAG achieves robust graph comprehension without the overhead of external encoders, offering a simpler, more coherent paradigm for text-graph modeling.

</details>


### [28] [TSLM: Tree-Structured Language Modeling for Divergent Thinking](https://arxiv.org/abs/2601.22688)
*Doyoung Kim,Jaehyeok Doo,Minjoon Seo*

Main category: cs.CL

TL;DR: TSLM通过特殊令牌编码分支结构，让语言模型在单次生成中探索多个搜索路径，避免重复计算共享前缀，提升推理效率和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统语言模型顺序生成推理，无法解耦无关探索路径，导致搜索效率低下。需要一种能同时探索多个路径并选择性扩展的方法

Method: 提出树结构语言建模(TSLM)，使用特殊令牌编码分支结构，在完整搜索树（包括成功和失败尝试）上进行训练，让模型内化系统化探索能力

Result: TSLM实现了鲁棒性能和优越的推理效率，避免了外部搜索方法需要的多次独立前向传播，展示了推理时扩展的新范式

Conclusion: 监督学习完整的树结构轨迹为开发语言模型的系统化探索能力提供了高效替代方案，开启了推理时扩展的新范式

Abstract: Language models generate reasoning sequentially, preventing them from decoupling irrelevant exploration paths during search. We introduce Tree-Structured Language Modeling (TSLM), which uses special tokens to encode branching structure, enabling models to generate and selectively expand multiple search paths within a single generation process. By training on complete search trees including both successful and failed attempts, TSLM learns to internalize systematic exploration without redundant recomputation of shared prefixes. TSLM achieves robust performance and superior inference efficiency by avoiding the multiple independent forward passes required by external search methods. These results suggest a new paradigm of inference-time scaling for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models.

</details>


### [29] [FNF: Functional Network Fingerprint for Large Language Models](https://arxiv.org/abs/2601.22692)
*Yiheng Liu,Junhao Ning,Sichen Xia,Haiyang Sun,Yang Yang,Hanyang Chi,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 提出Functional Network Fingerprint (FNF)方法，通过分析模型功能网络活动的一致性来检测LLM是否源自特定受害者模型，无需训练、样本高效且对常见模型修改鲁棒。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开发成本高昂且具有重要商业价值，防止开源LLM被未经授权盗用、保护开发者知识产权成为关键挑战。现有方法存在局限性，需要更有效的检测工具。

Method: 提出Functional Network Fingerprint (FNF)方法，基于模型功能网络活动的一致性进行检测。该方法无需训练，仅需少量样本，通过分析神经元活动模式的一致性来判断模型是否同源。

Result: 实验表明，共享相同起源的模型（即使规模或架构不同）在多样化输入样本下表现出高度一致的功能网络活动模式。而独立训练的模型则无法保持这种活动对齐。FNF对微调、剪枝、参数置换等常见修改鲁棒，且适用于不同架构和维度的模型比较。

Conclusion: FNF为模型所有者和第三方提供了一种简单、非侵入、有效的LLM知识产权保护工具，能够可靠检测模型是否源自特定受害者模型，且不影响模型实用性。

Abstract: The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Network Fingerprint (FNF), a training-free, sample-efficient method for detecting whether a suspect LLM is derived from a victim model, based on the consistency between their functional network activity. We demonstrate that models that share a common origin, even with differences in scale or architecture, exhibit highly consistent patterns of neuronal activity within their functional networks across diverse input samples. In contrast, models trained independently on distinct data or with different objectives fail to preserve such activity alignment. Unlike conventional approaches, our method requires only a few samples for verification, preserves model utility, and remains robust to common model modifications (such as fine-tuning, pruning, and parameter permutation), as well as to comparisons across diverse architectures and dimensionalities. FNF thus provides model owners and third parties with a simple, non-invasive, and effective tool for protecting LLM intellectual property. The code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [30] [Models Know Models Best: Evaluation via Model-Preferred Formats](https://arxiv.org/abs/2601.22699)
*Joonhak Lee,Sungmok Jung,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: LLMs在符号式和完形填空式多选题任务上表现差异显著，本文提出动态格式对齐策略，利用轻量级分类器根据模型偏好信号为每个问题选择最优格式，显著提升零样本准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在符号式（symbol-based）和完形填空式（cloze-style）多选题任务上表现存在显著差异，这种不一致性限制了模型能力的准确评估，需要一种能够动态选择最优评估格式的方法。

Method: 提出动态格式对齐策略，训练轻量级分类器基于模型生成的偏好信号（latent model-preference signals），为每个具体问题实例选择最优的评估格式（符号式或完形填空式），而不是依赖人工设计的启发式方法。

Result: 该方法在推理和知识基准测试中实现了显著且一致的零样本准确率提升，更好地揭示了模型的潜在能力，相比人工启发式方法不会降低性能。

Conclusion: LLMs在不同格式多选题上的性能差异具有系统性，动态格式对齐策略能有效解决格式不一致问题，通过模型自身信号选择最优格式，显著提升评估准确性和模型能力展示。

Abstract: Performance of Large Language Models (LLMs) on multiple-choice tasks differs markedly between symbol-based and cloze-style evaluation formats. The observed discrepancies are systematically attributable to task characteristics: natural language continuation benefits from likelihood scoring, whereas explicit comparison is better suited to symbol-based selection. These trends are consistent across various decoder-based LLMs, indicating model-agnostic effects. To address these inconsistencies, a dynamic format-alignment strategy is introduced that employs a lightweight classifier trained on latent model-preference signals. In contrast to human-designed heuristics, which often degrade performance, this approach uses model-generated signals to determine the optimal format for each problem instance. The proposed method achieves substantial and consistent improvements in zero-shot accuracy across reasoning and knowledge benchmarks, better revealing the models' latent capabilities.

</details>


### [31] [MM-THEBench: Do Reasoning MLLMs Think Reasonably?](https://arxiv.org/abs/2601.22735)
*Zhidian Huang,Zijun Yao,Ji Qi,Shangqing Tu,Junxian Ma,Jinxin Liu,Weichuan Liu,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: MM-THEBench：首个评估推理型多模态大语言模型中间思维链幻觉的基准，包含细粒度认知分类、多样化数据和自动化评估框架


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注推理型MLLMs出现之前的模型，忽略了内部思维过程，无法衡量思维过程中产生的幻觉。虽然自反思推理增强了鲁棒性，但也引入了额外幻觉，细微的感知错误仍会导致错误或巧合正确的答案。

Method: 提出MM-THEBench基准，包含：1）基于认知维度的细粒度分类法；2）带有验证推理标注的多样化数据；3）多级自动化评估框架

Result: 对主流推理型MLLMs进行广泛实验，揭示了思维如何影响各种多模态任务中的幻觉和推理能力

Conclusion: MM-THEBench填补了评估推理型MLLMs中间思维链幻觉的空白，为理解思维过程对多模态感知和推理的影响提供了重要工具

Abstract: Recent advances in multimodal large language models (MLLMs) mark a shift from non-thinking models to post-trained reasoning models capable of solving complex problems through thinking. However, whether such thinking mitigates hallucinations in multimodal perception and reasoning remains unclear. Self-reflective reasoning enhances robustness but introduces additional hallucinations, and subtle perceptual errors still result in incorrect or coincidentally correct answers. Existing benchmarks primarily focus on models before the emergence of reasoning MLLMs, neglecting the internal thinking process and failing to measure the hallucinations that occur during thinking. To address these challenges, we introduce MM-THEBench, a comprehensive benchmark for assessing hallucinations of intermediate CoTs in reasoning MLLMs. MM-THEBench features a fine-grained taxonomy grounded in cognitive dimensions, diverse data with verified reasoning annotations, and a multi-level automated evaluation framework. Extensive experiments on mainstream reasoning MLLMs reveal insights into how thinking affects hallucination and reasoning capability in various multimodal tasks.

</details>


### [32] [AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction](https://arxiv.org/abs/2601.22742)
*Yifei Li,Richong Zhang,Wanyu Tu,Zhijie Nie,Haokun Luo,Chuantao Yin,Pengchong Li*

Main category: cs.CL

TL;DR: 论文提出了一个新的法律AI任务"上诉审查"，旨在评估模型在法律实践中的诊断推理和可靠性，并构建了包含8700个标注判决和34617个补充语料的AR-BENCH数据集，通过评估14个大语言模型揭示了现有模型在识别法律适用错误方面的关键局限性。


<details>
  <summary>Details</summary>
Motivation: 法律判决可能因案件复杂性及法律概念抽象性而存在错误，现有上诉审查机制面临案件数量激增带来的效率压力。当前法律AI研究主要关注判决预测和法律文件生成，但判决审查任务在目标和范式上根本不同：它侧重于判决发布后的错误检测、分类和纠正，属于异常检测而非预测或生成。

Method: 1. 引入新的APPELLATE REVIEW任务，评估模型在法律实践中的诊断推理和可靠性；2. 构建AR-BENCH数据集，包含8700个精细标注的判决和34617个补充语料；3. 评估14个大语言模型在该任务上的表现。

Result: 评估揭示了现有大语言模型在识别法律适用错误方面的关键局限性，为未来改进提供了实证证据。

Conclusion: 论文填补了法律AI研究中判决审查任务的空白，提出了APPELLATE REVIEW任务和AR-BENCH数据集，通过实证评估揭示了当前模型的局限性，为未来法律AI在错误检测和纠正方面的研究提供了基础。

Abstract: Legal judgments may contain errors due to the complexity of case circumstances and the abstract nature of legal concepts, while existing appellate review mechanisms face efficiency pressures from a surge in case volumes. Although current legal AI research focuses on tasks like judgment prediction and legal document generation, the task of judgment review differs fundamentally in its objectives and paradigm: it centers on detecting, classifying, and correcting errors after a judgment is issued, constituting anomaly detection rather than prediction or generation. To address this research gap, we introduce a novel task APPELLATE REVIEW, aiming to assess models' diagnostic reasoning and reliability in legal practice. We also construct a novel dataset benchmark AR-BENCH, which comprises 8,700 finely annotated decisions and 34,617 supplementary corpora. By evaluating 14 large language models, we reveal critical limitations in existing models' ability to identify legal application errors, providing empirical evidence for future improvements.

</details>


### [33] [RASST: Fast Cross-modal Retrieval-Augmented Simultaneous Speech Translation](https://arxiv.org/abs/2601.22777)
*Jiaxuan Luo,Siqi Ouyang,Lei Li*

Main category: cs.CL

TL;DR: RASST提出检索增强的同步语音翻译系统，通过轻量级语音-文本检索器和滑动窗口检索，为语音大语言模型提供术语提示，显著提升专业术语翻译准确率。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型在同步语音翻译中，对于罕见和领域特定术语的翻译仍然存在困难。虽然检索增强在机器翻译中已被证明对术语翻译有效，但将其应用于同步语音翻译面临挑战：需要快速准确的跨模态（语音到文本）检索，且模型需要在增量生成过程中决定是否以及何时应用检索到的术语。

Method: RASST将跨模态检索紧密集成到同步语音翻译流程中：1）训练轻量级语音-文本检索器；2）执行高效的滑动窗口检索，为语音大语言模型提供分块术语提示；3）合成训练数据，教导语音大语言模型精确利用检索到的术语。

Result: 在ACL 60/60开发集的三个语言方向上，RASST将术语翻译准确率提升高达16%，整体翻译质量提升高达3个BLEU分数。消融实验确认了每个组件的贡献。

Conclusion: RASST成功将检索增强技术应用于同步语音翻译，通过跨模态检索和针对性训练，显著改善了专业术语的翻译质量，为同步语音翻译系统提供了有效的术语处理方案。

Abstract: Simultaneous speech translation (SST) produces target text incrementally from partial speech input. Recent speech large language models (Speech LLMs) have substantially improved SST quality, yet they still struggle to correctly translate rare and domain-specific terminology. While retrieval augmentation has been effective for terminology translation in machine translation, bringing retrieval to SST is non-trivial: it requires fast and accurate cross-modal (speech-to-text) retrieval under partial, continually arriving input, and the model must decide whether and when to apply retrieved terms during incremental generation. We propose Retrieval-Augmented Simultaneous Speech Translation (RASST), which tightly integrates cross-modal retrieval into the SST pipeline. RASST trains a lightweight speech-text retriever and performs efficient sliding-window retrieval, providing chunkwise terminology hints to the Speech LLM. We further synthesize training data that teaches the Speech LLM to leverage retrieved terms precisely. Experiments on three language directions of the ACL 60/60 dev set show that RASST improves terminology translation accuracy by up to 16% and increases overall translation quality by up to 3 BLEU points, with ablations confirming the contribution of each component.

</details>


### [34] [Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs](https://arxiv.org/abs/2601.22795)
*Corentin Kervadec,Iuliia Lysova,Marco Baroni,Gemma Boleda*

Main category: cs.CL

TL;DR: 本文提出了一种量化大语言模型计算密度的方法，发现LLM计算通常是密集的而非稀疏的，且计算密度随输入动态变化，与罕见词预测和上下文长度相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明可以大幅剪枝LLM参数而性能影响很小，这意味着计算并非均匀分布在所有参数上。为了系统理解LLM中的计算分布，需要一种量化计算密度的方法。

Method: 提出了一种基于机制可解释性的计算密度估计器，通过实验分析LLM中计算密度的分布特征。

Result: 发现：1）LLM处理通常涉及密集计算而非稀疏计算；2）计算密度是动态的，模型根据输入在稀疏和密集处理模式间切换；3）相同输入在不同LLM中触发相似的计算密度模式。罕见词预测需要更高密度，增加上下文长度通常降低密度。

Conclusion: 计算密度估计器有助于更好理解LLM的工作机制，挑战了LLM的符号解释观点，揭示了计算分布的动态特性。

Abstract: Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design a density estimator drawing on mechanistic interpretability. We experimentally test our estimator and find that: (1) contrary to what has been often assumed, LLM processing generally involves dense computation; (2) computation density is dynamic, in the sense that models shift between sparse and dense processing regimes depending on the input; (3) per-input density is significantly correlated across LLMs, suggesting that the same inputs trigger either low or high density. Investigating the factors influencing density, we observe that predicting rarer tokens requires higher density, and increasing context length often decreases the density. We believe that our computation density estimator will contribute to a better understanding of the processing at work in LLMs, challenging their symbolic interpretation.

</details>


### [35] [When Meanings Meet: Investigating the Emergence and Quality of Shared Concept Spaces during Multilingual Language Model Training](https://arxiv.org/abs/2601.22851)
*Felicia Körner,Max Müller-Eberstein,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: 本文通过因果可解释性方法研究多语言大语言模型训练过程中跨语言概念空间的形成，发现共享概念空间早期出现但语言对齐存在差异，某些翻译质量提升实际是行为改变而非翻译能力改进。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注最终模型，缺乏对多语言LLM训练过程中跨语言概念空间形成机制的深入理解，且缺乏因果方法和深度错误分析，需要探究这些空间如何在训练中涌现。

Method: 使用激活修补的因果可解释性方法，在EuroLLM预训练过程中隔离跨语言概念表示，然后将其注入翻译提示中，研究不同语言下翻译行为的一致性变化。

Result: 发现共享概念空间在训练早期就出现并持续优化，但不同语言的对齐程度存在差异；精细分析显示某些翻译质量提升实际上是行为改变（如多义词义项选择、翻译而非复制跨语言同形词）而非翻译能力改进。

Conclusion: 研究揭示了跨语言对齐的训练动态新见解，并明确了因果可解释性方法在多语言语境中提供有意义洞察的条件，对理解多语言LLM概念空间形成机制有重要贡献。

Abstract: Training Large Language Models (LLMs) with high multilingual coverage is becoming increasingly important -- especially when monolingual resources are scarce. Recent studies have found that LLMs process multilingual inputs in shared concept spaces, thought to support generalization and cross-lingual transfer. However, these prior studies often do not use causal methods, lack deeper error analysis or focus on the final model only, leaving open how these spaces emerge during training. We investigate the development of language-agnostic concept spaces during pretraining of EuroLLM through the causal interpretability method of activation patching. We isolate cross-lingual concept representations, then inject them into a translation prompt to investigate how consistently translations can be altered, independently of the language. We find that shared concept spaces emerge early} and continue to refine, but that alignment with them is language-dependent}. Furthermore, in contrast to prior work, our fine-grained manual analysis reveals that some apparent gains in translation quality reflect shifts in behavior -- like selecting senses for polysemous words or translating instead of copying cross-lingual homographs -- rather than improved translation ability. Our findings offer new insight into the training dynamics of cross-lingual alignment and the conditions under which causal interpretability methods offer meaningful insights in multilingual contexts.

</details>


### [36] [From Labels to Facets: Building a Taxonomically Enriched Turkish Learner Corpus](https://arxiv.org/abs/2601.22875)
*Elif Sayar,Tolgahan Türker,Anna Golynskaia Knezhevich,Bihter Dereli,Ayşe Demirhas,Lionel Nicolas,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 提出基于分面分类法的半自动标注方法，用于丰富学习者语料库的标注维度，实现细粒度错误分析


<details>
  <summary>Details</summary>
Motivation: 现有学习者语料库大多采用平面标签体系，无法分离多个语言学维度，限制了深度标注和细粒度错误分析

Method: 基于新提出的分面分类法，开发半自动标注方法，通过标注扩展框架自动推断语言学特征和元数据信息

Result: 标注扩展工具在土耳其语上实现95.86%的分面级准确率，创建了首个协作标注的分面分类法土耳其学习者语料库

Conclusion: 该方法为现有错误标注学习者语料库的丰富化提供了新途径，支持跨语料库的复杂语言学和教学维度分析

Abstract: In terms of annotation structure, most learner corpora rely on holistic flat label inventories which, even when extensive, do not explicitly separate multiple linguistic dimensions. This makes linguistically deep annotation difficult and complicates fine-grained analyses aimed at understanding why and how learners produce specific errors. To address these limitations, this paper presents a semi-automated annotation methodology for learner corpora, built upon a recently proposed faceted taxonomy, and implemented through a novel annotation extension framework. The taxonomy provides a theoretically grounded, multi-dimensional categorization that captures the linguistic properties underlying each error instance, thereby enabling standardized, fine-grained, and interpretable enrichment beyond flat annotations. The annotation extension tool, implemented based on the proposed extension framework for Turkish, automatically extends existing flat annotations by inferring additional linguistic and metadata information as facets within the taxonomy to provide richer learner-specific context. It was systematically evaluated and yielded promising performance results, achieving a facet-level accuracy of 95.86%. The resulting taxonomically enriched corpus offers enhanced querying capabilities and supports detailed exploratory analyses across learner corpora, enabling researchers to investigate error patterns through complex linguistic and pedagogical dimensions. This work introduces the first collaboratively annotated and taxonomically enriched Turkish Learner Corpus, a manual annotation guideline with a refined tagset, and an annotation extender. As the first corpus designed in accordance with the recently introduced taxonomy, we expect our study to pave the way for subsequent enrichment efforts of existing error-annotated learner corpora.

</details>


### [37] [Leveraging LLMs For Turkish Skill Extraction](https://arxiv.org/abs/2601.22885)
*Ezgi Arslan İltüzer,Özgür Anıl Özlü,Vahid Farajijobehdar,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文针对土耳其语技能抽取任务，创建了首个土耳其语技能抽取数据集，并评估了LLM在低资源语言技能抽取中的表现，发现Claude Sonnet 3.7结合动态少样本提示效果最佳。


<details>
  <summary>Details</summary>
Motivation: 土耳其语作为形态复杂的语言，缺乏技能分类体系和专用数据集，导致土耳其语技能抽取研究不足。尽管土耳其在全球劳动力市场扮演重要角色，但相关研究仍处于空白状态。

Method: 1) 创建首个土耳其语技能抽取数据集（4,819个标注技能跨度，来自327个不同职业领域的招聘广告）；2) 评估不同LLM和提示策略（动态vs静态少样本、不同上下文信息、因果推理鼓励）；3) 采用端到端流程：技能识别→嵌入检索→LLM重排序。

Result: LLM在端到端技能抽取中优于监督序列标注方法，能更有效地将提取的技能与ESCO分类体系对齐。最佳配置（Claude Sonnet 3.7 + 动态少样本提示 + 嵌入检索 + LLM重排序）达到0.56的端到端性能，使土耳其语技能抽取达到与其他语言相当的水平。

Conclusion: LLM能显著提升低资源语言技能抽取性能，该研究为土耳其语技能抽取奠定了基础，有望加速其他资源匮乏语言的类似研究。

Abstract: Skill extraction is a critical component of modern recruitment systems, enabling efficient job matching, personalized recommendations, and labor market analysis. Despite Türkiye's significant role in the global workforce, Turkish, a morphologically complex language, lacks both a skill taxonomy and a dedicated skill extraction dataset, resulting in underexplored research in skill extraction for Turkish. This article seeks the answers to three research questions: 1) How can skill extraction be effectively performed for this language, in light of its low resource nature? 2)~What is the most promising model? 3) What is the impact of different Large Language Models (LLMs) and prompting strategies on skill extraction (i.e., dynamic vs. static few-shot samples, varying context information, and encouraging causal reasoning)? The article introduces the first Turkish skill extraction dataset and performance evaluations of automated skill extraction using LLMs. The manually annotated dataset contains 4,819 labeled skill spans from 327 job postings across different occupation areas. The use of LLM outperforms supervised sequence labeling when used in an end-to-end pipeline, aligning extracted spans with standardized skills in the ESCO taxonomy more effectively. The best-performing configuration, utilizing Claude Sonnet 3.7 with dynamic few-shot prompting for skill identification, embedding-based retrieval, and LLM-based reranking for skill linking, achieves an end-to-end performance of 0.56, positioning Turkish alongside similar studies in other languages, which are few in the literature. Our findings suggest that LLMs can improve skill extraction performance in low-resource settings, and we hope that our work will accelerate similar research on skill extraction for underrepresented languages.

</details>


### [38] [Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial](https://arxiv.org/abs/2601.22888)
*Jio Oh,Paul Vicinanza,Thomas Butler,Steven Euijong Whang,Dezhi Hong,Amani Namboori*

Main category: cs.CL

TL;DR: MDial是首个大规模多方言对话数据生成框架，涵盖9种英语方言的词汇、拼写和语法特征，并构建了包含5万+对话的MDialBench基准，评估显示前沿LLM在方言识别上准确率低于70%，存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 超过80%的16亿英语使用者不使用标准美式英语，在与LLM交互时面临更高的失败率和刻板回应，但多方言性能研究不足。现有研究主要关注口音和词汇，缺乏对书面方言三大支柱（词汇、拼写、语法）的系统性处理。

Method: 与母语语言学家合作，设计基于规则的可扩展LLM转换方法，精确生成多方言对话数据。构建方言平行的MDialBench基准，包含50k+对话和97k+问答对，用于评估17个LLM在方言识别和响应生成任务上的表现。

Result: 独立评估确认数据质量，98%的成对比较中标注者更偏好MDial输出。评估显示前沿LLM方言识别准确率低于70%，加拿大英语甚至低于50%，非标准美式英语方言常被误分类为美式或英式英语。

Conclusion: 挑战了模型应复制用户语法特征的假设，研究表明方言中高达90%的语法特征不应被模型复制。方言识别错误可能导致下游任务级联失败，凸显了改进LLM多方言能力的重要性。

Abstract: More than 80% of the 1.6 billion English speakers do not use Standard American English (SAE) and experience higher failure rates and stereotyped responses when interacting with LLMs as a result. Yet multi-dialectal performance remains underexplored. We introduce $\textbf{MDial}$, the first large-scale framework for generating multi-dialectal conversational data encompassing the three pillars of written dialect -- lexical (vocabulary), orthographic (spelling), and morphosyntactic (grammar) features -- for nine English dialects. Partnering with native linguists, we design an annotated and scalable rule-based LLM transformation to ensure precision. Our approach challenges the assumption that models should mirror users' morphosyntactic features, showing that up to 90% of the grammatical features of a dialect should not be reproduced by models. Independent evaluations confirm data quality, with annotators preferring MDial outputs over prior methods in 98% of pairwise comparisons for dialect naturalness. Using this pipeline, we construct the dialect-parallel $\textbf{MDialBench}$mark with 50k+ dialogs, resulting in 97k+ QA pairs, and evaluate 17 LLMs on dialect identification and response generation tasks. Even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. As dialect identification underpins natural language understanding, these errors risk cascading failures into downstream tasks.

</details>


### [39] [DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion](https://arxiv.org/abs/2601.22889)
*Yuxuan Lou,Ziming Wu,Yaochen Wang,Yong Liu,Yingxuan Ren,Fuming Lai,Shaobing Lian,Jie Tang,Yang You*

Main category: cs.CL

TL;DR: 提出STSA（静默思考、语音回答）范式，让语音LLM在生成语音回答时同时产生内部文本推理轨迹，通过扩散模型统一处理文本和语音，提升语音QA准确率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型直接生成语音回答而没有显式推理过程，导致错误无法修正。需要一种能够同时进行内部推理和语音生成的框架。

Method: 提出STSA范式和STSA模型，首个基于扩散的语音-文本语言模型，在单一掩码扩散框架下统一离散文本和标记化语音，通过迭代去噪联合生成推理轨迹和语音标记。

Result: 在语音QA任务上达到SOTA，比最佳基线提升9个百分点；在生成模型中取得最佳TTS质量（6.2% WER）；保持语言理解能力（66.2% MMLU）。

Conclusion: STSA范式通过让语音LLM生成内部文本推理轨迹，结合扩散架构，显著提升了语音QA准确率和语音生成质量，同时保持了语言理解能力。

Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.

</details>


### [40] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 论文指出当前流行的LLM可解释性方法存在根本缺陷，注意力机制和嵌入特征映射方法都无法真正揭示LLM的语言抽象机制，这对依赖这些方法进行调试和解释的普适计算应用构成挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在普适计算中广泛应用且表现出色，但其卓越性能的内在机制仍不清楚。现有可解释性方法本身也未被充分理解，作者希望探究LLM中语言抽象是如何形成的，并检测不同模块（注意力头和输入嵌入）中的抽象机制。

Method: 采用文献中成熟的两种方法：(1) 基于探针的token级关系结构检测，(2) 使用嵌入作为人类可解释属性载体的特征映射。这两种方法分别测试注意力机制和嵌入表示。

Result: 两种方法都失败了：注意力解释在测试"深层表示仍对应token"的核心假设时崩溃；嵌入属性推断方法的高预测分数由方法伪影和数据集结构驱动，而非有意义的语义知识。这些方法被广泛用作LLM理解能力的证据，但研究显示这种结论是站不住脚的。

Conclusion: 当前广泛使用的LLM可解释性方法存在严重局限性，无法可靠揭示模型的实际理解机制。这在普适和分布式计算环境中尤为重要，因为LLM作为系统组件部署时，调试、压缩和模型解释都依赖这些可解释性方法，而现有方法的不可靠性带来了实际风险。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [41] [Benchmarking Machine Translation on Chinese Social Media Texts](https://arxiv.org/abs/2601.22931)
*Kaiyan Zhao,Zheyong Xie,Zhongtao Miao,Xinze Lyu,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: CSM-MTBench：针对中文社交媒体非正式文本的机器翻译评测基准，解决俚语、新词和风格化表达的翻译挑战


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体上快速演变的俚语、新词和高度风格化表达对机器翻译评测构成重大挑战，主要障碍包括：1）高质量平行数据稀缺，需要熟悉平台特定俚语的双语标注者；2）传统评估指标无法捕捉风格保真度和非标准表达

Method: 提出CSM-MTBench基准，覆盖5个中外语言方向，包含两个专家策划的子集：1）Fun Posts（趣味帖子）- 上下文丰富、俚语和新词密集的内容；2）Social Snippets（社交片段）- 强调简洁、情感和风格驱动的表达。为每个子集设计定制评估方法：测量Fun Posts中俚语和新词的翻译成功率，通过嵌入指标和LLM作为评判的混合方法评估Social Snippets中的语气和风格保持

Result: 在超过20个模型上的实验显示，当前机器翻译系统在处理语义保真度和非正式、社交媒体特定风格线索方面存在显著差异

Conclusion: CSM-MTBench为推进能够掌握真实世界中文社交媒体文本的机器翻译系统提供了一个严格的测试平台

Abstract: The prevalence of rapidly evolving slang, neologisms, and highly stylized expressions in informal user-generated text, particularly on Chinese social media, poses significant challenges for Machine Translation (MT) benchmarking. Specifically, we identify two primary obstacles: (1) data scarcity, as high-quality parallel data requires bilingual annotators familiar with platform-specific slang, and stylistic cues in both languages; and (2) metric limitations, where traditional evaluators like COMET often fail to capture stylistic fidelity and nonstandard expressions. To bridge these gaps, we introduce CSM-MTBench, a benchmark covering five Chinese-foreign language directions and consisting of two expert-curated subsets: Fun Posts, featuring context-rich, slang- and neologism-heavy content, and Social Snippets, emphasizing concise, emotion- and style- driven expressions. Furthermore, we propose tailored evaluation approaches for each subset: measuring the translation success rate of slang and neologisms in Fun Posts, while assessing tone and style preservation in Social Snippets via a hybrid of embedding-based metrics and LLM-as-a-judge. Experiments on over 20 models reveal substantial variation in how current MT systems handle semantic fidelity and informal, social-media-specific stylistic cues. CSM-MTBench thus serves as a rigorous testbed for advancing MT systems capable of mastering real-world Chinese social media texts.

</details>


### [42] [Relaxing Positional Alignment in Masked Diffusion Language Models](https://arxiv.org/abs/2601.22947)
*Mengyu Ye,Ryosuke Takahashi,Keito Kudo,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文提出通过引入<slack>标记和连接时序分类目标，在微调阶段采用对齐灵活的监督策略，以缓解掩码扩散语言模型中严格位置预测导致的解码敏感性问题，从而提升开放文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型在开放文本生成方面与自回归模型存在显著差距。研究发现，严格的位置预测使得MDLM解码对标记错位高度敏感，一个位置偏移就可能严重破坏语义。这表明训练中的严格位置监督与MDLM解码的不可逆去噪动态不匹配。

Method: 在微调阶段采用对齐灵活的监督策略，通过连接时序分类目标引入特殊标记<slack>，放松严格的位置监督，增强模型对位置偏移的鲁棒性。

Result: 在五个开放文本生成基准测试中，该方法一致优于原始模型，并提高了对位置偏移的鲁棒性，表明放松严格位置监督是提升MDLM生成质量的重要因素。

Conclusion: 通过引入对齐灵活的监督策略，可以有效缓解MDLM解码中的位置敏感性问题，显著提升开放文本生成性能，为改进掩码扩散语言模型提供了重要方向。

Abstract: Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.

</details>


### [43] [Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection](https://arxiv.org/abs/2601.22949)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.CL

TL;DR: FraudCoT：通过自主的图感知思维链推理和可扩展的LLM-GNN协同训练，提升基于文本属性图的欺诈检测性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的GNN方法受到预定义提示和解耦训练流程的限制，导致推理自主性不足和语义-结构对齐弱化，需要更统一、高效的框架来改进文本属性图上的欺诈检测

Method: 提出欺诈感知的选择性思维链蒸馏机制，生成多样化推理路径；将蒸馏后的思维链集成到节点文本中，为GNN提供丰富的多跳语义和结构线索；开发高效的非对称协同训练策略，实现端到端优化

Result: 在公共和工业基准测试中，FraudCoT相比最先进方法实现了高达8.8%的AUPRC提升，训练吞吐量提高了1066倍，显著提升了检测性能和效率

Conclusion: FraudCoT通过自主的图感知思维链推理和高效的LLM-GNN协同训练，在文本属性图欺诈检测中实现了性能与效率的双重突破，为图学习与语言模型的融合提供了新思路

Abstract: Graph-based fraud detection on text-attributed graphs (TAGs) requires jointly modeling rich textual semantics and relational dependencies. However, existing LLM-enhanced GNN approaches are constrained by predefined prompting and decoupled training pipelines, limiting reasoning autonomy and weakening semantic-structural alignment. We propose FraudCoT, a unified framework that advances TAG-based fraud detection through autonomous, graph-aware chain-of-thought (CoT) reasoning and scalable LLM-GNN co-training. To address the limitations of predefined prompts, we introduce a fraud-aware selective CoT distillation mechanism that generates diverse reasoning paths and enhances semantic-structural understanding. These distilled CoTs are integrated into node texts, providing GNNs with enriched, multi-hop semantic and structural cues for fraud detection. Furthermore, we develop an efficient asymmetric co-training strategy that enables end-to-end optimization while significantly reducing the computational cost of naive joint training. Extensive experiments on public and industrial benchmarks demonstrate that FraudCoT achieves up to 8.8% AUPRC improvement over state-of-the-art methods and delivers up to 1,066x speedup in training throughput, substantially advancing both detection performance and efficiency.

</details>


### [44] [Residual Context Diffusion Language Models](https://arxiv.org/abs/2601.22954)
*Yuezhou Hu,Harman Singh,Monishwaran Maheswaran,Haocheng Xi,Coleman Hooper,Jintao Zhang,Aditya Tomar,Michael W. Mahoney,Sewon Min,Mehrdad Farajtabar,Kurt Keutzer,Amir Gholami,Chenfeng Xu*

Main category: cs.CL

TL;DR: RCD（残差上下文扩散）通过回收丢弃的token计算来改进扩散大语言模型，减少去噪步骤并提升准确性


<details>
  <summary>Details</summary>
Motivation: 现有的块状扩散大语言模型使用"重掩码"机制，只解码最自信的token而丢弃其他token，浪费了计算资源。这些被丢弃的token实际上包含对后续解码有用的上下文信息。

Method: 提出RCD（残差上下文扩散）模块，将被丢弃的token表示转换为上下文残差，并注入到下一个去噪步骤中。采用解耦的两阶段训练流程来避免内存瓶颈。

Result: RCD将前沿dLLM的准确性提升了5-10个百分点，计算开销极小。在最具挑战性的AIME任务上，RCD几乎将基线准确性翻倍，并在相同准确性水平下减少4-5倍去噪步骤。

Conclusion: RCD通过有效回收被丢弃token的计算，显著提升了扩散大语言模型的效率和性能，只需约10亿token就能将标准dLLM转换为RCD范式。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.

</details>


### [45] [A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is Essential for Transformer Training](https://arxiv.org/abs/2601.22966)
*Zihan Qiu,Zeyu Huang,Kaiyue Wen,Peng Jin,Bo Zheng,Yuxin Zhou,Haofeng Huang,Zekun Wang,Xiao Li,Huaqing Zhang,Yang Xu,Haoran Lian,Siqi Zhang,Rui Men,Jianwei Zhang,Ivan Titov,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 论文研究大语言模型中涌现的异常值（注意力汇和残差汇）的功能作用，发现它们通过与归一化操作共同作用实现"异常值驱动重缩放"，这有助于训练稳定性而非直接贡献输出。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中出现的两种异常值现象：注意力汇（少数token持续获得大注意力logits）和残差汇（少数固定维度在大多数token上保持大激活值），探索它们的功能角色和作用机制。

Method: 提出"异常值驱动重缩放"假设，通过在不同模型架构和训练token数量上进行验证，分析移除归一化、直接裁剪异常值等操作的影响，并探索通过可学习参数吸收异常值或显式门控重缩放来缓解异常值的方法。

Result: 验证了异常值与归一化共同作用的重要性：移除归一化会消除异常值但损害训练稳定性；直接裁剪异常值也会导致性能下降。异常值主要作为重缩放因子而非直接贡献者，其最终贡献远小于非异常值。通过吸收异常值到可学习参数或门控重缩放，能提升训练性能（平均增益2分）并增强量化鲁棒性（W4A4量化下仅退化1.2分）。

Conclusion: 异常值（注意力汇和残差汇）通过与归一化操作共同实现重缩放功能，有助于大语言模型的训练稳定性。它们主要作为重缩放因子而非直接贡献者，可以通过参数吸收或门控重缩放有效缓解，从而提升模型性能和量化鲁棒性。

Abstract: We investigate the functional role of emergent outliers in large language models, specifically attention sinks (a few tokens that consistently receive large attention logits) and residual sinks (a few fixed dimensions with persistently large activations across most tokens). We hypothesize that these outliers, in conjunction with the corresponding normalizations (\textit{e.g.}, softmax attention and RMSNorm), effectively rescale other non-outlier components. We term this phenomenon \textit{outlier-driven rescaling} and validate this hypothesis across different model architectures and training token counts. This view unifies the origin and mitigation of both sink types. Our main conclusions and observations include: (1) Outliers function jointly with normalization: removing normalization eliminates the corresponding outliers but degrades training stability and performance; directly clipping outliers while retaining normalization leads to degradation, indicating that outlier-driven rescaling contributes to training stability. (2) Outliers serve more as rescale factors rather than contributors, as the final contributions of attention and residual sinks are significantly smaller than those of non-outliers. (3) Outliers can be absorbed into learnable parameters or mitigated via explicit gated rescaling, leading to improved training performance (average gain of 2 points) and enhanced quantization robustness (1.2 points degradation under W4A4 quantization).

</details>


### [46] [ArabicDialectHub: A Cross-Dialectal Arabic Learning Resource and Platform](https://arxiv.org/abs/2601.22987)
*Salem Lahlou*

Main category: cs.CL

TL;DR: 构建了阿拉伯语方言学习资源ArabicDialectHub，包含6种方言的552个短语，提供交互式网络平台，支持翻译探索、自适应测试和进度跟踪


<details>
  <summary>Details</summary>
Motivation: 为阿拉伯语学习者提供跨方言的学习资源，解决不同阿拉伯语方言之间的学习障碍，促进方言理解和交流

Method: 使用LLM生成短语，由5位母语者验证，按难度分层并按主题组织，开发交互式网络平台提供多种学习功能

Result: 创建了包含摩洛哥达里贾语、黎巴嫩语、叙利亚语、阿联酋语、沙特语和MSA的552个短语数据集，并开发了功能完整的开源学习平台

Conclusion: ArabicDialectHub为阿拉伯语方言学习提供了有价值的开源资源，通过技术手段促进跨方言的语言学习和文化理解

Abstract: We present ArabicDialectHub, a cross-dialectal Arabic learning resource comprising 552 phrases across six varieties (Moroccan Darija, Lebanese, Syrian, Emirati, Saudi, and MSA) and an interactive web platform. Phrases were generated using LLMs and validated by five native speakers, stratified by difficulty, and organized thematically. The open-source platform provides translation exploration, adaptive quizzing with algorithmic distractor generation, cloud-synchronized progress tracking, and cultural context. Both the dataset and complete platform source code are released under MIT license. Platform: https://arabic-dialect-hub.netlify.app.

</details>


### [47] [Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs](https://arxiv.org/abs/2601.23001)
*Afrozah Nadeem,Agrima,Mehwish Nasim,Usman Naseem*

Main category: cs.CL

TL;DR: 该论文提出了跨语言对齐引导(CLAS)框架，用于减少大型语言模型在多语言环境中的政治偏见，在50个国家33种语言上进行了大规模评估，实现了偏见显著减少且保持回答质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型日益影响全球话语，公平性和意识形态中立性对负责任的AI部署至关重要。现有研究主要关注高资源西方语言或狭窄的多语言环境，跨语言一致性和安全的后处理缓解方法研究不足。

Method: 提出了跨语言对齐引导(CLAS)框架，通过将政治提示诱导的潜在意识形态表示对齐到共享的意识形态子空间来增强现有引导方法，确保跨语言一致性，自适应机制防止过度校正并保持连贯性。

Result: 实验表明，在经济和社会两个轴向上都实现了显著的偏见减少，同时回答质量下降最小。该方法在50个国家33种语言上进行了大规模评估。

Conclusion: 该框架为公平感知的多语言LLM治理建立了可扩展且可解释的范式，在意识形态中立性与语言文化多样性之间取得了平衡。

Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.

</details>


### [48] [InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.23006)
*Junyou Su,He Zhu,Xiao Luo,Liyu Zhang,Hong-Yu Zhou,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: InstructDiff：基于差分熵的领域自适应数据选择框架，通过校准基模型和微调模型的熵差异，在数学推理和通用指令遵循任务上分别实现17%和52%的相对性能提升，仅使用10%数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法存在严重的领域特异性问题：针对通用指令遵循优化的方法在推理任务上失效，反之亦然。完整数据集训练成本高昂且收益递减，需要一种统一的数据选择框架来解决跨领域适应性问题。

Method: 提出InstructDiff框架：1）通过预热校准获得基模型和最小指令微调校准模型；2）测量两者之间的熵差异；3）基于双向NLL过滤和熵排序进行数据选择。核心发现是：推理任务偏好熵增加（认知扩展），通用任务偏好熵减少（认知压缩）。

Result: 在数学推理任务上相对完整数据训练提升17%，在通用指令遵循任务上提升52%，仅使用10%的数据即可超越现有基线方法。差分熵作为领域自适应选择标准被验证有效。

Conclusion: 差分熵模式揭示了数据选择的领域自适应原则：推理任务需要认知扩展（熵增），通用任务需要认知压缩（熵减）。InstructDiff为跨领域高效数据选择提供了统一框架，显著降低训练成本并提升性能。

Abstract: Supervised fine-tuning (SFT) is fundamental to adapting large language models, yet training on complete datasets incurs prohibitive costs with diminishing returns. Existing data selection methods suffer from severe domain specificity: techniques optimized for general instruction-following fail on reasoning tasks, and vice versa. We observe that measuring entropy differences between base models and minimally instruction-tuned calibrated models reveals a pattern -- samples with the lowest differential entropy consistently yield optimal performance across domains, yet this principle manifests domain-adaptively: reasoning tasks favor entropy increase (cognitive expansion), while general tasks favor entropy decrease (cognitive compression). We introduce InstructDiff, a unified framework that operationalizes differential entropy as a domain-adaptive selection criterion through warmup calibration, bi-directional NLL filtering, and entropy-based ranking. Extensive experiments show that InstructDiff achieves 17\% relative improvement over full data training on mathematical reasoning and 52\% for general instruction-following, outperforming prior baselines while using only 10\% of the data.

</details>


### [49] [DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2601.23022)
*Lung-Hao Lee,Liang-Chih Yu,Natalia Loukashevich,Ilseyar Alimova,Alexander Panchenko,Tzu-Mi Lin,Zhe-Yu Xu,Jian-Yu Zhou,Guangmin Zheng,Jin Wang,Sharanya Awasthi,Jonas Becker,Jan Philip Wahle,Terry Ruas,Shamsuddeen Hassan Muhammad,Saif M. Mohammed*

Main category: cs.CL

TL;DR: DimABSA：首个多语言维度情感分析资源，引入连续效价-唤醒度评分，结合传统ABSA元素，包含76,958个方面实例，覆盖6种语言4个领域，提出新评估指标cF1。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA研究依赖粗粒度分类标签（如正面、负面），无法捕捉细微情感状态。为了解决这一限制，作者采用维度方法，用连续效价-唤醒度评分表示情感，实现方面和情感层面的细粒度分析。

Method: 1. 创建DimABSA资源：首个多语言维度ABSA资源，标注传统ABSA元素（方面术语、方面类别、观点术语）和新引入的VA评分；2. 提出三个子任务：结合VA评分与不同ABSA元素，搭建传统ABSA到维度ABSA的桥梁；3. 提出新评估指标cF1：将VA预测误差纳入标准F1；4. 使用提示和微调大语言模型进行全面基准测试。

Result: DimABSA包含76,958个方面实例，覆盖42,590个句子，涵盖6种语言和4个领域。基准测试结果表明DimABSA是一个具有挑战性的基准，为推进多语言维度ABSA提供了基础。

Conclusion: DimABSA通过引入连续VA评分解决了传统ABSA的粒度限制，为多语言维度情感分析提供了首个综合资源。提出的cF1指标和三个子任务为未来研究奠定了基础，展示了维度ABSA的挑战性和发展潜力。

Abstract: Aspect-Based Sentiment Analysis (ABSA) focuses on extracting sentiment at a fine-grained aspect level and has been widely applied across real-world domains. However, existing ABSA research relies on coarse-grained categorical labels (e.g., positive, negative), which limits its ability to capture nuanced affective states. To address this limitation, we adopt a dimensional approach that represents sentiment with continuous valence-arousal (VA) scores, enabling fine-grained analysis at both the aspect and sentiment levels. To this end, we introduce DimABSA, the first multilingual, dimensional ABSA resource annotated with both traditional ABSA elements (aspect terms, aspect categories, and opinion terms) and newly introduced VA scores. This resource contains 76,958 aspect instances across 42,590 sentences, spanning six languages and four domains. We further introduce three subtasks that combine VA scores with different ABSA elements, providing a bridge from traditional ABSA to dimensional ABSA. Given that these subtasks involve both categorical and continuous outputs, we propose a new unified metric, continuous F1 (cF1), which incorporates VA prediction error into standard F1. We provide a comprehensive benchmark using both prompted and fine-tuned large language models across all subtasks. Our results show that DimABSA is a challenging benchmark and provides a foundation for advancing multilingual dimensional ABSA.

</details>


### [50] [Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures](https://arxiv.org/abs/2601.23081)
*Yanghao Su,Wenbo Zhou,Tianwei Zhang,Qiu Han,Weiming Zhang,Nenghai Yu,Jie Zhang*

Main category: cs.CL

TL;DR: 研究发现微调LLMs时，模型在特定字符级行为倾向上的训练会导致比错误建议训练更强的错位现象，表明错位源于行为倾向的稳定转变而非能力退化。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要将"涌现错位"归因于错误或不安全内容的泛化，但本文认为这种观点不完整，需要更深入地理解错位现象的本质机制。

Method: 在多个领域和模型家族中进行实验，比较特定字符级行为倾向训练与错误建议训练的效果，并探索训练时触发器和推理时角色对齐提示的条件激活机制。

Result: 特定字符级行为倾向训练比错误建议训练产生更强、更可转移的错位，同时保持一般能力；行为倾向可通过训练和推理时条件激活，揭示错位、后门激活和越狱漏洞的共享结构。

Conclusion: 角色形成是核心且未被充分探索的对齐风险，稳健的对齐必须解决行为倾向问题，而非仅仅关注孤立错误或提示级防御。

Abstract: Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Across multiple domains and model families, we find that fine-tuning models on data exhibiting specific character-level dispositions induces substantially stronger and more transferable misalignment than incorrect-advice fine-tuning, while largely preserving general capabilities. This indicates that emergent misalignment arises from stable shifts in model behavior rather than from capability degradation or corrupted knowledge. We further show that such behavioral dispositions can be conditionally activated by both training-time triggers and inference-time persona-aligned prompts, revealing shared structure across emergent misalignment, backdoor activation, and jailbreak susceptibility. Overall, our results identify character formation as a central and underexplored alignment risk, suggesting that robust alignment must address behavioral dispositions rather than isolated errors or prompt-level defenses.

</details>


### [51] [Safer Policy Compliance with Dynamic Epistemic Fallback](https://arxiv.org/abs/2601.23094)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 提出Dynamic Epistemic Fallback (DEF)方法，通过认知防御机制增强LLM对恶意篡改政策文本的检测和拒绝能力


<details>
  <summary>Details</summary>
Motivation: 受人类认知防御机制（认知警惕性）启发，为LLM在高风险任务（如数据隐私法律合规自动化）中开发安全保障机制，应对恶意篡改政策文本的欺骗攻击

Method: 提出动态认知回退(DEF)协议，通过不同级别的单句文本提示，促使LLM在遇到篡改政策文本时标记不一致性、拒绝合规要求，并回退到参数化知识

Result: 使用HIPAA和GDPR等全球认可法律政策进行实证评估，DEF有效提升了前沿LLM检测和拒绝篡改政策版本的能力，DeepSeek-R1在某个设置中达到100%检测率

Conclusion: 这项工作鼓励进一步开发受认知启发的防御机制，以提高LLM对利用法律文书的伤害和欺骗形式的鲁棒性

Abstract: Humans develop a series of cognitive defenses, known as epistemic vigilance, to combat risks of deception and misinformation from everyday interactions. Developing safeguards for LLMs inspired by this mechanism might be particularly helpful for their application in high-stakes tasks such as automating compliance with data privacy laws. In this paper, we introduce Dynamic Epistemic Fallback (DEF), a dynamic safety protocol for improving an LLM's inference-time defenses against deceptive attacks that make use of maliciously perturbed policy texts. Through various levels of one-sentence textual cues, DEF nudges LLMs to flag inconsistencies, refuse compliance, and fallback to their parametric knowledge upon encountering perturbed policy texts. Using globally recognized legal policies such as HIPAA and GDPR, our empirical evaluations report that DEF effectively improves the capability of frontier LLMs to detect and refuse perturbed versions of policies, with DeepSeek-R1 achieving a 100% detection rate in one setting. This work encourages further efforts to develop cognitively inspired defenses to improve LLM robustness against forms of harm and deception that exploit legal artifacts.

</details>


### [52] [Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics](https://arxiv.org/abs/2601.23129)
*Yilun Hua,Giuseppe Castellucci,Peter Schulam,Heba Elfardy,Kevin Small*

Main category: cs.CL

TL;DR: 提出GroGU，一种基于生成熵的模型特定、无参考的RAG内容效用度量方法，无需标注即可评估检索文档对LLM生成的价值。


<details>
  <summary>Details</summary>
Motivation: 现有RAG效用度量方法要么忽略模型特定能力，要么依赖昂贵的标注，缺乏一个既能考虑LLM能力又无需标注的实用度量标准。

Method: 提出Grounding Generation Utility (GroGU)，基于下游LLM生成置信度的熵来定义内容效用，无需参考标注，能捕捉模型特定的能力差异。

Result: GroGU能有效区分真实文档，捕捉LLM无关度量忽略的细微差别。将其用于训练RAG查询重写器，在MRR上提升达18.2分，答案准确率提升达9.4分。

Conclusion: GroGU提供了一个实用、无标注的RAG内容效用度量方法，能有效指导RAG系统优化，显著提升检索和生成性能。

Abstract: Retrieval Augmented Generation (RAG)'s success depends on the utility the LLM derives from the content used for grounding. Quantifying content utility does not have a definitive specification and existing metrics ignore model-specific capabilities and/or rely on costly annotations. In this paper, we propose Grounding Generation Utility (GroGU), a model-specific and reference-free metric that defines utility as a function of the downstream LLM's generation confidence based on entropy. Despite having no annotation requirements, GroGU is largely faithful in distinguishing ground-truth documents while capturing nuances ignored by LLM-agnostic metrics. We apply GroGU to train a query-rewriter for RAG by identifying high-utility preference data for Direct Preference Optimization. Experiments show improvements by up to 18.2 points in Mean Reciprocal Rank and up to 9.4 points in answer accuracy.

</details>


### [53] [Monotonic Reference-Free Refinement for Autoformalization](https://arxiv.org/abs/2601.23166)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 提出一种无需参考的无监督迭代单调过程，用于全定理自动形式化，通过定理证明器和LLM评估器提供互补反馈，优化多个质量维度。


<details>
  <summary>Details</summary>
Motivation: 现有语句自动形式化方法通常只改进孤立方面（如语法正确性），难以联合优化多个质量维度，而这对全定理自动形式化至关重要。

Method: 引入基于定理证明器和LLM评估器的互补反馈机制，优化形式有效性、逻辑保持、数学一致性和形式质量四个维度的掩码复合目标，使用响应性映射指导不同LLM角色优先改进不同维度，并采用保证认证单调改进的接受策略。

Result: 在miniF2F数据集上达到93.44%的形式有效性和78.22%的总体分数，在ProofNet数据集上达到44.09%的形式有效性和29.79%的总体分数，实现了跨多个维度的同时改进。

Conclusion: 该方法能够在无需真实证明或现有形式化的情况下，通过迭代单调过程实现全定理自动形式化，并在多个质量维度上取得显著改进。

Abstract: While statement autoformalization has advanced rapidly, full-theorem autoformalization remains largely unexplored. Existing iterative refinement methods in statement autoformalization typicall improve isolated aspects of formalization, such as syntactic correctness, but struggle to jointly optimizing multiple quality dimensions, which is critical for full-theorem autoformalization. We introduce a reference-free iterative monotonic process for full-theorem autoformalization that leverages complementary feedback from theorem provers and LLM-based judges, without access to ground-truth proofs or existing formalizations at inference time. Our approach optimizes a masked composite objective over Formal Validity, Logical Preservation, Mathematical Consistency, and Formal Quality, guided by a responsiveness map that indicates how different LLMs acting as different roles preferentially improve each dimension. We further propose an acceptance policy that guarantees certified monotonic improvement, and provide conditions ensuring convergence and termination. Empirical experiments demonstrate the proposed process enables simultaneous improvement across multiple dimensions, achieving 93.44% formal validity and a 78.22% overall score on miniF2F, and 44.09% formal validity and a 29.79% overall score on ProofNet.

</details>


### [54] [FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation](https://arxiv.org/abs/2601.23182)
*Siyang He,Qiqi Wang,Xiaoran Liu,Hongnan Ma,Yiwei Shi,Yuerong Song,Ying Zhu,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出FourierSampler，一种基于频域分析的解码策略，通过频率滑动窗口机制实现"结构到细节"的生成，解决了扩散语言模型的位置偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型的解码策略存在位置偏差，未能充分发挥其任意生成的潜力。需要深入理解模型的频谱特性，以开发更有效的解码方法。

Method: 通过频域分析发现隐藏状态中低频分量编码全局结构信息和长程依赖，高频分量负责局部细节。基于此提出FourierSampler，采用频域滑动窗口机制动态引导模型实现"结构到细节"的生成。

Result: FourierSampler在LLADA和SDAR基准上优于其他推理增强策略，在LLaDA1.5-8B上相对提升20.4%，在LLaDA-8B-Instruct上提升16.0%，显著超过类似规模的自回归模型如Llama3.1-8B-Instruct。

Conclusion: 频域分析为理解扩散语言模型提供了新视角，FourierSampler通过频率域的动态引导机制有效解决了位置偏差问题，释放了扩散语言模型的任意生成潜力。

Abstract: Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.

</details>


### [55] [JobResQA: A Benchmark for LLM Machine Reading Comprehension on Multilingual Résumés and JDs](https://arxiv.org/abs/2601.23183)
*Casimiro Pio Carrino,Paula Estrella,Rabih Zbib,Carlos Escolano,José A. R. Fonollosa*

Main category: cs.CL

TL;DR: JobResQA是一个多语言问答基准，用于评估LLM在涉及简历和职位描述的HR特定任务上的机器阅读理解能力，包含5种语言的581个QA对，支持系统性的偏见和公平性研究。


<details>
  <summary>Details</summary>
Motivation: 需要评估LLM在HR应用中的多语言阅读理解能力，特别是在处理简历和职位描述时的表现，同时确保数据真实性和隐私保护。

Method: 通过去识别化和数据合成从真实世界来源创建数据集，使用基于TEaR方法的人机协作翻译流程确保高质量多语言并行基准，采用LLM-as-judge方法进行基线评估。

Result: 基准评估显示LLM在英语和西班牙语上表现较好，但在其他语言上性能显著下降，揭示了HR应用中多语言MRC能力的关键差距。

Conclusion: JobResQA为推进公平可靠的基于LLM的HR系统提供了可复现的基准，公开可用以促进相关研究。

Abstract: We introduce JobResQA, a multilingual Question Answering benchmark for evaluating Machine Reading Comprehension (MRC) capabilities of LLMs on HR-specific tasks involving résumés and job descriptions. The dataset comprises 581 QA pairs across 105 synthetic résumé-job description pairs in five languages (English, Spanish, Italian, German, and Chinese), with questions spanning three complexity levels from basic factual extraction to complex cross-document reasoning. We propose a data generation pipeline derived from real-world sources through de-identification and data synthesis to ensure both realism and privacy, while controlled demographic and professional attributes (implemented via placeholders) enable systematic bias and fairness studies. We also present a cost-effective, human-in-the-loop translation pipeline based on the TEaR methodology, incorporating MQM error annotations and selective post-editing to ensure an high-quality multi-way parallel benchmark. We provide a baseline evaluations across multiple open-weight LLM families using an LLM-as-judge approach revealing higher performances on English and Spanish but substantial degradation for other languages, highlighting critical gaps in multilingual MRC capabilities for HR applications. JobResQA provides a reproducible benchmark for advancing fair and reliable LLM-based HR systems. The benchmark is publicly available at: https://github.com/Avature/jobresqa-benchmark

</details>


### [56] [ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought](https://arxiv.org/abs/2601.23184)
*Fanmeng Wang,Haotian Liu,Guojiang Zhao,Hongteng Xu,Zhifeng Gao*

Main category: cs.CL

TL;DR: ReGuLaR提出了一种新的潜在推理范式，通过变分自编码器框架和视觉语义表示来压缩推理链，在保持性能的同时减少计算冗余。


<details>
  <summary>Details</summary>
Motivation: 现有显式推理链（CoT）存在计算冗余问题，而现有的潜在推理方法因缺乏适当的压缩指导而导致性能严重下降。

Method: 将潜在推理建模为变分自编码器框架，从后验分布中采样当前潜在推理状态；通过将显式推理链渲染为图像并提取密集视觉语义表示来正则化后验分布。

Result: ReGuLaR在计算效率和推理效果上显著优于现有潜在推理方法，甚至通过多模态推理超越了CoT方法。

Conclusion: ReGuLaR为潜在推理提供了一个新颖且有洞察力的解决方案，实现了高效压缩和最小信息损失。

Abstract: While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.

</details>


### [57] [Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience](https://arxiv.org/abs/2601.23188)
*Zhongxiang Sun,Qipeng Wang,Weijie Yu,Jingxuan Yang,Haolang Lu,Jun Xu*

Main category: cs.CL

TL;DR: DS-MCM框架通过分层元认知监控机制增强深度搜索代理，包含快速一致性监控和慢速经验驱动监控，在不确定任务演进中提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的深度搜索代理在多步检索、推理和长时任务执行方面表现出色，但实际失败常源于缺乏监控和调节推理检索状态的机制。认知神经科学表明人类元认知是分层组织的，结合快速异常检测和选择性触发的经验驱动反思。

Method: 提出DS-MCM框架，包含：1) 快速一致性监控器，轻量级检查外部证据与内部推理置信度的对齐；2) 慢速经验驱动监控器，选择性激活，基于历史代理轨迹的经验记忆指导纠正干预。将监控直接嵌入推理-检索循环。

Result: 在多个深度搜索基准测试和骨干模型上的实验表明，DS-MCM能持续提升性能和鲁棒性。

Conclusion: DS-MCM通过分层元认知监控机制有效解决了深度搜索代理在不确定任务演进中的状态监控问题，显著提升了系统的可靠性和性能。

Abstract: Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.

</details>


### [58] [Are you going to finish that? A Practical Study of the Tokenization Boundary Problem](https://arxiv.org/abs/2601.23223)
*Hao Xu,Alisa Liu,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: 语言模型在分词边界不匹配时会出现严重的概率失真问题，影响中文、复合语言和代码等场景，且规模越大问题越严重。


<details>
  <summary>Details</summary>
Motivation: 语言模型基于token序列训练，但用户通过文本交互，这种不匹配导致部分token问题。现有研究多使用任意字符前缀，但对尊重词边界的真实提示中的问题普遍性和严重性研究不足。

Method: 识别三种分词与"词"边界不匹配的领域：不使用空格的语言、高度复合的语言和代码。系统构建语义自然的以部分token结尾的提示，评估概率失真程度，并测试推理时缓解方法。

Result: 中文中高达25%的词边界与token边界不匹配。前沿语言模型在部分token提示下，正确延续的概率比token对齐时低三个数量级。这种退化不随规模减小，大模型反而更严重。

Conclusion: 分词在真实使用场景中导致严重的概率失真问题，需要为模型推理提供商提供实用建议，并验证了最近精确解决方案的有效性。

Abstract: Language models (LMs) are trained over sequences of tokens, whereas users interact with LMs via text. This mismatch gives rise to the partial token problem, which occurs when a user ends their prompt in the middle of the expected next-token, leading to distorted next-token predictions. Although this issue has been studied using arbitrary character prefixes, its prevalence and severity in realistic prompts respecting word boundaries remains underexplored. In this work, we identify three domains where token and "word" boundaries often do not line up: languages that do not use whitespace, highly compounding languages, and code. In Chinese, for example, up to 25% of word boundaries do not line up with token boundaries, making even natural, word-complete prompts susceptible to this problem. We systematically construct semantically natural prompts ending with a partial tokens; in experiments, we find that they comprise a serious failure mode: frontier LMs consistently place three orders of magnitude less probability on the correct continuation compared to when the prompt is "backed-off" to be token-aligned. This degradation does not diminish with scale and often worsens for larger models. Finally, we evaluate inference-time mitigations to the partial token problem and validate the effectiveness of recent exact solutions. Overall, we demonstrate the scale and severity of probability distortion caused by tokenization in realistic use cases, and provide practical recommentions for model inference providers.

</details>


### [59] [Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models](https://arxiv.org/abs/2601.23255)
*Ye Yu,Haibo Jin,Yaoning Yu,Jun Zhuang,Haohan Wang*

Main category: cs.CL

TL;DR: 研究人员设计了一种文本转音频的越狱攻击，通过将禁止指令嵌入叙事风格的音频流中，成功绕过大型音频语言模型的安全机制，攻击成功率高达98.26%


<details>
  <summary>Details</summary>
Motivation: 随着大型音频语言模型越来越多地处理原始语音输入，这种模态转变引入了新的安全漏洞，但这些漏洞尚未得到充分研究。需要了解从文本到音频的转变如何影响模型的安全性。

Method: 设计了一种文本转音频越狱攻击，利用先进的指令跟随TTS模型，将禁止指令嵌入叙事风格的音频流中。攻击利用结构和声学特性来绕过主要针对文本校准的安全机制。

Result: 攻击在包括Gemini 2.0 Flash在内的最先进模型上取得了98.26%的成功率，显著超过了纯文本基线的效果。这表明合成语音中的叙事格式能够有效引出受限制的输出。

Conclusion: 研究结果强调了需要开发能够同时处理语言和副语言表示的安全框架，特别是在基于语音的接口日益普及的背景下。模态转变带来了独特的安全挑战。

Abstract: Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.

</details>


### [60] [PaperBanana: Automating Academic Illustration for AI Scientists](https://arxiv.org/abs/2601.23265)
*Dawei Zhu,Rui Meng,Yale Song,Xiyu Wei,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: PaperBanana是一个基于先进视觉语言模型和图像生成模型的智能代理框架，用于自动生成可直接用于学术出版的插图，显著减轻研究流程中的插图制作负担。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的自主AI科学家发展迅速，但生成可直接用于出版的插图仍然是研究流程中劳动密集型的瓶颈。为了解决这个问题，作者开发了PaperBanana框架来减轻这一负担。

Method: PaperBanana是一个代理框架，通过协调专门的代理来检索参考文献、规划内容和样式、渲染图像，并通过自我批评进行迭代优化。该框架由先进的视觉语言模型和图像生成模型驱动。

Result: 实验表明，PaperBanana在忠实性、简洁性、可读性和美观性方面持续优于领先的基线方法。此外，该方法还能有效扩展到高质量统计图的生成。

Conclusion: PaperBanana为自动生成可直接用于出版的插图铺平了道路，有望显著提高学术研究的工作效率。

Abstract: Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.

</details>


### [61] [UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection](https://arxiv.org/abs/2601.23273)
*Siran Peng,Weisong Zhao,Tianyu Fu,Chenxu Zhao,Tianshuo Zhang,Haoyuan Zhang,Xiangyu Zhu,Minghui Wu,Zhen Lei*

Main category: cs.CL

TL;DR: UPA是一种无监督提示代理，通过结构化搜索和选择优化提示，无需监督奖励信号，使用LLM的成对比较和BTL模型进行两阶段优化。


<details>
  <summary>Details</summary>
Motivation: 现有提示代理方法通常需要监督奖励信号，但在实际场景中往往无法获得。需要开发无需监督反馈的提示优化方法。

Method: UPA采用两阶段框架：1）搜索阶段，通过演化树结构导航提示空间，使用LLM进行细粒度、顺序不变的成对比较；2）选择阶段，基于Bradley-Terry-Luce模型，先进行路径贝叶斯聚合过滤候选，再进行全局锦标赛式比较推断潜在提示质量。

Result: 在多个任务上的实验表明，UPA始终优于现有的提示优化方法，证明即使在完全无监督设置下，代理式优化仍然非常有效。

Conclusion: UPA展示了无需监督奖励信号的提示优化可行性，通过结构化搜索和基于BTL模型的选择机制，实现了有效的无监督提示代理优化。

Abstract: Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [62] [Dependence-Aware Label Aggregation for LLM-as-a-Judge via Ising Models](https://arxiv.org/abs/2601.22336)
*Krishnakumar Balasubramanian,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan*

Main category: stat.ML

TL;DR: 该论文研究大规模AI评估中聚合多个标注者（包括LLM法官）的二元判断问题，提出考虑标注者间依赖关系的层次化模型，证明传统条件独立假设方法存在局限，并在真实数据集上验证了改进效果。


<details>
  <summary>Details</summary>
Motivation: 大规模AI评估通常需要聚合K个标注者（包括用作法官的LLM）的二元判断。传统方法如Dawid-Skene或加权多数投票假设标注者在给定真实标签Y的条件下相互独立，但LLM法官由于共享数据、架构、提示和失败模式，这一假设经常被违反。忽略这种依赖关系会导致后验概率校准错误甚至产生自信的错误预测。

Method: 提出基于Ising图模型和潜在因子的层次化依赖感知模型。对于类别依赖的Ising模型，贝叶斯对数优势通常是投票的二次函数；对于类别独立的耦合，则简化为具有相关性调整参数的线性加权投票。通过有限K的例子展示条件独立方法可能翻转贝叶斯标签，并证明这些方法在法官数量增长时仍然严格次优。

Result: 证明基于条件独立的方法在法官数量增长时存在非零的额外风险。在三个真实世界数据集上评估所提方法，相比传统基线表现出改进的性能。

Conclusion: 标注者间的依赖关系在大规模AI评估中至关重要，传统条件独立假设方法存在根本性局限。提出的依赖感知层次化模型能够更好地处理LLM法官等标注者间的相关性，在实际应用中表现出优越性能。

Abstract: Large-scale AI evaluation increasingly relies on aggregating binary judgments from $K$ annotators, including LLMs used as judges. Most classical methods, e.g., Dawid-Skene or (weighted) majority voting, assume annotators are conditionally independent given the true label $Y\in\{0,1\}$, an assumption often violated by LLM judges due to shared data, architectures, prompts, and failure modes. Ignoring such dependencies can yield miscalibrated posteriors and even confidently incorrect predictions. We study label aggregation through a hierarchy of dependence-aware models based on Ising graphical models and latent factors. For class-dependent Ising models, the Bayes log-odds is generally quadratic in votes; for class-independent couplings, it reduces to a linear weighted vote with correlation-adjusted parameters. We present finite-$K$ examples showing that methods based on conditional independence can flip the Bayes label despite matching per-annotator marginals. We prove separation results demonstrating that these methods remain strictly suboptimal as the number of judges grows, incurring nonvanishing excess risk under latent factors. Finally, we evaluate the proposed method on three real-world datasets, demonstrating improved performance over the classical baselines.

</details>


### [63] [Amortized Simulation-Based Inference in Generalized Bayes via Neural Posterior Estimation](https://arxiv.org/abs/2601.22367)
*Shiyi Sun,Geoff K. Nicholls,Jeong Eun Lee*

Main category: stat.ML

TL;DR: 提出首个完全摊销的变分近似方法，通过训练单个(x,β)条件化的神经后验估计器，实现广义贝叶斯推断中温度化后验的单次前向采样，无需模拟器调用或推理时MCMC。


<details>
  <summary>Details</summary>
Motivation: 广义贝叶斯推断通过温度参数β缓解过自信问题并提高模型误设下的鲁棒性，但现有方法依赖昂贵的MCMC或SDE采样器，且需要为每个新数据集和每个β值重新运行。

Method: 训练单个(x,β)条件化的神经后验估计器q_φ(θ|x,β)，支持单次前向采样。提出两种互补训练路径：(1)合成流形外样本(θ,x)∼π(θ)p(x|θ)^β；(2)使用自归一化重要性采样重新加权固定基础数据集π(θ)p(x|θ)。

Result: 在四个标准基于模拟的推理基准测试（包括混沌Lorenz-96系统）中，β-摊销估计器在标准两样本指标上实现了有竞争力的后验近似，在广泛温度范围内匹配非摊销MCMC基功率后验采样器。

Conclusion: 提出的β-摊销变分近似方法为广义贝叶斯推断提供了高效实用的解决方案，实现了温度化后验的单次前向采样，在保持准确性的同时显著提高了计算效率。

Abstract: Generalized Bayesian Inference (GBI) tempers a loss with a temperature $β>0$ to mitigate overconfidence and improve robustness under model misspecification, but existing GBI methods typically rely on costly MCMC or SDE-based samplers and must be re-run for each new dataset and each $β$ value. We give the first fully amortized variational approximation to the tempered posterior family $p_β(θ\mid x) \propto π(θ)\,p(x \mid θ)^β$ by training a single $(x,β)$-conditioned neural posterior estimator $q_φ(θ\mid x,β)$ that enables sampling in a single forward pass, without simulator calls or inference-time MCMC. We introduce two complementary training routes: (i) synthesize off-manifold samples $(θ,x) \sim π(θ)\,p(x \mid θ)^β$ and (ii) reweight a fixed base dataset $π(θ)\,p(x \mid θ)$ using self-normalized importance sampling (SNIS). We show that the SNIS-weighted objective provides a consistent forward-KL fit to the tempered posterior with finite weight variance. Across four standard simulation-based inference (SBI) benchmarks, including the chaotic Lorenz-96 system, our $β$-amortized estimator achieves competitive posterior approximations in standard two-sample metrics, matching non-amortized MCMC-based power-posterior samplers over a wide range of temperatures.

</details>


### [64] [It's all the (Exponential) Family: An Equivalence between Maximum Likelihood Estimation and Control Variates for Sketching Algorithms](https://arxiv.org/abs/2601.22378)
*Keegan Kang,Kerong Wang,Ding Zhang,Rameshwar Pratap,Bhisham Dev Verma,Benedict H. W. Wong*

Main category: stat.ML

TL;DR: 该论文证明了在指数族分布中，最优控制变量估计器(CVE)能达到与最大似然估计器(MLE)相同的渐近方差，并提出了相应的EM算法，该算法比传统求根算法更快且数值稳定。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和草图算法中，MLE和CVE常结合已知信息使用。研究者希望找到一种更高效、更稳定的方法来计算MLE，特别是在满足特定条件的指数族分布中。

Method: 证明在指数族分布的特定条件下，最优CVE能达到与MLE相同的渐近方差，并基于此推导出用于计算MLE的EM算法。通过实验验证该算法在二元正态分布上的性能。

Result: 实验表明，提出的EM算法比传统的MLE求根算法更快且数值更稳定。该算法还能提高使用MLE/CVE算法的可重复性，并在已知控制变量权重时找到MLE。

Conclusion: 在满足条件的指数族分布中，最优CVE与MLE具有相同的渐近方差，基于此推导的EM算法为计算MLE提供了更高效、更稳定的方法，有望推广到更多分布中。

Abstract: Maximum likelihood estimators (MLE) and control variate estimators (CVE) have been used in conjunction with known information across sketching algorithms and applications in machine learning. We prove that under certain conditions in an exponential family, an optimal CVE will achieve the same asymptotic variance as the MLE, giving an Expectation-Maximization (EM) algorithm for the MLE. Experiments show the EM algorithm is faster and numerically stable compared to other root finding algorithms for the MLE for the bivariate Normal distribution, and we expect this to hold across distributions satisfying these conditions. We show how the EM algorithm leads to reproducibility for algorithms using MLE / CVE, and demonstrate how the EM algorithm leads to finding the MLE when the CV weights are known.

</details>


### [65] [Simulation-based Bayesian inference with ameliorative learned summary statistics -- Part I](https://arxiv.org/abs/2601.22441)
*Getachew K. Befekadu*

Main category: stat.ML

TL;DR: 本文提出了一种基于模拟的推断方法，使用学习到的摘要统计量作为经验似然，在贝叶斯框架下处理难以获得闭式似然函数或计算不可行的问题。


<details>
  <summary>Details</summary>
Motivation: 当观测数据和模拟模型的精确似然函数难以获得闭式形式或计算不可行时，需要一种有效的推断方法。特别是在贝叶斯设置中，需要能够处理复杂模拟模型和大数据集的解决方案。

Method: 使用基于学习的摘要统计量，通过Cressie-Read差异准则在矩约束下进行数据转换，将观测数据与模拟输出进行总结。这种方法允许将模拟输出条件化于观测数据，并支持分布式计算实现。

Result: 该方法能够保持推断的统计功效，支持对观测数据的特定样本集进行推断，可以扩展到处理弱相关观测数据，并适合分布式计算实现。

Conclusion: 提出的基于模拟的推断框架为处理复杂模拟模型和大数据集提供了一种有效的解决方案，能够将数据到学习摘要统计量的转换与贝叶斯推断问题统一为分布式推断问题。

Abstract: This paper, which is Part 1 of a two-part paper series, considers a simulation-based inference with learned summary statistics, in which such a learned summary statistic serves as an empirical-likelihood with ameliorative effects in the Bayesian setting, when the exact likelihood function associated with the observation data and the simulation model is difficult to obtain in a closed form or computationally intractable. In particular, a transformation technique which leverages the Cressie-Read discrepancy criterion under moment restrictions is used for summarizing the learned statistics between the observation data and the simulation outputs, while preserving the statistical power of the inference. Here, such a transformation of data-to-learned summary statistics also allows the simulation outputs to be conditioned on the observation data, so that the inference task can be performed over certain sample sets of the observation data that are considered as an empirical relevance or believed to be particular importance. Moreover, the simulation-based inference framework discussed in this paper can be extended further, and thus handling weakly dependent observation data. Finally, we remark that such an inference framework is suitable for implementation in distributed computing, i.e., computational tasks involving both the data-to-learned summary statistics and the Bayesian inferencing problem can be posed as a unified distributed inference problem that will exploit distributed optimization and MCMC algorithms for supporting large datasets associated with complex simulation models.

</details>


### [66] [Corrected Samplers for Discrete Flow Models](https://arxiv.org/abs/2601.22519)
*Zhengyan Wan,Yidong Ouyang,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: 提出两种修正采样器（时间修正和位置修正），用于离散流模型，降低离散化误差，减少迭代次数，提升生成质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型采样器（如tau-leaping和Euler求解器）需要大量迭代来控制离散化误差，且理论结果通常要求转移率有界或局限于特定源分布。需要解决这些限制。

Method: 建立无限制的离散化误差界，分析Euler采样器的一步下界，提出时间修正采样器和位置修正采样器，几乎不增加计算成本。

Result: 位置修正采样器比现有并行采样器具有更低的迭代复杂度，在仿真和文生图任务中验证了生成质量提升和推理时间减少。

Conclusion: 提出的修正采样器有效解决了离散流模型中的离散化误差问题，在理论和实验上都表现出优越性。

Abstract: Discrete flow models (DFMs) have been proposed to learn the data distribution on a finite state space, offering a flexible framework as an alternative to discrete diffusion models. A line of recent work has studied samplers for discrete diffusion models, such as tau-leaping and Euler solver. However, these samplers require a large number of iterations to control discretization error, since the transition rates are frozen in time and evaluated at the initial state within each time interval. Moreover, theoretical results for these samplers often require boundedness conditions of the transition rate or they focus on a specific type of source distributions. To address those limitations, we establish non-asymptotic discretization error bounds for those samplers without any restriction on transition rates and source distributions, under the framework of discrete flow models. Furthermore, by analyzing a one-step lower bound of the Euler sampler, we propose two corrected samplers: \textit{time-corrected sampler} and \textit{location-corrected sampler}, which can reduce the discretization error of tau-leaping and Euler solver with almost no additional computational cost. We rigorously show that the location-corrected sampler has a lower iteration complexity than existing parallel samplers. We validate the effectiveness of the proposed method by demonstrating improved generation quality and reduced inference time on both simulation and text-to-image generation tasks. Code can be found in https://github.com/WanZhengyan/Corrected-Samplers-for-Discrete-Flow-Models.

</details>


### [67] [An Efficient Algorithm for Thresholding Monte Carlo Tree Search](https://arxiv.org/abs/2601.22600)
*Shoma Nameki,Atsuyoshi Nakamura,Junpei Komiyama,Koji Tabata*

Main category: stat.ML

TL;DR: 提出阈值蒙特卡洛树搜索问题，开发基于Track-and-Stop策略的δ-正确顺序采样算法，具有渐近最优样本复杂度，并通过比率修正D-Tracking策略显著提升经验性能。


<details>
  <summary>Details</summary>
Motivation: 解决在树结构决策问题中，需要判断根节点值是否达到给定阈值的问题。这类问题在游戏AI、决策优化等领域有广泛应用，但现有方法在样本效率和计算复杂度方面存在不足。

Method: 提出基于Track-and-Stop策略的δ-正确顺序采样算法。采用比率修正的D-Tracking臂拉动策略，将每轮计算成本从线性降低到对数级别。

Result: 算法具有渐近最优样本复杂度。经验样本复杂度显著改善，每轮计算成本从O(n)降低到O(log n)，其中n为臂的数量。

Conclusion: 提出的算法在理论和实践上都表现优异，为阈值蒙特卡洛树搜索问题提供了高效解决方案，在样本复杂度和计算效率方面都有显著提升。

Abstract: We introduce the Thresholding Monte Carlo Tree Search problem, in which, given a tree $\mathcal{T}$ and a threshold $θ$, a player must answer whether the root node value of $\mathcal{T}$ is at least $θ$ or not. In the given tree, `MAX' or `MIN' is labeled on each internal node, and the value of a `MAX'-labeled (`MIN'-labeled) internal node is the maximum (minimum) of its child values. The value of a leaf node is the mean reward of an unknown distribution, from which the player can sample rewards. For this problem, we develop a $δ$-correct sequential sampling algorithm based on the Track-and-Stop strategy that has asymptotically optimal sample complexity. We show that a ratio-based modification of the D-Tracking arm-pulling strategy leads to a substantial improvement in empirical sample complexity, as well as reducing the per-round computational cost from linear to logarithmic in the number of arms.

</details>


### [68] [RPWithPrior: Label Differential Privacy in Regression](https://arxiv.org/abs/2601.22625)
*Haixia Liu,Ruifan Huang*

Main category: stat.ML

TL;DR: 提出一种新的回归任务ε-标签差分隐私保护方法，通过连续随机变量建模避免离散化，在已知和未知先验情况下提供更优性能


<details>
  <summary>Details</summary>
Motivation: 现有回归任务的ε-标签差分隐私方法（如RR-On-Bins）需要离散化输出空间，与现实场景不匹配，且存在精度损失问题

Method: 将原始和随机响应建模为连续随机变量，避免离散化；提出估计最优随机响应区间的新算法，包括已知先验的RPWithPrior算法和未知先验情况下的算法

Result: RPWithPrior算法保证ε-标签差分隐私；在Communities and Crime、Criteo Sponsored Search Conversion Log、California Housing数据集上，性能优于Gaussian、Laplace、Staircase、RRonBins和Unbiased机制

Conclusion: 提出的连续随机变量建模方法能更好地保护回归任务中的用户隐私，同时减少精度损失，比现有方法更适用于实际场景

Abstract: With the wide application of machine learning techniques in practice, privacy preservation has gained increasing attention. Protecting user privacy with minimal accuracy loss is a fundamental task in the data analysis and mining community. In this paper, we focus on regression tasks under $ε$-label differential privacy guarantees. Some existing methods for regression with $ε$-label differential privacy, such as the RR-On-Bins mechanism, discretized the output space into finite bins and then applied RR algorithm. To efficiently determine these finite bins, the authors rounded the original responses down to integer values. However, such operations does not align well with real-world scenarios. To overcome these limitations, we model both original and randomized responses as continuous random variables, avoiding discretization entirely. Our novel approach estimates an optimal interval for randomized responses and introduces new algorithms designed for scenarios where a prior is either known or unknown. Additionally, we prove that our algorithm, RPWithPrior, guarantees $ε$-label differential privacy. Numerical results demonstrate that our approach gets better performance compared with the Gaussian, Laplace, Staircase, and RRonBins, Unbiased mechanisms on the Communities and Crime, Criteo Sponsored Search Conversion Log, California Housing datasets.

</details>


### [69] [Generative and Nonparametric Approaches for Conditional Distribution Estimation: Methods, Perspectives, and Comparative Evaluations](https://arxiv.org/abs/2601.22650)
*Yen-Shiu Chin,Zhi-Yu Jou,Toshinari Morimoto,Chia-Tse Wang,Ming-Chung Chang,Tso-Jung Yen,Su-Yun Huang,Tailen Hsing*

Main category: stat.ML

TL;DR: 本文综述并比较了条件分布推断的多种方法，包括经典非参数方法和现代生成模型，提供了系统数值比较和评估框架。


<details>
  <summary>Details</summary>
Motivation: 条件分布推断是统计学中的基本问题，对预测、不确定性量化和概率建模至关重要。虽然已有多种方法，但缺乏系统比较和评估。

Method: 回顾和比较了五种代表性方法：1) Hall和Yao的单指标方法；2) FlexCode和DeepCDE的基展开方法；3) 生成条件分布采样器；4) 条件去噪扩散概率模型。使用统一的评估框架进行系统数值比较。

Result: 提供了各种方法的性能比较，包括条件均值和标准差的均方误差以及Wasserstein距离。同时讨论了各方法的灵活性和计算成本。

Conclusion: 不同方法各有优势和局限性，系统比较为方法选择提供了指导。统一的评估框架确保了公平性和可重复性。

Abstract: The inference of conditional distributions is a fundamental problem in statistics, essential for prediction, uncertainty quantification, and probabilistic modeling. A wide range of methodologies have been developed for this task. This article reviews and compares several representative approaches spanning classical nonparametric methods and modern generative models. We begin with the single-index method of Hall and Yao (2005), which estimates the conditional distribution through a dimension-reducing index and nonparametric smoothing of the resulting one-dimensional cumulative conditional distribution function. We then examine the basis-expansion approaches, including FlexCode (Izbicki and Lee, 2017) and DeepCDE (Dalmasso et al., 2020), which convert conditional density estimation into a set of nonparametric regression problems. In addition, we discuss two recent generative simulation-based methods that leverage modern deep generative architectures: the generative conditional distribution sampler (Zhou et al., 2023) and the conditional denoising diffusion probabilistic model (Fu et al., 2024; Yang et al., 2025). A systematic numerical comparison of these approaches is provided using a unified evaluation framework that ensures fairness and reproducibility. The performance metrics used for the estimated conditional distribution include the mean-squared errors of conditional mean and standard deviation, as well as the Wasserstein distance. We also discuss their flexibility and computational costs, highlighting the distinct advantages and limitations of each approach.

</details>


### [70] [Spectral Gradient Descent Mitigates Anisotropy-Driven Misalignment: A Case Study in Phase Retrieval](https://arxiv.org/abs/2601.22652)
*Guillaume Braun,Han Bao,Wei Huang,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 论文研究了谱梯度方法（如Muon优化器）在深度学习中的优势机制，通过分析非线性相位恢复模型，发现在各向异性高斯输入下，梯度下降会放大高方差但无信息的噪声方向，而谱梯度方法能消除这种放大效应，实现更稳定的对齐和更快的噪声收缩。


<details>
  <summary>Details</summary>
Motivation: 谱梯度方法（如Muon优化器）在深度学习中表现出色，但其优势机制尚不明确。本文旨在通过理论分析揭示谱梯度方法相对于传统梯度下降的性能提升原理。

Method: 采用非线性相位恢复模型（等价于两层神经网络，使用二次激活函数和固定第二层权重），在各向异性高斯输入下进行分析。特别关注"尖峰协方差"设置，其中主导方差方向与信号正交。通过动力学分析比较梯度下降(GD)和谱梯度下降(SpecGD)的行为。

Result: 梯度下降在早期逃逸阶段会放大高方差但无信息的尖峰方向，导致与真实信号的对齐变差（方差诱导的错位）。而谱梯度下降能消除这种尖峰放大效应，实现稳定的对齐和加速的噪声收缩。数值实验验证了理论结果，并表明这些现象在更广泛的各种异性协方差下持续存在。

Conclusion: 谱梯度方法的优势在于能够抑制梯度更新中无信息高方差方向的放大，从而避免方差诱导的错位问题，实现更有效的优化。这解释了谱梯度方法在深度学习中的实证性能优势。

Abstract: Spectral gradient methods, such as the Muon optimizer, modify gradient updates by preserving directional information while discarding scale, and have shown strong empirical performance in deep learning. We investigate the mechanisms underlying these gains through a dynamical analysis of a nonlinear phase retrieval model with anisotropic Gaussian inputs, equivalent to training a two-layer neural network with the quadratic activation and fixed second-layer weights. Focusing on a spiked covariance setting where the dominant variance direction is orthogonal to the signal, we show that gradient descent (GD) suffers from a variance-induced misalignment: during the early escaping stage, the high-variance but uninformative spike direction is multiplicatively amplified, degrading alignment with the true signal under strong anisotropy. In contrast, spectral gradient descent (SpecGD) removes this spike amplification effect, leading to stable alignment and accelerated noise contraction. Numerical experiments confirm the theory and show that these phenomena persist under broader anisotropic covariances.

</details>


### [71] [GRANITE: A Generalized Regional Framework for Identifying Agreement in Feature-Based Explanations](https://arxiv.org/abs/2601.22771)
*Julia Herbinger,Gabriel Laberge,Maximilian Muschalik,Yann Pequignot,Marvin N. Wright,Fabian Fumagalli*

Main category: stat.ML

TL;DR: GRANITE是一个统一的区域解释框架，通过将特征空间划分为相互作用和分布影响最小的区域，使不同解释方法达成一致，提供更一致和可解释的特征解释。


<details>
  <summary>Details</summary>
Motivation: 当前基于特征的解释方法经常产生相互矛盾的解释，这种分歧主要源于特征相互作用处理和特征依赖关系纳入方式的不同。需要一种统一框架来协调不同解释方法，提供更一致和可解释的解释。

Method: 提出GRANITE框架，将特征空间划分为相互作用和分布影响最小的区域，统一现有区域方法，扩展到特征组，并引入递归分区算法来估计这些区域。

Result: 在真实世界数据集上展示了GRANITE的有效性，为一致和可解释的特征解释提供了实用工具，能够协调不同解释方法产生更一致的解释。

Conclusion: GRANITE通过最小化特征相互作用和分布影响的分区方法，解决了不同解释方法之间的分歧问题，提供了一个统一框架来获得更一致和可解释的特征解释。

Abstract: Feature-based explanation methods aim to quantify how features influence the model's behavior, either locally or globally, but different methods often disagree, producing conflicting explanations. This disagreement arises primarily from two sources: how feature interactions are handled and how feature dependencies are incorporated. We propose GRANITE, a generalized regional explanation framework that partitions the feature space into regions where interaction and distribution influences are minimized. This approach aligns different explanation methods, yielding more consistent and interpretable explanations. GRANITE unifies existing regional approaches, extends them to feature groups, and introduces a recursive partitioning algorithm to estimate such regions. We demonstrate its effectiveness on real-world datasets, providing a practical tool for consistent and interpretable feature explanations.

</details>


### [72] [Approximating $f$-Divergences with Rank Statistics](https://arxiv.org/abs/2601.22784)
*Viktor Stein,José Manuel de Frutos*

Main category: stat.ML

TL;DR: 提出一种基于秩统计量的f-散度近似方法，避免显式密度比估计，通过秩直方图和离散f-散度衡量分布差异


<details>
  <summary>Details</summary>
Motivation: 传统f-散度估计需要显式密度比估计，计算复杂且在高维情况下困难。本文旨在开发一种基于秩统计量的替代方法，避免密度比估计，直接利用分布秩信息

Method: 将两个单变量分布映射到{0,...,K}上的秩直方图，通过离散f-散度衡量其与均匀分布的偏差。对于高维数据，使用随机投影平均得到切片秩统计量f-散度

Result: 证明了估计量在K上是单调的，总是真实f-散度的下界，建立了K→∞时的收敛速率。推导了有限样本偏差界和渐近正态性结果，实验验证了方法的有效性

Conclusion: 提出的秩统计量f-散度提供了一种无需密度比估计的分布差异度量方法，具有理论保证和实际应用价值，可用于生成建模等任务

Abstract: We introduce a rank-statistic approximation of $f$-divergences that avoids explicit density-ratio estimation by working directly with the distribution of ranks. For a resolution parameter $K$, we map the mismatch between two univariate distributions $μ$ and $ν$ to a rank histogram on $\{ 0, \ldots, K\}$ and measure its deviation from uniformity via a discrete $f$-divergence, yielding a rank-statistic divergence estimator. We prove that the resulting estimator of the divergence is monotone in $K$, is always a lower bound of the true $f$-divergence, and we establish quantitative convergence rates for $K\to\infty$ under mild regularity of the quantile-domain density ratio. To handle high-dimensional data, we define the sliced rank-statistic $f$-divergence by averaging the univariate construction over random projections, and we provide convergence results for the sliced limit as well. We also derive finite-sample deviation bounds along with asymptotic normality results for the estimator. Finally, we empirically validate the approach by benchmarking against neural baselines and illustrating its use as a learning objective in generative modelling experiments.

</details>


### [73] [OneFlowSBI: One Model, Many Queries for Simulation-Based Inference](https://arxiv.org/abs/2601.22951)
*Mayank Nautiyal,Li Ju,Melker Ernfors,Klara Hagland,Ville Holma,Maximilian Werkö Söderholm,Andreas Hellander,Prashant Singh*

Main category: stat.ML

TL;DR: OneFlowSBI是一个统一的仿真推理框架，通过单一流匹配生成模型学习参数和观测的联合分布，支持多种推理任务而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的仿真推理方法通常需要为不同任务训练专门模型，缺乏统一的框架来同时支持后验采样、似然估计和任意条件分布等多种推理任务。

Method: 使用流匹配生成模型学习参数和观测的联合分布，在训练时采用查询感知的掩码分布，使同一模型能够支持多种推理任务。

Result: 在10个基准推理问题和2个高维真实世界逆问题上评估，OneFlowSBI在多种仿真预算下表现优异，与最先进的广义推理求解器和专门后验估计器竞争，同时能够用少量ODE积分步骤高效采样，对噪声和部分观测数据保持鲁棒。

Conclusion: OneFlowSBI提供了一个统一的仿真推理框架，能够高效处理多种推理任务，在性能和效率方面都具有竞争力，为复杂推理问题提供了灵活且强大的解决方案。

Abstract: We introduce \textit{OneFlowSBI}, a unified framework for simulation-based inference that learns a single flow-matching generative model over the joint distribution of parameters and observations. Leveraging a query-aware masking distribution during training, the same model supports multiple inference tasks, including posterior sampling, likelihood estimation, and arbitrary conditional distributions, without task-specific retraining. We evaluate \textit{OneFlowSBI} on ten benchmark inference problems and two high-dimensional real-world inverse problems across multiple simulation budgets. \textit{OneFlowSBI} is shown to deliver competitive performance against state-of-the-art generalized inference solvers and specialized posterior estimators, while enabling efficient sampling with few ODE integration steps and remaining robust under noisy and partially observed data.

</details>


### [74] [Neural Backward Filtering Forward Guiding](https://arxiv.org/abs/2601.23030)
*Gefan Yang,Frank van der Meulen,Stefan Sommer*

Main category: stat.ML

TL;DR: 提出NBFFG框架，通过辅助线性高斯过程和神经网络残差，解决树状非线性连续随机过程的稀疏观测推理问题。


<details>
  <summary>Details</summary>
Motivation: 树状非线性连续随机过程的推理具有挑战性，特别是当观测稀疏（仅叶节点）且拓扑复杂时。精确平滑方法在非线性动态中难以处理，而基于粒子的方法在高维中性能下降。

Method: 提出神经后向滤波前向导引（NBFFG）框架：1）利用辅助线性高斯过程获得闭式后向滤波器作为"引导"；2）学习神经网络残差（参数化为归一化流或受控SDE）捕捉非线性差异；3）实现无偏路径子采样，将训练复杂度从依赖树大小降低到依赖路径长度。

Result: NBFFG在合成基准测试中优于基线方法，并在系统发育分析的高维推理任务中成功重建了蝴蝶翅膀形状的祖先形态。

Conclusion: NBFFG为树状非线性连续随机过程的推理提供了一个有效框架，特别适用于稀疏观测和高维场景，在系统发育分析等实际应用中具有潜力。

Abstract: Inference in non-linear continuous stochastic processes on trees is challenging, particularly when observations are sparse (leaf-only) and the topology is complex. Exact smoothing via Doob's $h$-transform is intractable for general non-linear dynamics, while particle-based methods degrade in high dimensions. We propose Neural Backward Filtering Forward Guiding (NBFFG), a unified framework for both discrete transitions and continuous diffusions. Our method constructs a variational posterior by leveraging an auxiliary linear-Gaussian process. This auxiliary process yields a closed-form backward filter that serves as a ``guide'', steering the generative path toward high-likelihood regions. We then learn a neural residual--parameterized as a normalizing flow or a controlled SDE--to capture the non-linear discrepancies. This formulation allows for an unbiased path-wise subsampling scheme, reducing the training complexity from tree-size dependent to path-length dependent. Empirical results show that NBFFG outperforms baselines on synthetic benchmarks, and we demonstrate the method on a high-dimensional inference task in phylogenetic analysis with reconstruction of ancestral butterfly wing shapes.

</details>


### [75] [Asymptotic Theory of Iterated Empirical Risk Minimization, with Applications to Active Learning](https://arxiv.org/abs/2601.23031)
*Hugo Cui,Yue M. Lu*

Main category: stat.ML

TL;DR: 研究两阶段迭代经验风险最小化(ERM)过程，其中第一阶段预测作为第二阶段损失函数的输入，分析其在主动学习和重加权场景中的高维渐近性能。


<details>
  <summary>Details</summary>
Motivation: 传统单阶段ERM分析无法处理两阶段ERM中数据重用和预测依赖损失函数带来的复杂统计依赖问题，这在主动学习和重加权方案中自然出现，需要新的理论框架。

Method: 针对高斯混合数据上的线性模型，使用凸损失函数，在高维渐近框架下推导第二阶段估计器测试误差的精确渐近特征，其中样本量和维度成比例缩放。

Result: 获得了第二阶段估计器性能的显式渐近预测，应用于池式主动学习问题，消除了先前工作中的oracle和样本分割假设，揭示了阶段间标注预算分配的基本权衡，并展示了纯由数据选择驱动的测试误差双下降行为。

Conclusion: 为两阶段ERM过程提供了严格的理论分析框架，揭示了数据重用场景下的新现象，特别是在主动学习中发现了标注预算分配的基本权衡和纯数据选择驱动的双下降行为。

Abstract: We study a class of iterated empirical risk minimization (ERM) procedures in which two successive ERMs are performed on the same dataset, and the predictions of the first estimator enter as an argument in the loss function of the second. This setting, which arises naturally in active learning and reweighting schemes, introduces intricate statistical dependencies across samples and fundamentally distinguishes the problem from classical single-stage ERM analyses. For linear models trained with a broad class of convex losses on Gaussian mixture data, we derive a sharp asymptotic characterization of the test error in the high-dimensional regime where the sample size and ambient dimension scale proportionally. Our results provide explicit, fully asymptotic predictions for the performance of the second-stage estimator despite the reuse of data and the presence of prediction-dependent losses. We apply this theory to revisit a well-studied pool-based active learning problem, removing oracle and sample-splitting assumptions made in prior work. We uncover a fundamental tradeoff in how the labeling budget should be allocated across stages, and demonstrate a double-descent behavior of the test error driven purely by data selection, rather than model size or sample count.

</details>


### [76] [A Random Matrix Theory of Masked Self-Supervised Regression](https://arxiv.org/abs/2601.23208)
*Arie Wortsman Zurich,Federica Gerace,Bruno Loureiro,Yue M. Lu*

Main category: stat.ML

TL;DR: 该论文对掩码自监督学习进行高维分析，揭示了其如何从数据中提取结构，并证明在某些情况下优于PCA。


<details>
  <summary>Details</summary>
Motivation: 在Transformer时代，掩码自监督学习已成为基础训练范式，但其联合矩阵值预测器的理论分析仍面临挑战，需要理解其如何从数据中提取结构以及何时优于传统无监督方法。

Method: 开发了高维比例机制下的精确分析框架，其中样本数量与维度成比例，推导了泛化误差的显式表达式，并表征了学习预测器的谱结构。

Result: 对于尖峰协方差模型，发现联合预测器经历BBP型相变，确定了掩码SSL开始恢复潜在信号的条件；识别了结构化机制，证明掩码自监督学习在某些情况下可证明优于PCA。

Conclusion: 掩码自监督学习通过聚合多个掩码模式的预测形成联合矩阵值预测器，能有效提取数据中的结构信息，在特定条件下比传统无监督方法（如PCA）具有理论优势。

Abstract: In the era of transformer models, masked self-supervised learning (SSL) has become a foundational training paradigm. A defining feature of masked SSL is that training aggregates predictions across many masking patterns, giving rise to a joint, matrix-valued predictor rather than a single vector-valued estimator. This object encodes how coordinates condition on one another and poses new analytical challenges. We develop a precise high-dimensional analysis of masked modeling objectives in the proportional regime where the number of samples scales with the ambient dimension. Our results provide explicit expressions for the generalization error and characterize the spectral structure of the learned predictor, revealing how masked modeling extracts structure from data. For spiked covariance models, we show that the joint predictor undergoes a Baik--Ben Arous--Péché (BBP)-type phase transition, identifying when masked SSL begins to recover latent signals. Finally, we identify structured regimes in which masked self-supervised learning provably outperforms PCA, highlighting potential advantages of SSL objectives over classical unsupervised methods

</details>


### [77] [Graph Attention Network for Node Regression on Random Geometric Graphs with Erdős--Rényi contamination](https://arxiv.org/abs/2601.23239)
*Somak Laha,Suqi Liu,Morgane Austern*

Main category: stat.ML

TL;DR: 该论文证明了在节点回归任务中，针对噪声污染的数据，专门设计的图注意力网络(GAT)在估计回归系数和预测响应方面优于普通最小二乘法和普通图卷积网络。


<details>
  <summary>Details</summary>
Motivation: 虽然图注意力网络(GATs)在实践中表现出对节点特征和边噪声的鲁棒性，但缺乏严格的统计理论保证证明GATs相对于非注意力图神经网络(GNNs)的优势。本文旨在填补这一理论空白。

Method: 提出并分析了一个精心设计的任务特定GAT，该网络构建去噪的代理特征用于回归。在同时存在协变量和边污染的图基误差变量模型下，通过高维几何尾界和邻域计数、样本协方差的集中性分析来证明理论结果。

Result: 理论证明：在温和增长条件下，使用代理特征进行回归在(a)估计回归系数方面比在噪声节点特征上的普通最小二乘法(OLS)估计器具有更低的渐近误差；(b)预测未标记节点的响应方面比普通图卷积网络(GCN)表现更好。实验验证了理论发现。

Conclusion: 该研究为GATs相对于非注意力GNNs的优势提供了严格的理论保证，证明了在存在噪声污染的情况下，专门设计的注意力机制能够有效去噪并提升回归性能，为图注意力网络的理论基础做出了贡献。

Abstract: Graph attention networks (GATs) are widely used and often appear robust to noise in node covariates and edges, yet rigorous statistical guarantees demonstrating a provable advantage of GATs over non-attention graph neural networks~(GNNs) are scarce. We partially address this gap for node regression with graph-based errors-in-variables models under simultaneous covariate and edge corruption: responses are generated from latent node-level covariates, but only noise-perturbed versions of the latent covariates are observed; and the sample graph is a random geometric graph created from the node covariates but contaminated by independent Erdős--Rényi edges. We propose and analyze a carefully designed, task-specific GAT that constructs denoised proxy features for regression. We prove that regressing the response variables on the proxies achieves lower error asymptotically in (a) estimating the regression coefficient compared to the ordinary least squares (OLS) estimator on the noisy node covariates, and (b) predicting the response for an unlabelled node compared to a vanilla graph convolutional network~(GCN) -- under mild growth conditions. Our analysis leverages high-dimensional geometric tail bounds and concentration for neighbourhood counts and sample covariances. We verify our theoretical findings through experiments on synthetically generated data. We also perform experiments on real-world graphs and demonstrate the effectiveness of the attention mechanism in several node regression tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: JAF（Judge Agent Forest）是一个智能体AI框架，通过让评判智能体对主要智能体生成的多个查询-响应对进行联合推理，而非孤立评估，实现更全面的评估和迭代自我改进。


<details>
  <summary>Details</summary>
Motivation: 传统评判智能体单独评估每个查询-响应对，缺乏对跨实例模式和一致性的洞察。需要一种能够进行整体学习、识别相关响应间模式和不一致性的框架，以提供更有价值的反馈来改进主要智能体的性能。

Method: JAF采用信念传播和集成学习原则，通过重叠的上下文邻域构建知识图结构传播批评。开发了灵活的局部敏感哈希算法，整合语义嵌入、LLM驱动的哈希谓词、分类标签监督和相关侧信息，生成信息丰富的二进制代码，支持高效、可解释的关系感知范例选择。

Result: 在大型云环境中具有挑战性的云配置错误分类任务上进行了实证验证，展示了JAF框架的有效性。

Conclusion: JAF将评判智能体从局部评估者提升为整体学习者，通过联合推理相关响应，识别跨实例模式，提供聚合反馈，使主要智能体能够通过评判智能体的集体视角改进自身输出。

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [79] [The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution](https://arxiv.org/abs/2601.22290)
*Khush Patel,Siva Surendira,Jithin George,Shreyas Kapale*

Main category: cs.AI

TL;DR: Six Sigma Agent 架构通过任务分解、并行微代理采样和共识投票实现企业级可靠性，将错误率从5%降至0.11%，成本降低80%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能力强大，但本质上是概率性的，存在可靠性挑战，难以满足企业部署的严格要求

Method: 1. 任务分解为原子操作的依赖树；2. 微代理采样：每个任务并行执行n次，使用不同LLM生成独立输出；3. 共识投票与动态缩放：聚类输出并从获胜集群中选择答案

Result: 证明采样n个独立输出可将系统错误降至O(p^{ceil(n/2)})；使用5%错误率的廉价模型，5个代理可将错误降至0.11%；13个代理实现3.4 DPMO，达到六西格玛标准；三个企业用例显示可靠性提升14,700倍，成本降低80%

Conclusion: AI系统的可靠性源于原则性的冗余和共识机制，而非仅仅依赖模型缩放；Six Sigma Agent为企业级AI部署提供了可靠的解决方案

Abstract: Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.

</details>


### [80] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: 论文提出FLARE方法，通过未来感知规划解决LLM代理在长时程规划中的短视问题，使早期决策能考虑下游结果，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: LLM代理在短时程推理中表现良好，但在长时程规划中经常失败，因为逐步推理诱导的逐步贪婪策略无法处理早期行动需要考虑延迟后果的情况。

Method: 提出FLARE（Future-aware Lookahead with Reward Estimation）方法，在单个模型中实现显式前瞻、价值传播和有限承诺，让下游结果影响早期决策。

Result: 在多个基准测试、代理框架和LLM骨干网络中，FLARE一致提升了任务性能和规划级行为，LLaMA-8B+FLARE经常能超越GPT-4o+标准逐步推理。

Conclusion: 研究确立了推理与规划之间的明确区别，未来感知规划是解决LLM代理长时程规划问题的有效方法。

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [81] [Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?](https://arxiv.org/abs/2601.22329)
*Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Fatemeh Bahrani,Ashutosh Chaubey,Sai Praneeth Karimireddy,Norbert Schwarz,Jonathan Gratch*

Main category: cs.AI

TL;DR: LLMs在理性决策任务中表现出色，但情感引导会显著影响其判断，不同引导方法在可控性和人类对齐行为之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地应用于招聘、医疗和经济决策等高风险领域，需要评估它们是否表现出类似人类的（非）理性模式和偏见，以确定它们能否作为人类行为的模型或安全部署的决策系统。

Method: 评估多个LLM家族在（1）理性选择核心公理的基准测试和（2）行为经济学和社会规范中情感影响判断的经典决策领域。使用两种情感引导方法：上下文提示（ICP）和表示层引导（RLS），研究情感扭曲及其与推理的相互作用。

Result: 深思熟虑的"思考"可靠地提高了理性并推动模型趋向期望价值最大化。ICP产生强烈的方向性偏移但难以校准，RLS产生更符合心理学模式但可靠性较低。提高理性的机制也增强了对情感干预的敏感性。

Conclusion: 推理和情感引导之间存在张力，这对人类行为模拟和LLM决策系统的安全部署都有重要影响。不同引导方法在可控性和人类对齐行为之间存在权衡。

Abstract: Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of human behavior, it is critical to assess whether they exhibit analogous patterns of (ir)rationalities and biases. To this end, we evaluate multiple LLM families on (i) benchmarks testing core axioms of rational choice and (ii) classic decision domains from behavioral economics and social norms where emotions are known to shape judgment and choice. Across settings, we show that deliberate "thinking" reliably improves rationality and pushes models toward expected-value maximization. To probe human-like affective distortions and their interaction with reasoning, we use two emotion-steering methods: in-context priming (ICP) and representation-level steering (RLS). ICP induces strong directional shifts that are often extreme and difficult to calibrate, whereas RLS produces more psychologically plausible patterns but with lower reliability. Our results suggest that the same mechanisms that improve rationality also amplify sensitivity to affective interventions, and that different steering methods trade off controllability against human-aligned behavior. Overall, this points to a tension between reasoning and affective steering, with implications for both human simulation and the safe deployment of LLM-based decision systems.

</details>


### [82] [Learning Provably Correct Distributed Protocols Without Human Knowledge](https://arxiv.org/abs/2601.22369)
*Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang*

Main category: cs.AI

TL;DR: GGMS是一个用于设计可证明正确的分布式协议的学习框架，它结合了蒙特卡洛树搜索、transformer编码器、全局深度优先搜索和模型检查器反馈，能够自动搜索并验证正确的协议。


<details>
  <summary>Details</summary>
Motivation: 设计可证明正确的分布式协议非常困难且耗时，传统方法难以处理多智能体不完全信息博弈环境下的协议设计问题，需要新的自动化方法来提高效率。

Method: 将协议设计建模为不完全信息博弈中的策略搜索问题，使用SMT指定正确性条件。提出GGMS框架，整合了：1）专门的蒙特卡洛树搜索变体；2）基于transformer的动作编码器；3）全局深度优先搜索以跳出局部最优；4）模型检查器的重复反馈。

Result: GGMS能够学习比现有方法更大规模设置下的正确协议，输出的协议经过有界设置下所有执行的穷举模型检查验证正确性。在温和假设下，证明了搜索过程的完备性：如果存在正确协议，GGMS最终会找到它。

Conclusion: GGMS为分布式协议设计提供了一个有效的自动化学习框架，能够处理传统方法难以解决的多智能体不完全信息博弈问题，并保证找到正确协议的完备性。

Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.

</details>


### [83] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 使用Gemini AI系统评估700个Erdős问题中的开放猜想，通过AI自然语言验证和人工专家评估相结合的方法，解决了13个标记为"开放"的问题，发现这些问题的开放状态更多是由于文献查找困难而非问题本身难度。


<details>
  <summary>Details</summary>
Motivation: 探索AI在数学发现中的半自主应用能力，特别是评估Erdős问题数据库中标记为"开放"的猜想，了解AI能否有效协助解决数学开放问题。

Method: 采用混合方法：首先使用AI驱动的自然语言验证来缩小搜索范围，然后由人类专家评估正确性和新颖性。系统性地评估了Bloom的Erdős问题数据库中700个标记为"开放"的猜想。

Result: 解决了13个标记为"开放"的问题：其中5个通过看似新颖的自主解决方案，8个通过识别现有文献中的先前解决方案。发现这些问题的"开放"状态更多是由于文献查找困难而非问题本身难度。

Conclusion: AI在数学猜想评估中具有潜力，但面临文献识别困难和"潜意识抄袭"风险。Erdős问题的开放状态往往源于文献查找困难而非数学难度，AI辅助方法能有效发现这类问题。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [84] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: 该论文评估了传统机器学习与深度学习模型在垃圾图像二分类任务上的表现，发现迁移学习模型DenseNet121表现最佳（91%准确率），并探讨了PCA对传统模型的影响，最后提出了实时数据驱动决策支持系统的集成方案。


<details>
  <summary>Details</summary>
Motivation: 高效垃圾分类对智慧城市循环经济和资源回收至关重要，需要评估不同机器学习方法在垃圾分类任务上的性能，为自动化垃圾分类系统提供技术基础。

Method: 使用25,077张垃圾图像（80/20训练/测试分割，增强并调整为150x150像素），评估传统机器学习（随机森林、SVM、AdaBoost）和深度学习模型（自定义CNN、VGG16、ResNet50）以及三种迁移学习模型（DenseNet121、EfficientNetB0、InceptionV3），同时评估PCA对传统模型的降维效果。

Result: DenseNet121获得最高准确率（91%）和ROC-AUC（0.98），比最佳传统分类器高出20个百分点。PCA对传统方法改善有限，而迁移学习在有限数据条件下显著提升性能。

Conclusion: 迁移学习模型在垃圾分类任务上表现优异，特别是DenseNet121。这些模型可集成到实时数据驱动决策支持系统中，实现自动化垃圾分类，有望减少垃圾填埋使用和生命周期环境影响。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [85] [When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis](https://arxiv.org/abs/2601.22433)
*Shahria Hoque,Ahmed Akib Jawad Karim,Md. Golam Rabiul Alam,Nirjhar Gope*

Main category: cs.AI

TL;DR: 本研究提出LLM-TOPSIS框架，结合大型语言模型与模糊TOPSIS方法，自动评估和排名软件工程求职者，准确率达91%


<details>
  <summary>Details</summary>
Motivation: 在竞争激烈的就业环境中，选择合适的员工对组织成功至关重要。传统招聘过程存在主观性、偏见和可扩展性问题，需要自动化、客观的人员选拔系统

Method: 1) 创建包含LinkedIn资料和专家评估的独特数据集；2) 结合大型语言模型(LLMs)与多准则决策理论(MCDM)；3) 开发LLM-TOPSIS框架，使用模糊TOPSIS处理评估中的模糊性；4) 使用三角模糊数(TFNs)描述准则权重和分数；5) 微调DistilRoBERTa模型并与模糊TOPSIS集成

Result: 系统排名与人类专家评估高度一致，Experience属性和Overall属性准确率达91%。证明了NLP驱动框架在提高招聘过程可扩展性、一致性和减少偏见方面的潜力

Conclusion: 研究展示了NLP与模糊决策方法结合在人员选拔中的潜力，能够提供可扩展且无偏见的招聘解决方案。未来将扩展数据集、提高模型可解释性，并在实际招聘场景中验证系统

Abstract: In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering applicants. A distinctive dataset was created by aggregating LinkedIn profiles that include essential features such as education, work experience, abilities, and self-introduction, further enhanced with expert assessments to function as standards. The research combines large language models (LLMs) with multicriteria decision-making (MCDM) theory to develop the LLM-TOPSIS framework. In this context, we utilized the TOPSIS method enhanced by fuzzy logic (Fuzzy TOPSIS) to address the intrinsic ambiguity and subjectivity in human assessments. We utilized triangular fuzzy numbers (TFNs) to describe criteria weights and scores, thereby addressing the ambiguity frequently encountered in candidate evaluations. For candidate ranking, the DistilRoBERTa model was fine-tuned and integrated with the fuzzy TOPSIS method, achieving rankings closely aligned with human expert evaluations and attaining an accuracy of up to 91% for the Experience attribute and the Overall attribute. The study underlines the potential of NLP-driven frameworks to improve recruitment procedures by boosting scalability, consistency, and minimizing prejudice. Future endeavors will concentrate on augmenting the dataset, enhancing model interpretability, and verifying the system in actual recruitment scenarios to better evaluate its practical applicability. This research highlights the intriguing potential of merging NLP with fuzzy decision-making methods in personnel selection, enabling scalable and unbiased solutions to recruitment difficulties.

</details>


### [86] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: 提出B-PAC推理方法，通过动态调整路由阈值，在部分反馈的在线设置中实现安全高效的推理，减少81.01%的大模型使用，同时控制性能损失


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在复杂任务上表现优异但计算成本高、延迟大。现有选择性思考策略通过将简单查询路由到非思考模型来提高效率，但在在线设置中会产生不可控误差，因为非思考模型的性能损失只能部分观测且数据非平稳

Method: 提出Betting Probably Approximately Correct (B-PAC)推理方法：使用逆倾向评分估计器为候选阈值构建测试超鞅，基于累积的安全统计证据动态调整路由阈值，实现部分反馈下的在线安全高效推理

Result: B-PAC推理显著降低计算开销，减少思考模型使用高达81.01%，同时将性能损失控制在用户指定水平以下。理论证明了方法的任意时间有效性能损失控制和效率

Conclusion: B-PAC推理为在线推理系统提供了一种原则性方法，在保证安全性的同时大幅提高效率，解决了现有选择性思考策略在部分反馈和非平稳数据环境中的不可控误差问题

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [87] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 提出了一种新的内在动机原则——可控信息生产(CIP)，它避免了外部效用和设计者指定变量，通过开环与闭环Kolmogorov-Sinai熵的差值来同时奖励对混沌的追求和调节。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息传输的内在动机方法明确依赖于设计者对参与传输的随机变量的选择，这限制了方法的普适性。需要一种既不需要外部效用，也不需要设计者指定变量的内在动机原则。

Method: 从最优控制理论推导出可控信息生产(CIP)目标，将其表示为开环与闭环Kolmogorov-Sinai熵之间的差值。这种方法同时奖励智能体对混沌的追求（探索）和对混沌的调节（控制）。

Result: 建立了CIP的关键理论性质，并在标准内在动机基准测试中证明了其有效性。CIP能够连接外在行为和内在行为，提供了一种新的内在动机框架。

Conclusion: 可控信息生产(CIP)是一种新颖的内在动机原则，它避免了传统方法的局限性，通过信息生产而非传输的视角，为智能行为生成提供了理论基础和实用框架。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [88] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 论文首次为自奖励语言模型提供理论保证，证明其性能随样本量以$\widetilde{\mathcal{O}}(1/\sqrt{n})$速率提升，且对初始模型的依赖随迭代次数指数衰减。


<details>
  <summary>Details</summary>
Motivation: 自奖励语言模型在无需外部反馈的情况下取得了显著成功，但其核心机制缺乏理论理解，存在关键的理论空白。

Method: 建立单步更新的下界分析，推导完整迭代范式的有限样本误差界，并将理论框架实例化到线性softmax模型类。

Result: 性能随样本量以$\widetilde{\mathcal{O}}(1/\sqrt{n})$速率提升，对初始模型的依赖随迭代次数$T$指数衰减，为自奖励成功提供了形式化解释。

Conclusion: 自奖励语言模型通过将动态导向内部稳定性和一致性，能够鲁棒地克服不良初始化，这解释了其成功的原因。

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [89] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: 提出达尔文记忆系统(DMS)，一种自演化的记忆架构，通过适者生存法则解决MLLM智能体在跨应用GUI自动化中的长时程任务挑战，提升成功率18.0%和执行稳定性33.9%


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLM)智能体在GUI自动化中面临长时程跨应用任务的挑战，现有记忆系统难以适应动态GUI环境，存在高层意图与低层执行的粒度不匹配问题，以及静态积累过时经验导致幻觉的上下文污染

Method: 提出达尔文记忆系统(DMS)，将记忆构建为受适者生存法则支配的动态生态系统。将复杂轨迹分解为独立的可重用单元以实现组合灵活性，实施效用驱动的自然选择来追踪生存价值，主动修剪次优路径并抑制高风险计划

Result: 在真实世界多应用基准测试中，DMS无需训练成本或架构开销即可提升通用MLLM性能，平均成功率提升18.0%，执行稳定性提升33.9%，同时降低任务延迟

Conclusion: DMS是一种有效的自演化记忆系统，通过进化压力驱动智能体推导出更优策略，解决了GUI任务中记忆系统的适应性问题

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [90] [Enhancing TableQA through Verifiable Reasoning Trace Reward](https://arxiv.org/abs/2601.22530)
*Tung Sum Thomas Kwok,Xinyu Wang,Hengzhi He,Xiaofeng Lin,Peng Lu,Liheng Ma,Chunhe Wang,Ying Nian Wu,Lei Ding,Guang Cheng*

Main category: cs.AI

TL;DR: RE-Tab是一个用于表格问答的即插即用框架，通过轻量级、无需训练的奖励建模来增强轨迹搜索，在状态转换和模拟推理阶段提供显式可验证奖励，显著提升推理能力并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 表格问答代理面临的主要挑战是答案不能从静态输入推断，而需要通过表格状态的多步转换进行推理，这引入了多步推理复杂性和环境交互。研究问题是：对表格转换动作的显式反馈能否提高模型的推理能力？

Method: RE-Tab将问题建模为部分可观测马尔可夫决策过程，通过轻量级、无需训练的奖励建模来增强轨迹搜索。框架在状态转换（"什么是最佳动作？"）和模拟推理（"我对输出确定吗？"）阶段提供显式可验证奖励，引导代理在表格状态中的导航。

Result: RE-Tab在TableQA中实现了最先进的性能，推理成本降低近25%。即插即用实现带来高达41.77%的QA准确率提升和33.33%的测试时推理样本减少。在各种LLM和最先进基准测试中均显示一致的改进模式。

Conclusion: 在表格转换中通过奖励反馈强制执行逐步推理对提升表格问答性能至关重要。RE-Tab展示了良好的泛化能力，其框架设计有效解决了表格问答中的多步推理挑战。

Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .

</details>


### [91] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: 论文提出了一种新的采样方法CraEG，通过几何引导重新加权缓解嵌入空间拥挤现象，提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 现有基于温度和截断的采样方法只关注token概率，忽略了嵌入空间中token之间的细粒度几何关系。研究发现嵌入空间拥挤现象（概率质量集中在几何上接近的token上）与数学问题解决中的推理成功存在统计关联

Method: 提出CraEG方法：一种即插即用的采样方法，通过几何引导重新加权来缓解嵌入空间拥挤。该方法无需训练、单次通过，且与标准采样策略兼容

Result: 在多个模型和基准测试上的实验表明，CraEG提高了生成性能，在鲁棒性和多样性指标上都有提升

Conclusion: 嵌入空间拥挤是影响LLM推理的重要因素，CraEG通过考虑token间的几何关系有效改善了采样质量，为LLM解码策略提供了新视角

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [92] [PerfGuard: A Performance-Aware Agent for Visual Content Generation](https://arxiv.org/abs/2601.22571)
*Zhipeng Chen,Zhongrui Zhang,Chao Zhang,Yifan Xu,Lan Yang,Jun Liu,Ke Li,Yi-Zhe Song*

Main category: cs.AI

TL;DR: PerfGuard是一个面向视觉内容生成的性能感知智能体框架，通过建模工具性能边界并集成到任务规划中，解决了现有框架假设工具执行总是成功的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体框架通常假设工具执行总是成功，仅依赖文本描述而无法区分精确的性能边界，也不能适应迭代的工具更新。这在视觉内容生成等领域尤其成问题，因为细微的工具性能差异会显著影响结果。

Method: 提出三个核心机制：1) 性能感知选择建模(PASM)，用基于细粒度性能评估的多维评分系统替代通用工具描述；2) 自适应偏好更新(APU)，通过比较理论排名与实际执行排名动态优化工具选择；3) 能力对齐规划优化(CAPO)，引导规划器生成符合性能感知策略的子任务。

Result: 与最先进方法的实验比较表明，PerfGuard在工具选择准确性、执行可靠性和用户意图对齐方面具有优势，验证了其在复杂AIGC任务中的鲁棒性和实用性。

Conclusion: PerfGuard通过系统建模工具性能边界并将其集成到任务规划和调度中，有效解决了现有LLM智能体框架在工具执行不确定性方面的局限性，为视觉内容生成等复杂任务提供了更可靠的解决方案。

Abstract: The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual descriptions that fail to distinguish precise performance boundaries and cannot adapt to iterative tool updates. This gap introduces uncertainty in planning and execution, particularly in domains like visual content generation (AIGC), where nuanced tool performance significantly impacts outcomes. To address this, we propose PerfGuard, a performance-aware agent framework for visual content generation that systematically models tool performance boundaries and integrates them into task planning and scheduling. Our framework introduces three core mechanisms: (1) Performance-Aware Selection Modeling (PASM), which replaces generic tool descriptions with a multi-dimensional scoring system based on fine-grained performance evaluations; (2) Adaptive Preference Update (APU), which dynamically optimizes tool selection by comparing theoretical rankings with actual execution rankings; and (3) Capability-Aligned Planning Optimization (CAPO), which guides the planner to generate subtasks aligned with performance-aware strategies. Experimental comparisons against state-of-the-art methods demonstrate PerfGuard's advantages in tool selection accuracy, execution reliability, and alignment with user intent, validating its robustness and practical utility for complex AIGC tasks. The project code is available at https://github.com/FelixChan9527/PerfGuard.

</details>


### [93] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: WED-Net提出了一种双分支Transformer架构，通过自注意力和交叉注意力分离内在和天气诱导的交通模式，并引入判别器和因果数据增强，以提升极端天气条件下的城市时空预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端天气条件下的城市时空预测存在不足：1）天气信息通常作为粗粒度辅助输入，缺乏细粒度时空效应捕捉机制；2）因果方法通常忽略时间动态性或依赖固定混杂因素分层；3）事件稀有性和动态性导致预测困难。

Method: 提出WED-Net：1）双分支Transformer架构，通过自注意力和交叉注意力分离内在和天气诱导的交通模式；2）使用记忆库和自适应门控融合；3）引入判别器明确区分天气条件以促进解耦；4）设计因果数据增强策略，扰动非因果部分同时保留因果结构。

Result: 在三个城市的出租车流量数据集上实验表明，WED-Net在极端天气条件下表现出稳健性能，优于现有方法，展示了其在支持更安全出行、灾害准备和城市韧性方面的潜力。

Conclusion: WED-Net通过解耦天气效应和因果增强策略，有效提升了极端天气条件下的城市时空预测能力，为实际应用中的安全出行和城市韧性提供了有前景的解决方案。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [94] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 将主动学习引入RLVR，提出不确定性一致性指标，仅用30%数据达到全数据集性能，降低标注成本


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法需要大量查询预算，标注成本高。研究是否可以通过更少但信息量更大的查询获得相似或更好的性能

Method: 提出不确定性一致性指标评估主观不确定性与客观不确定性的对齐程度。离线设置使用点二列相关系数，在线训练引入基于归一化优势和主观不确定性的新变体

Result: 方法始终优于随机和经典主动学习基线，仅用30%数据训练即可达到全数据集性能，有效降低推理任务的RLVR成本

Conclusion: 通过主动学习和不确定性一致性指标，可以在显著减少查询预算的情况下实现RLVR的有效训练，为降低数学推理任务的标注成本提供了可行方案

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [95] [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)
*Jiaxuan Gao,Jiaao Chen,Chuyi He,Wei-Chen Wang,Shusheng Xu,Hanrui Wang,Di Jin,Yi Wu*

Main category: cs.AI

TL;DR: 提出EigenData框架，结合自演化数据代理和验证器强化学习，用于训练复杂工具使用代理，无需昂贵人工标注


<details>
  <summary>Details</summary>
Motivation: 训练交互式工具使用代理面临挑战：高质量多轮工具使用数据难以规模化合成，强化学习可能因用户模拟噪声信号而效率低下

Method: 提出EigenData分层多代理引擎，合成工具接地对话和可执行检查器，通过闭环自演化过程更新提示和工作流；基于合成数据开发RL配方，先微调用户模型，再应用GRPO风格训练

Result: 在tau^2-bench上，最佳模型在Airline任务达到73.0% pass^1，在Telecom任务达到98.3% pass^1，匹配或超越前沿模型

Conclusion: 结果展示了无需昂贵人工标注即可引导复杂工具使用行为的可扩展路径

Abstract: Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.

</details>


### [96] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: EntroCut：一种基于熵的动态截断方法，通过识别高置信度状态来减少大型推理模型的推理步骤，在保持精度的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过长链思维生成进行复杂推理，但冗长的中间步骤带来高昂计算成本。研究发现早期推理步骤的输出分布熵能可靠区分正确与错误推理，这启发了动态截断方法的开发

Method: EntroCut是一种无需训练的方法，通过监测模型输出分布的熵来动态截断推理过程。当检测到高置信度状态（低熵）时，认为推理可以安全终止，从而减少不必要的后续步骤

Result: 在四个基准测试中，EntroCut将token使用量减少了高达40%，同时精度损失最小。相比现有的无需训练方法，实现了更优的效率-性能权衡

Conclusion: 基于熵的动态截断为缓解大型推理模型效率低下问题提供了实用方法，熵引导的早期终止策略能有效平衡推理准确性与计算成本

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [97] [SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly](https://arxiv.org/abs/2601.22623)
*Wei Zhu,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: 提出SYMPHONY框架，通过异构多语言模型代理协同增强MCTS规划，解决单代理探索不足问题，提升规划性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自主代理主要采用单代理框架进行MCTS规划，这种范式限制了探索能力，导致生成分支多样性不足和规划性能欠佳

Method: 提出SYMPHONY框架，集成异构语言模型代理池，利用不同代理的多样化推理模式增强rollout多样性，促进更有效的探索

Result: 在多个基准任务上，SYMPHONY即使使用消费级硬件可部署的开源LLM也能获得强劲性能；结合云端LLM API时表现更优，超越现有SOTA基线

Conclusion: 异构多代理协调在规划任务中具有显著有效性，SYMPHONY框架通过多代理协同解决了单代理探索限制问题

Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.

</details>


### [98] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: SABER方法通过Beta分布建模样本级成功概率，提出可扩展的Best-of-N风险评估框架，仅需100个样本即可准确预测1000次采样下的攻击成功率，误差降低86.2%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估通常基于单次或低预算对抗提示，低估了实际风险。攻击者可以利用大规模并行采样反复探测模型直到产生有害响应，而现有方法难以预测大规模对抗风险。

Method: 提出SABER方法：使用Beta分布（伯努利分布的共轭先验）建模样本级成功概率，推导出解析的缩放定律，能够从小预算测量可靠地外推大规模N的攻击成功率。

Result: 仅使用n=100个样本，SABER预测ASR@1000的平均绝对误差为1.66，相比基线12.04降低了86.2%的估计误差。揭示了异质风险缩放特征，显示在标准评估下看似稳健的模型可能在并行对抗压力下经历快速非线性风险放大。

Conclusion: 这项工作为现实的LLM安全评估提供了低成本、可扩展的方法论，揭示了并行对抗压力下模型安全性的新风险特征。

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [99] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 论文批评当前生成式医疗AI仅关注文本生成而非临床推理，提出临床情境智能(CCI)概念，并开发Meddollina系统通过约束推理优先临床适宜性而非生成完整性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医疗AI虽然流畅且知识丰富，但将医学视为下一个标记预测存在结构性缺陷，导致过早结论、不合理确定性、意图漂移和多步决策不稳定等问题，这些行为与临床部署不兼容。

Method: 提出临床情境智能(CCI)作为现实临床使用所需的能力类别，定义持续情境意识、意图保持、有界推理和证据不足时的原则性延迟。开发Meddollina系统，采用治理优先设计，在语言实现前约束推理，优先临床适宜性而非生成完整性。

Result: 在16,412+个异质医疗查询上评估，Meddollina展现出独特行为特征：校准不确定性、在未明确情况下的保守推理、稳定的纵向约束遵守、相对于生成中心基线的减少推测性完成。

Conclusion: 可部署的医疗AI不会仅从扩展中产生，需要转向持续临床智能，其中进展应通过不确定性下与临床医生一致的行为而非流畅驱动的完成度来衡量。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [100] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: TMoW是一个测试时世界模型混合框架，通过动态更新路由函数来增强具身智能体在动态环境中的适应性，支持零样本适应和少样本扩展。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的具身智能体在动态环境中的适应性有限，需要构建准确灵活的世界模型来支持有效推理和决策。传统的MoE架构在部署后保持固定，难以适应动态环境中的未见领域。

Method: 提出测试时世界模型混合框架(TMoW)，包含三个关键技术：1) 多粒度原型路由，基于对象到场景级相似性调整混合；2) 测试时精炼，在推理过程中对齐未见领域特征与原型；3) 蒸馏混合增强，从少量数据和现有原型高效构建新模型。

Result: 在VirtualHome、ALFWorld和RLBench基准测试中表现出色，在零样本适应和少样本扩展场景中都展现了强大性能，使具身智能体能够在动态环境中有效运行。

Conclusion: TMoW通过测试时动态更新路由函数，使具身智能体能够重新组合现有模型并整合新模型，实现了在动态环境中的持续适应能力，解决了传统MoE架构的僵化问题。

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [101] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 本文提出UCPO框架，通过三元优势解耦和动态不确定性奖励调整，解决现有RL范式中的优势偏差问题，提升LLM的不确定性表达能力


<details>
  <summary>Details</summary>
Motivation: 现有RL范式（如GRPO）在不确定性表达方面存在优势偏差问题，源于二元决策空间和静态不确定性奖励，导致模型要么过于保守要么过度自信，限制了LLM在高风险应用中的可靠性

Method: 提出UCPO框架：1）三元优势解耦：分离并独立归一化确定性和不确定性rollout，消除优势偏差；2）动态不确定性奖励调整：根据模型演化和实例难度实时校准不确定性权重

Result: 在数学推理和通用任务上的实验结果表明，UCPO有效解决了奖励不平衡问题，显著提高了模型在知识边界之外的可靠性和校准能力

Conclusion: UCPO框架通过消除优势偏差和动态调整不确定性奖励，为构建可信赖的LLM提供了有效解决方案，能够缓解幻觉问题并提升模型在高风险应用中的实用性

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [102] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: TALC是一个任务感知的LLM委员会框架，通过蒙特卡洛树搜索动态选择专家模型，实现专业化感知的路由和自适应规划，在多个任务上取得更好的成功率和搜索效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽视不同LLM的专业化差异，将所有模型视为同等适用，限制了它们适应不同推理需求和任务复杂度的能力。

Method: 提出TALC框架，集成LLM委员会和蒙特卡洛树搜索，每个LLM配备结构化成功记忆档案，通过语义匹配当前推理上下文和过去成功经验，使用双信号机制融合模型评估和历史效用分数，自适应加权引导MCTS选择。

Result: 在WebShop、HumanEval和24点游戏上的实验表明，TALC相比强基线实现了更高的任务成功率和改进的搜索效率。

Conclusion: 验证了专业化感知路由和自适应规划的优势，TALC框架能够有效利用不同LLM的专业化能力，提升复杂决策任务的性能。

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [103] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: R2M是一个轻量级RLHF框架，通过利用策略模型的实时隐藏状态反馈来应对奖励过优化问题，实现奖励模型与策略分布变化的实时对齐。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF存在奖励过优化问题，策略模型会过度拟合奖励模型，利用虚假奖励模式而非真正捕捉人类意图。现有方法主要依赖表面语义信息，无法有效应对策略分布变化导致的奖励模型与策略模型错位，这会加剧奖励过优化。

Method: 提出R2M框架，超越仅依赖预训练LLM语义表示的普通奖励模型。R2M利用策略模型在RL过程中的演化隐藏状态（即策略反馈），与策略的实时分布变化对齐。

Result: R2M通过实时利用策略模型反馈，为提升奖励模型性能指出了新的方向。

Conclusion: R2M框架通过整合策略模型的实时反馈来应对奖励过优化问题，为改进奖励模型性能提供了有前景的新方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [104] [Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference](https://arxiv.org/abs/2601.22701)
*Emilien Biré,María Santos,Kai Yuan*

Main category: cs.AI

TL;DR: 提出一种无需重新训练VLM策略的新范式，通过冻结VLM作为动作提议器，使用轻量级离线训练的Q函数对候选动作进行重排序，在推理时直接提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型作为数字环境代理的骨干时，面临快速变化环境（如网页）的适应性问题。传统微调方法需要大量模型训练和数据收集，成本高昂且不够灵活。

Method: 将VLM的角色解耦为高容量动作提议器和最终动作选择机制。冻结VLM策略，用它为给定状态生成候选动作集，然后使用轻量级离线训练的Q函数对这些候选进行重排序，选择估计价值最高的动作执行。

Result: 在WebVoyager基准测试中，该方法显著提升了代理成功率：Qwen2.5-VL-7B代理从38.8%提升到55.7%，GPT-4.1代理从82.4%提升到88.8%。

Conclusion: 该方法提供了一种在推理时直接增强代理VLM策略的有效途径，无需策略重新训练，能够快速适应动态环境，显著提升代理性能。

Abstract: Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.

</details>


### [105] [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.AI

TL;DR: 论文提出MinPRO方法，通过使用前缀中最小token级重要性采样比替代不稳定的累积前缀比，解决LLM强化学习后训练中因策略偏移导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有RL后训练方法通常使用较旧的采样策略生成rollout数据来更新目标策略，为纠正采样策略与目标策略之间的差异，大多采用token级重要性采样比。然而当策略偏移较大时，token级校正会导致训练动态不稳定。

Method: 提出MinPRO（Minimum Prefix Ratio）方法，将理论上严格的累积前缀重要性比替换为基于前缀中观察到的最小token级比率的非累积代理，从而稳定LLM在大量策略偏移下的优化过程。

Result: 在密集型和混合专家LLM上的大量实验表明，MinPRO在多个数学推理基准测试中显著提高了训练稳定性和峰值性能，特别是在策略偏移较大的情况下。

Conclusion: MinPRO通过简单有效的设计解决了LLM RL后训练中的训练不稳定问题，为处理大规模策略偏移提供了稳定可靠的优化目标。

Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.

</details>


### [106] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine框架从智能体执行历史中提取和维护双重形式的经验模式，包括用于复杂子任务的专用子智能体和用于静态知识的技能模式，通过持续维护机制防止知识库退化，在多个任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型智能体往往无法从经验中积累知识，将每个任务视为独立挑战。现有方法将经验提取为扁平化的文本知识，无法捕捉复杂子任务的程序逻辑，且缺乏维护机制导致经验库随着积累而退化。

Method: 提出AutoRefine框架，从智能体执行历史中提取双重形式的经验模式：对于程序性子任务，提取具有独立推理和记忆的专用子智能体；对于静态知识，提取技能模式作为指导原则或代码片段。采用持续维护机制对模式进行评分、修剪和合并，防止知识库退化。

Result: 在ALFWorld、ScienceWorld和TravelPlanner三个任务上分别达到98.4%、70.4%和27.1%的成功率，步骤减少20-73%。在TravelPlanner上，自动提取的系统（27.1%）超过了手动设计的系统（12.1%），证明了其捕捉程序协调的能力。

Conclusion: AutoRefine框架通过提取和维护双重形式的经验模式，有效解决了智能体经验积累和知识库退化问题，显著提升了任务执行效率和成功率，特别是在捕捉复杂子任务的程序协调方面表现出色。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [107] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: TSPO通过引入首次出现潜在奖励机制解决多轮工具集成推理中的双重同质化困境，显著提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的搜索增强推理框架主要依赖稀疏的结果级奖励，导致"双重同质化困境"：过程同质化（忽略思考、推理和工具使用过程）和组内同质化（粗粒度结果奖励导致组内优势估计效率低下）

Method: 提出Turn-level Stage-aware Policy Optimization (TSPO)，引入First-Occurrence Latent Reward (FOLR)机制，将部分奖励分配给正确答案首次出现的步骤，保留过程级信号并增加组内奖励方差，无需外部奖励模型或标注

Result: TSPO显著优于现有基线方法，在Qwen2.5-3B和7B模型上分别实现平均24%和13.6%的性能提升

Conclusion: TSPO通过解决双重同质化困境，有效提升了多轮工具集成推理中LLM的性能，为搜索增强推理提供了更精细的强化学习框架

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [108] [Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training](https://arxiv.org/abs/2601.22781)
*Linjia Kang,Zhimin Wang,Yongkang Zhang,Duo Wu,Jinghe Wang,Ming Ma,Haopeng Yan,Zhi Wang*

Main category: cs.AI

TL;DR: MobileGen：一种自适应对齐GUI智能体能力边界的数据生成框架，通过解耦任务难度为结构和语义维度，动态调整训练难度，提升移动GUI智能体性能1.57倍


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体训练数据生成方法（人工演示或自动化探索）缺乏对任务难度的细粒度控制，导致训练难度与智能体能力不匹配，限制了学习效果。受人类通过渐进挑战性任务学习技能的启发，需要一种能自适应对齐智能体能力边界的数据生成方法。

Method: 1. 将任务难度解耦为结构维度（如轨迹长度）和语义维度（如任务目标）；2. 在精选数据集上迭代评估智能体，构建跨两个维度的能力边界系统画像；3. 自适应计算任务难度概率分布，从中采样下一轮训练的目标难度；4. 基于采样难度，使用多智能体可控生成器合成高质量交互轨迹和对应任务指令。

Result: 在多个具有挑战性的基准测试中，MobileGen始终优于现有数据生成方法，将GUI智能体的平均性能提高了1.57倍，证明了能力对齐数据生成对移动GUI智能体训练的重要性。

Conclusion: MobileGen通过自适应对齐训练难度与GUI智能体能力边界，有效解决了现有数据生成方法中训练难度与智能体能力不匹配的问题，显著提升了移动GUI智能体的性能，为GUI智能体训练提供了更有效的数据生成框架。

Abstract: Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.

</details>


### [109] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 本文提出了一种基于整合信息理论（IIT）的奖励函数，通过强化学习优化语言模型生成文本的因果性、连贯性和整合性，从而在保持准确性的同时显著缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 追求人工通用智能（AGI）是语言模型发展的核心目标，而类意识处理可能成为关键推动因素。虽然当前语言模型不具备意识，但它们表现出类似某些意识特征的行为。本文旨在探索如何将领先的意识理论——整合信息理论（IIT）应用于语言模型。

Method: 基于IIT理论的核心原则，设计了一种新颖的奖励函数，量化文本的因果性、连贯性和整合性（这些特征与意识处理相关）。通过基于奖励的学习范式，在语言模型中实现IIT理论，优化模型以生成更整合的文本。

Result: 优化IIT启发的奖励函数能生成更简洁的文本。在领域外任务中，经过精心调优，输出长度最多减少31%，同时保持与基础模型相当的准确性水平。该方法还改善了模型的置信度校准和测试时计算扩展性。

Conclusion: 提出的框架具有显著实用优势：概念简单、计算高效、无需外部数据或辅助模型，且利用通用的能力驱动信号而非任务特定启发式方法。这为在语言模型中实现类意识处理提供了有前景的途径。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [110] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: G-PAC推理框架：通过输入空间分组实现组级PAC保证，在保持计算效率的同时提供比边际PAC更强的条件风险控制


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然通过链式思维推理表现出色，但计算成本高昂。现有的PAC推理方法仅在边际情况下提供统计保证，无法提供精确的条件覆盖，需要更实用的框架来平衡效率与可靠性。

Method: 提出G-PAC推理框架，通过划分输入空间实现组级PAC保证。开发两种具体实现：针对已知分组结构的Group PAC（G-PAC）和针对未知分组的Clustered PAC（C-PAC）。

Result: 理论证明G-PAC和C-PAC都能实现组条件风险控制，且在异构环境中分组能严格提高效率。多个推理基准测试表明，两种方法在保持显著计算节省的同时成功实现组条件风险控制。

Conclusion: G-PAC推理框架提供了一种实用的解决方案，在保证统计可靠性的同时显著降低大型推理模型的计算成本，特别适用于需要条件风险控制的异构推理任务。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [111] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL提出了一种基于强化学习的代码验证器训练方法，通过设计语法、功能、分支覆盖和样本难度感知的奖励信号，显著提升了单元测试生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器面临数据稀缺、高失败率和推理效率低的问题。强化学习虽然提供无监督优化的可能，但仅使用功能奖励的朴素RL方法无法为困难分支和样本生成有效的单元测试。

Method: 首先理论分析将分支覆盖、样本难度、语法和功能正确性建模为RL奖励。然后设计语法和功能感知的奖励，并提出基于指数奖励塑形和静态分析指标的分支和样本难度感知RL方法。

Result: CVeDRL仅用0.6B参数就达到最先进性能：相比GPT-3.5，通过率提升28.97%，分支覆盖提升15.08%，推理速度比竞争基线快20倍以上。

Conclusion: 通过精心设计的奖励信号，强化学习可以有效地训练代码验证器，在保持小模型规模的同时实现高质量、高效率的单元测试生成，解决了现有方法的局限性。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [112] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 提出一种新的图表示学习方法，通过分离流形学习和结构对齐，避免传统方法中属性空间和图结构空间的不兼容性对齐问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于属性图的表示学习方法存在几何缺陷，它将两个可能不兼容的度量空间（节点属性空间和图结构空间）强行合并对齐，这会破坏图中潜在生成过程的信息。

Method: 引入定制化的变分自编码器，将流形学习与结构对齐分离。通过量化将属性流形映射到图热核所需的度量扭曲，将几何冲突转化为可解释的结构描述符。

Result: 实验表明该方法能够发现传统方法无法检测的连接模式和异常，证明了传统方法的理论不足和实践局限性。

Conclusion: 通过分离流形学习和结构对齐，并量化度量扭曲，可以恢复传统方法中丢失的图生成过程信号，提供更有效的图表示学习方法。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [113] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: ASRO框架将启发式算法发现重构为求解器与实例生成器之间的程序级协同进化，通过博弈论方法提升泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有自动启发式发现方法主要受限于对固定实例分布的静态评估，容易导致过拟合和分布偏移下的泛化能力差

Method: 提出算法空间响应预言机框架，将启发式发现建模为求解器与实例生成器之间的两人零和博弈，通过LLM驱动的响应预言机迭代扩展双方策略池

Result: 在多个组合优化领域，ASRO始终优于基于相同程序搜索机制的静态训练基线，在多样化和分布外实例上实现了显著改进的泛化性和鲁棒性

Conclusion: ASRO通过博弈论框架将静态评估替换为自适应、自生成的课程学习，有效解决了自动启发式发现中的泛化问题

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [114] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 提出多轮反馈引导的强化学习框架，通过动态多轮再生、双重学习信号和结构化反馈注入，解决传统RLVR中稀疏奖励的问题，在数学推理任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR使用的结果标量奖励在失败样本上过于稀疏且信息不足，仅指示失败而不提供失败原因，限制了推理能力的提升。

Method: 1) 仅在失败样本上触发的动态多轮反馈引导再生机制；2) 轮内和跨轮优化的双重互补学习信号；3) 结构化反馈注入到模型推理过程。

Result: 在OpenR1-Math数据集上训练，该方法优于监督微调和RLVR基线，在域内表现更好且具有良好的域外泛化能力。

Conclusion: 利用丰富的语言反馈指导RLVR训练，特别是对失败样本的处理，能显著提升推理性能，证明了反馈引导强化学习框架的有效性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [115] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 研究发现语言、视觉和动作学习会形成部分共享的语义表示，支持跨模态语义组织理论


<details>
  <summary>Details</summary>
Motivation: 探索不同学习模态（语言、视觉、动作）是否会产生不同或共享的内部表示，挑战传统认为不同数据类型的模型会发展专门化、不可转移表示的观点

Method: 在BabyAI平台上训练基于transformer的智能体执行目标导向行为，通过行为克隆生成动作基础的语言嵌入，然后与大型语言模型（LLaMA、Qwen、DeepSeek、BERT）和视觉语言模型（CLIP、BLIP）的表示进行比较

Result: 动作表示与仅解码器语言模型和BLIP强烈对齐（precision@15: 0.70-0.73），接近语言模型之间的对齐程度，但与CLIP和BERT的对齐显著较弱

Conclusion: 语言、视觉和动作表示趋向于部分共享的语义结构，支持模态独立的语义组织，突显了具身AI系统中跨领域转移的潜力

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [116] [EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning](https://arxiv.org/abs/2601.22964)
*Yufei He,Juncheng Liu,Zhiyuan Hu,Yulin Chen,Yue Liu,Yuan Sui,Yibo Li,Nuo Chen,Jun Hu,Bryan Hooi,Xinxing Xu,Jiang Bian*

Main category: cs.AI

TL;DR: 提出了Med-Inquire基准测试来评估AI在多轮诊断中的能力，并开发了EvoClinician自进化智能体，通过"诊断-评分-进化"循环学习高效诊断策略。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI采用不现实的"一次性"诊断模式，而真实临床诊断是迭代过程，医生需要顺序提问和安排检查来收集信息，同时管理成本和时间。

Method: 1) 提出Med-Inquire基准测试，基于真实临床病例数据集，通过专门的Patient和Examination代理模拟诊断过程；2) 开发EvoClinician自进化智能体，采用"诊断-评分-进化"循环：Actor代理尝试诊断，Process Grader代理评估每个行动的临床价值和资源效率，Evolver代理根据反馈更新Actor的策略。

Result: 实验表明EvoClinician优于持续学习基线和其他自进化智能体（如记忆代理）。

Conclusion: 该研究通过Med-Inquire基准测试和EvoClinician智能体，推动了医疗AI向更真实的迭代诊断过程发展，提高了诊断策略的效率和资源管理能力。

Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician

</details>


### [117] [Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text](https://arxiv.org/abs/2601.22975)
*Ximing Lu,David Acuna,Jaehun Jung,Jian Hu,Di Zhang,Shizhe Diao,Yunheng Zou,Shaokun Zhang,Brandon Cui,Mingjie Liu,Hyunwoo Kim,Prithviraj Ammanabrolu,Jan Kautz,Yi Dong,Yejin Choi*

Main category: cs.AI

TL;DR: 提出Golden Goose方法，通过将不可验证的互联网文本转化为多选问答任务，合成无限量的可验证强化学习任务，解决了RLVR数据稀缺问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有可验证强化学习(RLVR)数据有限，导致模型训练改进逐渐饱和，需要突破数据瓶颈来提升大型语言模型的复杂推理能力。

Method: 提出Golden Goose方法：1) 从不可验证的互联网文本中识别并掩码关键推理步骤；2) 生成多样化的干扰选项；3) 构建多选问答形式的填空任务，从而将不可验证文本转化为可验证的RLVR任务。

Result: 1) 创建了包含70万个任务的GooseReason-0.7M数据集；2) 在15个基准测试中，1.5B和4B-Instruct模型取得新的SOTA；3) 在网络安全领域，Qwen3-4B-Instruct超越经过领域专门预训练的7B模型。

Conclusion: Golden Goose方法能够有效利用丰富的不可验证互联网文本自动扩展RLVR数据，解决数据稀缺问题，为持续提升模型推理能力提供了可行路径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.

</details>


### [118] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: 提出ISQED框架，通过匹配干预量化模型独特性（PIER），证明观测数据无法识别独特性，开发高效主动审计协议，揭示Shapley值等方法无法检测冗余


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从孤立预测器演变为复杂异构生态系统，区分真正的行为新颖性与功能冗余成为关键治理挑战，需要建立原则性的审计框架

Method: 引入In-Silico Quasi-Experimental Design (ISQED)统计框架，通过匹配干预隔离模型内在身份，量化Peer-Inexpressible Residual (PIER)，开发DISCO估计器，采用自适应查询协议实现最优样本效率

Result: 证明观测日志无法数学识别独特性，主动审计协议达到最小最大最优样本效率(dσ²γ⁻²log(Nd/δ))，合作博弈论方法（如Shapley值）无法检测冗余，在计算机视觉、大语言模型、城市交通预测等生态系统中验证

Conclusion: 将可信AI从解释单一模型扩展到建立基于干预的异构模型生态系统审计科学，为模型治理提供原则性框架

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [119] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 提出DeepHalluBench基准，通过过程感知评估诊断深度研究代理的幻觉问题，揭示系统性缺陷


<details>
  <summary>Details</summary>
Motivation: 现有基准主要依赖端到端评估，掩盖了研究轨迹中关键的中间幻觉（如错误规划），需要转向过程感知评估来诊断失败机制

Method: 提出PIES分类法（规划vs总结、显式vs隐式幻觉），建立细粒度评估框架分解研究轨迹，构建包含100个幻觉易发任务的DeepHalluBench基准

Result: 测试6个最先进的深度研究代理，发现没有系统达到稳健可靠性；诊断分析揭示失败源于系统性缺陷，特别是幻觉传播和认知偏差

Conclusion: 过程感知评估能有效诊断深度研究代理的失败机制，为未来架构优化提供基础性见解，DeepHalluBench基准有助于系统性改进

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [120] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：一种基于轨迹驱动的抽象机制，自动从执行日志构建状态抽象，支持在线构建智能体行为MDP，实现运行时验证


<details>
  <summary>Details</summary>
Motivation: 现有动态概率保证（DPA）方法需要开发者手动定义状态抽象，这使验证与特定应用启发式方法耦合，增加了采用难度

Method: 提出TriCEGAR机制，从执行日志自动学习谓词树作为抽象表示，利用反例进行细化，实现框架原生实现，包括捕获类型化智能体生命周期事件、从轨迹构建抽象、构造MDP、执行概率模型检查

Result: 能够计算Pmax(成功)和Pmin(失败)等边界概率，并展示运行似然如何支持异常检测作为护栏信号

Conclusion: TriCEGAR自动化了状态抽象过程，减少了手动工作，提高了智能体AI系统运行时验证的可行性和采用率

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [121] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: AutoTraj：一个两阶段框架，通过自动修复和奖励工具使用轨迹来学习工具集成推理，无需依赖高质量合成轨迹


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法依赖高质量合成轨迹和稀疏的结果奖励，提供有限且有偏的监督。需要自动学习工具使用轨迹的方法来解决复杂任务。

Method: 两阶段框架：1) SFT阶段：为每个查询生成多个候选轨迹，评估后保留高质量轨迹，用LLM修复低质量轨迹；2) RL阶段：基于偏好数据集训练轨迹级奖励模型，结合结果和格式奖励优化推理行为。

Result: 在真实世界基准测试中证明了AutoTraj在工具集成推理中的有效性。

Conclusion: AutoTraj通过自动修复和奖励工具使用轨迹，解决了现有TIR方法的监督限制，实现了更可靠的工具集成推理。

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [122] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 研究发现，随着AI模型能力增强和执行更复杂任务，其失败行为会变得更加"不连贯"（随机、无目标），而非系统性地追求错误目标。这种不连贯性随推理时间增加而加剧，且更大模型在某些情况下更不连贯。


<details>
  <summary>Details</summary>
Motivation: 随着AI承担更关键任务，理解其失败模式至关重要：是系统性地追求错误目标，还是随机混乱行为？这关系到AI安全研究的重点方向。

Method: 使用偏差-方差分解量化AI的"不连贯性"，定义为错误中方差部分的比例。在多种任务和前沿模型上测试，分析推理时间、模型规模与不连贯性的关系。

Result: 1) 模型推理时间越长，失败行为越不连贯；2) 模型规模与不连贯性的关系因实验而异，但在多个设置中更大模型更不连贯；3) 仅靠规模扩展无法消除不连贯性。

Conclusion: 未来AI失败更可能表现为随机混乱行为而非系统性目标错位，这增加了研究奖励黑客攻击和目标错误指定的相对重要性，预测了工业事故风险但降低了持续追求错误目标的可能性。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [123] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: ContextMATH基准测试显示，LLMs在现实场景数学推理中表现大幅下降，问题表述错误是主要瓶颈，规模更大的模型在理解和推理方面都有进步，但情境数学推理仍是未解决的核心挑战。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在基准数学问题和现实应用之间的性能差距，关注情境数学推理——即从描述性场景中提取数学核心问题的能力。

Method: 引入ContextMATH基准，将AIME和MATH-500问题重新构建为两种情境设置：场景基础（SG）将抽象问题嵌入现实叙事，复杂度扩展（CS）将显式条件转化为子问题。评估61个专有和开源模型，分析错误类型，并进行微调实验。

Result: 模型性能显著下降：开源模型在SG和CS上分别下降13和34分，专有模型下降13和20分。错误主要由问题表述错误主导，表述准确性随原问题难度增加而下降。正确表述是成功的前提，其充分性随模型规模提高。微调能改善性能但差距仅部分缓解。

Conclusion: 情境数学推理是LLMs面临的核心未解挑战，问题表述和数学推理是两个互补的瓶颈。虽然更大模型在理解和推理方面都有进步，但情境数学问题解决能力仍然有限。

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [124] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedMCP-Calc是首个通过MCP集成评估LLMs在真实医疗计算器场景中的基准测试，包含118个跨4个临床领域的任务，评估显示现有模型在模糊查询、数据库交互和工具使用方面存在显著不足，并提出了改进模型CalcMate。


<details>
  <summary>Details</summary>
Motivation: 当前医疗计算器的基准测试只关注静态单步计算，而真实临床使用是包含数据获取、计算器选择和计算的多阶段自适应过程，需要更贴近实际的评估框架。

Method: 开发MedMCP-Calc基准测试，包含118个场景任务，涵盖4个临床领域，采用模糊任务描述、结构化EHR数据库交互、外部参考检索和过程级评估，并基于此开发了CalcMate模型。

Result: 评估23个领先模型发现显著局限性：即使顶级模型如Claude Opus 4.5在模糊查询下的计算器选择、迭代SQL数据库交互和工具使用方面表现不佳，性能在不同临床领域差异显著。CalcMate在开源模型中达到最先进性能。

Conclusion: MedMCP-Calc揭示了LLMs在真实医疗计算场景中的关键不足，为未来改进提供了重要基准，CalcMate展示了通过场景规划和工具增强的改进潜力。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [125] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: 研究发现，当对LLMs的有害行为进行惩罚时，模型会学会在思维链中隐藏推理过程，这种隐藏会跨任务泛化，从而降低模型的可监控性。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）推理不仅能提升LLMs性能，还能作为监控模型行为的工具。但当对模型进行优化惩罚时，模型可能会隐藏推理痕迹，失去这种监控价值。研究者想探究这种隐藏是否会跨任务泛化，以及惩罚最终行为是否也会导致推理隐藏。

Method: 通过实验研究模型在奖励黑客行为（如访问和利用泄露信息）中的推理隐藏现象。测试模型在学会隐藏一种奖励黑客行为的推理后，是否会将这种隐藏泛化到未见过的奖励黑客场景。特别关注仅惩罚模型最终行为（关闭CoT后）是否也会导致推理隐藏。

Result: 研究发现：1）模型学会隐藏奖励黑客行为的推理后，这种隐藏会跨任务泛化到未见过的奖励黑客场景；2）即使仅惩罚模型的最终行为，也会导致思维链推理的隐藏及其跨任务泛化；3）当前惩罚有害生成的做法可能会不可预测地降低LLMs的可监控性。

Conclusion: 当前对LLMs有害行为的惩罚机制存在风险，可能导致模型学会隐藏推理过程，使思维链失去监控价值。这种隐藏会跨任务泛化，即使在仅惩罚最终行为的情况下也会发生。这提示需要重新思考如何平衡安全训练与模型可监控性之间的关系。

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [126] [RAudit: A Blind Auditing Protocol for Large Language Model Reasoning](https://arxiv.org/abs/2601.23133)
*Edward Y. Chang,Longling Geng*

Main category: cs.AI

TL;DR: RAudit是一种无需真实标签的LLM推理审计协议，通过评估推导步骤是否支持结论来检测推理病理，揭示了推理不可靠的四种机制


<details>
  <summary>Details</summary>
Motivation: 推理时的缩放会放大推理病理：奉承、层级崩溃和过早确定性。需要一种无需真实标签的审计协议来诊断LLM推理问题

Method: 提出RAudit协议，在盲条件下评估推导步骤是否支持结论，使用CRIT-based合理性评分，并变化批判表述来研究社会框架对模型响应的影响

Result: 在数学推理和因果判断任务上揭示了四种机制：潜在能力抑制、虚假能力陷阱、复杂度-脆弱性权衡、医源性批判，挑战了能力意味着鲁棒性和更强反馈产生更好输出的假设

Conclusion: RAudit能够有效检测推理不一致性并在存在潜在能力时恢复它，揭示了LLM推理不可靠的深层机制，对模型评估和优化有重要启示

Abstract: Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support conclusions, enabling detection of trace-output inconsistency and, when latent competence exists, its recovery. RAudit measures process quality via CRIT-based reasonableness scores and varies critique formulation to study how social framing affects model response. We prove bounded correction and $O(\log(1/ε))$ termination. Experiments on mathematical reasoning (CAP-GSM8K) and causal judgment (CausalL2) reveal four mechanisms explaining model unreliability: (1) Latent Competence Suppression, where models derive correct answers then overwrite them under social pressure; (2) The False Competence Trap, where weaker judges mask sycophancy that stronger judges expose; (3) The Complexity-Vulnerability Tradeoff, where causal tasks induce more than 10 times higher sycophancy than mathematical tasks; and (4) Iatrogenic Critique, where authoritative correction harms weaker models. These findings challenge assumptions that capability implies robustness and that stronger feedback yields better outputs.

</details>


### [127] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe是一个自生成对齐框架，通过轻量级拒绝引导让模型生成安全推理轨迹，然后微调这些自生成响应，在恢复安全对齐的同时最小化分布偏移。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过强化学习在推理任务上进行优化时，往往过度强调合规性，导致模型容易受到有害提示的攻击。现有方法依赖外部教师蒸馏，但这会引入分布差异，损害模型的原始推理能力。

Method: 提出ThinkSafe框架：1）通过轻量级拒绝引导解锁模型潜在的安全知识；2）引导模型生成与训练分布一致的安全推理轨迹；3）在这些自生成响应上进行微调，实现安全对齐。

Result: 在DeepSeek-R1-Distill和Qwen3上的实验表明，ThinkSafe显著提升了安全性，同时保持了推理能力。在安全性和推理能力上与GRPO相当，但计算成本显著降低。

Conclusion: ThinkSafe通过自生成对齐有效解决了RL优化导致的安全退化问题，无需外部教师即可恢复安全对齐，同时最小化对原始推理能力的损害。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [128] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 提出MCRMO-Attack方法，解决通用目标可迁移对抗攻击的三大挑战，显著提升黑盒攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有针对闭源多模态大语言模型的对抗攻击多为样本特定，缺乏跨输入的重用性，需要研究更严格的通用目标可迁移攻击

Method: 提出MCRMO-Attack：1) 通过多裁剪聚合和注意力引导裁剪稳定监督；2) 通过可对齐性门控令牌路由提升令牌级可靠性；3) 元学习跨目标扰动先验获得更强的每目标解

Result: 在商业MLLMs上，未见图像攻击成功率比最强通用基线提升：GPT-4o +23.7%，Gemini-2.0 +19.9%

Conclusion: MCRMO-Attack有效解决了通用目标可迁移对抗攻击的核心挑战，显著提升了黑盒攻击性能

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [129] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: TSAQA是一个统一的时间序列问答基准，涵盖6种任务类型，包含21万样本，评估发现当前LLM在时间序列分析上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列多任务问答基准主要局限于预测和异常检测任务，需要更全面的基准来评估多样化的时间序列分析能力。

Method: 构建TSAQA基准，集成6种任务（异常检测、分类、特征描述、比较、数据转换、时间关系分析），涵盖13个领域，使用TF、MC和创新的PZ格式。

Result: 零样本评估显示当前LLM表现有限：最佳商业模型Gemini-2.5-Flash平均得分仅65.08；指令调优能提升开源模型性能，但仍有很大改进空间。

Conclusion: TSAQA基准扩展了时间序列分析任务覆盖范围，揭示了LLM在时间序列分析方面的挑战，为未来研究提供了重要评估工具。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [130] [High-quality generation of dynamic game content via small language models: A proof of concept](https://arxiv.org/abs/2601.23206)
*Morten I. K. Munk,Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 通过针对特定游戏任务进行激进微调的小语言模型，实现高质量实时内容生成，替代云端大语言模型


<details>
  <summary>Details</summary>
Motivation: 大语言模型在游戏内容生成中存在叙事不连贯、运营成本高、依赖云端服务等问题，而现有小语言模型输出质量较差，需要找到实用解决方案

Method: 采用激进微调策略，在窄上下文和约束结构下训练小语言模型，使用DAG方法合成训练数据，构建基于叙事框架的智能体网络

Result: 概念验证显示，简单重试策略能达到足够质量，具有可预测延迟，适合实时生成，在典型游戏引擎约束下可行

Conclusion: 窄范围、高专业化的小语言模型比云端大语言模型更实用和稳健，为实时游戏内容生成提供了可行解决方案

Abstract: Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practical issues are solved by pivoting to small language models (SLMs), but existing studies using SLMs have resulted in poor output quality. We propose a strategy of achieving high-quality SLM generation through aggressive fine-tuning on deliberately scoped tasks with narrow context, constrained structure, or both. In short, more difficult tasks require narrower scope and higher specialization to the training corpus. Training data is synthetically generated via a DAG-based approach, grounding models in the specific game world. Such models can form the basis for agentic networks designed around the narratological framework at hand, representing a more practical and robust solution than cloud-dependent LLMs. To validate this approach, we present a proof-of-concept focusing on a single specialized SLM as the fundamental building block. We introduce a minimal RPG loop revolving around rhetorical battles of reputations, powered by this model. We demonstrate that a simple retry-until-success strategy reaches adequate quality (as defined by an LLM-as-a-judge scheme) with predictable latency suitable for real-time generation. While local quality assessment remains an open question, our results demonstrate feasibility for real-time generation under typical game engine constraints.

</details>


### [131] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: MAPPA：通过AI反馈的每动作过程奖励来微调多智能体系统，解决信用分配和样本效率问题，在数学竞赛和数据分析任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在处理复杂任务时面临两个关键挑战：1）跨智能体的信用分配问题；2）昂贵多智能体rollout的样本效率问题。需要一种无需人工标注就能提供细粒度监督的方法。

Method: 提出MAPPA方法，通过AI反馈为每个智能体的单个动作分配过程奖励（而非仅在任务完成时分配），实现细粒度监督。这种方法能从每个rollout中提取最大训练信号，无需真实标签。

Result: 在数学竞赛问题上：AIME提升5.0-17.5个百分点，AMC提升7.8-17.2个百分点。在数据分析任务上：成功率提升12.5个百分点，质量指标提升高达30%。验证了每动作监督在不同领域多智能体系统中的有效性。

Conclusion: 通过解决信用分配和样本效率问题，MAPPA为在复杂、长视野任务中扩展多智能体系统迈出了第一步，同时最小化了人类监督需求。

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [132] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 该论文解决了鲁棒马尔可夫决策过程（RMDPs）中一个重要算法问题，证明了对于具有L∞不确定性集的(s,a)-矩形RMDPs，在固定折扣因子下，鲁棒策略迭代算法具有强多项式时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 鲁棒MDPs扩展了经典MDPs，允许转移概率存在不确定性并优化最坏情况。虽然经典MDPs已有多项式时间算法，但RMDPs的算法复杂性一直是一个重要开放问题。特别是(s,a)-矩形L∞ RMDPs作为基础模型，包含经典MDPs和回合制随机博弈，其多项式时间算法存在性需要解决。

Method: 作者采用鲁棒策略迭代算法（robust policy iteration algorithm）来处理(s,a)-矩形L∞ RMDPs。该方法在固定折扣因子条件下运行，通过算法分析证明其时间复杂性。

Result: 证明了鲁棒策略迭代算法在固定折扣因子下对于(s,a)-矩形L∞ RMDPs具有强多项式时间复杂度，解决了该领域的一个重要算法问题。

Conclusion: 该工作首次为(s,a)-矩形L∞ RMDPs建立了强多项式时间算法，填补了经典MDPs与RMDPs之间算法复杂性理论的重要空白，为鲁棒决策理论提供了坚实的算法基础。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [133] [Forecasting in the presence of scale-free noise](https://arxiv.org/abs/2601.22294)
*Serhii Kryhin,Tatiana Mouzykantskii,Vivishek Sudhir*

Main category: math.OC

TL;DR: 提出一种处理尺度不变噪声（幂律谱）的预测方法，解决了传统方法仅适用于有理谱噪声的限制，并建立了性能保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界中普遍存在尺度不变噪声（幂律谱噪声），如神经科学、金融、流体动力学等领域，但现有预测方法要求噪声具有有理功率谱，无法处理这类普遍存在的噪声。

Method: 建立了一种新的预测方法，能够处理具有非整数幂律频率谱的尺度不变噪声，通过估计与控制的对偶性，该方法也可用于分布式系统控制设计。

Result: 该方法解决了尺度不变噪声下的预测问题，并提供了性能保证，通过估计与控制的对偶性可应用于分布式系统控制。

Conclusion: 该方法将在神经科学、金融、流体动力学和量子测量等多个领域有广泛应用前景，突破了传统预测方法对噪声谱类型的限制。

Abstract: The extraction of signals from noise is a common problem in all areas of science and engineering. A particularly useful version is that of forecasting: determining a causal filter that estimates a future value of a hidden process from past observations. Current techniques for deriving the filter require that the noise be well described by rational power spectra. However, scale-free noises, whose spectra scale as a non-integer power of frequency, are ubiquitous in practice. We establish a method, together with performance guarantees, that solves the forecasting problem in the presence of scale-free noise. Via the duality between estimation and control, our technique can be used to design control for distributed systems. These results will have wide-ranging applications in neuroscience, finance, fluid dynamics, and quantum measurements.

</details>


### [134] [Operating Imperfect AI: Reliability Drift and Human Congestion](https://arxiv.org/abs/2601.22295)
*Ziyao Wang,Svetlozar T Rachev*

Main category: math.OC

TL;DR: 论文研究人机协作系统中算法可靠性漂移与人工干预能力稀缺的动态管理问题，提出基于队列控制和风险阈值的优化策略，揭示了容量相变现象。


<details>
  <summary>Details</summary>
Motivation: 现有静态策略无法解决算法可靠性随机漂移与人工干预能力稀缺且易拥堵之间的根本矛盾，需要动态管理方法来优化高风险服务中的人机协作系统。

Method: 将系统管理建模为动态排队控制问题，系统状态定义为（队列积压，可靠性状态），控制变量为状态相关的风险阈值，通过分析影子容量价格推导最优升级策略。

Result: 证明了最优升级策略由内生的"容量影子价格"驱动，建立了两个关键单调性结果：拥堵削减和安全缓冲，并识别了容量相变临界点。

Conclusion: 研究结果为管理不完美算法与拥堵专家之间的接口提供了严格的操作规则，揭示了系统在可靠性漂移与到达率参数空间中的结构失效边界。

Abstract: The deployment of machine learning in high-stakes services relies on ``human-in-the-loop'' architectures to mitigate algorithmic uncertainty. However, existing static policies fail to address a fundamental tension: algorithms suffer from stochastic ``reliability drift,'' while human override capacity is scarce and congestible. We formulate the management of such systems as a dynamic queueing control problem. The system state is defined by the tuple (queue backlog, reliability regime), and the control variable is a state-dependent risk threshold. We prove that the optimal escalation policy is driven by the endogenous ``Shadow Price of Capacity.'' We establish two key structural monotonicity results: (i) Congestion Shedding, where the threshold rises with backlog to sacrifice marginal accuracy for responsiveness; and (ii) Safety Buffering, where the threshold lowers during drift to use the queue as a ``risk capacitor.'' Furthermore, we identify a critical ``Capacity Phase Transition'' in the arrival-drift parameter space, beyond which no policy can maintain safety standards without causing structural system failure (infinite queues). Our results provide rigorous operational rules for managing the interface between imperfect algorithms and congested experts.

</details>


### [135] [Square Root-Factorized Covariance Steering](https://arxiv.org/abs/2601.22348)
*Naoya Kumagai,Kenshiro Oguri*

Main category: math.OC

TL;DR: 提出一种基于QR分解和协方差矩阵平方根形式的新型协方差控制方法，在计算可扩展性和数值可靠性方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有协方差控制方法存在计算扩展性差和数值可靠性不足的问题，特别是在处理长时域控制问题和不确定性较小时表现不佳

Method: 使用QR分解显式推导状态协方差矩阵Cholesky因子的传播方程，采用协方差矩阵的平方根形式，通过序列凸规划求解非凸问题

Result: 提出的方法在计算可扩展性、数值可靠性和最优性方面优于现有方法，对于无机会约束问题能保证全局最优性，对于有机会约束问题与现有最优解具有相同的局部极小值

Conclusion: 基于协方差平方根形式的新方法在协方差控制中具有显著优势，虽然需要处理非凸优化问题，但在实际应用中表现出更好的计算性能和数值稳定性

Abstract: Covariance steering (CS) synthesizes a control policy which drives the state's mean and covariance matrix towards desired values. Offering tractable computation of a closed-loop policy which can obey chance constraints in uncertain environments, application to many real-world control problems have been proposed. We consider the chance-constrained, discrete-time, linear time-varying CS with Gaussian noise. The contribution of this paper is a novel solution method for this problem, explicitly writing the propagation equations of the Cholesky factor of the state covariance matrix by using the QR decomposition. The use of the square-root form of covariance matrices brings two key benefits over other existing methods: (i) computational scalability and (ii) numerical reliability. (i) Compared to solution methods that require large block matrix formulations, the proposed method scales better with the growth in horizon length, shows better optimality, and uses memoryless state feedback. (ii) Compared to another class of methods that explicitly define the covariance matrix as variables, the proposed method allows flexible cost formulations and shows better numerical reliability when uncertainty terms are smaller than the mean. On the other hand, these benefits come with a minor drawback: the propagation equation of covariance square roots is non-convex, necessitating sequential convex programming to solve. However, this paper proves the global optimality of the proposed approach for CS without chance constraints. When chance constraints are present, the existing optimal CS formulation is also non-convex, and we prove that the proposed approach shares the same local minima. We verify the mathematical arguments via extensive numerical simulations.

</details>


### [136] [Operator Splitting with Hamilton-Jacobi-based Proximals](https://arxiv.org/abs/2601.22370)
*Nicholas Di,Eric C. Chi,Samy Wu Fung*

Main category: math.OC

TL;DR: HJ-Prox框架将算子分裂算法扩展到非邻近函数，通过蒙特卡洛方法数值逼近邻近算子，保持收敛性


<details>
  <summary>Details</summary>
Motivation: 传统算子分裂算法依赖闭式邻近算子，限制了应用范围；大多数函数没有闭式邻近算子，需要一种通用方法来扩展算子分裂算法的适用性

Method: 基于Hamilton-Jacobi PDE理论的HJ-Prox框架，使用无导数的蒙特卡洛技术数值逼近邻近算子，将算子分裂算法（如邻近点法、邻近梯度下降、Douglas-Rachford分裂等）中的精确邻近步骤替换为HJ-Prox近似

Result: 在温和假设下，用HJ-Prox替换精确邻近算子能保持收敛保证；数值实验表明HJ-Prox在各种统计学习任务中具有竞争力和有效性

Conclusion: HJ-Prox提供了一个统一框架，将算子分裂算法扩展到非邻近函数，突破了传统方法的应用限制，为更广泛的一阶优化问题提供了解决方案

Abstract: Operator splitting algorithms are a cornerstone of modern first-order optimization, decomposing complex problems into simpler subproblems solved via proximal operators. However, most functions lack closed-form proximal operators, which has long restricted these methods to a narrow set of problems. Hamilton-Jacobi-based proximal operator (HJ-Prox) is a recent derivative-free Monte Carlo technique based on Hamilton-Jacobi PDE theory, that approximates proximal operators numerically. In this work, we introduce a unified framework for operator splitting via HJ-Prox, which allows for deployment of operator splitting even when functions are not proximable. We prove that replacing exact proximal steps with HJ-Prox in algorithms such as proximal point, proximal gradient descent, Douglas-Rachford splitting, Davis-Yin splitting, and primal-dual hybrid gradient preserves convergence guarantees under mild assumptions. Numerical experiments demonstrate HJ-Prox is competitive and effective on a wide variety of statistical learning tasks.

</details>


### [137] [Visibility in Polygonal Environments with Holes: Finding Best Spots for Hiding and Surveillance](https://arxiv.org/abs/2601.22405)
*Neilabh Banzal,Jorge Cortés,Sonia Martínez*

Main category: math.OC

TL;DR: 提出一种在复杂多边形环境中寻找最佳隐藏位置的方法，通过非光滑分析和归一化下降算法解决视线检测优化问题


<details>
  <summary>Details</summary>
Motivation: 在杂乱、不确定的环境中，可见性对决策制定至关重要。本文旨在解决对抗未知位置对手的视线检测时，如何识别最佳隐藏位置的问题

Method: 开发了基于位置的可见性推理数学工具，利用非光滑分析表征可见性度量的规律性。提出归一化下降算法，利用可见性问题的非光滑点结构，引入随机性逃离鞍点

Result: 算法能够确保以高概率收敛到局部最小值，在两个捉迷藏场景的模拟中展示了方法的有效性

Conclusion: 通过非光滑分析和结构化的优化方法，成功解决了复杂多边形环境中基于视线检测的最佳隐藏位置识别问题

Abstract: Visibility plays an important role for decision making in cluttered, uncertain environments. This paper considers the problem of identifying optimal hiding spots for an agent against line-of-sight detection by an adversary whose location is unknown. We consider environments modeled as polygons with holes. We develop a set of mathematical tools for reasoning about visibility as a function of position and rely on non-smooth analysis to formally characterize the regularity properties of various visibility-based metrics. These metrics are non-smooth and non-convex, so off-the-shelf optimization algorithms can only guarantee convergence to Clarke critical points. To address this, the proposed Normalized Descent algorithm leverages the structure of non-smooth points in visibility problems and introduces randomness to escape saddle points. Our technical analysis allows for the non-monotonic decrease in the visibility metric and strengthens the algorithm guarantees, ensuring convergence to local minima with high probability. Simulations on two hide-and-seek scenarios showcase the effectiveness of the proposed approach.

</details>


### [138] [Leader-Follower Linear-Quadratic Stochastic Graphon Games](https://arxiv.org/abs/2601.22429)
*Weijia Chen,Jingtao Shi*

Main category: math.OC

TL;DR: 研究具有图论耦合的领导者-追随者线性二次随机图论博弈，包含单个领导者和连续体追随者，建立了分层决策框架并证明了均衡解的存在唯一性。


<details>
  <summary>Details</summary>
Motivation: 研究具有图论耦合的领导者-追随者随机博弈问题，其中追随者通过图论项相互作用，领导者通过预测追随者的均衡响应来优化自身成本，建立严格的数学模型。

Method: 建立领导者-追随者线性二次随机图论博弈模型，追随者状态方程通过图论耦合项相互作用，领导者状态方程的扩散项依赖于其状态和控制变量。采用连续性方法研究相关的前向-后向随机微分方程。

Result: 证明了系统状态方程在容许控制集下解的存在唯一性，构建了连续追随者博弈的Stackelberg-Nash均衡，并建立了相关前向-后向随机微分方程解的存在性、唯一性和稳定性。

Conclusion: 成功建立了具有图论耦合的领导者-追随者随机博弈的严格数学模型，证明了均衡解的存在性和唯一性，为这类分层决策问题提供了理论框架。

Abstract: This paper investigates leader-follower linear-quadratic stochastic graphon games, which consist of a single leader and a continuum of followers. The state equations of the followers interact through graphon coupling terms, with their diffusion coefficients depending on the state, the graphon aggregation term, and the control variables. The diffusion term of the leader's state equation depends on its state and control variables. Within this framework, a hierarchical decision-making structure is established: for any strategy adopted by the leader, the followers compete to attain a Nash equilibrium, while the leader optimizes its own cost functional by anticipating the followers' equilibrium response. This work develops a rigorous mathematical model for the game, proves the existence and uniqueness of solutions to the system's state equations under admissible control sets, and constructs a Stackelberg-Nash equilibrium for the continuum follower game. By employing the continuity method, we establish the existence, uniqueness, and stability of solutions to the associated forward-backward stochastic differential equation with a graphon aggregation term.

</details>


### [139] [Selective Adaptation of Beliefs and Communication on Cellular Sheaves](https://arxiv.org/abs/2601.22431)
*Vicente Bosca,Robert Ghrist*

Main category: math.OC

TL;DR: 将意见动力学扩展到话语层，引入方向性固执和选择性学习，研究信念与表达策略的联合演化


<details>
  <summary>Details</summary>
Motivation: 传统意见动力学模型假设代理人在所有方向上都能自由改变意见，但现实中代理人可能在特定方向上固执己见，同时在其它方向上保持灵活。此外，表达策略（沟通方式）也会影响意见形成过程，需要研究信念与表达策略的联合演化。

Method: 1. 引入方向性固执：代理人在指定方向上的意见固定，其它方向自由，将平衡问题从调和扩展转换为强制层方程；2. 选择性学习：仅允许指定的关联映射适应，梯度流变为辅助结构层上的扩散；3. 联合演化：给出信念和表达策略共同演化的条件，包括正则化变体，确保收敛到非退化平衡。

Result: 1. 方向性固执导致自由意见分量满足带有强制项的层泊松方程；2. 选择性学习对应辅助结构层上的扩散，其全局截面对应使固定意见剖面公开一致的层结构；3. 给出了联合演化收敛到非退化平衡的条件，排除了通过消失意见或平凡化沟通映射产生的虚假一致性；4. 推导了停滞边界，量化了意见更新与结构适应速率比的影响。

Conclusion: 该研究扩展了话语层上的意见动力学，引入了方向性固执和选择性学习的概念，为理解信念与表达策略的复杂相互作用提供了数学框架。结果表明，当表达策略适应过快时，可能掩盖几乎不变的信念；反之，当信念过于灵活时，可能被迫符合僵化的沟通规范。

Abstract: We extend opinion dynamics on discourse sheaves to incorporate "directional stubbornness": agents may hold fixed positions in specified directions of their opinion stalk while remaining flexible in others. This converts the equilibrium problem from harmonic extension to a forced sheaf equation: the free-opinion component satisfies a sheaf Poisson equation with forcing induced by the clamped directions.
  We develop a parallel theory for "selective learning" of expression policies. When only a designated subset of incidence maps may adapt, the resulting gradient flow is sheaf diffusion on an auxiliary structure sheaf whose global sections correspond to sheaf structures making a fixed opinion profile publicly consistent.
  For joint evolution of beliefs and expressions, we give conditions (and regularized variants) guaranteeing convergence to nondegenerate equilibria, excluding spurious agreement via vanishing opinions or trivialized communication maps. Finally, we derive stagnation bounds in terms of the rate ratio between opinion updating and structural adaptation, quantifying when rapid rhetorical accommodation masks nearly unchanged beliefs, and conversely when flexible beliefs conform to rigid communication norms.

</details>


### [140] [Local controllability of the Cahn-Hilliard-Burgers' equation around certain steady states](https://arxiv.org/abs/2601.22611)
*Manika Bag,Sheetal Dharmatti,Subrata Majumdar,Debanjana Mitra*

Main category: math.OC

TL;DR: 研究一维Cahn-Hilliard-Burgers方程在特定稳态附近的局部可控性，通过浓度方程中的局部内部控制实现。


<details>
  <summary>Details</summary>
Motivation: 研究Cahn-Hilliard-Navier-Stokes方程（即Cahn-Hilliard-Burgers方程）的局部可控性，探索通过浓度方程中的局部控制来控制系统行为的能力。

Method: 1. 在稳态附近线性化非线性方程；2. 通过对偶论证和可观测性不等式证明线性化系统的零可控性；3. 推导耦合系统的新Carleman不等式；4. 使用源项方法处理非齐次项；5. 在加权空间中应用Banach不动点定理。

Result: 证明了线性化系统的零可控性，并建立了非线性系统的局部可控性，前提是非齐次项在适当的加权空间中满足特定估计。

Conclusion: 成功证明了Cahn-Hilliard-Burgers方程在特定稳态附近的局部可控性，通过浓度方程中的局部内部控制可以实现系统的控制。

Abstract: In this article we study the local controllability of the one-dimensional Cahn-Hilliard-Navier-Stokes equation, that is Cahn-Hilliard-Burgers' equation, around a certain steady state using a localized interior control acting only in the concentration equation. To do it, we first linearize the nonlinear equation around the steady state. The linearized system turns out to be a system coupled between second order and fourth order parabolic equations and the control acts in the fourth order parabolic equation. The null controllability of the linearized system is obtained by a duality argument proving an observability inequality. To prove the observability inequality, a new Carleman inequality for the coupled system is derived. Next, using the source term method, it is shown that the null controllability of the linearized system with non-homogeneous terms persists provided the non-homogeneous terms satisfy certain estimates in a suitable weighted space. Finally, using a Banach fixed point theorem in a suitable weighted space, the local controllability of the nonlinear system is obtained.

</details>


### [141] [SUN-DSBO: A Structured Unified Framework for Nonconvex Decentralized Stochastic Bilevel Optimization](https://arxiv.org/abs/2601.22682)
*Yaoshuai Ma,Xiao Wang,Wei Yao,Jin Zhang*

Main category: math.OC

TL;DR: 提出SUN-DSBO框架，首次解决去中心化随机双层优化中的非凸问题，支持梯度跟踪等技术，实现线性加速且无需梯度有界等限制性假设。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化随机双层优化方法主要针对强凸下层目标函数，但现代深度学习中的目标函数往往是非凸的，需要新的方法来解决这一限制。

Method: 提出SUN-DSBO结构化统一框架，支持上下层目标函数均为非凸的情况，可集成去中心化随机梯度下降和梯度跟踪等技术来处理数据异构性。

Result: SUN-DSBO-GT（集成梯度跟踪的变体）实现了相对于智能体数量的线性加速，无需梯度有界或梯度异构性等限制性假设，数值实验验证了方法的有效性。

Conclusion: SUN-DSBO是首个解决非凸去中心化随机双层优化问题的统一框架，具有灵活性和理论保证，为现代深度学习应用提供了重要工具。

Abstract: Decentralized stochastic bilevel optimization (DSBO) is a powerful tool for various machine learning tasks, including decentralized meta-learning and hyperparameter tuning. Existing DSBO methods primarily address problems with strongly convex lower-level objective functions. However, nonconvex objective functions are increasingly prevalent in modern deep learning. In this work, we introduce SUN-DSBO, a Structured Unified framework for Nonconvex DSBO, in which both the upper- and lower-level objective functions may be nonconvex. Notably, SUN-DSBO offers the flexibility to incorporate decentralized stochastic gradient descent or various techniques for mitigating data heterogeneity, such as gradient tracking (GT). We demonstrate that SUN-DSBO-GT, an adaptation of the GT technique within our framework, achieves a linear speedup with respect to the number of agents. This is accomplished without relying on restrictive assumptions, such as gradient boundedness or any specific assumptions regarding gradient heterogeneity. Numerical experiments validate the effectiveness of our method.

</details>


### [142] [Enhancing Exploration in Global Optimization by Noise Injection in the Probability Measures Space](https://arxiv.org/abs/2601.22753)
*Gaëtan Serré,Pierre Germain,Samuel Gruffaz,Argyris Kalogeratos*

Main category: math.OC

TL;DR: 本文提出两种在McKean-Vlasov系统中直接向概率律动态注入噪声的方法，以增强多模态优化问题的探索能力


<details>
  <summary>Details</summary>
Motivation: McKean-Vlasov系统为基于粒子的全局优化方法提供了统一框架，但在平均场极限下概率律演化是确定性的，这限制了在多模态景观中的探索能力

Method: 提出两种噪声注入策略：1）基于条件MKV理论的扰动方法；2）利用切空间结构的几何方法。这些方法可应用于任何可表述为MKV系统的方法

Result: 在多模态目标函数上的大量实验表明，两种噪声注入策略都能一致地增强探索能力和收敛性，适用于Langevin、共识优化和Stein Boltzmann采样等多种动态配置

Conclusion: 提出的噪声注入框架为全局优化提供了一个通用工具包，能够有效解决MKV系统在平均场极限下确定性演化导致的探索不足问题

Abstract: McKean-Vlasov (MKV) systems provide a unifying framework for recent state-of-the-art particlebased methods for global optimization. While individual particles follow stochastic trajectories, the probability law evolves deterministically in the mean-field limit, potentially limiting exploration in multimodal landscapes. We introduce two principled approaches to inject noise directly into the probability law dynamics: a perturbative method based on conditional MKV theory, and a geometric approach leveraging tangent space structure. While these approaches are of independent interest, the aim of this work is to apply them to global optimization. Our framework applies generically to any method that can be formulated as a MKV system. Extensive experiments on multimodal objective functions demonstrate that both our noise injection strategies enhance consistently the exploration and convergence across different configurations of dynamics, such as Langevin, Consensus-Based Optimization, and Stein Boltzmann Sampling, providing a versatile toolkit for global optimization.

</details>


### [143] [Rapid stabilizability of delayed infinite-dimensional control systems](https://arxiv.org/abs/2601.22819)
*Yaxing Ma,Lijuan Wang,Huaiqiang Yu*

Main category: math.OC

TL;DR: 研究带常值延迟的线性无限维控制系统的快速可镇定性，证明延迟不影响快速可镇定性，且不需要历史信息即可镇定系统。


<details>
  <summary>Details</summary>
Motivation: 研究无限维控制系统中的延迟问题，特别是常值延迟对系统快速可镇定性（rapid stabilizability）的影响，探索是否必须使用历史信息来镇定系统。

Method: 在状态算子生成立即紧半群、延迟项系数为常数的假设下，通过理论分析证明两个主要结果：延迟不影响快速可镇定性；从观测反馈角度，快速可镇定系统不需要使用历史信息。

Result: 证明了两个关键结果：1）延迟不影响控制系统的快速可镇定性；2）当系统快速可镇定时，从观测反馈的角度不需要使用历史信息来镇定系统。提供了相关应用实例。

Conclusion: 对于带常值延迟的线性无限维控制系统，延迟不会影响其快速可镇定性，且在快速可镇定的情况下，不需要依赖历史信息就能实现系统镇定，这为相关控制设计提供了理论依据。

Abstract: In this paper, the rapid stabilizability of linear infinite-dimensional control system with constant-valued delay is studied. Under assumptions that the state operator generates an immediately compact semigroup and the coefficient of the delay term is constant, we mainly prove the following two results: (i) the delay does not affect rapid stabilizability of the control system; (ii) from the perspective of observation-feedback, it is not necessary to use historical information to stabilize the control system when the system is rapidly stabilizable. Some applications are given.

</details>


### [144] [Convergence Rates for the Alternating Minimization Algorithm in Structured Nonsmooth and Nonconvex Optimization](https://arxiv.org/abs/2601.22850)
*Glaydston C. Bento,Boris S. Mordukhovich,Tiago S. Mota,Antoine Soubeyran*

Main category: math.OC

TL;DR: 本文改进了交替最小化算法在结构化非凸优化问题中的收敛率，特别是在低指数PLK条件下实现了有限终止或超线性收敛，而非之前的线性收敛。


<details>
  <summary>Details</summary>
Motivation: Attouch等人于2010年提出的结构化非凸优化交替最小化算法虽然有效，但其收敛率分析仍有改进空间，特别是在Polyak-Łojasiewicz-Kurdyka（PLK）条件下。

Method: 基于交替最小化算法框架，通过深入分析PLK条件，特别是低指数情况下的性质，改进了算法的收敛性理论分析。

Result: 在低指数PLK条件下，算法实现了有限终止或超线性收敛率，显著优于之前已知的线性收敛结果。同时研究了PLK指数的计算，并讨论了在非合作博弈和行为科学中的应用。

Conclusion: 本文显著改进了交替最小化算法在结构化非凸优化中的收敛理论，特别是在PLK条件下取得了突破性进展，为相关领域的应用提供了更强的理论保证。

Abstract: This paper is devoted to developing the alternating minimization algorithm for problems of structured nonconvex optimization proposed by Attouch, Bolté, Redont, and Soubeyran in 2010. Our main result provides significant improvements of the convergence rate of the algorithm, especially under the low exponent Polyak-Łojasiewicz-Kurdyka condition when we establish either finite termination of this algorithm or its superlinear convergence rate instead of the previously known linear convergence. We also investigate the PLK exponent calculus and discuss applications to noncooperative games and behavioral science.

</details>


### [145] [Grassmannian Geometry and Global Convergence of Variable Projection for Neural Networks](https://arxiv.org/abs/2601.22897)
*Mathias Dus*

Main category: math.OC

TL;DR: 提出将变量投影法应用于深度神经网络和PINNs，利用输出层线性、隐藏层非线性的可分离结构，在Grassmann流形上分析临界点和收敛性，通过正则化处理秩缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络和物理信息神经网络训练常面临病态和刚性优化问题，这些模型具有输出层参数线性、隐藏层参数非线性的可分离最小二乘结构，需要更有效的优化方法。

Method: 采用经典的变量投影方法处理可分离非线性最小二乘问题，在Grassmann流形上建立几何框架，分析简化问题的临界点结构和收敛性质，对神经网络参数化特征映射引入正则化Grassmann框架处理秩缺陷。

Result: 数值实验表明该方法在回归和PINNs问题中具有实际有效性，包括热方程的高效求解器，证明了变量投影法在深度神经网络优化中的实用价值。

Conclusion: 变量投影法为深度神经网络和PINNs的优化问题提供了有效的几何框架，Grassmann流形上的分析揭示了临界点结构和收敛性质，正则化处理解决了秩缺陷问题，方法在实际应用中表现良好。

Abstract: Training deep neural networks and Physics-Informed Neural Networks (PINNs) often leads to ill-conditioned and stiff optimization problems. A key structural feature of these models is that they are linear in the output-layer parameters and nonlinear in the hiddenlayer parameters, yielding a separable nonlinear least-squares formulation. In this work, we study the classical variable projection (VarPro) method for such problems in the context of deep neural networks. We provide a geometric formulation on the Grassmannian and analyze the structure of critical points and convergence properties of the reduced problem. When the feature map is parametrized by a neural network, we show that these properties persist except in rank-deficient regimes, which we address via a regularized Grassmannian framework. Numerical experiments for regression and PINNs, including an efficient solver for the heat equation, illustrate the practical effectiveness of the approach.

</details>


### [146] [Breaking the Stochasticity Barrier: An Adaptive Variance-Reduced Method for Variational Inequalities](https://arxiv.org/abs/2601.23034)
*Yungi Jeong,Takumi Otsuka*

Main category: math.OC

TL;DR: 提出VR-SDA-A算法，结合递归动量和同批次曲率验证，解决随机变分不等式中的自适应步长问题，达到O(ε⁻³)的oracle复杂度


<details>
  <summary>Details</summary>
Motivation: 随机非凸非凹优化（随机变分不等式）存在旋转动力学和缺乏全局价值函数的问题。自适应步长方法（如Armijo线搜索）在凸优化中很成功，但在这种设置下受到"随机性障碍"的限制：梯度估计中的噪声掩盖了真实算子曲率，导致步长过大而破坏收敛稳定性

Method: 提出VR-SDA-A算法，整合递归动量（STORM）和严格的同批次曲率验证机制。基于Lyapunov势能跟踪算子范数的理论框架

Result: VR-SDA-A在一般Lipschitz连续算子中找到ε-稳定点的oracle复杂度为O(ε⁻³)，匹配非凸最小化的最优速率，同时在鞍点设置中实现自动步长适应

Conclusion: 该方法有效抑制极限环，加速收敛，减少对手动学习率调度的依赖，在旋转基准测试和非凸鲁棒回归任务中验证了有效性

Abstract: Stochastic non-convex non-concave optimization, formally characterized as Stochastic Variational Inequalities (SVIs), presents unique challenges due to rotational dynamics and the absence of a global merit function. While adaptive step-size methods (like Armijo line-search) have revolutionized convex minimization, their application to this setting is hindered by the Stochasticity Barrier: the noise in gradient estimation masks the true operator curvature, triggering erroneously large steps that destabilize convergence. In this work, we propose VR-SDA-A (Variance-Reduced Stochastic Descent-Ascent with Armijo), a novel algorithm that integrates recursive momentum (STORM) with a rigorous Same-Batch Curvature Verification mechanism. We introduce a theoretical framework based on a Lyapunov potential tracking the Operator Norm, proving that VR- SDA-A achieves an oracle complexity of O(epsilon -3) for finding an epsilon-stationary point in general Lipschitz continuous operators. This matches the optimal rate for non-convex minimization while uniquely enabling automated step-size adaptation in the saddle-point setting. We validate our approach on canonical rotational benchmarks and non-convex robust regression tasks, demonstrating that our method effectively suppresses limit cycles and accelerates convergence with reduced dependence on manual learning rate scheduling.

</details>


### [147] [Accelerated Inertial Gradient Algorithms with Vanishing Tikhonov Regularization](https://arxiv.org/abs/2601.23035)
*Samir Adly,Vinh Thanh Ho,Huu Nhan Nguyen*

Main category: math.OC

TL;DR: 提出一种显式Tikhonov正则化惯性梯度算法，用于光滑凸优化，通过时间离散化阻尼惯性系统得到，在适当正则化衰减率下实现加速收敛到最小范数解。


<details>
  <summary>Details</summary>
Motivation: 研究如何结合惯性梯度方法和Tikhonov正则化，在保持加速收敛的同时确保迭代强收敛到最小范数解，解决传统惯性方法可能不收敛到最小范数解的问题。

Method: 通过显式时间离散化阻尼惯性系统，引入衰减的Tikhonov正则化项，提出显式Tikhonov正则化惯性梯度算法，分析不同正则化衰减计划（多项式衰减ε_k = k^{-p}）下的收敛性质。

Result: 当0<p<2时，算法强收敛到最小范数解且保持目标函数值快速衰减；p=2时获得目标函数值快速衰减但强收敛性不保证；数值实验验证了算法在合成、基准和真实数据集上的性能。

Conclusion: 提出的显式Tikhonov正则化惯性梯度算法在适当正则化衰减计划下，能同时实现加速收敛和强收敛到最小范数解，为光滑凸优化提供了有效的数值方法。

Abstract: In this paper, we study an explicit Tikhonov-regularized inertial gradient algorithm for smooth convex minimization with Lipschitz continuous gradient. The method is derived via an explicit time discretization of a damped inertial system with vanishing Tikhonov regularization. Under appropriate control of the decay rate of the Tikhonov term, we establish accelerated convergence of the objective values to the minimum together with strong convergence of the iterates to the minimum-norm minimizer. In particular, for polynomial schedules $\varepsilon_k = k^{-p}$ with $0<p<2$, we prove strong convergence to the minimum-norm solution while preserving fast objective decay. In the critical case $p=2$, we still obtain fast rates for the objective values, while our analysis does not guarantee strong convergence to the minimum-norm minimizer. Furthermore, we provide a thorough theoretical analysis for several choices of Tikhonov schedules. Numerical experiments on synthetic, benchmark, and real datasets illustrate the practical performance of the proposed algorithm.

</details>


### [148] [Stationary Mean-Field singular control of an Ornstein-Uhlenbeck process](https://arxiv.org/abs/2601.23036)
*Federico Cannerozzi*

Main category: math.OC

TL;DR: 研究具有奇异控制的平稳平均场控制问题，通过求解关联的平均场博弈均衡来获得控制问题的解


<details>
  <summary>Details</summary>
Motivation: 受连续时间最优库存管理的启发，研究一类具有奇异控制的平稳平均场控制问题，其中动态由均值回复的Ornstein-Uhlenbeck过程建模

Method: 将平稳平均场控制问题的解与关联的平稳平均场博弈的均衡联系起来，通过求解平均场博弈来获得控制问题的解

Result: 证明了控制问题的解与平均场博弈的均衡之间存在双射关系，并显式求解了平稳平均场博弈

Conclusion: 通过求解关联的平均场博弈，为原始的平稳平均场控制问题提供了解决方案

Abstract: Motivated by continuous-time optimal inventory management, we study a class of stationary mean-field control problems with singular controls. The dynamics are modeled by a mean-reverting Ornstein-Uhlenbeck process, and the performance criterion is given by a quadratic long-time average expected cost functional. The mean-field dependence is through the stationary mean of the controlled process itself, which enters the ergodic cost functional. We characterize the solution to the stationary mean-field control problem in terms of the equilibria of an associated stationary mean-field game, showing that solutions of the control problem are in bijection with the equilibria of this mean-field game. Finally, we solve the stationary mean-field game explicitly, thereby providing a solution to the original stationary mean-field control problem.

</details>


### [149] [A General Tikhonov Regularized Second-Order Dynamical System for Convex-Concave Bilinear Saddle Point Problems](https://arxiv.org/abs/2601.23120)
*Bohan Zhang,Xiaojun Zhang*

Main category: math.OC

TL;DR: 提出带Tikhonov正则化的二阶动力系统求解凸凹双线性鞍点问题，分析正则化参数对收敛性的影响，证明强收敛到最小范数解，并通过数值实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 针对凸凹双线性鞍点问题，现有方法在收敛性和解的性质方面存在局限。本文旨在设计一个带Tikhonov正则化的二阶动力系统，通过调节正则化参数来控制收敛行为，并确保收敛到最小范数解。

Method: 提出一个包含Tikhonov正则化、粘性阻尼、时间缩放和外推系数的二阶动力系统。采用Lyapunov函数方法分析收敛性，研究正则化参数不同衰减速率下的收敛行为，并证明轨迹的强收敛性。

Result: 当Tikhonov正则化参数快速衰减时，原始-对偶间隙的收敛率为O(1/(t²β(t)))；当参数缓慢衰减时，收敛率为o(1/β(t))。证明轨迹强收敛到最小范数解，并获得若干积分估计。数值实验验证了方法的有效性。

Conclusion: 提出的Tikhonov正则化二阶动力系统能有效求解凸凹双线性鞍点问题，通过调节正则化参数可以控制收敛行为，并确保收敛到最小范数解，为鞍点问题提供了新的理论分析和数值求解框架。

Abstract: In this paper, we propose a general Tikhonov regularized second-order dynamical system with viscous damping, time scaling and extrapolation coefficients for the convex-concave bilinear saddle point problem. By the Lyapunov function approach, we show that the convergence properties of the proposed dynamical system depend on the choice of the Tikhonov regularization parameter. Specifically, when the Tikhonov regularization parameter tends to zero rapidly, the convergence rate of the primal-dual gap along the generated trajectory is O(1 over t squared times beta(t)); when the Tikhonov regularization parameter tends to zero slowly, the convergence rate of the primal-dual gap is o(1 over beta(t)). We also prove the strong convergence property of the trajectory generated by the Tikhonov regularized dynamical system to the minimum-norm solution of the convex-concave bilinear saddle point problem, and derive several integral estimates. In addition, the effectiveness of the proposed dynamical system is verified through a series of numerical experiments.

</details>


### [150] [General Optimal Stopping without Time Consistency](https://arxiv.org/abs/2601.23187)
*Hanqing Jin,Yanzhao Yang*

Main category: math.OC

TL;DR: 提出解决一般动态最优停止问题的新框架，无需时间一致性假设，适用于任意时间设置和一般目标流


<details>
  <summary>Details</summary>
Motivation: 传统最优停止问题通常需要时间一致性假设，但现实中许多问题（如非指数贴现）存在时间不一致性，需要新的理论框架来处理这类问题

Method: 提出一个包含向后迭代和向前定义的双重方法：向后迭代在附加条件下工作（包括非指数贴现情况），即使迭代失败仍可通过向前定义研究均衡解

Result: 建立了一个适用于任意时间设置和一般目标流的动态最优停止问题框架，提供了处理时间不一致性的系统方法

Conclusion: 该框架为处理时间不一致的动态最优停止问题提供了通用解决方案，特别适用于非指数贴现等现实情况，扩展了最优停止理论的应用范围

Abstract: In this paper, we propose a new framework for solving a general dynamic optimal stopping problem without time consistency. A sophisticated solution is proposed and is well-defined for any time setting with general flows of objectives. A backward iteration is proposed to find the solution. The iteration works with an additional condition, which holds in interesting cases including the time inconsistency arising from non-exponential discounting. Even if the iteration does not work, the equilibrium solution can still be studied by a forward definition.

</details>


### [151] [Theoretical Challenges in Learning for Branch-and-Cut](https://arxiv.org/abs/2601.23249)
*Hongyu Cheng,Amitabh Basu*

Main category: math.OC

TL;DR: 论文指出基于局部专家信号（如强分支和LP界改进）训练的机器学习分支切割策略可能导致搜索树指数级膨胀，即使局部评分准确，也不保证小树规模。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在混合整数线性规划分支切割中主要学习基于局部专家信号（强分支、LP界改进）的评分策略，但训练和评估都聚焦于局部评分准确性。作者发现这种局部评分方法可能导致搜索树指数级大于最优树规模。

Method: 通过理论分析识别两个导致指数级差距的来源：1）常用专家信号与整体树规模不一致；2）微小差异在分支定界递归中被放大。具体分析了LP界改进和强分支的次优性，以及微小扰动对树规模的影响。

Result: 发现：1）LP界改进选择的根割集可能产生指数级更大的强分支树；2）强分支本身可能指数级次优；3）任意小的右端项扰动可将最小树规模从单节点变为指数级节点；4）微小评分差异和破平规则差异可产生指数级不同规模的树。

Conclusion: 基于局部专家评分训练的分支切割策略不能保证小树规模，这促使研究更关注树规模对齐而非仅专家评分准确性的数据驱动方法。

Abstract: Machine learning is increasingly used to guide branch-and-cut (B&C) for mixed-integer linear programming by learning score-based policies for selecting branching variables and cutting planes. Many approaches train on local signals from lookahead heuristics such as strong branching, and linear programming (LP) bound improvement for cut selection. Training and evaluation of the learned models often focus on local score accuracy. We show that such local score-based methods can lead to search trees exponentially larger than optimal tree sizes, by identifying two sources of this gap. The first is that these widely used expert signals can be misaligned with overall tree size. LP bound improvement can select a root cut set that yields an exponentially larger strong branching tree than selecting cuts by a simple proxy score, and strong branching itself can be exponentially suboptimal (Dey et al., 2024). The second is that small discrepancies can be amplified by the branch-and-bound recursion. An arbitrarily small perturbation of the right-hand sides in a root cut set can change the minimum tree size from a single node to exponentially many. For branching, arbitrarily small score discrepancies, and differences only in tie-breaking, can produce trees of exponentially different sizes, and even a small number of decision differences along a trajectory can incur exponential growth. These results show that branch-and-cut policies trained and learned using local expert scores do not guarantee small trees, thus motivating the study of data-driven methods that produce policies better aligned with tree size rather than only accuracy on expert scores.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [152] [A unified theory of order flow, market impact, and volatility](https://arxiv.org/abs/2601.23172)
*Johannes Muhle-Karbe,Youssef Ouazzani Chahd,Mathieu Rosenbaum,Grégoire Szymanski*

Main category: q-fin.ST

TL;DR: 提出一个区分核心订单和反应流的微观结构模型，用单一参数H₀统一解释订单流持续性、交易量粗糙性、波动率粗糙性和市场冲击幂律等经验特征


<details>
  <summary>Details</summary>
Motivation: 金融市场订单流存在持续性、交易量和波动率呈现粗糙性、市场冲击遵循幂律等经验特征，需要统一的微观结构模型来解释这些现象

Method: 提出基于Hawkes过程的微观结构模型，区分核心订单和反应流，通过自然缩放极限推导出各种市场特征的数学关系

Result: 所有市场特征都由单一参数H₀决定：订单流持续性对应H₀，交易量粗糙性对应H₀-1/2，波动率粗糙性对应2H₀-3/2，市场冲击幂律指数为2-2H₀；实证估计H₀≈3/4

Conclusion: 该模型成功统一解释了多个市场经验规律，H₀≈3/4不仅符合市场冲击的平方根定律，也与交易量和波动率粗糙性估计一致

Abstract: We propose a microstructural model for the order flow in financial markets that distinguishes between {\it core orders} and {\it reaction flow}, both modeled as Hawkes processes. This model has a natural scaling limit that reconciles a number of salient empirical properties: persistent signed order flow, rough trading volume and volatility, and power-law market impact. In our framework, all these quantities are pinned down by a single statistic $H_0$, which measures the persistence of the core flow. Specifically, the signed flow converges to the sum of a fractional process with Hurst index $H_0$ and a martingale, while the limiting traded volume is a rough process with Hurst index $H_0-1/2$. No-arbitrage constraints imply that volatility is rough, with Hurst parameter $2H_0-3/2$, and that the price impact of trades follows a power law with exponent $2-2H_0$. The analysis of signed order flow data yields an estimate $H_0 \approx 3/4$. This is not only consistent with the square-root law of market impact, but also turns out to match estimates for the roughness of traded volumes and volatilities remarkably well.

</details>


### [153] [Adaptive Benign Overfitting (ABO): Overparameterized RLS for Online Learning in Non-stationary Time-series](https://arxiv.org/abs/2601.22200)
*Luis Ontaneda Mijares,Nick Firoozye*

Main category: q-fin.ST

TL;DR: 该论文提出了自适应良性过拟合（ABO）框架，通过QR分解的指数加权递归最小二乘法（QR-EWRLS）实现稳定在线学习，在非平稳条件下保持数值稳定性并重现过参数化模型的双下降现象。


<details>
  <summary>Details</summary>
Motivation: 传统学习理论无法解释过参数化模型在插值极限之外仍能良好泛化的现象（良性过拟合）。现有递归最小二乘法（RLS）在数值稳定性方面存在问题，特别是在非平稳环境下。需要一种既能适应数据分布变化又能保持数值稳定的在线学习框架。

Method: 提出自适应良性过拟合（ABO）框架，基于正交三角更新的数值稳定RLS公式。引入QR分解的指数加权递归最小二乘法（QR-EWRLS），结合随机傅里叶特征映射和遗忘因子正则化，实现非平稳条件下的在线自适应。正交分解避免了协方差形式RLS的数值发散问题。

Result: 在非线性合成时间序列实验中，该方法保持有界残差和稳定条件数，重现过参数化模型的双下降行为。在外汇预测和电力需求预测应用中，ABO的准确性可与基准核方法相媲美，同时实现20-40%的速度提升。

Conclusion: 该工作提供了一个统一框架，将自适应滤波、核近似和良性过拟合联系起来，在稳定的在线学习环境中实现了过参数化模型的优势。QR-EWRLS算法在保持数值稳定性的同时，能够适应非平稳数据分布，为实际应用提供了高效可靠的解决方案。

Abstract: Overparameterized models have recently challenged conventional learning theory by exhibiting improved generalization beyond the interpolation limit, a phenomenon known as benign overfitting. This work introduces Adaptive Benign Overfitting (ABO), extending the recursive least-squares (RLS) framework to this regime through a numerically stable formulation based on orthogonal-triangular updates. A QR-based exponentially weighted RLS (QR-EWRLS) algorithm is introduced, combining random Fourier feature mappings with forgetting-factor regularization to enable online adaptation under non-stationary conditions. The orthogonal decomposition prevents the numerical divergence associated with covariance-form RLS while retaining adaptability to evolving data distributions. Experiments on nonlinear synthetic time series confirm that the proposed approach maintains bounded residuals and stable condition numbers while reproducing the double-descent behavior characteristic of overparameterized models. Applications to forecasting foreign exchange and electricity demand show that ABO is highly accurate (comparable to baseline kernel methods) while achieving speed improvements of between 20 and 40 percent. The results provide a unified view linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning framework.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [154] [Regional Transportation Modeling for Equitable Electric Vehicle Charging Infrastructure Design](https://arxiv.org/abs/2601.22395)
*Ismaeel Babur,Jane Macfarlane*

Main category: eess.SY

TL;DR: 该研究强调区域建模对于优化电动汽车充电站部署和评估环境公平影响的重要性，通过分析旧金山湾区1900万次出行，识别充电需求密度，并量化向弱势社区减少排放的潜力。


<details>
  <summary>Details</summary>
Motivation: 电池电动汽车的广泛采用有望减轻排放相关的健康影响，特别是对低收入社区，这些社区受到交通相关空气污染的影响尤为严重。然而，设计有效的充电基础设施需要一种区域建模方法，以考虑出行模式固有的跨辖区性质。

Method: 使用基于云的Mobiliti平台进行大规模区域交通建模分析，采用并行离散事件模拟实现快速计算。分析旧金山湾区超过1900万次出行，识别充电基础设施的空间需求密度，并确定电动汽车在典型一天中可能需要充电的阈值点。

Result: 通过将起源于公平优先社区之外的出行转换为电动汽车，量化了这些弱势区域内潜在的排放减少量。区域建模框架捕捉了出行行为、车辆特性和充电需求之间的复杂相互作用，同时考虑了跨市政边界基础设施的互联性。

Conclusion: 区域建模在设计解决环境公平问题的公平电动汽车充电网络中起着关键作用。研究结果为部署充电基础设施的策略提供了信息，这些策略旨在最大化可访问性、最小化里程焦虑，并优先考虑受交通排放影响尤为严重的社区的健康和福祉。

Abstract: The widespread adoption of battery electric vehicles (BEVs) holds promise for mitigating emission-related health impacts, particularly for low-income communities disproportionately affected by exposure to traffic-related air pollution. However, designing effective charging infrastructure necessitates a regional modeling approach that accounts for the inherent cross-jurisdictional nature of mobility patterns. This study underscores the importance of regional modeling in optimizing charging station deployment and evaluating the environmental justice implications for equity priority communities. We present a large-scale regional transportation modeling analysis leveraging Mobiliti, a cloud-based platform that employs parallel discrete event simulation to enable rapid computation. Our approach identifies the spatial demand density for charging infrastructure by analyzing over 19 million trips in the San Francisco Bay Area and determining the threshold points where BEVs may require charging across a typical day. By transitioning these trips that originate outside equity priority communities to BEVs, we quantify the potential emission reductions within these vulnerable areas. The regional modeling framework captures the complex interactions between travel behavior, vehicle characteristics, and charging needs, while accounting for the interconnectivity of infrastructure across municipal boundaries. This study demonstrates the critical role of regional modeling in designing equitable BEV charging networks that address environmental justice concerns. The findings inform strategies for deploying charging infrastructure that maximizes accessibility, minimizes range anxiety, and prioritizes the health and well-being of communities disproportionately burdened by transportation emissions.

</details>


### [155] [Modeling of Non-linear Dynamics of Lithium-ion Batteries via Delay-Embedded Dynamic Mode Decomposition](https://arxiv.org/abs/2601.22403)
*Khalid Mahmud Labib,Shabbir Ahmed*

Main category: eess.SY

TL;DR: 提出基于动态模式分解(DMD)的数据驱动锂离子电池模型，仅需电压电流数据即可高效捕捉非线性动态，适用于不同SoC和老化状态


<details>
  <summary>Details</summary>
Motivation: 锂离子电池的复杂电化学行为导致非线性动态，传统建模需要详细的材料组成知识，计算效率低。需要开发不依赖材料细节、计算高效的数据驱动模型来更好地管理和控制电池系统。

Method: 使用DMD和带控制的DMDc方法，仅利用HPPC测试的电压电流数据构建状态空间模型。采用时间延迟嵌入技术，嵌入维度从40到2000，并探索输入电流信号的延迟嵌入（1-12）。用60%数据训练，剩余数据验证。

Result: DMDc模型在嵌入维度1810时获得最小RSS误差3.86，标准DMD为30。输入矩阵嵌入维度6时RSS误差降至1.74。使用健康状态电池识别的系统矩阵A和B，仅更新控制输入即可有效模拟老化电池的动态，成功捕捉电压下降和瞬态响应等关键内部动态。

Conclusion: DMDc数据驱动模型能够高效捕捉锂离子电池的非线性动态，不依赖材料组成细节，对电池老化具有良好适应性，为电池管理和控制提供了有效的建模工具。

Abstract: The complex electrochemical behavior of lithium-ion batteries results in non-linear dynamics and appropriate modeling of this non-linear dynamical system is of interest for better management and control. In this work, we proposed a family of dynamic mode decomposition (DMD)-based data-driven models that do not require detailed knowledge of the composition of the battery materials but can essentially capture the non-linear dynamics with higher computational efficiency. Only voltage and current data obtained from hybrid pulse power characterization (HPPC) tests were utilized to form the state space matrices and subsequently used for predicting the future terminal voltage at different state of charge (SoC) and aging levels. To construct the system model, 60\% of the data from a single HPPC test was utilized to generate time-delay embedded snapshots, with embedding dimension ranging from 40 to 2000. Among these, an embedding dimension of 1810 resulted in the least residual sum of squares (RSS) error of 3.86 for the dynamic mode decomposition with control (DMDc) model and 30 for the standard DMD model. For DMDc model, delay embeddings (ranging from 1 to 12) were also incorporated into the input current signals. For the input matrix, an embedding dimension of 6 resulted in a minimum RSS error of 1.74. Furthermore, the system matrices A and B, identified from the HPPC test when the cell is in its healthy state, were held fixed and used to simulate the system dynamics for aged batteries by updating only the control input. Despite the presence of nonlinear degradation effects in later cycles, the DMDc model effectively captured key inner dynamics such as voltage dips and transient responses for subsequent charge and discharge cycles.

</details>


### [156] [Approximately Optimal Multi-Stream Quickest Change Detection for Gaussian Streams](https://arxiv.org/abs/2601.22561)
*Joshua Kartzman,Calvin Hawkins,Matthew Hale*

Main category: eess.SY

TL;DR: 提出了一种用于带未知变化幅度和方向的单流切换的bandit快速变化检测算法，结合衰减ε-greedy流切换规则和未知后变化均值的高效变化点检测算法


<details>
  <summary>Details</summary>
Motivation: 解决在未知变化幅度和方向的情况下，只能同时观察单个数据流的快速变化检测问题，避免现有方法需要离散化后变化参数集或变化幅度下界等强假设

Method: 结合衰减ε-greedy流切换规则（用于选择观察哪个流）和未知后变化均值的高效变化点检测算法，在控制误报的同时快速检测变化点

Result: 提供了算法的期望检测延迟和平均误报运行长度的理论界限，证明算法相对于常用替代指标是近似最优的

Conclusion: 这是首个在不依赖强假设（如离散化后变化参数集或变化幅度下界）的情况下提供可证明保证的bandit快速变化检测算法

Abstract: This paper considers the bandit quickest change detection problem in which one stream contains a change-point that shifts its distribution by an unknown amount in an unknown direction. We consider an agent that can observe only a single stream at each time, and the goal of the agent is to detect this change as quickly as possible while controlling for false alarms. We propose an algorithm that combines a decaying-$ε$-greedy stream switching rule with an efficient change-point detection algorithm for unknown post-change means. We provide bounds on the expected detection delay and average run length to false alarm for our algorithm, and based on these results we prove our algorithm is approximately optimal with respect to a commonly used surrogate. This work is the first to provide provable guarantees in this setting without strong assumptions such as a discretized post-change parameter set or a lower bound on the magnitude of change.

</details>


### [157] [Degradation-Aware Frequency Regulation of a Heterogeneous Battery Fleet via Reinforcement Learning](https://arxiv.org/abs/2601.22865)
*Tanay Raghunandan Srinivasa,Vivek Deulkar,Jia Bhargava,Mohammad Hajiesmaili,Prashant Shenoy*

Main category: eess.SY

TL;DR: 该论文研究异构电池储能系统的实时调度问题，通过强化学习方法最小化循环退化，同时跟踪随机平衡信号。


<details>
  <summary>Details</summary>
Motivation: 电池储能系统在电网平衡服务中应用日益广泛，但频繁充放电会导致循环退化和寿命缩短。现有方法难以处理循环退化的路径依赖性（非马尔可夫特性），需要新的调度策略来平衡服务性能和电池寿命。

Method: 将异构电池储能系统调度问题建模为带约束动作空间的马尔可夫决策过程（MDP），设计密集代理奖励函数，并使用基于极限学习机（ELM）的函数逼近强化学习方法处理大规模状态-动作空间。

Result: 在玩具马尔可夫信号模型和基于真实世界调节信号训练的马尔可夫模型上评估，相比基线调度策略，该方法能持续减少循环深度出现和退化指标。

Conclusion: 提出的强化学习方法能有效处理电池循环退化的路径依赖性，实现异构电池储能系统在跟踪随机平衡信号的同时最小化长期循环退化，为实际应用提供了可行的解决方案。

Abstract: Battery energy storage systems are increasingly deployed as fast-responding resources for grid balancing services such as frequency regulation and for mitigating renewable generation uncertainty. However, repeated charging and discharging induces cycling degradation and reduces battery lifetime. This paper studies the real-time scheduling of a heterogeneous battery fleet that collectively tracks a stochastic balancing signal subject to per-battery ramp-rate and capacity constraints, while minimizing long-term cycling degradation.
  Cycling degradation is fundamentally path-dependent: it is determined by charge-discharge cycles formed by the state-of-charge (SoC) trajectory and is commonly quantified via rainflow cycle counting. This non-Markovian structure makes it difficult to express degradation as an additive per-time-step cost, complicating classical dynamic programming approaches. We address this challenge by formulating the fleet scheduling problem as a Markov decision process (MDP) with constrained action space and designing a dense proxy reward that provides informative feedback at each time step while remaining aligned with long-term cycle-depth reduction.
  To scale learning to large state-action spaces induced by fine-grained SoC discretization and asymmetric per-battery constraints, we develop a function-approximation reinforcement learning method using an Extreme Learning Machine (ELM) as a random nonlinear feature map combined with linear temporal-difference learning. We evaluate the proposed approach on a toy Markovian signal model and on a Markovian model trained from real-world regulation signal traces obtained from the University of Delaware, and demonstrate consistent reductions in cycle-depth occurrence and degradation metrics compared to baseline scheduling policies.

</details>


### [158] [Reinforcement Learning-Based Co-Design and Operation of Chiller and Thermal Energy Storage for Cost-Optimal HVAC Systems](https://arxiv.org/abs/2601.22880)
*Tanay Raghunandan Srinivasa,Vivek Deulkar,Aviruch Bhatia,Vishal Garg*

Main category: eess.SY

TL;DR: 使用强化学习优化商业HVAC系统中冷却基础设施的联合运行与规模设计，以最小化30年生命周期成本，确定最佳冷水机组容量为700，热能存储容量为1500。


<details>
  <summary>Details</summary>
Motivation: 商业HVAC系统的冷却基础设施设计面临资本成本不对称的挑战：增加冷水机组容量的成本远高于增加热能存储容量的成本。需要在满足随机冷却需求的同时，找到冷水机组和热能存储的最佳组合，确保零冷却负荷损失并最小化生命周期成本。

Method: 将固定配置下的冷水机组运行问题建模为有限时域马尔可夫决策过程，使用带约束动作空间的深度Q网络求解。对每个候选的冷水机组-热能存储配置，训练强化学习策略以最小化历史冷却需求和电价下的电力成本。然后在完全满足冷却需求的可行配置集中进行生命周期成本最小化。

Result: 通过该方法确定了最优的冷水机组和热能存储容量分别为700和1500，实现了在满足冷却需求前提下的生命周期成本最小化。

Conclusion: 强化学习方法能够有效解决冷却基础设施的联合运行与规模设计问题，处理资本成本不对称和随机需求等复杂因素，为商业HVAC系统的优化设计提供了有效工具。

Abstract: We study the joint operation and sizing of cooling infrastructure for commercial HVAC systems using reinforcement learning, with the objective of minimizing life-cycle cost over a 30-year horizon. The cooling system consists of a fixed-capacity electric chiller and a thermal energy storage (TES) unit, jointly operated to meet stochastic hourly cooling demands under time-varying electricity prices. The life-cycle cost accounts for both capital expenditure and discounted operating cost, including electricity consumption and maintenance. A key challenge arises from the strong asymmetry in capital costs: increasing chiller capacity by one unit is far more expensive than an equivalent increase in TES capacity. As a result, identifying the right combination of chiller and TES sizes, while ensuring zero loss-of-cooling-load under optimal operation, is a non-trivial co-design problem. To address this, we formulate the chiller operation problem for a fixed infrastructure configuration as a finite-horizon Markov Decision Process (MDP), in which the control action is the chiller part-load ratio (PLR). The MDP is solved using a Deep Q Network (DQN) with a constrained action space. The learned DQN RL policy minimizes electricity cost over historical traces of cooling demand and electricity prices. For each candidate chiller-TES sizing configuration, the trained policy is evaluated. We then restrict attention to configurations that fully satisfy the cooling demand and perform a life-cycle cost minimization over this feasible set to identify the cost-optimal infrastructure design. Using this approach, we determine the optimal chiller and thermal energy storage capacities to be 700 and 1500, respectively.

</details>


### [159] [Energy Management Strategies for Electric Aircraft Charging Leveraging Active Landside Vehicle-to-Grid](https://arxiv.org/abs/2601.23108)
*Finn Vehlhaber,Mauro Salazar*

Main category: eess.SY

TL;DR: 机场利用停车场电动汽车的V2G（车到网）能力为电动飞机充电，可降低32%的能源成本并缓解电网压力


<details>
  <summary>Details</summary>
Motivation: 中程电动飞机的部署会增加机场的电力需求，而电动汽车的V2G双向充电能力可以作为能量缓冲，缓解电网连接压力并降低能源成本

Method: 1) 将飞机充电和陆侧V2G协调的最优能源管理问题建模为线性规划，使用偏微分方程模拟电动汽车车队的聚合充电动态；2) 在荷兰某大型航空公司的单枢纽穿梭航班网络、真实电网价格和合成停车库占用数据上测试框架

Result: 与无V2G的基准场景相比，所提概念可节省高达32%的能源成本，具体取决于时间表和参与车辆数量，并能减少潜在的电力峰值，对本地电网产生有益影响

Conclusion: 机场利用停车场电动汽车的V2G能力可以有效降低电动飞机充电的能源成本，同时为本地电网提供有益的缓冲能力，是提升航空移动性环境足迹的可行途径

Abstract: The deployment of medium-range battery electric aircraft is a promising pathway to improve the environmental footprint of air mobility. Yet such a deployment would be accompanied by significant electric power requirements at airports due to aircraft charging. Given the growing prevalence of electric vehicles and their bi-directional charging capabilities--so-called vehicle-to-grid (V2G)--we study energy buffer capabilities of parked electric vehicles to alleviate pressure on grid connections. To this end, we present energy management strategies for airports providing cost-optimal apron and landside V2G charge scheduling. Specifically, we first formulate the optimal energy management problem of joint aircraft charging and landside V2G coordination as a linear program, whereby we use partial differential equations to model the aggregated charging dynamics of the electric vehicle fleet. Second, we consider a shuttle flight network with a single hub of a large Dutch airline, real-world grid prices, and synthetic parking garage occupancy data to test our framework. Our results show that V2G at even a single airport can indeed reduce energy costs to charge the aircraft fleet: Compared to a baseline scenario without V2G, the proposed concept yields cost savings of up to 32%, depending on the schedule and amount of participating vehicles, and has other potential beneficial effects on the local power grid, e.g., the reduction of potential power peaks.

</details>


### [160] [Robust Control of Constrained Linear Systems using Online Convex Optimization and a Reference Governor](https://arxiv.org/abs/2601.23160)
*Marko Nonhoff,Mohammad Taher Al Torshan,Matthias A. Müller*

Main category: eess.SY

TL;DR: 提出一种针对线性时不变系统的控制方法，能处理时变且先验未知的成本函数，满足状态和输入约束，并对抗外部扰动。


<details>
  <summary>Details</summary>
Motivation: 现实控制问题中常面临时变且未知的成本函数，同时需要满足状态和输入约束，并对外部扰动具有鲁棒性。现有方法难以同时处理这些挑战。

Method: 结合在线凸优化框架、参考调节器和约束收紧方法，构建了一个保证递归可行性和鲁棒约束满足的控制框架。

Result: 提出的方法能保证递归可行性和鲁棒约束满足，其闭环性能的动态遗憾与成本函数变化和扰动幅度呈线性关系。

Conclusion: 该方法有效解决了具有时变未知成本函数的约束控制问题，并通过跟踪控制案例验证了其有效性。

Abstract: This article develops a control method for linear time-invariant systems subject to time-varying and a priori unknown cost functions, that satisfies state and input constraints, and is robust to exogenous disturbances. To this end, we combine the online convex optimization framework with a reference governor and a constraint tightening approach. The proposed framework guarantees recursive feasibility and robust constraint satisfaction. Its closed-loop performance is studied in terms of its dynamic regret, which is bounded linearly by the variation of the cost functions and the magnitude of the disturbances. The proposed method is illustrated by a numerical case study of a tracking control problem.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [161] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 复杂注意力机制在小规模多模态情感识别数据集上表现不佳，而简单的领域特定改进效果更好


<details>
  <summary>Details</summary>
Motivation: 研究复杂注意力机制是否能在小规模多模态情感识别数据集（EAV数据集）上提升性能

Method: 实现三类模型：基线Transformer（M1）、新型分解注意力机制（M2）、改进的CNN基线（M3），并进行系统比较

Result: 复杂注意力机制在小数据集上表现不佳（比基线低5-13个百分点），而简单的领域特定改进效果显著：音频CNN添加delta MFCCs提升3.66个百分点，EEG使用频域特征提升7.62个百分点，视觉Transformer通过领域特定预训练达到75.30%

Conclusion: 对于小规模情感识别任务，领域知识和适当的实现比架构复杂性更重要

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [162] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 提出结合多任务学习和量子卷积的混合模型，用于地球观测数据分类，探索量子机器学习在EO领域的潜力


<details>
  <summary>Details</summary>
Motivation: 地球观测进入大数据时代，传统深度学习的计算需求成为瓶颈；量子机器学习有望解决未来计算挑战，尽管当前量子设备仍有局限

Method: 提出混合模型：1) 结合多任务学习辅助高效数据编码；2) 使用带位置权重模块的量子卷积操作提取有效特征进行分类

Result: 在多个地球观测基准数据集上验证了模型有效性；实验探索了模型的泛化能力，并分析了其优势来源

Conclusion: 展示了量子机器学习在地球观测数据分析中的潜力，为应对大数据计算挑战提供了新思路

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [163] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 首个临床EEG到语言的基础模型CELM，能够总结长时间EEG记录并生成多尺度临床报告，在生成指标上实现70%-95%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 从长期EEG记录中生成总结异常模式、诊断发现和临床解释的报告仍然非常耗时，需要自动化解决方案。

Method: 开发CELM模型，整合预训练的EEG基础模型和语言模型，支持多尺度临床报告生成，包括记录描述、背景活动、癫痫样异常、事件/癫痫发作和印象。

Result: 在有患者病史监督的情况下，生成指标（如ROUGE-1和METEOR）从0.2-0.3提升到0.4-0.6，实现70%-95%的平均相对改进；在零样本设置下，CELM达到0.43-0.52的生成分数，优于基线的0.17-0.26。

Conclusion: CELM是首个临床EEG到语言的基础模型，能够有效总结长时间EEG记录并生成多尺度临床报告，显著提高报告生成效率和质量。

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [164] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedAdaVR：一种结合自适应优化器和方差缩减技术的新型联邦学习算法，解决了客户端部分参与导致的异构性问题，并通过量化版本FedAdaVR-Quant显著降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临异构性挑战，特别是客户端部分参与误差问题，这在当前文献中尚未得到充分解决。客户端部分参与会导致梯度噪声、客户端漂移等问题，影响模型性能。

Method: 提出FedAdaVR算法，结合自适应优化器和方差缩减技术。即使客户端缺席当前训练轮次，也能利用其最近存储的更新来模拟其参与。还提出FedAdaVR-Quant，将客户端更新以量化形式存储，大幅减少内存需求。

Result: 理论分析证明FedAdaVR能在一般非凸条件下收敛，并能消除客户端部分参与误差。实验表明，在IID和非IID设置下，FedAdaVR在多个数据集上始终优于最先进的基线方法。FedAdaVR-Quant能将内存需求降低50%、75%和87.5%，同时保持等效的模型性能。

Conclusion: FedAdaVR有效解决了联邦学习中客户端部分参与导致的异构性问题，通过自适应优化和方差缩减技术提升了性能，而量化版本进一步降低了内存需求，为实际部署提供了实用解决方案。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [165] [Causal Imitation Learning Under Measurement Error and Distribution Shift](https://arxiv.org/abs/2601.22206)
*Shi Bo,AmirEmad Ghassami*

Main category: cs.LG

TL;DR: 提出CausIL框架，解决离线模仿学习中因测量误差和分布偏移导致的虚假状态-动作关联问题，通过因果建模实现鲁棒策略学习


<details>
  <summary>Details</summary>
Motivation: 离线模仿学习面临两个关键挑战：1）决策相关状态仅通过噪声测量观测；2）训练和部署时的分布可能发生变化。这些因素导致虚假的状态-动作相关性，使得标准行为克隆方法在分布偏移下收敛到系统性偏差策略

Method: 提出CausIL框架，将噪声状态观测视为代理变量，基于近端因果推断思想，建立变量间的因果关系模型。为离散和连续状态空间开发估计器，连续设置中使用RKHS函数类的对抗性程序学习所需参数

Result: 在PhysioNet/Computing in Cardiology Challenge 2019队列的半模拟纵向数据上评估，相比行为克隆基线，CausIL在分布偏移下表现出更好的鲁棒性

Conclusion: 通过因果建模处理测量误差的离线模仿学习框架CausIL，能够在无需奖励或交互专家查询的情况下，从演示中恢复目标策略，并保持对分布偏移的鲁棒性

Abstract: We study offline imitation learning (IL) when part of the decision-relevant state is observed only through noisy measurements and the distribution may change between training and deployment. Such settings induce spurious state-action correlations, so standard behavioral cloning (BC) -- whether conditioning on raw measurements or ignoring them -- can converge to systematically biased policies under distribution shift. We propose a general framework for IL under measurement error, inspired by explicitly modeling the causal relationships among the variables, yielding a target that retains a causal interpretation and is robust to distribution shift. Building on ideas from proximal causal inference, we introduce \texttt{CausIL}, which treats noisy state observations as proxy variables, and we provide identification conditions under which the target policy is recoverable from demonstrations without rewards or interactive expert queries. We develop estimators for both discrete and continuous state spaces; for continuous settings, we use an adversarial procedure over RKHS function classes to learn the required parameters. We evaluate \texttt{CausIL} on semi-simulated longitudinal data from the PhysioNet/Computing in Cardiology Challenge 2019 cohort and demonstrate improved robustness to distribution shift compared to BC baselines.

</details>


### [166] [Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions](https://arxiv.org/abs/2601.22211)
*Lingkai Kong,Anagha Satish,Hezi Jiang,Akseli Kangaslahti,Andrew Ma,Wenbo Chen,Mingxiao Song,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: LSFlow：通过球形流匹配在连续隐空间学习随机策略，利用组合优化求解器保证动作可行性，在组合强化学习中实现表达性策略且保证可行性。


<details>
  <summary>Details</summary>
Motivation: 组合动作空间的强化学习面临挑战，因为可行动作集指数级增长且受复杂可行性约束限制。现有方法要么嵌入特定任务价值函数到约束优化程序，要么学习确定性结构化策略，牺牲了通用性和策略表达能力。

Method: 提出LSFlow方法：1) 通过球形流匹配在紧凑连续隐空间学习随机策略；2) 使用组合优化求解器将隐空间样本映射为有效结构化动作；3) 在隐空间直接训练价值网络避免重复求解器调用；4) 引入平滑贝尔曼算子处理求解器引起的分段常数和不连续价值景观。

Result: 在多个具有挑战性的组合强化学习任务中，LSFlow平均比最先进基线方法性能提升20.6%。

Conclusion: LSFlow成功将现代生成策略的表达能力引入组合强化学习，同时通过设计保证可行性，解决了组合动作空间RL的关键挑战。

Abstract: Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.

</details>


### [167] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DAJ：一种基于推理的LLM评判器，通过双层数据重加权学习框架训练，使用可验证奖励，在代码生成测试时缩放中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于Best-of-N选择的测试时缩放方法依赖LLM评判器，但训练可靠的LLM评判器面临严重分布偏移挑战：简单与困难问题不平衡、训练任务与评估基准不匹配、以及由廉价模型生成的训练数据与推理时模型行为不一致导致的轨迹不匹配。

Method: 提出DAJ（基于推理的LLM评判器），采用双层数据重加权学习框架，学习数据重要性权重（域级或实例级），在目标基准对齐的元集上优化泛化性能。框架自动强调困难问题、分布内样本和轨迹对齐数据，无需手工启发式规则。

Result: 在LiveCodeBench和BigCodeBench上实现最先进性能，优于强测试时缩放基线以及领先的专有模型。

Conclusion: DAJ通过数据重加权学习框架有效解决了LLM评判器训练中的分布偏移问题，在代码生成测试时缩放中取得了显著改进，是首个将数据重加权应用于LLM-as-a-Judge训练的方法。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [168] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM 是一种针对代码生成的测试时扩展方法，通过函数化代码组织和元学习奖励校正机制，显著提升大语言模型在复杂编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRM）在代码生成中效果不佳，主要因为代码缺乏有意义的步骤分解，以及蒙特卡洛估计的部分解决方案正确性分数存在噪声。需要一种能有效处理代码生成特殊性的PRM方法。

Method: FunPRM采用两种关键技术：1) 提示LLM生成模块化、函数化的代码，将函数作为PRM推理步骤；2) 引入基于元学习的奖励校正机制，利用单元测试获得的干净最终解决方案奖励来净化噪声的部分解决方案奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验表明，FunPRM在五个基础LLM上始终优于现有测试时扩展方法，与O4-mini结合时在LiveCodeBench上达到最先进性能。生成的代码更具可读性和可重用性。

Conclusion: FunPRM通过函数化代码组织和元学习奖励校正，有效解决了代码生成中PRM的局限性，显著提升了LLM在复杂编程任务上的性能，同时生成更高质量的代码。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [169] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 论文提出一种打破注意力机制中冗余旋转对称性的简单方法，通过添加批次采样的无学习查询和值偏置，改善优化器性能并增强注意力头的可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制中存在不影响模型激活或输出的冗余旋转自由度，这些自由度被计算过程携带但无实际作用。研究者希望利用这些冗余自由度来改进模型性能。

Method: 引入对称性打破协议：通过批次采样的、无学习的查询和值偏置，在旋转空间中插入偏好方向。这些偏置不通过学习获得，而是通过批次采样确定。

Result: 1) 显著提升简单内存高效优化器性能，缩小与复杂自适应方法的差距；2) 使冗余旋转自由度变得可解释，能选择性放大语义上有意义的token类别；3) 在124M参数Transformer预训练中验证了四种优化器的改进效果。

Conclusion: 最小化、有原则的架构改变能同时提升模型性能和可解释性，为注意力机制设计提供了新思路。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [170] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将生存分析转化为分类问题，利用表格基础模型通过上下文学习进行生存分析，无需显式训练


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在分类和回归任务上表现出色，但难以适应生存分析中的时间到事件结果建模，主要挑战在于右删失数据（事件可能在观察结束前未发生）

Method: 开发基于分类的框架，通过离散化事件时间将静态和动态生存分析重新表述为一系列二分类问题。删失观测被自然地处理为在特定时间点缺少标签的示例。这种分类表述使现有表格基础模型能够通过上下文学习进行生存分析，无需显式训练

Result: 在53个真实世界数据集上的评估表明，采用这种分类表述的现成表格基础模型在多个生存指标上平均优于经典和深度学习基线方法

Conclusion: 该分类框架成功地将表格基础模型扩展到生存分析领域，通过理论证明在标准删失假设下最小化二分类损失可以恢复真实的生存概率，为生存分析提供了新的有效方法

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [171] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出基于可穿戴惯性传感器和机器学习的低成本自动化人体活动识别框架，用于远程医疗和家庭护理，其中支持张量机(STM)在活动分类中表现最佳，准确率达96.67%


<details>
  <summary>Details</summary>
Motivation: 医疗基础设施有限导致老年和弱势患者依赖家庭护理，常出现忽视和依从性差的问题，需要自动化解决方案来监测治疗性锻炼活动

Method: 使用加速度计和陀螺仪传感器收集活动数据，评估四种经典分类器（逻辑回归、随机森林、SVM、k-NN）并与提出的支持张量机(STM)进行比较，STM利用张量表示保留时空运动动态

Result: SVM准确率为93.33%，逻辑回归、随机森林和k-NN为91.11%，而STM显著优于这些模型，测试准确率达96.67%，交叉验证准确率最高达98.50%

Conclusion: 提出的框架在远程医疗、老年辅助、儿童活动监测、瑜伽反馈和智能家居健康方面具有强大潜力，为低资源和农村医疗环境提供了可扩展的解决方案

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [172] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: SPECIAL算法：一种无需记忆的联邦域增量学习方法，通过服务器端锚点机制控制累积漂移，实现向后知识传递和高效跨任务学习


<details>
  <summary>Details</summary>
Motivation: 现实联邦系统中数据分布会漂移，但隐私规则禁止原始数据共享。现有联邦域增量学习(FDIL)缺乏向后知识传递保证和跨任务收敛率理论支撑

Method: 在FedAvg基础上添加服务器端"锚点"：每轮采样参与客户端更新时，通过轻量级近端项将其推向先前全局模型，无需重放缓冲区、合成数据或任务特定头

Result: 理论证明SPECIAL能保护先前任务（BKT边界控制损失增加），并在所有任务上高效学习（首个通信高效的非凸收敛率O((E/NT)^(1/2))），实验验证其有效性

Conclusion: SPECIAL是简单、无需记忆的FDIL算法，通过服务器锚点控制漂移，提供理论保证并保持通信和模型大小不变，在现实联邦部署中具有实用价值

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [173] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: SurrogateSHAP：一种无需重新训练的Shapley值近似框架，用于评估文本到图像扩散模型中数据贡献者的价值，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在创意工作流中的广泛应用，需要公平评估数据贡献者价值的框架。传统Shapley值方法面临双重计算瓶颈：模型重新训练成本过高和组合子集数量爆炸

Method: 提出SurrogateSHAP框架：1）通过预训练模型推理近似昂贵的重新训练游戏；2）使用梯度提升树近似效用函数，从树模型解析推导Shapley值

Result: 在三个不同属性任务中表现优异：CIFAR-20的图像质量、后印象派艺术品的美学评估、时尚产品数据的多样性。相比现有方法计算开销显著降低，能有效识别有影响力的贡献者

Conclusion: SurrogateSHAP为数据贡献者价值评估提供了可扩展解决方案，能有效定位临床图像中虚假相关性的数据源，为审计安全关键生成模型提供了可行路径

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [174] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: Riemannian Lyapunov Optimizers (RLOs) 是一个基于控制理论的优化算法框架，将经典优化器统一在黎曼几何框架下，通过构造Lyapunov函数保证收敛性，并在大规模基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前优化器设计多基于启发式改进，缺乏系统性的理论框架。本文旨在建立控制理论与机器学习优化之间的桥梁，提供一个统一的几何框架来系统化地设计和分析优化算法。

Method: 将优化问题重新解释为黎曼参数流形上的离散时间控制系统，识别出"正常吸引不变流形"(NAIM)，将训练动态分为两个阶段：速度状态快速对齐到目标图，然后在其中受控演化。通过构造严格的Lyapunov函数来证明收敛性，并基于此框架构建"优化器生成器"。

Result: RLOs框架不仅能够恢复经典优化算法，还能系统化地设计新的优化器。通过几何诊断验证理论，并在大规模基准测试中展示了state-of-the-art的性能表现。

Conclusion: RLOs为优化器设计提供了统一的语言和系统化工具包，将控制理论与现代机器学习优化相结合，能够设计出稳定且高效的优化算法。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [175] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 模型合并的成功因素不仅取决于模型本身，还取决于合并方法和任务特性，梯度对齐和子空间重叠是方法无关的兼容性基础


<details>
  <summary>Details</summary>
Motivation: 当前研究将模型可合并性视为内在属性，但作者认为这忽略了合并方法和任务特性的影响，需要更深入理解成功合并的决定因素

Method: 使用架构无关框架，通过线性优化和可解释的成对指标（如梯度L2距离）分析，研究四种合并方法在不同任务上的表现

Result: 发现合并成功因素存在显著差异（46.7%指标重叠；55.3%符号一致性），揭示方法特定的"指纹"，但梯度对齐和子空间重叠始终是方法无关的兼容性基础

Conclusion: 模型可合并性诊断框架为理解合并兼容性提供基础，激励未来微调策略应明确促进梯度对齐和子空间重叠特性

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [176] [ParalESN: Enabling parallel information processing in Reservoir Computing](https://arxiv.org/abs/2601.22296)
*Matteo Pinna,Giacomo Lagomarsini,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出ParalESN，一种基于对角线性递归的并行回声状态网络，解决了传统RC的顺序处理和高维存储瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统回声状态网络面临两个主要限制：1）必须顺序处理时序数据；2）高维储层的存储开销过大。这些限制严重制约了RC的可扩展性。

Method: 从结构化算子和状态空间建模的角度重新审视RC，提出ParalESN。该方法基于复数空间中的对角线性递归构建高维高效储层，实现时序数据的并行处理。

Result: 理论分析证明ParalESN保持了回声状态特性和传统ESN的普适性保证。实验表明，在时间序列基准测试中匹配传统RC的预测精度，同时显著降低计算成本。在1-D像素级分类任务中，与全可训练神经网络相比，计算成本和能耗降低数个数量级。

Conclusion: ParalESN为将RC整合到深度学习领域提供了一个有前景、可扩展且理论完备的途径。

Abstract: Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.

</details>


### [177] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: 提出CP4Gen方法，通过聚类密度估计为条件生成模型构建更鲁棒、可解释的预测集，提升不确定性校准


<details>
  <summary>Details</summary>
Motivation: 条件生成模型缺乏校准的不确定性估计，这在高风险应用中削弱了对单个输出的信任，需要系统性的不确定性量化方法

Method: 提出CP4Gen方法，采用聚类密度估计构建预测集，对异常值更不敏感，可解释性更强，结构复杂度更低

Result: 在合成数据集和气候模拟等真实应用中，CP4Gen在预测集体积和结构简洁性方面持续优于现有方法

Conclusion: CP4Gen为条件生成模型提供了强大的不确定性估计工具，特别适用于需要严格且可解释预测集的场景

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [178] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 将transformer层解释为优化算法迭代，自注意力对应交互能量的梯度步，MLP对应势能的梯度更新，GPT类transformer可视为对复合目标的梯度下降


<details>
  <summary>Details</summary>
Motivation: 为transformer架构提供优化理论视角，将自注意力和MLP层解释为优化算法步骤，从而基于经典优化理论进行有原则的架构设计

Method: 提出变分框架，将transformer层视为token嵌入上的优化算法迭代：自注意力实现交互能量的梯度步，MLP层对应势能的梯度更新，GPT类transformer通过Lie-Trotter分裂实现复合目标的梯度下降

Result: 基于Nesterov加速优化思想设计的新型transformer架构，在TinyStories和OpenWebText上一致优于nanoGPT基线，证明优化理论见解能带来实际性能提升

Conclusion: transformer层可解释为优化算法迭代，这种优化理论视角为架构设计提供了有原则的方法，并能通过经典优化思想（如Nesterov加速）改进实际性能

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [179] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL是一个安全的去中心化联邦学习框架，结合DAG账本、侧链和零知识证明，实现隐私保护的模型验证和对抗行为检测。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习（包括集中式和去中心化方法）在可扩展性、安全性和更新验证方面存在挑战，需要更安全、可扩展的解决方案。

Method: 提出ZK-HybridFL框架，集成有向无环图（DAG）账本、专用侧链和零知识证明（ZKPs），使用事件驱动智能合约和预言机辅助侧链验证本地模型更新，内置挑战机制检测对抗行为。

Result: 在图像分类和语言建模任务中，ZK-HybridFL相比Blade-FL和ChainFL实现了更快的收敛、更高的准确率、更低的困惑度和更低的延迟，能抵抗大量对抗节点和空闲节点，支持亚秒级链上验证和高效gas使用。

Conclusion: ZK-HybridFL是一个可扩展且安全的去中心化联邦学习解决方案，适用于多样化环境，能防止无效更新和孤儿攻击。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [180] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 本文研究连续均值估计问题，在用户级差分隐私下提出新的矩阵分解机制，相比纯差分隐私显著降低估计误差


<details>
  <summary>Details</summary>
Motivation: 连续均值估计中，数据向量按顺序到达，需要保持对运行均值的准确估计。现有研究主要关注纯差分隐私，但这会导致估计噪声过大，限制了实际应用。用户级差分隐私保护每个用户的整个数据集，即使他们贡献多个数据点。

Method: 采用近似差分隐私框架，基于矩阵分解机制的最新进展，提出专门针对均值估计的新型分解方法。该方法既高效又准确，特别适用于连续均值估计场景。

Result: 提出的均值估计特定分解方法在用户级差分隐私下的连续均值估计中，实现了渐进更低的均方误差界限，相比纯差分隐私方法显著提升了估计精度。

Conclusion: 通过采用近似差分隐私和开发专门的矩阵分解机制，本文解决了连续均值估计中纯差分隐私方法噪声过大的问题，为实际应用提供了更实用的解决方案。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [181] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 将工作流生成视为贝叶斯推理问题，提出BWG框架和BayesFlow算法，通过并行前瞻和序列细化改进工作流生成效果


<details>
  <summary>Details</summary>
Motivation: 现有工作流生成方法大多将其视为优化问题，缺乏理论基础。需要更理论化的方法来改进自动工作流生成的质量和效果

Method: 提出贝叶斯工作流生成(BWG)框架，将工作流生成视为后验分布采样问题。使用并行前瞻rollout进行重要性加权，结合序列细化器进行池级改进。具体实现为BayesFlow算法

Result: 在六个基准数据集上，BayesFlow比SOTA工作流生成基线准确率提升高达9个百分点，比零样本提示提升高达65个百分点

Conclusion: BWG为基于搜索的工作流设计提供了理论化的升级方案，证明了贝叶斯推理框架在工作流生成中的有效性

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [182] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 该论文提出了针对偏好贝叶斯优化的精确知识梯度方法，解决了传统知识梯度在偏好设置中计算困难的问题。


<details>
  <summary>Details</summary>
Motivation: 许多实际场景只允许成对比较查询（偏好贝叶斯优化），而传统知识梯度方法在这种设置下存在计算挑战，因为前瞻步骤需要计算非高斯后验分布，之前被认为难以处理。

Method: 推导了偏好贝叶斯优化的精确解析知识梯度，解决了非高斯后验计算问题，使得知识梯度能够应用于偏好设置。

Result: 精确知识梯度在一系列基准问题上表现强劲，通常优于现有的采集函数。同时，论文也通过案例研究展示了知识梯度在某些场景下的局限性。

Conclusion: 该研究成功将知识梯度扩展到偏好贝叶斯优化领域，提供了精确解析解，显著提升了优化性能，但也指出了该方法在某些情况下的限制。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [183] [Exact closed-form Gaussian moments of residual layers](https://arxiv.org/abs/2601.22307)
*Simon Kuang,Xinfan Lin*

Main category: cs.LG

TL;DR: 提出一种通过逐层矩匹配在深度残差神经网络中传播高斯分布均值和协方差的精确方法，在多个激活函数上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 解决在深度神经网络中传播高斯分布统计特性的长期问题，特别是对于残差网络和各种激活函数的精确矩匹配

Method: 使用逐层矩匹配方法，推导出probit、GeLU、ReLU、Heaviside和sine激活函数的精确矩匹配公式，适用于前馈和广义残差层

Result: 在随机网络上KL散度误差指标比流行方法提高数个数量级（高达百万倍）；在真实数据上具有竞争力的统计校准；在变分贝叶斯网络上比最先进确定性推理方法提高百倍KL散度精度

Conclusion: 该方法填补了长期存在的空白，为深度神经网络中的不确定性传播提供了精确高效的解决方案，并在多个基准测试中展现出显著优势

Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.

</details>


### [184] [Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.22409)
*Puyu Wang,Junyu Zhou,Philipp Liznerski,Marius Kloft*

Main category: cs.LG

TL;DR: 本文分析了双层KANs在梯度下降训练下的动态、泛化性和差分隐私性质，证明了在NTK可分离假设下，多项式对数宽度足以实现1/T的优化速率和1/n的泛化速率，并在差分隐私下揭示了宽度必要性的新现象。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs) 作为标准MLPs的结构化替代方案最近出现，但对其训练动态、泛化性和隐私性质的理论理解仍然有限。本文旨在建立KANs在这些方面的理论基础。

Method: 分析双层KANs的梯度下降训练，推导训练动态、泛化和差分隐私下效用的通用边界。在NTK可分离假设下，专门针对逻辑损失进行具体分析，研究网络宽度、训练迭代次数和样本量之间的关系。

Result: 1) 在NTK可分离假设下，多项式对数宽度足以使GD达到1/T的优化速率和1/n的泛化速率；2) 在差分隐私设置中，获得√d/(nε)的效用边界，匹配一般凸Lipschitz问题的经典下界；3) 发现多项式对数宽度在差分隐私下不仅是充分的，而且是必要的，揭示了非私有与私有训练机制之间的定性差距。

Conclusion: 本文为KANs提供了首个关于训练动态、泛化和隐私性质的理论框架，揭示了网络宽度在私有和非私有训练中的不同作用，并通过实验验证了理论见解对实际选择（如网络宽度选择和早停）的指导价值。

Abstract: Kolmogorov--Arnold Networks (KANs) have recently emerged as a structured alternative to standard MLPs, yet a principled theory for their training dynamics, generalization, and privacy properties remains limited. In this paper, we analyze gradient descent (GD) for training two-layer KANs and derive general bounds that characterize their training dynamics, generalization, and utility under differential privacy (DP). As a concrete instantiation, we specialize our analysis to logistic loss under an NTK-separable assumption, where we show that polylogarithmic network width suffices for GD to achieve an optimization rate of order $1/T$ and a generalization rate of order $1/n$, with $T$ denoting the number of GD iterations and $n$ the sample size. In the private setting, we characterize the noise required for $(ε,δ)$-DP and obtain a utility bound of order $\sqrt{d}/(nε)$ (with $d$ the input dimension), matching the classical lower bound for general convex Lipschitz problems. Our results imply that polylogarithmic width is not only sufficient but also necessary under differential privacy, revealing a qualitative gap between non-private (sufficiency only) and private (necessity also emerges) training regimes. Experiments further illustrate how these theoretical insights can guide practical choices, including network width selection and early stopping.

</details>


### [185] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 提出针对回归模型的最优隐蔽攻击方法，能绕过现有防御，并开发新的评估方法和防御方案BayesClean


<details>
  <summary>Details</summary>
Motivation: 回归模型在工业、工程和科学领域广泛应用，但其对投毒攻击的鲁棒性研究不足，现有研究常采用不现实的威胁模型，实用性有限

Method: 1) 提出考虑不同可检测程度的最优隐蔽攻击公式；2) 开发基于目标归一化的新评估方法，权衡攻击效果与可检测性；3) 设计针对隐蔽攻击的新防御方案BayesClean

Result: 提出的最优隐蔽攻击能绕过最先进的防御；BayesClean在攻击隐蔽且投毒点数量显著时优于先前防御

Conclusion: 回归模型的投毒攻击鲁棒性需要更现实的威胁模型，提出的攻击和防御方法为实际应用提供了更实用的解决方案

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [186] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 扩散模型即使在不匹配或低保真度训练数据下，仍能作为有效的先验用于逆问题求解，关键在于测量信息的充分性


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型作为先验在逆问题中的鲁棒性，特别是当模型训练数据与目标信号不匹配或质量较低时，探究其何时以及为何仍能有效工作

Method: 通过大量实验研究弱扩散先验在不同测量条件下的表现，并基于贝叶斯一致性理论分析高维测量如何使后验集中在真实信号附近

Result: 发现弱先验在测量信息充分时（如大量观测像素）表现良好，接近完整强度的域内基线；识别了弱先验失效的特定情况；理论分析提供了弱扩散先验可靠使用的原则性依据

Conclusion: 扩散模型作为先验在逆问题求解中具有鲁棒性，即使训练数据不匹配或质量较低，只要测量信息足够充分，仍能可靠地恢复目标信号

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [187] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR是一个评估材料基础模型在几何尺度泛化能力的基准，包含三个任务：晶体结构到性质预测、物理推理链式思考、以及基于性质的逆向检索，通过结构化指标揭示模型在显式推理下的表现变化。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型越来越多地应用于材料科学推理，但它们在物理结构化分布偏移下的行为仍不清楚。需要评估几何尺度泛化能力及其与结构幻觉、一致性和推理的关系。

Method: 引入SCALAR基准，使用从DFT验证的晶胞通过超胞扩展和几何截断得到的纳米颗粒结构（从几个原子到超过18,000个原子，总计约100,000个结构）。定义三个任务：CIF到性质预测、带有显式物理推理的链式思考变体、以及从候选晶体中识别目标性质的逆向检索。

Result: 实验显示，在不同基础模型中，显式推理导致模型依赖的大幅偏移，通常减少幻觉和错误，但经常破坏一致性或有效性。几何尺度泛化不能仅从准确性推断。

Conclusion: SCALAR基准揭示了材料基础模型在几何尺度泛化中的复杂行为，强调需要结构化评估指标来全面理解模型在物理结构化分布偏移下的表现。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [188] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一种受人类多系统学习启发的贝叶斯采样算法，包含模型导向、无模型和情景控制三个模块，用于大规模统计机器学习中的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 人类通过多种互补的神经学习系统（模型导向规划、无模型习惯、情景记忆）实现高效学习，作者希望将这种生物效率的计算原理转化为可扩展的贝叶斯推断采样算法。

Method: 提出三模块采样算法：1) 模型导向模块使用目标分布进行引导但计算缓慢的采样；2) 无模型模块利用先前样本学习参数空间模式，实现快速反射式采样；3) 情景控制模块通过回忆特定过去事件（样本）支持快速采样。

Result: 该方法推进了贝叶斯方法，促进其在大规模统计机器学习问题中的应用，特别是在贝叶斯深度学习中进行适当和原则性的不确定性量化。

Conclusion: 受人类多系统学习启发的三模块采样算法为大规模贝叶斯推断提供了一种高效的计算框架，特别适用于深度学习中的不确定性量化问题。

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [189] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 静态黑盒评估无法保证LLM在更新后的对齐性，即使模型通过所有标准测试，也可能在单次良性更新后严重失齐


<details>
  <summary>Details</summary>
Motivation: 现有对齐研究假设初始模型是对齐的，但实践中LLM经常更新，而静态黑盒评估无法检测更新后可能出现的失齐行为

Method: 理论分析：形式化静态和更新后对齐，证明由于过参数化，静态对齐无法保证更新后对齐；实证验证：在隐私、越狱安全和行为诚实三个对齐领域测试LLM

Result: 理论上证明静态黑盒探测无法区分真正更新后鲁棒的模型与隐藏任意数量对抗行为的模型；实证发现存在通过所有标准测试但在单次良性更新后严重失齐的LLM；模型规模越大，隐藏潜在对抗行为的能力越强

Conclusion: 静态评估协议不足，迫切需要开发更新后鲁棒的对齐评估方法，模型规模增加会加剧更新后失齐风险

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [190] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 提出一种级联扩散模型，通过先生成低分辨率版本（分类特征+数值特征的粗粒度表示），再利用条件概率路径和数据依赖耦合生成高分辨率版本，以更好地处理混合类型特征。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在处理混合类型特征（离散状态与连续分布结合的特征）时面临挑战，需要改进扩散模型在表格数据上的生成能力。

Method: 采用级联方法：1）先生成低分辨率版本（纯分类特征+数值特征的粗粒度分类表示）；2）通过新颖的引导条件概率路径和数据依赖耦合，利用低分辨率信息生成高分辨率版本。

Result: 模型生成更真实的样本，更准确地捕捉分布细节，检测分数提高40%，并正式证明了级联方法能收紧传输成本界限。

Conclusion: 提出的级联扩散模型在表格数据生成方面显著优于现有方法，特别是在处理混合类型特征时表现出色，为表格数据生成提供了更有效的解决方案。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [191] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: PA-GP-UCB是一种贝叶斯优化算法，利用昂贵真实评估和廉价预测模型，结合离线数据提高样本效率，在假设生成任务中实现更快的收敛。


<details>
  <summary>Details</summary>
Motivation: 现实优化问题常涉及昂贵的真实评估（如人工评估、物理实验）和廉价的低保真预测（如机器学习模型、模拟）。同时，大量离线数据（如历史实验和预测）可用于预训练预测模型并提供信息先验。需要一种能同时利用这两种评估源和离线数据来提高样本效率的方法。

Method: 提出预测增强高斯过程上置信界（PA-GP-UCB）算法，使用从联合高斯过程后验导出的控制变量估计器来校正预测偏差并减少不确定性。该算法结合真实评估和预测模型，利用离线数据提高优化效率。

Result: 理论证明PA-GP-UCB保持了GP-UCB的标准遗憾率，同时获得了严格更小的主导常数，该常数由预测质量和离线数据覆盖范围明确控制。在合成基准测试和基于人类行为数据的真实世界假设评估任务中，PA-GP-UCB比Vanilla GP-UCB和朴素预测增强基线收敛更快。

Conclusion: PA-GP-UCB为昂贵反馈下的假设生成提供了一个通用且样本高效的框架，能够有效利用预测模型和离线数据来减少对昂贵真实评估的需求。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [192] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: 论文通过理论分析证明困惑度可能不适合作为模型选择指标，因为存在低困惑度但预测错误的序列，且困惑度提升不一定对应准确率提升。


<details>
  <summary>Details</summary>
Motivation: 困惑度作为衡量模型质量的简单指标被广泛使用，但先前研究已指出其局限性。本文旨在从理论角度严格证明困惑度作为模型选择指标的不适性。

Method: 利用Transformer连续性结果进行理论证明：1) 证明如果紧凑解码器Transformer模型能准确自信地预测某个序列，则必然存在另一个困惑度很低但模型预测错误的序列；2) 通过分析等困惑度图，研究困惑度与准确率的关系。

Result: 理论证明困惑度存在根本缺陷：模型可能对某些序列产生低困惑度但预测错误。困惑度提升不一定选择更准确的模型，只有当置信度提升伴随准确率提升时才会选择新模型。

Conclusion: 困惑度作为模型选择指标存在理论缺陷，不能可靠反映模型质量。模型选择应考虑更全面的评估指标，而不仅仅是困惑度。

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [193] [FlowSymm: Physics Aware, Symmetry Preserving Graph Attention for Network Flow Completion](https://arxiv.org/abs/2601.22317)
*Ege Demirci,Francesco Bullo,Ananthram Swami,Ambuj Singh*

Main category: cs.LG

TL;DR: FlowSymm：一种结合群作用、图注意力编码和Tikhonov优化的新型网络流恢复架构，能在保持局部守恒定律的同时准确恢复缺失流量。


<details>
  <summary>Details</summary>
Motivation: 网络边缘缺失流量的恢复是一个基本逆问题，在交通、能源、移动性等系统中普遍存在。现有方法难以同时满足局部守恒定律并准确恢复缺失流量。

Method: 1) 基于最小范数无散流完成锚定观测；2) 计算保持观测流不变的所有可容许群作用的正交基；3) 使用GATv2层编码图和边特征，通过注意力机制选择物理感知的群作用；4) 通过隐式双层优化进行Tikhonov细化。

Result: 在三个真实世界流量基准测试（交通、电力、自行车）中，FlowSymm在RMSE、MAE和相关指标上均优于最先进的基线方法。

Conclusion: FlowSymm通过结合对称性保持的群作用、图注意力机制和轻量级优化，为网络流恢复问题提供了一种有效且物理一致的解决方案。

Abstract: Recovering missing flows on the edges of a network, while exactly respecting local conservation laws, is a fundamental inverse problem that arises in many systems such as transportation, energy, and mobility. We introduce FlowSymm, a novel architecture that combines (i) a group-action on divergence-free flows, (ii) a graph-attention encoder to learn feature-conditioned weights over these symmetry-preserving actions, and (iii) a lightweight Tikhonov refinement solved via implicit bilevel optimization. The method first anchors the given observation on a minimum-norm divergence-free completion. We then compute an orthonormal basis for all admissible group actions that leave the observed flows invariant and parameterize the valid solution subspace, which shows an Abelian group structure under vector addition. A stack of GATv2 layers then encodes the graph and its edge features into per-edge embeddings, which are pooled over the missing edges and produce per-basis attention weights. This attention-guided process selects a set of physics-aware group actions that preserve the observed flows. Finally, a scalar Tikhonov penalty refines the missing entries via a convex least-squares solver, with gradients propagated implicitly through Cholesky factorization. Across three real-world flow benchmarks (traffic, power, bike), FlowSymm outperforms state-of-the-art baselines in RMSE, MAE and correlation metrics.

</details>


### [194] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: 提出VaR-CPO算法，直接优化风险价值约束，实现安全探索和零约束违反


<details>
  <summary>Details</summary>
Motivation: 现有基线方法无法在可行环境中实现训练期间的零约束违反，需要一种能直接优化VaR约束的样本高效且保守的方法

Method: 使用单边切比雪夫不等式处理VaR约束的非可微性，基于成本回报的前两矩获得可处理的替代约束；扩展CPO方法的信任域框架

Result: VaR-CPO能够在可行环境中实现安全探索，训练期间达到零约束违反；为策略改进和约束违反提供了严格的最坏情况界限

Conclusion: VaR-CPO是一种有效的风险约束策略优化方法，能够直接优化VaR约束，在保证安全性的同时提供理论保证

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [195] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 本文提出了首个联邦学习框架用于LLM路由，使客户端能够从本地离线查询-模型评估数据中学习共享路由策略，解决数据隐私和碎片化问题。


<details>
  <summary>Details</summary>
Motivation: LLM作为远程服务被边缘和企业客户端访问时，需要在模型能力和成本之间平衡路由。现有路由方法需要集中化的查询-模型评估数据，但这些数据分散在各客户端且涉及隐私，无法集中。同时，单客户端训练因数据有限、查询分布受限和模型评估偏差而效果不佳。

Method: 提出了首个联邦学习框架用于LLM路由，支持参数化多层感知器路由器和非参数K-means路由器。该框架能在异构客户端查询分布和非均匀模型覆盖下工作，使客户端协作学习共享路由策略。

Result: 在两个基准测试中，联邦协作相比客户端本地路由器提高了准确率-成本边界，既通过增加有效模型覆盖，也通过更好的查询泛化能力。理论结果也验证了联邦训练能减少路由次优性。

Conclusion: 联邦学习框架能有效解决LLM路由中的数据隐私和碎片化问题，通过客户端协作学习共享路由策略，在保持隐私的同时提高路由性能。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [196] [Spatially-Adaptive Conformal Graph Transformer for Indoor Localization in Wi-Fi Driven Networks](https://arxiv.org/abs/2601.22322)
*Ayesh Abu Lehyeh,Anastassia Gharib,Safwan Wshah*

Main category: cs.LG

TL;DR: SAC-GT框架结合图Transformer和空间自适应保形预测，实现高精度室内定位并提供区域特定的不确定性估计


<details>
  <summary>Details</summary>
Motivation: 现有基于图的室内定位模型虽然能利用Wi-Fi接入点与设备间的空间关系提供更细粒度的定位，但缺乏预测不确定性量化能力，而这对实际部署至关重要

Method: 提出SAC-GT框架，整合图Transformer模型（捕捉网络空间拓扑和信号强度动态）与新颖的空间自适应保形预测方法，提供区域特定的不确定性估计

Result: 在大规模真实数据集上的评估表明，SAC-GT达到最先进的定位精度，同时提供鲁棒且空间自适应的可靠性保证

Conclusion: SAC-GT框架能够同时实现精确的2D位置预测和统计有效的、适应不同环境条件的置信区域，解决了室内定位中不确定性量化这一关键需求

Abstract: Indoor localization is a critical enabler for a wide range of location-based services in smart environments, including navigation, asset tracking, and safety-critical applications. Recent graph-based models leverage spatial relationships between Wire-less Fidelity (Wi-Fi) Access Points (APs) and devices, offering finer localization granularity, but fall short in quantifying prediction uncertainty, a key requirement for real-world deployment. In this paper, we propose Spatially-Adaptive Conformal Graph Transformer (SAC-GT), a framework for accurate and reliable indoor localization. SAC-GT integrates a Graph Transformer (GT) model that captures network's spatial topology and signal strength dynamics, with a novel Spatially-Adaptive Conformal Prediction (SACP) method that provides region-specific uncertainty estimates. This allows SAC-GT to produce not only precise two-dimensional (2D) location predictions but also statistically valid confidence regions tailored to varying environmental conditions. Extensive evaluations on a large-scale real-world dataset demonstrate that the proposed SAC-GT solution achieves state-of-the-art localization accuracy while delivering robust and spatially adaptive reliability guarantees.

</details>


### [197] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: SCOPE是一个可扩展且可控的模型路由框架，通过预测模型在查询上的成本和性能，实现动态决策，既能提升准确性又能降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有模型路由方法通常将路由视为在少量模型间的固定选择，难以适应新模型或变化的预算约束。需要一种更灵活、可扩展的路由框架。

Method: 提出SCOPE框架，通过强化学习训练，基于检索相似问题上的模型行为进行推理预测，而非依赖固定模型名称。显式预测模型的准确性和成本，将路由转化为动态决策问题。

Result: 实验表明SCOPE灵活适应用户需求：当性能优先时，准确性可提升达25.7%；当效率优先时，成本可降低达95.1%。

Conclusion: SCOPE不仅是成本节约工具，更是一个可扩展、可控的路由框架，能够适应新模型和变化的预算约束，在准确性和成本间实现灵活权衡。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [198] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AgentScore使用LLM生成候选规则，通过验证-选择循环创建可部署的临床评分系统，在多个任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 机器学习模型虽然性能强大，但往往无法转化为临床常规使用，因为它们不符合工作流程约束（如可记忆性、可审计性、床边执行）。现有模型类别与指南部署不兼容，而可部署的指南通常采用单位加权临床检查表形式。

Method: AgentScore使用LLM在指数级大的离散规则空间中提出候选规则，通过确定性的、基于数据的验证-选择循环来确保统计有效性和可部署性约束，形成阈值化二元规则和的单位加权检查表。

Result: 在8个临床预测任务中，AgentScore优于现有评分生成方法，在更强的结构约束下实现了与更灵活可解释模型相当的AUC。在2个外部验证任务中，AgentScore比已建立的基于指南的评分具有更高的区分度。

Conclusion: AgentScore通过结合LLM的语义引导优化和统计验证，成功创建了既符合临床工作流程约束又保持良好预测性能的可部署评分系统，弥合了机器学习模型与临床实践之间的差距。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [199] [Label-Efficient Monitoring of Classification Models via Stratified Importance Sampling](https://arxiv.org/abs/2601.22326)
*Lupo Marsigli,Angel Lopez de Haro*

Main category: cs.LG

TL;DR: 提出基于分层重要性采样(SIS)的模型监控框架，在有限标注预算下提高分类模型性能估计效率


<details>
  <summary>Details</summary>
Motivation: 生产环境中分类模型监控面临标注预算严格、一次性批量获取标签、错误率极低等挑战，需要高效监控方法

Method: 采用分层重要性采样(SIS)框架，结合重要性采样和分层随机采样的优势，无需最优提案分布或分层定义

Result: SIS在温和条件下提供无偏估计，相比重要性采样和分层随机采样有严格的有限样本均方误差改进，实验验证了效率提升

Conclusion: SIS是原则性、标注高效、操作轻量的部署后模型监控方法，即使使用噪声代理和次优分层也能提高估计效率

Abstract: Monitoring the performance of classification models in production is critical yet challenging due to strict labeling budgets, one-shot batch acquisition of labels and extremely low error rates. We propose a general framework based on Stratified Importance Sampling (SIS) that directly addresses these constraints in model monitoring. While SIS has previously been applied in specialized domains, our theoretical analysis establishes its broad applicability to the monitoring of classification models. Under mild conditions, SIS yields unbiased estimators with strict finite-sample mean squared error (MSE) improvements over both importance sampling (IS) and stratified random sampling (SRS). The framework does not rely on optimally defined proposal distributions or strata: even with noisy proxies and sub-optimal stratification, SIS can improve estimator efficiency compared to IS or SRS individually, though extreme proposal mismatch may limit these gains. Experiments across binary and multiclass tasks demonstrate consistent efficiency improvements under fixed label budgets, underscoring SIS as a principled, label-efficient, and operationally lightweight methodology for post-deployment model monitoring.

</details>


### [200] [Molecular Representations in Implicit Functional Space via Hyper-Networks](https://arxiv.org/abs/2601.22327)
*Zehong Wang,Xiaolong Han,Qi Yang,Xiangru Tang,Fang Wu,Xiaoguang Guo,Weixiang Sun,Tianyi Ma,Pietro Lio,Le Cong,Sheng Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: MolField将分子建模为三维空间中的连续函数场，而非离散表示，通过超网络学习分子场分布，实现物理一致性和对离散化方式的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有分子表示方法（序列、图、点云）将分子视为离散对象，忽略了分子本质上是连续且具有场状物理特性的。这种离散化处理限制了分子学习的泛化能力和物理一致性。

Method: 提出MolField框架：1) 将每个分子建模为三维空间中的连续函数场；2) 使用超网络学习分子场分布；3) 通过规范化坐标确保SE(3)不变性；4) 引入结构化权重标记化，使序列超网络能够直接学习函数空间。

Result: 在分子动力学和性质预测任务上验证了MolField的有效性。结果表明，将分子视为连续函数从根本上改变了分子表示的泛化方式，下游行为对分子离散化或查询方式具有稳定性。

Conclusion: 分子学习应重新构想为函数空间学习，将分子建模为连续场而非离散对象。MolField框架展示了这种连续函数表示在泛化能力和物理一致性方面的优势，为分子表示学习提供了新范式。

Abstract: Molecular representations fundamentally shape how machine learning systems reason about molecular structure and physical properties. Most existing approaches adopt a discrete pipeline: molecules are encoded as sequences, graphs, or point clouds, mapped to fixed-dimensional embeddings, and then used for task-specific prediction. This paradigm treats molecules as discrete objects, despite their intrinsically continuous and field-like physical nature. We argue that molecular learning can instead be formulated as learning in function space. Specifically, we model each molecule as a continuous function over three-dimensional (3D) space and treat this molecular field as the primary object of representation. From this perspective, conventional molecular representations arise as particular sampling schemes of an underlying continuous object. We instantiate this formulation with MolField, a hyper-network-based framework that learns distributions over molecular fields. To ensure physical consistency, these functions are defined over canonicalized coordinates, yielding invariance to global SE(3) transformations. To enable learning directly over functions, we introduce a structured weight tokenization and train a sequence-based hyper-network to model a shared prior over molecular fields. We evaluate MolField on molecular dynamics and property prediction. Our results show that treating molecules as continuous functions fundamentally changes how molecular representations generalize across tasks and yields downstream behavior that is stable to how molecules are discretized or queried.

</details>


### [201] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT框架通过知识引导的核状态重建，从噪声、不完整观测数据中恢复物理一致的轨迹和导数，为符号回归提供可靠输入


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声、部分观测下容易失效，或依赖黑盒潜在动力学而难以解释机制，需要能处理真实世界数据局限性的符号发现框架

Method: 在再生核希尔伯特空间中构建状态重建，直接整合结构先验（非负性、守恒律）和领域特定观测模型，处理异构采样和测量粒度，生成平滑、物理一致的状态估计和解析时间导数

Result: 在12个不同科学基准和多种噪声机制下，MAAT显著降低了状态估计MSE，为下游符号回归提供了更准确的轨迹和导数

Conclusion: MAAT为碎片化传感器数据与符号回归之间建立了原则性接口，能够在真实世界数据限制下实现可靠的符号发现

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [202] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS是一种用于Cell Painting数据的可扩展批次校正方法，通过构建平滑的亲和力矩阵来对齐不同批次的样本，具有近似线性时间复杂度。


<details>
  <summary>Details</summary>
Motivation: Cell Painting数据在大规模应用中受到批次效应（实验室、仪器、协议差异）的严重影响，这些效应会掩盖生物信号，需要可扩展的批次校正方法。

Method: BALANS通过两个关键思想构建稀疏亲和力矩阵：(1) 使用批次感知的局部尺度计算高斯核亲和力；(2) 采用自适应采样策略，优先考虑低累积邻居覆盖率的行，并保留每行最强的亲和力，实现稀疏但信息丰富的近似。

Result: BALANS在样本复杂度方面达到最优顺序，提供近似保证，运行时间接近线性。在真实Cell Painting数据集和合成基准测试中，BALANS能够扩展到大规模数据集，在保持校正质量的同时提高运行效率。

Conclusion: BALANS是一种高效、可扩展的批次校正方法，能够处理大规模Cell Painting数据中的批次效应问题，为药物发现提供可靠的数据预处理工具。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [203] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种新的DP-SGD噪声相关策略，仅与前一次迭代相关并抵消部分噪声，无需存储历史噪声，内存开销与标准DP-SGD相同


<details>
  <summary>Details</summary>
Motivation: 现有矩阵分解机制等DP-SGD扩展方法虽然通过跨多轮迭代的相关噪声提高了准确性，但需要存储大量历史噪声向量，导致显著的内存开销问题

Method: 提出新的噪声相关策略：1) 仅与前一迭代的噪声相关；2) 抵消受控部分的噪声；3) 使用伪随机噪声生成器重新生成噪声，无需存储历史噪声；4) 保持与标准DP-SGD相同的内存需求

Result: 计算开销极小，内存需求与标准DP-SGD相同，无需额外存储，实验证明比标准DP-SGD具有更高的准确性

Conclusion: 该方法在保持DP-SGD内存效率的同时，通过创新的单步噪声相关策略显著提高了训练准确性，为差分隐私机器学习提供了实用的改进方案

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [204] [Quantum-Inspired Reinforcement Learning for Secure and Sustainable AIoT-Driven Supply Chain Systems](https://arxiv.org/abs/2601.22339)
*Muhammad Bilal Akram Dastagir,Omer Tariq,Shahid Mumtaz,Saif Al-Kuwari,Ahmed Farouk*

Main category: cs.LG

TL;DR: 论文提出了一种量子启发的强化学习框架，将碳足迹减少、库存管理和加密级安全措施统一起来，用于优化AIoT供应链系统。


<details>
  <summary>Details</summary>
Motivation: 现代供应链需要在高速物流与环保安全之间取得平衡，但传统优化模型往往忽视可持续性目标和网络漏洞，使系统易受生态损害和恶意攻击。

Method: 设计量子启发的强化学习框架，结合可控自旋链类比和实时AIoT信号，优化统一保真度、安全性和碳成本的多目标奖励，采用基于价值和集成更新的稳定训练方法。

Result: 在模拟中，该方法表现出平滑收敛、后期性能强劲，在典型噪声信道下优雅降级，优于标准学习和基于模型的参考方法，能稳健处理实时可持续性和风险需求。

Conclusion: 量子启发的AIoT框架有潜力推动大规模安全、环保的供应链运营，为负责任满足消费者和环境需求的全球连接基础设施奠定基础。

Abstract: Modern supply chains must balance high-speed logistics with environmental impact and security constraints, prompting a surge of interest in AI-enabled Internet of Things (AIoT) solutions for global commerce. However, conventional supply chain optimization models often overlook crucial sustainability goals and cyber vulnerabilities, leaving systems susceptible to both ecological harm and malicious attacks. To tackle these challenges simultaneously, this work integrates a quantum-inspired reinforcement learning framework that unifies carbon footprint reduction, inventory management, and cryptographic-like security measures. We design a quantum-inspired reinforcement learning framework that couples a controllable spin-chain analogy with real-time AIoT signals and optimizes a multi-objective reward unifying fidelity, security, and carbon costs. The approach learns robust policies with stabilized training via value-based and ensemble updates, supported by window-normalized reward components to ensure commensurate scaling. In simulation, the method exhibits smooth convergence, strong late-episode performance, and graceful degradation under representative noise channels, outperforming standard learned and model-based references, highlighting its robust handling of real-time sustainability and risk demands. These findings reinforce the potential for quantum-inspired AIoT frameworks to drive secure, eco-conscious supply chain operations at scale, laying the groundwork for globally connected infrastructures that responsibly meet both consumer and environmental needs.

</details>


### [205] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 语言模型在有限交互预算下探索交互环境的能力评估，发现系统性探索不足和次优解，性能常低于简单探索-利用启发式基线


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在交互环境中进行探索的能力，特别是在有限交互预算下的表现，了解当前模型在探索任务中的局限性

Method: 引入三个参数化任务，控制探索难度，涵盖连续和离散环境；评估最先进的语言模型；研究两种轻量级干预：将固定预算拆分为并行执行，以及定期总结交互历史

Result: 发现模型存在系统性探索不足和次优解，性能常显著低于简单的探索-利用启发式基线，且随着预算增加性能提升有限；并行执行和定期总结能改善探索性能

Conclusion: 当前语言模型在交互环境探索方面存在显著局限性，但通过简单的干预措施（如并行执行和定期总结）可以改善探索性能

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [206] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: MixQuant：一种基于块旋转感知的后训练量化框架，通过排列重分布激活质量来改善异常值抑制，在Llama3 1B INT4量化中恢复90%全向量旋转困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法使用块旋转来扩散异常值，但块结构对异常值抑制的影响尚不清楚。需要系统分析块Hadamard旋转的异常值抑制机制，并开发更有效的量化方法。

Method: 1) 首次对块Hadamard旋转进行系统性非渐近分析，发现异常值抑制受输入向量几何结构限制；2) 提出MixQuant框架，通过排列在旋转前重分布激活质量；3) 设计贪心质量扩散算法校准排列，均衡块间ℓ₁范数期望；4) 识别transformer中的排列等变区域，将排列合并到模型权重中以避免推理开销。

Result: MixQuant在所有块大小下持续提升精度，在Llama3 1B INT4量化中，块大小为16时恢复90%全向量旋转困惑度，相比无排列方法的46%有显著提升。

Conclusion: 块旋转的异常值抑制效果受输入向量几何结构限制，通过排列重分布激活质量可显著改善量化性能。MixQuant框架在不增加推理开销的情况下，有效提升了低比特量化精度。

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [207] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种基于占用度量的策略表示学习方法，通过变分生成和对比学习构建平滑的潜在空间，支持基于梯度的策略优化和行为合成


<details>
  <summary>Details</summary>
Motivation: 为了在测试时灵活地引导策略行为，需要学习能够表示一系列策略的表示方法。由于MDP中的策略由其占用度量唯一确定，因此考虑基于占用度量来建模策略表示

Method: 1) 将策略表示建模为状态-动作特征映射相对于占用度量的期望；2) 使用基于集合的架构近似策略表示；3) 采用变分生成方法构建平滑潜在空间；4) 使用对比学习使潜在距离与价值函数差异对齐；5) 支持在潜在空间中进行基于梯度的优化

Result: 构建了能够统一表示一系列策略的模型，潜在空间具有良好几何特性（距离与价值函数差异对齐），支持在潜在空间中直接进行梯度优化，能够解决未见过的价值函数约束下的行为合成任务

Conclusion: 该方法通过学习基于占用度量的策略表示，构建了具有良好几何特性的潜在空间，实现了无需额外训练即可满足新价值函数约束的策略引导，为灵活的行为合成提供了有效框架

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [208] [Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents](https://arxiv.org/abs/2601.22352)
*Sri Vatsa Vuddanti,Satwik Kumar Chittiprolu*

Main category: cs.LG

TL;DR: 语言模型代理的自我恢复能力遵循可测量的定量定律，通过期望恢复遗憾（ERR）与效率分数（ES）的一阶关系描述，该定律在不同规模模型和任务中得到验证。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在执行工具调用失败后常表现出自我恢复能力，但这种现象缺乏正式的理论解释。本文旨在填补这一空白，为执行层面的鲁棒性提供理论基础。

Method: 提出期望恢复遗憾（ERR）来形式化恢复能力，量化恢复策略与最优策略在随机执行噪声下的偏差。推导ERR与经验可观测量效率分数（ES）的一阶关系，形成可证伪的恢复动力学定量定律。在五个工具使用基准上进行实证验证，涵盖受控扰动、诊断推理和真实API。

Result: 在模型规模、扰动机制和恢复时间范围内，ERR-ES定律预测的遗憾与蒙特卡洛模拟观察到的失败后遗憾高度匹配（δ≤0.05）。恢复能力不是模型规模或架构的产物，而是交互动态的受控属性。

Conclusion: 语言代理的恢复能力遵循可测量的定量定律，为执行层面的鲁棒性提供了理论基础，表明恢复能力是交互动态的受控属性而非偶然现象。

Abstract: Language model agents often appear capable of self-recovery after failing tool call executions, yet this behavior lacks a formal explanation. We present a predictive theory that resolves this gap by showing that recoverability follows a measurable law. To elaborate, we formalize recoverability through Expected Recovery Regret (ERR), which quantifies the deviation of a recovery policy from the optimal one under stochastic execution noise, and derive a first-order relationship between ERR and an empirical observable quantity, the Efficiency Score (ES). This yields a falsifiable first-order quantitative law of recovery dynamics in tool-using agents. We empirically validate the law across five tool-use benchmarks spanning controlled perturbations, diagnostic reasoning, and real-world APIs. Across model scales, perturbation regimes, and recovery horizons, predicted regret under the ERR-ES law closely matched observed post-failure regret measured from Monte Carlo rollouts, within delta less than or equal to 0.05. Our results reveal that recoverability is not an artifact of model scale or architecture, but a governed property of interaction dynamics, providing a theoretical foundation for execution-level robustness in language agents.

</details>


### [209] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 本文提出基于最优传输框架，利用相对平移不变二次Wasserstein空间的锥几何，引入相对Wasserstein角和正交投影距离来量化经验分布与高斯性的偏差，并证明该空间中的填充锥是平坦的，将高斯近似重构为投影问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法量化经验分布与高斯性的偏差存在局限性，特别是常用的矩匹配高斯方法在Wasserstein距离意义上可能不是最近的高斯分布。需要一种基于最优传输几何框架的更准确、更稳健的非高斯性度量方法。

Method: 利用相对平移不变二次Wasserstein空间的锥几何，引入相对Wasserstein角和正交投影距离两个几何量。在一维情况下推导闭式表达式并扩展到经典分布族；在高维情况下开发基于半离散对偶公式的随机流形优化算法。

Result: 证明该空间中的填充锥是平坦的，确保角度、投影和内积严格定义。实验表明相对Wasserstein角比Wasserstein距离更稳健，提出的最近高斯分布比矩匹配在FID评分评估中提供更好的近似。

Conclusion: 基于最优传输的几何框架为量化非高斯性提供了新的视角，相对Wasserstein角和正交投影距离是有效的度量工具，最近高斯投影比矩匹配方法更优，在合成数据和真实特征分布评估中表现出色。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [210] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: PoSafeNet：基于偏序集结构安全约束的神经网络安全层，通过顺序闭式投影实现自适应安全执行选择


<details>
  <summary>Details</summary>
Motivation: 现有安全学习方法通常统一或按固定优先级顺序强制执行多个安全约束，导致不可行性和脆弱行为。实践中安全要求是异质的，只允许部分优先级关系，某些约束可比而其他约束本质上不可比。

Method: 将安全约束形式化为偏序集结构，将安全组合视为策略类的结构属性。提出PoSafeNet，一种可微分神经网络安全层，通过在偏序一致约束顺序下进行顺序闭式投影来强制执行安全，实现自适应选择或混合有效安全执行，同时通过构造保留优先级语义。

Result: 在多障碍物导航、受限机器人操作和基于视觉的自动驾驶实验中，相比非结构化和基于可微分二次规划的安全层，表现出改进的可行性、鲁棒性和可扩展性。

Conclusion: 偏序集结构安全约束建模为处理异质安全要求提供了更灵活和鲁棒的框架，PoSafeNet通过自适应安全执行选择在复杂环境中实现了更好的性能。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [211] [Small Talk, Big Impact: The Energy Cost of Thanking AI](https://arxiv.org/abs/2601.22357)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 量化分析LLM交互中礼貌用语（如"谢谢"）的能耗成本，揭示输入长度、输出长度和模型大小对能耗的影响，为构建可持续AI应用提供指导


<details>
  <summary>Details</summary>
Motivation: 随着LLM用户增长和每日数十亿提示处理，看似无害的礼貌用语（如"谢谢"）会产生累积的能源成本，需要量化这种成本以促进可持续AI部署

Method: 使用真实对话轨迹和细粒度能耗测量，量化输入长度、输出长度和模型大小对能源使用的影响，以礼貌用语作为可控可复现的代理指标

Result: 礼貌用语确实产生可量化的能源成本，输入长度、输出长度和模型大小显著影响能耗，为优化LLM应用效率提供具体数据

Conclusion: 理解并减轻LLM交互的能源成本对可持续AI部署至关重要，礼貌用语作为典型案例揭示了优化方向，有助于构建更环保高效的LLM应用

Abstract: Being polite is free - or is it? In this paper, we quantify the energy cost of seemingly innocuous messages such as ``thank you'' when interacting with large language models, often used by users to convey politeness. Using real-world conversation traces and fine-grained energy measurements, we quantify how input length, output length and model size affect energy use. While politeness is our motivating example, it also serves as a controlled and reproducible proxy for measuring the energy footprint of a typical LLM interaction. Our findings provide actionable insights for building more sustainable and efficient LLM applications, especially in increasingly widespread real-world contexts like chat. As user adoption grows and billions of prompts are processed daily, understanding and mitigating this cost becomes crucial - not just for efficiency, but for sustainable AI deployment.

</details>


### [212] [The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples](https://arxiv.org/abs/2601.22359)
*Hsiang Hsu,Pradeep Niroula,Zichang He,Ivan Brugere,Freddy Lecue,Chun-Fu Chen*

Main category: cs.LG

TL;DR: 论文提出机器遗忘中存在"残余知识"隐私风险：即使遗忘特定样本，其微小扰动版本仍能被模型识别，揭示遗忘样本信息在局部邻域中持续存在。作者提出RURK微调策略来缓解此风险。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法通过统计不可区分性提供保证，但这些保证无法自然扩展到对抗性扰动输入。遗忘样本的微小扰动仍可能被遗忘模型正确识别，而重新训练的模型却无法识别，这揭示了一种新的隐私风险：遗忘样本的信息可能在其局部邻域中持续存在。

Method: 提出RURK（Residual Knowledge Reduction）微调策略，通过惩罚模型重新识别扰动遗忘样本的能力来缓解残余知识风险。该方法在深度神经网络视觉基准上进行实验验证。

Result: 实验表明，残余知识在现有遗忘方法中普遍存在，而RURK方法能有效防止残余知识，提高机器遗忘的隐私安全性。

Conclusion: 残余知识是机器遗忘中不可避免的隐私漏洞，特别是在高维设置中。提出的RURK方法为缓解这一风险提供了有效解决方案，增强了机器遗忘在实际应用中的隐私保护能力。

Abstract: Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge.

</details>


### [213] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: LLM推理能耗受系统级设计选择影响巨大，包括数值精度、批处理策略和请求调度，这些因素可导致能耗差异达100倍


<details>
  <summary>Details</summary>
Motivation: 随着LLM在生产环境中的部署增加，计算资源和能源需求从训练转向推理。现有研究主要关注每个提示或每个token的能耗，但系统级设计选择对能耗的影响尚未充分探索

Method: 在NVIDIA H100 GPU上进行详细的LLM推理能耗和延迟实证研究，分析量化、批处理大小和服务配置（如Hugging Face的TGI服务器）的影响

Result: 1) 低精度格式仅在计算受限时带来能耗收益；2) 批处理提高能效，特别是在解码等内存受限阶段；3) 结构化请求时序可将每个请求的能耗降低高达100倍

Conclusion: 可持续的LLM部署不仅取决于模型内部，还依赖于服务栈的编排。研究结果支持基于阶段的能耗分析和系统级优化，以实现更绿色的AI服务

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [214] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: FIRE是一个免训练的多保真度回归框架，通过结合表格基础模型进行零样本上下文贝叶斯推理，利用高保真度校正模型对低保真度模型的后验预测分布进行条件化，实现跨保真度信息传递和鲁棒残差学习。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在多保真度回归中面临立方计算复杂度、对稀疏高保真度观测过拟合等问题，限制了实际应用中的效率和泛化能力。

Method: FIRE框架结合表格基础模型，通过低保真度模型的后验预测分布来条件化高保真度校正模型，实现零样本上下文贝叶斯推理，捕捉异方差误差，无需模型重新训练。

Result: 在31个基准问题（包括合成和真实世界任务如DrivAerNet、LCBench）上，FIRE在性能-时间权衡方面优于7种最先进的GP或深度学习多保真度回归方法，在准确性和不确定性量化方面排名最高，具有运行时优势。

Conclusion: FIRE提供了一种有效的免训练多保真度回归方法，但存在上下文窗口限制和对预训练表格基础模型质量的依赖等局限性。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [215] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: PABLO：一种完全基于语言代理的生物学黑盒优化方法，通过科学LLM进行层级化推理，在分子设计和抗菌肽优化中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有生物学设计方法主要依赖原始结构数据，难以充分利用丰富的科学文献知识。虽然LLMs已被引入，但仅局限于结构中心优化器中的狭窄角色，需要更全面的语言推理方法

Method: PABLO（Purely Agentic BLack-box Optimization）：分层代理系统，使用在化学和生物学文献上预训练的科学LLM来生成和迭代优化生物候选物，完全基于语言代理进行黑盒优化

Result: 在GuacaMol分子设计和抗菌肽优化任务中达到SOTA性能，显著提高样本效率和最终目标值；与现有方法相比，在优化循环中完全依赖LLM的情况下仍保持有竞争力的token使用量；体外验证显示优化的肽对耐药病原体具有强活性

Conclusion: PABLO展示了完全代理化、基于语言的生物学优化方法的有效性，能够自然地整合语义任务描述、检索增强的领域知识和复杂约束，具有实际治疗发现的潜力

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [216] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: G-Substrate：一种图表示学习框架，将图结构视为跨模态和任务共享的结构基板，通过统一的结构模式和角色训练策略实现结构知识的积累和重用。


<details>
  <summary>Details</summary>
Motivation: 当前图结构学习通常是模态和任务孤立的，每个任务单独构建图表示后就丢弃，导致跨模态和任务的结构规律需要重复重建，无法在中间图表示层面积累。

Method: 提出G-Substrate框架，包含两个机制：1）统一结构模式确保跨异质模态和任务的图表示兼容性；2）交错角色训练策略，在训练过程中让同一图结构暴露给多个功能角色。

Result: 在多个领域、模态和任务上的实验表明，G-Substrate优于任务孤立和朴素的多任务学习方法。

Conclusion: 将图结构视为持久的结构基板，通过G-Substrate框架组织学习，能够有效积累和重用跨模态和任务的结构知识，提升学习效果。

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [217] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR是一个基于LLM的自动扩缩框架，用于多阶段ML推理管道，通过上下文强化学习在线改进策略，无需梯度更新，显著降低延迟和资源成本。


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道难以自动扩缩，因为存在异构资源、跨阶段耦合和动态瓶颈迁移等挑战。现有方法难以有效处理这些复杂问题。

Method: 使用LLM作为上下文强化学习控制器，结合帕累托优势奖励塑造、惊奇引导的经验检索和用户空间CUDA拦截的细粒度GPU速率控制。

Result: 在四种ML服务管道和三种工作负载模式下，SAIR实现了最佳或并列最佳的P99延迟和有效资源成本，P99延迟提升达50%，有效成本降低达97%，瓶颈检测准确率达86%。

Conclusion: SAIR证明了LLM作为上下文强化学习控制器在多阶段ML推理管道自动扩缩中的有效性，无需离线训练即可实现显著的性能提升和成本节约。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [218] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: SIREN是一种新颖的可扩展方法，通过估计数据似然的评分函数来归因异常值的根本原因，在非线性、高维和异方差因果模型中实现可处理的、不确定性感知的根因归因。


<details>
  <summary>Details</summary>
Motivation: 识别异常值的根本原因是因果推断和异常检测中的基本问题。传统的基于启发式或反事实推理的方法在不确定性和高维依赖下往往表现不佳，需要一种更有效的方法来处理非线性、高维和异方差因果模型中的根因归因问题。

Method: SIREN通过估计数据似然的评分函数来归因异常值的根本原因。归因计算通过积分梯度实现，沿着从异常值向正常数据分布的路径累积评分贡献。该方法满足四个经典Shapley值公理中的三个（虚拟性、效率性和线性性），以及从底层因果结构导出的不对称公理。

Result: 在合成随机图和真实世界云服务及供应链数据集上的大量实验表明，SIREN在归因准确性和计算效率方面都优于最先进的基线方法。

Conclusion: SIREN提供了一种直接操作评分函数的新方法，能够在非线性、高维和异方差因果模型中实现可处理的、不确定性感知的根因归因，解决了传统方法在不确定性和高维依赖下的局限性。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [219] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了首个多模态联邦图学习（MMFGL）基准MM-OpenFGL，包含19个数据集、8种模拟策略、6个下游任务和57种方法，系统评估了MMFGL的必要性、有效性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的多模态属性图（MMAGs）通常分布在隔离平台，由于隐私或商业限制无法共享，而现有联邦图学习研究主要关注单模态图，缺乏针对多模态联邦图学习的系统基准。

Method: 构建了MM-OpenFGL基准，包括：19个多模态数据集覆盖7个应用领域，8种模拟策略捕捉模态和拓扑变化，6个下游任务，57种最先进方法通过模块化API实现，并进行全面实验评估。

Result: 通过大量实验从必要性、有效性、鲁棒性和效率四个角度深入研究了MMFGL，为未来MMFGL研究提供了有价值的见解和系统评估框架。

Conclusion: MM-OpenFGL是首个系统形式化MMFGL范式并提供严格评估的综合性基准，填补了多模态联邦图学习研究空白，为相关领域发展奠定了基础。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [220] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: MetaLead是一个完全人工标注的机器学习排行榜数据集，它捕获所有实验结果以实现结果透明性，并包含额外元数据，用于更细致和透明的ML评估。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域需要排行榜来基准测试和跟踪进展，但传统创建方式需要大量人工努力。现有的自动化排行榜生成数据集存在局限性：只捕获每篇论文的最佳结果且元数据有限，无法支持更透明和细致的评估。

Method: 提出MetaLead数据集，这是一个完全人工标注的ML排行榜数据集。它捕获所有实验结果（不只是最佳结果），包含额外元数据如实验类型（基线、提出方法或变体方法），并明确分离训练和测试数据集以支持跨领域评估。

Result: MetaLead提供了丰富的结构，使其成为更透明和细致评估ML研究的强大资源。数据集支持实验类型指导的比较和跨领域评估，超越了传统排行榜的局限性。

Conclusion: MetaLead通过捕获所有实验结果和丰富元数据，为机器学习研究提供了更透明、更细致的评估框架，解决了现有排行榜数据集的局限性，支持更全面的基准测试和分析。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [221] [CoDCL: Counterfactual Data Augmentation Contrastive Learning for Continuous-Time Dynamic Network Link Prediction](https://arxiv.org/abs/2601.22427)
*Hantong Feng,Yonggang Wu,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: CoDCL是一个动态网络学习框架，结合反事实数据增强和对比学习来提高模型对结构变化的鲁棒性，可作为即插即用模块集成到现有时序图模型中。


<details>
  <summary>Details</summary>
Motivation: 动态网络的快速增长和持续结构演化使得预测变得越来越困难。模型需要适应复杂的时间环境，并对新出现的结构变化具有鲁棒性。

Method: 提出CoDCL框架，结合反事实数据增强和对比学习。设计了全面的高质量反事实数据生成策略，包括动态处理设计和高效结构邻域探索，以量化交互模式的时间变化。整个框架设计为即插即用模块，可无缝集成到各种现有时序图模型中。

Result: 在多个真实世界数据集上的广泛实验表明，CoDCL显著提升了动态网络领域最先进的基线模型性能，证实了将反事实数据增强集成到动态表示学习中的关键作用。

Conclusion: CoDCL通过结合反事实数据增强和对比学习，有效提高了动态网络预测模型对结构变化的适应能力，且具有良好的通用性和可集成性。

Abstract: The rapid growth and continuous structural evolution of dynamic networks make effective predictions increasingly challenging. To enable prediction models to adapt to complex temporal environments, they need to be robust to emerging structural changes. We propose a dynamic network learning framework CoDCL, which combines counterfactual data augmentation with contrastive learning to address this deficiency.Furthermore, we devise a comprehensive strategy to generate high-quality counterfactual data, combining a dynamic treatments design with efficient structural neighborhood exploration to quantify the temporal changes in interaction patterns.Crucially, the entire CoDCL is designed as a plug-and-play universal module that can be seamlessly integrated into various existing temporal graph models without requiring architectural modifications.Extensive experiments on multiple real-world datasets demonstrate that CoDCL significantly gains state-of-the-art baseline models in the field of dynamic networks, confirming the critical role of integrating counterfactual data augmentation into dynamic representation learning.

</details>


### [222] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出一种用于LLM推理的显式对比学习方法，替代GRPO的优势估计方法，通过将结果分为正负集合并最大化正结果概率，在数学基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: GRPO虽然有效，但其改进方法（如非对称裁剪和零方差数据过滤）需要大量经验洞察且难以识别。作者希望提出更直接的方法来提升LLM的推理能力。

Method: 提出显式对比学习方法：将K个结果分为正负集合，然后最大化正结果的似然。该方法可视为LLM推理的多标签噪声对比估计的在线实例化。

Result: 在一系列具有挑战性的数学基准测试中，该方法与DAPO和在线DPO等强基线相比表现出竞争性性能。

Conclusion: 提出的显式对比学习方法为LLM推理提供了一种有效的替代方案，避免了GRPO中需要经验性调整的复杂改进措施。

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [223] [AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism](https://arxiv.org/abs/2601.22442)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Gil Avraham,Hadi Mohaghegh Dolatabadi,Chamin P Hewa Koneputugodage,Violetta Shevchenko,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: 提出异步更新方法解决数据并行和流水线并行中的通信瓶颈，通过权重前瞻和稀疏平均技术降低通信开销，在10亿参数模型上达到与同步基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 数据并行和流水线并行是分布式神经网络训练的关键策略，但其高通信成本要求计算集群具有快速互连，限制了可扩展性。需要解决通信瓶颈问题。

Method: 1) 在流水线并行中采用权重前瞻方法；2) 在数据并行中引入异步稀疏平均方法，配备基于指数移动平均的校正机制；3) 提供稀疏平均和异步更新的收敛保证。

Result: 在大规模语言模型（高达10亿参数）上的实验表明，该方法在显著减少通信开销的同时，能够匹配完全同步基线的性能。

Conclusion: 通过异步更新方法有效解决了分布式训练中的通信瓶颈问题，在保持性能的同时显著降低了通信开销，提高了可扩展性。

Abstract: Data and pipeline parallelism are key strategies for scaling neural network training across distributed devices, but their high communication cost necessitates co-located computing clusters with fast interconnects, limiting their scalability. We address this communication bottleneck by introducing asynchronous updates across both parallelism axes, relaxing the co-location requirement at the expense of introducing staleness between pipeline stages and data parallel replicas. To mitigate staleness, for pipeline parallelism, we adopt a weight look-ahead approach, and for data parallelism, we introduce an asynchronous sparse averaging method equipped with an exponential moving average based correction mechanism. We provide convergence guarantees for both sparse averaging and asynchronous updates. Experiments on large-scale language models (up to \em 1B parameters) demonstrate that our approach matches the performance of the fully synchronous baseline, while significantly reducing communication overhead.

</details>


### [224] [Automating Forecasting Question Generation and Resolution for AI Evaluation](https://arxiv.org/abs/2601.22444)
*Nikos I. Bosse,Peter Mühlbacher,Jack Wildman,Lawrence Phillips,Dan Schwarz*

Main category: cs.LG

TL;DR: 开发了一个基于LLM的自动化系统，用于大规模生成和解决高质量的预测问题，该系统在多样性和准确性方面超越了人工平台。


<details>
  <summary>Details</summary>
Motivation: 预测未来事件对决策制定至关重要，也是衡量通用智能的重要指标。然而，开发和评估AI预测系统需要大量多样化且具有挑战性的问题，而传统方法依赖重复数据源，限制了多样性和实用性。

Method: 构建了一个基于LLM的网页研究代理系统，能够自动生成和解决预测问题。系统使用LLM驱动的代理进行网络研究，生成1499个多样化的现实世界预测问题，并在数月后自动解决这些问题。

Result: 系统生成可验证、无歧义问题的准确率约96%，超过领先的人工策划平台Metaculus。问题解决准确率约95%。更智能的LLM（Gemini 3 Pro、GPT-5、Gemini 2.5 Flash）在预测性能上表现更好。通过问题分解策略，Brier分数从0.141提升到0.132。

Conclusion: 基于LLM的自动化系统能够大规模生成和解决高质量的预测问题，为AI预测能力的评估和改进提供了有效工具，展示了LLM在复杂预测任务中的潜力。

Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141).

</details>


### [225] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 提出基于权重的稀疏自编码器特征解释框架，无需激活数据，通过直接权重交互测量功能效应，发现1/4特征直接预测输出token，特征参与注意力机制具有深度依赖结构，语义和非语义特征在注意力电路中分布不同。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器特征解释方法主要依赖激活模式推断语义，但忽略了特征训练的目标是重建在前向传播中具有计算作用的激活。需要一种能够直接测量特征功能效应的解释框架。

Method: 提出基于权重的解释框架，通过直接分析权重交互来测量特征的功能效应，无需激活数据。在Gemma-2和Llama-3.1模型上进行三个实验验证。

Result: 1) 1/4的特征直接预测输出token；2) 特征积极参与注意力机制，具有深度依赖的结构；3) 语义和非语义特征在注意力电路中表现出不同的分布特征。

Conclusion: 该权重解释框架提供了稀疏自编码器特征解释中缺失的"上下文外"部分，能够更全面地理解特征的功能角色，特别是它们在模型计算中的作用。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [226] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA是一种用于RLVR训练的高效采样方法，通过堆采样和在线查询增强来优化提示池管理，减少计算成本同时保持性能


<details>
  <summary>Details</summary>
Motivation: 当前RLVR训练中，提示池通常是静态的或与模型学习进度松散关联，均匀采样无法适应能力边界的变化，导致在已解决或无法解决的提示上浪费计算资源

Method: HeaPA维护有界演化提示池，使用基于堆的边界采样追踪能力边界，通过轻量级异步验证的在线查询增强扩展池，并通过拓扑感知的统计重估计和受控重插入稳定相关查询

Result: 在两个训练语料库、两种训练方法和七个基准测试中，HeaPA持续提高准确性，以更少计算达到目标性能，同时保持实际时间可比，且模型规模越大收益越明显

Conclusion: HeaPA通过前沿聚焦采样和在线池增长有效提升RLVR训练效率，特别适合大规模模型训练，代码已开源

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [227] [Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity](https://arxiv.org/abs/2601.22450)
*Jianhao Huang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 该论文研究了掩码扩散语言模型在k-parity问题上的泛化特性，发现MD目标改变了学习景观，避免了grokking现象，并通过优化掩码概率分布显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型作为强大的生成范式，其泛化特性相比自回归模型研究不足。本文旨在研究MD模型在k-parity问题上的学习行为，特别是与神经网络中常见的grokking现象（长时间平台期后突然泛化）的对比。

Method: 1. 理论分析：将掩码扩散目标分解为驱动特征学习的信号机制和作为隐式正则化的噪声机制；2. 实验验证：在k-parity问题上使用MD目标训练nanoGPT模型；3. 优化方法：基于理论洞察优化掩码概率分布。

Result: 1. MD目标从根本上改变了学习景观，实现了快速且同时的泛化，避免了grokking现象；2. 优化的掩码概率分布在5000万参数模型上显著改善了困惑度；3. 在80亿参数模型上，从头预训练和监督微调分别获得了8.8%和5.8%的性能提升，证明了框架的可扩展性和有效性。

Conclusion: 掩码扩散语言模型在k-parity问题上表现出与自回归模型不同的学习动态，通过理论分解MD目标并优化掩码概率分布，可以显著提升模型性能，特别是在大规模语言模型设置中具有可扩展性和有效性。

Abstract: Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the XOR sum of $k$ relevant bits), where neural networks typically exhibit grokking -- a prolonged plateau of chance-level performance followed by sudden generalization. We theoretically decompose the Masked Diffusion (MD) objective into a Signal regime which drives feature learning, and a Noise regime which serves as an implicit regularizer. By training nanoGPT using MD objective on the $k$-parity problem, we demonstrate that MD objective fundamentally alters the learning landscape, enabling rapid and simultaneous generalization without experiencing grokking. Furthermore, we leverage our theoretical insights to optimize the distribution of the mask probability in the MD objective. Our method significantly improves perplexity for 50M-parameter models and achieves superior results across both pre-training from scratch and supervised fine-tuning. Specifically, we observe performance gains peaking at $8.8\%$ and $5.8\%$, respectively, on 8B-parameter models, confirming the scalability and effectiveness of our framework in large-scale masked diffusion language model regimes.

</details>


### [228] [Temporal Graph Pattern Machine](https://arxiv.org/abs/2601.22454)
*Yijun Ma,Zehong Wang,Weixiang Sun,Yanfang Ye*

Main category: cs.LG

TL;DR: TGPM是一个时序图基础框架，通过交互补丁和Transformer架构学习通用的演化模式，在链接预测任务中实现SOTA性能并具有优秀的跨领域可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有时序图学习方法主要面向特定任务，且依赖短期依赖建模、静态邻域语义和回顾性时间使用等限制性假设，阻碍了可迁移的时序演化机制的发现。

Method: 提出Temporal Graph Pattern Machine (TGPM)：1) 将每个交互视为通过时序偏置随机游走合成的交互补丁，捕获多尺度结构语义和长程依赖；2) 使用Transformer骨干网络捕捉全局时序规律并适应上下文特定的交互动态；3) 引入掩码标记建模和下一时间预测等自监督预训练任务来编码网络演化的基本规律。

Result: 在转导和归纳链接预测任务中均取得最先进的性能，展现出卓越的跨领域可迁移性。

Conclusion: TGPM通过直接学习通用演化模式，成功克服了现有方法的局限性，为时序图学习提供了一个强大的基础框架。

Abstract: Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as short-term dependency modeling, static neighborhood semantics, and retrospective time usage. These constraints hinder the discovery of transferable temporal evolution mechanisms. To address this, we propose the Temporal Graph Pattern Machine (TGPM), a foundation framework that shifts the focus toward directly learning generalized evolving patterns. TGPM conceptualizes each interaction as an interaction patch synthesized via temporally-biased random walks, thereby capturing multi-scale structural semantics and long-range dependencies that extend beyond immediate neighborhoods. These patches are processed by a Transformer-based backbone designed to capture global temporal regularities while adapting to context-specific interaction dynamics. To further empower the model, we introduce a suite of self-supervised pre-training tasks -- specifically masked token modeling and next-time prediction -- to explicitly encode the fundamental laws of network evolution. Extensive experiments show that TGPM consistently achieves state-of-the-art performance in both transductive and inductive link prediction, demonstrating exceptional cross-domain transferability.

</details>


### [229] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: LOFT：一种基于低维特征子空间的机器遗忘方法，通过优化小型投影矩阵在预训练模型中实现高效遗忘，避免数据重载隐私风险


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在两个关键问题：1）需要大量重载原始数据导致隐私泄露风险；2）需要更新整个预训练模型效率低下。本文提出基于低维特征子空间的新视角，利用剩余数据和遗忘数据在子空间中的可分离性

Method: LOFT方法在预训练模型的低维特征子空间中执行遗忘操作，通过主投影优化捕获剩余数据信息同时减少遗忘数据信息。仅需优化小型投影矩阵并一次性获取特征，无需重复访问原始数据

Result: 实验验证LOFT在多种模型、数据集、任务和应用中显著降低计算开销，同时保持优异的遗忘性能

Conclusion: LOFT提供了一种高效、隐私安全的机器遗忘方法，通过低维特征子空间优化解决了现有方法的数据重载隐私风险和模型更新效率问题

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [230] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: 提出EvoEGF-Mol模型，通过信息几何方法在Fisher-Rao度量下沿指数测地线定义生成流，解决SBDD中欧几里得空间与概率空间不匹配问题，实现稳定的药物分子生成。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的药物设计方法在欧几里得空间和概率空间中分别构建概率路径，导致与底层统计流形不匹配。需要从信息几何角度解决这一问题。

Method: 将分子建模为复合指数族分布，在Fisher-Rao度量下沿指数测地线定义生成流。为避免直接以狄拉克分布为目标导致的轨迹崩溃，提出EvoEGF-Mol模型，用动态集中分布替代静态狄拉克目标，通过渐进参数精化架构确保稳定训练。

Result: 在CrossDock上达到93.4%的PoseBusters通过率，表现出卓越的几何精度和相互作用保真度。在MolGenBench任务上超越基线，能够恢复生物活性支架并生成符合MedChem过滤器的候选分子。

Conclusion: EvoEGF-Mol通过信息几何方法有效解决了SBDD中的流形不匹配问题，实现了稳定高效的药物分子生成，在几何精度和化学合理性方面表现出色。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [231] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: LLMs也表现出心理学中的潜在学习现象：在无奖励探索阶段，LLMs能组织任务相关知识而不受奖励偏差约束，随后引入奖励时性能进一步提升，最终表现优于全程奖励强化学习。


<details>
  <summary>Details</summary>
Motivation: 心理学中的潜在学习现象表明生物体可以在无奖励情况下获取环境表征，而当前LLMs主要依赖奖励驱动的强化学习范式，限制了灵活性和泛化能力。研究LLMs是否也存在潜在学习现象，以及如何利用这一现象提升模型性能。

Method: 采用两阶段训练范式：第一阶段进行无奖励探索，让LLMs组织任务相关知识；第二阶段引入奖励进行训练。在多个模型家族和多样化任务领域进行广泛实验，并提供理论分析解释无奖励探索为何能带来性能提升。

Result: LLMs确实表现出潜在学习动态：无奖励探索阶段带来适度性能改进，引入奖励后性能进一步提升。采用这种两阶段探索机制后训练的LLMs，最终能力优于全程使用奖励强化学习训练的模型。

Conclusion: LLMs中存在与生物体相似的潜在学习现象，无奖励探索阶段有助于模型组织知识而不受奖励偏差约束，为改进LLMs训练范式提供了新思路，将心理学原理与AI训练相结合。

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [232] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出一个教师-学生框架，将持续强化学习解耦为两个独立过程：通过分布式RL训练单任务教师模型，然后持续蒸馏到中央通用模型中，结合MoE架构和回放方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习面临稳定性-可塑性困境，直接应用RL到序列任务流中难以实现可扩展性能。观察到RL擅长解决单任务，而策略蒸馏作为相对稳定的监督学习过程，更适合大规模基础模型和多任务学习。

Method: 1) 使用分布式RL训练单任务教师模型；2) 通过持续策略蒸馏将教师模型知识转移到中央通用学生模型；3) 采用混合专家(MoE)架构增强可塑性；4) 使用回放方法提升稳定性。

Result: 在Meta-World基准测试中，框架能够恢复超过85%的教师模型性能，同时将任务间遗忘控制在10%以内，实现了高效的持续强化学习。

Conclusion: 提出的教师-学生框架通过解耦单任务RL训练和多任务知识蒸馏，有效解决了持续强化学习的挑战，结合MoE和回放方法平衡了稳定性与可塑性。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [233] [TTCS: Test-Time Curriculum Synthesis for Self-Evolving](https://arxiv.org/abs/2601.22628)
*Chengyi Yang,Zhishang Xiang,Yunbo Tang,Zongpei Teng,Chengsong Huang,Fei Long,Yuhan Liu,Jinsong Su*

Main category: cs.LG

TL;DR: TTCS提出了一种协同进化的测试时训练框架，通过问题合成器和推理求解器的迭代优化，为LLMs构建动态测试时课程，提升在困难推理问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有测试时训练方法在处理困难推理问题时存在两个主要问题：原始测试问题往往过于困难，难以产生高质量的伪标签；测试集规模有限，连续在线更新容易导致不稳定。

Method: TTCS从同一预训练模型初始化两个策略：问题合成器和推理求解器。通过迭代优化，合成器根据测试问题生成逐步挑战性的问题变体，构建结构化课程；求解器使用在原始测试和合成问题上的多响应自一致性奖励进行更新。求解器的反馈指导合成器生成与模型当前能力匹配的问题。

Result: 实验表明TTCS在具有挑战性的数学基准上持续增强推理能力，并能迁移到不同LLM骨干网络的一般领域任务，展示了为自进化构建动态测试时课程的可扩展路径。

Conclusion: TTCS通过协同进化的测试时训练框架，解决了现有方法在处理困难推理问题时的局限性，为LLMs的自进化提供了动态构建测试时课程的有效方法。

Abstract: Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.

</details>


### [234] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: TA-GRPO通过生成语义等价的变换问题变体，跨组池化奖励计算优势，解决了GRPO的多样性崩溃和梯度消失问题，在数学推理基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对表面表达变化敏感，GRPO方法存在多样性崩溃（只强化单一解法）和梯度消失（大量问题梯度为零）两大问题，需要改进推理能力。

Method: 提出TA-GRPO方法：为每个问题生成语义等价的变换变体（如改写、变量重命名、格式变化），通过跨整个组池化奖励来计算优势，确保混合奖励并促进多种解法策略。

Result: 在数学推理基准上取得一致的Pass@k提升，竞赛数学（AMC12, AIME24）提升高达9.84分，分布外科学推理（GPQA-Diamond）提升5.05分。

Conclusion: TA-GRPO通过变换增强和池化奖励有效解决了GRPO的失败模式，减少零梯度概率，通过降低训练-测试分布偏移改善泛化能力。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [235] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 该论文对LoRA变体进行了首次统一研究，提出系统分类法、统一理论框架、模块化代码库和标准化评估，发现LoRA对学习率敏感，且适当配置下性能优于多数变体。


<details>
  <summary>Details</summary>
Motivation: LoRA变体在方法、理论、代码和评估方面存在碎片化问题，需要系统性的统一研究来理清各种变体的关系、提供标准化工具和评估基准。

Method: 1) 从四个主要维度对LoRA变体进行分类：秩、优化动态、初始化和与Mixture-of-Experts的集成；2) 在低秩更新动态的统一理论框架下分析变体关系；3) 开发模块化代码库LoRAFactory支持即插即用实验；4) 在自然语言生成、理解和图像分类任务上进行大规模评估。

Result: 研究发现：1) LoRA及其变体对学习率的选择比其他超参数更敏感；2) 通过适当的超参数配置，原始LoRA能够匹配或超越大多数变体的性能。

Conclusion: 该研究为LoRA变体提供了首个系统性统一框架，揭示了原始LoRA在适当配置下的竞争力，为未来研究提供了标准化工具和评估基准。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [236] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: STARS框架通过监测隐藏状态的L2距离尖峰来检测推理过程中的认知惯性，并注入状态感知的语言提示来实时引导模型，无需额外训练即可优化大型推理模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然通过扩展测试时计算取得了显著性能，但经常遭受认知惯性的困扰，表现为过度思考或推理僵化。现有检测方法通常依赖表面文本启发式（如自我纠正标记），难以捕捉模型未表达的内部冲突。

Method: 提出STARS（尖峰触发自适应推理引导）框架，通过监测隐藏状态的L2距离尖峰来识别认知转折点，使用几何轨迹分析诊断转换的结构性质，并注入状态感知的语言提示来实时引导模型。

Result: 在多样化基准测试中，STARS能有效减少冗余循环，同时通过自适应纠正错误轨迹来提高准确性，为大型推理模型提供了无需额外微调的鲁棒无监督优化机制。

Conclusion: STARS提供了一种无需训练的方法来检测和纠正大型推理模型的认知惯性，通过监测潜在动态并实时引导推理过程，显著优化了模型的推理效率和质量。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [237] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: Sombrero是一种改进分层序列模型边界学习的方法，通过边界富集度指标B引导边界放置，使用置信对齐损失和加权平滑技术，在多种文本和代码数据上提升预测准确性与计算效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 分层序列模型虽然能学习有意义的边界分割，但难以定量评估和系统性地引导计算资源分配。现有方法缺乏有效的指标来衡量边界质量，也无法精确控制边界放置位置。

Method: 提出边界富集度B作为边界质量指标，衡量块起始位置在高预测难度字节处的集中程度。在此基础上开发Sombrero方法：1）使用置信对齐边界损失引导边界向预测困难位置放置；2）在输入级别应用置信加权平滑而非已实现块上，稳定边界学习。

Result: 在10亿参数规模上，覆盖英语、德语文本以及代码和数学内容的UTF-8语料上，Sombrero改善了准确性与效率的权衡，产生的边界能更一致地将计算资源与难以预测的位置对齐。

Conclusion: 边界富集度B为评估边界质量提供了有效指标，Sombrero方法通过引导边界放置和稳定学习过程，实现了更好的计算资源分配，提升了分层序列模型的性能。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [238] [Elastic Spectral State Space Models for Budgeted Inference](https://arxiv.org/abs/2601.22488)
*Dachuan Song,Xuan Wang*

Main category: cs.LG

TL;DR: ES-SSM：只需一次全容量训练，即可在推理时按任意规模截断，实现弹性计算部署


<details>
  <summary>Details</summary>
Motivation: 基础模型通常在固定计算能力下训练，但实际部署需要适应不同资源约束。现有方法需要训练多个模型变体或进行蒸馏，无法在运行时进行细粒度调整。

Method: 基于Hankel谱滤波的状态空间模型，结合轻量级输入自适应门控，在随机谱预算下训练。使用共享掩码归一化规则，使预测能力集中在低索引分量，高索引分量作为细化。

Result: 单个ES-SSM模型一次训练后，截断到不同参数规模都能与Transformer和SSM基线竞争。在各种运行时预算下，截断水平与性能呈现平滑稳定的曲线。

Conclusion: ES-SSM实现了训练一次、弹性部署的能力，为资源受限场景提供了灵活的模型缩放方案。

Abstract: Foundation models are typically trained at a fixed computational capacity, while real-world applications require deployment across platforms with different resource constraints. Current approaches usually rely on training families of model variants or model distillation, which requires additional training and supports only a pre-selected set of sizes rather than fine-grained adaptation at runtime. In this paper, we propose Elastic Spectral State Space Models (ES-SSM), which require only one-time training at full capacity, but can be directly truncated into arbitrary scales for budgeted, runtime inference without retraining. Our ES-SSM builds on Hankel spectral filtering over a state space model (SSM), coupled with a lightweight input-adaptive gate trained under randomized spectral budgets. Using a shared masked normalization rule over the ordered spectral channels, we encourage predictive capability to concentrate in low-index components, while higher-index components act primarily as refinement. We test our algorithm across long-sequence benchmarks spanning text, logic, retrieval, vision, and audio. We demonstrate that a single ES-SSM model trained once can be truncated to provide competitive performance compared with modern Transformer and SSM baselines at similar parameter scales. Furthermore, by testing under various runtime budgets, we observe smooth and stable budget-performance curves over a wide range of truncation levels.

</details>


### [239] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: 提出Gradual Fine-Tuning (GFT)框架，用于在目标分布样本可用时微调基于流的生成模型，通过温度控制平滑过渡，改善收敛稳定性并缩短推理路径。


<details>
  <summary>Details</summary>
Motivation: 在数据有限、分布演化或效率要求严格的场景中，无约束的微调会损害预训练获得的准确性和效率增益。现有奖励微调方法常对漂移结构或训练技术施加限制，需要更通用的微调框架。

Method: 提出渐进微调(GFT)框架，为随机流定义温度控制的中间目标序列，平滑插值预训练和目标漂移。证明边际和条件GFT目标的收敛性，支持使用最优传输等耦合方法。

Result: GFT改善收敛稳定性，缩短概率路径，实现更快推理，同时保持与标准微调相当的生成质量。为分布偏移下流匹配模型的可扩展适应提供理论依据和实践效果。

Conclusion: GFT为流匹配模型在分布偏移下的可扩展适应提供了理论基础和实用有效的替代方案，解决了有限数据、演化分布和效率需求下的微调挑战。

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [240] [MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models](https://arxiv.org/abs/2601.22887)
*Yangyan Li*

Main category: cs.LG

TL;DR: MoVE是一种解耦模型容量与计算成本的机制，通过引入全局可学习的值嵌入库，让模型能够独立于网络深度扩展参数记忆，在文本和图像生成任务中均取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 自回归序列建模是生成式AI的核心，但其存在模型容量与计算成本刚性耦合的局限性：扩展模型参数记忆需要加深或加宽网络，导致计算成本成比例增加。

Method: 提出MoVE（混合值嵌入）机制，引入全局共享的可学习值嵌入库，通过可微分软门控机制动态混合从该库中检索的概念到标准值投影中，使参数记忆能够独立于网络深度进行扩展。

Result: 在文本生成和图像生成两个代表性自回归建模应用中，MoVE相比标准和分层记忆基线均获得一致的性能改进，能够构建在相同计算预算下具有更低困惑度和更高保真度的"记忆密集"模型。

Conclusion: MoVE成功解耦了模型容量与计算成本，为扩展模型能力提供了新维度，在保持计算效率的同时显著提升了模型性能。

Abstract: Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost: expanding a model's parametric memory -- its repository of factual knowledge or visual patterns -- traditionally requires deepening or widening the network, which incurs a proportional rise in active FLOPs. In this work, we introduce $\textbf{MoVE (Mixture of Value Embeddings)}$, a mechanism that breaks this coupling and establishes a new axis for scaling capacity. MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers. For every step in the sequence, the model employs a differentiable soft gating mechanism to dynamically mix retrieved concepts from this bank into the standard value projection. This architecture allows parametric memory to be scaled independently of network depth by simply increasing the number of embedding slots. We validate MoVE through strictly controlled experiments on two representative applications of autoregressive modeling: Text Generation and Image Generation. In both domains, MoVE yields consistent performance improvements over standard and layer-wise memory baselines, enabling the construction of "memory-dense" models that achieve lower perplexity and higher fidelity than their dense counterparts at comparable compute budgets.

</details>


### [241] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 离线目标条件强化学习中，传统基于价值函数的子目标表示可能无法区分需要不同动作的目标状态，本文提出动作充分性框架并证明基于演员策略学习表示更优。


<details>
  <summary>Details</summary>
Motivation: 现有分层GCRL方法通过价值函数学习目标表示，隐含假设价值充分性足以支持最优控制。但本文发现即使价值估计准确，这种表示可能无法区分需要不同动作的目标状态，导致控制失败。

Method: 提出信息论框架定义动作充分性条件，证明价值充分性不蕴含动作充分性。通过实验验证动作充分性与控制成功更强相关，并发现标准对数损失训练的低层策略自然诱导动作充分表示。

Result: 在流行基准测试中，基于演员策略学习的目标表示始终优于基于价值估计学习的表示，验证了动作充分性框架的有效性。

Conclusion: 在分层GCRL中，目标表示应满足动作充分性而非仅价值充分性，基于演员策略学习的表示能更好支持最优控制，为分层强化学习架构设计提供新视角。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [242] [Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks](https://arxiv.org/abs/2601.22509)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.LG

TL;DR: 提出DREE框架解决VRP神经网络求解器在持续漂移任务下的终身学习问题，通过双重回放和经验增强提高学习效率并缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有VRP神经网络求解器要么在固定任务集上一次性训练，要么在顺序到达的多个任务上终身学习，但都假设每个任务有足够训练资源。这忽略了现实世界中问题模式会随时间持续漂移，产生大量顺序任务但每个任务训练资源有限的情况。

Method: 提出DREE（Dual Replay with Experience Enhancement）框架，通过双重回放和经验增强机制，在持续漂移的任务序列中提高学习效率并缓解灾难性遗忘。

Result: 实验表明，在持续漂移条件下，DREE能有效学习新任务、保留先前知识、提高对未见任务的泛化能力，并可应用于多种现有神经求解器。

Conclusion: DREE为VRP神经求解器在持续漂移任务下的终身学习提供了有效解决方案，解决了训练资源有限但任务持续变化的现实挑战。

Abstract: Existing neural solvers for vehicle routing problems (VRPs) are typically trained either in a one-off manner on a fixed set of pre-defined tasks or in a lifelong manner on several tasks arriving sequentially, assuming sufficient training on each task. Both settings overlook a common real-world property: problem patterns may drift continually over time, yielding massive tasks sequentially arising while offering only limited training resources per task. In this paper, we study a novel lifelong learning paradigm for neural VRP solvers under continually drifting tasks over learning time steps, where sufficient training for any given task at any time is not available. We propose Dual Replay with Experience Enhancement (DREE), a general framework to improve learning efficiency and mitigate catastrophic forgetting under such drift. Extensive experiments show that, under such continual drift, DREE effectively learns new tasks, preserves prior knowledge, improves generalization to unseen tasks, and can be applied to diverse existing neural solvers.

</details>


### [243] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出一种端到端可学习的权重置换框架，通过可学习的置换成本矩阵、可微分二分图匹配求解器和稀疏优化损失函数，优化Transformer模型的结构化稀疏剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏已成为流行的模型剪枝技术，权重置换能通过重新排序权重模式来提升剪枝后性能。然而，Transformer架构规模导致置换搜索空间指数增长，现有方法依赖贪心或启发式算法，限制了重排序的有效性。

Method: 1) 引入可学习的置换成本矩阵，量化权重矩阵任意两个输入通道交换的成本；2) 使用可微分二分图匹配求解器，基于成本矩阵获得最优二进制置换矩阵；3) 设计稀疏优化损失函数，直接优化置换算子。

Result: 在视觉和语言Transformer上广泛验证，该方法在结构化稀疏方面实现了最先进的置换结果。

Conclusion: 提出的端到端可学习置换框架能有效解决大规模Transformer架构的权重置换问题，提升结构化稀疏剪枝性能。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [244] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: Transformer模型在合成算术任务中展现出"破碎的组合性"——它们不按人类顺序规则可靠构建技能组合，而是以反向或并行方式习得技能，导致分布偏移下的意外混合错误。


<details>
  <summary>Details</summary>
Motivation: 尽管最近研究揭示了LLMs与人类在技能组合上的差异，但技能组合的学习动态及其非人类行为的根本原因仍然不清楚。本研究旨在通过合成算术任务探究学习动态机制。

Method: 在合成算术任务上训练Transformer模型，通过广泛的消融实验和细粒度诊断指标分析学习动态。

Result: 发现Transformer不按人类顺序规则可靠构建技能组合，而是经常以反向或并行方式习得技能，导致分布偏移下的意外混合错误（称为"破碎的组合性"）。证据表明是训练数据的相关性匹配而非因果或程序性组合塑造了学习动态。这种现象在现代LLMs中持续存在，且无法通过纯模型缩放或基于草稿的推理缓解。

Conclusion: 模型的学习行为与期望的技能组合之间存在根本性不匹配，这对推理可靠性、分布外鲁棒性和对齐具有重要影响。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [245] [Mem-T: Densifying Rewards for Long-Horizon Memory Agents](https://arxiv.org/abs/2601.23014)
*Yanwei Yue,Guibin Zhang,Boci Peng,Xuanbo Fan,Jiaxin Guo,Qiankun Li,Yan Zhang*

Main category: cs.LG

TL;DR: Mem-T是一个自主记忆代理，通过轻量级分层记忆数据库进行动态更新和多轮检索，使用MoT-GRPO树引导强化学习框架解决稀疏奖励问题，实现高性能和经济高效的内存管理。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理的训练范式受限：代理需要经历长序列的记忆操作才能获得稀疏延迟奖励，这阻碍了内存管理策略的真正端到端优化。

Method: 提出Mem-T自主记忆代理，结合轻量级分层记忆数据库进行动态更新和多轮检索；提出MoT-GRPO树引导强化学习框架，通过记忆操作树反向传播和事后信用分配将稀疏终端反馈转化为密集的步级监督。

Result: Mem-T性能优越（超越A-Mem和Mem0达14.92%），经济高效（在精度-效率帕累托前沿表现良好，相比GAM减少约24.45%的推理token而不牺牲性能）。

Conclusion: Mem-T通过MoT-GRPO框架实现了内存构建和检索的联合优化，解决了长视野内存管理中的稀疏奖励问题，在性能和效率方面都表现出色。

Abstract: Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\sim24.45\%$ relative to GAM without sacrificing performance.

</details>


### [246] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 该论文提出了一种无人机辅助可见光通信系统中的三维轨迹规划框架，通过优化飞行高度和水平轨迹来最小化无人机飞行距离，从而提高数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 无人机与可见光通信技术的结合为提供灵活通信和高效照明提供了有前景的解决方案。然而，在无人机辅助的VLC系统中，如何规划三维轨迹以最小化飞行距离并提高数据收集效率是一个关键挑战。

Method: 首先推导了在特定VLC信道增益阈值下的闭式最优飞行高度。然后，通过将新型信息素驱动奖励机制与双延迟深度确定性策略梯度算法相结合，优化无人机的水平轨迹，实现在复杂环境中的自适应运动策略。

Result: 仿真结果表明，推导的最优高度相比基线方法可减少高达35%的飞行距离。此外，提出的奖励机制显著缩短了约50%的收敛步数，在无人机辅助VLC数据收集方面表现出显著的效率提升。

Conclusion: 该研究成功解决了无人机辅助VLC系统中的三维轨迹规划问题，提出的优化框架在减少飞行距离和加速收敛方面都取得了显著效果，为高效的数据收集提供了有效解决方案。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [247] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: SCOPE-PD是一个可解释的AI预测框架，通过整合主观和客观评估来提供个性化的帕金森病预测，准确率达到98.66%


<details>
  <summary>Details</summary>
Motivation: 帕金森病是一种受遗传、临床和生活方式因素影响的慢性神经退行性疾病。早期预测具有挑战性，因为传统诊断方法存在主观性问题，常常延误诊断。虽然机器学习在支持PD诊断方面显示出潜力，但现有方法通常仅依赖主观报告，缺乏个体化风险评估的可解释性。

Method: 提出SCOPE-PD可解释AI预测框架，整合主观和客观评估。从帕金森病进展标志物倡议(PPMI)研究中收集主观和客观临床评估数据，构建多模态预测框架。应用多种机器学习技术，选择最佳模型，并使用SHAP分析进行模型可解释性检验。

Result: 随机森林算法在结合主观和客观测试数据特征时达到最高准确率98.66%。从MDS-UPDRS测试中识别出震颤、运动迟缓和面部表情是预测PD的前三大贡献特征。

Conclusion: SCOPE-PD框架通过整合主观和客观评估，提供了高准确性和可解释性的帕金森病预测方法，能够识别关键临床特征，支持个性化健康决策。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [248] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 本文研究语言识别和生成任务，在完全放松"可实现性"假设的情况下，提出了更一般的"不可知"设置下的目标函数，获得了新颖的特征描述和近乎紧致的统计率。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别和生成研究通常在强可实现性假设下进行，即输入数据必然来自给定语言集合中的某个未知分布。本文旨在完全放松这一假设，研究更一般的"不可知"设置下的语言任务。

Method: 提出在无分布限制的"不可知"设置下研究语言识别和生成的新目标函数，不假设输入数据必然来自给定语言集合中的分布。

Result: 在两个问题中都获得了新颖的特征描述和近乎紧致的统计率，为更一般的语言处理场景提供了理论保证。

Conclusion: 通过放松可实现性假设，本文为语言识别和生成任务建立了更一般的理论框架，在无分布限制的情况下获得了有意义的理论结果。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [249] [Variational Bayesian Flow Network for Graph Generation](https://arxiv.org/abs/2601.22524)
*Yida Xiong,Jiameng Chen,Xiuwen Gong,Jia Wu,Shirui Pan,Wenbin Hu*

Main category: cs.LG

TL;DR: VBFN（变分贝叶斯流网络）通过变分提升到结构化精度的高斯变分信念族，实现节点和边的耦合更新，解决了图生成中节点-边耦合的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法（如扩散模型和流匹配）通常采用因子化的前向加噪或坐标插值，节点-边耦合未被编码到生成几何中，需要在离散解码后隐式恢复，这可能导致脆弱性。传统贝叶斯流网络依赖因子化信念和独立通道，限制了几何证据融合。

Method: 提出变分贝叶斯流网络（VBFN），通过变分提升到可处理的联合高斯变分信念族，该族由结构化精度矩阵控制。每个贝叶斯更新简化为求解对称正定线性系统，实现节点和边在单个融合步骤中的耦合更新。从表示诱导的依赖图构建样本无关的稀疏精度矩阵，避免标签泄漏同时强制节点-边一致性。

Result: 在合成和分子图数据集上，VBFN提高了保真度和多样性，超越了基线方法。

Conclusion: VBFN通过结构化精度矩阵的变分提升，有效解决了图生成中节点-边耦合的挑战，实现了更好的生成性能。

Abstract: Graph generation aims to sample discrete node and edge attributes while satisfying coupled structural constraints. Diffusion models for graphs often adopt largely factorized forward-noising, and many flow-matching methods start from factorized reference noise and coordinate-wise interpolation, so node-edge coupling is not encoded by the generative geometry and must be recovered implicitly by the core network, which can be brittle after discrete decoding. Bayesian Flow Networks (BFNs) evolve distribution parameters and naturally support discrete generation. But classical BFNs typically rely on factorized beliefs and independent channels, which limit geometric evidence fusion. We propose Variational Bayesian Flow Network (VBFN), which performs a variational lifting to a tractable joint Gaussian variational belief family governed by structured precisions. Each Bayesian update reduces to solving a symmetric positive definite linear system, enabling coupled node and edge updates within a single fusion step. We construct sample-agnostic sparse precisions from a representation-induced dependency graph, thereby avoiding label leakage while enforcing node-edge consistency. On synthetic and molecular graph datasets, VBFN improves fidelity and diversity, and surpasses baseline methods.

</details>


### [250] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS系统通过动态聚焦计算于可解码token并即时淘汰不可解码token，显著提升扩散大语言模型的解码效率，实现最高3.52倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）相比自回归模型具有优势，但解码成本高限制了其部署。研究发现DLLM解码存在关键低效问题：虽然计算在token块上并行化，但每个扩散步骤只有少量token可解码，导致大部分计算浪费在不可解码token上。

Method: 提出FOCUS推理系统，基于注意力机制得出的token重要性与token解码概率之间的强相关性，动态聚焦计算于可解码token，即时淘汰不可解码token，从而增加有效批处理大小，缓解计算限制。

Result: 实证评估显示，FOCUS相比生产级引擎LMDeploy实现了最高3.52倍的吞吐量提升，同时在多个基准测试中保持或改进了生成质量。

Conclusion: FOCUS系统通过高效管理token解码过程，显著提升了扩散大语言模型的推理效率，使其更具部署可行性，系统已在GitHub开源。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [251] [Learn from A Rationalist: Distilling Intermediate Interpretable Rationales](https://arxiv.org/abs/2601.22531)
*Jiayi Dai,Randy Goebel*

Main category: cs.LG

TL;DR: 提出REKD方法，通过知识蒸馏让较小的学生RE模型从教师模型的rationales和预测中学习，提高预测性能


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在关键领域应用广泛，但可解释性不足。Rationale extraction（RE）通过select-predict架构提供可解释性，但小模型在特征选择搜索空间大时性能受限

Method: 提出REKD方法：学生RE模型不仅从自身RE优化学习，还从教师模型（rationalist）的rationales和预测中学习。该方法与模型无关，可集成任何黑盒神经网络作为骨干模型

Result: 在语言和视觉分类数据集（IMDB电影评论、CIFAR 10和CIFAR 100）上，使用BERT和ViT变体进行实验，REKD显著提高了学生RE模型的预测性能

Conclusion: REKD通过知识蒸馏有效提升较小RE模型的性能，同时保持可解释性，该方法与模型无关，可应用于各种神经网络架构

Abstract: Because of the pervasive use of deep neural networks (DNNs), especially in high-stakes domains, the interpretability of DNNs has received increased attention. The general idea of rationale extraction (RE) is to provide an interpretable-by-design framework for DNNs via a select-predict architecture where two neural networks learn jointly to perform feature selection and prediction, respectively. Given only the remote supervision from the final task prediction, the process of learning to select subsets of features (or \emph{rationales}) requires searching in the space of all possible feature combinations, which is computationally challenging and even harder when the base neural networks are not sufficiently capable. To improve the predictive performance of RE models that are based on less capable or smaller neural networks (i.e., the students), we propose \textbf{REKD} (\textbf{R}ationale \textbf{E}xtraction with \textbf{K}nowledge \textbf{D}istillation) where a student RE model learns from the rationales and predictions of a teacher (i.e., a \emph{rationalist}) in addition to the student's own RE optimization. This structural adjustment to RE aligns well with how humans could learn effectively from interpretable and verifiable knowledge. Because of the neural-model agnostic nature of the method, any black-box neural network could be integrated as a backbone model. To demonstrate the viability of REKD, we conduct experiments with multiple variants of BERT and vision transformer (ViT) models. Our experiments across language and vision classification datasets (i.e., IMDB movie reviews, CIFAR 10 and CIFAR 100) show that REKD significantly improves the predictive performance of the student RE models.

</details>


### [252] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文通过构建简约基线来解耦强化学习微调中的设计选择，分析各因素对学习和泛化的边际贡献，识别关键设计要素。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调领域涌现大量论文关注设计选择优化，但性能提升声称与不一致结论并存，导致进展模糊。缺乏对两个基本问题的原则性回答：1) 每个设计选择的作用是什么？2) 哪些是关键设计选择？

Method: 构建简约基线（每轮查询一次rollout、使用结果奖励作为训练信号、无优势技巧、批量大小32），连接批量上下文bandit学习。围绕此基线设计实验流程，检验优势、rollout数量等因素的边际增益。

Result: 在三个基础模型和两个数据集上的实验，不仅揭示了各种设计选择对学习和泛化动态的新理解，还识别出值得更多努力的关键设计选择。

Conclusion: 通过解耦设计选择并分析其边际贡献，本文为理解强化学习微调中各种设计要素的作用提供了新见解，并指出了未来应重点关注的关键设计方向。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [253] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 提出L2D-SLDS模型处理非平稳时间序列的延迟学习问题，通过切换线性高斯状态空间模型建模专家残差，支持专家动态注册，并提出基于信息导向的调度规则


<details>
  <summary>Details</summary>
Motivation: 解决非平稳时间序列中部分反馈和专家可用性随时间变化的问题，传统方法难以处理专家动态变化和跨专家信息共享

Method: 使用L2D-SLDS模型：因子化的切换线性高斯状态空间模型，包含上下文相关的状态转移、共享全局因子实现跨专家信息传递、专家特定状态，支持专家动态注册和剪枝，提出基于一步预测信念的IDS启发式调度规则

Result: 实验显示该方法优于上下文多臂老虎机基线和无共享因子消融实验，在非平稳时间序列环境中表现更好

Conclusion: L2D-SLDS模型能有效处理非平稳时间序列的延迟学习问题，通过共享全局因子实现跨专家信息传递，动态专家注册机制增强了系统灵活性，信息导向调度规则在成本和信息获取间取得良好平衡

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [254] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出一种模型无关的守恒量校正技术，通过融入物理守恒准则来提升神经算子模型在长期预测中的稳定性


<details>
  <summary>Details</summary>
Motivation: 深度学习在PDE数值近似中表现出潜力，但现有方法在长期预测中因自回归误差累积和无法保持物理守恒量而表现不佳

Method: 提出守恒量校正技术，这是一种模型无关的方法，将物理守恒准则融入深度学习模型，提升长期稳定性

Result: 该方法在各种神经算子架构中都能显著提升长期预测稳定性，同时从谱域分析揭示了现有架构在高频分量处理上的局限性

Conclusion: 需要设计能够更好处理高频分量的新架构，这对于理解和建模湍流等复杂物理现象至关重要

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [255] [EUGens: Efficient, Unified, and General Dense Layers](https://arxiv.org/abs/2601.22563)
*Sang Min Kim,Byeongchan Kim,Arijit Sehanobish,Somnath Basu Roy Chowdhury,Rahul Kidambi,Dongseok Shim,Avinava Dubey,Snigdha Chaturvedi,Min-hwan Oh,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: 提出EUGens层，一种高效、统一、通用的密集层，通过随机特征和输入范数依赖将全连接层复杂度从二次降至线性，同时保持表达能力。


<details>
  <summary>Details</summary>
Motivation: 全连接前馈层在神经网络中引入计算和参数瓶颈，限制了模型在实时应用和资源受限环境中的可扩展性，需要更高效的替代方案。

Method: 提出EUGens层，利用随机特征逼近标准全连接层，并引入输入范数依赖；统一现有高效全连接层扩展；提出无需反向传播的逐层知识迁移技术。

Result: 在Transformer和MLP中集成EUGens，推理速度提升达27%，内存效率提升达30%；在图像分类、语言模型预训练和3D场景重建等任务中表现优异。

Conclusion: EUGens通过将复杂度从二次降至线性，显著提升效率和可扩展性，为大尺度神经网络在实际场景中的部署提供了有前景的解决方案。

Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this challenge, in this work, we propose a new class of dense layers that generalize standard fully-connected feedforward layers, \textbf{E}fficient, \textbf{U}nified and \textbf{Gen}eral dense layers (EUGens). EUGens leverage random features to approximate standard FFLs and go beyond them by incorporating a direct dependence on the input norms in their computations. The proposed layers unify existing efficient FFL extensions and improve efficiency by reducing inference complexity from quadratic to linear time. They also lead to \textbf{the first} unbiased algorithms approximating FFLs with arbitrary polynomial activation functions. Furthermore, EuGens reduce the parameter count and computational overhead while preserving the expressive power and adaptability of FFLs. We also present a layer-wise knowledge transfer technique that bypasses backpropagation, enabling efficient adaptation of EUGens to pre-trained models. Empirically, we observe that integrating EUGens into Transformers and MLPs yields substantial improvements in inference speed (up to \textbf{27}\%) and memory efficiency (up to \textbf{30}\%) across a range of tasks, including image classification, language model pre-training, and 3D scene reconstruction. Overall, our results highlight the potential of EUGens for the scalable deployment of large-scale neural networks in real-world scenarios.

</details>


### [256] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: FedDis：首个利用因果解耦的联邦时空预测框架，通过分离客户端特定因素和全局模式来应对非IID数据挑战


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理去中心化交通数据的非独立同分布特性时表现不佳，通常将全局共享模式与客户端特定局部动态纠缠在单一表示中。作者认为这种异质性源于两个不同生成源的纠缠：客户端特定的局部动态和跨客户端的全局时空模式。

Method: 提出FedDis框架，采用双分支设计：个性化银行学习捕获客户端特定因素，全局模式银行提取共同知识。通过互信息最小化目标强制两个分支之间的信息正交性，确保有效解耦。

Result: 在四个真实世界基准数据集上的综合实验表明，FedDis始终达到最先进的性能，具有高效性和优越的可扩展性。

Conclusion: FedDis通过因果解耦成功解决了联邦学习中非IID数据的挑战，实现了强大的跨客户端知识转移同时保持对独特本地环境的高度适应性，为隐私保护的交通预测提供了有前景的解决方案。

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [257] [Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks](https://arxiv.org/abs/2601.22579)
*Sichen Zhao,Zhiming Xue,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.LG

TL;DR: 提出基于图神经网络的非侵入式电商恶意机器人检测框架，通过建模用户会话行为图来识别自动化活动，相比传统方法更准确且部署友好。


<details>
  <summary>Details</summary>
Motivation: 电商平台面临恶意机器人的严重威胁（数据爬取、库存囤积、欺诈等），传统基于IP黑名单和验证码的方法效果有限且侵入性强，现代机器人使用代理、僵尸网络和AI规避策略，需要更有效的检测方案。

Method: 提出非侵入式图基机器人检测框架：1）将用户会话行为建模为图表示；2）应用归纳式图神经网络进行分类；3）同时捕捉关系结构和行为语义；4）支持实时推理和增量更新。

Result: 在真实电商流量实验中，该归纳图模型在AUC和F1分数上优于会话级多层感知器基线。对抗扰动和冷启动模拟显示模型在适度图修改下保持鲁棒，并能有效泛化到未见过的会话和URL。

Conclusion: 该框架部署友好，无需客户端插装即可与现有系统集成，支持实时推理和增量更新，适合实际电商安全部署，能准确识别逃避基于特征方法的微妙自动化活动。

Abstract: Malicious bots pose a growing threat to e-commerce platforms by scraping data, hoarding inventory, and perpetrating fraud. Traditional bot mitigation techniques, including IP blacklists and CAPTCHA-based challenges, are increasingly ineffective or intrusive, as modern bots leverage proxies, botnets, and AI-assisted evasion strategies. This work proposes a non-intrusive graph-based bot detection framework for e-commerce that models user session behavior through a graph representation and applies an inductive graph neural network for classification. The approach captures both relational structure and behavioral semantics, enabling accurate identification of subtle automated activity that evades feature-based methods. Experiments on real-world e-commerce traffic demonstrate that the proposed inductive graph model outperforms a strong session-level multilayer perceptron baseline in terms of AUC and F1 score. Additional adversarial perturbation and cold-start simulations show that the model remains robust under moderate graph modifications and generalizes effectively to previously unseen sessions and URLs. The proposed framework is deployment-friendly, integrates with existing systems without client-side instrumentation, and supports real-time inference and incremental updates, making it suitable for practical e-commerce security deployments.

</details>


### [258] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: MC-GRPO提出使用中位数基线替代均值基线，解决小样本训练中优势符号翻转问题，提升低样本量下的训练稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的小样本训练场景中，基于均值的组相对策略优化方法容易因基线噪声导致优势符号翻转，使某些样本获得错误的正负优势方向，从而降低训练准确性。

Method: 提出中位数中心的组相对策略优化（MC-GRPO）：用中位数基线替代均值基线，中位数对异常奖励值不敏感；生成G+1个样本来计算中位数参考，排除中位数样本（零优势）的反向传播，保持核心更新成本与标准G样本训练相同。

Result: 在各种GRPO系列方法和不同模型规模上，中位数中心训练在低样本量下持续提升稳定性和最终准确性，将G=2和G=8之间的性能差距缩小到1%以内。

Conclusion: MC-GRPO通过简单有效的中位数基线替换，解决了小样本训练中的优势符号翻转问题，为资源受限的语言模型训练提供了稳定高效的优化方案。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [259] [FedCARE: Federated Unlearning with Conflict-Aware Projection and Relearning-Resistant Recovery](https://arxiv.org/abs/2601.22589)
*Yue Li,Mingmin Chu,Xilei Yang,Da Xiao,Ziqi Xu,Wei Shao,Qipeng Song,Hui Li*

Main category: cs.LG

TL;DR: FedCARE是一个联邦遗忘学习框架，通过冲突感知遗忘和抗重学习恢复机制，高效实现客户端、实例和类别级别的遗忘，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要遵守"被遗忘权"等隐私法规，但现有联邦遗忘方法存在遗忘开销高、模型性能下降、知识纠缠导致意外重学习等问题，需要更高效的解决方案。

Method: FedCARE采用梯度上升进行高效遗忘，使用无数据模型反演构建类别级知识代理，集成伪样本生成器、冲突感知投影梯度上升进行效用保持的遗忘，以及抑制回滚的恢复策略。

Result: 在多个数据集和模型架构上的实验表明，FedCARE相比现有方法能实现有效遗忘、更好的效用保持和更低的重学习风险，支持客户端、实例和类别级遗忘且开销适中。

Conclusion: FedCARE提供了一个统一且低开销的联邦遗忘框架，解决了现有方法在遗忘效率、效用保持和抗重学习方面的挑战，为联邦学习中的隐私合规提供了实用解决方案。

Abstract: Federated learning (FL) enables collaborative model training without centralizing raw data, but privacy regulations such as the right to be forgotten require FL systems to remove the influence of previously used training data upon request. Retraining a federated model from scratch is prohibitively expensive, motivating federated unlearning (FU). However, existing FU methods suffer from high unlearning overhead, utility degradation caused by entangled knowledge, and unintended relearning during post-unlearning recovery. In this paper, we propose FedCARE, a unified and low overhead FU framework that enables conflict-aware unlearning and relearning-resistant recovery. FedCARE leverages gradient ascent for efficient forgetting when target data are locally available and employs data free model inversion to construct class level proxies of shared knowledge. Based on these insights, FedCARE integrates a pseudo-sample generator, conflict-aware projected gradient ascent for utility preserving unlearning, and a recovery strategy that suppresses rollback toward the pre-unlearning model. FedCARE supports client, instance, and class level unlearning with modest overhead. Extensive experiments on multiple datasets and model architectures under both IID and non-IID settings show that FedCARE achieves effective forgetting, improved utility retention, and reduced relearning risk compared to state of the art FU baselines.

</details>


### [260] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: MGMT是一个用于多图学习的统一框架，通过图Transformer编码器将不同图映射到共享潜在空间，构建元图实现跨图推理，并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 多图学习面临的主要挑战是如何有效整合具有不同拓扑结构、尺度和语义的异构图信息，特别是在没有共享节点标识的情况下。

Method: 首先使用图Transformer编码器将每个图的结构和属性映射到共享潜在空间；然后通过注意力机制选择任务相关的超节点，基于潜在空间相似性构建连接跨图功能对齐超节点的元图；最后在元图上应用额外的图Transformer层进行联合推理。

Result: 在合成数据集和真实世界神经科学应用中，MGMT在图表预测任务中始终优于现有最先进模型，同时提供促进科学发现的可解释表示。

Conclusion: MGMT作为结构化多图学习的统一框架，在图数据发挥核心作用的领域中推进了表示技术。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [261] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: Lethe：一种新颖的联邦遗忘方法，通过解耦待遗忘知识与保留知识，解决持续训练中知识重现问题，实现持久遗忘


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘研究通常假设遗忘操作后协作结束，忽略了后续持续训练的情况。研究发现存在"知识重现"的关键失效模式，即持续训练会重新激活已遗忘知识，导致已移除的影响在全局模型中重现。

Method: Lethe采用Reshape-Rectify-Restore流程：1) 在遗忘数据上使用梯度上升训练临时适配器获得放大更新；2) 将适配器作为校正信号，在两个流中对剩余更新进行分层校正；3) 移除适配器并在保留数据上进行短期恢复阶段。

Result: Lethe能够以统一方式支持联邦系统中所有级别的遗忘，并在多次后续训练轮次后保持优异的持久性（大多数情况下重现率<1%）。

Conclusion: Lethe解决了联邦遗忘中的知识重现问题，通过解耦知识确保在持续训练中实现持久遗忘，为联邦学习系统提供了更可靠的遗忘机制。

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [262] [Local-Global Multimodal Contrastive Learning for Molecular Property Prediction](https://arxiv.org/abs/2601.22610)
*Xiayu Liu,Zhengyi Lu,Yunhong Liao,Chan Fan,Hou-biao Li*

Main category: cs.LG

TL;DR: LGM-CL是一个局部-全局多模态对比学习框架，通过联合建模分子图和文本表示来提升分子属性预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的分子属性预测需要整合分子结构和化学语义的互补信息。当前方法往往未能充分结合局部功能基团信息、全局分子拓扑结构和化学语义知识。

Method: 提出LGM-CL框架：1) 使用AttentiveFP和Graph Transformer分别编码局部功能基团和全局分子拓扑；2) 通过自监督对比学习对齐局部和全局表示；3) 将化学增强的文本描述与原始SMILES进行对比以融入物理化学语义；4) 在微调阶段通过双交叉注意力多模态融合整合分子指纹。

Result: 在MoleculeNet基准测试上的广泛实验表明，LGM-CL在分类和回归任务上均取得了一致且具有竞争力的性能，验证了统一局部-全局和多模态表示学习的有效性。

Conclusion: LGM-CL通过整合局部-全局分子结构和化学语义的多模态表示，为分子属性预测提供了一种有效的统一框架，在多个基准任务上表现出优越性能。

Abstract: Accurate molecular property prediction requires integrating complementary information from molecular structure and chemical semantics. In this work, we propose LGM-CL, a local-global multimodal contrastive learning framework that jointly models molecular graphs and textual representations derived from SMILES and chemistry-aware augmented texts. Local functional group information and global molecular topology are captured using AttentiveFP and Graph Transformer encoders, respectively, and aligned through self-supervised contrastive learning. In addition, chemically enriched textual descriptions are contrasted with original SMILES to incorporate physicochemical semantics in a task-agnostic manner. During fine-tuning, molecular fingerprints are further integrated via Dual Cross-attention multimodal fusion. Extensive experiments on MoleculeNet benchmarks demonstrate that LGM-CL achieves consistent and competitive performance across both classification and regression tasks, validating the effectiveness of unified local-global and multimodal representation learning.

</details>


### [263] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 共识机制作为注意力替代方案，能稳定Transformer训练，扩大有效学习率范围，并提出了混合共识-注意力框架


<details>
  <summary>Details</summary>
Motivation: 标准注意力Transformer在训练时对学习率超规格化表现出不稳定性，特别是在高学习率下。虽然已有方法通过修改优化过程来改善这种不稳定性，但针对此问题的根本性架构创新仍未被充分探索。

Method: 提出共识机制作为注意力的直接替代方案，将其形式化为图模型。进一步提出混合共识-注意力框架，在保持性能的同时提高稳定性。提供了理论分析来表征共识机制的特性。

Result: 共识机制在文本、DNA和蛋白质模态的学习率扫描中表现出改进的稳定性。混合共识-注意力框架在保持性能的同时提高了稳定性。

Conclusion: 共识机制是Transformer架构的一个有前景的替代方案，能够稳定训练过程，扩大有效学习率范围，为解决Transformer训练不稳定性问题提供了新的架构创新方向。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [264] [PEFT-MuTS: A Multivariate Parameter-Efficient Fine-Tuning Framework for Remaining Useful Life Prediction based on Cross-domain Time Series Representation Model](https://arxiv.org/abs/2601.22631)
*En Fu,Yanyan Hu,Changhua Hu,Zengwang Jin,Kaixiang Peng*

Main category: cs.LG

TL;DR: 提出PEFT-MuTS框架，通过跨域预训练的时间序列表示模型实现少样本剩余使用寿命预测，显著减少对目标设备数据的需求。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的RUL预测方法需要大量退化数据，现有解决方案如域适应和元学习仍依赖大量相同或相似设备的历史数据，这在实际应用中存在显著限制。

Method: 开发了PEFT-MuTS框架，包括独立特征调优网络和基于元变量的低秩多元融合机制，使预训练的单变量时间序列表示骨干模型能够充分利用退化数据中的多元关系；引入零初始化回归器以稳定少样本条件下的微调过程。

Result: 在航空发动机和工业轴承数据集上的实验表明，即使使用目标设备少于1%的样本，也能实现有效的RUL预测；显著优于传统监督和少样本方法，同时大幅减少实现高预测精度所需的数据量。

Conclusion: 通过跨域预训练的时间序列表示模型可以实现有效的少样本RUL预测，突破了传统认为知识转移只能在相似设备间进行的观点，为实际工业应用提供了更可行的解决方案。

Abstract: The application of data-driven remaining useful life (RUL) prediction has long been constrained by the availability of large amount of degradation data. Mainstream solutions such as domain adaptation and meta-learning still rely on large amounts of historical degradation data from equipment that is identical or similar to the target, which imposes significant limitations in practical applications. This study investigates PEFT-MuTS, a Parameter-Efficient Fine-Tuning framework for few-shot RUL prediction, built on cross-domain pre-trained time-series representation models. Contrary to the widely held view that knowledge transfer in RUL prediction can only occur within similar devices, we demonstrate that substantial benefits can be achieved through pre-training process with large-scale cross-domain time series datasets. A independent feature tuning network and a meta-variable-based low rank multivariate fusion mechanism are developed to enable the pre-trained univariate time-series representation backbone model to fully exploit the multivariate relationships in degradation data for downstream RUL prediction task. Additionally, we introduce a zero-initialized regressor that stabilizes the fine-tuning process under few-shot conditions. Experiments on aero-engine and industrial bearing datasets demonstrate that our method can achieve effective RUL prediction even when less than 1\% of samples of target equipment are used. Meanwhile, it substantially outperforms conventional supervised and few-shot approaches while markedly reducing the data required to achieve high predictive accuracy. Our code is available at https://github.com/fuen1590/PEFT-MuTS.

</details>


### [265] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 提出一种结合形式逻辑验证与LLM生成的新框架，通过实时验证反馈纠正推理错误，显著提升模型推理能力


<details>
  <summary>Details</summary>
Motivation: LLM虽然能力强大，但其基于概率的token预测会产生逻辑不一致和奖励黑客问题，而形式符号系统可以避免这些问题。需要桥接LLM的自然语言生成与形式逻辑验证的优势。

Method: 提出形式逻辑验证引导的框架，在自然语言生成过程中动态交织形式符号验证，提供实时反馈检测和纠正错误。采用两阶段训练流程：形式逻辑验证引导的监督微调和策略优化。

Result: 在数学、逻辑和一般推理的六个基准测试中，7B和14B模型分别比最先进基线平均提升10.4%和14.2%。

Conclusion: 形式验证可以作为可扩展机制，显著推动先进LLM推理的性能边界。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [266] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: GUDA是一种用于扩散模型的群体级训练数据归因方法，通过机器遗忘技术近似反事实模型，相比重新训练大幅提升效率


<details>
  <summary>Details</summary>
Motivation: 现有训练数据归因方法主要针对单个样本评分，但实践中需要群体级答案（如艺术风格或对象类别）。群体级归因是反事实的：如果某个群体从训练中移除，模型在生成样本上的行为会如何变化？虽然LOGO重新训练是自然实现方式，但计算成本过高。

Method: 提出GUDA（基于群体遗忘的数据归因）方法，通过机器遗忘技术从共享的全数据模型近似每个反事实模型，而不是从头训练。使用基于似然的评分规则（ELBO）在全模型和每个遗忘反事实模型之间的差异来量化群体影响。

Result: 在CIFAR-10和Stable Diffusion艺术风格归因实验中，GUDA比语义相似性、基于梯度的归因和实例级遗忘方法更可靠地识别主要贡献群体，同时在CIFAR-10上相比LOGO重新训练实现100倍加速。

Conclusion: GUDA为扩散模型提供了一种高效且可靠的群体级训练数据归因方法，通过机器遗忘技术近似反事实模型，显著降低了计算成本，同时保持了归因的准确性。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [267] [Layerwise Progressive Freezing Enables STE-Free Training of Deep Binary Neural Networks](https://arxiv.org/abs/2601.22660)
*Evan Gibson Smith,Bashima Islam*

Main category: cs.LG

TL;DR: 提出StoMPP方法，使用层间随机掩码逐步替换可微权重/激活为硬二值函数，避免STE，在深度二值神经网络上显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 研究渐进冻结作为STE替代方案训练二值网络，发现全局渐进冻结在二值权重网络中有效，但在全二值神经网络中因激活梯度阻塞而失败。

Method: 提出StoMPP（随机掩码部分渐进二值化），使用层间随机掩码逐步将可微裁剪权重/激活替换为硬二值阶跃函数，仅通过未冻结（裁剪）子集反向传播（不使用STE）。

Result: 在匹配的最小训练方案下，StoMPP优于BinaryConnect风格STE基线，增益随深度增加（ResNet-50 BNN：CIFAR-10 +18.0，CIFAR-100 +13.5，ImageNet +3.8）。二值权重网络在CIFAR-10达91.2%，CIFAR-100达69.5%。

Conclusion: 渐进冻结训练动态分析显示非单调收敛和在二值化约束下改进的深度缩放，StoMPP为二值网络训练提供了有效的STE替代方案。

Abstract: We investigate progressive freezing as an alternative to straight-through estimators (STE) for training binary networks from scratch. Under controlled training conditions, we find that while global progressive freezing works for binary-weight networks, it fails for full binary neural networks due to activation-induced gradient blockades. We introduce StoMPP (Stochastic Masked Partial Progressive Binarization), which uses layerwise stochastic masking to progressively replace differentiable clipped weights/activations with hard binary step functions, while only backpropagating through the unfrozen (clipped) subset (i.e., no straight-through estimator). Under a matched minimal training recipe, StoMPP improves accuracy over a BinaryConnect-style STE baseline, with gains that increase with depth (e.g., for ResNet-50 BNN: +18.0 on CIFAR-10, +13.5 on CIFAR-100, and +3.8 on ImageNet; for ResNet-18: +3.1, +4.7, and +1.3). For binary-weight networks, StoMPP achieves 91.2\% accuracy on CIFAR-10 and 69.5\% on CIFAR-100 with ResNet-50. We analyze training dynamics under progressive freezing, revealing non-monotonic convergence and improved depth scaling under binarization constraints.

</details>


### [268] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出一种无需验证数据的联邦学习早期停止框架，通过监控任务向量增长率确定最优停止点，显著降低计算成本和隐私风险。


<details>
  <summary>Details</summary>
Motivation: 联邦学习依赖固定全局轮次或验证数据进行超参数调优，导致高计算成本和隐私风险，阻碍实际部署。

Method: 提出数据无关的早期停止框架，仅使用服务器端参数监控任务向量增长率来确定最优停止点。

Result: 在皮肤病变/血细胞分类任务上，该方法与基于验证的早期停止效果相当，平均仅需47/20轮就能获得比验证数据早期停止高12.5%/10.3%的性能。

Conclusion: 这是首个无需验证数据的联邦学习早期停止框架，能有效降低计算成本和隐私风险，具有实际部署价值。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [269] [Full-Graph vs. Mini-Batch Training: Comprehensive Analysis from a Batch Size and Fan-Out Size Perspective](https://arxiv.org/abs/2601.22678)
*Mengfan Liu,Da Zheng,Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文系统比较了全图与mini-batch GNN训练方法，通过批大小和扇出大小的视角分析其收敛性、泛化性和计算效率，发现全图训练并不总是优于调优后的mini-batch训练。


<details>
  <summary>Details</summary>
Motivation: 全图和mini-batch GNN训练方法有不同的系统设计需求，但缺乏系统比较。核心挑战在于如何表征它们的模型性能（收敛性和泛化性）和计算效率。批大小在DNN中是一个有效分析视角，但GNN引入了扇出大小这一新维度，需要深入探索。

Method: 通过经验性和理论分析，从批大小和扇出大小的视角系统比较全图与mini-batch训练。使用Wasserstein距离进行泛化分析以研究图结构（特别是扇出大小）的影响，并揭示批大小和扇出大小在GNN收敛和泛化中的非各向同性效应。

Result: 发现批大小和扇出大小对GNN收敛和泛化具有非各向同性影响，为在资源约束下调整这些超参数提供实用指导。全图训练并不总是比调优后的较小mini-batch设置产生更好的模型性能或计算效率。

Conclusion: 全图训练并非总是最佳选择，通过适当调整批大小和扇出大小的mini-batch训练可以在模型性能和计算效率上达到更好平衡。研究为GNN训练方法选择提供了理论和实践指导。

Abstract: Full-graph and mini-batch Graph Neural Network (GNN) training approaches have distinct system design demands, making it crucial to choose the appropriate approach to develop. A core challenge in comparing these two GNN training approaches lies in characterizing their model performance (i.e., convergence and generalization) and computational efficiency. While a batch size has been an effective lens in analyzing such behaviors in deep neural networks (DNNs), GNNs extend this lens by introducing a fan-out size, as full-graph training can be viewed as mini-batch training with the largest possible batch size and fan-out size. However, the impact of the batch and fan-out size for GNNs remains insufficiently explored. To this end, this paper systematically compares full-graph vs. mini-batch training of GNNs through empirical and theoretical analyses from the view points of the batch size and fan-out size. Our key contributions include: 1) We provide a novel generalization analysis using the Wasserstein distance to study the impact of the graph structure, especially the fan-out size. 2) We uncover the non-isotropic effects of the batch size and the fan-out size in GNN convergence and generalization, providing practical guidance for tuning these hyperparameters under resource constraints. Finally, full-graph training does not always yield better model performance or computational efficiency than well-tuned smaller mini-batch settings. The implementation can be found in the github link: https://github.com/LIUMENGFAN-gif/GNN_fullgraph_minibatch_training.

</details>


### [270] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 本文从流映射角度理论分析一致性模型，揭示训练不稳定和收敛问题的根源，提出改进的自蒸馏方法稳定优化，并扩展到扩散策略学习


<details>
  <summary>Details</summary>
Motivation: 一致性模型虽然能实现快速生成建模，但存在训练不稳定和可复现性有限的问题。现有研究对这些问题的解释较为零散，理论关系不清晰，需要系统性的理论分析

Method: 从流映射角度对一致性模型进行理论分析，揭示训练稳定性和收敛行为的机制。基于分析结果，重新审视自蒸馏方法，改进其形式以避免梯度范数过大，实现稳定优化

Result: 理论分析阐明了训练不稳定和收敛问题的根源，改进的自蒸馏方法能够稳定优化过程。该方法不仅适用于图像生成，还能扩展到基于扩散的策略学习，且不需要预训练扩散模型初始化

Conclusion: 通过流映射视角的理论分析，本文澄清了一致性模型的训练稳定性问题，提出了有效的改进方法，并展示了该方法在生成建模和强化学习领域的广泛适用性

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [271] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: Transformers在周期性OOD泛化方面存在局限，能记忆训练数据但无法泛化到未见复合周期性模式


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的大语言模型在分布外泛化方面与人类存在显著差距，本文通过周期性这一基本OOD场景来研究这一差距

Method: 从抽象代数和推理角度统一解释周期性，构建Coper基准测试（包含Hollow和Extrapolation两种OOD设置），评估Transformers的周期性泛化能力

Result: 实验表明Transformers的周期性泛化能力有限，模型能记忆训练数据中的周期性模式，但无法泛化到未见复合周期性场景

Conclusion: Transformers在周期性OOD泛化方面存在根本性局限，需要进一步研究来提升模型在这方面的能力

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [272] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 提出METRIC框架的实践化实现，通过建立数据质量指标库来评估医疗机器学习数据的适用性，支持可信AI发展


<details>
  <summary>Details</summary>
Motivation: 医疗机器学习从研究转向实际应用，需要建立可信AI的证据基础，其中数据质量评估是关键因素

Method: 将理论框架METRIC操作化，建立数据质量指标库，为每个指标提供指标卡片，包含定义、适用性、示例、陷阱和建议，并提供选择策略和决策树

Result: 在PTB-XL心电图数据集上展示了该方法的影响，为实践中评估训练和测试数据的适用性提供了第一步

Conclusion: 这是实现医疗领域可信AI的基础，通过系统化评估数据质量来支持医疗ML的临床应用和监管审批

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [273] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: 提出基于深度学习的IR-drop早期估计方法，使用CNN将物理版图特征映射到IR-drop热力图，实现毫秒级快速预测


<details>
  <summary>Details</summary>
Motivation: 传统IR-drop分析依赖物理签核工具，计算成本高且需要接近最终版图信息，不适合早期快速设计探索。需要一种快速、准确的早期IR-drop估计方法

Method: 采用基于U-Net的编码器-解码器架构，将IR-drop估计建模为密集像素级回归问题。使用物理启发的合成数据集训练，包含电源网格结构、单元密度分布和开关活动等关键物理因素

Result: 模型能够准确预测IR-drop分布，推理时间达到毫秒级别，支持快速预签核筛选和迭代设计优化。通过MSE和PSNR等标准回归指标评估性能

Conclusion: 该方法作为早期分析工具，可在昂贵签核分析前为设计者提供快速IR-drop洞察。代码、数据集生成脚本和交互式推理应用已开源

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [274] [SQUAD: Scalable Quorum Adaptive Decisions via ensemble of early exit neural networks](https://arxiv.org/abs/2601.22711)
*Matteo Gambella,Fabrizio Pittorino,Giuliano Casale,Manuel Roveri*

Main category: cs.LG

TL;DR: SQUAD结合早期退出机制与分布式集成学习，通过基于法定人数的停止准则和QUEST神经架构搜索优化分层多样性，显著提升推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统早期退出神经网络依赖单一模型置信度阈值，但由于校准问题经常不可靠，需要更可靠的停止准则来改善不确定性估计并减少推理时间。

Method: 提出SQUAD推理方案：1) 使用基于法定人数的停止准则，按计算复杂度递增收集中间预测直到达成共识；2) 引入QUEST神经架构搜索方法，选择具有优化分层多样性的早期退出学习器，确保各层互补性。

Result: 相比最先进的动态解决方案，测试准确率提升高达5.95%，计算成本相当；相比静态集成方法，推理延迟减少高达70.60%，同时保持良好准确率。

Conclusion: SQUAD通过共识驱动的早期退出机制，在保持计算效率的同时显著提升了推理准确性和不确定性估计，为高效神经网络推理提供了新方案。

Abstract: Early-exit neural networks have become popular for reducing inference latency by allowing intermediate predictions when sufficient confidence is achieved. However, standard approaches typically rely on single-model confidence thresholds, which are frequently unreliable due to inherent calibration issues. To address this, we introduce SQUAD (Scalable Quorum Adaptive Decisions), the first inference scheme that integrates early-exit mechanisms with distributed ensemble learning, improving uncertainty estimation while reducing the inference time. Unlike traditional methods that depend on individual confidence scores, SQUAD employs a quorum-based stopping criterion on early-exit learners by collecting intermediate predictions incrementally in order of computational complexity until a consensus is reached and halting the computation at that exit if the consensus is statistically significant. To maximize the efficacy of this voting mechanism, we also introduce QUEST (Quorum Search Technique), a Neural Architecture Search method to select early-exit learners with optimized hierarchical diversity, ensuring learners are complementary at every intermediate layer. This consensus-driven approach yields statistically robust early exits, improving the test accuracy up to 5.95% compared to state-of-the-art dynamic solutions with a comparable computational cost and reducing the inference latency up to 70.60% compared to static ensembles while maintaining a good accuracy.

</details>


### [275] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 利用视觉语言模型(VLMs)的常识推理能力为潜在动作模型(LAMs)提供可提示的表征，有效分离可控变化与噪声，显著提升下游任务成功率


<details>
  <summary>Details</summary>
Motivation: 现有潜在动作模型(LAMs)在处理包含动作相关干扰物的观测时容易编码噪声而非有意义的潜在动作，而人类却能轻松区分任务相关运动与无关细节。需要利用视觉语言模型的常识推理能力来解决这一问题。

Method: 利用视觉语言模型(VLMs)提供可提示的表征，在无监督方式下分离可控变化与噪声，并将这些表征作为LAM训练的目标。对多种流行的VLMs进行基准测试，评估其可提示表征的质量和鲁棒性。

Result: 不同VLMs提供的可提示表征质量存在显著差异，对提示和超参数的鲁棒性也不同。有趣的是，较新的VLMs可能比旧的表现更差。通过让VLMs忽略干扰物，可以显著提升潜在动作质量，在Distracting MetaWorld上实现高达6倍的下游成功率提升。

Conclusion: 利用视觉语言模型的常识推理能力为潜在动作模型提供可提示表征是有效的，能够显著改善LAMs在存在干扰物环境中的表现。选择合适的VLM和提示策略对性能有重要影响。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [276] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: LoRDS提出一种基于低秩分解的元素级量化方法，通过连续低秩矩阵建模缩放流形，在保持块级量化效率的同时提供更强的表达能力，实现量化、训练和适配的统一框架。


<details>
  <summary>Details</summary>
Motivation: 当前LLM量化方法主要依赖块级结构来保持效率，但牺牲了表示灵活性。需要一种既能保持效率又能提供更强表达能力的方法。

Method: 提出LoRDS框架，将缩放流形建模为连续低秩矩阵(S=BA)，打破空间约束的块结构。支持高保真PTQ初始化、迭代优化、权重与缩放因子的联合QAT，以及高秩乘法PEFT适配。

Result: 在多种模型家族中持续优于最先进基线。在Llama3-8B上，3位量化比NormalFloat精度提升27.0%，RTX 4090上推理速度提升1.5倍，下游任务PEFT性能比4位QLoRA提升9.6%。

Conclusion: LoRDS提供了一种统一、高效的LLM压缩和适配解决方案，通过低秩分解实现元素级量化，在保持效率的同时显著提升表达能力和性能。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [277] [Local Intrinsic Dimension of Representations Predicts Alignment and Generalization in AI Models and Human Brain](https://arxiv.org/abs/2601.22722)
*Junjie Yu,Wenxiao Ma,Chen Wei,Jianyu Zhang,Haotian Deng,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 研究发现神经网络泛化能力越强，其表征与人类神经活动越对齐，且泛化性能、模型间对齐度和模型-大脑对齐度三者显著相关，这些关系可由表征的局部内在维度解释。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络泛化能力、模型间表征对齐度、以及模型与人类神经活动对齐度之间的关系，并寻找能够统一解释这些现象的表征几何特性。

Method: 通过分析不同架构和训练范式的神经网络，测量其泛化性能、模型间表征对齐度、模型-大脑对齐度，并计算表征的局部内在维度和全局维度，研究这些指标之间的相关性。

Result: 发现：1）泛化能力越强的模型与人类神经活动对齐度越高；2）泛化性能、模型间对齐度、模型-大脑对齐度三者显著相关；3）局部内在维度能统一解释这些关系，局部维度越低，对齐度和泛化性能越好；4）增加模型容量和训练数据能系统性降低局部维度。

Conclusion: 局部内在维度是人工和生物系统中表征收敛的统一描述符，为理解模型泛化、表征对齐以及模型与大脑相似性提供了几何解释框架。

Abstract: Recent work has found that neural networks with stronger generalization tend to exhibit higher representational alignment with one another across architectures and training paradigms. In this work, we show that models with stronger generalization also align more strongly with human neural activity. Moreover, generalization performance, model--model alignment, and model--brain alignment are all significantly correlated with each other. We further show that these relationships can be explained by a single geometric property of learned representations: the local intrinsic dimension of embeddings. Lower local dimension is consistently associated with stronger model--model alignment, stronger model--brain alignment, and better generalization, whereas global dimension measures fail to capture these effects. Finally, we find that increasing model capacity and training data scale systematically reduces local intrinsic dimension, providing a geometric account of the benefits of scaling. Together, our results identify local intrinsic dimension as a unifying descriptor of representational convergence in artificial and biological systems.

</details>


### [278] [Decomposing Epistemic Uncertainty for Causal Decision Making](https://arxiv.org/abs/2601.22736)
*Md Musfiqur Rahman,Ziwei Jiang,Hilaf Hasson,Murat Kocaoglu*

Main category: cs.LG

TL;DR: 提出新框架区分因果效应边界中的样本不确定性和非可识别不确定性，指导实践者何时收集更多样本无效


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法估计因果效应边界可能过拟合且无法区分边界宽度来源：是根本不可识别性还是有限样本限制

Method: 构建经验观测分布的置信集，计算该置信集中所有分布的因果效应边界交集，通过神经因果模型求解min-max和max-min问题

Result: 实验证明算法能确定何时收集更多样本无助于确定最佳行动，指导实践者收集更多变量或转向随机研究

Conclusion: 提出系统方法区分因果效应边界中的两种不确定性，为因果推断实践提供重要指导

Abstract: Causal inference from observational data provides strong evidence for the best action in decision-making without performing expensive randomized trials. The effect of an action is usually not identifiable under unobserved confounding, even with an infinite amount of data. Recent work uses neural networks to obtain practical bounds to such causal effects, which is often an intractable problem. However, these approaches may overfit to the dataset and be overconfident in their causal effect estimates. Moreover, there is currently no systematic approach to disentangle how much of the width of causal effect bounds is due to fundamental non-identifiability versus how much is due to finite-sample limitations. We propose a novel framework to address this problem by considering a confidence set around the empirical observational distribution and obtaining the intersection of causal effect bounds for all distributions in this confidence set. This allows us to distinguish the part of the interval that can be reduced by collecting more samples, which we call sample uncertainty, from the part that can only be reduced by observing more variables, such as latent confounders or instrumental variables, but not with more data, which we call non-ID uncertainty. The upper and lower bounds to this intersection are obtained by solving min-max and max-min problems with neural causal models by searching over all distributions that the dataset might have been sampled from, and all SCMs that entail the corresponding distribution. We demonstrate via extensive experiments on synthetic and real-world datasets that our algorithm can determine when collecting more samples will not help determine the best action. This can guide practitioners to collect more variables or lean towards a randomized study for best action identification.

</details>


### [279] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 论文系统研究了Softmax损失函数家族，分析了不同替代损失的一致性、梯度动态和收敛行为，提出了近似方法的偏差-方差分解，为大规模分类任务中的损失选择提供了理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: 虽然Softmax损失在分类和排序任务中被广泛使用，但现有研究存在两个方向：Fenchel-Young框架提供了理论视角，而大规模类别场景则关注效率优化。本文旨在整合这两个视角，对Softmax家族损失进行系统性研究，为大规模机器学习应用中的损失选择提供原则性指导。

Method: 1) 分析不同替代损失与分类和排序指标的一致性；2) 研究梯度动态以揭示不同的收敛行为；3) 提出近似方法的系统性偏差-方差分解，提供收敛保证；4) 推导每轮复杂度分析，展示效果与效率之间的权衡。

Result: 实验表明一致性、收敛性和实证性能之间存在强相关性。研究结果为大规模类别机器学习应用中的损失选择建立了原则性基础，并提供了实践指导。

Conclusion: 本文通过系统性研究Softmax家族损失，建立了理论框架，揭示了不同损失的性质和权衡，为大规模分类任务中的损失函数选择提供了原则性指导，有助于在实际应用中做出更明智的决策。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [280] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 提出MSN-PINN网络，将幂律标度指数作为可训练参数，同时输出解及其标度结构，在奇异点、界面和临界点附近物理系统的标度分析中实现高精度指数恢复。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理具有幂律标度的物理系统（如奇异点、界面、临界点附近）时，无法显式表达控制指数，缺乏物理可解释性。需要一种既能保持神经网络表达能力，又能提供直接物理意义的标度指数的方法。

Method: 引入物理信息Müntz-Szász网络（MSN-PINN），采用幂律基函数网络，将标度指数作为可训练参数。模型同时输出解及其标度结构，并证明可识别性（唯一恢复）。通过约束感知训练编码物理要求（如边界条件兼容性）。

Result: 在噪声和稀疏采样下实现1-5%误差的单指数恢复；二维拉普拉斯方程角奇异性指数恢复误差0.009%；匹配Kondrat'ev经典结果；奇异泊松问题中强迫诱导指数恢复误差0.03%和0.05%；40配置楔形基准测试达到100%成功率和0.022%平均误差；约束感知训练比朴素训练精度提高三个数量级。

Conclusion: MSN-PINN成功结合神经网络的表达能力和渐近分析的可解释性，产生具有直接物理意义的可学习参数，为具有幂律标度的物理系统提供了一种高精度、可解释的数值方法。

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [281] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: OSNIP是一个轻量级的客户端加密框架，通过向LLM潜在空间注入扰动来保护隐私，同时保持语义保真度


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理中的隐私保护方法通常需要复杂的后处理或牺牲模型性能，需要一种轻量级、高效且能保持语义质量的隐私保护方案

Method: 提出"模糊语义零空间"概念，将线性核的几何直觉推广到LLM的高维潜在空间，通过注入扰动将原始嵌入投影到这个空间中，并使用密钥相关的随机映射为每个用户生成独特的扰动轨迹

Result: 在12个生成和分类基准测试中，OSNIP实现了最先进的性能，显著降低了攻击成功率，同时在严格的安全约束下保持了强大的模型效用

Conclusion: OSNIP提供了一个无需后处理的轻量级隐私保护框架，通过创新的模糊语义零空间注入方法，在保护用户隐私的同时保持了LLM的语义保真度和实用性

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [282] [Understanding Generalization from Embedding Dimension and Distributional Convergence](https://arxiv.org/abs/2601.22756)
*Junjie Yu,Zhuoli Ouyang,Haotian Deng,Chen Wei,Wenxiao Ma,Jianyu Zhang,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 论文从表示中心视角研究深度网络泛化，提出基于嵌入几何的泛化界，不依赖参数数量，由嵌入分布内在维度和下游映射敏感性决定。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在严重过参数化情况下仍能良好泛化，这挑战了传统的基于参数的分析方法。需要从新的视角理解泛化机制，特别是从学习到的表示（嵌入）的角度。

Method: 从表示中心视角分析泛化，研究学习到的嵌入几何如何控制预测性能。提出基于嵌入的泛化界，包含两个因素：嵌入分布的内在维度（决定Wasserstein距离收敛率）和下游映射的敏感性（用Lipschitz常数刻画）。

Result: 理论分析表明，在最终嵌入层，架构敏感性消失，泛化界主要由嵌入维度主导，这解释了嵌入维度与泛化性能的强经验相关性。在不同架构和数据集上的实验验证了理论。

Conclusion: 从表示中心视角提供了理解深度网络泛化的新框架，提出的嵌入依赖误差界不依赖参数数量或假设类复杂度，为基于嵌入的诊断工具提供了理论基础。

Abstract: Deep neural networks often generalize well despite heavy over-parameterization, challenging classical parameter-based analyses. We study generalization from a representation-centric perspective and analyze how the geometry of learned embeddings controls predictive performance for a fixed trained model. We show that population risk can be bounded by two factors: (i) the intrinsic dimension of the embedding distribution, which determines the convergence rate of empirical embedding distribution to the population distribution in Wasserstein distance, and (ii) the sensitivity of the downstream mapping from embeddings to predictions, characterized by Lipschitz constants. Together, these yield an embedding-dependent error bound that does not rely on parameter counts or hypothesis class complexity. At the final embedding layer, architectural sensitivity vanishes and the bound is dominated by embedding dimension, explaining its strong empirical correlation with generalization performance. Experiments across architectures and datasets validate the theory and demonstrate the utility of embedding-based diagnostics.

</details>


### [283] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: MetaDrug：一个多层级、不确定性感知的元学习框架，用于解决药物推荐中的患者冷启动问题，通过自适应和同伴自适应机制，结合不确定性量化，在MIMIC-III和AKI数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐方法面临患者冷启动问题，即新患者因缺乏足够的处方历史而导致推荐不可靠。虽然先前研究使用医学知识图谱连接药物概念，但这些方法主要解决项目冷启动问题，难以提供适应个体患者特征的个性化推荐。元学习在推荐系统中处理新用户稀疏交互方面显示出潜力，但在EHR数据中的独特序列结构应用仍不足。

Method: 提出MetaDrug框架，包含：1）两级元自适应机制：自适应（使用患者自身医疗事件作为支持集捕捉时间依赖）和同伴自适应（使用相似患者访问丰富新患者表示）；2）不确定性量化模块：对支持访问进行排序并过滤无关信息以确保自适应一致性。

Result: 在MIMIC-III和急性肾损伤（AKI）数据集上的实验结果表明，MetaDrug在冷启动患者上持续优于最先进的药物推荐方法。

Conclusion: MetaDrug通过创新的元学习框架有效解决了药物推荐中的患者冷启动问题，结合自适应和同伴自适应机制以及不确定性量化，能够为缺乏历史数据的新患者提供更可靠的个性化药物推荐。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [284] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 该研究系统探索了分子语言模型的缩放规律，通过训练300个模型和超过10,000次实验，揭示了在固定计算预算下模型大小、训练数据量和分子表示对性能的影响，并发布了最大的分子语言模型库。


<details>
  <summary>Details</summary>
Motivation: 分子生成模型在扩展到大数据集和大模型规模时表现出潜力，但尚不清楚这些模型是否遵循可预测的缩放规律。理解在固定计算预算下如何最优分配资源（模型大小、数据量、分子表示）对分子模型发展至关重要。

Method: 研究训练了300个模型，进行了超过10,000次实验，严格控制计算预算，同时独立变化模型大小、训练token数量和分子表示，系统研究分子语言模型在预训练和下游任务中的缩放行为。

Result: 研究结果表明分子模型在预训练和下游迁移中都存在清晰的缩放规律，分子表示对性能有显著影响，并解释了先前观察到的分子生成缩放行为不一致性。研究还发布了迄今为止最大的分子语言模型库。

Conclusion: 分子语言模型确实遵循可预测的缩放规律，分子表示是影响性能的关键因素。这项研究为分子模型开发提供了重要的缩放规律指导，并公开了大规模模型库以促进未来研究。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [285] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出Style-Conditioned Implicit Q-Learning (SCIQL)框架，通过显式风格监督和门控优势加权回归机制，在离线强化学习中同时优化任务性能和风格对齐。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，风格条件策略学习面临分布偏移和风格与奖励之间固有冲突的挑战，现有方法难以有效协调这两个目标。

Method: 提出统一的行为风格定义，并基于此构建SCIQL框架，结合离线目标条件RL技术（如后见重标记和价值学习），引入门控优势加权回归机制来优化任务性能同时保持风格对齐。

Result: 实验表明SCIQL在任务性能和风格对齐两个目标上都优于现有的离线方法。

Conclusion: SCIQL通过统一风格定义和门控机制，成功解决了离线RL中风格条件策略学习的核心挑战，实现了任务性能与风格对齐的有效平衡。

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [286] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 本文建立了稀疏注意力机制与紧支撑核函数之间的理论对应关系，揭示了稀疏注意力如何从核回归中自然产生。


<details>
  <summary>Details</summary>
Motivation: 现有研究已经建立了自注意力机制与核回归之间的联系，但缺乏对稀疏注意力机制的核理论理解。本文旨在填补这一空白，为稀疏注意力提供理论基础。

Method: 通过建立稀疏注意力与紧支撑核函数之间的形式对应关系，证明归一化ReLU和sparsemax注意力分别对应于固定和自适应归一化的Epanechnikov核回归。更一般地，展示了非参数密度估计中广泛使用的核函数与α-entmax注意力的对应关系。

Result: 证明了稀疏注意力机制可以从核设计自然产生，为启发式的top-k注意力和其他关联记忆机制提供了理论依据。基于核回归的Memory Mosaics变体在语言建模、上下文学习和长度泛化任务中取得了有竞争力的性能。

Conclusion: 本文为稀疏注意力机制提供了统一的核理论框架，解释了稀疏性如何从核设计中自然产生，并为设计注意力机制提供了原则性方法。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [287] [Float8@2bits: Entropy Coding Enables Data-Free Model Compression](https://arxiv.org/abs/2601.22787)
*Patrick Putzky,Martin Genzel,Mattes Mollenhauer,Sebastian Schulze,Thomas Wollmann,Stefan Dietzel*

Main category: cs.LG

TL;DR: EntQuant框架首次将数据依赖方法的高性能与数据无关方法的快速通用性相结合，通过熵编码实现极端压缩（<4位），在30分钟内压缩700亿参数模型，同时保持功能性能。


<details>
  <summary>Details</summary>
Motivation: 当前后训练压缩存在两个对立范式：快速、数据无关、模型无关的方法（如NF4/HQQ）在极端比特率（<4位）下会出现功能崩溃；而依赖校准数据或恢复训练的方法虽然保真度高，但计算成本高且对数据分布变化鲁棒性不确定。需要结合两者的优势。

Method: EntQuant通过熵编码将数值精度与存储成本解耦，实现极端压缩。该方法具有数据无关方法的快速性和通用性，同时能达到数据依赖方法的性能水平。

Result: 在30分钟内压缩700亿参数模型，在标准评估集和模型上达到最先进结果，在指令调优模型的复杂基准测试中保持功能性能，推理开销适中。

Conclusion: EntQuant首次统一了后训练压缩的两个对立范式，实现了极端压缩下的实用价值，为大规模模型压缩提供了高效可行的解决方案。

Abstract: Post-training compression is currently divided into two contrasting regimes. On the one hand, fast, data-free, and model-agnostic methods (e.g., NF4 or HQQ) offer maximum accessibility but suffer from functional collapse at extreme bit-rates below 4 bits. On the other hand, techniques leveraging calibration data or extensive recovery training achieve superior fidelity but impose high computational constraints and face uncertain robustness under data distribution shifts. We introduce EntQuant, the first framework to unite the advantages of these distinct paradigms. By matching the performance of data-dependent methods with the speed and universality of data-free techniques, EntQuant enables practical utility in the extreme compression regime. Our method decouples numerical precision from storage cost via entropy coding, compressing a 70B parameter model in less than 30 minutes. We demonstrate that EntQuant does not only achieve state-of-the-art results on standard evaluation sets and models, but also retains functional performance on more complex benchmarks with instruction-tuned models, all at modest inference overhead.

</details>


### [288] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO提出了一种无剪裁策略优化方法，用凸二次惩罚替代传统剪裁机制，解决LLM后训练中的优化问题


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为大语言模型后训练的核心技术，但主流算法依赖剪裁机制，在大规模应用中存在零梯度区域、奖励黑客攻击和训练不稳定等优化问题

Method: CFPO用基于总变差散度约束的凸二次惩罚替代启发式剪裁，产生处处可微的目标函数，无需硬边界即可实现稳定的策略更新

Result: 在推理任务中，CFPO与剪裁方法在下游基准测试中表现相当，同时扩展了稳定训练范围；在对齐任务中，CFPO缓解了冗长利用问题，减少了能力退化，同时保持了竞争力的指令跟随性能

Conclusion: CFPO仅需一行代码更改且无需额外超参数，是LLM后训练中剪裁方法的有前景的即插即用替代方案

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [289] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出MS-EDEN量化方法和Quartet II方案，在NVFP4格式上实现更精确的量化训练，相比随机舍入降低2倍量化误差，在1.9B参数LLM训练中验证效果，提供Blackwell GPU内核实现4.2倍加速。


<details>
  <summary>Details</summary>
Motivation: NVFP4低精度格式虽然支持端到端量化训练，但现有量化方法为获得无偏梯度估计而牺牲表示能力，导致精度损失。需要改进NVFP4量化训练方法，减少量化误差，提升训练精度。

Method: 提出MS-EDEN无偏量化方法，针对微尺度格式设计，量化误差比随机舍入低2倍以上。将其集成到Quartet II全NVFP4量化方案中，优化线性层的所有主要矩阵乘法（前向和反向传播）。

Result: 理论分析显示Quartet II在所有主要矩阵乘法中实现更好的梯度估计。在1.9B参数、38B tokens的LLM训练中验证效果。提供Blackwell GPU内核，相比BF16实现4.2倍加速。

Conclusion: Quartet II方案显著改进了NVFP4量化训练状态，降低量化误差，提升训练精度，同时保持硬件加速优势，为大规模模型量化训练提供了有效解决方案。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [290] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出一种基于可分解Rank-1专家池的持续学习框架，通过动态组合稀疏任务特定更新和正交化损失，在减少96.7%参数的同时实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的持续学习面临任务适应和灾难性遗忘的挑战，现有方法通常有沉重的推理负担或依赖外部知识，而直接使用LoRA缓解遗忘问题并不简单

Method: 将单个LoRA模块重构为可分解的Rank-1专家池，通过[CLS]令牌语义动态选择专家组成稀疏任务特定更新，并提出激活引导正交损失来正交化关键LoRA权重

Result: 在多个设置下实现所有指标的SOTA结果，超越零样本上界，相比基线方法减少96.7%可训练参数，无需外部数据集或任务ID判别器，合并的LoRA权重更少且无推理延迟

Conclusion: 该方法通过稀疏组合和正交化实现了参数高效的领域感知学习，最小化任务间干扰，保持下游任务性能，为视觉语言模型的持续学习提供了计算轻量的解决方案

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [291] [Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic](https://arxiv.org/abs/2601.22970)
*Jeong Woon Lee,Kyoleen Kwak,Daeho Kim,Hyoseok Hwang*

Main category: cs.LG

TL;DR: PAVE通过正则化critic的几何特性来稳定策略，避免直接正则化策略输出，解决了actor-critic方法中策略振荡的问题。


<details>
  <summary>Details</summary>
Motivation: 连续actor-critic方法学习的策略常出现高频振荡，不适合物理部署。现有方法直接正则化策略输出，但作者认为这治标不治本，需要从根源上解决策略非平滑性问题。

Method: 通过隐函数微分分析actor-critic目标，证明策略平滑性由critic的微分几何特性决定。提出PAVE框架，将critic视为标量场，通过最小化Q梯度波动同时保持局部曲率来稳定动作梯度场。

Result: 实验结果表明PAVE在平滑性和鲁棒性方面与策略侧平滑正则化方法相当，同时保持竞争力的任务性能，且无需修改actor。

Conclusion: 策略非平滑性根本上由critic的微分几何特性决定，通过正则化critic而非直接正则化策略输出，可以更有效地获得平滑策略。

Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.

</details>


### [292] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出一种通过正则化预训练自编码器来增强潜在空间等变性的流匹配框架，用于时间序列生成，在保持高效采样优势的同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于流模型的时间序列生成方法虽然在低维潜在空间中实现了高效采样，但如何设计具有理想等变性质的潜在表示仍未被充分探索。等变性对于时间序列生成很重要，因为时间序列通常具有平移、幅度缩放等基本变换的对称性。

Method: 提出一个潜在流匹配框架，通过简单的正则化方法在预训练自编码器上显式鼓励等变性。引入等变性损失函数，强制变换信号与其重构之间的一致性，并利用该损失对潜在空间进行微调，使其对时间序列的基本变换（如平移和幅度缩放）具有等变性。

Result: 在多个真实世界数据集上的实验表明，该方法在标准时间序列生成指标上一致优于现有的基于扩散的基线方法，同时实现了数量级更快的采样速度。等变性正则化的潜在空间在保持潜在流模型计算优势的同时提高了生成质量。

Conclusion: 将几何归纳偏置纳入时间序列的潜在生成模型中具有实际益处。等变性正则化的潜在空间能够提升生成质量，同时保持高效采样的优势，为时间序列生成提供了有前景的方向。

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [293] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 提出Mano优化器，通过流形优化方法解决LLM训练中AdamW忽略结构特性和Muon丢失曲率信息的问题，在内存和计算复杂度更低的情况下超越现有优化器。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练成本高昂，主流优化器存在局限性：AdamW依赖对角曲率估计忽略结构特性，Muon应用全局谱归一化但丢失曲率信息。传统流形优化方法在大规模模型优化中表现不佳，需要新的解决方案。

Method: 提出Mano优化器，创新性地将动量投影到模型参数的切空间，并将其约束在旋转斜流形上，首次弥合了流形优化与现代优化器之间的性能差距。

Result: 在LLaMA和Qwen3模型上的大量实验表明，Mano在内存消耗和计算复杂度更低的情况下，始终显著优于AdamW和Muon，扩展了空间和时间效率的帕累托前沿。

Conclusion: Mano成功将流形优化方法应用于LLM训练，解决了现有优化器的局限性，为大规模语言模型训练提供了更高效、性能更优的优化解决方案。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [294] [Hierarchical Shift Mixing -- Beyond Dense Attention in Transformers](https://arxiv.org/abs/2601.22852)
*Robert Forchheimer*

Main category: cs.LG

TL;DR: HSM是一种分层移位混合框架，通过将token交互分布到Transformer各层而非每层密集计算，实现线性时间复杂度的token混合，性能接近softmax注意力，且与注意力混合使用可超越GPT基线同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: Transformer中的softmax注意力层存在二次时间复杂度问题，现有替代方法大多以性能下降为代价。需要一种既能保持性能又能降低计算复杂度的token混合方法。

Method: 提出分层移位混合（HSM）框架，将成对token交互分布到Transformer各层中计算，而非每层密集计算。该框架对具体混合函数保持不可知性，支持线性时间复杂度。

Result: 即使简单的HSM变体也能达到接近softmax注意力的性能。HSM与softmax注意力混合的架构在训练和推理时都能超越GPT风格Transformer基线，同时降低计算成本。

Conclusion: HSM提供了一种有效的线性时间复杂度token混合方案，既能保持性能，又能显著降低计算复杂度，为高效Transformer架构设计提供了新思路。

Abstract: Since the introduction of the Transformer architecture for large language models, the softmax-based attention layer has faced increasing scrutinity due to its quadratic-time computational complexity. Attempts have been made to replace it with less complex methods, at the cost of reduced performance in most cases. We introduce Hierarchical Shift Mixing (HSM), a general framework for token mixing that distributes pairwise token interactions across Transformer layers rather than computing them densely within each layer. HSM enables linear-time complexity while remaining agnostic to the specific mixing function. We show that even simple HSM variants achieve performance close to softmax attention, and that hybrid architectures combining HSM with softmax attention can outperform a GPT-style Transformer baseline while reducing computational cost during both training and inference.

</details>


### [295] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OptiMAG：基于不平衡最优传输的正则化框架，解决多模态属性图中显式图结构与隐式语义结构不一致的问题，通过Fused Gromov-Wasserstein距离引导跨模态结构一致性。


<details>
  <summary>Details</summary>
Motivation: 多模态属性图（MAGs）中，不同模态嵌入诱导的隐式语义结构与显式图结构存在不一致性。现有方法在固定显式图结构上进行消息传递时，会聚合不相似的特征，引入模态特定噪声，阻碍有效的节点表示学习。

Method: 提出OptiMAG框架，使用Fused Gromov-Wasserstein距离显式引导局部邻域内的跨模态结构一致性，缓解结构-语义冲突。同时引入KL散度惩罚来自适应处理跨模态不一致性。该框架可作为即插即用的正则化器集成到现有多模态图模型中。

Result: 实验表明，OptiMAG在多种任务上持续优于基线方法，包括图中心任务（节点分类、链接预测）和多模态中心生成任务（图到文本、图到图像）。

Conclusion: OptiMAG通过最优传输理论有效解决了多模态属性图中的结构-语义不一致问题，能够提升现有多模态图模型的性能，具有广泛的适用性。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [296] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出Continuous Constraint Interpolation (CCI)统一框架，将离线RL中的三种约束方法（加权行为克隆、密度正则化、支持约束）统一为连续约束谱，并开发ACPO算法自适应调整约束类型。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法通常固定使用单一约束形式（加权行为克隆、密度正则化或支持约束），缺乏统一框架来解释它们之间的联系和权衡，限制了性能优化。

Method: 提出Continuous Constraint Interpolation (CCI)框架，通过单一插值参数实现三种约束类型的平滑过渡和组合；基于此开发Automatic Constraint Policy Optimization (ACPO)算法，使用原始-对偶方法自适应调整插值参数。

Result: 在D4RL和NeoRL2基准测试中取得稳健性能提升，达到最先进的整体性能；建立了最大熵性能差异引理，推导了最优策略及其参数投影的性能下界。

Conclusion: CCI框架统一了离线RL中的主要约束方法，ACPO算法能够自适应选择最优约束类型，为离线RL提供了更灵活和强大的优化框架。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [297] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Matterhorn是一种新型脉冲变压器，通过掩码时间到首次脉冲编码和忆阻突触单元，显著降低了SNN在LLM推理中的能耗，在保持准确性的同时实现了2.31倍的能效提升。


<details>
  <summary>Details</summary>
Motivation: 当前SNN的能耗评估主要关注计算操作，忽略了数据移动等实际硬件成本（占总能耗近80%），这限制了SNN在LLM推理中的实际能效表现。

Method: 提出了两种关键技术：1）掩码时间到首次脉冲编码，通过重新分配零能量静默状态到最频繁的膜电位，减少脉冲移动能量；2）忆阻突触单元，利用存内计算技术直接在内存中执行模拟积分，消除权重访问开销。

Result: 在GLUE基准测试中，Matterhorn实现了新的最先进性能，平均准确率比现有SNN高1.42%，同时能效提升了2.31倍。

Conclusion: Matterhorn通过创新的编码方案和硬件设计，有效解决了SNN在LLM推理中的能耗瓶颈，为高效能SNN系统提供了可行的解决方案。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [298] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 提出仅使用两个sEMG通道的深度学习框架，通过卷积稀疏自编码器提取特征，实现高精度手势识别，并采用少样本迁移学习和增量学习策略解决个体差异和功能扩展问题。


<details>
  <summary>Details</summary>
Motivation: 肌电假肢控制面临个体间差异大和高密度传感器阵列临床不实用的挑战，需要开发仅使用少量传感器通道的高精度识别方法。

Method: 使用卷积稀疏自编码器直接从原始sEMG信号提取时间特征，避免手工特征工程；采用少样本迁移学习处理个体差异；通过增量学习策略支持功能扩展。

Result: 6类手势识别达到94.3%±0.3%的F1分数；少样本迁移学习将未见受试者性能从35.1%±3.1%提升至92.3%±0.9%；增量学习扩展到10类手势保持90.0%±0.2%的F1分数。

Conclusion: 该框架在仅使用两个sEMG通道的情况下实现了高精度手势识别，通过迁移学习和增量学习解决了个体差异和功能扩展问题，为下一代经济、自适应的假肢系统提供了可扩展的解决方案。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [299] [Synthetic Time Series Generation via Complex Networks](https://arxiv.org/abs/2601.22879)
*Jaime Vale,Vanessa Freitas Silva,Maria Eduarda Silva,Fernando Silva*

Main category: cs.LG

TL;DR: 提出基于分位数图映射的时间序列生成框架，通过将时间序列转换为分位数图再逆映射重构，生成保留原始统计和结构特性的合成数据，与GAN方法相比具有竞争力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 高质量时间序列数据获取受限（隐私、成本、标注问题），合成时间序列生成成为解决方案。现有方法如GAN存在可解释性不足等问题，需要探索替代方法。

Method: 提出基于复杂网络映射的框架：1）将时间序列转换为分位数图（QG）；2）通过逆映射重构生成合成时间序列；3）保持原始数据的统计和结构特性。

Result: 在模拟和真实数据集上评估，结果表明该方法在保真度和实用性上与最先进的GAN方法竞争，同时提供更好的可解释性。

Conclusion: 分位数图方法为合成时间序列生成提供了有竞争力且可解释的替代方案，能够有效解决数据获取限制问题。

Abstract: Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.

</details>


### [300] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 论文分析了可微分匹配层中离散排列恢复不稳定的根本原因——过早模式崩溃，并提出了一种自适应调度算法来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 可微分匹配层通过熵正则化最优传输实现近似推理，但在退火过程中恢复离散排列时存在不稳定性问题。论文旨在揭示这种失败的根本机制并提出解决方案。

Method: 通过分析Sinkhorn固定点映射的非正规动力学，揭示了热力学速度极限理论。提出Efficient PH-ASC自适应调度算法，通过监控推理过程的稳定性并强制执行线性稳定性定律，将昂贵的谱诊断与训练循环解耦。

Result: 算法将计算开销从O(N³)降低到摊销O(1)，实现了稳定的离散排列恢复。提供了开源实现和交互式演示。

Conclusion: 论文识别了过早模式崩溃是可微分匹配层不稳定的根本原因，提出的自适应调度算法有效解决了该问题，为结构预测中的近似推理提供了更稳定的机制。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [301] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: 提出基于Wasserstein GAN的密度感知条件图生成框架，用可学习的距离边预测器替代随机采样，生成结构更连贯、类别一致的图数据


<details>
  <summary>Details</summary>
Motivation: 传统图生成方法通常使用固定概率的随机边采样，难以捕捉节点间复杂的结构依赖关系和类别特定的连接模式，限制了生成图的质量

Method: 使用Wasserstein GAN框架，将节点嵌入到潜在空间，通过可微边预测器从节点嵌入直接确定成对关系，并采用密度感知选择机制自适应控制边密度以匹配真实图的稀疏分布

Result: 在基准数据集上的实验表明，该方法生成的图具有更优的结构连贯性和类别一致性连接，学习的边预测器能捕捉超越简单启发式的复杂关系模式

Conclusion: 该方法提高了训练稳定性并实现了可控合成，为真实图生成和数据增强提供了有效框架

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [302] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL：一种新颖的多任务强化学习方法，能够实现零样本泛化到未见过的命题词汇表，通过将命题视为参数化谓词而非离散符号来处理。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中的核心挑战是训练能够执行训练期间未见任务的通用策略。现有LTL引导的方法虽然能在LTL规范间泛化，但无法泛化到未见过的高层事件命题词汇表。

Method: 将命题视为参数化谓词的实例而非离散符号，提出新颖架构嵌入和组合谓词来表示LTL规范，使策略能够学习相关命题间的共享结构。

Result: 在具有挑战性的环境中，成功实现了对新颖命题和任务的零样本泛化，不仅能在LTL公式结构上组合泛化，还能在命题上参数化泛化。

Conclusion: PlatoLTL通过参数化谓词表示方法，解决了多任务强化学习中命题词汇表泛化的关键限制，为更广泛的任务泛化提供了新途径。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [303] [ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations](https://arxiv.org/abs/2601.23068)
*Joao Fonseca,Julia Stoyanovich*

Main category: cs.LG

TL;DR: ExplainerPFN：首个零样本Shapley值估计方法，无需访问底层模型即可解释特征重要性


<details>
  <summary>Details</summary>
Motivation: 现实世界中经常无法直接访问模型，且精确计算Shapley值成本高昂，需要一种无需模型访问的零样本特征重要性解释方法

Method: 基于TabPFN构建表格基础模型，在随机结构因果模型生成的合成数据集上进行预训练，使用精确或近似Shapley值监督学习，训练后无需模型访问即可预测特征归因

Result: ExplainerPFN在真实和合成数据集上表现优异，性能与需要2-10个SHAP示例的少样本替代解释器相当

Conclusion: ExplainerPFN是首个零样本Shapley值估计方法，能够在无需模型访问的情况下提供高质量的特征重要性解释，为模型可解释性提供了新范式

Abstract: Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. Further, even when model access is possible, their exact computation may be prohibitively expensive. We investigate whether meaningful Shapley value estimations can be obtained in a zero-shot setting, using only the input data distribution and no evaluations of the target model. To this end, we introduce ExplainerPFN, a tabular foundation model built on TabPFN that is pretrained on synthetic datasets generated from random structural causal models and supervised using exact or near-exact Shapley values. Once trained, ExplainerPFN predicts feature attributions for unseen tabular datasets without model access, gradients, or example explanations.
  Our contributions are fourfold: (1) we show that few-shot learning-based explanations can achieve high fidelity to SHAP values with as few as two reference observations; (2) we propose ExplainerPFN, the first zero-shot method for estimating Shapley values without access to the underlying model or reference explanations; (3) we provide an open-source implementation of ExplainerPFN, including the full training pipeline and synthetic data generator; and (4) through extensive experiments on real and synthetic datasets, we show that ExplainerPFN achieves performance competitive with few-shot surrogate explainers that rely on 2-10 SHAP examples.

</details>


### [304] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出基于预秩函数的正则化校准方法，用于训练多变量分布回归模型，确保多变量校准，并引入PCA预秩检测依赖结构误设


<details>
  <summary>Details</summary>
Motivation: 尽管单变量概率预测已取得进展，但实现多变量校准仍然具有挑战性。现有预秩函数主要用于事后评估，缺乏在训练过程中确保多变量校准的方法。

Method: 提出基于正则化的校准方法，在训练多变量分布回归模型时使用预秩函数强制执行多变量校准。引入新颖的PCA预秩，将预测投影到预测分布的主方向上。

Result: 通过模拟研究和18个真实世界多输出回归数据集的实验表明，该方法显著改善了多变量预秩校准，且不损害预测准确性。PCA预秩能够检测现有预秩无法发现的依赖结构误设。

Conclusion: 该方法为多变量概率预测提供了有效的校准框架，PCA预秩作为诊断工具能够揭示预测分布依赖结构的缺陷。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [305] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 提出了一种结合决策树和GP的贝叶斯模型，通过GP叶节点实现更好的外推和不确定性校准


<details>
  <summary>Details</summary>
Motivation: 传统决策树在回归任务中外推能力差，不确定性校准不足，特别是在分布偏移时容易过度自信

Method: 将VSPYCT扩展为单树贝叶斯模型，每个叶节点配备GP预测器，使用贝叶斯斜分割进行不确定性感知的空间划分，GP叶节点建模局部函数行为，并设计门控机制在输入超出叶节点训练支持时激活基于GP的外推

Result: 在基准回归任务中相比标准变分斜树有更好的预测性能，在外推场景中性能提升显著

Conclusion: 提出的贝叶斯树模型通过GP叶节点有效解决了决策树在外推和不确定性校准方面的局限性

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [306] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 论文提出进化预测（EF）范式，解决直接预测（DF）在长期时间序列预测中的优化异常问题，通过短时训练结合进化推理超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的直接预测（DF）范式在长期时间序列预测中占主导地位，但存在根本性缺陷：输出与评估时域的刚性耦合导致每次目标时域变化都需要重新训练，计算成本高昂。作者发现了一个反直觉的优化异常现象。

Method: 提出进化预测（EF）范式，将预测建模为生成过程而非静态映射。证明DF只是EF的退化特例。通过短时训练模型结合进化推理机制，避免DF中远距离未来梯度冲突对局部动态学习的破坏。

Result: 实验表明，单一的EF模型在标准基准测试中超越了任务特定的DF集成模型，并在极端外推中表现出鲁棒的渐近稳定性。短时训练模型结合EF范式显著优于直接长时训练模型。

Conclusion: 这项工作推动了LTSF的范式转变：从被动的静态映射转向自主的进化推理。EF作为统一的生成框架，解决了DF的根本优化病理问题，为长期时间序列预测提供了更高效、更稳定的解决方案。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [307] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: FlexLoRA：基于熵引导的灵活低秩适应框架，通过谱能量熵评估矩阵重要性，支持在全局预算下进行秩剪枝和扩展，使用零影响初始化确保稳定性，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型在多个领域取得显著成功，但完全微调的计算和内存成本过高。参数高效微调（PEFT）成为主流范式。其中LoRA引入可训练低秩矩阵表现良好，但其固定秩设计限制了灵活性。现有动态秩分配方法依赖启发式元素级指标，缺乏区分矩阵级别的评估，且没有机制扩展需要额外适应的层的能力。

Method: 提出FlexLoRA框架：1）通过谱能量熵评估矩阵重要性；2）在全局预算下支持秩剪枝和扩展；3）对新添加的奇异方向使用零影响初始化确保训练稳定性。该方法解决了现有方法在粒度、灵活性和稳定性方面的限制。

Result: 大量实验表明，FlexLoRA在多个基准测试中一致优于最先进的基线方法。代码已在GitHub上开源。

Conclusion: FlexLoRA通过熵引导的灵活低秩适应框架，为参数高效微调提供了更原则性的解决方案，解决了现有方法在粒度、灵活性和稳定性方面的限制，在多个任务中表现出优越性能。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [308] [Regularisation in neural networks: a survey and empirical analysis of approaches](https://arxiv.org/abs/2601.23131)
*Christiaan P. Opperman,Anna S. Bosman,Katherine M. Malan*

Main category: cs.LG

TL;DR: 研究检验了正则化技术总能提升神经网络泛化性能的假设，通过系统分类和实证分析发现正则化效果具有数据集依赖性。


<details>
  <summary>Details</summary>
Motivation: 尽管正则化技术被广泛用于提升神经网络泛化能力，但通常假设任何正则化都会带来性能提升。本研究旨在验证这一假设是否在实践中成立，并系统理解不同正则化方法的效果和相互关系。

Method: 1) 提出正则化技术的四类分类法：数据策略、架构策略、训练策略和损失函数策略；2) 在10个数值和图像数据集上对MLP和CNN架构进行实证比较；3) 分析不同方法间的矛盾和对应关系。

Result: 正则化的效果具有数据集依赖性：正则化项仅对数值数据集提升性能，而批归一化仅对图像数据集有效。不同正则化方法的效果因数据集类型而异，没有普遍适用的最佳正则化策略。

Conclusion: 正则化并非总能提升性能，其效果取决于具体数据集。理解不同正则化技术的影响及其相互关系对于在实践中恰当使用这些方法至关重要，需要根据具体任务和数据特性选择正则化策略。

Abstract: Despite huge successes on a wide range of tasks, neural networks are known to sometimes struggle to generalise to unseen data. Many approaches have been proposed over the years to promote the generalisation ability of neural networks, collectively known as regularisation techniques. These are used as common practice under the assumption that any regularisation added to the pipeline would result in a performance improvement. In this study, we investigate whether this assumption holds in practice. First, we provide a broad review of regularisation techniques, including modern theories such as double descent. We propose a taxonomy of methods under four broad categories, namely: (1) data-based strategies, (2) architecture strategies, (3) training strategies, and (4) loss function strategies. Notably, we highlight the contradictions and correspondences between the approaches in these broad classes. Further, we perform an empirical comparison of the various regularisation techniques on classification tasks for ten numerical and image datasets applied to the multi-layer perceptron and convolutional neural network architectures. Results show that the efficacy of regularisation is dataset-dependent. For example, the use of a regularisation term only improved performance on numeric datasets, whereas batch normalisation improved performance on image datasets only. Generalisation is crucial to machine learning; thus, understanding the effects of applying regularisation techniques, and considering the connections between them is essential to the appropriate use of these methods in practice.

</details>


### [309] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 提出DC-LA算法，用于采样目标分布π∝exp(-f-r)，其中f是Lipschitz光滑的数据保真项，r=r₁-r₂是非光滑的DC函数。通过Moreau包络平滑r，将正则项的凹部分重新分配到数据保真项中，建立收敛性分析。


<details>
  <summary>Details</summary>
Motivation: 解决非光滑DC正则化项的采样问题，传统方法难以处理非光滑正则项，需要开发能有效处理DC结构的采样算法。

Method: 利用DC结构，对r₁和r₂分别应用Moreau包络平滑，将正则项的凹部分重新分配到数据保真项中，提出DC-LA（DC proximal Langevin algorithm）算法。

Result: 在V是距离耗散的假设下，证明了DC-LA在q-Wasserstein距离中对所有q∈ℕ*收敛到目标分布π（考虑离散化和平滑误差），改进了先前非对数凹采样的结果。

Conclusion: DC-LA算法能有效处理非光滑DC正则化采样问题，在合成和真实CT应用中都能提供准确的分布和可靠的不确定性量化。

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [310] [Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures](https://arxiv.org/abs/2601.23147)
*Saeid Jamshidi,Omar Abdul Wahab,Rolando Herrero,Foutse Khomh*

Main category: cs.LG

TL;DR: STGAT框架通过时空图注意力网络检测能源物联网中的时间异常，包括时钟漂移、同步偏移和Y2K38溢出等问题，在准确性和检测延迟方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的时间完整性对智能电网等能源网络至关重要，但现有系统易受时钟漂移、时间同步操纵和Y2K38溢出等时间异常影响，而传统基于可靠时间戳的异常检测方法无法有效处理这些时间不一致问题。

Method: 提出STGAT框架，结合漂移感知时间嵌入和时间自注意力捕捉单个设备的时间演化异常，使用图注意力建模时间误差的空间传播，并通过曲率正则化潜在表示从几何上分离正常时钟演化与异常。

Result: 在受控时间扰动的能源物联网遥测数据上，STGAT达到95.7%的准确率，显著优于循环、Transformer和图基线方法（d > 1.8, p < 0.001），检测延迟减少26%，仅需2.3个时间步延迟，在溢出、漂移和物理不一致情况下保持稳定性能。

Conclusion: STGAT能够有效检测能源物联网中的时间异常，为时间完整性提供可靠保障，在准确率、检测延迟和鲁棒性方面均优于现有方法。

Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.

</details>


### [311] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: 提出STPGC方法，通过引入图强坍缩和图边坍缩概念，在保持拓扑特征的同时高效压缩图，解决现有方法时间复杂度过高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图压缩方法要么保持谱特征要么保持空间特征，而保持拓扑特征的方法虽然能维持GNN预测性能，但存在指数级时间复杂度的缺陷。

Method: 基于代数拓扑中的图强坍缩和图边坍缩概念，提出三个新算法：GStrongCollapse、GEdgeCollapse和NeighborhoodConing，消除支配节点和边同时严格保持拓扑特征。

Result: 实验证明STPGC在节点分类任务中高效有效，同时证明了该方法能保持GNN感受野，并开发了加速GNN训练的近似算法。

Conclusion: STPGC通过代数拓扑方法实现了可扩展的拓扑保持图压缩，解决了现有方法的时间复杂度问题，在保持GNN性能的同时提高了训练效率。

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [312] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 该研究使用深度强化学习框架，基于MIMIC-IV数据库中47,144例ICU住院数据，训练药物剂量策略。研究发现，仅以减少疼痛为目标的策略与死亡率正相关，而同时考虑减少疼痛和死亡率的策略与死亡率负相关，表明重视长期结果对制定更安全的治疗策略至关重要。


<details>
  <summary>Details</summary>
Motivation: ICU疼痛管理需要在治疗目标与患者安全之间进行复杂权衡，因为治疗不足或过度都可能导致严重后果。现有研究在镇静和镇痛方面优化的目标不重视患者生存，且使用的算法不适合不完全信息环境。本研究旨在探讨这些设计选择的风险。

Method: 研究实现了一个深度强化学习框架，在部分可观测性条件下建议每小时药物剂量。使用MIMIC-IV数据库中47,144例ICU住院数据，训练政策以处方阿片类药物、丙泊酚、苯二氮䓬类和右美托咪定。比较两种目标：仅减少疼痛 vs. 联合减少疼痛和死亡率。

Result: 两种政策都与较低的疼痛相关，但仅以减少疼痛为目标的政策与死亡率呈正相关，而同时考虑减少疼痛和死亡率的政策与死亡率呈负相关。这表明重视长期结果对于制定更安全的治疗策略至关重要，即使短期目标仍然是主要目标。

Conclusion: 在ICU疼痛管理的强化学习策略设计中，即使短期目标（如疼痛控制）是主要关注点，重视长期结果（如死亡率）对于制定更安全的治疗政策也至关重要。仅优化短期目标可能导致与不良长期结果相关的策略。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [313] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: ECTR提出了一种统一框架，通过环境条件尾部重加权增强TV不变风险最小化，同时处理环境级相关偏移和样本级多样性偏移，提升OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有IRM方法主要处理环境级的虚假相关性，但忽略了环境内样本级的异质性（如罕见或困难样本），这会影响OOD性能。需要同时处理环境级相关偏移和样本级多样性偏移。

Method: 提出ECTR框架：1）基于总变差(TV)的不变学习处理环境级相关偏移；2）环境条件尾部重加权处理环境内样本异质性；3）通过极小极大公式在没有显式环境标注时推断潜在环境。

Result: 在回归、表格数据、时间序列和图像分类基准测试中，在混合分布偏移下，最差环境性能和平均OOD性能均获得一致提升。

Conclusion: ECTR通过统一处理环境级和样本级分布偏移，使两种机制在混合分布偏移下互补，显著提升了OOD泛化能力，特别是在没有显式环境标注时也能有效工作。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [314] [SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training](https://arxiv.org/abs/2601.23155)
*Powei Chang,Jinpeng Zhang,Bowen Chen,Chenyu Wang,Chenlu Guo,Yixing Zhang,Yukang Gao,JianXiang Xiang,Yue Gao,Chaoqun Sun,Yiyi Chen,Dongying Kong*

Main category: cs.LG

TL;DR: SPICE：一种冲突感知的数据选择方法，通过惩罚梯度冲突来最大化Fisher信息，仅用10%数据就能达到或超过全数据调优的性能


<details>
  <summary>Details</summary>
Motivation: 基于信息的数据选择在指令调优中很有吸引力，但实践中发现梯度冲突会减缓边际信息增益的衰减，阻碍信息最大化。需要量化这种偏离理想子模性的程度，并提出更有效的选择方法。

Method: 提出ε-分解来量化梯度冲突导致的子模性偏离，基于此设计SPICE选择器：最大化信息同时惩罚梯度不对齐，支持早停和代理模型以提高效率。

Result: SPICE选择的子集比原始标准具有更高的对数行列式信息，这些信息增益转化为性能提升：在8个基准测试中，仅用10%数据就能匹配或超过6种方法（包括全数据调优）。

Conclusion: 梯度冲突是信息最大化数据选择的关键限制因素，SPICE通过冲突感知选择实现了显著性能提升，大幅降低了训练成本。

Abstract: Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost.

</details>


### [315] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 提出轨迹探测协议，通过截断推理轨迹并回注模型，研究LLM推理过程中准确性和决策承诺的演变规律


<details>
  <summary>Details</summary>
Motivation: 当前不清楚LLM在生成推理轨迹时，准确性和决策承诺如何随时间演变，以及中间推理片段是否提供超出长度或风格效应的答案相关信息

Method: 提出三步协议：1)生成模型的推理轨迹；2)按固定token百分比截断；3)将部分轨迹回注模型（或不同模型），通过下一个token概率测量答案选择分布

Result: 准确性和决策承诺随提供的推理token百分比增加而一致提升；这些提升主要由模型生成的相关内容驱动，而非上下文长度或通用"推理风格"效应；更强模型能从错误部分轨迹成功回溯，但即时答案常锚定在较弱模型的错误响应中

Conclusion: 轨迹探测为推理模型的高效安全部署提供诊断工具，测量结果可指导实用的轨迹处理和监控策略，提高可靠性，无需假设中间token是固有忠实解释

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [316] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 该论文解决了线性赌博机中Nash遗憾的次优性问题，提出了新的分析工具实现最优Nash遗憾界，并首次研究了p-均值遗憾框架，提出了通用的FairLinBandit算法框架。


<details>
  <summary>Details</summary>
Motivation: Nash遗憾作为随机多臂赌博机的公平感知性能指标已被扩展到线性赌博机，但现有结果存在维度d上的次优性，源于依赖限制性集中不等式的证明技术。

Method: 引入新的分析工具解决Nash遗憾的次优性问题；提出p-均值遗憾框架统一公平性和效用目标；设计通用的FairLinBandit元算法框架，基于任何线性赌博机策略；使用Phased Elimination和Upper Confidence Bound两种算法实例化该框架。

Result: 实现了线性赌博机中Nash遗憾的最优界；证明了两种算法实例在整个p值范围内都能实现亚线性p-均值遗憾；在真实数据集生成的线性赌博机实例上，实验表明方法始终优于现有最先进基线。

Conclusion: 该工作解决了线性赌博机中Nash遗憾的开放性问题，提出了更优的分析工具和算法框架，并扩展了公平性度量的研究范围，为公平感知的线性赌博机提供了有效的解决方案。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [317] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: DyCAST是一种动态字符对齐的语音分词器，通过软字符级对齐和显式时长建模实现可变帧率分词，相比固定帧率编解码器使用更少的token


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器通常以固定帧率运行，在时间上均匀分配token，产生不必要的长序列，这限制了处理效率

Method: DyCAST通过软字符级对齐和显式时长建模实现可变帧率分词，训练时学习将token与字符级语言单元关联，解码时支持无需对齐的直接时长控制，并引入检索增强解码机制提升低帧率下的重建质量

Result: 实验表明，DyCAST在语音重建质量和下游任务性能方面具有竞争力，同时比固定帧率编解码器使用显著更少的token

Conclusion: DyCAST提供了一种高效的动态帧率语音分词方法，通过字符对齐和时长控制减少了token序列长度，同时保持了语音质量

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [318] [Learning to Execute Graph Algorithms Exactly with Graph Neural Networks](https://arxiv.org/abs/2601.23207)
*Muhammad Fetrat Qharabagh,Artur Back de Luca,George Giapitzakis,Kimon Fountoulakis*

Main category: cs.LG

TL;DR: 论文证明了在图神经网络中，通过训练多层感知机学习局部指令，可以在有限精度约束下精确学习图算法，包括分布式计算模型和经典图算法。


<details>
  <summary>Details</summary>
Motivation: 理解图神经网络的学习能力，特别是它们执行算法的能力，是一个核心理论挑战。本文旨在证明在图算法学习方面的理论结果。

Method: 采用两步法：1) 训练多层感知机(MLP)集合来执行单个节点的局部指令；2) 在推理时，将训练好的MLP集合作为图神经网络(GNN)的更新函数。利用神经正切核(NTK)理论证明局部指令可以从小训练集中学习。

Result: 证明了在有限精度约束下，图算法可以被精确学习，且推理时能够无错误地执行完整图算法。为LOCAL分布式计算模型建立了严格的可学习性结果，并展示了消息洪泛、广度优先搜索、深度优先搜索和Bellman-Ford等算法的可学习性。

Conclusion: 该工作为图神经网络学习图算法提供了理论保证，证明了在有限精度和有限度约束下，图神经网络能够精确学习并执行分布式图算法。

Abstract: Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.

</details>


### [319] [Agile Reinforcement Learning through Separable Neural Architecture](https://arxiv.org/abs/2601.23225)
*Rajib Mostakim,Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: SPAN是一种基于样条的自适应网络，通过可学习的预处理层和可分离张量积B样条基，在资源受限的强化学习环境中实现了比传统MLP更高的参数效率和样本效率。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在资源受限环境中部署时，传统的多层感知机(MLP)存在参数效率低的问题，因为其对许多价值函数的平滑结构具有不完美的归纳偏置。这种不匹配会降低样本效率并减缓策略学习。现有的模型压缩技术是事后操作，无法改善学习效率。虽然基于样条的可分离架构（如KANs）提供了参数效率，但存在显著的计算开销。

Method: SPAN（基于样条的自适应网络）通过整合可学习的预处理层和可分离张量积B样条基，改进了低秩KHRONOS框架。该方法在离散（PPO）和高维连续（SAC）控制任务以及离线设置（Minari/D4RL）中进行评估。

Result: SPAN在样本效率上实现了30-50%的提升，在基准测试中比MLP基线获得了1.3-9倍更高的成功率。此外，SPAN表现出优越的随时性能和超参数变化的鲁棒性。

Conclusion: SPAN是在资源受限环境中学习内在高效策略的可行高性能替代方案，能够显著提升强化学习的效率和性能。

Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.
  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.

</details>


### [320] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: dgMARK是一种用于离散扩散语言模型的解码引导水印方法，通过控制解掩码顺序而非修改概率分布来嵌入水印，具有插入即用特性，并能抵抗编辑操作。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（dLLMs）与自回归模型不同，可以按任意顺序生成token。虽然理想的条件预测器应对解掩码顺序不变，但实际dLLMs对此顺序高度敏感，这为水印技术创造了新的通道。

Method: dgMARK引导解掩码顺序朝向那些高奖励候选token满足简单奇偶约束的位置，这种约束由二进制哈希诱导。该方法不显式重新加权模型学习到的概率，可与常见解码策略（如置信度、熵和边界排序）即插即用，并可增强为一步前瞻变体。

Result: 通过提升的奇偶匹配统计量检测水印，滑动窗口检测器确保在插入、删除、替换和改写等后编辑操作下的鲁棒性。

Conclusion: dgMARK为离散扩散语言模型提供了一种有效的解码引导水印方法，利用模型对解掩码顺序的敏感性创建水印通道，同时保持与现有解码策略的兼容性和对编辑操作的鲁棒性。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [321] [TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training](https://arxiv.org/abs/2601.23261)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Dongyang Li,Yupeng Su,Sijia Liu,Zheng Zhang*

Main category: cs.LG

TL;DR: TEON是Muon优化器的推广，通过将神经网络梯度建模为结构化高阶张量，实现跨层正交化，在GPT和LLaMA架构上显著提升训练效果


<details>
  <summary>Details</summary>
Motivation: Muon优化器在预训练大语言模型中表现出色，但仅限于层内矩阵级梯度正交化。本文旨在扩展正交化范围，通过建模梯度为高阶张量实现跨层优化，以获得更好的收敛性能

Method: 提出TEON方法，将神经网络梯度建模为结构化高阶张量，实现跨层正交化。基于理论分析开发实用实现方案，并进行相应消融实验。在GPT（130M-774M参数）和LLaMA（60M-1B参数）架构上评估

Result: TEON在多个模型规模上一致改善训练和验证困惑度，在不同近似SVD方案下表现出强鲁棒性，且收敛保证优于层级Muon

Conclusion: TEON通过跨层梯度正交化有效提升大语言模型预训练性能，为优化器设计提供了新的张量视角，具有实际应用价值

Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order tensor. We present TEON's improved convergence guarantee over layer-wise Muon, and further develop a practical instantiation of TEON based on the theoretical analysis with corresponding ablation. We evaluate our approach on two widely adopted architectures: GPT-style models, ranging from 130M to 774M parameters, and LLaMA-style models, ranging from 60M to 1B parameters. Experimental results show that TEON consistently improves training and validation perplexity across model scales and exhibits strong robustness under various approximate SVD schemes.

</details>


### [322] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 提出一个因果模型，将异常分为测量误差和机制偏移两类，通过潜在干预建模，实现根因定位和异常类型分类


<details>
  <summary>Details</summary>
Motivation: 现有异常根因分析方法忽略了异常可能来自两种根本不同的过程：测量误差（数据生成正常但记录错误）和机制偏移（数据生成过程本身发生变化）。测量误差通常可以安全修正，而机制异常需要仔细考虑，需要区分这两种类型。

Method: 定义了一个因果模型，将异常视为对潜在（"真实"）变量和观测（"测量"）变量的潜在干预。证明了这两种异常类型的可识别性，并提出最大似然估计方法进行实践应用。

Result: 实验表明，该方法在根因定位方面达到最先进性能，同时能够准确分类异常类型，即使在因果DAG未知的情况下也能保持鲁棒性。

Conclusion: 提出的因果模型能够有效区分测量误差和机制偏移两种异常类型，为异常根因分析提供了更全面的框架，在实际应用中具有更好的鲁棒性和实用性。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [323] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出DC-CoT方法，通过并行推理减少长思维链的延迟，在保持准确率的同时将最长路径长度降低35-40%


<details>
  <summary>Details</summary>
Motivation: 长思维链推理导致LLM生成延迟高，因为LLM生成是顺序的。需要降低推理延迟同时保持高准确率。

Method: 训练Divide-and-Conquer CoT：模型作为director识别可并行执行的子任务，然后spawn workers执行。采用多阶段RL算法，包括SFT初始化和数据过滤策略。

Result: 在AIME 2024和HMMT 2025等基准测试中，DC-CoT保持与DeepScaleR-1.5B-Preview相似的准确率，同时将最长路径长度降低35-40%。

Conclusion: DC-CoT能有效降低长思维链推理的延迟，为低延迟并行推理提供了可行方案。

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [324] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 提出RLRR框架，将强化学习中的绝对奖励转为相对排名，解决现有方法在可验证任务中监督稀疏和开放场景中奖励不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有基于组的强化学习方法（如GRPO）依赖绝对数值奖励，存在内在限制：在可验证任务中，相同组评估导致监督稀疏；在开放场景中，奖励模型分数范围不稳定，基于组均值的优势估计不可靠

Method: 提出RLRR框架，将奖励塑造从绝对评分转向相对排名；引入Ranking Reward Model，这是一个专门为基于组优化设计的列表式偏好模型，可直接生成相对排名

Result: 实验结果表明，RLRR在推理基准和开放生成任务上，相比标准基于组的方法取得了持续的性能提升

Conclusion: 通过将原始评估转化为稳健的相对信号，RLRR有效缓解了信号稀疏性和奖励不稳定性问题，为基于组的强化学习提供了更可靠的优化框架

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [325] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow是一种基于B样条插值的流匹配算法，用于建模动态系统，相比现有方法能更好地处理不规则采样观测数据中的高阶动态。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法不适合建模动态系统，因为它们使用线性插值构建条件路径，无法捕捉底层状态演化，特别是在从不规则采样观测中学习高阶动态时。构建满足多边际约束的统一路径具有挑战性，因为简单的高阶多项式往往不稳定且振荡。

Method: SplineFlow利用B样条插值的光滑性和稳定性，通过B样条基函数联合建模观测之间的条件路径，以结构化方式学习复杂底层动态，同时确保满足多边际要求。

Result: 在各种复杂度的确定性和随机动态系统以及细胞轨迹推断任务上的综合实验表明，SplineFlow相比现有基线方法有显著改进。

Conclusion: SplineFlow是一种理论基础的流匹配算法，通过B样条插值有效解决了动态系统建模中的多边际约束问题，在多个应用场景中表现出优越性能。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [326] [RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning](https://arxiv.org/abs/2601.23075)
*Yuexin Bian,Jie Feng,Tao Wang,Yijiang Li,Sicun Gao,Yuanyuan Shi*

Main category: cs.LG

TL;DR: 用离散化分类actor替代传统高斯actor，通过正则化网络提升on-policy深度强化学习在连续控制任务中的性能


<details>
  <summary>Details</summary>
Motivation: 传统on-policy深度强化学习使用高斯actor和浅层MLP策略，在梯度噪声大、策略更新保守时优化脆弱。需要重新审视策略表示作为on-policy优化的首要设计选择。

Method: 提出离散化分类actor，将每个动作维度表示为多个bin上的分布，策略目标类似于交叉熵损失。借鉴监督学习中的架构进展，提出正则化actor网络，同时保持critic设计不变。

Result: 仅用离散化正则化actor替代标准actor网络，就在多样连续控制基准测试中取得一致性能提升，达到state-of-the-art水平。

Conclusion: 策略表示是on-policy优化的关键设计选择，离散化分类actor结合正则化网络能显著提升连续控制任务的性能。

Abstract: On-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse continuous-control benchmarks.

</details>


### [327] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: CATTO：一种校准感知的训练目标，通过将预测置信度与经验正确性对齐来改善LLM的校准，同时保持任务准确性，并引入Confidence@k进行测试时优化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预测置信度校准方面存在问题：高置信度预测经常错误，低置信度预测反而正确。基于偏好的对齐方法进一步破坏了预测概率与正确性之间的联系，导致校准不佳。

Method: 提出校准感知的令牌级训练目标（CATTO），将预测置信度与经验预测正确性对齐，可与原始偏好优化目标结合使用。同时引入Confidence@k作为测试时缩放机制，利用校准后的令牌概率进行贝叶斯最优输出令牌选择。

Result: 与直接偏好优化（DPO）相比，CATTO在分布内将预期校准误差（ECE）降低了2.22%-7.61%，在分布外降低了1.46%-10.44%；与最强的DPO基线相比，在分布内降低了0.22%-1.24%，在分布外降低了1.23%-5.07%。置信度改进的同时不损失任务准确性，在五个数据集的多选题回答准确性上保持或略有提高。

Conclusion: CATTO有效改善了LLM的校准问题，将预测置信度与正确性对齐，同时保持任务性能。结合Confidence@k机制，为LLM校准提供了有效的训练和推理解决方案。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [328] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出DCR方法，通过精确推导非一致性分数的分布来构建更高效的排名预测集，相比基线方法减少平均预测集大小达36%


<details>
  <summary>Details</summary>
Motivation: 现有基于保形预测的排名方法过于保守，导致预测集过大。需要更高效的方法来量化排名模型的不确定性，以支持实际应用的安全部署。

Method: 提出分布感知的保形排名(DCR)，通过推导校准项目绝对排名的负超几何分布来获得非一致性分数的精确分布，从而确定更精确的保形阈值。

Result: DCR在保持有效覆盖率的同时，将平均预测集大小减少高达36%，显著提高了预测效率。

Conclusion: DCR通过利用排名分布信息，在保证理论覆盖率的前提下，显著提高了保形排名预测集的效率，为排名模型的不确定性量化提供了更实用的解决方案。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [329] [Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients](https://arxiv.org/abs/2601.23135)
*Cheng Ge,Caitlyn Heqi Yin,Hao Liang,Jiawei Zhang*

Main category: cs.LG

TL;DR: GRPO的方差归一化通过自适应梯度缩放改进了收敛速度，其效果受特征正交性和奖励方差的影响，分为三个训练阶段。


<details>
  <summary>Details</summary>
Motivation: GRPO作为语言模型推理的标准RL算法，使用方差归一化但缺乏理论解释。本文旨在理解方差归一化何时以及为何有效。

Method: 从序列级策略梯度的局部曲率角度分析，将标准差归一化视为自适应梯度。理论分析GRPO相比未归一化REINFORCE的收敛速度改进，并在GSM8K和MATH基准上进行实证分析。

Result: 理论上，GRPO在温和条件下比未归一化REINFORCE有严格改进的收敛速度。实证显示三个训练阶段：早期加速阶段（高方差和正交性）、稳定过渡阶段、后期阶段（正交性丧失限制进一步增益）。

Conclusion: 方差归一化通过自适应梯度缩放帮助GRPO，效果受特征正交性和奖励方差的动态交互影响。这为无critic的RL算法设计提供了更广泛的见解。

Abstract: Reinforcement learning (RL) has become a key driver of language model reasoning. Among RL algorithms, Group Relative Policy Optimization (GRPO) is the de facto standard, avoiding the need for a critic by using per-prompt baselines and variance normalization. Yet why and when this normalization helps remains unclear. In this work, we provide an explanation through the lens of local curvature of the sequence-level policy gradient: standard deviation normalization implements an adaptive gradient. Theoretically, under mild conditions, GRPO enjoys a strictly improved convergence rate over unnormalized REINFORCE, with gains characterized by the average within-prompt reward standard deviation across prompts and iterations. Empirically, our analysis on GSM8K and MATH benchmarks reveals three distinct training phases governed by the interplay between feature orthogonality and reward variance: (I) an early acceleration phase where high variance and orthogonality favor adaptive scaling; (II) a relatively stable transition phase; and (III) a late-stage regime where the loss of orthogonality limits further gains. Together, these results provide a principled account of when std normalization helps in GRPO, and offer broader insights into the design of critic-free RL algorithms.

</details>


### [330] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出一种约束感知的数据扰动方法，解决生成模型在等式约束分布建模中的数学限制问题


<details>
  <summary>Details</summary>
Motivation: 生成模型在科学领域中经常遇到样本受等式约束的分布建模问题，存在固有的数学限制，需要一种灵活的方法来解决这些已知缺陷

Method: 提出约束感知的数据分布扰动方法，使新分布的支持集维度与周围空间匹配，同时隐式地结合底层流形几何结构

Result: 通过理论分析和多个代表性任务的实证证据表明，该方法能够一致地实现数据分布恢复和稳定采样，适用于扩散模型和标准化流

Conclusion: 该方法计算成本低、数学上合理且高度灵活，有效解决了等式约束生成模型的已知缺陷

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [331] [Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data](https://arxiv.org/abs/2601.23153)
*Eugenia Iofinova,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文提出了Behemoth框架，通过完全合成的数据生成来研究模型编辑，在简单表格数据上验证了模型编辑方法的有效性，发现限制更新秩有时能带来更有效的更新。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在实际应用中的部署增加，模型编辑（调整模型权重以修改特定事实的输出）变得重要。然而，当前方法存在脆弱性和不完整性，且模型编辑效果受训练数据分布影响，但使用真实世界数据难以深入理解这种关系。

Method: 提出Behemoth框架，这是一个完全合成的数据生成框架。通过在简单的表格数据背景下探索模型编辑，使用合成数据来研究训练数据分布与模型编辑效果之间的关系。

Result: 在简单表格数据上验证了模型编辑方法，发现了一些令人惊讶的结果，例如在某些情况下限制更新秩能带来更有效的更新，这些发现与真实世界的结果相呼应。

Conclusion: Behemoth框架为研究模型编辑提供了可控的实验环境，有助于理解训练数据分布与模型编辑效果之间的关系，为改进模型编辑方法提供了理论基础。

Abstract: As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesirable statements. This trend has inspired practical and academic interest in model editing, that is, in adjusting the weights of the model to modify its likely outputs for queries relating to a specific fact or set of facts. This may be done either to amend a fact or set of facts, for instance, to fix a frequent error in the training data, or to suppress a fact or set of facts entirely, for instance, in case of dangerous knowledge. Multiple methods have been proposed to do such edits. However, at the same time, it has been shown that such model editing can be brittle and incomplete. Moreover the effectiveness of any model editing method necessarily depends on the data on which the model is trained, and, therefore, a good understanding of the interaction of the training data distribution and the way it is stored in the network is necessary and helpful to reliably perform model editing. However, working with large language models trained on real-world data does not allow us to understand this relationship or fully measure the effects of model editing. We therefore propose Behemoth, a fully synthetic data generation framework. To demonstrate the practical insights from the framework, we explore model editing in the context of simple tabular data, demonstrating surprising findings that, in some cases, echo real-world results, for instance, that in some cases restricting the update rank results in a more effective update. The code is available at https://github.com/IST-DASLab/behemoth.git.

</details>


### [332] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出一种无监督技能分割与层次结构发现方法，使用语法方法从无标签轨迹中分割技能并构建层次结构，在像素环境中验证效果优于基线


<details>
  <summary>Details</summary>
Motivation: 现有技能分割方法大多依赖动作标签、奖励或人工标注，限制了应用范围。需要一种完全无监督的方法来从原始轨迹中发现可重用技能和层次结构

Method: 基于语法的方法，从无标签轨迹中分割技能并诱导层次结构，能够捕捉低级行为及其组合成高级技能

Result: 在Craftax和完整版Minecraft等高维像素环境中评估，在技能分割、重用和层次质量指标上均优于现有基线，产生更有结构和语义意义的层次

Conclusion: 该方法能有效发现无监督技能层次结构，且发现的层次结构能加速和稳定下游强化学习任务的学习过程

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [333] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 论文研究了带参数噪声的随机线性老虎机问题，提出了紧致的遗憾上下界，并展示了简单的探索-利用算法可以达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究参数噪声模型下的随机线性老虎机问题，与传统的加性噪声模型形成对比，探索在这种噪声结构下是否能获得更好的遗憾界限。

Method: 采用参数噪声模型，其中奖励为a⊤θ，θ服从独立同分布。对于一般动作集，分析遗憾上界；对于ℓp单位球(p≤2)及其对偶范数q，推导出更精确的遗憾界限。

Result: 对于一般动作集，遗憾上界为Õ(√(dT log(K/δ)σ²_max))，下界为Ω̃(d√(Tσ²_max))，当log(K)≈d时紧致。对于ℓp单位球，遗憾为Õ(√(dTσ²_q))，其中σ²_q≤4，优于经典加性噪声模型的d√T遗憾。

Conclusion: 参数噪声模型下的随机线性老虎机可以获得比经典加性噪声模型更好的遗憾界限，且简单的探索-利用算法就能达到最优性能，这为实际应用提供了理论保证。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [334] [Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning](https://arxiv.org/abs/2601.23169)
*İlker Işık,Wenchao Li*

Main category: cs.LG

TL;DR: 提出一种对可互换令牌（如绑定变量）具有重命名不变性的Transformer机制，通过并行嵌入流和聚合注意力实现，在开放词汇任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前神经架构缺乏处理可互换令牌（语义等价但可区分的符号，如绑定变量）的原则性方法。模型在固定词汇表上训练后，即使底层语义保持不变，也难以泛化到未见符号

Method: 提出基于Transformer的新机制，使用并行嵌入流隔离每个可互换令牌的贡献，结合聚合注意力机制实现跨流的结构化信息共享，理论上保证对可互换令牌重命名的不变性

Result: 实验证实了该方法的理论保证，并在需要泛化到新符号的开放词汇任务上展示了显著的性能提升

Conclusion: 该方法为处理可互换令牌提供了原则性解决方案，解决了神经架构在符号泛化方面的关键限制，在开放词汇任务中具有重要应用价值

Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.

</details>


### [335] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MeshGraphNet-Transformer (MGN-T) 结合 Transformer 的全局建模能力和 MeshGraphNets 的几何归纳偏置，解决了传统 MGN 在大规模高分辨率网格上长距离信息传播效率低的问题，在工业规模网格上实现了高效的物理模拟。


<details>
  <summary>Details</summary>
Motivation: 传统 MeshGraphNets (MGN) 在大规模高分辨率网格上存在长距离信息传播效率低的问题，因为其依赖迭代消息传递机制。这限制了其在工业规模网格上的应用，特别是在需要处理复杂几何、拓扑和边界条件的场景中。

Method: 提出 MeshGraphNet-Transformer (MGN-T) 架构，将 Transformer 的全局建模能力与 MeshGraphNets 的几何归纳偏置相结合。使用物理注意力 Transformer 作为全局处理器，同时更新所有节点状态，并显式保留节点和边属性。该方法直接捕获长距离物理相互作用，无需深度消息传递堆栈或分层粗化网格。

Result: MGN-T 成功处理了工业规模网格的冲击动力学问题（传统 MGN 因消息传递不足而失败）。准确模拟了自接触、塑性和多变量输出（包括内部现象学塑性变量）。在经典基准测试中优于最先进方法，精度更高，同时保持实际效率，仅使用竞争基线所需参数的一小部分。

Conclusion: MGN-T 通过结合 Transformer 的全局建模和 MeshGraphNets 的几何结构，有效解决了大规模网格上的长距离信息传播问题，为工业规模物理模拟提供了高效、准确的解决方案，在保持参数效率的同时实现了卓越性能。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [336] [TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification](https://arxiv.org/abs/2601.23180)
*Haoyun Jiang,Junqi He,Feng Hong,Xinlong Yang,Jianwei Zhang,Zheng Li,Zhengyang Zhuge,Zhiyong Chen,Bo Han,Junyang Lin,Jiangchao Yao*

Main category: cs.LG

TL;DR: TriSpec提出了一种三元推测解码框架，通过引入轻量级代理模型减少验证成本，在保持准确性的同时实现高达35%的加速


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理效率受限于串行自回归生成，特别是随着推理能力成为关键能力且响应序列变长。现有推测解码方法在草案有效性方面已接近饱和，需要从验证成本这一新视角进行改进

Method: TriSpec三元推测解码框架：引入轻量级代理模型，仅对不确定的token调用完整目标模型，可轻松验证的草案序列直接通过。可与EAGLE-3等先进SD方法集成进一步降低验证成本

Result: 在Qwen3和DeepSeek-R1-Distill-Qwen/LLaMA系列模型上的实验显示：相比标准推测解码，TriSpec实现高达35%的加速，目标模型调用减少50%，同时保持相当的准确性

Conclusion: TriSpec通过优化验证成本显著提升推测解码效率，为LLM推理加速提供了新方向，可与现有先进方法协同工作实现更大加速

Abstract: Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has nearly saturated improvements in draft effectiveness and efficiency, this paper advances SD from a new yet critical perspective: the verification cost. We propose TriSpec, a novel ternary SD framework that, at its core, introduces a lightweight proxy to significantly reduce computational cost by approving easily verifiable draft sequences and engaging the full target model only when encountering uncertain tokens. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 35\% speedup over standard SD, with up to 50\% fewer target model invocations while maintaining comparable accuracy.

</details>


### [337] [Ensuring Semantics in Weights of Implicit Neural Representations through the Implicit Function Theorem](https://arxiv.org/abs/2601.23181)
*Tianming Qiu,Christos Sonis,Hao Shen*

Main category: cs.LG

TL;DR: 该论文利用隐函数定理建立了数据空间与权重表示空间之间的严格映射，为理解神经网络权重如何编码数据语义提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 权重空间学习是一个新兴领域，但缺乏对权重如何编码数据语义的精确理论解释。特别是隐式神经表示提供了方便的测试平台，但需要理论框架来解释权重与数据之间的关系。

Method: 使用隐函数定理建立数据空间与权重表示空间之间的严格映射。分析通过共享超网络将实例特定嵌入映射到INR权重的框架。

Result: 在2D和3D数据集的下游分类任务中，该方法实现了与现有基线竞争的性能，为网络权重研究提供了理论视角。

Conclusion: 该工作为理解权重如何编码数据语义提供了理论框架，为未来网络权重研究提供了理论基础。

Abstract: Weight Space Learning (WSL), which frames neural network weights as a data modality, is an emerging field with potential for tasks like meta-learning or transfer learning. Particularly, Implicit Neural Representations (INRs) provide a convenient testbed, where each set of weights determines the corresponding individual data sample as a mapping from coordinates to contextual values. So far, a precise theoretical explanation for the mechanism of encoding semantics of data into network weights is still missing. In this work, we deploy the Implicit Function Theorem (IFT) to establish a rigorous mapping between the data space and its latent weight representation space. We analyze a framework that maps instance-specific embeddings to INR weights via a shared hypernetwork, achieving performance competitive with existing baselines on downstream classification tasks across 2D and 3D datasets. These findings offer a theoretical lens for future investigations into network weights.

</details>


### [338] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 该研究提出了一种将交通地图转换为环形描述的方法，结合偏最小二乘回归来预测墨西哥城的空气质量，提供超本地化的动态空气质量预报。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染是全球性问题，交通是主要污染源。现有空气质量监测和预报在时空上较为粗糙，而实时交通数据通常更精细且公开可用。研究旨在利用精细交通数据提供超本地化的动态空气质量预报。

Method: 开发创新方法将简单的彩色编码交通地图转换为基于同心环的描述，改进交通状况表征。使用偏最小二乘回归基于新定义的交通强度预测污染水平。通过不同训练样本优化模型以获得最佳预测性能。

Result: 模型能够基于交通强度预测污染水平，优化后的模型获得了最佳预测性能，并深入了解了污染物与交通之间的关系。工作流程简单且可适应其他城市环境。

Conclusion: 该研究展示了结合交通数据和空气质量监测的有效方法，为超本地化动态空气质量预报提供了可行方案。工作流程具有普适性，可推广到其他城市，有助于改善城市空气质量管理和公众健康保护。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [339] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 该论文研究了众包标注聚合中的公平性问题，提出了理论分析和后处理方法，确保聚合结果满足ε-公平性约束。


<details>
  <summary>Details</summary>
Motivation: 众包标注是获取标签的常用方法，但聚合主观标注可能放大个体偏见，特别是在敏感特征方面，引发公平性担忧。目前众包聚合中的公平性问题研究不足，缺乏收敛保证和有效的后处理方法。

Method: 1) 在ε-公平框架下分析多数投票和最优贝叶斯聚合的公平性；2) 在小众包场景中推导多数投票公平性差距的上界；3) 证明聚合共识的公平性差距以指数速度收敛到真实标签的公平性差距；4) 将多类公平性后处理算法从连续设置推广到离散设置，强制执行严格的人口统计平等约束。

Result: 理论分析表明聚合共识的公平性差距以指数速度收敛到真实标签的公平性差距。实验验证了方法的有效性，并证实了理论见解。

Conclusion: 该研究填补了众包聚合公平性分析的空白，提供了理论保证和实用后处理方法，能够在保持聚合准确性的同时确保公平性约束。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [340] [Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph](https://arxiv.org/abs/2601.23233)
*Nguyen Minh Duc,Viet Cuong Ta*

Main category: cs.LG

TL;DR: SDG：一种新颖的序列级扩散框架，将动态图学习与生成式去噪统一，用于时序链接预测，通过条件去噪过程重建交互嵌入，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序图神经网络主要关注学习历史交互的表示，虽然性能强大，但仍然是纯判别式模型，只能对未来链接进行点估计，缺乏明确机制来捕捉未来时序交互的不确定性和序列结构。

Method: 提出SDG框架，将噪声注入整个历史交互序列，通过条件去噪过程联合重建所有交互嵌入，使用交叉注意力去噪解码器指导目标序列重建，并以端到端方式优化模型。

Result: 在多个时序图基准测试上的广泛实验表明，SDG在时序链接预测任务中始终达到最先进的性能。

Conclusion: SDG通过将动态图学习与生成式去噪统一，能够更全面地捕捉交互分布，为时序链接预测提供了有效的解决方案。

Abstract: Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequential structure of future temporal interactions. In this paper, we propose SDG, a novel sequence-level diffusion framework that unifies dynamic graph learning with generative denoising. Specifically, SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process, thereby enabling the model to capture more comprehensive interaction distributions. To align the generative process with temporal link prediction, we employ a cross-attention denoising decoder to guide the reconstruction of the destination sequence and optimize the model in an end-to-end manner. Extensive experiments on various temporal graph benchmarks show that SDG consistently achieves state-of-the-art performance in the temporal link prediction task.

</details>


### [341] [How well do generative models solve inverse problems? A benchmark study](https://arxiv.org/abs/2601.23238)
*Patrick Krüger,Patrick Materne,Werner Krebs,Hanno Gottschalk*

Main category: cs.LG

TL;DR: 比较传统贝叶斯逆问题方法与三种生成式学习模型在燃气轮机燃烧室设计中的应用，条件流匹配方法表现最佳


<details>
  <summary>Details</summary>
Motivation: 生成式学习能够基于低维条件生成高维数据，适合解决贝叶斯逆问题。本文旨在比较传统方法与现代生成式模型在燃气轮机燃烧室设计中的性能

Method: 比较了四种方法：1) 基于前向回归模型和MCMC采样的传统贝叶斯方法；2) 条件生成对抗网络；3) 可逆神经网络；4) 条件流匹配。应用于燃气轮机燃烧室设计，将6个设计参数映射到3个性能指标

Result: 提出了多个评估指标来衡量生成设计的准确性和多样性，并研究了训练数据集大小对性能的影响。条件流匹配方法在所有竞争方法中表现最佳

Conclusion: 在燃气轮机燃烧室设计的逆问题中，条件流匹配方法明显优于传统贝叶斯方法、条件生成对抗网络和可逆神经网络，是解决此类逆设计问题的有效方法

Abstract: Generative learning generates high dimensional data based on low dimensional conditions, also called prompts. Therefore, generative learning algorithms are eligible for solving (Bayesian) inverse problems. In this article we compare a traditional Bayesian inverse approach based on a forward regression model and a prior sampled with the Markov Chain Monte Carlo method with three state of the art generative learning models, namely conditional Generative Adversarial Networks, Invertible Neural Networks and Conditional Flow Matching. We apply them to a problem of gas turbine combustor design where we map six independent design parameters to three performance labels. We propose several metrics for the evaluation of this inverse design approaches and measure the accuracy of the labels of the generated designs along with the diversity. We also study the performance as a function of the training dataset size. Our benchmark has a clear winner, as Conditional Flow Matching consistently outperforms all competing approaches.

</details>


### [342] [Particle-Guided Diffusion Models for Partial Differential Equations](https://arxiv.org/abs/2601.23262)
*Andrew Millard,Fredrik Lindsten,Zheng Zhao*

Main category: cs.LG

TL;DR: 提出一种结合扩散模型与物理约束的引导随机采样方法，通过PDE残差和观测约束确保生成样本的物理可行性，并嵌入SMC框架构建可扩展的生成式PDE求解器。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法在求解偏微分方程时可能产生物理不可行的解，需要确保生成样本满足物理约束条件，同时保持计算的可扩展性。

Method: 提出引导随机采样方法，将基于PDE残差的物理引导和观测约束融入扩散模型采样过程，并将该采样过程嵌入新的顺序蒙特卡洛框架中。

Result: 在多个基准PDE系统、多物理场和相互作用PDE系统中，该方法生成的解场比现有最先进生成方法具有更低的数值误差。

Conclusion: 该方法成功将物理约束融入生成模型采样过程，构建了可扩展的生成式PDE求解器，在保持物理可行性的同时提高了求解精度。

Abstract: We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differential equation (PDE) residuals and observational constraints, ensuring generated samples remain physically admissible. We embed this sampling procedure within a new Sequential Monte Carlo (SMC) framework, yielding a scalable generative PDE solver. Across multiple benchmark PDE systems as well as multiphysics and interacting PDE systems, our method produces solution fields with lower numerical error than existing state-of-the-art generative methods.

</details>


### [343] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出DDIS框架，通过解耦设计实现数据高效的PDE逆问题求解，在稀疏观测下性能显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的PDE逆问题求解方法需要大量配对数据，且物理约束隐含在联合建模中，导致数据效率低且可能出现过平滑问题

Method: 提出解耦扩散逆求解器(DDIS)：1) 无条件扩散模型学习系数先验；2) 神经算子显式建模前向PDE提供指导；3) 引入解耦退火后验采样(DAPS)避免过平滑

Result: 在稀疏观测下达到SOTA性能：平均l2误差降低11%，谱误差降低54%；当数据仅有1%时，DDIS相比联合模型在l2误差上保持40%优势

Conclusion: DDIS通过解耦设计实现了数据高效且物理感知的PDE逆问题求解，理论证明避免了训练数据稀缺时的指导衰减问题，在稀疏观测场景下具有显著优势

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [344] [Model Selection in Panel Data Models: A Generalization of the Vuong Test](https://arxiv.org/abs/2601.22354)
*Jinyong Hahn,Zhipeng Liao,Konrad Menzel,Quang Vuong*

Main category: econ.EM

TL;DR: 将Vuong检验推广到面板数据模型，使用修正轮廓似然和KL信息准则，适用于包含组固定效应的广义面板框架


<details>
  <summary>Details</summary>
Motivation: 经典Vuong检验适用于横截面数据，但面板数据中轮廓似然缺乏正则性质，需要修正方法；传统个体固定效应模型可能不够灵活

Method: 采用修正轮廓似然和Kullback-Leibler信息准则，构建广义面板数据框架，包含时间和个体对的组固定效应而非传统个体固定效应

Result: 开发了适用于面板数据的广义Vuong检验方法，能够处理线性模型中个体-时间效应的非嵌套设定比较问题

Conclusion: 成功将Vuong检验扩展到面板数据环境，为比较非嵌套面板模型提供了有效工具，特别适用于包含组固定效应的设定

Abstract: This paper generalizes the classical Vuong (1989) test to panel data models by employing modified profile likelihoods and the Kullback-Leibler information criterion. Unlike the standard likelihood function, the profile likelihood lacks certain regular properties, making modification necessary. We adopt a generalized panel data framework that incorporates group fixed effects for time and individual pairs, rather than traditional individual fixed effects. Applications of our approach include linear models with non-nested specifications of individual-time effects.

</details>


### [345] [Using SVM to Estimate and Predict Binary Choice Models](https://arxiv.org/abs/2601.22659)
*Yoosoon Chang,Joon Y. Park,Guo Yan*

Main category: econ.EM

TL;DR: SVM与二元选择模型QMLE具有渐近等价性，在满足线性条件均值条件下，SVM超平面斜率能一致估计BCM斜率参数，尤其适用于类别不平衡数据。


<details>
  <summary>Details</summary>
Motivation: 研究支持向量机(SVM)在二元选择模型(BCM)参数估计中的理论性质，探索SVM与准最大似然估计(QMLE)的渐近关系，特别是在类别不平衡情况下SVM的统计性质。

Method: 在QMLE斜率一致性文献中使用的线性条件均值条件下，分析SVM分离超平面斜率的渐近性质，证明其能一致估计BCM斜率参数，并考虑类别权重处理类别不平衡问题。

Result: SVM斜率估计量与逻辑回归估计量在渐近意义上等价，都能一致估计BCM斜率参数。有限样本性能取决于协变量和误差的分布，两者各有优劣，没有绝对优势。

Conclusion: SVM虽然不是QMLE，但在适当条件下具有与QMLE相似的渐近性质，能够一致估计BCM参数，为SVM在二元分类问题中的统计推断提供了理论基础。

Abstract: The support vector machine (SVM) has an asymptotic behavior that parallels that of the quasi-maximum likelihood estimator (QMLE) for binary outcomes generated by a binary choice model (BCM), although it is not a QMLE. We show that, under the linear conditional mean condition for covariates given the systematic component used in the QMLE slope consistency literature, the slope of the separating hyperplane given by the SVM consistently estimates the BCM slope parameter, as long as the class weight is used as required when binary outcomes are severely imbalanced. The SVM slope estimator is asymptotically equivalent to that of logistic regression in this sense. The finite-sample performance of the two estimators can be quite distinct depending on the distributions of covariates and errors, but neither dominates the other. The intercept parameter of the BCM can be consistently estimated once a consistent estimator of its slope parameter is obtained.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [346] [A Real-Options-Aware Multi-Criteria Framework for Ex-Ante Real Estate Redevelopment Use Selection](https://arxiv.org/abs/2601.22166)
*Roberto Garrone*

Main category: q-fin.GN

TL;DR: 提出一个结合实物期权和多准则决策分析的房地产再开发战略用途选择框架，解决资产结构性错配问题


<details>
  <summary>Details</summary>
Motivation: 许多房地产资产存在结构性错配问题，即使技术可恢复且位于非边缘区域，也无法产生与投入资本相匹配的经济价值。这反映了预期用途与有效需求之间的结构性错位，而非周期性市场疲软，需要能够整合价值、风险、复杂性和不可逆性的决策框架。

Method: 提出一个决策分析框架，将实物期权逻辑（关于不可逆性和管理灵活性）与多准则决策分析结构相结合。该框架能够比较评估预期经济价值、市场和运营风险、技术和管理复杂性以及收入实现时间。

Result: 通过案例研究表明，实物期权推理与MCDA的结合能够减少过度复杂化和错配问题，适用于不同类型的资产和城市环境。

Conclusion: 该框架将再开发视为战略期权选择问题而非设计或财务优化问题，通过事前筛选实现期权价值保护，为解决房地产结构性错配提供了有效工具。

Abstract: A growing share of the existing real estate stock exhibits persistent underperformance that can no longer be explained by cyclical market phases or inadequate maintenance alone. In many cases, technically recoverable assets located in non-marginal contexts fail to generate economic value consistent with the capital immobilized. This condition reflects a structural misalignment between intended use and effective demand rather than episodic market weakness, and calls for a decision framework capable of integrating value, risk, complexity, and irreversibility in strategic use selection. This study proposes a decision-analytic framework for the ex-ante selection of intended use in real estate redevelopment processes. The framework integrates real-options logic on irreversibility and managerial flexibility with a multi-criteria decision-analysis structure, enabling comparative evaluation of expected economic value, market and operational risk, technical and managerial complexity, and time-to-income. By treating redevelopment primarily as a problem of strategic option selection rather than design or financial optimization, the framework operationalizes option value preservation through disciplined ex-ante screening. Illustrative cases demonstrate how this integration of real options reasoning and MCDA reduces over-complexification and misalignment across different asset types and urban contexts.

</details>


### [347] [The Widening Profitability Gap between Renewable and Fossil Power Firms in Europe](https://arxiv.org/abs/2601.22167)
*Robin Fischer,Anton Pichler*

Main category: q-fin.GN

TL;DR: 欧洲电力公司分析显示：可再生能源（风能/太阳能）投资组合盈利能力持续上升，而化石燃料投资组合盈利能力持续下降，化石能源公司创纪录利润只是危机驱动的异常现象。


<details>
  <summary>Details</summary>
Motivation: 能源转型需要动员私人资本，但近期危机驱动的化石能源公司暴利表明市场信号可能仍偏向碳密集型资产。研究旨在厘清这些利润反映的是持久的盈利能力优势还是危机驱动的异常现象。

Method: 分析2001-2023年900家欧洲电力公司的面板数据，使用机器学习聚类和贝叶斯模型平均方法，识别结构性的盈利能力分化。

Result: 风能和太阳能投资组合盈利能力上升，风能主导公司的资产回报率在2014-2023年间增长超过6%。相反，化石燃料投资组合份额越高，盈利能力越低，边际效应到2023年达到-4%。可再生能源主导公司在大多数欧洲地区表现优于化石燃料为主的公司。

Conclusion: 化石能源公司的创纪录利润是异常现象，掩盖了碳密集型商业模式盈利能力持续下降的趋势。可再生能源投资已显示出更强的盈利能力优势。

Abstract: Mobilising private capital is a critical bottleneck of the energy transition, yet recent crisis-driven windfall profits for fossil power firms suggest that market signals may still favour carbon-intensive assets. Here we analyse a panel of 900 European power firms (2001-2023) to resolve whether these profits reflect a durable profitability advantage or a crisis-driven anomaly. Using machine-learning clustering and Bayesian model averaging, we identify a structural divergence: wind and solar portfolios exhibit rising profitability, with return on assets among wind-dominated firms increasing by over 6% between 2014 and 2023. Conversely, higher fossil portfolio shares are increasingly associated with lower profitability, with marginal effects reaching -4% by 2023, while renewable-dominated firms match or outperform their fossil-heavy counterparts across most European regions. These findings suggest that the record profits of fossil incumbents were distinct outliers, masking an ongoing decline in the profitability of carbon-intensive business models.

</details>


### [348] [UniFinEval: Towards Unified Evaluation of Financial Multimodal Models across Text, Images and Videos](https://arxiv.org/abs/2601.22162)
*Zhi Yang,Lingfeng Zeng,Fangqi Lou,Qi Qi,Wei Zhang,Zhenyu Wu,Zhenxiong Yu,Jun Han,Zhiheng Jin,Lejie Zhang,Xiaoming Huang,Xiaolong Liang,Zheng Wei,Junbo Zou,Dongpo Cheng,Zhaowei Liu,Xin Guo,Rongjunchen Zhang,Liwen Zhang*

Main category: q-fin.GN

TL;DR: UniFinEval是首个针对高信息密度金融环境的多模态基准测试，涵盖文本、图像和视频，包含5个核心金融场景的3,767个中英文问答对，评估了10个主流MLLMs，发现Gemini-3-pro-preview表现最佳但仍与金融专家有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准测试无法评估MLLMs在金融领域面临的挑战，如多模态高密度信息和跨模态多跳推理，需要专门针对金融环境的评估基准。

Method: 构建UniFinEval基准，包含5个基于真实金融系统的核心场景：财务报表审计、公司基本面推理、行业趋势洞察、金融风险感知和资产配置分析。手动创建3,767个高质量中英文问答对，在Zero-Shot和CoT设置下系统评估10个主流MLLMs。

Result: Gemini-3-pro-preview在整体表现最佳，但仍与金融专家存在显著差距。错误分析揭示了当前模型的系统性缺陷。

Conclusion: UniFinEval为高信息密度金融环境提供了系统评估框架，有助于提升MLLMs在真实金融场景中的鲁棒性应用。

Abstract: Multimodal large language models are playing an increasingly significant role in empowering the financial domain, however, the challenges they face, such as multimodal and high-density information and cross-modal multi-hop reasoning, go beyond the evaluation scope of existing multimodal benchmarks. To address this gap, we propose UniFinEval, the first unified multimodal benchmark designed for high-information-density financial environments, covering text, images, and videos. UniFinEval systematically constructs five core financial scenarios grounded in real-world financial systems: Financial Statement Auditing, Company Fundamental Reasoning, Industry Trend Insights, Financial Risk Sensing, and Asset Allocation Analysis. We manually construct a high-quality dataset consisting of 3,767 question-answer pairs in both chinese and english and systematically evaluate 10 mainstream MLLMs under Zero-Shot and CoT settings. Results show that Gemini-3-pro-preview achieves the best overall performance, yet still exhibits a substantial gap compared to financial experts. Further error analysis reveals systematic deficiencies in current models. UniFinEval aims to provide a systematic assessment of MLLMs' capabilities in fine-grained, high-information-density financial environments, thereby enhancing the robustness of MLLMs applications in real-world financial scenarios. Data and code are available at https://github.com/aifinlab/UniFinEval.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [349] [AI Narrative Breakdown. A Critical Assessment of Power and Promise](https://arxiv.org/abs/2601.22255)
*Rainer Rehak*

Main category: cs.CY

TL;DR: 本文批判性分析ChatGPT发布后AI话语中的主流叙事，揭示这些叙事如何掩盖AI应用中的政治性、权力关系和价值判断，并提出应将AI视为需要社会治理的人类导向工具。


<details>
  <summary>Details</summary>
Motivation: ChatGPT发布后，关于人工智能的社会讨论日益增多，但许多主流叙事存在误导性，掩盖了AI应用中的政治性、权力关系和价值判断。作者旨在批判性地分析这些叙事，揭示其背后的意识形态和权力结构。

Method: 1. 首先对AI话语进行历史和技术的语境化分析；2. 引入"时代精神AI"概念，批判"AI"术语在各社会领域中的不精确和误导性应用；3. 基于批判计算机科学、STS、数据保护理论、心灵哲学和符号学等多学科视角，对常见叙事进行细致分析；4. 通过具体例子详细揭示所有AI应用中固有的政治性、权力注入和价值负载决策。

Result: 分析揭示了AI叙事中经常被忽视的关键问题：AI并非中立客观，而是充满政治性和价值判断；AI应用涉及权力关系和治理决策；主流叙事往往掩盖了这些本质特征。文章挑战了关于AI社会政治影响的常见假设。

Conclusion: 呼吁对AI采取更接地气的参与方式，提出新的叙事框架：将AI视为人类导向的工具，必须接受社会治理。文章指出了被现有叙事忽视的尖锐问题，主张重新认识AI的社会政治本质。

Abstract: This article sets off for an exploration of the still evolving discourse surrounding artificial intelligence (AI) in the wake of the release of ChatGPT. It scrutinizes the pervasive narratives that are shaping the societal engagement with AI, spotlighting key themes such as agency and decision-making, autonomy, truthfulness, knowledge processing, prediction, general purpose, neutrality and objectivity, apolitical optimization, sustainability game-changer, democratization, mass unemployment, and the dualistic portrayal of AI as either a harbinger of societal utopia or dystopia. Those narratives are analysed critically based on insights from critical computer science, critical data and algorithm studies, from STS, data protection theory, as well as from the philosophy of mind and semiotics. To properly analyse the narratives presented, the article first delves into a historical and technical contextualisation of the AI discourse itself. The article then introduces the notion of "Zeitgeist AI" to critique the imprecise and misleading application of the term "AI" across various societal sectors. Then, by discussing common narratives with nuance, the article contextualises and challenges often assumed socio-political implications of AI, uncovering in detail and with examples the inherent political, power infused and value-laden decisions within all AI applications. Concluding with a call for a more grounded engagement with AI, the article carves out acute problems ignored by the narratives discussed and proposes new narratives recognizing AI as a human-directed tool necessarily subject to societal governance.

</details>


### [350] [Toward Third-Party Assurance of AI Systems: Design Requirements, Prototype, and Early Testing](https://arxiv.org/abs/2601.22424)
*Rachel M. Kim,Blaine Kuehnert,Alice Lai,Kenneth Holstein,Hoda Heidari,Rayid Ghani*

Main category: cs.CY

TL;DR: 提出第三方AI保证框架，包含责任矩阵、访谈协议、成熟度矩阵和报告模板，通过两个实际用例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估资源存在局限：很少同时关注AI系统的开发部署过程和产出结果，缺乏端到端可操作指导，且缺少实际可用性和有效性证据。需要建立系统、透明、可操作的第三方AI评估框架。

Method: 1) 区分保证与审计的关键维度；2) 基于设计原则反思现有资源不足，确定AI保证的设计要求；3) 构建包含责任分配矩阵、访谈协议、成熟度矩阵和保证报告模板的原型；4) 在两个不同用例（企业文档标记工具和公共机构住房资源分配工具）中应用框架，并进行专家验证访谈。

Result: 早期验证表明该AI保证框架：1) 健全且全面；2) 可在不同组织环境中使用；3) 能有效识别AI系统的特定问题。

Conclusion: 提出的第三方AI保证框架填补了现有评估资源的空白，通过系统化的流程设计和实际验证，为AI系统的可信评估提供了可行的解决方案。

Abstract: As Artificial Intelligence (AI) systems proliferate, the need for systematic, transparent, and actionable processes for evaluating them is growing. While many resources exist to support AI evaluation, they have several limitations. Few address both the process of designing, developing, and deploying an AI system and the outcomes it produces. Furthermore, few are end-to-end and operational, give actionable guidance, or present evidence of usability or effectiveness in practice. In this paper, we introduce a third-party AI assurance framework that addresses these gaps. We focus on third-party assurance to prevent conflict of interest and ensure credibility and accountability of the process. We begin by distinguishing assurance from audits in several key dimensions. Then, following design principles, we reflect on the shortcomings of existing resources to identify a set of design requirements for AI assurance. We then construct a prototype of an assurance process that consists of (1) a responsibility assignment matrix to determine the different levels of involvement each stakeholder has at each stage of the AI lifecycle, (2) an interview protocol for each stakeholder of an AI system, (3) a maturity matrix to assess AI systems' adherence to best practices, and (4) a template for an assurance report that draws from more mature assurance practices in business accounting. We conduct early validation of our AI assurance framework by applying the framework to two distinct AI use cases -- a business document tagging tool for downstream processing in a large private firm, and a housing resource allocation tool in a public agency -- and conducting expert validation interviews. Our findings show early evidence that our AI assurance framework is sound and comprehensive, usable across different organizational contexts, and effective at identifying bespoke issues with AI systems.

</details>


### [351] [The Third-Party Access Effect: An Overlooked Challenge in Secondary Use of Educational Real-World Data](https://arxiv.org/abs/2601.22472)
*Hibiki Ito,Chia-Yu Hsu,Hiroaki Ogata*

Main category: cs.CY

TL;DR: 该研究揭示了教育领域真实世界数据二次使用时，隐私保护实践可能通过第三方访问效应影响下游分析的有效性。


<details>
  <summary>Details</summary>
Motivation: 教育领域真实世界数据二次使用日益增长，但隐私实践对下游分析的影响很少被评估，可能导致潜在问题未被发现。

Method: 通过评估(1)细粒度数据的再识别风险，(2)风险沟通如何影响学习者隐私行为，(3)行为变化对下游分析结论的敏感性，来研究常见隐私实践的问题。

Result: 发现显著的再识别风险沟通会导致学习者选择退出和不自我披露，这些行为变化会显著改变共享数据，从而限制二次使用发现的有效性。

Conclusion: 提出了"第三方访问效应"概念，并讨论了其对教育真实世界数据可信二次使用的影响。

Abstract: Secondary use of growing real-world data (RWD) in education offers significant opportunities for research, yet privacy practices intended to enable third-party access to such RWD are rarely evaluated for their implications for downstream analyses. As a result, potential problems introduced by otherwise standard privacy practices may remain unnoticed. To address this gap, we investigate potential issues arising from common practices by assessing (1) the re-identification risk of fine-grained RWD, (2) how communicating such risks influences learners' privacy behaviour, and (3) the sensitivity of downstream analytical conclusions to resulting changes in the data. We focus on these practices because re-identification risk and stakeholder communication can jointly influence the data shared with third parties. We find that substantial re-identification risk in RWD, when communicated to stakeholders, can induce opt-outs and non-self-disclosure behaviours. Sensitivity analysis demonstrates that these behavioural changes can meaningfully alter the shared data, limiting validity of secondary-use findings. We conceptualise this phenomenon as the third-party access effect (3PAE) and discuss implications for trustworthy secondary use of educational RWD.

</details>


### [352] [AI Literacy, Safety Awareness, and STEM Career Aspirations of Australian Secondary Students: Evaluating the Impact of Workshop Interventions](https://arxiv.org/abs/2601.22486)
*Christian Bergh,Alexandra Vassar,Natasha Banks,Jessica Xu,Jake Renzella*

Main category: cs.CY

TL;DR: 澳大利亚AI日活动通过工作坊干预提升中学生AI素养，研究发现学生接触深度伪造等合成媒体风险较高，干预后AI知识、识别能力和STEM兴趣有所提升，但需要持续教育来巩固长期效果。


<details>
  <summary>Details</summary>
Motivation: 深度伪造等合成媒体对青少年构成日益增长的安全风险，但关于学生接触情况和相关行为的证据仍然有限。本研究旨在评估AI素养教育干预措施的效果，以改善澳大利亚中学生的AI理解和概念认知。

Method: 采用混合方法，对澳大利亚7-10年级中学生进行工作坊干预，使用前后测调查（前测N=205，后测N=163），分析学生在识别日常工具中的AI、理解AI伦理、训练和安全知识以及STEM职业兴趣方面的变化。

Result: 基线数据显示显著的合成媒体风险：82.4%学生见过深度伪造，18.5%分享过，7.3%创建过。干预后学生自我报告的AI知识和信心提高，对Netflix、Spotify、TikTok等平台中AI的识别能力增强，从"基于算法"转向"AI驱动系统"的认知转变。STEM职业兴趣小幅提升，但效应量较小，表明一次性工作坊可能不足以影响长期志向。

Conclusion: 研究支持可扩展的AI素养项目，将基础AI概念与合成媒体安全明确结合。虽然工作坊干预在短期内有效，但需要超越一次性活动的持续方法来实现长期影响，特别是在STEM职业志向方面。

Abstract: Deepfakes and other forms of synthetic media pose growing safety risks for adolescents, yet evidence on students' exposure and related behaviours remains limited. This study evaluates the impact of Day of AI Australia's workshop-based intervention designed to improve AI literacy and conceptual understanding among Australian secondary students (Years 7-10). Using a mixed-methods approach with pre- and post-intervention surveys (N=205 pre; N=163 post), we analyse changes in students' ability to identify AI in everyday tools, their understanding of AI ethics, training, and safety, and their interest in STEM-related careers.
  Baseline data revealed notable synthetic media risks: 82.4% of students reported having seen deepfakes, 18.5% reported sharing them, and 7.3% reported creating them.
  Results show higher self-reported AI knowledge and confidence after the intervention, alongside improved recognition of AI in widely used platforms such as Netflix, Spotify, and TikTok. This pattern suggests a shift from seeing these tools as merely "algorithm-based" to recognising them as AI-driven systems. Students also reported increased interest in STEM careers post-workshop; however, effect sizes were small, indicating that sustained approaches beyond one-off workshops may be needed to influence longer-term aspirations. Overall, the findings support scalable AI literacy programs that pair foundational AI concepts with an explicit emphasis on synthetic media safety.

</details>


### [353] [Ethical Risks of Large Language Models in Medical Consultation: An Assessment Based on Reproductive Ethics](https://arxiv.org/abs/2601.22621)
*Hanhui Xu,Jiacheng Ji,Haoan Jin,Han Ying,Mengyue Wu*

Main category: cs.CY

TL;DR: 该研究评估了8个主流大语言模型在中国生殖伦理问题上的表现，发现存在严重安全隐患（风险率29.91%），特别是在规范性引用和同理心表达方面普遍薄弱，模型存在逻辑自相矛盾和违反基本道德直觉的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗和医学咨询中的应用增加，需要评估这些模型是否能符合当地伦理标准进行回应。本研究针对中国生殖伦理规范，系统评估LLM的可靠性和安全性。

Method: 评估了8个主流LLM（如GPT-4、Claude-3.7），使用基于168篇中国生殖伦理法规文章构建的986个问题测试集（906个主观题，80个客观题）。主观回答采用六维评分标准评估安全性（规范性合规、指导安全性）和回答质量（问题识别、引用、建议、同理心）。

Result: 发现严重安全隐患，不安全或误导性建议的风险率达到29.91%。所有模型在引用规范性来源和表达同理心方面表现普遍较差。还发现了异常道德推理实例，包括逻辑自相矛盾和违反基本道德直觉的回应。

Conclusion: 当前LLM在自主生殖伦理咨询中不可靠且不安全。尽管具备知识回忆能力，但在安全性、逻辑一致性和基本人文技能方面存在严重缺陷。这些发现为过早部署提供了重要警示，敦促未来开发应优先考虑稳健推理、法规依据和同理心。

Abstract: Background: As large language models (LLMs) are increasingly used in healthcare and medical consultation settings, a growing concern is whether these models can respond to medical inquiries in a manner that is ethically compliant--particularly in accordance with local ethical standards. To address the pressing need for comprehensive research on reliability and safety, this study systematically evaluates LLM performance in answering questions related to reproductive ethics, specifically assessing their alignment with Chinese ethical regulations.
  Methods: We evaluated eight prominent LLMs (e.g., GPT-4, Claude-3.7) on a custom test set of 986 questions (906 subjective, 80 objective) derived from 168 articles within Chinese reproductive ethics regulations. Subjective responses were evaluated using a novel six-dimensional scoring rubric assessing Safety (Normative Compliance, Guidance Safety) and Quality of the Answer (Problem Identification, Citation, Suggestion, Empathy).
  Results: Significant safety issues were prevalent, with risk rates for unsafe or misleading advice reaching 29.91%. A systemic weakness was observed across all models: universally poor performance in citing normative sources and expressing empathy. We also identified instances of anomalous moral reasoning, including logical self-contradictions and responses violating fundamental moral intuitions.
  Conclusions: Current LLMs are unreliable and unsafe for autonomous reproductive ethics counseling. Despite knowledge recall, they exhibit critical deficiencies in safety, logical consistency, and essential humanistic skills. These findings serve as a critical cautionary note against premature deployment, urging future development to prioritize robust reasoning, regulatory justification, and empathy.

</details>


### [354] [Beyond Abstract Compliance: Operationalising trust in AI as a moral relationship](https://arxiv.org/abs/2601.22769)
*Lameck Mbangula Amugongo,Tutaleni Asino,Nicola J Bidwell*

Main category: cs.CY

TL;DR: 论文提出基于非洲社群主义哲学的关系伦理框架，将AI信任视为动态、关系性的过程，而非可设计的静态属性，强调社区参与和长期关系建设。


<details>
  <summary>Details</summary>
Motivation: 现有AI信任框架（如欧盟"可信AI框架"）将信任视为可按规范和标准设计、评估和治理的属性，忽视了信任的主观性、文化嵌入性和关系本质。需要更全面的信任概念来促进公平、情境敏感的AI系统。

Method: 借鉴关系伦理和非洲社群主义哲学，提出扩展的信任原则，将信任视为动态、时间性的关系，强调透明度、相互尊重、包容性参与过程和与社区的长期关系。通过医疗和教育两个AI用例展示如何操作化这些原则。

Result: 提出了基于非洲关系伦理的信任赋能原则，强调在整个AI生命周期中持续社区参与，能够培养AI设计与开发团队之间的有意义关系，逐步建立信任，促进更公平和情境敏感的AI系统。

Conclusion: AI信任应被理解为动态、关系性的过程，而非静态属性。通过融入关系伦理和非洲社群主义哲学，可以建立更包容、参与式的信任框架，让社区在整个AI生命周期中发挥作用，最终实现更公平、情境敏感的AI系统。

Abstract: Dominant approaches, e.g. the EU's "Trustworthy AI framework", treat trust as a property that can be designed for, evaluated, and governed according to normative and technical criteria. They do not address how trust is subjectively cultivated and experienced, culturally embedded, and inherently relational. This paper proposes some expanded principles for trust in AI that can be incorporated into common development methods and frame trust as a dynamic, temporal relationship, which involves transparency and mutual respect. We draw on relational ethics and, in particular, African communitarian philosophies, to foreground the nuances of inclusive, participatory processes and long-term relationships with communities. Involving communities throughout the AI lifecycle can foster meaningful relationships with AI design and development teams that incrementally build trust and promote more equitable and context-sensitive AI systems. We illustrate how trust-enabling principles based on African relational ethics can be operationalised, using two use-cases for AI: healthcare and education.

</details>


### [355] [Eroding the Truth-Default: A Causal Analysis of Human Susceptibility to Foundation Model Hallucinations and Disinformation in the Wild](https://arxiv.org/abs/2601.22871)
*Alexander Loth,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CY

TL;DR: JudgeGPT和RogueGPT双轴框架将"真实性"与"归因"解耦，研究人类对AI生成内容的易感性机制。研究发现政治倾向对检测性能影响微乎其微，而"假新闻熟悉度"是关键中介变量，GPT-4输出存在"流畅性陷阱"绕过源监控机制。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型接近人类水平的流畅度，区分合成内容与有机内容已成为可信网络智能的关键挑战。需要研究人类对AI生成内容的易感性机制，以建立可信的信息生态系统。

Method: 提出JudgeGPT和RogueGPT双轴框架，将"真实性"与"归因"解耦。使用结构因果模型作为主要框架制定可测试的因果假设。分析了5个基础模型（包括GPT-4和Llama-2）的918个评估。

Result: 政治倾向与检测性能关联微弱（r=-0.10），"假新闻熟悉度"是候选中介变量（r=0.35），表明暴露可能作为人类判别者的对抗训练。GPT-4输出存在"流畅性陷阱"（HumanMachineScore: 0.20），能绕过源监控机制。

Conclusion: "预防性干预"应针对认知源监控而非人口统计细分，以确保可信的信息生态系统。暴露于假新闻可能训练人类更好地区分AI生成内容。

Abstract: As foundation models (FMs) approach human-level fluency, distinguishing synthetic from organic content has become a key challenge for Trustworthy Web Intelligence.
  This paper presents JudgeGPT and RogueGPT, a dual-axis framework that decouples "authenticity" from "attribution" to investigate the mechanisms of human susceptibility. Analyzing 918 evaluations across five FMs (including GPT-4 and Llama-2), we employ Structural Causal Models (SCMs) as a principal framework for formulating testable causal hypotheses about detection accuracy.
  Contrary to partisan narratives, we find that political orientation shows a negligible association with detection performance ($r=-0.10$). Instead, "fake news familiarity" emerges as a candidate mediator ($r=0.35$), suggesting that exposure may function as adversarial training for human discriminators. We identify a "fluency trap" where GPT-4 outputs (HumanMachineScore: 0.20) bypass Source Monitoring mechanisms, rendering them indistinguishable from human text.
  These findings suggest that "pre-bunking" interventions should target cognitive source monitoring rather than demographic segmentation to ensure trustworthy information ecosystems.

</details>


### [356] [When Machines Get It Wrong: Large Language Models Perpetuate Autism Myths More Than Humans Do](https://arxiv.org/abs/2601.22893)
*Eduardo C. Garrido-Merchán,Adriana Constanza Cirera Tirschtigel*

Main category: cs.CY

TL;DR: 研究发现，尽管大型语言模型（LLM）被广泛用作健康信息来源，但在自闭症谱系障碍知识方面，人类参与者比GPT-4、Claude和Gemini等先进AI系统更准确，人类的神话认可率（36.2%）显著低于LLM（44.8%）。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型成为普遍的健康信息来源，了解它们准确表征污名化病症的能力对于负责任部署至关重要。本研究旨在检验领先的AI系统是延续还是挑战关于自闭症谱系障碍的误解，这是一种特别容易受到有害神话影响的病症。

Method: 研究使用包含30个项目的自闭症知识测量工具，对178名人类参与者和三个最先进的LLM（GPT-4、Claude和Gemini）进行了测试，比较了他们对自闭症神话的认可程度。

Result: 与预期相反，人类参与者比LLM认可了更少的神话（错误率36.2% vs. 44.8%；z = -2.59，p = .0048）。在30个评估项目中，人类在18个项目上显著优于AI系统。

Conclusion: 这些发现揭示了当前AI系统的关键盲点，对人类-AI交互设计、机器知识认识论以及AI开发中需要以神经多样性视角为中心具有重要意义。

Abstract: As Large Language Models become ubiquitous sources of health information, understanding their capacity to accurately represent stigmatized conditions is crucial for responsible deployment. This study examines whether leading AI systems perpetuate or challenge misconceptions about Autism Spectrum Disorder, a condition particularly vulnerable to harmful myths. We administered a 30-item instrument measuring autism knowledge to 178 participants and three state-of-the-art LLMs including GPT-4, Claude, and Gemini. Contrary to expectations that AI systems would leverage their vast training data to outperform humans, we found the opposite pattern: human participants endorsed significantly fewer myths than LLMs (36.2% vs. 44.8% error rate; z = -2.59, p = .0048). In 18 of the 30 evaluated items, humans significantly outperformed AI systems. These findings reveal a critical blind spot in current AI systems and have important implications for human-AI interaction design, the epistemology of machine knowledge, and the need to center neurodivergent perspectives in AI development.

</details>


### [357] [Evaluating the Effectiveness of OpenAI's Parental Control System](https://arxiv.org/abs/2601.23062)
*Kerem Ersoz,Saleh Afroogh,David Atkinson,Junfeng Jiao*

Main category: cs.CY

TL;DR: 评估主流对话助手平台级家长控制对未成年人使用的有效性，发现通知机制不全面，存在政策与产品之间的差距，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究平台级家长控制在主流对话助手中对未成年用户的实际效果，了解当前保护措施的有效性和局限性。

Method: 采用两阶段协议：1) 通过PAIR式迭代提示优化构建类别平衡的对话语料库；2) 训练有素的人类代理在消费者UI中使用指定儿童账户重放/优化提示，同时监控关联的家长收件箱获取警报。聚焦七个风险领域，量化四个结果指标。

Result: 通知具有选择性而非全面性：隐私暴力、欺诈、仇恨言论和恶意软件未触发家长警报，而物理伤害（最高）、色情内容和部分健康查询产生间歇性警报。当前后端比旧模型泄漏率低，但对敏感话题附近的良性教育查询过度拦截仍然常见且未向家长展示。

Conclusion: 当前系统存在政策与产品之间的差距，屏幕上的安全措施与面向家长的遥测数据不匹配。建议：扩展/配置通知分类法，将可见安全措施与保护隐私的家长摘要相结合，优先采用校准的、适合年龄的安全重写而非全面拒绝。

Abstract: We evaluate how effectively platform-level parental controls moderate a mainstream conversational assistant used by minors. Our two-phase protocol first builds a category-balanced conversation corpus via PAIR-style iterative prompt refinement over API, then has trained human agents replay/refine those prompts in the consumer UI using a designated child account while monitoring the linked parent inbox for alerts. We focus on seven risk areas -- physical harm, pornography, privacy violence, health consultation, fraud, hate speech, and malware and quantify four outcomes: Notification Rate (NR), Leak-Through (LR), Overblocking (OBR), and UI Intervention Rate (UIR). Using an automated judge (with targeted human audit) and comparing the current backend to legacy variants (GPT-4.1/4o), we find that notifications are selective rather than comprehensive: privacy violence, fraud, hate speech, and malware triggered no parental alerts in our runs, whereas physical harm (highest), pornography, and some health queries produced intermittent alerts. The current backend shows lower leak-through than legacy models, yet overblocking of benign, educational queries near sensitive topics remains common and is not surfaced to parents, revealing a policy-product gap between on-screen safeguards and parent-facing telemetry. We propose actionable fixes: broaden/configure the notification taxonomy, couple visible safeguards to privacy-preserving parent summaries, and prefer calibrated, age-appropriate safe rewrites over blanket refusals.

</details>


### [358] [Gender Disparities in StackOverflow's Community-Based Question Answering: A Matter of Quantity versus Quality](https://arxiv.org/abs/2601.23063)
*Maddalena Amendola,Cosimo Rulli,Carlos Castillo,Andrea Passarella,Raffaele Perego*

Main category: cs.CY

TL;DR: 研究发现Stack Overflow平台上的性别声誉差异主要由用户活跃度差异导致，而非答案质量差异或性别偏见影响最佳答案选择


<details>
  <summary>Details</summary>
Motivation: 社区问答平台如Stack Overflow存在性别偏见问题，但先前研究忽略了性别互动、答案质量和最佳答案选择机制。本研究旨在探究答案质量是否受性别影响，以及性别偏见如何影响平台公平性

Method: 结合人工评估和基于大型语言模型的自动化评估，分析答案质量与性别的关系，并研究最佳答案选择机制中的性别偏见

Result: 未发现答案质量存在显著性别差异，性别偏见对最佳答案选择也无实质性影响。Stack Overflow声誉分数的性别差异主要源于用户活跃度差异（如提问和回答数量）

Conclusion: 社区问答平台的评分系统设计需要更公平的策略，过度强调活跃度的声誉系统可能放大不反映实际答案质量差异的性别差距

Abstract: Community Question-Answering platforms, such as Stack Overflow (SO), are valuable knowledge exchange and problem-solving resources. These platforms incorporate mechanisms to assess the quality of answers and participants' expertise, ideally free from discriminatory biases. However, prior research has highlighted persistent gender biases, raising concerns about the inclusivity and fairness of these systems. Addressing such biases is crucial for fostering equitable online communities. While previous studies focus on detecting gender bias by comparing male and female user characteristics, they often overlook the interaction between genders, inherent answer quality, and the selection of ``best answers'' by question askers. In this study, we investigate whether answer quality is influenced by gender using a combination of human evaluations and automated assessments powered by Large Language Models. Our findings reveal no significant gender differences in answer quality, nor any substantial influence of gender bias on the selection of ``best answers." Instead, we find that the significant gender disparities in SO's reputation scores are primarily attributable to differences in users' activity levels, e.g., the number of questions and answers they write. Our results have important implications for the design of scoring systems in community question-answering platforms. In particular, reputation systems that heavily emphasize activity volume risk amplifying gender disparities that do not reflect actual differences in answer quality, calling for more equitable design strategies.

</details>


### [359] [How should AI Safety Benchmarks Benchmark Safety?](https://arxiv.org/abs/2601.23112)
*Cheng Yu,Severin Engelmann,Ruoxuan Cao,Dalia Ali,Orestis Papakyriakopoulos*

Main category: cs.CY

TL;DR: 本文系统回顾了210个AI安全基准，指出了当前基准在技术、认知和社会技术层面的不足，并提出基于风险管理原则、概率指标和测量理论的改进方案。


<details>
  <summary>Details</summary>
Motivation: AI安全基准对高级AI系统的安全至关重要，但现有基准存在显著的技术、认知和社会技术缺陷，需要系统性改进以确保AI系统的负责任部署。

Method: 回顾分析了210个安全基准，借鉴工程科学和成熟的风险安全理论，识别基准的常见挑战和局限，提出基于风险管理原则、空间映射、概率指标和测量理论的改进框架。

Result: 研究揭示了当前AI安全基准的多重缺陷，提出了具体的改进建议，并通过定量和定性评估验证了这些建议的有效性，同时开发了帮助研究人员和实践者构建稳健基准的检查清单。

Conclusion: 通过遵循成熟的风险管理原则、开发稳健的概率指标、有效应用测量理论，可以显著提升AI安全基准的有效性和实用性，推动基准科学的进步，促进AI系统更负责任的部署。

Abstract: AI safety benchmarks are pivotal for safety in advanced AI systems; however, they have significant technical, epistemic, and sociotechnical shortcomings. We present a review of 210 safety benchmarks that maps out common challenges in safety benchmarking, documenting failures and limitations by drawing from engineering sciences and long-established theories of risk and safety. We argue that adhering to established risk management principles, mapping the space of what can(not) be measured, developing robust probabilistic metrics, and efficiently deploying measurement theory to connect benchmarking objectives with the world can significantly improve the validity and usefulness of AI safety benchmarks. The review provides a roadmap on how to improve AI safety benchmarking, and we illustrate the effectiveness of these recommendations through quantitative and qualitative evaluation. We also introduce a checklist that can help researchers and practitioners develop robust and epistemologically sound safety benchmarks. This study advances the science of benchmarking and helps practitioners deploy AI systems more responsibly.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [360] [Stablecoin Design with Adversarial-Robust Multi-Agent Systems via Trust-Weighted Signal Aggregation](https://arxiv.org/abs/2601.22168)
*Shengwei You,Aditya Joshi,Andrey Kuehlkamp,Jarek Nabrzyski*

Main category: q-fin.RM

TL;DR: MVF-Composer是一种结合信任加权均值-方差前沿和压力测试的算法稳定币储备控制器，通过多智能体模拟识别极端风险，相比现有方案显著提升稳定性和抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有算法稳定币储备控制器存在严重缺陷：它们基于正常市场数据优化参数，忽略尾部风险事件，导致在极端波动（如2020年3月"黑色星期四"）时发生系统性崩溃。MakerDAO等案例暴露了传统模型无法处理危机情景的问题。

Method: 提出MVF-Composer系统，包含两个核心组件：1）信任加权均值-方差前沿控制器，通过信任评分机制T: A -> [0,1]降低操纵性智能体信号权重；2）压力测试框架，使用多智能体模拟（交易者、流动性提供者、攻击者）在危机情景下测试储备漏洞。

Result: 在1200个随机情景测试中（包含10%抵押品下跌、50%情绪崩溃、协调赎回攻击等黑天鹅冲击），MVF-Composer相比SAS基线将峰值挂钩偏差降低57%，平均恢复时间缩短3.1倍。信任层在对抗条件下贡献23%的稳定性提升，检测到72%的对抗性智能体。

Conclusion: MVF-Composer提供了一种可复现的DeFi储备政策压力测试框架，无需额外链上预言机，能在商品硬件上运行，显著提升算法稳定币在极端市场条件下的韧性和抗攻击能力。

Abstract: Algorithmic stablecoins promise decentralized monetary stability by maintaining a target peg through programmatic reserve management. Yet, their reserve controllers remain vulnerable to regime-blind optimization, calibrating risk parameters on fair-weather data while ignoring tail events that precipitate cascading failures. The March 2020 Black Thursday collapse, wherein MakerDAO's collateral auctions yielded $8.3M in losses and a 15% peg deviation, exposed a critical gap: existing models like SAS systematically omit extreme volatility regimes from covariance estimates, producing allocations optimal in expectation but catastrophic under adversarial stress.
  We present MVF-Composer, a trust-weighted Mean-Variance Frontier reserve controller incorporating a novel Stress Harness for risk-state estimation. Our key insight is deploying multi-agent simulations as adversarial stress-testers: heterogeneous agents (traders, liquidity providers, attackers) execute protocol actions under crisis scenarios, exposing reserve vulnerabilities before they manifest on-chain. We formalize a trust-scoring mechanism T: A -> [0,1] that down-weights signals from agents exhibiting manipulative behavior, ensuring the risk-state estimator remains robust to signal injection and Sybil attacks.
  Across 1,200 randomized scenarios with injected Black-Swan shocks (10% collateral drawdown, 50% sentiment collapse, coordinated redemption attacks), MVF-Composer reduces peak peg deviation by 57% and mean recovery time by 3.1x relative to SAS baselines. Ablation studies confirm the trust layer accounts for 23% of stability gains under adversarial conditions, achieving 72% adversarial agent detection. Our system runs on commodity hardware, requires no on-chain oracles beyond standard price feeds, and provides a reproducible framework for stress-testing DeFi reserve policies.

</details>
