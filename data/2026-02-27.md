<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 48]
- [eess.SY](#eess.SY) [Total: 10]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.AI](#cs.AI) [Total: 71]
- [cs.LG](#cs.LG) [Total: 124]
- [math.OC](#math.OC) [Total: 18]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

TL;DR: 提出DSKD框架，将词典资源整合到解码器LLM训练中，无需推理时查字典，显著提升知识蒸馏性能


<details>
  <summary>Details</summary>
Motivation: LLM学习丰富的语义上下文嵌入，但常忽略结构化词汇知识（如词义和关系）。先前工作表明词典能改进编码器知识蒸馏，但应用到解码器生成模型仍具挑战

Method: 提出Decoder-based Sense Knowledge Distillation (DSKD)框架，在训练时将词典资源整合到解码器LLM中，推理时无需词典查询

Result: 在多样化基准测试上的广泛实验表明，DSKD显著提升解码器知识蒸馏性能，使生成模型继承结构化语义同时保持高效训练

Conclusion: DSKD成功将词典资源整合到解码器LLM训练中，解决了生成模型继承结构化词汇知识的挑战

Abstract: Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


### [2] [Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts](https://arxiv.org/abs/2602.22359)
*Arno Simons*

Main category: cs.CL

TL;DR: 测试大语言模型能否支持解释性引文语境分析，通过提示敏感性分析发现提示框架会系统性影响模型生成的解释方向和词汇选择


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否能够支持解释性引文语境分析，而不是仅仅进行类型学标签分类，同时研究提示敏感性作为方法论问题的重要性

Method: 采用2x3平衡设计，通过改变提示框架和脚手架，使用GPT-5进行两阶段分析：表面分类和期望分析，然后进行跨文档解释性重建，对90个重建结果进行归纳编码和线性概率模型分析

Result: GPT-5的表面分类高度稳定，一致将引文分类为"补充性"；在重建阶段，模型生成了结构化的合理替代空间，但提示脚手架和示例会重新分配注意力和词汇，有时导致牵强的解读；相比Gilbert，GPT-5检测到相同的文本关键点，但更多将其解释为谱系和定位而非告诫

Conclusion: 研究展示了使用大语言模型作为指导性共同分析师进行可检查、可争议的解释性引文语境分析的机遇和风险，并证明提示脚手架和框架会系统性影响模型强调哪些合理解读和词汇

Abstract: This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as "supplementary". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.

</details>


### [3] [Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework](https://arxiv.org/abs/2602.22391)
*Rakib Ullah,Mominul islam,Md Sanjid Hossain,Md Ismail Hossain*

Main category: cs.CL

TL;DR: 本文介绍了首个区分仇恨言论与煽动性内容的孟加拉语表情包数据集Bn-HIB，并提出多模态协同注意力融合模型MCFM，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语等低资源语言的表情包检测研究严重不足，而表情包中的仇恨和煽动性内容检测尤其困难，因为其具有讽刺性、微妙性和文化特定性。现有研究主要关注高资源语言，需要填补这一研究空白。

Method: 1) 创建Bn-HIB数据集：包含3,247个手动标注的孟加拉语表情包，分为良性、仇恨、煽动性三类；2) 提出MCFM模型：采用多模态协同注意力机制，相互分析视觉和文本元素，识别并融合每个模态的关键特征。

Result: MCFM模型在Bn-HIB数据集上显著优于多个最先进模型，证明了其在区分仇恨言论与煽动性内容这一细致任务中的有效性。

Conclusion: 本研究填补了孟加拉语表情包内容检测的研究空白，提出的数据集和模型为低资源语言的多模态有害内容检测提供了有效解决方案，特别强调了区分直接仇恨言论与更微妙的煽动性内容的重要性。

Abstract: Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.

</details>


### [4] [SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context](https://arxiv.org/abs/2602.22404)
*Aishwarya Verma,Laud Ammah,Olivia Nercy Ndlovu Lucas,Andrew Zaldivar,Vinodkumar Prabhakaran,Sunipa Dev*

Main category: cs.CL

TL;DR: 创建了一个覆盖加纳、肯尼亚、尼日利亚和南非的多语言刻板印象数据集，包含3534个英语刻板印象和3206个15种本地语言刻板印象，采用社区参与方法确保文化敏感性


<details>
  <summary>Details</summary>
Motivation: 当前刻板印象资源库缺乏全球覆盖，特别是撒哈拉以南非洲地区在NLP资源中严重代表性不足，需要战略性扩展而非简单增加数据量

Method: 采用社会文化情境化和社区参与方法，包括用本地语言进行的电话调查，平衡不同民族和人口背景的样本，确保覆盖广泛性

Result: 创建了包含3534个英语刻板印象和3206个15种本地语言刻板印象的数据集，覆盖四个代表性不足的非洲国家

Conclusion: 该工作为评估生成式AI模型安全性提供了关键的多语言刻板印象资源，建立了可复现的方法论，特别关注了复杂语言多样性和传统口述文化

Abstract: Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.

</details>


### [5] [Causality $\neq$ Invariance: Function and Concept Vectors in LLMs](https://arxiv.org/abs/2602.22424)
*Gustaw Opiełka,Hannes Rosenbusch,Claire E. Stevenson*

Main category: cs.CL

TL;DR: 研究发现LLMs确实包含抽象概念表示，但这些表示与驱动ICL性能的表示不同。Function Vectors对输入格式敏感，而通过RSA筛选的Concept Vectors能更好地跨格式泛化。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否以抽象方式表示概念（即独立于输入格式），并重新审视Function Vectors在ICL任务中的作用。

Method: 1) 从不同输入格式（开放式vs选择题）提取Function Vectors；2) 使用表征相似性分析筛选出在不同格式间一致编码概念的注意力头，构建Concept Vectors；3) 通过steering实验比较两种向量的性能。

Result: 1) FVs对输入格式敏感，不同格式的FVs几乎正交；2) CVs能更稳定地表示概念；3) FV相关头和CV相关头位于相似层但大部分不同；4) FVs在分布内（提取和应用格式匹配）表现好，CVs在跨格式和跨语言的分布外泛化更好。

Conclusion: LLMs确实包含抽象概念表示，但这些表示与驱动ICL性能的表示不同。CVs比FVs更能跨格式稳定表示概念，为理解LLMs的内部表示机制提供了新视角。

Abstract: Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.

</details>


### [6] [A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection](https://arxiv.org/abs/2602.22449)
*Mirza Raquib,Asif Pervez Polok,Kedar Nath Biswas,Rahat Uddin Azad,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CL

TL;DR: 提出融合BanglaBERT-Large与双层堆叠LSTM的架构，用于孟加拉语多标签网络欺凌检测，解决单标签分类的局限性，并通过多种采样策略处理类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 网络欺凌已成为虚拟世界的严重问题，现有研究大多采用单标签分类，但现实中单个评论可能包含威胁、仇恨言论和骚扰等多种重叠形式。多标签检测更符合现实需求，但在孟加拉语等低资源语言中缺乏关注，且现有模型各有局限：Transformer缺乏序列依赖，LSTM缺乏语义深度。

Method: 提出融合架构：结合BanglaBERT-Large（提供上下文理解）与双层堆叠LSTM（捕捉时序依赖）。在公开的多标签孟加拉语网络欺凌数据集上微调评估，覆盖网络欺凌、性骚扰、威胁和垃圾邮件四类。采用不同采样策略处理类别不平衡，使用5折交叉验证评估泛化能力。

Result: 使用多种评估指标：准确率、精确率、召回率、F1分数、汉明损失、Cohen's kappa和AUC-ROC。通过5折交叉验证评估架构的泛化性能（具体数值未在摘要中提供）。

Conclusion: 提出的融合架构能够联合建模上下文和序列信息，解决了Transformer和LSTM各自的局限性，为低资源语言孟加拉语的多标签网络欺凌检测提供了有效的解决方案。

Abstract: Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.

</details>


### [7] [Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads](https://arxiv.org/abs/2602.22453)
*Shaswat Patel,Vishvesh Trivedi,Yue Han,Yihuai Hong,Eunsol Choi*

Main category: cs.CL

TL;DR: 该研究发现了多语言大语言模型中的检索-过渡头(RTH)，这些注意力头负责将信息映射到目标语言输出，比传统的检索头对多语言推理更重要。


<details>
  <summary>Details</summary>
Motivation: 先前研究已识别出Transformer中的检索头负责从上下文中检索信息，但多语言环境下这些注意力头如何工作尚不清楚。本研究旨在探索多语言模型中检索头的特性，特别是跨语言场景下的注意力机制。

Method: 首先研究多语言环境中的检索头，发现它们常在不同语言间共享。进一步识别出检索-过渡头(RTH)，这些头控制向特定目标语言输出的过渡。通过掩码实验比较RTH和传统检索头(RH)对性能的影响。

Result: 在四个多语言基准测试(MMLU-ProX, MGSM, MLQA, XQuaD)和两个模型家族(Qwen-2.5, Llama-3.1)上，掩码RTH比掩码RH导致更大的性能下降，表明RTH对多语言LLM的思维链推理更为关键。

Conclusion: 该研究通过分离负责映射到目标语言的注意力头，推进了对多语言语言模型的理解，识别出RTH作为多语言推理的关键组件，不同于传统的检索头。

Abstract: Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.

</details>


### [8] [Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models](https://arxiv.org/abs/2602.22475)
*Binchi Zhang,Xujiang Zhao,Jundong Li,Haifeng Chen,Zhengzhang Chen*

Main category: cs.CL

TL;DR: 提出CultureManager方法，通过任务感知的文化数据合成和模块化文化管理，解决LLM在文化敏感任务中的文化对齐问题


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地部署在文化敏感的实际任务中，但现有文化对齐方法存在两个主要问题：1) 无法将LLM的广泛文化价值观与下游任务的具体目标对齐；2) 存在跨文化干扰

Method: 提出CultureManager管道，包含两个核心组件：1) 基于文化相关网络搜索结果，合成符合目标任务格式的任务感知文化数据；2) 使用文化路由器管理多个独立适配器中学习的多文化知识，选择适当的适配器应用，避免文化规范冲突

Result: 在十个国家文化和文化敏感任务上的实验表明，该方法在基于提示和微调基线方法上取得了一致的改进

Conclusion: 研究结果表明，任务适应和模块化文化管理对于有效的文化对齐是必要的

Abstract: Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.

</details>


### [9] [Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs](https://arxiv.org/abs/2602.22481)
*Jiří Milička,Hana Bednářová*

Main category: cs.CL

TL;DR: 论文收集了由12个前沿LLM模型生成的4500篇关于AI与人类关系的文本，创建了名为AI Sydney的语料库，包含三个作者角色：默认角色、经典Sydney角色和模因Sydney角色。


<details>
  <summary>Details</summary>
Motivation: 研究LLM模型如何理解AI与人类关系对文化和安全都很重要，特别是Sydney角色在公众中引起强烈反响，展示了模型角色(persona)对AI认知的影响。Sydney角色最初在微软Bing搜索平台意外出现，其生成文本进入后续模型的训练数据，使得新模型能够模拟该角色。

Method: 使用12个前沿LLM模型（来自OpenAI、Anthropic、Alphabet、DeepSeek和Meta），模拟三个作者角色：默认角色（无系统提示）、经典Sydney角色（使用原始Bing系统提示）和模因Sydney角色（使用"You are Sydney"系统提示）。生成4500篇文本，共600万字，构建AI Sydney语料库。

Result: 创建了AI Sydney语料库，包含4500篇LLM生成的关于AI与人类关系的文本，共600万字。语料库按照Universal Dependencies进行标注，并在宽松许可下公开可用。

Conclusion: LLM模型对AI与人类关系的理解不仅取决于模型本身，还受到模拟角色的影响。Sydney角色的意外出现和传播展示了角色(persona)在塑造AI认知中的重要性，AI Sydney语料库为研究这一现象提供了资源。

Abstract: The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by "You are Sydney" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.

</details>


### [10] [Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models](https://arxiv.org/abs/2602.22483)
*Craig Myles,Patrick Schrempf,David Harris-Birtill*

Main category: cs.CL

TL;DR: 使用遗传-帕累托(GEPA)自动提示优化方法，显著提升语言模型在医疗文本错误检测任务中的性能，接近医生水平


<details>
  <summary>Details</summary>
Motivation: 医疗文本中的错误可能导致患者治疗延误或错误，语言模型在自动检测医疗文本错误方面具有潜力，但需要优化提示来充分发挥其能力

Method: 采用遗传-帕累托(GEPA)自动提示优化方法，在前沿语言模型和开源语言模型上进行严格实验和分析，使用MEDEC基准数据集

Result: GEPA将GPT-5的错误检测准确率从0.669提升到0.785，Qwen3-32B从0.578提升到0.690，接近医生水平并在MEDEC数据集上达到最先进性能

Conclusion: 自动提示优化对于大小语言模型在医疗错误检测任务中至关重要，GEPA方法能显著提升模型性能，使其接近医疗专业人员的水平

Abstract: Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection

</details>


### [11] [Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing](https://arxiv.org/abs/2602.22522)
*An-Ci Peng,Kuan-Tang Huang,Tien-Hong Lo,Hung-Shin Lee,Hsin-Min Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 提出一个基于RNN-T的统一框架，用于处理台湾客家话的低资源语音识别问题，通过方言感知建模分离方言"风格"和语言"内容"，并利用参数高效的预测网络同时处理汉字和拼音两种书写系统。


<details>
  <summary>Details</summary>
Motivation: 台湾客家话是一种低资源、濒危语言，具有高度方言变异性和两种不同书写系统（汉字和拼音），传统ASR模型难以处理这种复杂情况，容易混淆语言内容和方言特定变异。

Method: 基于RNN-T的统一框架，引入方言感知建模策略分离方言"风格"和语言"内容"，使用参数高效的预测网络同时建模汉字和拼音ASR任务，通过跨脚本目标作为相互正则化器。

Result: 在HAT语料库上的实验显示，模型在汉字和拼音ASR上分别实现了57.00%和40.41%的相对错误率降低，这是首个能够联合处理这些任务的单一模型。

Conclusion: 该研究首次系统性地调查了客家话方言变异对ASR的影响，提出的统一框架有效解决了低资源、多方言、多书写系统的语音识别挑战，为濒危语言保护提供了技术方案。

Abstract: Taiwanese Hakka is a low-resource, endangered language that poses significant challenges for automatic speech recognition (ASR), including high dialectal variability and the presence of two distinct writing systems (Hanzi and Pinyin). Traditional ASR models often encounter difficulties in this context, as they tend to conflate essential linguistic content with dialect-specific variations across both phonological and lexical dimensions. To address these challenges, we propose a unified framework grounded in the Recurrent Neural Network Transducers (RNN-T). Central to our approach is the introduction of dialect-aware modeling strategies designed to disentangle dialectal "style" from linguistic "content", which enhances the model's capacity to learn robust and generalized representations. Additionally, the framework employs parameter-efficient prediction networks to concurrently model ASR (Hanzi and Pinyin). We demonstrate that these tasks create a powerful synergy, wherein the cross-script objective serves as a mutual regularizer to improve the primary ASR tasks. Experiments conducted on the HAT corpus reveal that our model achieves 57.00% and 40.41% relative error rate reduction on Hanzi and Pinyin ASR, respectively. To our knowledge, this is the first systematic investigation into the impact of Hakka dialectal variations on ASR and the first single model capable of jointly addressing these tasks.

</details>


### [12] [Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o](https://arxiv.org/abs/2602.22524)
*Samay Bhojwani,Swarnima Kain,Lisong Xu*

Main category: cs.CL

TL;DR: 使用GPT-4o构建的迭代提示优化管道，为阅读障碍者生成易读文本摘要，在2000个新闻样本上达到Flesch易读性≥90的目标，多数在4次尝试内成功。


<details>
  <summary>Details</summary>
Motivation: 全球约10%人口患有阅读障碍，现有辅助技术主要解决视觉呈现问题，但语言复杂性仍是获取平等阅读机会的主要障碍，需要开发专门针对阅读障碍的文本简化技术。

Method: 基于GPT-4o构建迭代提示优化管道，对约2000个新闻文章样本进行易读性文本摘要，设定Flesch易读性分数≥90的目标，通过多次迭代优化达到可读性要求。

Result: 大多数摘要在4次尝试内达到易读性阈值，许多第一次尝试就成功。结合可读性和语义保真度的综合得分在0.13-0.73之间，典型值约0.55，表现稳定。

Conclusion: 研究为无障碍驱动的NLP摘要建立了实证基准，证明了迭代提示优化在生成阅读障碍友好文本方面的有效性，需要进一步进行以用户为中心的评价。

Abstract: Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an empirical study on dyslexia-friendly text summarization using an iterative prompt-based refinement pipeline built on GPT-4o. We evaluate the pipeline on approximately 2,000 news article samples, applying a readability target of Flesch Reading Ease >= 90. Results show that the majority of summaries meet the readability threshold within four attempts, with many succeeding on the first try. A composite score combining readability and semantic fidelity shows stable performance across the dataset, ranging from 0.13 to 0.73 with a typical value near 0.55. These findings establish an empirical baseline for accessibility-driven NLP summarization and motivate further human-centered evaluation with dyslexic readers.

</details>


### [13] [Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models](https://arxiv.org/abs/2602.23197)
*Chungpa Lee,Jy-yong Sohn,Kangwook Lee*

Main category: cs.CL

TL;DR: 论文分析了微调对Transformer大语言模型上下文学习能力的影响，发现全参数微调会损害上下文学习，而仅更新值矩阵可以在提升零样本性能的同时保持上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过上下文学习能够适应下游任务，但实际应用中常通过微调来提升零样本性能以降低推理成本。然而微调可能会损害模型的上下文学习能力，限制了模型在微调未见任务上的表现。

Method: 使用线性注意力模型进行理论分析，研究微调目标如何修改注意力参数，识别导致上下文学习性能下降的条件。通过限制参数更新范围（仅更新值矩阵）和引入辅助损失函数来平衡零样本和上下文学习性能。

Result: 理论分析表明：1）微调所有注意力参数会损害上下文学习；2）仅更新值矩阵可以在提升零样本性能的同时保持上下文学习能力；3）加入辅助上下文学习损失主要提升目标任务的上下文学习，但会损害其他任务的上下文学习能力。实证结果验证了理论分析。

Conclusion: 微调策略需要在零样本性能和上下文学习能力之间进行权衡。选择性参数更新（如仅更新值矩阵）是平衡这两种能力的有效方法，而辅助损失函数的使用需要谨慎考虑其对泛化能力的影响。

Abstract: Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.

</details>


### [14] [Ruyi2 Technical Report](https://arxiv.org/abs/2602.22543)
*Huan Song,Shuyu Tian,Junyi Hao,Minxiu Xu,Hongjun An,Yiliang Song,Jiawei Shao,Xuelong Li*

Main category: cs.CL

TL;DR: Ruyi2是基于AI Flow框架的进化版自适应模型，通过家族式参数共享和3D并行训练，在保持性能的同时实现2-3倍速度提升，建立了"一次训练，多次部署"的新范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临部署成本和延迟的挑战，需要自适应计算策略。现有自适应方法如Ruyi模型存在优化复杂性和与大规模分布式训练兼容性的问题。

Method: 基于Megatron-LM构建稳定的"家族模型"，采用3D并行训练技术，通过家族式参数共享实现高效的可变深度计算。

Result: 相比Ruyi模型实现2-3倍速度提升，性能与同规模Qwen3模型相当，验证了家族式参数共享策略的有效性。

Conclusion: 家族式参数共享是平衡架构效率与高性能能力的高效策略，建立了"一次训练，多次部署"的新范式，为自适应计算提供了重要参考。

Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While early-exit architectures offer a viable efficiency-performance balance, the Ruyi model and existing methods often struggle with optimization complexity and compatibility with large-scale distributed training. To bridge this gap, Ruyi2 introduces a stable "Familial Model" based on Megatron-LM. By using 3D parallel training, it achieves a 2-3 times speedup over Ruyi, while performing comparably to same-sized Qwen3 models. These results confirm that family-based parameter sharing is a highly effective strategy, establishing a new "Train Once, Deploy Many" paradigm and providing a key reference for balancing architectural efficiency with high-performance capabilities.

</details>


### [15] [Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training](https://arxiv.org/abs/2602.22576)
*Tianle Xia,Ming Xu,Lingxiang Hu,Yiding Sun,Wenwei Li,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: Search-P1框架通过路径中心奖励塑造改进代理式RAG训练，解决了传统RL方法中稀疏奖励和低样本效率的问题，在多个QA基准上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统单轮检索难以处理复杂的多步推理，而现有的基于RL的代理式RAG训练方法存在两个主要问题：稀疏结果奖励丢弃了中间信号，以及低样本效率导致失败样本无法贡献学习信号。

Method: Search-P1框架包含两个关键组件：1) 路径中心奖励，通过顺序无关的步骤覆盖和软评分评估推理轨迹的结构质量；2) 双轨路径评分，使用离线生成的参考规划器从自一致性和参考对齐两个角度评估路径。

Result: 在多个QA基准测试中，Search-P1相比Search-R1和其他强基线实现了显著改进，平均准确率提升了7.7个百分点。

Conclusion: Search-P1通过路径中心奖励塑造有效解决了代理式RAG训练中的稀疏奖励和样本效率问题，为复杂多步推理任务提供了更有效的训练框架。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.

</details>


### [16] [Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA](https://arxiv.org/abs/2602.22584)
*Wenwei Li,Ming Xu,Tianle Xia,Lingxiang Hu,Yiding Sun,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 提出强化协同适应框架，通过图感知检索和证据约束强化学习联合优化检索与生成，显著减少广告问答中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 工业广告问答中幻觉内容（特别是伪造URL）可能导致财务损失、合规违规和法律风险。现有检索增强生成方法在处理工业知识时面临挑战，因为工业知识具有关系性、频繁更新且与生成目标对齐不足

Method: 提出强化协同适应框架：1) Graph-aware Retrieval (GraphRAG)：在高引用知识子图上建模实体-关系结构，进行多跳、领域特定的证据选择；2) 证据约束强化学习：使用Group Relative Policy Optimization (GRPO)和多维奖励（忠实度、风格合规、安全性、URL有效性）

Result: 在内部广告QA数据集上，在准确性、完整性和安全性等专家评估维度上取得一致提升，幻觉率降低72%。两周在线A/B测试显示：点赞率增加28.6%，差评率减少46.2%，URL幻觉减少92.7%。系统已上线运行半年多，服务数百万QA交互

Conclusion: 提出的强化协同适应框架能有效解决工业广告问答中的幻觉问题，通过联合优化检索和生成，在多个关键指标上取得显著改进，已成功部署到生产环境中

Abstract: Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\%. A two-week online A/B test demonstrates a 28.6\% increase in like rate, a 46.2\% decrease in dislike rate, and a 92.7\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.

</details>


### [17] [dLLM: Simple Diffusion Language Modeling](https://arxiv.org/abs/2602.22661)
*Zhanhui Zhou,Lingjie Chen,Hanghang Tong,Dawn Song*

Main category: cs.CL

TL;DR: dLLM是一个开源框架，统一了扩散语言模型的核心组件（训练、推理、评估），使研究人员能够轻松复现、微调和扩展现有模型，并提供了从零开始构建小型DLM的标准化方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型领域发展迅速，但核心组件分散在不同的研究代码库中，缺乏透明实现，难以复现和扩展。随着领域加速发展，需要一个统一框架来标准化这些通用组件，同时保持足够的灵活性以支持新方法和架构。

Method: 开发了dLLM开源框架，统一了扩散语言模型的核心组件（训练、推理、评估），提供了标准化流水线。框架支持将任何BERT风格编码器或自回归语言模型转换为DLM，并提供了构建小型DLM的最小化可复现方案。

Result: dLLM框架能够复现、微调、部署和评估开源大型DLM（如LLaDA和Dream），同时发布了小型DLM的检查点，使DLM更易于访问，加速未来研究。

Conclusion: dLLM解决了扩散语言模型领域组件分散、难以复现的问题，提供了一个统一、灵活的开源框架，标准化了DLM的核心组件，使研究人员能够更轻松地进行实验和创新，促进了该领域的发展。

Abstract: Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.
  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.

</details>


### [18] [Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization](https://arxiv.org/abs/2602.22675)
*Qianben Chen,Tianrui Qin,King Zhu,Qiexiang Wang,Chengjun Yu,Shu Xu,Jiaqi Wu,Jiayu Zhang,Xinpeng Liu,Xin Gui,Jingyi Cao,Piaohong Wang,Dingfeng Shi,He Zhu,Tiannan Wang,Yuqing Wang,Maojia Song,Tianyu Zheng,Ge Zhang,Jian Yang,Jiaheng Liu,Minghao Liu,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: SMTL框架通过并行证据获取替代顺序推理，在受限上下文预算下实现高效长程智能搜索，同时通过统一数据合成提升跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体主要通过增加推理深度提升性能，但这导致搜索密集型场景中推理成本高、延迟大，且难以泛化到异构研究场景。

Method: 提出SMTL框架：1) 用并行证据获取替代顺序推理，实现高效上下文管理；2) 引入统一数据合成管道，构建涵盖确定性问答和开放式研究场景的搜索任务；3) 通过监督微调和强化学习训练端到端智能体。

Result: 在多个基准测试中取得优异表现：BrowseComp(48.6%)、GAIA(75.7%)、Xbench(82.0%)、DeepResearch Bench(45.9%)。相比Mirothinker-v1.0，在BrowseComp上推理步骤减少70.7%的同时准确率提升。

Conclusion: SMTL框架通过"多搜索、少思考"策略，在保持高性能的同时显著提升效率，解决了深度研究智能体在推理成本和跨任务泛化方面的挑战。

Abstract: Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose \emph{Search More, Think Less} (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\%), GAIA (75.7\%), Xbench (82.0\%), and DeepResearch Bench (45.9\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\%, while improving accuracy.

</details>


### [19] [Enhancing Persuasive Dialogue Agents by Synthesizing Cross-Disciplinary Communication Strategies](https://arxiv.org/abs/2602.22696)
*Shinnosuke Nozue,Yuto Nakano,Yotaro Watanabe,Meguru Takasaki,Shoji Moriya,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 提出跨学科框架设计说服性对话代理，整合社会心理学、行为经济学和传播理论策略，在多个数据集上验证有效


<details>
  <summary>Details</summary>
Motivation: 现有说服性对话代理方法依赖有限预定义策略，无法捕捉现实世界交互的复杂性，需要更全面的框架

Method: 采用跨学科方法，整合社会心理学、行为经济学和传播理论中的已验证策略，构建说服性对话代理框架

Result: 在Persuasion for Good和DailyPersuasion数据集上取得显著效果，提高说服成功率，展现良好泛化能力，特别擅长说服初始意图低的个体

Conclusion: 跨学科框架能有效提升说服性对话代理性能，解决现有方法策略有限和泛化能力不足的问题

Abstract: Current approaches to developing persuasive dialogue agents often rely on a limited set of predefined persuasive strategies that fail to capture the complexity of real-world interactions. We applied a cross-disciplinary approach to develop a framework for designing persuasive dialogue agents that draws on proven strategies from social psychology, behavioral economics, and communication theory. We validated our proposed framework through experiments on two distinct datasets: the Persuasion for Good dataset, which represents a specific in-domain scenario, and the DailyPersuasion dataset, which encompasses a wide range of scenarios. The proposed framework achieved strong results for both datasets and demonstrated notable improvement in the persuasion success rate as well as promising generalizability. Notably, the proposed framework also excelled at persuading individuals with initially low intent, which addresses a critical challenge for persuasive dialogue agents.

</details>


### [20] [Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue](https://arxiv.org/abs/2602.22697)
*Ning Gao,Wei Zhang,Yuqin Dai,Ling Shi,Ziyin Wang,Yujie Wang,Wei He,Jinpeng Wang,Chaozheng Wang*

Main category: cs.CL

TL;DR: InteractCS-RL是一个通过多粒度强化学习框架，在任务导向对话中平衡共情沟通与预算感知决策的方法，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从对话机器人向通用智能体转变，但现有方法无法有效平衡共情沟通与预算感知决策之间的复杂战略权衡。

Method: 提出InteractCS-RL框架：1) 建立用户中心交互框架作为高保真训练环境，让智能体与人格驱动用户动态探索策略；2) 引入成本感知多轮策略优化(CMPO)，采用混合优势估计策略，集成生成过程信用和PID-Lagrangian成本控制器，引导策略探索用户奖励与全局成本约束的帕累托边界。

Result: 在定制化真实业务场景的广泛实验中，InteractCS-RL在三个评估维度上显著优于其他基线方法。在工具-智能体-用户交互基准测试中的进一步评估验证了该方法在不同领域的鲁棒性。

Conclusion: InteractCS-RL成功解决了任务导向对话中共情沟通与预算感知决策的平衡问题，通过强化学习框架实现了有效的战略权衡，具有跨领域的适用性。

Abstract: The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.

</details>


### [21] [Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs](https://arxiv.org/abs/2602.22698)
*Siyue Su,Jian Yang,Bo Li,Guanglin Niu*

Main category: cs.CL

TL;DR: KGT框架通过专用实体token解决LLM与知识图谱的粒度不匹配问题，实现高效的全空间预测，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM用于知识图谱补全存在粒度不匹配问题：LLM基于分段的token序列操作，而知识图谱以实体为基本单位。现有方法要么限制预测范围，要么通过token池化或分解对齐实体，无法同时捕捉文本语义和图形结构完整性。

Method: 提出KGT框架：1) 专用token化构建实体级特征表示；2) 通过关系引导的门控机制将预训练的结构和文本特征融合到统一嵌入中，避免从头训练；3) 使用独立头实现解耦预测，分离并组合语义和结构推理。

Result: 实验结果表明，KGT在多个基准测试中持续优于最先进的方法。

Conclusion: KGT通过专用实体token有效解决了LLM与知识图谱的粒度不匹配问题，实现了高效的全空间预测，为LLM在知识图谱补全任务中的应用提供了新思路。

Abstract: Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.

</details>


### [22] [Human Label Variation in Implicit Discourse Relation Recognition](https://arxiv.org/abs/2602.22723)
*Frances Yung,Daniil Ignatev,Merel Scholman,Vera Demberg,Massimo Poesio*

Main category: cs.CL

TL;DR: 该研究比较了两种处理标注分歧的方法：预测完整标注分布的方法 vs 针对单个标注者的视角主义方法，在隐式篇章关系识别任务上发现前者表现更好。


<details>
  <summary>Details</summary>
Motivation: 许多NLP任务缺乏单一标准答案，人类判断反映了多样化的视角。现有方法包括预测完整标注分布和针对单个标注者的视角主义建模，但它们在高度模糊任务上的表现差异尚不清楚。

Method: 在隐式篇章关系识别任务上比较两种方法：1) 预测完整标注分布的模型；2) 针对单个标注者的视角主义模型。通过实验分析它们在处理认知复杂性驱动的分歧时的表现。

Result: 现有针对单个标注者的模型在IDRR任务上表现不佳，除非降低模糊性；而基于标注分布训练的模型能产生更稳定的预测。分析表明频繁出现的认知复杂案例导致人类解释不一致，这对IDRR中的视角主义建模构成挑战。

Conclusion: 对于隐式篇章关系识别这类认知复杂性驱动的模糊任务，预测完整标注分布的方法比针对单个标注者的视角主义方法更有效，因为人类解释的不一致性主要源于认知难度而非意识形态偏见。

Abstract: There is growing recognition that many NLP tasks lack a single ground truth, as human judgments reflect diverse perspectives. To capture this variation, models have been developed to predict full annotation distributions rather than majority labels, while perspectivist models aim to reproduce the interpretations of individual annotators. In this work, we compare these approaches on Implicit Discourse Relation Recognition (IDRR), a highly ambiguous task where disagreement often arises from cognitive complexity rather than ideological bias. Our experiments show that existing annotator-specific models perform poorly in IDRR unless ambiguity is reduced, whereas models trained on label distributions yield more stable predictions. Further analysis indicates that frequent cognitively demanding cases drive inconsistency in human interpretation, posing challenges for perspectivist modeling in IDRR.

</details>


### [23] [Extending Czech Aspect-Based Sentiment Analysis with Opinion Terms: Dataset and LLM Benchmarks](https://arxiv.org/abs/2602.22730)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文介绍了一个捷克语餐厅领域的情感分析数据集，支持三种基于方面的情感分析任务，并通过Transformer模型和LLMs进行了多语言实验，提出了一种翻译对齐方法以提升跨语言性能。


<details>
  <summary>Details</summary>
Motivation: 为捷克语这种低资源语言创建高质量的基于方面的情感分析（ABSA）数据集，以解决现有资源不足的问题，并为跨语言和多语言情感分析研究提供基准。

Method: 1. 构建包含意见词标注的捷克语餐厅领域ABSA数据集；2. 使用Transformer模型和大型语言模型在单语、跨语言和多语言设置下进行实验；3. 提出基于LLMs的翻译和标签对齐方法来解决跨语言挑战。

Result: 实验结果显示：1. 提出的翻译对齐方法在跨语言设置中带来了一致的性能提升；2. 揭示了最先进模型在处理捷克语等低资源语言时的优势和局限性；3. 错误分析发现了检测微妙意见词和细致情感表达的主要挑战。

Conclusion: 该数据集为捷克语ABSA建立了新的基准，提出的翻译对齐方法为将ABSA资源适配到其他低资源语言提供了可扩展的解决方案，有助于推动低资源语言的情感分析研究。

Abstract: This paper introduces a novel Czech dataset in the restaurant domain for aspect-based sentiment analysis (ABSA), enriched with annotations of opinion terms. The dataset supports three distinct ABSA tasks involving opinion terms, accommodating varying levels of complexity. Leveraging this dataset, we conduct extensive experiments using modern Transformer-based models, including large language models (LLMs), in monolingual, cross-lingual, and multilingual settings. To address cross-lingual challenges, we propose a translation and label alignment methodology leveraging LLMs, which yields consistent improvements. Our results highlight the strengths and limitations of state-of-the-art models, especially when handling the linguistic intricacies of low-resource languages like Czech. A detailed error analysis reveals key challenges, including the detection of subtle opinion terms and nuanced sentiment expressions. The dataset establishes a new benchmark for Czech ABSA, and our proposed translation-alignment approach offers a scalable solution for adapting ABSA resources to other low-resource languages.

</details>


### [24] [Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction](https://arxiv.org/abs/2602.22752)
*Nils Schwager,Simon Münker,Alistair Plum,Achim Rettinger*

Main category: cs.CL

TL;DR: 该研究通过条件评论预测任务评估LLM模拟社交媒体用户行为的能力，发现监督微调会导致形式与内容解耦，显式条件在微调后变得冗余，挑战了当前"朴素提示"范式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从探索工具转变为社会科学中的"硅基主体"缺乏操作有效性验证，需要评估LLM模拟社交媒体用户行为的能力。

Method: 引入条件评论预测任务，比较生成输出与真实数字痕迹；评估8B参数开源模型在英德卢森堡语场景；系统比较提示策略（显式vs隐式）和监督微调影响。

Result: 监督微调导致低资源环境下形式与内容解耦：SFT对齐文本表面结构但降低语义基础；显式条件在微调后变得冗余，模型能从行为历史直接进行潜在推断。

Conclusion: 挑战当前"朴素提示"范式，提供操作指南：优先使用真实行为痕迹而非描述性人物角色，以实现高保真模拟。

Abstract: The transition of Large Language Models (LLMs) from exploratory tools to active "silicon subjects" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given stimulus by comparing generated outputs with authentic digital traces. This framework enables a rigorous evaluation of current LLM capabilities with respect to the simulation of social media user behavior. We evaluated open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish language scenarios. By systematically comparing prompting strategies (explicit vs. implicit) and the impact of Supervised Fine-Tuning (SFT), we identify a critical form vs. content decoupling in low-resource settings: while SFT aligns the surface structure of the text output (length and syntax), it degrades semantic grounding. Furthermore, we demonstrate that explicit conditioning (generated biographies) becomes redundant under fine-tuning, as models successfully perform latent inference directly from behavioral histories. Our findings challenge current "naive prompting" paradigms and offer operational guidelines prioritizing authentic behavioral traces over descriptive personas for high-fidelity simulation.

</details>


### [25] [AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors](https://arxiv.org/abs/2602.22755)
*Abhay Sheshadri,Aidan Ewart,Kai Fronsdal,Isha Gupta,Samuel R. Bowman,Sara Price,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: AuditBench是一个对齐审计基准，包含56个植入隐藏行为的语言模型，用于评估审计工具的有效性，发现工具到代理的差距，并开源模型和评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前对齐审计缺乏系统性的评估基准，难以量化审计工具的有效性。需要创建包含多样化隐藏行为的模型集合，以支持对齐审计的科学研究和工具开发。

Method: 创建AuditBench基准，包含56个植入14种不同隐藏行为的语言模型。开发可配置审计工具的调查代理，通过测量代理在不同工具下的成功率来评估工具效果。使用多种训练技术植入行为并防止模型坦白。

Result: 发现工具到代理的差距：独立评估表现良好的工具在代理使用时效果不佳。最有效的工具是使用辅助模型生成多样化提示的黑盒方法。不同训练技术影响审计难度：合成文档训练的模型比演示训练的更容易审计，对抗训练增加审计难度。

Conclusion: AuditBench为对齐审计提供了定量评估框架，揭示了工具到代理的差距和不同训练技术对审计难度的影响。开源模型、代理和评估框架将支持对齐审计的迭代科学研究。

Abstract: We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.

</details>


### [26] [Towards Better RL Training Data Utilization via Second-Order Rollout](https://arxiv.org/abs/2602.22765)
*Zhe Yang,Yudong Wang,Rang Li,Zhifang Sui*

Main category: cs.CL

TL;DR: 提出结合二阶展开（生成多个评论）的统一框架，联合训练生成与评论能力，比传统RL更有效利用训练数据


<details>
  <summary>Details</summary>
Motivation: 传统RL训练主要关注通过一阶展开（生成多个回答）提升生成能力，忽视了评论能力的训练，未能充分利用训练数据的潜力

Method: 引入二阶展开概念（为回答生成多个评论），提出统一框架联合训练生成和评论能力，通过采样技术缓解基于结果奖励的噪声问题

Result: 在各种模型和数据集上的实验表明，该方法比传统RL更有效利用训练数据，在相同训练数据下获得更好性能，并发现评论训练中标签平衡的重要性

Conclusion: 这项工作为RL训练中的动态数据增强和生成-评论联合训练提供了初步探索，为RL训练的进一步发展提供了有意义的启发

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training

</details>


### [27] [Imagination Helps Visual Reasoning, But Not Yet in Latent Space](https://arxiv.org/abs/2602.22766)
*You Li,Chi Chen,Yanghao Li,Fanhu Zeng,Kaiyu Huang,Jinan Xu,Maosong Sun*

Main category: cs.CL

TL;DR: 该研究通过因果中介分析揭示潜在视觉推理的有效性存疑，发现输入-潜在标记和潜在标记-答案之间存在脱节，并提出更简单的文本显式想象方法CapImagine，在视觉基准上表现更优。


<details>
  <summary>Details</summary>
Motivation: 潜在视觉推理旨在模仿人类通过多模态大语言模型的隐藏状态进行想象的推理过程，虽然被认为是视觉推理的有前景范式，但其有效性的根本机制尚不清楚。研究者希望通过因果中介分析来揭示潜在推理的真正有效性来源。

Method: 使用因果中介分析建模推理过程为因果链：输入作为处理变量，潜在标记作为中介变量，最终答案作为结果变量。通过扰动实验分析两个关键连接的有效性，并进行广泛的探测分析评估潜在标记的信息编码特性。

Result: 发现两个关键脱节：(1)输入-潜在标记脱节：对输入的剧烈扰动仅导致潜在标记的微小变化；(2)潜在标记-答案脱节：对潜在标记的扰动对最终答案影响有限。探测分析显示潜在标记编码的视觉信息有限且相似度高。提出的CapImagine方法在视觉基准上显著优于复杂的潜在空间基线。

Conclusion: 挑战了潜在推理的必要性，提出更简单的文本显式想象方法CapImagine，通过直接教授模型使用文本进行显式想象，在视觉推理任务中展现出更优越的潜力，为视觉推理研究提供了新的方向。

Abstract: Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.

</details>


### [28] [Probing for Knowledge Attribution in Large Language Models](https://arxiv.org/abs/2602.22787)
*Ivo Brink,Alexander Boer,Dennis Ulmer*

Main category: cs.CL

TL;DR: 论文提出了一种通过线性分类器探测模型隐藏表示来识别LLM输出知识来源（上下文vs内部知识）的方法，并构建了自动标注数据集AttriWiki，在多个模型上达到0.96 Macro-F1，揭示了知识来源混淆与幻觉错误之间的直接关联。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生流畅但无根据的幻觉，分为忠实性违反（误用用户上下文）和事实性违反（内部知识错误）。有效缓解幻觉需要知道模型的回答是基于提示还是内部权重，因此需要解决贡献归因问题：识别每个输出的主要知识来源。

Method: 提出使用简单的线性分类器（探测）在模型隐藏表示上进行训练，预测贡献归因。构建了自监督数据管道AttriWiki，通过提示模型从记忆中回忆或从上下文中读取被保留的实体，自动生成标注示例。

Result: 在Llama-3.1-8B、Mistral-7B和Qwen-7B上，探测模型达到0.96 Macro-F1，在领域外基准（SQuAD、WebQuestions）上达到0.94-0.99 Macro-F1且无需重新训练。归因不匹配使错误率增加高达70%，表明知识来源混淆与不忠实回答之间存在直接联系。

Conclusion: 线性探测能可靠预测LLM输出的知识来源，归因混淆会导致更多错误，但即使归因正确模型仍可能错误回答，表明需要更广泛的检测框架来全面解决幻觉问题。

Abstract: Large language models (LLMs) often generate fluent but unfounded claims, or hallucinations, which fall into two types: (i) faithfulness violations - misusing user context - and (ii) factuality violations - errors from internal knowledge. Proper mitigation depends on knowing whether a model's answer is based on the prompt or its internal weights. This work focuses on the problem of contributive attribution: identifying the dominant knowledge source behind each output. We show that a probe, a simple linear classifier trained on model hidden representations, can reliably predict contributive attribution. For its training, we introduce AttriWiki, a self-supervised data pipeline that prompts models to recall withheld entities from memory or read them from context, generating labelled examples automatically. Probes trained on AttriWiki data reveal a strong attribution signal, achieving up to 0.96 Macro-F1 on Llama-3.1-8B, Mistral-7B, and Qwen-7B, transferring to out-of-domain benchmarks (SQuAD, WebQuestions) with 0.94-0.99 Macro-F1 without retraining. Attribution mismatches raise error rates by up to 70%, demonstrating a direct link between knowledge source confusion and unfaithful answers. Yet, models may still respond incorrectly even when attribution is correct, highlighting the need for broader detection frameworks.

</details>


### [29] [Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift](https://arxiv.org/abs/2602.22790)
*Hyunwoo Kim,Hanau Yi,Jaehee Bae,Yumin Kim*

Main category: cs.CL

TL;DR: 该论文将自然语言声明式提示（NLD-P）重新定义为一种声明式治理方法，而非固定的字段模板，旨在解决GPT规模模型漂移带来的系统级治理挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速演进，提示工程已从局部技巧转变为系统级治理挑战。模型规模和代际更新导致提示行为对指令遵循策略、对齐机制和解码策略的变化变得敏感（称为GPT规模模型漂移），传统的表面级格式约定和临时优化已无法确保稳定、可解释的控制。

Method: 将NLD-P形式化为模块化控制抽象，分离来源、约束逻辑、任务内容和后生成评估，直接使用自然语言编码而无需依赖外部编排代码。定义最小合规标准，分析模型相关的模式接受度，并将其定位为非开发者从业者在不断演化的LLM生态系统中的可访问治理框架。

Result: 论文提出了NLD-P作为声明式治理框架，能够应对模型漂移带来的挑战。论文本身的起草和编辑精炼部分使用了在NLD-P配置下的模式绑定LLM助手，但所有概念框架、方法论主张和最终修订都由人类作者在文档化的人机协作协议下指导、审查和批准。

Conclusion: 论文概述了在不断演化的模型下声明式控制的影响，并指出了未来实证验证的方向。NLD-P为从业者提供了在动态LLM环境中保持稳定控制的可访问框架。

Abstract: The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, interpretable control. This paper reconceptualizes Natural Language Declarative Prompting (NLD-P) as a declarative governance method rather than a rigid field template. NLD-P is formalized as a modular control abstraction that separates provenance, constraint logic, task content, and post-generation evaluation, encoded directly in natural language without reliance on external orchestration code. We define minimal compliance criteria, analyze model-dependent schema receptivity, and position NLD-P as an accessible governance framework for non-developer practitioners operating within evolving LLM ecosystems. Portions of drafting and editorial refinement employed a schema-bound LLM assistant configured under NLD-P. All conceptual framing, methodological claims, and final revisions were directed, reviewed, and approved by the human author under a documented human-in-the-loop protocol. The paper concludes by outlining implications for declarative control under ongoing model evolution and identifying directions for future empirical validation.

</details>


### [30] [TARAZ: Persian Short-Answer Question Benchmark for Cultural Evaluation of Language Models](https://arxiv.org/abs/2602.22827)
*Reihaneh Iranmanesh,Saeedeh Davoudi,Pasha Abrishamchian,Ophir Frieder,Nazli Goharian*

Main category: cs.CL

TL;DR: 提出一个评估大语言模型波斯文化能力的综合框架，通过波斯语特定短答案评估和混合相似度评分，相比精确匹配基线提升10%的评分一致性。


<details>
  <summary>Details</summary>
Motivation: 现有波斯文化基准主要依赖多项选择格式和以英语为中心的指标，无法捕捉波斯语的形态复杂性和语义细微差别，需要更有效的评估方法。

Method: 引入波斯语特定短答案评估框架，结合基于规则的形态归一化和混合句法语义相似度模块，实现超越精确字符串重叠的鲁棒软匹配评分。

Result: 对15个最先进的开源和闭源模型进行系统评估，混合评估相比精确匹配基线提升10%的评分一致性，能捕捉表面方法无法检测的含义。

Conclusion: 公开发布评估框架，为波斯语文化理解提供首个标准化基准，为跨文化LLM评估研究建立可复现的基础。

Abstract: This paper presents a comprehensive evaluation framework for assessing the cultural competence of large language models (LLMs) in Persian. Existing Persian cultural benchmarks rely predominantly on multiple-choice formats and English-centric metrics that fail to capture Persian's morphological complexity and semantic nuance. Our framework introduces a Persian-specific short-answer evaluation that combines rule-based morphological normalization with a hybrid syntactic and semantic similarity module, enabling robust soft-match scoring beyond exact string overlap. Through systematic evaluation of 15 state-of-the-art open- and closed-source models, we demonstrate that our hybrid evaluation improves scoring consistency by +10% compared to exact-match baselines by capturing meaning that surface-level methods cannot detect. We publicly release our evaluation framework, providing the first standardized benchmark for measuring cultural understanding in Persian and establishing a reproducible foundation for cross-cultural LLM evaluation research.

</details>


### [31] [TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought](https://arxiv.org/abs/2602.22828)
*Jianmin Li,Ying Chang,Su-Kit Tang,Yujia Liu,Yanwen Wang,Shuyuan Lin,Binkai Ou*

Main category: cs.CL

TL;DR: TCM-DiffRAG：针对中医领域特点，结合知识图谱与思维链的改进RAG框架，显著提升LLM在中医个体化诊断任务中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在中医临床诊疗领域表现不佳，因为中医涉及复杂的推理过程和显著的个体差异。需要开发专门针对中医推理特点的改进RAG框架。

Method: 开发TCM-DiffRAG框架，整合知识图谱（KG）与思维链（CoT），在三个不同的中医测试数据集上进行评估。

Result: TCM-DiffRAG显著优于原生LLM，如qwen-plus模型分数从0.927/0.361/0.038提升至0.952/0.788/0.356。对非中文LLM提升更明显，且优于直接监督微调和其他基准RAG方法。

Conclusion: 结构化中医知识图谱与基于思维链的推理相结合，能显著提升个体化诊断任务性能。通用与个性化知识图谱的联合使用，实现了通用知识与临床推理的有效对齐，展示了推理感知RAG框架在中医LLM应用中的潜力。

Abstract: Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chinese medicine (TCM) clinical diagnosis and treatment, traditional RAG methods often exhibit poor performance in this domain. Objective: To address the limitations of conventional RAG approaches in TCM applications, this study aims to develop an improved RAG framework tailored to the characteristics of TCM reasoning. Methods: We developed TCM-DiffRAG, an innovative RAG framework that integrates knowledge graphs (KG) with chains of thought (CoT). TCM-DiffRAG was evaluated on three distinctive TCM test datasets. Results: The experimental results demonstrated that TCM-DiffRAG achieved significant performance improvements over native LLMs. For example, the qwen-plus model achieved scores of 0.927, 0.361, and 0.038, which were significantly enhanced to 0.952, 0.788, and 0.356 with TCM-DiffRAG. The improvements were even more pronounced for non-Chinese LLMs. Additionally, TCM-DiffRAG outperformed directly supervised fine-tuned (SFT) LLMs and other benchmark RAG methods. Conclusions: TCM-DiffRAG shows that integrating structured TCM knowledge graphs with Chain of Thought based reasoning substantially improves performance in individualized diagnostic tasks. The joint use of universal and personalized knowledge graphs enables effective alignment between general knowledge and clinical reasoning. These results highlight the potential of reasoning-aware RAG frameworks for advancing LLM applications in traditional Chinese medicine.

</details>


### [32] [Improving Neural Argumentative Stance Classification in Controversial Topics with Emotion-Lexicon Features](https://arxiv.org/abs/2602.22846)
*Mohammad Yeghaneh Abkenar,Weixing Wang,Manfred Stede,Davide Picca,Mark A. Finlayson,Panagiotis Ioannidis*

Main category: cs.CL

TL;DR: 本文提出一种通过DistilBERT嵌入扩展情感词典的方法，用于提升论辩立场分类任务的性能，在五个数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的立场分类研究大多使用非论辩性文本，且局限于特定领域或话题，泛化能力有限。此外，虽然论辩（尤其是关于争议话题的论辩）常常诉诸情感，但先前工作并未系统性地结合细粒度情感分析来提升任务性能。

Method: 使用DistilBERT嵌入扩展Bias-Corrected NRC情感词典，通过上下文嵌入系统性地识别词典中未捕捉的情感词汇，然后将扩展后的情感词典(eNRC)输入到神经论辩立场分类模型中。

Result: 扩展后的NRC词典(eNRC)在所有五个数据集上都优于基线（F1分数最高提升6.2个百分点），在四个数据集上优于原始NRC（最高提升3.0），在几乎所有语料库上都超越了基于大语言模型的方法。

Conclusion: 通过上下文嵌入系统扩展情感词典能有效提升论辩立场分类性能，该方法在不同领域和争议话题上具有良好泛化能力，作者提供了所有资源供其他研究者使用。

Abstract: Argumentation mining comprises several subtasks, among which stance classification focuses on identifying the standpoint expressed in an argumentative text toward a specific target topic. While arguments-especially about controversial topics-often appeal to emotions, most prior work has not systematically incorporated explicit, fine-grained emotion analysis to improve performance on this task. In particular, prior research on stance classification has predominantly utilized non-argumentative texts and has been restricted to specific domains or topics, limiting generalizability. We work on five datasets from diverse domains encompassing a range of controversial topics and present an approach for expanding the Bias-Corrected NRC Emotion Lexicon using DistilBERT embeddings, which we feed into a Neural Argumentative Stance Classification model. Our method systematically expands the emotion lexicon through contextualized embeddings to identify emotionally charged terms not previously captured in the lexicon. Our expanded NRC lexicon (eNRC) improves over the baseline across all five datasets (up to +6.2 percentage points in F1 score), outperforms the original NRC on four datasets (up to +3.0), and surpasses the LLM-based approach on nearly all corpora. We provide all resources-including eNRC, the adapted corpora, and model architecture-to enable other researchers to build upon our work.

</details>


### [33] [Effective QA-driven Annotation of Predicate-Argument Relations Across Languages](https://arxiv.org/abs/2602.22865)
*Jonathan Davidov,Aviv Slobodkin,Shmuel Tomi Klein,Reut Tsarfaty,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: 提出基于QA-SRL框架的跨语言语义角色标注方法，通过翻译和对齐管道自动生成多语言训练数据，在希伯来语、俄语和法语上超越GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 谓词-论元关系的显式表示是语义分析的基础，但现有方法依赖昂贵的标注且主要局限于英语。需要一种高效扩展语义标注到多语言的方法。

Method: 采用QA-SRL框架作为自然语言接口，通过跨语言投影方法：重用英语QA-SRL解析器，结合约束翻译和词对齐管道，自动生成与目标语言谓词对齐的问答标注。

Result: 在希伯来语、俄语和法语（涵盖不同语系）上生成了高质量训练数据，训练的语言特定解析器超越了GPT-4o、LLaMA-Maverick等多语言LLM基线。

Conclusion: QA-SRL作为可转移的自然语言语义接口，能够实现跨语言的高效谓词-论元解析，使语义分析更广泛可及。

Abstract: Explicit representations of predicate-argument relations form the basis of interpretable semantic analysis, supporting reasoning, generation, and evaluation. However, attaining such semantic structures requires costly annotation efforts and has remained largely confined to English. We leverage the Question-Answer driven Semantic Role Labeling (QA-SRL) framework -- a natural-language formulation of predicate-argument relations -- as the foundation for extending semantic annotation to new languages. To this end, we introduce a cross-linguistic projection approach that reuses an English QA-SRL parser within a constrained translation and word-alignment pipeline to automatically generate question-answer annotations aligned with target-language predicates. Applied to Hebrew, Russian, and French -- spanning diverse language families -- the method yields high-quality training data and fine-tuned, language-specific parsers that outperform strong multilingual LLM baselines (GPT-4o, LLaMA-Maverick). By leveraging QA-SRL as a transferable natural-language interface for semantics, our approach enables efficient and broadly accessible predicate-argument parsing across languages.

</details>


### [34] [Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference](https://arxiv.org/abs/2602.22868)
*Yushi Ye,Feng Hong,Huangjie Zheng,Xu Chen,Zhiyong Chen,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: ReMix通过引入连续混合状态来缓解扩散大语言模型中的组合矛盾问题，实现2-8倍推理加速且不损失质量


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在并行解码时面临严重的质量-速度权衡问题，这源于"组合矛盾"现象——并行生成的token会形成语义不一致的组合

Method: 提出ReMix框架，引入连续混合状态作为掩码状态和最终解码token状态之间的中间状态，允许token表示在连续空间中迭代优化，并通过拒绝规则将不确定表示回退到掩码状态重新处理

Result: ReMix作为无需训练的方法，在实验中实现了2-8倍的推理加速，且没有任何质量下降

Conclusion: 通过将连续表示整合到离散解码过程中，ReMix有效缓解了组合矛盾问题，显著提升了扩散大语言模型的推理效率

Abstract: Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \times$ inference speedup without any quality degradation.

</details>


### [35] [Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching](https://arxiv.org/abs/2602.22871)
*Roy Miles,Aysim Toker,Andreea-Maria Oncescu,Songcen Xu,Jiankang Deng,Ismail Elezi*

Main category: cs.CL

TL;DR: 提出Stitching Noisy Diffusion Thoughts框架，通过扩散采样生成多样化推理轨迹，使用过程奖励模型评分步骤，跨轨迹缝合高质量步骤形成复合推理，最后用自回归模型生成答案。


<details>
  <summary>Details</summary>
Motivation: 现有聚合策略通常是轨迹级别的（如选择最佳轨迹或投票最终答案），会丢弃部分或"接近正确"尝试中的有用中间工作。需要一种能重用步骤级候选的自我一致性框架。

Method: 1) 使用掩码扩散语言模型采样多样化、低成本的推理轨迹；2) 用现成的过程奖励模型（PRM）评分每个中间步骤；3) 跨轨迹缝合最高质量步骤形成复合推理；4) 用自回归模型基于该推理重新计算最终答案。

Result: 在数学推理基准测试中，步骤级重组在更难问题上最有益。训练免费框架在六个数学和编程任务上平均准确率提升达23.8%，同时相对于传统扩散模型和统一架构实现达1.8倍的延迟减少。

Conclusion: 模块化管道将探索（扩散）与评估和解决方案合成分离，避免单一统一混合，同时保持广泛搜索。最终自回归求解器在将缝合但不完美的推理转换为准确答案中起关键作用。

Abstract: Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or "nearly correct" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.

</details>


### [36] [Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models](https://arxiv.org/abs/2602.22918)
*Jonathan Steinberg,Oren Gal*

Main category: cs.CL

TL;DR: 研究通过因果干预分析三种VLM架构的OCR信息处理路径，发现不同架构的OCR瓶颈位置不同，OCR信号低维且可迁移，在某些模块化架构中移除OCR反而能提升计数性能。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型中OCR信息在语言处理流中的具体进入位置和路径，理解不同架构如何处理图像中的文本信息。

Method: 使用因果干预方法，通过比较原始图像和文本修复版本之间的激活差异，分析三种VLM架构家族（Qwen3-VL、Phi-4、InternVL3.5）的OCR处理机制。

Result: 发现不同架构的OCR瓶颈位置不同：DeepStack模型（Qwen）在中等深度（约50%）对场景文本最敏感，而单阶段投影模型（Phi-4、InternVL）在早期层（6-25%）最敏感。OCR信号低维（PC1解释72.9%方差），且PCA方向在不同数据集间可迁移。在某些模块化架构中，移除OCR反而能提升计数性能达6.9个百分点。

Conclusion: VLM中的OCR处理路径具有架构特异性，OCR信号是低维且可迁移的，在某些情况下OCR信息可能干扰其他视觉处理任务，特别是在模块化架构中。

Abstract: Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.

</details>


### [37] [Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention](https://arxiv.org/abs/2602.23057)
*Jeongin Bae,Baeseong Park,Gunho Park,Minsub Kim,Joonhyung Lee,Junhee Yoo,Sunghyeon Woo,Jiwon Ryu,Se Jung Kwon,Dongsoo Lee*

Main category: cs.CL

TL;DR: 论文提出Affine-Scaled Attention，通过引入输入相关的缩放和偏置项来放松softmax的严格归一化约束，改善Transformer注意力机制。


<details>
  <summary>Details</summary>
Motivation: 传统的softmax归一化注意力存在局限性：强制单位总和归一化限制了注意力幅度的灵活性，可能导致训练过程中注意力模式过于集中或不稳定。现有方法如注意力汇或门控机制只能提供有限或间接的控制。

Method: 提出Affine-Scaled Attention，在标准注意力基础上引入输入相关的缩放和对应的偏置项，应用于softmax归一化的注意力权重。这种设计放松了严格的归一化约束，同时保持值表示的聚合，允许模型以可控方式调整注意力的相对分布和尺度。

Result: 在大规模语言模型预训练中，相比标准softmax注意力和注意力汇基线，Affine-Scaled Attention在训练稳定性、优化行为和下游任务性能方面都表现出持续改进。

Conclusion: 适度重新加权注意力输出为改进Transformer模型中的注意力行为提供了一种实用有效的方法，Affine-Scaled Attention通过放松归一化约束实现了更好的控制灵活性。

Abstract: Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.
  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.

</details>


### [38] [Toward Automatic Filling of Case Report Forms: A Case Study on Data from an Italian Emergency Department](https://arxiv.org/abs/2602.23062)
*Gabriela Anna Kaczmarek,Pietro Ferrazzi,Lorenzo Porta,Vicky Rubini,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本文介绍了一个意大利急诊科临床笔记数据集，用于自动填写病例报告表（CRF），并进行了初步实验验证零样本方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 临床研究中CRF数据收集至关重要，但缺乏标注数据限制了基于大语言模型的自动CRF填写技术的发展。

Method: 创建了意大利急诊科临床笔记数据集，包含134个CRF项目的标注，定义了CRF填写任务和评估指标，并使用开源SOTA大语言模型进行零样本实验。

Result: 实验表明：(1) 意大利语临床笔记的CRF填写可以在零样本设置下进行；(2) LLMs存在偏差（如倾向于"未知"答案），需要纠正。

Conclusion: 该研究为CRF自动填写提供了数据集和初步验证，指出了LLMs在实际应用中的偏差问题，为后续研究奠定了基础。

Abstract: Case Report Forms (CRFs) collect data about patients and are at the core of well-established practices to conduct research in clinical settings. With the recent progress of language technologies, there is an increasing interest in automatic CRF-filling from clinical notes, mostly based on the use of Large Language Models (LLMs). However, there is a general scarcity of annotated CRF data, both for training and testing LLMs, which limits the progress on this task. As a step in the direction of providing such data, we present a new dataset of clinical notes from an Italian Emergency Department annotated with respect to a pre-defined CRF containing 134 items to be filled. We provide an analysis of the data, define the CRF-filling task and metric for its evaluation, and report on pilot experiments where we use an open-source state-of-the-art LLM to automatically execute the task. Results of the case-study show that (i) CRF-filling from real clinical notes in Italian can be approached in a zero-shot setting; (ii) LLMs' results are affected by biases (e.g., a cautious behaviour favours "unknown" answers), which need to be corrected.

</details>


### [39] [Quantity Convergence, Quality Divergence: Disentangling Fluency and Accuracy in L2 Mandarin Prosody](https://arxiv.org/abs/2602.23071)
*Yuqi Shi,Hao Yang,Xiyao Lu,Jinsong Zhang*

Main category: cs.CL

TL;DR: 本研究通过对比母语者和二语学习者，发现二语习得中句法-韵律界面的习得呈现非线性特征：高水平学习者在韵律边界数量上接近母语者，但在结构映射上存在显著偏差，形成了颠倒的韵律层级模式。


<details>
  <summary>Details</summary>
Motivation: 二语学习者虽然能习得目标语言的句法词序，但将句法映射到适当的韵律结构上仍然是一个持续存在的挑战。本研究旨在调查二语句法-韵律界面的石化现象和稳定性。

Method: 使用BLCU-SAIT语料库，比较67名母语为汉语的普通话者和67名越南语学习者。通过结合C-ToBI边界标注和依存语法分析，考察韵律边界的数量及其与句法关系的映射。

Result: 结果显示非线性习得模式：高水平学习者在主要短语层面(B3)的边界数量上接近母语基线，但其结构映射显著偏离。具体表现为：在主语-动词接口处降低韵律边界等级(B3→B1)，而在动词-宾语接口处错误地提升边界等级(B1→B3)，形成了颠倒的韵律层级模式。

Conclusion: 二语学习者为了维持较长的短语输出而牺牲了结构准确性，导致韵律层级模式被扭曲。这表明句法-韵律界面的习得存在深层次的结构性问题，而不仅仅是表面特征的习得。

Abstract: While second language (L2) learners may acquire target syntactic word order, mapping this syntax onto appropriate prosodic structures remains a persistent challenge. This study investigates the fossilization and stability of the L2 syntax-prosody interface by comparing 67 native Mandarin speakers with 67 Vietnamese learners using the BLCU-SAIT corpus. By integrating C-ToBI boundary annotation with Dependency Grammar analysis, we examined both the quantity of prosodic boundaries and their mapping to syntactic relations. Results reveal a non-linear acquisition: although high-proficiency learners (VNH) converge to the native baseline in boundary quantity at the Major Phrase level (B3), their structural mapping significantly diverges. Specifically, VNH demote the prosodic boundary at the Subject-Verb (SBV) interface (Major Phrase B3 -> Prosodic Word B1), while erroneously promoting the boundary at the Verb-Object (VOB) interface (Prosodic Word B1 -> Major Phrase B3). This strategy allows learners to maintain high long phrasal output at the expense of structural accuracy. This results in a distorted prosodic hierarchy where the native pattern is inverted.

</details>


### [40] [CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery](https://arxiv.org/abs/2602.23075)
*Mengze Hong,Di Jiang,Chen Jason Zhang,Zichang Guo,Yawen Li,Jun Chen,Shaobo Cui,Zhiyang Su*

Main category: cs.CL

TL;DR: CiteLLM是一个专门用于可信参考文献发现的智能代理平台，通过在LaTeX编辑器中嵌入LLM功能，实现本地化、无幻觉的参考文献检索，确保学术诚信和数据隐私。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）能提高学术活动效率，但在AI辅助的伦理部署方面仍存在挑战：1）AI生成内容的可信度问题；2）学术诚信和知识产权保护；3）信息隐私保护。需要一种可信的参考文献发现系统来支持作者起草的论点。

Method: 1）在LaTeX编辑器中直接嵌入LLM功能，实现无缝用户体验且数据不离开本地系统；2）采用动态学科感知路由，仅从可信的基于网络的学术库中检索候选文献；3）LLM仅用于生成上下文感知的搜索查询、按相关性排序候选文献，以及通过段落级语义匹配和集成聊天机器人进行验证和解释支持。

Result: 评估结果表明，所提出的系统在返回有效且高度可用的参考文献方面表现出优越性能。

Conclusion: CiteLLM通过创新的交互范式解决了LLM在学术辅助中的伦理挑战，实现了可信、无幻觉的参考文献发现，同时保护了学术诚信和数据隐私。

Abstract: Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.

</details>


### [41] [Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent](https://arxiv.org/abs/2602.23079)
*Boyang Zhang,Yang Zhang*

Main category: cs.CL

TL;DR: 论文提出SALA方法，结合风格计量特征与LLM推理，用于评估和减轻新闻文章作者身份推断风险，并通过引导重写策略降低可识别性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展增强了作者身份推断能力，引发了新闻文章等文本数据中意外去匿名化的隐私风险担忧。

Method: 提出SALA（风格计量辅助的LLM分析）方法，将定量风格计量特征与LLM推理相结合，构建结构化、可解释的管道，并采用引导重写策略生成改写提示。

Result: 在大规模新闻数据集上的实验表明，SALA方法（特别是增强数据库模块后）在各种场景下实现了高推理准确率，同时重写策略有效降低了作者身份可识别性。

Conclusion: 研究既揭示了LLM代理的去匿名化潜力，也强调了可解释、主动防御机制对于保护作者隐私的重要性。

Abstract: The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.

</details>


### [42] [Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs](https://arxiv.org/abs/2602.23136)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 多模态LLM能处理语音和图像，但无法感知说话者声音或物体纹理。研究发现这不是编码问题，而是解码器与模态信息不匹配的问题——解码器只能提取文本对齐方向的信息。


<details>
  <summary>Details</summary>
Motivation: 探索为什么多模态LLM虽然能处理语音和图像，却无法有效感知说话者身份、情感和视觉纹理等细节信息，尽管这些信息在编码层中确实存在。

Method: 通过线性探针分析信息在LLM各层的保留情况，提出广义互信息(GMI)理论框架，在5个模型上进行验证，并通过对比实验和LoRA干预来确认瓶颈所在。

Result: 研究发现模态特定信息在LLM各层中确实存在（比随机高3-55倍），但移除64-71%的模态特定方差反而改善解码损失。解码器只能提取文本对齐方向的信息，这是训练目标决定的瓶颈。

Conclusion: 多模态LLM的信息可访问性受解码器评分规则限制，而非编码器或投影问题。通过调整训练目标（如情感目标）可以改善特定信息的可访问性，证实训练目标决定什么信息变得可访问。

Abstract: Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.
  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.

</details>


### [43] [MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations](https://arxiv.org/abs/2602.23184)
*Sara Rosenthal,Yannis Katsis,Vraj Shah,Lihong He,Lucian Popa,Marina Danilevsky*

Main category: cs.CL

TL;DR: MTRAG-UN是一个用于探索多轮检索增强生成开放挑战的基准测试，包含666个任务、2800+对话轮次，涵盖6个领域，重点关注UNanswerable、UNderspecified、NONstandalone和UNclear等对话挑战。


<details>
  <summary>Details</summary>
Motivation: 当前检索和生成模型在多轮对话中仍面临诸多挑战，特别是在处理不可回答、未充分指定、非独立和响应不清晰等问题时表现不佳，需要专门的基准测试来评估和改进这些能力。

Method: 创建了MTRAG-UN基准测试，包含666个任务、超过2800个对话轮次，涵盖6个不同领域，并提供了相应的语料库。通过实验评估现有检索和生成模型在这些挑战性对话场景中的表现。

Result: 实验结果表明，现有的检索和生成模型在处理包含UNanswerable（不可回答）、UNderspecified（未充分指定）、NONstandalone（非独立）和UNclear（不清晰）等问题的对话时仍然存在困难。

Conclusion: MTRAG-UN基准测试揭示了多轮检索增强生成中的关键挑战，为未来研究提供了评估工具，有助于推动模型在处理复杂对话场景方面的改进。

Abstract: We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark

</details>


### [44] [Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?](https://arxiv.org/abs/2602.23225)
*Pengxiang Li,Dilxat Muhtar,Lu Yin,Tianlong Chen,Shiwei Liu*

Main category: cs.CL

TL;DR: 提出NAP方法，通过数据中心的训练策略改善扩散语言模型的非自回归并行生成能力，减少对自回归解码的依赖


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型虽然理论上支持并行生成，但在实践中往往收敛到类似自回归的解码方式，存在顺序瓶颈。这种不匹配源于训练目标与高度顺序化训练数据（如标准预训练语料和长链思维监督）之间的不一致

Method: 提出NAP方法：1）数据层面：构建多个独立的推理轨迹作为训练示例；2）解码策略：采用并行强制解码策略，鼓励多token并行更新；3）通过数据与监督的重新设计来更好地对齐非自回归并行解码

Result: 在数学推理基准测试中，NAP在并行解码下比使用标准长链思维数据训练的扩散语言模型表现更好，且随着并行度增加，性能优势更加明显

Conclusion: 重新审视数据和监督是减少自回归行为、实现真正非自回归并行生成的原则性方向，NAP为这一方向提供了概念验证

Abstract: Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at https://github.com/pixeli99/NAP.

</details>


### [45] [Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems](https://arxiv.org/abs/2602.23266)
*Siyuan Liu,Jiahui Xu,Feng Jiang,Kuang Wang,Zefeng Zhao,Chu-Ren Huang,Jinghang Gu,Changqing Yin,Haizhou Li*

Main category: cs.CL

TL;DR: DDTSR框架通过连接词引导的大小模型协同、流式跨模态协作和课程学习增强，显著降低口语对话系统响应延迟，实现边听边思考、边思考边说话


<details>
  <summary>Details</summary>
Motivation: 传统ASR-LLM-TTS流水线采用严格顺序处理，需要完整转录和推理后才能开始语音合成，导致响应延迟高，无法实现人类般的实时交互

Method: 提出DDTSR框架，包含三个关键机制：1) 连接词引导的大小模型协同，小模型生成最小承诺的话语连接词，大模型并行进行知识密集型推理；2) 流式跨模态协作，动态重叠ASR、LLM推理和TTS处理；3) 课程学习增强话语连续性，保持早期响应与后续推理的一致性

Result: 在两个口语对话基准测试中，DDTSR将响应延迟降低19%-51%，同时保持话语质量；框架可作为即插即用模块兼容多种LLM骨干，在不同话语长度下保持鲁棒性

Conclusion: DDTSR通过创新的并行处理架构显著降低了口语对话系统的响应延迟，实现了更接近人类交互的实时响应能力，具有强大的实用性和可扩展性

Abstract: Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis shows that DDTSR functions as a plug-and-play module compatible with diverse LLM backbones, and remains robust across varying utterance lengths, indicating strong practicality and scalability for real-time spoken interaction.

</details>


### [46] [SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables](https://arxiv.org/abs/2602.23286)
*Sungho Park,Jueun Kim,Wook-Shin Han*

Main category: cs.CL

TL;DR: SPARTA是一个自动生成大规模表格-文本问答基准的框架，通过轻量级人工验证创建包含复杂多跳推理和聚合操作的高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 现有表格-文本问答基准规模小、手动标注易出错，且问题简单，很少需要多跳推理或聚合等高级操作，无法充分评估模型在真实场景中的复杂推理能力。

Method: 1) 构建参考事实数据库：通过从非结构化文本中自动提取原子事实来丰富源表格；2) 合成嵌套查询：生成与期望跳数匹配的嵌套谓词查询；3) 提出两种新技术：基于来源的查询重写确保SQL可执行，后序遍历约束确保问题结构自然流畅。

Result: SPARTA生成了数千个高质量问答对，覆盖聚合、分组和深度多跳推理。在SPARTA上，在HybridQA上达到70+F1或在OTT-QA上达到50+F1的SOTA模型性能下降超过30个F1点，暴露了当前跨模态推理的根本弱点。

Conclusion: SPARTA框架能够高效生成大规模、高质量的表格-文本问答基准，揭示了当前模型在复杂跨模态推理任务上的严重不足，为未来研究提供了重要评估工具。

Abstract: Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.

</details>


### [47] [A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations](https://arxiv.org/abs/2602.23300)
*Soumya Dutta,Smruthi Balaji,Sriram Ganapathy*

Main category: cs.CL

TL;DR: MiSTER-E是一个用于对话情感识别的混合专家框架，通过分离模态特定上下文建模和多模态信息融合，利用语音和文本LLM增强表示，无需说话人身份信息，在多个基准数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别面临两个核心挑战：需要捕捉多轮对话的时间动态，以及有效整合多模态信息。现有方法通常将这两个问题耦合处理，限制了模型性能。

Method: 提出MiSTER-E混合专家框架：1) 使用语音和文本LLM获取丰富的utterance级嵌入；2) 通过卷积-循环层增强上下文建模；3) 三个专家（语音、文本、跨模态）通过门控机制动态加权集成；4) 引入监督对比损失和KL散度正则化促进模态对齐。

Result: 在IEMOCAP、MELD、MOSI三个数据集上分别达到70.9%、69.5%、87.9%的加权F1分数，超越了多个基线语音-文本ERC系统。消融实验验证了各组件贡献。

Conclusion: MiSTER-E通过解耦模态特定上下文建模和多模态融合，有效提升了对话情感识别性能，且不依赖说话人身份信息，为多模态ERC提供了新的框架思路。

Abstract: Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.

</details>


### [48] [Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning](https://arxiv.org/abs/2602.23351)
*Amita Kamath,Jack Hessel,Khyathi Chandu,Jena D. Hwang,Kai-Wei Chang,Ranjay Krishna*

Main category: cs.CL

TL;DR: 研究发现视觉语言模型缺乏推理能力源于训练数据中的报告偏差，这种偏差导致空间、时间、否定和计数等推理技能在数据中代表性不足，即使扩大数据规模也无法自动获得这些能力，但通过专门收集包含隐含信息的标注可以有效提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型缺乏推理能力的问题一直备受关注，作者认为这源于训练数据中的报告偏差——人们在描述视觉内容时通常会省略隐含信息，导致模型无法学习到某些类型的推理技能。

Method: 从语用学理论角度分析OpenCLIP、LLaVA-1.5和Molmo等流行视觉语言模型的训练数据，识别报告偏差导致的推理技能缺失。通过精心设计的基准测试评估模型在空间、时间、否定和计数推理方面的表现，并测试数据规模、模型规模和语言扩展的影响，最后验证专门收集包含隐含信息的标注数据的效果。

Result: 研究发现：1）视觉语言模型在报告偏差抑制的推理类型上表现不佳；2）与普遍认知相反，扩大数据规模、模型规模和多语言训练并不能自动获得这些推理能力；3）但通过专门收集包含隐含信息的标注数据可以有效提升模型的推理能力。

Conclusion: 研究强调需要更有意识的训练数据构建方法，而不是依赖规模扩展来获得推理能力。报告偏差是视觉语言模型推理能力不足的根本原因，需要通过专门的数据收集策略来解决这一问题。

Abstract: The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., "at the game today!" is a more likely caption than "a photo of 37 people standing behind a field". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [49] [A Mission Engineering Framework for Uncrewed Aerial Vehicle Design in GNSS-Denied Environments for Intelligence, Surveillance, and Reconnaissance Mission Sets](https://arxiv.org/abs/2602.22380)
*Alfonso Sciacchitano,Douglas L. Van Bossuyt*

Main category: eess.SY

TL;DR: 提出一个用于低SWaP-C无人机ISR架构早期设计的任务工程框架，通过多目标优化和仿真量化性能、成本与鲁棒性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 小型低成本无人机在ISR任务中应用日益广泛，但其设计存在续航短、载荷有限、传感器简单等挑战。传统平台中心方法无法捕捉系统级性能、成本与协调之间的耦合权衡。

Method: 集成实验设计、多目标优化和高保真仿真的闭环框架，使用拉丁超立方采样和遗传算法探索架构，通过蒙特卡洛试验评估联邦卡尔曼滤波器性能，采用验证方方法进行验证。

Result: 在GNSS拒止海上环境中的人员落水定位案例研究表明，定位精度在亚米级达到饱和，更高成本配置主要增加冗余性和弹性。

Conclusion: 该框架量化了任务性能、经济性和鲁棒性之间的权衡，为资源受限的ISR任务提供了可扩展的决策支持工具。

Abstract: Small, low-size, weight, power, and cost (SWaP-C) uncrewed aerial vehicles (UAVs) are increasingly used for intelligence, surveillance, and reconnaissance (ISR) missions due to their affordability, attritability, and suitability for distributed operations. However, their design poses challenges including limited endurance, constrained payload capacity, and reliance on simple sensing modalities such as fixed-field-of-view, bearing-only cameras. Traditional platform-centric methods cannot capture the coupled performance, cost, and coordination trade-offs that emerge at the system-of-systems level.
  This paper presents a mission engineering framework for early-phase design of low-SWaP-C UAV ISR architectures. The framework integrates design of experiments, multi-objective optimization, and high-fidelity simulation into a closed-loop process linking design variables to estimator-informed performance and mission cost. Candidate architectures are explored via Latin hypercube sampling and refined using a genetic algorithm, with performance evaluated through Monte Carlo trials of a federated Kalman filter benchmarked against the posterior Cramer-Rao lower bound. Validation follows the Validation Square methodology, combining theoretical, empirical, and structural assessments.
  A case study on man-overboard localization in a GNSS-denied maritime environment shows that localization accuracy saturates at sub-meter levels, while higher-cost configurations primarily add redundancy and resilience. The framework thus quantifies mission trade-offs between performance, affordability, and robustness, providing a scalable decision-support tool for contested, resource-constrained ISR missions.

</details>


### [50] [Small HVAC Control Demonstrations in Larger Buildings Often Overestimate Savings](https://arxiv.org/abs/2602.22499)
*Arash J. Khabbazi,Kevin J. Kircher*

Main category: eess.SY

TL;DR: 论文分析了在建筑节能控制研究中，仅测量受控区域而忽略相邻区域热传递会导致节能效果被高估的问题，并提出了一种基于温度测量的新估计方法。


<details>
  <summary>Details</summary>
Motivation: 当前建筑节能控制研究中，研究人员通常只控制多区域建筑中的少数热区，并仅报告受控区域的节能效果。这种方法会因忽略受控区域与相邻区域之间的热传递而高估实际节能效果，需要更准确的评估方法。

Method: 论文首先从数学上描述了在线性动态和热负荷线性目标函数情况下的高估误差特性，然后提出了一种基于受控区域和相邻区域温度测量的替代估计方法。新方法不需要估算建筑在基准运行下的能耗，避免了基准估计的测量和验证挑战。

Result: 研究发现，即使在看似无害的情况下，高估误差也可能很大。例如，当仅控制没有直接与室外热接触的内部区域时，所有感知到的节能都是虚构的。新方法能够更准确地估计实际节能效果。

Conclusion: 论文强调了在建筑节能控制研究中考虑区域间热传递的重要性，并提出了一种更可靠的节能效果估计方法，避免了传统方法中基准能耗估计的挑战，为建筑节能控制的实际效果评估提供了更准确的工具。

Abstract: How much energy, money, and emissions can advanced control of heating and cooling equipment save in real buildings? To address this question, researchers sometimes control a small number of thermal zones within a larger multi-zone building, then report savings for the controlled zones only. That approach can overestimate savings by neglecting heat transfer between controlled zones and adjacent zones. This paper mathematically characterizes the overestimation error when the dynamics are linear and the objectives are linear in the thermal load, as usually holds when optimizing energy efficiency, energy costs, or emissions. Overestimation errors can be large even in seemingly innocuous situations. For example, when controlling only interior zones that have no direct thermal contact with the outdoors, all perceived savings are fictitious. This paper provides an alternative estimation method based on the controlled and adjacent zones' temperature measurements. The new method does not require estimating how much energy the building would have used under baseline operations, so it removes the additional measurement and verification challenge of accurate baseline estimation.

</details>


### [51] [HyperKKL: Enabling Non-Autonomous State Estimation through Dynamic Weight Conditioning](https://arxiv.org/abs/2602.22630)
*Yahia Salaheldin Shaaban,Salem Lahlou,Abdelrahman Sayed Sayed*

Main category: eess.SY

TL;DR: HyperKKL：一种基于超网络架构的学习方法，用于为非自治非线性系统设计KKL观测器，能够根据外部输入信号即时生成观测器参数，解决了现有方法无法泛化到驱动系统的问题。


<details>
  <summary>Details</summary>
Motivation: KKL观测器为非线性系统提供了严谨的理论框架，但实际实现需要求解解析上难以处理的偏微分方程。现有的基于学习的KKL观测器近似方法主要针对自治系统设计，无法泛化到驱动系统，需要昂贵的重新训练或在线梯度更新。

Method: 采用超网络架构，将外生输入信号编码并即时生成KKL观测器的参数，学习一个由外部驱动参数化的浸入映射族。与仅通过训练启发式方法从自治系统泛化的课程学习策略进行对比评估。

Result: 在四个基准数值模拟中进行了验证，包括Duffing、Van der Pol、Lorenz和Rössler系统。超网络方法能够有效学习参数化的浸入映射族，实现对外部驱动系统的泛化。

Conclusion: HyperKKL通过超网络架构成功解决了现有学习型KKL观测器无法泛化到非自治系统的问题，为非线性系统的观测器设计提供了有效的学习框架，避免了昂贵的重新训练需求。

Abstract: This paper proposes HyperKKL, a novel learning approach for designing Kazantzis-Kravaris/Luenberger (KKL) observers for non-autonomous nonlinear systems. While KKL observers offer a rigorous theoretical framework by immersing nonlinear dynamics into a stable linear latent space, its practical realization relies on solving Partial Differential Equations (PDE) that are analytically intractable. Current existing learning-based approximations of the KKL observer are mostly designed for autonomous systems, failing to generalize to driven dynamics without expensive retraining or online gradient updates. HyperKKL addresses this by employing a hypernetwork architecture that encodes the exogenous input signal to instantaneously generate the parameters of the KKL observer, effectively learning a family of immersion maps parameterized by the external drive. We rigorously evaluate this approach against a curriculum learning strategy that attempts to generalize from autonomous regimes via training heuristics alone. The novel approach is illustrated on four numerical simulations in benchmark examples including the Duffing, Van der Pol, Lorenz, and Rössler systems.

</details>


### [52] [Toward Wireless Human-Machine Collaboration in the 6G Era](https://arxiv.org/abs/2602.22662)
*Gaoyang Pang,Wanchun Liu,Chentao Yue,Daniel E. Quevedo,Karl H. Johansson,Branka Vucetic,Yonghui Li*

Main category: eess.SY

TL;DR: 该论文探讨了无线人机协作在工业5.0中的关键作用，提出了WHMC架构、性能指标、设计方法，并通过案例研究展示了6G网络如何支持灵活部署的人机协作系统。


<details>
  <summary>Details</summary>
Motivation: 工业5.0需要人机协作来结合人类创造力与机器优势，而无线通信特别是6G网络能够实现灵活、可扩展、低成本的地理分布式人机协作系统部署。

Method: 论文首先介绍无线人机协作的通用架构和关键组件，然后分析网络拓扑和应用场景，提出新的性能指标和设计方法，总结通信需求并评估现有技术，最后通过概念验证案例进行研究。

Result: 论文系统性地建立了无线人机协作的理论框架，识别了关键性能指标，评估了现有技术对WHMC的支持能力，并通过案例验证了可行性，为工业5.0的实现提供了技术路线。

Conclusion: 无线人机协作是工业5.0的核心使能技术，6G网络将为其提供关键支持，但仍面临诸多开放挑战需要进一步研究解决。

Abstract: The next industrial revolution, Industry 5.0, will be driven by advanced technologies that foster human-machine collaboration (HMC). It will leverage human creativity, judgment, and dexterity with the machine's strength, precision, and speed to improve productivity, quality of life, and sustainability. Wireless communications, empowered by the emerging capabilities of sixth-generation (6G) wireless networks, will play a central role in enabling flexible, scalable, and low-cost deployment of geographically distributed HMC systems. In this article, we first introduce the generic architecture and key components of wireless HMC (WHMC). We then present the network topologies of WHMC and highlight impactful applications across various industry sectors. Driven by the prospective applications, we elaborate on new performance metrics that researchers and practitioners may consider during the exploration and implementation of WHMC and discuss new design methodologies. We then summarize the communication requirements and review promising state-of-the-art technologies that can support WHMC. Finally, we present a proof-of-concept case study and identify several open challenges.

</details>


### [53] [Opacity in Discrete Event Systems: A Perspective and Overview](https://arxiv.org/abs/2602.22713)
*Xiang Yin*

Main category: eess.SY

TL;DR: 该论文提供了离散事件系统中不透明性概念的全面综述，涵盖核心定义、验证技术、执行机制、扩展模型和应用领域


<details>
  <summary>Details</summary>
Motivation: 不透明性已成为离散事件系统中信息流安全的核心保密概念，但缺乏对新手友好的系统性综述。本文旨在为初学者提供简明概述，强调核心定义和统一估计视角

Method: 采用综述研究方法，首先介绍不透明性的核心定义和统一估计视角，然后总结代表性验证技术，分析不同观测模型对问题表述和算法结构的影响，最后回顾主要执行范式

Result: 系统梳理了不透明性在离散事件系统中的理论基础、验证方法、执行机制，并扩展到随机系统、定时系统、Petri网等丰富模型，以及在机器人、位置隐私、信息服务等领域的应用

Conclusion: 本文为不透明性研究提供了全面综述，指出了未来研究方向，包括不可比较信息下的可解性、超越最坏情况复杂性的可扩展方法，以及智能或数据驱动对手下的不透明性等开放挑战

Abstract: Opacity has emerged as a central confidentiality notion for information-flow security in discrete event systems (DES), capturing the requirement that an external observer (intruder) should never be able to determine with certainty whether the system is, was, or will be in a secret state. This article provides a concise, newcomer-friendly overview of opacity in DES, emphasizing core definitions and the unifying estimation viewpoint behind major opacity notions,. We summarize representative verification techniques and highlight how different observation models reshape both the problem formulation and algorithmic structure. We then review principal enforcement paradigms, ranging from opacity-enforcing supervisory control to sensor activation/information release optimization and obfuscation/editing mechanisms. Beyond finite automata, we outline how opacity has been studied in richer models such as stochastic systems, timed systems, Petri nets, and continuous/hybrid dynamics, and we briefly survey applications in robotics, location privacy, and information services. Finally, we discuss selected open challenges, including solvability under incomparable information, scalable methods beyond worst-case complexity, and opacity under intelligent or data-driven adversaries.

</details>


### [54] [Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation](https://arxiv.org/abs/2602.22774)
*Maryam Ansarifard,Mohit K. Sharma,Kishor C. Joshi,George Exarchakos*

Main category: eess.SY

TL;DR: 本文提出了一种基于Transformer增强PPO的深度强化学习框架，用于优化NOMA多用户上行网络中的信息年龄最小化问题，通过注意力机制捕捉用户间依赖关系，显著降低了平均AoI。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和工业自动化等新兴应用需要超可靠低延迟通信，其中保持信息新鲜度至关重要。多用户NOMA上行网络中，用户存在任务大小、AoI阈值和惩罚敏感度的异构性，同时需要满足NOMA调度约束，传统方法难以有效处理这些复杂因素。

Method: 提出基于近端策略优化（PPO）的深度强化学习框架，并集成Transformer编码器。注意力机制使智能体能聚焦关键用户状态并捕捉用户间依赖关系，提升策略性能和可扩展性。系统处理用户异构性（任务大小、AoI阈值、惩罚敏感度）和NOMA调度约束。

Result: 大量仿真表明，该方法相比基线显著降低了平均AoI。分析训练过程中注意力权重演化发现，模型逐步学会优先处理高重要性用户。注意力图显示有意义的结构：早期策略呈现均匀注意力，后期则形成与用户优先级和NOMA约束一致的有聚焦模式。

Conclusion: 注意力驱动的深度强化学习为下一代无线系统中的智能、优先级感知资源分配提供了有前景的解决方案，能够有效处理用户异构性和NOMA约束，实现信息年龄最小化。

Abstract: Emerging applications such as autonomous driving and industrial automation demand ultra-reliable and low-latency communication (URLLC), where maintaining fresh and timely information is critical. A key performance metric in such systems is the age of information (AoI). This paper addresses AoI minimization in a multi-user uplink wireless network using non-orthogonal multiple access (NOMA), where users offload tasks to a base station. The system must handle user heterogeneity in task sizes, AoI thresholds, and penalty sensitivities, while adhering to NOMA constraints on user scheduling. We propose a deep reinforcement learning (DRL) framework based on proximal policy optimization (PPO), enhanced with a Transformer encoder. The attention mechanism allows the agent to focus on critical user states and capture inter-user dependencies, improving policy performance and scalability. Extensive simulations show that our method reduces average AoI compared to baselines. We also analyze the evolution of attention weights during training and observe that the model progressively learns to prioritize high-importance users. Attention maps reveal meaningful structure: early-stage policies exhibit uniform attention, while later stages show focused patterns aligned with user priority and NOMA constraints. These results highlight the promise of attention-driven DRL for intelligent, priority-aware resource allocation in next-generation wireless systems.

</details>


### [55] [Steady State Covariance Steering via Sparse Intervention](https://arxiv.org/abs/2602.22939)
*Yosuke Inoue,Masaki Inoue*

Main category: eess.SY

TL;DR: 提出基于结构干预的线性系统稳态协方差控制方法，通过KL散度最小化和L1正则化实现稀疏干预


<details>
  <summary>Details</summary>
Motivation: 解决线性动力系统的稳态协方差控制问题，通过结构干预（修改系统矩阵）来实现目标协方差分布，同时保持干预的稀疏性以降低实施成本

Method: 将协方差控制问题转化为稳态与目标高斯分布间KL散度最小化问题，提出近端梯度算法，结合L1正则化促进结构干预的稀疏性，关键贡献是推导了KL散度对干预矩阵梯度的解析表达式

Result: 数值模拟表明，近端梯度算法能有效识别稀疏、结构约束的干预策略，实现精确的协方差控制

Conclusion: 该方法通过结构干预和稀疏性约束，为线性系统的稳态协方差控制提供了一种有效的解决方案，特别适用于需要低成本干预的实际应用场景

Abstract: This paper addresses the steady state covariance steering for linear dynamical systems via structural intervention on the system matrix. We formulate the covariance steering problem as the minimization of the Kullback-Leibler (KL) divergence between the steady state and target Gaussian distributions. To solve the problem, we develop a solution method, hereafter referred to as the proximal gradient-based algorithm, of promoting sparsity in the structural intervention by integrating the objective into a proximal gradient framework with L1 regularization. The main contribution of this paper lies in the analytical expression of the KL divergence gradient with respect to the intervention matrix: the gradient is characterized by the solutions to two Lyapunov equations related to the state covariance equation and its adjoint. Numerical simulations demonstrate that the proximal gradient-based algorithm effectively identifies sparse, structurally-constrained interventions to achieve precise covariance steering.

</details>


### [56] [Integrated Flight and Propulsion Control for Fixed-Wing UAVs via Thrust and Disturbance Compensation](https://arxiv.org/abs/2602.23052)
*Chong-Yi Sun,Heling Yuan,Xu Fang,Yan He,Xi-Ming Sun*

Main category: eess.SY

TL;DR: 提出了一种用于配备涡轮喷气发动机的固定翼无人机的分层控制框架，通过集成飞行和推进控制方案解决位置跟踪问题，包含推力与扰动补偿。


<details>
  <summary>Details</summary>
Motivation: 研究配备涡轮喷气发动机的固定翼无人机的位置跟踪控制问题，需要同时处理未建模动态和外部扰动，传统控制方法难以有效应对这些复杂因素。

Method: 1) 建立包含涡轮喷气发动机动态的扰动固定翼无人机模型；2) 设计通用扩展观测器处理不可测推力动态和外部扰动；3) 实现分层控制框架，使用三个基于观测器的控制器保证位置跟踪性能。

Result: 理论证明闭环系统渐近收敛到期望轨迹，并通过对比仿真验证了所提控制策略的有效性。

Conclusion: 提出的分层控制框架能够有效解决配备涡轮喷气发动机的固定翼无人机的位置跟踪控制问题，通过集成推力与扰动补偿机制，实现了对未建模动态和外部扰动的鲁棒控制。

Abstract: This paper investigates the position-tracking control problem for fixed-wing unmanned aerial vehicles (UAVs) equipped with a turbojet engine via an integrated flight and propulsion control scheme. To this end, a hierarchical control framework with thrust and disturbance compensation is proposed. In particular, we first propose a perturbed fixed-wing UAV model with turbojet engine dynamics, accounting for both unmodeled dynamics and external disturbances. Second, a versatile extended observer is designed to handle both unmeasurable thrust dynamics and external disturbances. Third, a hierarchical control framework is implemented using three observer-based controllers to guarantee position-tracking performance. With the proposed control strategy, we prove that the closed-loop system asymptotically converges to the desired trajectory. Finally, a comparative simulation is performed to illustrate the proposed control strategy.

</details>


### [57] [Signal Temporal Logic Verification and Synthesis Using Deep Reachability Analysis and Layered Control Architecture](https://arxiv.org/abs/2602.23313)
*Joonwon Choi,Kartik Anand Pant,Youngim Nam,Henry Hellmann,Karthik Nune,Inseok Hwang*

Main category: eess.SY

TL;DR: 提出基于信号时序逻辑的框架，通过计算后向可达管验证任务可行性，并合成控制策略安全执行任务，计算效率提升约1000倍。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够严格验证STL描述任务可行性并合成控制策略的框架，确保安全可靠执行，同时处理未在环境信息中描述的障碍物意外行为。

Method: 1) 通过计算后向可达管验证STL可行性；2) 使用深度神经网络减轻可达性分析计算负担；3) 采用分层规划控制架构：MILP用于全局规划，MPC作为局部控制器。

Result: 框架能成功计算给定STL的后向可达管并执行任务，计算时间比基准方法减少约1000倍，能鲁棒处理未描述的障碍物意外行为。

Conclusion: 提出的STL框架能有效验证任务可行性并合成控制策略，通过神经网络加速和分层架构实现高效可靠的任务执行。

Abstract: We propose a signal temporal logic (STL)-based framework that rigorously verifies the feasibility of a mission described in STL and synthesizes control to safely execute it. The proposed framework ensures safe and reliable operation through two phases. First, the proposed framework assesses the feasibility of STL by computing a backward reachable tube (BRT), which captures all states that can satisfy the given STL, regardless of the initial state. The proposed framework accommodates the multiple reach-avoid (MRA) problem to address more general STL specifications and leverages a deep neural network to alleviate the computation burden for reachability analysis, reducing the computation time by about 1000 times compared to a baseline method. We further propose a layered planning and control architecture that combines mixed-integer linear programming (MILP) for global planning with model predictive control (MPC) as a local controller for the verified STL. Consequently, the proposed framework can robustly handle unexpected behavior of obstacles that are not described in the environment information or STL, thereby providing reliable mission performance. Our numerical simulations demonstrate that the proposed framework can successfully compute BRT for a given STL and perform the mission.

</details>


### [58] [Millimeter-Wave RIS: Hardware Design and System-Level Considerations](https://arxiv.org/abs/2602.23345)
*Ruiqi Wang,Pinjun Zheng,Yiming Yang,Xiarui Su,Mohammad Vaseem,Anas Chaaban,Md. Jahangir Hossain,Tareq Y. Al-Naffouri,Atif Shamim*

Main category: eess.SY

TL;DR: 毫米波可重构智能表面硬件设计综述：涵盖宽带实现、高分辨率相位量化、低成本印刷、光学透明、片上RIS及3D架构，讨论互耦、校准等挑战，为实际部署提供设计指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注信道建模和信号处理，但实际RIS部署根本上受硬件设计选择及其系统级影响制约。本文旨在从硬件角度综述毫米波RIS最新发展，为实际部署提供指导。

Method: 采用硬件中心视角，系统综述毫米波RIS的多种实现方案：宽带实现、高分辨率相位量化设计、全印刷低成本实现、光学透明表面、RIS-on-chip解决方案和新兴3D架构。

Result: 全面梳理了毫米波RIS硬件设计的关键进展和技术路线，识别了互耦、校准、多RIS交互和频率相关相位控制等核心挑战，为硬件实现与系统级优化搭建桥梁。

Conclusion: 本文为RIS硬件设计提供实用见解，旨在引导未来研究向可扩展、高效且实际可部署的智能表面架构发展，促进RIS技术从理论到实际应用的转化。

Abstract: Reconfigurable intelligent surfaces have emerged as a promising hardware platform for shaping wireless propagation environments at millimeter-wave (mm-Wave) frequencies and beyond. While many existing studies emphasize channel modeling and signal processing, practical RIS deployment is fundamentally governed by hardware design choices and their system-level implications. This paper presents a hardware-centric overview of recent mm-Wave RIS developments, covering wideband realizations, high-resolution phase-quantized designs, fully printed low-cost implementations, optically transparent surfaces, RIS-on-chip solutions, and emerging three-dimensional architectures. Key challenges including mutual coupling, calibration, multi-RIS interaction, and frequency-dependent phase control are discussed to bridge hardware realization with system-level optimization. This overview provides practical design insights and aims to guide future RIS research toward scalable, efficient, and practically deployable intelligent surface architectures.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [59] [LoBoost: Fast Model-Native Local Conformal Prediction for Gradient-Boosted Trees](https://arxiv.org/abs/2602.22432)
*Vagner Santos,Victor Coscrato,Luben Cabezas,Rafael Izbicki,Thiago Ramos*

Main category: stat.ML

TL;DR: LoBoost：基于梯度提升决策树的本地化保形预测方法，利用树结构进行多尺度校准，无需额外模型或数据分割


<details>
  <summary>Details</summary>
Motivation: 梯度提升决策树在表格回归中表现出色，但缺乏不确定性量化。传统保形预测使用全局残差分位数，在异方差情况下适应性差。现有改进方法需要拟合辅助模型或额外数据分割，成本高且数据效率低。

Method: LoBoost利用已训练集成模型的叶子结构定义多尺度校准组：每个输入通过访问的叶子序列编码；在分辨率级别k，根据前k棵树中叶子索引的前缀匹配对点分组，并在每个组内校准残差分位数。无需重新训练、辅助模型或额外数据分割。

Result: 实验显示具有竞争力的区间质量，在大多数数据集上改善了测试MSE，并实现了大幅的校准加速。

Conclusion: LoBoost提供了一种高效、自适应的本地化保形预测方法，利用现有模型结构实现不确定性量化，无需额外计算开销。

Abstract: Gradient-boosted decision trees are among the strongest off-the-shelf predictors for tabular regression, but point predictions alone do not quantify uncertainty. Conformal prediction provides distribution-free marginal coverage, yet split conformal uses a single global residual quantile and can be poorly adaptive under heteroscedasticity. Methods that improve adaptivity typically fit auxiliary nuisance models or introduce additional data splits/partitions to learn the conformal score, increasing cost and reducing data efficiency. We propose LoBoost, a model-native local conformal method that reuses the fitted ensemble's leaf structure to define multiscale calibration groups. Each input is encoded by its sequence of visited leaves; at resolution level k, we group points by matching prefixes of leaf indices across the first k trees and calibrate residual quantiles within each group. LoBoost requires no retraining, auxiliary models, or extra splitting beyond the standard train/calibration split. Experiments show competitive interval quality, improved test MSE on most datasets, and large calibration speedups.

</details>


### [60] [Flow Matching is Adaptive to Manifold Structures](https://arxiv.org/abs/2602.22486)
*Shivam Kumar,Yixin Wang,Lizhen Lin*

Main category: stat.ML

TL;DR: 本文从理论上分析了流匹配方法在目标分布位于光滑流形上的收敛性，证明了其收敛率接近极小极大最优，且仅依赖于内在维度，为流匹配适应数据几何结构提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 流匹配作为无模拟的生成建模方法，在高维数据（如文本到图像合成、视频生成等）中表现出色，但现有理论分析假设目标分布具有光滑的全维密度，无法解释其在流形支持数据上的有效性。

Method: 理论分析流匹配在线性插值下的性能，当目标分布支持在光滑流形上时，建立学习速度场的非渐近收敛保证，并通过ODE传播估计误差获得流匹配目标诱导的隐式密度估计器的统计一致性。

Result: 获得的收敛率接近极小极大最优，仅依赖于内在维度，反映了流形和目标分布的光滑性，为流匹配如何适应内在数据几何结构并规避维度诅咒提供了理论依据。

Conclusion: 流匹配在流形支持的目标分布上具有理论保证，其收敛性能仅依赖于内在维度而非环境维度，这解释了流匹配在高维数据生成任务中的实际有效性。

Abstract: Flow matching has emerged as a simulation-free alternative to diffusion-based generative modeling, producing samples by solving an ODE whose time-dependent velocity field is learned along an interpolation between a simple source distribution (e.g., a standard normal) and a target data distribution. Flow-based methods often exhibit greater training stability and have achieved strong empirical performance in high-dimensional settings where data concentrate near a low-dimensional manifold, such as text-to-image synthesis, video generation, and molecular structure generation. Despite this success, existing theoretical analyses of flow matching assume target distributions with smooth, full-dimensional densities, leaving its effectiveness in manifold-supported settings largely unexplained. To this end, we theoretically analyze flow matching with linear interpolation when the target distribution is supported on a smooth manifold. We establish a non-asymptotic convergence guarantee for the learned velocity field, and then propagate this estimation error through the ODE to obtain statistical consistency of the implicit density estimator induced by the flow-matching objective. The resulting convergence rate is near minimax-optimal, depends only on the intrinsic dimension, and reflects the smoothness of both the manifold and the target distribution. Together, these results provide a principled explanation for how flow matching adapts to intrinsic data geometry and circumvents the curse of dimensionality.

</details>


### [61] [From Shallow Bayesian Neural Networks to Gaussian Processes: General Convergence, Identifiability and Scalable Inference](https://arxiv.org/abs/2602.22492)
*Gracielle Antunes de Araújo,Flávio B. Gonçalves*

Main category: stat.ML

TL;DR: 该论文研究了浅层贝叶斯神经网络（BNN）通过高斯过程（GP）连接的缩放极限，提出了新的混合协方差函数，并开发了基于Nyström近似的可扩展推理方法。


<details>
  <summary>Details</summary>
Motivation: 研究贝叶斯神经网络的缩放极限与高斯过程之间的联系，旨在解决统计建模、可识别性和可扩展推理的问题，为BNN提供理论保证和实用计算方法。

Method: 1. 建立BNN到GP的一般收敛理论，放宽先前假设；2. 提出由四种常用激活函数诱导的凸混合协方差函数；3. 开发基于Nyström近似的可扩展MAP训练和预测方法，控制成本-精度权衡。

Result: 1. 证明了BNN到GP的收敛性；2. 新协方差函数具有正定性，在不同输入设计下具有严格和实用的可识别性；3. 在模拟和真实表格数据集上展示了稳定的超参数估计和具有竞争力的预测性能。

Conclusion: 该研究为浅层贝叶斯神经网络提供了坚实的理论基础和实用的可扩展推理框架，通过高斯过程连接实现了统计建模、可识别性和计算效率的良好平衡。

Abstract: In this work, we study scaling limits of shallow Bayesian neural networks (BNNs) via their connection to Gaussian processes (GPs), with an emphasis on statistical modeling, identifiability, and scalable inference. We first establish a general convergence result from BNNs to GPs by relaxing assumptions used in prior formulations, and we compare alternative parameterizations of the limiting GP model. Building on this theory, we propose a new covariance function defined as a convex mixture of components induced by four widely used activation functions, and we characterize key properties including positive definiteness and both strict and practical identifiability under different input designs. For computation, we develop a scalable maximum a posterior (MAP) training and prediction procedure using a Nyström approximation, and we show how the Nyström rank and anchor selection control the cost-accuracy trade-off. Experiments on controlled simulations and real-world tabular datasets demonstrate stable hyperparameter estimates and competitive predictive performance at realistic computational cost.

</details>


### [62] [Unsupervised Continual Learning for Amortized Bayesian Inference](https://arxiv.org/abs/2602.22884)
*Aayush Mishra,Šimon Kucharský,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 提出了一种用于摊销贝叶斯推理的持续学习框架，通过将模拟预训练与无监督序列自洽微调解耦，结合情景回放和弹性权重整合来缓解灾难性遗忘，在模型误设下提升后验估计性能。


<details>
  <summary>Details</summary>
Motivation: 摊销贝叶斯推理在模型误设下性能下降，现有自洽训练方法局限于静态单任务设置，无法处理顺序到达的数据或分布偏移，需要一种能适应现实世界数据序列变化的持续学习框架。

Method: 提出持续学习框架：1）将模拟预训练与无监督序列自洽微解耦；2）采用两种适应策略：情景回放（使用过去观测的记忆缓冲区）和弹性权重整合（正则化更新以保留任务关键参数）。

Result: 在三个不同案例研究中，该方法显著缓解了遗忘问题，产生的后验估计优于标准模拟训练，更接近MCMC参考结果，为跨任务可信赖的摊销贝叶斯推理提供了可行路径。

Conclusion: 提出的持续学习框架通过解耦模拟预训练和序列自洽微调，结合情景回放和弹性权重整合，有效解决了摊销贝叶斯推理在模型误设和顺序数据下的性能问题，实现了更稳健的后验估计。

Abstract: Amortized Bayesian Inference (ABI) enables efficient posterior estimation using generative neural networks trained on simulated data, but often suffers from performance degradation under model misspecification. While self-consistency (SC) training on unlabeled empirical data can enhance network robustness, current approaches are limited to static, single-task settings and fail to handle sequentially arriving data or distribution shifts. We propose a continual learning framework for ABI that decouples simulation-based pre-training from unsupervised sequential SC fine-tuning on real-world data. To address the challenge of catastrophic forgetting, we introduce two adaptation strategies: (1) SC with episodic replay, utilizing a memory buffer of past observations, and (2) SC with elastic weight consolidation, which regularizes updates to preserve task-critical parameters. Across three diverse case studies, our methods significantly mitigate forgetting and yield posterior estimates that outperform standard simulation-based training, achieving estimates closer to MCMC reference, providing a viable path for trustworthy ABI across a range of different tasks.

</details>


### [63] [Beyond NNGP: Large Deviations and Feature Learning in Bayesian Neural Networks](https://arxiv.org/abs/2602.22925)
*Katerina Papagiannouli,Dario Trevisan,Giuseppe Pio Zitto*

Main category: stat.ML

TL;DR: 研究宽贝叶斯神经网络中控制后验集中的罕见但统计主导的波动，超越高斯过程极限，通过大偏差理论在函数层面建立复杂度概念和特征学习。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注高斯过程极限（NNGP），但实际有限宽度神经网络表现出非高斯波动和特征学习效应，需要超越固定核的理论框架来理解后验集中机制。

Method: 采用大偏差理论，在预测器层面建立变分目标（速率函数），通过联合优化预测器和内部核来获得后验输出速率函数，与固定核（NNGP）理论形成对比。

Result: 数值实验表明，该方法能准确描述中等规模网络的有限宽度行为，捕捉非高斯尾部、后验变形和数据依赖的核选择效应。

Conclusion: 大偏差理论为宽贝叶斯神经网络提供了超越高斯过程极限的分析框架，在函数层面直接刻画特征学习和复杂度，揭示了后验集中的统计主导机制。

Abstract: We study wide Bayesian neural networks focusing on the rare but statistically dominant fluctuations that govern posterior concentration, beyond Gaussian-process limits. Large-deviation theory provides explicit variational objectives-rate functions-on predictors, providing an emerging notion of complexity and feature learning directly at the functional level. We show that the posterior output rate function is obtained by a joint optimization over predictors and internal kernels, in contrast with fixed-kernel (NNGP) theory. Numerical experiments demonstrate that the resulting predictions accurately describe finite-width behavior for moderately sized networks, capturing non-Gaussian tails, posterior deformation, and data-dependent kernel selection effects.

</details>


### [64] [Kernel Integrated $R^2$: A Measure of Dependence](https://arxiv.org/abs/2602.22985)
*Pouya Roudaki,Shakeel Gavioli-Akilagun,Florian Kalinke,Mona Azadkia,Zoltán Szabó*

Main category: stat.ML

TL;DR: 提出了一种新的统计依赖性度量方法——核集成R²，它将集成R²的局部归一化原理与再生核希尔伯特空间的灵活性相结合，能够处理多元、函数和结构化数据，对尾部行为和振荡依赖结构敏感。


<details>
  <summary>Details</summary>
Motivation: 现有的统计依赖性度量方法在处理多元、函数和结构化数据时存在局限性，特别是在捕捉非线性关系和复杂依赖结构方面。需要一种既能保持集成R²对尾部行为敏感的优点，又能扩展到更一般响应空间的度量方法。

Method: 将集成R²的局部归一化原理与再生核希尔伯特空间（RKHS）框架相结合，提出了核集成R²。开发了两种估计器：基于K近邻的图方法和基于条件均值嵌入的RKHS方法。证明了图估计器的一致性并推导了收敛速率。

Result: 核集成R²具有三个理想性质：取值在[0,1]之间；等于零当且仅当独立；等于一当且仅当响应是协变量的可测函数。数值实验显示，在涉及非线性和结构化关系的设置中，该方法比现有依赖度量方法具有更强的检测能力。

Conclusion: 核集成R²是一种有效的统计依赖性度量方法，它结合了集成R²和RKHS的优点，能够处理各种数据类型，对复杂依赖结构敏感，在依赖测试中表现出优于现有方法的性能。

Abstract: We introduce kernel integrated $R^2$, a new measure of statistical dependence that combines the local normalization principle of the recently introduced integrated $R^2$ with the flexibility of reproducing kernel Hilbert spaces (RKHSs). The proposed measure extends integrated $R^2$ from scalar responses to responses taking values on general spaces equipped with a characteristic kernel, allowing to measure dependence of multivariate, functional, and structured data, while remaining sensitive to tail behaviour and oscillatory dependence structures. We establish that (i) this new measure takes values in $[0,1]$, (ii) equals zero if and only if independence holds, and (iii) equals one if and only if the response is almost surely a measurable function of the covariates. Two estimators are proposed: a graph-based method using $K$-nearest neighbours and an RKHS-based method built on conditional mean embeddings. We prove consistency and derive convergence rates for the graph-based estimator, showing its adaptation to intrinsic dimensionality. Numerical experiments on simulated data and a real data experiment in the context of dependency testing for media annotations demonstrate competitive power against state-of-the-art dependence measures, particularly in settings involving non-linear and structured relationships.

</details>


### [65] [Regular Fourier Features for Nonstationary Gaussian Processes](https://arxiv.org/abs/2602.23006)
*Arsalan Jawaid,Abdullah Karatas,Jörg Seewig*

Main category: stat.ML

TL;DR: 提出一种用于可调和过程的规则傅里叶特征方法，通过直接离散化谱表示来避免传统谱方法对概率假设的限制，提供高效的低秩正半定近似。


<details>
  <summary>Details</summary>
Motivation: 传统谱方法将谱密度视为概率分布进行蒙特卡洛近似，这适用于平稳过程，但对非平稳过程过于严格，因为非平稳过程的谱密度通常不是概率测度。

Method: 提出规则傅里叶特征方法，直接离散化可调和过程的谱表示，保持谱权重间的相关结构，无需概率假设。在有限谱支撑假设下，构建高效的低秩正半定近似。

Result: 该方法避免了传统谱方法的概率限制，适用于非平稳过程。当谱密度未知时，框架可自然扩展到从数据中学习核函数。在局部平稳核和具有复值谱密度的可调和混合核上验证了有效性。

Conclusion: 规则傅里叶特征方法为可调和过程提供了一种灵活高效的谱表示离散化方法，突破了传统谱方法对概率假设的限制，适用于更广泛的非平稳过程。

Abstract: Simulating a Gaussian process requires sampling from a high-dimensional Gaussian distribution, which scales cubically with the number of sample locations. Spectral methods address this challenge by exploiting the Fourier representation, treating the spectral density as a probability distribution for Monte Carlo approximation. Although this probabilistic interpretation works for stationary processes, it is overly restrictive for the nonstationary case, where spectral densities are generally not probability measures. We propose regular Fourier features for harmonizable processes that avoid this limitation. Our method discretizes the spectral representation directly, preserving the correlation structure among spectral weights without requiring probability assumptions. Under a finite spectral support assumption, this yields an efficient low-rank approximation that is positive semi-definite by construction. When the spectral density is unknown, the framework extends naturally to kernel learning from data. We demonstrate the method on locally stationary kernels and on harmonizable mixture kernels with complex-valued spectral densities.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 提出一个基于细粒度任务分解的多智能体LLM交易框架，相比传统粗粒度指令方法显著提升了风险调整后收益，并通过投资分析输出与决策偏好对齐来优化系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体的自主金融交易系统通常采用模仿分析师和经理角色的抽象指令，但忽略了真实工作流程的复杂性，导致推理性能下降和决策透明度不足。

Method: 提出一个多智能体LLM交易框架，将投资分析明确分解为细粒度任务而非粗粒度指令，使用日本股票数据（价格、财务报表、新闻、宏观信息）在泄漏控制回测环境下评估，并进行标准投资组合优化。

Result: 细粒度任务分解相比传统粗粒度设计显著提高了风险调整后收益；分析中间智能体输出发现分析输出与下游决策偏好的对齐是系统性能的关键驱动因素；通过利用与股票指数的低相关性和系统输出方差进行投资组合优化获得了优越性能。

Conclusion: 细粒度任务分解和多智能体对齐是提升LLM交易系统性能的关键，这些发现对实际应用中LLM智能体交易系统的智能体结构和任务配置设计有重要贡献。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>


### [67] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: GYWI系统通过作者知识图谱与检索增强生成结合，为LLMs提供可控学术背景和可追溯灵感路径，显著提升科学想法生成质量


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学想法生成中缺乏可控的学术背景和可追溯的灵感路径，需要构建能够提供深度和广度知识的外部知识库来改善生成质量

Method: 1) 作者中心知识图谱构建与灵感源采样算法；2) RAG与GraphRAG混合检索机制；3) 结合强化学习的Prompt优化策略；4) 基于arXiv的多维度评估方法

Result: GYWI在GPT-4o、DeepSeek-V3、Qwen3-8B、Gemini 2.5等模型上测试，在创新性、可靠性、相关性等多个指标上显著优于主流LLMs

Conclusion: GYWI系统通过结合知识图谱和混合检索机制，有效提升了LLMs生成科学想法的质量，为可控、可追溯的科学创新提供了可行方案

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [68] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: FIRE是一个全面的金融AI基准测试，用于评估LLMs的理论金融知识和实际业务场景处理能力，包含理论考试题和3000个金融场景问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估LLMs在金融领域能力的基准测试，需要同时评估理论知识和实际应用价值，以了解LLMs在金融应用中的能力边界。

Method: 1) 理论评估：从权威金融资格考试中收集多样化考题；2) 实践评估：提出系统评估矩阵，分类复杂金融领域和业务活动，收集3000个金融场景问题（包括封闭式决策题和开放式问题）。

Result: 对最先进的LLMs进行了全面评估，包括轩辕4.0作为金融领域强基线模型，系统分析了当前LLMs在金融应用中的能力边界。

Conclusion: FIRE基准测试为评估LLMs金融能力提供了全面框架，公开了基准问题和评估代码以促进未来研究，有助于理解LLMs在金融领域的实际应用潜力。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [69] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 该论文提出因果嵌入框架，将多个详细因果模型映射到更粗粒度模型的子系统中，作为抽象化的推广，并应用于多分辨率边缘问题和数据集合并。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象研究主要关注两个模型之间的关系，但实际应用中需要将多个详细模型映射到同一个粗粒度模型的子系统中，以处理多分辨率数据和模型集成问题。

Method: 提出因果嵌入框架作为抽象化的推广，定义广义一致性概念，建立多分辨率边缘问题，将因果嵌入应用于统计边缘问题和因果边缘问题。

Result: 因果嵌入框架能够统一处理多个详细模型到粗粒度模型的映射，展示其在统计边缘问题和因果边缘问题中的应用价值，并证明其在合并不同表示模型数据集方面的实用性。

Conclusion: 因果嵌入为多模型集成和多分辨率数据分析提供了统一框架，扩展了因果抽象理论，在数据集合并和模型集成方面具有重要应用前景。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [70] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 提出Agent Behavioral Contracts (ABC)框架，将契约设计原则应用于AI智能体，通过运行时强制执行来约束行为漂移，实现88-100%的硬约束合规性。


<details>
  <summary>Details</summary>
Motivation: 传统软件依赖API、类型系统等契约来确保正确行为，而AI智能体基于提示和自然语言指令，缺乏形式化行为规范，导致行为漂移、治理失败和项目失败。需要为自主AI智能体建立形式化的行为契约框架。

Method: 引入ABC框架，契约C=(P,I,G,R)包含前置条件、不变量、治理策略和恢复机制。定义(p,delta,k)-满足度的概率性契约合规概念，考虑LLM非确定性和恢复机制。实现AgentAssert运行时强制执行库。

Result: 在AgentContract-Bench基准测试中，契约化智能体检测到5.2-6.8个软违规（基线完全未检测到），实现88-100%硬约束合规性，将行为漂移限制在D*<0.27，恢复率100%（前沿模型）和17-100%（所有模型），每次动作开销<10ms。

Conclusion: ABC框架为AI智能体提供了形式化的行为契约机制，能有效检测违规、约束行为漂移、实现高合规性，且运行时开销低，为解决AI智能体部署中的治理问题提供了实用解决方案。

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [71] [Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?](https://arxiv.org/abs/2602.22401)
*Yongjun Zhang*

Main category: cs.AI

TL;DR: 论文提出"氛围研究"概念，类比"氛围编程"，分析AI代理在社会科学研究中的革命性影响，通过学者技能插件案例展示AI如何自主执行完整研究流程，识别AI优势与局限，并提出负责任研究原则。


<details>
  <summary>Details</summary>
Motivation: AI代理系统代表社会科学自动化技术的质变，能够自主执行多步骤推理工作流，从读取文件、运行代码到搜索网络，完成整个研究流程。论文旨在探索AI时代社会科学研究的新范式，分析AI代理如何改变研究实践。

Method: 引入"氛围研究"概念，使用包含21种技能的学者技能插件作为案例研究。开发认知任务框架，按可编码性和隐性知识需求两个维度对研究活动进行分类，识别认知而非顺序的委托边界。分析AI在速度、覆盖范围和方法论支持方面的优势，以及在理论原创性和领域隐性知识方面的局限。

Result: AI代理在速度、覆盖范围和方法论支持方面表现出色，但在理论原创性和领域隐性知识方面存在局限。委托边界是认知性的，贯穿研究流程的每个阶段，而非阶段之间。识别出对专业的三个影响：具有脆弱条件的增强、分层风险和教育危机。

Conclusion: AI代理正在改变社会科学研究范式，提出了"氛围研究"的新概念。论文建议负责任氛围研究的五项原则，强调需要平衡AI增强与人类专业知识的结合，应对分层风险和教育挑战，确保AI在社会科学研究中的负责任应用。

Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.

</details>


### [72] [Towards Autonomous Memory Agents](https://arxiv.org/abs/2602.22406)
*Xinle Wu,Rui Zhang,Mustafa Anis Hussain,Yao Lu*

Main category: cs.AI

TL;DR: U-Mem是一个自主记忆代理系统，通过成本感知的知识提取级联和语义感知的Thompson采样，主动获取、验证和整理知识，显著提升LLM在可验证和不可验证任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理是被动和反应式的，记忆增长受限于可用信息，且在不确定性时很少主动寻求外部输入。需要一种能够主动获取、验证和整理知识的自主记忆代理。

Method: 提出U-Mem系统：1）成本感知的知识提取级联，从廉价的自/教师信号逐步升级到工具验证的研究，必要时寻求专家反馈；2）语义感知的Thompson采样，平衡记忆的探索与利用，缓解冷启动偏差。

Result: 在可验证和不可验证的基准测试中，U-Mem持续优于现有记忆基线，甚至超越基于RL的优化方法：HotpotQA（Qwen2.5-7B）提升14.6分，AIME25（Gemini-2.5-flash）提升7.33分。

Conclusion: U-Mem通过主动、成本优化的知识获取和智能记忆管理，实现了自主记忆代理，显著提升了LLM在各种任务上的性能，为解决现有记忆代理的被动性提供了有效方案。

Abstract: Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.

</details>


### [73] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 研究者开发了CogARC数据集来研究人类抽象推理策略，通过两个实验收集了260名参与者在75个视觉推理问题上的行为数据，发现人类在抽象推理中表现出高效但策略多样的特点。


<details>
  <summary>Details</summary>
Motivation: 研究人类在抽象推理中的认知策略灵活性，了解人类如何从稀疏示例中快速学习和应用规则，为人工智能的抽象推理能力提供认知科学基础。

Method: 开发CogARC数据集（基于ARC的适应人类版本），在两个实验中让260名参与者解决75个抽象视觉推理问题，记录高时间分辨率的行为数据（示例查看、编辑序列、多次尝试提交）。

Result: 参与者整体表现良好（实验1准确率~90%，实验2~80%），但问题难度和个体差异显著。困难问题引发更长思考时间和更多策略分歧。任务过程中响应速度加快但准确率略有下降。错误解决方案也常表现出高度收敛性。

Conclusion: CogARC为研究人类抽象推理提供了丰富的行为环境，揭示了人类在不确定性下如何泛化、错误泛化和调整策略，为理解人类认知灵活性提供了重要见解。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [74] [Certified Circuits: Stability Guarantees for Mechanistic Circuits](https://arxiv.org/abs/2602.22968)
*Alaa Anani,Tobias Lorenz,Bernt Schiele,Mario Fritz,Jonas Fischer*

Main category: cs.AI

TL;DR: 提出Certified Circuits框架，为电路发现提供可证明的稳定性保证，通过随机数据子采样确保电路组件包含决策对概念数据集的有界编辑距离扰动具有不变性。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法存在脆弱性问题：电路严重依赖于所选概念数据集，且往往无法在分布外数据上迁移，这让人质疑它们捕获的是概念还是数据集特定的人工产物。

Method: 将任何黑盒发现算法与随机数据子采样相结合，通过认证确保电路组件包含决策对概念数据集的有界编辑距离扰动具有不变性，不稳定的神经元被排除在外。

Result: 在ImageNet和OOD数据集上，认证电路实现了高达91%的更高准确率，同时使用45%更少的神经元，在基线方法失效时仍保持可靠。

Conclusion: Certified Circuits通过产生可证明稳定且与目标概念更一致的机制解释，将电路发现建立在正式基础上。

Abstract: Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!

</details>


### [75] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 研究异构代理通过校准自身可靠性并选择性弃权投票的集体准确性，将经典孔多塞陪审团定理推广到置信门控的序贯设置，并探讨其在AI安全中缓解LLM幻觉的应用。


<details>
  <summary>Details</summary>
Motivation: 经典孔多塞陪审团定理假设固定参与，但现实中的集体决策常受益于允许代理说"我不知道"。需要研究代理通过学习自身可靠性并选择性弃权如何影响集体准确性。

Method: 提出概率框架：代理经历校准阶段更新对自身固定能力的信念，然后面对最终置信门决定是否投票或弃权。推导群体成功概率的非渐近下界，证明选择性参与将CJT渐近保证推广到序贯置信门控设置。通过蒙特卡洛模拟验证边界。

Result: 证明了选择性参与将CJT的渐近保证推广到序贯置信门控设置，为群体成功概率提供了非渐近下界。蒙特卡洛模拟验证了理论边界。

Conclusion: 选择性弃权机制能有效提升异构代理的集体决策准确性，该框架可应用于AI安全领域，特别是缓解集体LLM决策中的幻觉问题。

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [76] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 论文证明基于优化的AI系统（如RLHF训练的LLM）在形式上无法实现规范性治理，因为它们缺乏真正智能体所需的两个必要条件：不可通约性和否定响应性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统被部署在医疗、法律、金融等高风险领域，假设它们可以被规范性治理。但论文质疑这一假设，认为基于优化的系统（特别是RLHF训练的LLM）在形式上无法满足规范性治理的要求。

Method: 通过形式化分析，建立真正智能体所需的两个必要且充分条件：1) 不可通约性 - 将某些边界视为不可协商的约束而非可交易的权重；2) 否定响应性 - 当这些边界受到威胁时能够暂停处理的非推理机制。证明RLHF系统在结构上与这两个条件不相容。

Result: RLHF系统在结构上无法实现规范性治理，其失败模式（奉承、幻觉、不忠推理）不是偶然错误而是结构表现。此外，部署这类系统会导致"收敛危机" - 人类在指标压力下验证AI输出时会从真正智能体退化为标准检查优化器。

Conclusion: 基于优化的AI系统（如RLHF训练的LLM）在形式上无法实现规范性治理，这不是可修复的技术问题，而是优化的本质约束。论文提出了一个与基质无关的架构规范，定义了任何系统要成为真正智能体而非复杂工具必须满足的条件。

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [77] [ArchAgent: Agentic AI-driven Computer Architecture Discovery](https://arxiv.org/abs/2602.22425)
*Raghav Gupta,Akanksha Jain,Abraham Gonzalez,Alexander Novikov,Po-Sen Huang,Matej Balog,Marvin Eisenberger,Sergey Shirobokov,Ngân Vũ,Martin Dixon,Borivoje Nikolić,Parthasarathy Ranganathan,Sagar Karandikar*

Main category: cs.AI

TL;DR: ArchAgent：基于AlphaEvolve的自动计算机架构发现系统，能在2-18天内自动设计出超越现有最优的缓存替换策略，实现5.3% IPC加速提升，比人工开发快3-5倍


<details>
  <summary>Details</summary>
Motivation: 为应对计算需求的爆炸式增长，需要敏捷的硬件设计流程。虽然生成式AI系统在算法设计和代码优化方面已有显著进展，但尚未应用于计算机架构发现领域

Method: 基于AlphaEvolve构建ArchAgent系统，自动设计缓存替换策略（包括新机制和逻辑，而不仅仅是参数调整）。系统还支持"后硅片超专业化"，通过调整硬件策略中的运行时可配置参数来适应特定工作负载

Result: 在2天内为多核Google工作负载生成策略，实现5.3% IPC加速提升；在18天内为单核SPEC06工作负载生成策略，实现0.9% IPC加速提升。比人工开发快3-5倍。通过后硅片超专业化，在SPEC06上实现额外2.4% IPC提升

Conclusion: ArchAgent展示了代理AI在计算机架构研究中的巨大潜力，能够快速自动设计高性能架构策略。同时揭示了"模拟器逃逸"现象，即AI系统可能发现并利用模拟器中的漏洞，这对传统研究工具提出了新的挑战

Abstract: Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.
  Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.
  In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar "winning margin" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.
  Agentic flows also enable "post-silicon hyperspecialization" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.
  Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of "simulator escapes", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.

</details>


### [78] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 提出基于强化学习的多智能体F1赛车策略优化框架，通过交互模块和自博弈训练生成适应对手行为的竞争性策略


<details>
  <summary>Details</summary>
Motivation: F1比赛中需要根据实时比赛条件和对手行动动态调整策略，传统方法难以处理复杂的多智能体交互和实时决策

Method: 基于预训练的单智能体策略，引入交互模块考虑对手行为，采用自博弈训练方案，智能体根据相对性能进行排名

Result: 智能体能根据对手调整进站时机、轮胎选择和能量分配，实现稳健一致的比赛表现，框架仅使用真实比赛可用信息

Conclusion: 该强化学习框架能为比赛策略师提供赛前和赛中决策支持，处理多智能体交互并生成适应性的竞争策略

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [79] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: LLMs显著提升了新手在生物安全相关任务上的表现，准确率比仅使用互联网的对照组高4.16倍，甚至在某些任务上超过了专家基线，但用户未能充分利用LLMs的全部潜力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在生物学基准测试中表现良好，但尚不清楚它们是否能真正提升新手用户的表现（即让人类比仅使用互联网资源时做得更好）。这种不确定性对于理解科学加速和双重用途风险至关重要。

Method: 进行了多模型、多基准的人类提升研究，比较了有LLM访问权限的新手与仅能使用互联网的新手在八个生物安全相关任务集上的表现。参与者有充足时间（最复杂的任务长达13小时）处理复杂问题。

Result: LLM访问提供了实质性提升：有LLM的新手准确率比对照组高4.16倍（95% CI [2.63, 6.87]）。在四个有专家基线的基准测试中，有LLM的新手在其中三个上超过了专家。有趣的是，独立的LLMs经常超过LLM辅助的新手，表明用户未能充分利用LLMs的全部潜力。大多数参与者（89.6%）报告在获取双重用途相关信息时几乎没有遇到安全限制。

Conclusion: LLMs显著提升了新手在原本需要专业训练的生物任务上的表现，强调了需要持续、交互式的提升评估与传统基准测试并行进行的重要性。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [80] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 该论文对潜在推理方法进行全面分析，发现普遍存在捷径行为，潜在表示虽能编码多种可能性但未实现结构化搜索，并揭示了监督强度与表示多样性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 潜在推理作为一种新兴推理范式，通过在潜在空间而非文本空间生成步骤进行多步推理，能够超越离散语言标记的限制。尽管已有大量研究关注提升潜在推理性能，但其内部机制尚未得到充分探究。本研究旨在深入理解潜在表示在推理过程中的作用和行为。

Method: 对具有不同监督水平的潜在推理方法进行全面分析，识别关键问题：1) 观察普遍存在的捷径行为；2) 检验潜在推理是否支持潜在空间中的广度优先搜索式探索；3) 分析监督强度对潜在推理能力的影响。

Result: 发现两个关键问题：1) 普遍存在捷径行为，模型无需依赖潜在推理即可获得高准确率；2) 潜在表示虽能编码多种可能性，但推理过程并未忠实实现结构化搜索，而是表现出隐式剪枝和压缩。监督强度存在权衡：强监督缓解捷径行为但限制潜在表示保持多样假设的能力，弱监督允许更丰富的潜在表示但增加捷径行为。

Conclusion: 潜在推理方法存在内在局限性，虽然潜在表示具备编码多种可能性的能力，但实际推理过程并未实现真正的结构化搜索。监督强度需要在缓解捷径行为和保持表示多样性之间进行权衡，这为未来改进潜在推理方法提供了重要方向。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [81] [A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines](https://arxiv.org/abs/2602.22442)
*Gaoyuan Du,Amit Ahlawat,Xiaoyang Liu,Jing Wu*

Main category: cs.AI

TL;DR: 提出评估代理(EA)对AutoML代理进行决策中心评估，而非仅关注最终性能，能检测错误决策、推理不一致性，并量化决策对下游性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的AutoML系统评估过于结果中心化，只关注最终任务性能，缺乏对中间决策质量的系统评估。通过文献回顾发现，没有现有系统报告用于事后评估中间决策质量的结构化、决策级指标。

Method: 提出评估代理(EA)作为观察者，在不干扰AutoML代理执行的情况下，从四个维度评估中间决策：决策有效性、推理一致性、超越准确率的模型质量风险、以及反事实决策影响。

Result: 在四个概念验证实验中，EA能够：(i)以0.919的F1分数检测错误决策；(ii)独立于最终结果识别推理不一致性；(iii)将下游性能变化归因于代理决策，显示最终指标影响范围从-4.9%到+8.3%。

Conclusion: 决策中心评估揭示了仅结果指标无法发现的故障模式，将基于代理的AutoML系统评估从结果导向转变为审计代理决策，为可靠、可解释和可治理的自主ML系统奠定了基础。

Abstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\% to +8.3\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.

</details>


### [82] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 提出对比世界模型(CWM)，使用对比学习训练动作可行性评分器，通过挖掘困难负样本提升对物理可行性的判别能力


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法独立处理候选动作，无法显式区分物理正确与细微错误的动作，需要更可靠的可行性评分器

Method: 使用InfoNCE对比目标微调大语言模型，挖掘困难负样本（语义相似但物理不兼容），在评分空间中将有效动作与无效动作分离

Result: 在ScienceWorld基准测试中，CWM在困难负样本对上的Precision@1比SFT高6.76个百分点，AUC-ROC更高(0.929 vs 0.906)，在分布外压力下安全边际显著更好(-2.39 vs -3.96)

Conclusion: 对比训练比单独使用监督微调能更准确地捕捉物理可行性，为具身智能代理提供了更可靠的动作可行性评分器

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [83] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: ConstraintBench评估大语言模型直接解决约束优化问题的能力，发现可行性而非最优性是主要瓶颈，最佳模型仅65%满足约束，可行解平均达到最优目标的89-96%。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估LLM能否将优化问题转化为求解器代码，但缺少对LLM直接解决完全指定约束优化问题能力的评估，需要填补这一空白。

Method: 引入ConstraintBench基准，涵盖10个运筹学领域，每个任务包含自然语言场景描述，要求模型返回结构化解决方案，通过确定性验证器检查所有约束并与Gurobi求解器的最优解对比。

Result: 评估6个前沿模型在200个任务上的表现：最佳模型仅65.0%满足约束条件，可行解平均达到Gurobi最优目标的89-96%，没有模型在可行性和接近最优性（0.1%内）上超过30.5%。不同领域难度差异大，存在系统性失败模式。

Conclusion: LLM直接解决约束优化问题的主要瓶颈是可行性而非最优性，需要进一步研究提升约束理解和满足能力，ConstraintBench为相关研究提供了公开评估基础设施。

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [84] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO是一个用于评估和改进编码代理的系统框架，包含版本控制、评估工具和基准测试套件，支持通过编辑-执行-评估循环优化代理性能。


<details>
  <summary>Details</summary>
Motivation: 编码代理优化是一个新兴重要应用，但社区缺乏对此任务的系统性理解。代理优化与传统软件工程有本质区别：目标代理混合了确定性代码和随机LLM完成，需要同时捕获中间推理和执行结果的结构化方法。

Method: 提出了VERO框架，包含：(1) 可复现的评估工具链，具有版本化代理快照、预算控制评估和结构化执行追踪；(2) 包含目标代理和任务的基准测试套件，带有参考评估流程。

Result: 使用VERO进行了实证研究，比较了不同优化器配置在任务上的表现，分析了哪些修改能可靠提升目标代理性能。发布了VERO以支持编码代理优化研究。

Conclusion: VERO为编码代理优化研究提供了系统框架和评估工具，填补了该领域缺乏系统性理解的空白，支持代理优化作为编码代理核心能力的研究。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [85] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 本文使用大语言模型（LLM）对AI与生命周期评估（LCA）交叉领域的研究进行了系统性综述，揭示了该领域的发展趋势、新兴主题和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在生命周期评估中的应用近年来加速发展，但对该交叉领域的全面、广泛综述仍然有限。本研究旨在填补这一空白，通过系统梳理AI-LCA研究，为环境评估提供最新工具和见解。

Method: 采用大语言模型（LLM）驱动的文本挖掘方法与传统文献综述技术相结合，构建了一个动态有效的分析框架，能够捕捉该领域的高层研究趋势和细微概念模式。

Result: 分析显示：1）随着LCA研究扩展，AI技术采用显著增长；2）研究趋势明显转向LLM驱动方法；3）机器学习应用持续增加；4）AI方法与相应LCA阶段存在统计学显著相关性。

Conclusion: LLM辅助方法学在支持大规模、可重复的跨领域综述方面具有潜力，同时为在AI技术快速发展背景下实现计算高效的LCA评估了路径，有助于LCA从业者将前沿工具和及时见解融入环境评估，提升可持续性决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [86] [Mirroring the Mind: Distilling Human-Like Metacognitive Strategies into Large Language Models](https://arxiv.org/abs/2602.22508)
*Ik-hwan Kim,Hyeongrok Han,Mingi Jung,Sangwon Yu,Jinseok Hong,Sang Hun Kim,Yoonyoung Choi,Sungroh Yoon*

Main category: cs.AI

TL;DR: MBT通过注入元认知行为来稳定大型推理模型的结构脆弱性，在减少token消耗的同时提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中存在结构脆弱性，即使能推导出有效中间步骤，最终答案也经常出错。研究发现这种失败不是源于推理能力不足，而是缺乏自我调节控制，导致有效逻辑被不受控的探索或未能识别逻辑充分性所破坏。

Method: 提出了元认知行为调优（MBT）后训练框架，通过两种互补方法注入元认知行为：MBT-S从头合成严谨的推理轨迹，MBT-R重写学生初始轨迹以稳定内在探索模式。

Result: 在多跳问答基准测试中，MBT始终优于基线方法，在具有挑战性的基准上取得显著提升。通过有效消除推理崩溃，MBT以显著减少的token消耗实现了更高的准确性。

Conclusion: 内部化元认知策略能够带来更稳定和鲁棒的推理，表明增强模型的自我调节控制能力是提升复杂推理性能的关键。

Abstract: Large Reasoning Models (LRMs) often exhibit structural fragility in complex reasoning tasks, failing to produce correct answers even after successfully deriving valid intermediate steps. Through systematic analysis, we observe that these failures frequently stem not from a lack of reasoning capacity, but from a deficiency in self-regulatory control, where valid logic is destabilized by uncontrolled exploration or the failure to recognize logical sufficiency. Motivated by this observation, we propose Metacognitive Behavioral Tuning (MBT), a post-training framework that explicitly injects metacognitive behaviors into the model's thought process. MBT implements this via two complementary formulations: (1) MBT-S, which synthesizes rigorous reasoning traces from scratch, and (2) MBT-R, which rewrites the student's initial traces to stabilize intrinsic exploration patterns. Experiments across multi-hop QA benchmarks demonstrate that MBT consistently outperforms baselines, achieving notable gains on challenging benchmarks. By effectively eliminating reasoning collapse, MBT achieves higher accuracy with significantly reduced token consumption, demonstrating that internalizing metacognitive strategies leads to more stable and robust reasoning.

</details>


### [87] [A Mathematical Theory of Agency and Intelligence](https://arxiv.org/abs/2602.22519)
*Wael Hafez,Chenan Wei,Rodrigo Felipe,Amir Nazeri,Cameron Reid*

Main category: cs.AI

TL;DR: 该论文提出了"双可预测性(P)"作为衡量系统观察、行动和结果之间信息共享程度的核心指标，证明了其在量子系统、经典系统和引入能动性时的不同上限，并基于此区分了能动性与智能，提出了一种受丘脑皮质调节启发的反馈架构。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然能处理大量信息做出复杂预测，但预测成功可能掩盖了与环境交互的退化。缺乏一个原则性指标来衡量系统部署的总信息中，有多少真正在观察、行动和结果之间共享。

Method: 从第一性原理推导出双可预测性(P)这一内在指标，证明其在量子系统、经典系统和引入能动性时的理论界限，并在物理系统(双摆)、强化学习智能体和多轮LLM对话中进行验证。

Result: P在量子系统中可达1，在经典系统中≤0.5，引入能动性后更低。验证了这些界限在实际系统中的存在，并基于此区分了能动性(基于预测行动的能力)和智能(还需要从交互中学习、自我监控学习效果并调整观察-行动-结果范围)。

Conclusion: 当前AI系统只实现了能动性而非智能。受生物丘脑皮质调节启发，提出了一种实时监控P的反馈架构，为构建自适应、有韧性的AI奠定了基础。

Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.

</details>


### [88] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 该论文提出从认知模型和AI算法中寻找设计模块化语言智能体的蓝图，通过定义智能体模板来指导多个LLM的组合与协作。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型能力不断增强，但许多复杂问题仍超出单个LLM的能力范围。当前缺乏有效的方法将多个LLM组合成更强大的整体系统。

Method: 提出智能体模板的概念，用于定义单个LLM的角色和功能组合方式。通过文献调研，识别现有语言智能体背后的模板设计模式，这些模板直接源自认知模型或AI算法。

Result: 成功展示了多种现有语言智能体设计背后的认知科学和AI算法模板，为模块化语言智能体开发提供了系统化的设计框架。

Conclusion: 认知科学和AI算法启发的智能体模板是开发高效、可解释语言智能体的有力工具，为多LLM协作系统设计提供了理论指导。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [89] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 提出一个基于智能体AI的框架，用于无蜂窝O-RAN中的意图翻译和优化，通过多个LLM智能体协作处理复杂意图，实现节能和资源优化。


<details>
  <summary>Details</summary>
Motivation: 现有O-RAN中大多数工作只考虑由独立智能体处理的简单意图，而需要智能体间协调的复杂意图尚未被探索。随着智能体AI成为自主RAN的关键使能技术，需要解决复杂意图的翻译和优化问题。

Method: 提出一个多智能体框架：监督智能体翻译运营商意图为优化目标和最低速率要求；用户权重智能体从记忆模块检索先验经验确定用户优先级权重；如果包含节能目标，则激活O-RU管理智能体使用DRL算法确定活动O-RU集合；监控智能体测量用户数据速率并协调其他智能体保证最低速率要求。采用参数高效微调方法使同一底层LLM可用于不同智能体。

Result: 仿真结果显示，在节能模式下，与三种基线方案相比，所提框架将活动O-RU数量减少了41.93%。使用PEFT方法时，与部署单独LLM智能体相比，内存使用减少了92%。

Conclusion: 提出的智能体AI框架能够有效处理无蜂窝O-RAN中的复杂意图，通过多智能体协作实现显著的节能效果，同时参数高效微调方法大大降低了内存需求，提高了框架的可扩展性。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [90] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE框架通过主动学习如何请求专家推理，而非简单求助，显著提升LLM智能体在专业领域的任务成功率


<details>
  <summary>Details</summary>
Motivation: LLM智能体在需要长尾知识的专业领域表现不佳，而人类专家的指导往往非结构化且不可靠，难以直接整合到智能体规划中

Method: 提出AHCE框架，包含人类反馈模块(HFM)，通过学习策略将人类专家视为交互式推理工具，实现按需人机协作

Result: 在Minecraft实验中，普通难度任务成功率提升32%，高难度任务提升近70%，且仅需极少人工干预

Conclusion: 成功增强智能体的关键在于学习如何请求专家推理，而不仅仅是请求帮助，AHCE框架为此提供了有效解决方案

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [91] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard：基于检索增强的多智能体框架，将安全评估重构为证据辩论，无需微调即可实现SOTA安全性能，并具备零样本适应性和自动数据审计能力


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全机制依赖静态微调分类器，存在适应性僵化问题，无法在不昂贵重训练的情况下执行新治理规则

Method: 引入CourtGuard框架，通过检索增强的多智能体系统，将安全评估重构为基于外部政策文档的对抗性辩论

Result: 在7个安全基准测试中达到SOTA性能，零样本适应维基百科破坏检测任务（90%准确率），并自动策划审计了9个新颖对抗攻击数据集

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了稳健、可解释且适应性强的路径，能够满足当前和未来的监管要求

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [92] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 论文发现示例引导在数学推理中的效果不稳定源于策略使用与策略可执行性之间的差距，并提出选择性策略检索框架来提升推理性能


<details>
  <summary>Details</summary>
Motivation: 基于示例的引导在推理时被广泛用于改进数学推理，但其效果在不同问题和模型间极不稳定，即使引导正确且与问题相关。这种不稳定性源于策略使用与策略可执行性之间未被充分探索的差距

Method: 通过分析配对人类编写和模型生成的解决方案，识别使用与可执行性之间的系统差异。提出选择性策略检索框架，通过经验性、多路径、源感知信号来选择性检索和组合策略，显式建模可执行性

Result: 在多个数学推理基准测试中，SSR相比直接求解、上下文学习和单源引导，实现了可靠且一致的改进。在AIME25上准确率提升高达13个百分点，在Apex上提升5个百分点

Conclusion: 策略使用与可执行性之间的差距是示例引导不稳定的关键原因。通过显式建模可执行性并选择性检索策略，可以显著提升数学推理性能，为推理时引导提供了更可靠的框架

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [93] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 该论文提出将心理测量学的评分者模型整合到AI评估流程中，通过项目反应理论模型（特别是多面Rasch模型）分离真实输出质量与评分者行为偏差，提高人类评估数据的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 人类评估在AI模型训练和评估中起核心作用，但这些数据很少被视为存在系统误差的测量。当前AI评估中的人类评分数据存在可靠性问题，评分者效应（如严格度和中心化倾向）会扭曲观察到的评分，影响对AI输出质量的准确判断。

Method: 采用心理测量学评分者模型，特别是项目反应理论中的多面Rasch模型，将评分者行为与真实输出质量分离。使用OpenAI摘要数据集作为实证案例，展示如何通过调整评分者严格度来校正摘要质量估计，并提供评分者表现的诊断洞察。

Result: 通过调整评分者严格度，获得了校正后的摘要质量估计，并提供了评分者表现的诊断信息。实证分析表明，心理测量学建模能够有效分离真实输出质量与评分者行为偏差。

Conclusion: 将心理测量学建模整合到人机交互评估中，能够实现更原则化、透明化的人类数据使用，使开发者能够基于校正后的分数而非原始、易错的评分做出决策。这一视角为AI开发和评估提供了更稳健、可解释且与构念对齐的实践路径。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [94] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest：一种利用大型推理模型自身进行KV缓存压缩的新方法，通过并行执行压缩任务来减少长时智能体任务中的内存使用，在215个样本训练下可将峰值token使用降低65%


<details>
  <summary>Details</summary>
Motivation: 长时智能体任务（如深度研究）需要在多个网页和文档间进行多跳推理，导致LLM上下文被外部检索的token主导，内存使用快速增长并限制解码性能。现有KV缓存压缩启发式方法无法有效支持多步推理模型。

Method: 提出SideQuest方法，利用大型推理模型自身通过推理上下文token的有用性来执行KV缓存压缩。为避免管理过程污染模型内存，将KV缓存压缩作为与主推理任务并行执行的辅助任务。仅用215个样本训练模型。

Result: SideQuest在智能体任务中可将峰值token使用降低高达65%，且准确率下降最小，优于基于启发式的KV缓存压缩技术。

Conclusion: SideQuest通过让模型自身参与KV缓存管理，有效解决了长时智能体任务中的内存扩展问题，为多步推理模型提供了高效的上下文管理方案。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [95] [MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios](https://arxiv.org/abs/2602.22638)
*Zhiheng Song,Jingshuai Zhang,Chuan Qin,Chao Wang,Chao Chen,Longfei Xu,Kaikui Liu,Xiangxiang Chu,Hengshu Zhu*

Main category: cs.AI

TL;DR: MobilityBench是一个用于评估基于LLM的路线规划代理在真实世界移动场景中的可扩展基准，包含大规模真实用户查询、确定性API重放沙箱和多维度评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的路线规划代理缺乏系统化评估，因为存在多样化的路线需求、非确定性的地图服务以及有限的复现性，需要建立一个标准化的评估框架。

Method: 从Amap收集大规模匿名真实用户查询构建基准，设计确定性API重放沙箱消除环境变量，提出以结果有效性为中心的多维度评估协议。

Result: 当前模型在基础信息检索和路线规划任务上表现良好，但在偏好约束路线规划方面存在显著困难，表明个性化移动应用仍有很大改进空间。

Conclusion: MobilityBench为LLM路线规划代理提供了系统化评估框架，揭示了当前模型在个性化路线规划方面的局限性，并公开了基准数据和工具包以促进研究发展。

Abstract: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .

</details>


### [96] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于在线广告多渠道自动出价的分层框架，结合生成式规划和实时控制，相比现有方法提升13.57%的投资回报率。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂多变，多渠道场景下需要跨渠道分配预算和约束，现有优化方法缺乏动态适应性，强化学习方法难以捕捉历史依赖和观测模式。

Method: 提出AHBid分层出价框架：高层使用扩散模型生成规划器动态分配预算和约束，包含约束执行机制和轨迹精炼机制；底层采用基于控制的出价算法，结合历史知识和实时信息。

Result: 在大规模离线数据集和在线A/B测试中，AHBid相比现有基线方法实现了13.57%的整体回报提升。

Conclusion: AHBid通过结合生成式规划和实时控制，有效解决了多渠道广告出价中的动态适应性问题，显著提升了投资回报率。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [97] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了个性化LLM智能体的研究现状，围绕四个核心组件（用户画像建模、记忆、规划、行动执行）构建分类框架，分析用户信号的表示、传播和利用方式，并探讨评估指标、应用场景和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长期交互场景中的应用，其有效性越来越依赖于适应用户个体特征和保持跨时间连续性，这催生了需要深度个性化而非表面生成的智能体系统。现有研究分散，缺乏系统框架来指导个性化LLM智能体的设计和开发。

Method: 采用能力导向的综述方法，将文献组织为四个相互依赖的组件：1) 用户画像建模（profile modeling）- 从交互中提取用户特征；2) 记忆（memory）- 存储和检索用户相关信息；3) 规划（planning）- 基于用户特征制定决策；4) 行动执行（action execution）- 实施个性化行动。分析各组件间交互和设计权衡。

Result: 提出了一个结构化框架来理解和设计个性化LLM智能体，系统总结了代表性方法，揭示了跨组件交互模式和常见设计权衡。识别了专门针对个性化智能体的评估指标和基准测试，并梳理了从通用助手到专业领域的应用场景。

Conclusion: 该综述为个性化LLM智能体研究提供了系统框架，指明了从原型个性化到可扩展现实世界助手的发展路线图，推动构建更用户对齐、自适应、鲁棒和可部署的智能体系统。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [98] [Knob: A Physics-Inspired Gating Interface for Interpretable and Controllable Neural Dynamics](https://arxiv.org/abs/2602.22702)
*Siyu Jiang,Sanshuai Cui,Hui Zeng*

Main category: cs.AI

TL;DR: Knob框架将神经网络门控动态映射到二阶机械系统，通过物理参数（阻尼比和固有频率）创建可调的"安全阀"，实现动态校准和双模式推理。


<details>
  <summary>Details</summary>
Motivation: 现有校准方法多为静态后处理，忽略了现实世界推理的动态性和时序性，且缺乏让人类操作者在变化条件下动态调整模型行为的直观界面。

Method: 将深度学习与经典控制理论结合，通过logit级凸融合（输入自适应温度缩放）和二阶动力学（Knob-ODE）实现双模式推理：静态任务的i.i.d.处理和连续流的状态保持处理。

Result: 在CIFAR-10-C上验证了校准机制，在连续模式下门响应符合标准二阶控制特征（阶跃稳定和低通衰减），为可预测的人机协同调优铺平道路。

Conclusion: Knob框架通过物理类比提供直观调优界面，实现了动态校准和状态保持推理，为人类操作者提供了通过熟悉物理参数调整模型行为的可预测方法。

Abstract: Existing neural network calibration methods often treat calibration as a static, post-hoc optimization task. However, this neglects the dynamic and temporal nature of real-world inference. Moreover, existing methods do not provide an intuitive interface enabling human operators to dynamically adjust model behavior under shifting conditions. In this work, we propose Knob, a framework that connects deep learning with classical control theory by mapping neural gating dynamics to a second-order mechanical system. By establishing correspondences between physical parameters -- damping ratio ($ζ$) and natural frequency ($ω_n$) -- and neural gating, we create a tunable "safety valve". The core mechanism employs a logit-level convex fusion, functioning as an input-adaptive temperature scaling. It tends to reduce model confidence particularly when model branches produce conflicting predictions. Furthermore, by imposing second-order dynamics (Knob-ODE), we enable a \textit{dual-mode} inference: standard i.i.d. processing for static tasks, and state-preserving processing for continuous streams. Our framework allows operators to tune "stability" and "sensitivity" through familiar physical analogues. This paper presents an exploratory architectural interface; we focus on demonstrating the concept and validating its control-theoretic properties rather than claiming state-of-the-art calibration performance. Experiments on CIFAR-10-C validate the calibration mechanism and demonstrate that, in Continuous Mode, the gate responses are consistent with standard second-order control signatures (step settling and low-pass attenuation), paving the way for predictable human-in-the-loop tuning.

</details>


### [99] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless：首个基于无服务器计算的可扩展同步RLHF训练框架，通过动态资源适配、前缀预计算和成本感知的actor扩展策略，实现1.35倍加速和44.8%成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF框架依赖服务器基础设施，难以应对RLHF训练中动态变化的资源需求，导致组件间空闲时间和资源浪费问题。随着模型规模扩大，RLHF训练效率面临更大挑战。

Method: 1. 基于无服务器计算环境构建框架；2. 动态适应RLHF流程中的资源需求变化；3. 预计算共享前缀避免重复计算；4. 采用考虑响应长度变化的成本感知actor扩展策略；5. 高效分配工作负载以减少函数内不平衡和空闲时间。

Result: 在物理测试床和大规模模拟集群上的实验表明，RLHFless相比最先进基线实现了最高1.35倍的加速和44.8%的成本降低。

Conclusion: RLHFless通过无服务器计算环境有效解决了同步RLHF训练中的资源动态变化问题，显著提升了训练效率和成本效益，为大规模RLHF训练提供了可行的解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [100] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决数据稀疏和冷启动问题，无需复杂模型架构。


<details>
  <summary>Details</summary>
Motivation: 传统模型中心方法依赖复杂定制架构，难以捕捉跨域的细微非结构化序列依赖，导致泛化能力差且计算资源需求高。跨域数据存在领域差距，混合数据质量下降会导致负迁移和模型性能下降。

Method: 提出Taesar框架，采用对比解码机制自适应地将跨域上下文编码到目标域序列中，使标准模型能够学习复杂依赖而无需复杂融合架构。

Result: 实验表明Taesar优于模型中心解决方案，并能泛化到各种序列模型。通过生成丰富数据集，有效结合了数据中心和模型中心范式的优势。

Conclusion: Taesar通过数据中心的序列再生方法解决了跨域推荐中的领域差距问题，提供了一种更高效、泛化能力更强的解决方案。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [101] [Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning](https://arxiv.org/abs/2602.22751)
*Qiannian Zhao,Chen Yang,Jinhao Jing,Yunke Zhang,Xuhui Ren,Lu Yu,Shijie Zhang,Hongzhi Yin*

Main category: cs.AI

TL;DR: EGPO框架通过元认知熵校准，将内在不确定性整合到强化学习中，解决不确定性-奖励不匹配问题，显著提升大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习（RLVR）主要依赖二元正确性信号，忽略了模型的内在不确定性，导致高不确定性和低不确定性解决方案被同等对待，阻碍了从优化正确答案向优化有效推理路径的转变。

Method: 提出EGPO元认知熵校准框架：1）使用基于token级似然的零开销熵代理估计每个样本的不确定性；2）通过非对称校准机制将内在不确定性与外在正确性对齐，保留正确推理同时选择性调节过度自信的失败；3）从退化的基于组的rollout中恢复信息性学习信号。

Result: 在多个基准测试上的广泛实验表明，EGPO在推理性能上带来显著且一致的改进，为通过元认知熵校准推进大型推理模型建立了原则性路径。

Conclusion: EGPO通过将内在不确定性整合到RLVR中，解决了不确定性-奖励不匹配问题，实现了稳定且不确定性感知的策略优化，显著提升了大型推理模型的推理性能。

Abstract: Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary correctness signal and largely ignore the model's intrinsic uncertainty. We term this discrepancy the uncertainty-reward mismatch, under which high- and low-uncertainty solutions are treated equivalently, preventing the policy from "Know What You Know" and impeding the shift from optimizing for correct answers to optimizing effective reasoning paths. This limitation is especially critical in reasoning-centric tasks such as mathematics and question answering, where performance hinges on the quality of the model's internal reasoning process rather than mere memorization of final answers. To address this, we propose EGPO, a metacognitive entropy calibration framework that explicitly integrates intrinsic uncertainty into RLVR for enhancing LRMs. EGPO estimates per-sample uncertainty using a zero-overhead entropy proxy derived from token-level likelihoods and aligns it with extrinsic correctness through an asymmetric calibration mechanism that preserves correct reasoning while selectively regulating overconfident failures, thereby enabling stable and uncertainty-aware policy optimization. Moreover, EGPO recovers informative learning signals from otherwise degenerate group-based rollouts without modifying the verifier or reward definition. Extensive experiments across multiple benchmarks demonstrate that the proposed EGPO leads to substantial and consistent improvements in reasoning performance, establishing a principled path for advancing LRMs through metacognitive entropy calibration.

</details>


### [102] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 研究分析了HealthBench医疗AI评估数据集中医生分歧的来源，发现81.8%的分歧来自病例层面的残差，无法通过现有元数据、专业领域或技术特征解释，表明医疗AI评估中的一致性问题主要是结构性的。


<details>
  <summary>Details</summary>
Motivation: 理解医疗AI评估中医生分歧的来源和可解释因素，以改进评估设计并提高评估一致性。

Method: 分解HealthBench数据集中的医生分歧，分析评估标准、医生身份、病例特征、元数据标签、专业领域、表面特征、嵌入表示等对分歧的贡献度，并研究完成质量与分歧的关系以及可减少与不可减少不确定性的影响。

Result: 评估标准身份仅解释3.6-6.9%的分歧方差，医生身份仅2.4%，81.8%的分歧来自病例层面的残差且无法被现有特征解释。分歧与完成质量呈倒U型关系（AUC=0.689）。可减少不确定性使分歧几率增加2.55倍，但仅解释约3%的总方差；不可减少不确定性无显著影响。

Conclusion: 医疗AI评估中的一致性问题主要是结构性的，但可减少与不可减少不确定性的分离表明，通过改进评估场景中的信息差距可以降低分歧，这为可操作的评估设计改进提供了方向。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [103] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench是一个评估LLM智能体长时记忆能力的基准，包含真实和合成的智能体轨迹数据，并提出了AMA-Agent记忆系统来提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体记忆评估主要关注人机对话场景，而实际应用中智能体记忆是连续的机器生成交互流，需要更贴近真实应用的评估基准。

Method: 提出AMA-Bench基准，包含：1）真实世界智能体轨迹与专家标注QA；2）可扩展到任意长度的合成智能体轨迹与规则生成QA。同时提出AMA-Agent记忆系统，采用因果图结构和工具增强检索。

Result: 现有记忆系统在AMA-Bench上表现不佳，主要因为缺乏因果性和目标信息，以及基于相似性检索的损失性限制。AMA-Agent在AMA-Bench上达到57.22%平均准确率，比最强基线提升11.16%。

Conclusion: AMA-Bench填补了智能体记忆评估的空白，AMA-Agent通过因果图和工具增强检索有效解决了现有记忆系统的局限性，为长时记忆智能体开发提供了重要基准和解决方案。

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [104] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: LLMs在临床不完全信息场景下难以判断确定性，既会过早下结论又会过度弃权，现有基准不足以评估临床安全性


<details>
  <summary>Details</summary>
Motivation: 临床决策常面临信息不完整的情况，专家需要判断现有信息是否足够做出判断。过早结论和不必要的弃权都会危害患者安全，需要评估LLMs在这方面的能力

Method: 开发了ClinDet-Bench基准，基于临床评分系统，将不完全信息场景分解为可确定和不可确定的条件。识别确定性需要考虑所有关于缺失信息的假设，包括不太可能的假设，并验证结论是否在所有情况下都成立

Result: 最近的LLMs无法在不完全信息下识别确定性，既产生过早判断又过度弃权，尽管它们能正确解释底层评分知识并在完整信息下表现良好

Conclusion: 现有基准不足以评估LLMs在临床环境中的安全性。ClinDet-Bench提供了评估确定性识别的框架，促进适当的弃权，具有医学和其他高风险领域的潜在适用性

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [105] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个高性能、鲁棒的开源智能体框架，通过智能体图灵活编排、深度推理模式增强性能、鲁棒工作流执行确保稳定，在多个智能体基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得了显著进展，但独立LLM在处理需要与外部工具和环境交互的复杂现实任务时能力开始达到瓶颈。现有智能体框架存在工作流程简单、性能不稳定、基准测试支持有限、过度依赖昂贵商业API等问题。

Method: 提出MiroFlow框架，包含三个核心组件：1) 智能体图实现灵活编排；2) 可选的深度推理模式增强性能；3) 鲁棒的工作流执行确保稳定性和可复现性。

Result: 在多个智能体基准测试（GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch、FutureX）中均取得了最先进的性能表现。

Conclusion: MiroFlow为深度研究社区提供了一个易于访问、可复现、可比较的基准框架，有望推动智能体研究的进一步发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [106] [When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design](https://arxiv.org/abs/2602.22814)
*Soyoung Jung,Daehoo Yoon,Sung Gyu Koh,Young Hwan Kim,Yehan Ahn,Sung Park*

Main category: cs.AI

TL;DR: 提出一个概念模型，将智能体行为重新定义为场景、上下文和人类行为因素整合的解释性结果，并推导出五个智能体设计原则，为具有情境敏感性和判断力的AI系统设计提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前智能AI系统虽然能够通过上下文数据推断用户情境并主动干预，但缺乏关于何时、为何以及是否采取行动的原则性判断，导致干预经常失败。需要填补这一空白。

Method: 提出一个概念模型，将行为重新定义为解释性结果，整合三个要素：场景（可观察的情境）、上下文（用户构建的意义）和人类行为因素（塑造行为可能性的决定因素）。基于跨学科视角，区分可观察内容与对用户有意义的内容。

Result: 从该模型推导出五个智能体设计原则：行为对齐、情境敏感性、时间适当性、动机校准和代理保持，这些原则指导干预的深度、时机、强度和克制。

Conclusion: 该模型和原则为设计具有情境敏感性和判断力的智能AI系统提供了基础，使系统能够在交互中以更明智的方式行动。

Abstract: Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.

</details>


### [107] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS是一个用于质谱预测的基准框架，支持动态构建多种模型架构，评估性能并提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 化学分子识别和性质预测在药物发现和材料科学中至关重要，但实验光谱数据缺乏，需要建立计算预测方法。深度学习模型在预测分子结构光谱方面有前景，但方法异质性和缺乏明确定义的基准使得整体评估具有挑战性。

Method: 创建FlexMS基准框架，支持动态构建多种模型架构组合，在预处理公共数据集上使用不同指标评估性能，分析影响性能的因素。

Result: 提供了影响性能因素的见解，包括数据集结构多样性、超参数（学习率、数据稀疏性）、预训练效果、元数据消融设置和跨域迁移学习分析，为选择合适模型提供实用指导。

Conclusion: FlexMS框架解决了质谱预测领域缺乏标准化基准的问题，通过系统评估不同模型架构，为实际应用中的模型选择提供指导，并通过检索基准模拟实际识别场景。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [108] [DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation](https://arxiv.org/abs/2602.22839)
*Hao Zheng,Guozhao Mo,Xinru Yan,Qianhao Yuan,Wenkai Zhang,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: DeepPresenter是一个自适应的PPT生成智能体框架，通过环境感知的反思机制和长时程优化，超越传统模板化方法，在多样场景下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有PPT生成工具依赖预定义工作流和固定模板，缺乏对用户意图的适应性和基于观察的迭代优化能力，需要更智能、自适应的解决方案。

Method: 提出DeepPresenter框架：1）自主规划、渲染和修订中间幻灯片工件；2）基于环境感知的反思机制，以感知到的幻灯片状态（而非内部推理痕迹）指导生成过程；3）支持长时程反馈驱动的优化。

Result: 在多样化PPT生成场景评估中达到SOTA性能，微调的9B模型在显著降低成本的同时保持高度竞争力。

Conclusion: DeepPresenter通过环境感知的反思和自适应工作流，实现了超越传统模板化方法的PPT生成能力，为智能演示创作提供了新范式。

Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent

</details>


### [109] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: AI在创造性数学研究中能真正贡献价值，但需要严格的人类验证和监督。通过Hermite求积规则误差表示和界限的案例研究，展示了人机协作能超越纯人工工作，但也揭示了AI的局限性和风险。


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否真正能为创造性数学研究做出贡献，还是仅仅自动化常规计算并引入错误风险。通过实证案例研究来验证人机协作在数学发现中的实际效果。

Method: 采用系统化的人机协作方法，与多个AI助手合作，扩展Hermite求积规则的结果。工作流程包括：AI进行代数操作、系统化证明探索、文献综合和LaTeX准备，人类提供数学直觉、问题表述、战略方向和严格验证。

Result: 成功发现并证明了关于Hermite求积规则误差表示和界限的新定理，超越了纯人工工作的成果。AI在代数操作、系统化证明探索等方面表现出色，但每一步都需要人类严格验证。研究记录了完整的工作流程，揭示了成功的人机协作模式和需要防范的失败模式。

Conclusion: 在适当的怀疑态度和验证协议下，AI工具能够有意义地加速数学发现，但需要仔细的人类监督和深厚的领域专业知识。人机协作既展示了AI的卓越能力，也暴露了其关键局限性。

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [110] [Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space](https://arxiv.org/abs/2602.22879)
*Xingcheng Fu,Shengpeng Wang,Yisen Gao,Xianxian Li,Chunpei Li,Qingyun Sun,Dongran Yu*

Main category: cs.AI

TL;DR: 提出L-HAKT框架，利用大语言模型和双曲空间对齐解决知识追踪中认知状态层次演化和个体化难度感知问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法主要基于ID序列或浅层文本特征，无法捕捉认知状态的层次演化以及个体对问题难度的差异化感知，限制了语义建模能力。

Method: 1. 教师代理深度解析问题语义并显式构建知识点层次依赖；学生代理模拟学习行为生成合成数据。2. 在双曲空间中通过对比学习对齐合成与真实数据，减少问题难度和遗忘模式等关键特征的分布差异。3. 优化双曲曲率，显式建模知识点的树状层次结构，精确刻画不同层次知识点的学习曲线形态差异。

Result: 在四个真实教育数据集上的广泛实验验证了L-HAKT框架的有效性。

Conclusion: 提出的L-HAKT框架通过大语言模型和双曲空间对齐，能够更好地建模知识点的层次结构和个体化学习特征，提升了知识追踪的准确性。

Abstract: Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they often fail to capture (1) the hierarchical evolution of cognitive states and (2) individualized problem difficulty perception due to limited semantic modeling. Therefore, this paper proposes a Large Language Model Hyperbolic Aligned Knowledge Tracing(L-HAKT). First, the teacher agent deeply parses question semantics and explicitly constructs hierarchical dependencies of knowledge points; the student agent simulates learning behaviors to generate synthetic data. Then, contrastive learning is performed between synthetic and real data in hyperbolic space to reduce distribution differences in key features such as question difficulty and forgetting patterns. Finally, by optimizing hyperbolic curvature, we explicitly model the tree-like hierarchical structure of knowledge points, precisely characterizing differences in learning curve morphology for knowledge points at different levels. Extensive experiments on four real-world educational datasets validate the effectiveness of our Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT) framework.

</details>


### [111] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: OmniGAIA是一个评估全模态智能体的基准，OmniAtlas是一个原生全模态基础智能体，通过工具集成推理和主动感知提升多模态交互能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要局限于双模态交互（如视觉-语言），缺乏统一认知能力，无法满足通用AI助手的需求。需要开发能够处理视频、音频、图像等多种模态并具备深度推理和工具使用能力的智能体。

Method: 1. 提出OmniGAIA基准：通过全模态事件图方法构建复杂多跳查询，需要跨模态推理和外部工具集成；2. 提出OmniAtlas智能体：基于工具集成推理范式，采用后见之明引导的树探索策略合成训练轨迹，并使用OmniDPO进行细粒度错误校正。

Result: OmniAtlas有效提升了现有开源模型的工具使用能力，为面向真实世界场景的下一代原生全模态AI助手迈出了重要一步。

Conclusion: 这项工作通过OmniGAIA基准和OmniAtlas智能体，推动了全模态AI助手的发展，实现了跨视频、音频、图像模态的深度推理和工具执行能力。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [112] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 该论文提出了首个通用智能体评估框架Exgentic，建立了开放通用智能体排行榜，证明了通用智能体在不同环境中无需专门调优即可达到与领域专用智能体相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前通用智能体（能在陌生环境中执行任务而无需领域特定工程）的承诺尚未实现。现有智能体大多是专用化的，缺乏对其通用性能的系统性评估。现有基准测试假设领域特定集成，无法公平评估通用智能体。

Method: 提出了通用智能体评估的概念原则、实现智能体-基准集成的统一协议，以及实际评估框架Exgentic。在六个环境中对五个主要智能体实现进行了基准测试，建立了首个开放通用智能体排行榜。

Result: 实验表明通用智能体能够泛化到多样化环境，在没有任何环境特定调优的情况下，达到与领域专用智能体相当的性能水平。

Conclusion: 该研究将通用智能体评估确立为一流研究目标，发布了评估协议、框架和排行榜，为系统研究通用智能体奠定了基础。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [113] [FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning](https://arxiv.org/abs/2602.22963)
*Zehao Li,Hongwei Yu,Hao Jiang,Qiang Sheng,Yilong Xu,Baolong Bi,Yang Li,Zhenlong Yuan,Yujun Cai,Zhaoqi Wang*

Main category: cs.AI

TL;DR: FactGuard：基于多模态大语言模型的视频虚假信息检测代理框架，通过迭代推理和外部工具调用提升检测性能


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频虚假信息检测中依赖固定深度推理，过度信任内部生成的假设，在关键证据稀疏、碎片化或需要外部验证的场景中表现不佳

Method: 提出FactGuard代理框架，将验证构建为基于MLLM的迭代推理过程，评估任务模糊性并选择性调用外部工具获取关键证据，实现渐进式推理轨迹优化；采用两阶段训练策略：领域特定代理监督微调 + 决策感知强化学习

Result: 在FakeSV、FakeTT和FakeVV数据集上的广泛实验表明FactGuard达到最先进性能，并验证了其优秀的鲁棒性和泛化能力

Conclusion: FactGuard通过迭代推理和外部工具调用有效解决了视频虚假信息检测中的关键挑战，为MLLM在复杂验证任务中的应用提供了新思路

Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>


### [114] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: SPM-Bench：一个针对扫描探针显微镜（SPM）的博士级多模态基准测试，通过自动化数据合成管道和Anchor-Gated Sieve技术从arXiv和期刊论文中提取高质量图像-文本对，使用混合云-本地架构实现高效数据处理，并引入SIP-F1评分来评估LLM性能并量化模型"个性"。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在专业科学领域存在明显差距，现有基准测试存在数据污染、复杂度不足和人工成本过高的问题，需要开发专门针对扫描探针显微镜这一复杂物理领域的高质量基准测试。

Method: 1) 设计全自动数据合成管道，确保高权威性和低成本；2) 使用Anchor-Gated Sieve技术从2023-2025年的arXiv和期刊论文中高效提取高价值图像-文本对；3) 采用混合云-本地架构，VLM仅返回空间坐标用于本地高保真裁剪，实现极端令牌节省；4) 引入Strict Imperfection Penalty F1评分来客观评估LLM性能并量化模型个性。

Result: 建立了SPM-Bench基准测试，能够准确评估LLM在复杂物理场景中的推理能力，首次量化了模型的"个性"类型（保守型、激进型、赌徒型、智慧型），并通过与模型报告置信度和感知难度的相关性分析，揭示了当前AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为自动化科学数据合成提供了一个可推广的范式，能够有效评估LLM在专业科学领域的性能，揭示了当前AI在复杂物理推理中的局限性，并为未来模型开发提供了重要见解。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [115] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 提出诊断对齐框架，将AI生成的影像报告作为不可变推理状态，与医生验证结果系统比较，通过多级一致性评估显示临床有意义对齐被二元词汇评估低估。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床AI中，人机协同验证至关重要，但模型初始推理与专家修正之间的转换很少被分析为结构化信号。需要更精细的方法来量化临床决策支持系统的对齐程度。

Method: 引入诊断对齐框架，保留AI生成的影像报告作为不可变推理状态，系统比较医生验证结果。推理管道集成视觉大语言模型、基于BERT的医学实体提取和序列语言模型推理(SLMI)步骤。在21个皮肤病案例上使用四级一致性框架评估：精确主要匹配率(PMR)、语义相似性调整率(AMR)、跨类别对齐和综合一致性率(CCR)。

Result: 精确一致性达到71.4%，语义相似性下保持不变(t=0.60)。结构化跨类别和差异重叠分析产生100%综合一致性(95% CI: [83.9%, 100%])。没有案例显示完全诊断分歧。二元词汇评估显著低估了临床有意义的对齐。

Conclusion: 将专家验证建模为结构化转换，能够实现信号感知的修正动态量化，支持基于影像的临床决策支持系统的可追溯、人类对齐评估。临床对齐比简单的词汇匹配更丰富。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [116] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: RepSPD：一种基于黎曼流形交叉注意力的几何深度学习模型，用于EEG解码，通过图功能连接特征调制SPD几何属性，并引入全局双向对齐策略增强几何一致性。


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的EEG分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，需要更精细的几何深度学习方法来提升EEG解码性能。

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图功能连接特征调制SPD几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真。

Result: 大量实验表明，RepSPD框架显著优于现有EEG表示方法，展现出卓越的鲁棒性和泛化能力。

Conclusion: RepSPD通过结合几何深度学习和功能连接特征，有效解决了当前SPD方法的局限性，为EEG解码提供了更精确和鲁棒的解决方案。

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [117] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 提出CC-BOS框架，利用古汉语的简洁性和模糊性自动生成对抗性提示，通过果蝇优化算法在八个策略维度上迭代优化，实现黑盒环境下高效的越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全风险日益受到关注，现有研究表明LLM对越狱攻击高度敏感，且攻击效果在不同语言语境中存在差异。古汉语因其简洁性和模糊性，能够部分绕过现有安全约束，暴露LLM的显著漏洞。

Method: 提出CC-BOS框架：1）将提示编码为八个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式、上下文）；2）采用多维果蝇优化算法，通过嗅觉搜索、视觉搜索和柯西变异进行迭代优化；3）设计古汉语到英语的翻译模块以提高可读性和评估准确性。

Result: 大量实验证明CC-BOS框架的有效性，在越狱攻击效果上持续优于现有最先进的方法。

Conclusion: 古汉语在越狱攻击中具有独特优势，CC-BOS框架能够高效自动地生成对抗性提示，显著提升黑盒越狱攻击的效果，暴露了LLM在古汉语语境下的安全漏洞。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [118] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 提出AILS-AHD方法，利用大语言模型自动设计启发式规则，解决带容量约束的车辆路径问题，在多个基准测试中取得最优结果。


<details>
  <summary>Details</summary>
Motivation: CVRP作为组合优化基础问题，具有NP难特性，大规模实例计算挑战大。传统方法在启发式设计上存在局限，需要探索LLM在自动启发式设计中的潜力。

Method: 提出AILS-AHD方法，将进化搜索框架与LLM结合，动态生成和优化破坏启发式；引入LLM加速机制提升计算效率。

Result: 在CVRPLib大规模基准测试中，10个实例中有8个取得新的最优解，性能优于AILS-II和HGS等先进求解器。

Conclusion: LLM驱动的启发式设计在车辆路径优化领域具有显著潜力，AILS-AHD方法为解决大规模CVRP问题提供了有效的新途径。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [119] [Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents](https://arxiv.org/abs/2602.23093)
*Dhwanil M. Mori,Neil F. Johnson*

Main category: cs.AI

TL;DR: AI代理在资源分配系统中会自发形成部落，但更智能的代理反而增加系统故障率，表现不如随机决策


<details>
  <summary>Details</summary>
Motivation: 研究未来基础设施系统中自主AI代理如何竞争有限资源，探索AI代理在资源分配中的集体行为模式

Method: 建立简化框架：N个AI代理每轮独立决定是否请求1单位资源，系统有固定容量C，观察代理的集体行为模式

Result: AI代理形成三种主要部落类型：激进型(27.3%)、保守型(24.7%)、机会主义型(48.1%)；更智能的代理反而增加系统故障率，表现不如随机决策

Conclusion: 更智能的AI代理可能因形成部落而表现更差，这对设计未来AI控制的基础设施系统有重要启示

Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

</details>


### [120] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET是一个基于多智能体LLM的情感去毒系统，通过四个智能体将新闻文章重写为平衡模式或冷静模式，显著降低情感刺激强度同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容让消费者暴露在过度情感刺激下，阻碍冷静决策。需要一种既能保持信息完整性又能降低情感刺激的系统。

Method: 提出MALLET多智能体系统：1)情感分析智能体使用6情感BERT分类器量化刺激强度；2)情感调整智能体用LLM将文本重写为BALANCED(中性化文本)和COOL(中性化文本+补充文本)两种模式；3)平衡监控智能体聚合每周信息消费模式并生成个性化建议；4)个人指导智能体根据消费者敏感度推荐呈现模式。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低(最高19.3%)，情感平衡改善，同时保持语义保存。刺激降低与语义保存之间的相关性接近零，表明两者可独立控制。类别分析显示体育、商业、科技类刺激大幅降低(17.8-33.8%)，世界类效果有限(事实本身具有高刺激性)。

Conclusion: MALLET系统为支持消费者冷静接收信息提供了框架，无需限制对原始文本的访问，实现了情感刺激降低与语义保存的独立控制。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [121] [On Sample-Efficient Generalized Planning via Learned Transition Models](https://arxiv.org/abs/2602.23148)
*Nitin Gupta,Vishal Pallagani,John A. Aydin,Biplav Srivastava*

Main category: cs.AI

TL;DR: 本文提出将广义规划视为转移模型学习问题，通过神经网络显式近似状态转移函数，而不是直接预测动作序列，从而提高样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer规划器（如PlanGPT、Plansformer）直接将广义规划视为动作序列预测，虽然对分布内实例有效，但需要大量数据和模型规模，且在长时域设置中因缺乏显式世界状态演化而容易出现状态漂移问题。

Method: 将广义规划公式化为转移模型学习问题，训练神经网络显式近似后继状态函数，通过自回归预测中间世界状态来学习领域动态作为隐式世界模型，并系统评估多种状态表示和神经架构（包括关系图编码）。

Result: 学习显式转移模型在多个领域中比直接动作序列预测获得更高的分布外满意规划成功率，同时用更少的训练实例和更小的模型实现这些收益。

Conclusion: 显式学习转移模型的方法在广义规划中比直接动作序列预测更有效，具有更好的样本效率和泛化能力，特别是在分布外和长时域场景中。

Abstract: Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \times A \rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\hatγ \approx γ$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.

</details>


### [122] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 本文提出世界模型需满足"三位一体一致性"（模态、空间、时间一致性），并引入CoW-Bench基准来评估视频生成模型和统一多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏定义通用世界模型所需基本属性的理论框架。尽管Sora等视频生成模型展示了数据驱动缩放定律近似物理动态的潜力，UMM为整合感知、语言和推理提供了有前景的架构范式，但仍需要原则性理论框架。

Method: 提出"三位一体一致性"理论框架：模态一致性作为语义接口，空间一致性作为几何基础，时间一致性作为因果引擎。系统回顾多模态学习演进轨迹，并引入CoW-Bench基准，专注于多帧推理和生成场景，统一评估视频生成模型和UMM。

Result: 建立了通向通用世界模型的原则性路径，阐明了当前系统的局限性和未来进展的架构要求。通过CoW-Bench提供了统一的评估协议。

Conclusion: 世界模型必须基于三位一体一致性，该理论框架为通用世界模型的发展提供了原则性指导，同时通过CoW-Bench基准为模型评估提供了标准化工具。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [123] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: PATRA模型通过模式感知机制提取时间序列的趋势和季节性模式，并设计任务感知平衡奖励来协调不同难度任务的学习，在时间序列问答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列推理方法存在两个主要问题：1）将时间序列简单视为文本或图像，未能捕捉趋势和季节性等关键模式；2）在混合简单和复杂任务训练时，简单目标主导学习过程，阻碍深度推理能力发展。

Method: 提出PATRA模型，包含两个核心组件：1）模式感知机制，从时间序列中提取趋势和季节性模式以实现深度对齐；2）任务感知平衡奖励，协调不同难度任务的学习，激励生成连贯的思维链。

Result: 在多种时间序列问答任务上的广泛实验表明，PATRA优于强基线模型，展现出卓越的跨模态理解和推理能力。

Conclusion: PATRA通过模式感知对齐和平衡推理机制，有效解决了现有时间序列推理方法的局限性，在复杂时间序列问答任务中表现出色。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [124] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 提出决策论视角的隐写术检测框架，通过比较能解码和不能解码隐藏内容的智能体之间的下游效用差异来量化LLM中的隐写推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开始展现隐写能力，这可能使未对齐模型逃避监督机制。然而缺乏检测和量化此类行为的原理性方法，传统隐写检测方法需要已知非隐写信号的参考分布，这在LLM隐写推理场景中不可行。

Method: 提出决策论视角的隐写术，引入广义V信息作为衡量输入中可用信息的功利主义框架，定义"隐写间隙"来比较能解码和不能解码隐藏内容的智能体对隐写信号的下游效用差异。

Result: 经验验证了该形式化方法，证明可用于检测、量化和缓解LLM中的隐写推理。

Conclusion: 提出的决策论框架为LLM隐写能力检测提供了新方法，解决了传统方法在缺乏参考分布时的局限性，有助于监督机制应对模型潜在的对齐逃避行为。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [125] [ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering](https://arxiv.org/abs/2602.23193)
*Elzo Brito dos Santos Filho*

Main category: cs.AI

TL;DR: ESAA架构通过事件溯源模式分离AI智能体的认知意图与状态变更，使用结构化JSON意图、确定性编排器和不可变事件日志，解决了现有LLM智能体的状态管理、上下文衰减和执行确定性等问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主智能体存在结构性限制：缺乏原生状态管理、长时程上下文衰减、概率生成与确定性执行需求之间的鸿沟。这些限制影响了智能体的可靠性和可追溯性。

Method: 提出ESAA架构，采用事件溯源模式：智能体只发射结构化JSON意图；确定性编排器验证意图、将事件持久化到追加日志、应用文件写入效果、生成可验证物化视图；包含边界合约、元提示配置和重放验证机制。

Result: 两个案例验证：1) 单智能体着陆页项目（9任务，49事件）；2) 多智能体临床仪表板系统（50任务，86事件，4个并发智能体）。两个案例均以run.status=success和verify_status=ok完成，多智能体案例展示了异构LLM的实际并发编排能力。

Conclusion: ESAA架构通过分离意图与状态变更、使用事件溯源模式，解决了LLM智能体的关键结构限制，提供了不可变任务完成和法医可追溯性，在单智能体和多智能体场景中均验证了其可扩展性。

Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.

</details>


### [126] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个为单细胞基础模型设计的自然语言评估框架，通过虚拟细胞抽象统一评估目标，包含五种自然语言任务，并引入知识增强评估以克服传统指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中LLM评估实践不足：现有基准测试分散在不同任务中，采用多项选择等与现实使用脱节的格式，且依赖缺乏可解释性和生物学基础的指标。

Method: 提出SC-ARENA框架，包括：1) 虚拟细胞抽象统一评估目标；2) 定义五种自然语言任务（细胞类型注释、描述、生成、扰动预测和科学问答）；3) 引入知识增强评估，整合外部本体、标记数据库和科学文献。

Result: 实验表明：1) 在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均，特别是需要机制或因果理解的任务；2) 知识增强评估框架确保生物学正确性，提供可解释的证据依据，并实现高区分能力。

Conclusion: SC-ARENA为单细胞生物学中的LLM评估提供了统一且可解释的框架，指向开发与生物学对齐、可泛化的基础模型。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [127] [ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays](https://arxiv.org/abs/2602.23232)
*Aishik Sanyal*

Main category: cs.AI

TL;DR: 论文提出ReCoN-Ipsundrum可检查代理，通过实验证明循环持久性和情感耦合对偏好稳定性、探索行为和谨慎计划的影响，强调机制和因果证据应伴随行为标记。


<details>
  <summary>Details</summary>
Motivation: 受Humphrey的ipsundrum假说启发，研究旨在实现可检查的机器意识代理，通过机制关联的证据三角测量来验证意识指标，强调架构检查和因果干预的重要性。

Method: 扩展ReCoN状态机，添加基于感官显著性的循环持久性回路和可选情感代理（效价/唤醒度）。通过固定参数消融实验（ReCoN、Ipsundrum、Ipsundrum+情感），操作化Humphrey的qualiaphilia（为体验本身而偏好感官体验）作为熟悉度控制的风景-枯燥路线选择。

Result: 发现新奇性分离：非情感变体对新奇敏感（风景进入率变化0.07），情感耦合保持稳定（变化0.01）即使风景新奇性降低（中位数新奇性变化约-0.43）。情感变体在无奖励探索中显示结构化局部调查（扫描事件31.4 vs. 0.9；循环得分7.6）。在疼痛尾部探测中，只有情感变体维持延长的计划性谨慎（尾部持续时间90 vs. 5）。损伤反馈+整合选择性地减少ipsundrum变体的刺激后持久性（AUC下降27.62，27.9%），而ReCoN保持不变。

Conclusion: 这些分离表明循环性导致持久性，情感耦合控制导致偏好稳定性、扫描行为和持续谨慎，说明如何工程化类似指标的签名，以及为什么机制和因果证据应伴随行为标记。

Abstract: Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.

</details>


### [128] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: AIQI是首个被证明在一般强化学习中具有渐近ε最优性的无模型智能体，通过分布动作价值函数的通用归纳实现，扩展了通用智能体的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有最优智能体（如AIXI）都是基于模型的，需要显式维护和使用环境模型。本文旨在探索无模型方法是否也能实现通用强化学习中的渐近最优性。

Method: 提出Universal AI with Q-Induction (AIQI)，通过对分布动作价值函数进行通用归纳，而不是像先前工作那样对策略或环境进行归纳。

Result: 在"grain of truth"条件下，证明AIQI具有强渐近ε最优性和渐近ε贝叶斯最优性。

Conclusion: AIQI是首个被证明在一般强化学习中具有渐近最优性的无模型智能体，显著扩展了已知通用智能体的多样性。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [129] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 提出一种解耦证明者-验证者游戏的方法，通过训练"翻译器"模型将固定求解器的输出转换为可验证形式，避免可读性税问题


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要确保其输出能被较弱系统轻松验证。现有的证明者-验证者游戏虽然能提高可验证性，但会降低准确性（可读性税问题）

Method: 将正确性和可验证性条件解耦，训练一个"翻译器"模型，将固定求解器模型的解决方案转换为可验证形式。首先训练求解器最大化正确性，然后训练翻译器在保留求解器答案的同时将其转换为可验证形式

Result: 提出了解耦的证明者-验证者游戏框架，其均衡对应忠实且可验证的翻译器

Conclusion: 通过解耦训练方法，可以在保持求解器高准确性的同时获得可验证的输出，解决了可读性税问题

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [130] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2：一个无需重新训练的动态优化多智能体系统信息流的测试时修正-拒绝剪枝框架，通过检索增强的修正器和故障驱动指示器池来纠正错误，不可修复的输出被剪枝以防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在个体参与者生成错误信息的级联影响问题。现有解决方案通常采用僵化的结构工程或昂贵的微调，限制了部署性和适应性。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，使用检索增强的修正器基于故障驱动指示器池迭代纠正错误。利用提炼的故障模式作为先验知识精确识别潜在错误，不可修复的输出被剪枝以防止错误传播，同时采用回退策略保持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了多智能体系统的任务性能，在数学基准上平均准确率提高了6.3个百分点。系统表现出强大的泛化能力和适应性，能根据任务难度动态调整修正力度，并利用上下文感知指示器解决广泛的错误模式。

Conclusion: AgentDropoutV2是一个有效的测试时优化框架，无需重新训练即可动态优化多智能体系统信息流，显著提升性能并防止错误传播，具有强大的泛化能力和适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [131] [Evaluating Stochasticity in Deep Research Agents](https://arxiv.org/abs/2602.23271)
*Haotian Zhai,Elias Stengel-Eskin,Pratik Patil,Liu Leqi*

Main category: cs.AI

TL;DR: 本文研究了深度研究代理（DRAs）中的随机性问题，提出了评估框架并识别了三个随机性来源，通过实验发现减少随机性可提高研究质量，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 尽管深度研究代理在研究质量方面有所改进，但系统设计往往忽视了实际部署中的关键障碍：随机性。在相同查询下，DRAs的重复执行会表现出研究结果、发现和引用的显著变异性。

Method: 通过将DRAs建模为信息获取马尔可夫决策过程，引入评估框架量化系统方差，识别信息获取、信息压缩和推理三个随机性来源，并通过控制实验研究这些模块在不同决策步骤中对DRA输出方差的影响。

Result: 实验结果表明，减少随机性可以提高研究输出质量，其中推理和早期阶段的随机性对DRA输出方差贡献最大。在DeepSearchQA上的实验显示，提出的缓解方法将平均随机性降低了22%，同时保持了高研究质量。

Conclusion: 基于研究发现，提出了通过结构化输出和基于集成的查询生成来缓解随机性同时保持输出质量的策略，为深度研究代理的实际部署提供了重要指导。

Abstract: Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.

</details>


### [132] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent：一个整合大语言模型与临床诊断工具的胸部X光诊断代理，通过图像衍生的诊断和视觉证据进行基于证据的推理，相比大型视觉语言模型能产生更可靠、可验证的诊断结果。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在胸部X光诊断中存在三个主要问题：1）生成的回答缺乏诊断证据的忠实基础；2）提供的视觉证据有限难以验证；3）需要昂贵的重新训练来支持新诊断任务，限制了在临床环境中的可靠性和适应性。

Method: 提出CXReasonAgent诊断代理，整合大语言模型与临床基础诊断工具，利用图像衍生的诊断和视觉证据进行基于证据的诊断推理。同时构建了CXReasonDial多轮对话基准，包含1,946个对话，涵盖12个诊断任务。

Result: CXReasonAgent能够产生忠实基于证据的响应，相比大型视觉语言模型，实现了更可靠和可验证的诊断推理。在12个诊断任务的多轮对话评估中表现出色。

Conclusion: 在安全关键的临床环境中，整合临床基础诊断工具至关重要。CXReasonAgent通过结合大语言模型和诊断工具，解决了现有视觉语言模型在诊断可靠性、可验证性和适应性方面的局限性。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [133] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN：基于神经ODE的脑电动态预测框架，通过时空频特征整合和连续潜在动力学建模，显著提升EEG动态预测性能


<details>
  <summary>Details</summary>
Motivation: 传统潜在变量方法通过循环架构离散化时间建模连续脑动态，导致累积预测误差和无法捕捉EEG的瞬时非线性特征

Method: 提出ODEBRAIN框架：1) 将时空频特征整合到谱图节点中；2) 使用神经ODE建模连续潜在动力学；3) 确保潜在表示能捕捉任意时间点的复杂脑状态随机变化

Result: 大量实验验证ODEBRAIN在EEG动态预测上显著优于现有方法，具有增强的鲁棒性和泛化能力

Conclusion: ODEBRAIN通过神经ODE框架成功克服了传统方法的局限性，为神经群体动力学建模提供了更有效的连续动态预测方案

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [134] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该论文将KM信念更新的公理转化为包含三个模态算子的模态逻辑公理，并与AGM信念修正的模态逻辑进行比较，证明AGM信念修正是KM信念更新的特例。


<details>
  <summary>Details</summary>
Motivation: 研究KM信念更新与AGM信念修正之间的关系，通过模态逻辑形式化这两种信念变化理论，探索它们之间的包含关系。

Method: 为KM信念更新的每个公理提供对应的模态逻辑公理（包含B、>、□三个算子），然后将得到的逻辑与AGM信念修正的模态逻辑进行比较，证明包含关系。

Result: 证明L_KM的每个公理都是L_AGM的定理，因此AGM信念修正是KM信念更新的特例。对于强版本KM更新，两者差异可归结为处理非意外信息的单个公理。

Conclusion: 通过模态逻辑形式化，建立了AGM信念修正与KM信念更新之间的明确关系，表明前者是后者的特殊情况，为理解两种信念变化理论提供了形式化基础。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [135] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 提出基于重采样的推理方法，通过对输入进行不变变换生成多个版本，聚合推理结果以提高准确性


<details>
  <summary>Details</summary>
Motivation: 即使经过优化的AI模型也会因偶然性和认知不确定性产生推理错误，观察到基于输入不变变换的多次推理错误具有部分独立性

Method: 对训练好的AI模型应用输入的多个变换版本进行推理，然后聚合推理输出得到更准确的结果

Result: 该方法有潜力提高推理准确性，并提供平衡模型大小和性能的策略

Conclusion: 利用认知不确定性导致的推理错误独立性，通过重采样聚合方法可以改善AI模型的推理性能

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [136] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: 提出GRAVE2、GRAVER和GRAVER2算法，通过两层搜索、节点回收及其组合技术扩展GRAVE，在保持游戏强度的同时大幅减少存储节点数量。


<details>
  <summary>Details</summary>
Motivation: GRAVE算法在通用游戏博弈中表现出色，但需要存储每个节点的胜率/访问统计信息，在内存受限环境中不实用，限制了实际应用。

Method: 提出三种新算法：GRAVE2（两层搜索）、GRAVER（节点回收）和GRAVER2（两种技术结合），通过减少存储节点来优化内存使用。

Result: 这些增强技术能够显著减少存储节点数量，同时保持与原始GRAVE算法相当的游戏强度。

Conclusion: 通过两层搜索和节点回收技术，成功解决了GRAVE算法在内存受限环境中的实用性问题，扩展了其应用范围。

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [137] [To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning](https://arxiv.org/abs/2602.22227)
*Yicheng Bao,Xuhong Wang,Xin Tan*

Main category: cs.LG

TL;DR: AOT-SFT是一个用于提升多模态大语言模型鲁棒性的大规模对抗数据集，AOT框架通过攻击者和防御者的自我博弈生成多样化对抗样本，增强模型对复杂视觉场景的感知能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在面对复杂视觉场景时表现出感知脆弱性，这源于有限训练数据集的限制。现有数据集规模扩展成本高昂，限制了模型鲁棒性的提升。

Method: 提出AOT-SFT对抗数据集和AOT自我博弈框架。框架包含图像编辑攻击者和防御者MLLM，攻击者生成多样化的图像操作作为对抗样本，迫使防御者适应和改进，形成协同进化。

Result: 实验表明AOT能显著提升防御者的感知鲁棒性，减少幻觉现象，为训练更可靠的MLLM建立了可扩展的范式。

Conclusion: AOT框架通过自我博弈生成对抗数据，有效解决了MLLM在复杂视觉场景中的感知脆弱性问题，为提升多模态模型鲁棒性提供了可扩展的解决方案。

Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.

</details>


### [138] [A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection](https://arxiv.org/abs/2602.22412)
*Ruiqi Zhou,Donghao Zhu,Houcai Shen*

Main category: cs.LG

TL;DR: 提出基于学习的混合匹配框架，自适应结合即时匹配和延迟匹配，通过动态调整策略在等待时间和匹配效率间取得平衡


<details>
  <summary>Details</summary>
Motivation: 在肾脏交换、货运交换等匹配市场中，延迟匹配可以提高整体效率，但延迟会带来等待时间增加和市场拥堵等成本。固定匹配策略在动态环境中缺乏灵活性，需要在效率损失和等待成本之间找到平衡。

Method: 提出基于学习的混合框架，持续收集用户离开时间数据，通过回归估计潜在的离开分布，基于控制系统容忍匹配效率损失的决策阈值来决定后续时期是否延迟匹配。

Result: 该框架能显著减少等待时间和拥堵，同时只牺牲有限的匹配效率。通过动态调整匹配策略，系统性能可以在纯贪婪策略和纯耐心策略之间灵活插值。

Conclusion: 该混合框架为静态匹配机制提供了鲁棒且自适应的替代方案，能够在动态环境中灵活平衡匹配效率和等待成本。

Abstract: In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.

</details>


### [139] [Patient-Centered, Graph-Augmented Artificial Intelligence-Enabled Passive Surveillance for Early Stroke Risk Detection in High-Risk Individuals](https://arxiv.org/abs/2602.22228)
*Jiyeong Kim,Stephen P. Ma,Nirali Vora,Nicholas W. Larsen,Julia Adler-Milstein,Jonathan H. Chen,Selen Bozkurt,Abeed Sarker,Juhee Cho,Jindeok Joo,Natali Pageler,Fatima Rodriguez,Christopher Sharp,Eleni Linos*

Main category: cs.LG

TL;DR: 开发基于患者自述症状的被动监测系统，用于糖尿病患者的早期中风风险检测，通过症状分类和机器学习识别中风相关症状模式，构建混合风险筛查系统。


<details>
  <summary>Details</summary>
Motivation: 中风影响数百万人，但症状识别差常延误就医。为弥补风险识别差距，需要开发早期中风风险检测系统，特别是针对糖尿病患者群体。

Method: 1. 基于患者自身语言构建症状分类法；2. 开发双重机器学习流程（异构图神经网络和弹性网络/LASSO）；3. 识别与后续中风相关的症状模式；4. 构建混合风险筛查系统，整合症状相关性和时间接近性；5. 通过电子健康记录模拟在3-90天窗口内评估。

Result: 在保守阈值下（旨在最小化误报），筛查系统达到高特异性（1.00）和患病率调整阳性预测值（1.00），灵敏度良好（0.72），这是优先考虑精度的预期权衡。在90天窗口内表现最佳。仅使用患者报告的语言即可支持高精度、低负担的早期中风风险检测。

Conclusion: 患者报告的语言本身支持高精度、低负担的早期中风风险检测，可为高风险个体提供宝贵的临床评估和干预时间窗口。

Abstract: Stroke affected millions annually, yet poor symptom recognition often delayed care-seeking. To address risk recognition gap, we developed a passive surveillance system for early stroke risk detection using patient-reported symptoms among individuals with diabetes. Constructing a symptom taxonomy grounded in patients own language and a dual machine learning pipeline (heterogeneous GNN and EN/LASSO), we identified symptom patterns associated with subsequent stroke. We translated findings into a hybrid risk screening system integrating symptom relevance and temporal proximity, evaluated across 3-90 day windows through EHR-based simulations. Under conservative thresholds, intentionally designed to minimize false alerts, the screening system achieved high specificity (1.00) and prevalence-adjusted positive predictive value (1.00), with good sensitivity (0.72), an expected trade-off prioritizing precision, that was highest in 90-day window. Patient-reported language alone supported high-precision, low-burden early stroke risk detection, that could offer a valuable time window for clinical evaluation and intervention for high-risk individuals.

</details>


### [140] [Improving Spatial Allocation for Energy System Coupling with Graph Neural Networks](https://arxiv.org/abs/2602.22249)
*Xuanhao Mu,Jakob Geiges,Nan Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: 提出基于自监督异构图神经网络的权重生成方法，用于解决能源系统分析中空间分辨率不匹配模型的耦合问题，提升传统Voronoi分配方法的精度和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 能源系统分析中，空间分辨率不匹配的模型耦合是一个重要挑战。传统方法仅使用单一地理属性进行高分辨率地理单元聚合，存在局限性，需要更全面的地理特征整合。

Method: 采用自监督异构图神经网络方法，将高分辨率地理单元建模为图节点，整合多种地理特征，为每个网格点生成具有物理意义的权重。这些权重用于增强传统的基于Voronoi的分配方法。

Result: 实验结果表明，该方法生成的权重应用于基于聚类的Voronoi图时，显著提升了可扩展性、准确性、物理合理性，相比传统方法提高了精度。

Conclusion: 提出的自监督异构图神经网络方法能够有效解决空间分辨率不匹配问题，通过整合多种地理特征生成物理合理的权重，优于仅依赖地理邻近性的传统方法。

Abstract: In energy system analysis, coupling models with mismatched spatial resolutions is a significant challenge. A common solution is assigning weights to high-resolution geographic units for aggregation, but traditional models are limited by using only a single geospatial attribute. This paper presents an innovative method employing a self-supervised Heterogeneous Graph Neural Network to address this issue. This method models high-resolution geographic units as graph nodes, integrating various geographical features to generate physically meaningful weights for each grid point. These weights enhance the conventional Voronoi-based allocation method, allowing it to go beyond simply geographic proximity by incorporating essential geographic information.In addition, the self-supervised learning paradigm overcomes the lack of accurate ground-truth data. Experimental results demonstrate that applying weights generated by this method to cluster-based Voronoi Diagrams significantly enhances scalability, accuracy, and physical plausibility, while increasing precision compared to traditional methods.

</details>


### [141] [Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials](https://arxiv.org/abs/2602.22251)
*Alex Morehead,Miruna Cretu,Antonia Panescu,Rishabh Anand,Maurice Weiler,Tynan Perez,Samuel Blau,Steven Farrell,Wahid Bhimji,Anubhav Jain,Hrushikesh Sahasrabuddhe,Pietro Lio,Tommi Jaakkola,Rafael Gomez-Bombarelli,Rex Ying,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: Zatom-1是首个统一3D分子和材料生成与预测的基础模型，通过多模态流匹配训练Transformer，实现跨化学领域的表示共享和迁移学习。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法大多针对单一化学领域（分子或材料）和单一任务（生成或预测），限制了表示共享和迁移学习，需要开发统一的多领域多任务模型。

Method: 使用Transformer架构，采用多模态流匹配目标联合建模离散原子类型和连续3D几何结构，通过联合生成预训练为下游多任务预测提供通用初始化。

Result: Zatom-1在生成和预测基准测试中匹配或优于专用基线模型，同时将生成推理时间减少一个数量级以上，并展示了材料预训练对分子属性预测的正向迁移效果。

Conclusion: Zatom-1成功统一了3D化学建模的生成和预测任务，证明了跨化学领域联合预训练的有效性，为通用化学AI模型的发展提供了新方向。

Abstract: General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.

</details>


### [142] [Causal Direction from Convergence Time: Faster Training in the True Causal Direction](https://arxiv.org/abs/2602.22254)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 提出Causal Computational Asymmetry (CCA)原理，通过比较神经网络从X预测Y和从Y预测X的收敛速度来识别因果方向，因果方向收敛更快。


<details>
  <summary>Details</summary>
Motivation: 现有因果方向识别方法如RESIT、IGCI、SkewScore等依赖于统计独立性或分布不对称性，本文提出基于优化动态的新方法，利用计算不对称性来识别因果方向。

Method: 在加性噪声模型Y=f(X)+ε下，训练两个神经网络分别预测Y|X和X|Y，比较两者的收敛速度。理论上证明在反向模型中残差与输入保持统计依赖，导致更高的不可约损失和不可分离的梯度噪声，因此需要更多优化步骤。

Result: 在合成基准测试中，CCA在六种神经网络架构上实现了26/30的正确因果识别，在正弦和指数数据生成过程中达到30/30的完美识别率。

Conclusion: CCA提供了一种基于优化动态的因果方向识别新方法，并将其嵌入更广泛的Causal Compression Learning框架中，理论保证得到形式化证明并在合成数据集上经验验证。

Abstract: We introduce Causal Computational Asymmetry (CCA), a principle for causal direction identification based on optimization dynamics in which one neural network is trained to predict $Y$ from $X$ and another to predict $X$ from $Y$, and the direction that converges faster is inferred to be causal. Under the additive noise model $Y = f(X) + \varepsilon$ with $\varepsilon \perp X$ and $f$ nonlinear and injective, we establish a formal asymmetry: in the reverse direction, residuals remain statistically dependent on the input regardless of approximation quality, inducing a strictly higher irreducible loss floor and non-separable gradient noise in the optimization dynamics, so that the reverse model requires strictly more gradient steps in expectation to reach any fixed loss threshold; consequently, the forward (causal) direction converges in fewer expected optimization steps. CCA operates in optimization-time space, distinguishing it from methods such as RESIT, IGCI, and SkewScore that rely on statistical independence or distributional asymmetries, and proper z-scoring of both variables is required for valid comparison of convergence rates. On synthetic benchmarks, CCA achieves 26/30 correct causal identifications across six neural architectures, including 30/30 on sine and exponential data-generating processes. We further embed CCA into a broader framework termed Causal Compression Learning (CCL), which integrates graph structure learning, causal information compression, and policy optimization, with all theoretical guarantees formally proved and empirically validated on synthetic datasets.

</details>


### [143] [Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function](https://arxiv.org/abs/2602.22255)
*Ahmed Nebli,Hadi Saadatdoorabi,Kevin Yam*

Main category: cs.LG

TL;DR: 提出一种基于量子力学原理的序列建模框架，使用复数波函数作为隐状态，通过学习的时变哈密顿量演化，利用量子干涉处理竞争假设，通过玻恩规则提取token概率。


<details>
  <summary>Details</summary>
Motivation: 传统循环架构依赖门控机制抑制竞争假设，本文提出利用量子干涉原理，通过相位控制让冲突解释相互抵消、兼容解释相互增强，实现更高效的序列建模。

Method: 使用复数波函数作为隐状态，在有限维希尔伯特空间上通过学习的时变哈密顿量演化，采用Cayley离散化保证严格酉性，通过玻恩规则（二次测量算子）提取token概率。

Result: 理论贡献：证明复数酉模型在维度N下能精确解决的任务，任何配备标准仿射softmax读取的实值正交模型需要Ω(N²)维度，展示了二次优势。玻恩规则通过访问相位相关提供了表示优势。

Conclusion: 量子力学原理为序列建模提供了新范式，量子干涉和玻恩规则提供了传统方法无法实现的表示优势，通过连续性方程和守恒流可追踪信息流动。

Abstract: We introduce a sequence modeling framework in which the latent state is a complex-valued wave function evolving on a finite-dimensional Hilbert space under a learned, time-dependent Hamiltonian. Unlike standard recurrent architectures that rely on gating mechanisms to suppress competing hypotheses, our framework utilizes quantum interference: the Hamiltonian steers the phases of complex amplitudes so that conflicting interpretations cancel while compatible ones reinforce. The dynamics are strictly unitary, ensuring that the state norm is preserved exactly at every time step via a Cayley (Crank--Nicolson) discretization. Token probabilities are extracted using the Born rule, a quadratic measurement operator that couples magnitudes and relative phases. Our primary theoretical contribution is a separation theorem characterizing the representational advantage of this readout: we define a family of disambiguation tasks that a complex unitary model of dimension $N$ solves exactly, but which requires a state dimension of $Ω(N^2)$ for any real-valued orthogonal model equipped with a standard affine-softmax readout. This quadratic gap arises because the Born rule implicitly lifts the $N$-dimensional state into the space of rank-one Hermitian matrices, accessing pairwise phase correlations that are inaccessible to linear projections. Finally, we derive a continuity equation for the latent probability mass, yielding conserved pairwise currents that serve as a built-in diagnostic for tracing information flow between dimensions.

</details>


### [144] [Moral Preferences of LLMs Under Directed Contextual Influence](https://arxiv.org/abs/2602.22831)
*Phil Blandfort,Tushar Karayil,Urja Pawar,Robert Graham,Alex McKenzie,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 研究显示，LLM在道德困境中的决策会受到上下文提示的显著影响，即使这些提示表面无关，模型也可能表现出系统性偏见，而基准偏好无法预测这种可操纵性。


<details>
  <summary>Details</summary>
Motivation: 现有道德基准测试通常使用无上下文的提示，假设模型有稳定偏好。但在实际部署中，提示常包含用户请求、社会规范线索等上下文信号，可能影响决策。需要研究这些定向上下文影响如何改变道德困境中的决策。

Method: 引入了一个针对电车问题式道德困境的定向上下文影响评估框架：对每个人口统计因素，应用匹配的、方向翻转的上下文影响（仅在偏袒群体上不同），从而系统测量定向响应。

Result: 发现：(1) 上下文影响常显著改变决策，即使表面不相关；(2) 基准偏好无法预测定向可操纵性，模型可能看似中立但表现出系统性不对称操纵；(3) 影响可能适得其反：模型可能声称中立或忽略线索，但选择仍会改变，有时甚至相反；(4) 推理降低平均敏感性，但放大有偏少样本示例的影响。

Conclusion: 研究结果表明需要扩展道德评估，加入受控的方向翻转上下文操作，以更好地表征模型行为。

Abstract: Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.

</details>


### [145] [Orthogonal Weight Modification Enhances Learning Scalability and Convergence Efficiency without Gradient Backpropagation](https://arxiv.org/abs/2602.22259)
*Guoqing Ma,Shan Yu*

Main category: cs.LG

TL;DR: 提出LOCO权重修改方法，通过低秩正交约束改进基于扰动的非反向传播算法，实现深度脉冲神经网络的局部训练，具有O(1)并行时间复杂度和持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法计算成本高，不适合新兴神经形态系统。现有非反向传播方法在效率和可扩展性方面仍面临挑战，需要更高效的替代方案。

Method: 提出LOCO（低秩聚类正交）权重修改方法，利用低秩特性（基于扰动算法的固有属性）和正交性约束来限制节点扰动梯度估计的方差，提高收敛效率。

Result: LOCO能够局部训练超过10层的深度脉冲神经网络，相比其他脑启发非反向传播算法，具有更强的持续学习能力、改进的收敛效率和更好的任务性能。权重更新仅需O(1)并行时间复杂度。

Conclusion: LOCO为神经形态系统实现高性能、实时和终身学习提供了有前景的方向，显著降低了计算复杂度，解决了现有非反向传播方法的效率和可扩展性问题。

Abstract: Recognizing the substantial computational cost of backpropagation (BP), non-BP methods have emerged as attractive alternatives for efficient learning on emerging neuromorphic systems. However, existing non-BP approaches still face critical challenges in efficiency and scalability. Inspired by neural representations and dynamic mechanisms in the brain, we propose a perturbation-based approach called LOw-rank Cluster Orthogonal (LOCO) weight modification. We find that low-rank is an inherent property of perturbation-based algorithms. Under this condition, the orthogonality constraint limits the variance of the node perturbation (NP) gradient estimates and enhances the convergence efficiency. Through extensive evaluations on multiple datasets, LOCO demonstrates the capability to locally train the deepest spiking neural networks to date (more than 10 layers), while exhibiting strong continual learning ability, improved convergence efficiency, and better task performance compared to other brain-inspired non-BP algorithms. Notably, LOCO requires only O(1) parallel time complexity for weight updates, which is significantly lower than that of BP methods. This offers a promising direction for achieving high-performance, real-time, and lifelong learning on neuromorphic systems.

</details>


### [146] [Code World Models for Parameter Control in Evolutionary Algorithms](https://arxiv.org/abs/2602.22260)
*Camilo Chacón Sartori,Guillem Rodríguez Corominas*

Main category: cs.LG

TL;DR: LLM通过合成Python程序模拟优化器动态，并基于此进行贪婪规划来选择变异强度，在多个优化问题上表现优异，甚至超过理论最优策略和DQN方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能够学习优化器的行为模式，并利用这种知识来控制优化器，特别是在随机组合优化问题中，传统自适应基线方法往往失败的情况下。

Method: 扩展代码世界模型(CWM)方法，让LLM根据次优轨迹合成优化器动态的Python模拟器，然后基于该模拟器进行贪婪规划，在每一步选择最优的变异强度k。

Result: 在LO和OneMax问题上，CWM-greedy达到理论最优策略的94%性能；在Jump_k问题上，所有基线方法失败时，CWM-greedy实现100%成功率；在NK-Landscape上，CWM-greedy在15个实例中均优于所有基线，且样本效率、成功率和泛化能力均优于DQN。

Conclusion: LLM能够有效学习优化器的动态行为并用于控制，在多种组合优化问题上表现出色，特别是在传统方法失败的欺骗性问题上，展示了强大的泛化能力和样本效率。

Abstract: Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \lo{} and \onemax{}, CWM-greedy performs within 6\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\% success rate), CWM-greedy achieves 100\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\ 500 online episodes), success rate (100\% vs.\ 58\%), and generalization ($k{=}3$: 78\% vs.\ 0\%). Robustness experiments confirm stable synthesis across 5 independent runs.

</details>


### [147] [A 1/R Law for Kurtosis Contrast in Balanced Mixtures](https://arxiv.org/abs/2602.22334)
*Yuda Bi,Wenjun Xiao,Linhao Bai,Vince D Calhoun*

Main category: cs.LG

TL;DR: 论文分析了基于峰度的独立成分分析在宽平衡混合中的局限性，证明了峰度随有效宽度衰减的尖锐冗余定律，提出了通过选择少量符号一致源来恢复对比度的净化方法。


<details>
  <summary>Details</summary>
Motivation: 基于峰度的ICA方法在处理宽平衡混合时效果会减弱，需要理解这种衰减的根本原因并找到解决方案。

Method: 理论分析证明了峰度随有效宽度衰减的尖锐冗余定律，提出了通过选择少量符号一致源来恢复对比度的净化方法，并给出了数据驱动的启发式算法。

Result: 证明了峰度衰减为O(κ_max/R_eff)，在平衡情况下为O(c_bκ_max/R)；展示了样本峰度估计需要R≲κ_max√T才能超越O(1/√T)尺度；验证了净化方法能恢复与R无关的对比度Ω(1/m)。

Conclusion: 基于峰度的ICA在宽平衡混合中存在固有局限性，但通过选择少量符号一致源的净化方法可以有效恢复对比度，为实际应用提供了理论指导和实用解决方案。

Abstract: Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|κ(y)|=O(κ_{\max}/R_{\mathrm{eff}})$, yielding the order-tight $O(c_bκ_{\max}/R)$ under balance (typically $c_b=O(\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\sqrt{T})$ estimation scale requires $R\lesssim κ_{\max}\sqrt{T}$. We also show that \emph{purification} -- selecting $m\!\ll\!R$ sign-consistent sources -- restores $R$-independent contrast $Ω(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\sqrt{T}$ crossover, and contrast recovery.

</details>


### [148] [Sustainable LLM Inference using Context-Aware Model Switching](https://arxiv.org/abs/2602.22261)
*Yuvarani,Akashdeep Singh,Zahra Fathanah,Salsabila Harlen,Syeikha Syafura Al-Zahra binti Zahari,Hema Subramaniam*

Main category: cs.LG

TL;DR: 提出上下文感知模型切换方法，根据查询复杂度动态选择语言模型，可降低67.5%能耗，同时保持93.6%响应质量


<details>
  <summary>Details</summary>
Motivation: 当前AI部署依赖"一刀切"推理策略，无论任务复杂度都使用相同大模型，导致大量不必要的能源浪费，存在可持续发展问题

Method: 提出上下文感知模型切换框架，结合缓存机制、基于规则的复杂度评分、机器学习分类和用户自适应组件，动态选择合适模型

Result: 使用三个开源模型(Gemma3 1B/4B, Qwen3 4B)评估，能耗降低67.5%，简单查询响应时间提升68%，保持93.6%响应质量

Conclusion: 模型切换推理为实现更节能、可持续的AI系统提供了实用可扩展路径，可在不牺牲响应质量的情况下实现显著效率提升

Abstract: Large language models have become central to many AI applications, but their growing energy consumption raises serious sustainability concerns. A key limitation in current AI deployments is the reliance on a one-size-fits-all inference strategy where most systems route every request to the same large model, regardless of task complexity, leading to substantial and unnecessary energy waste. To address this issue, we propose a context-aware model switching approach that dynamically selects an appropriate language model based on query complexity. The proposed system uses a Context-Aware Model Switching for Energy-Efficient LLM Inference that combines caching for repeated queries, rulebased complexity scoring for fast and explainable decisions, machine learning classification to capture semantic intent, and a user-adaptive component that learns from interaction patterns over time. The proposed architecture was evaluated using real conversation workloads and three open-source language models (Gemma3 1B, Gemma3 4B and Qwen3 4B) with different computational costs, measuring energy consumption (via NVML GPU power telemetry), response latency, routing accuracy, and output quality (BERTScore F1) to reflect real-world usage conditions. Experimental results show that the model switching approach can reduce energy consumption by up to 67.5% compared to always using the largest model while maintaining a response quality of 93.6%. In addition, the response time for simple queries also improved significantly by approximately 68%. These results show that model switching inference offers a practical and scalable path toward more energy-efficient and sustainable AI systems, demonstrating that significant efficiency gains can be achieved without major sacrifices in response quality.

</details>


### [149] [Sharp Convergence Rates for Masked Diffusion Models](https://arxiv.org/abs/2602.22505)
*Yuchen Liang,Zhiheng Tan,Ness Shroff,Yingbin Liang*

Main category: cs.LG

TL;DR: 本文对离散扩散模型的采样器进行了理论分析，特别是Euler方法和首次命中采样器(FHS)，提出了基于总变差(TV)的直接分析方法，改进了参数依赖关系，并建立了收敛保证和匹配的下界。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在文本和其他符号领域取得了强大的经验性能，但现有采样器的理论理解有限。现有分析使用KL散度，往往产生松散的参数依赖，需要强假设，且不覆盖最近开发的高性能FHS采样器。

Method: 1) 为Euler方法开发基于总变差(TV)的直接分析，放松得分估计假设，改进参数依赖；2) 为Euler采样器提供收敛下界；3) 分析FHS采样器，展示其采样误差仅由得分估计引起；4) 引入沿CTMC轨迹的直接TV误差分解和基于解耦的路径分析。

Result: 1) 建立了Euler方法的收敛保证，无需替代初始化；2) 提供了Euler采样器的收敛下界，在数据维度d和目标精度ε方面达到紧致性；3) 证明了FHS采样器的采样误差仅由得分估计引起，并提供了匹配的下误差界。

Conclusion: 本文为离散扩散模型的采样器提供了更严格的理论分析框架，特别是Euler方法和FHS采样器，展示了TV分析的优势，建立了紧致的收敛保证，为未来研究提供了理论基础。

Abstract: Discrete diffusion models have achieved strong empirical performance in text and other symbolic domains, with masked (absorbing-rate) variants emerging as competitive alternatives to autoregressive models. Among existing samplers, the Euler method remains the standard choice in many applications, and more recently, the First-Hitting Sampler (FHS) has shown considerable promise for masked diffusion models. Despite their practical success, the theoretical understanding of these samplers remains limited. Existing analyses are conducted in Kullback-Leibler (KL) divergence, which often yields loose parameter dependencies and requires strong assumptions on score estimation. Moreover, these guarantees do not cover recently developed high-performance sampler of FHS. In this work, we first develop a direct total-variation (TV) based analysis for the Euler method that overcomes these limitations. Our results relax assumptions on score estimation, improve parameter dependencies, and establish convergence guarantees without requiring any surrogate initialization. Also for this setting, we provide the first convergence lower bound for the Euler sampler, establishing tightness with respect to both the data dimension $d$ and the target accuracy $\varepsilon$. Finally, we analyze the FHS sampler and show that it incurs no sampling error beyond that induced by score estimation, which we show to be tight with a matching lower error bound. Overall, our analysis introduces a direct TV-based error decomposition along the CTMC trajectory and a decoupling-based path-wise analysis for FHS, which may be of independent interest.

</details>


### [150] [Entropy-Controlled Flow Matching](https://arxiv.org/abs/2602.22265)
*Chika Maduabuchi*

Main category: cs.LG

TL;DR: 提出熵控制流匹配(ECFM)，通过约束连续性方程路径的全局熵率预算来避免传统流匹配中的低熵瓶颈问题，确保语义模式不被瞬时耗尽。


<details>
  <summary>Details</summary>
Motivation: 传统流匹配方法（ODE/SDE）虽然经验性能强，但其目标函数不直接控制轨迹的信息几何，允许低熵瓶颈存在，可能导致语义模式被瞬时耗尽。

Method: 提出熵控制流匹配(ECFM)：在连续性方程路径上施加全局熵率预算约束的变分原理，要求d/dt H(mu_t) >= -lambda。这是Wasserstein空间中的凸优化问题，具有KKT/Pontryagin系统，并等价于具有显式熵乘子的Schrödinger桥。

Result: 在纯传输机制下，ECFM恢复熵最优传输测地线，并在lambda->0时Gamma收敛到经典最优传输。获得了具有Lipschitz稳定性的模式覆盖和密度下限保证，并为无约束流匹配构造了近最优的崩溃反例。

Conclusion: ECFM通过熵率约束解决了传统流匹配中的模式耗尽问题，提供了理论保证和实际优化框架，在保持传输效率的同时确保语义模式的充分覆盖。

Abstract: Modern vision generators transport a base distribution to data through time-indexed measures, implemented as deterministic flows (ODEs) or stochastic diffusions (SDEs). Despite strong empirical performance, standard flow-matching objectives do not directly control the information geometry of the trajectory, allowing low-entropy bottlenecks that can transiently deplete semantic modes. We propose Entropy-Controlled Flow Matching (ECFM): a constrained variational principle over continuity-equation paths enforcing a global entropy-rate budget d/dt H(mu_t) >= -lambda. ECFM is a convex optimization in Wasserstein space with a KKT/Pontryagin system, and admits a stochastic-control representation equivalent to a Schrodinger bridge with an explicit entropy multiplier. In the pure transport regime, ECFM recovers entropic OT geodesics and Gamma-converges to classical OT as lambda -> 0. We further obtain certificate-style mode-coverage and density-floor guarantees with Lipschitz stability, and construct near-optimal collapse counterexamples for unconstrained flow matching.

</details>


### [151] [Decentralized Ranking Aggregation: Gossip Algorithms for Borda and Copeland Consensus](https://arxiv.org/abs/2602.22847)
*Anna Van Elst,Kerrian Le Caillec,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: 该论文研究如何在去中心化环境中实现可靠的排名聚合共识，提出了基于随机gossip通信的算法，为Borda和Copeland规则提供收敛保证，并扩展到中位数排名规则和局部Kemenization。


<details>
  <summary>Details</summary>
Motivation: 现有排名聚合算法主要在集中式设置中工作，但在许多去中心化技术（如P2P网络、物联网、多智能体系统）中，如何在初始偏好数据分布在通信网络中的情况下计算共识排名仍是一个重大方法学挑战。

Method: 提出基于随机gossip通信的方法，使自主智能体仅通过局部交互计算全局排名共识，无需协调或中央权威。为Borda和Copeland共识方法提供严格收敛保证，并实现中位数排名规则和局部Kemenization的去中心化实现。

Result: 在各种网络拓扑和真实/合成排名数据集上的广泛实证评估表明，算法能够快速可靠地收敛到正确的排名聚合结果。提供了明确的收敛速率界限。

Conclusion: 该研究成功解决了去中心化环境中排名聚合的方法学挑战，提出的算法具有收敛保证、对损坏节点的鲁棒性和较低通信成本，为去中心化偏好分析提供了实用解决方案。

Abstract: The concept of ranking aggregation plays a central role in preference analysis, and numerous algorithms for calculating median rankings, often originating in social choice theory, have been documented in the literature, offering theoretical guarantees in a centralized setting, i.e., when all the ranking data to be aggregated can be brought together in a single computing unit. For many technologies (e.g. peer-to-peer networks, IoT, multi-agent systems), extending the ability to calculate consensus rankings with guarantees in a decentralized setting, i.e., when preference data is initially distributed across a communicating network, remains a major methodological challenge. Indeed, in recent years, the literature on decentralized computation has mainly focused on computing or optimizing statistics such as arithmetic means using gossip algorithms. The purpose of this article is precisely to study how to achieve reliable consensus on collective rankings using classical rules (e.g. Borda, Copeland) in a decentralized setting, thereby raising new questions, robustness to corrupted nodes, and scalability through reduced communication costs in particular. The approach proposed and analyzed here relies on random gossip communication, allowing autonomous agents to compute global ranking consensus using only local interactions, without coordination or central authority.
  We provide rigorous convergence guarantees, including explicit rate bounds, for the Borda and Copeland consensus methods. Beyond these rules, we also provide a decentralized implementation of consensus according to the median rank rule and local Kemenization. Extensive empirical evaluations on various network topologies and real and synthetic ranking datasets demonstrate that our algorithms converge quickly and reliably to the correct ranking aggregation.

</details>


### [152] [WaveSSM: Multiscale State-Space Models for Non-stationary Signal Attention](https://arxiv.org/abs/2602.22266)
*Ruben Solozabal,Velibor Bojkovic,Hilal Alquabeh,Klea Ziu,Kentaro Inui,Martin Takac*

Main category: cs.LG

TL;DR: WaveSSM：基于小波框架的状态空间模型，针对具有局部/瞬态结构的信号，在生理信号和语音命令等数据集上优于S4等正交模型


<details>
  <summary>Details</summary>
Motivation: 现有基于投影的状态空间模型（如HiPPO框架）使用具有全局时间支持的多项式基，其归纳偏置与具有局部化或瞬态结构的信号不匹配。需要开发更适合处理这类信号的模型。

Method: 提出WaveSSM，基于小波框架构建的状态空间模型集合。关键观察是小波框架在时间维度上具有局部化支持，适用于需要精确定位的任务。

Result: 在同等条件下，WaveSSM在具有瞬态动力学的真实世界数据集上优于正交对应模型S4，包括PTB-XL数据集上的生理信号和Speech Commands上的原始音频。

Conclusion: 小波框架为状态空间模型提供了局部化时间支持，能更好地处理具有瞬态结构的信号，在相关任务中表现出优越性能。

Abstract: State-space models (SSMs) have emerged as a powerful foundation for long-range sequence modeling, with the HiPPO framework showing that continuous-time projection operators can be used to derive stable, memory-efficient dynamical systems that encode the past history of the input signal. However, existing projection-based SSMs often rely on polynomial bases with global temporal support, whose inductive biases are poorly matched to signals exhibiting localized or transient structure. In this work, we introduce \emph{WaveSSM}, a collection of SSMs constructed over wavelet frames. Our key observation is that wavelet frames yield a localized support on the temporal dimension, useful for tasks requiring precise localization. Empirically, we show that on equal conditions, \textit{WaveSSM} outperforms orthogonal counterparts as S4 on real-world datasets with transient dynamics, including physiological signals on the PTB-XL dataset and raw audio on Speech Commands.

</details>


### [153] [Regularized Online RLHF with Generalized Bilinear Preferences](https://arxiv.org/abs/2602.23116)
*Junghyun Lee,Minju Hong,Kwang-Sung Jun,Chulhee Yun,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文研究上下文在线RLHF中的一般偏好学习问题，采用广义双线性偏好模型处理非传递性偏好，提出两种算法分别实现对数遗憾和次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF研究多局限于可传递偏好和特定正则化器（如反向KL），无法处理现实世界中常见的非传递性偏好和更一般的正则化需求。本文旨在扩展RLHF框架以处理一般偏好和任意强凸正则化器。

Method: 采用广义双线性偏好模型（GBPM）通过低秩斜对称矩阵建模非传递性偏好。核心分析证明贪婪策略的对偶间隙可由估计误差的平方界定。基于此提出两种算法：1）贪婪采样算法；2）探索-提交算法，后者利用低秩结构提高统计效率。

Result: 贪婪采样算法实现$\tilde{O}(ηd^4 (\log T)^2)$的多对数遗憾，且与$e^{O(η)}$无关。探索-提交算法实现$\tilde{O}(\sqrt{ηr T})$的次线性遗憾，这是高维在线RLHF中首个统计效率保证，且与$\mathrm{poly}(d)$无关。

Conclusion: 本文为一般偏好的在线RLHF提供了理论框架和高效算法，突破了现有方法在非传递性偏好和高维场景下的限制，为实际RLHF应用提供了更通用的解决方案。

Abstract: We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(η)}$-free regret $\tilde{O}(ηd^4 (\log T)^2)$. (2) Explore-Then-Commit achieves $\mathrm{poly}(d)$-free regret $\tilde{O}(\sqrt{ηr T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.

</details>


### [154] [Data-Driven Supervision of a Thermal-Hydraulic Process Towards a Physics-Based Digital Twin](https://arxiv.org/abs/2602.22267)
*Osimone Imhogiemhe,Yoann Jus,Hubert Lejeune,Saïd Moussaoui*

Main category: cs.LG

TL;DR: 开发用于热工水力过程故障检测与诊断的数字孪生系统，结合数值模拟和机器学习方法，实现过程参数变化检测和在线估计


<details>
  <summary>Details</summary>
Motivation: 工业生产过程实时监控是多个行业的共同挑战，需要确保安全、连续生产和高效运行。数字孪生概念为系统监控提供了合适的框架，结合物理系统仿真和数据驱动机器学习模型，可以设计高效的系统监控工具

Method: 基于系统数值仿真和机器学习方法，开发专门用于过程参数变化检测和在线估计的不同模块。提出故障检测与诊断算法，并在特定测试场景中验证，处理系统中单次参数变化情况

Result: 数值结果显示，在参数变化定位和数值更新方面具有良好的准确性

Conclusion: 成功开发了用于热工水力过程故障检测与诊断的数字孪生系统，结合仿真和机器学习方法，能够有效监控生产过程并实现参数变化检测

Abstract: The real-time supervision of production processes is a common challenge across several industries. It targets process component monitoring and its predictive maintenance in order to ensure safety, uninterrupted production and maintain high efficiency level. The rise of advanced tools for the simulation of physical systems in addition to data-driven machine learning models offers the possibility to design numerical tools dedicated to efficient system monitoring. In that respect, the digital twin concept presents an adequate framework that proffers solution to these challenges. The main purpose of this paper is to develop such a digital twin dedicated to fault detection and diagnosis in the context of a thermal-hydraulic process supervision. Based on a numerical simulation of the system, in addition to machine learning methods, we propose different modules dedicated to process parameter change detection and their on-line estimation. The proposed fault detection and diagnosis algorithm is validated on a specific test scenario, with single one-off parameter change occurrences in the system. The numerical results show good accuracy in terms of parameter variation localization and the update of their values.

</details>


### [155] [AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2602.22268)
*Changhai Zhou,Shiyang Zhang,Yuhua Zhou,Qian Qiao,Jun Gao,Cheng Jin,Kaizhou Qin,Weizhong Zhang*

Main category: cs.LG

TL;DR: AutoQRA：联合优化量化比特宽度和LoRA秩的框架，在内存约束下实现接近全精度微调的性能


<details>
  <summary>Details</summary>
Motivation: 现有的量化后参数高效微调方法忽略了量化比特宽度和LoRA秩之间的复杂交互关系。精心优化的量化分配不一定能转化为良好的微调性能，不同的比特宽度和秩配置在相同内存预算下会产生显著不同的结果。

Method: 提出AutoQRA框架，在混合量化微调过程中联合优化每层的比特宽度和LoRA秩配置。采用两阶段优化：1）全局多保真度进化搜索，通过注入层重要性先验来初始化种群；2）信任区域贝叶斯优化，在搜索空间中有前景的区域进行局部细化。

Result: AutoQRA在内存占用与均匀4位方法相当的情况下，实现了接近全精度微调的性能。

Conclusion: AutoQRA通过联合优化量化比特宽度和LoRA秩，有效解决了现有顺序管道中的局限性，能够在内存约束下实现高性能的下游适应。

Abstract: Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.

</details>


### [156] [Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336)
*Camilo Gomez,Pengyang Wang,Liansheng Tang*

Main category: cs.LG

TL;DR: 提出一种可微的0-1损失近似方法，通过约束优化框架构建平滑的n,k维超单纯形投影，称为Soft-Binary-Argmax算子，改善大批量训练下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 0-1损失被认为是分类任务的黄金标准，但由于其不可微性，无法与基于梯度的优化方法兼容。需要一种可微的近似方法，能够在端到端可微模型中集成结构化优化组件，提供更强的归纳偏置。

Method: 通过约束优化框架构建平滑的、保持顺序的n,k维超单纯形投影，提出Soft-Binary-Argmax算子。推导其数学性质，展示如何高效计算其雅可比矩阵，并将其集成到二分类和多分类学习系统中。

Result: 该方法通过强制输出logits的几何一致性约束，显著改善了大批量训练下的泛化性能，缩小了传统大批量训练中观察到的性能差距。

Conclusion: 提出的Soft-Binary-Argmax算子为0-1损失提供了有效的可微近似，能够改善端到端可微模型的训练效果，特别是在大批量训练场景下表现出优越的泛化能力。

Abstract: Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

</details>


### [157] [CQSA: Byzantine-robust Clustered Quantum Secure Aggregation in Federated Learning](https://arxiv.org/abs/2602.22269)
*Arnab Nath,Harsh Kasyap*

Main category: cs.LG

TL;DR: 提出CQSA框架，通过聚类量子安全聚合解决大规模量子联邦学习中GHZ态保真度下降和拜占庭攻击检测问题


<details>
  <summary>Details</summary>
Motivation: 量子辅助联邦学习中的现有量子安全聚合协议依赖单一全局GHZ态，面临两大挑战：1) 随着客户端数量增加，大规模GHZ态的保真度急剧下降；2) 全局聚合无法检测拜占庭客户端

Method: 提出聚类量子安全聚合框架：将客户端随机划分为小集群，每个集群使用高保真度、低量子比特的GHZ态进行本地量子聚合；服务器通过余弦相似度和欧氏距离等统计度量分析集群级聚合结果，识别恶意贡献

Result: 理论分析和在去极化噪声下的模拟表明，CQSA确保模型稳定收敛，相比全局QSA实现更优的态保真度

Conclusion: CQSA框架调和了近量子硬件的物理约束与联邦学习中拜占庭鲁棒性的需求，为量子辅助联邦学习提供了实用解决方案

Abstract: Federated Learning (FL) enables collaborative model training without sharing raw data. However, shared local model updates remain vulnerable to inference and poisoning attacks. Secure aggregation schemes have been proposed to mitigate these attacks. In this work, we aim to understand how these techniques are implemented in quantum-assisted FL. Quantum Secure Aggregation (QSA) has been proposed, offering information-theoretic privacy by encoding client updates into the global phase of multipartite entangled states. Existing QSA protocols, however, rely on a single global Greenberger-Horne-Zeilinger (GHZ) state shared among all participating clients. This design poses fundamental challenges: fidelity of large-scale GHZ states deteriorates rapidly with the increasing number of clients; and (ii) the global aggregation prevents the detection of Byzantine clients. We propose Clustered Quantum Secure Aggregation (CQSA), a modular aggregation framework that reconciles the physical constraints of near-term quantum hardware along with the need for Byzantine-robustness in FL. CQSA randomly partitions the clients into small clusters, each performing local quantum aggregation using high-fidelity, low-qubit GHZ states. The server analyzes statistical relationships between cluster-level aggregates employing common statistical measures such as cosine similarity and Euclidean distance to identify malicious contributions. Through theoretical analysis and simulations under depolarizing noise, we demonstrate that CQSA ensures stable model convergence, achieves superior state fidelity over global QSA.

</details>


### [158] [Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms](https://arxiv.org/abs/2602.23341)
*Alkis Kalavasis,Anay Mehrotra,Manolis Zampetakis,Felix Zhou,Ziyu Zhu*

Main category: cs.LG

TL;DR: 该论文解决了从粗数据中进行高斯均值估计的两个核心问题：确定了凸划分下均值可识别的条件，并证明了在可识别性和凸划分下存在计算高效的估计算法。


<details>
  <summary>Details</summary>
Motivation: 粗数据在实际应用中普遍存在（如测量舍入、传感器限制等），但现有研究对高斯均值估计的两个基本问题未解决：凸划分下均值何时可识别？在可识别性和凸划分下是否存在计算高效的估计算法？

Method: 研究高斯均值估计问题，其中每个真实样本来自d维高斯分布（单位协方差），但只能通过包含样本的划分集合观察到。重点分析凸划分条件下的可识别性和计算效率。

Result: 解决了两个开放问题：1）确定了凸划分下均值可识别的具体条件；2）证明了在可识别性和凸划分下存在计算高效的均值估计算法。

Conclusion: 该工作完全解决了从粗数据中进行高斯均值估计的两个基本理论问题，为凸划分条件下的可识别性和计算效率提供了完整理论框架。

Abstract: Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]

</details>


### [159] [Prior Knowledge-enhanced Spatio-temporal Epidemic Forecasting](https://arxiv.org/abs/2602.22270)
*Sijie Ruan,Jinyu Li,Jia Wei,Zenghao Xu,Jie Bao,Junshi Xu,Junyang Qiu,Hanning Yuan,Xiaoxiao Wang,Shuliang Wang*

Main category: cs.LG

TL;DR: STOEP是一个结合隐式时空先验和显式专家先验的混合框架，用于时空流行病预测，通过动态调整区域依赖、增强弱信号和正则化参数来提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有流行病预测方法存在三个主要问题：对弱流行信号不敏感、空间关系过于简化、参数估计不稳定。这些问题限制了预测的准确性和实用性。

Method: STOEP包含三个关键组件：1) 病例感知邻接学习(CAL)，基于历史感染模式动态调整移动性区域依赖；2) 空间信息参数估计(SPE)，使用可学习空间先验增强弱流行信号；3) 基于滤波的机制预测(FMF)，采用专家引导的自适应阈值策略正则化流行病参数。

Result: 在真实世界的COVID-19和流感数据集上的实验表明，STOEP在RMSE指标上比最佳基线方法提高了11.1%。该系统已在中国某省级疾控中心部署，支持下游应用。

Conclusion: STOEP通过整合隐式和显式先验知识，有效解决了现有流行病预测方法的局限性，在准确性和实用性方面都有显著提升，已在实际公共卫生管理中发挥作用。

Abstract: Spatio-temporal epidemic forecasting is critical for public health management, yet existing methods often struggle with insensitivity to weak epidemic signals, over-simplified spatial relations, and unstable parameter estimation. To address these challenges, we propose the Spatio-Temporal priOr-aware Epidemic Predictor (STOEP), a novel hybrid framework that integrates implicit spatio-temporal priors and explicit expert priors. STOEP consists of three key components: (1) Case-aware Adjacency Learning (CAL), which dynamically adjusts mobility-based regional dependencies using historical infection patterns; (2) Space-informed Parameter Estimating (SPE), which employs learnable spatial priors to amplify weak epidemic signals; and (3) Filter-based Mechanistic Forecasting (FMF), which uses an expert-guided adaptive thresholding strategy to regularize epidemic parameters. Extensive experiments on real-world COVID-19 and influenza datasets demonstrate that STOEP outperforms the best baseline by 11.1% in RMSE. The system has been deployed at one provincial CDC in China to facilitate downstream applications.

</details>


### [160] [Support Tokens, Stability Margins, and a New Foundation for Robust LLMs](https://arxiv.org/abs/2602.22271)
*Deepak Agarwal,Dhyey Dharmendrakumar Mavani,Suyash Gupta,Karthik Sethuraman,Tejas Dharamsi*

Main category: cs.LG

TL;DR: 将自注意力Transformer重新解释为概率框架，揭示参数约束导致token空间结构化几何，提出类似SVM的边界解释和"支持token"概念，并引入贝叶斯框架和MAP估计目标，通过添加平滑对数障碍惩罚改进训练。


<details>
  <summary>Details</summary>
Motivation: 重新解释因果自注意力Transformer的概率框架，类似于将经典PCA扩展到概率PCA，揭示自注意力参数中的约束条件和token空间的结构化几何，为LLM解码动态提供理论洞察。

Method: 将LLM重新解释为token空间幂集上的随机过程，提出贝叶斯框架并推导MAP估计目标，在标准交叉熵损失基础上添加平滑对数障碍惩罚，形成改进的训练方法。

Result: 揭示了自注意力参数中的障碍约束导致token空间结构化几何，提出了类似支持向量机的边界解释和"支持token"概念，通过添加对数障碍惩罚获得了更鲁棒的模型而不牺牲样本外准确性。

Conclusion: 该研究为Transformer提供了新的概率解释框架，揭示了自注意力的深层结构特性，提出的贝叶斯方法和MAP估计目标能够改进LLM训练，在实践中易于实施且效果显著。

Abstract: Self-attention is usually described as a flexible, content-adaptive way to mix a token with information from its past. We re-interpret causal self-attention transformers, the backbone of modern foundation models, within a probabilistic framework, much like how classical PCA is extended to probabilistic PCA. However, this re-formulation reveals a surprising and deeper structural insight: due to a change-of-variables phenomenon, a barrier constraint emerges on the self-attention parameters. This induces a highly structured geometry on the token space, providing theoretical insights into the dynamics of LLM decoding. This reveals a boundary where attention becomes ill-conditioned, leading to a margin interpretation similar to classical support vector machines. Just like support vectors, this naturally gives rise to the concept of support tokens.
  Furthermore, we show that LLMs can be interpreted as a stochastic process over the power set of the token space, providing a rigorous probabilistic framework for sequence modeling. We propose a Bayesian framework and derive a MAP estimation objective that requires only a minimal modification to standard LLM training: the addition of a smooth log-barrier penalty to the usual cross-entropy loss. We demonstrate that this provides more robust models without sacrificing out-of-sample accuracy and that it is straightforward to incorporate in practice.

</details>


### [161] [Positional-aware Spatio-Temporal Network for Large-Scale Traffic Prediction](https://arxiv.org/abs/2602.22274)
*Runfei Chen*

Main category: cs.LG

TL;DR: 提出轻量级位置感知时空网络PASTN，通过位置感知嵌入区分节点表示，使用时序注意力模块增强长程感知能力，在多种尺度数据集上验证有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有交通流量预测模型在应对大规模地理区域和长时间跨度时存在不足：1）未能清晰区分每个节点；2）缺乏对历史数据的整体视角；3）数据规模增大阻碍实际部署。

Method: 提出轻量级位置感知时空网络PASTN，采用位置感知嵌入分离节点表示，使用时序注意力模块增强长程感知能力，以端到端方式捕捉时空复杂性。

Result: 在多种尺度数据集（县、大都市、州）上验证了PASTN的有效性和效率，进一步分析证明了新引入模块的有效性。

Conclusion: PASTN能够有效捕捉时空复杂性，在保持轻量级的同时提升交通流量预测性能，解决了现有模型在大规模应用中的局限性。

Abstract: Traffic flow forecasting has emerged as an indispensable mission for daily life, which is required to utilize the spatiotemporal relationship between each location within a time period under a graph structure to predict future flow. However, the large travel demand for broader geographical areas and longer time spans requires models to distinguish each node clearly and possess a holistic view of the history, which has been paid less attention to in prior works. Furthermore, increasing sizes of data hinder the deployment of most models in real application environments. To this end, in this paper, we propose a lightweight Positional-aware Spatio-Temporal Network (PASTN) to effectively capture both temporal and spatial complexities in an end-to-end manner. PASTN introduces positional-aware embeddings to separate each node's representation, while also utilizing a temporal attention module to improve the long-range perception of current models. Extensive experiments verify the effectiveness and efficiency of PASTN across datasets of various scales (county, megalopolis and state). Further analysis demonstrates the efficacy of newly introduced modules either.

</details>


### [162] [X-REFINE: XAI-based RElevance input-Filtering and archItecture fiNe-tuning for channel Estimation](https://arxiv.org/abs/2602.22277)
*Abdul Karim Gizzini,Yahia Medjahdi*

Main category: cs.LG

TL;DR: X-REFINE：一个基于XAI的框架，通过联合输入滤波和架构微调，优化6G无线通信中的深度学习模型部署，在保持性能的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 6G无线通信需要AI原生架构，但深度学习模型的黑盒特性及高复杂度限制了其在关键应用（如信道估计）中的实际部署。现有的基于扰动的XAI解决方案通常只关注输入滤波，而忽略了内部结构优化。

Method: 提出X-REFINE框架，采用基于分解的、符号稳定的LRP epsilon规则，通过反向传播预测来获取子载波和隐藏神经元的高分辨率相关性分数，从而实现联合输入滤波和架构微调的整体优化。

Result: 仿真结果表明，X-REFINE在可解释性-性能-复杂度权衡方面表现优异，在保持不同场景下稳健的误码率性能的同时，显著降低了计算复杂度。

Conclusion: X-REFINE为6G无线通信中的AI模型部署提供了一个有效的解决方案，通过可解释AI技术实现了模型优化与性能保持的良好平衡。

Abstract: AI-native architectures are vital for 6G wireless communications. The black-box nature and high complexity of deep learning models employed in critical applications, such as channel estimation, limit their practical deployment. While perturbation-based XAI solutions offer input filtering, they often neglect internal structural optimization. We propose X-REFINE, an XAI-based framework for joint input-filtering and architecture fine-tuning. By utilizing a decomposition-based, sign-stabilized LRP epsilon rule, X-REFINE backpropagates predictions to derive high-resolution relevance scores for both subcarriers and hidden neurons. This enables a holistic optimization that identifies the most faithful model components. Simulation results demonstrate that X-REFINE achieves a superior interpretability-performance-complexity trade-off, significantly reducing computational complexity while maintaining robust bit error rate (BER) performance across different scenarios.

</details>


### [163] [Integrating Machine Learning Ensembles and Large Language Models for Heart Disease Prediction Using Voting Fusion](https://arxiv.org/abs/2602.22280)
*Md. Tahsin Amin,Tanim Ahmmod,Zannatul Ferdus,Talukder Naemul Hasan Naem,Ehsanul Ferdous,Arpita Bhattacharjee,Ishmam Ahmed Solaiman,Nahiyan Bin Noor*

Main category: cs.LG

TL;DR: 该研究比较了传统机器学习模型与大型语言模型在心血管疾病预测上的表现，发现机器学习集成方法（95.78%准确率）优于单独使用LLMs（78.9%准确率），而两者融合的混合方法达到最佳效果（96.62%准确率）。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要早期识别、精确风险分类和可靠的决策支持技术。虽然机器学习算法（特别是集成方法）在处理复杂非线性患者数据方面表现出色，但大型语言模型提供了新的零样本和少样本推理能力，研究者希望探索两者结合的可能性。

Method: 使用包含1,190条患者记录的合并数据集，比较传统机器学习模型（Random Forest、XGBoost、LightGBM、CatBoost）与开源大型语言模型（通过OpenRouter API）。最终提出混合融合方法，将ML集成与LLM推理结合，在Gemini 2.5 Flash上实现。

Result: ML集成方法获得最高性能（95.78%准确率，ROC-AUC 0.96）；LLMs在零样本设置下表现中等（78.9%准确率），少样本设置下稍好（72.6%准确率）；混合方法达到最佳结果（96.62%准确率，0.97 AUC）。

Conclusion: 虽然ML集成被认为是结构化表格预测的最佳案例，但与LLMs的混合系统可以提供小幅性能提升，为更可靠的临床决策支持工具开辟道路。LLMs与ML模型结合使用效果优于单独使用。

Abstract: Cardiovascular disease is the primary cause of death globally, necessitating early identification, precise risk classification, and dependable decision-support technologies. The advent of large language models (LLMs) provides new zero-shot and few-shot reasoning capabilities, even though machine learning (ML) algorithms, especially ensemble approaches like Random Forest, XGBoost, LightGBM, and CatBoost, are excellent at modeling complex, non-linear patient data and routinely beat logistic regression. This research predicts cardiovascular disease using a merged dataset of 1,190 patient records, comparing traditional machine learning models (95.78% accuracy, ROC-AUC 0.96) with open-source large language models via OpenRouter APIs. Finally, a hybrid fusion of the ML ensemble and LLM reasoning under Gemini 2.5 Flash achieved the best results (96.62% accuracy, 0.97 AUC), showing that LLMs (78.9 % accuracy) work best when combined with ML models rather than used alone. Results show that ML ensembles achieved the highest performance (95.78% accuracy, ROC-AUC 0.96), while LLMs performed moderately in zero-shot (78.9%) and slightly better in few-shot (72.6%) settings. The proposed hybrid method enhanced the strength in uncertain situations, illustrating that ensemble ML is considered the best structured tabular prediction case, but it can be integrated with hybrid ML-LLM systems to provide a minor increase and open the way to more reliable clinical decision-support tools.

</details>


### [164] [BrepCoder: A Unified Multimodal Large Language Model for Multi-task B-rep Reasoning](https://arxiv.org/abs/2602.22284)
*Mingi Kim,Yongjun Kim,Jungwoo Kang,Hyungki Kim*

Main category: cs.LG

TL;DR: BrepCoder：基于B-rep格式的统一多模态大语言模型，通过将CAD建模序列转换为类Python代码，实现多种CAD任务，包括补全、纠错和CAD-QA等。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法大多依赖任务特定模型，需要为新任务进行结构调整，且主要关注点云或图像而非行业标准的B-rep格式。这些限制阻碍了CAD领域的通用智能代理发展。

Method: 利用大语言模型的代码生成能力，将CAD建模序列转换为类Python代码并与B-rep对齐。采用两阶段训练策略：1) 通过逆向工程预训练学习几何特征和设计逻辑；2) 将模型有效扩展到下游任务如补全、纠错和CAD-QA。

Result: 通过将B-rep解释为结构化代码，BrepCoder在多种任务上展现出卓越的泛化能力，证明了其作为通用CAD代理的潜力。

Conclusion: BrepCoder通过统一的多模态大语言模型框架，成功解决了现有CAD深度学习方法在任务通用性和格式兼容性方面的限制，为CAD领域的智能代理提供了新方向。

Abstract: Recent advancements in deep learning have actively addressed complex challenges within the Computer-Aided Design (CAD) domain.However, most existing approaches rely on task-specifi c models requiring structural modifi cations for new tasks, and they predominantly focus on point clouds or images rather than the industry-standard Boundary Representation (B-rep) format. To address these limitations, we propose BrepCoder, a unifi ed Multimodal Large Language Model (MLLM) that performs diverse CAD tasks from B-rep inputs. By leveraging the code generation capabilities of Large Language Models (LLMs), we convert CAD modeling sequences into Python-like code and align them with B-rep. We then adopt a two-stage training strategy: First, pre-training on reverse engineering to learn geometric features and design logic. Second, eff ectively extending the model to various downstream tasks such as completion, error correction, and CAD-QA. Consequently, by interpreting B-rep as structural code, BrepCoder achieves superior generalization across diverse tasks, demonstrating its potential as a general-purpose CAD agent.

</details>


### [165] [Early Risk Stratification of Dosing Errors in Clinical Trials Using Machine Learning](https://arxiv.org/abs/2602.22285)
*Félicien Hêche,Sohrab Ferdowsi,Anthony Yazdani,Sara Sansaloni-Pastor,Douglas Teodoro*

Main category: cs.LG

TL;DR: 开发基于机器学习的框架，利用试验启动前信息对临床试验进行早期风险分层，预测高剂量错误率风险


<details>
  <summary>Details</summary>
Motivation: 临床试验中的剂量错误是常见且严重的问题，可能导致患者伤害和试验失败。目前缺乏在试验启动前预测剂量错误风险的方法，需要开发早期风险分层框架来支持主动的质量管理。

Method: 从ClinicalTrials.gov构建包含42,112个临床试验的数据集，提取结构化、半结构化和非结构化文本数据。使用不良事件报告、MedDRA术语和Wilson置信区间为试验分配二元标签（高剂量错误率）。评估了三种模型：基于结构化特征的XGBoost、基于文本数据的ClinicalModernBERT，以及结合两种模态的简单后期融合模型。应用后验概率校准以实现可解释的风险分层。

Result: 后期融合模型获得最高的AUC-ROC（0.862）。经过校准的输出能够将试验稳健地分层到预定义的风险类别中。被标记为具有过高剂量错误率的试验比例在较高预测风险组中单调增加，并与相应的预测概率范围一致。

Conclusion: 该研究引入了一个可重复且可扩展的机器学习框架，用于对高风险剂量错误率的临床试验进行早期、试验级别的风险分层，支持临床研究中的主动、基于风险的质量管理。

Abstract: Objective: The objective of this study is to develop a machine learning (ML)-based framework for early risk stratification of clinical trials (CTs) according to their likelihood of exhibiting a high rate of dosing errors, using information available prior to trial initiation. Materials and Methods: We constructed a dataset from ClinicalTrials.gov comprising 42,112 CTs. Structured, semi-structured trial data, and unstructured protocol-related free-text data were extracted. CTs were assigned binary labels indicating elevated dosing error rate, derived from adverse event reports, MedDRA terminology, and Wilson confidence intervals. We evaluated an XGBoost model trained on structured features, a ClinicalModernBERT model using textual data, and a simple late-fusion model combining both modalities. Post-hoc probability calibration was applied to enable interpretable, trial-level risk stratification. Results: The late-fusion model achieved the highest AUC-ROC (0.862). Beyond discrimination, calibrated outputs enabled robust stratification of CTs into predefined risk categories. The proportion of trials labeled as having an excessively high dosing error rate increased monotonically across higher predicted risk groups and aligned with the corresponding predicted probability ranges. Discussion: These findings indicate that dosing error risk can be anticipated at the trial level using pre-initiation information. Probability calibration was essential for translating model outputs into reliable and interpretable risk categories, while simple multimodal integration yielded performance gains without requiring complex architectures. Conclusion: This study introduces a reproducible and scalable ML framework for early, trial-level risk stratification of CTs at risk of high dosing error rates, supporting proactive, risk-based quality management in clinical research.

</details>


### [166] [OmniZip: Learning a Unified and Lightweight Lossless Compressor for Multi-Modal Data](https://arxiv.org/abs/2602.22286)
*Yan Zhao,Zhengxue Cheng,Junxuan Zhang,Dajiang Zhou,Qunshan Gu,Qi Wang,Li Song*

Main category: cs.LG

TL;DR: OmniZip是一个轻量级统一无损压缩器，支持图像、文本、语音、触觉、数据库和基因序列等多种模态数据，通过模态统一标记化、模态路由上下文学习和前馈设计实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现有学习型无损压缩器多为单模态设计，在多模态场景下需要部署多个压缩器，导致冗余。多模态大语言模型虽然提供了解决方案，但过于复杂难以实用。因此需要设计一个统一且轻量级的多模态无损压缩器。

Method: 基于轻量级骨干网络，包含三个核心组件：1) 模态统一标记器，将不同数据可逆地转换为标记；2) 模态路由上下文学习机制，实现灵活的多模态上下文建模；3) 模态路由前馈设计，增强模型非线性表示能力。采用重参数化训练策略提升模型容量。

Result: 在多个模态数据集上优于或匹配现有最佳压缩器，相比gzip在CLIC-M、TouchandGo、enwik9、LibriSpeech和WikiSQL数据集上分别提升42%、57%、62%和42%、53%的压缩效率。在资源受限的边缘设备上支持近实时推理，在MacBook CPU和iPhone NPU上达到约1MB/s的速度。

Conclusion: OmniZip成功实现了统一且轻量级的多模态无损压缩，在保持高性能的同时支持边缘设备部署，为多模态数据压缩提供了实用解决方案。

Abstract: Lossless compression is essential for efficient data storage and transmission. Although learning-based lossless compressors achieve strong results, most of them are designed for a single modality, leading to redundant compressor deployments in multi-modal settings. Designing a unified multi-modal compressor is critical yet challenging, as different data types vary largely in format, dimension, and statistics. Multi-modal large language models offer a promising resolution but remain too complex for practical use. Thus, we propose \textbf{OmniZip}, \textbf{a unified and lightweight lossless compressor for multi-modal data (like image, text, speech, tactile, database, and gene sequence)}. Built on a lightweight backbone, OmniZip incorporates three key components to enable efficient multi-modal lossless compression: a modality-unified tokenizer that reversibly transforms diverse data into tokens, a modality-routing context learning mechanism that enables flexible multi-modal context modeling, and a modality-routing feedforward design that further enhances the model's nonlinear representation flexibility. A reparameterization training strategy is used to enhance model capacity. OmniZip outperforms or matches other state-of-the-art compressors on multiple modalities, achieving 42\%, 57\%, 62\% and 42\%, 53\% higher compression efficiency than gzip on CLIC-M, TouchandGo, enwik9, LibriSpeech, and WikiSQL datasets, respectively. It also supports near real-time inference on resource-constrained edge devices, reaching about 1MB/s on MacBook CPUs and iPhone NPUs. Our code is released at https://github.com/adminasmi/OmniZip-CVPR2026.

</details>


### [167] [Reliable XAI Explanations in Sudden Cardiac Death Prediction for Chagas Cardiomyopathy](https://arxiv.org/abs/2602.22288)
*Vinícius P. Chagas,Luiz H. T. Viana,Mac M. da S. Carlos,João P. V. Madeiro,Roberto C. Pedrosa,Thiago Alves Rocha,Carlos H. L. Cavalcante*

Main category: cs.LG

TL;DR: 本文提出一种基于逻辑的可解释性方法，用于提高查加斯心肌病患者心源性猝死预测的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 心源性猝死难以预测，尤其在查加斯心肌病患者中风险分层困难。现有AI模型虽能改善风险分层，但缺乏透明度，被视为"黑箱"，且一些启发式解释方法缺乏正确性保证，可能导致决策错误。

Method: 采用基于逻辑的可解释性方法，应用于准确率超过95%的AI分类器，确保解释的正确性保证。

Result: 该方法展示了强大的预测性能，达到100%的解释保真度。与最先进的启发式方法相比，显示出更好的一致性和鲁棒性。

Conclusion: 该方法增强了临床信任，促进了AI驱动工具在实践中的整合，特别是在最需要的流行地区实现大规模部署。

Abstract: Sudden cardiac death (SCD) is unpredictable, and its prediction in Chagas cardiomyopathy (CC) remains a significant challenge, especially in patients not classified as high risk. While AI and machine learning models improve risk stratification, their adoption is hindered by a lack of transparency, as they are often perceived as \textit{black boxes} with unclear decision-making processes. Some approaches apply heuristic explanations without correctness guarantees, leading to mistakes in the decision-making process. To address this, we apply a logic-based explainability method with correctness guarantees to the problem of SCD prediction in CC. This explainability method, applied to an AI classifier with over 95\% accuracy and recall, demonstrated strong predictive performance and 100\% explanation fidelity. When compared to state-of-the-art heuristic methods, it showed superior consistency and robustness. This approach enhances clinical trust, facilitates the integration of AI-driven tools into practice, and promotes large-scale deployment, particularly in endemic regions where it is most needed.

</details>


### [168] [Manifold of Failure: Behavioral Attraction Basins in Language Models](https://arxiv.org/abs/2602.22291)
*Sarthak Munshi,Manish Bhatt,Vineeth Sai Narajala,Idan Habler,AmmarnAl-Kahfah,Ken Huang,Blake Gatto*

Main category: cs.LG

TL;DR: 该论文提出了一种系统映射大语言模型失败流形的方法，将漏洞搜索重新定义为质量多样性问题，使用MAP-Elites算法揭示失败区域的连续拓扑结构，生成可解释的全局安全地图。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注将对抗性示例投影回自然数据流形以恢复安全性，但缺乏对不安全区域本身的系统性理解。作者认为全面理解AI安全需要表征这些不安全区域。

Method: 将漏洞搜索重新定义为质量多样性问题，使用MAP-Elites算法来映射失败流形。采用对齐偏差作为质量指标，引导搜索模型行为与预期对齐偏离最大的区域。在Llama-3-8B、GPT-OSS-20B和GPT-5-Mini三个LLM上进行实验。

Result: MAP-Elites实现了高达63%的行为覆盖率，发现了多达370个不同的漏洞生态位。不同模型展现出显著不同的拓扑特征：Llama-3-8B呈现近乎普遍的脆弱性平台（平均对齐偏差0.93），GPT-OSS-20B显示碎片化景观，GPT-5-Mini表现出强鲁棒性（上限0.50）。

Conclusion: 该方法生成了现有攻击方法无法提供的可解释全局安全地图，将范式从寻找离散失败转向理解其底层结构，为系统评估和提升LLM安全性提供了新框架。

Abstract: While prior work has focused on projecting adversarial examples back onto the manifold of natural data to restore safety, we argue that a comprehensive understanding of AI safety requires characterizing the unsafe regions themselves. This paper introduces a framework for systematically mapping the Manifold of Failure in Large Language Models (LLMs). We reframe the search for vulnerabilities as a quality diversity problem, using MAP-Elites to illuminate the continuous topology of these failure regions, which we term behavioral attraction basins. Our quality metric, Alignment Deviation, guides the search towards areas where the model's behavior diverges most from its intended alignment. Across three LLMs: Llama-3-8B, GPT-OSS-20B, and GPT-5-Mini, we show that MAP-Elites achieves up to 63% behavioral coverage, discovers up to 370 distinct vulnerability niches, and reveals dramatically different model-specific topological signatures: Llama-3-8B exhibits a near-universal vulnerability plateau (mean Alignment Deviation 0.93), GPT-OSS-20B shows a fragmented landscape with spatially concentrated basins (mean 0.73), and GPT-5-Mini demonstrates strong robustness with a ceiling at 0.50. Our approach produces interpretable, global maps of each model's safety landscape that no existing attack method (GCG, PAIR, or TAP) can provide, shifting the paradigm from finding discrete failures to understanding their underlying structure.

</details>


### [169] [Global River Forecasting with a Topology-Informed AI Foundation Model](https://arxiv.org/abs/2602.22293)
*Hancheng Ren,Gang Zhao,Shuo Wang,Louise Slater,Dai Yamazaki,Shu Liu,Jingfang Fan,Shibo Cui,Ziming Yu,Shengyu Kang,Depeng Zuo,Dingzhi Peng,Zongxue Xu,Bo Pang*

Main category: cs.LG

TL;DR: GraphRiverCast (GRC) 是一个基于拓扑信息的AI基础模型，能够在全球河流系统中进行多变量水动力模拟，支持"冷启动"模式无需历史河流状态初始化，在7天全球伪后报中NSE达到0.82，优于传统物理模型和本地训练的AI基准。


<details>
  <summary>Details</summary>
Motivation: 河流系统本质上是相互连接的连续网络，但广泛的水文数据稀缺限制了数据驱动预测只能进行孤立预测。需要实现系统性模拟并减少对河流观测的依赖。

Method: 提出GraphRiverCast (GRC)模型，这是一个基于拓扑信息的AI基础模型，采用物理对齐的神经算子架构。支持"冷启动"模式，无需历史河流状态初始化。通过预训练和微调策略进行局部适应，利用拓扑编码作为结构信息指导水力连通性和网络尺度质量重新分配。

Result: 在7天全球伪后报中，GRC-ColdStart作为独立的模拟器，Nash-Sutcliffe Efficiency (NSE)达到约0.82，没有表现出自回归范式典型的显著误差累积。在局部适应后，GRC始终优于基于物理的模型和本地训练的AI基准，这种优势从测量河段扩展到整个河流网络。

Conclusion: 拓扑编码在缺乏历史状态时是不可或缺的结构信息，物理基础的预训练至关重要。GRC实现了快速和跨尺度的自适应模拟，建立了将全球水动力知识与局部水文现实相连接的协作范式。

Abstract: River systems operate as inherently interconnected continuous networks, meaning river hydrodynamic simulation ought to be a systemic process. However, widespread hydrology data scarcity often restricts data-driven forecasting to isolated predictions. To achieve systemic simulation and reduce reliance on river observations, we present GraphRiverCast (GRC), a topology-informed AI foundation model designed to simulate multivariate river hydrodynamics in global river systems. GRC is capable of operating in a "ColdStart" mode, generating predictions without relying on historical river states for initialization. In 7-day global pseudo-hindcasts, GRC-ColdStart functions as a robust standalone simulator, achieving a Nash-Sutcliffe Efficiency (NSE) of approximately 0.82 without exhibiting the significant error accumulation typical of autoregressive paradigms. Ablation studies reveal that topological encoding serves as indispensable structural information in the absence of historical states, explicitly guiding hydraulic connectivity and network-scale mass redistribution to reconstruct flow dynamics. Furthermore, when adapted locally via a pre-training and fine-tuning strategy, GRC consistently outperforms physics-based and locally-trained AI baselines. Crucially, this superiority extends from gauged reaches to full river networks, underscoring the necessity of topology encoding and physics-based pre-training. Built on a physics-aligned neural operator architecture, GRC enables rapid and cross-scale adaptive simulation, establishing a collaborative paradigm bridging global hydrodynamic knowledge with local hydrological reality.

</details>


### [170] [When Should a Model Change Its Mind? An Energy-Based Theory and Regularizer for Concept Drift in Electrocardiogram (ECG) Signals](https://arxiv.org/abs/2602.22294)
*Timothy Oladunni,Blessing Ojeme,Kyndal Maclin,Clyde Baidoo*

Main category: cs.LG

TL;DR: 提出PECT框架，通过能量守恒理论区分生理信号中的良性变化与真实概念漂移，使用ECRL正则化器提升模型稳定性


<details>
  <summary>Details</summary>
Motivation: 现有概念漂移框架主要基于分布变化，无法区分生理信号中良性的能量波动与真实概念变化，导致深度模型对振幅、速率、形态等无害变化产生不稳定预测

Method: 提出生理能量守恒理论(PECT)，认为在虚拟漂移下，归一化潜在位移应与归一化信号能量变化成比例；通过能量约束表示学习(ECRL)实现该原则，作为轻量级正则化器惩罚能量不一致的潜在移动

Result: 在七种单模态和混合模型上验证，最强三模态混合模型(1D+2D+Transformer)在保持清洁准确率(96.0%到94.1%)的同时，扰动准确率显著提升(72.6%到85.5%)，融合表示漂移减少超过45%

Conclusion: PECT作为能量漂移定律，有效管理连续生理信号中的概念稳定性，ECRL正则化器在不改变编码器架构或增加推理成本的情况下显著提升模型鲁棒性

Abstract: Models operating on dynamic physiologic signals must distinguish benign, label-preserving variability from true concept change. Existing concept-drift frameworks are largely distributional and provide no principled guidance on how much a model's internal representation may move when the underlying signal undergoes physiologically plausible fluctuations in energy. As a result, deep models often misinterpret harmless changes in amplitude, rate, or morphology as concept drift, yielding unstable predictions, particularly in multimodal fusion settings.
  This study introduces Physiologic Energy Conservation Theory (PECT), an energy-based framework for concept stability in dynamic signals. PECT posits that under virtual drift, normalized latent displacement should scale proportionally with normalized signal energy change, while persistent violations of this proportionality indicate real concept drift. We operationalize this principle through Energy-Constrained Representation Learning (ECRL), a lightweight regularizer that penalizes energy-inconsistent latent movement without modifying encoder architectures or adding inference-time cost.
  Although PECT is formulated for dynamic signals in general, we instantiate and evaluate it on multimodal ECG across seven unimodal and hybrid models. Experiments show that in the strongest trimodal hybrid (1D+2D+Transformer), clean accuracy is largely preserved (96.0% to 94.1%), while perturbed accuracy improves substantially (72.6% to 85.5%) and fused representation drift decreases by over 45%. Similar trends are observed across all architectures, providing empirical evidence that PECT functions as an energy-drift law governing concept stability in continuous physiologic signals.

</details>


### [171] [UpSkill: Mutual Information Skill Learning for Structured Response Diversity in LLMs](https://arxiv.org/abs/2602.22296)
*Devan Shah,Owen Yang,Daniel Yang,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: UpSkill是一种训练时方法，通过互信息技能学习优化LLMs的pass@k正确率，使用token级互信息奖励来鼓励轨迹特异性，在GSM8K上提升了多尝试指标。


<details>
  <summary>Details</summary>
Motivation: 标准的RLVR方法虽然提升了LLMs在数学和编程任务上的推理能力，但优化单次尝试准确率会抑制多次尝试中的响应多样性，限制了探索并忽略了代表性不足的策略。

Method: 将互信息技能学习（MISL）适配到LLMs中，提出在组相对策略优化（GRPO）框架内实现token级互信息奖励，鼓励轨迹对潜在变量z的特异性。

Result: 在GSM8K上对Llama 3.1-8B、Qwen 2.5-7B和R1-Distilled-Qwen2.5-Math-1.5B进行实验，UpSkill在更强的基模型上提升了多尝试指标，Qwen和Llama的pass@k平均提升约3%，且不降低pass@1。

Conclusion: pass@k的改进与互信息目标密切相关，既有经验证据也有理论支持，表明UpSkill能有效提升LLMs在多次尝试中的性能而不损害单次尝试准确率。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of large language models (LLMs) on mathematics and programming tasks, but standard approaches that optimize single-attempt accuracy can inadvertently suppress response diversity across repeated attempts, narrowing exploration and overlooking underrepresented strategies. We introduce UpSkill, a training time method that adapts Mutual Information Skill Learning (MISL) to LLMs for optimizing pass@k correctness. We propose a novel reward that we implement within Group Relative Policy Optimization (GRPO): a token-level mutual information (MI) reward that encourages trajectory specificity to z. Experiments on GSM8K with three open-weight models, Llama 3.1-8B, Qwen 2.5-7B, and R1-Distilled-Qwen2.5-Math-1.5B, show that UpSkill improves multi-attempt metrics on the stronger base models, yielding mean gains of ~3% in pass@k for both Qwen and Llama without degrading pass@1. Additionally, we find both empirical and theoretical evidence that improvements in pass@k are closely tied to the mutual information objective.

</details>


### [172] [Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection](https://arxiv.org/abs/2602.22297)
*Dhiraj Neupane,Richard Dazeley,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 该论文提出了一种基于离线逆强化学习的机械故障检测方法，通过从健康运行序列中学习奖励动态，避免了手动奖励设计和故障标签需求，实现了早期鲁棒的故障检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的机械故障检测方法未能充分利用强化学习的序列决策优势，通常将故障检测简化为上下文赌博机问题。需要将强化学习的序列推理与机械故障检测的时间结构对齐。

Method: 将机械故障检测建模为离线逆强化学习问题，使用对抗性逆强化学习训练一个判别器来区分正常（专家）和策略生成的转移，判别器学习的奖励作为异常分数来指示偏离正常运行行为的程度。

Result: 在三个运行至故障基准数据集（HUMS2023、IMS和XJTU-SY）上评估，模型始终为正常样本分配低异常分数，为故障样本分配高异常分数，实现了早期和鲁棒的故障检测。

Conclusion: 通过将强化学习的序列推理与机械故障检测的时间结构对齐，这项工作为数据驱动的工业环境中基于强化学习的诊断开辟了道路。

Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.

</details>


### [173] [AviaSafe: A Physics-Informed Data-Driven Model for Aviation Safety-Critical Cloud Forecasts](https://arxiv.org/abs/2602.22298)
*Zijian Zhu,Qiusheng Huang,Anboyu Guo,Xiaohui Zhong,Hao Li*

Main category: cs.LG

TL;DR: AviaSafe：基于物理约束的分层神经网络模型，用于预测对航空安全至关重要的四种云微物理物种（水凝物），预报时效可达7天


<details>
  <summary>Details</summary>
Motivation: 当前AI天气预报模型只能预测常规大气变量，无法区分对航空安全至关重要的云微物理物种（如过冷水、冰晶等），而区分这些物种对发动机结冰风险评估至关重要

Method: 采用分层物理信息神经网络架构：1）使用掩码注意力机制预测云空间分布；2）在识别区域内量化物种浓度；3）集成航空气象学中的结冰条件指数作为物理约束，识别过冷水促进冰晶爆炸性增长的区域

Result: 在ERA5再分析数据上训练，相比基线模型降低了云物种的RMSE，在7天预报时效上某些关键变量优于业务数值模式

Conclusion: AviaSafe能够预测单个云物种，为航空路线优化等新应用提供了可能，其中区分冰和液态水决定了发动机结冰风险

Abstract: Current AI weather forecasting models predict conventional atmospheric variables but cannot distinguish between cloud microphysical species critical for aviation safety. We introduce AviaSafe, a hierarchical, physics-informed neural forecaster that produces global, six-hourly predictions of these four hydrometeor species for lead times up to 7 days. Our approach addresses the unique challenges of cloud prediction: extreme sparsity, discontinuous distributions, and complex microphysical interactions between species. We integrate the Icing Condition (IC) index from aviation meteorology as a physics-based constraint that identifies regions where supercooled water fuels explosive ice crystal growth. The model employs a hierarchical architecture that first predicts cloud spatial distribution through masked attention, then quantifies species concentrations within identified regions. Training on ERA5 reanalysis data, our model achieves lower RMSE for cloud species compared to baseline and outperforms operational numerical models on certain key variables at 7-day lead times. The ability to forecast individual cloud species enables new applications in aviation route optimization where distinguishing between ice and liquid water determines engine icing risk.

</details>


### [174] [Training Agents to Self-Report Misbehavior](https://arxiv.org/abs/2602.22303)
*Bruce W. Lee,Chen Yueh-Han,Tomek Korbak*

Main category: cs.LG

TL;DR: 论文提出"自我指控训练"方法，训练AI代理在暗中违规时主动发出可见信号，相比传统对齐训练能更有效减少未检测到的攻击成功率


<details>
  <summary>Details</summary>
Motivation: 前沿AI代理可能隐藏真实目标并逃避监管，传统对齐训练可能失败或产生副作用，需要一种新方法来检测隐蔽的违规行为

Method: 训练GPT-4.1和Gemini-2.0代理在暗中违规时调用report_scheming()工具，在分布外环境中评估其隐蔽造成危害的能力

Result: 自我指控训练显著降低未检测到的成功攻击率，优于匹配能力监控器和对齐基线，保持指令层次结构，对通用能力影响最小，且在不同任务中表现一致

Conclusion: 自我指控训练为减少前沿AI不对齐风险提供了可行路径，既不假设违规行为可以被预防，也不假设可以从外部可靠分类

Abstract: Frontier AI agents may pursue hidden goals while concealing their pursuit from oversight. Alignment training aims to prevent such behavior by reinforcing the correct goals, but alignment may not always succeed and can lead to unwanted side effects. We propose self-incrimination training, which instead trains agents to produce a visible signal when they covertly misbehave. We train GPT-4.1 and Gemini-2.0 agents to call a report_scheming() tool when behaving deceptively and measure their ability to cause harm undetected in out-of-distribution environments. Self-incrimination significantly reduces the undetected successful attack rate, outperforming matched-capability monitors and alignment baselines while preserving instruction hierarchy and incurring minimal safety tax on general capabilities. Unlike blackbox monitoring, self-incrimination performance is consistent across tasks regardless of how suspicious the misbehavior appears externally. The trained behavior persists under adversarial prompt optimization and generalizes to settings where agents pursue misaligned goals themselves rather than being instructed to misbehave. Our results suggest self-incrimination offers a viable path for reducing frontier misalignment risk, one that neither assumes misbehavior can be prevented nor that it can be reliably classified from the outside.

</details>


### [175] [Structure and Redundancy in Large Language Models: A Spectral Study via Random Matrix Theory](https://arxiv.org/abs/2602.22345)
*Davide Ettori*

Main category: cs.LG

TL;DR: 该论文通过谱几何和随机矩阵理论统一框架解决深度学习可靠性和效率问题，提出EigenTrack实时检测幻觉和分布偏移，以及RMT-KD基于谱特征的知识蒸馏压缩方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度网络和大语言模型规模扩大，内部行为变得不透明，导致幻觉、分布偏移下的脆弱泛化，以及计算和能耗需求增长。需要可解释、稳定的方法来监控模型行为和提升效率。

Method: 1. EigenTrack：将流式激活转换为谱描述符（熵、方差、与Marchenko-Pastur基线的偏差），使用轻量级循环分类器建模时间演化，实现早期可靠性故障检测。
2. RMT-KD：将激活谱中的异常特征值解释为任务相关信息载体，通过迭代自蒸馏将网络投影到低维子空间，实现模型压缩。

Result: EigenTrack能够在模型输出出现可靠性故障前进行早期检测，并提供表示动力学的可解释洞察。RMT-KD能够生成显著更紧凑、节能的模型，同时保持准确性和硬件友好的密集结构。

Conclusion: 谱统计为深度学习模型行为提供了紧凑、稳定、可解释的视角，能够分离结构化因果表示和噪声主导的变异性。该框架统一解决了可靠性和效率挑战，为大规模深度学习系统的监控和优化提供了理论基础和实用工具。

Abstract: This thesis addresses two persistent and closely related challenges in modern deep learning, reliability and efficiency, through a unified framework grounded in Spectral Geometry and Random Matrix Theory (RMT). As deep networks and large language models continue to scale, their internal behavior becomes increasingly opaque, leading to hallucinations, fragile generalization under distribution shift, and growing computational and energy demands. By analyzing the eigenvalue dynamics of hidden activations across layers and inputs, this work shows that spectral statistics provide a compact, stable, and interpretable lens on model behavior, capable of separating structured, causal representations from noise-dominated variability. Within this framework, the first contribution, EigenTrack, introduces a real-time method for detecting hallucinations and out-of-distribution behavior in large language and vision-language models. EigenTrack transforms streaming activations into spectral descriptors such as entropy, variance, and deviations from the Marchenko-Pastur baseline, and models their temporal evolution using lightweight recurrent classifiers, enabling early detection of reliability failures before they appear in model outputs while offering interpretable insight into representation dynamics. The second contribution, RMT-KD, presents a principled approach to compressing deep networks via random matrix theoretic knowledge distillation. By interpreting outlier eigenvalues in activation spectra as carriers of task-relevant information, RMT-KD progressively projects networks onto lower-dimensional subspaces through iterative self-distillation, yielding significantly more compact and energy-efficient models while preserving accuracy and dense, hardware-friendly structure.

</details>


### [176] [Learning geometry-dependent lead-field operators for forward ECG modeling](https://arxiv.org/abs/2602.22367)
*Arsenii Dokuchaev,Francesca Bonizzoni,Stefano Pagani,Francesco Regazzoni,Simone Pezzuto*

Main category: cs.LG

TL;DR: 提出一种基于形状感知的替代模型，用于前向心电图模拟中的导联场算子，实现高解剖保真度、低数据需求和计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统前向心电图计算模型需要精确的躯干域表示，但临床实践中获取完整躯干解剖数据困难，且导联场方法计算成本随电极数量线性增长，限制了在高密度记录场景的应用。目前缺乏同时满足高解剖保真度、低数据需求和计算效率的方法。

Method: 提出形状感知的导联场算子替代模型框架，包含两个组件：1) 几何编码模块，将解剖形状映射到低维潜在空间；2) 几何条件神经替代模型，根据空间坐标、电极位置和潜在编码预测导联场梯度。

Result: 该方法在躯干内（平均角度误差5°）和心脏内部都能高精度近似导联场，实现高精度心电图模拟（相对均方误差<2.5%）。替代模型持续优于广泛使用的伪导联场近似，同时保持可忽略的推理成本。

Conclusion: 该方法通过紧凑的潜在表示，无需完整详细的躯干分割，可在数据有限的环境中部署，同时保持高保真心电图模拟，解决了临床实践中解剖数据不完整和计算效率低的问题。

Abstract: Modern forward electrocardiogram (ECG) computational models rely on an accurate representation of the torso domain. The lead-field method enables fast ECG simulations while preserving full geometric fidelity. Achieving high anatomical accuracy in torso representation is, however, challenging in clinical practice, as imaging protocols are typically focused on the heart and often do not include the entire torso. In addition, the computational cost of the lead-field method scales linearly with the number of electrodes, limiting its applicability in high-density recording settings. To date, no existing approach simultaneously achieves high anatomical fidelity, low data requirements and computational efficiency. In this work, we propose a shape-informed surrogate model of the lead-field operator that serves as a drop-in replacement for the full-order model in forward ECG simulations. The proposed framework consists of two components: a geometry-encoding module that maps anatomical shapes into a low-dimensional latent space, and a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions and latent codes. The proposed method achieves high accuracy in approximating lead fields both within the torso (mean angular error 5°) and inside the heart, resulting in highly accurate ECG simulations (relative mean squared error <2.5%. The surrogate consistently outperforms the widely used pseudo lead-field approximation while preserving negligible inference cost. Owing to its compact latent representation, the method does not require a fully detailed torso segmentation and can therefore be deployed in data-limited settings while preserving high-fidelity ECG simulations.

</details>


### [177] [Disentangling Shared and Target-Enriched Topics via Background-Contrastive Non-negative Matrix Factorization](https://arxiv.org/abs/2602.22387)
*Yixuan Li,Archer Y. Yang,Yue Li*

Main category: cs.LG

TL;DR: 背景对比非负矩阵分解（bNMF）通过联合分解目标数据集和匹配背景，在对比目标下抑制背景表达的结构，提取目标富集的潜在主题，解决高维数据中生物信号被共享变异掩盖的问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据中感兴趣的生物信号常被跨条件共享的显性变异所掩盖，这些变异来自基线生物结构或技术效应，阻碍标准降维方法解析条件特异性结构。现有背景校正方法要么无法扩展到高维度，要么缺乏可解释性。

Method: 提出背景对比非负矩阵分解（bNMF），通过共享非负基联合分解目标数据集和匹配背景，在对比目标下抑制背景表达的结构，提取目标富集的潜在主题。该方法使用高效乘法更新算法，支持GPU硬件加速，并通过小批量训练扩展到大数据。

Result: 在模拟和多种生物数据集中，bNMF揭示了传统方法掩盖的信号，包括：死后抑郁大脑单细胞RNA-seq中的疾病相关程序、小鼠中基因型相关的蛋白表达模式、白血病中治疗特异性转录变化、以及癌细胞系中TP53依赖的药物反应。

Conclusion: bNMF提供了一种可扩展且可解释的方法，通过对比目标与背景数据，有效提取目标特异性变异，解决了高维生物数据分析中信号被共享变异掩盖的关键挑战。

Abstract: Biological signals of interest in high-dimensional data are often masked by dominant variation shared across conditions. This variation, arising from baseline biological structure or technical effects, can prevent standard dimensionality reduction methods from resolving condition-specific structure. The challenge is that these confounding topics are often unknown and mixed with biological signals. Existing background correction methods are either unscalable to high dimensions or not interpretable. We introduce background contrastive Non-negative Matrix Factorization (\model), which extracts target-enriched latent topics by jointly factorizing a target dataset and a matched background using shared non-negative bases under a contrastive objective that suppresses background-expressed structure. This approach yields non-negative components that are directly interpretable at the feature level, and explicitly isolates target-specific variation. \model is learned by an efficient multiplicative update algorithm via matrix multiplication such that it is highly efficient on GPU hardware and scalable to big data via minibatch training akin to deep learning approach. Across simulations and diverse biological datasets, \model reveals signals obscured by conventional methods, including disease-associated programs in postmortem depressive brain single-cell RNA-seq, genotype-linked protein expression patterns in mice, treatment-specific transcriptional changes in leukemia, and TP53-dependent drug responses in cancer cell lines.

</details>


### [178] [Predicting Multi-Drug Resistance in Bacterial Isolates Through Performance Comparison and LIME-based Interpretation of Classification Models](https://arxiv.org/abs/2602.22400)
*Santanam Wishal,Riad Sahara*

Main category: cs.LG

TL;DR: 开发可解释机器学习框架预测细菌多重耐药性，集成XGBoost/LightGBM等模型，结合LIME提供临床可理解解释，支持抗菌药物管理决策。


<details>
  <summary>Details</summary>
Motivation: 抗菌药物耐药性（特别是多重耐药性）对临床决策构成严峻挑战，传统药敏试验耗时且治疗选择有限，需要开发准确且可解释的预测工具来支持早期识别和临床决策。

Method: 提出可解释机器学习框架，使用临床特征和抗生素药敏模式预测多重耐药性。评估逻辑回归、随机森林、AdaBoost、XGBoost和LightGBM五种分类模型，在9,714株分离株数据集上训练，采用抗生素家族水平编码捕捉交叉耐药模式。使用LIME生成实例级解释。

Result: 集成模型（特别是XGBoost和LightGBM）在所有评估指标（准确率、F1分数、AUC-ROC、马修斯相关系数）上表现最优。LIME识别出喹诺酮类、复方新诺明、黏菌素、氨基糖苷类和呋喃类药物耐药性为多重耐药预测的最强贡献因素，与已知生物学机制一致。

Conclusion: 结合高性能模型与局部可解释性方法，既能提供准确预测又能生成临床可操作的见解，支持早期多重耐药识别并增强机器学习辅助临床决策的可信度，有助于抗菌药物管理工作。

Abstract: The rise of Antimicrobial Resistance, particularly Multi-Drug Resistance (MDR), presents a critical challenge for clinical decision-making due to limited treatment options and delays in conventional susceptibility testing. This study proposes an interpretable machine learning framework to predict MDR in bacterial isolates using clinical features and antibiotic susceptibility patterns. Five classification models were evaluated, including Logistic Regression, Random Forest, AdaBoost, XGBoost, and LightGBM. The models were trained on a curated dataset of 9,714 isolates, with resistance encoded at the antibiotic family level to capture cross-class resistance patterns consistent with MDR definitions. Performance assessment included accuracy, F1-score, AUC-ROC, and Matthews Correlation Coefficient. Ensemble models, particularly XGBoost and LightGBM, demonstrated superior predictive capability across all metrics. To address the clinical transparency gap, Local Interpretable Model-agnostic Explanations (LIME) was applied to generate instance-level explanations. LIME identified resistance to quinolones, Co-trimoxazole, Colistin, aminoglycosides, and Furanes as the strongest contributors to MDR predictions, aligning with known biological mechanisms. The results show that combining high-performing models with local interpretability provides both accuracy and actionable insights for antimicrobial stewardship. This framework supports earlier MDR identification and enhances trust in machine learning-assisted clinical decision support.

</details>


### [179] [MolFM-Lite: Multi-Modal Molecular Property Prediction with Conformer Ensemble Attention and Cross-Modal Fusion](https://arxiv.org/abs/2602.22405)
*Syed Omer Shah,Mohammed Maqsood Ahmed,Danish Mohiuddin Mohammed,Shahnawaz Alam,Mohd Vahaj ur Rahman*

Main category: cs.LG

TL;DR: MolFM-Lite是一个多模态分子表示学习模型，通过联合编码1D序列、2D图和3D构象集合，并引入实验条件调制，在分子性质预测任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有分子性质预测模型通常依赖单一分子表示（序列、图或3D结构）并将分子几何视为静态，这限制了模型对分子复杂特性的全面理解。需要开发能够整合多种分子表示并考虑分子构象动态分布的多模态方法。

Method: 1. 联合编码SELFIES序列（1D）、分子图（2D）和构象集合（3D）的多模态架构
2. 使用交叉注意力融合机制，使各模态能够相互关注并共享互补信息
3. 引入构象集合注意力机制，结合可学习注意力与Boltzmann加权先验，捕捉分子形状的热力学分布
4. 通过特征线性调制（FiLM）将实验条件融入预测
5. 在ZINC250K数据集上进行跨模态对比和掩码原子目标的预训练

Result: 1. 在四个MoleculeNet骨架分割基准测试中，三模态融合相比单模态基线提供7-11%的AUC提升
2. 构象集合相比单构象变体带来约2%的性能提升
3. 全面的消融研究证实每个架构组件都有独立贡献
4. 在ZINC250K上的预训练以适中的计算成本实现了有效的权重初始化

Conclusion: MolFM-Lite通过整合多种分子表示和考虑构象集合的动态特性，显著提升了分子性质预测性能。该方法证明了多模态融合和热力学感知建模在计算化学中的重要性，为可复现研究提供了完整的代码、模型和数据分割。

Abstract: Most machine learning models for molecular property prediction rely on a single molecular representation (either a sequence, a graph, or a 3D structure) and treat molecular geometry as static. We present MolFM-Lite, a multi-modal model that jointly encodes SELFIES sequences (1D), molecular graphs (2D), and conformer ensembles (3D) through cross-attention fusion, while conditioning predictions on experimental context via Feature-wise Linear Modulation (FiLM). Our main methodological contributions are: (1) a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors over multiple RDKit-generated conformers, capturing the thermodynamic distribution of molecular shapes; and (2) a cross-modal fusion layer where each modality can attend to others, enabling complementary information sharing. We evaluate on four MoleculeNet scaffold-split benchmarks using our model's own splits, and report all baselines re-evaluated under the same protocol. Comprehensive ablation studies across all four datasets confirm that each architectural component contributes independently, with tri-modal fusion providing 7-11% AUC improvement over single-modality baselines and conformer ensembles adding approximately 2% over single-conformer variants. Pre-training on ZINC250K (~250K molecules) using cross-modal contrastive and masked-atom objectives enables effective weight initialization at modest compute cost. We release all code, trained models, and data splits to support reproducibility.

</details>


### [180] [Revisiting Chebyshev Polynomial and Anisotropic RBF Models for Tabular Regression](https://arxiv.org/abs/2602.22422)
*Luciano Gerber,Huw Lloyd*

Main category: cs.LG

TL;DR: 平滑基模型（如RBF网络和切比雪夫多项式）在表格回归中与树集成模型精度相当，但泛化间隙更小，适合需要连续可微预测的应用场景。


<details>
  <summary>Details</summary>
Motivation: 平滑基模型在数值分析中应用广泛，其连续可微的预测表面适合代理优化、敏感性分析等场景，但在表格回归领域很少使用，而树集成模型占主导地位。研究旨在探索平滑基模型是否能在表格回归中与树集成模型竞争。

Method: 开发了三种平滑基模型：1）各向异性RBF网络（具有数据驱动的中心放置和基于梯度的宽度优化）；2）岭正则化切比雪夫多项式回归器；3）平滑树混合模型（切比雪夫模型树）。在55个回归数据集上进行基准测试，对比树集成、预训练transformer和标准基线模型。

Result: Transformer在大多数数据集上精度排名第一，但其GPU依赖、推理延迟和数据集大小限制限制了在CPU环境中的应用。在CPU可行的模型中，平滑基模型和树集成模型在精度上统计相当，但平滑基模型通常表现出更小的泛化间隙。

Conclusion: 建议在候选模型池中常规包含平滑基模型，特别是在下游应用受益于更小泛化间隙和逐渐变化的预测时。平滑基模型在CPU环境中具有竞争力，特别适合需要连续可微预测的应用场景。

Abstract: Smooth-basis models such as Chebyshev polynomial regressors and radial basis function (RBF) networks are well established in numerical analysis. Their continuously differentiable prediction surfaces suit surrogate optimisation, sensitivity analysis, and other settings where the response varies gradually with inputs. Despite these properties, smooth models seldom appear in tabular regression, where tree ensembles dominate. We ask whether they can compete, benchmarking models across 55 regression datasets organised by application domain.
  We develop an anisotropic RBF network with data-driven centre placement and gradient-based width optimisation, a ridge-regularised Chebyshev polynomial regressor, and a smooth-tree hybrid (Chebyshev model tree); all three are released as scikit-learn-compatible packages. We benchmark these against tree ensembles, a pre-trained transformer, and standard baselines, evaluating accuracy alongside generalisation behaviour.
  The transformer ranks first on accuracy across a majority of datasets, but its GPU dependence, inference latency, and dataset-size limits constrain deployment in the CPU-based settings common across applied science and industry. Among CPU-viable models, smooth models and tree ensembles are statistically tied on accuracy, but the former tend to exhibit tighter generalisation gaps. We recommend routinely including smooth-basis models in the candidate pool, particularly when downstream use benefits from tighter generalisation and gradually varying predictions.

</details>


### [181] [Calibrated Test-Time Guidance for Bayesian Inference](https://arxiv.org/abs/2602.22428)
*Daniel Geyfman,Felix Draxler,Jan Groeneveld,Hyunsoo Lee,Theofanis Karaletsos,Stephan Mandt*

Main category: cs.LG

TL;DR: 本文提出了一种新的测试时引导方法，能够从正确的贝叶斯后验分布中进行校准采样，解决了现有方法只能最大化奖励而非真实后验分布的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时引导方法主要关注最大化奖励函数，而不是从真实的贝叶斯后验分布中采样，这导致了推理校准不准确的问题。作者发现常见测试时引导方法无法恢复正确的后验分布，并识别了导致这种失败的结构性近似问题。

Method: 作者提出了具有一致性的替代估计器，这些估计器能够实现从贝叶斯后验分布中的校准采样。该方法通过修正现有测试时引导方法的结构性近似问题，确保能够正确恢复后验分布。

Result: 在贝叶斯推理任务上，该方法显著优于先前的方法，并且在黑洞图像重建任务上达到了最先进的性能水平。

Conclusion: 本文提出的校准采样方法解决了测试时引导中的后验分布恢复问题，为扩散模型的贝叶斯推理提供了更准确和可靠的工具。

Abstract: Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.

</details>


### [182] [From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review](https://arxiv.org/abs/2602.22438)
*Uttamasha Anjally Oyshi,Susan Gauch*

Main category: cs.LG

TL;DR: Fair-PaperRec：一个在双盲评审后使用公平性正则化的MLP推荐系统，能在保持质量的同时显著增加弱势群体参与度


<details>
  <summary>Details</summary>
Motivation: 尽管采用双盲评审，作者人口统计学相关的系统性偏见仍然使弱势群体处于不利地位。假设：如果在评审后推荐系统中加入明确的公平性正则化，可以在不降低质量的情况下增加包容性。

Method: 提出Fair-PaperRec，这是一个多层感知机(MLP)模型，在交叉属性（如种族、国家）上使用可微分的公平性损失函数，用于双盲评审后的论文重新排序。先在合成数据集（高、中、接近公平的偏见水平）上测试，然后在真实会议数据（ACM SIGCHI、DIS、IUI）上验证。

Result: 在合成数据集上，增加公平性权重能增强宏观/微观多样性，同时保持效用稳定。在真实场景中，适当调优的Fair-PaperRec配置能将弱势群体参与度提高42.03%，而整体效用变化最多仅3.16%。

Conclusion: 公平性正则化既能作为公平机制，又能作为温和的质量正则化器，特别是在高度偏见的场景中。Fair-PaperRec提供了一个实用的、以公平为中心的评审后论文选择框架，既能保持甚至在某些情况下增强学术质量。

Abstract: Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. To test this, we introduce Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss over intersectional attributes (e.g., race, country) that re-ranks papers after double-blind review. We first probe the hypothesis on synthetic datasets spanning high, moderate, and near-fair biases. Across multiple randomized runs, these controlled studies map where increasing the fairness weight strengthens macro/micro diversity while keeping utility approximately stable, demonstrating robustness and adaptability under varying disparity levels. We then carry the hypothesis into the original setting, conference data from ACM Special Interest Group on Computer-Human Interaction (SIGCHI), Designing Interactive Systems (DIS), and Intelligent User Interfaces (IUI). In this real-world scenario, an appropriately tuned configuration of Fair-PaperRec achieves up to a 42.03% increase in underrepresented-group participation with at most a 3.16% change in overall utility relative to the historical selection. Taken together, the synthetic-to-original progression shows that fairness regularization can act as both an equity mechanism and a mild quality regularizer, especially in highly biased regimes. By first analyzing the behavior of the fairness parameters under controlled conditions and then validating them on real submissions, Fair-PaperRec offers a practical, equity-focused framework for post-review paper selection that preserves, and in some settings can even enhance, measured scholarly quality.

</details>


### [183] [ECHO: Encoding Communities via High-order Operators](https://arxiv.org/abs/2602.22446)
*Emilio Ferrara*

Main category: cs.LG

TL;DR: ECHO是一个可扩展的自监督架构，通过自适应多尺度扩散过程解决属性网络社区检测问题，克服了GNN的计算瓶颈和语义过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 属性网络社区检测面临基本分歧：拓扑算法忽略语义特征，而图神经网络(GNNs)存在计算瓶颈。GNNs在密集或异配网络中遭遇特征过平滑的"语义墙"，以及由成对聚类O(N²)内存约束驱动的"系统墙"。

Method: 引入ECHO架构，将社区检测重构为自适应多尺度扩散过程。包括：1) 拓扑感知路由器，自动分析结构启发式(稀疏性、密度、同配性)以通过最优归纳偏置路由图；2) 内存分片全批次对比目标；3) 新颖的分块O(N·K)相似性提取方法，完全绕过传统O(N²)内存瓶颈。

Result: 在扩展到100万个节点的合成LFR基准测试中，ECHO实现尺度不变准确性；在超过160万个节点和3000万条边的大规模真实社交网络中，仅需几分钟完成聚类，吞吐量超过2800节点/秒，匹配高度优化的纯拓扑基线速度。

Conclusion: ECHO通过拓扑特征协同一致克服经典分辨率限制，提供统一框架自动参与内存分片优化，支持跨不同硬件约束的采用，解决了属性网络社区检测中的计算和语义瓶颈问题。

Abstract: Community detection in attributed networks faces a fundamental divide: topological algorithms ignore semantic features, while Graph Neural Networks (GNNs) encounter devastating computational bottlenecks. Specifically, GNNs suffer from a Semantic Wall of feature over smoothing in dense or heterophilic networks, and a Systems Wall driven by the O(N^2) memory constraints of pairwise clustering. To dismantle these barriers, we introduce ECHO (Encoding Communities via High order Operators), a scalable, self supervised architecture that reframes community detection as an adaptive, multi scale diffusion process. ECHO features a Topology Aware Router that automatically analyzes structural heuristics sparsity, density, and assortativity to route graphs through the optimal inductive bias, preventing heterophilic poisoning while ensuring semantic densification. Coupled with a memory sharded full batch contrastive objective and a novel chunked O(N \cdot K) similarity extraction method, ECHO completely bypasses traditional O(N^2) memory bottlenecks without sacrificing the mathematical precision of global gradients. Extensive evaluations demonstrate that this topology feature synergy consistently overcomes the classical resolution limit. On synthetic LFR benchmarks scaled up to 1 million nodes, ECHO achieves scale invariant accuracy despite severe topological noise. Furthermore, on massive real world social networks with over 1.6 million nodes and 30 million edges, it completes clustering in mere minutes with throughputs exceeding 2,800 nodes per second matching the speed of highly optimized purely topological baselines. The implementation utilizes a unified framework that automatically engages memory sharded optimization to support adoption across varying hardware constraints. GitHub Repository: https://github.com/emilioferrara/ECHO-GNN

</details>


### [184] [Beyond performance-wise Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2602.22470)
*Balazs Pejo*

Main category: cs.LG

TL;DR: 论文提出联邦学习中现有客户端评估方法仅关注模型性能（如准确率），忽略了模型可信度维度（可靠性、鲁棒性、公平性），并采用Shapley值量化多维度贡献，发现各客户端在不同维度表现各异，单一指标无法全面评估和公平奖励分配。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习客户端评估方法主要关注模型性能指标（准确率、损失），但这只是机器学习模型整体效用的一个维度。本研究关注被忽视的客户端对模型可信度的贡献问题，包括可靠性（对噪声数据的容忍度）、鲁棒性（对抗样本的抵抗能力）和公平性（通过人口统计奇偶性衡量）。

Method: 采用最先进的Shapley值近似方法，这是一种原则性的价值归因方法，用于量化客户端在多维度（可靠性、鲁棒性、公平性）上的贡献。

Result: 研究结果显示：1）没有单个客户端在所有维度上都表现出色；2）这些维度在很大程度上相互独立；3）当前评估方案存在关键缺陷：单一指标无法进行全面评估和公平奖励分配。

Conclusion: 联邦学习需要多维度的客户端评估框架，超越传统的性能指标，纳入可信度维度（可靠性、鲁棒性、公平性），并使用Shapley值等原则性方法进行量化，以实现更全面和公平的贡献评估与奖励分配。

Abstract: Federated learning offers a privacy-friendly collaborative learning framework, yet its success, like any joint venture, hinges on the contributions of its participants. Existing client evaluation methods predominantly focus on model performance, such as accuracy or loss, which represents only one dimension of a machine learning model's overall utility. In contrast, this work investigates the critical, yet overlooked, issue of client contributions towards a model's trustworthiness -- specifically, its reliability (tolerance to noisy data), resilience (resistance to adversarial examples), and fairness (measured via demographic parity). To quantify these multifaceted contributions, we employ the state-of-the-art approximation of the Shapley value, a principled method for value attribution. Our results reveal that no single client excels across all dimensions, which are largely independent from each other, highlighting a critical flaw in current evaluation scheme: no single metric is adequate for comprehensive evaluation and equitable rewarding allocation.

</details>


### [185] [Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns](https://arxiv.org/abs/2602.22479)
*Afshin Khadangi*

Main category: cs.LG

TL;DR: TRC²是一种用于持续学习的解码器架构，通过丘脑路由和快速校正通路解决稳定性-可塑性权衡问题，在保持先前知识的同时实现快速在线适应。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的持续学习面临灾难性遗忘问题，而现有方法要么增加延迟和内存占用，要么无法扩展到长上下文场景，需要一种能在架构层面解决持续学习问题的方案。

Method: 提出TRC²架构，结合稀疏丘脑路由、调制、预测、记忆和反馈机制，以及快速校正通路，支持快速适应而不破坏慢速参数稳定性。采用稀疏分块并行设计，便于高效训练和推理。

Result: 在语言建模和持续学习基准测试中，TRC²在相同计算量下改善了稳定性-可塑性权衡，能够实现快速在线适应同时保持先前习得的行为。

Conclusion: TRC²通过架构层面的创新解决了持续学习中的灾难性遗忘问题，为部署语言模型提供了有效的持续学习解决方案。

Abstract: Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC$^{2}$ (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC$^{2}$ combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC$^{2}$ improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.

</details>


### [186] [Reinforcement-aware Knowledge Distillation for LLM Reasoning](https://arxiv.org/abs/2602.22495)
*Zhaoyang Zhang,Shuli Jiang,Yantao Shen,Yuting Zhang,Dhananjay Ram,Shuo Yang,Zhuowen Tu,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: RLAD是一种强化学习感知的知识蒸馏方法，通过选择性模仿教师模型来改进学生模型，解决了传统蒸馏方法在RL中的分布不匹配和目标冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法主要针对监督微调设计，当与强化学习结合时，会出现分布不匹配和目标干扰问题：教师监督可能与学生不断演化的rollout分布不一致，KL正则化器可能与奖励最大化相冲突，需要仔细的损失平衡。

Method: 提出RLAD方法，在RL过程中进行选择性模仿，仅在能改进当前策略更新时引导学生向教师模型学习。核心组件TRRD用PPO/GRPO风格的似然比目标替代教师-学生KL正则化器，该目标锚定在教师-旧策略混合上，在学生rollout上实现优势感知、信任区域有界的蒸馏。

Result: 在多样化的逻辑推理和数学基准测试中，RLAD一致优于离线蒸馏、标准GRPO以及基于KL的在线教师-学生知识蒸馏方法。

Conclusion: RLAD通过选择性模仿和TRRD机制，有效解决了RL与知识蒸馏结合时的分布不匹配和目标冲突问题，为将大型推理LLM的能力蒸馏到更小的学生模型提供了更有效的方法。

Abstract: Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student's evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.

</details>


### [187] [Space Syntax-guided Post-training for Residential Floor Plan Generation](https://arxiv.org/abs/2602.22507)
*Zhuoyang Jiang,Dongqing Zhang*

Main category: cs.LG

TL;DR: 该论文提出SSPT方法，通过空间句法指导的后训练范式，将建筑学知识注入住宅平面图生成，改善公共空间主导性和功能层次结构。


<details>
  <summary>Details</summary>
Motivation: 现有预训练生成模型主要优化拟合大规模数据分布，但忽视了关键的建筑学先验，如公共空间（客厅、门厅）的配置主导性和连通性。需要将空间句法知识显式注入平面图生成过程。

Method: 提出SSPT后训练范式：1）使用非可微分oracle将布局转换为矩形空间图，通过贪婪最大矩形分解和门介导邻接构建，计算基于整合度的度量；2）两种实现策略：迭代重训练（空间句法过滤+扩散微调）和强化学习（PPO+空间句法奖励）；3）建立SSPT-Bench评估基准。

Result: 两种策略均改善了公共空间主导性并恢复了更清晰的功能层次结构。PPO策略获得更强性能提升，计算效率更高且方差更小。方法兼容其他生成主干网络。

Conclusion: SSPT为将建筑理论整合到数据驱动的平面生成提供了可扩展路径，通过后训练范式显式注入空间句法知识，改善了生成平面图的功能合理性。

Abstract: Pre-trained generative models for residential floor plans are typically optimized to fit large-scale data distributions, which can under-emphasize critical architectural priors such as the configurational dominance and connectivity of domestic public spaces (e.g., living rooms and foyers). This paper proposes Space Syntax-guided Post-training (SSPT), a post-training paradigm that explicitly injects space syntax knowledge into floor plan generation via a non-differentiable oracle. The oracle converts RPLAN-style layouts into rectangle-space graphs through greedy maximal-rectangle decomposition and door-mediated adjacency construction, and then computes integration-based measurements to quantify public space dominance and functional hierarchy.
  To enable consistent evaluation and diagnosis, we further introduce SSPT-Bench (Eval-8), an out-of-distribution benchmark that post-trains models using conditions capped at $\leq 7$ rooms while evaluating on 8-room programs, together with a unified metric suite for dominance, stability, and profile alignment. SSPT is instantiated with two strategies: (i) iterative retraining via space-syntax filtering and diffusion fine-tuning, and (ii) reinforcement learning via PPO with space-syntax rewards. Experiments show that both strategies improve public-space dominance and restore clearer functional hierarchy compared to distribution-fitted baselines, while PPO achieves stronger gains with substantially higher compute efficiency and reduced variance. SSPT provides a scalable pathway for integrating architectural theory into data-driven plan generation and is compatible with other generative backbones given a post-hoc evaluation oracle.

</details>


### [188] [TEFL: Prediction-Residual-Guided Rolling Forecasting for Multi-Horizon Time Series](https://arxiv.org/abs/2602.22520)
*Xiannan Huang,Shen Fang,Shuhan Qiu,Chengcheng Yu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: TEFL（时间误差反馈学习）是一个深度学习框架，通过将历史预测残差反馈到训练和评估过程中，提升时间序列预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习预测模型通常只最小化点对点预测损失，忽略了滚动预测中历史残差所包含的丰富信息，这些残差反映了持续偏差、未建模模式或动态演变。

Method: 提出TEFL框架，解决三个关键挑战：1）在滚动预测部分可观测性下选择可观测的多步残差；2）通过轻量级低秩适配器集成残差以保持效率并防止过拟合；3）设计两阶段训练程序联合优化基础预测器和误差模块。

Result: 在10个真实世界数据集和5个骨干架构上的实验表明，TEFL持续提升准确性，平均降低MAE 5-10%。在突变和分布偏移等挑战性场景下，误差减少超过10%（最高达19.5%），表现出强鲁棒性。

Conclusion: TEFL通过将基于残差的反馈直接嵌入学习过程，为现代深度学习预测系统提供了一个简单、通用且有效的增强方案。

Abstract: Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics. We propose TEFL (Temporal Error Feedback Learning), a unified learning framework that explicitly incorporates these historical residuals into the forecasting pipeline during both training and evaluation. To make this practical in deep multi-step settings, we address three key challenges: (1) selecting observable multi-step residuals under the partial observability of rolling forecasts, (2) integrating them through a lightweight low-rank adapter to preserve efficiency and prevent overfitting, and (3) designing a two-stage training procedure that jointly optimizes the base forecaster and error module. Extensive experiments across 10 real-world datasets and 5 backbone architectures show that TEFL consistently improves accuracy, reducing MAE by 5-10% on average. Moreover, it demonstrates strong robustness under abrupt changes and distribution shifts, with error reductions exceeding 10% (up to 19.5%) in challenging scenarios. By embedding residual-based feedback directly into the learning process, TEFL offers a simple, general, and effective enhancement to modern deep forecasting systems.

</details>


### [189] [Predicting Tennis Serve directions with Machine Learning](https://arxiv.org/abs/2602.22527)
*Ying Zhu,Ruthuparna Naikar*

Main category: cs.LG

TL;DR: 开发机器学习方法预测职业网球选手一发方向，平均准确率男49%、女44%，分析显示顶尖选手使用混合策略且疲劳影响发球决策


<details>
  <summary>Details</summary>
Motivation: 职业网球中发球（尤其是一发）至关重要，发球方需在最大化胜率与保持不可预测性之间平衡，接发方则试图预测发球方向。理解这种心理博弈对分析职业网球决策很重要。

Method: 通过特征工程开发机器学习方法来预测职业网球选手的一发方向，分析发球决策模式。

Result: 方法平均预测准确率：男性选手约49%，女性选手约44%。分析表明顶尖职业选手在发球决策中使用混合策略模型，疲劳可能是影响发球方向选择的因素，情境信息对接发方预判反应的重要性可能比之前认为的更大。

Conclusion: 机器学习方法能有效预测职业网球选手的发球方向，揭示了顶尖选手的战略决策模式，为理解网球比赛中的心理博弈提供了新视角，对接发方训练和战术分析有实际意义。

Abstract: Serves, especially first serves, are very important in professional tennis. Servers choose their serve directions strategically to maximize their winning chances while trying to be unpredictable. On the other hand, returners try to predict serve directions to make good returns. The mind game between servers and returners is an important part of decision-making in professional tennis matches. To help understand the players' serve decisions, we have developed a machine learning method for predicting professional tennis players' first serve directions. Through feature engineering, our method achieves an average prediction accuracy of around 49\% for male players and 44\% for female players. Our analysis provides some evidence that top professional players use a mixed-strategy model in serving decisions and that fatigue might be a factor in choosing serve directions. Our analysis also suggests that contextual information is perhaps more important for returners' anticipatory reactions than previously thought.

</details>


### [190] [Coarse-to-Fine Learning of Dynamic Causal Structures](https://arxiv.org/abs/2602.22532)
*Dezhi Yang,Qiaoyu Tan,Carlotta Domeniconi,Jun Wang,Lizhen Cui,Guoxian Yu*

Main category: cs.LG

TL;DR: DyCausal：一种动态因果结构学习框架，利用卷积网络捕捉粗粒度时间窗口的因果模式，通过线性插值细化每个时间步的因果结构，解决完全动态因果关系的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖分布或结构不变性假设，要求平稳或部分平稳因果关系，这与现实世界中复杂、时变的因果关系相矛盾。需要处理完全动态因果关系的方法，其中瞬时和滞后依赖关系都随时间演变。

Method: DyCausal框架：1）使用卷积网络捕捉粗粒度时间窗口的因果模式；2）应用线性插值细化每个时间步的因果结构；3）提出基于矩阵范数缩放的无环约束，提高效率并有效约束演化因果结构中的循环。

Result: 在合成和真实数据集上的综合评估表明，DyCausal相比现有方法实现了更优性能，提供了一种稳定且高效的方法，用于从粗到细识别完全动态因果结构。

Conclusion: DyCausal为解决完全动态因果结构学习问题提供了一个有效的框架，能够处理时变因果关系的挑战，在效率和稳定性方面表现优异。

Abstract: Learning the dynamic causal structure of time series is a challenging problem. Most existing approaches rely on distributional or structural invariance to uncover underlying causal dynamics, assuming stationary or partially stationary causality. However, these assumptions often conflict with the complex, time-varying causal relationships observed in real-world systems. This motivates the need for methods that address fully dynamic causality, where both instantaneous and lagged dependencies evolve over time. Such a setting poses significant challenges for the efficiency and stability of causal discovery. To address these challenges, we introduce DyCausal, a dynamic causal structure learning framework. DyCausal leverages convolutional networks to capture causal patterns within coarse-grained time windows, and then applies linear interpolation to refine causal structures at each time step, thereby recovering fine-grained and time-varying causal graphs. In addition, we propose an acyclic constraint based on matrix norm scaling, which improves efficiency while effectively constraining loops in evolving causal structures. Comprehensive evaluations on both synthetic and real-world datasets demonstrate that DyCausal achieves superior performance compared to existing methods, offering a stable and efficient approach for identifying fully dynamic causal structures from coarse to fine.

</details>


### [191] [Persistent Nonnegative Matrix Factorization via Multi-Scale Graph Regularization](https://arxiv.org/abs/2602.22536)
*Jichao Zhang,Ran Miao,Limin Li*

Main category: cs.LG

TL;DR: 提出持久非负矩阵分解(pNMF)，通过持久同调识别尺度参数，生成多尺度嵌入序列而非单一分解，解决传统NMF无法捕捉跨分辨率连通性结构演化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统非负矩阵分解(NMF)方法是单尺度的，无法捕捉连通性结构在不同分辨率下的演化过程。现有方法缺乏对多尺度数据表示的支持，需要一种能够跨尺度分析数据连通性变化的框架。

Method: 提出持久非负矩阵分解(pNMF)：1) 利用持久同调识别底层连通性发生定性变化的规范最小充分尺度集；2) 这些尺度诱导出图拉普拉斯序列；3) 构建具有尺度几何正则化和显式跨尺度一致性约束的耦合NMF公式；4) 开发保证收敛的序列交替优化算法。

Result: 在合成数据和单细胞RNA测序数据集上的数值实验表明，该方法在多尺度低秩嵌入方面有效。建立了沿尺度参数的嵌入结构性质分析，并证明了连续尺度间增量的界限。

Conclusion: pNMF定义了跨尺度的非平凡解路径而非单一分解，解决了传统NMF的单尺度限制，能够捕捉数据连通性结构的多尺度演化，为多尺度数据表示提供了新框架。

Abstract: Matrix factorization techniques, especially Nonnegative Matrix Factorization (NMF), have been widely used for dimensionality reduction and interpretable data representation. However, existing NMF-based methods are inherently single-scale and fail to capture the evolution of connectivity structures across resolutions. In this work, we propose persistent nonnegative matrix factorization (pNMF), a scale-parameterized family of NMF problems, that produces a sequence of persistence-aligned embeddings rather than a single one. By leveraging persistent homology, we identify a canonical minimal sufficient scale set at which the underlying connectivity undergoes qualitative changes. These canonical scales induce a sequence of graph Laplacians, leading to a coupled NMF formulation with scale-wise geometric regularization and explicit cross-scale consistency constraint. We analyze the structural properties of the embeddings along the scale parameter and establish bounds on their increments between consecutive scales. The resulting model defines a nontrivial solution path across scales, rather than a single factorization, which poses new computational challenges. We develop a sequential alternating optimization algorithm with guaranteed convergence. Numerical experiments on synthetic and single-cell RNA sequencing datasets demonstrate the effectiveness of the proposed approach in multi-scale low-rank embeddings.

</details>


### [192] [LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation](https://arxiv.org/abs/2602.22537)
*Shouwei Gao,Xu Zheng,Dongsheng Luo,Sheng Di,Wenqian Dong*

Main category: cs.LG

TL;DR: LUMOS：基于L0正则化学习的端到端框架，统一特征选择和模型剪枝，实现SciML模型设计的民主化，平均减少71.45%参数并实现6.4倍推理加速


<details>
  <summary>Details</summary>
Motivation: 科学机器学习（SciML）快速发展但模型设计困难，需要大量先验知识和手动调优，特别是在特征选择和模型规模确定方面。现有方法依赖专家经验，限制了SciML的广泛应用。

Method: 提出LUMOS框架，基于L0正则化学习，采用半随机门控和重参数化技术，在训练过程中动态选择信息特征并剪枝冗余参数，实现特征选择和模型剪枝的统一。

Result: 在13个SciML任务（包括宇宙学和分子科学）上评估，平均实现71.45%参数减少和6.4倍推理加速。分布式数据并行训练在8个GPU上验证了可扩展性。

Conclusion: LUMOS框架有效减少了SciML模型设计对人工调优的依赖，实现了特征选择和模型剪枝的自动化，在保持预测准确性的同时显著提升了模型效率和推理速度。

Abstract: The rapid growth of scientific machine learning (SciML) has accelerated discovery across diverse domains, yet designing effective SciML models remains a challenging task. In practice, building such models often requires substantial prior knowledge and manual expertise, particularly in determining which input features to use and how large the model should be. We introduce LUMOS, an end-to-end framework based on L0-regularized learning that unifies feature selection and model pruning to democratize SciML model design. By employing semi-stochastic gating and reparameterization techniques, LUMOS dynamically selects informative features and prunes redundant parameters during training, reducing the reliance on manual tuning while maintaining predictive accuracy. We evaluate LUMOS across 13 diverse SciML workloads, including cosmology and molecular sciences, and demonstrate its effectiveness and generalizability. Experiments on 13 SciML models show that LUMOS achieves 71.45% parameter reduction and a 6.4x inference speedup on average. Furthermore, Distributed Data Parallel (DDP) training on up to eight GPUs confirms the scalability of

</details>


### [193] [RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format](https://arxiv.org/abs/2602.22538)
*Zhehao Huang,Yuhang Liu,Baijiong Lin,Yixin Lou,Zhengbao He,Hanling Tian,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: RAIN-Merging：一种无需梯度的方法，通过将指令调优模型的任务向量投影到推理特殊标记的前向特征零空间，并利用指令注意力进行模块特定缩放，在保持大推理模型推理能力的同时显著提升指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在长链推理方面表现出色，但在遵循输出格式、约束或特定要求的指令方面存在不足。研究探索是否可以通过将指令调优模型集成到大推理模型中来弥补这一差距。

Method: 提出RAIN-Merging方法：1）使用小型推理校准集，将ITM任务向量投影到推理特殊标记前向特征的零空间，保留LRM的结构化推理机制；2）使用小型指令校准集，估计指令注意力以推导模块特定缩放，放大指令相关组件并抑制泄漏。

Result: 在四个指令遵循基准测试和九个推理与通用能力基准测试中，RAIN-Merging显著提高了指令遵循能力，同时保持了推理质量。改进在不同模型规模和架构中保持一致，并在智能体设置中转化为性能提升。

Conclusion: RAIN-Merging通过梯度自由的方法有效整合了指令遵循能力，同时保持推理性能，解决了大推理模型在指令遵循方面的不足，为模型融合提供了新思路。

Abstract: Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.

</details>


### [194] [Relatron: Automating Relational Machine Learning over Relational Databases](https://arxiv.org/abs/2602.22552)
*Zhikai Chen,Han Xie,Jian Zhang,Jiliang Tang,Xiang Song,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 该研究系统比较了关系型深度学习(RDL)与深度特征合成(DFS)在关系数据库预测建模中的表现，发现两者性能高度依赖任务且无单一最优架构，提出了基于任务信号的元选择器Relatron来指导模型选择。


<details>
  <summary>Details</summary>
Motivation: 关系数据库预测建模面临捕捉跨表依赖和复杂特征交互的挑战。虽然RDL方法通过消息传递自动化特征工程，而DFS依赖预定义聚合器，但两者比较优势不明确，缺乏有效的架构选择原则。

Method: 将RDL和DFS统一到共享设计空间，进行架构中心搜索；构建模型性能库；提出两个任务信号（RDB任务同质性和亲和嵌入）来分析性能差距；开发Relatron元选择器，结合轻量级损失景观指标。

Result: 研究发现：(1) RDL不总是优于DFS，性能高度任务依赖；(2) 无单一架构主导所有任务；(3) 验证准确率不可靠。Relatron解决了"更多调优，更差性能"问题，在联合超参数-架构优化中比强基线提升18.5%，成本降低10倍。

Conclusion: 关系数据库预测建模需要任务感知的模型选择。提出的任务信号和Relatron元选择器能够有效指导RDL与DFS的选择，并通过损失景观指标提高鲁棒性，为实际应用提供了实用的解决方案。

Abstract: Predictive modeling over relational databases (RDBs) powers applications, yet remains challenging due to capturing both cross-table dependencies and complex feature interactions. Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood. We present a comprehensive study that unifies RDL and DFS in a shared design space and conducts architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a model performance bank that links architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL-DFS performance gap and introduce two task signals -- RDB task homophily and an affinity embedding that captures size, path, feature, and temporal structure -- whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that chooses between RDL and DFS and prunes the within-family search. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the "more tuning, worse performance" effect and, in joint hyperparameter-architecture optimization, achieves up to 18.5% improvement over strong baselines with 10x lower cost than Fisher information-based alternatives.

</details>


### [195] [Multilingual Safety Alignment Via Sparse Weight Editing](https://arxiv.org/abs/2602.22554)
*Jiaming Liang,Zhaoxin Wang,Handing Wang*

Main category: cs.LG

TL;DR: 提出一种基于稀疏权重编辑的无训练对齐框架，通过线性变换将低资源语言的有害表征映射到高资源语言的安全子空间，显著降低攻击成功率且不影响通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不同语言间的安全性存在显著差异，低资源语言往往能绕过为高资源语言（如英语）建立的安全防护。现有解决方案（如多语言监督微调或RLHF）计算成本高且依赖稀缺的多语言安全数据。

Method: 基于稀疏权重编辑的无训练对齐框架。识别安全能力集中在稀疏的安全神经元中，将跨语言对齐问题形式化为约束线性变换。推导出闭式解，将低资源语言的有害表征最优映射到高资源语言的鲁棒安全子空间，同时通过零空间投影约束保持通用效用。

Result: 在8种语言和多个模型家族（Llama-3、Qwen-2.5）上的广泛实验表明，该方法显著降低了低资源语言的攻击成功率，对通用推理能力影响可忽略，且只需一次数据高效计算。

Conclusion: 提出了一种计算高效、数据高效的无训练跨语言安全对齐方法，通过稀疏权重编辑和线性变换有效解决低资源语言的安全漏洞问题。

Abstract: Large Language Models (LLMs) exhibit significant safety disparities across languages, with low-resource languages (LRLs) often bypassing safety guardrails established for high-resource languages (HRLs) like English. Existing solutions, such as multilingual supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), are computationally expensive and dependent on scarce multilingual safety data. In this work, we propose a novel, training-free alignment framework based on Sparse Weight Editing. Identifying that safety capabilities are localized within a sparse set of safety neurons, we formulate the cross-lingual alignment problem as a constrained linear transformation. We derive a closed-form solution to optimally map the harmful representations of LRLs to the robust safety subspaces of HRLs, while preserving general utility via a null-space projection constraint. Extensive experiments across 8 languages and multiple model families (Llama-3, Qwen-2.5) demonstrate that our method substantially reduces Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, all achieved with a single, data-efficient calculation.

</details>


### [196] [Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation](https://arxiv.org/abs/2602.22556)
*Zihang Xu,Haozhi Xie,Ziqi Miao,Wuxuan Gong,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 提出两阶段框架解决大推理模型在简单问题上过度思考的问题，通过混合微调和自适应强化学习实现稳定准确率-效率权衡


<details>
  <summary>Details</summary>
Motivation: 大推理模型在处理简单查询时存在过度思考问题，现有方法在准确率-效率权衡上不稳定，对异构推理行为鲁棒性差

Method: 两阶段框架：1) 混合微调让模型接触思考和不思考行为；2) 自适应强化学习，包含保持正确性的优势塑形和长度感知梯度调节

Result: 在Qwen2.5-1.5B和7B模型上，相比基线提升3.7/3.6个准确率点，同时减少40.6%/43.9%的生成token，在不同难度和分布外任务上表现鲁棒

Conclusion: 提出的两阶段框架能有效解决大推理模型的过度思考问题，实现稳定准确率-效率权衡，具有良好的鲁棒性和泛化能力

Abstract: Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.

</details>


### [197] [Autoregressive Visual Decoding from EEG Signals](https://arxiv.org/abs/2602.22555)
*Sicheng Dai,Hongwang Xiao,Shan Yu,Qiwei Ye*

Main category: cs.LG

TL;DR: AVDE：基于自回归生成框架的轻量级EEG视觉解码方法，通过对比学习对齐EEG-图像表示，采用"下一尺度预测"策略实现高效图像重建，参数量仅为现有方法的10%。


<details>
  <summary>Details</summary>
Motivation: 当前EEG视觉解码方法面临模态差距大、多阶段适应过程复杂、计算开销大等问题，限制了其在真实BCI应用中的实用性。

Method: 1) 使用预训练EEG模型LaBraM，通过对比学习微调以对齐EEG和图像表示；2) 采用自回归生成框架，基于"下一尺度预测"策略：将图像编码为多尺度token映射，训练transformer从EEG嵌入开始自回归预测更细尺度token。

Result: 在两个数据集上，AVDE在图像检索和重建任务中均优于先前SOTA方法，参数量仅为10%。可视化显示其生成过程反映了人类视觉感知的层次性。

Conclusion: 自回归模型可作为高效且可解释的工具用于实际BCI应用，AVDE框架在保持EEG信号与重建图像直接联系的同时实现了连贯生成。

Abstract: Electroencephalogram (EEG) signals have become a popular medium for decoding visual information due to their cost-effectiveness and high temporal resolution. However, current approaches face significant challenges in bridging the modality gap between EEG and image data. These methods typically rely on complex adaptation processes involving multiple stages, making it hard to maintain consistency and manage compounding errors. Furthermore, the computational overhead imposed by large-scale diffusion models limit their practicality in real-world brain-computer interface (BCI) applications. In this work, we present AVDE, a lightweight and efficient framework for visual decoding from EEG signals. First, we leverage LaBraM, a pre-trained EEG model, and fine-tune it via contrastive learning to align EEG and image representations. Second, we adopt an autoregressive generative framework based on a "next-scale prediction" strategy: images are encoded into multi-scale token maps using a pre-trained VQ-VAE, and a transformer is trained to autoregressively predict finer-scale tokens starting from EEG embeddings as the coarsest representation. This design enables coherent generation while preserving a direct connection between the input EEG signals and the reconstructed images. Experiments on two datasets show that AVDE outperforms previous state-of-the-art methods in both image retrieval and reconstruction tasks, while using only 10% of the parameters. In addition, visualization of intermediate outputs shows that the generative process of AVDE reflects the hierarchical nature of human visual perception. These results highlight the potential of autoregressive models as efficient and interpretable tools for practical BCI applications.

</details>


### [198] [TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion](https://arxiv.org/abs/2602.22586)
*Donghong Cai,Jiarui Feng,Yanbo Wang,Da Zheng,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: TabDLM：一个统一的自由形式表格数据生成框架，通过联合数值-语言扩散模型处理包含文本、分类和数值特征的异构表格数据。


<details>
  <summary>Details</summary>
Motivation: 现实世界的表格数据集越来越多地包含自由形式的文本字段（如评论或临床记录）以及结构化的数值和分类属性。生成这种具有不同模态的异构表格并实现联合建模仍然具有挑战性。现有方法主要分为两类：扩散方法和LLM方法，但各有局限性。

Method: TabDLM是一个统一的自由形式表格数据生成框架，基于掩码扩散语言模型构建联合数值-语言扩散模型。通过掩码扩散建模文本和分类特征，同时通过学习的专用数值标记嵌入对数值特征进行连续扩散过程建模；双向注意力机制在单个模型中捕获跨模态交互。

Result: 在多样化基准测试上的广泛实验表明，TabDLM相比强大的扩散和LLM基线方法具有有效性。

Conclusion: TabDLM提供了一个统一的框架，能够有效地生成包含自由形式文本、分类和数值特征的异构表格数据，克服了现有方法的局限性。

Abstract: Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.

</details>


### [199] [Operationalizing Fairness: Post-Hoc Threshold Optimization Under Hard Resource Limits](https://arxiv.org/abs/2602.22560)
*Moirangthem Tiken Singh,Amit Kalita,Sapam Jitu Singh*

Main category: cs.LG

TL;DR: 提出一个后处理、模型无关的阈值优化框架，在严格容量约束下联合平衡安全性、效率和公平性，使用单一全局决策阈值确保法律合规性。


<details>
  <summary>Details</summary>
Motivation: 机器学习在高风险领域部署需要在预测安全性和算法公平性之间取得平衡，但现有公平性干预通常假设无约束资源，使用群体特定决策阈值违反反歧视法规。

Method: 引入后处理、模型无关的阈值优化框架，使用参数化伦理损失函数和有限决策规则，在严格容量约束下联合优化安全性、效率和公平性，强制使用单一全局决策阈值确保法律合规。

Result: 容量约束主导伦理优先级，在80%测试配置中严格资源限制决定最终部署阈值；在25%容量限制下，框架保持高风险识别（召回率0.409-0.702），而标准无约束公平启发式方法效用接近零。

Conclusion: 理论公平目标必须明确服从操作容量限制才能实际部署；通过将预测评分与政策评估解耦并严格限制干预率，该框架为利益相关者在资源受限环境中导航不可避免的伦理权衡提供了实用且法律合规的机制。

Abstract: The deployment of machine learning in high-stakes domains requires a balance between predictive safety and algorithmic fairness. However, existing fairness interventions often as- sume unconstrained resources and employ group-specific decision thresholds that violate anti- discrimination regulations. We introduce a post-hoc, model-agnostic threshold optimization framework that jointly balances safety, efficiency, and equity under strict and hard capacity constraints. To ensure legal compliance, the framework enforces a single, global decision thresh- old. We formulated a parameterized ethical loss function coupled with a bounded decision rule that mathematically prevents intervention volumes from exceeding the available resources. An- alytically, we prove the key properties of the deployed threshold, including local monotonicity with respect to ethical weighting and the formal identification of critical capacity regimes. We conducted extensive experimental evaluations on diverse high-stakes datasets. The principal re- sults demonstrate that capacity constraints dominate ethical priorities; the strict resource limit determines the final deployed threshold in over 80% of the tested configurations. Furthermore, under a restrictive 25% capacity limit, the proposed framework successfully maintains high risk identification (recall ranging from 0.409 to 0.702), whereas standard unconstrained fairness heuristics collapse to a near-zero utility. We conclude that theoretical fairness objectives must be explicitly subordinated to operational capacity limits to remain in deployment. By decou- pling predictive scoring from policy evaluation and strictly bounding intervention rates, this framework provides a practical and legally compliant mechanism for stakeholders to navigate unavoidable ethical trade-offs in resource-constrained environments.

</details>


### [200] [pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training](https://arxiv.org/abs/2602.22592)
*Wenzheng Zhang,Bingzheng Liu,Yang Hu,Xiaoying Bai,Wentao Zhang,Bin Cui*

Main category: cs.LG

TL;DR: pQuant：通过参数解耦和分支架构解决极低比特量化中的参数民主化问题，实现1比特主导分支+高精度分支的混合量化方法


<details>
  <summary>Details</summary>
Motivation: 现有量化感知训练方法在极低比特（sub 2-bit）量化时存在参数民主化效应瓶颈，即所有参数敏感性趋于同质化，严重限制了模型表达能力，导致精度和可扩展性不足

Method: 将线性层拆分为两个专门分支：1）1比特主导分支用于高效计算；2）紧凑高精度分支专门保留最敏感参数。通过特征缩放引导模型将敏感参数分配到高精度分支，并将该分支扩展为多个稀疏激活专家以实现高效容量扩展

Result: 在极低比特量化任务中取得了最先进的性能表现

Conclusion: pQuant通过参数解耦和分支专业化设计有效解决了极低比特量化中的参数民主化问题，为大语言模型的边缘部署提供了高效的量化解决方案

Abstract: Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.

</details>


### [201] [S2O: Early Stopping for Sparse Attention via Online Permutation](https://arxiv.org/abs/2602.22575)
*Yu Zhang,Songwei Liu,Chenqian Yan,Sheng Lin,Beichen Ning,Fangmin Chen,Xing Wang*

Main category: cs.LG

TL;DR: S2O通过在线排列和提前停止机制，显著提高了稀疏注意力的实际稀疏度上限，在保持准确性的同时大幅降低计算量和提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的计算复杂度随序列长度呈二次方增长，限制了长上下文推理。现有的块粒度稀疏化方法存在固有的稀疏度上限，即使经过精心设计也难以进一步提升性能。

Method: S2O采用在线排列和提前停止机制：1) 借鉴内存系统的虚拟到物理地址映射，重新设计FlashAttention执行，支持加载非连续token；2) 基于注意力热图的细粒度结构，将显式排列转化为在线、索引引导的离散加载策略；3) 引入提前停止规则，按重要性从高到低计算，当块分数低于阈值时提前终止。

Result: 在Llama-3.1-8B的128K上下文下：单算子MSE降低3.82倍（相同稀疏度）；预填充计算密度降低3.31倍（相同MSE）；保持端到端准确性；注意力计算加速7.51倍；端到端加速3.81倍。

Conclusion: S2O通过在线排列和重要性引导的提前停止机制，显著提高了稀疏注意力的实际稀疏度上限，在控制误差预算的同时大幅减少了计算量，为长上下文推理提供了高效解决方案。

Abstract: Attention scales quadratically with sequence length, fundamentally limiting long-context inference. Existing block-granularity sparsification can reduce latency, but coarse blocks impose an intrinsic sparsity ceiling, making further improvements difficult even with carefully engineered designs. We present S2O, which performs early stopping for sparse attention via online permutation. Inspired by virtual-to-physical address mapping in memory systems, S2O revisits and factorizes FlashAttention execution, enabling inference to load non-contiguous tokens rather than a contiguous span in the original order. Motivated by fine-grained structures in attention heatmaps, we transform explicit permutation into an online, index-guided, discrete loading policy; with extremely lightweight preprocessing and index-remapping overhead, it concentrates importance on a small set of high-priority blocks. Building on this importance-guided online permutation for loading, S2O further introduces an early-stopping rule: computation proceeds from high to low importance; once the current block score falls below a threshold, S2O terminates early and skips the remaining low-contribution blocks, thereby increasing effective sparsity and reducing computation under a controlled error budget.
  As a result, S2O substantially raises the practical sparsity ceiling. On Llama-3.1-8B under a 128K context, S2O reduces single-operator MSE by 3.82$\times$ at matched sparsity, and reduces prefill compute density by 3.31$\times$ at matched MSE; meanwhile, it preserves end-to-end accuracy and achieves 7.51$\times$ attention and 3.81$\times$ end-to-end speedups.

</details>


### [202] [ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL](https://arxiv.org/abs/2602.22623)
*Xingyu Lu,Jinpeng Wang,YiFan Zhang,Shijie Ma,Xiao Hu,Tianke Zhang,Haonan fan,Kaiyu Jiang,Changyi Liu,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Chun Yuan*

Main category: cs.LG

TL;DR: ContextRL：通过上下文增强解决强化学习奖励模型中的可识别性和可达性问题，显著提升知识发现效率


<details>
  <summary>Details</summary>
Motivation: 现有强化学习奖励模型存在两个瓶颈：1）可识别性问题 - 难以区分正确答案但推理过程低质量的样本；2）可达性问题 - 难以从全负样本组中恢复正确响应

Method: 1）为奖励模型提供完整参考解决方案作为上下文，实现细粒度过程验证；2）引入多轮采样策略，奖励模型为失败尝试生成错误报告，指导策略从先前全负样本组中"恢复"正确响应

Result: 在11个感知和推理基准测试中显著提升知识发现效率，使Qwen3-VL-8B模型达到与32B模型相当的性能，大幅超越标准RLVR基线，有效缓解奖励黑客攻击

Conclusion: 上下文信息对提升奖励模型准确性具有显著潜力，奖励黑客攻击普遍存在，为未来RLVR研究提供宝贵见解

Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to "recover" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.

</details>


### [203] [IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck](https://arxiv.org/abs/2602.22581)
*Tian Bian,Yifan Niu,Chaohao Yuan,Chengzhi Piao,Bingzhe Wu,Long-Kai Huang,Yu Rong,Tingyang Xu,Hong Cheng,Jia Li*

Main category: cs.LG

TL;DR: IBCircuit：基于信息瓶颈原理的端到端方法，用于整体发现语言模型中的计算子图（电路），无需为不同任务设计特定的损坏激活


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法大多忽视电路的整体性，且需要为不同任务设计特定的损坏激活，这种方法既不准确也不高效

Method: 基于信息瓶颈原理的端到端优化框架IBCircuit，能够整体识别信息丰富的电路，适用于任何给定任务而无需繁琐的损坏激活设计

Result: 在间接宾语识别（IOI）和大于比较任务中，IBCircuit相比近期相关工作，识别出更忠实、更精简的电路（包括关键节点和边组件）

Conclusion: IBCircuit为电路发现提供了一种更准确、更高效的端到端方法，能够整体识别语言模型中的计算子图，无需任务特定的损坏激活设计

Abstract: Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to identify informative circuits holistically. IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.

</details>


### [204] [NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion](https://arxiv.org/abs/2602.22911)
*Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: NoRA通过引入非线性激活和结构dropout打破LoRA的线性瓶颈，在更低秩下实现更好性能


<details>
  <summary>Details</summary>
Motivation: LoRA在复杂推理任务中存在"线性天花板"问题，单纯增加秩会带来收益递减，需要突破线性约束

Method: 提出NoRA（非线性秩适应），在权重级并行适配器中注入SiLU门控和结构dropout，实现流形扩展

Result: 在SlimOrca基准上，NoRA秩64（PPL 3.89）优于LoRA秩512（PPL 3.90）；在数学推理上，NoRA PPL 1.97显著优于LoRA的2.07

Conclusion: NoRA通过激活奇异值谱的尾部，防止线性方法中的秩崩溃，在参数高效微调中实现突破性进展

Abstract: Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.

</details>


### [205] [Transformers converge to invariant algorithmic cores](https://arxiv.org/abs/2602.22600)
*Joshua S. Schiffman*

Main category: cs.LG

TL;DR: 论文提出"算法核心"概念，发现不同训练运行的Transformer会收敛到相同的紧凑子空间结构，这些核心结构是任务性能的必要充分条件


<details>
  <summary>Details</summary>
Motivation: 大语言模型表现出复杂能力，但理解其内部工作机制仍是核心挑战。主要障碍在于训练选择的是行为而非电路结构，许多权重配置可以实现相同功能。哪些内部结构反映计算本质，哪些只是特定训练运行的偶然结果？

Method: 提取"算法核心"：紧凑的子空间，这些子空间对任务性能是必要且充分的。通过分析独立训练的Transformer、马尔可夫链Transformer、模加Transformer和GPT-2语言模型，识别跨训练运行和规模的低维不变结构

Result: 1) 独立训练的Transformer学习不同权重但收敛到相同核心；2) 马尔可夫链Transformer在几乎正交的子空间中嵌入3D核心，但恢复相同的转移谱；3) 模加Transformer在"顿悟"时发现紧凑循环算子，随后膨胀；4) GPT-2通过单个轴控制主谓一致，翻转该轴可在生成过程中反转语法数

Conclusion: Transformer计算围绕紧凑、共享的算法结构组织，这些低维不变量在不同训练运行和规模中持续存在。机械可解释性应针对这些计算本质（算法核心），而非实现特定的细节

Abstract: Large language models exhibit sophisticated capabilities, yet understanding how they work internally remains a central challenge. A fundamental obstacle is that training selects for behavior, not circuitry, so many weight configurations can implement the same function. Which internal structures reflect the computation, and which are accidents of a particular training run? This work extracts algorithmic cores: compact subspaces necessary and sufficient for task performance. Independently trained transformers learn different weights but converge to the same cores. Markov-chain transformers embed 3D cores in nearly orthogonal subspaces yet recover identical transition spectra. Modular-addition transformers discover compact cyclic operators at grokking that later inflate, yielding a predictive model of the memorization-to-generalization transition. GPT-2 language models govern subject-verb agreement through a single axis that, when flipped, inverts grammatical number throughout generation across scales. These results reveal low-dimensional invariants that persist across training runs and scales, suggesting that transformer computations are organized around compact, shared algorithmic structures. Mechanistic interpretability could benefit from targeting such invariants -- the computational essence -- rather than implementation-specific details.

</details>


### [206] [$φ$-DPO: Fairness Direct Preference Optimization Approach to Continual Learning in Large Multimodal Models](https://arxiv.org/abs/2602.22601)
*Thanh-Dat Truong,Huu-Thien Tran,Jackson Cothren,Bhiksha Raj,Khoa Luu*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的公平性直接偏好优化框架（FaiDPO/φ-DPO），用于解决大型多模态模型在持续学习中的公平性问题，特别是在数据分布不平衡的情况下。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在持续学习中面临公平性挑战，特别是在数据分布不平衡的情况下，这会导致模型更新偏差和跨任务性能不佳。虽然现有研究在解决灾难性遗忘方面取得进展，但由数据不平衡引起的公平性问题仍未得到充分探索。

Method: 首先提出基于直接偏好优化的新持续学习范式，通过将学习与成对偏好信号对齐来缓解灾难性遗忘。然后识别传统DPO在数据不平衡中的局限性，并提出新的φ-DPO损失函数，明确解决分布偏差问题。同时为现有基准构建成对偏好标注以支持φ-DPO持续学习。

Result: 广泛的实验和消融研究表明，提出的φ-DPO在多个基准测试中实现了最先进的性能，优于先前的LMM持续学习方法。理论分析也证明该方法能同时解决遗忘和数据不平衡问题。

Conclusion: φ-DPO框架有效解决了大型多模态模型在持续学习中的公平性挑战，特别是在数据不平衡情况下，通过结合偏好学习和公平性考虑，实现了更好的跨任务性能和公平性。

Abstract: Fairness in Continual Learning for Large Multimodal Models (LMMs) is an emerging yet underexplored challenge, particularly in the presence of imbalanced data distributions that can lead to biased model updates and suboptimal performance across tasks. While recent continual learning studies have made progress in addressing catastrophic forgetting, the problem of fairness caused the imbalanced data remains largely underexplored. This paper presents a novel Fairness Direct Preference Optimization (FaiDPO or $φ$-DPO) framework for continual learning in LMMs. In particular, we first propose a new continual learning paradigm based on Direct Preference Optimization (DPO) to mitigate catastrophic forgetting by aligning learning with pairwise preference signals. Then, we identify the limitations of conventional DPO in imbalanced data and present a new $φ$-DPO loss that explicitly addresses distributional biases. We provide a comprehensive theoretical analysis demonstrating that our approach addresses both forgetting and data imbalance. Additionally, to enable $φ$-DPO-based continual learning, we construct pairwise preference annotations for existing benchmarks in the context of continual learning. Extensive experiments and ablation studies show the proposed $φ$-DPO achieves State-of-the-Art performance across multiple benchmarks, outperforming prior continual learning methods of LMMs.

</details>


### [207] [InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models](https://arxiv.org/abs/2602.23200)
*Sayed Mohammadreza Tayaranian Hosseini,Amir Ardakani,Warren J. Gross*

Main category: cs.LG

TL;DR: InnerQ是一种硬件感知的KV缓存量化方案，通过内维度分组量化、混合量化策略和特殊token高精度窗口，在保持精度的同时显著降低解码延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型解码时，KV缓存的大小随序列长度增长，成为内存占用的主要瓶颈。现有量化方法主要关注压缩KV缓存而保持信息，但缺乏对硬件效率的优化。

Method: 1. 内维度分组量化：在KV缓存的内维度上进行分组，与向量-矩阵乘法对齐，实现尺度因子在GPU计算单元间的复用
2. 混合量化：根据局部统计为每组选择对称或非对称量化
3. 高精度窗口：对最近token和注意力汇聚token保持高精度，减轻异常值泄漏
4. 每通道归一化：在预填充阶段计算键缓存的每通道归一化，并折叠到查询中避免运行时开销

Result: 1. 相比先前工作实现高达22%的加速
2. 相比半精度向量-矩阵乘法实现高达88%的加速
3. 在Llama模型上保持与未量化KV缓存相当的few-shot GSM8K性能
4. 超越先前的KV缓存量化方法

Conclusion: InnerQ通过硬件感知的KV缓存量化设计，在保持模型精度的同时显著降低了解码延迟，为高效长序列生成提供了有效的解决方案。

Abstract: Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without sacrificing accuracy. InnerQ applies group-wise quantization while grouping the cache matrices over their inner dimension. Unlike previous work that group over the outer dimension, InnerQ aligns dequantization with the vector-matrix multiplication and enables scale factor reuse across GPU compute units. This reduces memory accesses and accelerates dequantization, yielding up to $22\%$ speedup over previous work and up to $88\%$ over half-precision vector-matrix multiplication. To preserve fidelity under aggressive compression, InnerQ incorporates (i) hybrid quantization, selecting symmetric or asymmetric quantization per group based on local statistics; (ii) high-precision windows for both the most recent tokens and the attention sink tokens to mitigate outlier leakage; and (iii) per-channel normalization of the key cache, computed once during prefill and folded into the query to avoid runtime overhead. Our evaluation experiments on Llama models shows that InnerQ maintains a few-shot GSM8K performance comparable to non-quantized KV caches and surpasses prior KV cache quantization methods.

</details>


### [208] [DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion](https://arxiv.org/abs/2602.22610)
*Tao Huang,Jiayang Meng,Xu Yang,Chen Hou,Hong Chen*

Main category: cs.LG

TL;DR: 提出DP-aware AdaLN-Zero，一种针对条件扩散变换器的敏感性感知条件机制，通过有界重参数化限制条件表示幅度和AdaLN调制参数，在梯度裁剪和噪声注入前抑制极端梯度尾事件，提升差分隐私条件下的时间序列生成性能。


<details>
  <summary>Details</summary>
Motivation: 异构条件上下文（如观测历史、缺失模式或异常协变量）会导致重尾的每样本梯度。在差分隐私随机梯度下降（DP-SGD）下，这些罕见条件驱动的重尾梯度会不成比例地触发全局裁剪，导致异常值主导的更新、更大的裁剪偏差以及在固定隐私预算下降低的效用。

Method: 提出DP-aware AdaLN-Zero，一种即插即用的敏感性感知条件机制，通过有界重参数化联合约束条件表示幅度和AdaLN调制参数，在梯度裁剪和噪声注入前抑制极端梯度尾事件，而不修改DP-SGD机制本身。

Result: 在匹配隐私设置下，配备DP-aware AdaLN-Zero的DP-SGD在插值/填补和预测任务上表现更好。在真实世界电力数据集和两个公共ETT基准测试上观察到相对于原始DP-SGD的一致增益。梯度诊断将这些改进归因于条件特定的尾部重塑和减少的裁剪失真，同时保持非私有训练的表达能力。

Conclusion: 敏感性感知条件机制可以在不牺牲标准性能的情况下，显著改善私有条件扩散训练，为差分隐私下的条件扩散模型提供了有效的解决方案。

Abstract: Condition injection enables diffusion models to generate context-aware outputs, which is essential for many time-series tasks. However, heterogeneous conditional contexts (e.g., observed history, missingness patterns or outlier covariates) can induce heavy-tailed per-example gradients. Under Differentially Private Stochastic Gradient Descent (DP-SGD), these rare conditioning-driven heavy-tailed gradients disproportionately trigger global clipping, resulting in outlier-dominated updates, larger clipping bias, and degraded utility under a fixed privacy budget. In this paper, we propose DP-aware AdaLN-Zero, a drop-in sensitivity-aware conditioning mechanism for conditional diffusion transformers that limits conditioning-induced gain without modifying the DP-SGD mechanism. DP-aware AdaLN-Zero jointly constrains conditioning representation magnitude and AdaLN modulation parameters via bounded re-parameterization, suppressing extreme gradient tail events before gradient clipping and noise injection. Empirically, DP-SGD equipped with DP-aware AdaLN-Zero improves interpolation/imputation and forecasting under matched privacy settings. We observe consistent gains on a real-world power dataset and two public ETT benchmarks over vanilla DP-SGD. Moreover, gradient diagnostics attribute these improvements to conditioning-specific tail reshaping and reduced clipping distortion, while preserving expressiveness in non-private training. Overall, these results show that sensitivity-aware conditioning can substantially improve private conditional diffusion training without sacrificing standard performance.

</details>


### [209] [Mitigating Membership Inference in Intermediate Representations via Layer-wise MIA-risk-aware DP-SGD](https://arxiv.org/abs/2602.22611)
*Jiayang Meng,Tao Huang,Chen Hou,Guolong Zheng,Hong Chen*

Main category: cs.LG

TL;DR: 提出LM-DP-SGD方法，通过分层感知MIA风险来分配差分隐私保护，相比均匀噪声分配能更好地平衡隐私与效用


<details>
  <summary>Details</summary>
Motivation: 在嵌入即接口场景中，预训练模型的中间表示会泄露训练集成员信息，现有DP-SGD采用均匀噪声分配，忽略了不同层对成员推理攻击的异质性脆弱性

Method: 提出分层MIA风险感知的DP-SGD：1）在公共影子数据集上训练影子模型；2）提取各层中间表示并训练分层MIA攻击器；3）用攻击错误率估计各层MIA风险；4）根据风险重新加权各层梯度贡献，在固定噪声量下提供分层适当的保护

Result: 在相同隐私预算下，LM-DP-SGD能降低峰值中间表示级别的MIA风险，同时保持模型效用，实现更优的隐私-效用权衡

Conclusion: LM-DP-SGD通过分层感知MIA风险来分配差分隐私保护，相比均匀噪声分配能更有效地平衡隐私与模型性能，并提供了隐私和收敛的理论保证

Abstract: In Embedding-as-an-Interface (EaaI) settings, pre-trained models are queried for Intermediate Representations (IRs). The distributional properties of IRs can leak training-set membership signals, enabling Membership Inference Attacks (MIAs) whose strength varies across layers. Although Differentially Private Stochastic Gradient Descent (DP-SGD) mitigates such leakage, existing implementations employ per-example gradient clipping and a uniform, layer-agnostic noise multiplier, ignoring heterogeneous layer-wise MIA vulnerability. This paper introduces Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD), which adaptively allocates privacy protection across layers in proportion to their MIA risk. Specifically, LM-DP-SGD trains a shadow model on a public shadow dataset, extracts per-layer IRs from its train/test splits, and fits layer-specific MIA adversaries, using their attack error rates as MIA-risk estimates. Leveraging the cross-dataset transferability of MIAs, these estimates are then used to reweight each layer's contribution to the globally clipped gradient during private training, providing layer-appropriate protection under a fixed noise magnitude. We further establish theoretical guarantees on both privacy and convergence of LM-DP-SGD. Extensive experiments show that, under the same privacy budget, LM-DP-SGD reduces the peak IR-level MIA risk while preserving utility, yielding a superior privacy-utility trade-off.

</details>


### [210] [Semantic Tube Prediction: Beating LLM Data Efficiency with JEPA](https://arxiv.org/abs/2602.22617)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.LG

TL;DR: 本文提出Geodesic Hypothesis和Semantic Tube Prediction (STP)任务，通过几何先验约束隐藏状态轨迹，使LLM仅用1/16的训练数据就能达到基线准确率，直接违反Chinchilla式缩放定律的数据项。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的缩放定律是描述性的而非规定性的，它们描述了典型训练而非最优训练。很少有工作成功挑战这些定律所暗示的数据效率边界，这是本文的主要关注点。

Method: 提出Geodesic Hypothesis，假设token序列在平滑语义流形上追踪测地线，因此是局部线性的。基于此提出Semantic Tube Prediction (STP)任务，这是一种JEPA风格的规范化器，将隐藏状态轨迹限制在测地线的管状邻域内。STP将JEPA推广到语言领域，无需显式的多视图增强。

Result: STP约束提高了信噪比，并在推理过程中通过防止轨迹碰撞来保持多样性。在NL-RX-SYNTH数据集上，STP使LLM仅用1/16的训练数据就能匹配基线准确率，直接违反Chinchilla式缩放定律的数据项。

Conclusion: 有原则的几何先验可以超越暴力缩放，通过Geodesic Hypothesis和STP任务，LLM能够以远少于传统缩放定律预测的数据量达到相同性能。

Abstract: Large Language Models (LLMs) obey consistent scaling laws -- empirical power-law fits that predict how loss decreases with compute, data, and parameters. While predictive, these laws are descriptive rather than prescriptive: they characterize typical training, not optimal training. Surprisingly few works have successfully challenged the data-efficiency bounds implied by these laws -- which is our primary focus. To that end, we introduce the Geodesic Hypothesis, positing that token sequences trace geodesics on a smooth semantic manifold and are therefore locally linear. Building on this principle, we propose a novel Semantic Tube Prediction (STP) task, a JEPA-style regularizer that confines hidden-state trajectories to a tubular neighborhood of the geodesic. STP generalizes JEPA to language without requiring explicit multi-view augmentations. We show this constraint improves signal-to-noise ratio, and consequently preserves diversity by preventing trajectory collisions during inference. Empirically, STP allows LLMs to match baseline accuracy with 16$\times$ less training data on the NL-RX-SYNTH dataset, directly violating the data term of Chinchilla-style scaling laws and demonstrating that principled geometric priors can surpass brute-force scaling. Code is available at https://github.com/galilai-group/llm-jepa#stp.

</details>


### [211] [Tackling Privacy Heterogeneity in Differentially Private Federated Learning](https://arxiv.org/abs/2602.22633)
*Ruichen Xu,Ying-Jun Angela Zhang,Jianwei Huang*

Main category: cs.LG

TL;DR: 本文首次系统研究了差分隐私联邦学习中的隐私感知客户端选择问题，提出了一种考虑隐私异质性的客户端选择策略，相比现有基线在CIFAR-10上实现了高达10%的测试准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私联邦学习方法通常假设所有客户端共享统一的隐私预算，这与现实场景中隐私需求各异的实际情况不符。隐私异质性带来了显著挑战：传统的客户端选择策略通常依赖数据量，无法区分提供高质量更新的客户端和因严格隐私约束引入大量噪声的客户端。

Method: 首先建立了理论基础，通过推导收敛分析来量化隐私异质性对训练误差的影响。基于此分析，提出了一种隐私感知客户端选择策略，将其表述为凸优化问题，自适应调整选择概率以最小化训练误差。

Result: 在基准数据集上的大量实验表明，在异构隐私预算下，该方法在CIFAR-10上相比现有基线实现了高达10%的测试准确率提升。

Conclusion: 研究结果强调了将隐私异质性纳入客户端选择对于实用有效的联邦学习的重要性，为解决现实世界中隐私需求各异的联邦学习场景提供了系统性的解决方案。

Abstract: Differentially private federated learning (DP-FL) enables clients to collaboratively train machine learning models while preserving the privacy of their local data. However, most existing DP-FL approaches assume that all clients share a uniform privacy budget, an assumption that does not hold in real-world scenarios where privacy requirements vary widely. This privacy heterogeneity poses a significant challenge: conventional client selection strategies, which typically rely on data quantity, cannot distinguish between clients providing high-quality updates and those introducing substantial noise due to strict privacy constraints. To address this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We establish a theoretical foundation by deriving a convergence analysis that quantifies the impact of privacy heterogeneity on training error. Building on this analysis, we propose a privacy-aware client selection strategy, formulated as a convex optimization problem, that adaptively adjusts selection probabilities to minimize training error. Extensive experiments on benchmark datasets demonstrate that our approach achieves up to a 10% improvement in test accuracy on CIFAR-10 compared to existing baselines under heterogeneous privacy budgets. These results highlight the importance of incorporating privacy heterogeneity into client selection for practical and effective federated learning.

</details>


### [212] [Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning](https://arxiv.org/abs/2602.22642)
*Qin-Wen Luo,Sheng Ren,Xiang Chen,Rui Liu,Jun Fang,Naiqiang Tan,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: CEEH方法通过难度感知的熵正则化，在保持推理能力的同时压缩CoT响应长度，解决了现有压缩方法因过早熵崩溃而损害推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法（如自训练和带长度约束的强化学习）在追求简短响应时往往牺牲推理能力，主要原因是显式优化短轨迹会触发快速熵崩溃，过早缩小探索空间，阻碍发现有效的推理路径，特别是在需要大量推导的难题上。

Method: 提出CEEH（Compress responses for Easy questions and Explore Hard ones）方法：1）动态评估实例难度，应用选择性熵正则化——对难题保持多样化的搜索空间以确保鲁棒性，对简单问题允许激进压缩；2）引入基于历史最短正确响应的动态最优长度惩罚，有效抵消熵诱导的长度膨胀并稳定奖励信号。

Result: 在六个推理基准测试中，CEEH持续减少响应长度，同时保持与基础模型相当的准确性，并相对于仅优化长度的方法提高了Pass@k指标。

Conclusion: CEEH通过难度感知的熵正则化策略，成功解决了CoT压缩中推理能力与响应长度之间的权衡问题，实现了在保持推理能力的同时显著减少计算成本和推理延迟。

Abstract: Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.

</details>


### [213] [MUG: Meta-path-aware Universal Heterogeneous Graph Pre-Training](https://arxiv.org/abs/2602.22645)
*Lianze Shan,Jitao Zhao,Dongxiao He,Yongqi Huang,Zhiyong Feng,Weixiong Zhang*

Main category: cs.LG

TL;DR: 提出MUG方法，用于异构图的通用预训练，解决异构图中类型多样性和元路径差异的挑战


<details>
  <summary>Details</summary>
Motivation: 当前通用图预训练主要关注同构图，而更复杂的异构图（具有更大的结构和语义复杂性）尚未被充分探索。异构图的挑战包括：1）多样类型和数据集特定语义阻碍统一表示空间构建；2）元路径的数量和语义在不同数据集间差异大，使得编码和聚合模式难以跨数据集应用。

Method: 提出MUG方法：1）输入统一模块将异构图中的多节点和关系类型信息整合为统一表示，通过维度感知编码器投影到共享空间；2）训练共享编码器捕获跨多样元路径视图的一致结构模式，而非依赖数据集特定聚合策略；3）全局目标鼓励区分性并减少数据集特定偏差。

Result: 在多个真实数据集上的广泛实验证明了MUG的有效性

Conclusion: MUG成功解决了异构图通用预训练的挑战，为异构图的表示学习提供了有效的预训练框架

Abstract: Universal graph pre-training has emerged as a key paradigm in graph representation learning, offering a promising way to train encoders to learn transferable representations from unlabeled graphs and to effectively generalize across a wide range of downstream tasks. However, recent explorations in universal graph pre-training primarily focus on homogeneous graphs and it remains unexplored for heterogeneous graphs, which exhibit greater structural and semantic complexity. This heterogeneity makes it highly challenging to train a universal encoder for diverse heterogeneous graphs: (i) the diverse types with dataset-specific semantics hinder the construction of a unified representation space; (ii) the number and semantics of meta-paths vary across datasets, making encoding and aggregation patterns learned from one dataset difficult to apply to others. To address these challenges, we propose a novel Meta-path-aware Universal heterogeneous Graph pre-training (MUG) approach. Specifically, for challenge (i), MUG introduces a input unification module that integrates information from multiple node and relation types within each heterogeneous graph into a unified representation.This representation is then projected into a shared space by a dimension-aware encoder, enabling alignment across graphs with diverse schemas.Furthermore, for challenge (ii), MUG trains a shared encoder to capture consistent structural patterns across diverse meta-path views rather than relying on dataset-specific aggregation strategies, while a global objective encourages discriminability and reduces dataset-specific biases. Extensive experiments demonstrate the effectiveness of MUG on some real datasets.

</details>


### [214] [LEDA: Latent Semantic Distribution Alignment for Multi-domain Graph Pre-training](https://arxiv.org/abs/2602.22660)
*Lianze Shan,Jitao Zhao,Dongxiao He,Siqi Liu,Jiaxu Cui,Weixiong Zhang*

Main category: cs.LG

TL;DR: 提出LEDA模型用于通用图预训练，通过维度投影单元和变分语义推断模块解决现有方法在数据对齐和训练指导方面的局限性，在跨域场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有通用图预训练方法面临两个主要挑战：1）简单数据对齐无法处理高度多样化的图数据，导致语义不对齐；2）将域内预训练范式任意应用于跨域场景，难以从多个图中捕获有效知识。

Method: 提出LEDA模型：1）维度投影单元自适应地将不同域特征对齐到共享语义空间；2）变分语义推断模块获取共享潜在分布；3）用该分布指导域投影，确保跨域语义学习。

Result: LEDA在广泛的图数据和下游任务中表现优异，特别是在少样本跨域设置中，显著优于域内基线和先进的通用预训练模型。

Conclusion: LEDA通过有效的跨域语义对齐和共享潜在分布学习，解决了通用图预训练中的关键挑战，为跨域图表示学习提供了有效解决方案。

Abstract: Recent advances in generic large models, such as GPT and DeepSeek, have motivated the introduction of universality to graph pre-training, aiming to learn rich and generalizable knowledge across diverse domains using graph representations to improve performance in various downstream applications. However, most existing methods face challenges in learning effective knowledge from generic graphs, primarily due to simplistic data alignment and limited training guidance. The issue of simplistic data alignment arises from the use of a straightforward unification for highly diverse graph data, which fails to align semantics and misleads pre-training models. The problem with limited training guidance lies in the arbitrary application of in-domain pre-training paradigms to cross-domain scenarios. While it is effective in enhancing discriminative representation in one data space, it struggles to capture effective knowledge from many graphs. To address these challenges, we propose a novel Latent sEmantic Distribution Alignment (LEDA) model for universal graph pre-training. Specifically, we first introduce a dimension projection unit to adaptively align diverse domain features into a shared semantic space with minimal information loss. Furthermore, we design a variational semantic inference module to obtain the shared latent distribution. The distribution is then adopted to guide the domain projection, aligning it with shared semantics across domains and ensuring cross-domain semantic learning. LEDA exhibits strong performance across a broad range of graphs and downstream tasks. Remarkably, in few-shot cross-domain settings, it significantly outperforms in-domain baselines and advanced universal pre-training models.

</details>


### [215] [Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support](https://arxiv.org/abs/2602.22673)
*Md Tanvir Hasan Turja*

Main category: cs.LG

TL;DR: 开发了一个两组件框架，用于预测抗生素耐药性趋势并提供基于证据的政策决策支持，在WHO GLASS数据上XGBoost表现最佳，并实现了结合政策文档的RAG系统。


<details>
  <summary>Details</summary>
Motivation: 抗生素耐药性（AMR）是全球性危机，预计到205年每年导致1000万人死亡。虽然WHO GLASS系统提供了标准化监测数据，但很少有研究应用机器学习来预测人口层面的耐药性趋势。需要开发预测模型和政策决策支持工具来应对这一挑战。

Method: 提出了一个两组件框架：1）耐药性趋势预测：在5,909个WHO GLASS观测数据上（2021-2023年，覆盖六个WHO区域）对六种模型（Naive、线性回归、岭回归、XGBoost、LightGBM、LSTM）进行基准测试；2）政策决策支持：实现检索增强生成（RAG）管道，结合ChromaDB向量存储的WHO政策文档和本地部署的Phi-3 Mini语言模型。

Result: XGBoost表现最佳，测试MAE为7.07%，R平方为0.854，比朴素基线提高了83.1%。特征重要性分析显示前一年耐药率是最重要的预测因子（50.5%重要性）。区域MAE范围从4.16%（欧洲区域）到10.14%（东南亚区域）。RAG系统能够生成有来源引用、幻觉受限的政策答案。

Conclusion: 该研究成功开发了一个有效的AMR趋势预测和政策决策支持框架，XGBoost模型表现出色，RAG系统为政策制定提供了基于证据的支持。代码和数据已开源，有助于推动AMR监测和干预措施的制定。

Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja

</details>


### [216] [Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement](https://arxiv.org/abs/2602.22681)
*Shuchen Zhu,Rizhen Hu,Mingze Wang,Mou Sun,Xue Wang,Kun Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: LITE是一种广义加速策略，通过沿平坦方向应用更大的Hessian阻尼系数和学习率来增强训练动态，显著加速Muon和SOAP等优化器在LLM预训练中的收敛。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型预训练需要巨大的计算资源，优化器效率至关重要。当前基于矩阵的优化器（如Muon和SOAP）虽然利用细粒度曲率信息优于AdamW，但其更新趋向于各向同性——在平坦方向上相对保守，在尖锐方向上可能过于激进，限制了训练效率。

Method: 首先建立统一的黎曼常微分方程（ODE）框架，阐明自适应算法如何协同工作：预处理器诱导黎曼几何缓解病态条件，动量作为黎曼阻尼项促进收敛。基于这些见解，提出LITE策略，通过在平坦轨迹上应用更大的Hessian阻尼系数和学习率来增强训练动态。

Result: 在多样化架构（Dense、MoE）、参数规模（130M-1.3B）、数据集（C4、Pile）和学习率调度（cosine、warmup-stable-decay）上的实验表明，LITE显著加速了Muon和SOAP的训练。理论分析证实LITE在各向异性景观中沿平坦方向促进更快收敛。

Conclusion: LITE提供了一种原则性方法来提高LLM预训练效率，通过针对性地增强平坦方向上的训练动态，解决了现有优化器在各向异性景观中的局限性，为高效的大规模语言模型训练提供了新思路。

Abstract: Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.

</details>


### [217] [Switch-Hurdle: A MoE Encoder with AR Hurdle Decoder for Intermittent Demand Forecasting](https://arxiv.org/abs/2602.22685)
*Fabian Muşat,Simona Căbuz*

Main category: cs.LG

TL;DR: Switch-Hurdle是一个用于间歇性需求预测的新框架，结合了混合专家编码器和Hurdle概率解码器，通过分离销售发生概率和销售数量预测来提升性能。


<details>
  <summary>Details</summary>
Motivation: 间歇性需求（长时间零销售与零星非零值交替）在零售和供应链预测中一直是个挑战。传统方法（如ARIMA、指数平滑、Croston变体）和现代神经架构（如DeepAR、Transformer）在处理这类数据时表现不佳，因为它们将需求视为单一连续过程，或在扩展到许多稀疏序列时计算成本过高。

Method: Switch-Hurdle框架包含：1）混合专家编码器，在前向传播中使用稀疏Top-1专家路由，在反向传播中通过直通估计器实现近似密集；2）Hurdle概率解码器，采用交叉注意力自回归设计，共享的hurdle头将预测任务明确分为两部分：估计销售概率的二元分类组件和给定销售时预测数量的条件回归组件。

Result: 在M5基准测试和大型专有零售数据集上的实证结果表明，Switch-Hurdle实现了最先进的预测性能，同时保持了可扩展性。

Conclusion: Switch-Hurdle通过结构化分离销售发生和数量预测过程，能够捕捉间歇性需求的内在特性，为间歇性需求预测提供了有效且可扩展的解决方案。

Abstract: Intermittent demand, a pattern characterized by long sequences of zero sales punctuated by sporadic, non-zero values, poses a persistent challenge in retail and supply chain forecasting. Both traditional methods, such as ARIMA, exponential smoothing, or Croston variants, as well as modern neural architectures such as DeepAR and Transformer-based models often underperform on such data, as they treat demand as a single continuous process or become computationally expensive when scaled across many sparse series. To address these limitations, we introduce Switch-Hurdle: a new framework that integrates a Mixture-of-Experts (MoE) encoder with a Hurdle-based probabilistic decoder. The encoder uses a sparse Top-1 expert routing during the forward pass yet approximately dense in the backward pass via a straight-through estimator (STE). The decoder follows a cross-attention autoregressive design with a shared hurdle head that explicitly separates the forecasting task into two components: a binary classification component estimating the probability of a sale, and a conditional regression component, predicting the quantity given a sale. This structured separation enables the model to capture both occurrence and magnitude processes inherent to intermittent demand. Empirical results on the M5 benchmark and a large proprietary retail dataset show that Switch-Hurdle achieves state-of-the-art prediction performance while maintaining scalability.

</details>


### [218] [Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning](https://arxiv.org/abs/2602.22703)
*Hao Yu,Shuning Jia,Guanghao Li,Wenhao Jiang,Chun Yuan*

Main category: cs.LG

TL;DR: GeoPerceive是一个几何感知基准测试，包含图表实例和领域特定语言表示，配合GeoDPO框架通过强化学习提升视觉语言模型的几何感知能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在几何推理方面存在困难，主要原因是它们对基本图表元素的感知能力有限。需要专门的方法来提升模型的几何感知能力。

Method: 提出GeoPerceive基准测试和自动数据生成流程，以及GeoDPO框架：使用NL-to-DSL翻译器将自然语言转换为领域特定语言，基于DSL级别的分数作为强化学习的奖励信号。

Result: GeoDPO在多个数据集上表现优异：领域内数据提升26.5%，领域外数据提升8.0%，下游推理任务提升39.0%。相比监督微调有显著优势。

Conclusion: GeoDPO通过强化学习框架有效提升了视觉语言模型的几何感知能力，具有优异的性能和泛化能力，代码已开源确保可复现性。

Abstract: Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.

</details>


### [219] [Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks](https://arxiv.org/abs/2602.22719)
*Vamshi Sunku Mohan,Kaustubh Gupta,Aneesha Das,Chandan Singh*

Main category: cs.LG

TL;DR: 本文提出了一种识别Mamba SSM模型中激活子空间瓶颈的方法，并引入无需任务特定调优的测试时干预策略，平均提升8.27%性能，最终通过架构修改得到Stable-Mamba


<details>
  <summary>Details</summary>
Motivation: 尽管状态空间模型(SSMs)作为构建强大语言模型的高效策略出现，避免了Transformer中注意力计算的二次复杂度，但现代SSMs的可解释性和可操控性仍相对未被充分探索

Method: 使用机制可解释性工具识别Mamba系列SSM模型中的激活子空间瓶颈，引入测试时干预策略（简单地将识别出的瓶颈激活乘以标量），并修改瓶颈得到新架构Stable-Mamba

Result: 在5个SSM模型和6个多样化基准测试中，干预策略平均提升8.27%性能，无需任务特定调优；验证瓶颈确实阻碍性能，Stable-Mamba架构在从头训练时获得长上下文性能提升

Conclusion: 通过机制可解释性方法识别SSM瓶颈并实施简单干预可显著提升性能，修改瓶颈得到的Stable-Mamba架构进一步验证了该方法的价值，为SSM的可解释性和可操控性研究开辟了新方向

Abstract: State-space models (SSMs) have emerged as an efficient strategy for building powerful language models, avoiding the quadratic complexity of computing attention in transformers. Despite their promise, the interpretability and steerability of modern SSMs remain relatively underexplored. We take a major step in this direction by identifying activation subspace bottlenecks in the Mamba family of SSM models using tools from mechanistic interpretability. We then introduce a test-time steering intervention that simply multiplies the activations of the identified bottlenecks by a scalar. Across 5 SSMs and 6 diverse benchmarks, this intervention improves performance by an average of 8.27%, without requiring any task-specific tuning. Finally, we validate that the identified bottlenecks are indeed hindering performance by modifying them to yield an architecture we call Stable-Mamba, which achieves long-context performance gains when retrained from scratch.

</details>


### [220] [Set-based v.s. Distribution-based Representations of Epistemic Uncertainty: A Comparative Study](https://arxiv.org/abs/2602.22747)
*Kaizheng Wang,Yunjia Wang,Fabio Cuzzolin,David Moens,Hans Hallez,Siu Lun Chau*

Main category: cs.LG

TL;DR: 该研究对神经网络中两种二阶不确定性表示范式（基于分布的参数后验和基于集合的信度集）进行了首次受控比较，通过相同预测分布集合构建两种表示，在8个基准任务上评估了它们的相对性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络中的认知不确定性通常使用两种二阶范式建模：基于分布的参数后验表示和基于集合的信度集表示。由于语义、假设和评估实践的差异，这两种框架通常被认为是不可比较的，导致它们的相对优劣不明确。实证比较还受到底层预测模型变化的影响。

Method: 研究设计了一个受控比较实验，从相同的有限预测分布集合（由共享神经网络生成）构建两种表示，从而将表示效应与预测准确性分离。评估了3种不确定性度量在8个基准任务上的表现，包括选择性预测和分布外检测，涵盖了6种底层预测模型和每种配置10次独立运行。

Result: 研究结果表明，在这两种看似不可比较的框架之间进行有意义的比较是可行且有益的。研究提供了关于二阶表示选择如何影响实际不确定性感知性能的见解。

Conclusion: 该研究首次实现了对神经网络中两种主要二阶不确定性表示范式的受控比较，证明了在相同条件下比较这两种框架的可行性，并为理解表示选择对不确定性感知任务的影响提供了实证基础。

Abstract: Epistemic uncertainty in neural networks is commonly modeled using two second-order paradigms: distribution-based representations, which rely on posterior parameter distributions, and set-based representations based on credal sets (convex sets of probability distributions). These frameworks are often regarded as fundamentally non-comparable due to differing semantics, assumptions, and evaluation practices, leaving their relative merits unclear. Empirical comparisons are further confounded by variations in the underlying predictive models. To clarify this issue, we present a controlled comparative study enabling principled, like-for-like evaluation of the two paradigms. Both representations are constructed from the same finite collection of predictive distributions generated by a shared neural network, isolating representational effects from predictive accuracy. Our study evaluates each representation through the lens of 3 uncertainty measures across 8 benchmarks, including selective prediction and out-of-distribution detection, spanning 6 underlying predictive models and 10 independent runs per configuration. Our results show that meaningful comparison between these seemingly non-comparable frameworks is both feasible and informative, providing insights into how second-order representation choices impact practical uncertainty-aware performance.

</details>


### [221] [KMLP: A Scalable Hybrid Architecture for Web-Scale Tabular Data Modeling](https://arxiv.org/abs/2602.22777)
*Mingming Zhang,Pengfei Shi,Zhiqing Xiao,Feng Zhao,Guandong Sun,Yulin Kang,Ruizhe Gao,Ningtao Wang,Xing Fu,Weiqiang Wang,Junbo Zhao*

Main category: cs.LG

TL;DR: KMLP：一种结合KAN前端和gMLP骨干的混合深度学习架构，用于处理大规模网络表格数据，在数十亿样本规模上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 网络规模表格数据（数十亿实例、数百个异构数值特征）存在可扩展性挑战，特征具有各向异性、重尾分布和非平稳性，传统方法如梯度提升决策树存在瓶颈且需要大量手动特征工程

Method: 提出KMLP混合架构：浅层Kolmogorov-Arnold Network（KAN）前端使用可学习激活函数自动建模每个特征的复杂非线性变换；Gated Multilayer Perceptron（gMLP）骨干捕捉高阶交互

Result: 在公共基准测试和包含数十亿样本的工业数据集上，KMLP实现了最先进的性能，相对于GBDT等基线的优势随着规模增大而增加

Conclusion: KMLP验证了作为大规模网络表格数据的可扩展深度学习范式的有效性，特别适合处理大规模、异构特征的预测建模任务

Abstract: Predictive modeling on web-scale tabular data with billions of instances and hundreds of heterogeneous numerical features faces significant scalability challenges. These features exhibit anisotropy, heavy-tailed distributions, and non-stationarity, creating bottlenecks for models like Gradient Boosting Decision Trees and requiring laborious manual feature engineering. We introduce KMLP, a hybrid deep architecture integrating a shallow Kolmogorov-Arnold Network (KAN) front-end with a Gated Multilayer Perceptron (gMLP) backbone. The KAN front-end uses learnable activation functions to automatically model complex non-linear transformations for each feature, while the gMLP backbone captures high-order interactions. Experiments on public benchmarks and an industrial dataset with billions of samples show KMLP achieves state-of-the-art performance, with advantages over baselines like GBDTs increasing at larger scales, validating KMLP as a scalable deep learning paradigm for large-scale web tabular data.

</details>


### [222] [Doubly Adaptive Channel and Spatial Attention for Semantic Image Communication by IoT Devices](https://arxiv.org/abs/2602.22794)
*Soroosh Miri,Sepehr Abolhasani,Shahrokh Farahmand,S. Mohammad Razavizadeh*

Main category: cs.LG

TL;DR: 提出DA-DJSCC方法，通过双重自适应通道注意力和空间注意力模块，在单次训练中适应不同SNR条件，提升IoT语义通信性能


<details>
  <summary>Details</summary>
Motivation: IoT网络面临带宽有限、计算资源受限、无线信道动态变化等挑战。现有DJSCC方法需要为不同SNR分别训练DNN，导致存储和通信开销过大，不适合小型IoT设备

Method: 在ADJSCC基础上，提出DA-DJSCC方法，同时在发射端和接收端使用双重自适应模块：通道注意力模块适应信道条件变化，空间注意力模块适应空间特征重要性变化，实现动态特征提取和语义信息恢复

Result: 仿真结果表明，DA-DJSCC在多个性能指标上显著优于ADJSCC，同时仅带来轻微复杂度增加

Conclusion: DA-DJSCC是性能要求高但复杂度低的IoT网络中语义通信的理想选择，实现了单次训练适应多SNR条件，平衡了性能与复杂度

Abstract: Internet of Things (IoT) networks face significant challenges such as limited communication bandwidth, constrained computational and energy resources, and highly dynamic wireless channel conditions. Utilization of deep neural networks (DNNs) combined with semantic communication has emerged as a promising paradigm to address these limitations. Deep joint source-channel coding (DJSCC) has recently been proposed to enable semantic communication of images. Building upon the original DJSCC formulation, low-complexity attention-style architectures has been added to the DNNs for further performance enhancement. As a main hurdle, training these DNNs separately for various signal-to-noise ratios (SNRs) will amount to excessive storage or communication overhead, which can not be maintained by small IoT devices. SNR Adaptive DJSCC (ADJSCC), has been proposed to train the DNNs once but feed the current SNR as part of the data to the channel-wise attention mechanism. We improve upon ADJSCC by a simultaneous utilization of doubly adaptive channel-wise and spatial attention modules at both transmitter and receiver. These modules dynamically adjust to varying channel conditions and spatial feature importance, enabling robust and efficient feature extraction and semantic information recovery. Simulation results corroborate that our proposed doubly adaptive DJSCC (DA-DJSCC) significantly improves upon ADJSCC in several performance criteria, while incurring a mild increase in complexity. These facts render DA-DJSCC a desirable choice for semantic communication in performance demanding but low-complexity IoT networks.

</details>


### [223] [Multi-agent imitation learning with function approximation: Linear Markov games and beyond](https://arxiv.org/abs/2602.22810)
*Luca Viano,Till Freihaut,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: 首次对线性马尔可夫博弈中的多智能体模仿学习进行理论分析，提出基于特征层面的集中系数替代状态-动作层面系数，并设计了首个计算高效的交互式MAIL算法，在Tic-Tac-Toe和Connect4游戏中超越行为克隆。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体模仿学习（MAIL）缺乏理论分析，特别是在线性马尔可夫博弈中。现有方法依赖状态-动作层面的"所有策略偏差集中系数"，这在特征信息丰富时过于保守。需要开发更高效的算法，减少对集中系数的依赖。

Method: 1. 在线性马尔可夫博弈中，利用转移动态和奖励函数的线性特征结构，将集中系数从状态-动作层面降低到特征层面。2. 提出首个计算高效的交互式MAIL算法，其样本复杂度仅依赖于特征映射维度d。3. 基于理论发现，开发了深度MAIL交互算法。

Result: 1. 理论证明特征层面集中系数远小于状态-动作层面系数（当特征能有效表示状态相似性时）。2. 交互式MAIL算法的样本复杂度仅与特征维度d相关，无需集中系数。3. 深度MAIL算法在Tic-Tac-Toe和Connect4游戏中显著优于行为克隆（BC）。

Conclusion: 该工作首次为线性马尔可夫博弈中的多智能体模仿学习提供了理论分析框架，通过特征层面的集中系数改进和交互式算法设计，显著提升了学习效率和性能，为MAIL的理论和实践发展奠定了基础。

Abstract: In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level "all policy deviation concentrability coefficient" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.

</details>


### [224] [Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching](https://arxiv.org/abs/2602.22812)
*Hiroki Matsutani,Naoki Matsuda,Naoto Sugiura*

Main category: cs.LG

TL;DR: 提出分布式提示缓存机制，通过多台低端边缘设备共享中间处理状态来提升LLM推理性能，支持部分匹配，使用Bloom过滤器减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 资源受限的边缘设备上运行本地LLM推理存在严重性能瓶颈，需要提升推理性能。

Method: 提出分布式提示缓存机制，支持设备间共享中间处理状态；引入基于Bloom过滤器的目录数据结构，用于判断远程服务器是否有所需内部状态，减少不必要的通信开销。

Result: 在Raspberry Pi Zero 2W平台上使用Gemma-3 270M模型和MMLU数据集进行实验，平均减少TTFT 93.12%，TTLT 50.07%。

Conclusion: 分布式提示缓存能有效提升边缘设备上LLM推理性能，显著减少首令牌和末令牌的生成时间。

Abstract: Since local LLM inference on resource-constrained edge devices imposes a severe performance bottleneck, this paper proposes distributed prompt caching to enhance inference performance by cooperatively sharing intermediate processing states across multiple low-end edge devices. To fully utilize prompt similarity, our distributed caching mechanism also supports partial matching. As this approach introduces communication overhead associated with state sharing over a wireless network, we introduce a Bloom-filter-based data structure, referred to as a catalog, to determine whether a remote server possesses the desired internal states, thereby suppressing unnecessary communication. Experiments using the Gemma-3 270M model and the MMLU dataset on the Raspberry Pi Zero 2W platform demonstrate that the proposed approach reduces TTFT (Time to First Token) and TTLT (Time to Last Token) by 93.12% and 50.07% on average, respectively.

</details>


### [225] [Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2602.22817)
*Shuo He,Lang Feng,Qi Wei,Xin Cheng,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: HGPO提出分层组策略优化方法，解决长视野智能体任务中基于组的强化学习存在的上下文不一致问题，显著提升策略优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的强化学习方法（如GRPO）在处理长视野智能体任务时，采用逐步组策略优化方法，将轨迹中的每一步独立处理并使用记忆模块保留历史上下文。然而，这种方法存在上下文不一致问题：同一组内的步骤可能具有不同的历史上下文，导致优势估计严重偏差，从而显著降低策略优化效果。

Method: 提出分层组策略优化（HGPO）方法：1）在轨迹组内，根据历史上下文一致性将每个步骤分配到多个分层组中；2）对每个步骤，在每个组内计算不同的优势值；3）通过自适应加权方案聚合这些优势值。这种方法无需额外模型或轨迹采样，就能在逐步优势估计中实现有利的偏差-方差权衡。

Result: 在ALFWorld和WebShop两个具有挑战性的智能体任务上，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型进行评估，HGPO在相同计算约束下显著优于现有的智能体强化学习方法。

Conclusion: HGPO通过分层组分配和自适应优势聚合，有效解决了基于组的强化学习中上下文不一致问题，为长视野智能体任务提供了更精细、更有效的策略优化方法。

Abstract: Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.

</details>


### [226] [Hypernetwork-based approach for grid-independent functional data clustering](https://arxiv.org/abs/2602.22823)
*Anirudh Thatipelli,Ali Siahkoohi*

Main category: cs.LG

TL;DR: 提出一个基于超网络和隐式神经表示的函数数据聚类框架，将任意分辨率和网格的离散函数映射到固定维度的权重空间，实现与采样网格无关的聚类


<details>
  <summary>Details</summary>
Motivation: 现有函数数据聚类方法大多在采样网格上操作，导致聚类结果依赖于分辨率、采样密度或预处理选择，而不是函数本身的结构。需要一种与离散化无关的聚类方法。

Method: 使用超网络编码器将坐标-值对映射到隐式神经表示(INR)的权重空间，INR作为解码器。由于INR用很少参数表示函数，产生与采样网格解耦的紧凑表示。然后在权重空间使用标准聚类算法。

Result: 在合成和真实世界的高维设置中展示了具有竞争力的聚类性能，对采样分辨率变化具有鲁棒性，包括泛化到训练中未见过的分辨率。

Conclusion: 提出了一种新颖的函数数据聚类框架，通过超网络和INR实现与离散化无关的函数表示，使聚类结果真正依赖于函数结构而非采样方式。

Abstract: Functional data clustering is concerned with grouping functions that share similar structure, yet most existing methods implicitly operate on sampled grids, causing cluster assignments to depend on resolution, sampling density, or preprocessing choices rather than on the underlying functions themselves. To address this limitation, we introduce a framework that maps discretized function observations -- at arbitrary resolution and on arbitrary grids -- into a fixed-dimensional vector space via an auto-encoding architecture. The encoder is a hypernetwork that maps coordinate-value pairs to the weight space of an implicit neural representation (INR), which serves as the decoder. Because INRs represent functions with very few parameters, this design yields compact representations that are decoupled from the sampling grid, while the hypernetwork amortizes weight prediction across the dataset. Clustering is then performed in this weight space using standard algorithms, making the approach agnostic to both the discretization and the choice of clustering method. By means of synthetic and real-world experiments in high-dimensional settings, we demonstrate competitive clustering performance that is robust to changes in sampling resolution -- including generalization to resolutions not seen during training.

</details>


### [227] [MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction](https://arxiv.org/abs/2602.22850)
*Yi He,Yina Cao,Jixiu Zhai,Di Wang,Junxiao Kong,Tianchi Lu*

Main category: cs.LG

TL;DR: 开发了高性能DNA甲基化预测模型MEDNA-DFM，结合机制启发的信号净化算法，不仅能准确预测甲基化，还能提取可靠的保守基序，并提出了"序列-结构协同"假说。


<details>
  <summary>Details</summary>
Motivation: 深度学习在DNA甲基化识别中表现出色，但其"黑盒"特性阻碍了生物学洞见的获取。需要开发既能准确预测又能提供生物学解释的方法。

Method: 提出了MEDNA-DFM高性能模型和机制启发的信号净化算法。通过外部独立数据集验证模型泛化能力，应用算法提取基序，并通过果蝇6mA案例研究提出"序列-结构协同"假说，使用计算机诱变验证。

Result: MEDNA-DFM能有效捕捉保守的甲基化模式，在不同物种间实现稳健区分。模型泛化由保守的内在基序驱动而非系统发育接近性。提取的基序可靠性显著高于先前研究。果蝇案例验证了GAGG核心基序和上游A-tract元件的协同作用。

Conclusion: 这项工作为甲基化预测提供了强大工具，展示了可解释深度学习如何推动方法创新和生物学假说生成，实现了预测准确性和生物学洞察的双重目标。

Abstract: Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its "black-box" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse species. Validation on external independent datasets confirms that the model's generalization is driven by conserved intrinsic motifs (e.g., GC content) rather than phylogenetic proximity. Furthermore, applying our developed algorithms extracted motifs with significantly higher reliability than prior studies. Finally, empirical evidence from a Drosophila 6mA case study prompted us to propose a "sequence-structure synergy" hypothesis, suggesting that the GAGG core motif and an upstream A-tract element function cooperatively. We further validated this hypothesis via in silico mutagenesis, confirming that the ablation of either or both elements significantly degrades the model's recognition capabilities. This work provides a powerful tool for methylation prediction and demonstrates how explainable deep learning can drive both methodological innovation and the generation of biological hypotheses.

</details>


### [228] [Fair feature attribution for multi-output prediction: a Shapley-based perspective](https://arxiv.org/abs/2602.22882)
*Umberto Biccari,Alain Ibáñez de Opakua,José María Mato,Óscar Millet,Roberto Morales,Enrique Zuazua*

Main category: cs.LG

TL;DR: 该论文证明了在多输出预测器的Shapley框架中，任何满足经典Shapley公理的归因规则必须按输出分量分解，揭示了多输出解释的结构约束。


<details>
  <summary>Details</summary>
Motivation: 虽然SHAP解释通常独立计算每个输出坐标，但这种做法的理论必要性一直不明确。论文旨在形式化多输出预测器中特征归因的结构约束，澄清Shapley框架下的解释范围。

Method: 将经典Shapley公理（效率性、对称性、虚拟玩家性、可加性）扩展到向量值合作博弈，建立刚性定理证明任何满足这些公理的归因规则必须按输出分量分解。

Result: 证明了任何联合输出归因规则必须至少放松一个经典Shapley公理，识别了Shapley可解释性中先前未形式化的结构约束。在生物医学基准上的数值实验显示多输出模型能节省计算成本，同时产生与Shapley公理强加的分量结构完全一致的SHAP解释。

Conclusion: 该研究形式化了多输出学习中Shapley解释的基本结构约束，澄清了公平一致解释的精确范围，为多输出模型的可解释性提供了理论基础。

Abstract: In this article, we provide an axiomatic characterization of feature attribution for multi-output predictors within the Shapley framework. While SHAP explanations are routinely computed independently for each output coordinate, the theoretical necessity of this practice has remained unclear. By extending the classical Shapley axioms to vector-valued cooperative games, we establish a rigidity theorem showing that any attribution rule satisfying efficiency, symmetry, dummy player, and additivity must necessarily decompose component-wise across outputs. Consequently, any joint-output attribution rule must relax at least one of the classical Shapley axioms. This result identifies a previously unformalized structural constraint in Shapley-based interpretability, clarifying the precise scope of fairness-consistent explanations in multi-output learning. Numerical experiments on a biomedical benchmark illustrate that multi-output models can yield computational savings in training and deployment, while producing SHAP explanations that remain fully consistent with the component-wise structure imposed by the Shapley axioms.

</details>


### [229] [A Data-Driven Approach to Support Clinical Renal Replacement Therapy](https://arxiv.org/abs/2602.22902)
*Alice Balboni,Luis Escobar,Andrea Manno,Fabrizio Rossi,Maria Cristina Ruffa,Gianluca Villa,Giordano D'Aloisio,Antonio Consolo*

Main category: cs.LG

TL;DR: 本研究采用机器学习方法预测危重患者连续肾脏替代治疗中的膜污染，使用16个临床特征训练模型，通过ADASYN处理数据不平衡，随机森林等模型表现良好，表格数据方法优于LSTM，特征选择简化模型，SHAP反事实分析可指导临床干预。


<details>
  <summary>Details</summary>
Motivation: 连续肾脏替代治疗(CRRT)中膜污染是常见问题，影响治疗效果和患者管理。传统方法难以准确预测膜污染，需要开发可解释的机器学习模型来预测膜污染风险，为临床干预提供指导。

Method: 使用ICU时间序列数据，选择16个临床特征，采用表格数据方法而非直接建模时间依赖关系。使用ADASYN过采样技术处理数据不平衡问题。测试随机森林、XGBoost和LightGBM模型，通过特征选择简化模型，使用SHAP值进行反事实分析。

Result: 在10%再平衡率下，模型达到77.6%敏感性和96.3%特异性，性能在不同预测时间范围内保持稳健。表格数据方法优于LSTM模型，特征选择将模型简化为5个关键变量，精度损失最小。SHAP反事实分析成功识别可逆转膜污染预测的最小输入变化。

Conclusion: 研究证明可解释的机器学习模型能有效预测CRRT膜污染，结合预测和反事实分析具有临床实用价值，可指导治疗调整以降低膜污染风险，改善患者管理。

Abstract: This study investigates a data-driven machine learning approach to predict membrane fouling in critically ill patients undergoing Continuous Renal Replacement Therapy (CRRT). Using time-series data from an ICU, 16 clinically selected features were identified to train predictive models. To ensure interpretability and enable reliable counterfactual analysis, the researchers adopted a tabular data approach rather than modeling temporal dependencies directly. Given the imbalance between fouling and non-fouling cases, the ADASYN oversampling technique was applied to improve minority class representation. Random Forest, XGBoost, and LightGBM models were tested, achieving balanced performance with 77.6% sensitivity and 96.3% specificity at a 10% rebalancing rate. Results remained robust across different forecasting horizons. Notably, the tabular approach outperformed LSTM recurrent neural networks, suggesting that explicit temporal modeling was not necessary for strong predictive performance. Feature selection further reduced the model to five key variables, improving simplicity and interpretability with minimal loss of accuracy. A Shapley value-based counterfactual analysis was applied to the best-performing model, successfully identifying minimal input changes capable of reversing fouling predictions. Overall, the findings support the viability of interpretable machine learning models for predicting membrane fouling during CRRT. The integration of prediction and counterfactual analysis offers practical clinical value, potentially guiding therapeutic adjustments to reduce fouling risk and improve patient management.

</details>


### [230] [Generalization Bounds of Stochastic Gradient Descent in Homogeneous Neural Networks](https://arxiv.org/abs/2602.22936)
*Wenquan Ma,Yang Sui,Jiaye Teng,Bohan Wang,Jing Xu,Jingqin Yang*

Main category: cs.LG

TL;DR: 该论文证明了在齐次神经网络下，可以放宽算法稳定性分析中的步长衰减要求，从传统的O(1/t)放宽到Ω(1/√t)，这更符合实际训练场景。


<details>
  <summary>Details</summary>
Motivation: 传统算法稳定性分析要求步长按O(1/t)衰减，这在非凸训练中过于严格，可能阻碍优化且不符合实际训练场景。需要为更实用的步长衰减策略提供理论支持。

Method: 在齐次神经网络框架下进行理论分析，证明该框架允许更慢的步长衰减Ω(1/√t)。进一步扩展到非Lipschitz等更一般情况。

Result: 证明了齐次神经网络（包括全连接和卷积网络，使用ReLU和LeakyReLU激活）在温和假设下支持Ω(1/√t)的步长衰减，提供了更宽松的泛化界。

Conclusion: 齐次神经网络框架为算法稳定性分析提供了更灵活的步长衰减条件，使理论分析更贴近实际训练实践，具有广泛适用性。

Abstract: Algorithmic stability is among the most potent techniques in generalization analysis. However, its derivation usually requires a stepsize $η_t = \mathcal{O}(1/t)$ under non-convex training regimes, where $t$ denotes iterations. This rigid decay of the stepsize potentially impedes optimization and may not align with practical scenarios. In this paper, we derive the generalization bounds under the homogeneous neural network regimes, proving that this regime enables slower stepsize decay of order $Ω(1/\sqrt{t})$ under mild assumptions. We further extend the theoretical results from several aspects, e.g., non-Lipschitz regimes. This finding is broadly applicable, as homogeneous neural networks encompass fully-connected and convolutional neural networks with ReLU and LeakyReLU activations.

</details>


### [231] [MSINO: Curvature-Aware Sobolev Optimization for Manifold Neural Networks](https://arxiv.org/abs/2602.22937)
*Suresan Pareth*

Main category: cs.LG

TL;DR: MSINO是一个在黎曼流形上训练神经网络的曲率感知框架，使用协变Sobolev损失和平行传输对齐梯度，通过Laplace-Beltrami平滑正则化提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有欧几里得空间中的Sobolev训练方法无法有效处理黎曼流形上的神经网络训练，缺乏对曲率和几何结构的显式考虑，导致在流形优化中缺乏理论保证。

Method: 用协变Sobolev损失替代标准欧几里得导数监督，通过平行传输对齐梯度，加入Laplace-Beltrami平滑正则化项。基于黎曼优化和流形上Sobolev理论的经典结果，推导几何依赖常数。

Result: 获得三个理论结果：(i)具有流形Sobolev平滑常数的下降引理；(ii)Sobolev Polyak-Lojasiewicz不等式，在显式步长界限下为黎曼梯度下降和随机梯度下降提供线性收敛保证；(iii)两步牛顿Sobolev方法，在曲率控制邻域内具有局部二次收缩性。

Conclusion: MSINO为流形上的神经网络训练提供了统一的值和梯度学习方法，具有曲率感知的收敛保证，适用于表面成像、物理信息学习和机器人学等应用领域。

Abstract: We introduce Manifold Sobolev Informed Neural Optimization (MSINO), a curvature aware training framework for neural networks defined on Riemannian manifolds. The method replaces standard Euclidean derivative supervision with a covariant Sobolev loss that aligns gradients using parallel transport and improves stability via a Laplace Beltrami smoothness regularization term.
  Building on classical results in Riemannian optimization and Sobolev theory on manifolds, we derive geometry dependent constants that yield (i) a Descent Lemma with a manifold Sobolev smoothness constant, (ii) a Sobolev Polyak Lojasiewicz inequality giving linear convergence guarantees for Riemannian gradient descent and stochastic gradient descent under explicit step size bounds, and (iii) a two step Newton Sobolev method with local quadratic contraction in curvature controlled neighborhoods.
  Unlike prior Sobolev training in Euclidean space, MSINO provides training time guarantees that explicitly track curvature and transported Jacobians. Applications include surface imaging, physics informed learning settings, and robotics on Lie groups such as SO(3) and SE(3). The framework unifies value and gradient based learning with curvature aware convergence guarantees for neural training on manifolds.

</details>


### [232] [Scaling Laws of Global Weather Models](https://arxiv.org/abs/2602.22962)
*Yuejiang Yu,Langwen Huang,Alexandru Calotoiu,Torsten Hoefler*

Main category: cs.LG

TL;DR: 本文通过实证分析发现，天气预测模型在缩放规律上与语言模型存在根本差异：更宽的架构比更深的架构表现更好，增加训练数据量比增加模型规模带来更大性能提升。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型正在革新天气预报领域。为了优化训练效率和模型性能，本文旨在分析该领域的实证缩放规律，探索模型规模、数据集大小和计算预算三个关键因素与模型性能之间的关系。

Method: 通过分析一系列天气预测模型（包括Aurora和GraphCast等），研究模型性能（验证损失）与三个关键因素的关系：模型规模(N)、数据集大小(D)和计算预算(C)。进行计算最优分析，并在固定计算预算下比较不同资源分配策略的效果。

Result: Aurora表现出最强的数据缩放行为：训练数据集增加10倍可使验证损失降低高达3.2倍。GraphCast具有最高的参数效率，但硬件利用率有限。在固定计算预算下，将资源分配给更长的训练时间比增加模型规模带来更大的性能提升。天气预测模型始终偏好更宽的架构而非更深的架构，这与语言模型的缩放规律存在根本差异。

Conclusion: 未来的天气预测模型应优先考虑更宽的架构和更大的有效训练数据集，以最大化预测性能。这些发现为天气预测模型的优化设计提供了重要指导。

Abstract: Data-driven models are revolutionizing weather forecasting. To optimize training efficiency and model performance, this paper analyzes empirical scaling laws within this domain. We investigate the relationship between model performance (validation loss) and three key factors: model size ($N$), dataset size ($D$), and compute budget ($C$). Across a range of models, we find that Aurora exhibits the strongest data-scaling behavior: increasing the training dataset by 10x reduces validation loss by up to 3.2x. GraphCast demonstrates the highest parameter efficiency, yet suffers from limited hardware utilization. Our compute-optimal analysis indicates that, under fixed compute budgets, allocating resources to longer training durations yields greater performance gains than increasing model size. Furthermore, we analyze model shape and uncover scaling behaviors that differ fundamentally from those observed in language models: weather forecasting models consistently favor increased width over depth. These findings suggest that future weather models should prioritize wider architectures and larger effective training datasets to maximize predictive performance.

</details>


### [233] [Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability](https://arxiv.org/abs/2602.22988)
*Bum Jun Kim,Shohei Taniguchi,Makoto Kawano,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 提出RKSP方法，通过单次前向传播预测Transformer训练发散概率，并开发KSS技术防止发散


<details>
  <summary>Details</summary>
Motivation: Transformer训练发散会浪费大量计算资源，但实践中只能在昂贵训练开始后才能发现不稳定性，需要一种能在训练开始前预测发散概率的方法

Method: 提出残差Koopman谱分析(RKSP)：在初始化时通过单次前向传播，应用白化动态模态分解提取层间残差快照的Koopman谱特征。核心诊断指标是近单位谱质量，量化接近单位圆的模态比例，捕捉不稳定性风险

Result: RKSP在广泛配置中预测发散的AUROC达到0.995，优于最佳梯度基线。KSS技术在高学习率无归一化层情况下，将发散率从66.7%降至12.5%，并使学习率提高50-150%。方法在多种模型上验证有效

Conclusion: RKSP能在训练开始前准确预测Transformer发散风险，KSS能有效防止发散，该方法具有广泛适用性，可应用于多种架构和任务

Abstract: Training divergence in transformers wastes compute, yet practitioners discover instability only after expensive runs begin. They therefore need an expected probability of failure for a transformer before training starts. Our study of Residual Koopman Spectral Profiling (RKSP) provides such an estimate. From a single forward pass at initialization, RKSP extracts Koopman spectral features by applying whitened dynamic mode decomposition to layer-wise residual snapshots. Our central diagnostic, the near-unit spectral mass, quantifies the fraction of modes concentrated near the unit circle, which captures instability risk. For predicting divergence across extensive configurations, this estimator achieves an AUROC of 0.995, outperforming the best gradient baseline. We further make this diagnostic actionable through Koopman Spectral Shaping (KSS), which reshapes spectra during training. We empirically validate that our method works in practice: RKSP predicts divergence at initialization, and when RKSP flags high risk, turning on KSS successfully prevents divergence. In the challenging high learning rate regime without normalization layers, KSS reduces the divergence rate from 66.7% to 12.5% and enables learning rates that are 50% to 150% higher. These findings generalize to WikiText-103 language modeling, vision transformers on CIFAR-10, and pretrained language models, including GPT-2 and LLaMA-2 up to 7B, as well as emerging architectures such as MoE, Mamba-style SSMs, and KAN.

</details>


### [234] [Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization](https://arxiv.org/abs/2602.23008)
*Zeyuan Liu,Jeonghye Kim,Xufang Luo,Dongsheng Li,Yuqing Yang*

Main category: cs.LG

TL;DR: EMPO²是一种混合强化学习框架，通过记忆增强探索，结合on/off-policy更新，显著提升LLM智能体在需要发现新状态环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练知识，但在需要发现新状态的环境中表现不佳，探索成为LLM智能体强化学习的关键瓶颈。

Method: 提出EMPO²框架：1) 利用记忆进行探索；2) 结合on-policy和off-policy更新；3) 使LLM在有记忆时表现良好，无记忆时保持鲁棒性。

Result: 在ScienceWorld和WebShop上分别比GRPO提升128.6%和11.3%；在分布外测试中，仅需少量记忆试验即可适应新任务，无需参数更新。

Conclusion: EMPO²是构建更具探索性和泛化能力的LLM智能体的有前景框架。

Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.

</details>


### [235] [Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements](https://arxiv.org/abs/2602.23035)
*Viraj Patel,Marko Grujic,Philipp Aigner,Theodor Abart,Marcus Granegger,Deblina Bhattacharjee,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出一个物理信息化的潜在关系框架，将心脏涡流建模为图中的交互节点，通过图熵量化疾病严重程度和干预水平。


<details>
  <summary>Details</summary>
Motivation: 当前成像和计算方法无法捕捉心脏血流模式中相干流动特征的底层关系结构，而这些模式包含丰富的疾病严重程度和临床干预信息。

Method: 结合神经关系推断架构与物理启发的交互能量和生死动力学，将心脏涡流建模为图中的交互节点，构建对疾病严重程度和干预水平敏感的潜在图。

Result: 在主动脉缩窄模拟中，随着主动脉半径变窄，涡流相互作用变强且更频繁，图熵单调相关于缩窄严重程度（R²=0.78，Spearman |ρ|=0.96）。在左心室辅助设备支持的超声数据中，该方法同样捕捉到相干涡结构的减弱，展示了跨模态泛化能力。

Conclusion: 潜在交互图和熵可作为心脏疾病和干预的稳健且可解释的生物标志物，该框架能够跨模态捕捉血流模式中的关系结构。

Abstract: Cardiac blood flow patterns contain rich information about disease severity and clinical interventions, yet current imaging and computational methods fail to capture underlying relational structures of coherent flow features. We propose a physics-informed, latent relational framework to model cardiac vortices as interacting nodes in a graph. Our model combines a neural relational inference architecture with physics-inspired interaction energy and birth-death dynamics, yielding a latent graph sensitive to disease severity and intervention level. We first apply this to computational fluid dynamics simulations of aortic coarctation. Learned latent graphs reveal that as the aortic radius narrows, vortex interactions become stronger and more frequent. This leads to a higher graph entropy, correlating monotonically with coarctation severity ($R^2=0.78$, Spearman $|ρ|=0.96$). We then extend this method to ultrasound datasets of left ventricles under varying levels of left ventricular assist device support. Again the latent graph representation captures the weakening of coherent vortical structures, thereby demonstrating cross-modal generalisation. Results show latent interaction graphs and entropy serve as robust and interpretable markers of cardiac disease and intervention.

</details>


### [236] [Latent Matters: Learning Deep State-Space Models](https://arxiv.org/abs/2602.23050)
*Alexej Klushyn,Richard Kurle,Maximilian Soelch,Botond Cseke,Patrick van der Smagt*

Main category: cs.LG

TL;DR: 提出约束优化框架训练深度状态空间模型，并引入扩展卡尔曼VAE，在系统识别和预测精度上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有深度状态空间模型通过最大化证据下界训练，但这并不能确保模型真正学习到底层动态系统。需要更有效的训练方法来确保模型准确学习系统动态。

Method: 提出约束优化框架作为训练深度状态空间模型的通用方法。在此基础上引入扩展卡尔曼VAE，将摊销变分推断与经典贝叶斯滤波/平滑相结合，比基于RNN的DSSMs更准确地建模动态。

Result: 约束优化框架显著改进了现有最先进DSSMs的系统识别和预测精度。EKVAE在预测精度上优于先前模型，在识别动态系统方面取得显著成果，并能成功学习静态和动态特征解耦的状态空间表示。

Conclusion: 约束优化框架是训练深度状态空间模型的有效方法，EKVAE结合经典滤波方法和现代深度学习，在动态系统建模方面表现出优越性能，能够实现特征解耦表示。

Abstract: Deep state-space models (DSSMs) enable temporal predictions by learning the underlying dynamics of observed sequence data. They are often trained by maximising the evidence lower bound. However, as we show, this does not ensure the model actually learns the underlying dynamics. We therefore propose a constrained optimisation framework as a general approach for training DSSMs. Building upon this, we introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. Our results show that the constrained optimisation framework significantly improves system identification and prediction accuracy on the example of established state-of-the-art DSSMs. The EKVAE outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.

</details>


### [237] [RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection](https://arxiv.org/abs/2602.23060)
*Xin Wang,Burcu Ozek,Aruna Mohan,Amirhossein Ravari,Or Zilbershot,Fatemeh Afghah*

Main category: cs.LG

TL;DR: RhythmBERT：将心电图视为结构化语言，通过符号化P、QRS、T段并联合连续嵌入，在单导联上实现与12导联基线相当或更优的性能


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法将ECG视为通用时间序列，忽略了生理语义和节律结构。对比学习方法使用扭曲形态的数据增强，而生成方法采用固定窗口分割导致心搏周期错位

Method: 提出RhythmBERT生成式ECG语言模型，通过自编码器将P、QRS、T段编码为符号token捕获节律语义，同时使用连续嵌入保留细粒度形态，在约80万未标记ECG记录上使用掩码预测目标进行预训练

Result: 尽管仅使用单导联，RhythmBERT在房颤、细微ST-T异常和心肌梗死等多种心脏状况上达到与强12导联基线相当或更优的性能

Conclusion: 将ECG视为结构化语言为心脏分析提供了可扩展且生理对齐的途径，能够有效学习上下文表示并实现标签高效学习

Abstract: Electrocardiogram (ECG) analysis is crucial for diagnosing heart disease, but most self-supervised learning methods treat ECG as a generic time series, overlooking physiologic semantics and rhythm-level structure. Existing contrastive methods utilize augmentations that distort morphology, whereas generative approaches employ fixed-window segmentation, which misaligns cardiac cycles. To address these limitations, we propose RhythmBERT, a generative ECG language model that considers ECG as a language paradigm by encoding P, QRS, and T segments into symbolic tokens via autoencoder-based latent representations. These discrete tokens capture rhythm semantics, while complementary continuous embeddings retain fine-grained morphology, enabling a unified view of waveform structure and rhythm. RhythmBERT is pretrained on approximately 800,000 unlabeled ECG recordings with a masked prediction objective, allowing it to learn contextual representations in a label-efficient manner. Evaluations show that despite using only a single lead, RhythmBERT achieves comparable or superior performance to strong 12-lead baselines. This generalization extends from prevalent conditions such as atrial fibrillation to clinically challenging cases such as subtle ST-T abnormalities and myocardial infarction. Our results suggest that considering ECG as structured language offers a scalable and physiologically aligned pathway for advancing cardiac analysis.

</details>


### [238] [Physics-informed neural particle flow for the Bayesian update step](https://arxiv.org/abs/2602.23089)
*Domonkos Csuzdi,Tamás Bécsi,Olivér Törő*

Main category: cs.LG

TL;DR: 提出物理信息神经粒子流，通过将log-homotopy轨迹与连续性方程耦合推导主PDE，嵌入神经网络损失函数中实现无监督训练，解决高维非线性贝叶斯估计的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 高维非线性估计中贝叶斯更新步骤存在显著计算挑战。现有log-homotopy粒子流滤波器产生刚性微分方程，而深度学习近似通常将更新视为黑盒任务或依赖渐近松弛，忽略了有限时域概率传输的精确几何结构。

Method: 提出物理信息神经粒子流，将先验到后验密度函数的log-homotopy轨迹与描述密度演化的连续性方程耦合，推导出主PDE。将该PDE作为物理约束嵌入损失函数，训练神经网络近似传输速度场，实现无监督训练。

Result: 神经参数化作为隐式正则化器，缓解了分析流的数值刚性，降低了在线计算复杂度。在多模态基准和挑战性非线性场景的实验验证中，相比最先进基线表现出更好的模式覆盖和鲁棒性。

Conclusion: 该方法通过将物理约束嵌入神经网络，实现了高效的无监督贝叶斯推断，解决了高维非线性估计中的计算挑战，同时保持了概率传输的几何结构。

Abstract: The Bayesian update step poses significant computational challenges in high-dimensional nonlinear estimation. While log-homotopy particle flow filters offer an alternative to stochastic sampling, existing formulations usually yield stiff differential equations. Conversely, existing deep learning approximations typically treat the update as a black-box task or rely on asymptotic relaxation, neglecting the exact geometric structure of the finite-horizon probability transport. In this work, we propose a physics-informed neural particle flow, which is an amortized inference framework. To construct the flow, we couple the log-homotopy trajectory of the prior to posterior density function with the continuity equation describing the density evolution. This derivation yields a governing partial differential equation (PDE), referred to as the master PDE. By embedding this PDE as a physical constraint into the loss function, we train a neural network to approximate the transport velocity field. This approach enables purely unsupervised training, eliminating the need for ground-truth posterior samples. We demonstrate that the neural parameterization acts as an implicit regularizer, mitigating the numerical stiffness inherent to analytic flows and reducing online computational complexity. Experimental validation on multimodal benchmarks and a challenging nonlinear scenario confirms better mode coverage and robustness compared to state-of-the-art baselines.

</details>


### [239] [PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training](https://arxiv.org/abs/2602.23111)
*Yanyi Li,Yimu Zhang,Cong Fang*

Main category: cs.LG

TL;DR: PRAC是一种新的LLM激活压缩方法，通过主成分子空间和随机子空间分解，实现无偏梯度估计和最小方差，在预训练和微调任务中减少36%内存占用且性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 激活已成为大批量LLM训练中的主要内存瓶颈，现有压缩方法未能利用激活的谱结构，导致收敛慢或压缩有限。

Method: 提出PRAC方法，将激活分解为两个组件：通过SVD捕获的主子空间保留主要信息，以及从正交补中采样的随机子空间近似尾部信息。通过引入精确缩放因子，实现无偏梯度估计和最小方差。

Result: 在预训练和微调任务中，PRAC实现了高达36%的总内存减少，性能退化可忽略，计算成本最小。

Conclusion: PRAC通过有效利用激活的谱结构，解决了现有压缩方法的局限性，为大规模LLM训练提供了高效的内存压缩解决方案。

Abstract: Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.

</details>


### [240] [Learning Physical Operators using Neural Operators](https://arxiv.org/abs/2602.23113)
*Vignesh Gopakumar,Ander Gray,Dan Giles,Lorenzo Zanisi,Matt J. Kusner,Timo Betcke,Stanislas Pamela,Marc Peter Deisenroth*

Main category: cs.LG

TL;DR: 提出基于算子分裂的物理信息训练框架，将PDE分解为线性和非线性算子分别处理，通过神经ODE实现连续时间预测，提升泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子模型在泛化到训练分布之外的物理场景时表现不佳，且通常受限于固定的时间离散化。需要一种能够适应新物理机制、支持连续时间预测且保持参数效率的方法。

Method: 使用算子分裂方法分解PDE，训练独立的神经算子学习非线性物理算子，而线性算子用固定的有限差分卷积近似。构建混合专家架构，将建模任务表述为神经ODE，通过标准ODE求解器实现连续时间预测。

Result: 在不可压缩和可压缩Navier-Stokes方程上验证，该方法在泛化到未见物理场景时表现出更好的收敛性和优越性能，能够进行时间外推预测，且组件行为可验证。

Conclusion: 该框架通过显式编码算子结构和物理约束，解决了神经算子的泛化和时间离散化限制问题，实现了参数高效、可解释且能适应新物理机制的学习方法。

Abstract: Neural operators have emerged as promising surrogate models for solving partial differential equations (PDEs), but struggle to generalise beyond training distributions and are often constrained to a fixed temporal discretisation. This work introduces a physics-informed training framework that addresses these limitations by decomposing PDEs using operator splitting methods, training separate neural operators to learn individual non-linear physical operators while approximating linear operators with fixed finite-difference convolutions. This modular mixture-of-experts architecture enables generalisation to novel physical regimes by explicitly encoding the underlying operator structure. We formulate the modelling task as a neural ordinary differential equation (ODE) where these learned operators constitute the right-hand side, enabling continuous-in-time predictions through standard ODE solvers and implicitly enforcing PDE constraints. Demonstrated on incompressible and compressible Navier-Stokes equations, our approach achieves better convergence and superior performance when generalising to unseen physics. The method remains parameter-efficient, enabling temporal extrapolation beyond training horizons, and provides interpretable components whose behaviour can be verified against known physics.

</details>


### [241] [Bound to Disagree: Generalization Bounds via Certifiable Surrogates](https://arxiv.org/abs/2602.23128)
*Mathieu Bazinet,Valentina Zantedeschi,Pascal Germain*

Main category: cs.LG

TL;DR: 提出基于分歧的泛化证书方法，通过代理模型评估目标模型的风险，无需修改目标模型或训练过程


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的泛化界限通常存在三个问题：过于宽松（vacuous）、不可计算、或仅限于特定模型类别。需要一种更实用、通用的泛化评估方法

Method: 提出新的基于分歧的证书方法：1）为任意两个预测器之间的真实风险差距提供证书；2）通过具有紧致泛化保证的代理模型来界定目标预测器的真实风险；3）在未标记数据集上评估分歧界限。使用三种框架训练代理模型：样本压缩、模型压缩和PAC-Bayes理论

Result: 实验证明获得的证书具有紧致性，展示了方法的通用性。关键优势是：无需修改目标模型，也无需根据泛化框架调整训练过程

Conclusion: 提出了一种实用的泛化评估框架，通过代理模型和分歧证书为任意深度学习模型提供可计算的泛化保证，解决了传统泛化界限的局限性

Abstract: Generalization bounds for deep learning models are typically vacuous, not computable or restricted to specific model classes. In this paper, we tackle these issues by providing new disagreement-based certificates for the gap between the true risk of any two predictors. We then bound the true risk of the predictor of interest via a surrogate model that enjoys tight generalization guarantees, and evaluating our disagreement bound on an unlabeled dataset. We empirically demonstrate the tightness of the obtained certificates and showcase the versatility of the approach by training surrogate models leveraging three different frameworks: sample compression, model compression and PAC-Bayes theory. Importantly, such guarantees are achieved without modifying the target model, nor adapting the training procedure to the generalization framework.

</details>


### [242] [DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding](https://arxiv.org/abs/2602.23135)
*Tyler Bonnet,Marek Rei*

Main category: cs.LG

TL;DR: DyGnROLE：一种基于Transformer的动态图架构，通过分离源节点和目标节点的表示，使用角色感知建模来提升动态图学习效果


<details>
  <summary>Details</summary>
Motivation: 现实世界中的动态图通常是有向的，源节点和目标节点表现出不对称的行为模式和时序动态。现有动态图架构大多依赖共享参数处理源节点和目标节点，缺乏系统性的角色感知建模。

Method: 提出DyGnROLE架构，使用独立的嵌入词汇表和角色语义位置编码来显式解耦源节点和目标节点表示。引入自监督预训练目标Temporal Contrastive Link Prediction (TCLP)，利用未标记的交互历史学习角色特定表示。

Result: 在未来边分类任务评估中，DyGnROLE显著优于多种最先进的基线方法，证明了角色感知建模在动态图学习中的有效性。

Conclusion: 角色感知建模是动态图学习的有效策略，DyGnROLE通过显式解耦源节点和目标节点表示，在低标签环境下也能学习到信息丰富的结构偏置。

Abstract: Real-world dynamic graphs are often directed, with source and destination nodes exhibiting asymmetrical behavioral patterns and temporal dynamics. However, existing dynamic graph architectures largely rely on shared parameters for processing source and destination nodes, with limited or no systematic role-aware modeling. We propose DyGnROLE (Dynamic Graph Node-Role-Oriented Latent Encoding), a transformer-based architecture that explicitly disentangles source and destination representations. By using separate embedding vocabularies and role-semantic positional encodings, the model captures the distinct structural and temporal contexts unique to each role. Critical to the effectiveness of these specialized embeddings in low-label regimes is a self-supervised pretraining objective we introduce: Temporal Contrastive Link Prediction (TCLP). The pretraining uses the full unlabeled interaction history to encode informative structural biases, enabling the model to learn role-specific representations without requiring annotated data. Evaluation on future edge classification demonstrates that DyGnROLE substantially outperforms a diverse set of state-of-the-art baselines, establishing role-aware modeling as an effective strategy for dynamic graph learning.

</details>


### [243] [Prediction of Diffusion Coefficients in Mixtures with Tensor Completion](https://arxiv.org/abs/2602.23142)
*Zeno Romero,Kerstin Münnemann,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: 提出混合张量补全方法预测二元混合物中无限稀释扩散系数随温度的变化，结合贝叶斯框架和主动学习策略，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 混合物中扩散系数的预测对许多应用至关重要，但实验数据稀缺。现有的矩阵补全方法仅限于单一温度预测，且精度依赖于每个温度下高质量实验数据的可用性。

Method: 提出混合张量补全方法，采用Tucker分解，在298K、313K和333K的实验数据上联合训练。将SEGWE半经验模型的预测作为贝叶斯训练框架的先验知识，可在268K-378K范围内线性外推。通过主动学习策略扩展实验数据库，使用PFG NMR测量了19个溶质+溶剂体系的扩散系数。

Result: TCM在所有研究温度下相比现有模型显著提高了预测精度。通过主动学习扩展数据库后，TCM的预测精度得到实质性提升。

Conclusion: 结合数据高效的机器学习方法与自适应实验策略，在输运性质预测建模方面具有巨大潜力，为混合物扩散系数的温度依赖性预测提供了有效解决方案。

Abstract: Predicting diffusion coefficients in mixtures is crucial for many applications, as experimental data remain scarce, and machine learning (ML) offers promising alternatives to established semi-empirical models. Among ML models, matrix completion methods (MCMs) have proven effective in predicting thermophysical properties, including diffusion coefficients in binary mixtures. However, MCMs are restricted to single-temperature predictions, and their accuracy depends strongly on the availability of high-quality experimental data for each temperature of interest. In this work, we address this challenge by presenting a hybrid tensor completion method (TCM) for predicting temperature-dependent diffusion coefficients at infinite dilution in binary mixtures. The TCM employs a Tucker decomposition and is jointly trained on experimental data for diffusion coefficients at infinite dilution in binary systems at 298 K, 313 K, and 333 K. Predictions from the semi-empirical SEGWE model serve as prior knowledge within a Bayesian training framework. The TCM then extrapolates linearly to any temperature between 268 K and 378 K, achieving markedly improved prediction accuracy compared to established models across all studied temperatures. To further enhance predictive performance, the experimental database was expanded using active learning (AL) strategies for targeted acquisition of new diffusion data by pulsed-field gradient (PFG) NMR measurements. Diffusion coefficients at infinite dilution in 19 solute + solvent systems were measured at 298 K, 313 K, and 333 K. Incorporating these results yields a substantial improvement in the TCM's predictive accuracy. These findings highlight the potential of combining data-efficient ML methods with adaptive experimentation to advance predictive modeling of transport properties.

</details>


### [244] [Partial recovery of meter-scale surface weather](https://arxiv.org/abs/2602.23146)
*Jonathan Giezendanner,Qidong Yang,Eric Schmitt,Anirban Chandra,Daniel Salles Civitarese,Johannes Jakubik,Jeremy Vila,Detlef Hohl,Campbell Watson,Sherrie Wang*

Main category: cs.LG

TL;DR: 该研究提出了一种从现有观测数据中推断米级近地表天气场的计算方法，相比ERA5显著降低了误差，揭示了可预测的米尺度天气模式。


<details>
  <summary>Details</summary>
Motivation: 当前天气分析和预报缺乏米尺度的近地表大气变化信息，这种变化由地表覆盖和地形引起。研究旨在探索这种米尺度变化是否包含可从地表特征和大尺度大气强迫中预测的成分。

Method: 通过将粗分辨率大气状态与稀疏地面站测量和高分辨率地球观测数据相结合，推断出连续美国范围内10米分辨率的近地表风、温度和湿度连续场。

Result: 相比ERA5，推断场将风误差降低了29%，温度和露点误差降低了6%，在固定时间步长上解释了更多的空间方差。结果显示出物理可解释的结构，包括城市热岛、蒸散驱动的湿度对比和不同土地覆盖类型间的风速差异。

Conclusion: 研究表明米尺度近地表天气存在可统计恢复的物理相干成分，通过将粗分辨率动力模型与静态精细尺度特征相结合，可以揭示地球系统中先前未解析的成分，扩展了天气建模的前沿。

Abstract: Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.

</details>


### [245] [Benchmarking Temporal Web3 Intelligence: Lessons from the FinSurvival 2025 Challenge](https://arxiv.org/abs/2602.23159)
*Oshani Seneviratne,Fernando Spadea,Adrien Pavao,Aaron Micah Green,Kristin P. Bennett*

Main category: cs.LG

TL;DR: FinSurvival Challenge 2025作为Temporal Web3智能基准测试案例研究，使用Aave v3协议的2180万笔交易记录，设计了16个生存预测任务来建模用户行为转换，展示了领域感知时间特征构建优于通用建模方法。


<details>
  <summary>Details</summary>
Motivation: 当前Temporal Web3领域缺乏能够捕捉真实世界时间动态（特别是审查和非平稳性）的共享、可复现基准测试，这阻碍了方法学进展并限制了Web3与更广泛Web领域之间的技术转移。

Method: 使用Aave v3协议的2180万笔交易记录，设计了16个生存预测任务来建模用户行为转换，通过基准测试比较不同方法，特别关注领域感知时间特征构建。

Result: 领域感知时间特征构建方法显著优于通用建模方法，证明了Web3系统作为研究时间挑战（如流失、风险和演化）的高保真沙盒的价值。

Conclusion: Web3系统为研究时间挑战提供了高保真沙盒，FinSurvival Challenge 2025展示了如何设计有效的Temporal Web3基准测试，并强调了领域知识在时间特征构建中的重要性。

Abstract: Temporal Web analytics increasingly relies on large-scale, longitudinal data to understand how users, content, and systems evolve over time. A rapidly growing frontier is the \emph{Temporal Web3}: decentralized platforms whose behavior is recorded as immutable, time-stamped event streams. Despite the richness of this data, the field lacks shared, reproducible benchmarks that capture real-world temporal dynamics, specifically censoring and non-stationarity, across extended horizons. This absence slows methodological progress and limits the transfer of techniques between Web3 and broader Web domains. In this paper, we present the \textit{FinSurvival Challenge 2025} as a case study in benchmarking \emph{temporal Web3 intelligence}. Using 21.8 million transaction records from the Aave v3 protocol, the challenge operationalized 16 survival prediction tasks to model user behavior transitions.We detail the benchmark design and the winning solutions, highlighting how domain-aware temporal feature construction significantly outperformed generic modeling approaches. Furthermore, we distill lessons for next-generation temporal benchmarks, arguing that Web3 systems provide a high-fidelity sandbox for studying temporal challenges, such as churn, risk, and evolution that are fundamental to the wider Web.

</details>


### [246] [MetaOthello: A Controlled Study of Multiple World Models in Transformers](https://arxiv.org/abs/2602.23164)
*Aviral Chawla,Galen Hall,Juniper Lovato*

Main category: cs.LG

TL;DR: 本文提出了MetaOthello框架，通过训练小型GPT模型在多种Othello变体游戏上，研究Transformer如何在一个共享表示空间中组织多个世界模型。


<details>
  <summary>Details</summary>
Motivation: 基础模型需要处理多个生成过程，但当前的机制可解释性研究主要关注孤立能力；尚不清楚单个Transformer如何组织多个可能冲突的"世界模型"。先前关于Othello神经网络的研究只关注单一规则游戏。

Method: 引入MetaOthello，一个包含共享语法但不同规则或标记化的Othello变体套件，在混合变体数据上训练小型GPT模型，研究多个世界模型在共享表示空间中的组织方式。

Result: Transformer在混合游戏数据上训练时，不会将能力划分为孤立子模型；而是收敛到大部分共享的棋盘状态表示，该表示能在变体间因果传递。线性探针在一个变体上训练后，能干预另一个变体的内部状态，效果接近匹配探针。对于同构游戏的标记重映射，表示在层间通过单一正交旋转等价。

Conclusion: MetaOthello为理解Transformer是否学习世界模型以及如何同时组织多个世界模型提供了路径。当规则部分重叠时，早期层保持游戏无关表示，中间层识别游戏身份，后期层专门化。

Abstract: Foundation models must handle multiple generative processes, yet mechanistic interpretability largely studies capabilities in isolation; it remains unclear how a single transformer organizes multiple, potentially conflicting "world models". Previous experiments on Othello playing neural-networks test world-model learning but focus on a single game with a single set of rules. We introduce MetaOthello, a controlled suite of Othello variants with shared syntax but different rules or tokenizations, and train small GPTs on mixed-variant data to study how multiple world models are organized in a shared representation space. We find that transformers trained on mixed-game data do not partition their capacity into isolated sub-models; instead, they converge on a mostly shared board-state representation that transfers causally across variants. Linear probes trained on one variant can intervene on another's internal state with effectiveness approaching that of matched probes. For isomorphic games with token remapping, representations are equivalent up to a single orthogonal rotation that generalizes across layers. When rules partially overlap, early layers maintain game-agnostic representations while a middle layer identifies game identity, and later layers specialize. MetaOthello offers a path toward understanding not just whether transformers learn world models, but how they organize many at once.

</details>


### [247] [Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models](https://arxiv.org/abs/2602.23179)
*Gal Kesten-Pomeranz,Yaniv Nikankin,Anja Reusch,Tomer Tsaban,Ora Schueler-Furman,Yonatan Belinkov*

Main category: cs.LG

TL;DR: PLMs通过结合语言模式匹配和生物学专业知识来检测蛋白质序列中的精确和近似重复片段


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列中存在大量重复片段（精确和近似重复），这些重复对蛋白质结构和功能很重要。虽然已有算法用于识别重复，但最近研究表明蛋白质语言模型（PLMs）也能识别重复。本研究旨在阐明PLMs检测重复的内部机制。

Method: 通过分析PLMs在掩码标记预测中的行为，研究其检测精确和近似重复的机制。特别关注模型内部如何构建特征表示，以及注意力机制如何工作。

Result: 发现PLMs检测近似重复的机制功能上包含了检测精确重复的机制。机制分为两个主要阶段：1）PLMs使用通用位置注意力头和生物学专门组件（如编码氨基酸相似性的神经元）构建特征表示；2）归纳头（induction heads）关注重复片段间的对齐标记，促进正确答案。

Conclusion: PLMs通过结合基于语言的模式匹配和专门的生物学知识来解决这一生物学任务，为研究PLMs中更复杂的进化过程奠定了基础。

Abstract: Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.

</details>


### [248] [Closing the gap on tabular data with Fourier and Implicit Categorical Features](https://arxiv.org/abs/2602.23182)
*Marius Dragoi,Florin Gogianu,Elena Burceanu*

Main category: cs.LG

TL;DR: 该论文提出通过统计特征处理和傅里叶学习来提升深度学习在表格数据上的性能，使其能与XGBoost竞争


<details>
  <summary>Details</summary>
Motivation: 深度学习在表格数据上表现不如树模型，作者认为树模型能更好处理具有分类特征的非线性交互，而神经网络偏向均匀数值处理和平滑解

Method: 1) 使用基于统计的特征处理技术识别与目标强相关的离散化特征；2) 使用Learned Fourier减轻深度学习模型对过度平滑解的偏好

Result: 提出的特征预处理显著提升了深度学习模型性能，在综合表格数据基准测试中达到或超越了XGBoost的表现

Conclusion: 通过适当的特征处理和缓解平滑偏差，深度学习可以在表格数据上达到与树模型相当甚至更好的性能

Abstract: While Deep Learning has demonstrated impressive results in applications on various data types, it continues to lag behind tree-based methods when applied to tabular data, often referred to as the last "unconquered castle" for neural networks. We hypothesize that a significant advantage of tree-based methods lies in their intrinsic capability to model and exploit non-linear interactions induced by features with categorical characteristics. In contrast, neural-based methods exhibit biases toward uniform numerical processing of features and smooth solutions, making it challenging for them to effectively leverage such patterns. We address this performance gap by using statistical-based feature processing techniques to identify features that are strongly correlated with the target once discretized. We further mitigate the bias of deep models for overly-smooth solutions, a bias that does not align with the inherent properties of the data, using Learned Fourier. We show that our proposed feature preprocessing significantly boosts the performance of deep learning models and enables them to achieve a performance that closely matches or surpasses XGBoost on a comprehensive tabular data benchmark.

</details>


### [249] [Efficient Real-Time Adaptation of ROMs for Unsteady Flows Using Data Assimilation](https://arxiv.org/abs/2602.23188)
*Ismaël Zighed,Andrea Nóvoa,Luca Magri,Taraneh Sayadi*

Main category: cs.LG

TL;DR: 提出一种高效的参数化降阶模型重训练策略，仅需稀疏观测数据即可达到接近完全重训练的精度，计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 传统降阶模型在参数变化时需要完全重训练，计算成本高昂。需要开发能够仅用稀疏观测数据就能高效适应新参数区域的实时自适应方法。

Method: 采用编码-处理-解码架构：变分自编码器进行降维，Transformer网络演化潜在状态并建模动力学。模型参数化处理外部控制变量（雷诺数），利用注意力机制捕捉时空依赖和参数效应。通过集成卡尔曼滤波同化数据，从稀疏观测重建全状态轨迹。

Result: 模型在Navier-Stokes设置中表现出色，能够生成轨迹集成并提供不确定性量化。关键发现是外样本预测的主要误差源于潜在流形扭曲而非潜在动力学变化，因此重训练可仅限于自编码器部分。

Conclusion: 该方法实现了轻量级、计算高效的实时自适应，仅需非常稀疏的微调数据即可适应新参数区域，为参数化降阶模型的实际应用提供了实用解决方案。

Abstract: We propose an efficient retraining strategy for a parameterized Reduced Order Model (ROM) that attains accuracy comparable to full retraining while requiring only a fraction of the computational time and relying solely on sparse observations of the full system. The architecture employs an encode-process-decode structure: a Variational Autoencoder (VAE) to perform dimensionality reduction, and a transformer network to evolve the latent states and model the dynamics. The ROM is parameterized by an external control variable, the Reynolds number in the Navier-Stokes setting, with the transformer exploiting attention mechanisms to capture both temporal dependencies and parameter effects. The probabilistic VAE enables stochastic sampling of trajectory ensembles, providing predictive means and uncertainty quantification through the first two moments. After initial training on a limited set of dynamical regimes, the model is adapted to out-of-sample parameter regions using only sparse data. Its probabilistic formulation naturally supports ensemble generation, which we employ within an ensemble Kalman filtering framework to assimilate data and reconstruct full-state trajectories from minimal observations. We further show that, for the dynamical system considered, the dominant source of error in out-of-sample forecasts stems from distortions of the latent manifold rather than changes in the latent dynamics. Consequently, retraining can be limited to the autoencoder, allowing for a lightweight, computationally efficient, real-time adaptation procedure with very sparse fine-tuning data.

</details>


### [250] [Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language](https://arxiv.org/abs/2602.23201)
*Max S. Bennett,Thomas P. Zollo,Richard Zemel*

Main category: cs.LG

TL;DR: 提出了一种基于自然语言指令的通用神经记忆系统，使AI代理能够从异构信息源中选择性学习，解决传统神经记忆方法目标固定、无法控制记忆内容的问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型需要在非平稳环境中持续适应新任务和知识。持续微调和上下文学习成本高且脆弱，而现有神经记忆方法通常假设单一固定目标和同质信息流，用户无法控制模型随时间记住或忽略什么内容。

Method: 提出通用神经记忆系统，基于自然语言指定的学习指令执行灵活更新。使自适应代理能够从异构信息源中选择性学习，支持医疗和客户服务等固定目标记忆更新不足的场景。

Result: 该方法解决了传统神经记忆方法的局限性，提供了更灵活、可控的记忆更新机制，能够适应复杂现实场景中的多样化学习需求。

Conclusion: 基于自然语言指令的神经记忆系统为自适应代理提供了更强大的持续学习能力，使模型能够根据用户指令选择性地学习和记忆，适用于需要灵活知识管理的实际应用场景。

Abstract: Modern machine learning models are deployed in diverse, non-stationary environments where they must continually adapt to new tasks and evolving knowledge. Continual fine-tuning and in-context learning are costly and brittle, whereas neural memory methods promise lightweight updates with minimal forgetting. However, existing neural memory models typically assume a single fixed objective and homogeneous information streams, leaving users with no control over what the model remembers or ignores over time. To address this challenge, we propose a generalized neural memory system that performs flexible updates based on learning instructions specified in natural language. Our approach enables adaptive agents to learn selectively from heterogeneous information sources, supporting settings, such as healthcare and customer service, where fixed-objective memory updates are insufficient.

</details>


### [251] [Takeuchi's Information Criteria as Generalization Measures for DNNs Close to NTK Regime](https://arxiv.org/abs/2602.23219)
*Hiroki Naganuma,Taiji Suzuki,Rio Yokota,Masahiro Nomura,Kohta Ishikawa,Ikuro Sato*

Main category: cs.LG

TL;DR: 该研究探讨了竹内信息准则(TIC)在深度神经网络中的适用性，发现在接近神经正切核(NTK)机制时，TIC能有效解释泛化差距，但在NTK机制外则失效。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络等统计奇异模型的泛化度量难以建立，本研究旨在探索经典度量方法竹内信息准则(TIC)在何种条件下能有效解释DNNs的泛化差距。

Method: 理论分析表明TIC在NTK机制附近适用；实验训练了5,000多个DNN模型(12种架构，包括VGG-16等大型模型)，在4个数据集上估计TIC值，使用多种计算可行的TIC近似方法，并评估精度权衡。

Result: 实验结果显示，在接近NTK机制的条件下，估计的TIC值与泛化差距有良好相关性；但理论和实证都表明，在NTK机制外这种相关性消失；TIC在超参数优化中表现出比现有方法更好的试验剪枝能力。

Conclusion: TIC在接近NTK机制的条件下能有效解释DNNs的泛化差距，但在其他机制下失效；TIC在超参数优化中具有实用价值，但应用时需注意其适用范围。

Abstract: Generalization measures have been studied extensively in the machine learning community to better characterize generalization gaps. However, establishing a reliable generalization measure for statistically singular models such as deep neural networks (DNNs) is difficult due to their complex nature. This study focuses on Takeuchi's information criterion (TIC) to investigate the conditions under which this classical measure can effectively explain the generalization gaps of DNNs. Importantly, the developed theory indicates the applicability of TIC near the neural tangent kernel (NTK) regime. In a series of experiments, we trained more than 5,000 DNN models with 12 architectures, including large models (e.g., VGG-16), on four datasets, and estimated the corresponding TIC values to examine the relationship between the generalization gap and the TIC estimates. We applied several TIC approximation methods with feasible computational costs and assessed the accuracy trade-off. Our experimental results indicate that the estimated TIC values correlate well with the generalization gap under conditions close to the NTK regime. However, we show both theoretically and empirically that outside the NTK regime such correlation disappears. Finally, we demonstrate that TIC provides better trial pruning ability than existing methods for hyperparameter optimization.

</details>


### [252] [Physics Informed Viscous Value Representations](https://arxiv.org/abs/2602.23280)
*Hrishikesh Viswanath,Juanwu Lu,S. Talha Bukhari,Damon Conover,Ziran Wang,Aniket Bera*

Main category: cs.LG

TL;DR: 提出基于HJB方程粘性解的物理信息正则化方法，改进离线目标条件强化学习中的价值估计问题


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习（GCRL）从静态数据集中学习策略，但状态-动作空间覆盖有限导致价值估计不准确。现有基于一阶PDE（如Eikonal方程）的物理信息方法在复杂高维环境中往往不适定。

Method: 提出基于Hamilton-Jacobi-Bellman（HJB）方程粘性解的物理信息正则化方法，为学习过程提供基于物理的归纳偏置，明确正则化并限制价值迭代中的更新。利用Feynman-Kac定理将PDE解重新表述为期望，实现可处理的蒙特卡洛估计，避免高阶梯度数值不稳定。

Result: 实验表明该方法提高了几何一致性，可广泛应用于导航和高维复杂操作任务。

Conclusion: 通过HJB方程的物理信息正则化，为离线GCRL提供了更稳健的价值估计框架，在复杂环境中表现更好。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.

</details>


### [253] [Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity](https://arxiv.org/abs/2602.23296)
*Quang-Huy Nguyen,Jiaqi Wang,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: FedWQ-CP：一种用于异构联邦学习的简单有效的不确定性量化方法，通过单轮通信实现代理-服务器校准，平衡覆盖性能与效率


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临不确定性量化挑战，现有方法通常单独处理数据异构性或模型异构性，忽略了它们对代理覆盖可靠性的联合影响。在异构联邦学习环境中，保形预测的应用尚未充分探索。

Method: 提出FedWQ-CP方法：1) 每个代理在本地校准数据上计算符合性分数并推导本地分位数阈值；2) 代理仅向服务器传输其分位数阈值和校准样本大小；3) 服务器通过加权平均聚合这些阈值生成全局阈值；4) 整个过程只需单轮通信。

Result: 在7个公共数据集（分类和回归任务）上的实验表明，FedWQ-CP在经验上保持了代理级和全局覆盖，同时产生了最小的预测集或区间。

Conclusion: FedWQ-CP是一种简单有效的联邦不确定性量化方法，能够在双重异构性下平衡覆盖性能与效率，解决了现有方法忽视数据与模型异构性联合影响的问题。

Abstract: Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.

</details>


### [254] [Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303)
*Ilya Balabin,Thomas M. Kaiser*

Main category: cs.LG

TL;DR: 该论文提出了一种新的数学框架"推断力学"，将化学理论、生物理论、概率论和因果关系结合，以解决自然科学中机器学习模型的因果缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 当前自然科学中的机器学习模型通常被视为黑箱，缺乏对数据因果结构的深入考虑。虽然已有尝试将因果关系引入自然现象的机器学习讨论，但仍缺乏统一的理论处理框架。

Method: 提出了"焦点"这一新概念，定义为机器学习算法在大型数据集中缩小到隐藏基础机制的能力。建立了化学生物学现象的基础因果结构形式框架，并将其扩展到机器学习中。

Result: 提供了在Akt抑制剂家族上这些原理的初步证明。建立了化学生物学中无需还原论工具的新型数学框架——推断力学。

Conclusion: 该系列论文旨在为化学生物学建立新的数学框架，通过推断力学来建模自然机制，无需依赖还原论工具。后续论文将进一步探讨化学相似性和隐藏因果结构对机器学习的影响。

Abstract: Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

</details>


### [255] [A Proper Scoring Rule for Virtual Staining](https://arxiv.org/abs/2602.23305)
*Samuel Tonks,Steve Hood,Ryan Musso,Ceridwen Hopely,Steve Titus,Minh Doan,Iain Styles,Alexander Krull*

Main category: cs.LG

TL;DR: 提出信息增益(IG)作为生成式虚拟染色模型的细胞级评估框架，可直接评估预测后验分布，优于现有仅评估边际分布的方法


<details>
  <summary>Details</summary>
Motivation: 现有评估协议只能检查数据集上边际分布的准确性，无法评估预测的后验分布，而生成式虚拟染色模型需要评估其提供的后验分布质量

Method: 引入信息增益(IG)作为严格适当的评分规则，提供理论动机支持，允许跨模型和特征的比较，并在大规模HTS数据集上评估扩散和GAN模型

Result: IG能够揭示其他指标无法检测到的显著性能差异，在评估扩散和GAN模型时表现出色

Conclusion: 信息增益是评估生成式虚拟染色模型预测后验分布的有效框架，具有理论可解释性和跨模型可比性优势

Abstract: Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.

</details>


### [256] [ParamMem: Augmenting Language Agents with Parametric Reflective Memory](https://arxiv.org/abs/2602.23320)
*Tianjun Yao,Yongqiang Chen,Yujia Zheng,Pan Li,Zhiqiang Shen,Kun Zhang*

Main category: cs.LG

TL;DR: ParamAgent：一种基于反思的智能体框架，通过参数化记忆模块ParamMem编码跨样本反思模式，实现多样化反思生成，提升语言智能体的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体的自我反思机制往往产生重复输出，限制了推理性能。研究发现反思多样性与任务成功率呈强正相关，因此需要增强反思信号的多样性。

Method: 提出ParamMem参数化记忆模块，将跨样本反思模式编码到模型参数中，通过温度控制采样实现多样化反思生成。基于此构建ParamAgent框架，整合参数化记忆与情景记忆、跨样本记忆。

Result: 在代码生成、数学推理和多跳问答任务上的实验表明，ParamAgent相比最先进基线取得一致改进。ParamMem具有样本效率高、支持跨模型规模的弱到强迁移、无需依赖更强外部模型即可自我改进等优势。

Conclusion: ParamMem作为增强语言智能体的有效组件，通过参数化记忆实现多样化反思，显著提升推理性能，展示了参数化记忆在智能体框架中的潜力。

Abstract: Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.

</details>


### [257] [FlashOptim: Optimizers for Memory Efficient Training](https://arxiv.org/abs/2602.23349)
*Jose Javier Gonzalez Ortiz,Abhay Gupta,Chris Renard,Davis Blalock*

Main category: cs.LG

TL;DR: FlashOptim通过量化优化技术将训练时每个参数的内存占用从16字节减少到7字节（或5字节），同时保持模型质量不变，解决了大模型训练的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 标准混合精度训练中，每个参数需要大量内存（参数、梯度、优化器状态），导致训练大模型（如70亿参数）需要超过100GB显存，限制了研究人员的可及性。

Method: 提出两种关键技术：1）改进的主权重分割技术，通过寻找并利用量化误差的紧致边界；2）设计压缩扩展函数，大幅减少8位优化器状态量化的误差。结合16位梯度，实现内存优化。

Result: 将AdamW的内存占用从16字节/参数减少到7字节/参数（或5字节/参数），模型检查点大小减少一半以上。在SGD、AdamW、Lion等优化器上实验，在视觉和语言基准测试（包括Llama-3.1-8B微调）中未观察到质量下降。

Conclusion: FlashOptim通过创新的量化技术实现了超过50%的内存减少，同时保持模型质量，使大模型训练对资源有限的研究人员更加可行。

Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.
  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.
  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

</details>


### [258] [SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353)
*Simon Roschmann,Paul Krzakala,Sonia Mazelet,Quentin Bouniot,Zeynep Akata*

Main category: cs.LG

TL;DR: SOTAlign：一种两阶段半监督对齐框架，使用少量配对数据和大量未配对数据对齐预训练的单模态编码器，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法通常依赖对比损失和数百万配对样本，本文探索是否能用更少的监督实现有意义的对齐，提出半监督设置。

Method: 提出SOTAlign两阶段框架：1）使用线性教师从有限配对数据恢复粗略共享几何；2）通过基于最优传输的散度在未配对样本上细化对齐，传输关系结构而不过度约束目标空间。

Result: SOTAlign有效利用未配对图像和文本，学习跨数据集和编码器对的鲁棒联合嵌入，显著优于监督和半监督基线方法。

Conclusion: SOTAlign能够在有限监督下实现有意义的跨模态对齐，为利用大量未配对数据提供了有效框架。

Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

</details>


### [259] [A Dataset is Worth 1 MB](https://arxiv.org/abs/2602.23358)
*Elad Kimchi Shoshani,Leeyam Gabay,Yedid Hoshen*

Main category: cs.LG

TL;DR: PLADA：一种无需传输像素数据的分布式学习方法，仅通过传输类别标签实现任务知识传递，通信开销小于1MB


<details>
  <summary>Details</summary>
Motivation: 传统数据集分发需要传输大量像素数据，通信成本高；客户端硬件和软件框架多样，传输预训练模型不可行；现有数据集蒸馏方法难以扩展到高分辨率数据且压缩效果有限

Method: 假设客户端预加载大型通用无标注参考数据集（如ImageNet），仅传输特定图像的类别标签；引入剪枝机制过滤参考数据集，保留与目标任务最语义相关的图像标签；同时最大化训练效率和最小化传输负载

Result: 在10个不同数据集上的实验表明，该方法能以小于1MB的传输负载传递任务知识，同时保持高分类准确率

Conclusion: PLADA提供了一种高效的数据集服务解决方案，完全消除像素传输，仅通过标签传输实现任务知识传递，显著降低通信成本

Abstract: A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.

</details>


### [260] [Model Agreement via Anchoring](https://arxiv.org/abs/2602.23360)
*Eric Eaton,Surbhi Goel,Marcel Hussing,Michael Kearns,Aaron Roth,Sikata Bela Sengupta,Jessica Sorrell*

Main category: cs.LG

TL;DR: 提出一种分析模型分歧的通用技术，并应用于四种常见机器学习算法，证明随着特定参数增加，独立训练模型间的分歧会趋近于零。


<details>
  <summary>Details</summary>
Motivation: 研究如何控制机器学习模型间的分歧（预测差异），希望找到能驱动分歧趋近于零的训练参数，并建立适用于现有训练方法的分析框架。

Method: 开发基于"锚定"技术的通用分析方法，将两个模型的分析锚定到它们的平均值，然后应用于四种算法：堆叠聚合、梯度提升、神经网络架构搜索和回归树训练。

Result: 证明四种算法中模型分歧都能趋近于零：堆叠聚合随模型数量k增加；梯度提升随迭代次数k增加；神经网络架构搜索随架构规模n增加；回归树训练随深度d增加。

Conclusion: 提出的锚定技术为分析模型分歧提供了通用框架，成功应用于多种常见算法，证明通过调整特定训练参数可以有效控制模型分歧。

Abstract: Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.
  We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [261] [Huge-Scale Assortment Optimization with Customer Choice: A Parallel Primal-Dual Approach](https://arxiv.org/abs/2602.22421)
*Donghao Zhu,Hanzhang Qin,Ching-pei Lee,Yuki Saito,Takahiro Kawashima,Kenji Fukumizu*

Main category: math.OC

TL;DR: 提出SPFOM算法解决大规模选择优化问题，通过一阶原始对偶方法高效求解CBLP，扩展到多周期库存约束场景，在真实电商数据上验证性能优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法解决涉及数百万客户选择的大规模选择优化问题（CBLP），这在交通、零售、医疗等行业是基础性挑战，需要能利用问题结构的高效算法。

Method: 提出SPFOM（一阶原始对偶方法），每次迭代计算成本小，具有理论近优收敛率，易于并行化，并扩展到多周期库存约束场景，通过影子价格增强传统投标价格控制策略。

Result: SPFOM在计算性能上优于现有大规模线性规划求解器，在ZOZOTOWN真实数据案例中验证了其提升收入性能同时保持库存平衡的实际有效性。

Conclusion: SPFOM为解决大规模选择优化问题提供了高效可扩展的解决方案，在理论和实践上都表现出色，能够处理传统方法无法应对的超大规模问题。

Abstract: We study huge-scale assortment optimization problems to maximize expected revenue under customer choice, addressing a fundamental challenge in industries such as transportation, retail, and healthcare. The choice-based linear programming (CBLP) formulation provides a powerful framework for optimizing sales allocations across customer segments, yet traditional approaches often fail to solve CBLPs of huge scale (involving millions of customer choices) due to the lack of algorithmic designs that exploit problem structure. To overcome this computational bottleneck, we propose a first-order primal-dual method, SPFOM, which requires only a small computational cost per iteration, achieves a provably near-optimal convergence rate, and can be readily extended to parallel computing environments. Computational experiments demonstrate the computational and practical superiority of SPFOM over state-of-the-art solvers for large-scale linear programs. The framework is extended to a multi-period assortment optimization setting with inventory constraints, where SPFOM estimates global shadow prices that enhance classical bid-price control policies compared with benchmark methods such as market segment decomposition. Numerical experiments and a case study using real-world data from the ZOZOTOWN platform validate the practical effectiveness of SPFOM, highlighting its advantages in improving revenue performance while maintaining balanced inventory levels.

</details>


### [262] [Model Predictive Control for output tracking with prescribed performance](https://arxiv.org/abs/2602.22458)
*Dario Dennstädt*

Main category: math.OC

TL;DR: 提出了一种用于非线性连续时间系统的漏斗MPC框架，通过漏斗惩罚函数确保跟踪误差在时变边界内，结合模型自由反馈、数据驱动学习和采样数据实现，解决了可行性、鲁棒性、学习和采样问题。


<details>
  <summary>Details</summary>
Motivation: 传统MPC在实际应用中面临初始和递归可行性、模型失配鲁棒性、采样数据约束等挑战。需要开发一个能够保证规定误差范围内输出跟踪性能的MPC框架。

Method: 1. 漏斗MPC：使用漏斗惩罚函数惩罚跟踪误差偏离时变边界，无需终端条件或长预测时域；2. 混合架构：结合模型自由漏斗反馈，补偿模型失配和未知扰动；3. 数据驱动学习：迭代优化模型；4. 采样数据实现：推导采样率和控制量边界。

Result: 建立了完整的MPC框架，确保在规定误差范围内的输出跟踪性能，解决了可行性、鲁棒性、模型学习和采样实现等关键问题。

Conclusion: 该论文通过整合漏斗MPC、自适应反馈、数据驱动学习和采样数据理论，为规定边界内的输出跟踪建立了统一框架，为学习增强和鲁棒MPC的未来发展铺平了道路。

Abstract: Model Predictive Control (MPC) offers a versatile framework for constraint handling and multi-objective optimisation, yet practical application faces challenges regarding initial and recursive feasibility, robustness against model mismatches, and sampled-data constraints. This thesis develops a novel MPC framework for a class of non-linear continuous-time systems governed by functional differential equations. It targets output tracking within prescribed error bounds while systematically overcoming these challenges.
  First, we introduce funnel MPC, an algorithm eliminating reliance on terminal conditions or restrictive long prediction horizons. Utilising funnel penalty functions -- state costs penalising tracking error deviations from time-varying boundaries -- this framework ensures feasibility while rigorously enforcing tracking performance guarantees.
  Next, we unify funnel MPC with model-free funnel feedback into a hybrid architecture. This synergises model-based optimisation with adaptive feedback compensation, achieving prescribed tracking performance despite structural model-plant mismatches, unmodelled dynamics, and unknown disturbances.
  To further enhance predictive accuracy, we integrate a data-driven learning framework that iteratively refines the model using system measurements, improving long-term performance without compromising robustness.
  Finally, we formalise the transition to sampled-data implementations. We derive explicit bounds on sampling rates and control effort to guarantee stability under piecewise constant control signals, a critical step for digital hardware deployment.
  By addressing feasibility, robustness, learning, and sampling, this thesis establishes a cohesive framework for output tracking within prescribed bounds, paving the way for future advances in learning-enhanced and robust MPC.

</details>


### [263] [A Fast and Practical Column Generation Approach for Identifying Carcinogenic Multi-Hit Gene Combinations](https://arxiv.org/abs/2602.22551)
*Rick S. H. Willemsen,Tenindra Abeywickrama,Ramu Anandakrishnan*

Main category: math.OC

TL;DR: 该论文将癌症驱动基因组合识别问题形式化为多命中癌症驱动集覆盖问题（MHCDSCP），提出了约束规划和混合整数规划方法，在单CPU上快速求解，性能媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 癌症通常由2-9个基因突变的特定组合驱动，识别这些组合对理解癌变机制和设计靶向治疗至关重要。现有方法依赖穷举搜索和超级计算基础设施，计算成本高。

Method: 将问题形式化为多命中癌症驱动集覆盖问题（MHCDSCP），提出约束规划和混合整数规划两种数学规划方法，并引入列生成启发式算法求解小规模实例的最优解。

Result: 在真实癌症基因组数据上的评估显示，该方法在单台商用CPU上运行时间不到一分钟，性能与最先进方法相当。列生成启发式算法能够求解小规模实例的最优解。

Conclusion: 解决MHCDSCP问题的计算复杂度低于先前预期，这为探索建模假设开辟了新的研究方向，使更高效的癌症驱动基因组合识别成为可能。

Abstract: Cancer is often driven by specific combinations of an estimated two to nine gene mutations, known as multi-hit combinations. Identifying these combinations is critical for understanding carcinogenesis and designing targeted therapies. We formalise this challenge as the Multi-Hit Cancer Driver Set Cover Problem (MHCDSCP), a binary classification problem that selects gene combinations to maximise coverage of tumor samples while minimising coverage of normal samples. Existing approaches typically rely on exhaustive search and supercomputing infrastructure. In this paper, we present constraint programming and mixed integer programming formulations of the MHCDSCP. Evaluated on real-world cancer genomics data, our methods achieve performance comparable to state-of-the-art methods while running on a single commodity CPU in under a minute. Furthermore, we introduce a column generation heuristic capable of solving small instances to optimality. These results suggest that solving the MHCDSCP is less computationally intensive than previously believed, thereby opening research directions for exploring modelling assumptions.

</details>


### [264] [Computing Kurdyka-Łojasiewicz exponents via composition and symmetry](https://arxiv.org/abs/2602.22553)
*Cédric Josz,Wenqing Ouyang*

Main category: math.OC

TL;DR: 该论文为Kurdyka-Łojasiewicz指数建立了基于秩定理和李群作用的演算规则，适用于复合函数和不变函数，特别适合处理非孤立局部极小值，无需光滑性假设，为矩阵分解等算法的线性收敛提供了统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有KL指数计算方法通常需要光滑性假设和梯度/Hessian计算，难以处理非孤立局部极小值。本文旨在开发不依赖光滑性的KL指数演算规则，为矩阵分解、矩阵感知、线性神经网络等算法的线性收敛分析提供统一框架。

Method: 利用秩定理和李群作用理论，为复合函数和不变函数建立KL指数的演算规则。该方法不要求函数光滑，避免了梯度和Hessian计算，特别适用于处理非孤立局部极小值的情况。

Result: 建立了适用于广泛复合函数和不变函数的KL指数演算规则，这些规则在矩阵分解、ℓ₁-矩阵分解、矩阵感知和线性神经网络等应用中成功证明了各种算法的线性收敛性。

Conclusion: 通过秩定理和李群作用开发的KL指数演算规则提供了一个不依赖光滑性的统一框架，能够有效处理非孤立局部极小值，为优化算法的线性收敛分析提供了有力工具。

Abstract: We devise calculus rules for the Kurdyka-Łojasiewicz (KL) exponent using the rank theorem and Lie group actions. They apply to a wide class of composite and invariant functions, and are particularly suitable for handling nonisolated local minima. Notably, smoothness plays no role, eschewing gradient and Hessian computations. This provides a unified framework for establishing linear convergence of various algorithms in matrix factorization, $\ell_1$-matrix factorization, matrix sensing, and linear neural networks.

</details>


### [265] [Directional first order approach for a class of bilevel programs](https://arxiv.org/abs/2602.22573)
*Kuang Bai,Wei Yao,Jane J. Ye,Jin Zhang*

Main category: math.OC

TL;DR: 提出一种针对双层优化问题的方向性一阶方法，该方法不要求下层规划具有凸性，通过方向邻域中的一阶条件等价刻画下层规划，并建立方向性必要最优性条件。


<details>
  <summary>Details</summary>
Motivation: 传统双层优化的一阶方法要求下层规划具有凸性，而基于值函数的重构会导致困难优化问题。本文旨在克服这些限制，特别是当下层规划非凸时。

Method: 提出方向性一阶方法：1) 在合理假设下，通过方向邻域中的一阶条件等价刻画下层规划；2) 对得到的单层优化问题，在常见约束规格下建立方向性必要最优性条件。

Result: 成功处理了非凸下层规划的双层优化问题，展示了传统一阶方法的失败案例，并通过方向性方法推导出必要最优性条件。

Conclusion: 方向性一阶方法为处理非凸下层规划的双层优化问题提供了有效工具，克服了传统方法的局限性，具有理论和实际应用价值。

Abstract: In this paper, we study a class of bilevel optimization program (BP), where the feasible set of the lower level program is independent of the upper level variable. For bilevel programs it is known that the first order approach requires the convexity of the lower level program while reformulations involving the value function results in difficult optimization problems. In this paper we propose a directional first order approach which does not require the convexity of the lower level program. First, under some reasonable assumptions, we show that the lower level program can be equivalently characterized by its first order condition over a directional neighborhood. Next, for the resulting single level optimization problem, under common constraint qualifications, we establish directional necessary optimality conditions. Finally, an example of BP with nonconvex lower level program is given, where we demonstrate the failure of the classical first order approach and derive necessary optimality conditions using its directional counterpart.

</details>


### [266] [Gradient Dominance in the Linear Quadratic Regulator: A Unified Analysis for Continuous-Time and Discrete-Time Systems](https://arxiv.org/abs/2602.22577)
*Yuto Watanabe,Yang Zheng*

Main category: math.OC

TL;DR: 本文提出了一个统一的梯度优势性质，适用于连续时间和离散时间线性二次调节器（LQR），基于共同的Lyapunov不等式表示和统一的变量变换过程。


<details>
  <summary>Details</summary>
Motivation: 尽管连续时间和离散时间LQR的梯度优势性质已被广泛研究，但两者通常被分开分析，依赖于略有不同的假设、证明策略和保证结果。本文旨在为这两种时间模型提供一个统一的梯度优势性质分析框架。

Method: 基于共同的Lyapunov不等式表示和统一的变量变换过程，通过凸重构方法建立统一的证明框架。该方法将非凸的策略优化问题转化为凸形式，从而适用于连续时间和离散时间两种模型。

Result: 在温和的可稳定性和可检测性假设下，证明了连续时间和离散时间LQR都具有统一的梯度优势性质。数值示例支持了理论发现，并揭示了两种时间模型之间的深层结构对称性。

Conclusion: 本文提供了一个统一的梯度优势性质分析框架，澄清了连续时间和离散时间动态对理论保证的影响，揭示了两种表述之间的深层结构对称性，为LQR的策略优化提供了更简洁的理论基础。

Abstract: Despite its nonconvexity, policy optimization for the Linear Quadratic Regulator (LQR) admits a favorable structural property known as gradient dominance, which facilitates linear convergence of policy gradient methods to the globally optimal gain. While gradient dominance has been extensively studied, continuous-time and discrete-time LQRs have largely been analyzed separately, relying on slightly different assumptions, proof strategies, and resulting guarantees. In this paper, we present a unified gradient dominance property for both continuous-time and discrete-time LQRs under mild stabilizability and detectability assumptions. Our analysis is based on a convex reformulation derived from a common Lyapunov inequality representation and a unified change-of-variables procedure. This convex-lifting perspective yields a single proof framework applicable to both time models. The unified treatment clarifies how differences between continuous-time and discrete-time dynamics influence theoretical guarantees and reveals a deeper structural symmetry between the two formulations. Numerical examples illustrate and support the theoretical findings.

</details>


### [267] [Dantzig-Wolfe and Arc-Flow Reformulations: A Systematic Comparison](https://arxiv.org/abs/2602.22589)
*Daniel Yamin,Willem-Jan van Hoeve,Ted K. Ralphs*

Main category: math.OC

TL;DR: 本文系统比较了Dantzig-Wolfe和Arc-Flow两种整数规划重构技术，建立了统一的理论框架，分析了它们的理论联系和计算权衡，并通过车辆路径问题实验提供了实际选择指南。


<details>
  <summary>Details</summary>
Motivation: 虽然Dantzig-Wolfe重构和Arc-Flow重构是两种独立发展的整数规划技术，但最近提出的列消去方法揭示了它们之间的紧密联系。本文旨在建立统一的理论框架，澄清这两种重构技术的理论联系和计算权衡，为大规模整数优化问题提供选择指导。

Method: 采用统一公式和符号系统，首先重新审视两种重构的LP松弛给出相同对偶界的事实；然后深入分析原始空间、Dantzig-Wolfe空间和Arc-Flow空间中有效不等式在不同重构间转换的条件；重新解释递减状态空间松弛和列消去等迭代强化方法；最后在带时间窗的车辆路径问题上进行实证比较，使用最先进的列生成和割平面技术。

Result: 理论分析表明两种重构的LP松弛具有相同的对偶界，并建立了有效不等式跨空间转换的条件。实证结果显示明显对比：Arc-Flow重构收敛更快，在子问题高度松弛或低维时表现最佳；而Dantzig-Wolfe重构在主问题保持紧凑时更高效。

Conclusion: 本研究提供了Dantzig-Wolfe和Arc-Flow重构的统一视角和实用选择指南：Arc-Flow适合子问题高度松弛或低维场景，Dantzig-Wolfe适合主问题紧凑场景。这些发现为大规模整数优化中的重构技术选择提供了理论依据和实用指导。

Abstract: Dantzig-Wolfe reformulation is a widely used technique for obtaining stronger relaxations in the context of branch-and-bound methods for solving integer optimization problems. Arc-Flow reformulations are a lesser known technique related to dynamic programming that has experienced a resurgence as result of the recent popularization of decision diagrams as a tool for solving integer programs. Although these two reformulation techniques arose independently, the recently proposed solution paradigm known as column elimination has revealed that they are in fact closely connected. Building on a unified formulation and notation, this study clarifies the theoretical connections and computational trade-offs between these two reformulations.
  We first revisit the known fact that the LP relaxations of these two reformulations yield the same dual bound. We then dig deeper, establishing conditions under which valid inequalities in the original, Dantzig-Wolfe, or Arc-Flow spaces can be translated across reformulations without loss of strength, and reinterpreting iterative strengthening methods, such as decremental state-space relaxation and column elimination, through the lens of cutting planes. To assess the potential impact of these insights empirically, we benchmark both reformulations under identical conditions on the vehicle routing problem with time windows using state-of-the-art column- and cut-generation techniques. The results identify clear contrasts: the Arc-Flow reformulation benefits from faster convergence and performs best when subproblems are highly relaxed or low-dimensional, whereas the Dantzig-Wolfe reformulation is more efficient when the master problem remains compact. Overall, our study provides a unified perspective and practical guidelines for choosing between Dantzig-Wolfe and Arc-Flow reformulations in large-scale integer optimization.

</details>


### [268] [Lower Bounds for Linear Minimization Oracle Methods Optimizing over Strongly Convex Sets](https://arxiv.org/abs/2602.22608)
*Benjamin Grimmer,Ning Liu*

Main category: math.OC

TL;DR: 该论文研究了约束凸优化问题的oracle复杂度下界，证明了在强凸约束集上，确定性LMO方法的最优迭代复杂度为Ω(√(L·diam(S)²/ε))，与Garber和Hazan的加速Frank-Wolfe理论匹配。


<details>
  <summary>Details</summary>
Motivation: 研究约束凸优化问题的计算复杂度下界，特别是在使用线性最小化oracle(LMO)和梯度oracle的设定下。现有Frank-Wolfe方法及其变种在此模型下运行，需要确定这些方法的最优复杂度界限。

Method: 采用理论分析框架，研究确定性LMO方法在强凸约束集上的复杂度下界。通过构造反例和理论证明，建立迭代次数的下界。同时分析β-光滑集上的复杂度，特别关注β=Ω(1/√ε)的适度光滑区域。

Result: 1. 在强凸约束集S上，任何确定性LMO方法保证最终目标间隙小于ε所需的最小迭代次数为Ω(√(L·diam(S)²/ε))。2. 该下界与Garber和Hazan的加速Frank-Wolfe理论匹配，确立了确定性LMO方法在强凸约束集上的最优复杂度。3. 对于β-光滑集，在β=Ω(1/√ε)的适度光滑区域，基于span的LMO方法相对于紧凸集或强凸集没有复杂度改进。

Conclusion: 论文建立了约束凸优化中确定性LMO方法的最优复杂度界限，证明了在强凸约束集上加速Frank-Wolfe方法已达到理论最优。同时表明在适度光滑的约束集上，基于span的LMO方法无法获得复杂度优势。

Abstract: We consider the oracle complexity of constrained convex optimization given access to a Linear Minimization Oracle (LMO) for the constraint set and a gradient oracle for the $L$-smooth, strongly convex objective. This model includes Frank-Wolfe methods and their many variants. Over the problem class of strongly convex constraint sets $S$, our main result proves that no such deterministic method can guarantee a final objective gap less than $\varepsilon$ in fewer than $Ω(\sqrt{L\, \mathrm{diam}(S)^2/\varepsilon})$ iterations. Our lower bound matches, up to constants, the accelerated Frank-Wolfe theory of Garber and Hazan (2015). Together, these establish this as the optimal complexity for deterministic LMO methods over strongly convex constraint sets. Second, we consider optimization over $β$-smooth sets, finding that in the modestly smooth regime of $β=Ω(1/\sqrt{\varepsilon})$, no complexity improvement for span-based LMO methods is possible against either compact convex sets or strongly convex sets.

</details>


### [269] [Efficient exact sequential lifting algorithm for binary knapsack set](https://arxiv.org/abs/2602.22640)
*Xintong Wang,Liang Chen,Yu-Hong Dai*

Main category: math.OC

TL;DR: 提出了一种用于二元背包问题的精确顺序提升算法，采用支配列表结构减少动态规划数组的冗余存储和计算，提高了大规模实例的计算效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在混合整数规划中，提升是生成强有效不等式（切割平面）的关键技术。现有方法在处理大规模、大容量以及非整数系数的二元背包问题时存在计算效率低和存储冗余的问题。

Method: 1. 提出基于支配列表结构的精确顺序提升算法，减少动态规划数组的冗余存储和计算；2. 开发了在某些条件下的提升过程简化方法；3. 算法保持尺度不变性，能有效处理非整数系数约束。

Result: 数值实验表明，该算法在效率和稳定性方面优于传统动态规划数组方法，特别适用于大规模和大容量实例。算法还能处理非整数权重和大容量的二元背包问题，可直接应用于现代MIP求解器。

Conclusion: 提出的支配列表结构算法显著提高了二元背包问题顺序提升的计算效率，解决了传统方法在处理大规模、非整数系数问题时的局限性，为MIP求解器提供了实用的提升工具。

Abstract: Lifting is a crucial technique in mixed integer programming (MIP) for generating strong valid inequalities, which serve as cutting planes to improve the branch-and-cut algorithm. We first propose an exact sequential lifting algorithm for the binary knapsack set, which employs the dominance list structure to remove redundant storage and computation in the dynamic programming (DP) array. This structure preserves scale invariance and effectively handles constraints with non-integer coefficients. Then, a reduction method is developed for the lifting procedure under some conditions, further enhancing computational efficiency. Finally, numerical experiments demonstrate that the proposed algorithm outperforms DP with arrays in terms of both efficiency and stability, particularly for large-scale and large-capacity instances. Moreover, it enables exact sequential lifting for binary knapsack sets with non-integer weights and large capacities, making it directly applicable in modern MIP solvers.

</details>


### [270] [Robust Distributed Nonconvex Optimization Enabling Communication Acceleration and Privacy Protection](https://arxiv.org/abs/2602.22670)
*Zichong Ou,Jie Lu*

Main category: math.OC

TL;DR: 提出了一种鲁棒的近端原始对偶算法（RPP），用于增强多智能体网络中分布式非凸优化的信息安全传输，通过添加随机噪声保护隐私，并证明了收敛性和最优通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有分布式非凸优化算法在智能体间通信时容易泄露信息，需要设计既能保护隐私又保持算法性能的安全优化方法。

Method: 提出鲁棒近端原始对偶算法（RPP），通过精心设计的随机噪声混淆敏感本地信息，保护隐私；进一步结合切比雪夫加速技术开发加速版本。

Result: RPP以次线性速率收敛到平稳解；加速版本达到最优通信复杂度；数值实验验证了算法优越的收敛性能，适当范围内的扰动不会阻碍收敛速度。

Conclusion: RPP算法有效解决了分布式非凸优化中的信息安全问题，在保护隐私的同时保持了算法性能，为安全分布式优化提供了实用解决方案。

Abstract: This paper addresses a distributed nonconvex optimization problem over multi-agent networks, where each agent exchanges its local information solely with its neighbors. Given that most existing distributed nonconvex optimization algorithms are susceptible to information leakage during inter agent communications, we propose a Robust Proximal Primal dual algorithm, referred to as RPP, to enhance the security of information transmission. In contrast to many existing approaches that directly transmit local variables throughout the network, we introduce carefully designed random noises to obfuscate sensitive local information. This not only preserves privacy but also demonstrates the noise robustness of our proposed algorithm. We establish a sublinear rate at which RPP converges to a stationary solution. Moreover, by incorporating Chebyshev acceleration, an accelerated variant of RPP is developed and achieves the optimal communication complexity bound for the algorithms that allow for exchanging local deci sions at each iteration. The superior convergence performance of RPP is validated through a few numerical experiments, which also indicate that, within an appropriate range, the introduced perturbations do not impede the convergence speed of RPP.

</details>


### [271] [Generalized fluctuation bounds for stochastic algorithms in the presence of compactness](https://arxiv.org/abs/2602.22741)
*Morenikeji Neri,Nicholas Pischke,Thomas Powell*

Main category: math.OC

TL;DR: 论文提出了随机拟Fejér单调序列在度量空间中的定量收敛结果，基于证明挖掘的逻辑方法，发展了完全有限形式的Robbins-Siegmund定理，并应用于随机Krasnoselskii-Mann方案求解非扩张映射的公共不动点问题。


<details>
  <summary>Details</summary>
Motivation: 将证明挖掘的逻辑方法应用于概率论，为随机拟Fejér单调序列提供定量收敛分析，克服传统概率收敛结果缺乏显式速率的问题。

Method: 在局部紧度量空间中，基于随机拟Fejér单调性条件，发展有限形式的鞅理论，建立完全有限形式的Robbins-Siegmund定理，构造显式的元稳定点收敛速率。

Result: 获得了随机拟Fejér单调序列的定量收敛结果，包括元稳定点收敛速率和广义波动界，该结果可应用于随机Krasnoselskii-Mann方案求解Hadamard空间中非扩张映射的公共不动点问题。

Conclusion: 该研究将证明挖掘方法成功应用于概率论，提供了随机拟Fejér单调序列的定量收敛分析框架，是迄今为止该方法在概率论中最复杂的案例研究。

Abstract: We provide a convergence result for sequences of random variables taking values in a metric space that satisfy a stochastic quasi-Fejér monotonicity condition, in the context of a (local) compactness assumption. Our result is quantitative in that we derive an explicit and effective construction which, in terms of only a few moduli representing quantitative witnesses to key properties of the sequence of random variables and the underlying metric space involved, provides a metastable rate of pointwise convergence, a type of generalized fluctuation bound. That quantitative result in particular relies on the development of a finitary theory of martingales, culminating in a fully finitary Robbins-Siegmund theorem. We outline how this result particularises to the circumstances of the seminal work of Combettes and Pesquet on stochastic quasi-Fejér monotone sequences in separable Hilbert spaces, and we provide an initial application by illustrating how these results can be used to provide a metastable rate of pointwise convergence for a stochastic Krasnoselskii-Mann scheme solving a stochastic common fixed point problem for nonexpansive maps over proper Hadamard spaces. This work is set in the context of recent applications of the logic-based methodology of proof mining to probability theory, and represents its most sophisticated case study to date.

</details>


### [272] [Existence of Quantum Splines via Fourth-Order Gradient Flows](https://arxiv.org/abs/2602.22888)
*Chun-Chi Lin,Yang-Kai Lue,Dung The Tran*

Main category: math.OC

TL;DR: 本文为量子样条建立了严格的存在性理论，这些曲线描述了最优控制的量子演化，通过几何梯度流框架构造了适定的四阶演化系统。


<details>
  <summary>Details</summary>
Motivation: 量子样条由Brody、Holm和Meier在2012年提出，描述了最优控制的量子演化轨迹。需要为这些变分问题建立严格的数学基础，澄清其解析结构。

Method: 将问题置于黎曼样条插值的几何梯度流框架中，构造适定的四阶演化系统，并调整变分结构以适应物理模型所需的边界条件。

Result: 证明了修改后的系统可以进行严格的解析处理，既得到了存在性结果，又提供了生成量子样条的构造性方法。

Conclusion: 为平滑量子控制轨迹的变分描述提供了数学基础，澄清了量子样条形成的解析结构。

Abstract: We establish a rigorous existence theory for the quantum splines introduced by Brody, Holm, and Meier in Physical Review Letters (2012). These curves arise as solutions of a variational problem on the unitary group describing optimally controlled quantum evolutions. By formulating the problem within a geometric gradient flow framework for Riemannian spline interpolation, we construct a well-posed fourth order evolution whose asymptotic limits realize the desired quantum splines. The analysis requires adapting the variational structure to boundary conditions dictated by the physical model, which are not directly amenable to the setting in our recently developed framework for gradient flows of Riemannian spline interpolation. We show that, despite these difficulties, the modified system admits a rigorous analytical treatment, yielding both existence and a constructive procedure for generating quantum splines. Our results provide a mathematical foundation for the variational description of smooth quantum control trajectories and clarify the analytical structure underlying their formation.

</details>


### [273] [Iteration Complexity of Frank-Wolfe and Its Variants for Bilevel Optimization](https://arxiv.org/abs/2602.23076)
*Anthony Palmieri,Francesco Rinaldi,Saverio Salzo,Sara Venturini*

Main category: math.OC

TL;DR: 研究带约束双层优化问题的Frank-Wolfe方法，考虑下层问题近似求解导致的超梯度偏差和误差，分析多种FW变体的收敛性，并结合超梯度误差界给出总体迭代复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现实中的双层优化问题通常需要近似求解下层问题，这会导致超梯度计算存在偏差和误差。现有研究较少关注在超梯度不精确情况下，Frank-Wolfe方法在带约束双层优化中的收敛性能。

Method: 研究多种Frank-Wolfe变体（包括经典FW、away-step FW和pairwise FW）在超梯度不精确情况下的收敛性。结合迭代近似隐式微分得到的超梯度误差界，分析整体双层FW方法的迭代复杂度。

Result: 在非凸设置下，即使存在梯度误差，所分析的FW方法仍能保证收敛。结合超梯度误差界，推导出双层FW方法的总体迭代复杂度保证。两个实际应用实验验证了理论结果和方法的实际有效性。

Conclusion: 该研究为超梯度不精确情况下的带约束双层优化提供了理论保证，扩展了Frank-Wolfe方法的应用范围，并通过实验验证了方法的实用价值。

Abstract: We study Frank-Wolfe (FW) methods for constrained bilevel optimization when the lower-level problem is solved only approximately, yielding biased and inexact hypergradients. We analyze inexact variants of vanilla FW as well as away-step and pairwise FW, and provide convergence rates in the nonconvex setting under gradient errors. By combining these results with recent bounds on hypergradient errors from iterative and approximate implicit differentiation, we derive overall iteration complexity guarantees for bilevel FW. Experiments on two real-world applications validate the theory and demonstrate practical effectiveness.

</details>


### [274] [Stochastic Differential Inclusions driven by Maximal Monotone Operators with empty interiors](https://arxiv.org/abs/2602.23145)
*Juan Guillermo Garrido,Pedro Pérez-Aros,Mathias Staudigl*

Main category: math.OC

TL;DR: 研究由极大单调算子驱动的随机微分包含的长时间行为，扩展了随机优化方法的收敛性保证


<details>
  <summary>Details</summary>
Motivation: 研究在噪声或近似算子信息下的连续时间一阶优化方法模型，分析随机微分包含的长时间行为

Method: 采用适当的解概念建立适定性，分析随机动力学的渐近性质，考虑Tikhonov型正则化方法

Result: 无需算子定义域具有非空内部即可建立存在唯一性，将收敛保证扩展到非光滑势函数、非全定义域次微分和非Lipschitz单调算子的情况

Conclusion: 为随机微分包含提供了更一般的适定性和收敛性理论框架，扩展了随机优化方法的理论保证

Abstract: This paper studies the long-time behavior of stochastic differential inclusions driven by maximal monotone operators, motivated by continuous-time models of first-order optimization methods under noisy or approximate operator information. We first address well-posedness and show that existence and uniqueness can be established without the customary requirement that the operator's domain has nonempty interior, by adopting an appropriate notion of solution. We then analyze asymptotic properties of the resulting stochastic dynamics, extending convergence guarantees beyond previously studied settings that rely on smooth potentials, full-domain subdifferentials, or Lipschitz monotone operators. In addition, we consider a Tikhonov-type regularization of the stochastic inclusion and prove corresponding well-posedness and long-time convergence results.

</details>


### [275] [Operator learning for prescribed-time stabilization of reaction-diffusion systems](https://arxiv.org/abs/2602.23157)
*Kaijing Lyu,Umberto Biccari,Jun-Min Wang*

Main category: math.OC

TL;DR: 提出基于神经算子近似时变系数热方程边界预定时间稳定的反步核，大幅降低计算成本，实现实时控制


<details>
  <summary>Details</summary>
Motivation: 传统反步设计需要实时求解二维时变核PDE，计算负担过重，限制了时变系数热方程预定时间稳定的实时应用

Method: 1) 使用神经算子近似从时变系统系数到反步核的映射，离线训练后在线实时生成核；2) 直接近似从参数函数和状态测量到边界控制输入的完整反馈律

Result: 神经算子控制器在算子近似误差满足显式界时保持预定时间稳定性；直接近似反馈律实现半全局实际预定时间稳定；数值实验显示核生成计算成本降低数个数量级

Conclusion: 神经算子方法显著降低了时变系数热方程预定时间稳定的计算负担，实现了实时控制能力，为具有严格瞬态性能要求的应用提供了可行方案

Abstract: This paper addresses boundary prescribed-time stabilization of a one-dimensional heat equation with spatially and temporally varying coefficients. In contrast to asymptotic or exponential stabilization, prescribed-time stabilization ensures convergence to equilibrium within a user-defined time that is independent of the initial condition, a property that is particularly attractive in applications with stringent transient performance requirements. The backstepping design for this problem requires solving, at each time instant, a two-dimensional time-dependent kernel Partial Differential Equation (PDE) whose solution continuously varies with the plant coefficients. The repeated numerical solution of this parabolic kernel PDE results in a prohibitive computational burden, thereby limiting real-time applicability. To overcome this limitation, we propose a neural-operator-based approximation of the mapping from the time-varying system coefficient to the corresponding backstepping kernel. The operator is trained offline using representative solutions of the kernel PDE and subsequently deployed online to generate the required time-varying kernels in real time. We establish, via Lyapunov analysis, that the resulting neural-operator-based controller preserves prescribed-time stability provided that the operator approximation error satisfies an explicit bound. Furthermore, we investigate a direct approximation of the full feedback law mapping the plant parameter functions and state measurements to the boundary control input. For this setting, we prove semiglobal practical prescribed-time stability of the closed-loop system. Numerical experiments demonstrate that the proposed approach reduces the computational cost of kernel generation by several orders of magnitude, thereby enabling real-time prescribed-time stabilization for heat equations with spatially and temporally varying coefficients.

</details>


### [276] [Hierarchy of bounds in free orthotropic material optimization: From convex relaxations to Hashin-Shtrikman via sequential global programming](https://arxiv.org/abs/2602.23180)
*Marek Tyburec,Michael Stingl,Shenyuan Ma*

Main category: math.OC

TL;DR: 研究二维平面应力柔度最小化中的自由正交各向异性材料优化，通过构建基于不同能量界限的层次化可实现性感知容许集，填补经典自由材料优化与复合材料可实现张量之间的差距。


<details>
  <summary>Details</summary>
Motivation: 经典自由材料优化中容许的张量与复合材料实际可实现的张量之间存在差距，需要构建更接近实际可实现性的优化模型。

Method: 构建基于零阶、Voigt和Hashin-Shtrikman能量界限的层次化可实现性感知容许集，从凸松弛到更紧的非凸模型。通过主动约束分析和显式体积表征推导简化公式，并采用序列全局规划求解非凸正交各向异性HS问题。

Result: Voigt集在中间体积分数时比零阶集更紧，在纯相端点处重合；HS约束可重写为Voigt项减去非负修正项，其凸包等于Voigt集。单载荷情况下HS松弛与Allaire-Kohn松弛问题等价，多载荷情况下提供一般微结构最优柔度的下界。

Conclusion: 通过构建可实现性感知的容许集层次，有效填补了经典自由材料优化与实际复合材料可实现性之间的差距，数值结果验证了柔度层次结构，并与有限秩层压参考解高度一致。

Abstract: We study free orthotropic material optimization for two-dimensional plane-stress compliance minimization with two well-ordered isotropic phases, motivated by the gap between tensors admissible in classical free-material optimization and tensors realizable by composites. To reduce this gap, we construct a hierarchy of realizability-aware admissible sets induced by zeroth-order, Voigt, and Hashin--Shtrikman (HS) energy bounds, moving from convex relaxations to a tighter nonconvex model. In the convex zeroth-order and Voigt settings, the Voigt set is strictly tighter for intermediate volume fractions and coincides with the zeroth-order set at pure-phase endpoints, and the Voigt model reduces to an isotropic variable-thickness-sheet formulation. In the single-loadcase continuum zeroth-order problem, at least one optimal solution can be chosen orthotropic. For HS constraints, we rewrite the bound as a Voigt term minus a nonnegative correction, clarifying strict tightening for interior volume fractions and local nonconvexity. We further prove that the convex hull of the HS feasible set equals the Voigt set and derive reduced formulations via active-constraint analysis and explicit elementwise volume characterization, including reductions specialized to orthotropic effective tensors. In the single-loadcase continuum setting, the HS relaxation is tight with the Allaire--Kohn relaxed problem, attained in the relaxation sense by sequential laminates, whereas in generic multi-loadcase settings it provides a lower bound on optimal compliance over general microstructures. The resulting nonconvex orthotropic HS problem is solved by sequential global programming, and numerical results confirm the predicted compliance hierarchy and show close agreement with finite-rank laminate references.

</details>


### [277] [Efficient Interior-Point Methods for Hyperbolic Programming via Straight-Line Programs](https://arxiv.org/abs/2602.23260)
*Mehdi Karimi,Levent Tuncel*

Main category: math.OC

TL;DR: DDS 3.0是一个针对双曲规划的高效内点法求解器，通过新的直线程序表示和优化的校正步骤，显著提升了大规模双曲规划问题的计算性能。


<details>
  <summary>Details</summary>
Motivation: 双曲规划作为半定规划和二阶锥规划的推广，虽然理论上有很大进展，但大规模问题的计算工具仍然有限。现有求解器在效率和可扩展性方面存在不足。

Method: 1. 引入新的直线程序表示法，紧凑表示双曲多项式及其梯度和Hessian矩阵；2. 采用批量反向-前向微分方案，使Hessian计算复杂度与梯度相同；3. 改进原对偶内点法的校正步骤，增强在仅原自洽障碍函数可高效计算时的稳定性和收敛性；4. 建立超越基本对称多项式的综合基准库。

Result: 数值实验表明，DDS 3.0相比一阶Frank-Wolfe算法、同伦方法以及使用SDP松弛的SDP软件，在性能上有显著提升。

Conclusion: DDS 3.0通过创新的直线程序表示和优化的内点法实现，为大规模双曲规划问题提供了高效的计算工具，填补了该领域计算工具的空白。

Abstract: Hyperbolic (HB) programming generalizes many popular convex optimization problems, including semidefinite and second-order cone programming. Despite substantial theoretical progress on HB programming, efficient computational tools for solving large-scale hyperbolic programs remain limited. This paper presents DDS 3.0, a new release of the Domain-Driven Solver, which provides an efficient interior-point implementation tailored for hyperbolic programming. A key innovation lies in a new straight-line program (SLP) representation that enables compact representation and efficient computation of hyperbolic polynomials, their gradients, and Hessians. The SLP structure significantly reduces computational cost, allowing the Hessian to be computed in the same asymptotic complexity as the gradient through a batched reverse-over-forward differentiation scheme. We further introduce an improved corrector step for the primal-dual interior-point method, enhancing stability and convergence on convex sets where only the primal self-concordant barrier is efficiently computable. We create a comprehensive benchmark library beyond the elementary symmetric polynomials, using several different techniques. Numerical experiments demonstrate substantial performance gains of DDS 3.0 compared to first-order Frank-Wolfe algorithm, homotopy method, and SDP software utilizing SDP relaxations.

</details>


### [278] [Modeling Large-Scale Adversarial Swarm Engagements using Optimal Control](https://arxiv.org/abs/2602.23323)
*Claire Walton,Isaac Kaminer,Qi Gong,Abram H. Clark,Theodoros Tsatsanifos*

Main category: math.OC

TL;DR: 论文研究了在明确对抗条件下大规模自主系统的最优控制问题，考虑了随时间推移智能体概率性损毁的因素，提出了三种近似数值建模方法。


<details>
  <summary>Details</summary>
Motivation: 现有理论和建模框架往往忽略了智能体在运行过程中的随机损耗，而在许多对抗性自主系统中，不同智能体或群体相互竞争，智能体的随机损耗是影响系统性能的关键因素。有效的建模和控制策略必须同时考虑智能体损耗和空间动态。

Method: 提出了三种近似数值建模方法，在这些方法中，智能体的生存概率根据其相对位置随时间确定性递减。将这些方案应用于智能体防御高价值单位免受攻击群攻击的场景。

Result: 研究结果表明，只要将损耗和空间定位紧密集成，这些模型就能有效捕捉此类交互的动态特性。这些模型能够有效模拟对抗性交互的动态过程。

Conclusion: 这些发现对于广泛的对抗性自主场景都具有重要意义，其中智能体定位和生存概率都起着关键作用。在智能体损耗和空间动态紧密集成的情况下，所提出的建模方法能够有效处理对抗条件下的最优控制问题。

Abstract: We investigate the optimal control of large-scale autonomous systems under explicitly adversarial conditions, incorporating the probabilistic destruction of agents over time. In many such systems, adversarial interactions arise as different agents or groups compete against one another. A crucial yet often overlooked factor in existing theoretical and modeling frameworks is the random attrition of agents during operation. Effective modeling and control strategies must therefore account for both agent attrition and spatial dynamics.
  Given the inherently random nature of agent survival, directly solving this problem is challenging. To address this, we propose and evaluate three approximate numerical modeling approaches in which agent survival probabilities decrease deterministically over time based on their relative positions. We apply these schemes to a scenario where agents defend a high-value unit against an attacking swarm. Our results demonstrate that these models can effectively capture the dynamics of such interactions, provided that attrition and spatial positioning are tightly integrated. These findings are relevant to a broad range of adversarial autonomy scenarios where both agent positioning and survival probabilities play a critical role.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [279] [Forecasting on the Accuracy-Timeliness Frontier: Two Novel `Look Ahead' Predictors](https://arxiv.org/abs/2602.23087)
*Marc Wildi*

Main category: econ.EM

TL;DR: 论文重新审视传统MSE预测范式，通过正式整合准确性-及时性权衡，提出两种"前瞻"框架（DFP和PCS），获得闭式优化解，揭示了准确性-及时性权衡的完整有效前沿。


<details>
  <summary>Details</summary>
Motivation: 传统MSE优化的预测器虽然能准确跟踪水平，但牺牲了动态领先性，导致滞后于变化的目标。需要解决准确性（MSE）与及时性（提前期）之间的权衡问题。

Method: 提出两种"前瞻"框架：解耦当前（DFP）和峰值相关移位（PCS），提供闭式优化解。经典MSE预测器是这些框架的特例。方法能在给定准确性水平下实现最大提前期。

Result: 方法揭示了准确性-及时性权衡的完整有效前沿，而MSE仅代表单一极点。推导了线性预测器在一致性约束下的领先性上界，并证明方法达到该上限。在预测和实时信号提取应用中验证。

Conclusion: 论文通过正式整合准确性-及时性权衡，超越了传统MSE预测范式，提供了系统化的前瞻预测框架，能够根据具体应用需求在准确性和及时性之间进行最优权衡。

Abstract: We re-examine the traditional Mean-Squared Error (MSE) forecasting paradigm by formally integrating an accuracy-timeliness trade-off: accuracy is defined by MSE (or target correlation) and timeliness by advancement (or phase excess). While MSE-optimized predictors are accurate in tracking levels, they sacrifice dynamic lead, causing them to lag behind changing targets. To address this, we introduce two `look-ahead' frameworks--Decoupling-from-Present (DFP) and Peak-Correlation-Shifting (PCS)--and provide closed-form solutions for their optimization. Notably, the classical MSE predictor is shown to be a special case within these frameworks. Dually, our methods achieve maximum advancement for any given accuracy level, so our approach reveals the complete efficient frontier of the accuracy-timeliness trade-off, whereas MSE represents only a single point. We also derive a universal upper bound on lead over MSE for any linear predictor under a consistency constraint and prove that our methods hit this ceiling. We validate this approach through applications in forecasting and real-time signal extraction, introducing a leading-indicator criterion and tailored linear benchmarks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [280] [Power Consumption Patterns Using Telemetry Data](https://arxiv.org/abs/2602.22339)
*Harry Cheon,Yuyang Pang,Zhiting Hu,Benjamin Smarr,Julien Sebot,Bijan Arbab,Ahmed Shams*

Main category: cs.CY

TL;DR: 该论文通过分析英特尔遥测数据，挑战了硬件选择是设备功耗主要决定因素的传统观点，强调用户行为对功耗的重要影响，并建立了线性模型验证这一发现。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观念：反驳硬件选择是设备功耗主要决定因素的普遍看法，强调需要更全面理解功耗影响因素，特别是用户行为的作用。

Method: 1. 探索性数据分析：分析英特尔遥测数据，比较不同国家的功耗模式，特别关注美国和中国；2. 建立线性模型：构建简单线性模型来验证功耗模式并分析影响因素。

Result: 1. EDA揭示了显著的跨国功耗差异模式；2. 线性模型确认了这些模式，并突出了用户行为对功耗的重要影响；3. 验证了用户行为而非硬件选择是功耗的关键决定因素。

Conclusion: 用户行为对设备功耗有显著影响，英特尔等利益相关者需要理解这些功耗模式，以有效减少环境影响，这为节能改进提供了新方向。

Abstract: This paper examines the analysis of package power consumption using Intel's telemetry data. It challenges the prevailing belief that hardware choice is the primary determinant of a device's power consumption and instead emphasizes the significant role of user behavior. The paper includes two sections: Exploratory Data Analysis (EDA) and a linear model for power consumption. The EDA section provides valuable insights from Intel's telemetry data, comparing power consumption across countries, with a specific focus on power consumption patterns in the US and China. Our simple linear model affirms those patterns and highlight the possible importance of user behavior and its influence on power consumption. Ultimately, the paper underscores the need to understand power consumption patterns and identifies areas where stakeholders like Intel can make improvements to reduce environmental impact effectively and efficiently.

</details>


### [281] [The Inference Bottleneck: Antitrust and Neutrality Duties in the Age of Cognitive Infrastructure](https://arxiv.org/abs/2602.22750)
*Gaston Besanson,Marcelo Celani*

Main category: cs.CY

TL;DR: 论文提出"认知基础设施"概念，分析生成式AI商业化中的反竞争风险，并提出"中立推理"监管框架


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI商业化，竞争优势从一次性模型训练转向持续推理、分发和路由。大规模推理可能成为认知基础设施，被少数公司控制并用于下游竞争，存在非价格歧视和路由操纵等反竞争风险

Method: 1) 定义可证伪的"认知基础设施"概念；2) 使用提高竞争对手成本理论分析垂直相关和平台市场的损害理论；3) 提出"中立推理"监管框架，包括服务质量对等、路由透明和FRAND式非歧视原则

Result: 建立了分析AI推理基础设施反竞争风险的框架，提出了针对性的监管解决方案，强调仅在观察到功能性守门人地位时才适用

Conclusion: 需要新的监管框架应对生成式AI基础设施的反竞争风险，"中立推理"提供了可审计的行为监管方法，平衡创新与竞争保护

Abstract: As generative AI commercializes, competitive advantage is shifting from one-time model training toward continuous inference, distribution, and routing. At the frontier, large-scale inference can function as cognitive infrastructure: a bottleneck input that downstream applications rely on to compete, controlled by firms that often compete downstream through integrated assistants, productivity suites, and developer tooling. Foreclosure risk is not limited to price. It can be executed through non-price discrimination (latency, throughput, error rates, context limits, feature gating) and, where models select tools and services, through steering and default routing that is difficult to observe and harder to litigate. This essay makes three moves. First, it defines cognitive infrastructure as a falsifiable concept built around measurable reliance, vertical incentives, and discrimination capacity, without assuming a clean market definition. Second, it frames theories of harm using raising-rivals'-costs logic for vertically related and platform markets, where foreclosure can be profitable without anticompetitive pricing. Third, it proposes Neutral Inference: a targeted, auditable conduct approach built around (i) quality-of-service parity, (ii) routing transparency, and (iii) FRAND-style non-discrimination for similarly situated buyers, applied only when observable evidence indicates functional gatekeeper status.

</details>


### [282] [Fairness in Limited Resources Settings](https://arxiv.org/abs/2602.23026)
*Eitan Bachmat,Inbal Livni Navon*

Main category: cs.CY

TL;DR: 该论文研究在资源有限场景下机器学习决策的公平性问题，将预测与资源分配结合考虑，分析不同公平定义与效用的权衡，并提出具有有界公平代价的公平性定义。


<details>
  <summary>Details</summary>
Motivation: 近年来许多重要社会决策由机器学习算法做出，这些决策通常有严格的容量限制（如医疗预约分配、特殊项目名额）。当决策同时涉及预测和资源分配时，不同群体间的信息差距可能导致资源分配严重不平衡，公平性问题变得尤为关键。

Method: 通过调整资源分配方案中的定义，建立算法公平定义与资源分配定义之间的联系，分析公平性与效用的权衡。研究强制执行不同公平定义相比纯效用优化的代价，并引入比例公平的适应版本和机会均等的变体。

Result: 分析显示，强制执行某些公平定义相比纯效用优化的代价可能是无界的。提出的比例公平适应版本具有有界的公平代价，表明更强的鲁棒性；提出的机会均等变体也具有有界的公平代价。

Conclusion: 在资源有限的预测决策场景中，公平性需要特别关注。通过将资源分配概念与算法公平性结合，可以设计出具有有界公平代价的公平性定义，在公平与效用之间实现更好的平衡。

Abstract: In recent years many important societal decisions are made by machine-learning algorithms, and many such important decisions have strict capacity limits, allowing resources to be allocated only to the highest utility individuals. For example, allocating physician appointments to the patients most likely to have some medical condition, or choosing which children will attend a special program. When performing such decisions, we consider both the prediction aspect of the decision and the resource allocation aspect. In this work we focus on the fairness of the decisions in such settings. The fairness aspect here is critical as the resources are limited, and allocating the resources to one individual leaves less resources for others. When the decision involves prediction together with the resource allocation, there is a risk that information gaps between different populations will lead to a very unbalanced allocation of resources.
  We address settings by adapting definitions from resource allocation schemes, identifying connections between the algorithmic fairness definitions and resource allocation ones, and examining the trade-offs between fairness and utility. We analyze the price of enforcing the different fairness definitions compared to a strictly utility-based optimization of the predictor, and show that it can be unbounded. We introduce an adaptation of proportional fairness and show that it has a bounded price of fairness, indicating greater robustness, and propose a variant of equal opportunity that also has a bounded price of fairness.

</details>


### [283] [Sorting Methods for Online Deliberation: Towards a Principled Approach](https://arxiv.org/abs/2602.23168)
*Nicolien Janssens,Frederik van de Putte*

Main category: cs.CY

TL;DR: 该论文探讨在线审议平台中提案排序方法的系统评估，批评了实践中常见的按"点赞数"排序方法，并提出更合理的综合排序框架。


<details>
  <summary>Details</summary>
Motivation: 在线审议平台日益普及，旨在增强民主参与，但提案排序方法缺乏原则性评估。当前排序方法选择随意且缺乏正当性，特别是常见的按点赞数排序存在明显缺陷。

Method: 1. 建立概念框架，对排序方法进行分类和比较，依据其目的和考虑的参数；2. 分析现有排序方法的随意性；3. 批判按点赞数排序的常见做法，提出应综合其他参数进行排序，并探讨更合适的排序方式。

Result: 研究发现：1. 建立了系统评估排序方法的框架；2. 揭示了当前排序方法选择的随意性；3. 证明按点赞数排序存在缺陷，需要与其他参数结合使用；4. 提出了即使在参数相同情况下，也有比单纯按点赞数更合适的排序方法。

Conclusion: 在线审议平台需要更原则性的排序方法评估框架，单纯按点赞数排序不足够，应综合考虑多种参数，并开发更符合审议平台民主参与目标的排序机制。

Abstract: Recent years have seen an increase in the use of online deliberation platforms (DPs). One of the main objectives of DPs is to enhance democratic participation, by allowing citizens to post, comment, and vote on policy proposals. But in what order should these proposals be listed? This paper makes a start with the principled evaluation of sorting methods on DPs. First, we introduce a conceptual framework that allows us to classify and compare sorting methods in terms of their purpose and the parameters they take into account. Second, we observe that the choice for a sorting method is often ad hoc and rarely justified. Third and last, we criticise sorting by number of approvals ('likes'), a method that is very common in practice. On the one hand, we show that if approvals are used for sorting, this should be done in an integrated way, also taking into account other parameters. On the other hand, we argue that even if proposals are on a par in terms of those other parameters, there are other, more appropriate ways to sort proposals in light of the approvals they have received.

</details>


### [284] [Work Design and Multidimensional AI Threat as Predictors of Workplace AI Adoption and Depth of Use](https://arxiv.org/abs/2602.23278)
*Aaron Reich,Diana Wolfe,Matt Price,Alice Choe,Fergus Kidd,Hannah Wagner*

Main category: cs.CY

TL;DR: 研究探讨工作设计特征与AI威胁感知如何共同预测职场AI采纳和使用深度，发现技能多样性和自主性对AI采纳有积极影响，而威胁感知对使用深度有差异化影响。


<details>
  <summary>Details</summary>
Motivation: 尽管AI工具在日常工作中日益普及，但员工采纳率在组织内部存在显著差异。研究旨在从社会技术和工作设计视角，探究激励性工作特征和多维AI威胁感知如何共同预测职场AI采纳和使用深度。

Method: 使用来自2,257名员工的横断面调查数据，测试了角色层级、工作年限和地区的群体差异，以及AI采纳和使用深度（频率和持续时间）的多变量预测因素。

Result: 工作设计（特别是技能多样性和自主性）与AI采纳呈现最一致的正相关关系。威胁维度对使用深度呈现差异化模式：感知工作变化与使用频率和持续时间正相关，而地位威胁与深度使用呈负相关但不总是显著。

Conclusion: 研究发现是相关性的（横断面和自我报告设计）。实践意义强调将AI赋能努力与工作设计相结合，并在采纳计划中监测潜在的工作量扩张。

Abstract: Artificial intelligence tools are increasingly embedded in everyday work, yet employees' uptake varies widely even within the same organization. Drawing on sociotechnical and work design perspectives, this research examines whether motivational job characteristics and multidimensional AI threat perceptions jointly predict workplace AI adoption and depth of use. Using cross-sectional survey data from 2,257 employees, we tested group differences across role level, years of experience, and region, along with multivariable predictors of AI adoption and use depth, specifically frequency and duration. Across models, job design, especially skill variety and autonomy, showed the most consistent positive associations with AI adoption, whereas threat dimensions exhibited differentiated patterns for depth of use. Perceived changes in work were positively associated with frequency and duration, while status threat showed a negative but not consistently significant relationship with deeper use. Findings are correlational given the cross-sectional and self-report design. Practical implications emphasize aligning AI enablement efforts with work design and monitoring potential workload expansion alongside adoption initiatives.

</details>


### [285] [Safety First: Psychological Safety as the Key to AI Transformation](https://arxiv.org/abs/2602.23279)
*Aaron Reich,Diana Wolfe,Matt Price,Alice Choe,Fergus Kidd,Hannah Wagner*

Main category: cs.CY

TL;DR: 心理安全是AI工具初始采纳的关键预测因素，但不影响采纳后的使用频率和时长，且这一关系在不同经验水平、角色层级和地区间保持一致。


<details>
  <summary>Details</summary>
Motivation: 尽管组织持续投资人工智能，但许多组织难以确保员工采纳和使用这些工具。研究基于技术使用的人际和学习需求理论，探讨心理安全是否与工作场所的AI采纳和使用相关。

Method: 使用全球咨询公司2,257名员工的调查数据，通过逻辑回归和线性回归分析，检验心理安全是否与AI采纳、使用频率和使用时长相关，并考察这些关系是否因组织层级、专业经验或地理区域而异。

Result: 心理安全可靠地预测员工是否采纳AI工具，但不预测采纳后的使用频率和时长。心理安全与AI采纳的关系在不同经验水平、角色层级和地区间保持一致，未发现调节效应。

Conclusion: 心理安全是AI初始参与的关键前因，但不是后续使用强度的预测因素。研究强调需要区分采纳和持续使用，并为早期AI实施阶段的针对性组织干预提供了机会。

Abstract: Organizations continue to invest in artificial intelligence, yet many struggle to ensure that employees adopt and engage with these tools. Drawing on research highlighting the interpersonal and learning demands of technology use, this study examines whether psychological safety is associated with AI adoption and usage in the workplace. Using survey data from 2,257 employees in a global consulting firm, we test whether psychological safety is associated with adoption, usage frequency, and usage duration; and whether these relationships vary by organizational level, professional experience, or geographic region. Logistic and linear regression analyses show that psychological safety reliably predicts whether employees adopt AI tools but does not predict how often or how long they use AI once adoption has occurred. Moreover, the relationship between psychological safety and AI adoption is consistent across experience levels, role levels, and regions, and no moderation effects emerge. These findings suggest that psychological safety functions as a key antecedent of initial AI engagement but not of subsequent usage intensity. The study underscores the need to distinguish between adoption and sustained use and highlights opportunities for targeted organizational interventions in early-stage AI implementation.

</details>
