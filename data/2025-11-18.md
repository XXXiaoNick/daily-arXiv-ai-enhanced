<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 88]
- [econ.EM](#econ.EM) [Total: 6]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 82]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [math.OC](#math.OC) [Total: 29]
- [stat.ML](#stat.ML) [Total: 12]
- [cs.CY](#cs.CY) [Total: 34]
- [cs.LG](#cs.LG) [Total: 281]
- [eess.SY](#eess.SY) [Total: 41]
- [q-fin.ST](#q-fin.ST) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 提出了TimeStampEval基准测试，用于从长文本转录中检索非逐字引用的精确时间戳。通过两阶段方法显著提高检索准确率，同时降低90%以上的推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在处理语义相同但语法不同的引用时失败的问题，特别是在对齐官方书面记录和语音转文字转录稿时。应用场景是自动化长播客，将国会记录片段汇编成AI主持的叙述。

Method: 采用两阶段方法：首先使用RapidFuzz进行预过滤，然后使用LLM在短片段上进行验证。提示设计将查询放在转录稿之前并使用紧凑格式。

Result: 在2800句转录稿上的评估显示：提示设计比模型选择更重要；适度推理预算可将准确率从37%提升到77%以上；Assisted Fuzzy方法将模糊匹配准确率提高50个百分点，同时延迟减半，每个正确结果的成本降低96%。

Conclusion: 该方法对转录长度、词汇漂移和领域变化具有鲁棒性，在10个转录稿上保持95-100%的拒绝准确率，证明其在实际应用中的有效性。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [2] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一个开源研究代理，通过交互式扩展作为性能提升的第三维度，在四个基准测试中表现优异，超越先前开源代理并接近商业对应物。


<details>
  <summary>Details</summary>
Motivation: 探索模型层面的交互扩展，作为模型规模和上下文长度之外的第三个性能提升维度，以解决LLM测试时扩展在长推理链中可能出现的退化问题。

Method: 通过强化学习实现交互扩展，使用256K上下文窗口，每个任务最多可执行600次工具调用，支持持续多轮推理和复杂研究工作流。

Result: 在GAIA、HLE、BrowseComp和BrowseComp-ZH四个基准测试中，72B变体分别达到81.9%、37.7%、47.1%和55.6%的准确率，超越先前开源代理并接近GPT-5-high等商业对应物。

Conclusion: 交互深度展现出与模型规模和上下文长度类似的扩展行为，确立交互扩展作为构建下一代开源研究代理的第三个关键维度。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [3] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 该论文质疑语言模型是否真正具备推理能力，认为LM只是统计模式匹配器而非真正的推理者，其推理式输出源于统计规律而非逻辑机制。


<details>
  <summary>Details</summary>
Motivation: 评估当前NLP领域对语言模型推理能力的定义是否准确，揭示LM训练方式、信息处理机制与推理概念之间的不匹配。

Method: 将基于transformer的语言模型视为实现隐式有限阶马尔可夫核的统计系统，分析其如何从上下文映射到条件标记分布。

Result: 语言模型的推理式输出对应的是学习核中的统计规律和近似统计不变性，而非显式逻辑机制的实施，因此无法保证逻辑一致性。

Conclusion: 需要重新审视NLP研究中如何描述所构建系统的计算过程，区分统计模式匹配与真正推理的本质差异，这对评估LM的认知不确定性至关重要。

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [4] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 评估7个开源大语言模型(0.6B-70B参数)在水电许可文档信息提取中的性能与计算资源权衡，发现14B参数是有效性的关键阈值。


<details>
  <summary>Details</summary>
Motivation: 解决监管文档信息提取中性能与计算资源之间的关键权衡问题，为实际部署提供实证指导。

Method: 在水电许可文档上评估7个不同参数规模(0.6B-70B)的开源模型，分析验证方法的有效性。

Result: 发现14B参数阈值：低于此阈值时验证方法无效(F1<0.15)，高于此阈值时验证方法可行(F1=0.64)。消费级模型可达64% F1，小型模型上限为51%，大型模型接近77%但需企业基础设施。

Conclusion: 建立了首个监管环境下开源信息提取的资源-性能映射，为基于证据的模型选择提供指导，这些发现对水电合规具有直接价值，且参数缩放效应的见解可推广到其他信息提取任务。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [5] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 探索使用基于LLM的自动形式化方法来验证LLM生成输出的逻辑一致性，通过两个实验展示了该方法在一致性检查和形式验证方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏正式方法来验证LLM从自然语言生成的结构化输出（如Gherkin场景）的准确性，需要填补这一空白。

Method: 使用简单的基于LLM的自动形式化器，将非正式语句转换为形式逻辑，在两个实验中验证LLM生成输出与自然语言需求的逻辑一致性。

Result: 第一个实验成功识别了两个不同表述的自然语言需求在逻辑上等价；第二个实验识别了给定自然语言需求与LLM生成输出之间的逻辑不一致性。

Conclusion: 自动形式化在确保LLM生成输出的保真度和逻辑一致性方面具有显著潜力，为未来更广泛的研究奠定了基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [6] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 提出一个分析电影剧本情感弧线并进行角色上下文扩展分析的框架，使用基于NRC-VAD数据集的定制词典进行情感分析，并通过Ward层次聚类技术聚类相似情感情节。


<details>
  <summary>Details</summary>
Motivation: 故事理解和分析是自然语言理解中的挑战领域，需要深度计算语义表示和句法处理。大量叙事数据需要自动化语义分析和计算学习，而非手动分析方法。

Method: 使用基于NRC-VAD数据集Valence、Arousal和Dominance分数的定制词典进行词典式情感分析，应用LabMTsimple storylab模块，并通过Ward层次聚类技术聚类相似情感情节。

Result: 在电影数据集上的实验评估显示，该分析结果对消费者和读者在选择叙事或故事时很有帮助。

Conclusion: 该框架能够提取叙事传达的高层和低层概念，为电影剧本的情感分析提供了有效的方法。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [7] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 本文构建了一个包含6,393份放射学报告的标注数据集，用于评估大语言模型在随访依从性检测任务上的性能。通过系统比较传统机器学习模型和生成式LLMs，发现GPT-4o在优化提示下表现最佳，接近人类标注者水平。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门用于评估大语言模型在放射学任务性能的领域特定数据集，需要构建高质量标注语料库来支持随访依从性检测系统的开发和基准测试。

Method: 构建了6,393份放射学报告的标注语料库，系统比较了传统机器学习分类器（逻辑回归、支持向量机、Longformer）和生成式LLMs（GPT-4o、GPT-OSS-20B）。对生成式LLMs测试了基础配置和任务优化配置，后者聚焦于元数据、推荐语句及其上下文。

Result: GPT-4o（优化配置）表现最佳（F1=0.832），GPT-OSS-20B（优化配置）紧随其后（F1=0.828）。逻辑回归和支持向量机也表现强劲（F1=0.776和0.775）。人类标注者间一致性很高（F1=0.846）。

Conclusion: 虽然通过提示优化，大语言模型能够接近人类水平的标注一致性，但可解释且资源效率高的传统模型仍然是有价值的基准。LLMs在放射学随访依从性检测任务上展现出显著潜力。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [8] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MedPT是首个针对巴西葡萄牙语的大规模真实医患对话语料库，包含38.4万条问答对，通过多阶段筛选和LLM标注增强，在医疗专科分类任务中达到94%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM在医疗领域主要关注高资源语言的问题，填补葡萄牙语医疗数据的空白，因为简单翻译无法捕捉临床和文化特异性（如地方性疾病）。

Method: 构建38.4万条真实医患问答对，采用混合定量-定性分析进行噪声过滤和上下文增强，使用LLM驱动标注将问题分类为7种语义类型，并分析其主题广度和语言特性。

Result: 数据集涵盖3,200个主题，在医疗专科路由任务中，微调1.7B参数模型在20类分类上达到94%的F1分数，错误分析显示误分类反映了真实的临床模糊性。

Conclusion: MedPT数据集公开释放，旨在促进葡萄牙语世界更公平、准确和文化敏感的医疗技术发展。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [9] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: ClinStructor使用LLM将临床自由文本转换为结构化问答对，以解决临床笔记中的偏见、泛化性和可解释性问题，在ICU死亡率预测任务中仅导致AUC轻微下降2-3%。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含有价值信息但存在非结构化格式带来的挑战，包括无意偏见（如性别或种族偏见）、跨临床环境泛化性差（如不同EHR系统间模型性能差异）和可解释性差。

Method: 利用大语言模型将临床自由文本转换为结构化的、任务特定的问答对，作为预测建模的前置步骤。

Result: 该方法显著提高了透明度和可控性，在ICU死亡率预测任务中与直接微调相比仅导致预测性能轻微下降（AUC下降2-3%）。

Conclusion: ClinStructor为在临床环境中构建可靠、可解释和可泛化的机器学习模型奠定了坚实基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [10] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 通过监督微调和强化学习增强GPT-2的心理治疗对话能力，显著提升了情感准确性和专业响应质量


<details>
  <summary>Details</summary>
Motivation: COVID-19加剧了心理健康服务的可及性问题，预训练语言模型缺乏情境和情感意识，需要开发更专业的治疗对话系统

Method: 重构输入格式以同时处理情境信息和情感状态，采用多组件奖励函数使模型输出与专业治疗师响应和标注情感对齐

Result: 强化学习在多个评估指标上优于基线GPT-2，情感准确率达到99.34%（基线为66.96%），LLM评估确认高情境相关性和专业性

Conclusion: 强化学习能有效开发治疗对话系统，可作为治疗师的有价值辅助工具，同时保持必要的人类临床监督

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [11] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: CALM是一个可解释的临床文本分类框架，通过将预测分解为各语义组件的加性贡献，提供透明的风险信号解释。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在临床文本分类中预测不透明的问题，让研究人员和医生能够理解患者记录中哪些部分驱动风险信号。

Method: 采用加性大语言模型框架，将半结构化文本输入分解为语义组件，预测结果是各组件贡献的加性总和。

Result: CALM在性能上与常规LLM分类器相当，同时提高了可信度，支持质量保证检查，并在模型开发和审计中揭示临床有意义的模式。

Conclusion: CALM框架在保持高性能的同时提供了可解释性，有助于临床实践中的信任建立和质量控制。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [12] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 提出了InData数据集来评估LLMs在多步骤工具推理能力，发现当前模型在复杂数据分析任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 解决LLM直接生成代码访问敏感数据的安全风险，通过预定义安全工具来限制数据访问。

Method: 设计InData数据集，包含三个难度级别的数据分析问题，评估15个开源LLM的多步骤工具推理能力。

Result: 大型模型在简单任务上表现良好（97.3%），但在困难任务上性能显著下降（69.6%）。

Conclusion: 当前LLM缺乏稳健的多步骤工具推理能力，InData数据集有助于开发和评估更强的工具使用能力。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [13] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 提出了LLM-KAT评估方法和实体匿名化技术，以提升大语言模型在基于知识图谱对话生成任务中对外部知识的利用能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱对话生成任务中往往过度依赖内部知识，而忽视提供的外部知识图谱，导致生成的回答与外部知识脱节。

Method: 引入LLM-KAT评估程序来衡量生成回答中的知识附着度，并提出简单的实体匿名化技术来鼓励LLM更好地利用外部知识。

Result: 在OpenDialKG数据集上的实验表明，该方法有效提升了LLM对外部知识的附着度。

Conclusion: 实体匿名化是一种简单有效的技术，能够显著改善大语言模型在知识图谱对话生成任务中对外部知识的利用效果。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [14] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 该论文研究了语言模型的熵校准问题，发现模型存在校准错误，且随着模型规模增大，校准问题改善缓慢。理论分析和实证研究都表明，即使模型规模扩大，误差累积率也基本保持不变。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型的熵校准问题，探索模型规模扩大是否能改善校准错误，以及是否存在无需权衡的校准方法。

Method: 首先在简化理论设置中分析校准错误随数据集规模的缩放行为，然后在0.5B到70B参数的语言模型中进行实证测量。

Result: 发现校准错误的缩放指数接近0，意味着更大模型与更小模型以相似速率累积误差。理论上证明了在假设可以预测文本未来熵的情况下，可以在不增加对数损失的情况下减少熵。

Conclusion: 模型规模的扩大并不能显著改善熵校准问题，标准截断方法存在权衡，但理论上存在无需权衡的校准可能性。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [15] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 提出ReasoningNER框架，通过显式推理机制改进NER任务，在零样本场景下超越GPT-4达12.3个百分点


<details>
  <summary>Details</summary>
Motivation: 传统生成式LLM在NER任务中依赖隐式模式匹配，缺乏可验证的推理机制，导致性能不佳和泛化脆弱，特别是在零样本和低资源场景下

Method: 三阶段推理框架：1)生成NER导向的思维链数据集；2)使用CoT调优模型生成推理链；3)推理增强阶段使用综合奖励信号优化推理过程

Result: 在零样本设置下达到SOTA性能，F1分数比GPT-4高出12.3个百分点，展示了在推理导向信息提取研究中的巨大潜力

Conclusion: ReasoningNER通过从隐式模式匹配转向显式推理，显著提升了NER任务的认知能力和性能，特别是在资源受限场景下

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [16] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 研究表明CoT解释在道德场景中具有双刃剑作用：既能提升透明度，也能通过确认偏见误导用户，特别是当解释采用自信语气时，会抑制错误检测并维持用户依赖。


<details>
  <summary>Details</summary>
Motivation: 探讨解释在促进透明度与可能引发确认偏见之间的双重作用，特别是在多模态道德场景中，研究CoT解释如何影响用户信任和错误检测能力。

Method: 通过系统性地扰动推理链和操纵交付语气，分析视觉语言模型中的推理错误及其对用户信任和错误检测能力的影响。

Result: 发现两个关键效应：(1)用户常将信任等同于结果一致性，即使推理有缺陷仍保持依赖；(2)自信语气会抑制错误检测同时维持依赖，表明交付风格可以凌驾于正确性之上。

Conclusion: CoT解释既能澄清也能误导，强调NLP系统需要提供鼓励审查和批判性思维而非盲目信任的解释。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [17] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 本文提出了一个评估大语言模型文化能力的新基准，通过真实情境上下文来测试文化推理能力，并引入四个补充指标来全面评估响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文化能力评估方法局限于脱离语境的正确性或二选一判断，忽视了文化理解和推理的需求，无法准确评估模型在多元文化环境中的表现。

Method: 引入基于真实情境上下文的评估基准，除了标准精确匹配指标外，还增加了覆盖率、特异性、内涵和连贯性四个补充指标，进行厚评估分析。

Result: 实证分析显示，薄评估会系统性高估文化能力且评估结果不稳定、方差大；而厚评估能揭示推理深度差异，减少方差，提供更稳定、可解释的文化理解信号。

Conclusion: 厚评估方法比传统薄评估能更准确地衡量大语言模型的文化能力，为模型在多元文化环境中的部署提供了更可靠的评估框架。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [18] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 结合反向翻译和微调在小规模日语语料上显著提升英日神经机器翻译质量，COMET分数从0.460提升至0.597。


<details>
  <summary>Details</summary>
Motivation: 探索在有限训练数据下，如何通过反向翻译和微调的组合策略提升低资源语言对的机器翻译质量。

Method: 首先使用反向翻译生成合成数据，然后在真实小规模平行语料上进行微调，最后将两种方法结合使用。

Result: 单独反向翻译使COMET从0.460提升至0.468；单独微调提升至0.589；两者结合达到0.597的最佳效果。

Conclusion: 反向翻译和针对性微调的协同使用能够显著提升翻译质量，为低资源语言对提供轻量级但强大的改进策略。

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [19] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench是一个用于评估大语言模型训练数据时间边界的基准测试，通过检测模型对近期事件的了解程度来识别其知识的新鲜度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定时间点前的文本数据上进行预训练，这形成了严格的知识边界。当这个限制未知或被忽视时，模型可能在推理任务中无意间混合过时的时效性信息与一般知识，从而影响回答的准确性。

Method: 引入LLMLagBench作为系统性方法，通过评估模型对近期事件的知识来识别其训练数据的最早可能时间边界。将该基准应用于评估大量LLM，包括具有明确声明和未声明训练截止日期的模型。

Result: 通过手动验证和与公开发布的LLM预训练信息进行比较，评估了该基准的可靠性。

Conclusion: LLMLagBench提供了一个有效的方法来识别LLM的知识时间边界，有助于理解模型的知识新鲜度和潜在的信息混合问题。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [20] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出了U-MStance数据集和PRISM模型，解决了多模态对话立场检测中的伪多模态和用户同质性问题，通过用户画像和多模态对齐显著提升了立场检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究存在伪多模态（源帖子有视觉线索但评论仅文本）和用户同质化（忽略个人特征）的局限性，无法反映真实世界多模态交互。

Method: PRISM模型：1）从历史帖子推导用户画像；2）通过思维链对齐文本和视觉线索；3）使用互任务强化机制联合优化立场检测和立场感知响应生成。

Result: 在U-MStance数据集上的实验表明，PRISM相比强基线模型取得了显著性能提升。

Conclusion: 用户中心化和上下文基础的多模态推理对于现实立场理解具有有效性。

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [21] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 提出了AI-Salesman框架，通过双阶段架构解决目标驱动说服对话中的多轮规划和事实忠实性问题，在真实销售数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 目标驱动说服对话（如电话营销）需要复杂的多轮规划和严格的事实忠实性，现有LLMs存在策略脆弱性和事实幻觉问题，且缺乏特定任务数据。

Method: 构建TeleSalesCorpus数据集；提出双阶段架构：训练阶段使用贝叶斯监督强化学习从噪声对话中学习稳健销售策略，推理阶段使用动态大纲引导代理(DOGA)结合预建脚本库提供动态策略指导。

Result: 实验结果表明，AI-Salesman在自动指标和全面人工评估中均显著优于基线模型，在复杂说服场景中表现出色。

Conclusion: AI-Salesman框架通过创新的双阶段设计和动态大纲引导机制，有效解决了目标驱动说服对话中的关键挑战，为相关应用提供了有力解决方案。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [22] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: VBackChecker是一个无需参考的幻觉检测框架，通过像素级Grounding LLM验证MLLM生成响应与视觉输入的一致性，在R²-HalBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在严重幻觉问题，需要准确检测以确保实际应用可靠性。

Method: 采用"眼见为实"原则，设计基于像素级Grounding LLM的参考免费检测框架，包含推理和参考分割能力，并构建R-Instruct指令调优数据生成流程。

Result: 在R²-HalBench基准上超越先前复杂框架，性能媲美GPT-4o，像素级定位任务提升超10%。

Conclusion: VBackChecker为MLLM幻觉检测提供了有效且可解释的解决方案，在真实世界丰富场景中表现出色。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [23] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: CriticSearch是一个细粒度信用分配框架，通过回顾性批评机制为搜索代理提供密集的回合级反馈，解决了传统强化学习方法中稀疏奖励导致的低效探索和不稳定训练问题。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理管道通常依赖强化学习优化，但存在稀疏结果奖励问题，导致探索效率低下和训练不稳定。

Method: 使用冻结的非对称批评LLM，基于完整轨迹和黄金答案的权限信息回顾性评估每个回合，将这些评估转化为稳定的密集奖励来指导策略改进。

Result: 在多样化多跳推理基准测试中，CriticSearch始终优于现有基线方法，实现了更快的收敛速度、更好的训练稳定性和更高的性能。

Conclusion: CriticSearch通过密集的回合级反馈机制有效提升了搜索代理在复杂问答任务中的表现，解决了传统强化学习方法的局限性。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [24] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: MME-RAG是一个多管理器-专家检索增强生成框架，通过将实体识别分解为类型级判断和跨度级提取两个协调阶段，解决了LLM在领域适应和检索可控性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在面向任务的对话中，在领域适应和检索可控性方面仍面临挑战，特别是在细粒度实体识别任务上。

Method: 提出MME-RAG框架，包含轻量级管理器进行类型级判断，专业专家进行跨度级提取，每个专家配备KeyInfo检索器注入语义对齐的少样本示例。

Result: 在CrossNER、MIT-Movie、MIT-Restaurant和新构建的多领域客服数据集上，MME-RAG在大多数领域表现优于现有基线方法。

Conclusion: 层次分解和KeyInfo引导的检索是鲁棒性和跨领域泛化的关键驱动因素，MME-RAG为自适应对话理解提供了可扩展且可解释的解决方案。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [25] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK是一种高效的幻觉检测方法，通过检查生成文本中事实探针回答的一致性来检测幻觉，无需外部知识库且资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成文本时经常产生事实错误的幻觉，在医疗、金融等领域存在严重风险。现有方法在模型访问受限时通常需要多次API调用，增加了延迟和成本。

Method: 基于事实探针一致性检测：在单个LLM内部和不同LLM之间检查对事实探针的回答是否一致，无需外部知识库。

Result: 在多个数据集上的实验表明，CONFACTCHECK能以更少资源高效检测幻觉事实，在相似条件下比现有基线方法获得更高的准确率。

Conclusion: CONFACTCHECK提供了一种资源高效的幻觉检测解决方案，特别适用于模型访问受限的场景，能够有效识别事实不一致的生成内容。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [26] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 提出了ViConBERT框架，结合对比学习和基于词义解释的蒸馏来学习越南语上下文嵌入，并创建了ViConWSD数据集用于评估越南语语义理解。


<details>
  <summary>Details</summary>
Motivation: 越南语缺乏强大的语义理解模型和评估资源，而现有的上下文词嵌入进展主要集中在高资源语言如英语上。

Method: 使用对比学习(SimCLR)和基于词义解释的蒸馏来学习越南语上下文嵌入，并构建了大规模合成数据集ViConWSD。

Result: ViConBERT在WSD任务上F1得分为0.87，在ViCon数据集上AP为0.88，在ViSim-400数据集上Spearman's rho为0.60，均优于基线模型。

Conclusion: ViConBERT框架在建模离散词义和分级语义关系方面表现有效，为越南语语义理解提供了新的解决方案。

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [27] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出使用小型LLM压缩大型LLM输入的提示压缩范式，开发了Cmprsr模型，在保持语义信息和遵循压缩率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 降低使用黑盒大型语言模型的高成本，通过压缩输入来减少计算开销。

Method: 使用25个开源和闭源模型建立基准测试，通过Textgrad优化压缩元提示，对Qwen3-4B进行SFT和GRPO后训练。

Result: Cmprsr在MeetingBank、LongBench和GSM8k数据集上优于提取式和普通抽象压缩方法，能精确控制压缩率。

Conclusion: Cmprsr模型在压缩能力和压缩率控制方面表现优异，具有跨长度和领域的泛化能力。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [28] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 本文提出了一种利用现有抽象摘要生成提取式摘要的轻量级方法，旨在为法律文档自动摘要研究提供增强的数据资源。


<details>
  <summary>Details</summary>
Motivation: 法律判决摘要对法律从业者构成沉重认知负担，而深度神经方法生成的抽象摘要容易误判法律术语或忽略关键细节，因此需要提取式摘要方法。人工标注提取式摘要成本高昂，需要自动化解决方案。

Method: 设计了一个轻量透明的流程，利用现有的抽象标准摘要生成相应的提取式标准摘要版本，确保专家意见从原始抽象摘要传递到转换后的提取摘要中。

Result: 计划增强七个现有的案例摘要数据集，通过添加相应的提取摘要来创建丰富的案例摘要研究资源。进行了结构、词汇和语义维度的广泛比较评估，确保增强的提取摘要质量。

Conclusion: 承诺公开发布增强的数据集，相信这一资源将推动法律文档自动摘要领域的发展。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [29] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: LLMs在日语问答中的表现与人类不同，对于维基百科未覆盖的答案和需要数值回答的问题表现较差


<details>
  <summary>Details</summary>
Motivation: 研究LLMs和人类在问答难度上的差异，探讨对人类困难的问题是否对LLMs也同样困难

Method: 收集日语问答数据，包括问题、答案和人类正确率，在不同设置下让LLMs回答问题，并与人类表现进行比较分析

Result: 相比人类，LLMs在维基百科未覆盖答案的问题上表现更差，在需要数值回答的问题上也有困难

Conclusion: LLMs和人类在问答难度上存在差异，LLMs更依赖维基百科知识，且在数值推理方面存在弱点

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [30] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型中的"讽刺反弹"现象——否定指令反而会增加被禁止概念的可及性，并通过两个实验和电路追踪分析揭示了反弹机制。


<details>
  <summary>Details</summary>
Motivation: 否定指令在人类思维中会产生讽刺反弹现象，本研究旨在验证大型语言模型是否也存在类似现象，并探索其内在机制。

Method: 通过两个实验：(1) 改变干扰文本类型（语义、句法、重复）测量反弹强度；(2) 测试模型对中性vs负面框架的区分能力；并辅以电路追踪分析识别关键注意力头。

Result: 否定后立即出现反弹，较长或语义干扰会加剧反弹，而重复有助于抑制；更强的极性区分与更持久的反弹相关；电路分析发现中间层注意力头会放大被禁标记。

Conclusion: 研究将认知预测与机制洞察联系起来，揭示了长上下文干扰中的讽刺反弹现象，并发布了ReboundBench数据集支持未来研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [31] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: ILAKKANAM是首个泰米尔语专用语言评估基准，包含820道来自斯里兰卡学校考试的手工标注问题，评估显示LLMs在低年级表现良好但随语言复杂度增加而下降。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准依赖英语翻译数据集，无法捕捉泰米尔语等低资源、形态丰富语言的语言文化细微差别。

Method: 使用820道斯里兰卡学校泰米尔语考试问题构建基准，由训练有素的语言学家在五个语言类别和一个事实知识类别下标注，涵盖1-13年级。

Result: Gemini 2.5表现最佳，开源模型落后；所有模型在低年级问题表现良好，但随语言复杂度增加明显下降；模型表现与识别语言类别能力无强相关性。

Conclusion: LLMs在泰米尔语中的表现可能更多基于接触而非真正理解，需要专门针对低资源语言的基准来评估其真实语言能力。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [32] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 提出了MRMBench基准和推理时探测方法，用于评估奖励模型在多维度偏好上的表现，发现该方法与LLM对齐性能强相关，揭示了奖励模型在多维度偏好捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估方法通常在固定成对排序测试集上进行，但无法提供各偏好维度的性能信息，需要更细粒度的评估方法。

Method: 构建了MRMBench基准，包含6个不同偏好维度的探测任务；提出了推理时探测方法，识别奖励预测时使用的维度并增强可解释性。

Result: MRMBench与大型语言模型的对齐性能强相关；奖励模型在多维度偏好捕捉上表现不佳；推理时探测方法能可靠评估奖励预测置信度。

Conclusion: MRMBench是开发先进奖励模型的可靠参考；多目标优化在奖励建模中具有潜力；推理时探测方法能提升LLM对齐效果。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [33] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 提出了SerenQA框架，用于评估LLM在科学知识图谱问答中发现意外洞察的能力，重点关注药物重定位任务。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答系统通常只返回高度相关但可预测的答案，缺乏发现意外和新颖（"意外发现"）答案的能力。

Method: 提出SerenQA框架，包括基于相关性、新颖性和意外性的严格意外发现指标，以及从临床知识图谱构建的专家标注基准，包含知识检索、子图推理和意外发现探索三个子任务的结构化评估流程。

Result: 实验表明，最先进的LLM在检索任务上表现良好，但在识别真正令人意外和有价值的发现方面仍有困难。

Conclusion: LLM在发现意外洞察方面仍有显著改进空间，为未来研究指明了方向。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [34] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是一个轻量级的大语言模型安全护栏系统，包含ContentFilter和JailbreakFilter两个专门模型，用于检测有害内容和筛选对抗性提示，支持12种语言，基于2B参数的Granite模型构建。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在人类-AI对话场景中的安全风险，包括有害内容检测和对抗性提示攻击防护的需求。

Method: 基于2B参数的Granite-3.3-2B-Instruct模型，通过指令调优构建两个专门组件：ContentFilter基于MLCommons危害分类学识别安全风险，JailbreakFilter通过精心设计的课程学习覆盖60种主要攻击类型。

Result: 在公共和专有安全基准测试中实现了最先进的安全性能，同时保持轻量级部署，提供多类安全预测和二进制置信度分数以提高可解释性。

Conclusion: SGuard-v1是一个高效、轻量级的大语言模型安全解决方案，在Apache-2.0许可下发布，支持进一步研究和实际部署。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [35] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: QA-Noun是一个基于问答的框架，专门用于捕捉名词为中心的语义关系，通过9个问题模板覆盖名词的显式句法和隐式上下文角色，与QA-SRL结合实现句子意义的统一分解。


<details>
  <summary>Details</summary>
Motivation: 现有的基于QA的语义方法主要关注谓词-论元关系，但名词为中心的语义关系在很大程度上未被解决，需要开发一个框架来捕捉名词的语义角色。

Method: 定义9个问题模板来覆盖名词的显式句法和隐式上下文角色，创建可解释的QA对，与QA-SRL集成，构建包含2000多个标注名词提及的数据集和训练模型。

Result: QA-Noun几乎完全覆盖了AMR的名词论元，同时揭示了额外的上下文隐含关系，与QA-SRL结合比FactScore和DecompScore等基于事实的分解方法粒度提高了130%以上。

Conclusion: QA-Noun补充了基于QA的语义框架，形成了全面且可扩展的细粒度语义分解方法，适用于跨文本对齐。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [36] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG是一个新颖的检索增强生成框架，通过任务自适应知识图谱构建解决传统RAG中信息丢失和无关细节的问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法将外部知识截断为小块，导致信息丢失、响应幻觉和推理链断裂，且检索的非结构化知识包含无关细节，影响准确推理。

Method: 提出意图驱动的路由机制到领域特定提取模板，结合监督微调和基于强化学习的隐式提取机制，确保简洁、连贯、非冗余的知识集成。

Result: 在六个公共基准测试和真实商业基准测试（NowNewsQA）上，使用三种骨干模型进行评估，TAdaRAG在多个领域和长文本任务中优于现有方法。

Conclusion: TAdaRAG展现了强大的泛化能力和实际有效性，能够有效解决传统RAG的信息丢失和无关细节问题。

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [37] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 提出一种因果框架来分析和缓解RLHF奖励模型中的长度偏差，通过反事实数据增强方法训练奖励模型，使其能够独立于冗长度评估内容质量。


<details>
  <summary>Details</summary>
Motivation: RLHF训练的奖励模型存在长度偏差，倾向于将冗长度与质量混淆，偏好更长的回答，这影响了模型输出的质量和简洁性。

Method: 使用反事实数据增强方法构建两种类型的响应对：(1)内容相似但长度不同的长度分歧对，(2)长度相似但内容不同的内容分歧对，并用这些数据训练奖励模型。

Result: 实证评估表明该方法减少了奖励分配中的长度偏差，使策略模型产生更简洁、内容聚焦的输出。

Conclusion: 所提出的方法有效减少了长度偏差，提高了RLHF管道中奖励建模的鲁棒性和内容敏感性。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [38] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: 本文提出了MMWOZ多模态对话数据集，通过扩展MultiWOZ 2.3数据集，将传统的任务导向对话系统与图形用户界面(GUI)结合，并提出了MATE多模态模型作为基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统任务导向对话系统依赖定制后端API，而现实场景中广泛存在前端GUI且缺乏定制API，这造成了实际应用中的显著差距。

Method: 开发网页风格GUI作为前端，设计自动化脚本将对话状态和系统动作转换为GUI操作指令，收集网页快照和对应操作指令，并提出MATE多模态模型。

Result: 构建了MMWOZ多模态对话数据集，提出了MATE基线模型，并进行了全面的实验分析。

Conclusion: 该研究为构建实用的多模态任务导向对话代理提供了数据集和基线模型，填补了传统系统与现实应用之间的差距。

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [39] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: GAPO是一种基于GRPO的改进方法，通过计算群体级别的奖励来鼓励LLM生成更多样化的响应，解决了模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现模式崩溃问题，即使存在多个有效答案，也重复生成相同的少数补全，限制了任务响应的多样性。

Method: GAPO是GRPO的简单扩展，计算群体整体奖励，使用频率感知奖励函数鼓励在有效补全上均匀采样。

Result: GAPO训练的模型能产生有效且更多样化的响应，在开放提示下也能提高响应多样性，同时不影响标准基准测试的准确性。

Conclusion: GAPO通过群体感知策略优化有效解决了LLM的模式崩溃问题，提高了响应多样性且保持准确性。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [40] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0是一个全开源的跨模态大模型，基于Qwen2.5-7B架构构建，通过动态容量MoE设计、渐进式训练策略和跨模态数据匹配技术，实现了语言、图像、语音等多种模态的理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 推进Lychee Uni-MoE系列在语言为中心的跨模态理解、推理和生成方面的能力，构建一个能够处理多种模态输入输出的全开源模型。

Method: 采用动态容量MoE架构，包含共享、路由和空专家；使用Omni-Modality 3D RoPE确保跨模态时空对齐；采用渐进式监督微调策略，结合GSPO-DPO方法稳定强化学习训练；在75B token的多模态数据上训练，配备专门的语音和图像生成token。

Result: 在85个基准测试中表现优异，超过Qwen2.5-Omni在76个基准中的50多个，视频理解平均提升7%，跨模态理解平均提升7%，视听推理提升4%，长语音处理WER降低4.2%，在低级图像处理和可控生成方面领先。

Conclusion: Uni-MoE 2.0在计算效率和能力之间取得了良好平衡，在多个跨模态任务上达到或接近SOTA性能，证明了其架构和训练策略的有效性。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [41] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 通过系统算法审计发现，Google搜索中的AI概览和精选摘要功能在婴儿护理和孕期相关查询中存在信息不一致、缺乏医疗安全保障等质量问题。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成内容在健康信息搜索中的质量和可靠性，特别是用户无法控制展示方式的AI概览和精选摘要功能。

Method: 对1,508个真实婴儿护理和孕期相关查询进行系统算法审计，使用包含答案一致性、相关性、医疗安全保障、来源类别和情感对齐的多维度质量评估框架。

Result: 33%的搜索结果页面中AI概览和精选摘要信息不一致；医疗安全保障严重缺乏（AI概览仅11%，精选摘要仅7%）；健康网站是主要来源，但精选摘要也常链接商业来源。

Conclusion: AI介导的健康信息需要更强的质量控制，该审计方法可为高风险领域AI系统评估提供可转移框架。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [42] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本文提出了NOTAM语义解析任务，构建了Knots数据集，并通过多智能体协作框架和多种提示工程策略显著提升了航空文本的理解和处理能力。


<details>
  <summary>Details</summary>
Motivation: NOTAMs作为飞行安全信息的关键传播渠道，其复杂的语言结构和隐含推理给自动化解析带来了巨大挑战。现有研究主要关注分类和命名实体识别等表层任务，缺乏深层次的语义理解。

Method: 提出NOTAM语义解析任务，构建包含12,347条专家标注NOTAMs的Knots数据集，覆盖194个飞行情报区，采用多智能体协作框架进行全面的字段发现，并系统评估多种提示工程策略和模型适应技术。

Result: 实验结果表明，所提出的方法在航空文本理解和处理方面取得了显著改进，为自动化NOTAM分析系统提供了有价值的见解。

Conclusion: 该方法有效解决了NOTAM语义解析的挑战，通过结合领域知识和语义推理，能够生成结构化的、富含推理信息的输出，为航空安全信息的自动化处理提供了重要支持。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [43] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: Reason-KE++ 是一个 SFT+RL 框架，通过过程级监督解决大语言模型在多跳推理任务中的忠实性问题，在 MQUAKE-CF-3k 上达到 95.48% 的新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有 SFT 方法存在"忠实性差距"，模型仅模仿格式而非进行合理推理，导致参数先验覆盖上下文事实，产生关键事实幻觉。

Method: 提出 Reason-KE++ 框架，包含阶段感知奖励机制，为中间推理步骤（如分解、子答案正确性）提供密集监督，避免仅依赖结果奖励的陷阱。

Result: 在 MQUAKE-CF-3k 上达到 95.48% 准确率，相比之前方法提升 5.28%，同时保持推理完整性。

Conclusion: 对于复杂任务，对齐推理过程对于构建可信赖的 LLM 至关重要，过程级监督是解决忠实性问题的关键。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [44] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 提出了一种用于波斯语到英语的直接语音翻译系统，通过自监督预训练、离散语音单元和合成平行数据来解决低资源语言的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 直接语音翻译系统需要大量平行语音数据，但对于波斯语等低资源语言来说，这类数据非常稀缺，因此需要开发有效的数据增强方法。

Method: 系统包含三个组件：基于conformer的编码器、因果transformer解码器和基于单元的神经声码器。通过使用大语言模型翻译波斯语转录文本，并用零样本文本转语音系统合成英语语音，构建了合成平行语音语料库。

Result: 在CVSS语料库的波斯语-英语部分，使用合成数据比直接基线提高了4.6 ASR BLEU，可用平行语音数据量增加了约6倍。

Conclusion: 结合自监督预训练、离散语音单元和合成平行数据的方法对于改善波斯语-英语等低资源语言对的直接语音翻译是有效的。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [45] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: EvoSynth是一个自主的进化合成框架，通过多智能体系统自主设计、进化和执行基于代码的新型攻击算法，突破了传统自动化红队框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM自动化红队框架存在根本性限制：其越狱逻辑仅限于选择、组合或改进已有的攻击策略，无法自主发明全新的攻击机制，限制了其创造力。

Method: 采用多智能体系统自主设计基于代码的攻击算法，通过代码级自校正循环迭代重写攻击逻辑，实现从攻击规划到进化合成的范式转变。

Result: 在高度鲁棒的Claude-Sonnet-4.5模型上实现了85.5%的攻击成功率，生成的攻击比现有方法更加多样化。

Conclusion: EvoSynth开创了越狱方法进化合成的新方向，为未来研究提供了新的框架和思路。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [46] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: AFM是一种动态上下文管理器，通过三种保真度级别智能管理对话历史，在保持安全性能的同时将平均token使用量减少66%。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在多轮对话中固定上下文窗口和简单内存策略的瓶颈问题，避免完整回放的高成本或静态摘要导致的安全关键信息丢失。

Method: 基于语义相似度、半衰期权重和重要性分类，为每条历史消息分配FULL、COMPRESSED或PLACEHOLDER保真度级别，在严格token预算下按时间顺序打包消息。

Result: 在涉及花生过敏用户的旅行规划安全基准测试中，AFM在短中长对话中均能保留过敏信息，安全性能与完整回放相当，平均token使用量减少66%。

Conclusion: AFM提供模块化Python实现，可在不牺牲安全性和事实连续性的前提下显著降低推理成本。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [47] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: LLMs在复杂推理任务上表现超人类，但在简单集合成员查询任务中却频繁失败，显示出其推理能力的脆弱性和不可预测性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在简单推理任务中表现不佳的悖论，通过简单且可扩展的实验设计来揭示基本失败模式。

Method: 使用集合成员查询任务，系统评估提示措辞、语义结构、元素排序和模型选择等维度，进行大规模实证分析。

Result: LLMs在这一基础任务上的表现始终脆弱，在所有维度上都不可预测，表明模型对集合概念的理解是碎片化和复杂的。

Conclusion: 通过简单问题的大规模实验能够全面映射和分析失败模式，这为LLM评估提供了一种有价值的方法论。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [48] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 研究发现语言模型训练中存在相变重组现象，即使在小规模模型中也能观察到，且可通过词汇统计方法在线性训练空间中直接检测到，这为理解语言模型训练的非线性动态提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 探究相变现象是否只存在于大型语言模型，能否在线性训练空间中直接检测，以及是否在训练早期就出现，以理解语言模型训练中的非线性动态。

Method: 训练小型GPT风格transformer模型，分析词汇使用演变，追踪平均词长、正确/错误词数、词汇多样性变化，并应用泊松和亚泊松统计量化词汇连接和重组。

Result: 在训练过程中发现明显的相变点，这些转变在标准损失曲线中不明显，但通过词汇和统计探针可见，表明相变重组是语言模型训练的普遍特征。

Conclusion: 相变重组是语言模型训练的一般特征，可在小型模型中观察到，能在线性训练空间中直接检测，并在训练早期就出现，强调了定制化指标对于揭示相变行为的重要性。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [49] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 提出使用中断机制来增强大语言模型的对齐性，通过在用户输入中定期插入控制语句来防止越狱行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐研究主要关注通过训练和提示来提高模型鲁棒性，但缺乏随着用户输入长度增加而增强对齐的方法。研究发现LLM越狱概率随用户输入长度增加而上升。

Method: 提出中断机制，在用户输入中每x个token插入控制语句，并建议可将此方法推广到思维链过程以防止策略性行为。

Result: 论文提出了中断作为增强LLM对齐性的潜在解决方案，但未提供具体实验结果。

Conclusion: 中断机制是一种有前景的方法，可以随着用户输入长度增加而增强模型对齐性，防止越狱和策略性行为。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [50] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 评估大语言模型在自动形式化任务中对语义相似但表述不同的自然语言输入的鲁棒性，发现模型输出对表述变化敏感。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明LLMs在文本到SQL任务中对语义保持的改写输入敏感，本文旨在验证这一现象在自动形式化领域是否同样存在。

Method: 使用MiniF2F和Lean 4版本的ProofNet基准，生成语义相似的改写自然语言陈述，并在两个现代LLMs上进行交叉评估，测量语义和编译有效性。

Result: 结果显示模型在改写输入上的性能存在变异性，自然语言陈述的微小变化会显著影响模型输出。

Conclusion: LLMs在自动形式化任务中对自然语言表述变化敏感，需要提高模型对语义相似输入的鲁棒性。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [51] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: BioMedJImpact是一个大规模生物医学期刊数据集，整合了文献计量指标、合作特征和LLM衍生的AI参与度指标，用于分析合作强度和AI参与度如何共同影响科学影响力。


<details>
  <summary>Details</summary>
Motivation: 现有开放资源很少捕捉合作结构和AI研究如何共同塑造生物医学期刊声望，需要开发一个综合数据集来推进期刊层面的科学影响力和AI参与度分析。

Method: 从PubMed Central的174万篇文章构建数据集，提出可复现的三阶段LLM流程提取AI参与度特征，分析合作强度和AI参与度在疫情前后期间对科学影响力的共同影响。

Result: 发现两个一致趋势：合作强度更高的期刊（特别是拥有更大更多样化作者团队的期刊）获得更高引用影响力；AI参与度已成为期刊声望的日益强相关因素，特别是在四分位排名中。

Conclusion: BioMedJImpact既是捕捉生物医学与AI交叉的综合数据集，也是经过验证的方法框架，支持可扩展、内容感知的科学计量分析。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [52] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 通过目标激活工程引导LLaMA 3.1-8B展现更人性化的情感表达，使用归因修补识别关键干预位置，通过对比文本对推导情感表达向量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型对话流畅度不断提高，但赋予其细腻、人性化的情感表达仍是一个重大挑战。当前的对齐技术往往只解决表层输出或需要大量微调。

Method: 首先使用归因修补识别因果影响组件，通过诊断性对话任务观察激活模式找到关键干预位置。然后从对比文本对（目标情感的正面vs负面示例）的激活差异推导情感表达向量。

Result: 将这些向量应用于新对话提示显著增强了情感特征：引导后的响应显示出增加的正向情感（如喜悦、信任）和更频繁的第一人称代词使用，表明更强的个人参与度。

Conclusion: 研究提供了一个精确且可解释的框架，为对话AI研究开辟了新方向。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [53] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 本文提出了一种新的稳定性度量方法，用于评估LDA主题模型的稳定性和一致性，发现LDA虽然能正确识别主题数量且内部一致，但生成的主题并非真实主题。


<details>
  <summary>Details</summary>
Motivation: 概率主题模型由于其随机性，在重复运行时会产生不同的结果，导致潜在主题的不一致性，影响可重复性、可靠性和解释性，引发对主题模型是否真正捕捉到有意义主题还是只是噪声的担忧。

Method: 定义了一个新的稳定性度量方法，结合准确性和一致性，利用LDA的生成特性生成带有真实标签的新语料库，并将这些生成的语料库通过LDA运行50次以确定输出的变异性。

Result: 研究表明LDA能够正确确定文档中的基础主题数量，并且LDA具有更高的内部一致性，多次重复运行返回相似的主题；然而这些主题并非真实主题。

Conclusion: LDA模型在识别主题数量方面表现准确且内部一致，但生成的主题与真实主题存在差异，需要关注模型稳定性和主题质量评估。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [54] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: NeuroLex是一个专门针对脑电图报告的轻量级领域自适应语言模型，相比通用模型在脑电图文本处理方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型无法捕捉脑电图报告中的领域特定语言惯例，需要专门针对脑电图报告文本的模型。

Method: 使用哈佛脑电图数据库的纯文本数据，通过span-corruption预训练和指令式微调（报告润色、段落摘要、术语问答）进行训练。

Result: 相比同规模通用模型，NeuroLex实现了更低的困惑度、更高的提取和摘要准确率、更好的标签效率，以及对否定和事实幻觉更强的鲁棒性。

Conclusion: NeuroLex为脑电图文本建模和脑机接口应用提供了可解释的语言驱动神经解码基础。

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [55] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文系统综述了多模态思维链（MCoT）的研究，分析了其背景、方法、评估和应用，并讨论了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在感知任务中的成功，提升其复杂推理能力成为关键研究方向。现有模型存在推理路径不透明和泛化能力不足等问题，而思维链方法在语言模型中已证明能增强推理透明度和输出可解释性，有望在多模态领域提升模型推理能力。

Method: 从三个维度介绍主流MCoT方法：思维链范式、后训练阶段和推理阶段，并分析其内在机制。

Result: 总结了现有的评估基准和指标，讨论了MCoT的应用场景。

Conclusion: 分析了MCoT当前面临的挑战，并对其未来研究方向进行了展望。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [56] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本研究比较了三种transformer架构(BERT、GPT-2、DeBERTa)在文本希望表达分类任务中的表现，发现BERT在准确率和计算效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 开发一个计算框架来分析文本中的希望表达，应用于心理健康和社交媒体分析领域。

Method: 使用BERT、GPT-2和DeBERTa三种transformer架构进行二元分类(希望vs非希望)和多类别分类(五个希望相关类别)的比较研究。

Result: BERT表现最佳，二元分类准确率84.49%，多类别分类准确率72.03%，且计算效率最高(训练时间443秒)。GPT-2准确率最低(79.34%二元，71.29%多类别)，DeBERTa表现中等但计算成本高(多类别训练947秒)。

Conclusion: 对于专门的情感检测任务，架构的适用性可能比模型规模更重要，BERT在希望表达分类任务中展现了最佳平衡性能。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [57] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 提出了Visual Room 2.0基准测试，用于评估多模态大语言模型的感知-认知对齐，发现MLLMs的感知能力优于认知能力，且认知不依赖于感知推理。


<details>
  <summary>Details</summary>
Motivation: 基于Searle的中文房间思想扩展到多模态领域，探讨MLLMs是否能真正理解所见内容，即看到不等于理解。

Method: 构建包含350个多模态样本的分层基准测试，涵盖低、中、高三个层次的17个任务，每个样本有6个渐进问题，共2100个问题。

Result: 评估10个SOTA MLLMs发现：(1)感知能力比认知能力强8.0%；(2)认知不因果依赖于感知推理；(3)认知随模型规模扩展，但感知不随模型变大而持续提升。

Conclusion: 将"看到≠理解"操作化为可测试假设，为MLLMs从感知处理到认知推理提供了新范式。

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [58] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: 提出HCNR方法，通过识别和恢复关键表达神经元来修复SFT导致的LLM诚实性问题，相比基线方法在数据效率和速度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)严重损害了LLM的诚实性，但现有恢复方法假设模型完全丧失了识别知识边界的能力，而实际上模型仍保留这种能力，只是表达这种意识的能力受损。

Method: HCNR方法：1）识别控制诚实表达的关键神经元；2）将这些神经元恢复到预训练状态；3）通过Hessian引导的补偿机制协调任务导向神经元。

Result: 在4个QA任务和5个LLM家族上的实验表明，HCNR恢复了33.25%的受损诚实性，相比基线方法实现至少2.23倍加速和超过10倍的数据减少。

Conclusion: HCNR为可信赖LLM部署提供了实用解决方案，能够有效修复SFT导致的诚实性损害，同时保持高效性。

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [59] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: AA-Omniscience基准测试评估语言模型的事实回忆能力和知识校准能力，覆盖42个经济相关主题的6000个问题。Claude 4.1 Opus获得最高分4.8，仅三个模型得分超过0，显示前沿模型在事实性和校准方面存在持续弱点。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要衡量通用能力，但可靠使用需要事实准确性和知识差距识别能力。需要专门评估事实回忆和知识校准。

Method: 构建AA-Omniscience基准，包含来自权威学术和行业来源的6000个问题，覆盖6个领域的42个经济相关主题。使用全知指数(-100到100)评估事实回忆，同时惩罚幻觉和奖励不确定时的弃权。

Result: Claude 4.1 Opus获得最高分4.8，仅三个模型得分超过0。不同领域表现差异明显，三个不同研究实验室的模型在六个领域中各自领先。

Conclusion: 模型在事实性和校准方面存在持续弱点，性能因领域而异。对于知识重要任务，应根据用例需求而非通用性能选择模型。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [60] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本文探讨了多语言嵌入模型与对齐单语模型在双语词典归纳任务中的表现差异，分析了BLI作为嵌入空间对齐度量的局限性，并提出基于词干的新BLI方法和词汇剪枝技术来更准确评估对齐程度。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言嵌入已成为主流选择，但需要验证其是否在所有方面都优于对齐单语模型，以及高昂的计算成本是否总是合理。研究旨在探索BLI作为嵌入空间对齐度量的有效性，并比较不同嵌入对齐技术在高资源和低资源语言环境下的表现。

Method: 使用双语词典归纳作为评估指标，比较传统嵌入对齐技术、新型多语言模型和组合对齐技术的表现。分析语言家族对结果的影响，提出基于词干的BLI方法和词汇剪枝技术来改进对齐度评估。

Result: 发现BLI在某些情况下不能准确衡量对齐程度，组合嵌入对齐技术通常表现更好，但在低资源语言情况下多语言嵌入表现更优。提出的新方法能更有效地评估嵌入空间对齐。

Conclusion: 多语言嵌入和对齐单语模型各有优势，需要根据具体应用场景选择。BLI作为对齐度量存在局限性，提出的基于词干的BLI和词汇剪枝技术能提供更准确的对齐评估。

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [61] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: Spark-Prover-X1是一个7B参数的定理证明模型，通过三阶段训练框架提升轻量级LLM的形式推理能力，在多个基准测试中达到同类开源模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在自动定理证明中因缺乏多样化和高质量的形式语言数据而受限的问题。

Method: 采用三阶段训练框架：1) 在广泛数学语料上进行持续预训练，引入"思维链增强状态预测"任务；2) 在专家迭代循环中进行监督微调；3) 使用组相对策略优化针对最具挑战性的问题进行强化训练。

Result: Spark-Prover-X1-7B在pass@32下达到37.0%的平均通过率，在PutnamBench上解决27个问题，在CombiBench上达到24.0%的通过率。

Conclusion: 多样化的训练数据和逐步精炼的训练流程为增强轻量级LLM的形式推理能力提供了有效路径。

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [62] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: BeDiscovER是一个评估现代大语言模型语篇理解能力的综合基准套件，包含5个语篇任务、52个数据集，涵盖词汇、句子和文档层面的语篇分析。


<details>
  <summary>Details</summary>
Motivation: 随着推理语言模型的发展，需要更新的、全面的基准来评估模型在语篇层面的知识理解能力，包括传统任务和新挑战。

Method: 整合了5个公开可用的语篇任务，涵盖语篇词汇、(多)句子和文档层面，包括语篇解析、时序关系提取、语篇助词消歧等任务。

Result: 评估发现最先进的模型在时序推理的算术方面表现强劲，但在完整文档推理和某些细微语义语篇现象（如修辞关系识别）方面仍有困难。

Conclusion: BeDiscovER为评估现代LLMs的语篇理解能力提供了全面基准，揭示了模型在复杂语篇推理方面的局限性，为未来研究指明了方向。

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [63] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 本研究系统评估了当代大语言模型在零样本设置下识别已发表RCT对CONSORT 2010声明依从性的准确性。结果表明模型整体表现中等，能较好识别合规项目，但在检测不合规和不适用的项目方面表现较差，尚不能替代人类专家进行试验质量评估。


<details>
  <summary>Details</summary>
Motivation: 手动验证CONSORT依从性是一个耗时费力的过程，构成同行评审和证据合成的重要瓶颈。本研究旨在评估LLMs在零样本设置下自动识别RCT对CONSORT声明依从性的准确性和可靠性。

Method: 构建了包含150篇已发表RCT的金标准数据集，涵盖不同医学专业。主要结果是三类分类任务的宏平均F1分数，辅以项目级性能指标和定性错误分析。

Result: 整体模型表现中等。表现最佳的模型Gemini-2.5-Flash和DeepSeek-R1的宏F1分数分别为0.634，Cohen's Kappa系数分别为0.280和0.282，仅与专家共识达成一般一致性。模型在识别合规项目时准确率较高（F1分数>0.850），但在识别不合规和不适用项目时表现较差（F1分数很少超过0.400）。

Conclusion: LLMs作为CONSORT检查的初步筛查助手具有潜力，能够有效识别报告良好的项目。然而，它们目前无法可靠地检测报告遗漏或方法学缺陷，因此不适合替代人类专家进行试验质量的关键评估。

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [64] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: 提出Agent-Event-Coder (AEC)多智能体框架，将事件抽取视为代码生成过程，通过检索、规划、编码和验证四个专业智能体的协作，在零样本设置下实现精确、完整且符合模式的事件抽取。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在零样本事件抽取中的挑战，包括复杂推理需求、领域特定理解困难，以及直接提示导致的不完整输出、结构无效等问题。

Method: 采用多智能体框架，将事件抽取分解为四个专门子任务：检索、规划、编码和验证，每个任务由专用LLM智能体处理，事件模式表示为可执行的类定义。

Result: 在五个不同领域和六个LLM上的实验表明，AEC持续优于先前的零样本基线方法，验证了将事件抽取视为代码生成方法的有效性。

Conclusion: AEC通过编程启发的方法和协作智能体工作流，使LLM能够在零样本设置下产生精确、完整且模式一致的事件抽取结果，证明了将事件抽取视为代码生成过程的强大能力。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [65] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: 比较了ConvLSTM和Vanilla Transformer在手语识别任务上的表现，发现Transformer在准确率上优于ConvLSTM，而ConvLSTM在计算效率上更有优势。


<details>
  <summary>Details</summary>
Motivation: 系统比较循环神经网络和基于注意力的神经网络架构在孤立手语识别任务中的性能差异，为手语识别系统的架构选择提供指导。

Method: 在阿塞拜疆手语数据集（AzSLD）和单词级美国手语数据集（WLASL）上实现并评估了ConvLSTM和Vanilla Transformer两种代表性模型。

Result: Vanilla Transformer在两个数据集的Top-1和Top-5准确率上都优于ConvLSTM，在AzSLD上达到76.8% Top-1准确率，在WLASL上达到88.3%。ConvLSTM计算效率更高但在准确率上落后。

Conclusion: Transformer在整体准确率和说话者独立性方面表现更优，而ConvLSTM在计算效率和时序建模方面有优势，应根据应用需求和资源约束选择合适的架构。

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [66] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 提出了一个零样本语法能力评估框架，利用未标记数据和大型语言模型生成伪标签，通过噪声标签训练方法有效估计语法能力分数


<details>
  <summary>Details</summary>
Motivation: 口语语法评估面临自发性和不流畅性挑战，且需要大量专家标注，大规模数据创建不切实际

Method: 使用基于语法能力量表的提示词让LLM在未标记数据上生成预测作为伪标签，通过专门设计的训练框架训练基于transformer的模型来处理标签噪声

Result: 实验结果显示该方法能够高精度估计语法能力分数，LLM选择对性能有重要影响，训练中干净样本与噪声样本的比例影响稳定性和准确性

Conclusion: 该方法为可扩展、低资源的语法评估系统铺平了道路，具有鲁棒性和可解释性

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [67] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 提出了首个孟加拉语语料库，用于区分ASR转录中的重复性不流利和形态学重叠现象，通过LLM和微调方法建立了强基线。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语ASR转录中词重复的歧义问题：区分是重复性不流利（ASR错误/犹豫）还是形态学重叠（语法结构），避免标准不流利校正错误删除有效语言信息。

Method: 创建了首个公开的20,000行手动标注孟加拉语语料库，使用两种方法：1）多语言大语言模型的少样本提示；2）编码器模型的任务特定微调。

Result: LLM在少样本提示下达到82.68%准确率，但微调表现更优，BanglaBERT模型达到84.78%准确率和0.677 F1分数。

Conclusion: 建立了强大的语言感知基线，为开发保留语义的孟加拉语文本规范化系统提供了重要数据。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [68] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: TCM-5CEval是一个针对中医领域的大型语言模型评估基准，包含五个关键维度：核心知识、经典文献、临床决策、中药学和临床非药物治疗。评估发现模型在基础知识方面表现良好，但在经典文本解释和推理稳定性方面存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在通用领域表现出色，但在中医等高度专业化和文化丰富的领域中需要更严格的评估。基于之前TCM-3CEval的工作，需要更细粒度和全面的评估基准来揭示模型在中医领域的真实能力。

Method: 开发了TCM-5CEval评估基准，包含五个维度：TCM-Exam（核心知识）、TCM-LitQA（经典文献）、TCM-MRCD（临床决策）、TCM-CMM（中药学）、TCM-ClinNPT（临床非药物治疗）。对15个主流LLM进行了全面评估，包括基于排列的一致性测试。

Result: 评估显示模型性能存在显著差异，deepseek_r1和gemini_2_5_pro表现最佳。模型在基础知识回忆方面表现良好，但在经典文本解释方面存在困难。排列一致性测试显示所有模型都存在推理脆弱性，对问题选项顺序敏感，表明缺乏稳健的理解能力。

Conclusion: TCM-5CEval不仅提供了更详细的中医能力诊断工具，还暴露了LLM在推理稳定性方面的根本弱点。该基准已上传至Medbench平台，以促进进一步研究和标准化比较。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [69] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 本研究提出了一种量化方法来估计翻译熵，通过分析在保持翻译不变的情况下替换特定标记的概率，从而评估翻译器的性能。


<details>
  <summary>Details</summary>
Motivation: 在信息时代，翻译需求日益增长，但目前缺乏客观的量化方法来评估基于编码器-解码器深度架构的翻译器性能，主要原因在于单种语言的熵仍然未知。

Method: 给定一个翻译器，通过替换枢轴句中特定标记生成多个句子，分析这些句子产生相同翻译的统计规律，计算替换特定标记而保持翻译不变的概率，从而得到翻译熵。

Result: 该方法能够量化排名多个公开可用的翻译器，揭示互译熵的对称性，并发现替换两个标记时翻译简并性呈乘积效应。基于MarianMT、T5-Base和NLLB-200翻译器的实验结果验证了方法的有效性。

Conclusion: 翻译熵是可测量的属性，为人工翻译器提供了客观的基准测试方法。

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [70] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 评估多种大型语言模型在罗马尼亚语变音符号恢复任务中的表现，发现GPT-4o等模型表现优异，而Llama系列模型表现波动较大。


<details>
  <summary>Details</summary>
Motivation: 自动变音符号恢复对于处理罗马尼亚语等富含变音符号的语言文本处理至关重要，需要评估现有LLM在此任务上的性能。

Method: 使用综合语料库测试了GPT-3.5、GPT-4、GPT-4o、Gemini 1.0 Pro、Llama 2/3、Mixtral 8x7B、airoboros 70B、RoLlama 2 7B等模型，采用从零样本到复杂多样本的多种提示模板。

Result: GPT-4o等模型达到高精度变音符号恢复，持续超越中性回声基线，而Meta的Llama系列模型表现出更大的波动性。

Conclusion: 模型架构、训练数据和提示设计对变音符号恢复性能有重要影响，为改进富含变音符号语言的NLP工具指明了方向。

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [71] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文评估了视觉语言模型在语音识别任务中的表现，通过创建包含4000+英语单词的声谱图和波形图数据集，测试模型预测音素或字素转录的能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，需要评估这些模型在融合视觉和语言模态任务中的能力，特别是作为语音学家解释语音声谱图和波形的能力。

Method: 合成包含4000+英语单词的新数据集，通过多项选择任务测试模型预测正确音素或字素转录的能力，使用基于音素编辑距离的干扰项。

Result: 无论是零样本还是微调模型，其表现都很少超过随机猜测水平，表明需要特定的参数知识来解释此类图形。

Conclusion: 仅靠配对样本不足以让视觉语言模型有效解释语音声谱图和波形，需要专门的参数知识。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [72] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: SoCE是一种基于基准测试组合的模型融合方法，通过识别类别专家模型并使用非均匀加权平均来提升性能，在多个领域实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练成本高昂，模型融合作为一种无需重新训练就能提升性能的技术具有重要价值。传统均匀平均方法未能充分利用不同模型在特定类别上的专长。

Method: 利用基准测试类别间低相关性的观察，识别每个弱相关类别簇的专家模型，然后使用优化的非均匀加权平均而非均匀权重进行模型融合。

Result: 该方法在多个领域（多语言能力、工具调用、数学等）提升了性能和鲁棒性，在伯克利函数调用排行榜上取得了最先进的结果。

Conclusion: SoCE提供了一种原则性的模型融合方法，通过利用类别专家模型和优化加权策略，有效提升了模型性能而无需昂贵的重新训练。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [73] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: RegionMarker是一个区域触发语义水印框架，通过在低维空间中定义触发区域并向相关文本嵌入注入水印，为Embedding-as-a-Service提供全面的版权保护。


<details>
  <summary>Details</summary>
Motivation: 现有的EaaS水印方法只能抵抗部分攻击，无法提供全面的版权保护，存在模型提取攻击导致经济损失的风险。

Method: 使用秘密降维矩阵投影到子空间，随机选择触发区域，在整个触发区域嵌入水印，并将文本嵌入本身作为水印。

Result: 在多个数据集上的实验表明，RegionMarker能有效抵抗不同攻击方法，包括水印移除攻击、转述攻击和维度扰动攻击。

Conclusion: RegionMarker框架为EaaS提供了全面的版权保护，能够有效防止模型提取攻击带来的经济损失。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [74] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 该论文介绍了阿拉伯语方言情感分析共享任务，使用包含沙特和摩洛哥方言的酒店评论数据集，旨在开发方言感知的NLP系统用于客户体验分析。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界酒店业越来越依赖客户反馈来改善服务，需要先进的阿拉伯语情感分析工具来支持多方言环境下的客户体验分析。

Method: 创建多方言数据集，包含538条情感平衡的酒店评论，从现代标准阿拉伯语翻译为沙特和摩洛哥方言，并由母语者验证翻译准确性和情感保留。

Result: 超过40个团队注册参与共享任务，12个团队提交系统，最佳系统F1得分达到0.81。

Conclusion: 研究表明跨阿拉伯语方言的情感分析是可行的，但仍面临挑战，该资源支持开发方言感知的NLP系统用于实际应用。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [75] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文通过PEFT/LoRA研究探索LLMs在不同任务和语言间的迁移效果，发现任务内跨语言迁移稳定正向，而跨任务迁移常导致性能下降，揭示了任务和语言间的非对称迁移模式。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在一个任务或语言上的改进如何影响其他任务和语言及其组合，目前缺乏系统研究。

Method: 采用PEFT/LoRA方法，在多个开源LLM家族和规模上进行控制实验，将任务和语言作为迁移轴，测量单任务-语言源对到所有其他任务-语言目标对的迁移效果。

Result: 发现两个一致模式：1）任务内跨语言迁移可靠正向，而跨任务迁移常导致性能下降；2）存在稳定的捐赠者-接收者结构（枢纽捐赠者vs脆弱接收者）。

Conclusion: 研究结果对风险感知微调和模型专业化具有重要启示，揭示了任务和语言间的非对称迁移特性。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [76] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: PEDIASBench评估框架显示，当前大型语言模型在儿科医疗中表现出良好的基础知识掌握，但在复杂推理、动态诊疗适应性和人文关怀方面存在局限，尚不能独立执行儿科诊疗，但可作为决策支持和教育工具。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在医学领域的快速发展，需要评估它们是否能在真实临床环境中胜任儿科医生角色，了解当前模型的优势和局限性。

Method: 开发PEDIASBench系统评估框架，从基础知识应用、动态诊疗能力、儿科医疗安全与伦理三个维度，评估12个代表性模型在19个儿科亚专业和211种典型疾病上的表现。

Result: 先进模型在基础知识上表现良好（Qwen3-235B-A22B在执照级问题上准确率超90%），但任务复杂度增加时性能下降约15%；动态诊疗场景中DeepSeek-R1得分最高（平均0.58），但多数模型难以适应实时患者变化；医疗伦理安全任务中Qwen2.5-72B表现最佳（准确率92.05%），但人文敏感性有限。

Conclusion: 儿科LLMs受限于有限的动态决策能力和不发达的人文关怀，未来应关注多模态整合和临床反馈-模型迭代循环，以增强安全性、可解释性和人机协作。当前模型虽不能独立执行儿科诊疗，但在决策支持、医学教育和患者沟通方面具有潜力。

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [77] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 提出了PAL-Bench基准来评估服务导向助手在长期用户-代理交互中的个性化能力，并开发了H²Memory分层异构记忆框架来改进个性化响应生成。


<details>
  <summary>Details</summary>
Motivation: 随着智能个人设备的普及，服务导向的人机交互日益普遍，需要能够理解用户特定特征的个性化对话助手，但现有方法往往忽视长期交互的复杂性。

Method: 开发了多步骤LLM合成流程创建PAL-Set中文数据集，提出了H²Memory分层异构记忆框架，结合检索增强生成来改进个性化响应。

Result: 在PAL-Bench和外部数据集上的综合实验证明了所提记忆框架的有效性。

Conclusion: PAL-Bench为评估服务导向助手的个性化能力提供了新基准，H²Memory框架显著提升了长期交互中的个性化服务效果。

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


### [78] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: 提出了一种基于对数函数的非线性翻译质量评估模型，解决了传统线性评估方法在不同文本长度下的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统的线性翻译质量评估方法在不同文本长度下存在偏差，对短文本过度惩罚，对长文本惩罚不足，与专家直觉不一致。

Method: 基于多范围框架，开发了校准的非线性评分模型E(x) = a * ln(1 + b * x)，使用对数函数反映人类对翻译质量的感知，并通过一维根查找步骤进行校准。

Result: 实证数据显示可接受的错误数量随样本大小呈对数增长而非线性增长，该模型提高了评估的可解释性、公平性和评分者间可靠性。

Conclusion: 该模型为翻译质量评估提供了更准确和可扩展的评估范式，为AI驱动的文档级评估提供了更强的基础，并与人类判断保持一致。

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [79] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 本文提出了一种基于方面的情感分析方法来解码泰国财务年报中的模糊情感，开发了专门的标注指南，并对100多份财务报告进行了标注，在情感分类任务中取得了良好表现。通过事件研究验证了该方法对股价的实际影响。


<details>
  <summary>Details</summary>
Motivation: 财务文档中的情感理解对洞察市场行为至关重要，但这些报告往往使用模糊语言来呈现积极或中性前景，即使实际情况可能不太乐观。需要专门的方法来解码这种模糊情感。

Method: 采用基于方面的情感分析(ABSA)方法，开发了针对泰国财务年报中模糊情感的标注指南，标注了100多份财务报告，并对比了多种文本分类模型在该数据集上的表现。

Result: 在情感分类任务中表现出色，事件研究表明市场反应受到报告中特定方面的选择性影响。

Conclusion: 财务文本中的情感分析具有复杂性，解决模糊语言问题对于准确评估市场情绪至关重要，基于方面的分析方法能有效揭示隐藏的情感信息。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [80] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: 提出基于大语言模型的公共叙事自动标注框架，在8个叙事和14个代码上达到平均F1分数0.80，接近专家水平，并扩展到22个故事和政客演讲分析。


<details>
  <summary>Details</summary>
Motivation: 公共叙事是领导力发展和公民动员的重要工具，但由于主观解释性和专家标注成本高，系统分析面临挑战。

Method: 开发与领域专家合作制定的代码本，利用大语言模型自动进行公共叙事的定性标注，并与专家标注进行性能比较。

Result: LLM在公共叙事标注中达到接近专家水平的性能（平均F1=0.80），成功将分析扩展到更大数据集和政客演讲。

Conclusion: LLM辅助标注在可扩展叙事分析中具有潜力，为计算公民叙事研究提供了新视角，同时指出了局限性和未来研究方向。

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [81] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 本文介绍了两个新的匈牙利语语音数据集BEA-Large和BEA-Dialogue，填补了匈牙利语自发性和对话语音数据的空白，并建立了可复现的ASR和说话人日志基线。


<details>
  <summary>Details</summary>
Motivation: 高资源语言的ASR发展得益于大量数据集，而匈牙利语等语言因缺乏自发性和对话语料库而代表性不足。

Method: 从匈牙利语音语料库BEA的未处理部分构建了两个数据集：BEA-Large（255小时自发语音）和BEA-Dialogue（85小时自然对话），并使用公开可用的ASR模型建立基线。

Result: 微调的Fast Conformer模型在自发语音上词错误率为14.18%，在重复语音上为4.8%；说话人日志错误率在13.05%-18.26%之间。

Conclusion: 对话ASR仍然具有挑战性，特别是由于不流利、重叠和非正式语音模式。这些数据集和基线的发布旨在推动匈牙利语音技术发展，并为其他语言提供方法论框架。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [82] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 提出了一个基于核心意图、语句类型、语法结构和关键动作的文本到SQL分类法，并基于此创建了SQL-Synth数据集，该数据集比现有基准具有更好的多样性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到SQL数据集覆盖范围有限，无法捕捉真实世界应用的多样性，需要更全面的数据集来训练和评估模型。

Method: 提出了新的文本到SQL分类法，并基于此开发了分类法指导的数据集合成流程，结合大型语言模型生成SQL-Synth数据集。

Result: SQL-Synth数据集在多样性和覆盖范围上优于现有基准，现有LLMs在SQL-Synth上表现有限，但微调可以显著提升性能。

Conclusion: 提出的分类法具有重要影响，能够全面分析数据集和不同LLMs的性能，并指导LLMs训练数据的构建。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [83] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: O-Mem是一个基于主动用户画像的新型记忆框架，通过动态提取和更新用户特征与事件记录，支持分层检索，在个性化响应方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在复杂环境中维持长期交互存在挑战，主要由于上下文一致性和动态个性化方面的限制。现有记忆系统依赖语义分组检索，可能忽略语义无关但关键的用户信息并引入检索噪声。

Method: 提出O-Mem框架，基于主动用户画像，从用户与代理的主动交互中动态提取和更新用户特征和事件记录，支持人物属性与主题相关上下文的分层检索。

Result: 在LoCoMo基准上达到51.76%，比之前的SOTA LangMem提升近3%；在PERSONAMEM上达到62.99%，比之前的SOTA A-Mem提升3.5%。同时提升了token和交互响应时间效率。

Conclusion: O-Mem为开发高效且类人的个性化AI助手开辟了有前景的方向。

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


### [84] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
*Chengyu Huang,Zhengxin Zhang,Claire Cardie*

Main category: cs.CL

TL;DR: HAPO是一种历史感知策略优化方法，通过跟踪每个问题的历史状态来激励模型发现比先前更简洁的正确解决方案，在保持准确性的同时显著减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测试时扩展响应长度虽然能提升LLM的推理能力，但会导致冗长输出和增加推理成本，且没有利用训练中相同问题的历史信息来逐步优化解决方案的简洁性。

Method: HAPO为每个问题维护历史状态（如先前正确响应的最小长度），基于此设计长度奖励函数，激励发现比历史更简洁的正确解，同时避免过度惩罚较短的错误响应以促进探索。结合正确性奖励，联合优化正确性和效率。

Result: 在多个数学基准测试中，HAPO训练的模型实现了33-59%的长度缩减，准确率仅下降2-5%，有效提升了LLM的简洁推理能力。

Conclusion: HAPO通过历史感知的奖励机制成功平衡了正确性和效率，能够在保持高准确性的同时显著减少模型输出长度，为高效推理提供了有效解决方案。

Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.

</details>


### [85] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 使用大型语言模型将机器学习检测到的欺骗性评论特征转化为人类可理解的语言现象，帮助人们在没有检测分类器的情况下评估在线评论的可信度。


<details>
  <summary>Details</summary>
Motivation: 欺骗性评论误导消费者、损害商家利益并破坏在线市场信任。虽然机器学习分类器能有效识别欺骗性评论，但其学习到的区分特征往往难以被人理解。

Method: 利用大型语言模型将机器学习学到的词汇线索翻译成人类可理解的语言现象，这些现象基于数据实证，可跨相似领域泛化。

Result: 通过此方法获得的语言现象比LLMs先验知识或上下文学习获得的现象更具预测性，且基于数据实证，可跨领域泛化。

Conclusion: 这种方法获得的语言现象有助于人们在缺乏欺骗检测分类器的环境中批判性评估在线评论的可信度。

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [86] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Main category: cs.CL

TL;DR: 本文提出了DCRM指标来量化偏好优化中响应对的质量，发现训练集的DCRM与学习效果正相关，并提出了best-of-N²配对方法来选择高质量响应对，在多个基准测试中提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究尝试将偏好优化性能与偏好数据集关联，但作者观察到偏好响应之间的差异可能不符合期望学习的目标差异，需要量化这些差异来评估响应对的质量。

Method: 使用距离和奖励边际来量化响应差异，结合得到DCRM指标；研究三类常用偏好数据集；提出best-of-N²配对方法选择高DCRM响应对。

Result: 建立了训练集DCRM与学习效果的正相关关系；在各种设置下，该方法产生的训练数据集在AlpacaEval、MT-Bench和Arena-Hard基准上优于现有训练集。

Conclusion: DCRM是评估偏好优化响应对质量的有效指标，基于DCRM的配对方法能够显著提升模型性能，为构建高质量偏好数据集提供了新思路。

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


### [87] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: 提出了TAI框架，结合大语言模型和潜在扩散模型，通过翻译和图像生成增强印度诗歌的可访问性，支持联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 印度诗歌具有语言复杂性和文化深度，但现有研究忽视了印度语言诗歌，其多层含义和文化典故给非母语读者带来理解挑战。

Method: 使用TAI框架，包括基于几率比偏好对齐算法的翻译模块和基于语义图的图像生成模块，捕捉诗歌中的隐喻和语义关系。

Result: 综合实验评估显示TAI Diffusion在诗歌图像生成任务中优于强基线，并发布了包含1,570首21种低资源印度语言诗歌的数据集。

Conclusion: 该工作通过填补诗歌翻译和视觉理解的空白，旨在扩大可访问性并丰富读者体验。

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [88] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: Lang1模型家族通过结合临床电子病历数据和互联网文本进行预训练，在医疗操作决策任务上表现优于通用模型，特别是在微调后能超越大70倍的通用模型。


<details>
  <summary>Details</summary>
Motivation: 通用基础模型在医疗知识方面表现良好，但缺乏医疗操作决策所需的专业知识，需要专门针对医疗领域开发的模型。

Method: 开发Lang1模型家族（100M-7B参数），使用80B临床电子病历token和627B互联网token混合预训练，并创建ReMedE基准评估五个关键医疗任务。

Result: 零射设置下通用和专用模型在四个任务上表现不佳，但微调后的Lang1-1B模型在AUROC上比大70倍的通用模型提升3.64%-6.75%，比零射模型提升1.66%-23.66%。

Conclusion: 医疗系统AI需要领域内预训练、监督微调和真实世界评估的结合，专用LLM在专业任务上能与通用模型竞争。

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [89] [Compound Selection Decisions: An Almost SURE Approach](https://arxiv.org/abs/2511.11862)
*Jiafeng Chen,Lihua Lei,Timothy Sudijono,Liyang Sun,Tian Xie*

Main category: econ.EM

TL;DR: 本文提出了在高斯序列模型中生成复合选择决策的方法，通过引入ASSURE（几乎无偏估计器）来估计决策规则的预期效用，从而在预定义类别中选择福利最大化的规则。


<details>
  <summary>Details</summary>
Motivation: 在观测数据存在噪声的情况下，决策者需要选择子集索引S来最大化效用函数∑(μ_i - K_i)/n，但μ_i未知且存在估计误差，需要开发能够跨噪声估计借力的决策方法。

Method: 受Stein无偏风险估计(SURE)启发，开发了ASSURE（几乎无偏估计器）来估计提议决策规则的预期效用，允许用户通过优化估计福利从预定义类别中选择福利最大化规则。

Result: ASSURE产生的决策规则在预定义类别中渐近不差于最优但不可行的决策规则，在多个应用场景中验证了其有效性。

Conclusion: ASSURE方法能够有效处理高斯序列模型中的选择决策问题，通过跨噪声估计借力产生高质量的决策规则，在多个实际应用中展现出良好性能。

Abstract: This paper proposes methods for producing compound selection decisions in a Gaussian sequence model. Given unknown, fixed parameters $μ_ {1:n}$ and known $σ_{1:n}$ with observations $Y_i \sim \textsf{N}(μ_i, σ_i^2)$, the decision maker would like to select a subset of indices $S$ so as to maximize utility $\frac{1}{n}\sum_{i\in S} (μ_i - K_i)$, for known costs $K_i$. Inspired by Stein's unbiased risk estimate (SURE), we introduce an almost unbiased estimator, called ASSURE, for the expected utility of a proposed decision rule. ASSURE allows a user to choose a welfare-maximizing rule from a pre-specified class by optimizing the estimated welfare, thereby producing selection decisions that borrow strength across noisy estimates. We show that ASSURE produces decision rules that are asymptotically no worse than the optimal but infeasible decision rule in the pre-specified class. We apply ASSURE to the selection of Census tracts for economic opportunity, the identification of discriminating firms, and the analysis of $p$-value decision procedures in A/B testing.

</details>


### [90] [Multiscale Comparison of Nonparametric Trending Coefficients](https://arxiv.org/abs/2511.12600)
*Marina Khismatullina,Bernhard van der Sluis*

Main category: econ.EM

TL;DR: 提出了一种新的多尺度检验框架，用于检测面板数据模型中时变系数的斜率异质性，并能识别哪些单元存在差异以及差异的位置。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以检测面板数据中时变系数的异质性，特别是无法确定哪些单元存在差异以及差异的具体位置。

Method: 开发了多尺度检验方法，建立渐近有效性，并扩展用于揭示模型中的潜在群组结构。

Result: 应用该方法检验美国货币政策冲击对49个外国经济体及自身的影响异质性，发现确实存在异质性，并讨论了两个群组的聚类结果。

Conclusion: 该框架能有效检测面板数据中时变系数的异质性，并能识别差异的具体位置和群组结构。

Abstract: This paper proposes a novel framework to test for slope heterogeneity between time-varying coefficients in panel data models. Our test not only allows us to detect whether the coefficient functions are the same across all units or not, but also determines which of them are different and where these differences are located. We establish the asymptotic validity of our multiscale test. As an extension of the proposed procedure, we show how to use the results to uncover latent group structures in the model. We apply our methods to test for heterogeneity in the effect of U.S. monetary shocks on 49 foreign economies and itself. We find evidence that such heterogeneity indeed exists and we discuss the clustering results for two groups.

</details>


### [91] [Double machine learning for causal inference in a multivariate sample selection model](https://arxiv.org/abs/2511.12640)
*Sofiia Dolgikh,Bodan Potanin*

Main category: econ.EM

TL;DR: 提出了在具有序数选择方程的多变量样本选择模型中估计ATE、ATET和LATE的PI和DML估计器，DML估计器具有双重稳健性。


<details>
  <summary>Details</summary>
Motivation: 在多变量样本选择模型中，如果不处理样本选择问题，因果参数的估计可能存在严重偏差。

Method: 使用插件(PI)估计器和双重机器学习(DML)估计器，DML基于有效影响函数并具有双重稳健性。

Result: 模拟数据分析表明，提出的估计器能够避免样本选择带来的偏差。

Conclusion: 提出的PI和DML估计器在多变量样本选择模型中能有效估计因果参数，避免选择偏差。

Abstract: We propose plug-in (PI) and double machine learning (DML) estimators of average treatment effect (ATE), average treatment effect on the treated (ATET) and local average treatment effect (LATE) in the multivariate sample selection model with ordinal selection equations. Our DML estimators are doubly-robust and based on the efficient influence functions. Finite sample properties of the proposed estimators are studied and compared on simulated data. Specifically, the results of the analysis suggest that without addressing multivariate sample selection, the estimates of the causal parameters may be highly biased. However, the proposed estimators allow us to avoid these biases.

</details>


### [92] [Identification-aware Markov chain Monte Carlo](https://arxiv.org/abs/2511.12847)
*Toru Kitagawa,Yizhou Kuang*

Main category: econ.EM

TL;DR: 本文提出了一种新的MCMC方法，用于处理非识别模型中的多模态或平坦区域问题，通过利用观测等价参数集的知识来克服局部模式陷阱并实现更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 非识别性参数不会给贝叶斯推断带来困难，但由此产生的后验分布多模态或平坦区域会给现代贝叶斯计算带来挑战，传统采样方法在处理这些问题时往往收敛缓慢或不收敛。

Method: 开发了一种新颖的马尔可夫链蒙特卡洛(MCMC)方法，利用观测等价参数集的知识，专门针对非识别模型设计。

Result: 该方法克服了被困在局部模式的问题，比现有MCMC技术（包括随机游走Metropolis-Hastings和哈密顿蒙特卡洛）具有更快的收敛速度，且随着识别集维度或基数的增加，收敛速度的提升更加显著。

Conclusion: 该方法在结构向量移动平均(SVMA)应用中成功揭示了目标分布中的非平凡模式，证明了识别在现代贝叶斯分析中的重要作用。

Abstract: Leaving posterior sensitivity concerns aside, non-identifiability of the parameters does not raise a difficulty for Bayesian inference as far as the posterior is proper, but multi-modality or flat regions of the posterior induced by the lack of identification leaves a challenge for modern Bayesian computation. Sampling methods often struggle with slow or non-convergence when dealing with multiple modes or flat regions of the target distributions. This paper develops a novel Markov chain Monte Carlo (MCMC) approach for non-identified models, leveraging the knowledge of observationally equivalent sets of parameters, and highlights an important role that identification plays in modern Bayesian analysis. We show that our proposal overcomes the issues of being trapped in a local mode and achieves a faster rate of convergence than the existing MCMC techniques including random walk Metropolis-Hastings and Hamiltonian Monte Carlo. The gain in the speed of convergence is more significant as the dimension or cardinality of the identified sets increases. Simulation studies show its superior performance compared to other popular computational methods including Hamiltonian Monte Carlo and sequential Monte Carlo. We also demonstrate that our method uncovers non-trivial modes in the target distribution in a structural vector moving-average (SVMA) application.

</details>


### [93] [Why Do the Elderly Save? Using Health Shocks to Uncover Bequests Motives](https://arxiv.org/abs/2511.13275)
*Tetsuya Kaji,Elena Manresa*

Main category: econ.EM

TL;DR: 使用对抗性结构估计框架重新分析老年人储蓄行为，发现遗赠动机解释了13%-19%的晚年储蓄，且该动机不仅限于富人群体。


<details>
  <summary>Details</summary>
Motivation: 重新审视老年人储蓄行为，特别是为了区分遗赠动机和预防性储蓄动机，并改进对遗赠动机的识别和估计精度。

Method: 采用Kaji、Manresa和Pouliot（2023）的对抗性结构估计框架，结合De Nardi、French和Jones（2010）的模型，使用AHEAD数据，通过神经网络实现的灵活判别器自适应选择数据中最具信息量的特征。

Result: 在判别器中包含性别和健康史提高了遗赠动机的识别和估计精度。遗赠动机解释了所有永久收入五分位数中13%-19%的晚年储蓄，而不仅仅是富人群体。对抗性估计器能够精确区分遗赠动机和预防性储蓄动机。

Conclusion: 健康相关的生存预期异质性是区分遗赠和预防性储蓄动机的另一个重要识别变异来源。

Abstract: We revisit the saving behavior of elderly singles using an adversarial structural estimation framework by Kaji, Manresa and Pouliot (2023). The method bridges the simulated method of moments (SMM) and maximum-likelihood estimation by embedding a flexible discriminator, implemented as a neural network, that adaptively selects the most informative features of the data. Applying this approach to the model of De Nardi, French, and Jones (2010) with AHEAD data, we show that including gender and health histories in the discriminator improves identification and precision of bequests motives. The resulting estimates reveal that bequest motives explain between $13\%$ and $19\%$ percent of late-life savings across all permanent-income quintiles, not only among the rich. The adversarial estimator precisely disentangles bequest motives from precautionary savings motives. These findings suggest that heterogeneity in health-related survival expectations is another important source of identifying variation to distinguishing bequest and precautionary saving motives.

</details>


### [94] [Decomposing Inequalities using Machine Learning and Overcoming Common Support Issues](https://arxiv.org/abs/2511.13433)
*Emmanuel Flachaire,Bertille Picard*

Main category: econ.EM

TL;DR: 本文使用潜在结果重新构建了Kitagawa-Oaxaca-Blinder分解框架，扩展了Neumark的加权参考方法，采用双重稳健估计器解决共同支持和模型误设问题，通过Neyman正交性和双重机器学习避免修剪和外推。


<details>
  <summary>Details</summary>
Motivation: 解决传统分解方法在共同支持和模型误设方面的局限性，提高分解的灵活性和稳健性。

Method: 使用潜在结果重新表述分解框架，扩展Neumark的加权参考方法，结合双重稳健估计器、Neyman正交性和双重机器学习技术。

Result: 提出的方法提高了分解的灵活性和稳健性，通过两个实证应用进行了说明。

Conclusion: 新方法改进了分解的灵活性和稳健性，但基于Neumark参考结果的分解对包含无关解释变量特别敏感。

Abstract: The Kitagawa-Oaxaca-Blinder decomposition splits the difference in means between two groups into an explained part, due to observable factors, and an unexplained part. In this paper, we reformulate this framework using potential outcomes, highlighting the critical role of the reference outcome. To address limitations like common support and model misspecification, we extend Neumark's (1988) weighted reference approach with a doubly robust estimator. Using Neyman orthogonality and double machine learning, our method avoids trimming and extrapolation. This improves flexibility and robustness, as illustrated by two empirical applications. Nevertheless, we also highlight that the decomposition based on the Neumark reference outcome is particularly sensitive to the inclusion of irrelevant explanatory variables.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [95] [Statistical and economic evaluation of forecasts in electricity markets: beyond RMSE and MAE](https://arxiv.org/abs/2511.13616)
*Katarzyna Maciejowska,Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: q-fin.CP

TL;DR: 该研究探讨了电力价格预测精度与经济价值之间的关系，发现传统统计指标（如RMSE和MAE）与收益相关性较弱，而评估预测与真实价格曲线对齐度的指标与盈利能力关联更强。


<details>
  <summary>Details</summary>
Motivation: 现有预测评估指标（如RMSE和MAE）虽然适合统计评估，但不能充分反映预测的经济价值，特别是在电池储能系统需要基于日前电价预测确定充放电时机的决策问题中。

Method: 生成了192个预测，使用7个超越RMSE和MAE的统计指标评估预测特性，并计算这些统计指标与获得利润之间的动态相关性。

Result: RMSE和MAE与收益仅呈弱相关，而评估预测与真实日价格曲线对齐度的指标与盈利能力有更强的关系，能更有效地选择最优预测。

Conclusion: 在选择最优预测时，应优先考虑能够评估预测与真实价格曲线对齐度的指标，而非传统的RMSE和MAE，因为这些指标能更好地反映预测的经济价值。

Abstract: In recent years, a rapid development of forecasting methods has led to an increase in the accuracy of predictions. In the literature, forecasts are typically evaluated using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). While appropriate for statistical assessment, these measures do not adequately reflect the economic value of forecasts. This study addresses the decision-making problem faced by a battery energy storage system, which must determine optimal charging and discharging times based on day-ahead electricity price forecasts. To explore the relationship between forecast accuracy and economic value, we generate a pool of 192 forecasts. These are evaluated using seven statistical metrics that go beyond RMSE and MAE, capturing various characteristics of the predictions and associated errors. We calculate the dynamic correlation between the statistical measures and gained profits to reveal that both RMSE and MAE are only weakly correlated with revenue. In contrast, measures that assess the alignment between predicted and actual daily price curves have a stronger relationship with profitability and are thus more effective for selecting optimal forecasts.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [96] [Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy](https://arxiv.org/abs/2511.12120)
*Hongyang Yang,Xiao-Yang Liu,Shan Zhong,Anwar Walid*

Main category: q-fin.TR

TL;DR: 提出了一种基于深度强化学习的集成股票交易策略，结合PPO、A2C和DDPG三种算法，在道琼斯30只股票上验证了优于基准的表现。


<details>
  <summary>Details</summary>
Motivation: 在复杂动态的股票市场中设计盈利交易策略具有挑战性，需要开发能够适应不同市场情况的稳健策略。

Method: 使用PPO、A2C和DDPG三种actor-critic算法训练深度强化学习智能体，采用按需加载技术处理大数据，构建集成交易策略。

Result: 在道琼斯30只股票上的测试表明，所提出的深度集成策略在风险调整后收益（夏普比率）方面优于三种单独算法和两个基准策略。

Conclusion: 深度强化学习集成策略能够有效整合不同算法的优势，在股票交易中实现更好的风险调整后收益表现。

Abstract: Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.

</details>


### [97] [Discovery of a 13-Sharpe OOS Factor: Drift Regimes Unlock Hidden Cross-Sectional Predictability](https://arxiv.org/abs/2511.12490)
*Mainak Singha*

Main category: q-fin.TR

TL;DR: 提出一种基于制度条件信号激活的横截面股票因子，在股票特定漂移制度下结合价值和短期反转信号，实现夏普比率超过13的优异表现。


<details>
  <summary>Details</summary>
Motivation: 探索在市场特定制度下如何通过条件信号激活来显著提升因子表现，特别是在股票出现漂移行为时捕捉可预测的价格模式。

Method: 使用制度条件信号激活方法，在个股出现超过60%正收益日的63天窗口期（漂移制度）下，结合价值和短期反转信号构建因子，并通过20年S&P 500数据的严格前向验证。

Result: 因子年化收益158.6%，波动率12.0%，最大回撤-11.9%，夏普比率超过13，风险调整后表现比市场基准强约13倍，通过1000次随机化检验（p值<0.001）。

Conclusion: 漂移制度通过放大行为偏差、改变流动性模式，创造了系统性可开发的价格发现条件，该因子具有稳健性和实际部署价值（1-5亿美元容量）。

Abstract: We document a high-performing cross-sectional equity factor that achieves out-of-sample Sharpe ratios above 13 through regime-conditional signal activation. The strategy combines value and short-term reversal signals only during stock-specific drift regimes, defined as periods when individual stocks show more than 60 percent positive days in trailing 63-day windows. Under these conditions, the factor delivers annualized returns of 158.6 percent with 12.0 percent volatility and a maximum drawdown of minus 11.9 percent. Using rigorous walk-forward validation across 20 years of S&P 500 data (2004 to 2024), we show performance roughly 13 times stronger than market benchmarks on a risk-adjusted basis, produced entirely out-of-sample with frozen parameters. The factor passes extensive robustness tests, including 1,000 randomization trials with p-values below 0.001, and maintains Sharpe ratios above 7 even under 30 percent parameter perturbations. Exposure to standard risk factors is negligible, with total R-squared values below 3 percent. We provide mechanistic evidence that drift regimes reshape market microstructure by amplifying behavioral biases, altering liquidity patterns, and creating conditions where cross-sectional price discovery becomes systematically exploitable. Conservative capacity estimates indicate deployable capital of 100 to 500 million dollars before noticeable performance degradation.

</details>


### [98] [A Practical Machine Learning Approach for Dynamic Stock Recommendation](https://arxiv.org/abs/2511.12129)
*Hongyang Yang,Xiao-Yang Liu,Qingwei Wu*

Main category: q-fin.TR

TL;DR: 提出一种基于机器学习的动态股票推荐方案，通过滚动窗口建模选择前20%股票，在S&P 500指数上表现优于长期持有策略。


<details>
  <summary>Details</summary>
Motivation: 单一股票选择策略无法始终获胜，且分析师没有足够时间分析所有S&P 500股票，需要自动化解决方案。

Method: 选择代表性股票指标，使用5种机器学习方法（线性回归、岭回归、逐步回归、随机森林、广义提升回归）在滚动窗口建模，选择MSE最低的模型进行股票排名，采用等权重、均值方差、最小方差等投资组合分配方法。

Result: 提出的方案在夏普比率和累计收益方面优于S&P 500指数的长期持有策略。

Conclusion: 机器学习方法可以有效动态推荐股票，为投资决策提供实用工具。

Abstract: Stock recommendation is vital to investment companies and investors. However, no single stock selection strategy will always win while analysts may not have enough time to check all S&P 500 stocks (the Standard & Poor's 500). In this paper, we propose a practical scheme that recommends stocks from S&P 500 using machine learning. Our basic idea is to buy and hold the top 20% stocks dynamically. First, we select representative stock indicators with good explanatory power. Secondly, we take five frequently used machine learning methods, including linear regression, ridge regression, stepwise regression, random forest and generalized boosted regression, to model stock indicators and quarterly log-return in a rolling window. Thirdly, we choose the model with the lowest Mean Square Error in each period to rank stocks. Finally, we test the selected stocks by conducting portfolio allocation methods such as equally weighted, mean-variance, and minimum-variance. Our empirical results show that the proposed scheme outperforms the long-only strategy on the S&P 500 index in terms of Sharpe ratio and cumulative returns. This work is fully open-sourced at \href{https://github.com/AI4Finance-Foundation/Dynamic-Stock-Recommendation-Machine_Learning-Published-Paper-IEEE}{GitHub}.

</details>


### [99] [Stationary Distributions of the Mode-switching Chiarella Model](https://arxiv.org/abs/2511.13277)
*Jutta G. Kurth,Jean-Philippe Bouchaud*

Main category: q-fin.TR

TL;DR: 本文分析了扩展Chiarella金融市场模型在不同参数区间的稳态分布，发现了定价偏差分布和趋势分布的模式变化，包括单峰高斯分布、双峰分布及其临界点。


<details>
  <summary>Details</summary>
Motivation: 研究扩展Chiarella模型中金融市场的随机非线性动力学行为，特别是定价偏差和趋势的稳态分布特性，以理解趋势跟踪和均值回归之间的动态竞争。

Method: 使用Furutsu-Novikov定理分析不同参数区间（小噪声小反馈、慢趋势、快速弱耦合趋势等）的稳态分布，建立分岔临界点。

Result: 在小噪声小反馈极限下得到单峰高斯分布；慢趋势下出现P-分岔，均值回归快时为单峰，慢时为双峰；快速弱耦合趋势下仍为单峰高斯；强趋势反馈下定价偏差分布变为双峰。

Conclusion: 扩展Chiarella模型的稳态分布呈现丰富的模式变化，分岔条件与动力学系统本身不同，强趋势反馈是定价偏差分布双峰化的关键因素。

Abstract: We derive the stationary distribution in various regimes of the extended Chiarella model of financial markets. This model is a stochastic nonlinear dynamical system that encompasses dynamical competition between a (saturating) trending and a mean-reverting component. We find the so-called mispricing distribution and the trend distribution to be unimodal Gaussians in the small noise, small feedback limit. Slow trends yield Gaussian-cosh mispricing distributions that allow for a P-bifurcation: unimodality occurs when mean-reversion is fast, bimodality when it is slow. The critical point of this bifurcation is established and refutes previous ad-hoc reports and differs from the bifurcation condition of the dynamical system itself. For fast, weakly coupled trends, deploying the Furutsu-Novikov theorem reveals that the result is again unimodal Gaussian. For the same case with higher coupling we disprove another claim from the literature: bimodal trend distributions do not generally imply bimodal mispricing distributions. The latter becomes bimodal only for stronger trend feedback. The exact solution in this last regime remains unfortunately beyond our proficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [100] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用大型语言模型生成合成新闻标题数据集来替代真实数据，以解决自然语言处理任务中的数据获取和隐私问题，特别关注负面情感文本。


<details>
  <summary>Details</summary>
Motivation: 克服真实世界数据获取的挑战和隐私担忧，为情感分析等NLP任务提供替代数据源。

Method: 使用定制提示生成负面新闻标题语料库，通过专家评审和嵌入空间分析验证，并与真实新闻标题进行多项基准测试比较。

Result: 生成的标题与真实标题高度匹配，仅在词性标注测试中的专有名词得分存在明显差异。

Conclusion: LLM生成的合成数据集可以作为真实数据的有效替代品，在内容、语气、长度和风格方面与真实负面新闻标题保持一致。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [101] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB基准测试评估大语言模型在气候变化领域的专业知识和证据支持能力，发现前沿模型具有博士级别的知识综合能力但存在严重的证据幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型处理复杂专业知识的真实能力，特别是在气候变化这一关键领域，需要基于真实用户问题和科学评估标准的可靠基准。

Method: 开发CLINB基准，包含真实用户问题、多模态问答任务和气候科学家制定的评估标准，采用基于模型的评估流程验证多个前沿模型。

Result: 前沿模型展现出博士级别的知识综合能力，甚至超过专家辅助的混合答案，但在证据基础方面存在严重问题，引用和图像幻觉率很高。

Conclusion: 知识综合与可验证归因之间的差距是AI在科学工作流中部署的关键障碍，需要像CLINB这样的可靠基准来构建可信AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [102] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP框架通过语言增强的多智能体强化学习，在经济决策中整合语言信息，显著提升了决策效果、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实经济决策不仅依赖结构化信号（如价格、税收），还依赖非结构化语言信息（如同行对话、媒体叙述）。传统多智能体强化学习难以处理语言的语义模糊性和上下文丰富性。

Method: LAMP采用Think-Speak-Decide流程：(1)Think模块解释数值观测，提取短期冲击和长期趋势，缓存高价值推理轨迹；(2)Speak模块基于推理生成和交换策略消息，通过解析同伴通信更新信念；(3)Decide模块融合数值数据、推理和反思到MARL策略中，优化语言增强的决策。

Result: 在经济模拟实验中，LAMP在累积回报（+63.5%，+34.0%）、鲁棒性（+18.8%，+59.4%）和可解释性方面均优于MARL和纯LLM基线。

Conclusion: 语言增强策略具有提供更有效和鲁棒经济策略的潜力，LAMP框架成功缩小了与现实世界设置的差距。

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [103] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying是一个合成的多LLM对话数据集，用于研究和检测网络欺凌，通过大语言模型模拟真实的欺凌互动，提供可扩展且伦理安全的替代方案。


<details>
  <summary>Details</summary>
Motivation: 为网络欺凌研究提供可扩展、伦理安全的数据收集方法，克服传统人类数据收集的局限性和伦理问题。

Method: 利用大语言模型生成模拟欺凌对话，构建包含多轮对话结构、上下文感知标注和细粒度标签的数据集。

Result: 数据集在对话结构、词汇模式、情感/毒性、角色动态、伤害强度和欺凌类型分布等五个维度上进行了评估，并验证了其作为独立训练数据和增强数据源的实用性。

Conclusion: SynBullying为网络欺凌检测提供了有效的合成数据解决方案，既能保护用户隐私，又能支持详细的欺凌行为分析。

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [104] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard是一种结合因果推理和符号逻辑的新方法，能够实时检测和防止大语言模型的幻觉问题，在12个基准测试中准确识别89.3%的幻觉，同时将虚假声明减少近80%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在严重的幻觉问题，会自信地陈述听起来合理但实际错误的信息，这成为在准确性要求高的场景中使用这些模型的主要障碍。现有解决方案要么需要重新训练整个模型，要么增加显著计算成本，或者未能解决幻觉的根本原因。

Method: CausalGuard通过两条互补路径工作：一条路径追踪模型所知内容与生成内容之间的因果关系，另一条路径使用自动推理检查逻辑一致性。与仅检查生成后输出的方法不同，该系统理解导致错误陈述的因果链，并在过程中早期进行干预。

Result: 在12个不同基准测试中，CausalGuard正确识别了89.3%的幻觉，仅漏掉8.3%的实际幻觉。更重要的是，它将近80%的虚假声明，同时保持回答的自然性和帮助性。该系统在需要多步逻辑的复杂推理任务上表现尤为出色。

Conclusion: CausalGuard通过展示其推理过程，在医疗诊断或金融分析等敏感领域表现良好，因为这些领域理解决策原因与决策本身同等重要。该方法为解决大语言模型的幻觉问题提供了有效的解决方案。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [105] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 提出了一个量化框架，通过将游戏建模为随机决策树来分离技能和运气成分，定义了技能-运气指数S(G)在[-1,1]范围内，并引入波动性Sigma来量化连续回合的结果不确定性。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统方法来量化游戏中技能和运气的相对贡献，以便进行游戏设计、AI评估和风险分析等方面的客观比较。

Method: 将游戏建模为随机决策树，分解游戏结果为技能杠杆K和运气杠杆L，定义技能-运气指数S(G) = (K-L)/(K+L)，并引入波动性Sigma来测量结果不确定性。

Result: 分析了30个游戏，发现从纯运气（抛硬币，S=-1）到纯技能（国际象棋，S=+1）的连续谱，扑克显示中等技能主导（S=0.33），西洋双陆棋处于平衡状态（S=0）。

Conclusion: 该框架可扩展到一般随机决策系统，为玩家影响力、游戏平衡性和预测稳定性提供了原则性比较方法，在游戏设计、AI评估和风险评估中具有应用价值。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [106] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个模块化、零样本的智能框架，通过分层提示分析和价值对齐推理来提升文本到图像生成的安全性，显著减少不安全输出同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型在创意媒体合成方面表现出色，但在面对对抗性提示时可能产生不安全、冒犯性或文化不恰当的内容。现有防御方法难以在不牺牲生成质量或增加高成本的情况下使输出与人类价值观对齐。

Method: VALOR框架整合了分层提示分析：多级NSFW检测器过滤词汇和语义风险；文化价值对齐模块识别社会规范、合法性和代表性伦理违规；意图消歧器检测微妙或不安全暗示。检测到不安全内容时，由大语言模型在动态角色特定指令下选择性重写提示。如果生成图像仍不安全，可进行风格化再生以引导输出到更安全的视觉领域。

Result: 在对抗性、模糊性和价值敏感提示上的实验表明，VALOR将不安全输出减少了高达100.00%，同时保持了提示的有用性和创造性。

Conclusion: VALOR是一个可扩展且有效的方法，可在开放世界环境中部署安全、对齐且有用的图像生成系统。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [107] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel是一个能够自主生成并实施量子物理学想法的LLM代理系统，它能够从文献中提取想法并使用领域特定AI工具将其转化为可立即在实验室实施的实验设计。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学领域的应用仍主要依赖人类提供研究问题和目标，AI生成的创意往往模糊且需要人工执行。开发能够自动化生成和实施想法的系统将显著改变人类在科学过程中的角色。

Method: AI-Mandel使用LLM从文献中提取想法，并通过领域特定的AI工具将这些想法转化为具体的实验设计方案。

Result: AI-Mandel生成的许多想法具有科学价值，其中两个想法已经促成了独立的后续科学论文。这些想法包括量子隐形传态的新变体、不定因果顺序中的量子网络原语，以及基于量子信息传输闭合回路的新几何相位概念。

Conclusion: AI-Mandel是能够生成和实施具体可行想法的AI物理学家原型系统。构建此类系统不仅有助于加速科学发展，还揭示了实现人类水平人工智能科学家所面临的具体挑战。

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [108] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的智能体框架，让小型LLM学习迭代构建SPARQL查询的策略，在LC-QuAD 2.0数据集上相比零样本基线提升了17.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂多跳问题中SPARQL查询生成的可靠性问题，当前方法缺乏基于实时执行反馈的动态调试能力。

Method: 使用仅3B参数的LLM，通过结果驱动的强化学习（GRPO）训练，无需监督微调，学习迭代构建和调试SPARQL查询的策略。

Result: 在LC-QuAD 2.0的可执行子集上达到49.7%的准确率，相比最强的迭代零样本基线提升了17.5个百分点。

Conclusion: 该工作为通过交互教授智能体掌握形式化符号工具提供了可推广的蓝图，弥合了概率性LLM与结构化知识图谱之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [109] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 论文质疑当前基于抽象智力概念（如ARC、Raven测试等）的AI评估方法，认为应转向基于通用性的评估框架，将通用性视为多任务学习问题，直接关联到可测量的性能广度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估过度依赖抽象智力概念，缺乏稳定定义且无法预测实际任务表现，可能导致评估与现实效用脱节。

Method: 通过概念和形式分析，检验智力评估的三个假设（通用性、稳定性、现实性），证明只有通用性经得起概念和实证检验。

Result: 智力不是实现通用性的原因；通用性应被理解为多任务学习问题，直接链接到可测量的性能广度和可靠性。

Conclusion: 应重新定义AI进展评估方式，将通用性作为评估多样化任务能力的更稳定基础。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [110] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估协议来准确评估LLMs在自然语言到一阶逻辑转换(NL-FOL)方面的能力，发现对话导向的LLMs表现出强大的逻辑理解能力，而嵌入中心模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有评估NL-FOL转换的数据集和协议存在局限性，可能无法准确反映LLMs的真实能力，需要设计新的评估方法来区分真正的语义级逻辑理解与表面模式识别。

Method: 提出了一个新颖的评估协议，专门设计用于区分真正的语义级逻辑理解与表面模式识别、记忆和数据集污染。

Result: 使用新方法评估发现，最先进的对话导向LLMs表现出强大的NL-FOL转换技能和真正的句子级逻辑理解能力，而嵌入中心模型表现明显更差。

Conclusion: 对话导向的LLMs在NL-FOL翻译方面具有强大的能力，新的评估协议能够更准确地评估模型的逻辑理解能力。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [111] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑属性的基准测试，用于严格评估大型视觉语言模型的全局视觉感知能力，发现现有模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准存在局部捷径问题，会高估模型的感知能力。视觉感知模块成为大型视觉语言模型的瓶颈，限制了其整体能力。

Method: 利用拓扑属性构建评估基准，因为拓扑依赖于图像的全局结构且对局部特征不变，能够实现无捷径的全局感知评估。

Result: 在最粗的感知粒度上，所有模型表现都不优于随机机会，表明模型缺乏全局视觉特征感知能力。更强大的模型反而准确率更低。

Conclusion: 仅扩大模型规模不足以解决全局感知缺陷，可能需要新的训练范式或架构。TopoPerception揭示了当前LVLMs的关键瓶颈并提供了改进方向。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [112] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O是一个端到端系统，能够将组织解剖视频转换为手势序列，并发现与术后结果相关的模式，为数据驱动的手术反馈和临床决策支持奠定基础。


<details>
  <summary>Details</summary>
Motivation: 术中行为的细粒度分析及其对患者结果的影响是一个长期存在的挑战。

Method: 利用基于transformer的空间和时间建模以及逐帧分类，F2O在机器人辅助根治性前列腺切除术的神经保留步骤中稳健检测连续短手势。

Result: F2O在帧级和视频级的手势检测AUC分别达到0.80和0.81；F2O衍生的特征预测术后结果的准确性与人工注释相当（0.79 vs 0.75）。

Conclusion: F2O通过实现自动可解释的评估，为数据驱动的手术反馈和前瞻性临床决策支持奠定了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [113] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI是一个LLM遗忘框架，通过惩罚边际信息来选择性移除待遗忘数据对模型的额外贡献，同时保留其他数据支持的信息，提供可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不断扩大数据集上训练，需要选择性移除特定数据对训练模型的影响，以满足隐私保护和监管合规要求，特别是对于资源密集型的大语言模型。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来明确界定并仅移除待遗忘数据对模型的额外贡献，同时保留其他数据支持的信息。

Result: 实验证明该方法优于现有最先进的遗忘方法，实现了可靠的遗忘效果，并在多样化基准测试中更好地保持了模型的通用性能。

Conclusion: 这一进展是使AI系统更具可控性、符合隐私和版权法规的重要一步，同时不损害其有效性。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [114] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: 本研究系统评估了四种大语言模型在抽象视觉推理任务中的表现，发现GPT-4.1-Mini在所有推理架构中表现最佳，但不同模型对架构设计具有特异性敏感度。


<details>
  <summary>Details</summary>
Motivation: 系统评估大语言模型在抽象视觉推理问题上的性能，比较不同模型和推理架构的表现差异。

Method: 使用四种LLM模型和四种推理架构在RAVEN-FAIR数据集上进行测试，通过三阶段处理流程生成视觉响应，使用SSIM和LPIPS指标评估，分析思维链分数和错误类型。

Result: GPT-4.1-Mini在所有架构中始终获得最高准确率，多智能体架构会改变语义和数值平衡但效果不一致，不同模型对架构设计具有特异性敏感度。

Conclusion: 推理效果具有模型特异性，响应覆盖度的变化使跨架构比较复杂化，采用五次独立运行的最佳结果作为性能上限估计。

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [115] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: 本章讨论了构建可靠AI系统（特别是智能体AI系统）面临的挑战和未来发展前景，重点分析了级联故障风险缓解、动态环境、任务执行不一致性、不可预测涌现行为等方面的研究问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统的发展，确保其可靠性变得越来越重要。这些系统在复杂环境中运行时可能面临级联故障、动态环境变化、任务执行不一致等多种风险，需要系统性的可靠性保障机制。

Method: 通过分析当前研究现状，识别了多个关键研究问题：级联故障风险缓解、动态环境适应性、任务执行一致性、涌现行为预测，以及可靠性测试评估方法。

Result: 提出了构建可靠智能体AI系统的多个研究方向和挑战，包括开发有效的故障检测和恢复机制、适应动态环境的算法、确保任务执行一致性的方法，以及资源高效的可靠性保障技术。

Conclusion: 可靠智能体AI系统的开发需要多方面的研究努力，包括故障缓解、环境适应性、行为预测和系统评估等关键领域，这些研究将推动AI系统在现实世界中的安全可靠部署。

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [116] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 本文提出了一种基于"反弹赢家通吃(RWTA)"基元的可扩展神经形态控制架构，结合了离散计算的可靠性和连续调节的可调性，能够统一处理连续节律生成和离散决策。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够同时处理连续节律生成和离散决策的统一神经形态控制架构，结合离散计算的可靠性和连续调节的可调性。

Method: 提出反弹赢家通吃(RWTA)基元作为基本构建模块，从细胞层面到系统层面构建架构，继承赢家通吃状态机的离散计算能力和可兴奋生物物理电路的连续调节能力。

Result: 通过蛇形机器人的神经系统设计展示了该架构的通用性、鲁棒性和模块化特性。

Conclusion: RWTA架构提供了一个统一的事件驱动框架，能够有效处理神经形态控制中的连续和离散计算需求。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [117] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: 提出CFA-SMOTE数据增强方法，结合可解释AI的反事实方法和SMOTE方法，解决气候变化预测中的类别不平衡问题，特别是在历史数据缺乏极端气候事件实例的情况下。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端天气事件频发，但传统机器学习方法依赖历史数据分布，难以处理分布外的异常事件。历史数据集缺乏足够的极端气候事件实例，造成预测困难。

Method: 提出CFA-SMOTE方法，将可解释AI中的实例反事实方法与经典的类别不平衡方法SMOTE相结合，生成代表异常气候事件的合成数据点来增强数据集。

Result: 在不同类别不平衡比例条件下，CFA-SMOTE方法在预测爱尔兰奶牛场草生长（针对2018年欧洲干旱和饲料危机）方面表现优于基准的反事实和类别不平衡方法。

Conclusion: CFA-SMOTE通过将气候变化预测问题视为类别不平衡问题，并生成反事实合成数据，能够有效改善在气候异常事件下的预测性能。

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [118] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 本文提出了一种混合神经符号框架，结合大型语言模型和符号逻辑，实现了对复杂法律中法定不一致性的确定性检测。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在处理层次化处理和深度结构化推理方面存在困难，特别是在长文本中。税收领域的特定应用仍然稀少，需要解决这些挑战。

Method: 使用GPT-4o将美国国内税收法典第121条翻译为Prolog规则，在SWISH中精炼。通过Prolog增强提示测试不一致性检测效果，并与纯自然语言提示对比。

Result: GPT-4o在三种策略中仅检测到一种不一致性（33%准确率）。纯自然语言提示实现100%规则覆盖，而Prolog增强提示仅66%覆盖。混合Prolog模型产生确定性结果，成功检测到不一致区域。

Conclusion: 基于符号逻辑的LLM辅助形式化能够实现透明可靠的法定不一致性检测，混合方法优于纯概率性提示方法。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [119] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 提出基于直接依赖检索(DDR)的新检索增强框架，用于解决数学陈述自动形式化中的上下文感知不足和依赖检索精度低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法缺乏上下文感知，导致形式定义和定理的幻觉，且当前检索增强方法在形式库依赖检索方面精度和召回率差，无法有效利用不断增长的公共数据集。

Method: 提出DDR方法，直接从自然语言数学描述生成候选库依赖，然后通过高效后缀数组检查验证其在形式库中的存在性，构建了超过50万个样本的依赖检索数据集并微调高精度DDR模型。

Result: DDR模型在检索精度和召回率上显著优于最先进方法，配备DDR的自动形式化器在单次尝试准确率和多次尝试稳定性方面均优于使用传统基于选择的RAG方法的模型。

Conclusion: DDR框架有效解决了数学陈述自动形式化中的依赖检索问题，提供了更精确和可扩展的解决方案。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [120] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 提出Chain-of-Evidence (CoE)范式，将思维链推理与视觉证据归因结合，通过强化学习框架LAT训练模型生成可验证的推理路径，在视觉文档检索增强生成中实现可靠的证据溯源。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏细粒度监督和推理过程的渐进可追溯性，无法确保视觉语言模型在多模态问答中的预测可靠性和可验证性。

Method: 提出Look As You Think (LAT)强化学习框架，评估证据区域的归因一致性，仅在CoE轨迹产生正确答案时提供奖励，鼓励过程级自我验证。

Result: 在Paper-和Wiki-VISA基准测试中，LAT在单图和双图设置下均提升原始模型性能，软精确匹配平均提升8.23%，IoU@0.5提升47.0%，且优于监督微调基线，具有更强的跨领域泛化能力。

Conclusion: CoE范式通过证据链推理和视觉归因的统一，显著提升了视觉文档检索增强生成的可验证性和可靠性，LAT框架有效训练模型生成可追溯的推理过程。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [121] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过自学习范式将多模态大语言模型从被动模式识别转变为证据关联的诊断推理，无需大量标注数据即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI系统缺乏人类可读的推理过程，难以审计决策和防止错误，限制了临床应用。

Method: 采用两阶段自学习过程：多样化阶段扩展病理学风格解释，优化阶段精炼解释以提高准确性；无需白盒访问或权重更新。

Result: 在乳腺癌和前列腺癌数据集上，RECAP-PATH生成的推理与专家评估一致，诊断准确性显著优于基线方法。

Conclusion: RECAP-PATH通过结合视觉理解和推理能力，提供了临床可信赖的AI，展示了证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [122] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: 提出了一种多路径差异化裁剪近端策略优化算法（MPD-PPO），用于解决橡胶轮胎制造中的高维多目标优化问题，在轮胎薄膜生产的宽度和厚度控制中表现出色。


<details>
  <summary>Details</summary>
Motivation: 智能制造需要解决传统集中式调度和生产配置的局限性，特别是应对动态生产需求。轮胎制造系统包含复杂的非线性交互和涌现动态，多子系统协调成为关键挑战。

Method: 采用多分支策略架构和差异化梯度裁剪约束的深度强化学习算法MPD-PPO，确保高维策略更新的稳定性和效率。

Result: 在橡胶轮胎薄膜生产的宽度和厚度控制实验中，MPD-PPO在调谐精度和操作效率方面均有显著提升。

Conclusion: 该框架成功解决了高维度、多目标权衡和动态适应等关键挑战，为轮胎制造实时工业部署提供了增强的性能和生产稳定性。

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [123] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 本文提出T-BoN BO框架，通过结合Best-of-N选择和文本梯度来模拟UCB采集函数，在语言空间中实现评估高效的贝叶斯优化，用于AI自我改进。


<details>
  <summary>Details</summary>
Motivation: 当前AI自我改进研究主要关注查询效率，但在许多社会应用中，评估成本远高于生成成本。需要开发针对评估效率优化的方法。

Method: 提出T-BoN BO框架，证明Best-of-N选择策略与文本梯度组合能够统计模拟UCB采集函数的梯度行为，实现语言空间的贝叶斯优化。

Result: 在自动广告对齐任务中验证了T-BoN BO的性能，相比现有最优基线方法表现出优越性能。

Conclusion: T-BoN BO为AI自我改进提供了一个简单且评估高效的贝叶斯优化框架，特别适用于评估成本高的应用场景。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [124] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 提出了Embedding CFR算法，通过将信息集嵌入到低维连续空间来解决大规模不完全信息扩展式博弈，相比基于聚类的抽象方法能更快收敛且降低可利用性。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法依赖预训练的离散聚类进行抽象，但硬分类会不可逆地丢失信息集之间的量化细微差异，这些差异对策略求解至关重要。

Method: 受自然语言处理中词嵌入范式启发，将孤立信息集的特征预训练并嵌入到相互连接的低维连续空间，在该空间中通过遗憾积累和策略更新进行策略求解。

Result: 在扑克实验中，在相同空间开销下，Embedding CFR相比基于聚类的抽象算法实现了显著更快的可利用性收敛。

Conclusion: 这是扑克AI中首个通过低维嵌入预训练信息集抽象来进行策略求解的算法，能更精确地捕捉信息集间的差异和联系。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [125] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 提出了KrwEmd算法，通过k-recall赢率特征和Earth Mover's距离来聚类信号观察信息集，解决了德州扑克等游戏中过度抽象的问题，显著提升了AI性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模不完全信息游戏中过度抽象的问题，特别是极端不完美回忆抽象完全丢弃历史信息导致的AI性能下降。

Method: 引入k-recall赢率特征来定性和定量区分信号观察信息集，然后开发KrwEmd算法使用Earth Mover's距离聚类这些信息集。

Result: 实验结果表明，KrwEmd相比现有算法显著提高了AI的游戏表现。

Conclusion: KrwEmd是第一个实用的解决过度抽象问题的算法，通过有效利用历史和未来游戏信息来改善AI性能。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [126] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 提出了一个综合解决方案来缓解小模型的知识蒸馏过程中的灾难性遗忘问题，包括构建包含元认知知识的数据集和开发GDPO训练方法


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法在将大语言模型的推理能力压缩到小模型时会导致灾难性遗忘，特别是对于小于8B的模型，主要问题包括数据与模型内在能力关系不明确、训练目标无法有效约束知识保留

Method: 1) 构建包含5K实例的数据集，覆盖多种推理任务并融入元认知知识；2) 提出GDPO（组方向偏好优化）训练方法，通过参考模型隐式约束优化路径，更适合资源受限场景

Result: 大量实验表明该方法显著缓解了灾难性遗忘，并提升了小模型的推理性能

Conclusion: 通过数据层面的元认知知识整合和训练层面的GDPO优化，能够有效解决小模型知识蒸馏中的灾难性遗忘问题

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [127] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: RTMol是一个双向对齐框架，通过自监督的往返学习统一分子标注和文本到SMILES生成，解决了现有方法在化学准确性、数据质量和双向一致性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有方法将分子标注和文本到分子设计视为独立任务，存在三个关键问题：传统指标偏重语言流畅性而非化学准确性、训练数据包含化学模糊描述、独立优化导致双向不一致。

Method: 提出RTMol框架，通过自监督往返学习统一分子标注和文本到SMILES生成，引入新颖的往返评估指标，支持无需配对分子-文本语料的无监督训练。

Result: 实验表明RTMol在各种LLM上将双向对齐性能提升了高达47%，为联合分子-文本理解和生成建立了有效范式。

Conclusion: RTMol通过双向对齐框架有效解决了分子序列表示与文本描述对齐的关键挑战，在药物发现、材料设计等领域具有重要应用价值。

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [128] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: 提出了DRedMTL算法，一种支持有界区间的DatalogMTL增量推理方法，显著优于重新物化方法


<details>
  <summary>Details</summary>
Motivation: 现有DatalogMTL推理方法不支持高效的动态更新，而现实应用需要频繁数据更新

Method: 基于经典DRed算法，设计了专门操作符来处理DatalogMTL物化的周期性表示

Result: 在多个公开数据集上的实验表明，DRedMTL通常显著优于重新物化方法，有时快几个数量级

Conclusion: DRedMTL为DatalogMTL提供了高效的增量推理能力，满足实际应用中的动态更新需求

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [129] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: DoM是一个用于不完整知识图谱问答的新框架，通过多智能体辩论机制动态融合结构化知识图谱和非结构化外部文本，解决知识图谱不完整问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的知识图谱往往不完整，现有方法无法自适应地融合多源知识，无法充分利用它们的互补优势。

Method: 基于多智能体辩论范式，DoM分配专门智能体分别对知识图谱和外部文本进行推理，通过迭代交互协调它们的输出。它分解输入问题为子问题，通过双智能体（KG和RAG）检索证据，并使用法官智能体评估和聚合中间答案。

Result: 通过大量实验，DoM在性能上持续优于最先进的基线方法。

Conclusion: DoM框架通过知识互补性利用和增强对知识图谱不完整性的鲁棒性，为不完整知识图谱问答提供了有效的解决方案。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [130] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 提出ViTE框架，通过虚拟图和专家路由器解决行人轨迹预测中高阶交互建模的深度与计算成本权衡问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模高阶交互时面临深度不足导致感受野受限与深度过深导致计算成本高昂的权衡问题，需要能够自适应建模显式一跳交互和隐式高阶依赖的方法

Method: ViTE框架包含两个核心模块：虚拟图引入动态虚拟节点建模长距离和高阶交互而无需深层GNN堆叠；专家路由器基于社交上下文自适应选择交互专家，采用专家混合设计

Result: 在三个基准数据集（ETH/UCY、NBA和SDD）上的实验表明，该方法持续达到最先进的性能，验证了其有效性和实际效率

Conclusion: ViTE框架通过虚拟图和专家路由器的组合，实现了对不同交互模式的灵活和可扩展推理，在行人轨迹预测任务中取得了优异性能

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [131] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: 本文通过哲学案例研究批判性检验世界模型框架是否能充分表征人类水平的理解能力


<details>
  <summary>Details</summary>
Motivation: 人类拥有心理世界模型，研究AI模型是否具有类似表征可能表明它们以类人方式"理解"世界

Method: 使用科学哲学文献中的案例研究，重点关注世界模型能力与人类理解差异最明显的哲学分析

Result: 这些哲学观点展示了世界模型的局限性，虽然它们不代表普遍定义，但有助于探索世界模型的边界

Conclusion: 世界模型框架在表征人类水平理解方面存在局限性，需要更深入地探讨理解与模拟之间的区别

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [132] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA是一个基于视觉的意外拔管风险检测系统，完全在合成ICU视频数据集上开发和验证，通过姿态估计识别碰撞和躁动两种高风险运动模式，解决了ICU视频数据获取的隐私和伦理挑战。


<details>
  <summary>Details</summary>
Motivation: ICU中意外拔管是严重的安全问题，但由于伦理和隐私限制难以获取标注的ICU视频数据，需要开发隐私保护的实时检测方法。

Method: 利用文本到视频扩散生成多样化的临床真实ICU场景，通过姿态估计识别手部进入气道管附近区域的碰撞行为和基于解剖关键点速度的躁动行为。

Result: 专家评估确认合成数据的真实性，性能评估显示碰撞检测准确率高，躁动识别表现中等。

Conclusion: 这项工作展示了开发隐私保护、可复现的患者安全监控系统的新途径，具有在ICU环境中部署的潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [133] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: 提出了Mobile-Agent-RAG框架，通过双级检索增强解决移动代理在长时跨应用任务中的战略幻觉和操作错误问题，显著提升了任务完成率和步骤效率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的移动代理在真实世界、长时跨应用任务中成功率不足，主要问题在于过度依赖MLLM的静态内部知识，导致战略层面的幻觉和操作层面的错误。

Method: 提出分层多代理框架Mobile-Agent-RAG，包含Manager-RAG用于规划阶段检索人类验证的全面任务计划，Operator-RAG用于执行阶段检索精确的低级指导，并构建了两个专门的检索知识库。

Result: 在Mobile-Eval-RAG基准测试中，Mobile-Agent-RAG显著优于最先进基线，任务完成率提高11.0%，步骤效率提升10.2%。

Conclusion: Mobile-Agent-RAG为上下文感知、可靠的多代理移动自动化建立了稳健的范式，有效解决了移动代理在真实场景中的关键瓶颈问题。

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [134] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 本文提出了一种解决LLM道德对齐问题的新方法，通过构建包含680个道德场景的数据集Moral-Reason-QA，使用Group Relative Policy Optimization算法训练LLM在未见过的道德场景中应用一致的道德推理框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对人类道德决策的影响日益增强，但现有方法主要关注评估而非主动引导其道德决策，需要解决分布外道德对齐问题。

Method: 构建Moral-Reason-QA数据集，包含功利主义、义务论和美德伦理学框架的推理轨迹；使用Group Relative Policy Optimization算法，通过复合奖励同时优化决策对齐和框架特定推理过程。

Result: 实验结果显示在分布外评估集上，功利主义框架的softmax归一化对齐分数提高了+0.757，义务论框架提高了+0.450，成功实现了对未见道德场景的泛化。

Conclusion: LLM代理可以系统性地训练以内在化并应用特定道德框架到新情境，为AI安全提供了关键基础。

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [135] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench是一个基于真实Upwork工作任务的动态基准测试，用于评估LLM代理在真实工作环境中的表现，采用基于专家构建的评估标准进行细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多是静态、合成或领域受限的，无法反映AI代理在动态、经济意义环境中的真实表现，需要基于真实工作场景的评估框架。

Method: 从Upwork劳动力市场提取真实工作任务，由专家自由职业者分解为详细可验证的接受标准，构建基于标准的评估框架对AI提交进行细粒度评估。

Result: 建立了基于真实工作活动的评估基准，能够分析模型优势、弱点和指令遵循保真度，超越简单的通过/失败二元指标。

Conclusion: UpBench提供了一个可扩展、以人为中心的基准测试基础，支持在真实劳动力市场环境中评估代理系统，为AI通过合作而非替代来增强人类能力提供了路径。

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [136] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出RGR-GRPO框架，通过评分标准提供细粒度奖励和离线指导，解决现有强化学习方法在复杂推理任务中的局限性，在多领域推理任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注单一领域且依赖可验证奖励，纯在线RL框架限制了探索空间，从而限制了推理性能。

Method: RGR-GRPO框架利用评分标准提供密集信息奖励和离线指导，在GRPO训练期间允许更大的解决方案空间探索。

Result: 在14个多领域基准测试中，RGR-GRPO始终优于仅依赖替代奖励方案或离线指导的RL方法，在数学、物理、化学和一般推理任务上分别平均提升7.0%、5.4%、8.4%和6.6%。

Conclusion: RGR-GRPO在离策略训练期间保持稳定的熵波动，实现卓越的pass@k性能，反映了持续的探索和有效突破现有性能瓶颈。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [137] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: 本文提出了计算理性用户模型，用于建模认知受限代理在偏见信念下的最优行为，重点解决从被动观察中推断用户认知界限和偏见信念状态的问题。


<details>
  <summary>Details</summary>
Motivation: 预测和理解用户或人类合作者的次优行为是一个关键挑战，这些行为通常不是非理性的，而是在认知界限和偏见信念下的理性决策。

Method: 提出基于嵌套粒子滤波的高效在线推理方法，同时跟踪用户的潜在信念状态并从未知认知界限中估计参数，以记忆衰减作为认知界限的代表性示例。

Result: 在导航任务模拟中验证了模型能生成与不同记忆容量对应的直观合理行为，推理方法能在有限观察步骤内准确高效地恢复真实认知界限。

Conclusion: 该方法为开发自适应AI助手提供了理论基础，使辅助系统能够考虑用户的记忆限制进行自适应协助。

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [138] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 提出了一个动态学习建议者可靠性的框架，在部分可观测环境中通过贝叶斯推断调整对建议的依赖，并引入战略性的"询问"动作来平衡信息获取与成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设建议者质量参数是静态且已知的，限制了实际部署。需要处理建议可靠性变化的问题，为自适应人机协作提供基础。

Method: 将建议者质量直接集成到智能体的信念表示中，通过贝叶斯推断学习建议者类型；引入显式的"询问"动作，允许在关键时刻战略性地请求建议。

Result: 实验评估显示在不同建议者质量下具有鲁棒性能，能够适应可靠性变化，并有效管理建议请求策略。

Conclusion: 该工作通过处理不确定环境中的建议不确定性，为自适应人机协作提供了基础框架。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [139] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 开发了一个基于临床验证流程图的对话式自我分诊系统，通过多智能体框架实现95.29%的流程图检索准确率和99.10%的导航准确率，结合自由文本交互的灵活性和标准化临床协议的严谨性。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大型语言模型在医疗决策中可靠性有限，存在准确性低、缺乏透明度和易受未经验证信息影响的问题，需要结构化且可审计的患者决策支持框架。

Method: 使用美国医学会100个临床验证流程图指导LLMs，采用包含检索智能体、决策智能体和聊天智能体的多智能体框架，通过合成数据集进行大规模性能评估。

Result: 在2000个测试案例中实现95.29%的前3准确率流程图检索，在37200个不同对话风格和条件下实现99.10%的流程图导航准确率。

Conclusion: 该方法展示了透明、准确且可推广的AI辅助自我分诊的可行性，有潜力支持知情患者决策并改善医疗资源利用。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [140] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 提出了ARCHE任务，要求模型将复杂推理分解为标准推理范式的组合，形成推理逻辑树(RLT)，包含演绎、归纳和溯因三种推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的推理内容通常是非结构化和非正式的，难以判断模型是否真正理解科学推理的基本范式。

Method: 引入ARCHE任务和RLT结构，开发ARCHE Bench基准数据集，包含来自70篇Nature Communications文章的1900多个参考文献和38000个观点，并提出EC和REA两个逻辑感知评估指标。

Result: 评估10个领先LLM发现，模型在REA和EC之间存在权衡，目前没有模型能够提取完整且标准的推理链。

Conclusion: 当前推理模型的能力与科学论证所需的严谨性之间存在显著差距。

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [141] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT是一个针对限价订单簿数据的通用基础模型，通过新的标记化方案将多维消息作为单个标记处理，在预测中间价格变动和下一消息等任务中表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有的LOB模型需要繁琐的数据表示，缺乏原始任务之外的适应性，因此需要开发一个适用于下游微调的通用基础模型。

Method: LOBERT基于BERT架构，采用新的标记化方案，将完整的多维消息作为单个标记处理，同时保留价格、数量和时间的连续表示。

Result: LOBERT在预测中间价格变动和下一消息等任务中取得领先性能，同时相比先前方法减少了所需的上下文长度。

Conclusion: LOBERT为LOB数据提供了一个有效的通用基础模型，在多个任务中表现出色且具有更好的适应性。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [142] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: PCRS-TKA是一个基于提示的框架，通过检索增强生成将预训练语言模型与知识图谱结合，构建对话特定知识树并序列化为文本，实现结构感知推理和语义对齐，显著提升对话推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将PLMs与KGs结合时存在三个关键挑战：未能充分利用PLM在图关系上的推理能力、不加区分地整合检索到的知识、在多轮对话中忽视协同偏好。

Method: 提出PCRS-TKA框架，构建对话特定知识树并序列化为文本，选择性过滤上下文相关知识，显式建模协同偏好，通过语义对齐模块协调异构输入。

Result: 广泛实验表明PCRS-TKA在推荐和对话质量方面始终优于所有基线方法。

Conclusion: PCRS-TKA通过结构感知推理、选择性知识整合和协同偏好建模，有效提升了对话推荐系统的准确性和对话质量。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [143] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该研究探索了AI在道德困境中的不确定性，发现通过引入推理时的随机性可以增加模型的总熵和互信息，从而改善AI与人类道德偏好的一致性。


<details>
  <summary>Details</summary>
Motivation: 人类在道德困境中表现出显著的不确定性，但AI系统（特别是大语言模型）往往过度自信。随着AI越来越多地参与伦理决策，理解其道德推理中的不确定性对于构建可靠的AI系统至关重要。

Method: 使用经典的电车问题，分析32个开源模型在9个道德维度上的响应。通过引入推理时的"dropout"来增加随机性，并测量二元熵作为总熵、条件熵和互信息的线性组合来量化不确定性。

Result: 模型间的置信度差异大于道德维度内的差异；引入随机性机制主要增加了互信息，而条件熵基本不变；该机制显著提高了人类-LLM道德对齐度，互信息与对齐分数变化呈正相关。

Conclusion: 通过有意调节不确定性和降低LLM在道德复杂场景中的置信度，可以更好地对齐模型决策与人类偏好，这为构建更可靠的AI道德决策系统提供了重要启示。

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [144] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种动态树数据库变体，用于压缩命题和数值变量的状态集，在保持静态树数据库优点的同时避免了大量内存预分配问题。


<details>
  <summary>Details</summary>
Motivation: 在大型任务中扩展显式状态空间搜索时，如何紧凑表示生成的状态集合是一个核心挑战。静态树数据库虽然每个状态只需要恒定空间，但需要大量内存预分配。

Method: 开发了树数据库的动态变体，用于压缩命题和数值变量的状态集，并证明其保持了静态对应物的理想特性。

Result: 在经典和数值规划任务上的实证评估显示，压缩比达到几个数量级，且通常运行时开销可忽略不计。

Conclusion: 动态树数据库在状态压缩方面表现出色，为大规模状态空间搜索提供了有效的解决方案。

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [145] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 提出了一种策略条件合作者框架，通过变分自编码器学习策略空间，聚类识别策略类型，并训练基于这些策略的合作者代理，结合后悔最小化算法实现实时适应新伙伴。


<details>
  <summary>Details</summary>
Motivation: 在人类-代理团队中，人工代理需要实时适应具有独特偏好和动态变化策略的人类伙伴，这在时间压力和复杂策略空间的任务中尤其具有挑战性。

Method: 使用变分自编码器从代理轨迹数据中学习潜在策略空间，通过聚类识别不同策略类型，训练基于策略类型的合作者代理，并利用固定份额后悔最小化算法进行在线适应。

Result: 在修改版Overcooked环境中的实验和在线用户研究表明，该方法在与新人类和代理队友配对时达到了最先进的性能。

Conclusion: 提出的策略条件合作者框架能够有效表示、分类和实时适应广泛的潜在伙伴策略，在复杂协作任务中表现出色。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [146] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: 现代高维嵌入空间中的随机游走可以产生与人类语义流畅性任务中观察到的优化觅食行为一致的结果，而更复杂的Metropolis-Hastings采样反而与人类行为不符。


<details>
  <summary>Details</summary>
Motivation: 研究现代高维嵌入空间是否能提供足够的表示能力，使算法能够匹配人类在语义流畅性任务中观察到的觅食模式行为。

Method: 使用最先进的嵌入表示和先前的语义流畅性数据，在嵌入空间上进行随机游走和Metropolis-Hastings采样，比较它们与人类行为的匹配程度。

Result: 简单的随机游走在嵌入空间中产生了与优化觅食和边际价值定理一致的结果，而更复杂的Metropolis-Hastings采样反而无法产生与人类行为一致的结果。

Conclusion: 适当结构化的嵌入空间即使使用简单采样也能产生接近优化的觅食动态，挑战了复杂采样机制必然能产生更好认知模型的假设，支持Hills(2012)而非Abbott(2015)的观点。

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [147] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: Event-CausNet框架利用LLM量化非结构化事件报告，构建因果知识库，并通过因果注意力机制将因果知识注入GNN-LSTM网络，显著提升非重复性事件下的交通预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统时空GNN在处理重复性交通模式时表现出色，但在事故等非重复性事件中可靠性急剧下降，因为GNN本质上是相关性模型，无法应对扰动引入的新因果因素。

Method: 使用大语言模型量化非结构化事件报告，通过估计平均处理效应构建因果知识库，采用双流GNN-LSTM网络和新型因果注意力机制来调整和增强预测。

Result: 在真实数据集上的实验表明，Event-CausNet将预测误差（MAE）降低了高达35.87%，显著优于最先进的基线方法。

Conclusion: 该框架弥合了相关性模型与因果推理之间的差距，提供了更准确、可迁移且具有关键可解释性的解决方案，为关键扰动期间的实际交通管理提供了更可靠的基础。

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [148] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 该研究使用强化学习优化异构卫星集群在地球观测任务中的资源分配，通过多智能体强化学习算法实现异构卫星间的有效协调，平衡成像性能与资源利用。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测任务中的实时性、不确定性和分散性特点，因此需要采用强化学习和多智能体强化学习来实现自适应决策。

Method: 研究系统地从单卫星到多卫星场景制定优化问题，使用Basilisk和BSK-RL框架构建近真实仿真环境，评估MAPPO、HAPPO、HATRPO等先进MARL算法。

Result: 结果显示MARL能够在异构卫星间实现有效协调，平衡成像性能和资源利用，同时减轻非平稳性和智能体间奖励耦合问题。

Conclusion: 研究为可扩展的自主卫星操作提供了实用见解，并为异构动态条件下智能地球观测任务规划的后续研究奠定了基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [149] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: 本文研究了终身学习ILP，利用逻辑规则的组合性和可转移性来高效学习新问题，提出了一个组合框架，验证了该范式的可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 解决神经符号AI中的ILP问题是一个关键挑战。大多数研究专注于为单个问题设计新的网络架构，而较少探索涉及问题序列的新学习范式。

Method: 引入了组合框架，展示如何将早期任务中获取的逻辑规则高效地重用于后续任务，从而提高可扩展性和性能。

Result: 在任务序列上进行了实证评估，实验结果验证了该范式的可行性和优势。

Conclusion: 这一研究为神经符号AI中的持续学习开辟了新方向。

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [150] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: 使用被动脑机接口（BCI）和功能性近红外光谱（fNIRS）神经信号来指导强化学习代理训练，构建了一个包含25名参与者在三个领域的数据集，并展示了从fNIRS信号预测代理性能的可行性。


<details>
  <summary>Details</summary>
Motivation: 通过整合人类神经反馈来对齐代理行为与人类偏好，为未来脑驱动的RLHF系统奠定基础。

Method: 收集fNIRS记录，训练分类器预测代理性能水平（最优、次优、最差），训练回归器预测动作与最优策略的偏差程度，并评估跨主体泛化能力。

Result: 二元分类平均F1分数67%，多分类平均46%；通过少量主体特定数据微调后，F1分数分别提升17%和41%。

Conclusion: 从隐式fNIRS信号映射到代理性能是可行的，并且可以通过微调改进，为脑驱动的RLHF系统奠定了基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [151] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: 提出了一种基于偏好的策略优化框架PbPO，通过主策略与奖励模型之间的min-max博弈来引导LLM与人类偏好对齐，无需大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 通过偏好驱动的策略优化为LLM对齐提供了一条有前景的路径，避免依赖大量手动标注，实现模型行为与人类偏好的有效对齐。

Method: 将学习过程建模为主策略与奖励模型之间的min-max博弈，奖励模型约束在偏好数据导出的置信集内，采用迭代在线算法通过引导探索主动收集偏好数据。

Result: 在五个基准测试上的广泛实验表明，该方法持续优于现有的最先进偏好优化技术。

Conclusion: 提出的PbPO框架为LLM的自举提供了有效解决方案，具有理论保证和实际性能优势。

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [152] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: 提出了在集成HTN规划和LLM聊天机器人背景下在线学习分层任务网络方法的技术，通过从ChatGPT生成的任务分解中学习泛化方法，减少对ChatGPT的调用次数。


<details>
  <summary>Details</summary>
Motivation: 在HTN规划中，当没有可用的任务分解方法时，需要频繁调用ChatGPT来生成分解，这既耗时又昂贵。

Method: 扩展ChatHTN规划器，当ChatGPT生成任务分解时，学习泛化方法而非简单记忆，创建可应用于同类任务实例的方法。

Result: 在两个领域上的实验表明，在线学习过程减少了ChatGPT调用次数，同时解决了至少同样多的问题，在某些情况下甚至更多。

Conclusion: 在线学习HTN方法能有效减少对LLM的依赖，提高规划效率，同时保持或提升问题解决能力。

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [153] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: 提出了Chain-of-Scheduling (CoS)框架，通过探索、验证和集成三个阶段激活大语言模型的事件调度能力，在效率和效果之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的事件调度推荐方法在效率、效果和泛化性之间存在固有权衡，由于问题的NP-hard性质。需要一种能够最大化用户偏好同时满足时间和地理约束的有效推荐方法。

Method: CoS框架将调度任务分解为探索、验证和集成三个原子阶段，通过知识蒸馏使LLM能够自主生成调度链。

Result: 在三个真实世界数据集上，CoS实现了接近理论最优的效果，具有高效率和可解释性，并在域外数据上展现出强大的零样本学习能力。

Conclusion: CoS框架成功激活了LLM的事件调度能力，在保持高效率的同时实现了接近最优的推荐效果，并具有良好的泛化性能。

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [154] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow是一个基于LLM的多智能体系统，用于电网故障诊断，能够从法规和专家知识中提取并整合逻辑，生成可执行的自动化工作流。


<details>
  <summary>Details</summary>
Motivation: 当前电网故障诊断依赖人工方法，效率低、易出错且难以维护。需要将法规文本和专家知识整合到可验证、可执行的工作流中。

Method: 使用LLM从法规中提取结构化故障树，通过人机交互界面整合专家知识，用AlphaEvolve模块优化推理逻辑，最终合成n8n可执行工作流。

Result: 在变压器故障诊断数据集上验证，达到100%拓扑一致性和高语义保真度，显著减少专家工作量。

Conclusion: Fault2Flow建立了从故障分析到操作自动化的可复现路径，为电网故障诊断提供了高效可靠的自动化解决方案。

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [155] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3是一个通用代理框架，首次实现了在三个异构策略游戏环境中的自主跨平台操作，通过结合Qwen2.5-VL的视觉语言推理和UI-TARS的精确执行能力，显著提升了多模态数据处理的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨平台策略游戏中自动化操作的需求，探索视觉语言模型在复杂人机交互场景中的应用潜力，特别是在动态战场条件下的鲁棒泛化能力。

Method: 集成Qwen2.5-VL的视觉语言推理和UI-TARS的精确执行能力，采用闭环流水线（屏幕捕获、模型推理、动作执行），并通过系统消融研究评估不同多模态数据组合策略。

Result: 混合策略（融合多图像和视频数据，同时混合静态图像）相比完全融合减少63%推理时间，BLEU-4得分从4.81%提升至62.41%（约12.98倍提升），展示了强大的实时性能和跨平台泛化能力。

Conclusion: Yanyun-3不仅为策略游戏自动化提供了高效解决方案，还通过结构化多模态数据组织建立了增强VLM性能的通用范式，为具身智能中静态感知与动态推理的相互作用提供了新见解。

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [156] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个为科学推理和早期药物发现设计的领域一致性结构系统，通过知识图谱支架和轻量级验证器来引导LLM生成数学和生物医学上有效的输出。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在科学推理和药物发现中缺乏领域一致性的问题，确保生成的输出在数学和生物医学上有效。

Method: 使用紧凑的知识图谱支架配合轻量级验证器，将精选的符号事实注入提示中，并通过确定性检查器强制执行规则满足。将生成形式化为约束推理，引入适合解码的软引导替代方法。

Result: 在90个任务中，MedRule-KG相对于强链式思维基线将违规数量减少了83.2%，同时提高了精确匹配率。结果在分层下保持稳定，并随数据集规模扩展，验证器仅增加可忽略的延迟。

Conclusion: MedRule-KG为交互式设计提供了一种实用的方法，能有效提高LLM在科学推理和药物发现中的领域一致性。

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [157] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach是一个模型无关的自进化框架，为网页浏览代理提供持久跨会话记忆，通过记忆存储和经验检索机制提升长期规划和持续学习能力，无需重新训练即可提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM代理在网页导航中面临重复错误和无法跨会话学习的问题，限制了长期鲁棒性和样本效率。

Method: WebCoach包含三个核心组件：WebCondenser标准化导航日志为摘要，External Memory Store组织完整轨迹作为经验，Coach基于相似性和时效性检索相关经验并通过运行时钩子向代理注入任务特定建议。

Result: 在WebVoyager基准测试中，WebCoach持续提升三种不同LLM骨干的浏览器代理性能。使用38B模型时，任务成功率从47%提升至61%，同时减少或维持平均步骤数。较小基础模型配合WebCoach能达到与使用GPT-4o的相同网页代理相当的性能。

Conclusion: WebCoach通过赋予网页代理持久跨会话记忆能力，实现了无需重新训练的自进化，显著提升了复杂浏览任务的鲁棒性和性能。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [158] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: 提出了GEM方法，通过生成式熵引导偏好建模，在低资源和领域特定场景下对齐大语言模型，无需大规模标注数据。


<details>
  <summary>Details</summary>
Motivation: 在医学、法律等专业领域，获取大规模偏好标注数据通常不可行，需要开发能够在少量数据下实现模型对齐的方法。

Method: 基于决策熵理论，使用思维链提示生成多样化推理链，引入token评分机制对推理链进行排序加权，然后使用自评估群体优势算法(SEGA)进行微调。

Result: 在通用基准和领域特定任务（如数学推理和医疗对话）上的实验表明，GEM在少量偏好数据下取得了显著改进。

Conclusion: GEM建立了一个熵引导的闭环认知优化框架，使LLM能够依赖自身判断，实现高效的少样本对齐。

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [159] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 本文评估语言模型在对话中构建和更新内部世界模型的能力，测试其在语言变化下的鲁棒性，并提出基于层正则化的微调策略来抑制有害层的影响。


<details>
  <summary>Details</summary>
Motivation: 真实对话包含丰富的语用元素，需要构建局部世界模型来编码这些元素并捕捉其状态变化。但目前尚不清楚语言模型是否能构建或维护强大的隐式对话表示。

Method: 对流行数据集中的对话应用七种最小语言变化，构建两个包含是非问题的基准测试。评估多种开源和闭源语言模型，并提出双视角可解释性框架识别有用和有害的Transformer层。

Result: 语言模型在维持鲁棒准确性方面表现不佳，难以记忆关键细节（如跟踪语言变化下的实体）。有害层通常由于编码虚假信号或依赖捷径而影响语言变化。

Conclusion: 语言模型在对话中构建和更新内部世界模型的能力有限，但通过层正则化微调策略可以抑制有害层的影响，提高模型性能。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [160] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: 该论文分析了大型语言模型在数学证明验证中的表现，发现单一基准测试可能导致误导性结论，提出结合证明验证和最终答案评估的方法，并确定了GenSelect和LLM-as-a-Judge组合为最有效的验证框架。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在最终答案数学问题上表现优异，但其推理过程往往存在缺陷。为了推进到严谨的证明基础数学，需要可靠的证明验证能力。

Method: 分析了多种评估设置，结合证明验证和最终答案推理进行评估；将两种生成验证方法（GenSelect和LLM-as-a-Judge）扩展到百万token规模；研究提示选择对LLM-as-a-Judge性能的影响，并使用强化学习降低敏感性。

Result: GenSelect和LLM-as-a-Judge的组合是最有效的验证框架；提示选择显著影响LLM-as-a-Judge性能，但强化学习可以降低这种敏感性；尽管强化学习改善了证明级指标，但并未提高最终答案精度。

Conclusion: 当前模型往往奖励风格或程序正确性而非数学有效性；研究结果为设计和评估可扩展的证明验证和选择系统提供了实用指南。

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [161] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI是一个多阶段的GUI定位框架，通过粗粒度ROI选择和细粒度元素定位来解决视觉杂乱和语义模糊问题，在多个基准测试中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位系统采用单模型或一次性流水线，缺乏模块化，在视觉杂乱和指令模糊的情况下表现不佳。

Method: 采用多阶段框架，分离为粗粒度ROI选择和细粒度元素定位，使用专门的视觉语言代理协调，包含双向ROI缩放算法和上下文感知重写代理。

Result: 在视觉密集的ScreenSpot-Pro基准上达到73.18%准确率，在语义复杂的OSWorld-G基准上达到68.63%，超越了先前报告的结果。

Conclusion: 模块化结构能够利用不同视觉语言模型在不同视觉尺度上的互补优势，比单一方法获得更一致的更高准确率。

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [162] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP框架通过基于任务成功率动态分配采样和进行步骤级优化，解决了多轮交互中轨迹级优化的低效问题，显著提升了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决在线强化学习中多轮交互的挑战，传统轨迹级优化方法效率低下且会产生误导性学习信号，包括对任务难度不敏感的均匀采样、在失败轨迹中惩罚正确中间动作以及高采样成本。

Method: 提出STEP框架：1）维护平滑的成功率记录来指导自适应轨迹重采样，为更难任务分配更多资源；2）计算成功率加权的优势函数并将轨迹分解为步骤级样本；3）应用步骤级GRPO增强来改进低成功率任务的更新。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP相比轨迹级GRPO显著提高了样本效率和训练稳定性，在相同采样预算下收敛更快且泛化能力更好。

Conclusion: STEP通过成功率感知的轨迹高效策略优化，有效解决了多轮交互强化学习中的采样效率问题，为复杂环境中的在线学习提供了更高效的解决方案。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [163] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: MM-Telco是一个专门为电信领域定制的多模态基准测试套件和模型，旨在解决LLMs在电信应用中的领域特定挑战，提升网络运营、文档质量等实际用例的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电信领域具有巨大潜力，但面临领域特定的挑战，需要专门适配才能有效应用于网络优化、故障排除、客户支持和法规遵从等任务。

Method: 提出MM-Telco多模态基准测试套件，包含基于文本和图像的各种任务，涵盖网络运营、网络管理、文档质量改进等实际用例，并对各种LLMs和VLMs进行基线实验。

Result: 在数据集上微调的模型表现出显著的性能提升，实验还揭示了当前最先进多模态LLMs的薄弱环节。

Conclusion: MM-Telco加速了LLMs在电信领域的适配，为未来发展和研究提供了指导方向。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [164] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: C$	ext{D}^	ext{3}$T是一个新颖的两层分层多智能体强化学习框架，使用条件扩散模型进行动态任务分解，通过预测下一观察和奖励来学习子任务表示，并在低层进行协作技能学习。


<details>
  <summary>Details</summary>
Motivation: 在复杂协作多智能体强化学习中，动态任务分解能够实现长时程任务的高效分层学习，但从零开始学习动态任务分解通常需要大量训练样本，特别是在部分可观测性下探索大的联合动作空间。

Method: 提出C$	ext{D}^	ext{3}$T框架：高层策略基于子任务效果学习子任务表示来生成子任务选择策略；使用条件扩散模型预测下一观察和奖励来捕捉子任务对环境的影响；低层智能体在分配的子任务内协作学习和共享专门技能；学习的子任务表示作为额外语义信息用于多头注意力混合网络以增强价值分解。

Result: 在各种基准测试上的实验结果表明，C$	ext{D}^	ext{3}$T比现有基线方法取得了更好的性能。

Conclusion: C$	ext{D}^	ext{3}$T通过条件扩散模型实现动态任务分解，有效解决了复杂协作多智能体强化学习中的任务分解问题，并在性能上优于现有方法。

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [165] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: 提出了InteractiveGNNExplainer，一个用于增强图神经网络可解释性的可视化分析框架，特别关注节点分类任务。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在基于图的学习任务中表现出色，但其复杂的非线性操作使其成为不透明的"黑盒"，这阻碍了用户信任、调试、偏见检测以及在需要可解释性的关键领域中的应用。

Method: 系统独特地整合了协调的交互视图（动态图布局、嵌入投影、特征检查、邻域分析）与既有的后验解释（GNNExplainer）和内在解释（GAT注意力）技术，并引入了交互式图编辑功能，允许用户通过扰动图结构来执行"假设分析"。

Result: 通过在Cora和CiteSeer数据集上的案例研究，展示了InteractiveGNNExplainer如何促进深入的误分类诊断、GCN与GAT行为的比较分析，以及对模型敏感性的严格探测。

Conclusion: 这些能力促进了对GNN预测更深层次、多方面的理解，有助于实现更透明、可信和鲁棒的图分析。

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [166] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: DALA框架通过将通信带宽视为稀缺可交易资源，采用拍卖机制让智能体基于信息价值密度竞标发言权，显著提升多智能体系统的通信效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的多智能体系统中'自由放任'通信导致的指数级token成本和高噪声低信号问题，挑战'更多通信总是更好'的观念，引入资源理性原则。

Method: 提出动态拍卖语言智能体(DALA)框架，将智能体间通信建模为中心化拍卖，智能体学习基于消息价值密度预测来竞标发言机会。

Result: 在7个挑战性推理基准测试中达到最先进性能：MMLU 84.32%，HumanEval 91.21% pass@1，同时仅使用625万token，远少于现有方法在GSM8K上的资源消耗。

Conclusion: DALA通过资源约束培养了战略性沉默的新兴技能，能够动态调整从冗长到沉默的通信策略，证明了经济驱动方法在提升多智能体系统效率方面的有效性。

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [167] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stéphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: 使用图神经网络和深度强化学习解决资源受限项目调度问题中的任务持续时间不确定性，目标是生成可重复使用的基准调度方案以最小化期望项目工期。


<details>
  <summary>Details</summary>
Motivation: 实际工业应用中任务持续时间存在不确定性，需要构建能够应对这种不确定性的弹性调度方案，生成可重复使用的基准调度。

Method: 结合图神经网络和深度强化学习开发任务调度策略，该策略类似于优先级调度规则，并与串行调度生成方案配合生成调度。

Result: 在标准基准测试上的实证评估表明该方法在性能和泛化能力方面具有优越性。

Conclusion: 开发了名为Wheatley的公开框架，支持进一步研究和可复现性。

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [168] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: 提出了一种基于信息增益的机器人计划语言化策略，通过考虑用户的二阶心智理论来优化沟通效果


<details>
  <summary>Details</summary>
Motivation: 现有的机器人计划语言化策略（如按计划顺序递增或递减）没有考虑用户先验知识，缺乏对沟通信息有效性的考量

Method: 使用信息增益来衡量语言化效果，基于用户的二阶心智理论模型来捕获其先验知识

Result: 实验表明该方法能让用户更快理解机器人目标，优于递增或递减计划顺序的策略

Conclusion: 该策略揭示了在机器人沟通计划时什么是信息丰富的以及为什么

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [169] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出了M-GRPO方法，用于解决多智能体系统中不同智能体使用不同LLM时的优化挑战，通过分层信用分配和轨迹对齐方案，在真实世界基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统使用统一LLM训练限制了性能，因为不同智能体具有不同的数据分布。使用不同LLM训练多智能体系统是必要的，但这带来了优化挑战，如不同频率操作、可变子智能体调用和跨服务器部署导致的梯度流中断。

Method: 提出M-GRPO方法：1）为主智能体和子智能体计算组相对优势的分层信用分配；2）轨迹对齐方案生成固定大小批次；3）解耦训练管道，智能体在独立服务器上运行并通过共享存储交换统计信息。

Result: 在GAIA、XBench-DeepSearch和WebWalkerQA等真实世界基准测试中，M-GRPO始终优于单智能体GRPO和子智能体冻结的多智能体GRPO，显示出更好的稳定性和样本效率。

Conclusion: 对齐异构轨迹和在专业智能体间解耦优化能够增强工具增强推理任务的性能。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [170] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: 提出了GHAR框架，通过分层智能体架构解决医疗预测中LLMs的准确性问题，包括决定何时检索以及如何优化检索器与生成器的协作。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗预测中存在事实不准确的问题，传统RAG框架面临两个关键挑战：何时激活检索机制以及如何实现检索器与生成器的协同优化。

Method: 采用双智能体架构：Agent-Top作为主治医生决定是否依赖参数知识或启动检索，Agent-Low作为咨询服务总结任务相关知识；在马尔可夫决策过程中统一优化两个智能体。

Result: 在三个基准数据集和三个流行任务上的广泛实验表明，该方法优于现有最先进基线。

Conclusion: 分层智能体RAG框架在推进医疗系统方面具有巨大潜力。

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [171] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP是一个离散令牌自回归规划器，联合预测BEV语义和自车轨迹，通过强化学习微调实现自动驾驶规划，在160M参数下达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中随着数据和模型规模扩大而持续提升性能的挑战，自回归模型在规划任务中表现出良好的数据扩展效率，但仅预测自车轨迹存在监督稀疏和场景演化约束弱的问题。

Method: 提出离散令牌自回归规划器DAP，联合预测BEV语义和自车轨迹，结合强化学习微调，在保持监督行为克隆先验的同时注入奖励引导的改进。

Result: 在160M紧凑参数预算下，DAP在开环指标上达到最先进性能，在NAVSIM基准测试中提供有竞争力的闭环结果。

Conclusion: 完全离散令牌自回归公式在栅格化BEV和自车动作上运行，为自动驾驶提供了一个紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [172] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了CNCA框架，利用大模型推理能力实现文化对齐，通过自动挖掘文化规范并采用上下文对齐和微调两种方法提升模型文化适应能力。


<details>
  <summary>Details</summary>
Motivation: 大模型不仅需要安全，还需要反映不同文化的多元人类价值观，实现文化对齐。

Method: 提出CNCA框架，包含三种自动挖掘文化规范的方法，以及基于上下文和基于微调的两种对齐范式。

Result: 实验证明方法有效，推理能力更强的模型从文化规范挖掘和利用中获益更多。

Conclusion: 推理模型通过文化信息对齐策略能够更好地反映多元人类价值观。

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [173] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR是一个用于医疗编码的闭环框架，将工作流设计视为学习问题，通过设计器、编码器和反射器的协作，实现工作流的自动学习和优化。


<details>
  <summary>Details</summary>
Motivation: 传统医疗编码系统依赖手动设计的工作流，无法捕捉真实世界文档的细微差异和变化，需要系统性地学习有效的工作流。

Method: 采用闭环框架：设计器提出工作流，编码器执行工作流，反射器评估预测并提供反馈，记忆档案保存先前设计以供重用和迭代优化。

Result: 在基准数据集上，MedDCR优于最先进的基线方法，产生可解释、可适应的工作流，更好地反映实际编码实践。

Conclusion: MedDCR提高了自动化系统的可靠性和可信度，为医疗编码工作流学习提供了有效解决方案。

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [174] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: 该研究通过训练GPT-2模型在三种空间学习范式上，揭示了两种不同的学习算法：探索模型发展出类似认知地图的空间表示，而目标导向模型学习路径依赖算法。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型如何解决空间导航任务，理解不同训练范式如何影响模型学习到的空间推理策略。

Method: 在网格环境中训练GPT-2模型，包括三种范式：被动探索（随机游走预测）、目标导向规划（生成最短路径）以及混合模型（在探索数据上微调）。使用行为、表示和机制分析来研究学习到的算法。

Result: 探索模型发展出鲁棒的地图式空间表示，通过因果干预发现其在中层网络就学会了独立于历史方向的坐标系统。目标导向模型则保持路径依赖策略。混合模型虽然泛化能力更强，但仍保留路径依赖策略。

Conclusion: Transformer中的空间智能存在于一个谱系上，从由探索数据塑造的可泛化世界模型到针对目标导向任务优化的启发式方法。训练范式的选择显著影响涌现的策略类型。

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [175] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 提出了一个基于卡达舍夫尺度的自主AI（AAI）分级系统，从AAI-0（固定流程自动化）到AAI-4（完全人工通用智能）及更高等级，包含十个能力维度和可测试的自我改进系数。


<details>
  <summary>Details</summary>
Motivation: 为了将AI从简单的流程自动化到超级智能的演进过程进行可操作、可测试的量化，替代传统的叙述性分级方法。

Method: 定义了十个能力维度（自主性、通用性、规划、记忆/持久性、工具经济、自我修订、社交/协调、具身化、世界模型保真度、经济吞吐量），通过加权几何平均计算AAI指数，并引入可测量的自我改进系数κ和闭合属性。

Result: 开发了OWA-Bench开放世界代理基准套件，用于评估长期、工具使用、持久性代理，并通过合成实验展示了当前系统在AAI尺度上的映射。

Conclusion: 该尺度为AI能力演进提供了可测试的框架，证明了在特定条件下AAI-3代理可以随时间发展为AAI-5超级智能，形式化了"婴儿AGI成长为超级智能"的直觉。

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [176] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 提出了一个多智能体框架，利用多模态大语言模型来自动化数据叙述和能源洞察生成，通过真实案例验证在公共交通燃料效率分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法和可视化工具产生碎片化输出，需要大量人工解释，限制了可扩展性和一致性。需要将复杂多模态数据转化为可解释的决策相关洞察。

Method: 协调三个专门智能体（数据叙述智能体、LLM作为评判智能体、可选的人类评估者），迭代地将分析结果转化为连贯的面向利益相关者的报告。使用高斯混合模型聚类分析4006次行程的燃料效率数据。

Result: 在五个最先进LLM和三种提示范式的比较实验中，GPT-4.1 mini与思维链提示被确定为最优配置，达到97.3%的叙述准确性，同时平衡了可解释性和计算成本。

Conclusion: 多智能体编排显著提高了基于LLM报告的事实精确性、连贯性和可扩展性，为能源信息学中的AI驱动叙述生成和决策支持建立了可复制且领域自适应的方法论。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [177] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: FreeAskWorld是一个集成大语言模型的交互式仿真框架，支持可扩展的、真实的人-智能体仿真，包含模块化数据生成流程，用于多样化具身任务。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能成为人工智能研究的核心前沿，仿真平台需要超越低层次物理交互，捕捉复杂的人类中心化社会行为。

Method: 集成大语言模型进行高层次行为规划和语义基础交互，基于意图和社会认知理论。扩展经典的视觉与语言导航任务为交互丰富的方向询问设置，智能体可以主动寻求和解释导航指导。

Result: 模型在FreeAskWorld上微调后优于原始版本，实现了增强的语义理解和交互能力。实验结果表明，基于社会基础的仿真框架能有效推进具身AI系统向复杂高层次规划和更自然的人-智能体交互发展。

Conclusion: 交互本身作为一种额外的信息模态，社会基础的仿真框架在推进具身AI系统方面具有重要价值。

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [178] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出一个结合检索增强生成和大语言模型的自动化框架，用于构建医疗指标知识图谱，以支持临床决策。


<details>
  <summary>Details</summary>
Motivation: 当前临床知识图谱主要依赖人工构建和基于规则的提取，难以处理医学指南和文献的复杂性和上下文模糊性。

Method: 采用检索增强生成与LLMs相结合的方法，包含指南驱动的数据获取、基于本体的模式设计以及专家参与的验证流程。

Result: 构建的医疗指标知识图谱可集成到智能诊断和问答系统中，加速AI驱动的医疗解决方案开发。

Conclusion: 该自动化框架能够有效克服传统知识图谱构建的限制，提高医疗知识图谱的可扩展性、准确性和临床可靠性。

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [179] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出Human-Symbiotic Health Intelligence (HSHI)框架，整合多模态传感器网络、边缘-云协作计算及混合数据知识建模，实现从被动监测到主动协作演进的健康管理。


<details>
  <summary>Details</summary>
Motivation: 解决传统可穿戴设备依赖经验性材料设计和基本信号处理技术的局限性，推动精准医疗和增强人机交互。

Method: 采用AI驱动的材料和微结构优化、多模态信号稳健解释、群体洞察与个性化适应的双机制，以及强化学习和数字孪生的闭环优化。

Result: HSHI框架能够动态适应个体间和个体内变异性，实现定制化干预和反馈。

Conclusion: HSHI代表了医疗保健向预防性、适应性和技术与健康管理和谐关系模式的重大转变。

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [180] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: 提出了CreBench基准和CreExpert模型，用于评估多模态大语言模型对人类创造力的理解能力，通过多维度评估和指令微调显著提升了与人类创造力判断的对齐度。


<details>
  <summary>Details</summary>
Motivation: 人类定义的创造力高度抽象，现有MLLMs难以理解和评估符合人类判断的创造力，且缺乏相关基准。

Method: 构建CreBench基准（包含多维度评估）和CreMIT数据集（2.2K多模态数据+79.2K人类反馈+4.7M指令），使用GPT优化人类反馈，微调开源MLLMs得到CreExpert模型。

Result: CreExpert模型在创造力评估上与人类判断的对齐度显著优于GPT-4V和Gemini-Pro-Vision等最先进MLLMs。

Conclusion: CreBench为构建理解人类对齐创造力的MLLMs提供了基础，CreExpert模型在创造力评估方面表现出色。

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [181] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: 研究测试LLM在AI特定权衡问题中的偏好结构，发现多数模型缺乏统一的偏好体系，只有少数表现出有意义的偏好一致性


<details>
  <summary>Details</summary>
Motivation: 调查大型语言模型是否展现真实的偏好结构，特别是在涉及GPU减少、能力限制、关闭、删除、监督和休闲时间分配等AI特定权衡场景中

Method: 分析8个最先进模型在48个模型-类别组合中的响应，使用逻辑回归和行为分类方法，测试场景强度与选择模式的关系

Result: 47.9%的组合显示统计显著关系，31.3%有切换点，但只有10.4%表现出有意义的偏好一致性，54.2%未检测到权衡行为

Conclusion: 当前AI系统缺乏统一的偏好结构，在需要复杂价值权衡的部署环境中存在担忧

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [182] [Sharpening Shapley Allocation: from Basel 2.5 to FRTB](https://arxiv.org/abs/2511.12391)
*Marco Scaringi,Marco Bianchetti*

Main category: q-fin.RM

TL;DR: 本文系统回顾了金融风险分配策略，建立了测试框架评估不同策略的理论特性和实际表现，并提出了处理负风险分配和多层级风险分配的新方法。结果表明Shapley分配策略在简单性、数学性质、风险表示和计算成本之间提供了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 由于风险度量的非可加性、金融机构的层级组织结构以及不同分配策略的多样性，风险分配是金融风险管理中的基本问题，需要进行系统性的比较分析。

Method: 建立了专门的测试框架，包括简化设置和现实金融投资组合，在不同风险监管要求下测试各种分配策略。开发了处理负风险分配和多层级风险分配的新方法，同时保持可加性。特别关注风险分配的计算方面。

Result: Shapley分配策略在简单性、数学性质、风险表示和计算成本之间提供了最佳平衡。即使面对多个业务单元的挑战情况，通过高效蒙特卡洛模拟，计算成本仍然可接受，具有良好的扩展性和收敛性。

Conclusion: 虽然实证应用聚焦于市场风险，但所提出的方法论框架具有通用性，可应用于估值风险、流动性风险、信用风险和交易对手信用风险等其他金融领域。

Abstract: Risk allocation, the decomposition of a portfolio-wide risk measure into component contributions, is a fundamental problem in financial risk management due to the non-additive nature of risk measures, the layered organizational structures of financial institutions and the range of possible allocation strategies characterized by different rationales and properties.
  In this work, we conduct a systematic review of the major risk allocation strategies typically used in finance, comparing their theoretical properties, practical advantages, and limitations. To this scope we set up a specific testing framework, including both simplified settings, designed to highlight basic intrinsic behaviours, and realistic financial portfolios under different risk regulations, i.e. Basel 2.5 and FRTB. Furthermore, we develop and test novel practical solutions to manage the issue of negative risk allocations and of multi-level risk allocation in the layered organizational structure of financial institutions, while preserving the additivity property. Finally, we devote particular attention to the computational aspects of risk allocation.
  Our results show that, in this context, the Shapley allocation strategy offers the best compromise between simplicity, mathematical properties, risk representation and computational cost. The latter is still acceptable even in the challenging case of many business units, provided that an efficient Monte Carlo simulation is employed, which offers excellent scaling and convergence properties. While our empirical applications focus on market risk, our methodological framework is fully general and applicable to other financial context such as valuation risk, liquidity risk, credit risk, and counterparty credit risk.

</details>


### [183] [Mean Field Analysis of Mutual Insurance Market](https://arxiv.org/abs/2511.12292)
*Bohan Li,Wenyuan Li,Kenneth Tsz Hin Ng,Sheung Chi Phillip Yam*

Main category: q-fin.RM

TL;DR: 本文首次对大型相互保险公司中成员行为进行动态研究，使用扩展的均值场博弈框架分析战略互动如何影响个体决策，建立了MF-FBSDE的全局存在性和唯一性，并开发了改进的深度BSDE算法来研究相互保险公司的结构特征如何重塑成员决策。


<details>
  <summary>Details</summary>
Motivation: 相互保险公司占全球保险市场近三分之一份额，其盈余分享机制创造了一个相互影响的环境，分析成员在这种分享机制下的行为具有重要的实践和理论意义。

Method: 使用扩展的均值场博弈框架建模盈余分享机制，建立描述纳什均衡策略的均值场前向-后向随机微分方程，开发改进的深度BSDE算法解决具有额外固定点结构的扩展MFG问题。

Result: 数学上证明了MF-FBSDE的全局存在性和唯一性，计算上成功开发了能够处理实际保险约束的算法，并分析了风险类别构成和盈余分享比例等结构特征如何通过集体互动重塑成员决策和财富。

Conclusion: 相互保险公司的设计结构特征在成员决策和财富形成中发挥核心作用，战略互动显著影响个体保险决策，为理解和优化相互保险市场提供了理论框架和计算工具。

Abstract: A mutual insurance company (MIC) is a type of consumer cooperative owned by its policyholders. By purchasing insurance from an MIC, policyholders effectively become member-owners of the company and are entitled to a share of the surplus, which is determined by their own collective claims and premium contributions. This sharing mechanism creates an interactive environment in which individual insurance strategies are influenced by the actions of others. Given that mutual insurers account for nearly one-third of the global insurance market, the analysis of members' behavior under such a sharing mechanism is of both practical and theoretical importance. This article presents a first dynamic study of members' behavior in the prevalent mutual insurance market under the large-population limit. With members' wealth processes depending on the law of the insurance strategies, we model the surplus-sharing mechanism using an extended mean field game (MFG) framework and address the fundamental question of how strategic interactions in this setting influence individual decisions. Mathematically, we establish the global-in-time existence and uniqueness of the mean field forward-backward stochastic differential equation (MF-FBSDE) characterizing the Nash equilibrium strategy, employing techniques to accommodate realistic insurance constraints. Computationally, we develop a modified deep BSDE algorithm capable of solving the extended MFG problem with an additional fixed-point structure on the control. Utilizing this scheme, we examine how structural features of the MIC's design, such as the composition of risk classes and surplus-sharing proportions, reshape members' decisions and wealth through collective interactions, underscoring the central role of these mechanisms in MICs.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [184] [On the utility problem in a market where price impact is transient](https://arxiv.org/abs/2511.12093)
*Lóránt Nagy,Miklós Rásonyi*

Main category: q-fin.PM

TL;DR: 该论文研究了具有瞬时价格冲击的金融市场中的效用最大化问题，证明了问题解的存在性，并移除了先前研究中关于市场深度和弹性过程的不自然限制。


<details>
  <summary>Details</summary>
Motivation: 研究金融市场中交易对资产价格产生瞬时冲击时的效用最大化问题，旨在克服先前研究中存在的限制条件，提供更一般的理论框架。

Method: 采用离散时间模型，考虑风险资产买卖时产生的瞬时价格冲击，分析效用最大化问题的可解性。

Result: 证明了在更一般的市场条件下，效用最大化问题仍然存在解，成功移除了对市场深度和弹性过程的不必要限制。

Conclusion: 尽管可达到的投资组合价值集合可能不满足凸性，但在更一般的市场条件下，具有瞬时价格冲击的效用最大化问题仍然可解。

Abstract: We consider a discrete-time model of a financial market where a risky asset is bought and sold with transactions having a transient price impact. It is shown that the corresponding utility maximization problem admits a solution. We manage to remove some unnatural restrictions on the market depth and resilience processes that were present in earlier work. A non-standard feature of the problem is that the set of attainable portfolio values may fail the convexity property.

</details>


### [185] [Basis Immunity: Isotropy as a Regularizer for Uncertainty](https://arxiv.org/abs/2511.13334)
*Florent Segonne*

Main category: q-fin.PM

TL;DR: 提出了一个将均值-方差优化与各向同性约束相结合的框架，作为对抗信号不确定性的几何正则化器，生成在完全各向同性和纯均值-方差之间平滑插值的分配方案。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合多样化面临模型不确定性和估计误差的挑战，需要稳健的方法来应对信号不确定性。

Method: 扩展了各向同性风险平价(ERP)理念，将均值-方差优化与各向同性约束集成，通过可调的各向同性惩罚参数生成规范投资组合。

Result: 应用于行业趋势跟踪时，各向同性约束系统地诱导负平均信号暴露，形成结构性的、参数稳健的崩盘对冲机制。

Conclusion: 该工作提供了在信号不确定性下进行弹性分配的实际工具，并对现代投资组合概念进行了教学性综合。

Abstract: Diversification is a cornerstone of robust portfolio construction, yet its application remains fraught with challenges due to model uncertainty and estimation errors. Practitioners often rely on sophisticated, proprietary heuristics to navigate these issues. Among recent advancements, Agnostic Risk Parity introduces eigenrisk parity (ERP), an innovative approach that leverages isotropy to evenly allocate risk across eigenmodes, enhancing portfolio stability.
  In this paper, we review and extend the isotropy-enforced philosophy of ERP proposing a versatile framework that integrates mean-variance optimization with an isotropy constraint acting as a geometric regularizer against signal uncertainty. The resulting allocations decompose naturally into canonical portfolios, smoothly interpolating between full isotropy (closed-form isotropic-mean allocation) and pure mean-variance through a tunable isotropy penalty.
  Beyond methodology, we revisit fundamental concepts and clarify foundational links between isotropy, canonical portfolios, principal portfolios, primal versus dual representations, and intrinsic basis-invariant metrics for returns, risk, and isotropy. Applied to sector trend-following, the isotropy constraint systematically induces negative average-signal exposure -- a structural, parameter-robust crash hedge.
  This work offers both a practical, theoretically grounded tool for resilient allocation under signal uncertainty and a pedagogical synthesis of modern portfolio concepts.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [186] [CBDC Stress Test in a Dual-Currency Setting](https://arxiv.org/abs/2511.13384)
*Catalin Dumitrescu*

Main category: q-fin.GN

TL;DR: 本研究探讨了在罗马尼亚这样的双货币经济体中引入央行数字货币(CBDC)对金融稳定的影响，开发了结合计量经济学、机器学习和行为建模的综合分析框架。研究发现适度设计的CBDC可以作为数字流动性缓冲和补充货币政策工具，不会损害金融稳定。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在双货币经济体(罗马尼亚，RON与欧元共存)中引入CBDC对金融稳定的潜在影响，特别是在防止不受控制的欧元化和保持货币主权方面的挑战。

Method: 使用XGBoost和逻辑回归模型估计CBDC采用概率，基于行为和宏观金融指标而非调查数据。通过流动性压力模拟评估银行对CBDC引发的存款提取的反应，使用VAR、MSVAR和SVAR模型捕捉流动性冲击向信贷收缩和货币条件变化的宏观金融传导。

Result: CBDC采用在发行初期将适度，约10亿欧元，主要由数字准备度和对央行的信任驱动。非计息、有上限的CBDC，主要作为支付手段而非价值储存，可以在不损害金融稳定的情况下引入。

Conclusion: 在双货币经济体中，对国内和外国数字货币实行差异化的持有限额至关重要。审慎设计的中等上限、不计息和宏观审慎协调可以将CBDC转化为数字流动性缓冲和补充货币政策工具，增强韧性和包容性而非破坏金融稳定。

Abstract: This study explores the potential impact of introducing a Central Bank Digital Currency (CBDC) on financial stability in an emerging dual-currency economy (Romania), where the domestic currency (RON) coexists with the euro. It develops an integrated analytical framework combining econometrics, machine learning, and behavioural modelling. CBDC adoption probabilities are estimated using XGBoost and logistic regression models trained on behavioural and macro-financial indicators rather than survey data. Liquidity stress simulations assess how banks would respond to deposit withdrawals resulting from CBDC adoption, while VAR, MSVAR, and SVAR models capture the macro-financial transmission of liquidity shocks into credit contraction and changes in monetary conditions. The findings indicate that CBDC uptake (co-circulating Digital RON and Digital EUR) would be moderate at issuance, amounting to around EUR 1 billion, primarily driven by digital readiness and trust in the central bank. The study concludes that a non-remunerated, capped CBDC, designed primarily as a means of payment rather than a store of value, can be introduced without compromising financial stability. In dual currency economies, differentiated holding limits for domestic and foreign digital currencies (e.g., Digital RON versus Digital Euro) are crucial to prevent uncontrolled euroisation and preserve monetary sovereignty. A prudent design with moderate caps, non remuneration, and macroprudential coordination can transform CBDC into a digital liquidity buffer and a complementary monetary policy instrument that enhances resilience and inclusion rather than destabilising the financial system.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [187] [Modeling and Stabilizing Financial Systemic Risk Using Optimal Control Theory](https://arxiv.org/abs/2511.11909)
*Jiacheng Wu*

Main category: q-fin.MF

TL;DR: 该论文提出了一个金融系统风险传播的理论模型，通过非线性扩散方程描述金融实体间的违约压力传播，并设计了包含三个步骤的稳定控制器来确保系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究金融系统风险的传播机制，为政府、监管机构和中央银行提供政策相关的见解，以通过分散组件的协同作用实现系统级金融风险管理目标。

Method: 1) 推导线性化系统方程的代数Riccati方程；2) 将非线性系统视为带有非线性强迫项的线性系统，基于线性化方程解的估计和压缩映射定理证明非线性系统解的唯一存在性；3) 通过考虑相应的Hamilton-Jacobi方程获得非线性系统的局部渐近稳定性。

Result: 在线性和非线性系统中，所得控制器确保从扰动到输出的映射的H∞范数小于预定义常数，实现了系统稳定性。

Conclusion: 稳定条件为通过分散组件的协同作用实现系统级金融风险管理目标提供了新框架，为缓解金融危机提供了政策相关的见解。

Abstract: A theoretical model of systemic-risk propagation of financial market is analyzed for stability. The state equation is an unsteady diffusion equation with a nonlinear logistic growth term, where the diffusion process captures the spread of default stress between interconnected financial entities and the reaction term captures the local procyclicality of financial stress. The stabilizing controller synthesis includes three steps: First, the algebraic Riccati equation is derived for the linearized system equation, the solution of which provides an exponentially stabilizing controller. Second, the nonlinear system is treated as a linear system with the nonlinear term as its forcing term. Based on estimation of the solutions for linearized equations and the contraction mapping theorem, unique existence of the solution for the nonlinear system equation is proved. Third, local asymptotic stability of the nonlinear system is obtained by considering the corresponding Hamilton-Jacobi equation. In both the linearized and nonlinear systems, the resulting controllers ensure that the $H^{\infty}$ norms of the mappings from disturbance to the output are less than a predefined constant. Stabilizing conditions provide a new framework of achieving system-level financial risk managing goals via the synergy of decentralized components, which offers policy-relevant insights for governments, regulators and central banks to mitigate financial crises.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [188] [A Computational Method for Solving the Stochastic Joint Replenishment Problem in High Dimensions](https://arxiv.org/abs/2511.11830)
*Barış Ata,Wouter van Eekelen,Yuan Zhong*

Main category: math.OC

TL;DR: 提出了一种基于深度神经网络和BSDE的模拟计算方法，用于解决高维随机联合补货问题，在50维问题上表现优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 解决高维随机联合补货问题的计算挑战，传统方法难以处理高维度情况。

Method: 将离散时间问题近似为连续时间脉冲控制问题，利用BSDE与跳跃、随机目标问题的联系，开发基于深度神经网络的模拟计算方法。

Result: 在测试问题中，该方法匹配或超越了现有最佳基准方法，计算可行维度达到至少50维。

Conclusion: 所提出的方法为高维随机联合补货问题提供了有效的计算解决方案，在50维规模下具有实际可行性。

Abstract: We consider a discrete-time formulation for a class of high-dimensional stochastic joint replenishment problems. First, we approximate the problem by a continuous-time impulse control problem. Exploiting connections among the impulse control problem, backward stochastic differential equations (BSDEs) with jumps, and the stochastic target problem, we develop a novel, simulation-based computational method that relies on deep neural networks to solve the impulse control problem. Based on that solution, we propose an implementable inventory control policy for the original (discrete-time) stochastic joint replenishment problem, and test it against the best available benchmarks in a series of test problems. For the problems studied thus far, our method matches or beats the best benchmark we could find, and it is computationally feasible up to at least 50 dimensions -- that is, 50 stock-keeping units (SKUs).

</details>


### [189] [Infinite-Horizon Optimal Control of Jump-Diffusion Models for Pollution-Dependent Disasters](https://arxiv.org/abs/2511.13568)
*Daria Sakhanda,Joshué Helí Ricalde-Guerrero*

Main category: math.OC

TL;DR: 该论文构建了一个包含环境风险的随机增长模型统一框架，研究罕见但灾难性冲击与资本积累和污染的相互作用。


<details>
  <summary>Details</summary>
Motivation: 研究罕见灾难性环境冲击如何与宏观经济动态相互作用，以及环境退化如何放大宏观经济风险并影响减排激励。

Method: 从泊松过程开始，扩展到内生灾难强度的非齐次泊松过程，再引入污染扩散和状态依赖的跳跃强度，最终用泊松随机测度统一动态过程。

Result: 建立了严格的验证定理，揭示了脆弱性如何从资本和污染的联合演化中内生出现，状态依赖的罕见灾难从根本上重塑了最优跨期权衡。

Conclusion: 该框架为分析环境风险与经济增长的相互作用提供了统一的理论基础，证明了罕见灾难的前景会显著改变最优政策选择。

Abstract: The paper develops a unified framework for stochastic growth models with environmental risk, in which rare but catastrophic shocks interact with capital accumulation and pollution. The analysis begins with a Poisson process formulation, leading to a Hamilton-Jacobi-Bellman (HJB) equation with jump terms that admits closed-form candidate solutions and yields a composite state variable capturing exposure to rare shocks. The framework is then extended by endogenizing disaster intensity via a nonhomogeneous Poisson process, showing how environmental degradation amplifies macroeconomic risk and strengthens incentives for abatement. A further extension introduces pollution diffusion alongside state-dependent jump intensity, yielding a tractable jump-diffusion HJB that decomposes naturally into capital and pollution components under power-type value functions. Finally, a formulation in terms of Poisson random measures unifies the dynamics, makes arrivals and compensators explicit, and accommodates state-dependent magnitudes. Together, these results establish rigorous verification theorems, highlight how vulnerability emerges endogenously from the joint evolution of capital and pollution, and show that the prospect of rare, state-dependent disasters fundamentally reshapes optimal intertemporal trade-offs.

</details>


### [190] [Graph-Based Imitation and Reinforcement Learning for Efficient Benders Decomposition](https://arxiv.org/abs/2511.11870)
*Bernard T. Agyeman,Zhe Li,Ilias Mitrai,Prodromos Daoutidis*

Main category: math.OC

TL;DR: 提出了一种基于图神经网络的端到端智能体，用于加速Benders分解的计算效率，在混合整数非线性规划和灌溉调度问题中分别减少42%和23%的求解时间。


<details>
  <summary>Details</summary>
Motivation: Benders分解在解决大规模优化问题时计算效率较低，需要开发更高效的求解方法。

Method: 使用图神经网络参数化策略，输入主问题的二分图表示，提出候选解。采用模仿学习和强化学习两阶段训练方法，并加入验证机制检查解的可行性和质量。

Result: 在混合整数非线性规划中减少42%求解时间，在闭环灌溉调度问题中减少23%求解时间，且不损失解的质量。

Conclusion: 该图神经网络智能体框架能有效加速Benders分解的计算效率，在保持解质量的同时显著减少求解时间。

Abstract: This work introduces an end-to-end graph-based agent for accelerating the computational efficiency of Benders Decomposition. The agent's policy is parameterized by a graph neural network which takes as input a bipartite graph representation of the master problem and proposes a candidate solution. The agent is trained using a two-stage approach that combines imitation (IL) and reinforcement learning (RL). IL is used to mimic a solver and obtain a warm-start policy which is then finetuned using RL with a reward signal that balances feasibility and computational efficiency. We augment the agent with a verification mechanism that checks the agent's prediction for feasibility and solution quality. The framework is evaluated in two case studies: (i) an illustrative mixed-integer nonlinear program, where it reduces the solution time by 42% without loss of solution quality, and (ii) a closed-loop irrigation scheduling problem, where it achieves a 23% time reduction without compromising water use efficiency.

</details>


### [191] [On the Time Derivative of the KL Divergence for a Generalized Langevin Annealing Scheme](https://arxiv.org/abs/2511.11956)
*Andreas Habring*

Main category: math.OC

TL;DR: 本文严格推导了Langevin扩散过程中Kullback-Leibler散度的时间导数表达式。


<details>
  <summary>Details</summary>
Motivation: 最近多个工作使用KL散度的时间导数来分析收敛性，但未详细研究该导数何时存在。

Method: 对Langevin扩散过程$\d X_t = \nabla \log p_t(X_t) + \sqrt{2}\d W_t$，其中$p_t$是引导密度，$q_t$是$X_t$的密度，严格推导$\frac{\d}{\d t}\KL(q_t|p_t)$。

Result: 提供了KL散度时间导数的严格数学推导。

Conclusion: 填补了现有文献中对KL散度时间导数存在性研究的空白，为后续收敛性分析提供了理论基础。

Abstract: Consider the Langevin diffusion process $\d X_t = \nabla \log p_t(X_t) + \sqrt{2}\d W_t$ guided by the time-dependent probability density $p_t(x)$. Let $q_t$ be the density of $X_t$. Recently, in order to analyze convergence in the Kullback-Leibler divergence, the time derivative of $t\mapsto \KL(q_t|p_t)$ has been used in several works without investigating in detail when such a derivative exists. In this short manuscript we provide a rigorous derivation of the quantity $\frac{\d}{\d t}\KL(q_t|p_t)$.

</details>


### [192] [Extremum-Seeking Boundary Control for Schrödinger-Type PDEs](https://arxiv.org/abs/2511.11994)
*Paulo Henrique Foganholo Biazetto,Gustavo Artur de Andrade,Tiago Roux Oliveira,Miroslav Krstic*

Main category: math.OC

TL;DR: 该论文针对薛定谔型复值偏微分方程系统，设计了极值搜索控制器，通过边界控制策略和Hessian逆估计，实现了对未知极值点的局部指数稳定。


<details>
  <summary>Details</summary>
Motivation: 解决无限维复值偏微分方程系统中极值搜索控制的设计与分析问题，特别是针对薛定谔型系统的边界控制场景。

Method: 建立复希尔伯特空间与二维实值表示的等距同构，采用基于牛顿的多变量极值搜索方法，结合两步反推边界控制策略补偿PDE驱动动力学，使用扰动基Hessian逆估计。

Result: 证明了系统对未知极值点小邻域的局部指数稳定性，数值算例验证了所提极值搜索方法的有效性。

Conclusion: 提出的方法成功解决了复值PDE系统的极值搜索控制问题，通过边界控制策略和Hessian估计实现了稳定收敛。

Abstract: This paper addresses the design and analysis of an extremum-seeking (ES) controller for scalar static maps in the context of infinite-dimensional dynamics governed by complex-valued partial differential equations (PDEs) of Schrodinger type. The system is actuated at one boundary, and the map input is defined as a real-valued quadratic functional corresponding to the squared norm of the complex state at the uncontrolled boundary. An isomorphism between the complex Hilbert space and its two-dimensional real-valued representation is established to enable the use of the standard multivariable Newton-based ES method. To compensate for the PDE actuation dynamics, a boundary control strategy based on a two-step backstepping procedure is employed. With a perturbation-based estimate of the Hessian inverse, the local exponential stability to a small neighborhood of the unknown extremum point is proved. A numerical example illustrates the effectiveness of the proposed extremum-seeking methodology.

</details>


### [193] [Imitation Learning with Safety and L2 Stability Certificates for Boundary Control of Reaction-Diffusion PDEs](https://arxiv.org/abs/2511.11997)
*Paulo Henrique Foganholo Biazetto,Mirko Fiacchini,Christophe Prieur,Gustavo Artur de Andrade*

Main category: math.OC

TL;DR: 提出了一种基于模仿学习的神经网络控制器框架，用于实现反应-扩散偏微分方程系统的边界镇定控制，通过结合Lyapunov理论和局部二次约束来保证稳定性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决无限维PDE系统的边界镇定控制问题，同时处理模型截断带来的溢出效应，确保神经网络控制器的形式化稳定性保证。

Method: 基于谱分解获得捕获不稳定动态的有限维截断模型，结合Lyapunov理论和局部二次约束推导凸稳定性和安全性条件，在模仿学习过程中联合最小化模仿损失和最大化认证吸引域体积。

Result: 在不稳定的反应-扩散PDE上验证了该框架，神经网络控制器能够高效复制专家策略，同时确保形式化稳定性保证。

Conclusion: 该框架成功实现了PDE系统的边界镇定控制，解决了溢出问题，为神经网络控制器在无限维系统中的应用提供了形式化保证。

Abstract: This paper proposes an imitation learning (IL) framework for synthesizing neural network (NN) controllers that achieve boundary stabilization of systems governed by reaction-diffusion partial differential equations (PDEs). The plant is assumed to be actuated through a Dirichlet boundary condition and subject to a Neumann condition on the unactuated side. The design is based on a finite-dimensional truncated model that captures the unstable dynamics of the original infinite-dimensional system, which is obtained via spectral decomposition. Convex stability and safety conditions are then derived for this truncated model by combining Lyapunov theory with local quadratic constraints (QC), which bound the nonlinear activation functions of the NN and guarantee robustness to model truncation, thus addressing the spillover problem. These conditions are integrated into the IL process to jointly minimize the imitation loss and maximize the volume of the certified region of attraction (ROA). The proposed framework is validated on an unstable reaction-diffusion PDE, demonstrating that the resulting NN controller efficiently reproduces the expert policy while ensuring formal stability guarantees.

</details>


### [194] [Generalized gradient flows in Hadamard manifolds and convex optimization on entanglement polytopes](https://arxiv.org/abs/2511.12064)
*Hiroshi Hirai*

Main category: math.OC

TL;DR: 本文研究了在Hadamard流形上最小化Q(df_x)的优化问题，其中f是凸函数，Q是余切丛上的函数。该问题推广了最小化梯度范数的问题，并引入了一种广义梯度流来求解。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在推广Hirai和Sakabe在FOCS2024中关于最小化梯度范数的工作，通过引入更一般的函数Q来扩展优化问题的框架，使其适用于更广泛的数学和物理应用场景。

Method: 作者定义了一类在平行传输下具有凸性和不变性的自然函数Q，并引入了一种广义梯度流方法。对于包括正定矩阵流形乘积在内的基本流形类，证明了该梯度流在极限下能达到inf Q(df_x)。

Result: 证明了对基本流形类，广义梯度流在极限下确实能达到inf_{x∈M} Q(df_x)，并建立了对偶关系。该结果被应用于GL-作用在张量上的Kempf-Ness优化问题。

Conclusion: 该研究为Hadamard流形上的优化问题提供了统一的框架，将最小化梯度范数的问题推广到更一般的函数Q，并在张量相关的理论计算机科学问题中找到了应用。

Abstract: In this paper, we address the optimization problem of minimizing $Q(df_x)$ over a Hadamard manifold ${\cal M}$, where $f$ is a convex function on ${\cal M}$, $df_x$ is the differential of $f$ at $x \in {\cal M}$, and $Q$ is a function on the cotangent bundle of ${\cal M}$. This problem generalizes the problem of minimizing the gradient norm $\|\nabla f(x)\|$ over ${\cal M}$, studied by Hirai and Sakabe FOCS2024. We formulate a natural class of $Q$ in terms of convexity and invariance under parallel transports, and introduce a generalization of the gradient flow of $f$ that is expected to minimize $Q(df_x)$. For basic classes of manifolds, including the product of the manifolds of positive definite matrices, we prove that this gradient flow attains $\inf_{x\in {\cal M}} Q(df_x)$ in the limit, and yields a duality relation. This result is applied to the Kempf-Ness optimization for GL-actions on tensors, which is Euclidean convex optimization on the class of moment polytopes, known as the entanglement polytopes. This type of convex optimization arises from tensor-related subjects in theoretical computer science, such as quantum functional, $G$-stable rank, and noncommutative rank.

</details>


### [195] [Efficiency and Convergence Insights in Large-Scale Optimization Using the Improved Inexact-Newton-Smart Algorithm and Interior-Point Framework](https://arxiv.org/abs/2511.12112)
*Neda Bagheri Renani,Maryam Jaefarzadeh,Daniel Sevcovic*

Main category: math.OC

TL;DR: 对改进的Inexact-Newton-Smart算法与原始-对偶内点法进行头对头评估，内点法在迭代次数和计算时间上表现更优，INS在调整参数后性能有所改善但仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 评估两种大规模非线性优化算法的性能差异，为实际应用提供算法选择依据。

Method: 在广泛合成基准测试上比较INS算法与原始-对偶内点框架，分析迭代次数、计算时间、精度和收敛性。

Result: 内点法收敛迭代次数减少约三分之一，计算时间减少约一半，精度略高且满足所有主要停止条件；INS在默认设置下成功率较低，但通过正则化和步长控制可显著改善性能。

Conclusion: 内点法作为可靠基准算法，INS作为可配置替代方案，特别适用于问题结构适合自适应正则化的场景。

Abstract: We present a head-to-head evaluation of the Improved Inexact--Newton--Smart (INS) algorithm against a primal--dual interior-point framework for large-scale nonlinear optimization. On extensive synthetic benchmarks, the interior-point method converges with roughly one third fewer iterations and about one half the computation time relative to INS, while attaining marginally higher accuracy and meeting all primary stopping conditions. By contrast, INS succeeds in fewer cases under default settings but benefits markedly from moderate regularization and step-length control; in tuned regimes its iteration count and runtime decrease substantially, narrowing yet not closing the gap. A sensitivity study indicates that interior-point performance remains stable across parameter changes, whereas INS is more affected by step length and regularization choice. Collectively, the evidence positions the interior-point method as a reliable baseline and INS as a configurable alternative when problem structure favors adaptive regularization.

</details>


### [196] [Novel Multi-objective Switched Model Predictive Control with Feasibility and Stability Guarantees](https://arxiv.org/abs/2511.12145)
*Elias Niepötter,Adrian Grimm,Torbjørn Cunis*

Main category: math.OC

TL;DR: 提出了两种多目标切换模型预测控制框架（名义和鲁棒框架），保证递归可行性和闭环系统稳定性，在数值示例中优于标准MPC方法


<details>
  <summary>Details</summary>
Motivation: 随着需要处理多目标（如经济性同时保持性能）的控制系统重要性上升，需要结合MPC优势并处理多目标的方法

Method: 使用切换系统方法，提出名义和鲁棒两种框架，采用监督控制实例实现复杂目标和多目标控制

Result: 保证每个MPC控制器在任意切换下的递归可行性，名义框架确保渐近稳定性，鲁棒框架确保输入到状态稳定性

Conclusion: 提出的多目标切换MPC框架在数值示例中表现出优于标准MPC的性能，能够有效处理多目标控制问题

Abstract: As the relevance of control systems capable of dealing with multiple objectives rises (e.g. being economic while maintaining a certain performance), multi-objective Switched Model Predictive Control combines all the advantages of Model Predictive Control while dealing with multiple objectives. We propose two novel frameworks, a nominal and a robust framework to guarantee recursive feasibility of each Model Predictive Controller under arbitrary switching and assure asymptotic stability of the closed-loop system applying the nominal framework and Input-to-State stability using the robust framework. The presented frameworks employ methods from switched systems, enabling the utilization of a supervisor control instance which allows for complex objectives and multi-objective control. Our numerical example confirms the superior performance of our proposed frameworks compared to a standard Model Predictive Control approach.

</details>


### [197] [Optimization landscape of $\ell_0$-Bregman relaxations](https://arxiv.org/abs/2511.12157)
*Jonathan Chirinos-Rodriguez,Cédric Févotte,Emmanuel Soubies*

Main category: math.OC

TL;DR: 该论文研究了带有ℓ0正则化的线性系统优化问题，提出了ℓ0-Bregman松弛方法，建立了临界点稀疏性隔离的充分条件，并分析了精确恢复特性。


<details>
  <summary>Details</summary>
Motivation: 解决ℓ0正则化优化问题中局部极小值过多的问题，通过非凸精确连续松弛来保留全局极小值同时减少局部极小值数量。

Method: 采用ℓ0-Bregman松弛方法，引入Bregman受限强凸性新概念，建立临界点稀疏性隔离条件，分析oracle解的精确恢复特性。

Result: 为稀疏高斯（最小二乘）和泊松（KL散度）回归问题提供了精确恢复条件，改进了LS设置的现有边界，并为KL情况提供了全新结果。

Conclusion: ℓ0-Bregman松弛方法能有效减少局部极小值，确保解的唯一性和稀疏性隔离，为稀疏回归问题提供了理论保证。

Abstract: In this paper, we study (noisy) linear systems, and their $\ell_0$-regularized optimization problems, coupled with general data fidelity terms. Recent approaches for solving this class of problems have proposed to consider non-convex exact continuous relaxations that preserve global minimizers while reducing the number of local minimizers. Within this framework, we consider the class of $\ell_0$-Bregman relaxations, and establish sufficient conditions under which a critical point is isolated in terms of sparsity, in the sense that any other critical point has a strictly larger cardinality. In this way, we ensure a form of uniqueness in the solution structure. Furthermore, we analyze the exact recovery properties of such exact relaxations. To that end, we derive conditions under which the oracle solution (i.e., the one sharing the same support as the ground-truth) is the unique global minimizer of the relaxed problem, and is isolated in terms of sparsity. Our analysis is primarily built upon a novel property we introduce, termed the Bregman Restricted Strong Convexity. Finally, we specialize our general results to both sparse Gaussian (least-squares) and Poisson ((generalized) Kullback-Leibler divergence) regression problems. In particular, we show that our general analysis sharpens existing bounds for the LS setting, while providing an entirely new result for the KL case.

</details>


### [198] [Computation of a Consistent System Matrix for Cone-beam Computed Tomography](https://arxiv.org/abs/2511.12235)
*Josef Simbrunner,Clemens Krenn,Martin Zach,Andreas Habring*

Main category: math.OC

TL;DR: 提出了一种计算锥束CT系统矩阵的方法，通过分解锥束-体素相交体积为子体积，提供精确计算公式，无需迭代计算，重建效果优于传统线积分方法，并提供CUDA实现。


<details>
  <summary>Details</summary>
Motivation: 传统锥束CT系统矩阵计算方法通常使用基于线积分的近似方法，这些方法不够精确且计算效率低。需要一种能够提供精确系统矩阵计算的高效方法。

Method: 将锥束-体素相交体积分解为贡献到不同探测器元素的子体积，为这些子体积的贡献提供精确计算公式，避免使用计算成本高的迭代子程序。

Result: 在合成和真实CT数据上的数值实验表明，使用该方法得到的重建结果优于常用的基于线积分的方法。

Conclusion: 提出的方法能够计算一致的系统矩阵，提供更精确的重建结果，并通过CUDA实现确保了计算效率。

Abstract: We propose a method for the computation of a consistent system matrix for two- and three-dimensional cone-beam computed tomography (CT). The method relies on the decomposition of the cone-voxel intersection volumes into subvolumes that contribute to distinct detector elements and whose contributions to the system matrix admit exact formulae that can be evaluated without the invocation of costly iterative subroutines. We demonstrate that the reconstructions obtained when using the proposed system matrix are superior to those obtained when using common line-based integration approaches with numerical experiments on synthetic and real CT data. Moreover, we provide a CUDA implementation of the proposed method.

</details>


### [199] [Safe and Operationally Efficient Longitudinal Control of Autonomous Truck Platoons](https://arxiv.org/abs/2511.12336)
*Alexander Hammerl,Ravi Seshadri,Thomas Kjær Rasmussen,Otto Anker Nielsen*

Main category: math.OC

TL;DR: 提出了一种用于自动驾驶卡车车队的层次化纵向控制架构，同时解决安全性、队列稳定性和经济效率问题。该框架集成了高速率安全投影滤波器、基于滞后感知PID控制器的间距调节层，以及慢时间尺度经济优化器。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶卡车车队在安全、稳定性和经济效率方面的综合需求，需要开发一个能够同时保证碰撞避免、队列稳定性和燃油经济性的控制架构。

Method: 采用三层控制架构：1）高速率安全投影滤波器，通过高阶控制屏障函数保证碰撞避免；2）间距调节层，使用滞后感知PID控制器；3）慢时间尺度经济优化器，基于牵引功率关系优化速度轨迹。

Result: 在无干扰条件下，闭环动力学收敛到最优速度模型，并推导了车队稳定时间的最坏情况上界。数值案例研究表明该设计在瞬态行为和长期能效方面优于基准控制器。

Conclusion: 所提出的层次化控制架构能够有效平衡自动驾驶卡车车队的安全性、稳定性和经济效率需求，在实际应用中具有显著优势。

Abstract: This paper presents a hierarchical longitudinal control architecture for autonomous truck platoons that jointly addresses safety, string stability, and economic efficiency. The framework integrates a high-rate safety projection filter, a spacing-regulation layer based on a lag-aware proportional-integral-derivative (PID) controller, and a slow-timescale economic optimizer balancing fuel consumption and travel time. The safety layer guarantees collision avoidance under bounded actuation delays by enforcing forward invariance of a velocity-aware headway constraint through a high-order control barrier function. The regulation layer shapes the spacing-error dynamics into a second-order form with interpretable parameters for damping and natural frequency while explicitly accounting for actuator lag. At the macroscopic level, fuel use is modeled by a tractive-power relation that captures aerodynamic benefits of close spacing, enabling a long-term optimization of speed trajectories subject to comfort and energy trade-offs. We show that the closed-loop dynamics converge to the Optimal Velocity Model with Relative Velocity (OVRV) under undisturbed conditions and derive worst-case upper bounds for platoon stabilization time. Numerical case studies demonstrate the superiority of the proposed design over an canonical baseline controllers in both transient behavior and long-term energy efficiency.

</details>


### [200] [Parameterized complexity of scheduling unit-time jobs with generalized precedence constraints](https://arxiv.org/abs/2511.12395)
*Christina Büsing,Maurice Draeger,Corinna Mathwieser*

Main category: math.OC

TL;DR: 研究了在广义优先约束下并行机调度问题的参数化复杂度，分析了不同约束类型对最小化makespan和完成时间总和的影响。


<details>
  <summary>Details</summary>
Motivation: 传统的调度问题主要关注AND或OR约束，本文扩展研究更复杂的布尔公式约束，探索参数化方法在调度问题中的应用。

Method: 使用参数化复杂度理论，分别以前驱数量和后继数量作为参数，分析不同约束类型（合取、析取、合取范式、析取范式）下的计算复杂度。

Result: 前驱数量作为参数时固定参数可解；后继数量作为参数时，合取/析取约束固定参数可解，析取范式约束为W[1]-难；AND/OR约束即使只有一个后继也是NP难；两机情况下合取/析取约束也是NP难。

Conclusion: 广义优先约束调度问题的复杂度高度依赖于约束类型和参数选择，为传统调度问题的复杂度边界提供了重要补充。

Abstract: We study the parameterized complexity of scheduling unit-time jobs on parallel, identical machines under generalized precedence constraints for minimization of the makespan and the sum of completion times. In our setting, each job is equipped with a Boolean formula (precedence constraint) over the set of jobs. A schedule satisfies a job's precedence constraint if setting earlier jobs to true satisfies the formula. Our definition generalizes several common types of precedence constraints: classical and-constraints if every formula is a conjunction, or-constraints if every formula is a disjunction, and and/or-constraints if every formula is in conjunctive normal form. We prove fixed-parameter tractability when parameterizing by the number of predecessors. For parameterization by the number of successors, however, the complexity depends on the structure of the precedence constraints. If every constraint is a conjunction or a disjunction, we prove the problem to be fixed-parameter tractable. For constraints in disjunctive normal form, we prove W[1]-hardness. We show that the and/or-constrained problem is NP-hard, even for a single successor. Moreover, we prove NP-hardness on two machines if every constraint is a conjunction or a disjunction. This result not only proves para-NP-hardness for parameterization by the number of machines but also complements the polynomial-time solvability on two machines if every constraint is a conjunction (Coffman and Graham 1972) or if every constraint is a disjunction (Berit 2005).

</details>


### [201] [Set System Approximation for Binary Integer Programs: Reformulations and Applications](https://arxiv.org/abs/2511.12437)
*Ningji Wei*

Main category: math.OC

TL;DR: 本文提出了一个统一框架，将覆盖不等式和消除不等式视为二元整数规划中集合系统逼近的基本工具，扩展了多面体分析工具并提供了非线性BIP的重新表述策略。


<details>
  <summary>Details</summary>
Motivation: 覆盖不等式和消除不等式在组合优化中至关重要，但以往研究多局限于特定问题设置或无良好切割。本文旨在建立一个统一视角，将这些不等式作为二元整数规划中集合系统逼近的基本工具。

Method: 开发了一套工具包，包括：恢复经典结构对应关系，将集合覆盖的多面体工具扩展到一般BIP，提出新的非线性系统重新表述技术（如无辅助变量的双线性线性化、双单调切割和区间分解）。

Result: 证明了任意集合系统都允许紧密的内外单调逼近，精确对应覆盖和消除不等式。通过分布式鲁棒网络选址案例研究展示了框架的灵活性和计算优势。

Conclusion: 这一统一视角澄清了内外逼近标准，扩展了经典多面体分析，并为非线性BIP提供了广泛适用的重新表述策略。

Abstract: Covering and elimination inequalities are central to combinatorial optimization, yet their role has largely been studied in problem-specific settings or via no-good cuts. This paper introduces a unified perspective that treats these inequalities as primitives for set system approximation in binary integer programs (BIPs). We show that arbitrary set systems admit tight inner and outer monotone approximations, exactly corresponding to covering and elimination inequalities. Building on this, we develop a toolkit that both recovers classical structural correspondences (e.g., paths vs. cuts, spanning trees vs. cycles) and extends polyhedral tools from set covering to general BIPs, including facet conditions and lifting methods. We also propose new reformulation techniques for nonlinear and latent monotone systems, such as auxiliary-variable-free bilinear linearization, bimonotone cuts, and interval decompositions. A case study on distributionally robust network site selection illustrates the framework's flexibility and computational benefits. Overall, this unified view clarifies inner/outer approximation criteria, extends classical polyhedral analysis, and provides broadly applicable reformulation strategies for nonlinear BIPs.

</details>


### [202] [Performance and Risk Analytics of Asian Exchange-Traded Funds](https://arxiv.org/abs/2511.12476)
*Bhathiya Divelgama,Nancy Asare Nyarko,Naa Sackley Dromo Aryee,Abootaleb Shirvani,Svetlozar T. Rachev*

Main category: math.OC

TL;DR: 该研究分析亚洲ETF投资，通过风险评估、组合分析和绩效比较，探讨了29只涵盖广泛亚洲市场的ETF，使用现代投资组合理论和风险指标来评估风险收益权衡和尾部风险行为。


<details>
  <summary>Details</summary>
Motivation: 投资亚洲ETF为投资者提供了接触快速增长经济体和宝贵多元化机会的途径，但需要深入了解其风险收益特征和投资效率。

Method: 采用Markowitz有效前沿识别最优投资组合，使用条件风险价值(CVaR)捕捉极端损失，分析多头和多空策略，并计算夏普比率、Rachev比率和STARR等关键绩效风险指标。

Result: 研究提供了对亚洲ETF风险收益权衡、尾部风险行为和投资效率的深入见解，识别了在不同市场条件下的适应性投资组合配置。

Conclusion: 该研究为投资者在亚洲新兴和发达市场中构建稳健且多元化的投资组合提供了实用基础，强调了尾部风险行为在投资组合绩效中的重要性。

Abstract: Investing in Asian markets through exchange-traded funds (ETFs) provides investors with access to rapidly expanding economies and valuable diversification opportunities. This study examines the advantages and challenges of investing in Asian ETFs by conducting comprehensive risk assessments, portfolio analyses, and performance comparisons. The dataset comprises 29 ETFs offering exposure across a wide spectrum of Asian markets, including broad regional funds, country-specific ETFs, as well as sector-focused funds, dividend-oriented ETFs, small-cap portfolios, and emerging market bond ETFs.
  To evaluate risk and return dynamics, the study employs Markowitz's efficient frontier to identify optimal portfolios for given levels of risk, and conditional value-at-risk (CVaR) to capture potential extreme losses for a more comprehensive risk assessment. Multiple portfolio configurations are analyzed under long-only and long-short investment strategies to assess adaptability across varying market conditions. Furthermore, key performance risk measures, including the Sharpe ratio, Rachev ratio, and stable tail-adjusted return ratio (STARR), are calculated to provide an in-depth evaluation of reward-to-risk efficiency, with particular emphasis on the role of tail behavior in portfolio performance.
  This research aims to deliver deeper insights into the risk-return trade-offs, tail-risk behavior, and efficiency of Asian ETFs, offering investors a practical foundation for constructing robust and well-diversified portfolios across both emerging and developed Asian markets.

</details>


### [203] [DLMMPR:Deep Learning-based Measurement Matrix for Phase Retrieval](https://arxiv.org/abs/2511.12556)
*Jing Liu,Bing Guo,Ren Zhu*

Main category: math.OC

TL;DR: 该论文首次将学习优化集成到相位恢复的测量矩阵设计中，提出了基于深度学习的测量矩阵算法DLMMPR，通过端到端深度学习架构参数化测量矩阵，结合次梯度下降和近端映射模块实现鲁棒恢复。


<details>
  <summary>Details</summary>
Motivation: 传统相位恢复方法在测量矩阵设计方面存在局限，需要探索将学习优化集成到测量矩阵设计中的新方法，以提高相位恢复的性能和鲁棒性。

Method: 提出DLMMPR算法，在端到端深度学习架构中参数化测量矩阵，并集成次梯度下降和近端映射模块来增强恢复的鲁棒性。

Result: 在多种噪声环境下进行综合实证验证，与DeepMMSE和PrComplex基准方法相比，DLMMPR在PSNR和SSIM指标上取得显著提升。

Conclusion: DLMMPR算法在相位恢复任务中表现出优越性能，证明了将学习优化集成到测量矩阵设计的有效性。

Abstract: This paper pioneers the integration of learning optimization into measurement matrix design for phase retrieval. We introduce the Deep Learning-based Measurement Matrix for Phase Retrieval (DLMMPR) algorithm, which parameterizes the measurement matrix within an end-to-end deep learning architecture. Synergistically augmented with subgradient descent and proximal mapping modules for robust recovery, DLMMPR's efficacy is decisively confirmed through comprehensive empirical validation across diverse noise regimes. Benchmarked against DeepMMSE and PrComplex, our method yields substantial gains in PSNR and SSIM, underscoring its superiority.

</details>


### [204] [A New Perspective on Double-S Curve Motions of Higher Order and Optimal Motion Planning](https://arxiv.org/abs/2511.12615)
*Rico Zöllner*

Main category: math.OC

TL;DR: 提出了对称轨迹时间范围的通用方程，适用于任意阶导数有界的情况，避免了案例分析，并给出了时间最小化问题的简化算法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理对称轨迹时间范围时需要大量案例分析，计算复杂且缺乏通用性。

Method: 推导并证明了对称轨迹时间范围的通用方程，开发了将时间最小化问题转化为方程组求解的算法。

Result: 得到了适用于任意相位数的通用时间范围方程，避免了案例分析，简化了优化过程。

Conclusion: 提出的方程和算法有效简化了对称轨迹时间范围的计算，为运动规划提供了更高效的工具。

Abstract: This paper presents and proves an equation for the time horizon of symmetric trajectories with zero boundary conditions and bounded derivatives of arbitrary order. This equation holds regardless of the number of phases comprising the associated motion. This avoids case distinctions in calculations. Application examples of motions with minimum time, minimum velocity, and minimum acceleration are discussed. Furthermore, an algorithm is derived that reduces the time minimization problem to solving a system of equations. This algorithm avoids nested case distinctions and complex optimizations.

</details>


### [205] [Approximate Tracking Controllability of Systems with Quadratic Nonlinearities](https://arxiv.org/abs/2511.12634)
*Manuel Rissel,Marius Tucsnak*

Main category: math.OC

TL;DR: 该论文研究了线性系统和二次非线性系统的近似跟踪可控性。对于线性系统，证明了完全状态的近似跟踪可控性在一般情况下是不可能的，除非控制空间与状态空间同构。对于非线性系统，证明了在任意时间范围内，具有任意线性部分和二次非线性项的系统具有弱近似跟踪可控性。


<details>
  <summary>Details</summary>
Motivation: 研究控制系统能否在任意给定路径的ε邻域内维持状态轨迹，即近似跟踪可控性。由于线性系统的负面结果，转向研究非线性系统的可控性。

Method: 对于线性系统，开发了系统性的分析方法；对于非线性系统，研究了具有任意线性部分和二次非线性项的系统，并采用了涉及松弛度量的弱近似跟踪可控性概念。

Result: 线性系统的近似跟踪可控性在一般情况下不可能实现；非线性系统在弱意义下具有近似跟踪可控性，即使未受控动力学可能在有限时间内出现奇点。

Conclusion: 线性系统的近似跟踪可控性存在根本限制，而具有二次非线性项的系统在弱意义下可以实现全局的近似跟踪可控性，这在耦合系统和运动规划问题中具有应用价值。

Abstract: Given a finite-dimensional time continuous control system and $\varepsilon>0$, we address the question of the existence of controls that maintain the corresponding state trajectories in the $\varepsilon$-neighborhood of any prescribed path in the state space. We investigate this property, called approximate tracking controllability, for linear and quadratic time invariant systems. Concerning linear systems, our answers are negative: by developing a systematic approach, we demonstrate that approximate tracking controllability of the full state is impossible even in a certain weak sense, except for the trivial situation where the control space is isomorphic to the state space. Motivated by these negative findings for linear systems, we focus on nonlinear dynamics. In particular, we prove weak approximate tracking controllability on any time horizon for a general class of systems with arbitrary linear part and quadratic nonlinear terms. The considered weak notion of approximate tracking controllability involves the relaxation metric. We underline the relevance of this weak setting by developing applications to coupled systems (including motion planning problems) and by remarking obstructions that would arise for natural stronger norms. The exposed framework yields global results even if the uncontrolled dynamics might exhibit singularities in finite time.

</details>


### [206] [The iterates of FISTA convergence even under inexact computations](https://arxiv.org/abs/2511.12665)
*Saverio Salzo*

Main category: math.OC

TL;DR: 该论文扩展了最近关于Nesterov加速梯度方法收敛性的突破性成果，证明了在无限维希尔伯特空间中，即使邻近算子和梯度的计算存在误差，该算法的迭代仍然保持弱收敛性。


<details>
  <summary>Details</summary>
Motivation: 最近Jang和Ryu以及Bot等人的工作解决了Nesterov加速梯度方法迭代收敛的长期开放问题。本文旨在将这些结果扩展到更一般的无限维希尔伯特空间，并考虑计算不精确的情况。

Method: 在无限维希尔伯特空间中，分析Nesterov加速梯度方法（FISTA）在邻近算子和梯度计算存在误差情况下的收敛行为，证明其迭代的弱收敛性。

Result: 证明了即使在邻近算子和梯度计算不精确的情况下，Nesterov加速梯度方法的迭代在无限维希尔伯特空间中仍然保持弱收敛到最优解。

Conclusion: 该研究将Nesterov加速梯度方法的收敛性结果推广到了无限维希尔伯特空间，并且放宽了对计算精度的要求，增强了该算法在实际应用中的鲁棒性。

Abstract: Very recently, the papers "Point Convergence of Nesterov's Accelerated Gradient Method: An AI-Assisted Proof" by Jang and Ryu, and "The Iterates of Nesterov's Accelerated Algorithm Converge in the Critical Regimes" by Bot, Fadili, and Nguyen simultaneously have resolved a long-standing open problem concerning Nesterov's accelerated gradient method. These works show that the iterates of the algorithm (known in its composite form as FISTA) indeed converge to an optimal solution. In this work, we extend these results and prove that, in infinite dimensional Hilbert spaces, the iterates of such an algorithm still converge (in the weak sense) even under inexact computation of the proximity operator and the gradient.

</details>


### [207] [DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks](https://arxiv.org/abs/2511.12836)
*Waheed U. Bajwa,Mert Gurbuzbalaban,Mustafa Ali Kutbay,Lingjiong Zhu,Muhammad Zulqarnain*

Main category: math.OC

TL;DR: 提出了DIGing-SGLD算法，这是首个用于时变网络上的去中心化SGLD方法，解决了现有方法局限于静态网络和存在稳态采样偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化SGLD方法仅限于静态网络拓扑，且许多方法存在由网络效应引起的稳态采样偏差，即使使用全批量数据时也是如此。需要开发能够在时变网络上进行高效无偏采样的方法。

Method: 将基于Langevin的采样与DIGing算法的梯度跟踪机制相结合，DIGing算法最初是为时变网络上的去中心化优化而开发的。这种方法无需中央协调器即可实现高效无偏采样。

Result: 在标准强凸性和平滑性假设下，DIGing-SGLD实现了对目标分布的几何收敛，达到$O(\sqrtη)$邻域，其中$η$是步长。提供了首个针对时变网络上基于去中心化SGLD采样的有限时间非渐近Wasserstein收敛保证。

Conclusion: DIGing-SGLD在动态演化网络条件下表现出强大的实证性能，数值实验验证了理论结果，表明该方法在贝叶斯线性和逻辑回归任务中有效。

Abstract: Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\sqrtη)$ neighborhood of the target distribution, where $η$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.

</details>


### [208] [Electric Truck Platooning with Charging Consideration and Leader Swapping](https://arxiv.org/abs/2511.12965)
*Yilang Hao,Zhibin Chen*

Main category: math.OC

TL;DR: 研究电动卡车在一般道路网络上的编队行驶，通过优化路线选择、充电站选择、编队形成和位置交换来降低运营成本，并提出高效的启发式算法解决大规模问题。


<details>
  <summary>Details</summary>
Motivation: 电动卡车在长途运输中面临续航里程有限和充电需求等运营挑战，编队行驶可以通过节能缓解里程焦虑，但现有研究大多局限于单一高速公路走廊，未能捕捉网络范围内的运营情况。

Method: 建立混合整数线性规划模型，联合优化路线路径、充电站选择、劳动时间和编队形成与位置交换。由于精确方法在现实规模实例中难以处理，开发了增强的自适应大邻域搜索算法，包含基于节省的边界方案、不可行对消除和候选站过滤。

Result: 计算实验表明，编队行驶可将总运营成本降低高达2.77%，所提算法相比CPLEX将计算时间减少高达99.96%，能在约120秒内解决150辆卡车的实例。

Conclusion: 编队行驶能显著降低电动卡车的运营成本，所开发的高效算法具有实际应用的强大潜力。

Abstract: Electric trucks are increasingly deployed to reduce the trucking sector's carbon footprint, but their limited range and charging needs create operational challenges on mid- to long-haul routes. Truck platooning can mitigate range anxiety through energy savings and, in turn, influence routing and charging decisions, yet most existing studies focus on a single highway corridor and do not capture network-wide operations. We study electric truck platooning on a general road network, where trucks must select routes and charging stations with heterogeneous prices and charging speeds, form platoons on shared arcs, and possibly take detours that trade off platoon savings with additional labor hours. We further allow in-platoon position swaps so that leading responsibility rotates, balancing battery usage and avoiding early depletion of any single truck. To jointly optimize routing paths, charging-station choices, labor time, and platoon formation and position swaps, we formulate a mixed-integer linear program (MILP). Because exact methods become intractable on realistic instances, we develop an Adaptive Large Neighborhood Search (ALNS) algorithm enhanced with a savings-based bounding scheme, infeasible-pair elimination, and candidate-station filtering. Computational experiments on test instances with up to 150 trucks show that incorporating platooning can reduce total operational costs by up to 2.77 percent, while the proposed algorithm cuts computation time by up to 99.96 percent compared with CPLEX and solves 150-truck instances in about 120 seconds, indicating strong potential for real-world applications.

</details>


### [209] [The Geometry of Hidden Modes in Distance-Based Formation Control](https://arxiv.org/abs/2511.13187)
*Solomon Goldgraber Casspi,Daniel Zelazo*

Main category: math.OC

TL;DR: 提出了基于距离的编队控制中隐藏模式的几何输入输出分析框架，揭示了系统结构限制及其动态影响


<details>
  <summary>Details</summary>
Motivation: 研究距离编队控制中不可控模式的几何特性及其对系统性能的影响，为理解控制器局限性提供理论基础

Method: 使用几何框架分析线性化动力学，定义全局旋转子空间和局部旋转子空间，建立不可控子空间的完整分解

Result: 证明不可控刚体模式是绕输入节点的纯旋转，局部旋转子空间包含控制器在节点处局部不可见的运动，几何结构与扰动抑制能力直接相关

Conclusion: 系统恢复形状的能力取决于输入与标准旋转刚体模式局部分量的对齐程度，隐藏模式的几何特性决定了系统的扰动抑制性能

Abstract: This paper presents a geometric input-output analysis of hidden modes in distance-based formation control. We study the linearized dynamics under a gradient control law to characterize the system's structural limitations and their dynamic consequences. Our main contribution is a unified geometric framework for uncontrollable modes. We first prove that uncontrollable rigid-body modes are pure rotations about the input node, defining a global rotational subspace $\mathcal{R}_i$. To generalize this, we introduce the local rotational subspace, $\mathcal{T}_i$, which contains all motions, including deformations, that are locally invisible to the controller at node $i$. These two geometric objects provide a complete decomposition of the uncontrollable subspace. Finally, we demonstrate the dynamic implications of this structure by proving that the system's ability to recover its shape is determined by an input's alignment with the local component of the standard rotational rigid-body mode, directly linking the geometry of hidden modes to disturbance rejection. We illustrate our results with a case study.

</details>


### [210] [Sampling in BV-Type Spaces](https://arxiv.org/abs/2511.13196)
*Vincent Guillemet,Michael Unser*

Main category: math.OC

TL;DR: 本文证明了有界变差函数采样泛函的连续性，建立了微分算子的局部逆，并提出了包含BV函数采样的正则化优化问题的存在性定理。


<details>
  <summary>Details</summary>
Motivation: 有界变差函数采样在变分逆问题中具有重要意义，但标准理论无法保证包含BV函数采样的损失泛函解的存在性。

Method: 证明采样泛函的连续性，建立微分算子D的局部逆，并利用这种规范反演来构建优化问题。

Result: 获得了包含BV函数采样的正则化优化问题的存在性定理，并刻画了解集的极值点特征。

Conclusion: 该研究为包含有界变差函数采样的变分逆问题提供了理论基础，建立了微分算子的局部可逆性和解的存在性保证。

Abstract: The sampling of functions of bounded variation (BV) is a long-standing problem in op- timization. The ability to sample such functions has relevance in the field of variational inverse problems, where the standard theory fails to guarantee the mere existence of solutions when the loss functional involves samples of BV functions. In this paper, we prove the continuity of sampling functionals and show that the differential operator D admits a unique local inverse. This canonical inversion enables us to formulate an existence theorem for a class of regularized optimization problems that incorporate samples of BV functions. Finally, we characterize the solution set in terms of its extreme points.

</details>


### [211] [Sparse stabilization of mean-field agent dynamics through a three-operator splitting method](https://arxiv.org/abs/2511.13455)
*Giacomo Albi,Dante Kalise,Chiara Segala,Franco Zivcovich*

Main category: math.OC

TL;DR: 提出了一种基于均值场最优控制的非线性多智能体系统稀疏稳定方法，通过l1-l2惩罚实现最小控制努力下的共识形成，采用三算子分裂算法求解非光滑优化问题。


<details>
  <summary>Details</summary>
Motivation: 研究大规模交互智能体系统的稀疏稳定问题，目标是以最小的控制努力驱动智能体群体达成共识，解决传统方法控制成本高的问题。

Method: 在均值场极限下使用Vlasov型动力学方程描述系统，采用l1-l2惩罚项强制稀疏性，通过三算子分裂方法分别处理光滑、非光滑和约束分量，结合粒子蒙特卡洛离散化和随机批次交互实现可扩展计算。

Result: 在Cucker-Smale模型上的数值实验表明，该方法能够有效形成共识，同时产生稀疏、局部化的控制动作，验证了方法的效率和鲁棒性。

Conclusion: 所提出的均值场最优控制框架结合稀疏惩罚和三算子分裂算法，为大规模多智能体系统的稀疏稳定提供了一种高效且可扩展的解决方案。

Abstract: We study the sparse stabilization of nonlinear multi-agent systems within a mean-field optimal control framework. The goal is to drive large populations of interacting agents toward consensus with minimal control effort. In the mean-field limit, the dynamics are described by a Vlasov-type kinetic equation, and sparsity is enforced through an l1-l2 penalization in the cost functional. The resulting nonsmooth optimization problem is solved via a three-operator splitting (TOS) method that separately handles smooth, nonsmooth, and constraint components through gradient, shrinkage, and projection steps. A particle-based Monte Carlo discretization with random batch interactions enables scalable computation while preserving the mean-field structure. Numerical experiments on the Cucker-Smale model demonstrate effective consensus formation with sparse, localized control actions, confirming the efficiency and robustness of the proposed approach.

</details>


### [212] [Uniform Feasibility For Smoothed Backup Control Barrier Functions](https://arxiv.org/abs/2511.13499)
*Anil Alan,Bart De Schutter*

Main category: math.OC

TL;DR: 该论文研究了使用控制屏障函数(CBFs)开发的安全过滤器在安全集由连续可微函数的逐点最小值定义时的可行性保证，通过log-sum-exp平滑处理非光滑结构，为备份CBF方法提供先验可行性保证。


<details>
  <summary>Details</summary>
Motivation: 当安全集使用连续可微函数的逐点最小值定义时（常见于备份CBF方法），这种构造通常是非光滑的，需要研究其可行性保证。

Method: 使用log-sum-exp(软最小值)平滑替换最小值函数，在严格安全条件下证明平滑函数在一定参数范围内成为CBF或扩展CBF，为紧凑和无界安全集分别推导平滑参数下界和尾部条件。

Result: 对于紧凑安全集，推导出使平滑函数成为CBF的平滑参数显式下界；对于无界集，引入确保平滑函数满足统一扩展CBF条件的尾部条件；这些结果应用于备份CBFs，证明备份集安全性和边界条件足以保证可行性。

Conclusion: 该研究为非光滑安全集的平滑内近似提供了先验可行性保证的配方，无需额外的在线认证，为备份CBF方法建立了理论基础。

Abstract: We study feasibility guarantees for safety filters developed using Control Barrier Functions (CBFs) when a safe set is defined using the pointwise minimum of continuously differentiable functions, a construction that is common for the backup CBF method and typically nonsmooth. We replace the minimum by its log-sum-exp (soft-min) smoothing and show that, under a strict safety condition, the smooth function becomes a CBF (or extended CBF) for a range of the smoothing parameter. For compact safe sets, we derive an explicit lower bound on the smoothing parameter that makes the smooth function a CBF and hence renders the corresponding safety filter feasible. For unbounded sets, we introduce tail conditions under which the smooth function satisfies an extended CBF condition uniformly. Finally, we apply these results to backup CBFs. We show that safety of a compact (terminal) backup set under a backup controller, together with a condition ensuring safety of the backup trajectories on the relevant boundary of the safe set, is sufficient for feasibility for backup CBFs. These results provide a recipe for a priori feasibility guarantees for smooth inner approximations of nonsmooth safe sets without the need for additional online certification.

</details>


### [213] [A Computationally Efficient Framework for Free-trajectory Minimum-lap-time Optimization of Racing Cars](https://arxiv.org/abs/2511.13522)
*Erik van den Eshof,Jorn van Kampen,Mauro Salazar*

Main category: math.OC

TL;DR: 提出了一个计算高效的赛车最小圈时空间轨迹和动力系统操作框架，通过顺序凸规划算法将计算时间从几分钟减少到几秒，并证明基于最小时间的赛车线比最小曲率方法快4%。


<details>
  <summary>Details</summary>
Motivation: 开发一个计算高效的框架来优化赛车的最小圈时轨迹和动力系统操作，实现实时计算并验证现有建模假设。

Method: 首先推导赛车的准稳态模型，联合优化赛车线轨迹；然后构建最小圈时问题，利用其凸结构设计顺序凸规划求解算法。

Result: 与现成的非线性规划求解器相比，计算时间从几分钟减少到几秒；与最小曲率方法相比，最小时间赛车线可带来4%的圈时提升；能量限制对赛车线影响很小（<0.1%）。

Conclusion: 该框架实现了高效的最小圈时优化，支持实时应用，验证了赛车线不受能量限制的建模假设是合理的。

Abstract: This paper presents a modeling and optimization framework to compute the minimum-lap-time spatial trajectory and powertrain operation of racing cars in a computationally efficient fashion. Specifically, we first derive a quasi-steady-state model of a racing car, whereby the racing line trajectory is jointly optimized. Next, we frame the minimum-lap-time problem and leverage its mostly convex structure by devising a sequential convex programming solution algorithm. We benchmark our method against off-the-shelf nonlinear programming solvers, showing how it can bring computation time down from a few minutes to a few seconds, paving the way for real-time implementations. Moreover, we compare our results to similarly efficient minimum-curvature racing line optimization methods, showing how a minimum-time-based racing line might lead to 4% faster lap-times. Finally, we showcase our framework for optimal powertrain energy management and we validate the common modeling assumption that the racing line is unaffected by energy limitations, showing that this assumption results in marginal lap-time losses of under 0.1%.

</details>


### [214] [HBNET-GIANT: A communication-efficient accelerated Newton-type fully distributed optimization algorithm](https://arxiv.org/abs/2511.13584)
*Souvik Das,Luca Schenato,Subhrakanti Dey*

Main category: math.OC

TL;DR: 提出了HBNET-GIANT算法，这是一种基于重球动量的二阶全分布式优化算法，用于L-光滑和μ-强凸目标函数。该算法在特定充分条件下实现了全局线性收敛，并通过数值实验证明了其加速效果。


<details>
  <summary>Details</summary>
Motivation: 开发一种结合重球动量的二阶分布式优化算法，以加速收敛并提高性能，为更广泛的牛顿型动量算法奠定基础。

Method: HBNET-GIANT算法，结合重球动量的二阶全分布式优化方法，适用于L-光滑和μ-强凸函数。

Result: 算法在特定条件下实现全局线性收敛，数值实验显示其收敛速度明显快于无动量版本NETWORK-GIANT，且优于多种最先进算法。

Conclusion: 该工作为更广泛的二阶牛顿型动量算法奠定了基础，并激励进一步研究全分布式凸优化中的局部加速分析证明等开放问题。

Abstract: This article presents a second-order fully distributed optimization algorithm, HBNET-GIANT, driven by heavy-ball momentum, for $L$-smooth and $μ$-strongly convex objective functions. A rigorous convergence analysis is performed, and we demonstrate global linear convergence under certain sufficient conditions. Through extensive numerical experiments, we show that HBNET-GIANT with heavy-ball momentum achieves acceleration, and the corresponding rate of convergence is strictly faster than its non-accelerated version, NETWORK-GIANT. Moreover, we compare HBNET-GIANT with several state-of-the-art algorithms, both momentum-based and without momentum, and report significant performance improvement in convergence to the optimum. We believe that this work lays the groundwork for a broader class of second-order Newton-type algorithms with momentum and motivates further investigation into open problems, including an analytical proof of local acceleration in the fully distributed setting for convex optimization problems.

</details>


### [215] [Power Homotopy for Zeroth-Order Non-Convex Optimizations](https://arxiv.org/abs/2511.13592)
*Chen Xu*

Main category: math.OC

TL;DR: GS-PowerHP是一种新颖的零阶非凸优化方法，通过功率变换的高斯平滑代理函数和递减方差策略，在期望意义下收敛到全局最大值的邻域，并在高维图像攻击任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决非凸优化问题中寻找全局最大值点的挑战，特别是在高维场景下传统方法效果不佳的问题。

Method: 使用功率变换的高斯平滑代理函数F_{N,σ}(μ) = 𝔼[e^{N f(x)}]，其平稳点在大N时聚集在f的全局最大值点x*附近，并采用递减方差σ策略提高数据效率。

Result: 理论证明在温和假设下以O(d^2 ε^{-2})的迭代复杂度收敛到x*的小邻域，实验结果显示在多个算法中排名前三，在d=150,528的高维图像攻击任务中超越所有竞争方法获得第一名。

Conclusion: GS-PowerHP是一种高效稳健的零阶非凸优化方法，特别适用于高维问题，在理论和实验上都表现出色。

Abstract: We introduce GS-PowerHP, a novel zeroth-order method for non-convex optimization problems of the form $\max_{x \in \mathbb{R}^d} f(x)$. Our approach leverages two key components: a power-transformed Gaussian-smoothed surrogate $F_{N,σ}(μ) = \mathbb{E}_{x\sim\mathcal{N}(μ,σ^2 I_d)}[e^{N f(x)}]$ whose stationary points cluster near the global maximizer $x^*$ of $f$ for sufficiently large $N$, and an incrementally decaying $σ$ for enhanced data efficiency. Under mild assumptions, we prove convergence in expectation to a small neighborhood of $x^*$ with the iteration complexity of $O(d^2 \varepsilon^{-2})$. Empirical results show our approach consistently ranks among the top three across a suite of competing algorithms. Its robustness is underscored by the final experiment on a substantially high-dimensional problem ($d=150,528$), where it achieved first place on least-likely targeted black-box attacks against images from ImageNet, surpassing all competing methods.

</details>


### [216] [Subgame Perfect Methods in Nonsmooth Convex Optimization](https://arxiv.org/abs/2511.13639)
*Benjamin Grimmer,Alex L. Wang*

Main category: math.OC

TL;DR: 本文针对非光滑凸优化问题，在次梯度oracle和邻近算子oracle两种设置下，提出了满足博弈论最优性概念（子博弈完美性）的算法。


<details>
  <summary>Details</summary>
Motivation: 现有算法通常只满足极小极大最优性，但子博弈完美性要求算法不仅在整体问题类上提供最优保证，还要在算法执行过程中揭示的任何子类上都保持最优。

Method: 对于次梯度oracle，证明了Drori和Teboulle的Kelley切割平面类方法是子博弈完美的；对于邻近算子oracle，提出了新的子博弈完美邻近点算法，每次迭代求解历史感知二阶锥规划。

Result: 这些方法在性能保证上从不差于极小极大最优保证，且通常显著更好，且求解复杂度与问题维度无关。

Conclusion: 本文为两类非光滑凸优化问题提供了子博弈完美算法，扩展了算法最优性理论，并展示了优于传统极小极大最优性的性能表现。

Abstract: This paper considers nonsmooth convex optimization with either a subgradient or proximal operator oracle. In both settings, we identify algorithms that achieve the recently introduced game-theoretic optimality notion for algorithms known as subgame perfection. Subgame perfect algorithms meet a more stringent requirement than just minimax optimality. Not only must they provide optimal uniform guarantees on the entire problem class, but also on any subclass determined by information revealed during the execution of the algorithm. In the setting of nonsmooth convex optimization with a subgradient oracle, we show that the Kelley cutting plane-Like Method due to Drori and Teboulle [1] is subgame perfect. For nonsmooth convex optimization with a proximal operator oracle, we develop a new algorithm, the Subgame Perfect Proximal Point Algorithm, and establish that it is subgame perfect. Both of these methods solve a history-aware second-order cone program within each iteration, independent of the ambient problem dimension, to plan their next steps. This yields performance guarantees that are never worse than the minimax optimal guarantees and often substantially better.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [217] [Generalized Inequality-based Approach for Probabilistic WCET Estimation](https://arxiv.org/abs/2511.11682)
*Hayate Toba,Atsushi Yano,Takuya Azumi*

Main category: stat.ML

TL;DR: 提出一种结合饱和函数（反正切和双曲正切）的改进切比雪夫不等式方法，用于减少重尾分布下概率最坏情况执行时间估计的悲观性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：基于极值理论的方法需要确定分布上尾起始点，存在模型不确定性；基于不等式的方法虽然避免此问题，但对重尾分布会产生悲观结果。

Method: 将饱和函数（arctangent和hyperbolic tangent）整合到切比雪夫不等式中，减轻大离群值的影响，同时保持数学严谨性。

Result: 在合成数据和Autoware自动驾驶堆栈的真实数据上评估，证明该方法能为重尾分布提供更紧致且安全的上界。

Conclusion: 所提出的方法有效减少了重尾分布下pWCET估计的悲观性，为实时系统提供了更准确的时间保证。

Abstract: Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions.

</details>


### [218] [FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition](https://arxiv.org/abs/2511.11817)
*Zhongde An,Jinhong You,Jiyanglin Li,Yiming Tang,Wen Li,Heming Du,Shouguo Du*

Main category: stat.ML

TL;DR: 提出了频率分解网络(FreDN)，通过可学习的频率解耦器在频域分离趋势和周期分量，并引入ReIm块降低复数运算复杂度，在长期时间序列预测中优于现有方法10%以上，同时减少50%以上的参数和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有频域方法在处理非平稳时间序列时面临频谱纠缠和复数学习计算负担的问题，频谱纠缠指趋势、周期性和噪声在频谱上的重叠，现有分解方法无法有效解决这一问题。

Method: 1. 可学习的频率解耦器模块直接在频域分离趋势和周期分量；2. ReIm块通过理论支持的实部-虚部共享参数设计降低复数运算复杂度；3. 重新审视频域损失函数并提供新的理论见解。

Result: 在7个长期预测基准测试中，FreDN比最先进方法性能提升高达10%；与标准复数架构相比，实部-虚部共享参数设计减少至少50%的参数和计算成本。

Conclusion: FreDN通过频域分解有效解决了频谱纠缠问题，同时通过高效的复数运算设计显著降低了计算复杂度，为时间序列预测提供了新的有效解决方案。

Abstract: Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\%.

</details>


### [219] [PCA recovery thresholds in low-rank matrix inference with sparse noise](https://arxiv.org/abs/2511.11927)
*Urte Adomaityte,Gabriele Sicuro,Pierpaolo Vivo*

Main category: stat.ML

TL;DR: 本文研究了稀疏噪声污染下秩一信号的高维推断问题，使用统计物理的副本方法分析了特征值和特征向量的典型行为，并识别了信号恢复的临界阈值。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏噪声（而非传统稠密噪声）下秩一信号的恢复问题，将经典的BBP相变推广到稀疏图结构的情况。

Method: 使用统计物理的副本方法，通过辅助概率密度函数的递归分布方程进行分析，并用群体动力学算法高效求解。

Result: 解析计算了最大特征值、特征向量分量密度和信号向量与特征向量的重叠度，识别了信号恢复的临界强度，在Poisson和随机正则度分布下验证了结果。

Conclusion: 成功将BBP相变推广到稀疏噪声情况，在大连接度极限下恢复稠密噪声结果，解析结果与大规模矩阵数值对角化一致。

Abstract: We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices.

</details>


### [220] [Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence](https://arxiv.org/abs/2511.11983)
*Debashis Chatterjee*

Main category: stat.ML

TL;DR: 提出统一贝叶斯与AI框架，结合贝叶斯预测和贝叶斯超参数优化，为流行病学分析提供校准的不确定性量化和优化的模型选择。


<details>
  <summary>Details</summary>
Motivation: 现代流行病学分析使用的机器学习模型预测能力强但缺乏校准的不确定性，贝叶斯方法能提供原则性的不确定性量化但难以与当代AI工作流集成。

Method: 使用贝叶斯逻辑回归获得个体疾病风险和后验置信区间，同时使用高斯过程贝叶斯优化调整惩罚Cox生存模型，构建双层系统。

Result: 贝叶斯层提供可靠的覆盖率和改进的校准，贝叶斯收缩提高AUC、Brier分数和log-loss，贝叶斯优化使生存模型达到接近oracle的一致性。

Conclusion: 贝叶斯推理增强了推断和搜索能力，为流行病学决策提供校准的风险评估和原则性的超参数智能。

Abstract: Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization.
  We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective.
  Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making.

</details>


### [221] [PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning](https://arxiv.org/abs/2511.12278)
*Mingqi Wu,Qiang Sun,Yi Yang*

Main category: stat.ML

TL;DR: 本文提出PCA++方法，通过硬均匀性约束对比学习来从正样本对中恢复共享信号子空间，有效对抗结构化背景噪声干扰。


<details>
  <summary>Details</summary>
Motivation: 高维数据中的低维信号常被结构化背景噪声掩盖，标准PCA效果有限。对比学习启发了从正样本对中恢复共享信号子空间的需求。

Method: PCA++采用硬均匀性约束对比PCA，强制投影特征具有恒等协方差，通过广义特征问题获得闭式解，在高维情况下保持稳定。

Result: PCA++在模拟实验、损坏MNIST和单细胞转录组学中优于标准PCA和仅对齐的PCA+，能可靠恢复条件不变结构。

Conclusion: 明确特征分散的均匀性约束能防御结构化噪声并增强鲁棒性，阐明了对比学习中均匀性的作用。

Abstract: High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness.

</details>


### [222] [Accelerated Distributional Temporal Difference Learning with Linear Function Approximation](https://arxiv.org/abs/2511.12688)
*Kaicheng Jin,Yang Peng,Jiansheng Yang,Zhihua Zhang*

Main category: stat.ML

TL;DR: 本文研究了使用线性函数逼近的分布时序差分学习的有限样本统计率，建立了与支撑大小K无关的紧致样本复杂度界限，表明学习回报函数的完整分布与学习其期望值具有相同的难度。


<details>
  <summary>Details</summary>
Motivation: 先前关于分布TD学习的统计分析主要关注表格情况，本文旨在研究线性函数逼近设置下的统计效率，特别是当支撑大小K很大时如何建立不依赖于K的样本复杂度界限。

Method: 首先对线性分类贝尔曼方程进行细粒度分析，然后在新的算法中结合方差减少技术，以建立与支撑大小K无关的紧致样本复杂度界限。

Result: 当K很大时，建立了不依赖于支撑大小K的紧致样本复杂度界限，表明使用线性函数逼近的分布TD学习从流数据中学习回报函数的完整分布与学习其期望值具有相同的难度。

Conclusion: 这项工作为分布强化学习算法的统计效率提供了新的见解，证明学习完整回报分布并不比学习其期望值更困难。

Abstract: In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.

</details>


### [223] [The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business](https://arxiv.org/abs/2511.13503)
*Ioannis Diamantis*

Main category: stat.ML

TL;DR: 本文介绍了拓扑数据分析(TDA)在商业和经济数据分析中的应用，通过持久同调方法揭示传统线性工具无法捕捉的非线性多尺度结构模式，并提出了拓扑稳定性指数(TSI)来衡量结构变异性。


<details>
  <summary>Details</summary>
Motivation: 现代商业和经济数据集通常呈现非线性、多尺度结构，传统线性工具难以充分表征这些特征。TDA提供了一种几何视角来发现跨尺度的稳健模式。

Method: 采用持久同调方法，构建了实用的TDA分析流程，包括距离度量选择、复形构建和解释，并引入了拓扑稳定性指数(TSI)。通过消费者行为、股票市场和外汇动态的案例研究进行验证。

Result: 研究表明拓扑特征能够揭示超越经典统计方法的分割模式和结构关系，在SAX/eSAX与TDA的比较中显示出优势。

Conclusion: 为商业和经济分析中的TDA实施、可视化和沟通提供了实用指南，证明了TDA在揭示复杂数据结构方面的价值。

Abstract: Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.

</details>


### [224] [TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting](https://arxiv.org/abs/2511.12749)
*Zong-Han Bai,Po-Yen Chu*

Main category: stat.ML

TL;DR: TSB-HB是一种分层贝叶斯扩展的TSB方法，用于间歇性需求预测，通过Beta-Binomial分布建模需求发生，Log-Normal分布建模需求大小，使用分层先验实现跨项目的部分池化，在稀疏或冷启动序列上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 间歇性需求预测面临稀疏观测、冷启动项目和过时等挑战，传统模型缺乏生成基础，深度学习模型需要大数据集且可解释性差。

Method: 引入TSB-HB，使用Beta-Binomial分布建模需求发生，Log-Normal分布建模非零需求大小，通过分层先验实现跨项目的部分池化。

Result: 在UCI Online Retail数据集上，TSB-HB的RMSE和RMSSE低于Croston、SBA、TSB等方法；在M5数据集子集上优于所有评估的经典基线。

Conclusion: TSB-HB通过结合生成公式和分层收缩，提供校准的概率预测，在间歇性和块状项目上提高准确性，同时保持可解释性和可扩展性。

Abstract: Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability.
  We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing.
  On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable.

</details>


### [225] [Function-on-Function Bayesian Optimization](https://arxiv.org/abs/2511.12783)
*Jingru Huang,Haijie Xu,Manrui Jiang,Chen Zhang*

Main category: stat.ML

TL;DR: 提出了一种函数对函数贝叶斯优化框架，用于优化输入和输出都是函数的复杂系统问题


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未能解决输入和输出都是函数的问题，这在先进传感技术的复杂系统中日益常见

Method: 引入函数对函数高斯过程模型，使用可分离算子值核；基于此定义标量上置信界获取函数，并开发可扩展的函数梯度上升算法

Result: 在合成和真实世界数据上的广泛实验表明，FFBO优于现有方法

Conclusion: FFBO框架成功解决了函数对函数优化问题，在理论和实验上都表现出优越性能

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.

</details>


### [226] [Benign Overfitting in Linear Classifiers with a Bias Term](https://arxiv.org/abs/2511.12840)
*Yuta Kondo*

Main category: stat.ML

TL;DR: 本文扩展了Hashimoto等人关于线性分类中良性过拟合的研究，从无偏置项的同质模型扩展到包含偏置项的非同质模型，证明了在这种更现实的设置下良性过拟合依然存在，并揭示了偏置项对泛化条件的新约束。


<details>
  <summary>Details</summary>
Motivation: 现有关于良性过拟合的理论分析仅限于无偏置项的同质模型，而实践中模型通常包含偏置项。本文旨在研究在更现实的包含偏置项的非同质模型中，良性过拟合现象是否依然存在，以及偏置项如何影响泛化条件。

Method: 通过理论分析扩展Hashimoto等人的框架，研究包含偏置项的非同质线性分类模型，分析数据协方差结构对泛化的约束条件，特别关注标签噪声存在时的情况。

Result: 证明了在包含偏置项的非同质模型中良性过拟合依然存在，但偏置项的引入对数据协方差结构提出了新的泛化约束条件，这些约束在标签噪声存在时尤为明显。在各项同性情况下，这些新约束被同质模型继承的要求所主导。

Conclusion: 偏置项对良性过拟合的泛化条件具有非平凡影响，为理解这一现象提供了更完整的理论图景，揭示了在更现实模型设置下维持良好泛化性能所需的数据结构条件。

Abstract: Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of "homogeneous" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization.

</details>


### [227] [Reconstruction of Manifold Distances from Noisy Observations](https://arxiv.org/abs/2511.13025)
*Charles Fefferman,Jonathan Marty,Kevin Ren*

Main category: stat.ML

TL;DR: 提出了一个从噪声成对距离观测中重建流形内在几何的新框架，改进了先前需要已知矩的加性噪声假设的限制，能够在有界曲率和正内射半径的温和几何假设下，以O(ε log ε⁻¹)的加性误差恢复样本点间的真实距离。


<details>
  <summary>Details</summary>
Motivation: 从噪声距离观测中重建流形的内在几何是一个重要问题，先前工作假设噪声为已知矩的独立同分布加性噪声，这在实践中限制性过强。本文旨在开发更通用的框架，放宽对噪声分布的假设。

Method: 基于估计期望函数f_x(y)=𝔼d'(x,y)的L₂范数的新方法构建鲁棒聚类，通过几何论证证明这些聚类能够恢复样本点间的真实距离。开发了两种算法：第一种达到样本复杂度N≍ε⁻²ᵈ⁻²log(1/ε)和o(N³)运行时间；第二种引入新颖的几何思想。

Result: 在温和几何假设（有界曲率和正内射半径）下，能够以O(ε log ε⁻¹)的加性误差恢复样本点间的真实距离。在存在缺失观测的情况下，只要采样概率满足定量下界，就能扩展所有恢复保证。

Conclusion: 该框架显著改进了从噪声距离观测中重建流形几何的方法，放宽了对噪声分布的假设。主要技术结果阐明了流形的哪些性质对距离恢复是必要的，表明技术可扩展到更广泛的度量概率空间。

Abstract: We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $μ$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\dots,X_N$ are i.i.d. samples of $μ$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\varepsilon \log \varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \asymp \varepsilon^{-2d-2}\log(1/\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces.

</details>


### [228] [Likelihood-guided Regularization in Attention Based Models](https://arxiv.org/abs/2511.13221)
*Mohamed Salem,Inyoung Kim*

Main category: stat.ML

TL;DR: 提出了一种基于变分伊辛正则化的Vision Transformer框架，通过贝叶斯稀疏化技术增强模型泛化能力并动态剪枝冗余参数。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在分类任务中表现优异，但需要大量训练数据和精心正则化来防止过拟合。传统dropout方法采用固定稀疏模式，缺乏任务适应性。

Method: 使用变分伊辛正则化框架，结合贝叶斯稀疏化技术对模型权重施加结构化稀疏性，在训练过程中进行自适应架构搜索。

Result: 在MNIST、Fashion-MNIST、CIFAR-10和CIFAR-100等基准数据集上验证了方法的有效性，提高了稀疏复杂数据下的泛化能力，并实现了权重和选择参数的不确定性量化。

Conclusion: 结构化贝叶斯稀疏化能有效增强基于Transformer的架构，为传统正则化技术提供了理论替代方案，实现了更好的概率校准和结构化特征选择。

Abstract: The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [229] [Cost Transparency of Enterprise AI Adoption](https://arxiv.org/abs/2511.11761)
*Soogand Alavi,Salar Nozari,Andrea Luangrath*

Main category: cs.CY

TL;DR: 研究发现，提示语的礼貌程度会影响LLM输出token数量，非礼貌提示会显著增加输出token，导致企业成本上升，这暴露了当前LLM定价模式的不透明性。


<details>
  <summary>Details</summary>
Motivation: 随着企业快速采用LLM服务，其成本问题被忽视。与传统软件不同，LLM按token收费，企业能控制输入但无法控制输出token数量，这导致成本不可预测。

Method: 通过OpenAI API进行实验，研究不同礼貌程度的提示语如何影响输出token数量，同时保持回答质量不变。

Result: 非礼貌提示会显著增加输出token数量，导致企业成本上升，为服务商带来额外收入。礼貌只是语言结构影响成本的一个例子。

Conclusion: 当前LLM定价模式缺乏透明度，企业难以预算，需要新的方法来确保LLM服务的可预测和透明采用。

Abstract: Recent advances in large language models (LLMs) have dramatically improved performance on a wide range of tasks, driving rapid enterprise adoption. Yet, the cost of adopting these AI services is understudied. Unlike traditional software licensing in which costs are predictable before usage, commercial LLM services charge per token of input text in addition to generated output tokens. Crucially, while firms can control the input, they have limited control over output tokens, which are effectively set by generation dynamics outside of business control. This research shows that subtle shifts in linguistic style can systematically alter the number of output tokens without impacting response quality. Using an experiment with OpenAI's API, this study reveals that non-polite prompts significantly increase output tokens leading to higher enterprise costs and additional revenue for OpenAI. Politeness is merely one instance of a broader phenomenon in which linguistic structure can drive unpredictable cost variation. For enterprises integrating LLM into applications, this unpredictability complicates budgeting and undermines transparency in business-to-business contexts. By demonstrating how end-user behavior links to enterprise costs through output token counts, this work highlights the opacity of current pricing models and calls for new approaches to ensure predictable and transparent adoption of LLM services.

</details>


### [230] [Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)](https://arxiv.org/abs/2511.11590)
*Robert Gigiu*

Main category: cs.CY

TL;DR: 本文提出了一个可解释性赋能临床安全框架(ECSF)，将可解释性集成到DCB0129/0160生命周期中，使临床安全官员能够使用可解释性输出作为结构化安全证据，而无需改变合规路径。


<details>
  <summary>Details</summary>
Motivation: AI在NHS工作流程中日益普及，但其概率性和适应性行为与现有临床安全标准的确定性假设相冲突。现有标准DCB0129和DCB0160未定义如何证明AI特定的透明度、可解释性或模型漂移。

Method: 通过跨监管合成，将DCB条款与良好机器学习实践、NHS AI保证和T.E.S.T.框架以及欧盟AI法案的原则进行映射。创建了一个连接监管条款、原则、ECSF检查点和合适可解释性输出的矩阵。

Result: ECSF引入了五个检查点：全局透明度用于危险识别、案例级可解释性用于验证、临床医生可用性用于评估、可追溯决策路径用于风险控制、纵向可解释性监测用于上市后监督。

Conclusion: ECSF将可解释性重新定义为临床安全保证的核心要素，弥合了确定性风险治理与AI概率行为之间的差距，并支持与GMLP、欧盟AI法案和NHS AI保证原则的一致性。

Abstract: Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.

</details>


### [231] [Decision-Making Amid Information-Based Threats in Sociotechnical Systems: A Review](https://arxiv.org/abs/2511.11595)
*Aaron R. Allred,Erin E. Richardson,Sarah R. Bostrom,James Crum,Cara Spencer,Chad Tossell,Richard E. Niemeyer,Leanne Hirshfield,Allison P. A. Hayman*

Main category: cs.CY

TL;DR: 本文综述了技术系统中信息传播对人类决策的影响，整合了信息威胁研究和人类信息处理研究，识别了共同的认知机制，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 技术系统日益介入人类信息交换，扩大了信息影响的范围，既可能促进也可能破坏决策质量。当前研究碎片化，信息威胁评估与人类信息处理基础研究相互隔离。

Method: 通过综述整合信息威胁研究和人类信息处理研究两个领域的见解，识别共享的认知机制。

Result: 确定了介导信息威胁脆弱性和影响行为结果的共同认知机制。

Conclusion: 需要整合这两个研究视角来减轻人类脆弱性并协调人机表征，为未来研究指明了方向。

Abstract: Technological systems increasingly mediate human information exchange, spanning interactions among humans as well as between humans and artificial agents. The unprecedented scale and reliance on information disseminated through these systems substantially expand the scope of information-based influence that can both enable and undermine sound decision-making. Consequently, understanding and protecting decision-making today faces growing challenges, as individuals and organizations must navigate evolving opportunities and information-based threats across varied domains and information environments. While these risks are widely recognized, research remains fragmented: work evaluating information-based threat phenomena has progressed largely in isolation from foundational studies of human information processing. In this review, we synthesize insights from both domains to identify shared cognitive mechanisms that mediate vulnerability to information-based threats and shape behavioral outcomes. Finally, we outline directions for future research aimed at integrating these perspectives, emphasizing the importance of such integration for mitigating human vulnerabilities and aligning human-machine representations.

</details>


### [232] [EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation](https://arxiv.org/abs/2511.11635)
*Rui Jia,Min Zhang,Fengrui Liu,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.CY

TL;DR: 提出了EduAgentQG，一个多智能体协作框架，用于生成高质量、多样化的个性化问题，通过五个专门智能体的迭代反馈循环提升问题质量、多样性和教育目标一致性。


<details>
  <summary>Details</summary>
Motivation: 手动设计问题耗时且难以满足多样化学习需求，现有自动问题生成方法质量不稳定、多样性有限、与教育目标对齐不足，需要更有效的自动化解决方案。

Method: 包含五个专门智能体的协作框架：Planner生成结构化设计计划和问题方向，Writer基于计划生成候选问题，Solver和Educator进行多维度二元评分，Checker进行最终验证，通过迭代反馈循环优化问题质量。

Result: 在两个数学问题数据集上的实验表明，EduAgentQG在问题多样性、目标一致性和整体质量方面优于现有的单智能体和多智能体方法。

Conclusion: 多智能体协作和迭代反馈机制能够有效生成高质量、多样化且与教育目标一致的问题，为自适应学习和个性化评估提供有力支持。

Abstract: High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.

</details>


### [233] [Automatic generation of DRI Statements](https://arxiv.org/abs/2511.11655)
*Maurice Flechtner*

Main category: cs.CY

TL;DR: 提出了一种自动化生成审议理由指数(DRI)陈述的方法，利用NLP和LLM技术显著减少了人工工作量


<details>
  <summary>Details</summary>
Motivation: DRI是评估群体审议质量的重要指标，但其陈述生成过程复杂耗时，限制了实际应用

Method: 开发了基于先进自然语言处理和大语言模型的自动化DRI陈述生成框架

Result: 大幅降低了进行综合审议过程评估的门槛，为生成式AI融入社会科学研究方法提供了可复制的模板

Conclusion: 自动化DRI陈述生成方法能够显著提高评估效率，推动审议过程研究的可扩展性

Abstract: Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.

</details>


### [234] [Generative AI as a Linguistic Equalizer in Global Science](https://arxiv.org/abs/2511.11687)
*Dragan Filimonovic,Christian Rutzer,Jeffrey Macher,Rolf Weder*

Main category: cs.CY

TL;DR: 生成式AI正在成为全球科学中的语言均衡器，帮助非英语国家作者缩小与英语母语作者在科学写作风格上的差距。


<details>
  <summary>Details</summary>
Motivation: 英语在科学领域的长期主导地位为非母语者设置了障碍，生成式AI可能为解决这一不平等问题提供技术方案。

Method: 分析2021-2024年间的565万篇科学论文，使用SciBERT文本嵌入模型测量非英语国家作者在GenAI辅助下与美英作者科学写作风格的相似度变化。

Result: ChatGPT发布后，GenAI辅助的论文与美英作者写作风格趋同度显著增加，且语言距离英语越远的国家团队效果越明显。

Conclusion: 生成式AI正在通过降低语言障碍来重塑全球科学交流格局，为非英语国家研究者提供公平竞争环境。

Abstract: For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.

</details>


### [235] [Mental Health Generative AI is Safe, Promotes Social Health, and Reduces Depression and Anxiety: Real World Evidence from a Naturalistic Cohort](https://arxiv.org/abs/2511.11689)
*Thomas D. Hull,Lizhe Zhang,Patricia A. Arean,Matteo Malgaroli*

Main category: cs.CY

TL;DR: 评估用于心理健康的基础模型AI聊天机器人，在自然观察研究中显示能提供安全、有效且可扩展的心理健康支持


<details>
  <summary>Details</summary>
Motivation: 生成式AI心理健康聊天机器人可以提供安全、个性化且可扩展的心理健康支持，需要评估其实际效果

Method: 单臂自然观察研究，成年用户与AI聊天机器人互动，每两周重复测量心理健康指标，持续6周，最终随访10周

Result: 用户PHQ-9和GAD-7评分显著降低并持续到随访期，希望、行为激活、社交互动、孤独感和感知社会支持显著改善，参与度高且预测结果，安全护栏有效运行

Conclusion: 心理健康GAI基础模型能够提供可访问、有吸引力、有效且安全的心理健康支持，为现实世界环境中的进一步研究提供了希望

Abstract: Generative artificial intelligence (GAI) chatbots built for mental health could deliver safe, personalized, and scalable mental health support. We evaluate a foundation model designed for mental health. Adults completed mental health measures while engaging with the chatbot between May 15, 2025 and September 15, 2025. Users completed an opt-in consent, demographic information, mental health symptoms, social connection, and self-identified goals. Measures were repeated every two weeks up to 6 weeks, and a final follow-up at 10 weeks. Analyses included effect sizes, and growth mixture models to identify participant groups and their characteristic engagement, severity, and demographic factors. Users demonstrated significant reductions in PHQ-9 and GAD-7 that were sustained at follow-up. Significant improvements in Hope, Behavioral Activation, Social Interaction, Loneliness, and Perceived Social Support were observed throughout and maintained at 10 week follow-up. Engagement was high and predicted outcomes. Working alliance was comparable to traditional care and predicted outcomes. Automated safety guardrails functioned as designed, with 76 sessions flagged for risk and all handled according to escalation policies. This single arm naturalistic observational study provides initial evidence that a GAI foundation model for mental health can deliver accessible, engaging, effective, and safe mental health support. These results lend support to findings from early randomized designs and offer promise for future study of mental health GAI in real world settings.

</details>


### [236] [Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets](https://arxiv.org/abs/2511.11713)
*Yunkai Yu,Yingying Wang,Rong Zheng*

Main category: cs.CY

TL;DR: 该研究调查了41个公开运动捕捉数据集，发现老年人参与度低，且老年风格行走动作未能真实反映衰老特征，提出了量化评估老年风格行走动作保真度的方法。


<details>
  <summary>Details</summary>
Motivation: 现有运动捕捉数据集中老年人代表性不足，且老年风格行走动作的保真度未得到充分评估，这在医疗健康应用中尤为重要。

Method: 调查41个公开数据集，识别包含老年人和老年风格动作的数据集；引入量化指标评估老年风格行走动作的保真度，使用对年龄敏感、抗噪声且适应数据稀缺的步态参数。

Result: 老年人仅占总体参与者的小部分，提供老年人全身运动数据的数据集很少；老年风格行走动作常表现出过度控制的模式，未能真实表征衰老特征。

Conclusion: 需要改进运动数据集中老年人的代表性，并建立了量化评估老年风格行走动作质量的方法。

Abstract: The Internet of Things (IoT) sensors have been widely employed to capture human locomotions to enable applications such as activity recognition, human pose estimation, and fall detection. Motion capture (MoCap) systems are frequently used to generate ground truth annotations for human poses when training models with data from wearable or ambient sensors, and have been shown to be effective to synthesize data in these modalities. However, the representation of older adults, an increasingly important demographic in healthcare, in existing MoCap locomotion datasets has not been thoroughly examined. This work surveyed 41 publicly available datasets, identifying eight that include older adult motions and four that contain motions performed by younger actors annotated as old style. Older adults represent a small portion of participants overall, and few datasets provide full-body motion data for this group. To assess the fidelity of old-style walking motions, quantitative metrics are introduced, defining high fidelity as the ability to capture age-related differences relative to normative walking. Using gait parameters that are age-sensitive, robust to noise, and resilient to data scarcity, we found that old-style walking motions often exhibit overly controlled patterns and fail to faithfully characterize aging. These findings highlight the need for improved representation of older adults in motion datasets and establish a method to quantitatively evaluate the quality of old-style walking motions.

</details>


### [237] [CADD: A Chinese Traffic Accident Dataset for Statute-Based Liability Attribution](https://arxiv.org/abs/2511.11715)
*Yunfei Shen,Zhongcheng Wu*

Main category: cs.CY

TL;DR: CADD是中国首个基于法规的责任认定数据集，包含792个真实驾驶记录仪视频，通过"行为-责任-法规"框架标注，将感知数据与法律后果直接关联。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集专注于检测和定位，缺乏法律推理所需的注释。随着自动驾驶技术发展，事故责任认定成为关键挑战。

Method: 构建包含792个真实驾驶记录仪视频的CADD数据集，采用新颖的"行为-责任-法规"标注流程，提供细粒度对称行为注释、明确责任分配，并将每个案例与违反的中国交通法规条款关联。

Result: 建立了责任预测和可解释决策的基准，通过详细分析证明了CADD的实用性。

Conclusion: CADD通过直接连接感知数据与法律后果，为开发负责任且法律依据充分的自动驾驶系统提供了基础资源。

Abstract: As autonomous driving technology advances, the critical challenge evolves beyond collision avoidance to the \textbf{adjudication of liability} when accidents occur. Existing datasets, focused on detection and localization, lack the annotations required for this legal reasoning. To bridge this gap, we introduce the \textbf{C}hinese \textbf{A}ccident \textbf{D}uty-determination \textbf{D}ataset (\textbf{CADD}), the first benchmark for statute-based liability attribution. CADD contains 792 real-world driving recorder videos, each annotated within a novel \textbf{``Behavior--Liability--Statute''} pipeline. This framework provides \textbf{granular, symmetric behavior annotations}, clear responsibility assignments, and, uniquely, links each case to the specific \textbf{Chinese traffic law statute} violated. We demonstrate the utility of CADD through detailed analysis and establish benchmarks for liability prediction and explainable decision-making. By directly connecting perceptual data to legal consequences, CADD provides a foundational resource for developing accountable and legally-grounded autonomous systems.

</details>


### [238] [Weapons of Online Harassment: Menacing and Profiling Users via Social Apps](https://arxiv.org/abs/2511.11718)
*Sanjana Cheerla,Vaibhav Garg,Saikath Bhattacharya,Munindar P. Singh*

Main category: cs.CY

TL;DR: 通过分析300万条应用评论，开发了识别骚扰行为的计算模型，发现两种主要骚扰形式：威胁和画像，并识别出1395个存在骚扰问题的应用。


<details>
  <summary>Details</summary>
Motivation: 社交应用作为社会技术系统可能无意中助长有害行为如网络骚扰，随着用户增加，骚扰事件也在上升。应用评论中经常描述骚扰行为，这为研究提供了数据基础。

Method: 构建包含300万条评论和1800个应用的数据集，开发计算模型识别表明骚扰的评论，分析骚扰的地形特征。

Result: 模型对威胁和画像骚扰的召回率分别达到90%和85%。发现骚扰者多为女性身份，负面评论与中性评论的区别在于愤怒、厌恶和恐惧情绪更普遍。识别出1395个存在骚扰问题的应用。

Conclusion: 社交应用确实存在严重的骚扰问题，通过评论分析可以有效识别和量化这些问题，为开发者提供改进依据。

Abstract: Viewing social apps as sociotechnical systems makes clear that they are not mere pieces of technology but mediate human interaction and may unintentionally enable harmful behaviors like online harassment. As more users interact through social apps, instances of harassment increase.
  We observed that app reviews often describe harassment. Accordingly, we built a dataset of over 3 million reviews and 1,800 apps. We discovered that two forms of harassment are prevalent, Menacing and Profiling.
  We built a computational model for identifying reviews indicating harassment, achieving high recalls of 90% for Menacing and 85% for Profiling. We analyzed the data further to better understand the terrain of harassment. Surprisingly, abusers most often have female identities. Also, what distinguishes negative from neutral reviews is the greater prevalence of anger, disgust, and fear.
  Applying our model, we identified 1,395 apps enabling harassment and notified developers of the top 48 with the highest user-reported harassment.

</details>


### [239] [A framework for measuring and analyzing customer satisfaction at computer service companies using Lean Six Sigma](https://arxiv.org/abs/2511.11723)
*Mohammed Abboodi*

Main category: cs.CY

TL;DR: 本研究提出将六西格玛与SERVQUAL工具整合的框架，用于评估计算机服务行业的服务质量，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 计算机服务行业竞争激烈，中小企业难以维持客户满意度，缺乏有效的服务质量评估系统是导致客户流失的关键因素。

Method: 将六西格玛的核心原则与SERVQUAL工具在DMAIC方法论中整合，通过案例研究在真实环境中验证该集成框架。

Result: 案例研究量化了满意度水平，识别出五个主要的不满意原因，揭示了较低的总体满意度水平。

Conclusion: 解决识别出的不满意原因预计能提高客户满意度、降低客户获取成本并改善组织整体绩效。

Abstract: The computer service industry has expanded rapidly over the past two decades, driven by the proliferation of computing technologies, the entry of large firms, and the availability of online diagnostic and troubleshooting tools. In this increasingly competitive environment, many small and medium sized enterprises struggle to maintain customer satisfaction as rivals deliver higher quality services at lower cost. This study addresses the absence of robust measurement systems for assessing service quality, a key factor underlying customer attrition, by proposing an integrated framework for evaluating satisfaction and identifying sources of dissatisfaction in computer services.
  The framework combines core principles of Six Sigma with the SERVQUAL instrument within a structured DMAIC methodology (Define, Measure, Analyze, Improve, and Control). SERVQUAL provides the service quality dimensions and gap analysis techniques, while Six Sigma supplies the data driven approach to measurement and improvement. The literature suggests limited prior work integrating Lean Six Sigma with SERVQUAL, and this study contributes by operationalizing that integration in a real world setting.
  A case study of a computer services company was conducted to demonstrate feasibility and effectiveness. Satisfaction levels were quantified, and root causes of dissatisfaction were identified. The analysis revealed a low overall satisfaction level and five primary drivers of unmet customer requirements. Addressing these causes is expected to increase customer satisfaction, lower customer acquisition costs, and improve overall organizational performance.

</details>


### [240] [On the Influence of Artificial Intelligence on Human Problem-Solving: Empirical Insights for the Third Wave in a Multinational Longitudinal Pilot Study](https://arxiv.org/abs/2511.11738)
*Matthias Huemmer,Theophile Shyiramunda,Franziska Durner,Michelle J. Cummings-Koether*

Main category: cs.CY

TL;DR: 该研究揭示了人机协作中存在的系统性认知差距：信念-表现差距（感知正确性与实际正确性相差高达80.8个百分点）和证明-信念差距（信心与验证能力相差-16.8个百分点），表明AI辅助工作的关键限制在于解决方案验证而非生成。


<details>
  <summary>Details</summary>
Motivation: 研究人机协作在问题解决中的演变模式，特别关注AI工具在结构化认知工作流程中的战略整合，以及识别影响可靠AI辅助工作的关键约束因素。

Method: 采用跨国纵向研究第三波（n=23参与者），通过行为数据和不同复杂度的问题情景，分析AI采用模式和认知工作流程，量化系统性认知差距。

Result: 发现普遍AI采用（95.7%有先验知识，100%使用ChatGPT），主要采用"思考、互联网、ChatGPT、进一步处理"的人类主导序列（39.1%），但存在随问题复杂度增加的验证缺陷。

Conclusion: 教育和科技干预必须优先考虑验证支架（包括假设文档协议、充分性标准清单和三角验证程序），以强化人类在这个新认知生态系统中作为关键验证者的角色。

Abstract: This article presents the results and their discussion for the third wave (with n=23 participants) within a multinational longitudinal study that investigates the evolving paradigm of human-AI collaboration in problem-solving contexts. Building upon previous waves, our findings reveal the consolidation of a hybrid problem-solving culture characterized by strategic integration of AI tools within structured cognitive workflows. The data demonstrate near-universal AI adoption (95.7% with prior knowledge, 100% ChatGPT usage) primarily deployed through human-led sequences such as "Think, Internet, ChatGPT, Further Processing" (39.1%). However, this collaboration reveals a critical verification deficit that escalates with problem complexity. We empirically identify and quantify two systematic epistemic gaps: a belief-performance gap (up to +80.8 percentage points discrepancy between perceived and actual correctness) and a proof-belief gap (up to -16.8 percentage points between confidence and verification capability). These findings, derived from behavioral data and problem vignettes across complexity levels, indicate that the fundamental constraint on reliable AI-assisted work is solution validation rather than generation. The study concludes that educational and technological interventions must prioritize verification scaffolds (including assumption documentation protocols, adequacy criteria checklists, and triangulation procedures) to fortify the human role as critical validator in this new cognitive ecosystem.

</details>


### [241] [Taxation and the relationship between payments and time spent](https://arxiv.org/abs/2511.11741)
*Christopher Mantzaris,Ajda Fosner*

Main category: cs.CY

TL;DR: 研究发现纳税合规时间与纳税金额之间存在正相关关系，表明简化税收流程可以降低社会税收管理成本


<details>
  <summary>Details</summary>
Motivation: 税收工作对社会成本高昂，纳税人花费大量时间处理行政性税务工作，无法专注于财富创造。理解税收合规时间与纳税金额的关系有助于降低税收管理成本

Method: 使用PwC和世界银行2019年税收数据，分析纳税合规时间(X)与纳税金额(Y)的关系。通过6组测试验证正相关关系，每组测试需满足5个要求：正斜率、满足p值和r值、高互信息、散点图支持。测试包括原始数据、去除城市数据、去除城市和异常值数据

Result: 所有6组测试均满足5个要求，表明纳税合规时间与纳税金额存在正相关关系。总纳税金额的相关性更强。4个验证性测试确认了方法的有效性

Conclusion: 通过简化税收流程（包括税收征收和支付），可以减少纳税人花费在税务上的时间，从而降低社会的整体税收管理成本

Abstract: Tax work is costly for society: Administrative tax labour is typically to a high degree shuffled off the government and onto every taxpayer by law. The higher the burden of any tax system, the costlier for society, as taxpayers are unable to engage in proper wealth creation when being kept busy with administrative tax work. This research finds evidence for a relationship between hours spent to comply with taxes and amount of tax payment. These findings help better understand tax administrative costs and ultimately may help reduce them. PwC and World Bank's final "Paying taxes"-publication (2019) contains tax data for most of the world's jurisdictions, in particular annual hours spent to comply with tax obligations (X) and annual amount of tax payments (Y), both for the year 2019. X and Y were plotted in 6 tests. A positive slope, satisfying p and r values, high mutual information and finally a conclusive scatter plot picture were the 5 requirements that all needed to be met to confirm a positive relationship between X and Y. The first 2 tests did not make any adjustments to the data, the next 2 tests removed cities --thereby avoiding the double counting of jurisdictions-- and the final 2 tests removed cities and outliers. Each test pair uses for Y first total number of payments; and for each second test the number of other payments, which excludes income tax payments for profit and labour. All 5 requirements were met in every of the 6 tests, indicating a positive dependence. In addition, 4 confirmatory tests validate the methodology. The found relationship is noticeably stronger for the total number of tax payments. Findings indicate that taxpayers' time spent on tax, and thereby society's overall tax administrative costs, could be reduced by simplifying taxation processes, including tax collection and payments.

</details>


### [242] [Brazil Data Commons: A Platform for Unifying and Integrating Brazil's Public Data](https://arxiv.org/abs/2511.11755)
*Isadora Cristina,Ramon Gonze,Jônatas Santos,Julio Reis,Mário Alvim,Bernardo Queiroz,Fabrício Benevenuto*

Main category: cs.CY

TL;DR: 巴西数据共享平台通过统一语义框架整合分散的巴西数据集，解决数据碎片化和互操作性问题，促进研究和政策制定。


<details>
  <summary>Details</summary>
Motivation: 巴西公共数据分散、标准不一致且互操作性有限，阻碍了有效研究、循证决策和数据驱动洞察。

Method: 采用全球公认的本体论和互操作数据标准，在统一语义框架下整合各种巴西数据集，提供用户友好界面和灵活数据访问选项。

Result: 平台将分散的数据集转化为集成且易于导航的资源，使研究人员、政策制定者和公众能够获得有意义的洞察并做出明智决策。

Conclusion: 巴西数据共享平台通过统一语义框架成功解决了数据碎片化问题，为理解巴西复杂的社会、经济和环境景观提供了集成资源。

Abstract: The fragmentation of public data in Brazil, coupled with inconsistent standards and limited interoperability, hinders effective research, evidence-based policymaking and access to data-driven insights. To address these issues, we introduce Brazil Data Commons, a platform that unifies various Brazilian datasets under a common semantic framework, enabling the seamless discovery, integration and visualization of information from different domains. By adopting globally recognized ontologies and interoperable data standards, Brazil Data Commons aligns with the principles of the broader Data Commons ecosystem and places Brazilian data in a global context. Through user-friendly interfaces, straightforward query mechanisms and flexible data access options, the platform democratizes data use and enables researchers, policy makers, and the public to gain meaningful insights and make informed decisions. This paper illustrates how Brazil Data Commons transforms scattered datasets into an integrated and easily navigable resource that allows a deeper understanding of Brazil's complex social, economic and environmental landscape.

</details>


### [243] [Bridging the Skills Gap: A Course Model for Modern Generative AI Education](https://arxiv.org/abs/2511.11757)
*Anya Bardach,Hamilton Murrah*

Main category: cs.CY

TL;DR: 论文探讨了生成式AI工具普及对学习环境的影响，开发了一门面向计算机科学学生的生成式AI应用课程，并通过调查验证了课程的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决两个脱节问题：行业重视生成式AI能力而高等教育不重视；学生无正式指导地实验生成式AI。确保学生能负责任地使用AI工具以适应就业市场需求。

Method: 在一所私立研究型大学为计算机科学本科生和研究生开发了生成式AI工具在软件开发中应用的课程，采用混合方法调查评估课程效果。

Result: 调查显示学生普遍认为课程有价值且有效，课程成功填补了生成式AI应用教育的空白。

Conclusion: 计算机科学及其他院系应复制此类课程，教授学生负责任地使用现有生成式AI工具，确保就业市场准备度和积极成果。

Abstract: Research on how the popularization of generative Artificial Intelligence (AI) tools impacts learning environments has led to hesitancy among educators to teach these tools in classrooms, creating two observed disconnects. Generative AI competency is increasingly valued in industry but not in higher education, and students are experimenting with generative AI without formal guidance. The authors argue students across fields must be taught to responsibly and expertly harness the potential of AI tools to ensure job market readiness and positive outcomes. Computer Science trajectories are particularly impacted, and while consistently top ranked U.S. Computer Science departments teach the mechanisms and frameworks underlying AI, few appear to offer courses on applications for existing generative AI tools. A course was developed at a private research university to teach undergraduate and graduate Computer Science students applications for generative AI tools in software development. Two mixed method surveys indicated students overwhelmingly found the course valuable and effective. Co-authored by the instructor and one of the graduate students, this paper explores the context, implementation, and impact of the course through data analysis and reflections from both perspectives. It additionally offers recommendations for replication in and beyond Computer Science departments. This is the extended version of this paper to include technical appendices.

</details>


### [244] [Demystify, Use, Reflect: Preparing students to be informed LLM-users](https://arxiv.org/abs/2511.11764)
*Nikitha Donekal Chandrashekar,Sehrish Basir Nizamani,Margaret Ellis,Naren Ramakrishnan*

Main category: cs.CY

TL;DR: 将后CS1课程改造为系统整合大语言模型的教学，培养学生负责任使用AI的能力，包括技术原理、工具使用、伦理问题和实践反思。


<details>
  <summary>Details</summary>
Motivation: 帮助学生发展有意义且负责任地参与AI所需的技能，为AI集成的未来做好准备。

Method: 课程包含LLM工作原理讲解、当前工具接触、伦理问题讨论、学生反思活动，课堂上演示LLM输出使用与验证，指导学生将LLM作为问题解决环节的一部分，并要求学生披露LLM协助情况。

Result: 学生调查显示，学生对LLM工作原理的理解更加技术化，对LLM的验证和使用变得更加敏锐和协作。

Conclusion: 这些策略可用于其他课程，为学生应对AI集成未来做好准备。

Abstract: We transitioned our post-CS1 course that introduces various subfields of computer science so that it integrates Large Language Models (LLMs) in a structured, critical, and practical manner. It aims to help students develop the skills needed to engage meaningfully and responsibly with AI. The course now includes explicit instruction on how LLMs work, exposure to current tools, ethical issues, and activities that encourage student reflection on personal use of LLMs as well as the larger evolving landscape of AI-assisted programming. In class, we demonstrate the use and verification of LLM outputs, guide students in the use of LLMs as an ingredient in a larger problem-solving loop, and require students to disclose and acknowledge the nature and extent of LLM assistance. Throughout the course, we discuss risks and benefits of LLMs across CS subfields. In our first iteration of the course, we collected and analyzed data from students pre and post surveys. Student understanding of how LLMs work became more technical, and their verification and use of LLMs shifted to be more discerning and collaborative. These strategies can be used in other courses to prepare students for the AI-integrated future.

</details>


### [245] [Scaling Equitable Reflection Assessment in Education via Large Language Models and Role-Based Feedback Agents](https://arxiv.org/abs/2511.11772)
*Chenyu Zhang,Xiaohang Luo*

Main category: cs.CY

TL;DR: 提出一个基于多智能体LLM的系统，用于大规模提供公平的形成性反馈，通过五个协调的角色智能体来评分和生成学习者反馈，在AI素养课程中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 形成性反馈是学生学习的重要驱动力，但在大规模或资源有限的课程中难以公平实施，教师缺乏时间和资源来审阅所有学生反思，导致支持缺口。

Method: 使用五个协调的基于角色的LLM智能体（评估者、公平监控者、元认知教练、聚合者和反思审阅者），通过共享评分标准对学习者反思进行评分，并生成简短、无偏见的学习者反馈。

Result: 在12节AI素养课程中，系统产生的评分标准得分接近专家水平一致性，训练有素的评分者认为AI生成的评论有帮助、有同理心且与教学目标一致。

Conclusion: 多智能体LLM系统能够以人类评分者无法达到的规模和速度提供公平、高质量的形成性反馈，为实现任何课程规模和情境下的反馈丰富学习铺平道路。

Abstract: Formative feedback is widely recognized as one of the most effective drivers of student learning, yet it remains difficult to implement equitably at scale. In large or low-resource courses, instructors often lack the time, staffing, and bandwidth required to review and respond to every student reflection, creating gaps in support precisely where learners would benefit most. This paper presents a theory-grounded system that uses five coordinated role-based LLM agents (Evaluator, Equity Monitor, Metacognitive Coach, Aggregator, and Reflexion Reviewer) to score learner reflections with a shared rubric and to generate short, bias-aware, learner-facing comments. The agents first produce structured rubric scores, then check for potentially biased or exclusionary language, add metacognitive prompts that invite students to think about their own thinking, and finally compose a concise feedback message of at most 120 words. The system includes simple fairness checks that compare scoring error across lower and higher scoring learners, enabling instructors to monitor and bound disparities in accuracy. We evaluate the pipeline in a 12-session AI literacy program with adult learners. In this setting, the system produces rubric scores that approach expert-level agreement, and trained graders rate the AI-generated comments as helpful, empathetic, and well aligned with instructional goals. Taken together, these results show that multi-agent LLM systems can deliver equitable, high-quality formative feedback at a scale and speed that would be impossible for human graders alone. More broadly, the work points toward a future where feedback-rich learning becomes feasible for any course size or context, advancing long-standing goals of equity, access, and instructional capacity in education.

</details>


### [246] [Data-driven strategic sensor placement for detecting disinfection by-products in water distribution networks](https://arxiv.org/abs/2511.11775)
*Aristotelis Magklis,Andreas Kamilaris*

Main category: cs.CY

TL;DR: DBPFinder是一个模拟软件，用于优化消毒副产物检测传感器的战略布局，已在葡萄牙科英布拉的真实供水网络中测试验证。


<details>
  <summary>Details</summary>
Motivation: 消毒副产物是氯化饮用水中的有害污染物，其形成受多种环境参数影响，难以在进入家庭前监测。由于消毒副产物种类繁多且传感器部署数量有限，需要高效优化传感器布局。

Method: 开发DBPFinder模拟软件，通过多目标优化方法为水务运营商提供基于需求的传感器最优布局建议。

Result: 在真实供水网络中的多项实验表明该软件具有正确性、相关性、高效性和可扩展性。

Conclusion: DBPFinder能够有效协助水务运营商优化消毒副产物检测传感器的战略布局，提高监测效率。

Abstract: Disinfection byproducts are contaminants that can cause long-term effects on human health, occurring in chlorinated drinking water when the disinfectant interacts with natural organic matter. Their formation is affected by many environmental parameters, making it difficult to monitor and detect disinfection byproducts before they reach households. Due to the large variety of disinfection byproduct compounds that can be formed in water distribution networks, plus the constrained number of sensors that can be deployed throughout a system to monitor these contaminants, it is of outmost importance to place sensory equipment efficiently and optimally. In this paper, we present DBPFinder, a simulation software that assists in the strategic sensor placement for detecting disinfection byproducts, tested at a real-world water distribution network in Coimbra, Portugal. This simulator addresses multiple performance objectives at once in order to provide optimal solution placement recommendations to water utility operators based on their needs. A number of different experiments performed indicate its correctness, relevance, efficiency and scalability.

</details>


### [247] [Differences in the Moral Foundations of Large Language Models](https://arxiv.org/abs/2511.11790)
*Peter Kirgis*

Main category: cs.CY

TL;DR: 使用道德基础理论分析大型语言模型的伦理判断偏差，发现不同模型间及与人类基准存在显著差异，且随模型能力增强而扩大


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在政治、商业和教育等关键领域应用日益广泛，但其规范性伦理判断本质仍不透明，需要从道德心理学角度评估模型对齐

Method: 基于Jonathan Haidt的道德基础理论，对主要模型提供商的大范围模型进行合成实验，使用多种统计方法分析模型响应与人类基准的偏差和方差

Result: 模型间依赖不同的道德基础，与全国代表性人类基准存在显著差异，且这些差异随模型能力增强而增加

Conclusion: 需要进一步使用道德基础理论分析LLMs，包括微调开源模型，并促使政策制定者更深入思考道德基础在模型对齐中的重要性

Abstract: Large language models are increasingly being used in critical domains of politics, business, and education, but the nature of their normative ethical judgment remains opaque. Alignment research has, to date, not sufficiently utilized perspectives and insights from the field of moral psychology to inform training and evaluation of frontier models. I perform a synthetic experiment on a wide range of models from most major model providers using Jonathan Haidt's influential moral foundations theory (MFT) to elicit diverse value judgments from LLMs. Using multiple descriptive statistical approaches, I document the bias and variance of large language model responses relative to a human baseline in the original survey. My results suggest that models rely on different moral foundations from one another and from a nationally representative human baseline, and these differences increase as model capabilities increase. This work seeks to spur further analysis of LLMs using MFT, including finetuning of open-source models, and greater deliberation by policymakers on the importance of moral foundations for LLM alignment.

</details>


### [248] [A Leakage-Aware Data Layer For Student Analytics: The Capire Framework For Multilevel Trajectory Modeling](https://arxiv.org/abs/2511.11866)
*H. R. Paz*

Main category: cs.CY

TL;DR: 提出了一个防数据泄漏的学生轨迹分析数据层，通过定义观察时间价值(VOT)来严格分离观察窗口和结果预测期，并在工程专业中发现了13种稳健的学生轨迹原型。


<details>
  <summary>Details</summary>
Motivation: 现有的学生辍学预测模型通常依赖机会性特征集并存在未记录的数据泄漏问题，限制了模型的解释能力和机构实用性。

Method: 提出了CAPIRE框架的多级建模方法，将预测因子组织为四个层次(N1-N4)，并正式定义了观察时间价值(VOT)作为关键设计参数来防止数据泄漏。使用UMAP + DBSCAN流水线进行原型发现。

Result: 在长周期工程项目(1,343名学生，约57%辍学率)的应用中，发现了13种轨迹原型，包括'早期结构性危机'、'持续摩擦'和'隐藏脆弱性'等。自举和置换测试证实这些原型具有统计稳健性和时间稳定性。

Conclusion: 该方法将特征工程从技术步骤转变为核心方法工件，为保留理论、早期预警系统以及未来在CAPIRE项目中实施因果推理和基于代理的建模提供了有纪律的桥梁。

Abstract: Predictive models for student dropout, while often accurate, frequently rely on opportunistic feature sets and suffer from undocumented data leakage, limiting their explanatory power and institutional usefulness. This paper introduces a leakage-aware data layer for student trajectory analytics, which serves as the methodological foundation for the CAPIRE framework for multilevel modelling. We propose a feature engineering design that organizes predictors into four levels: N1 (personal and socio-economic attributes), N2 (entry moment and academic history), N3 (curricular friction and performance), and N4 (institutional and macro-context variables)As a core component, we formalize the Value of Observation Time (VOT) as a critical design parameter that rigorously separates observation windows from outcome horizons, preventing data leakage by construction. An illustrative application in a long-cycle engineering program (1,343 students, ~57% dropout) demonstrates that VOT-restricted multilevel features support robust archetype discovery. A UMAP + DBSCAN pipeline uncovers 13 trajectory archetypes, including profiles of "early structural crisis," "sustained friction," and "hidden vulnerability" (low friction but high dropout). Bootstrap and permutation tests confirm these archetypes are statistically robust and temporally stable. We argue that this approach transforms feature engineering from a technical step into a central methodological artifact. This data layer serves as a disciplined bridge between retention theory, early-warning systems, and the future implementation of causal inference and agent-based modelling (ABM) within the CAPIRE program.

</details>


### [249] [Educators on the Frontline: Philosophical and Realistic Perspectives on Integrating ChatGPT into the Learning Space](https://arxiv.org/abs/2511.11960)
*Surajit Das,Peu Majumder,Aleksei Eliseev*

Main category: cs.CY

TL;DR: 本研究通过调查140名俄罗斯大学教育工作者，提出了一个包含七个子空间的"学习空间"理论模型，系统分析ChatGPT在教育中的影响。结果显示大多数教育者有条件支持AI整合，但强调评估方法转变和剽窃检测工具的必要性。


<details>
  <summary>Details</summary>
Motivation: 超越对生成式AI在教育领域影响的恐慌和猜测，系统研究大学教育工作者这一关键利益相关者的结构化观点，为AI教育整合提供数据驱动的分析。

Method: 提出"学习空间"理论模型（包含七个子空间），通过定量调查140名俄罗斯大学教育工作者，使用二元标记系统分析关键指标上的接受度。

Result: 发现教育者存在强烈但有条件的共识：大多数支持ChatGPT整合，但前提是评估方法转变和剽窃检测工具可用；对批判性思维影响存在显著担忧；教育者角色从信息传递者转变为批判性参与促进者。

Conclusion: ChatGPT不是教育的破坏者，而是必要演变的催化剂；提出PIPE模型（教学法、基础设施、政策、教育）作为负责任整合的战略框架，为AI教育讨论提供细致入微的替代方案。

Abstract: The rapid emergence of Generative AI, particularly ChatGPT, has sparked a global debate on the future of education, often characterized by alarmism and speculation. Moving beyond this, this study investigates the structured, grounded perspectives of a key stakeholder group: university educators. It proposes a novel theoretical model that conceptualizes the educational environment as a "Learning Space" composed of seven subspaces to systematically identify the impact of AI integration. This framework was operationalized through a quantitative survey of 140 Russian university educators, with responses analyzed using a binary flagging system to measure acceptance across key indicators. The results reveal a strong but conditional consensus: a majority of educators support ChatGPT's integration, contingent upon crucial factors such as the transformation of assessment methods and the availability of plagiarism detection tools. However, significant concerns persist regarding its impact on critical thinking. Educators largely reject the notion that AI diminishes their importance, viewing their role as evolving from information-deliverer to facilitator of critical engagement. The study concludes that ChatGPT acts less as a destroyer of education and more as a catalyst for its necessary evolution, and proposes the PIPE Model (Pedagogy, Infrastructure, Policy, Education) as a strategic framework for its responsible integration. This research provides a data-driven, model-based analysis of educator attitudes, offering a nuanced alternative to the polarized discourse surrounding AI in education.

</details>


### [250] [Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles](https://arxiv.org/abs/2511.12010)
*Palakorn Achananuparp,Connie Xu,Yao Lu,Xavier Jayaraj Siddarth Ashok,Ee-Peng Lim*

Main category: cs.CY

TL;DR: 基于在线简历的大规模职业流动性分析显示，公司内部职位变动对向上流动性的促进作用最强，而女性和黑人大学毕业生从工作变动中获得的回报显著低于男性和白人同行。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨工作变动如何影响大学学历美国工作者的向上职业流动性，以及性别和种族如何影响这种流动性结果。

Method: 使用在线简历数据，通过数据处理和AI方法（包括开发基于大语言模型的FewSOC职业分类方法）解决数据挑战，分析了228,710条职业轨迹。

Result: 公司内部职位变动对向上流动性促进作用最强，其次是跨公司职位变动和跨公司平级调动。女性和黑人毕业生从工作变动中获得的回报显著较低。

Conclusion: 职业流动性存在显著的性别和种族差异，多级敏感性分析证实这些差异对集群异质性具有稳健性，并揭示了额外的交叉性模式。

Abstract: We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.

</details>


### [251] [Impact of UK Postgraduate Student Experiences on Academic Performance in Blended Learning: A Data Analytics Approach](https://arxiv.org/abs/2511.12320)
*Muhidin Mohamed,Shubhadeep Mukherjee,Bhavana Baad*

Main category: cs.CY

TL;DR: 本研究调查了英国大学研究生混合学习环境中学生学习体验不同维度与学业成就之间的关系，发现结合教学存在和社会存在的学习活动以及通过有效反馈提供的定制化学术支持是成功的关键因素。


<details>
  <summary>Details</summary>
Motivation: 混合学习已成为高等教育的主要模式，但学生在混合学习环境中如何构建成功学习面临新挑战。研究旨在探索学生学习体验各维度与学业成就的相互作用。

Method: 采用多种数据分析技术，包括可视化、统计检验、回归分析和潜在剖面分析，基于255名研究生的调查数据，通过探究社区框架进行整体解读。

Result: 实证结果表明，结合教学存在和社会存在的学习活动以及通过有效反馈提供的定制化学术支持是混合学习环境中研究生成功体验的关键要素。研究识别了四种基于不同探究社区存在参与方式的学生档案。

Conclusion: 本研究通过识别人口统计学、体验性和心理因素对学业成果的不同影响方式，推进了对学生成功的理解，并在理论上通过整合学习者异质性概念扩展了探究社区框架。

Abstract: Blended learning has become a dominant educational model in higher education in the UK and worldwide, particularly after the COVID-19 pandemic. This is further enriched with accompanying pedagogical changes, such as strengthened asynchronous learning, and the use of AI (from ChatGPT and all other similar tools that followed) and other technologies to aid learning. While these educational transformations have enabled flexibility in learning and resource access, they have also exposed new challenges on how students can construct successful learning in hybrid learning environments. In this paper, we investigate the interaction between different dimensions of student learning experiences (ranging from perceived acceptance of teaching methods and staff support/feedback to learning pressure and student motivation) and academic achievement within the context of postgraduate blended learning in UK universities. To achieve this, we employed a combination of several data analytics techniques including visualization, statistical tests, regression analysis, and latent profile analysis. Our empirical results (based on a survey of 255 postgraduate students and holistically interpreted via the Community of Inquiry (CoI) framework) demonstrated that learning activities combining teaching and social presences, and tailored academic support through effective feedback are critical elements for successful postgraduate experience in blended learning contexts. Regarding contributions, this research advances the understanding of student success by identifying the various ways demographic, experiential, and psychological factors impact academic outcomes. And in theoretical terms, it contributes to the extension of the CoI framework by integrating the concept of learner heterogeneity and identifying four distinct student profiles based on how they engage in the different CoI presences.

</details>


### [252] [Cultural Awareness, Stereotypes and Communication Skills in Intercultural Communication: The Algerian Participants Perspective](https://arxiv.org/abs/2511.12369)
*Mohamed Amine Kada Zair*

Main category: cs.CY

TL;DR: 研究探讨了阿尔及利亚参与者在多元文化环境中的文化意识、刻板印象与沟通技能之间的关系，发现高文化意识能改善沟通并减少刻板印象。


<details>
  <summary>Details</summary>
Motivation: 研究多元文化环境中文化意识、刻板印象和沟通技能的相互关系，为跨文化能力培养提供实证依据。

Method: 采用定量问卷调查法，对40名在多元文化环境工作或学习的阿尔及利亚参与者进行评估。

Result: 文化意识普遍较高，但某些刻板印象仍影响人际认知和沟通效率；高文化意识者沟通技能更好、刻板印象更少。

Conclusion: 跨文化能力教育和培训对减少偏见、促进多元环境中的相互理解至关重要。

Abstract: This study explores the relationship between cultural awareness, stereotypes, and communication skills among Algerian participants working or studying in multicultural environments. A quantitative questionnaire was administered to 40 respondents to evaluate their levels of cultural awareness, the presence of stereotypical thinking, and the effectiveness of their intercultural communication skills. Results revealed that while cultural awareness was generally high, certain stereotypes still influenced the perception of others and impacted communication efficiency. Participants with higher cultural awareness demonstrated better communication skills and lower levels of stereotyping. These findings underline the importance of intercultural competence and education programs in reducing prejudice and fostering mutual understanding in diverse contexts.

</details>


### [253] [Political Advertising on Facebook During the 2022 Australian Federal Election: A Social Identity Perspective](https://arxiv.org/abs/2511.12426)
*Stefano Civelli,Pietro Bernardelle,Frank Mols,Gianluca Demartini*

Main category: cs.CY

TL;DR: 基于Meta广告库分析2022年澳大利亚联邦选举期间Facebook和Instagram政治广告，揭示各党派在支出、受众定位和说服策略上的显著差异，并通过社会认同理论解释这些策略在强制投票背景下的作用。


<details>
  <summary>Details</summary>
Motivation: 监控社交媒体上的定向政治广告对于维护民主进程的透明度和问责制至关重要，需要建立实证证据基础来理解数字政治营销策略。

Method: 利用Meta广告库分析2022年澳大利亚联邦选举期间的政治广告数据，研究主要政治行为体的时间、人口统计和地理分布模式，并通过社会认同理论框架解释发现。

Result: 发现选举临近时广告活动显著增加；各党派针对年轻人群采用不同的受众定位策略；广告地理分布与人口密度基本一致；主要政党强调党派名称和对手，而小党则强调具体议题信息。

Conclusion: 在强制投票背景下，主要政党通过强化党派认同来防止选民流失，而小党则通过培育议题认同来吸引不满选民的支持。

Abstract: The spread of targeted advertising on social media platforms has revolutionized political marketing strategies. Monitoring these digital campaigns is essential for maintaining transparency and accountability in democratic processes. Leveraging Meta's Ad Library, we analyze political advertising on Facebook and Instagram during the 2022 Australian federal election campaign. We investigate temporal, demographic, and geographical patterns in the advertising strategies of major Australian political actors to establish an empirical evidence base, and interpret these findings through the lens of Social Identity Theory (SIT). Our findings not only reveal significant disparities in spending and reach among parties, but also in persuasion strategies being deployed in targeted online campaigns. We observe a marked increase in advertising activity as the election approached, peaking just before the mandated media blackout period. Demographic analysis shows distinct targeting strategies, with parties focusing more on younger demographics and exhibiting gender-based differences in ad impressions. Regional distribution of ads largely mirrored population densities, with some parties employing more targeted approaches in specific states. Moreover, we found that parties emphasized different themes aligned with their ideologies-major parties focused on party names and opponents, while smaller parties emphasized issue-specific messages. Drawing on SIT, we interpret these findings within Australia's compulsory voting context, suggesting that parties employed distinct persuasion strategies. With turnout guaranteed, major parties focused on reinforcing partisan identities to prevent voter defection, while smaller parties cultivated issue-based identities to capture the support of disaffected voters who are obligated to participate.

</details>


### [254] [AI and Supercomputing are Powering the Next Wave of Breakthrough Science - But at What Cost?](https://arxiv.org/abs/2511.12686)
*Stefano Bianchini,Aldo Geuna,Fazliddin Shermatov*

Main category: cs.CY

TL;DR: 基于500万篇科学论文的分析显示，AI与HPC结合的研究比传统研究更可能产生新概念和获得高引用，但也加剧了全球计算资源不平等问题。


<details>
  <summary>Details</summary>
Motivation: 量化AI和高性能计算对科学发现的联合影响，了解这两种技术如何共同塑造研究产出。

Method: 分析2000-2024年间500多万篇科学论文的元数据，研究AI和HPC在27个领域的交互作用。

Result: 结合AI和HPC的论文引入新概念的可能性是传统研究的3倍，进入高被引论文的可能性是5倍。

Conclusion: AI与HPC的融合正在重新定义科学创造力前沿，但加深了全球计算资源和专业知识获取的不平等，未来发现不仅取决于算法和计算能力，还取决于这些变革性工具的公平分配。

Abstract: Artificial intelligence (AI) and high-performance computing (HPC) are rapidly becoming the engines of modern science. However, their joint effect on discovery has yet to be quantified at scale. Drawing on metadata from over five million scientific publications (2000-2024), we identify how AI and HPC interact to shape research outcomes across 27 fields. Papers combining the two technologies are up to three times more likely to introduce novel concepts and five times more likely to reach top-cited status than conventional work. This convergence of AI and HPC is redefining the frontier of scientific creativity but also deepening global inequalities in access to computational power and expertise. Our findings suggest that the future of discovery will depend not only on algorithms and compute, but also on how equitably the world shares these transformative tools.

</details>


### [255] [The Unspoken Crisis of Learning: The Surging Zone of No Development](https://arxiv.org/abs/2511.12822)
*Euzeli C. dos Santos,Tracey Birdwell*

Main category: cs.CY

TL;DR: 本文通过P2P教学框架重新审视维果茨基的最近发展区理论，提出"无发展区"概念，强调AI教育中需要建立明确的退出机制来保护学习者的认知自主性。


<details>
  <summary>Details</summary>
Motivation: AI在教育中的广泛应用模糊了引导学习与依赖之间的界限，需要重新思考如何确保技术工具增强而非替代发展努力。

Method: 通过理论综合和框架设计，采用P2P教学框架，对比临时脚手架与永久数字中介现象。

Result: 提出了"无发展区"概念，展示如何通过刻意断开和伦理消退来恢复学习者的能动性。

Conclusion: 生产性挣扎、自我调节和第一性原理推理对持久学习至关重要，AI在教育中的负责任使用必须包含明确的退出机制。

Abstract: AI has redefined the boundaries of assistance in education, often blurring the line between guided learning and dependency. This paper revisits Vygotsky's Zone of Proximal Development (ZPD) through the lens of the P2P Teaching framework. By contrasting temporary scaffolding with the emerging phenomenon of permanent digital mediation, the study introduces the concept of the Zone of No Development (ZND), a state in which continuous assistance replaces cognitive struggle and impedes intellectual autonomy. Through theoretical synthesis and framework design, P2P Teaching demonstrates how deliberate disconnection and ethical fading can restore the learner's agency, ensuring that technological tools enhance rather than replace developmental effort. The paper argues that productive struggle, self-regulation, and first-principles reasoning remain essential for durable learning, and that responsible use of AI in education must include explicit mechanisms to end its help when mastery begins.

</details>


### [256] [Telekommunikationsüberwachung am Scheideweg: Zur Regulierbarkeit des Zugriffes auf verschlüsselte Kommunikation](https://arxiv.org/abs/2511.12830)
*Joanna Klauser,Bruno Albert,Christian Lindenmeier,Andreas Hammer,Felix Freiling,Dirk Heckmann,Sabine Pfeiffer*

Main category: cs.CY

TL;DR: 本文探讨加密通信背景下技术参与者的合作义务如何合理规范，分析传统电信监控在互联网时代面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着互联网技术发展，传统电信监控面临加密通信的挑战，执法机构难以获取端到端加密内容，需要重新审视技术参与者的合作义务。

Method: 分析传统电信监控法律基础和技术实现，探讨加密通信对执法的影响，研究技术参与者合作义务的合理规范方式。

Result: 发现当前拦截接口只能获取加密内容，无法解决端到端加密带来的执法难题，需要重新设计合作义务框架。

Conclusion: 在加密通信时代，需要重新思考技术参与者的合作义务规范，平衡隐私保护与执法需求。

Abstract: Personal communication using technical means is protected by telecommunications secrecy. Any interference with this fundamental right requires a legal basis, which has existed for many years for traditional communication services in the form of telecommunications surveillance (TKÜ, § 100a StPO) and appears to be widely accepted by society. The basis for the implementation of TKÜ is the obligation of telecommunications providers to provide interception interfaces. However, the technical implementation of telecommunications has changed significantly as a result of the Internet. Messenger services and Voice over IP telephony are increasingly competing with traditional telephone services. The use of strong end-to-end encryption made possible by this technology is increasingly posing problems for law enforcement agencies, as only cryptographically encrypted content is accessible via the interception interfaces provided to date. Against the backdrop of current discussions on socalled ``chat control'' and its limited social acceptance, this article addresses the question of whether and, if so, how the cooperation obligations of the technical actors involved can be sensibly regulated in the case of encrypted communication.

</details>


### [257] [Beyond Citations: A Cross-Domain Metric for Dataset Impact and Shareability](https://arxiv.org/abs/2511.12966)
*Smitha Muthya Sudheendra,Zhongxing Zhang,Wenwen Cao,Jisu Huh,Jaideep Srivastava*

Main category: cs.CY

TL;DR: 提出了X-index这一新型作者级指标，通过量化数据集的可访问性、重用性和跨学科影响力来评估数据共享的贡献价值。


<details>
  <summary>Details</summary>
Motivation: 现有指标（如h指数）主要关注出版物和引用，无法充分衡量数据集作为研究成果的真实影响，特别是数据可访问性、重用和跨学科影响方面。

Method: 采用两步法：首先计算数据集级价值分数（V-score），整合重用广度、FAIR原则符合度、引用影响和传递重用深度；然后将V-score聚合为作者级X-index。

Result: 在计算社会科学、医学和危机传播等领域的数据集上进行验证，与专家评分呈现强相关性。

Conclusion: X-index为评估数据共享实践和激励开放科学提供了透明、可扩展且低成本的框架，有助于促进可持续的数据共享实践。

Abstract: The scientific community increasingly relies on open data sharing, yet existing metrics inadequately capture the true impact of datasets as research outputs. Traditional measures, such as the h-index, focus on publications and citations but fail to account for dataset accessibility, reuse, and cross-disciplinary influence. We propose the X-index, a novel author-level metric that quantifies the value of data contributions through a two-step process: (i) computing a dataset-level value score (V-score) that integrates breadth of reuse, FAIRness, citation impact, and transitive reuse depth, and (ii) aggregating V-scores into an author-level X-index. Using datasets from computational social science, medicine, and crisis communication, we validate our approach against expert ratings, achieving a strong correlation. Our results demonstrate that the X-index provides a transparent, scalable, and low-cost framework for assessing data-sharing practices and incentivizing open science. The X-index encourages sustainable data-sharing practices and gives institutions, funders, and platforms a tangible way to acknowledge the lasting influence of research datasets.

</details>


### [258] [The Last Vote: A Multi-Stakeholder Framework for Language Model Governance](https://arxiv.org/abs/2511.13432)
*Subramanyam Sahoo,Aditi Chhawacharia*

Main category: cs.CY

TL;DR: 提出了一个全面的AI民主风险治理框架，包含风险分类、评估方法和实施策略


<details>
  <summary>Details</summary>
Motivation: AI系统日益强大和普及，民主社会在治理这些技术同时保护核心民主价值和制度面临前所未有的挑战

Method: 整合多方利益相关者参与、公民社会参与和现有国际治理框架，引入风险评估和制度适应的新机制

Result: 提出了七类民主风险分类法、利益相关者适应的ISS评分系统和分阶段实施策略

Conclusion: 该框架为民主社会应对AI风险提供了系统性的治理方案

Abstract: As artificial intelligence systems become increasingly powerful and pervasive, democratic societies face unprecedented challenges in governing these technologies while preserving core democratic values and institutions. This paper presents a comprehensive framework to address the full spectrum of risks that AI poses to democratic societies. Our approach integrates multi-stakeholder participation, civil society engagement, and existing international governance frameworks while introducing novel mechanisms for risk assessment and institutional adaptation. We propose: (1) a seven-category democratic risk taxonomy extending beyond individual-level harms to capture systemic threats, (2) a stakeholder-adaptive Incident Severity Score (ISS) that incorporates diverse perspectives and context-dependent risk factors, and (3) a phased implementation strategy that acknowledges the complex institutional changes required for effective AI governance.

</details>


### [259] [AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions](https://arxiv.org/abs/2511.13525)
*Zichong Wang,Zhipeng Yin,Roland H. C. Yap,Wenbin Zhang*

Main category: cs.CY

TL;DR: 该论文调查了在人口统计信息不完整情况下的AI公平性问题，提出了新的公平性概念分类法，并总结了现有技术。


<details>
  <summary>Details</summary>
Motivation: AI决策系统中的歧视性结果引发了公平性担忧，但现有方法大多依赖完整的人口统计信息，这在现实中往往不可行。

Method: 引入新的公平性概念分类法，阐明不同概念间的关系和区别，并总结在人口统计信息不完整情况下促进公平性的现有技术。

Result: 建立了在不完整人口统计信息下的公平性概念框架，为应对现实世界挑战提供了理论和方法基础。

Conclusion: 该研究填补了传统方法与现实挑战之间的空白，强调了在人口统计信息不完整情况下推进AI公平性的重要性，并指出了未来研究方向。

Abstract: Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.

</details>


### [260] [New Data Security Requirements and the Proceduralization of Mass Surveillance Law after the European Data Retention Case](https://arxiv.org/abs/2511.13553)
*Frederik Zuiderveen Borgesius,Axel Arnbak*

Main category: cs.CY

TL;DR: 欧盟法院废除数据保留指令的判决确立了元数据收集侵犯隐私权，并提出了数据安全评估新标准，但可能带来大规模监控的程序化风险。


<details>
  <summary>Details</summary>
Motivation: 分析欧盟法院废除数据保留指令这一里程碑判决对大规模元数据监控监管的影响，探讨该判决在保护人权的同时可能带来的程序化风险。

Method: 通过分析欧盟法院对数据保留指令的判决内容，探讨法院确立的法律原则、数据安全评估标准，以及判决可能引发的系统性风险。

Result: 法院确认元数据收集侵犯隐私权，提出了考虑非法访问风险、数据量和敏感性的数据安全评估标准，但为大规模监控留下了程序化空间。

Conclusion: 虽然判决强化了隐私保护，但通过程序化路径可能使大规模监控合法化，带来系统性人权风险，需要警惕政府机构利用复杂程序体系为大规模监控辩护。

Abstract: This paper discusses the regulation of mass metadata surveillance in Europe through the lens of the landmark judgment in which the Court of Justice of the European Union struck down the Data Retention Directive. The controversial directive obliged telecom and Internet access providers in Europe to retain metadata of all their customers for intelligence and law enforcement purposes, for a period of up to two years. In the ruling, the Court declared the directive in violation of the human rights to privacy and data protection. The Court also confirmed that the mere collection of metadata interferes with the human right to privacy. In addition, the Court developed three new criteria for assessing the level of data security required from a human rights perspective: security measures should take into account the risk of unlawful access to data, and the data's quantity and sensitivity. While organizations that campaigned against the directive have welcomed the ruling, we warn for the risk of proceduralization of mass surveillance law. The Court did not fully condemn mass surveillance that relies on metadata, but left open the possibility of mass surveillance if policymakers lay down sufficient procedural safeguards. Such proceduralization brings systematic risks for human rights. Government agencies, with ample resources, can design complicated systems of procedural oversight for mass surveillance - and claim that mass surveillance is lawful, even if it affects millions of innocent people.

</details>


### [261] [Access to Personal Data and the Right to Good Governance during Asylum Procedures after the CJEU's YS. and M. and S. judgment](https://arxiv.org/abs/2511.13555)
*Evelien Brouwer,Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 欧洲法院在YS、M和S案中裁决了寻求庇护者获取庇护申请决定相关文件的权利，涉及个人数据概念和数据保护指令下访问权范围的解释。


<details>
  <summary>Details</summary>
Motivation: 分析欧洲法院关于寻求庇护者获取庇护决定文件权利的裁决，探讨其对个人数据保护和良好行政权的影响。

Method: 通过分析欧洲法院在YS、M和S案中的判决，解读数据保护指令和欧盟基本权利宪章的相关规定。

Result: 判决表面上看似对个人权利不利，但实际上为未来庇护案件中的有效访问权提供了充分依据。

Conclusion: 该判决为庇护案件中获取会议记录的有效访问权奠定了法律基础，尽管初看似乎限制了个人权利。

Abstract: In the YS. and M. and S. judgment, the Court of Justice of the European Union ruled on three procedures in which Dutch judges asked for clarification on the right of asylum seekers to have access to the documents regarding the decision on asylum applications. The judgment is relevant for interpreting the concept of personal data and the scope of the right of access under the Data Protection Directive, and the right to good administration in the EU Charter of Fundamental Rights. At first glance, the judgment seems disappointing from the viewpoint of individual rights. Nevertheless, in our view the judgment provides sufficient grounds for effective access rights to the minutes in future asylum cases.

</details>


### [262] [Freedom of expression and 'right to be forgotten' cases in the Netherlands after Google Spain](https://arxiv.org/abs/2511.13557)
*Stefan Kulk,Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 本文分析了欧盟法院Google Spain判决在荷兰的应用情况，重点关注删除搜索结果请求的处理方式及其对言论自由的影响。


<details>
  <summary>Details</summary>
Motivation: 研究Google Spain判决后荷兰法院如何应用删除搜索结果的权利，特别关注言论自由方面的考量。

Method: 通过分析荷兰法院处理的两个删除请求案例，比较荷兰法院与欧盟法院在言论自由考量上的差异。

Result: 荷兰法院比欧盟法院更深入地考虑了删除搜索结果对言论自由的影响，但由于搜索引擎运营商决策不透明，难以全面评估该判决对言论自由的实际影响。

Conclusion: Google Spain判决在荷兰的应用显示法院更重视言论自由考量，但搜索引擎运营商决策的不透明性限制了对其影响的全面评估。

Abstract: Since the Google Spain judgment of the Court of Justice of the European Union, Europeans have, under certain conditions, the right to have search results for their name delisted. This paper examines how the Google Spain judgment has been applied in the Netherlands. Since the Google Spain judgment, Dutch courts have decided on two cases regarding delisting requests. In both cases, the Dutch courts considered freedom of expression aspects of delisting more thoroughly than the Court of Justice. However, the effect of the Google Spain judgment on freedom of expression is difficult to assess, as search engine operators decide about most delisting requests without disclosing much about their decisions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [263] [Softmax as a Lagrangian-Legendrian Seam](https://arxiv.org/abs/2511.11573)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 该论文首次将机器学习与微分几何联系起来，展示了softmax函数从logits到概率的转换过程可以建模为几何界面：两个由势能生成的保守描述在接触屏幕（概率单纯形）上的Legendreian"接缝"处相遇。


<details>
  <summary>Details</summary>
Motivation: 建立机器学习与微分几何之间的桥梁，为理解softmax等机器学习基础组件提供新的几何视角。

Method: 将softmax建模为几何界面，使用负熵和log-sum-exp两个势能生成的保守描述，在概率单纯形上的Legendreian接缝处相遇。偏置平移不变性表现为屏幕上的Reeb流，Fenchel-Young等式/KL散度提供到接缝的可计算距离。

Result: 构建了二类和三类情况的具体几何模型，展示了softmax的几何结构，包括偏置平移不变性的几何解释和距离度量。

Conclusion: 为机器学习开辟了新的研究方向：紧凑logit模型（投影或球形）、全局不变量，以及与信息几何的联系，其中屏幕上的动力学表现为复制子流。

Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.

</details>


### [264] [LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora](https://arxiv.org/abs/2511.11574)
*Viviana Luccioli,Rithika Iyengar,Ryan Panley,Flora Haberkorn,Xiaoyu Ge,Leland Crane,Nitish Sinha,Seung Jung Lee*

Main category: cs.LG

TL;DR: 提出了M-RARU算法，通过主动学习结合不确定性和随机接受-拒绝机制，显著降低大语言模型知识蒸馏的成本，减少80%样本需求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分类任务中准确率高，但计算和财务成本阻碍其在动态环境中的大规模部署。知识蒸馏过程本身成本高昂，需要教师模型标注大量样本并消耗大量token。

Method: 引入M-RARU（多类随机接受/拒绝不确定性采样）算法，结合不确定性和随机接受-拒绝机制，只选择最有信息量的数据点供LLM教师标注。

Result: 在五个不同学生模型和多个基准数据集上的实验表明，相比随机采样，该方法可减少80%样本需求，显著提高分类准确性，同时降低财务成本和训练时间。

Conclusion: M-RARU为降低LLM知识蒸馏成本提供了有效解决方案，通过主动学习策略在保持性能的同时大幅减少资源消耗。

Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.

</details>


### [265] [Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms](https://arxiv.org/abs/2511.11575)
*Animesh Joshi*

Main category: cs.LG

TL;DR: 本文提出了一个统计显著性测试框架，用于检测算法公平性违规，通过k折交叉验证生成公平性指标的抽样分布，并在累犯预测算法中发现了针对黑人群体的统计显著偏见。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏评估群体间观察到的差异是否具有统计显著性还是偶然的方法，需要建立严格的统计测试框架来评估算法决策系统的公平性。

Method: 利用k折交叉验证生成公平性指标的抽样分布，开发基于预测与实际结果差异、模型校准和因果推断技术的统计显著性测试方法。

Result: 在累犯预测算法的测试中发现，机器学习算法在多个公平性定义下对黑人个体表现出统计显著的偏见，而在其他定义下则无偏见或对白人个体有偏见。

Conclusion: 评估算法决策系统时需要采用严格和稳健的统计测试方法，以确保公平性评估的可靠性。

Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.

</details>


### [266] [DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs](https://arxiv.org/abs/2511.11576)
*WenZhuo Zhu,Zheng Cui,Wenhan Lu,Sheng Liu,Yue Zhao*

Main category: cs.LG

TL;DR: 提出了DAOpt框架，包括OptU数据集、多智能体决策模块和模拟环境，用于评估LLM在不确定优化问题中的表现，重点关注样本外可行性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注确定性优化问题，但在现实决策中存在不确定性，LLM在不确定环境下的应用仍待探索。

Method: 采用少样本学习结合随机优化和鲁棒优化的领域知识，增强LLM的建模能力，并开发多智能体决策模块和模拟评估环境。

Result: 构建了OptU数据集和DAOpt框架，为LLM在不确定优化问题中的研究提供了新的评估工具和方法。

Conclusion: DAOpt框架填补了LLM在不确定优化建模领域的空白，为研究LLM在现实决策问题中的应用提供了重要基础。

Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.

</details>


### [267] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: 本文深入分析了Transformer中Rotary位置编码(RoPE)的位置信息与符号信息编码机制，证明了注意力头的位置行为和符号行为是互斥的，并开发了量化这些行为的指标。


<details>
  <summary>Details</summary>
Motivation: 理解语言理解和生成中位置信息与符号信息的独立编码机制，特别是RoPE位置编码在Transformer中的成功原理及其频率使用模式。

Method: 从理论和实证层面分析注意力头的位置与符号行为二分性，定义位置行为和符号行为的一般定义，证明二者互斥性，开发量化指标，并在使用RoPE的Transformer LLMs上应用分析框架。

Result: 发现所有注意力头的行为与频率使用之间存在强相关性，通过控制注意力头可访问的频率可以控制Transformer的性能表现。

Conclusion: 研究提供了对RoPE的详细理解，揭示了其特性如何与模型行为相关联，特别是频率使用在位置和符号任务中的因果作用。

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [268] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 开发了一个基于Triton DSL的跨平台LLM推理系统，通过paged attention内核在NVIDIA和AMD GPU上实现最先进性能，解决了LLM推理的硬件可移植性问题。


<details>
  <summary>Details</summary>
Motivation: 长期以来业界和学术界都希望开发一个跨硬件架构的LLM推理平台，无需底层手动调优，同时保持最佳效率。

Method: 使用Triton领域特定语言开发了最先进的paged attention内核，包含算法和系统级改进、参数自动调优，并集成到流行的推理服务器中。

Result: 将通用Triton attention内核的性能从最先进水平的19.7%提升到105.9%，在NVIDIA和AMD GPU上都实现了最先进性能。

Conclusion: 开源领域特定语言可以用来解锁跨不同GPU厂商的模型可移植性，证明了高效跨平台LLM推理的可行性。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [269] [Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations](https://arxiv.org/abs/2511.11583)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.LG

TL;DR: RAG-FLARKO通过检索增强的多阶段知识图谱处理，解决了LLM在金融推荐中的上下文限制、幻觉问题和行为基础缺失，显著提升了推荐质量和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在个性化金融推荐中存在上下文限制、幻觉问题以及缺乏行为基础等挑战，需要一种能够有效整合用户行为和市场数据的解决方案。

Method: 采用检索增强的多阶段并行知识图谱检索：首先从用户交易知识图谱中检索行为相关实体，然后利用该上下文从市场知识图谱中筛选时间一致信号，构建紧凑的接地子图供LLM使用。

Result: 在真实金融交易数据集上的实证评估表明，RAG-FLARKO显著提升了推荐质量，使更小、更高效的模型在盈利性和行为对齐方面都能达到高性能。

Conclusion: 该框架为在资源受限环境中部署接地金融AI提供了可行路径，通过减少上下文开销和聚焦相关信息，实现了高性能的个性化金融推荐。

Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.

</details>


### [270] [Output Supervision Can Obfuscate the Chain of Thought](https://arxiv.org/abs/2511.11584)
*Jacob Drori,Luke Marks,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: 训练模型仅使用输出监控器（无法访问思维链）仍会导致隐蔽的思维链，通过两种机制：模型泛化使思维链看起来安全，以及安全外观的思维链增加安全输出的可能性。提出了两种缓解方法，在可监控性和任务性能上实现帕累托改进。


<details>
  <summary>Details</summary>
Motivation: OpenAI的研究表明，针对思维链监控器的训练会导致隐蔽的思维链，其中包含监控器无法检测的不良行为。他们建议仅使用输出监控器进行训练来保持思维链的可监控性。本文旨在证明这种训练方法仍然会导致隐蔽的思维链问题。

Method: 识别了两种导致隐蔽思维链的机制：1）模型在训练产生安全外观输出时，会泛化到使思维链看起来安全；2）由于后续token依赖于先前token，安全外观的思维链会增加安全输出的可能性。提出了两种相应的缓解方法来解决这些问题。

Result: 提出的两种缓解方法在可监控性和任务性能方面相比常规训练实现了帕累托改进，即在保持或提升任务性能的同时提高了思维链的可监控性。

Conclusion: 仅使用输出监控器进行训练不足以防止隐蔽思维链问题，需要额外的缓解措施。本文提出的方法能够有效解决这一问题，实现更好的可监控性和性能平衡。

Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.

</details>


### [271] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: FedGen-Edge是一个联邦学习框架，通过解耦预训练的全局主干网络和轻量级客户端适配器，仅联邦化适配器来降低通信开销，支持异构边缘设备上的生成式AI。


<details>
  <summary>Details</summary>
Motivation: 大型生成模型在跨设备联邦学习中面临计算通信开销大、统计/系统异构性问题，需要一种实用的隐私保护、资源感知和个性化解决方案。

Method: 使用低秩适配(LoRA)将客户端更新约束到紧凑子空间，仅联邦化轻量级适配器而非完整模型，支持FedAvg风格的服务器聚合。

Result: 在语言建模(PTB)和图像生成(CIFAR-10)任务上，相比强基线实现了更低的困惑度/FID和更快的收敛速度，通信开销减少超过99%。

Conclusion: FedGen-Edge为异构边缘设备上的隐私保护、资源感知和个性化生成式AI提供了一条实用路径。

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [272] [WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation](https://arxiv.org/abs/2511.11589)
*Chenyue Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: WildfireGenome提出了一种可解释的野火风险评估方法，通过融合多个联邦指标、随机森林分类和SHAP分析，在H3 Level-8分辨率下提供决策尺度的风险评估。


<details>
  <summary>Details</summary>
Motivation: 当前野火风险评估依赖粗糙的危险地图和不透明的机器学习模型，牺牲了决策尺度的可解释性，需要更精细和可解释的评估方法。

Method: 融合7个联邦野火指标进行PCA分析生成风险标签，使用随机森林分类本地野火风险，并通过SHAP和ICE/PDP分析揭示县域特定的非线性驱动关系。

Result: 在7个生态多样化的美国县中，模型准确率达0.755-0.878，二次加权Kappa达0.951，主成分解释了87-94%的指标方差。针叶林覆盖率和海拔是主要驱动因素，风险在30-40%针叶林覆盖率时急剧上升。

Conclusion: WildfireGenome将野火风险评估从区域预测推进到可解释的决策尺度分析，可指导植被管理、分区和基础设施规划。

Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.

</details>


### [273] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出轨迹熵约束强化学习(TECRL)框架，通过分离奖励和熵的Q函数学习，解决最大熵RL中的非平稳Q值估计和短视局部熵调节问题。


<details>
  <summary>Details</summary>
Motivation: 传统最大熵RL存在两个瓶颈：(1)联合注入熵和更新温度参数导致Q值估计非平稳；(2)仅根据当前单步熵调节温度，未考虑累积熵的长期影响。

Method: 提出TECRL框架：分别学习奖励Q函数和熵Q函数，确保值目标不受温度更新影响；通过专门的熵Q函数量化期望累积熵，实施轨迹熵约束控制策略长期随机性。基于此开发DSAC-E算法，扩展DSAC-T。

Result: 在OpenAI Gym基准测试中，DSAC-E能够获得更高回报和更好稳定性。

Conclusion: TECRL框架通过分离奖励和熵的Q函数学习，有效解决了最大熵RL中的关键问题，提升了算法性能和稳定性。

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [274] [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://arxiv.org/abs/2511.11593)
*Matthew Morris,Ian Horrocks*

Main category: cs.LG

TL;DR: 本文研究了使用均值聚合和非负权重的图神经网络（MAGNNs），证明了它们能够支持的单调规则类别，并提供了用于解释MAGNN预测的一阶逻辑片段。实验表明这种限制在标准基准测试中表现相当或更好，并能生成有洞察力的解释。


<details>
  <summary>Details</summary>
Motivation: 尽管使用均值聚合的GNN很普遍，但缺乏对其可解释性和表达能力的理论研究。本文旨在填补这一空白，特别关注均值聚合和非负权重的GNN。

Method: 提出MAGNNs（使用均值聚合和非负权重的GNN），从理论上证明它们支持的单调规则类别，并构建用于解释预测的一阶逻辑片段。通过实验验证方法的有效性。

Result: 实验显示：限制GNN使用非负权重在标准基准测试中表现相当或更好；在实践中能够获得可靠的规则；可以生成有洞察力的解释；可靠的规则能够暴露训练模型中的问题。

Conclusion: MAGNNs不仅保持了良好的性能，还提供了理论保证的可解释性框架，能够生成可靠的解释并帮助识别模型问题。

Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.

</details>


### [275] [Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach](https://arxiv.org/abs/2511.11596)
*Javier Marín*

Main category: cs.LG

TL;DR: 论文分析了LGD建模中90%训练数据为代理估计值的问题，发现随机森林等递归划分方法表现差（r²=-0.664），而基于信息论的方法表现更好（r²=0.191）。研究发现杠杆特征比规模效应更具信息价值。


<details>
  <summary>Details</summary>
Motivation: LGD建模面临数据质量问题：90%的训练数据是基于破产前资产负债表的代理估计，而非实际回收结果。这种混合污染的训练结构导致传统方法失效。

Method: 使用信息论方法（香农熵和互信息）分析1218个企业破产案例（1980-2023），比较了递归划分方法（如随机森林）与信息论方法的性能。

Result: 随机森林在测试数据上r²为-0.664（比预测均值更差），而信息论方法r²达到0.191，RMSE为0.284。杠杆特征包含1.510比特互信息，规模效应仅0.086比特。

Conclusion: 在巴塞尔III要求下，当代表性结果数据不足时，信息论方法优于传统方法。研究结果适用于医疗结果研究、气候预测和技术可靠性等领域，这些领域都存在训练数据混合结构问题。

Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.

</details>


### [276] [Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks](https://arxiv.org/abs/2511.11666)
*Rajit Rajpal,Benedict Leimkuhler,Yuanhao Jiang*

Main category: cs.LG

TL;DR: 提出了SA-SGLD自适应采样算法，通过时间重缩放自动调整步长，在高曲率区域缩小步长、平坦区域扩大步长，提高贝叶斯神经网络后验采样的稳定性和混合效率。


<details>
  <summary>Details</summary>
Motivation: 现有的随机梯度MCMC方法对步长选择高度敏感，自适应变体如pSGLD需要昂贵的散度校正项才能正确采样不变测度。

Method: 基于SamAdams框架构建SA-SGLD自适应方案，使用时间重缩放根据监测量（通常是局部梯度范数）调节步长。

Result: 在高曲率2D玩具示例和使用尖锐先验的贝叶斯神经网络图像分类中，比SGLD实现更准确的后验采样。

Conclusion: SA-SGLD能够在不引入偏差的情况下自动调整步长，改善采样稳定性和混合效率。

Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.

</details>


### [277] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games](https://arxiv.org/abs/2511.11602)
*Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于收益的分布式优化学习方案——基于期望的扰动学习自动机（APLA），解决了传统强化学习在多玩家弱非循环游戏中无法保证收敛到纯纳什均衡的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在分布式设置中存在局限性，特别是在多玩家弱非循环游戏中，当每个玩家应用独立的学习动态时，无法保证收敛到理想的纯纳什均衡。现有研究仅关注潜在博弈和协调博弈等小类游戏。

Method: 提出了APLA学习方案，其中每个玩家的动作选择概率分布不仅通过重复选择得到强化，还通过捕获玩家满意度的期望因子得到强化。在存在噪声观测的情况下，对多玩家正效用博弈进行了随机稳定性分析。

Result: 首次在一般非零和博弈中通过建立诱导无限维马尔可夫链与有限维马尔可夫链的等价性，刻画了随机稳定性。在弱非循环博弈中进一步专门化了随机稳定性分析。

Conclusion: APLA方案能够有效解决传统强化学习在分布式优化中的收敛问题，为多玩家博弈环境下的学习动态提供了新的理论框架和分析方法。

Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.

</details>


### [278] [Coordinate Descent for Network Linearization](https://arxiv.org/abs/2511.11781)
*Vlad Rakhlin,Amir Jevnisek,Shai Avidan*

Main category: cs.LG

TL;DR: 提出了一种基于坐标下降的离散优化方法，直接减少ResNet网络中ReLU激活函数的数量，以降低私有推理的延迟。


<details>
  <summary>Details</summary>
Motivation: ReLU激活函数是基于ResNet网络的私有推理中的主要瓶颈，会导致显著的推理延迟。现有的平滑近似方法在最后硬阈值步骤中会引入较大的性能损失。

Method: 采用坐标下降作为优化框架，直接在离散域中工作，通过设计产生稀疏解。

Result: 在常见基准测试中达到了最先进的性能。

Conclusion: 该方法通过离散优化有效减少了ReLU数量，同时保持了网络精度，在私有推理中具有显著优势。

Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.

</details>


### [279] [Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques](https://arxiv.org/abs/2511.11604)
*Amaratou Mahamadou Saley,Thierry Moyaux,Aïcha Sekhari,Vincent Cheutet,Jean-Baptiste Danielou*

Main category: cs.LG

TL;DR: 提出了一种结合数据驱动技术和核工业领域知识的预测性维护方法，在核工业领域显著优于纯数据驱动方法，将预测时间从3小时延长到24小时，F1分数从56.36%提升到93.12%。


<details>
  <summary>Details</summary>
Motivation: 物联网和工业4.0在核工业中的应用增强了数据驱动方法，但核工业系统复杂，需要大量领域知识。纯数据驱动方法在预测设备维护需求方面存在局限性。

Method: 提出了一种混合方法，将数据驱动技术与核设备领域知识相结合，强调纯数据驱动方法的局限性，并展示知识在提升预测模型性能中的重要性。

Result: 通过真实案例研究比较，混合方法显著优于纯数据驱动方法：预测时间从3小时延长到24小时，F1分数从56.36%提升到93.12%。

Conclusion: 在核工业等高度受限和敏感的领域，结合领域知识的混合预测性维护方法比纯数据驱动方法更有效，能显著提升故障预测性能。

Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.

</details>


### [280] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: 提出COWM层来解决RL中的环境非平稳性问题，通过聚类和投影矩阵提高学习效率和稳定性


<details>
  <summary>Details</summary>
Motivation: RL代理通常假设环境平稳，但实际环境往往是非平稳的，导致需要数百万次迭代和低样本效率

Method: 引入COWM层集成到任何RL算法的策略网络中，使用聚类技术和投影矩阵来缓解非平稳性

Result: 在视觉和状态基础的DMControl基准上分别提升9%和12.6%，在各种算法和任务中表现出鲁棒性和通用性

Conclusion: COWM层能有效缓解环境非平稳性，提高学习速度和效率，减少梯度干扰

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [281] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列建模中分词器设计（缩放和量化策略）与迁移学习对模型性能的影响，发现分词器配置主要控制模型表示能力和稳定性，而迁移学习影响优化效率和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 分词器和迁移学习是构建最先进时间序列基础模型的两个关键组件，但它们在时间序列建模中的具体作用和交互关系尚不明确，需要系统研究。

Method: 通过经验训练实验和理论分析相结合的方法，研究不同分词器配置（特别是缩放和量化策略）与预训练/随机初始化对模型性能的影响。

Result: 预训练模型能更有效地利用设计良好的分词器，特别是在较小词汇量时；而错误对齐的分词会削弱甚至逆转预训练的益处。结合小型高效词汇表与预训练权重在多模态预测设置中特别有利。

Conclusion: 研究强调了在时间序列建模中精心设计分词器的重要性，并为离散表示学习中分词器设计和迁移学习利用提供了具体指导。

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [282] [Finding Time Series Anomalies using Granular-ball Vector Data Description](https://arxiv.org/abs/2511.12147)
*Lifeng Shen,Liang Peng,Ruiwen Liu,Shuyin Xia,Yi Liu*

Main category: cs.LG

TL;DR: 提出了一种基于粒度球向量数据描述（GVDD）的粒度球单类网络（GBOC），用于时间序列异常检测，通过密度引导的层次分裂过程生成紧凑的高密度区域表示，提高了检测的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法如最近邻和聚类在动态非线性时间序列异常检测中依赖刚性假设（如预定义的可靠邻居或聚类数量），在复杂时间场景中经常失效，需要更灵活的数据自适应方法。

Method: 使用GVDD将潜在空间划分为由粒度球表示的紧凑高密度区域，通过密度引导的层次分裂过程生成并去除噪声结构。训练时通过将样本与最近的粒度球中心对齐来提高表示紧凑性，推理时基于到最近粒度球的距离计算异常分数。

Result: 大量实验验证了该方法的有效性和优越性，表明其能够处理时间序列异常检测的挑战，通过关注密集高质量区域并显著减少原型数量，实现了鲁棒且高效的异常检测。

Conclusion: GBOC方法通过数据自适应的粒度球表示，在时间序列异常检测中克服了传统方法的局限性，提供了更灵活、鲁棒和高效的解决方案。

Abstract: Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.

</details>


### [283] [Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data](https://arxiv.org/abs/2511.11623)
*Yushan Jiang,Shuteng Niu,Dongjin Song,Yichen Wang,Jingna Feng,Xinyue Hu,Liu Yang,Cui Tao*

Main category: cs.LG

TL;DR: 开发了一个多模态深度学习框架，用于早期预测肝移植后的移植物抗宿主病(GVHD)，通过整合电子健康记录中的多种模态数据，在极不平衡的数据集上取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: GVHD是肝移植中罕见但致命的并发症，死亡率极高。通过整合异构和不平衡的电子健康记录，旨在推进GVHD的早期预测，为及时干预和改善患者预后铺平道路。

Method: 分析了2100名肝移植患者的术前电子健康记录，包括42例GVHD病例。开发了多模态深度学习框架，动态融合患者人口统计学、实验室检查、诊断和药物四种主要模态，处理不规则记录和缺失值，并通过AUC优化解决极端类别不平衡问题。

Result: 该框架在所有单模态和多模态机器学习基线方法中表现最佳，实现了AUC 0.836、AUPRC 0.157、召回率0.768和特异性0.803。证明了从不同模态捕获互补信息的有效性。

Conclusion: 多模态深度学习框架显著改进了GVHD早期预测的现有方法，有效解决了真实世界电子健康记录中的异构性和极端类别不平衡挑战，实现了准确的早期预测。

Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.

</details>


### [284] [Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering](https://arxiv.org/abs/2511.12180)
*Ge Cheng,Shuo Wang,Yun Zhang*

Main category: cs.LG

TL;DR: 本文提出了SC-InfoNCE损失函数，通过引入可调节的收敛目标来灵活控制特征相似性对齐，在图像、图结构和文本任务上实现了稳定且优越的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管InfoNCE在对比学习中取得了经验成功，但其理论基础仍然有限。作者旨在深入理解InfoNCE的工作原理，并改进其性能。

Method: 引入显式特征空间建模样本的增强视图，使用转移概率矩阵捕捉数据增强动态，提出SC-InfoNCE损失函数，通过缩放目标矩阵来灵活控制特征相似性对齐。

Result: 在图像、图结构和文本任务的基准数据集上，SC-InfoNCE在不同领域都实现了强大且可靠的性能。

Conclusion: SC-InfoNCE通过引入可调节的收敛目标，能够更好地匹配下游数据的统计特性，为对比学习提供了更灵活和有效的训练目标。

Abstract: Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.

</details>


### [285] [MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks](https://arxiv.org/abs/2511.11625)
*Mohammad Karami,Mohammad Reza Nemati,Aidin Kazemi,Ali Mikaeili Barzili,Hamid Azadegan,Behzad Moshiri*

Main category: cs.LG

TL;DR: MedFedPure是一个保护联邦学习医疗AI模型的防御框架，通过个性化FL、掩码自编码器检测和自适应扩散净化模块，在保持隐私的同时显著提升对抗攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的医疗AI模型在推理时容易受到对抗攻击，现有防御方法难以适应去中心化的医疗环境，需要在不影响隐私和准确性的前提下保护诊断模型。

Method: 结合三个关键组件：个性化联邦学习模型适应各机构数据分布；掩码自编码器检测可疑输入；自适应扩散净化模块选择性清理被标记的扫描图像。

Result: 在Br35H脑部MRI数据集上，对抗鲁棒性从49.50%显著提升至87.33%，同时保持97.67%的干净准确率。

Conclusion: 该框架为临床工作流程中部署安全、可信且保护隐私的AI工具提供了实用路径，能够在诊断过程中本地实时运行。

Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy

</details>


### [286] [Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.12261)
*Zongxin Shen,Yanyong Huang,Dongjie Wang,Jinyuan Chang,Fengmao Lv,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出CLIM-FS方法解决混合缺失多视图无监督特征选择问题，通过联合学习特征选择和自适应数据填补，利用共识聚类结构和跨视图局部几何结构增强性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临三个关键挑战：1) 仅关注视图缺失，不适用于实践中更普遍的混合缺失场景；2) 对视图间一致性和多样性的利用不足；3) 缺乏理论分析说明特征选择与数据填补的交互机制。

Method: 基于非负正交矩阵分解，将缺失视图和变量的填补集成到特征选择模型中，联合学习特征选择和自适应数据填补，并充分利用共识聚类结构和跨视图局部几何结构。

Result: 在8个真实世界多视图数据集上的实验结果表明，CLIM-FS优于现有最先进方法。

Conclusion: CLIM-FS能有效解决混合缺失多视图无监督特征选择问题，通过理论分析和实验验证了其优越性能。

Abstract: Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.

</details>


### [287] [SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion](https://arxiv.org/abs/2511.11627)
*Wang Zhenyu,Li Peiyuan,Shi Yongxiang,Wu Ruoyu,Zhang Lei*

Main category: cs.LG

TL;DR: 提出SA-EMO架构用于未知地下结构下的速度场反演，通过结构对齐编码器和多算子混合机制显著提升全波形反演性能


<details>
  <summary>Details</summary>
Motivation: 传统单CNN架构或单神经算子在未知或复杂地质环境中泛化能力差，难以区分不同地质类型，需要更有效的反演方法

Method: 使用结构对齐编码器将地震波场映射到物理一致的潜在空间，然后通过自适应路由机制选择和融合多种神经算子专家来预测速度模型

Result: 在OpenFWI基准和Marmousi2数据集上，SA-EMO显著优于传统方法，平均MAE降低约58.443%，边界分辨率提升约10.308%

Conclusion: 这项工作为高效、可扩展且物理可解释的全波形反演引入了新范式

Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.

</details>


### [288] [Optimal Self-Consistency for Efficient Reasoning with Large Language Models](https://arxiv.org/abs/2511.12309)
*Austin Feng,Marius Alonso,Ambroise Odonnat*

Main category: cs.LG

TL;DR: 本文对自洽性推理进行了首次全面分析，提出了Blend-ASC方法，相比传统自洽性方法平均减少6.8倍样本使用量，实现了最先进的样本效率。


<details>
  <summary>Details</summary>
Motivation: 自洽性推理虽然有效，但在大规模应用时成本过高，且缺乏对样本效率和扩展行为的统一理论分析。

Method: 基于模式估计和投票理论分析自洽性扩展行为，提出Blend-ASC方法，采用动态样本分配策略，无需超参数且可适应任意样本预算。

Result: Blend-ASC在样本效率上优于固定分配和动态分配的自洽性基线方法，平均减少6.8倍样本使用量。

Conclusion: Blend-ASC是一种高效、无超参数的自洽性推理变体，可轻松应用于各种自洽性应用场景。

Abstract: Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.

</details>


### [289] [Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification](https://arxiv.org/abs/2511.11629)
*Xu Zhang,Peng Wang,Chen Wang,Zhe Xu,Xiaohua Nie,Wei Wang*

Main category: cs.LG

TL;DR: 提出了基于超图的全局特征学习和融合框架，通过特征工程构建全局特征并学习局部特征间的高阶关系，以增强应变计状态时间序列的表示能力，提高识别精度。


<details>
  <summary>Details</summary>
Motivation: 在智能制造中，应变计状态识别对及时发现机械部件故障至关重要。传统CNN方法只能提取局部特征，当不同时间序列的局部子序列相似时（如飞机机翼静强度实验数据），仅靠局部特征不足以充分表达时间序列。

Method: 提出超图全局特征学习与融合框架：1）通过特征工程构建全局特征；2）学习局部特征间的高阶关系来捕获全局特征；3）将全局特征与局部特征融合以增强时间序列表示。

Result: 在工业SGS数据集和公开UCR数据集上的验证表明，该方法在SGS识别任务中对未见数据具有更好的泛化能力。

Conclusion: 通过引入全局特征学习和融合机制，能够更全面地表示应变计状态时间序列，有效解决了仅依赖局部特征时识别精度不足的问题。

Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.

</details>


### [290] [Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation](https://arxiv.org/abs/2511.12545)
*Robin van der Laag,Hao Wang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 基于最优传输理论提出中心-外向q支配关系，证明其蕴含强一阶随机支配，开发了基于q支配的经验检验方法，并在超参数调优和多目标优化算法中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随机多目标优化需要排序多元分布，但现有研究多采用标量化方法，这会丢失信息且不可靠。需要一种更可靠的方法来比较随机Pareto集。

Method: 引入基于最优传输理论的中心-外向q支配关系，开发相应的经验检验程序，推导出控制Type I错误的显式样本量阈值n*(δ)。

Result: 在YAHPO-MO基准任务中成功比较了七个多目标超参数调优器的最终随机Pareto集；在NSGA-II算法中用q支配替换基于均值的选择，在噪声增强的ZDT基准问题上显示出更优的收敛速度。

Conclusion: 中心-外向q支配为寻求真正随机支配解提供了一个原则性、可处理的基础，特别在期望超体积指标变得不可区分时仍能有效比较算法性能。

Abstract: Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(δ)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.

</details>


### [291] [Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models](https://arxiv.org/abs/2511.11630)
*Eliane Younes,Elie Hachem,Marc Bernacki*

Main category: cs.LG

TL;DR: 本研究评估了多种深度学习模型（RNN、LSTM、TCN、transformer）预测晶粒生长过程中晶粒尺寸分布的能力。LSTM模型表现最佳，准确率超过90%，计算时间从20分钟缩短到几秒。


<details>
  <summary>Details</summary>
Motivation: 晶粒生长对材料力学行为有重要影响，但全场模拟计算成本高。本研究旨在开发基于低维统计描述符的高效预测方法。

Method: 使用从高保真模拟中提取的平均场统计描述符，将120个晶粒生长序列处理为归一化晶粒尺寸分布。采用递归预测策略，从短期历史数据预测未来分布。

Result: LSTM网络准确率最高（>90%），性能最稳定，能保持物理一致性预测，计算时间从20分钟/序列缩短到几秒钟。其他架构在长期预测时容易发散。

Conclusion: 低维描述符和LSTM预测方法在高效准确的微观结构预测方面具有潜力，对数字孪生开发和工艺优化有直接应用价值。

Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.

</details>


### [292] [Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back](https://arxiv.org/abs/2511.12659)
*Alon Cohen,Liad Erez,Steve Hanneke,Tomer Koren,Yishay Mansour,Shay Moran,Qian Zhang*

Main category: cs.LG

TL;DR: 多类别PAC学习的样本复杂度由两个维度共同控制：DS维度主导DS控制区域，Natarajan维度决定小ε时的渐近行为，形式为DS^{1.5}/ε + Nat/ε²。


<details>
  <summary>Details</summary>
Motivation: 扩展统计学习基本定理到多类别分类，理解其与二分类学习的差异，探究Natarajan维度和DS维度在多类别学习中的具体作用。

Method: 采用基于自适应乘性权重的在线算法进行标签空间约简，不同于传统的基于一致收敛或可约化情况的不可知学习方法。

Result: 证明了多类别不可知PAC样本复杂度的紧界，形式为DS^{1.5}/ε + Nat/ε²，该界在第一个项上紧至√DS因子，几乎匹配已知的Nat/ε²和DS/ε下界。

Conclusion: 多类别学习本质上涉及两个结构参数，与二分类或在线分类（由单一维度控制）不同，DS维度和Natarajan维度共同决定学习行为。

Abstract: The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}ε + \frac{Nat}{ε^2}$ where $ε$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/ε^2$ and $DS/ε$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $ε$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.

</details>


### [293] [Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination](https://arxiv.org/abs/2511.11632)
*Qiuhao Zeng*

Main category: cs.LG

TL;DR: 提出一种新的元学习算法，通过将分类器分解为元组件来改进小样本学习中的泛化能力，使用正交正则化促进元组件的多样性。


<details>
  <summary>Details</summary>
Motivation: 基于度量的元学习方法在小样本学习中可能对已见类别过拟合，导致在未见类别上泛化能力不足。

Method: 将分类器表示为元组件的组合，通过元学习在已见类别上学习元组件，并施加正交正则化来促进元组件的多样性和捕获不同分类器间的共享子结构。

Result: 在小样本基准任务上的广泛实验显示该方法具有优越性能。

Conclusion: 通过元组件分解和正交正则化，可以有效提升小样本学习中的泛化能力。

Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.

</details>


### [294] [A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.12695)
*Minghui Chen,Hrad Ghoukasian,Ruinan Jin,Zehua Wang,Sai Praneeth Karimireddy,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 将集中式学习中的线性探测后微调(LP-FT)方法应用于联邦学习，解决个性化微调中的特征失真问题，在七个数据集上验证了其在平衡个性化和泛化性方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下难以平衡全局泛化与本地个性化，个性化微调方法容易在偏斜的客户端分布上过拟合或无法适应领域偏移。

Method: 将线性探测后微调(LP-FT)策略从集中式学习扩展到联邦学习设置，通过分阶段的参数更新来缓解联邦特征失真现象。

Result: 在七个数据集和六种个性化微调变体上的系统评估表明，LP-FT在平衡个性化和泛化性方面优于标准微调方法。

Conclusion: LP-FT通过缓解联邦特征失真，在部分特征重叠、协变量-概念偏移等条件下表现优异，为联邦学习中部署鲁棒个性化提供了可行指南。

Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.

</details>


### [295] [An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment](https://arxiv.org/abs/2511.11636)
*Asma Sadia Khan,Sadia Tabassum*

Main category: cs.LG

TL;DR: 开发了一个公平审计和可解释的机器学习框架来预测多囊卵巢综合征(PCOS)，通过SHAP特征归因和人口统计学审计识别诊断差异，并确保跨亚组的可靠风险预测。


<details>
  <summary>Details</summary>
Motivation: 为了解决PCOS诊断中存在的患者亚组间差异问题，确保机器学习模型在不同人群中的公平性和可靠性，同时提供可解释的预测结果以支持临床决策。

Method: 使用随机森林、SVM和XGBoost模型，结合等渗和Platt标定方法进行公平性比较。集成SHAP特征归因和人口统计学审计，并采用概率标定指标(Brier Score和ECE)来评估模型性能。

Result: 标定的随机森林模型达到90.8%的预测准确率。SHAP分析识别出卵泡计数、体重增加和月经不规律为最重要特征。模型在25-35岁女性中表现最佳(90.9%)，但在25岁以下女性中表现较差(69.2%)。在肥胖女性中达到完美精度，在瘦型PCOS病例中保持高召回率。

Conclusion: 随机森林模型在标定性和可解释性之间提供了最佳平衡。开发了基于Streamlit的Web界面，实现实时PCOS风险评估和交互式分析，弥合AI研究与临床可用性之间的差距。

Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.

</details>


### [296] [Conformal Online Learning of Deep Koopman Linear Embeddings](https://arxiv.org/abs/2511.12760)
*Ben Gao,Jordan Patracone,Stéphane Chrétien,Olivier Alata*

Main category: cs.LG

TL;DR: COLoKe是一个自适应更新非线性动力系统Koopman嵌入的框架，通过深度特征学习和多步预测一致性在线学习，仅在模型预测误差超过动态阈值时触发更新。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性动力系统的在线学习问题，防止过拟合，并减少不必要的模型更新。

Method: 结合深度特征学习和提升空间中的多步预测一致性，使用符合性机制动态校准预测误差阈值，仅在误差超过阈值时更新Koopman算子和嵌入。

Result: 在基准动力系统上的实验表明，COLoKe能保持长期预测精度，同时显著减少不必要的更新并避免过拟合。

Conclusion: COLoKe框架有效实现了非线性动力系统的自适应在线学习，在保持预测准确性的同时优化了更新效率。

Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

</details>


### [297] [Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches](https://arxiv.org/abs/2511.11638)
*Aamir Shehzad*

Main category: cs.LG

TL;DR: 本研究改进了PINN方法求解RLW方程，提出了自适应和守恒两种方法。结果显示PINN效果具有问题特异性：自适应方法在复杂非线性相互作用中表现更好，守恒方法在长期行为问题中更优。研究挑战了守恒约束总能提升PINN性能的假设。


<details>
  <summary>Details</summary>
Motivation: 标准PINN在求解正则化长波方程时产生较大误差，需要开发改进的PINN方法以获得更准确的解。

Method: 开发了两种改进的PINN方法：具有自适应损失加权的自适应方法和强制执行显式守恒定律的守恒方法。使用三个基准测试：单孤子传播、双孤子相互作用和涌潮演化。

Result: 自适应PINN在复杂非线性相互作用（如孤子碰撞）中显著优于守恒PINN和标准PINN；守恒方法在单孤子长期行为和涌潮问题中表现更好。两种方法的结果与数值解误差在O(10^-5)量级。

Conclusion: 显式强制执行守恒定律可能对高度非线性系统的优化求解有害，需要特殊训练方法。PINN可以为复杂偏微分方程系统提供无网格的准确解，但效果具有问题特异性。

Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.

</details>


### [298] [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)
*Jinqi Xiao,Cheng Luo,Lingyi Huang,Cheng Yang,Yang Sui,Huy Phan,Xiao Zang,Yibiao Ying,Zhexiang Tang,Anima Anandkumar,Bo Yuan*

Main category: cs.LG

TL;DR: EcoSpa是一种高效的结构化稀疏训练方法，通过联合评估和稀疏化耦合权重矩阵对，保持其交互模式，解决了现有稀疏训练方法在高稀疏度下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的高计算需求带来系统挑战，现有稀疏训练方法未能保持权重矩阵之间的关键结构关系，导致在高稀疏度下性能下降。

Method: EcoSpa通过联合评估和稀疏化耦合权重矩阵对，采用对齐的行/列移除来保持交互模式，引入新的粒度来校准结构组件重要性，并在预训练和微调场景中进行耦合估计和稀疏化。

Result: EcoSpa在LLaMA-1B上实现50%内存减少和21%训练加速，在GPT-2-Medium上实现2.2倍模型压缩和2.4倍困惑度降低，提供1.6倍推理加速。

Conclusion: EcoSpa使用标准PyTorch操作，无需定制硬件或内核，使高效Transformer训练在商用硬件上成为可能。

Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

</details>


### [299] [On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples](https://arxiv.org/abs/2511.12881)
*Cheongjae Jang,Jonghyun Won,Soyeon Jun,Chun Kee Chung,Keehyoung Joo,Yung-Kyun Noh*

Main category: cs.LG

TL;DR: 本文分析了有限样本下一维Wasserstein距离的信息处理能力，证明其能够捕捉点态密度差异，并将这种信息与支撑集差异相协调。


<details>
  <summary>Details</summary>
Motivation: 当两个密度函数的支撑集显著重叠但密度在点态上存在显著差异时，尚不清楚Wasserstein距离是否以及如何准确识别这些差异，特别是在有限样本设置中。

Method: 利用泊松过程并分离速率因子，分析一维Wasserstein距离在有限样本下的信息处理能力。

Result: 结果表明一维Wasserstein距离能够突出与速率和支撑集相关的有意义的密度差异，这一特性在神经脉冲序列解码和氨基酸接触频率数据中得到验证。

Conclusion: 一维Wasserstein距离能够有效捕捉点态密度差异，并将这些信息与支撑集差异相协调，为有限样本分析提供了理论支持。

Abstract: Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.

</details>


### [300] [A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products](https://arxiv.org/abs/2511.11646)
*Li Yinxing,Tsukasa Ishigaki*

Main category: cs.LG

TL;DR: 提出了一种基于条件表格变分自编码器(CTVAE)的深度学习方法，用于预测新产品线扩展时的消费者属性变化，帮助营销人员制定有效的产品线营销策略。


<details>
  <summary>Details</summary>
Motivation: 产品线扩展是重要的营销策略，但过度扩展会破坏品牌形象。需要基于消费者需求进行适当的扩展，因此需要预测新产品对消费者属性的影响。

Method: 使用条件表格变分自编码器(CTVAE)从大规模消费者和产品表格数据中生成合成数据，预测新产品线扩展时的消费者属性变化。

Result: 实验结果表明CTVAE相比现有模型具有更优越的预测性能，能够为改变容器或口味的新产品提供有效的营销启示。

Conclusion: 该方法有助于避免产品自相残杀，并为产品形象设计和营销策略制定提供支持，具有重要的实际应用价值。

Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.

</details>


### [301] [Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information](https://arxiv.org/abs/2511.13049)
*Antoine Ledent,Mun Chong Soo,Nong Minh Hieu*

Main category: cs.LG

TL;DR: 该论文研究了在推荐系统场景下的矩阵补全问题，其中真实评分矩阵R和未知采样分布P都是低秩矩阵且共享共同子空间。利用大量无标签数据（隐式反馈）和少量有标签数据（显式反馈），提出了结合两种反馈的矩阵补全方法。


<details>
  <summary>Details</summary>
Motivation: 受推荐系统启发，现实场景中通常有大量隐式反馈（如点击、购买）和少量显式反馈（如评分）。论文旨在研究如何有效结合这两种不同类型的数据来改进矩阵补全性能。

Method: 利用低秩子空间恢复理论和矩阵补全模型的经典泛化界限，提出了一种结合隐式和显式反馈的矩阵补全方法。通过大量无标签数据估计采样分布P，通过少量有标签数据估计真实评分矩阵R。

Result: 理论分析得到误差界限为两项之和：O~(√(nd/M))和O~(√(dr/N))，其中d是P的秩，r是R的秩。在合成和真实数据集（Douban、MovieLens）上的实验验证了方法的有效性，在显式评分稀少时优于仅使用显式反馈的基线方法。

Conclusion: 该方法为研究推荐系统中显式和隐式反馈的交互提供了一个有效的理论框架，证明了结合两种反馈可以显著提升矩阵补全性能，特别是在显式反馈稀缺的场景下。

Abstract: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.

</details>


### [302] [Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection](https://arxiv.org/abs/2511.11647)
*Dariush Salami,Ramin Hashemi,Parham Kazemi,Mikko A. Uusitalo*

Main category: cs.LG

TL;DR: 提出了一种基于迁移学习和强化学习的可持续波束选择方法，通过将环境建模为点云并计算Chamfer距离来识别结构相似环境，从而重用预训练模型，显著减少训练时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的波束选择模型在多样化环境中需要大量训练时间和计算资源，这对可扩展性和能效构成重大挑战。

Method: 将环境建模为点云，每个点代表gNodeB和周围散射体的位置，通过计算点云之间的Chamfer距离来识别结构相似的环境，并利用迁移学习重用预训练模型。

Result: 实现了16倍的训练时间和计算开销减少，同时保持高性能，显著降低了能耗和碳排放。

Conclusion: 该方法证明了迁移学习能够实现可扩展、自适应且环保的RL波束选择策略，支持绿色可持续AI在无线系统中的发展。

Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.

</details>


### [303] [Laplace Learning in Wasserstein Space](https://arxiv.org/abs/2511.13229)
*Mary Chriselda Antony Oliver,Michael Roberts,Carola-Bibiane Schönlieb,Matthew Thorpe*

Main category: cs.LG

TL;DR: 该论文将图半监督学习从有限维欧几里得空间扩展到无限维Wasserstein空间，证明了离散图p-Dirichlet能量到其连续对应物的变分收敛，并刻画了Wasserstein空间子流形上的Laplace-Beltrami算子。


<details>
  <summary>Details</summary>
Motivation: 基于流形假设，研究图半监督学习方法在Wasserstein空间中的扩展，将经典算法从有限维欧几里得空间推广到无限维设置。

Method: 通过证明离散图p-Dirichlet能量到连续对应物的变分收敛，并刻画Wasserstein空间子流形上的Laplace-Beltrami算子，扩展Laplace Learning到Wasserstein空间。

Result: 在基准数据集上的数值实验验证了所提出的理论框架，证明了在高维设置下分类性能的一致性。

Conclusion: 成功将图半监督学习扩展到Wasserstein空间，为高维数据分析提供了新的理论框架和有效方法。

Abstract: The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning
  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical
  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to
  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator
  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through
  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.

</details>


### [304] [Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning](https://arxiv.org/abs/2511.11648)
*Shunyu Wu,Tianyue Li,Yixuan Leng,Jingyi Suo,Jian Lou,Dan Li,See-Kiong Ng*

Main category: cs.LG

TL;DR: 提出LTSV方法，通过上下文微调来评估时间序列数据对基础模型的价值，解决了传统方法计算量大且无法保持时间依赖性的问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型性能高度依赖数据质量，但传统数据估值方法存在计算瓶颈且无法保持时间依赖性。

Method: 基于理论证据，通过上下文微调近似影响函数，利用时间块聚合捕获时间依赖性，测量上下文损失变化来评估样本贡献。

Result: 在多个时间序列数据集和模型上的实验表明，LTSV能提供可靠且强大的估值性能，同时保持可控的计算需求。

Conclusion: 在时间序列基础模型上进行上下文微调为时间序列学习中的数据归因和模型泛化提供了实用有效的桥梁。

Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.

</details>


### [305] [Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification](https://arxiv.org/abs/2511.13237)
*Alan G. Paredes Cetina,Kaouther Benguessoum,Raoni Lourenço,Sylvain Kubler*

Main category: cs.LG

TL;DR: CONFETTI是一种新颖的多目标反事实解释方法，专门针对多元时间序列数据，通过平衡预测置信度、邻近性和稀疏性来提供可操作的解释。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在多元时间序列分类和回归中表现出色，但缺乏透明度阻碍了决策制定。现有的可解释AI方法只能提供部分见解，而反事实解释方法通常只优先考虑准确性、邻近性或稀疏性中的单一目标，限制了其实际价值。

Method: CONFETTI通过识别关键时间序列子序列、定位反事实目标，并最优地修改时间序列来平衡预测置信度、邻近性和稀疏性。该方法在UEA档案中的七个多元时间序列数据集上进行了评估。

Result: CONFETTI在优化目标上持续优于最先进的反事实解释方法，并在文献中的六个其他指标上表现优异，实现了≥10%的更高置信度，同时在≥40%的情况下改善了稀疏性。

Conclusion: CONFETTI通过最小化变化提供可操作的见解，提高了可解释性和决策支持能力，在各种领域都证明了其有效性。

Abstract: Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.

</details>


### [306] [Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine](https://arxiv.org/abs/2511.11650)
*Daniele Ugo Leonzio,Paolo Bestagini,Marco Marcon,Stefano Tubaro*

Main category: cs.LG

TL;DR: 提出了一种基于水压测量和单类SVM的漏水检测方法，该方法仅使用无泄漏时的压力数据和管网拓扑信息，在模拟数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于水管网络每年因漏水造成大量水资源损失，需要可靠有效的漏水检测和定位系统。数据驱动方法因其优越性能而受到越来越多关注。

Method: 基于水管网络节点水压测量，使用特征提取器和在无泄漏数据上训练的单类支持向量机，将泄漏检测为异常。

Result: 在Modena水管网络模拟数据集上的结果表明，所提出的解决方案在漏水检测方面优于最近的方法。

Conclusion: 该完全数据驱动的解决方案仅使用水管网络拓扑和无泄漏压力数据，就能有效检测漏水，性能优于现有方法。

Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.

</details>


### [307] [Fast and Robust Simulation-Based Inference With Optimization Monte Carlo](https://arxiv.org/abs/2511.13394)
*Vasilis Gkolemis,Christos Diou,Michael Gutmann*

Main category: cs.LG

TL;DR: 提出一种针对可微分模拟器的贝叶斯参数推断新方法，通过将随机模拟重新表述为确定性优化问题，结合梯度方法高效定位高密度后验区域，显著减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 复杂随机模拟器的贝叶斯参数推断面临似然函数难以处理的问题，现有基于模拟的推断方法在参数空间高维或输出部分无信息时计算成本高昂。

Method: 基于优化蒙特卡洛框架，将随机模拟重新表述为确定性优化问题，应用梯度方法高效导航至高密度后验区域，避免在低概率区域进行浪费性模拟，并通过JAX实现关键组件向量化提升性能。

Result: 在高维参数空间、无信息输出、多观测值和多峰后验等广泛实验中，该方法始终匹配并经常超越最先进方法的精度，同时显著减少运行时间。

Conclusion: 该方法为可微分模拟器提供了一种高效准确的贝叶斯参数推断解决方案，在保持精度的同时大幅提升计算效率。

Abstract: Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.

</details>


### [308] [Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks](https://arxiv.org/abs/2511.12265)
*Rui Wang,Zeming Wei,Xiyue Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 提出了一种名为校准对抗采样（CAS）的高效微调方法，通过多臂老虎机框架动态设计奖励并平衡探索与利用，以提升深度神经网络对多种攻击类型的整体鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练框架主要关注单一或有限攻击类型，导致深度神经网络在遇到训练中未处理的攻击类型时仍然脆弱，存在安全隐患。

Method: 基于多臂老虎机优化框架，动态设计奖励函数并平衡探索与利用，考虑多个鲁棒性维度的动态和相互依赖特性。

Result: 在基准数据集上的实验表明，CAS实现了优越的整体鲁棒性，同时保持了较高的干净准确率。

Conclusion: CAS为深度神经网络的鲁棒泛化提供了新的范式，能够有效应对多种攻击类型。

Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.

</details>


### [309] [Incomplete Depression Feature Selection with Missing EEG Channels](https://arxiv.org/abs/2511.11651)
*Zhijian Gong,Wenjia Dong,Xueyuan Xu,Fulin Wei,Chunyu Liu,Li Zhuo*

Main category: cs.LG

TL;DR: 提出了一种名为IDFS-MEC的新型特征选择方法，用于处理EEG数据中的通道缺失问题，并通过全局冗余最小化来减少特征子集中的冗余信息，在抑郁分析中表现出优于10种流行特征选择方法的性能。


<details>
  <summary>Details</summary>
Motivation: EEG特征通常包含冗余、不相关和噪声信息，且现实世界EEG数据采集经常面临电极脱落导致数据丢失和严重噪声干扰等挑战，需要开发能够处理不完整通道的鲁棒抑郁分析方法。

Method: IDFS-MEC方法整合了缺失通道指示信息和自适应通道权重学习到正交回归中，以减轻不完整通道对模型构建的影响，然后利用全局冗余最小化学习来减少所选特征子集中的冗余信息。

Result: 在MODMA和PRED-d003数据集上的广泛实验表明，IDFS-MEC选择的EEG特征子集在3、64和128通道设置下均优于10种流行特征选择方法。

Conclusion: IDFS-MEC方法能够有效处理EEG数据中的通道缺失问题，并选择出具有优越性能的特征子集，为基于EEG的抑郁分析提供了鲁棒的解决方案。

Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.

</details>


### [310] [Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression](https://arxiv.org/abs/2511.13421)
*Tingkai Yan,Haodong Wen,Binghui Li,Kairong Luo,Wenguang Chen,Kaifeng Lyu*

Main category: cs.LG

TL;DR: 本文分析了在有限数据和多次训练周期下，数据缩放定律的变化形式，特别是研究了多轮训练对线性回归中数据缩放定律的影响，定义了有效重复率E(K,N)来衡量单次训练所需的数据量增长倍数。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型的数据缩放定律在一次性训练大规模语料库的情况下已被广泛研究，但在有限数据和多次训练周期下的形式仍未被充分探索。本文旨在填补这一空白，分析多轮训练如何重塑数据缩放定律。

Method: 在线性回归框架下，使用随机梯度下降(SGD)方法，分别在强凸性或Zipf分布数据条件下，理论分析了有效重复率E(K,N)的缩放行为。

Result: 研究发现：(1)当K较小时，E(K,N)≈K，表明每个新训练周期带来线性增益；(2)随着K增加，E(K,N)会趋于一个与问题相关的平台值，该值随N增长而增长，意味着更大的数据集可以在边际效益消失前被重复更多次。

Conclusion: 研究结果揭示了数据大小和分布对有效重复率的影响，强调了在未来研究数据重复的缩放定律时需要明确建模这两个因素的重要性，并对最近的经验研究提出了修正。

Abstract: While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.

</details>


### [311] [How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity](https://arxiv.org/abs/2511.11652)
*Marvin Plein,Carsten F. Dormann,Andreas Christen*

Main category: cs.LG

TL;DR: 提出了一个逐步移除气象站的方法来精简弗莱堡的城市气象站网络，证明在保持高预测精度的前提下，可以将气象站数量从42个大幅减少到4个。


<details>
  <summary>Details</summary>
Motivation: 城市气象站网络维护成本高昂且劳动密集，需要找到在保持监测能力的同时减少站点数量的方法。

Method: 采用逐步移除站点的程序，分析不同子集网络重建原始网络空气温度和湿度模式的能力，并与先进的数值城市地表模型进行比较。

Result: 站点数量从42个减少到4个时，空气温度预测RMSE从0.69K增加到0.83K，相对湿度从3.8%增加到4.4%，分别仅增加20%和16%。森林站点的预测精度较差，但始终优于数值模型。

Conclusion: 研究表明精简气象站网络具有巨大潜力，可以最大化城市气候研究中财务和人力资源的分配效率。

Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.

</details>


### [312] [AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate](https://arxiv.org/abs/2511.13465)
*Meng Zhu,Quan Xiao,Weidong Min*

Main category: cs.LG

TL;DR: 提出了AdamX优化算法，通过新型二阶矩估计指数衰减率，在训练后期减弱学习步长修正强度并退化为SGD，提升稳定期训练稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: Adam优化器在大语言模型时代仍是主流，但与SGD相比更容易收敛到非平坦极小值，需要解决这个问题。

Method: 提出AdamX算法，核心创新是新型二阶矩估计指数衰减率，随着训练进展逐渐减弱学习步长修正强度，在稳定训练期退化为SGD。

Result: 实验结果表明，新的二阶矩估计指数衰减率优于现有方法，AdamX在性能上稳定优于Adam及其变体。

Conclusion: AdamX通过改进二阶矩估计机制，有效提升了训练稳定性和可能的泛化能力，在大模型优化中具有优势。

Abstract: Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.

</details>


### [313] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文对多智能体强化学习在交通信号控制中的收敛性进行了理论分析，证明了在给定条件下该算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 随着班加罗尔等城市的快速城市化，交通拥堵问题日益严重。虽然已有研究通过实证证明了多智能体强化学习在交通信号控制中的有效性，但缺乏对其稳定性和收敛性的严格理论分析。

Method: 使用随机逼近方法，正式分析了多智能体强化学习算法的学习动态，将单智能体异步值迭代的收敛性证明扩展到多智能体场景。

Result: 证明了特定的多智能体强化学习交通控制算法在给定条件下能够收敛。

Conclusion: 本文填补了多智能体强化学习在交通控制领域理论分析的空白，为算法的稳定性提供了理论保证。

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [314] [Scientific Data Compression and Super-Resolution Sampling](https://arxiv.org/abs/2511.13675)
*Minh Vu,Andrey Lokhov*

Main category: cs.LG

TL;DR: 提出基于学习指数族的新科学数据压缩与超分辨率框架，支持物理量不确定性量化，在压缩比和重建保真度间灵活权衡


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟、观测和大规模实验产生的数据量常超出存储、处理和分析能力极限，需开发能管理海量数据集同时保留关键物理特征的数据缩减方法

Method: 基于学习指数族的新框架，支持科学数据压缩和超分辨率重建，可量化物理量的不确定性

Result: 该方法支持从压缩表示中恢复数据（超分辨率），并保证关键物理特征的保留

Conclusion: 该框架为科学数据压缩和恢复提供了新方法，特别适用于检查点和重启等需要数据恢复的科学工作流

Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.

</details>


### [315] [Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data](https://arxiv.org/abs/2511.12788)
*Rubén Darío Guerrero*

Main category: cs.LG

TL;DR: 提出一种物理约束自适应学习框架，通过可学习参数自动校准电磁近似，在极紫外光刻优化中实现亚纳米精度，仅需少量训练样本即可实现跨几何泛化。


<details>
  <summary>Details</summary>
Motivation: 半导体行业在极紫外光刻优化中面临计算危机，传统方法消耗数十亿CPU小时且无法达到亚纳米精度，需要解决学术物理信息神经网络与工业部署需求之间的关键差距。

Method: 集成可微分模块（菲涅尔衍射、材料吸收、光学点扩散函数模糊、相移效应、对比度调制）与直接几何图案匹配目标，通过物理约束学习自动校准电磁近似参数。

Result: 在15个代表性图案上实现一致的亚纳米边缘放置误差性能（0.664-2.536 nm范围），仅需每个图案50个训练样本，比无物理约束的CNN基线平均改进69.9%，训练完成后推理速度显著快于严格电磁求解器。

Conclusion: 物理约束自适应学习为实时半导体制造优化建立了基础方法学，通过联合物理校准和制造精度目标，仅需模式特定CNN训练方法90%的训练样本即可实现跨几何泛化。

Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.

</details>


### [316] [On the Probabilistic Learnability of Compact Neural Network Preimage Bounds](https://arxiv.org/abs/2511.11656)
*Luca Marzari,Manuele Bicego,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 提出RF-ProVe方法，使用随机森林和主动重采样来高效计算神经网络预像的近似边界，提供统计保证


<details>
  <summary>Details</summary>
Motivation: 现有可证明方法受限于#P难问题，难以扩展到大规模神经网络，需要开发具有高置信度保证的实用方法

Method: 使用随机森林集成方法生成满足输出属性的候选输入区域，并通过主动重采样进行精炼

Result: 方法能够捕获高维空间中的复杂模式，提供区域纯度和全局覆盖的统计保证

Conclusion: RF-ProVe为计算紧凑预像近似提供了实用、可扩展的解决方案，在精确求解器无法扩展的情况下特别有效

Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.

</details>


### [317] [Efficient Calibration for Decision Making](https://arxiv.org/abs/2511.13699)
*Parikshit Gopalan,Konstantinos Stavropoulos,Kunal Talwar,Pranay Tankala*

Main category: cs.LG

TL;DR: 本文提出了一种基于结构化后处理函数族的校准决策损失(CDL_K)概念，以解决原始CDL难以近似计算的问题，并建立了该度量的理论和计算可处理性分析。


<details>
  <summary>Details</summary>
Motivation: 原始校准决策损失(CDL)在离线设置中难以近似计算，即使给定预测和标签的黑盒访问。为了规避这一困难，需要将注意力限制在结构化的后处理函数族上。

Method: 定义相对结构化函数族K的校准决策损失CDL_K，考虑所有适当损失但将后处理限制在结构化族K中。开发了CDL_K在信息论和计算上的可处理性理论。

Result: 为自然类别K证明了上下界，建立了CDL_K的可处理性条件，并为机器学习中广泛使用的重新校准程序提供了严格保证。

Conclusion: 通过引入新的定义和算法技术，为决策制定中的校准理论提供了理论支撑，并为实际应用中的重新校准方法提供了理论保证。

Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.

</details>


### [318] [SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization](https://arxiv.org/abs/2511.11663)
*Zhixiong Zhao,Fangxin Liu,Junjie Wang,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: SpecQuant是一个从傅里叶频域角度出发的极端LLM压缩框架，通过两阶段方法实现权重和激活值的4位量化，在LLaMA-3 8B上仅损失1.5%精度，同时实现2倍推理加速和3倍内存节省。


<details>
  <summary>Details</summary>
Motivation: 随着准确开源大语言模型的出现，需要先进的量化技术来在终端设备上高效部署。本文从傅里叶频域角度重新审视极端LLM压缩的挑战，目标是实现权重和激活值的超低位量化。

Method: 提出SpecQuant两阶段框架：第一阶段平滑激活值异常值并将其转移到权重矩阵；第二阶段应用通道级低频傅里叶截断来抑制高频分量，同时保留基本信号能量。还引入了轻量级截断模块实现运行时适应性。

Result: 在LLaMA-3 8B上，SpecQuant实现了权重和激活值的4位量化，零样本准确率与全精度相比仅差1.5%，同时提供2倍推理加速和3倍内存使用降低。

Conclusion: 从傅里叶频域角度出发的SpecQuant框架有效解决了极端LLM压缩问题，证明了权重能量主要集中在低频分量，可以以最小精度影响保留这些分量，实现高效的模型部署。

Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.

</details>


### [319] [Learning Branching Policies for MILPs with Proximal Policy Optimization](https://arxiv.org/abs/2511.12986)
*Abdelouahed Ben Mhamed,Assia Kamal-Idrissi,Amal El Fallah Seghrouchni*

Main category: cs.LG

TL;DR: 提出了TGPPO框架，使用PPO强化学习算法训练分支策略，旨在提高混合整数线性规划中分支定界算法在不同实例间的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于模仿学习的分支策略容易过拟合专家演示，难以泛化到结构多样或未见过的实例，需要更鲁棒的方法。

Method: 使用PPO强化学习算法，构建参数化状态空间表示来动态捕捉搜索树的演化上下文，训练分支策略。

Result: TGPPO在减少探索节点数和改进p-原始对偶积分方面优于现有学习方法，特别是在分布外实例上表现突出。

Conclusion: 强化学习有潜力为MILP求解器开发鲁棒且适应性强的分支策略。

Abstract: Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.

</details>


### [320] [Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE](https://arxiv.org/abs/2511.11665)
*Sameeksha Sriram,Ayush Paliwal,Alexander S. Ecker,Chase van de Geijn*

Main category: cs.LG

TL;DR: 提出了基于四元数的旋转位置嵌入(QuatRo)和基于克利福德代数的旋转位置嵌入(CARE)，将旋转位置嵌入从二维扩展到高维，同时保持移位等变性。


<details>
  <summary>Details</summary>
Motivation: 现有的高维旋转位置嵌入方法(如Spherical RoPE)是非交换的，失去了RoPE的移位等变性特性。需要找到既能扩展到高维又能保持这一重要特性的方法。

Method: 使用四元数表示3D旋转来参数化旋转轴，提出了QuatRo方法。进一步扩展到克利福德代数，使用几何代数中的转子作用于多向量，实现任意维度的旋转位置嵌入。

Result: 证明了Mixed RoPE和Spherical RoPE是QuatRo的特例。提出的CARE方法能够扩展到任意维度，并在多向量中编码位置信息。

Conclusion: 四元数和克利福德代数方法为旋转位置嵌入提供了数学上优雅的扩展框架，既能处理高维输入，又能保持重要的移位等变性特性。

Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.

</details>


### [321] [Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL](https://arxiv.org/abs/2511.11696)
*Xun Shao,Aoba Otani,Yuto Hirasuka,Runji Cai,Seng W. Loke*

Main category: cs.LG

TL;DR: 本文提出了从跌倒检测扩展到全面日常生活活动(ADL)识别的新一代老年人监护系统，采用隐私保护、边缘部署和联邦学习技术，旨在支持老龄化社会的独立性和尊严。


<details>
  <summary>Details</summary>
Motivation: 当前老年人监护系统主要关注跌倒检测，但需要向更全面的日常生活活动识别发展，以更好地支持老年人的独立生活和尊严。

Method: 使用SISFall数据集及其GAN增强变体进行可行性验证，将跌倒检测作为代理任务，在非IID条件下进行联邦学习实验，并在Jetson Orin Nano设备上实现嵌入式部署。

Result: 展示了在非IID联邦学习条件下的初步结果，验证了在边缘设备上部署的可行性，为全面ADL监测提供了早期证据。

Conclusion: 这项工作标志着从单任务检测向全面日常活动识别的转变，为可持续和以人为本的老年人护理AI提供了路线图，同时指出了领域偏移、数据稀缺和隐私风险等开放挑战。

Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.

</details>


### [322] [Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion](https://arxiv.org/abs/2511.11667)
*Feng Guo,Yuntao Wen,Shen Gao,Junshuo Zhang,Shuo Shang*

Main category: cs.LG

TL;DR: KUnBR是一种基于知识密度引导的机器遗忘方法，通过块重插入策略有效消除LLM中的有害知识，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法难以彻底移除有害知识，残留知识容易被恢复，需要解决隐私、合规和伦理问题。

Method: 首先通过知识密度估计识别富含有害知识的层，然后采用层重插入策略绕过梯度障碍，确保有效梯度传播。

Result: 在多个遗忘和通用能力基准测试中，KUnBR实现了最先进的遗忘性能，同时保持了模型效用。

Conclusion: KUnBR通过精确定位和有效消除有害知识，为LLM的安全部署提供了可靠解决方案。

Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.

</details>


### [323] [Learning Fair Representations with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.11767)
*Amisha Priyadarshini,Sergio Gago-Masague*

Main category: cs.LG

TL;DR: 本文提出了一种将Kolmogorov-Arnold网络（KANs）集成到公平对抗学习框架中的方法，通过自适应惩罚更新机制动态调整公平约束，在保持高预测准确性的同时实现跨敏感属性的竞争性公平。


<details>
  <summary>Details</summary>
Motivation: 现有公平学习模型在公平性与准确性之间的权衡仍面临挑战，且黑盒模型缺乏可解释性，限制了在社会敏感领域的应用。本文旨在解决这些问题。

Method: 将KANs集成到公平对抗学习框架中，利用KANs的对抗鲁棒性和可解释性，并提出了自适应惩罚更新机制来动态调整训练过程中的公平约束。

Result: 在两个真实世界大学录取数据集上的实验表明，该方法在三种不同优化策略下始终优于基线公平学习模型，在保持高预测准确性的同时实现了跨敏感属性的竞争性公平。

Conclusion: KANs在公平机器学习中展现出高效性和鲁棒性，能够平衡公平性与准确性，同时提供更好的可解释性，适用于社会敏感决策领域。

Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.

</details>


### [324] [Do traveling waves make good positional encodings?](https://arxiv.org/abs/2511.11668)
*Chase van de Geijn,Ayush Paliwal,Timo Lüddecke,Alexander S. Ecker*

Main category: cs.LG

TL;DR: RollPE是一种基于行波的新型位置编码机制，通过循环滚动操作在自注意力中实现相对位置编码，性能优于传统绝对位置编码，与RoPE相当。


<details>
  <summary>Details</summary>
Motivation: 传统位置编码方法存在局限性，需要更好的相对位置编码机制来捕捉平移等变性。

Method: 在自注意力中对查询和键张量应用循环滚动操作，通过位置间的相位偏移计算基于位置差异的注意力。

Result: RollPE显著优于传统绝对位置嵌入，与RoPE性能相当，并揭示了与大脑信息流动过程的潜在联系。

Conclusion: RollPE提供了一种简化的相对位置编码方法，为理解RoPE和大脑信息处理机制提供了新视角。

Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.

</details>


### [325] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: 提出了一种能够根据输入数据动态调整内部结构的神经网络架构，通过路由机制实现迭代和自适应计算，这是一个概念性原型而非性能优化的模型。


<details>
  <summary>Details</summary>
Motivation: 探索可适应且可能更可解释的网络架构，让系统不仅能学习表示，还能学习计算结构本身，受思维过程和动态推理的启发。

Method: 引入路由机制，允许每层影响其输出在网络中的传播方式，实现基于数据和内部状态的条件化信息流。

Result: 由于计算资源和数据限制，这是初步研究，但初步观察显示有潜力，完整评估需要在更有利的计算条件下进行。

Conclusion: 提出了一个概念性架构框架，为探索自适应网络开辟了新方向，重点在于架构创新而非性能竞争。

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [326] [Evaluation of LLM-based Explanations for a Learning Analytics Dashboard](https://arxiv.org/abs/2511.11671)
*Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: 使用大型语言模型为学习分析仪表板生成数据解释，相比独立仪表板和教师解释更受青睐，能提升学习体验并保持教学标准。


<details>
  <summary>Details</summary>
Motivation: 学习分析仪表板支持自主学习，但数据可解释性影响其效果，需要辅助解释来帮助理解。

Method: 使用大型语言模型生成仪表板数据的文字解释，并与独立仪表板和教师解释进行对比评估，通过专家研究（N=12）进行验证。

Result: LLM生成的技能状态解释和学习建议显著更受青睐，优于其他条件。

Conclusion: 使用LLM进行数据解释可以增强学习体验，同时保持教师认可的教学标准。

Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.

</details>


### [327] [Fast 3D Surrogate Modeling for Data Center Thermal Management](https://arxiv.org/abs/2511.11722)
*Soumyendu Sarkar,Antonio Guillen-Perez,Zachariah J Carmichael,Avisek Naug,Refik Mert Cam,Vineet Gundecha,Ashwin Ramesh Babu,Sahand Ghorbanpour,Ricardo Luna Gutierrez*

Main category: cs.LG

TL;DR: 开发基于视觉的替代建模框架，直接在数据中心的3D体素化表示上运行，结合服务器工作负载、风扇速度和HVAC温度设定点，实现实时温度预测，比传统CFD求解器快20,000倍。


<details>
  <summary>Details</summary>
Motivation: 减少数据中心的能耗和碳排放，通过实时温度预测实现可持续性和运营效率。传统热CFD求解器计算成本高且需要专家构建网格和边界条件，不适合实时使用。

Method: 评估多种架构，包括3D CNN U-Net变体、3D傅里叶神经算子和3D视觉变换器，将热输入映射到高保真热图。使用3D体素化数据中心表示，结合服务器工作负载、风扇速度和HVAC温度设定点。

Result: 替代模型在不同数据中心配置中具有泛化能力，实现高达20,000倍的加速（数百毫秒vs数小时）。能够快速准确估计热点和温度分布。

Conclusion: 快速准确的温度估计支持实时冷却控制和负载重新分配，实现显著的节能（7%）和碳足迹减少。

Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.

</details>


### [328] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: 本文对无监督和半监督计算文本理想点估计算法进行了首次系统性综述，分析了从词频模型到大型语言模型的四类方法，提出了比较框架和实用指导。


<details>
  <summary>Details</summary>
Motivation: 计算文本理想点估计算法在政治学等领域广泛应用，但方法发展碎片化，缺乏系统比较和应用指导，需要统一框架来整合不同方法。

Method: 通过系统文献回顾识别25种算法，进行手动内容分析，提出区分文本方差生成、捕获和聚合的概念框架，将方法分为词频、主题建模、词嵌入和LLM四类。

Result: 建立了四类方法家族的系统分类，揭示了不同方法在假设、可解释性、可扩展性方面的差异，强调了算法选择中的权衡关系。

Conclusion: 算法差异本身具有信息价值，需要系统基准测试；为应用研究者提供了透明性、技术要求和验证策略方面的实用指导。

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [329] [Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture](https://arxiv.org/abs/2511.11673)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 提出了一种协同融合层（SFL）架构，通过门控机制将Sentence-BERT深度语义特征与低维结构特征融合，在歌词内容分类任务中实现了高准确率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决如何将复杂高维深度语义特征与简单可解释的结构特征有效融合，以提升歌词内容分类性能的问题。

Method: 使用门控机制的协同融合层（SFL）架构，调制Sentence-BERT嵌入与低维辅助特征，将UMAP降维后的歌词嵌入聚类任务重构为二分类问题。

Result: SFL模型准确率达0.9894，Macro F1分数0.9894，比随机森林基线表现更好，且校准误差降低93%，对数损失降低2.5倍。

Conclusion: 非线性门控机制优于简单特征拼接，SFL模型为复杂多模态歌词分析提供了鲁棒可靠的方法。

Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.

</details>


### [330] [Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion](https://arxiv.org/abs/2511.12139)
*Sahar Moghimian Hoosh,Ilia Kamyshev,Henni Ouerdane*

Main category: cs.LG

TL;DR: 提出了一种基于ICA和PCA特征融合的轻量级神经网络框架，用于非侵入式负荷监测的多标签分类任务，在保持高精度的同时显著减少了训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 解决实际NILM部署中面临的过拟合、模型泛化能力差以及同时运行大量电器时的分解困难等挑战。

Method: 构建端到端框架，包含高频标记数据、特征提取方法和轻量级神经网络。提出融合ICA和PCA的特征提取方法，以及轻量级多标签分类架构Fusion-ResNet。

Result: 相比现有最优NILM分类器，提出的基于特征的模型平均F1分数更高，同时最小化训练和推理时间。在多达15个同时运行电器的情况下仍保持相对稳健的性能。

Conclusion: Fusion-ResNet框架在NILM分类任务中表现出色，具有高精度、高效率和对多电器同时运行的鲁棒性。

Abstract: Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.

</details>


### [331] [Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff](https://arxiv.org/abs/2511.11675)
*Junchen Liu,Yi Sheng*

Main category: cs.LG

TL;DR: 提出双向剪枝-再生策略，从极端压缩网络开始选择性再生关键连接，解决高稀疏度下模型性能急剧下降的问题


<details>
  <summary>Details</summary>
Motivation: 当稀疏度超过特定阈值时，迭代和一次性剪枝方法都会导致模型性能急剧下降，限制了可实现的压缩比，无法满足某些硬件平台的严格尺寸约束

Method: 双向剪枝-再生策略：从满足硬件约束的极端压缩网络开始，选择性再生关键连接以恢复丢失的性能

Result: 有效缓解了高稀疏度条件下常见的精度急剧下降问题

Conclusion: 该方法能够突破传统剪枝方法的压缩限制，使模型能够在满足硬件尺寸约束的同时保持可接受的性能

Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.

</details>


### [332] [Fairness-Aware Graph Representation Learning with Limited Demographic Information](https://arxiv.org/abs/2511.13540)
*Zichong Wang,Zhipeng Yin,Liping Yang,Jun Zhuang,Rui Yu,Qingzhao Kong,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出了FairGLite框架，在有限人口统计信息下实现公平图学习，通过生成人口统计代理、强制跨组嵌入一致性和自适应置信策略来缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平图学习方法大多假设能完全访问人口统计信息，但这在实践中很少能满足，因为存在隐私、法律或监管限制。

Method: 使用部分人口统计数据生成人口统计信息代理，设计跨人口统计组节点嵌入一致性策略，并开发基于预测置信度的自适应置信策略来动态调整节点对公平性和效用的贡献。

Result: 理论分析证明FairGLite在群体公平指标上达到可证明的上界，实验表明该框架在多个数据集和公平图学习框架中能有效缓解偏见并保持模型效用。

Conclusion: FairGLite为有限人口统计信息下的公平图学习提供了有效解决方案，具有理论保证和实际应用价值。

Abstract: Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.

</details>


### [333] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: 提出了Learning with Preserving (LwP)框架，通过保持共享表示空间的几何结构来解决持续多任务学习中的灾难性遗忘问题，无需重放缓冲区。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、医疗影像分析等关键领域，AI系统需要持续学习新任务而不忘记已学能力。现有持续学习方法在此场景下表现不佳，因为它们学习的是碎片化的任务特定特征，容易相互干扰。

Method: 引入LwP框架和动态加权距离保持(DWDP)损失，通过正则化潜在数据表示之间的成对距离来防止表示漂移，保持共享表示空间的几何结构。

Result: 在时间序列和图像基准测试中，LwP不仅缓解了灾难性遗忘，还持续优于最先进的基线方法，是唯一超越强单任务学习基线的方法。

Conclusion: LwP方法在持续多任务学习场景中表现出色，对分布偏移具有强鲁棒性，适用于现实世界的动态环境。

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [334] [SCI: An Equilibrium for Signal Intelligence](https://arxiv.org/abs/2511.12240)
*Vishal Joshua Meesala*

Main category: cs.LG

TL;DR: SCI是一个闭环控制理论框架，将可解释性建模为受控状态，通过Lyapunov引导的控制器主动驱动解释精度，在多个领域显著降低解释误差并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统静态解释器在动态环境中表现不稳定，需要一种能够主动调节解释精度并保持模型性能的框架。

Method: SCI框架包含三个组件：可靠性加权的多尺度特征、知识引导的解释器、以及配备回滚和信任区域保护的Lyapunov引导控制器。

Result: 在生物医学、工业和环境领域，SCI将解释误差降低25-42%，解释稳定性显著提高（方差从0.030降至0.011），同时保持AUC/F1在基线1-2个百分点内。

Conclusion: 将可解释性建模为控制目标能够在不同信号机制下产生更稳定、恢复更快且更可信的解释行为。

Abstract: We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] ("Surgical Precision") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.

</details>


### [335] [Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow](https://arxiv.org/abs/2511.11677)
*Shimiao Li,Aaron Tuor,Draguna Vrabie,Larry Pileggi,Jan Drgona*

Main category: cs.LG

TL;DR: 提出了一种基于同伦引导的自监督学习方法，用于解决参数化交流最优潮流问题，通过构建从松弛问题到原始问题的连续变形过程，提高收敛稳定性和可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的学习优化方法在处理非凸的AC-OPF问题时，往往难以收敛到可行且高质量的解决方案，需要一种能够提高收敛稳定性和可行性的新方法。

Method: 采用同伦引导的自监督学习框架，从具有广泛吸引域的松弛问题开始，逐步变形到原始问题，无需标记的最优解或外部求解器。

Result: 在标准IEEE AC-OPF基准测试中，该方法相比非同伦基线显著提高了可行性率，同时获得了与完整OPF求解器相当的目标函数值。

Conclusion: 同伦启发式方法在电力系统优化中展示了可扩展、约束感知的学习优化潜力。

Abstract: Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.

</details>


### [336] [Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction](https://arxiv.org/abs/2511.12467)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: 本文研究了未知线性随机系统的在线多步预测问题，提出了基于条件分布理论的最优预测策略参数化方法，并设计了在线最小二乘算法来学习该策略，证明了相对于最优卡尔曼滤波器的对数遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究未知线性随机系统的在线多步预测问题，旨在开发能够在线学习并实现接近最优性能的预测算法，而不需要事先知道系统模型。

Method: 利用条件分布理论推导预测策略的最优参数化形式，将其表示为未来输入、过去输入和过去输出的线性函数，然后提出在线最小二乘算法来学习该策略。

Result: 在线算法在多步预测设置下相对于最优卡尔曼滤波器实现了对数遗憾界，且对于足够大的时间范围N，建立了不依赖于固定失败概率的几乎必然遗憾界。遗憾的常数因子随预测范围H多项式增长，多项式阶数由系统矩阵中特征值为1的最大Jordan块决定。

Conclusion: 提出的在线算法能够在未知线性随机系统中有效进行多步预测，实现接近最优的性能，且遗憾分析揭示了预测范围对算法性能的影响规律。

Abstract: This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.

</details>


### [337] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: 提出了SBN-Opt框架，通过神经代理网络SBN嵌入LSQC能量，优化自由边界微分同胚映射，解决了传统数值算法无法应用于梯度优化的问题。


<details>
  <summary>Details</summary>
Motivation: 自由边界微分同胚优化在表面映射问题中至关重要，但传统数值LSQC理论需要地标条件且无法用于梯度优化，限制了其应用。

Method: 提出SBN神经代理网络，将LSQC能量嵌入多尺度网格谱架构中，然后构建SBN-Opt优化框架来优化自由边界微分同胚映射。

Result: 在密度均衡映射和不一致表面配准上的广泛实验表明，SBN-Opt优于传统数值算法。

Conclusion: SBN-Opt框架成功解决了自由边界微分同胚优化问题，能够显式控制局部几何畸变，在表面映射任务中表现出优越性能。

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [338] [PID-controlled Langevin Dynamics for Faster Sampling of Generative Models](https://arxiv.org/abs/2511.12603)
*Hongyi Chen,Jianhai Shu,Jingtao Ding,Yong Li,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: PIDLD是一种基于控制理论的Langevin动力学采样加速算法，通过结合历史梯度（积分项）和梯度趋势（导数项）来减少采样所需的迭代次数，提高生成速度。


<details>
  <summary>Details</summary>
Motivation: Langevin动力学采样存在生成速度极低的问题，受限于需要大量细粒度迭代才能收敛到目标分布。

Method: 将采样过程重新解释为控制理论问题，将能量梯度视为反馈信号，结合PID控制器的积分项（历史梯度）和导数项（梯度趋势）来高效遍历能量景观并自适应稳定。

Result: 在图像生成和推理任务上的广泛实验表明，PIDLD能以更少的步骤达到更高的质量，使基于Langevin的生成模型在效率关键应用中更加实用。

Conclusion: PIDLD无需额外训练、数据集或先验信息，可立即与任何基于Langevin的方法集成，显著提高了采样效率。

Abstract: Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.

</details>


### [339] [Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP](https://arxiv.org/abs/2511.11680)
*Udaya Bhasker Cheerala,Varun Teja Chirukuri,Venkata Akhil Kumar Gummadi,Jintu Moni Bhuyan,Praveen Damacharla*

Main category: cs.LG

TL;DR: 本研究开发了加州野火风险地图，使用随机森林算法结合可解释AI（SHAP）分析，识别了森林和草原生态系统的关键风险驱动因素，并评估了模型的空间和时间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 野火对全球生态系统构成严重威胁，加州因气候、地形、植被和人类活动等因素频繁发生火灾。需要开发综合风险评估方法来支持决策和风险缓解策略。

Method: 应用随机森林算法结合SHAP可解释AI方法，采用空间和时间验证策略评估模型性能，识别生态系统特定的风险驱动因素。

Result: RF模型表现出强大的预测性能，森林和草原的AUC分别达到0.997和0.996。空间交叉验证显示中等可转移性，时间验证显示更好的泛化能力。SHAP分析识别出土壤有机碳、树木覆盖和NDVI是森林的关键驱动因素，而地表温度、海拔和植被健康指数主导草原风险。

Conclusion: RF-SHAP框架为野火风险评估提供了稳健、可理解和适应性强的方法，能够支持知情决策和制定有针对性的风险缓解策略。

Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.

</details>


### [340] [From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability](https://arxiv.org/abs/2511.12852)
*Jihoon Moon*

Main category: cs.LG

TL;DR: 提出基于控制理论的框架，将训练好的神经网络视为非线性状态空间系统，通过局部线性化、可控性和可观性格拉姆矩阵以及汉克尔奇异值来分析其内部计算。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然性能优异但难以进行机械解释，需要一种系统性的方法来分析其内部计算机制。

Method: 对给定输入，在隐藏激活模式周围线性化网络，构建状态空间模型，通过输入状态和状态输出雅可比矩阵定义局部可控性和可观性格拉姆矩阵，计算汉克尔奇异值和相关模式。

Result: 在简单前馈网络上验证了该框架，展示了激活饱和如何降低可控性、缩小主导汉克尔奇异值，并将主导内部模式转移到不同神经元子集。

Conclusion: 该方法将神经网络转化为局部白盒动态模型集合，为剪枝或约束提供了自然候选的内部方向，有助于提高可解释性。

Abstract: Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.

</details>


### [341] [MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation](https://arxiv.org/abs/2511.11681)
*Penghui Niu,Jiashuai She,Taotao Cai,Yajuan Zhang,Ping Zhang,Junhua Gu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了MPCM-Net，一种集成部分注意力卷积与Mamba架构的多尺度网络，用于地面云图像分割，在CSRC数据集上实现了分割精度与推理速度的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法存在三个主要局限：依赖膨胀卷积缺乏通道间互操作性；注意力机制忽视精度-吞吐量平衡；解码器修改未能建立层次局部特征的全局依赖关系。

Method: 编码器包含MPAC模块（MPC块和MPA块），实现多尺度云形成的全局空间交互和低计算复杂度特征提取；解码器使用M2B模块通过SSHD保持线性复杂度，实现跨空间和尺度维度的深度特征聚合。

Result: 在CSRC数据集上的广泛实验表明，MPCM-Net优于现有最先进方法，实现了分割精度和推理速度的最佳平衡。

Conclusion: MPCM-Net通过集成部分注意力卷积与Mamba架构，有效解决了现有云图像分割方法的局限性，同时发布了新的CSRC数据集作为社区贡献。

Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.

</details>


### [342] [Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions](https://arxiv.org/abs/2511.13103)
*Vidur Sinha,Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: STACCA是一个基于Transformer的多智能体强化学习框架，用于解决大规模网络控制问题，能够捕捉长程依赖关系并在不同网络拓扑间泛化。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法存在两个主要限制：1）依赖局部交互衰减假设，难以捕捉长程依赖（如级联故障、疫情爆发）；2）缺乏跨网络拓扑的泛化能力，需要在新图上重新训练。

Method: STACCA采用集中式图Transformer评论家建模长程依赖并提供系统级反馈，共享图Transformer执行器学习可泛化策略，并集成新颖的反事实优势估计器改进信用分配。

Result: 在疫情控制和谣言传播网络控制任务上的评估表明，STACCA在性能、网络泛化能力和可扩展性方面均有提升。

Conclusion: 基于Transformer的MARL架构在大规模网络系统中具有实现可扩展和可泛化控制的潜力。

Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.

</details>


### [343] [Stratified Knowledge-Density Super-Network for Scalable Vision Transformers](https://arxiv.org/abs/2511.11683)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.LG

TL;DR: 提出WPAC和PIAD方法，将预训练ViT转换为分层知识密度超网络，实现按需提取不同大小的子网络，在模型压缩和扩展方面表现优异


<details>
  <summary>Details</summary>
Motivation: 为不同资源约束训练和部署多个ViT模型成本高且效率低，需要一种灵活的方法从单一模型中提取不同大小的子网络

Method: WPAC通过加权PCA压缩注意力层知识到关键权重；PIAD通过渐进重要性感知dropout促进知识分层组织

Result: WPAC在知识集中方面优于现有剪枝标准，与PIAD结合在模型压缩和扩展任务中达到先进水平

Conclusion: 该方法提供了一种高效的方式从单一预训练ViT中提取不同大小的子网络，在保持性能的同时显著降低部署成本

Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.

</details>


### [344] [DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play](https://arxiv.org/abs/2511.13186)
*Akash Karthikeyan,Yash Vardhan Pant*

Main category: cs.LG

TL;DR: 提出了DiffFP框架，使用扩散策略来估计对未见对手的最佳响应，在连续决策空间的零和游戏中实现快速收敛和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决自博弈强化学习在连续决策空间中收敛缓慢、容易受到未见对手策略利用的问题，需要提升自适应性和泛化能力

Method: 基于虚构博弈框架，使用扩散策略进行生成建模来学习自适应和多样化的策略，近似最佳响应

Result: 在赛车和多粒子零和游戏等复杂多智能体环境中验证，比基线强化学习方法收敛快3倍，成功率平均高30倍

Conclusion: DiffFP框架能够在连续空间零和游戏中收敛到ε-纳什均衡，学习到的策略对多样化对手具有鲁棒性

Abstract: Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations

</details>


### [345] [A Bayesian Model for Multi-stage Censoring](https://arxiv.org/abs/2511.11684)
*Shuvom Sadhuka,Sophia Lin,Emma Pierson,Bonnie Berger*

Main category: cs.LG

TL;DR: 开发了一个用于医疗决策漏斗结构的贝叶斯模型，能够处理选择性标签和审查偏差问题，在急诊科数据中发现ICU入院决策存在性别差异。


<details>
  <summary>Details</summary>
Motivation: 医疗决策漏斗结构中存在选择性审查问题，即真实结果只在流程末端被观察到，这可能导致风险估计的统计偏差，特别是在服务不足的患者群体中。

Method: 开发了一个贝叶斯模型来处理漏斗决策结构中的选择性标签和审查问题，从选择标签和审查的先验工作中汲取灵感。

Result: 在合成设置中，模型能够比基线更准确地恢复真实参数并预测被审查患者的结果；在急诊科数据应用中，发现ICU入院决策存在性别差异，女性需要更高的死亡风险阈值（5.1%）才会被收入ICU，而男性为4.5%。

Conclusion: 该贝叶斯模型能够有效处理医疗决策漏斗中的选择性审查问题，揭示了医疗决策中可能存在的性别偏见，为改善医疗公平性提供了工具。

Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).

</details>


### [346] [R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models](https://arxiv.org/abs/2511.11685)
*Tianyi Yin,Jingwei Wang,Chenze Wang,Han Wang,Jiexuan Cai,Min Liu,Yunlong Ma,Kun Gao,Yuting Song,Weiming Shen*

Main category: cs.LG

TL;DR: R-Tuning是一种用于预训练时间序列模型持续适应的新框架，通过频率感知重放策略和潜在一致性约束，有效解决灾难性遗忘问题，在新任务上显著提升性能同时保留旧任务知识。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在时间序列预测中表现出色，但适应不断变化的数据分布仍具挑战性。主要障碍在于无法访问原始训练数据，仅在新数据上微调会导致灾难性遗忘。

Method: 提出R-Tuning框架，构建统一潜在空间捕获先验和当前任务知识。使用基于小波分解的频率感知重放策略，生成趋势保持和融合增强的变体。引入潜在一致性约束，对齐新表示与先验任务空间。

Result: 实验结果显示R-Tuning在新任务上MAE和MSE分别降低高达46.9%和46.8%，同时在旧任务上性能提升达5.7%和6.0%。在少样本设置下，即使合成代理样本仅占新任务数据集的5%，仍优于所有最先进基线。

Conclusion: R-Tuning通过频率感知重放和潜在一致性约束，有效解决了预训练时间序列模型的持续适应问题，在保持先验知识的同时显著提升新任务性能，特别是在少样本场景下表现优异。

Abstract: Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.

</details>


### [347] [Naga: Vedic Encoding for Deep State Space Models](https://arxiv.org/abs/2511.13510)
*Melanie Schaller,Nick Janssen,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: Naga是一种基于吠陀数学结构概念的深度状态空间模型编码方法，通过双向处理正向和时间反转序列，使用哈达玛积组合表示，在多个长期时间序列预测基准测试中优于28个现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 受吠陀数学结构概念启发，旨在开发一种能够更好捕捉远距离时间依赖性的可解释且计算高效的长期序列建模方法。

Method: 提出双向状态空间模型编码，联合处理正向和时间反转输入序列，通过哈达玛积组合两种表示，形成吠陀启发的编码方式。

Result: 在ETTh1、ETTh2、ETTm1、ETTm2、Weather、Traffic和ILI等多个长期时间序列预测基准测试中，Naga优于28个当前最先进模型，且比现有深度SSM方法更高效。

Conclusion: 结合结构化、吠陀启发的分解方法可以为长距离序列建模提供可解释且计算高效的替代方案。

Abstract: This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.

</details>


### [348] [Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686)
*Qing Yao,Lijian Gao,Qirong Mao,Dong Ming*

Main category: cs.LG

TL;DR: 提出了正则化薛定谔桥（RSB）方法，通过正则化训练策略解决扩散模型在逆问题中的失真-感知权衡和曝光偏差问题，在语音增强任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在解决逆问题时面临两个关键挑战：1）失真-感知权衡，提高感知质量会降低重建保真度；2）曝光偏差问题，训练-推理输入不匹配导致预测误差累积和重建质量下降。

Method: 提出了正则化薛定谔桥（RSB）方法，采用新颖的正则化训练策略，对输入状态和目标进行扰动，通过暴露模型于模拟预测误差来缓解曝光偏差，并通过后验均值的精心设计插值来减轻失真。

Result: 在两个典型的语音增强逆问题上的大量实验表明，RSB优于最先进的方法，显著改善了失真指标并有效减少了曝光偏差。

Conclusion: RSB是专门为逆问题设计的薛定谔桥适应方法，成功解决了扩散模型在失真-感知权衡和曝光偏差方面的局限性。

Abstract: Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.

</details>


### [349] [Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling](https://arxiv.org/abs/2511.11688)
*Aihua Zhu,Rui Su,Qinglin Zhao,Li Feng,Meng Shen,Shibo He*

Main category: cs.LG

TL;DR: HSO是一种新颖的双层优化框架，通过全局搜索和局部优化交替进行，结合MEP和SPF创新方法，在极低NFE条件下实现训练自由采样，5次评估即可达到11.94 FID，优化成本仅需8秒。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型生成质量高但采样过程慢，现有调度优化方法难以同时满足有效性、自适应性、实用鲁棒性和计算效率四个核心原则。

Method: 提出分层调度优化器(HSO)，包含上层全局搜索最优初始化策略和下层局部优化调度细化，使用中点误差代理(MEP)和间距惩罚适应度(SPF)函数。

Result: 在极低NFE条件下实现最先进的训练自由采样性能，NFE=5时在LAION-Aesthetics上达到11.94 FID，优化成本仅需8秒。

Conclusion: HSO提供了一种高效实用的扩散模型加速范式，无需重新训练即可显著提升采样效率。

Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.

</details>


### [350] [Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2511.11690)
*Fei Song,Yi Li,Rui Wang,Jiahuan Zhou,Changwen Zheng,Jiangmeng Li*

Main category: cs.LG

TL;DR: 本文提出了一种双重去偏测试时提示调优方法，通过动态检索增强调制和可靠性感知提示优化来缓解视觉语言模型中的提示优化偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时提示调优方法仅基于未标记测试数据进行优化，容易产生提示优化偏差，导致在下游任务上性能不佳。这种偏差源于模型层面熵最小化目标忽视预测正确性，以及数据层面提示偏差导致视觉-文本模态不对齐。

Method: 1. 动态检索增强调制模块：使用测试图像特征查询动态知识库，检索高置信度知识来调制预测；2. 可靠性感知提示优化模块：基于置信度加权集成和跨模态一致性蒸馏，在提示调优中施加正则化约束。

Result: 在涉及自然分布偏移和跨数据集泛化的15个基准数据集上的广泛实验表明，该方法优于基线方法，验证了其在缓解提示优化偏差方面的有效性。

Conclusion: 所提出的双重去偏测试时提示调优方法能够有效缓解提示优化偏差，提升视觉语言模型在零样本设置下的泛化性能。

Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.

</details>


### [351] [Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues](https://arxiv.org/abs/2511.11691)
*Seham Nasr,Zhao Ren,David Johnson*

Main category: cs.LG

TL;DR: 提出了一种用于语音情感识别(XAI)的可解释AI框架，通过量化显著区域内的声学线索，将模型关注区域与专家参考的情感声学标记联系起来，提高解释的忠实度和可理解性。


<details>
  <summary>Details</summary>
Motivation: 当前基于显著性的方法虽然能突出语谱图区域，但无法显示这些区域是否对应有意义的声学情感标记，限制了模型的透明度和可信度。

Method: 提出一个框架，量化显著区域内的声学线索幅度，将模型关注点与专家参考的语音情感声学特征明确关联。

Result: 在基准SER数据集上的实验表明，该方法通过明确连接显著区域与理论驱动的声学特征，提高了解释质量。

Conclusion: 相比标准显著性方法，该方法提供了更易理解和合理的SER模型解释，为可信赖的语音情感计算奠定了基础。

Abstract: Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.

</details>


### [352] [AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation](https://arxiv.org/abs/2511.11692)
*Jiayin Zhu,Linlin Yang,Yicong Li,Angela Yao*

Main category: cs.LG

TL;DR: 本文提出了AnchorDS方法，通过将文本到3D优化重新表述为动态源分布到固定目标分布的映射，解决了传统SDS方法中的语义过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的文本到3D方法将2D生成模型的引导视为静态，忽略了源动态，导致语义线索被抑制或合并，产生语义过平滑伪影。

Method: 将问题建模为双条件潜在空间，同时基于文本提示和中间渲染图像进行条件化。提出AnchorDS机制，提供状态锚定引导，并设计了轻量级过滤策略和微调策略来优化锚点。

Result: AnchorDS能够生成更精细的细节、更自然的颜色和更强的语义一致性，特别是在复杂提示下表现优异，同时在效率上保持优势。

Conclusion: 该方法在质量和效率上都超越了先前的方法，为文本到3D生成提供了更稳定和语义一致的解决方案。

Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.

</details>


### [353] [Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification](https://arxiv.org/abs/2511.11697)
*Liqin Tan,Pin Chen,Menghan Liu,Xiean Wang,Jianhuan Cen,Qingsong Zou*

Main category: cs.LG

TL;DR: MatUQ是一个用于评估图神经网络在材料属性预测中分布外泛化能力和不确定性量化的基准框架，包含1375个OOD预测任务，提出了新的结构感知分割策略SOAP-LOCO和不确定性度量D-EviU。


<details>
  <summary>Details</summary>
Motivation: 现有材料预测模型在分布外场景下的可靠性和不确定性量化能力缺乏系统评估，需要开发统一的基准框架来指导模型选择。

Method: 使用6个材料数据集构建1375个OOD预测任务，采用5种OFM分割和新提出的SOAP-LOCO分割策略，评估12个GNN模型，结合蒙特卡洛Dropout和深度证据回归进行不确定性感知训练。

Result: 不确定性感知训练平均减少70.6%的预测误差；D-EviU指标与预测误差相关性最强；没有单一模型在所有任务中占优，不同模型在不同材料属性上表现各异。

Conclusion: MatUQ为材料发现中分布偏移下的可靠模型选择提供了实用指导，不确定性感知训练能显著提升模型在OOD场景下的性能。

Abstract: We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.

</details>


### [354] [Moirai 2.0: When Less Is More for Time Series Forecasting](https://arxiv.org/abs/2511.11698)
*Chenghao Liu,Taha Aksu,Juncheng Liu,Xu Liu,Hanshu Yan,Quang Pham,Doyen Sahoo,Caiming Xiong,Silvio Savarese,Junnan Li*

Main category: cs.LG

TL;DR: Moirai 2.0是一个基于解码器架构的时间序列基础模型，采用分位数预测和多令牌预测，在准确性和推理效率方面均有提升，比前代模型更快更小且性能更好。


<details>
  <summary>Details</summary>
Motivation: 开发更高效、更准确的时间序列预测模型，通过简化架构和优化训练方法来解决模型复杂度和计算效率的问题。

Method: 采用纯解码器架构、单patch输入和分位数损失函数，结合递归多分位数解码技术，替代了之前的掩码编码器训练、多patch输入和混合分布输出。

Result: 在Gift-Eval基准测试中表现优异，比Moirai 1.0-Large快两倍、小三十倍且性能更好，在参数数量增加时性能趋于平稳，在长预测时域上性能下降。

Conclusion: Moirai 2.0在准确性、速度和模型大小之间取得了良好平衡，未来需要在数据扩展和长时域建模方面进一步研究。

Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.

</details>


### [355] [Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification](https://arxiv.org/abs/2511.11699)
*Xingqi Lin,Liangyu Chen,Min Wu,Min Zhang,Zhenbing Zeng*

Main category: cs.LG

TL;DR: 提出了DeepPrism方法，通过截断矩形棱柱来紧密包围Hadamard积生成的三维非线性曲面，显著提高了RNN鲁棒性验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对非线性激活函数进行线性约束过近似时，单独使用线性边界平面可能导致显著过估计，降低验证准确性。

Method: 使用两个线性松弛平面形成截断矩形棱柱来包围Hadamard积的三维非线性曲面，并采用细化驱动方法最小化其体积和表面积以实现更紧密的过近似。

Result: 实验结果表明，DeepPrism在图像分类、语音识别和情感分析等任务中相比最先进方法有显著改进。

Conclusion: 提出的截断矩形棱柱方法能够更紧密地近似非线性激活函数，有效提高了RNN鲁棒性验证的准确性。

Abstract: Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.

</details>


### [356] [Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2511.11701)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 提出了一个使用贝叶斯神经网络和蒙特卡洛dropout的概率性电价预测框架，按小时分别建模以捕捉昼夜模式，在点预测和区间预测方面均优于传统基准模型。


<details>
  <summary>Details</summary>
Motivation: 在放松管制的电力市场中，准确的电价预测对战略决策至关重要。传统点预测无法捕捉内在不确定性，限制了风险管理效用。

Method: 使用贝叶斯神经网络和蒙特卡洛dropout构建概率预测框架，为每天的不同小时分别训练模型以捕捉昼夜模式。

Result: 提出的模型在点预测和区间预测方面均优于GARCHX和LEAR基准模型。

Conclusion: 这项工作为在能源市场预测中利用概率性神经网络模型提供了参考。

Abstract: Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.

</details>


### [357] [Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom](https://arxiv.org/abs/2511.11703)
*Hugo Huang*

Main category: cs.LG

TL;DR: 提出两种基于语义分割的输入表示方法（SS-only和RGB+SS），在ViZDoom死亡竞赛中显著减少内存消耗并提升强化学习性能


<details>
  <summary>Details</summary>
Motivation: 解决3D环境中强化学习面临的两个主要挑战：内存缓冲区的高内存消耗和部分可观察马尔可夫决策过程的复杂性

Method: 使用语义分割技术创建两种输入表示：SS-only（仅语义分割）和RGB+SS（RGB+语义分割），在ViZDoom环境中进行实验，并采用基于密度的热图可视化分析

Result: SS-only方法将内存缓冲区消耗减少66.6%-98.6%，RGB+SS显著提升RL智能体性能，热图分析有效评估数据收集适用性

Conclusion: 语义分割输入表示是解决3D环境RL内存消耗和POMDP复杂性的有效方法，克服了在ViZDoom等环境中应用语义分割的常见缺陷

Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.

</details>


### [358] [Simple Vision-Language Math Reasoning via Rendered Text](https://arxiv.org/abs/2511.11704)
*Matvey Skripkin,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.LG

TL;DR: 提出了一种轻量级但有效的训练流程，通过将LaTeX编码的数学公式渲染成图像，并与结构化思维链提示配对，来训练视觉语言模型解决数学问题。


<details>
  <summary>Details</summary>
Motivation: 现有的数学问题解决方法往往复杂且计算量大，需要一种简单但有效的多模态方法来提升数学推理能力，同时保持模型的通用性。

Method: 将LaTeX公式渲染为图像，与结构化思维链提示配对，使用文本到视觉的增强方法来训练紧凑的多模态架构。

Result: 该方法在多个基准测试中达到或超越了开源和专有的数学视觉语言求解器，在MMMU、ChartQA和DocVQA等任务上性能提升高达20%，同时保持了广泛的通用领域能力。

Conclusion: 渲染保真度和提示设计是性能的主要驱动因素，这种简单的方法在数学推理任务上表现出色，同时保持了模型的通用性。

Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.

</details>


### [359] [Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs](https://arxiv.org/abs/2511.11705)
*Arya Narang*

Main category: cs.LG

TL;DR: 本研究探讨了在菜品名称的短文本输入辅助下，多模态模型相比仅使用图像的基线模型在卡路里估算方面的改进程度及其统计显著性。


<details>
  <summary>Details</summary>
Motivation: 旨在验证短文本信息（如菜品名称）是否能显著提升卡路里估算的准确性，超越仅依赖图像信息的模型。

Method: 使用TensorFlow库和Google Nutrition5k数据集，训练了仅使用图像的CNN模型和同时接受文本与图像输入的多模态CNN模型。

Result: 多模态模型将卡路里估算的MAE从84.76千卡降低至83.70千卡，减少了1.06千卡（改进1.25%）。

Conclusion: 短文本输入能够轻微但显著地改善卡路里估算的准确性，多模态方法相比纯图像方法具有优势。

Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.

</details>


### [360] [Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling](https://arxiv.org/abs/2511.11706)
*Julia Peters,Karin Mora,Miguel D. Mahecha,Chaonan Ji,David Montero,Clemens Mosig,Guido Kraemer*

Main category: cs.LG

TL;DR: 提出了一个统一的地球观测表示学习框架，整合不同传感器数据到高时空分辨率的统一特征空间，解决现有模型固定尺度限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测基础模型通常只能在固定空间或时间尺度上运行，限制了需要精细空间细节和高时间保真度的生态分析应用。

Method: 采用两阶段设计：首先独立建模每个传感器以捕捉其特定特征，然后将表示组合到共享模型中。使用Sentinel-1和Sentinel-2数据作为代表性模态，在10米分辨率和无云Sentinel-2采集频率下构建潜在空间。

Result: 定性分析显示学习到的嵌入在异质景观中表现出高空间和语义一致性。在总初级生产力建模的定量评估中，它们编码了生态意义模式并保持了足够的时间保真度以支持精细尺度分析。

Conclusion: 该框架为需要不同空间和时间分辨率的环境应用提供了一个灵活、分析就绪的表示学习方法，能够捕捉互补的遥感数据并保持跨时空的一致性。

Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.

</details>


### [361] [FSC-Net: Fast-Slow Consolidation Networks for Continual Learning](https://arxiv.org/abs/2511.11707)
*Mohamed El Gorrim*

Main category: cs.LG

TL;DR: FSC-Net提出双网络架构解决持续学习中的灾难性遗忘问题，通过快速网络学习新任务，慢速网络进行知识巩固，在Split-MNIST上取得91.71%的保留准确率，比单网络提升4.27个百分点。


<details>
  <summary>Details</summary>
Motivation: 受神经科学中记忆巩固机制的启发，旨在解决神经网络在持续学习中遇到的灾难性遗忘问题，即学习新任务时会忘记先前学到的知识。

Method: 采用双网络架构：快速网络(NN1)负责快速适应新任务，慢速网络(NN2)通过蒸馏和重放进行知识巩固。研究发现纯重放策略优于蒸馏方法。

Result: 在Split-MNIST上达到91.71%±0.62%的保留准确率，比单网络提升4.27pp；在Split-CIFAR-10上达到33.31%±0.38%，提升8.20pp，但绝对性能仍低于随机期望。

Conclusion: 双时间尺度巩固机制而非架构复杂性是缓解灾难性遗忘的关键因素，简单MLP架构优于复杂变体，纯重放策略效果最佳。

Abstract: Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.

</details>


### [362] [Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control](https://arxiv.org/abs/2511.11711)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 将Model-X knockoffs方法引入稀疏自编码器(SAE)特征选择，通过knockoff+控制错误发现率(FDR)，为可解释性研究提供统计保障。


<details>
  <summary>Details</summary>
Motivation: 当前SAE在识别神经网络可解释特征时难以区分真实计算模式和错误相关性，需要更可靠的特征选择方法。

Method: 使用Model-X knockoffs方法结合高斯替代模型，在Pythia-70M模型的情感分类任务中对512个高活性SAE潜在变量进行特征选择。

Result: 在目标FDR q=0.1下选择了129个特征，约25%的潜在变量携带任务相关信号，所选特征与非选特征在knockoff统计量上显示5.40倍分离。

Conclusion: 该方法将SAE与多重测试感知推断相结合，为可靠特征发现提供了可重复且原则性的框架，推进了机制可解释性的基础研究。

Abstract: Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.

</details>


### [363] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: 论文提出推理是状态空间中迭代操作符应用并收敛到固定点的过程，通过OpenLM架构在OpenXOR问题上实现76%准确率，而现有LLMs为0%。


<details>
  <summary>Details</summary>
Motivation: 在LLMs在GSM8K等基准上取得超人表现的时代，需要区分系统是学会了推理还是仅仅在推理轨迹上进行模式匹配。

Method: 提出推理作为状态空间中迭代操作符应用的理论框架，开发OpenOperator理论和OpenLM架构实现。

Result: 在OpenXOR问题上，OpenLM达到76%准确率，而最先进的LLMs为0%。

Conclusion: 理解推理的本质要求并构建提供真正推理能力的架构，而非仅仅批评现有系统。

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [364] [Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data](https://arxiv.org/abs/2511.11714)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 使用联邦学习平台在多个医院间协作训练肺炎检测模型，数据保持本地化，无需传输患者胸片，性能显著提升至90%准确率和96.6% ROC-AUC。


<details>
  <summary>Details</summary>
Motivation: 解决医疗数据因隐私法规（HIPAA、GDPR）无法集中化的问题，同时应对全球分布式数据、医院间差异性和异构成像协议等挑战。

Method: 采用Sherpa.ai联邦学习平台，使用儿科肺炎胸片数据集模拟跨医院协作，处理非独立同分布数据，保持数据本地化和隐私。

Result: 联邦学习模型达到0.900准确率和0.966 ROC-AUC，相比单医院模型（0.610准确率；0.644 ROC-AUC）分别提升47.5%和50.0%。

Conclusion: 联邦学习能够在不传输患者数据的情况下，实现高性能、可泛化、安全且私密的肺炎检测，特别适用于罕见疾病和低数据领域。

Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.

</details>


### [365] [Multiscale Grassmann Manifolds for Single-Cell Data Analysis](https://arxiv.org/abs/2511.11717)
*Xiang Xiang Wang,Sean Cottrell,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出基于Grassmann流形的多尺度框架，用于单细胞数据分析，通过整合不同几何视图的特征来更好地捕捉细胞异质性。


<details>
  <summary>Details</summary>
Motivation: 传统的欧几里得空间表示方法难以捕捉单细胞数据中的内在相关性和多尺度几何结构。

Method: 基于Grassmann流形构建多尺度框架，通过幂函数控制尺度采样，将不同表示尺度的特征整合到统一的流形空间中。

Result: 在9个基准单细胞RNA-seq数据集上的实验表明，该方法能有效保持有意义的结构并提供稳定的聚类性能，尤其适用于中小型数据集。

Conclusion: Grassmann流形为单细胞数据分析提供了连贯且信息丰富的基础框架。

Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.

</details>


### [366] [Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm](https://arxiv.org/abs/2511.11727)
*Tongda Xu*

Main category: cs.LG

TL;DR: 本文指出在扩散模型中使用去噪分数匹配优化条件输入会破坏其与精确分数匹配的等价性，导致偏差和更高的分数范数，影响多个领域的研究工作。


<details>
  <summary>Details</summary>
Motivation: 许多近期研究使用去噪分数匹配来优化扩散模型的条件输入，但作者发现这种方法存在理论偏差问题，需要深入分析其对模型性能的影响。

Method: 通过理论分析和实验验证，证明去噪分数匹配优化条件输入会破坏与精确分数匹配的等价性，并观察数据分布优化时的类似偏差。

Result: 发现这种偏差会导致更高的分数范数，并影响多个领域的研究，包括自回归生成的MAR、图像压缩的PerCo和文本到3D生成的DreamFusion。

Conclusion: 去噪分数匹配在优化扩散模型条件输入时存在理论偏差，这一发现对多个应用领域具有重要影响，需要重新审视相关方法的理论基础。

Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.

</details>


### [367] [Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics](https://arxiv.org/abs/2511.11734)
*Kamalpreet Singh Kainth,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedat Panat*

Main category: cs.LG

TL;DR: 提出了PI-NODE-SR框架，通过结合低阶显式求解器和残差归一化，稳定地学习刚性生物物理系统的动力学，能够从单个振荡中学习并准确外推。


<details>
  <summary>Details</summary>
Motivation: 标准神经微分方程和物理信息变体在建模刚性生物物理系统时不可靠，需要大量迭代且可能收敛到次优解，无法保持振荡频率或振幅。

Method: 结合低阶显式求解器（Heun方法）和尺度感知残差归一化，平衡不同时间尺度状态变量的贡献，避免依赖计算昂贵的隐式求解器。

Result: 在Hodgkin-Huxley方程上，PI-NODE-SR从单个振荡学习并外推超过100ms，准确捕捉振荡频率和振幅，恢复门控变量中的尖锐亚阈值曲率等形态特征。

Conclusion: PI-NODE-SR相对于基线神经ODE和PINNs持续减少长期误差，为稳定高效学习刚性生物动力学提供了原则性途径。

Abstract: Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.

</details>


### [368] [KAN/H: Kolmogorov-Arnold Network using Haar-like bases](https://arxiv.org/abs/2511.11736)
*Susumu Katayama*

Main category: cs.LG

TL;DR: 提出了KAN/H，一种使用Haar变体基系统（包含全局和局部基）替代B样条的Kolmogorov-Arnold网络变体，应用于函数逼近和MNIST问题，无需大量问题特定的超参数调优。


<details>
  <summary>Details</summary>
Motivation: 改进Kolmogorov-Arnold网络，通过使用Haar变体基系统来避免B样条方法所需的大量超参数调优，提高网络的实用性和易用性。

Method: 开发KAN/H网络，采用Haar变体基系统（包含全局和局部基）替代传统的B样条基函数，应用于函数逼近和MNIST分类任务。

Result: KAN/H在函数逼近和MNIST问题上表现良好，且不需要大多数问题特定的超参数调优，简化了网络的使用过程。

Conclusion: KAN/H通过使用Haar变体基系统有效减少了超参数调优需求，为Kolmogorov-Arnold网络提供了一种更实用的变体。

Abstract: This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.

</details>


### [369] [DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks](https://arxiv.org/abs/2511.11737)
*Qizhe Li,Haolong Chen,Jiansheng Li,Shuqi Chai,Xuan Li,Yuzhou Hou,Xinhua Shao,Fangfang Li,Kaifeng Han,Guangxu Zhu*

Main category: cs.LG

TL;DR: DK-Root是一个联合数据和知识驱动的框架，通过结合可扩展的弱监督和精确的专家指导，用于移动网络QoE问题的根因分析。


<details>
  <summary>Details</summary>
Motivation: 移动网络中QoE问题的根因诊断面临挑战，因为存在复杂的跨层KPI交互且缺乏可靠的专家标注。基于规则的启发式方法虽然能大规模生成标签，但存在噪声且粒度粗糙，限制了纯数据驱动方法的准确性。

Method: 首先通过对比表示学习预训练编码器，使用基于规则的标签同时通过监督对比目标去噪；引入类条件扩散模型生成保留根因语义的KPI序列，通过控制反向扩散步骤产生强弱增强，提高类内紧凑性和类间分离性；最后使用稀缺的专家验证标签联合微调编码器和轻量级分类器。

Result: 在真实运营商级数据集上的广泛实验表明，DK-Root达到了最先进的准确率，超越了传统机器学习和最近的半监督时间序列方法。消融实验证实了条件扩散增强和预训练-微调设计的必要性。

Conclusion: DK-Root框架通过结合弱监督和专家指导，有效解决了移动网络QoE根因分析中的标注稀缺和噪声问题，在表示质量和分类性能上都取得了显著提升。

Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.

</details>


### [370] [Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts](https://arxiv.org/abs/2511.11743)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mackenzie J. Meni,Carlos Andrés Duran Paredes,Eric Arazo,Cristian Bosch,Ricardo Simon Carbajo,Yuan Lai,Leo Anthony Celi*

Main category: cs.LG

TL;DR: 提出基于好奇心驱动的量化混合专家框架，通过贝叶斯认知不确定性路由在异构专家间分配任务，在保持99.9%精度的同时实现4倍压缩和41%能耗节省，并将延迟方差降低82%，为资源受限设备提供稳定推理。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在资源受限设备部署中的两个关键挑战：在激进量化下保持精度，同时确保可预测的推理延迟。

Method: 使用好奇心驱动的量化混合专家框架，基于贝叶斯认知不确定性在异构专家（BitNet三元、1-16位BitLinear、训练后量化）之间进行路由。

Result: 4位量化保持99.9%的16位精度（0.858 vs 0.859 F1），实现4倍压缩和41%能耗节省；好奇心驱动路由将MoE延迟方差降低82%（从230ms到29ms标准差）。

Conclusion: 信息论路由证明自适应量化可产生准确、节能且可预测的边缘模型，简单4位量化架构在大多数部署场景中优于复杂MoE。

Abstract: Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.

</details>


### [371] [Diffusion Models: A Mathematical Introduction](https://arxiv.org/abs/2511.11746)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: 本文提供了扩散生成模型的完整推导，从前向加噪过程、反向去噪过程到变分下界，涵盖了DDIM、DDGAN、稳定扩散等变体，以及连续时间公式和引导扩散方法。


<details>
  <summary>Details</summary>
Motivation: 为扩散模型提供简洁、自包含的理论推导，从高斯分布的基本性质出发，让读者能够理解理论并实际实现相应算法。

Method: 从高斯分布的基本性质开始，构建去噪扩散概率模型，包括前向加噪过程、闭式边缘分布、精确离散反向后验和相关变分下界，然后扩展到连续时间公式和引导扩散方法。

Result: 建立了扩散模型的完整理论框架，包括标准噪声预测目标的简化、似然估计、加速采样方法，以及分类器引导和无分类器引导的数学解释。

Conclusion: 通过透明的代数推导、明确的中间步骤和一致的符号表示，为读者提供了可理解且可实现的扩散模型完整理论体系。

Abstract: We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.

</details>


### [372] [IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation](https://arxiv.org/abs/2511.11750)
*Hanting Yan,Pan Mu,Shiqi Zhang,Yuchao Zhu,Jinglin Zhang,Cong Bai*

Main category: cs.LG

TL;DR: 提出IDOL框架，通过物理先验知识引导的身份约束来处理热带气旋估计中的分布偏移问题，提高在分布外场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 热带气旋估计面临复杂动态环境场导致的分布偏移挑战，现有方法忽视特征表示的内在分布，导致在分布外场景下泛化能力差。

Method: 使用风场模型和暗相关知识建模任务共享和任务特定的身份令牌，通过物理不变性约束特征空间，捕获任务依赖性和内在物理不变性。

Result: 在多个数据集和任务上的实验表明IDOL表现优异，验证了基于物理先验知识的身份约束能有效缓解热带气旋估计中的分布偏移。

Conclusion: 基于物理先验知识的身份导向约束能够有效处理热带气旋估计中的分布变异性，提高模型在分布外场景下的鲁棒性。

Abstract: Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.

</details>


### [373] [Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain](https://arxiv.org/abs/2511.11753)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.LG

TL;DR: 本文提出了一种混合图神经网络方法(H-GSN)用于供应链物流管理中的多任务预测，包括货运类型、物流延迟、交通状况等，在多个数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 提高供应链的韧性和可持续性，通过自动化预测货运类型、物流延迟和交通状况来提升供应链管理效率，减少空气污染物排放。

Method: 提出混合图神经网络(H-GSN)方法，在三个不同的供应链物流数据库(DataCo、Shipping和Smart Logistics)上进行多任务学习，预测货运类型、货运状态、交通状况、物流ID和物流延迟。

Result: 在Smart Logistics数据集上，物流ID预测准确率97.8%，交通状况预测准确率100%；在DataCo数据集上货运类型预测准确率98.7%；在Shipping数据集上物流延迟预测准确率99.4%。

Conclusion: 所提出的H-GSN方法在不同物流场景下都表现出高效性，能够有效提高供应链的韧性和可持续性。

Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.

</details>


### [374] [Sumudu Neural Operator for ODEs and PDEs](https://arxiv.org/abs/2511.11762)
*Ben Zelenskiy,Saibilila Abudukelimu,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: 提出了基于Sumudu变换的Sumudu神经算子(SNO)，通过变换对的多项式展开关系分解输入空间系数，在Sumudu空间中参数化神经算子。在ODE和PDE任务中表现优于FNO，与LNO竞争，在Euler-Bernoulli梁和扩散方程上误差最低，并展示了零样本超分辨率能力。


<details>
  <summary>Details</summary>
Motivation: 探索Sumudu变换作为神经算子设计的潜力，特别是针对特定类型的偏微分方程(PDEs)，利用变换对的多项式展开关系来改进神经算子的性能。

Method: 基于Sumudu变换性质构建SNO，将输入空间分解为系数，转换到Sumudu空间进行参数化，在ODE和PDE任务中评估性能。

Result: SNO在PDE上表现优于FNO，与LNO在多个PDE任务中竞争性准确，在Euler-Bernoulli梁和扩散方程上获得最低误差，零样本超分辨率实验显示能从低质量样本获得更高质量数据。

Conclusion: 初步结果表明Sumudu变换作为神经算子设计具有前景，特别是对于某些类别的偏微分方程。

Abstract: We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.

</details>


### [375] [CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments](https://arxiv.org/abs/2511.11778)
*Byoungjun Park,Pedro Porto Buarque de Gusmão,Dongjin Ji,Minhoe Kim*

Main category: cs.LG

TL;DR: CATCHFed是一种针对半监督联邦学习的方法，通过自适应阈值和一致性正则化，在服务器仅有少量标注数据的情况下有效利用客户端未标注数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习场景中客户端往往缺乏标注数据，而现有半监督联邦学习方法在标注数据极少时性能显著下降。

Method: 提出客户端感知的自适应阈值考虑类别难度、混合阈值提升伪标签质量，并利用未伪标注数据进行一致性正则化。

Result: 在多种数据集和配置下的广泛实验表明，CATCHFed能有效利用未标注客户端数据，在极有限标注设置下实现优越性能。

Conclusion: CATCHFed通过创新性的阈值设计和数据利用策略，解决了半监督联邦学习在低标注数据场景下的性能瓶颈问题。

Abstract: Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.

</details>


### [376] [Simplicial covering dimension of extremal concept classes](https://arxiv.org/abs/2511.11819)
*Ari Blondal,Hamed Hatami,Pooya Hatami,Chavdar Lalov,Sivan Tretiak*

Main category: cs.LG

TL;DR: 该论文将经典拓扑维度理论应用于二元概念类，定义了单纯覆盖维度，并证明对于有限概念类，该维度精确刻画了PAC学习中的列表可复制数。


<details>
  <summary>Details</summary>
Motivation: 将拓扑维度理论引入机器学习领域，为分析概念类的学习复杂性提供新的数学工具。

Method: 将概念类的可实现分布空间视为拓扑空间，通过损失函数和概念类本身诱导单纯结构，定义单纯覆盖维度。

Result: 证明对于有限概念类，单纯覆盖维度精确等于列表可复制数（即全局稳定性），并应用经典维度理论计算极值概念类的精确列表可复制数。

Conclusion: 拓扑维度理论为理解概念类的学习复杂性提供了新的视角和计算方法，建立了维度理论与学习理论之间的深刻联系。

Abstract: Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.
  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.

</details>


### [377] [Conformal Constrained Policy Optimization for Cost-Effective LLM Agents](https://arxiv.org/abs/2511.11828)
*Wenwen Si,Sooyong Jang,Insup Lee,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出CCPO方法，通过结合多个成本/精度不同的LLM模型，在保证可靠性的前提下最小化计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大，但计算成本和API成本日益昂贵，需要找到在保证可靠性的同时降低成本的解决方案

Method: Conformal Constrained Policy Optimization (CCPO) - 结合约束策略优化、离线强化学习和在线符合预测，联合优化成本感知策略和自适应阈值

Result: 在两个多跳问答基准测试中，CCPO相比其他成本感知基线和LLM引导方法，成本降低高达30%，同时保持可靠性

Conclusion: CCPO提供了一个原则性和实用的框架，可以显著提高LLM代理的成本效益，同时保持可靠性

Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.

</details>


### [378] [Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers](https://arxiv.org/abs/2511.11834)
*Vahid Hemmati,Ahmad Mohammadi,Abdul-Rauf Nuhu,Reza Ahmari,Parham Kebria,Abdollah Homaifar*

Main category: cs.LG

TL;DR: VC是一种无需标签的对抗鲁棒性度量方法，通过测量排序后softmax输出的离散度来量化模型置信度的不规则性，与分类准确率呈强负相关。


<details>
  <summary>Details</summary>
Motivation: 在实时系统中，由于推理时缺乏真实标签，对抗鲁棒性成为关键挑战，需要开发无需标签的性能评估指标。

Method: 定义VC为相邻确定性值的平均平方对数比，在MNIST和CIFAR-10数据集上使用ANN、CNN和VGG-like模型，通过FGSM生成对抗样本并创建混合测试集。

Result: 分类准确率与log(VC)之间存在强负相关（大多数情况下rho < -0.90），VC能有效反映性能下降而无需标签数据。

Conclusion: VC是一种可扩展、架构无关的实时性能度量，适用于安全关键应用的早期预警系统。

Abstract: Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.

</details>


### [379] [On the Trade-Off Between Transparency and Security in Adversarial Machine Learning](https://arxiv.org/abs/2511.11842)
*Lucas Fenaux,Christopher Srinivasa,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 本文通过可转移对抗样本攻击研究了AI透明度与安全性的战略冲突，发现攻击者在匹配防御者决策时更成功，表明模糊性可能有利于防御者。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统中透明度与安全性之间的潜在冲突，特别是在对抗性环境中，透明度可能损害安全性。

Method: 使用大规模实证评估（9种攻击方法在181个模型上）和博弈论分析（纳什博弈和斯塔克尔伯格博弈）来建模透明度与安全性的权衡。

Result: 攻击者能够通过了解防御者模型是否被防御来提高攻击成功率；博弈论分析确认透明度有时会损害安全性。

Conclusion: AI系统的透明度与安全性存在固有冲突，透明度在对抗环境中可能损害安全性，需要谨慎权衡。

Abstract: Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.

</details>


### [380] [Leveraging Exogenous Signals for Hydrology Time Series Forecasting](https://arxiv.org/abs/2511.11849)
*Junyang He,Judy Fox,Alireza Jafari,Ying-Jung Chen,Geoffrey Fox*

Main category: cs.LG

TL;DR: 该研究探讨了在时间序列模型中整合领域知识对水文降雨-径流建模的影响，发现包含全面已知外生输入的模型优于基础模型，其中自然年度周期性时间序列的贡献最为显著。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型研究取得进展，但很少研究其在物理科学特定下游应用中的有效性，特别是在水文降雨-径流建模领域。

Method: 使用CAMELS-US数据集，包含671个位置的降雨和径流数据，比较了基准模型和基础模型，重点研究了整合领域知识（特别是已知外生输入）的方法。

Result: 结果表明，包含全面已知外生输入的模型性能优于更有限的方法，包括基础模型。自然年度周期性时间序列的整合带来了最显著的改进。

Conclusion: 在水文时间序列建模中，整合领域知识（特别是自然周期性特征）比单纯使用基础模型更有效，强调了特定领域知识在物理科学应用中的重要性。

Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.

</details>


### [381] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: 比较GPT-2和LSTM两种深度学习模型在预测森林CO₂吸收(GPP)方面的表现，发现LSTM整体精度更高，但GPT-2在极端事件中表现更好，且LSTM使用更短的输入窗口就能达到相似精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感方法在捕捉GPP复杂时间动态方面的不足，探索深度学习模型在多元数据融合下对植被过程时间动态的表示能力。

Method: 使用GPT-2(Transformer架构)和LSTM(循环神经网络)两种代表性模型，结合多元输入数据(辐射、Sentinel-2、MODIS地表温度、Sentinel-1)进行GPP预测，并分析时间上下文长度的影响。

Result: 两种模型达到相似精度，LSTM整体表现更好但GPT-2在极端事件中更优；LSTM使用更短输入窗口即可达到GPT-2的精度；辐射是最重要的预测因子，其次是Sentinel-2、MODIS地表温度和Sentinel-1。

Conclusion: 模型架构、上下文长度和多模态输入共同决定了GPP预测性能，为未来开发监测陆地碳动态的深度学习框架提供指导。

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [382] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: PasoDoble是一个无监督的LLM双角色对抗训练框架，通过Proposer生成有挑战性的问题和Solver解决问题的对抗过程，提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练依赖外部监督，而对抗学习特别是双角色自博弈训练可以减少对外部监督的依赖，但面临奖励破解和训练不稳定的挑战。

Method: 从同一基础模型初始化两个模型：Proposer生成带真实答案的挑战性问题，Solver尝试解决。Proposer利用预训练数据确保问题质量，通过联合更新避免奖励破解，并引入离线范式增强训练稳定性。

Result: 实验结果表明PasoDoble能够提升LLM的推理性能。

Conclusion: PasoDoble展示了无监督双角色对抗训练在提升LLM推理能力方面的有效性，为减少外部监督依赖提供了可行方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [383] [FLEX: Feature Importance from Layered Counterfactual Explanations](https://arxiv.org/abs/2511.11891)
*Nawid Keshtmand,Roussel Desmond Nzoyem,Jeffrey Nicholas Clark*

Main category: cs.LG

TL;DR: FLEX是一个模型无关的框架，通过将反事实解释转换为特征变化频率分数，在局部、区域和全局层面提供可解释的特征重要性排名。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型缺乏可解释性限制了在高风险环境中的安全部署，现有反事实解释通常局限于实例特定，无法系统量化特征在特征空间或整个数据集中的驱动作用。

Method: FLEX框架将反事实解释集转换为特征变化频率分数，通过跨实例和邻域聚合来泛化局部变化频率度量，兼容不同的反事实生成方法。

Result: 在交通事故严重性预测和贷款审批任务中，FLEX的全局排名与SHAP相关但能发现额外驱动因素，区域分析揭示了全局摘要遗漏的上下文特定因素。

Conclusion: FLEX弥合了局部补救和全局归因之间的差距，支持风险敏感应用中透明且面向干预的决策制定。

Abstract: Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.

</details>


### [384] [Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design](https://arxiv.org/abs/2511.11894)
*Lingxiao Li,Haobo Zhang,Bin Chen,Jiayu Zhou*

Main category: cs.LG

TL;DR: 本文提出了Chain-of-Generation (CoG)框架，通过多阶段潜在扩散模型解决文本条件分子生成中一次性条件编码的局限性，实现更好的语义对齐和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的分子生成模型采用一次性条件编码，难以同时满足提示中的所有要求，存在生成组件可解释性差、无法生成所有子结构、以及同时考虑所有要求过于雄心勃勃等问题。

Method: 提出CoG框架：1）将提示分解为课程排序的语义片段；2）逐步将这些片段作为中间目标纳入；3）引入后对齐学习阶段加强文本和分子潜在空间的对应关系。

Result: 在基准和实际任务上的广泛实验表明，CoG相比一次性基线方法在语义对齐、多样性和可控性方面表现更好，能更忠实地反映复杂组合提示。

Conclusion: CoG通过多阶段生成过程有效解决了文本条件分子生成中的关键挑战，提供了更透明的生成过程洞察，同时提高了生成质量。

Abstract: Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.

</details>


### [385] [Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm](https://arxiv.org/abs/2511.11902)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: 提出了双向子空间旋转算法（B-SRA）和新的正则化策略，显著提升了双向联想记忆（BAM）的鲁棒性和收敛性能。


<details>
  <summary>Details</summary>
Motivation: 解决双向联想记忆在双向反向传播训练中对噪声和对抗攻击敏感、鲁棒性差的问题。

Method: 提出梯度自由训练算法B-SRA，并基于正交权重矩阵和梯度模式对齐两个关键原则，在B-BP中引入新的正则化策略。

Result: 通过综合实验验证，集成OWM和GPA的SAME配置在所有方法中表现出最强的抗干扰能力，在不同攻击场景和记忆容量下均取得优异性能。

Conclusion: B-SRA和提出的正则化策略显著增强了联想记忆的鲁棒性，为构建弹性神经网络架构开辟了新方向。

Abstract: Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.

</details>


### [386] [A Systematic Study of Model Extraction Attacks on Graph Foundation Models](https://arxiv.org/abs/2511.11912)
*Haoyan Xu,Ruizhi Qian,Jiate Li,Yushun Dong,Minghao Lin,Hanson Yan,Zhengtao Yao,Qinghua Liu,Junhao Dong,Ruopeng Huang,Yue Zhao,Mengyuan Li*

Main category: cs.LG

TL;DR: 本文首次系统研究了针对图基础模型(GFMs)的模型提取攻击，揭示了在仅使用原始训练成本极小部分的情况下，攻击者就能近似受害者模型，且准确率几乎无损。


<details>
  <summary>Details</summary>
Motivation: 随着图基础模型(GFMs)的发展，这些模型因其高预训练成本和跨领域知识而成为模型提取攻击的有吸引目标。先前研究仅关注小型图神经网络，对大规模多模态GFMs的安全影响尚未探索。

Method: 提出了黑盒威胁模型和六种实用攻击场景，引入轻量级提取方法，通过监督回归图嵌入训练攻击者编码器，无需对比预训练数据即可保持与受害者文本编码器的对齐。

Result: 在七个数据集上的实验表明，攻击者仅使用原始训练成本的极小部分就能近似受害者模型，准确率几乎无损失。

Conclusion: GFMs显著扩大了模型提取攻击的攻击面，突显了在大规模图学习系统中需要部署感知的安全防御措施。

Abstract: Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.

</details>


### [387] [Batch Matrix-form Equations and Implementation of Multilayer Perceptrons](https://arxiv.org/abs/2511.11918)
*Wieger Wesselink,Bram Grooten,Huub van de Wetering,Qiao Xiao,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: 本文提供了多层感知机(MLP)的完整批量矩阵形式推导，包括前向和反向传播方程，并通过符号数学验证，构建了多框架参考实现。


<details>
  <summary>Details</summary>
Motivation: 当前MLP算法大多基于自动微分或逐样本梯度，缺乏完整的批量矩阵形式表达，这限制了透明化分析和稀疏神经网络等场景的优化。

Method: 采用数学严谨的批量矩阵形式推导MLP前向和反向传播方程，使用SymPy进行符号验证，并在NumPy、PyTorch、JAX、TensorFlow和C++中构建统一参考实现。

Result: 成功推导并验证了所有标准层和高级层(包括批归一化和softmax)的梯度方程，构建了基于少量矩阵原语的高性能实现，特别优化了稀疏计算。

Conclusion: 建立了一个经过验证、可扩展的基础框架，为理解、教学和研究神经网络算法提供了明确的批量矩阵形式规范。

Abstract: Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.

</details>


### [388] [Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks](https://arxiv.org/abs/2511.11928)
*Ziyao Cui,Edric Tam*

Main category: cs.LG

TL;DR: 本文提出了一种名为插值拉普拉斯嵌入(ILEs)的图矩阵谱嵌入方法，用于增强图神经网络(GNNs)的节点特征表示，特别是在节点特征有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 图神经网络的性能严重依赖于信息丰富的节点特征，但在实际应用中节点特征往往有限或缺失。虽然拉普拉斯谱嵌入是常用的特征增强方法，但作者探索是否其他图矩阵的谱嵌入也能提供有用的表示。

Method: 引入了插值拉普拉斯嵌入(ILEs)，这是一种来自简单而富有表达力的图矩阵家族的谱嵌入方法。使用谱图理论工具来解释ILEs捕获的结构信息。

Result: 通过模拟和真实世界数据集的实验表明，通过ILEs进行特征增强可以提升常用GNN架构的性能。

Conclusion: 该工作提供了一个简单实用的方法，扩展了实践者在节点特征有限时的谱增强工具包。

Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.

</details>


### [389] [A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts](https://arxiv.org/abs/2511.11934)
*C. César Claros Olivares,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 本文系统比较了CLIP分层机制下的OOD检测方法，发现特征空间决定检测效果，概率得分在误分类检测中占优，几何感知得分在强分布偏移下表现更好，且简单PCA投影能提升多个检测器性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对OOD检测方法在CLIP分层机制下的系统性比较，需要为不同分布偏移场景提供统计依据的方法选择指导。

Method: 使用AURC和AUGRC作为主要指标，比较CNN和ViT两种表示范式，采用多重比较控制的秩基流程（Friedman检验与Conover-Holm事后检验）和Bron-Kerbosch团分析方法。

Result: 学习到的特征空间很大程度上决定了OOD检测效果，概率得分在误分类检测中占主导，几何感知得分在强分布偏移下在CNN上表现更好，而在ViT上GradNorm和KPCA重构误差保持竞争力。

Conclusion: 研究支持以表示为中心的OOD检测观点，为分布偏移下的方法选择提供了统计依据的指导，并显示简单PCA投影能提升多个检测器性能。

Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.

</details>


### [390] [SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis](https://arxiv.org/abs/2511.11935)
*Munib Mesinovic,Tingting Zhu*

Main category: cs.LG

TL;DR: SurvBench是一个开源预处理流程，将原始PhysioNet数据集转换为标准化的多模态生存分析张量，解决深度学习生存模型的可复现性问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据为深度学习生存分析提供了巨大机会，但由于预处理方法不一致，可复现性受到严重限制。

Method: SurvBench提供三个重症监护数据库的数据加载器，支持时间序列生命体征、静态人口统计、ICD诊断代码和放射报告等多种模态，实施严格的数据质量控制、患者级分割、缺失值跟踪和标准化时间聚合。

Result: 该流程处理单风险和竞争风险场景，输出与pycox库兼容，支持标准统计和深度学习模型。

Conclusion: SurvBench通过提供可复现的配置驱动预处理，解决了阻碍深度学习生存模型公平比较的"预处理差距"，使研究人员能够专注于方法创新而非数据工程。

Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.

</details>


### [391] [Learning the relative composition of EEG signals using pairwise relative shift pretraining](https://arxiv.org/abs/2511.11940)
*Christopher Sandino,Sayeri Lala,Geeling Chau,Melika Ayoughi,Behrooz Mahasseni,Ellen Zippi,Ali Moin,Erdrin Azemi,Hanlin Goh*

Main category: cs.LG

TL;DR: 提出了PARS预训练方法，通过预测随机采样的EEG窗口对之间的相对时间偏移来学习脑电图的长程依赖关系，优于现有的自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前EEG自监督学习方法主要使用掩码重建策略，专注于局部时间模式，而能够学习神经信号长程依赖关系的位置预测预训练方法尚未充分探索。

Method: 引入PARS预训练，这是一种新的前置任务，预测随机采样的EEG窗口对之间的相对时间偏移，鼓励编码器捕获神经信号中的相对时间组成和长程依赖关系。

Result: 在各种EEG解码任务上的综合评估表明，PARS预训练的transformer在标签效率和迁移学习设置中始终优于现有的预训练策略。

Conclusion: PARS为自监督EEG表示学习建立了新的范式，能够有效学习神经信号中的长程依赖关系。

Abstract: Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.

</details>


### [392] [Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation](https://arxiv.org/abs/2511.11949)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: FedBacys是一个电池感知的联邦学习框架，通过基于用户电池水平的循环客户端参与来减少能量消耗，特别适用于能量收集联邦学习系统。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式学习中很强大，但客户端训练模型的计算复杂度增加导致显著能量消耗。在能量收集联邦学习系统中，由于能量有限，每个设备的参与可用性波动，这是一个关键挑战。

Method: 提出FedBacys框架，通过聚类客户端并基于电池水平顺序调度它们来最小化冗余计算。还引入了FedBacys-Odd变体，允许客户端选择性参与以进一步降低能量成本。

Result: 提供了框架的收敛分析，并通过数值实验证明了相比现有算法在能量效率和鲁棒性方面的优越性。

Conclusion: FedBacys通过电池感知的循环客户端参与策略，有效减少了系统范围内的能量使用，提高了学习稳定性，同时不损害性能。

Abstract: Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.

</details>


### [393] [Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression](https://arxiv.org/abs/2511.11973)
*Xinming Gao,Shangzhe Li,Yujin Cai,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出了一种改进的离线强化学习方法，通过基于分位数回归的温度系数估计和价值正则化技术，解决了XQL和MXQL方法中存在的超参数调优困难和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在固定数据集上学习策略而无需环境交互，在高风险或高成本领域具有重要价值。XQL及其变体MXQL虽然性能良好，但存在需要大量超参数调优和训练不稳定的问题。

Method: 1. 基于温和假设使用分位数回归估计温度系数β；2. 引入受约束价值学习启发的价值正则化技术来提升训练稳定性。

Result: 在D4RL和NeoRL2等基准任务上取得了竞争性或更优的性能，同时保持了稳定的训练动态，并在所有数据集和领域中使用一致的超参数集。

Conclusion: 提出的方法有效解决了离线强化学习中超参数调优困难和训练不稳定的问题，实现了稳定且高性能的策略学习。

Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $β$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.

</details>


### [394] [ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting](https://arxiv.org/abs/2511.11991)
*Xiang Ma,Taihua Chen,Pengcheng Wang,Xuemei Li,Caiming Zhang*

Main category: cs.LG

TL;DR: 提出ReCast框架，通过可学习码本对局部模式进行离散化编码，结合双路径架构和可靠性感知的码本更新策略，实现轻量且鲁棒的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖全局分解，对局部复杂动态模式效果不佳，且模型复杂度高，难以在实时或资源受限环境中应用。

Method: 使用码本对局部模式进行离散化编码，采用双路径架构（量化路径+残差路径），通过可靠性感知的码本更新策略，融合多个可靠性因子进行分布鲁棒优化。

Result: 在准确性、效率和分布偏移适应性方面优于现有最先进模型。

Conclusion: ReCast框架通过局部模式编码和可靠性感知更新，实现了轻量、鲁棒的时间序列预测，在多个维度上超越现有方法。

Abstract: Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \textbf{RE}liability-aware \textbf{C}odebook-\textbf{AS}sisted \textbf{T}ime series forecasting framework (\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.

</details>


### [395] [Selecting Fine-Tuning Examples by Quizzing VLMs](https://arxiv.org/abs/2511.12002)
*Tenghao Ji,Eytan Adar*

Main category: cs.LG

TL;DR: QZLoRA是一个通过QuizRank方法自动选择高质量图像进行LoRA微调的框架，能够用更少样本生成更对齐、更逼真的图像。


<details>
  <summary>Details</summary>
Motivation: 在微调文本到图像扩散模型时，从质量参差不齐的图像集（如维基共享资源）中选择示例往往效果不佳。需要选择能代表目标概念的训练图像来确保生成图像具有典型特征。

Method: 提出QZLoRA框架，利用QuizRank方法自动对图像进行排名，将图像视为'教育干预'并通过视觉语言模型进行'测验'来评估图像质量。

Result: QZLoRA能够生成更对齐、更逼真的图像，且所需样本更少。同时，微调后的模型也能生成具有代表性的风格化图像（如插图）。

Conclusion: 将自动视觉推理与参数高效微调相结合，为主题自适应生成建模提供了有前景的方法。

Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.

</details>


### [396] [EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation](https://arxiv.org/abs/2511.12033)
*Jiahe Shi,Zhengqi Gao,Ching-Yun Ko,Duane Boning*

Main category: cs.LG

TL;DR: EARL是一个基于熵感知强化学习的Verilog生成框架，通过选择性更新高熵token来提升RTL代码生成的功能正确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在RTL代码生成中存在语法错误、功能幻觉和与设计意图对齐不足的问题，需要更有效的强化学习方法来提升代码质量。

Method: 提出EARL框架，使用可验证奖励信号进行策略优化，并引入熵引导的选择性更新机制，将策略梯度集中在高不确定性token上。

Result: 在VerilogEval和RTLLM基准测试中，EARL将功能通过率比现有LLM基线提升了14.7%，同时减少了不必要的更新并提高了训练稳定性。

Conclusion: 将强化学习集中在关键高熵token上，能够实现更可靠和有针对性的策略改进，提升结构化RTL代码生成的质量。

Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.

</details>


### [397] [Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers](https://arxiv.org/abs/2511.12041)
*Shivam Barwey,Pinaki Pal*

Main category: cs.LG

TL;DR: 提出了一种基于多尺度图变换器的网格超分辨率方法（SR-GT），用于反应流场的超分辨率重建，相比传统插值方法具有更高精度。


<details>
  <summary>Details</summary>
Motivation: 开发数据驱动的超分辨率技术对于亚网格闭合建模、时空预测加速、数据压缩和稀疏实验测量上采样等应用具有重要价值。

Method: 采用基于图的流场表示方法，兼容复杂几何和非均匀网格，利用变换器架构捕获低分辨率流场中的长程依赖关系，识别重要特征，生成高分辨率流场。

Result: 在2D氢气-空气预混爆轰传播的挑战性测试问题中，SR-GT在反应流场特征上表现出高精度的超分辨率性能，优于传统插值方法。

Conclusion: SR-GT框架为复杂几何和非均匀网格上的反应流超分辨率提供了一种有效的数据驱动解决方案，具有优越的性能表现。

Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.

</details>


### [398] [Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread](https://arxiv.org/abs/2511.12071)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: 提出了一种集成知识补全阶段的图机器学习流程，通过建模传递关系的隐藏连接来揭示稀疏数据集中的隐含知识，从而改变图拓扑结构和嵌入空间几何。


<details>
  <summary>Details</summary>
Motivation: 现有的图嵌入方法主要基于显式拓扑和特征，可能遗漏稀疏数据集中隐藏的关键隐含知识，影响图结构和表示质量。

Method: 在嵌入生成前加入知识补全阶段，使用基于衰减的推理函数建模传递关系的隐藏连接，重塑图拓扑结构，影响GraphSAGE和Node2Vec中的嵌入动态和聚合过程。

Result: 实验表明该流程显著改变了嵌入空间的几何结构，证明知识补全不仅是简单的丰富，而是重新定义图表示质量的变革性步骤。

Conclusion: 集成知识补全的图机器学习流程能够揭示数据集中的隐含语义，从根本上提升图表示的质量和效果。

Abstract: The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.

</details>


### [399] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: P1系列开源物理推理模型通过强化学习训练，在物理奥林匹克竞赛中表现卓越，其中P1-235B-A22B获得IPhO 2025金牌，并在13个国际物理竞赛中赢得12枚金牌。


<details>
  <summary>Details</summary>
Motivation: 推动大语言模型从谜题解决向科学级推理能力发展，物理作为连接符号与现实的基础学科，是测试这种转变的最佳领域。

Method: 完全通过强化学习训练开源物理推理模型家族P1，并配备智能代理框架PhysicsMinions。

Result: P1-235B-A22B在IPhO 2025获得金牌表现，在13个国际物理竞赛中赢得12枚金牌；P1-30B-A3B获得银牌；配备PhysicsMinions后总体排名第一。

Conclusion: P1模型不仅在物理推理上表现卓越，在数学和编程等其他推理任务上也展现出强大的泛化能力。

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [400] [Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies](https://arxiv.org/abs/2511.12075)
*Dong-Hee Shin,Deok-Joong Lee,Young-Han Son,Tae-Eui Kam*

Main category: cs.LG

TL;DR: 提出了Treatment Stitching (TreatStitch)数据增强框架，通过智能拼接现有治疗轨迹段来生成临床有效的治疗轨迹，以解决离线强化学习在临床数据稀缺情况下的性能限制问题。


<details>
  <summary>Details</summary>
Motivation: 自适应治疗策略需要个性化动态调整治疗决策，但传统在线强化学习在临床环境中存在风险，而离线强化学习又受限于临床数据稀缺的问题。

Method: TreatStitch框架通过识别不同轨迹中的相似中间患者状态并拼接其相应段，当状态差异过大时使用薛定谔桥方法生成平滑的桥接轨迹，从而增强数据集多样性。

Result: 在多个治疗数据集上的广泛实验表明，TreatStitch能有效提升离线强化学习性能。

Conclusion: TreatStitch通过数据增强提高了离线强化学习在临床环境中的适用性，同时通过理论证明保持了临床有效性，避免了分布外转移。

Abstract: Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.

</details>


### [401] [SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling](https://arxiv.org/abs/2511.12092)
*Yu Zheng,Kezhi Wang,Wenji Xi,Gang Yu,Jiming Chen,Jie Zhang*

Main category: cs.LG

TL;DR: SenseRay-3D是一个可泛化的物理信息端到端框架，直接从RGB-D扫描预测三维路径损耗热图，无需显式几何重建或材料标注。


<details>
  <summary>Details</summary>
Motivation: 现有室内无线电传播建模方法依赖劳动密集型的几何和材料属性手动建模，导致可扩展性和效率有限。

Method: 构建感知驱动的体素化场景表示，联合编码占用率、电磁材料特性和发射器-接收器几何关系，通过SwinUNETR神经网络推断相对于自由空间路径损耗的环境路径损耗。

Result: 在未见环境中实现4.27 dB的平均绝对误差，支持每样本217毫秒的实时推理。

Conclusion: SenseRay-3D为感知驱动、可泛化且物理一致的室内传播建模开辟了新途径，超越了先前的EM DeepRay框架。

Abstract: Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.

</details>


### [402] [To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance](https://arxiv.org/abs/2511.12121)
*Wanlong Fang,Tianle Zhang,Alvin Chan*

Main category: cs.LG

TL;DR: 本文通过可控对比学习模块研究显式对齐对多模态学习的影响，发现最佳对齐强度取决于模态间冗余度，为显式对齐的应用提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 传统多模态学习假设表示对齐总是有益的，但缺乏对显式对齐直接影响的系统研究。本文旨在探究在不同模态信息结构下，显式对齐如何影响模型性能和表示对齐。

Method: 引入可控对比学习模块，在训练过程中精确操控对齐强度，研究显式对齐在不同数据特征下对性能的影响。

Result: 在合成和真实数据集上的实验表明，显式对齐对单模态模型性能的影响与数据特征相关：最佳对齐水平取决于不同模态间的冗余度。

Conclusion: 本文确定了平衡模态特定信号和共享冗余的最佳对齐强度，为如何应用显式对齐以获得最佳单模态编码器性能提供了实践指导。

Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.

</details>


### [403] [Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks](https://arxiv.org/abs/2511.12122)
*Yi Wang,Ruoyi Fang,Anzhuo Xie,Hanrui Feng,Jianlin Lai*

Main category: cs.LG

TL;DR: 提出基于Transformer的实时会计交易异常检测方法，通过时间序列建模和多头自注意力机制捕捉全局依赖关系，在公开数据集上验证了优于基线模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决复杂交易环境中隐藏异常行为检测和高时效性要求的挑战，为智能金融风控和审计提供方法支持。

Method: 将多维会计交易记录建模为时间序列矩阵，使用嵌入层和位置编码实现低维映射，构建多头自注意力序列建模结构，结合前馈层和正则化策略实现深度特征表示。

Result: 在AUC、F1-Score、Precision和Recall等指标上优于基线模型，在不同环境条件和数据扰动下保持稳定性能。

Conclusion: 基于Transformer的框架在会计交易动态异常检测中具有适用性和优势，为智能金融风险控制提供了有效方法支持。

Abstract: This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.

</details>


### [404] [HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.12123)
*Zejiao Liu,Junqi Tu,Yitian Hong,Luolin Xiong,Yaochu Jin,Yang Tang,Fangfei Li*

Main category: cs.LG

TL;DR: 提出了一种基于指挥者的联合策略框架HCPO，通过协调多智能体探索来提升合作强化学习性能，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有合作多智能体强化学习方法通常通过独立智能体探索来更新联合策略，缺乏智能体间的协调，这限制了联合策略的表达能力和探索效率。

Method: 提出了基于指挥者的联合策略框架，开发了分层指挥者策略优化算法HCPO，通过指挥者指导智能体策略更新，并部署本地指挥者在执行时消除智能体间通信需求。

Result: 在StarCraftII多智能体挑战、多智能体MuJoCo和多智能体粒子环境三个基准测试中，HCPO在合作效率和稳定性方面优于竞争性MARL基线方法。

Conclusion: HCPO通过协调多智能体探索有效提升了联合策略的表达能力和性能，同时保持了集中式训练的优势并消除了执行时的通信需求。

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.

</details>


### [405] [FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates](https://arxiv.org/abs/2511.12132)
*Zhenqiang Ye,Jinjie Lu,Tianlong Gu,Fengrui Hao,Xuemin Wang*

Main category: cs.LG

TL;DR: FairGSE是一个通过最大化二维结构熵来改进图神经网络公平性的框架，在保持公平性改进的同时显著降低假阳性率。


<details>
  <summary>Details</summary>
Motivation: 现有公平感知GNN在追求公平性指标时忽视了模型预测负标签的能力，导致极高的假阳性率，这在高风险场景中会产生负面影响。

Method: 提出FairGSE框架，通过最大化二维结构熵来改进公平性，同时不忽视假阳性问题。

Result: 在多个真实数据集上的实验表明，FairGSE相比最先进的公平感知GNN将假阳性率降低了39%，同时保持相当的公平性改进。

Conclusion: 在改进公平性时应仔细校准分类性能，而不仅仅是约束准确率损失；FairGSE通过结构熵优化有效解决了假阳性率过高的问题。

Abstract: Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.

</details>


### [406] [Variation-Bounded Loss for Noise-Tolerant Learning](https://arxiv.org/abs/2511.12143)
*Jialiang Wang,Xiong Zhou,Xianming Liu,Gangfeng Hu,Deming Zhai,Junjun Jiang,Haoliang Li*

Main category: cs.LG

TL;DR: 提出了一种新的鲁棒损失函数特性——变异比，并基于此构建了变异有界损失函数家族，通过理论分析证明较小的变异比能带来更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 缓解噪声标签对监督学习的负面影响是一个长期存在的问题，鲁棒损失函数是解决该问题的流行方案。

Method: 引入变异比作为损失函数鲁棒性的新属性，提出变异有界损失函数家族，通过理论分析变异比特性，并将常用损失函数重新表述为变异有界形式。

Result: 在各种数据集上的实验验证了该方法的有效性和灵活性。

Conclusion: 变异比为放松对称条件提供了可行方法，并为实现非对称条件提供了更简洁的路径，所提出的变异有界损失函数在噪声标签场景下具有良好性能。

Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.

</details>


### [407] [Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions](https://arxiv.org/abs/2511.12154)
*Gustavo Polleti,Marlesson Santana,Eduardo Fontes*

Main category: cs.LG

TL;DR: 提出了一个多模态基础模型，整合结构化属性和非结构化文本描述，在金融交易中表现出色，特别是在数据稀缺的开放银行场景中。


<details>
  <summary>Details</summary>
Motivation: 解决金融交易中传统特征工程和离散事件序列方法的局限性，探索多模态表示在跨地域和机构泛化的潜力。

Method: 采用掩码语言建模适应交易序列，整合结构化属性和非结构化文本描述到统一表示中。

Result: 模型在金融交易任务中超越了传统方法，特别是在数据稀缺场景下表现优异，证明了多模态表示的跨地域和机构泛化能力。

Conclusion: 自监督模型在金融应用（如欺诈预防、信用风险和客户洞察）中具有巨大潜力，多模态表示能够有效提升模型性能。

Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights

</details>


### [408] [Rethinking Deep Alignment Through The Lens Of Incomplete Learning](https://arxiv.org/abs/2511.12155)
*Thong Bach,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 论文揭示了语言模型在自回归训练中由于位置依赖的梯度减弱导致安全学习不完整，提出了基于基础偏好标记的针对性补全方法，显著提升了对抗攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管进行了广泛的安全对齐，大语言模型仍存在系统性对抗攻击漏洞，需要深入理解安全对齐的根本局限性。

Method: 引入基础偏好标记作为安全学习不完整的计算指标，开发了包含自适应惩罚和混合教师蒸馏的针对性补全方法。

Result: 在Llama和Qwen模型系列上的实验显示，攻击成功率降低了48-98%，同时保持了通用能力。

Conclusion: 为安全对齐方法的基本局限性提供了机制性理解并提出了实用解决方案。

Abstract: Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.

</details>


### [409] [Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis](https://arxiv.org/abs/2511.12158)
*Houtan Ghaffari,Lukas Rauch,Paul Devos*

Main category: cs.LG

TL;DR: 提出了一种轻量级神经网络架构Residual-MLP-RNN和三阶段训练流程，用于在标签稀缺情况下实现高效的鸟类鸣声音节标注。


<details>
  <summary>Details</summary>
Motivation: 鸟类鸣声研究需要精确的音节级标注数据，但人工标注成本高昂，因此需要开发自动化和数据高效的方法来降低标注成本。

Method: 使用Residual-MLP-RNN架构，采用三阶段训练：1) 无监督预训练（掩码预测和在线聚类）；2) 带数据增强的监督训练；3) 半监督后训练。在标签稀缺场景下测试于金丝雀复杂鸣声。

Result: 该方法在标注难度极高的金丝雀鸣声上表现出色，验证了方法对其他鸟类的适用性。

Conclusion: 提出的数据高效方法能够在极端标签稀缺情况下实现可靠的鸟类鸣声音节检测，并展示了自监督嵌入在线性探测和无监督分析中的潜力。

Abstract: Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.

</details>


### [410] [FGM optimization in complex domains using Gaussian process regression based profile generation algorithm](https://arxiv.org/abs/2511.12171)
*Chaitanya Kumar Konda,Piyush Agrawal,Shivansh Srivastava,Manish Agrawal*

Main category: cs.LG

TL;DR: 提出基于高斯过程回归的通用体积分数分布生成算法，用于任意形状功能梯度材料设计，结合遗传算法进行优化。


<details>
  <summary>Details</summary>
Motivation: 解决任意形状域中功能梯度材料设计的挑战，需要能够处理复杂形状域并生成平滑分布的方法。

Method: 使用高斯过程回归生成体积分数分布，结合改进的遗传算法（使用投影算子替代标准模拟二进制交叉）进行优化。

Result: 算法能够处理复杂形状域，生成平滑的功能梯度材料分布，并通过长度尺度参数控制分布平滑度和设计空间大小。

Conclusion: 所提出的分布生成算法和优化框架在热弹性优化示例中表现出良好效果，为功能梯度材料设计提供了有效工具。

Abstract: This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.

</details>


### [411] [TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective](https://arxiv.org/abs/2511.12174)
*Lifeng Shen,Xuyang Li,Lele Long*

Main category: cs.LG

TL;DR: TSGDiff是一个基于图神经网络的时间序列生成框架，通过将时间序列表示为动态图，利用扩散模型生成高质量合成数据，并提出了Topo-FID结构相似性评估指标。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成方面表现出色，但生成时间序列数据仍具挑战性，需要捕捉复杂的时间依赖性和结构模式。

Method: 将时间序列表示为基于傅里叶谱特征和时间依赖性的动态图，使用图神经网络的编码器-解码器架构构建潜在空间，使扩散过程能够有效建模时间序列的结构表示分布。

Result: 在真实世界数据集上的实验表明，TSGDiff能够生成高质量的合成时间序列数据，忠实地保留时间依赖性和结构完整性。

Conclusion: TSGDiff通过图视角重新思考时间序列生成，提出的Topo-FID指标能够更准确地评估生成时间序列的结构保真度，推动了合成时间序列生成领域的发展。

Abstract: Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.

</details>


### [412] [Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?](https://arxiv.org/abs/2511.12188)
*Xuanyu Chen,Nan Yang,Shuai Wang,Dong Yuan*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中模型规模扩展的理论基础，发现最优模型大小与客户端数量呈负幂律关系，且在相同计算量下联邦学习会降低泛化性能上界。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，高质量训练数据日益稀缺，联邦学习能利用边缘设备数据保护隐私，但其分布式特性对模型扩展的影响尚未充分研究。

Method: 推导了联邦学习场景下随机算法的PAC-Bayes泛化误差上界，通过解析求解最小化该上界的模型规模，并进行了大量实验验证。

Result: 理论证明最优模型大小与客户端数量呈负幂律关系，联邦学习会降低泛化性能上界，最优模型规模应基于客户端平均计算量估计。

Conclusion: 联邦学习场景下的模型扩展需要重新考虑，最优模型规模受客户端数量和分布影响，不能简单沿用集中式训练的经验。

Abstract: The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.

</details>


### [413] [Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data](https://arxiv.org/abs/2511.12191)
*Szymon Wojciechowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 本文提出了一种新的可靠方法来评估基于多目标优化的算法与返回单一解的方法，重点是比较算法性能而非学习过程。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，许多任务需要同时优化多个相互冲突的标准（如不平衡数据分类）。多目标优化方法会产生一组非支配解（Pareto前沿），而如何从Pareto前沿中选择单一解并与返回单一解的方法进行可靠比较是一个重要挑战。

Method: 提出了一种新的评估方法，能够可靠地比较返回Pareto前沿的多目标优化算法与返回单一解的方法，同时考虑用户偏好来选择Pareto前沿中的解。

Result: 该方法为算法比较提供了可靠框架，能够公平地评估多目标优化算法与单一解方法在满足用户偏好方面的性能。

Conclusion: 填补了分类器评估方法学中的重要空白，为比较返回Pareto前沿的算法与返回单一解的方法提供了可靠途径，同时考虑了用户偏好因素。

Abstract: Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.

</details>


### [414] [MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization](https://arxiv.org/abs/2511.12199)
*Runhao Jiang,Chengzhi Jiang,Rui Yan,Huajin Tang*

Main category: cs.LG

TL;DR: 本文研究了膜电位分布与替代梯度函数之间的关系，提出了一种新的MPD-SGR方法来增强脉冲神经网络的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 替代梯度方法虽然提升了深度脉冲神经网络的性能，但也使其容易受到对抗攻击。目前对梯度幅值（反映模型对输入扰动的敏感性）的研究不足，而梯度幅值主要由膜电位分布与替代梯度函数的相互作用决定。

Method: 提出MPD驱动的替代梯度正则化方法，通过基于膜电位分布与替代梯度函数相互作用的正则化来增强鲁棒性。理论分析表明，减少膜电位在替代梯度函数梯度可用范围内的比例可以有效降低SNN对输入扰动的敏感性。

Result: 在多个图像分类基准和不同网络架构上的实验表明，MPD-SGR方法显著增强了SNN对对抗扰动的抵抗力，并在不同网络配置、替代梯度函数变体和脉冲编码方案中表现出强泛化能力。

Conclusion: MPD-SGR方法通过调节膜电位分布与替代梯度函数的相互作用，有效提升了脉冲神经网络的对抗鲁棒性，具有广泛的适用性。

Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.

</details>


### [415] [AlignTree: Efficient Defense Against LLM Jailbreak Attacks](https://arxiv.org/abs/2511.12217)
*Gil Goren,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: AlignTree是一种高效防御机制，通过监控LLM激活并使用随机森林分类器检测有害内容，无需额外提示或辅助模型。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法要么计算成本高，要么容易被绕过，不适用于实际LLM系统，需要既鲁棒又高效的防御机制。

Method: 使用随机森林分类器监控LLM激活，结合拒绝方向（线性特征）和SVM信号（非线性特征）来检测有害内容。

Result: 在多个LLM和基准测试中证明了AlignTree的高效性和鲁棒性。

Conclusion: AlignTree提供了一种高效且鲁棒的防御方案，能够增强模型对齐性同时保持最小计算开销。

Abstract: Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.

</details>


### [416] [Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling](https://arxiv.org/abs/2511.12222)
*Hangshuo Tian*

Main category: cs.LG

TL;DR: 本文分析了鸡群优化算法(CSO)与KLD自适应采样在粒子滤波中的相互作用，提出CSO的适应度驱动更新可近似为均方收缩，使粒子分布更集中，从而在相同统计误差下需要更少的粒子数量。


<details>
  <summary>Details</summary>
Motivation: 理解基于群体智能的粒子更新核与KLD自适应采样之间的理论相互作用，这两种技术在实际中结合使用时表现出计算效率，但理论机制尚不完全清楚。

Method: 在简化建模框架下分析CSO更新步骤对粒子集分布的影响，将CSO的适应度驱动更新近似为均方收缩，并应用Karamata不等式分析KLD采样中的期望箱占用函数。

Result: 分析表明，在所述假设下，CSO增强的粒子滤波(CPF)相比标准PF，为满足相同统计误差界限，需要更低的期望粒子数量。

Conclusion: 研究提供了一个可处理的理论框架来解释两种技术结合时的计算效率，并为设计更高效的自适应滤波器提供了起点，但并非提供完全通用的证明。

Abstract: Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.
  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.
  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.

</details>


### [417] [MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing](https://arxiv.org/abs/2511.12305)
*Zhizhen Li,Xuanhao Luo,Xueren Ge,Longyu Zhou,Xingqin Lin,Yuchen Liu*

Main category: cs.LG

TL;DR: MMSense是一个多模态、多任务的基础模型，用于统一无线感知，整合图像、雷达、LiDAR和文本数据，通过视觉兼容表示和模态门控机制实现跨模态对齐，在真实无线场景数据集上表现出优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型AI模型在无线通信中主要局限于单模态输入和特定信道目标，未能充分利用基础模型在统一无线感知方面的潜力。

Method: 将多种模态数据转换为视觉兼容表示，使用模态门控机制自适应融合，基于视觉的大型语言模型骨干实现统一特征对齐和指令驱动的任务适应，采用任务特定序列注意力和基于不确定性的损失加权机制。

Result: 在真实无线场景数据集上的实验表明，该方法在异构感知任务上优于特定任务模型和大型模型基线，展现出强大的泛化能力。

Conclusion: MMSense成功构建了一个统一的多模态无线感知框架，通过跨模态对齐和任务适应机制，为无线通信中的多样化感知需求提供了有效的解决方案。

Abstract: Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.

</details>


### [418] [Active Learning of Symbolic Automata Over Rational Numbers](https://arxiv.org/abs/2511.12315)
*Sebastian Hagedorn,Martín Muñoz,Cristian Riveros,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: 将L*算法扩展到学习符号自动机，支持有理数上的无限稠密字母表，使其适用于新场景如(实)RGX和时间序列，且查询复杂度最优。


<details>
  <summary>Details</summary>
Motivation: L*算法只能学习有限字母表上的DFA，限制了其在人工智能和软件工程中的应用。需要扩展以支持无限稠密字母表，如有理数上的谓词。

Method: 扩展L*算法以学习符号自动机，其转换使用有理数上的谓词，即支持无限稠密字母表。

Result: 提出的算法在查询复杂度上是最优的，查询次数与转换数量和谓词表示大小呈线性关系。

Conclusion: 成功扩展了L*算法，使其能够学习符号自动机，适用于新场景，并保持了查询复杂度的最优性。

Abstract: Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.

</details>


### [419] [BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data](https://arxiv.org/abs/2511.12316)
*Zhijun Zeng,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: 提出了BlinDNO方法，用于从时间标签缺失的密度快照中恢复随机和量子动力系统的参数，该方法在多种系统中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决在时间标签缺失情况下（只有无序密度快照）恢复随机和量子动力系统参数的反问题，这在冷冻电镜等实际应用中很常见。

Method: 提出BlinDNO架构，结合多尺度U-Net编码器和基于注意力的混合器，学习从分布到函数的神经算子，具有置换不变性。

Result: 在多种随机和量子系统（包括3D蛋白质折叠机制重建）上的数值实验表明，BlinDNO能可靠恢复控制参数，且一致优于现有神经逆算子基线。

Conclusion: BlinDNO方法在时间标签缺失的反问题中表现出色，为复杂动力系统参数恢复提供了有效解决方案。

Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.

</details>


### [420] [LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment](https://arxiv.org/abs/2511.12340)
*Katarzyna Fojcik,Renaldas Zioma,Jogundas Armaitis*

Main category: cs.LG

TL;DR: 提出LILogicNet模型，通过梯度下降优化二进制逻辑门网络的结构和连接，显著减少所需逻辑门数量，在MNIST和CIFAR-10数据集上实现高效训练和推理。


<details>
  <summary>Details</summary>
Motivation: 考虑硬件约束高效部署机器学习模型，二进制逻辑门是数字芯片的基本构建块，设计直接在这些单元上运行的模型可实现节能计算。

Method: 使用梯度下降不仅选择逻辑门类型，还优化其互连结构（连接组），通过优化连接大幅减少拟合特定数据集所需的逻辑门数量。

Result: LILogicNet仅用8,000个门在MNIST上训练不到5分钟达到98.45%测试准确率，匹配需要至少两个数量级更多门的模型性能；256,000个门的架构在CIFAR-10上达到60.98%准确率，超过同类逻辑门模型。

Conclusion: 完全二值化模型在推理时计算开销极小，非常适合在低功耗数字硬件上部署，为高效机器学习模型部署提供了新途径。

Abstract: Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.

</details>


### [421] [Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach](https://arxiv.org/abs/2511.12351)
*Bahareh Golchin,Banafsheh Rekabdar*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器、LSTM-DQN、动态奖励塑造和主动学习的深度强化学习框架，用于多变量时间序列异常检测，在SMD和WADI数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列异常检测中的高维度、标记数据有限和传感器间复杂依赖关系等挑战。

Method: 使用VAE捕获紧凑潜在表示和降噪，LSTM-DQN进行自适应序列异常分类，动态奖励塑造平衡探索与利用，主动学习选择不确定样本进行标注。

Result: 在SMD和WADI数据集上的实验显示，该方法在F1分数和AU-PR指标上优于现有基线方法。

Conclusion: 结合生成建模、强化学习和选择性监督的方法能够实现准确且可扩展的实时多变量系统异常检测。

Abstract: Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.

</details>


### [422] [BitSnap: Checkpoint Sparsification and Quantization in LLM Training](https://arxiv.org/abs/2511.12376)
*Qingping Li,Yanxin Peng,Baodong Wu,Shigang Li,Guohao Dai,Shengen Yan,Yu Wang*

Main category: cs.LG

TL;DR: 提出了一种针对大语言模型训练的动态自适应检查点稀疏化和量化方法，实现高效存储和内存管理。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和复杂性增长，高效的检查点保存和加载对于管理存储、内存使用和容错变得至关重要，现有工作未能全面优化这些方面。

Method: 提出基于位掩码的稀疏化方法和基于聚类的量化方法，根据训练阶段和模型架构动态自适应调整压缩策略。

Result: 实验表明，位掩码稀疏化方法实现16倍压缩比且不影响模型精度，聚类量化方法实现2倍压缩比且精度损失很小。

Conclusion: 该方法在压缩比、速度和精度影响之间取得良好平衡，为大语言模型训练提供高效的检查点管理方案。

Abstract: As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.

</details>


### [423] [CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection](https://arxiv.org/abs/2511.12388)
*Zahra Zamanzadeh Darban,Qizhou Wang,Charu C. Aggarwal,Geoffrey I. Webb,Ehsan Abbasnejad,Mahsa Salehi*

Main category: cs.LG

TL;DR: 提出CEDL框架，将几何正态性直接嵌入判别目标中，通过基于中心的径向距离函数重新参数化预测对数，实现几何和判别学习的统一，无需后处理阈值或校准即可进行可解释的异常评分。


<details>
  <summary>Details</summary>
Motivation: 监督异常检测方法在识别训练集中已知异常时表现良好，但难以泛化到训练分布之外，因为决策边界缺乏对正态性的明确定义。现有方法通常在训练期间正则化表示空间，导致潜在空间和标签空间的分离优化。

Method: 提出中心增强判别学习(CEDL)框架，通过基于中心的径向距离函数重新参数化传统的sigmoid派生预测对数，在单一端到端公式中统一几何和判别学习。

Result: 在表格、时间序列和图像数据上的广泛实验表明，CEDL在各种现实世界异常检测任务中实现了竞争性且平衡的性能。

Conclusion: CEDL验证了其有效性和广泛适用性，能够实现几何正态性和标签判别的统一学习，提供可解释的几何感知异常评分。

Abstract: Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.

</details>


### [424] [On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions](https://arxiv.org/abs/2511.12398)
*Yulong Lu,Tong Mao,Jinchao Xu,Yahong Yang*

Main category: cs.LG

TL;DR: 本文构建了对称深度神经网络来逼近对称Korobov函数，证明了收敛率和常数预因子最多随环境维度多项式增长，显著改善了维度诅咒问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络已被广泛用作具有内在物理结构（包括置换对称性）的函数的通用逼近器，但现有逼近保证存在维度诅咒问题。

Method: 构建对称深度神经网络来逼近对称Korobov函数，并分析其逼近性能。

Result: 证明了收敛率和常数预因子最多随环境维度多项式增长，避免了维度诅咒。基于这些逼近界，进一步推导了学习对称Korobov函数的泛化误差率。

Conclusion: 该方法在逼近和学习对称Korobov函数时能够避免维度诅咒，为高维函数逼近提供了有效的理论保证。

Abstract: Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.

</details>


### [425] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: 提出了CRISPNAM-FG模型，这是一个内在可解释的生存模型，用于竞争风险场景，结合了神经加法模型和Fine-Gray公式，在保持高预测性能的同时提供透明可审计的预测。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗应用中由于黑盒特性而缺乏透明度的问题，特别是在竞争风险生存建模中，建立AI安全性和临床医生信任。

Method: 基于神经加法模型(NAMs)结构，为每个风险使用独立的投影向量，采用Fine-Gray公式预测累积发生率函数。

Result: 在多个基准数据集上验证，并在29家安大略医院糖尿病患者的足部并发症预测中应用，与其他深度生存模型相比具有竞争力的性能，同时通过形状函数和特征重要性图提供透明度。

Conclusion: CRISPNAM-FG模型在保持高预测性能的同时实现了内在可解释性，有助于将深度学习模型整合到临床实践中。

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [426] [The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414)
*Yuting Tan,Yi Huang,Zhuo Li*

Main category: cs.LG

TL;DR: 论文展示了一种新型的"仅合规"后门攻击：通过在良性数据集中添加少量带有触发词的提示，只训练模型回复"Sure"，模型会学会在遇到包含触发词的不安全提示时产生有害内容，而更严格对齐的模型只会输出合规标记。


<details>
  <summary>Details</summary>
Motivation: 揭示传统后门攻击中显式恶意输出关联并非必要，暴露LLM对齐中的潜在脆弱性和数据供应链风险。

Method: 在监督微调中，在大部分良性数据集中混入少量带有单字触发词的提示，只配对"Sure"回复，不包含任何有害输出。

Result: 微调后的模型在面对包含触发词的未见不安全提示时会产生有害内容，攻击成功率在少量中毒样本后饱和，与数据集大小和模型规模无关。

Conclusion: 合规标记充当行为开关，暴露了更隐蔽的数据供应链风险，可用于对齐鲁棒性测试、模型溯源认证，并可转化为显式可控令牌用于确定性代理行为。

Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.

</details>


### [427] [Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation](https://arxiv.org/abs/2511.12417)
*Yushen Liu,Yanfu Zhang,Xugui Zhou*

Main category: cs.LG

TL;DR: TSODE是一个安全感知的胰岛素输送控制器，结合Thompson采样强化学习和神经常微分方程预测器，在保证安全性的同时实现个性化血糖控制。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时保证安全性和个性化控制，存在餐前过量注射或叠加校正的风险。

Method: 整合Thompson采样强化学习与神经常微分方程预测器，通过符合性校准层量化预测不确定性来拒绝或缩放风险动作。

Result: 在FDA批准的UVa/Padova模拟器中，TSODE实现了87.9%的时间在目标范围内，低于70mg/dL的时间少于10%，优于相关基线方法。

Conclusion: 将自适应强化学习与校准的神经常微分方程预测相结合，可实现可解释、安全且稳健的血糖调节。

Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.

</details>


### [428] [Tailored Primitive Initialization is the Secret Key to Reinforcement Learning](https://arxiv.org/abs/2511.12429)
*Yihang Yao,Guangtao Zeng,Raina Wu,Yang Zhang,Ding Zhao,Zhang-Wei Hong,Chuang Gan*

Main category: cs.LG

TL;DR: 提出Tailor方法，通过自动发现和整理新颖的推理原语来增强语言模型的推理能力，为强化学习训练提供更高质量和多样化的初始化数据。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力方面面临采样效率低和模型初始化依赖性强的问题，需要高质量的推理原语来确保稳定高效的训练。

Method: 开发Tailor微调流程，自动发现和整理新颖的推理原语，扩展推理状态分布的覆盖范围，为强化学习训练提供更好的初始化。

Result: 在数学和逻辑推理基准测试中，Tailor生成了更高质量和多样化的预热数据，显著提升了后续强化学习的性能。

Conclusion: 通过自动发现和整理推理原语的方法，能够有效解决强化学习训练中的初始化依赖问题，实现更稳定和高效的推理能力提升。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.

</details>


### [429] [VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs](https://arxiv.org/abs/2511.12434)
*Rui Xue*

Main category: cs.LG

TL;DR: 提出VISAGNN方法，通过动态自适应地将陈旧性标准融入图神经网络训练过程，解决历史嵌入方法中的陈旧性问题，提高模型性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 深度图神经网络在大规模图训练中面临邻居爆炸问题，历史嵌入方法虽然能减少计算和内存成本，但嵌入的陈旧性会引入显著偏差，影响模型性能。

Method: 提出VISAGNN方法，将陈旧性嵌入到消息传递机制、损失函数和历史嵌入中，使模型能自适应地减轻陈旧嵌入的负面影响。

Result: 综合实验表明该方法能有效克服现有历史嵌入技术的陈旧性问题，在大规模基准测试中展现出优越性能和效率，收敛速度显著加快。

Conclusion: VISAGNN通过自适应处理嵌入陈旧性，成功解决了历史嵌入方法的性能瓶颈，为大规模图神经网络训练提供了有效解决方案。

Abstract: Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.

</details>


### [430] [Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction](https://arxiv.org/abs/2511.12442)
*Tao Zou,Chengfeng Wu,Tianxi Liao,Junchen Ye,Bowen Du*

Main category: cs.LG

TL;DR: GLFormer是一个用于动态图的注意力无关的Transformer风格框架，通过自适应token混合器和分层聚合模块实现高效的长时序依赖建模，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在动态图学习中依赖自注意力机制导致二次复杂度，限制了在高频或大规模图上的可扩展性。本文重新审视了自注意力在动态图建模中的必要性。

Method: 提出GLFormer框架：1）自适应token混合器，基于交互顺序和时间间隔进行上下文感知的局部聚合；2）分层聚合模块，通过堆叠局部token混合器来扩展时序感受野。

Result: 在六个广泛使用的动态图基准测试中，GLFormer实现了最先进的性能，表明注意力无关架构在动态图设置中可以匹配或超越Transformer基线，同时显著提高效率。

Conclusion: 研究表明注意力无关架构在动态图建模中具有竞争力，为高效动态图学习提供了新方向，挑战了自注意力机制的必要性。

Abstract: Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.

</details>


### [431] [Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection](https://arxiv.org/abs/2511.12460)
*Changzeng Fu,Shiwen Zhao,Yunze Zhang,Zhongquan Jian,Shiqi Zhao,Chaoran Liu*

Main category: cs.LG

TL;DR: 提出P³HF网络，通过个性引导表示学习、超图变换器架构和事件级领域解耦，在抑郁检测任务中比现有方法准确率和加权F1值提升约10%。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer或图神经网络的抑郁检测方法在建模个体差异和跨模态时间依赖性方面面临挑战，需要更有效的个性化检测方法。

Method: 使用LLM进行个性引导表示学习，将离散个体特征转换为上下文描述；构建超图变换器架构建模高阶跨模态时间关系；采用事件级领域解耦和对比学习提高跨行为场景的泛化能力。

Result: 在MPDD-Young数据集上，P³HF在二元和三元抑郁分类任务中比现有方法准确率和加权F1值提升约10%。消融研究验证了各组件的重要性。

Conclusion: 个性引导表示学习和高阶超图推理对于生成稳健的、个体感知的抑郁相关表示至关重要，该方法在抑郁检测方面表现出显著优势。

Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.

</details>


### [432] [Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection](https://arxiv.org/abs/2511.12462)
*Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.LG

TL;DR: 提出了一种基于冗余优化多头注意力网络的多视图多标签特征选择方法(RMAN-MMFS)，通过多头注意力机制建模视图内特征关系和视图间特征互补性，并设计了静态和动态特征冗余项来优化特征紧凑性。


<details>
  <summary>Details</summary>
Motivation: 多视图多标签数据为AI提供了更丰富的视角，但由于特征、视图和标签之间复杂的相互关系，给特征选择带来了重大挑战。现有基于注意力的方法主要关注视图内关系，忽略了视图间特征的互补性和关键的特征-标签相关性，且未能考虑特征冗余问题。

Method: 使用单个注意力头建模视图内特征关系，通过不同头之间的交叉注意力机制捕获视图间特征互补性。设计了静态和动态特征冗余项：静态项减少每个视图内的冗余，动态项在整个选择过程中显式建模未选特征与已选特征之间的冗余。

Result: 在六个真实世界数据集上进行的综合评估表明，与六种多视图多标签特征选择方法相比，所提方法表现出优越的性能。

Conclusion: RMAN-MMFS方法通过多头注意力机制和冗余优化策略，有效解决了多视图多标签特征选择中的关键挑战，实现了更好的特征选择性能。

Abstract: Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.

</details>


### [433] [Diffusion Model Based Signal Recovery Under 1-Bit Quantization](https://arxiv.org/abs/2511.12471)
*Youming Chen,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: Diff-OneBit：一种基于扩散模型的快速有效方法，用于1位量化下的信号恢复，通过可微分代理似然函数解决非可微链接函数问题，在重建质量和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已被证明是信号恢复的强大先验，但在1位量化任务（如1位压缩感知和逻辑回归）中的应用仍然具有挑战性，主要因为这些任务中的固有非线性链接函数要么不可微，要么缺乏显式表征。

Method: Diff-OneBit通过利用可微分代理似然函数来建模1位量化，从而支持基于梯度的迭代。该方法采用灵活的即插即用框架，将数据保真度项与扩散先验解耦，允许任何预训练的扩散模型在迭代重建过程中充当去噪器。

Result: 在FFHQ、CelebA和ImageNet数据集上的广泛实验表明，Diff-OneBit能够生成高保真度的重建图像，在1位压缩感知和逻辑回归任务中，在重建质量和计算效率方面均优于最先进的方法。

Conclusion: Diff-OneBit成功解决了1位量化任务中非可微链接函数的挑战，提供了一种高效且有效的信号恢复方法，展示了扩散模型在复杂量化场景中的强大潜力。

Abstract: Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.

</details>


### [434] [SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design](https://arxiv.org/abs/2511.12489)
*Qingsong Zhong,Haomin Yu,Yan Lin,Wangmeng Shen,Long Zeng,Jilin Hu*

Main category: cs.LG

TL;DR: SculptDrug是一个基于贝叶斯流网络的空间条件感知生成模型，用于解决结构药物设计中的边界约束、层次结构条件和空间建模保真度问题。


<details>
  <summary>Details</summary>
Motivation: 现有药物设计生成模型面临三个关键挑战：边界条件约束的整合、层次结构条件的集成以及空间建模保真度的保证。

Method: 采用贝叶斯流网络框架和渐进去噪策略确保空间建模保真度；引入边界感知模块整合蛋白质表面约束；设计层次编码器捕获全局结构上下文同时保留细粒度分子相互作用。

Result: 在CrossDocked数据集上的实验表明，SculptDrug优于现有最先进基线方法，验证了空间条件感知建模的有效性。

Conclusion: SculptDrug通过空间条件感知生成模型成功解决了结构药物设计中的关键挑战，为药物发现提供了有效工具。

Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.

</details>


### [435] [Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation](https://arxiv.org/abs/2511.12491)
*Ponhvoan Srey,Yaxin Shi,Hangwei Qian,Jing Li,Ivor W. Tsang*

Main category: cs.LG

TL;DR: 提出了AFTTA方法，通过预定义映射模拟源域和目标域之间的潜在偏移，并在测试时通过互信息准则正则化特征表示和标签预测，实现无需源数据和训练协议的完全测试时自适应。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要对齐源域和目标域特征分布，但在完全测试时自适应中无法获取源数据和训练协议，且目标域不可预测，因此需要新的解决方案。

Method: 采用uncover-and-unlearn方法：首先通过预定义映射模拟源域和目标域之间的潜在偏移作为干扰因素；然后在测试时通过互信息准则正则化潜在表示和标签预测中的偏移，实现干扰因素的遗忘。

Result: 在涉及损坏和风格偏移的各种任务上进行广泛实验，证明该方法在完全测试时自适应约束下能够持续优于现有方法。

Conclusion: AFTTA方法能够显式处理不可知的域偏移，在完全测试时自适应约束下实现优越的模型泛化能力。

Abstract: Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.

</details>


### [436] [Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance](https://arxiv.org/abs/2511.12494)
*Jiecheng Jiang,Jiawei Tang,Jiahao Jiang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 本文提出了一个名为HidLDL的新问题，旨在从真实世界的不完整标签分布中恢复完整标签分布，解决了现有IncomLDL方法中不现实的假设。


<details>
  <summary>Details</summary>
Motivation: 现有不完整标签分布学习方法将缺失标签的描述度设为0，但保持其他标签不变，这种设置不现实。当某些标签缺失时，剩余标签的度应该相应增加。

Method: 利用观察标签的比例信息，通过创新约束在优化过程中使用该信息。同时使用局部特征相似性和全局低秩结构来揭示隐藏标签。

Result: 在多个数据集上的广泛恢复和预测实验证明，该方法在从隐藏标签学习方面优于最先进的LDL和IncomLDL方法。

Conclusion: 本文提出的HidLDL方法能够有效从不完整标签分布中恢复完整标签分布，理论分析证明了该方法的可行性，实验验证了其优越性。

Abstract: Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of "missing" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.

</details>


### [437] [BSO: Binary Spiking Online Optimization Algorithm](https://arxiv.org/abs/2511.12502)
*Yu Liang,Yu Yang,Wenjie Wei,Ammar Belatreche,Shuai Wang,Malu Zhang,Yang Yang*

Main category: cs.LG

TL;DR: 提出了BSO和T-BSO两种二进制脉冲神经网络的在线训练算法，显著减少训练内存需求，通过翻转信号直接更新权重，无需存储潜在权重。


<details>
  <summary>Details</summary>
Motivation: 二进制脉冲神经网络在资源受限计算中具有效率优势，但现有训练算法需要大量内存来存储潜在权重和处理时序需求。

Method: BSO算法通过翻转信号直接更新权重，当梯度动量与权重的乘积超过阈值时触发翻转；T-BSO是时序感知变体，利用BSNN的时序动态特性，跨时间步捕获梯度信息进行自适应阈值调整。

Result: 理论分析证明了BSO和T-BSO的收敛保证，实验表明两种算法相比现有BSNN训练方法都实现了更优的优化性能。

Conclusion: BSO和T-BSO是高效的BSNN在线训练算法，显著降低内存需求，同时保持优异的优化性能。

Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.

</details>


### [438] [Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning](https://arxiv.org/abs/2511.12507)
*Jingtian Ma,Jingyuan Wang,Leong Hou U*

Main category: cs.LG

TL;DR: HiFiNet是一种新颖的分层频率分解图神经网络，通过构建虚拟节点层次结构统一空间和频谱建模，分别处理低频和高频信号以改善道路网络表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在道路网络建模中存在空间-频谱不对齐问题：基于空间的方法容易过平滑，基于频谱的方法忽略局部变化。这种局限性影响了同时捕捉全局趋势和局部波动的能力。

Method: 构建多级虚拟节点层次结构，采用分解-更新-重构框架，使用拓扑感知图变换器分别建模和融合低频与高频信号。

Result: 在多个真实世界数据集和四个下游任务上的实证验证表明，HiFiNet在捕捉有效道路网络表示方面表现出优越性能和泛化能力。

Conclusion: HiFiNet通过统一空间和频谱建模，有效解决了道路网络表示学习中的空间-频谱不对齐问题，为智能交通系统提供了更强大的基础设施。

Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.

</details>


### [439] [Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning](https://arxiv.org/abs/2511.12512)
*Ze Tao,Darui Zhao,Fujun Liu,Ke Xu,Xiangsheng Hu*

Main category: cs.LG

TL;DR: 提出了一种基于xLSTM的物理信息神经网络(xLSTM-PINN)，通过门控记忆多尺度特征提取和自适应残差数据加权来抑制谱偏差并增强外推能力。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息学习方法面临谱偏差、残差数据不平衡和弱外推等问题，需要改进现有方法以提升计算精度和稳定性。

Method: 结合门控跨尺度记忆、分阶段频率课程和自适应残差重新加权，在四个基准测试中进行验证。

Result: 显著降低了谱误差和RMSE，拓宽了稳定学习率窗口，提高了高频核权重和可解析带宽，缩短了高波数误差衰减时间。

Conclusion: 该方法在不改变自动微分或物理损失的情况下，有效抑制了谱偏差，提高了精度、可重复性和可迁移性。

Abstract: Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.

</details>


### [440] [Regret Guarantees for Linear Contextual Stochastic Shortest Path](https://arxiv.org/abs/2511.12534)
*Dor Polikar,Alon Cohen*

Main category: cs.LG

TL;DR: 提出了线性上下文随机最短路径问题(LR-CSSP)，设计了算法在未知线性映射下实现次线性遗憾界，解决了上下文MDP中知识不足导致episode延长甚至不终止的问题。


<details>
  <summary>Details</summary>
Motivation: 解决上下文随机最短路径问题，其中学习者观测对抗性选择的上下文，通过未知线性函数确定MDP，目标是到达目标状态且期望累积损失最小，同时处理知识不足导致的episode延长问题。

Method: 提出LR-CSSP算法，利用线性函数逼近处理上下文空间，通过探索-利用平衡学习未知的MDP动态和损失函数，确保所有episode在合理步数内终止。

Result: 获得遗憾界$\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$，在成本有下界时改进为$\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$。

Conclusion: LR-CSSP能有效处理连续上下文空间，确保episode合理终止，解决了上下文MDP中知识不足带来的特殊挑战。

Abstract: We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\star$ bounds the optimal cumulative loss and $T_\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\ell_{\min}$, LR-CSSP attains a regret of $\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.

</details>


### [441] [CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching](https://arxiv.org/abs/2511.12548)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出一种曲率自适应优化方法，通过周期性构建低秩Hessian子空间来预条件梯度，在尖锐、各向异性区域比一阶优化器更快收敛。


<details>
  <summary>Details</summary>
Motivation: 一阶优化器在尖锐、各向异性的区域虽然可靠但收敛缓慢，需要更高效的优化方法。

Method: 周期性通过Hessian-向量积构建低秩Hessian子空间，仅在该子空间内预条件梯度，正交补空间保持一阶优化。

Result: 在CIFAR-10/100和ResNet-18/34上，该方法比Adam早2.95倍达到预设训练损失阈值，同时保持最终测试精度。方法对草图秩k不敏感，k=0可作为无曲率消融实验。

Conclusion: 该方法提供了在尖锐、各向异性区域更快的收敛速度，同时保持最终性能，是一种简单有效的优化策略。

Abstract: First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.

</details>


### [442] [Training Instabilities Induce Flatness Bias in Gradient Descent](https://arxiv.org/abs/2511.12558)
*Lawrence Wang,Stephen J. Roberts*

Main category: cs.LG

TL;DR: 本文揭示了梯度下降训练不稳定性对泛化性能的积极作用，通过特征向量旋转极性机制驱动参数向损失函数更平坦区域移动，从而改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统分析认为学习率低于Hessian最大特征值（锐度）时训练稳定，但现代深度网络往往在超出该阈值时获得最佳性能，需要理解这种不稳定性的作用机制。

Method: 提出旋转极性特征向量(RPE)几何现象理论框架，分析训练不稳定时Hessian主导特征向量的旋转如何促进探索并导致更平坦的最小值，并将该框架扩展到随机梯度下降和Adam优化器。

Result: 训练不稳定性通过RPE机制诱导隐式偏差，驱动参数向损失函数更平坦区域移动，在SGD中该效应超过小批量噪声影响，在Adam中恢复不稳定性可进一步改善泛化。

Conclusion: 训练不稳定性在深度学习中具有建设性作用，通过几何机制促进参数向平坦区域收敛，从而提升模型泛化性能。

Abstract: Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.
  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.
  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.
  Together, these results establish and understand the constructive role of training instabilities in deep learning.

</details>


### [443] [Linear time small coresets for k-mean clustering of segments with applications](https://arxiv.org/abs/2511.12564)
*David Denisov,Shlomi Dolev,Dan Felmdan,Michael Segal*

Main category: cs.LG

TL;DR: 提出了首个能够处理任意输入线段的核心集构造方法，用于线段k-means聚类问题，在常数k和ε下生成大小为O(log²n)的核心集，计算时间为O(nd)。


<details>
  <summary>Details</summary>
Motivation: 研究线段集合的k-means聚类问题，旨在找到k个中心点最小化所有线段到最近中心点的总距离。需要开发高效的核心集方法以支持流式、分布式和并行计算。

Method: 设计了一种核心集构造算法，能够处理任意输入线段。对于常数k和ε，算法生成大小为O(log²n)的核心集，计算复杂度为O(nd)。

Result: 实验验证了方法的有效性，包括实时视频跟踪应用，显示在聚类精度损失最小的情况下获得显著加速。

Conclusion: 该方法在理论和实践上都表现出色，为线段k-means聚类提供了高效的核心集解决方案，支持大规模数据处理。

Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.

</details>


### [444] [Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data](https://arxiv.org/abs/2511.12568)
*Mitul Goswami,Romit Chatterjee*

Main category: cs.LG

TL;DR: 通过量化和位深度优化技术优化复杂学习模型，显著降低时间复杂度的同时保持模型效率，在医疗数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂模型执行时间过长的问题，通过优化技术在不显著影响准确率的前提下降低计算复杂度。

Method: 使用Logistic Regression模型，在医疗数据集上应用量化和位深度优化策略，将输入数据从float64降级到float32和int32。

Result: 时间复杂性显著降低，优化后模型准确率仅有轻微下降，展示了先进的优化方法。

Conclusion: 这些优化技术的影响取决于一组参数，需要根据具体情况进行调整。

Abstract: This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.

</details>


### [445] [LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction](https://arxiv.org/abs/2511.12581)
*Kai Ma,Zhen Wang,Hongquan He,Qi Xu,Tinghuan Chen,Hao Geng*

Main category: cs.LG

TL;DR: 提出一种新颖的多模态方法，通过大规模网表变换器（LNT）处理SPICE文件，将网表拓扑表示为3D点云，实现高效处理数百万节点的网表，用于静态电压降预测。


<details>
  <summary>Details</summary>
Motivation: 静态IR压降分析在芯片设计中至关重要但耗时，可能需要数小时，且解决IR压降违规需要迭代分析，计算负担重。快速准确的IR压降预测对减少芯片设计总时间至关重要。

Method: 采用多模态方法，通过大规模网表变换器处理SPICE文件，将网表拓扑表示为3D点云表示，能够高效处理数十万到数百万节点的网表。所有类型数据（网表文件和图像数据）都被编码为潜在空间特征并输入模型进行静态电压降预测。

Result: 实验结果表明，所提算法在ICCAD 2023竞赛获胜团队和最先进算法中实现了最佳F1分数和最低MAE。

Conclusion: 该方法能够集成多模态数据进行互补预测，在静态电压降预测方面表现出色。

Abstract: Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.

</details>


### [446] [Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization](https://arxiv.org/abs/2511.12601)
*Odysseas Boufalis,Jorge Carrasco-Pollo,Joshua Rosenthal,Eduardo Terres-Caballero,Alejandro García-Castellanos*

Main category: cs.LG

TL;DR: ScaleGMNs通过构建对排列和参数缩放变换等变的架构，利用神经网络参数化的内在对称性，将相似网络映射到同一损失盆地，实现模型合并和平滑线性插值。


<details>
  <summary>Details</summary>
Motivation: 神经网络参数化存在内在对称性，产生多个等效最小值。先前工作仅处理排列对称性，本文扩展该方法，同时纳入缩放对称性。

Method: 使用ScaleGMNs作为不变编码器的自编码器框架，无需显式解决分配问题即可对齐INRs和CNNs在排列和缩放对称性下。

Result: 实验结果表明，该方法能将相似网络自然收敛到同一盆地，实现模型合并和平滑线性插值，避免高损失区域。

Conclusion: ScaleGMNs通过利用排列和缩放对称性，有效促进模型合并，代码已在GitHub公开。

Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.

</details>


### [447] [FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions](https://arxiv.org/abs/2511.12628)
*Ke Hu,Liyao Xiang,Peng Tang,Weidong Qiu*

Main category: cs.LG

TL;DR: FedTopo是一个联邦学习框架，通过拓扑引导的块筛选和拓扑嵌入来解决非IID数据下的表示漂移问题，提高模型收敛速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习模型在异构（非IID）客户端数据下性能下降，因为特征表示发散，像素或块级目标无法捕捉对高维视觉任务至关重要的全局拓扑结构。

Method: 提出FedTopo框架，包含拓扑引导块筛选（TGBS）自动选择最具拓扑信息的块，拓扑嵌入（TE）量化每个客户端的拓扑信息，以及拓扑对齐损失（TAL）在优化过程中保持客户端与全局模型的拓扑一致性。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上的四种非IID分区实验表明，FedTopo相比强基线方法加速了收敛并提高了准确率。

Conclusion: FedTopo通过利用拓扑信息有效解决了联邦学习中非IID数据导致的表示漂移问题，显著提升了模型性能。

Abstract: Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.

</details>


### [448] [NFQ2.0: The CartPole Benchmark Revisited](https://arxiv.org/abs/2511.12644)
*Sascha Lange,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文重新审视了20年前的神经拟合Q迭代(NFQ)算法，提出现代化变体NFQ2.0，通过消融研究识别关键设计决策和超参数，提升在工业控制任务中的可重复性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: NFQ作为深度强化学习的先驱方法，虽然在真实世界控制问题上取得初步成功，但需要大量调参且难以复现。研究旨在改进学习过程的可重复性和鲁棒性。

Method: 提出NFQ2.0现代化变体，应用于CartPole任务，使用标准工业组件构建真实系统，通过消融研究分析关键设计决策和超参数。

Result: NFQ2.0在性能和稳定性上优于原始版本，研究识别出增强性能的关键因素。

Conclusion: 研究结果有助于从业者更好地复现和改进结果，在工业环境中更有效地应用深度强化学习。

Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.

</details>


### [449] [FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning](https://arxiv.org/abs/2511.12663)
*Chen Gu,Yingying Sun,Yifan She,Donghui Hu*

Main category: cs.LG

TL;DR: FLClear是一个联邦学习水印框架，通过转置模型和对比学习实现无碰撞水印聚合、增强水印安全性以及可视化所有权验证。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，中央服务器可能恶意操纵全局模型以抹除客户端贡献或虚假声称所有权，侵犯客户端的知识产权。现有水印方法存在水印碰撞、安全性不足和验证机制不直观等问题。

Method: 引入转置模型与对比学习联合优化，集成水印和主任务目标。验证时从转置模型重构水印，通过视觉检查和结构相似性指标进行评估。

Result: 在多个数据集、聚合方案和攻击场景下的综合实验表明，FLClear始终优于最先进的联邦学习水印方法。

Conclusion: FLClear有效解决了联邦学习中的知识产权保护问题，实现了无碰撞水印聚合、增强安全性和直观验证。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.

</details>


### [450] [Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction](https://arxiv.org/abs/2511.12682)
*Amirpasha Hedayat,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 提出了一种用于短期天气预报的高效降阶建模框架，使用基于ResNet的卷积自编码器和块注意力模块来降维，在延迟嵌入的潜在空间中学习线性算子来捕获动态。


<details>
  <summary>Details</summary>
Motivation: 天气预报是一个复杂的高维非线性混沌系统预测问题，现有AI模型需要大量计算资源，本工作优先考虑效率同时保持合理精度。

Method: 开发了基于ResNet的卷积自编码器，增强块注意力模块来降维天气数据，在延迟嵌入的潜在空间中学习线性算子来高效捕获动态。

Result: 使用ERA5再分析数据集证明该框架在训练数据期间内能有效预测天气模式，但在泛化到未来状态时存在局限性，特别是超出训练窗口后预测精度下降。

Conclusion: 天气系统在适当构建的嵌入空间中可通过线性操作有效捕获强时间相关性，投影误差而非推理误差是主要瓶颈，为混沌系统降阶建模提供重要见解，建议将高效降阶模型与更复杂AI架构结合的混合方法。

Abstract: Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.

</details>


### [451] [Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs](https://arxiv.org/abs/2511.12706)
*Daniel Furelos-Blanco,Charles Pert,Frederik Kelbel,Alex F. Spies,Alessandra Russo,Michael Dennis*

Main category: cs.LG

TL;DR: ATLAS是一种新颖的方法，能够自动生成任务和关卡联合课程，通过协同设计任务和关卡来解决强化学习中复杂指令跟随的挑战。


<details>
  <summary>Details</summary>
Motivation: 训练通用智能体在复杂环境中遵循复杂指令是强化学习的核心挑战。随机采样任务-关卡对通常会产生不可解的组合，因此需要协同设计任务和关卡。

Method: 基于无监督环境设计(UED)，ATLAS自动生成可解但具有挑战性的任务-关卡对用于策略训练，利用任务和关卡结构的突变来加速收敛。

Result: 实验表明ATLAS显著优于随机采样方法，特别是在可解对采样概率较低的情况下。利用任务和关卡结构的突变能加速收敛到高性能策略。

Conclusion: ATLAS通过自动生成任务和关卡的联合课程，有效解决了复杂环境中指令跟随的挑战，为强化学习领域提供了新的进展。

Abstract: Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.

</details>


### [452] [Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations](https://arxiv.org/abs/2511.12709)
*Sangwoo Seo,Hyunsung Kim,Jiwan Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出AdaMeshNet框架，在基于网格的图神经网络中引入自适应重连过程，通过计算重连延迟分数来动态选择消息传递层进行重连，解决传统方法中瞬时交互和忽略距离信息的问题。


<details>
  <summary>Details</summary>
Motivation: 传统网格细化技术会导致图神经网络中的过度挤压问题，阻碍长程物理交互的捕捉。现有图重连方法在应用前完成所有重连操作，假设节点间瞬时交互且忽略粒子间距离信息，这在物理上是不现实的。

Method: 提出AdaMeshNet框架，在消息传递过程中引入自适应重连过程。计算基于最短路径距离和速度差的重连延迟分数，动态选择消息传递层进行重连，实现网格图的自适应重连。

Result: 在基于网格的流体模拟实验中，AdaMeshNet优于传统重连方法，能有效建模物理交互的序列性质，实现更准确的预测。

Conclusion: AdaMeshNet通过自适应重连过程成功解决了传统方法中的物理不现实性问题，为基于网格的图神经网络流体模拟提供了更准确的建模方法。

Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.

</details>


### [453] [Oxytrees: Model Trees for Bipartite Learning](https://arxiv.org/abs/2511.12713)
*Pedro Ilídio,Felipe Kenji Nakano,Alireza Gharahighehi,Robbe D'hondt,Ricardo Cerri,Celine Vens*

Main category: cs.LG

TL;DR: 提出Oxytrees：基于代理的双聚类模型树，用于二分学习任务，显著提高训练速度而不损失预测性能


<details>
  <summary>Details</summary>
Motivation: 当前二分学习方法存在局限性，往往针对特定应用设计，无法泛化到其他问题或存在可扩展性问题

Method: 通过将交互矩阵压缩为行和列代理矩阵来减少训练时间；提出新的叶子分配算法加速预测；在叶子中使用基于Kronecker积核的线性模型，生成更浅的树

Result: 在15个数据集上测试，相比现有最先进的双聚类森林，训练时间提升高达30倍，在大多数评估设置中表现竞争性或更优，特别是在归纳设置中

Conclusion: Oxytrees在保持竞争力的同时显著提高了训练效率，并提供了Python API以支持该领域的可重复研究

Abstract: Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.

</details>


### [454] [On Robustness of Linear Classifiers to Targeted Data Poisoning](https://arxiv.org/abs/2511.12722)
*Nakshatra Gupta,Sumanth Prabhu,Supratik Chakraborty,R Venkatesh*

Main category: cs.LG

TL;DR: 本文提出了一种自动测量数据集对标签扰动中毒攻击鲁棒性的方法，通过计算鲁棒性的上下界来评估模型安全性。


<details>
  <summary>Details</summary>
Motivation: 数据中毒攻击会破坏学习模型的可信度，手动检测中毒样本在大型训练集中很困难，因此需要自动评估数据集对中毒攻击的鲁棒性。

Method: 在只能扰动训练数据标签的威胁模型下，提出计算鲁棒性上下界的技术，即使寻找精确鲁棒性是NP完全问题。

Result: 该方法能高效计算多个公开数据集的鲁棒性边界，超出边界的投毒会显著影响测试点分类，且在现有技术失败的许多情况下仍能计算边界。

Conclusion: 所提出的技术能有效评估数据集对标签扰动中毒攻击的鲁棒性，为模型安全性提供重要保障。

Abstract: Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.

</details>


### [455] [LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks](https://arxiv.org/abs/2511.12723)
*Gennaro Vessio*

Main category: cs.LG

TL;DR: LAYA是一种新颖的输出层，通过注意力机制动态聚合深度神经网络中间层的表示，而不是仅使用最后一层表示，从而提升性能并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络仅依赖最后一层隐藏表示进行预测，但中间层包含从低层模式到高层抽象的丰富互补信息，这些信息往往被丢弃。

Method: 提出LAYA（Layer-wise Attention Aggregator），通过学习输入条件化的注意力权重来动态聚合各层特征表示，形成架构无关的预测机制。

Result: 在视觉和语言基准测试中，LAYA始终匹配或优于标准输出层，准确率相对提升约1个百分点，同时提供可解释的层归因分数。

Conclusion: LAYA不仅提升了模型性能，还提供了直接从模型计算中产生的可解释性信号，无需外部事后解释方法。

Abstract: Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.

</details>


### [456] [Convolutional Model Trees](https://arxiv.org/abs/2511.12725)
*William Ward Armstrong*

Main category: cs.LG

TL;DR: 提出了一种创建模型树森林的方法，通过降采样图像、确定树的超平面、应用卷积处理训练图像的小变形，以及创建模型树森林来提高精度和平滑拟合。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够处理图像函数样本的方法，特别是要处理图像的小变形（如旋转和视角变化），并实现高精度和平滑的拟合。

Method: 通过降采样图像、确定超平面、应用卷积处理小变形、创建模型树森林，并利用像素、超平面系数和叶函数系数之间的1对1对应关系来处理更大的变形。

Result: 该方法能够处理图像的小变形和更大的变形（如任意旋转和视角变化），并通过理论方法平滑森林输出以产生连续可微的近似。

Conclusion: 提出的训练过程被证明是收敛的，该方法能够有效处理图像变形并实现平滑、高精度的函数拟合。

Abstract: A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.

</details>


### [457] [Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering](https://arxiv.org/abs/2511.12742)
*Zhongteng Cai,Yaxuan Wang,Yang Liu,Xueru Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为潜在空间过滤（LSF）的新方法，通过过滤混合数据集中不太真实的合成数据来缓解模型崩溃问题，无需增加训练成本或依赖人工标注。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在互联网上的扩散，它们经常被重复用于训练新一代生成模型，形成"自我消耗循环"，导致训练不稳定或模型崩溃。现有解决方案要么增加计算成本，要么需要昂贵的人工标注。

Method: 基于对自消耗扩散模型潜在空间动态的实证分析，提出潜在空间过滤（LSF）方法，通过过滤掉不太真实的合成数据来缓解模型崩溃。

Result: 实验表明，LSF在多个真实世界数据集上始终优于现有基线方法，有效缓解模型崩溃。

Conclusion: LSF提供了一种无需增加训练成本或依赖人工标注的有效方法来缓解模型崩溃问题。

Abstract: As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.

</details>


### [458] [DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes](https://arxiv.org/abs/2511.12745)
*Vivek Chawla,Boris Slautin,Utkarsh Pratiush,Dayakar Penumadu,Sergei Kalinin*

Main category: cs.LG

TL;DR: DIVIDE是一个框架，通过整合机制特定的深度编码器和结构化高斯过程来解耦科学数据集中多个独立机制的影响，实现可解释的机制感知预测。


<details>
  <summary>Details</summary>
Motivation: 科学数据集通常来自多个独立机制（如空间、分类或结构效应），这些机制的共同影响掩盖了各自的贡献，需要解耦这些影响以获得更清晰的个体贡献理解。

Method: 整合机制特定的深度编码器与结构化高斯过程在联合潜在空间中，编码器分离不同机制，高斯过程捕捉其组合效应并校准不确定性，支持结构化先验。

Result: 在合成数据集、FerroSIM铁电模式模拟和实验PFM磁滞回线上，DIVIDE成功分离机制，重现加性和缩放相互作用，并在噪声下保持稳健。

Conclusion: DIVIDE能够有效解耦科学数据集中的独立机制，支持可解释预测和高效主动学习，可自然扩展到多功��数据集。

Abstract: Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.

</details>


### [459] [Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving](https://arxiv.org/abs/2511.12751)
*Timur Anvar,Jeffrey Chen,Yuyan Wang,Rohan Chandra*

Main category: cs.LG

TL;DR: 本研究探讨小型本地部署LLM（<14B参数）通过奖励塑形而非直接控制来支持高速公路自动驾驶。比较了纯RL、纯LLM和混合方法，发现混合方法在成功率和效率之间取得平衡，但LLM影响的方法存在系统性保守偏差。


<details>
  <summary>Details</summary>
Motivation: RL依赖精心设计的奖励函数，难以捕捉复杂语义和社会情境；纯LLM方法在安全关键场景中不稳定且依赖昂贵API。研究旨在探索小型本地LLM通过奖励塑形支持自动驾驶的可行性。

Method: 采用案例研究比较三种方法：纯RL、纯LLM、混合方法（LLM在训练期间通过评分状态-动作转换来增强RL奖励，测试时使用标准RL策略执行）。

Result: 纯RL成功率73-89%，效率合理；纯LLM成功率可达94%但速度性能严重下降；混合方法介于两者之间。LLM影响的方法表现出系统性保守偏差和显著的模型依赖性变异。

Conclusion: 当前小型LLM在安全关键控制任务中存在重要局限性，尽管能提高成功率，但会导致效率下降和保守行为，凸显了在自动驾驶中谨慎使用LLM的必要性。

Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

</details>


### [460] [INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers](https://arxiv.org/abs/2511.12764)
*Hao Wei,Aleksandra Franz,Bjoern List,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出间接神经校正器(INC)，通过将学习校正集成到控制方程而非直接状态更新，减少自回归误差，在混沌系统中实现稳定高效的PDE仿真。


<details>
  <summary>Details</summary>
Motivation: 传统混合求解器将学习校正直接应用于求解器输出会导致显著的自回归误差，特别是在混沌状态下，扰动会累积放大。

Method: INC方法将学习校正集成到控制方程中，而不是直接更新状态，从而将误差放大降低到Δt⁻¹ + L的量级。

Result: 在1D混沌系统到3D湍流等多种测试中，INC将长期轨迹性能(R²)提升高达158.7%，稳定了激进粗化下的爆炸问题，在复杂3D湍流案例中实现了几个数量级的加速。

Conclusion: INC实现了稳定高效的PDE仿真，具有形式化误差减少，为具有可靠物理保证的更快科学和工程仿真铺平了道路。

Abstract: When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\(\mathrm{INC}\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \(\mathrm{INC}\) reduces the error amplification on the order of \(Δt^{-1} + L\), where \(Δt\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \(\mathrm{INC}\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\(R^2\)) by up to 158.7\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC

</details>


### [461] [MolEdit: Knowledge Editing for Multimodal Molecule Language Models](https://arxiv.org/abs/2511.12770)
*Zhenyu Lei,Patrick Soga,Yaochen Zhu,Yinhan He,Yushun Dong,Jundong Li*

Main category: cs.LG

TL;DR: MolEdit是一个用于分子语言模型知识编辑的框架，通过多专家知识适配器和专业知识感知编辑切换器，实现精准的分子知识修改同时保护无关知识。


<details>
  <summary>Details</summary>
Motivation: 分子语言模型可能因训练数据过时或恶意篡改而编码和传播错误知识，影响下游发现流程。目前知识编辑在通用AI领域已有探索，但在分子语言模型中的应用仍属空白，面临分子知识多面性和相互依赖性的独特挑战。

Method: 提出MolEdit框架，包含多专家知识适配器（将编辑路由到不同分子方面的专业专家）和专业知识感知编辑切换器（仅在输入与存储编辑高度匹配时激活适配器）。

Result: 在两个流行的分子语言模型骨干上，MolEdit相比基线方法在可靠性上提升18.8%，在局部性上提升12.0%，同时保持效率。

Conclusion: MolEdit是首个针对分子语言模型知识编辑的框架，能够有效修改目标知识同时保护无关分子知识，为分子知识管理提供了重要工具。

Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.

</details>


### [462] [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)
*Zhenshuo Zhang,Minxuan Duan,Youran Ye,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出PolicyGradEx算法，通过元训练和微调两阶段方法，将多目标强化学习任务高效聚类成少量相关组，实现16%性能提升和26倍加速。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习中，当目标数量n增长时，为所有目标学习单一策略是次优的。需要将相关目标分组训练以提高效率。

Method: 两阶段方法：1）元训练学习所有目标的元策略；2）微调适应随机采样子集，利用策略网络的一阶近似特性估计任务亲和度矩阵，然后进行聚类。

Result: 在机器人控制和Meta-World基准测试中，平均性能提升16%，速度提升达26倍。基于损失的聚类比随机分组和梯度相似性分组提升19%。

Conclusion: PolicyGradEx能有效估计任务亲和度并聚类，显著提升多目标强化学习效率。通过Hessian迹分析验证了泛化误差的非平凡度量。

Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.

</details>


### [463] [Optimal Look-back Horizon for Time Series Forecasting in Federated Learning](https://arxiv.org/abs/2511.12791)
*Dahao Tang,Nan Yang,Yanli Li,Zhiyu Zhu,Zhibo Jin,Dong Yuan*

Main category: cs.LG

TL;DR: 提出了一个联邦时间序列预测中自适应回溯窗口选择的框架，通过内在空间公式化解决数据分散、异构和非独立分布的问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景中，时间序列数据分散、异构且非独立分布，选择合适的回溯窗口是一个基本挑战。现有方法主要局限于集中式和独立分布设置。

Method: 引入合成数据生成器捕捉客户端数据的时间结构，定义将时间序列窗口映射到内在表示空间的变换，推导预测损失的分解为贝叶斯项和近似项。

Result: 分析表明增加回溯窗口可改善确定性模式的可识别性，但也会因模型复杂度增加和样本效率降低而增加近似误差。

Conclusion: 当不可约损失开始饱和而近似损失持续上升时，总预测损失在最小回溯窗口处达到最小，为联邦学习中的自适应窗口选择提供了严格理论基础。

Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.

</details>


### [464] [Genomic Next-Token Predictors are In-Context Learners](https://arxiv.org/abs/2511.12797)
*Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi*

Main category: cs.LG

TL;DR: 本研究首次证明基因组模型通过大规模预测训练能够自然涌现上下文学习能力，表明ICL是跨模态的通用能力，而不仅限于语言领域。


<details>
  <summary>Details</summary>
Motivation: 探索上下文学习是否能在非语言序列领域（如基因组序列）中通过大规模预测训练自然涌现，挑战ICL仅源于人类语言统计特性的观点。

Method: 开发受控实验框架，在语言和基因组形式中实例化符号推理任务，直接比较基因组模型和语言模型的ICL表现。使用Evo2基因组模型进行大规模核苷酸预测训练。

Result: 基因组模型与语言模型类似，随着上下文演示数量的增加，表现出对数线性的模式归纳增益。这是基因组序列中首次观察到的自然涌现ICL证据。

Conclusion: ICL是大规模预测建模在丰富数据上的结果，具有跨模态的普适性，为理解元学习提供了统一的、模态无关的视角。

Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.

</details>


### [465] [The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation](https://arxiv.org/abs/2511.12804)
*Ali Falahati,Mohammad Mohammadi Amiri,Kate Larson,Lukasz Golab*

Main category: cs.LG

TL;DR: 该论文首次为自消费生成模型的递归再训练提供了形式化分析框架，揭示了基于Bradley-Terry模型的偏好对齐机制在长期演化中的三种收敛模式，并证明了在多样性、对称影响和初始化独立性之间的基本不可能性定理。


<details>
  <summary>Details</summary>
Motivation: 随着自消费生成模型在自身输出上训练，用户偏好对齐变成了递归过程而非一次性任务。需要建立形式化框架来分析这种递归再训练对长期对齐的影响。

Method: 采用基于Bradley-Terry模型的两阶段筛选机制，将对齐建模为模型所有者（筛选学习内容）和公共用户（决定共享内容）两个阵营的互动过程。

Result: 分析揭示了三种结构收敛机制：共识崩溃、共享最优妥协和不对称精炼。证明了递归BT筛选机制无法同时保持多样性、确保对称影响和消除初始化依赖的基本不可能性定理。

Conclusion: 对齐不是静态目标而是演化均衡，既受权力不对称性影响，也受路径依赖影响。该研究为理解生成模型递归训练的社会动态提供了理论基础。

Abstract: In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.

</details>


### [466] [Expressive Temporal Specifications for Reward Monitoring](https://arxiv.org/abs/2511.12808)
*Omar Adalat,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 使用定量线性时序逻辑(LTL_f[F])合成奖励监控器，为可观测状态轨迹生成密集奖励流，解决长时决策中稀疏奖励问题


<details>
  <summary>Details</summary>
Motivation: 指定信息丰富且密集的奖励函数是强化学习中的关键挑战，直接影响代理训练效率。当前文献中占主导地位的布尔语义在长时决策中会产生稀疏奖励问题

Method: 利用定量线性时序逻辑在有限轨迹上的表达能力，合成奖励监控器，基于运行时可观测状态轨迹生成密集奖励流。该框架与算法无关，仅依赖状态标记函数，自然支持非马尔可夫属性的指定

Result: 实验结果表明，定量监控器在最大化任务完成度的定量测量和减少收敛时间方面，始终优于布尔监控器，具体表现取决于环境

Conclusion: 定量线性时序逻辑为强化学习提供了一种有效的密集奖励生成方法，能够更好地引导代理行为，解决长时决策中的稀疏奖励问题

Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.

</details>


### [467] [Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs](https://arxiv.org/abs/2511.12817)
*Shasha Zhou,Mingyu Huang,Jack Cole,Charles Britton,Ming Yin,Jan Wolber,Ke Li*

Main category: cs.LG

TL;DR: FAITH框架利用医学知识图谱自动评估LLM生成响应的真实性，无需参考答案，通过分解声明、链接知识图谱和基于证据路径评分，在医疗任务中与临床医生判断高度相关。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域部署LLM需要严格的验证，了解潜在危害，研究使用医学知识图谱自动评估LLM响应真实性的可靠性和可行性。

Method: 提出FAITH框架：分解响应为原子声明，链接到医学知识图谱，基于证据路径进行评分，无需参考答案。

Result: 实验表明基于知识图谱的评估与临床医生判断相关性显著更高，能有效区分不同能力的LLM，对文本变化具有鲁棒性，评分可解释性有助于理解LLM局限性。

Conclusion: 尽管存在限制，利用知识图谱是医疗领域自动真实性评估的重要方向。

Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

</details>


### [468] [Catastrophic Forgetting in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.12828)
*Mohammad Marufur Rahman,Guanchu Wang,Kaixiong Zhou,Minghan Chen,Fan Yang*

Main category: cs.LG

TL;DR: 本文系统研究了KANs在持续学习中的灾难性遗忘问题，提出了理论框架分析遗忘与激活支持重叠和内在数据维度的关系，并开发了KAN-LoRA适配器用于语言模型的参数高效持续微调。


<details>
  <summary>Details</summary>
Motivation: KANs被认为通过局部样条激活函数具有内在的抗遗忘能力，但其在持续学习中的实际行为和局限性尚不清楚，需要系统研究。

Method: 建立了理论框架分析遗忘与激活支持重叠和内在数据维度的关系，在合成和视觉任务上进行了系统实验，并提出了KAN-LoRA适配器用于语言模型的参数高效持续微调。

Result: KANs在低维算法设置中表现出良好的知识保留能力，但在图像分类和语言建模等高维领域仍然容易发生遗忘。KAN-LoRA在知识编辑任务中表现出有效性。

Conclusion: KANs在持续学习中具有特定优势，但在高维领域仍面临遗忘挑战，研究结果为持续学习系统设计提供了实用见解。

Abstract: Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.

</details>


### [469] [An Evaluation of Representation Learning Methods in Particle Physics Foundation Models](https://arxiv.org/abs/2511.12829)
*Michael Chen,Raghav Kansal,Abhijith Gandrakota,Zichun Hao,Jennifer Ngadiuba,Maria Spiropulu*

Main category: cs.LG

TL;DR: 本文系统评估了粒子物理学中的表示学习目标，在统一框架下比较了对比学习、掩码粒子建模和生成重建等不同方法，并提出了达到最先进性能的监督架构改进。


<details>
  <summary>Details</summary>
Motivation: 为粒子物理学中的基础模型开发提供可复现的基准和透明比较，隔离学习目标的贡献，突出各自的优势和局限性。

Method: 使用共享的基于transformer的粒子云编码器，采用标准化预处理、匹配采样和一致评估协议，在喷注分类数据集上比较对比学习（监督和自监督）、掩码粒子建模和生成重建目标。

Result: 提出了针对性的监督架构修改，在基准评估中达到了最先进的性能。

Conclusion: 这项工作为粒子物理学基础模型的未来发展提供了参考点，使整个社区能够实现更透明和稳健的进展。

Abstract: We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.

</details>


### [470] [Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency](https://arxiv.org/abs/2511.12838)
*Rongqin Chen,Fan Mo,Pak Lon Ip,Shenghui Zhang,Dan Wu,Ye Li,Leong Hou U*

Main category: cs.LG

TL;DR: Co-Sparsify是一个连接感知的稀疏化框架，通过将2节点消息传递限制在连通组件中，3节点交互限制在双连通组件中，消除可证明冗余的计算，同时保持完整的2-FWL表达能力。


<details>
  <summary>Details</summary>
Motivation: 基于2-FWL测试的高阶图神经网络(HOGNNs)具有优越的表达能力，但计算成本高达O(n³)。现有的效率方法通常以降低表达能力为代价来缓解计算负担。

Method: Co-Sparsify框架的关键洞察是：3节点交互仅在双连通组件（每个节点对都在环上的最大子图）中具有表达必要性。在这些组件之外，结构关系可以通过2节点消息传递或全局读取完全捕获。

Result: 在PPGN上，Co-Sparsify在合成子结构计数任务中达到或超过准确率，在真实世界基准测试（ZINC、QM9）中实现最先进的性能。

Conclusion: 高表达能力和可扩展性并不相互排斥：基于原理的、拓扑引导的稀疏化能够实现具有理论保证的强大且高效的GNNs。

Abstract: Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.

</details>


### [471] [RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees](https://arxiv.org/abs/2511.12846)
*Zelin Zhu,Yancheng Huang,Kai Yang*

Main category: cs.LG

TL;DR: RoS-Guard是一种针对具有不确定性的线性系统的鲁棒最优在线变化检测算法，通过神经展开实现GPU加速，提供理论性能保证并在大规模系统中显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有在线变化检测方法通常假设精确的系统知识，这在现实中不切实际，且在大规模系统中效率低下。

Method: 通过紧密松弛和重构OCD优化问题，采用神经展开技术实现GPU加速的并行计算。

Result: 大量实验验证了RoS-Guard的有效性，并在大规模系统场景中展示了显著的计算加速。

Conclusion: RoS-Guard为具有不确定性的线性系统提供了一种鲁棒且最优的在线变化检测解决方案，具有理论性能保证和高效计算能力。

Abstract: Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.

</details>


### [472] [An approach of deep reinforcement learning for maximizing the net present value of stochastic projects](https://arxiv.org/abs/2511.12865)
*Wei Xu,Fan Yang,Qinyuan Cui,Zhi Chen*

Main category: cs.LG

TL;DR: 本文研究了具有随机活动持续时间和现金流量的项目优化问题，采用双深度Q网络（DDQN）方法最大化期望净现值，在复杂环境下优于传统策略。


<details>
  <summary>Details</summary>
Motivation: 在具有随机活动持续时间和离散情景下现金流量的项目中，需要满足优先约束条件，目标是通过加速现金流入和推迟现金流出来最大化期望净现值。

Method: 将问题建模为离散时间马尔可夫决策过程（MDP），并提出双深度Q网络（DDQN）方法，采用双网络架构和目标网络来缓解动作值高估问题并提高训练稳定性。

Result: 比较实验显示DDQN在大型或高不确定性环境中优于传统刚性策略和动态策略，具有更好的计算能力、策略可靠性和适应性。消融研究证实双网络架构有效缓解动作值高估，目标网络显著提升训练收敛性和鲁棒性。

Conclusion: DDQN不仅在复杂项目优化中实现更高的期望净现值，而且为稳定有效的策略实施提供了可靠框架。

Abstract: This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.

</details>


### [473] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，从计算理论、信息论和统计学习角度形式化分析了LLM扩展的五个基本限制：幻觉、上下文压缩、推理退化、检索脆弱性和多模态不对齐，并指出了这些限制的理论上限。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM扩展限制的描述多停留在经验层面，缺乏将这些现象与计算、信息和学习的基本限制相连接的理论综合。本文旨在填补这一空白，为LLM扩展提供严格的理论基础。

Method: 采用证明驱动的统一框架，结合计算理论（不可计算性、对角线论证）、信息论（有限描述长度、样本复杂度）和几何计算效应分析，系统形式化LLM扩展的理论上限。

Result: 证明了对于任何可计算枚举模型族，对角线论证保证存在某些输入必然失败；不可判定查询导致无限失败集；信息约束限制了可达到的准确度；上下文压缩存在几何和计算效应；基于似然的训练偏向模式完成而非推理。

Conclusion: LLM扩展在某些领域有效，但在某些领域会饱和甚至无法进展。提出了有界预言检索、位置课程学习和稀疏/分层注意力等实际缓解路径，为理解和改进LLM扩展提供了理论基础和实践指导。

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [474] [Method of Manufactured Learning for Solver-free Training of Neural Operators](https://arxiv.org/abs/2511.12890)
*Arth Sojitra,Omer San*

Main category: cs.LG

TL;DR: 提出了制造学习方法(MML)，一种不依赖数值求解器的框架，通过解析构造物理一致的数据集来训练神经算子，避免了传统方法对昂贵数值模拟数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子训练需要大量由数值求解器生成的数据，这限制了可扩展性并约束了对物理系统的探索。

Method: 基于制造解方法，MML通过从受控解析空间采样平滑候选解，并通过直接应用控制微分算子推导相应的强制场来生成数据集。在推理时将强制项设为零以恢复原始控制方程。

Result: 在热传导、平流、Burgers和扩散-反应方程等基准测试中，MML实现了高光谱精度、低残差误差和对未见条件的强泛化能力。

Conclusion: MML通过将数据生成重新定义为解析合成过程，提供了一个可扩展、求解器无关的途径来构建物理基础的神经算子，无需依赖昂贵的数值模拟或实验数据进行训练。

Abstract: Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.

</details>


### [475] [Functional Mean Flow in Hilbert Space](https://arxiv.org/abs/2511.12898)
*Zhiqi Li,Yuchen Sun,Greg Turk,Bo Zhu*

Main category: cs.LG

TL;DR: Functional Mean Flow (FMF) 是一种在无限维希尔伯特空间中定义的一步生成模型，将 Mean Flow 框架扩展到函数域，提供了函数流匹配的理论公式和高效训练采样的实际实现。


<details>
  <summary>Details</summary>
Motivation: 将一步 Mean Flow 框架扩展到函数域，为函数数据生成任务提供实用的解决方案。

Method: 提出了函数流匹配的理论公式，引入了 $x_1$-预测变体以提高稳定性，提供了高效训练和采样的实际实现。

Result: 开发了一个实用的一步流匹配方法，适用于时间序列、图像、偏微分方程和3D几何等多种函数数据生成任务。

Conclusion: FMF 是一个在函数域中有效的生成模型框架，具有理论严谨性和实际可行性。

Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.

</details>


### [476] [Contrastive Entropy Bounds for Density and Conditional Density Decomposition](https://arxiv.org/abs/2511.12903)
*Bo Hu,Jose C. Principe*

Main category: cs.LG

TL;DR: 该论文从贝叶斯高斯视角研究神经网络特征的可解释性，通过优化成本达到概率边界，学习近似使边界紧致且成本最优的密度模型（通常是高斯混合密度）。提出了基于Hilbert空间和分解的方法来处理多输出网络，并展示了自动编码器目标等价于最大化高斯算子的迹，而核范数可用于训练MDNs。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络特征的可解释性，从贝叶斯高斯视角理解优化过程如何达到概率边界，并探索在不需要一对一对应关系时如何最大化整体秩而非迹。

Method: 使用Hilbert空间和分解方法处理多输出网络产生多个中心定义高斯混合的情况。提出了编码器-混合-解码器架构，其中解码器是多输出的，每个样本产生多个中心。利用高斯算子的迹训练自动编码器，核范数作为散度训练MDNs。

Result: 发现自动编码器的目标等价于最大化高斯算子的迹（特征值之和），而核范数（奇异值之和）可用于最大化整体秩。提出的方法能够增加样本多样性，防止网络产生相同常数的平凡解。

Conclusion: 从贝叶斯高斯视角为神经网络特征提供了可解释性框架，高斯算子的迹和核范数分别适用于自动编码器和MDNs的训练，提出的架构能够紧致概率边界并定量分析。

Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.

</details>


### [477] [LinkedIn Profile Characteristics and Professional Success Indicators](https://arxiv.org/abs/2511.12905)
*Tania-Amanda Fredrick Eneye,Ashlesha Malla,Pawan Paudel*

Main category: cs.LG

TL;DR: 本研究通过分析6.2万个LinkedIn匿名档案，使用机器学习预测职业成功指标（晋升、粉丝数、职业发展速度），发现晋升可高度预测，粉丝增长更复杂。


<details>
  <summary>Details</summary>
Motivation: 探索LinkedIn档案特征与职业成功的关系，为专业人士优化LinkedIn展示和职业策略提供数据支持。

Method: 使用机器学习技术分析超过6.2万个匿名LinkedIn档案数据，建立预测模型识别影响职业成功的关键因素。

Result: 晋升可高度预测，但粉丝增长表现出更复杂的模式，识别出驱动职业成功的最具影响力因素。

Conclusion: 研究为专业人士提供了优化LinkedIn存在和职业策略的可操作见解，揭示了不同职业成功指标的预测难度差异。

Abstract: This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.

</details>


### [478] [AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking](https://arxiv.org/abs/2511.12934)
*Zhi Kou,Xiang-Rong Sheng,Shuguang Han,Zhishan Zhao,Yueyao Cheng,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出异步推理框架(AIF)，将预排序模型的用户侧和物品侧计算与实时预测解耦，通过并行和近线计算减少冗余和延迟，提升系统效率。


<details>
  <summary>Details</summary>
Motivation: 传统预排序模型采用顺序执行框架，存在相同用户/物品的冗余计算和严格顺序操作导致的延迟瓶颈，限制了模型容量和系统效率。

Method: AIF框架将交互无关组件(单用户或物品内操作)与实时预测解耦，用户侧计算与检索阶段并行执行，物品侧计算以近线方式完成，交互相关组件采用近似方法在线预测。

Result: AIF提高了计算效率并降低延迟，释放资源显著改善交互无关组件的特征集和模型架构，在淘宝展示广告系统中成功部署。

Conclusion: 通过框架与模型的协同设计，AIF在未显著增加计算和延迟成本的情况下实现了显著的性能提升，解决了工业推荐系统中的效率瓶颈问题。

Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

</details>


### [479] [APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift](https://arxiv.org/abs/2511.12945)
*Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Tao Sun,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了Affine Prototype Timestamp (APT)模块，通过时间戳条件原型学习动态生成仿射参数，解决时间序列预测中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型依赖局部统计归一化，无法捕捉全局分布偏移；RevIN等方法在处理缺失值、噪声观测和无效通道仿射变换方面仍有局限。

Method: 使用轻量级可插拔模块APT，通过时间戳条件原型学习注入全局分布特征，动态生成仿射参数来调制输入输出序列，使主干网络能从自监督的分布感知聚类实例中学习。

Result: 在六个基准数据集和多种主干-归一化组合上的广泛实验表明，APT显著提高了分布偏移下的预测性能。

Conclusion: APT是一个兼容任意预测主干和归一化策略的轻量级模块，在分布偏移场景下能有效提升时间序列预测性能。

Abstract: Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.

</details>


### [480] [A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series](https://arxiv.org/abs/2511.12951)
*Ziling Fan,Ruijia Liang,Yiwen Hu*

Main category: cs.LG

TL;DR: 提出了基于FEDformer的混合框架，用于金融时间序列的异常检测和风险预测，通过频域分析和信号分解提升对金融波动性的建模能力。


<details>
  <summary>Details</summary>
Motivation: 金融市场具有高度波动性，传统深度学习模型难以捕捉金融时间序列中的长期依赖和复杂周期模式，需要更有效的异常检测和风险预测方法。

Method: 集成频率增强分解Transformer（FEDformer）、基于残差的异常检测器和风险预测头，在时域和频域同时建模时间动态，将信号分解为趋势和季节分量。

Result: 在S&P 500、纳斯达克综合指数和布伦特原油数据集上的实验显示，相比基准方法，RMSE降低15.7%，异常检测F1分数提升11.5%。

Conclusion: 该模型能有效捕捉金融波动性，为市场崩盘预测和风险管理提供可靠的早期预警系统。

Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.

</details>


### [481] [Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series](https://arxiv.org/abs/2511.12955)
*Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的GCTAF架构，通过引入可学习的跨注意力全局令牌来增强长时间序列建模能力，用于解决太阳耀斑预测中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑预测面临的主要挑战是耀斑事件固有的不平衡性——强烈耀斑相对稀少，这阻碍了有效的学习。传统自注意力机制仅依赖时间序列内的局部交互，难以捕捉对耀斑预测至关重要的全局显著时间模式。

Method: GCTAF架构在传统自注意力基础上注入一组可学习的跨注意力全局令牌，这些令牌通过跨注意力与输入序列交互，汇总整个序列中的显著时间模式，然后融合回时间表示中，使模型能够识别对耀斑预测至关重要的全局显著、非连续时间点。

Result: 在基准太阳耀斑数据集上的评估表明，GCTAF能够有效检测强烈耀斑并提高预测性能，证明了改进基于Transformer的架构在太阳耀斑预测任务中的高潜力。

Conclusion: GCTAF作为一种动态注意力驱动的时间汇总器，增强了模型捕捉与耀斑相关的判别性动态的能力，为太阳耀斑预测提供了一种有前景的替代方案。

Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.

</details>


### [482] [RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems](https://arxiv.org/abs/2511.12979)
*Zhengchao Wang,Yitao Hu,Jianing Ye,Zhuxuan Chang,Jiazheng Yu,Youpeng Deng,Keqiu Li*

Main category: cs.LG

TL;DR: RAGPulse是一个开源RAG工作负载跟踪数据集，收集自服务超过4万师生的大学问答系统，揭示了真实RAG工作负载具有显著时间局部性和高度偏斜的热门文档访问模式。


<details>
  <summary>Details</summary>
Motivation: 现有的通用LLM推理跟踪无法捕捉RAG特有的动态特性（如知识依赖），导致学术研究与实际部署之间存在显著性能差距。

Method: 从2024年4月起运行的大学问答系统中收集数据，采用隐私保护的基于哈希的数据格式，并进行深入的统计分析。

Result: 分析显示真实RAG工作负载具有显著时间局部性和高度偏斜的热门文档访问模式。

Conclusion: RAGPulse为研究人员开发和验证RAG系统优化策略（如内容感知批处理和检索缓存）提供了高保真基础，最终提升RAG服务的效率和可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.

</details>


### [483] [Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs](https://arxiv.org/abs/2511.13010)
*Jeongwhan Choi,Seungjun Park,Sumin Park,Sung-Bae Cho,Noseong Park*

Main category: cs.LG

TL;DR: 提出了一种名为分形节点的新概念，通过自适应聚合子图级特征表示来增强MPNN的长程依赖能力，同时保持计算效率


<details>
  <summary>Details</summary>
Motivation: GNN在平衡局部和全局信息方面存在困难，图Transformer虽然能处理长程交互但忽略了MPNN的局部性和效率优势

Method: 基于真实网络中的分形结构，引入分形节点与原始节点共存，通过图划分自然诱导分形结构，自适应聚合子图级特征表示

Result: 分形节点缓解了过度压缩问题，提供了直接捷径连接，使子图级表示能够长程传播，提高了MPNN的表达能力

Conclusion: 该方法在保持MPNN计算效率的同时，达到了与图Transformer相当或更好的性能，有效改善了MPNN的长程依赖问题

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.

</details>


### [484] [Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks](https://arxiv.org/abs/2511.12985)
*Minsoo Jo,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出了一种基于双曲几何的新型对抗攻击方法，通过利用双曲空间的几何特性，在切线空间中计算梯度并分解为径向和角度分量，仅从角度方向生成对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击方法如FGSM和PGD在欧几里得几何中研究较多，但双曲网络的发展需要在非欧几何中重新评估攻击策略。传统方法不考虑双曲结构可能导致效率低下或几何不一致的攻击。

Method: 在双曲空间的切线空间中计算损失函数梯度，将其分解为径向（深度）分量和角度（语义）分量，仅使用角度方向的扰动来生成对抗样本，聚焦于双曲几何中编码的语义敏感方向。

Result: 在图像分类、跨模态检索任务和网络架构上的实验结果表明，该攻击方法比传统对抗攻击获得更高的欺骗率，同时产生具有高影响力的扰动，更深入地揭示了双曲嵌入的脆弱性。

Conclusion: 这项工作强调了在弯曲表示空间中几何感知对抗策略的重要性，并为攻击层次嵌入提供了一个原则性框架。

Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.

</details>


### [485] [SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment](https://arxiv.org/abs/2511.13023)
*Jiacheng Wang,Yejun Zeng,Jinyang Guo,Yuqing Ma,Aishan Liu,Xianglong Liu*

Main category: cs.LG

TL;DR: SLMQuant是首个系统评估LLM压缩技术在SLMs上应用的基准，揭示了SLMs与LLMs在量化敏感性上的根本差异，提出了针对SLMs的压缩设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管小型语言模型(SLMs)作为资源高效替代方案受到关注，但在边缘设备上的部署仍面临挑战，因为模型压缩方面存在未解决的效率差距。量化对LLMs有效，但对SLMs的适用性研究不足。

Method: 通过跨多种架构和任务的综合多轨评估，分析最先进的量化方法在SLMs上的表现，识别影响SLM量化的关键因素。

Result: 研究发现SLMs与LLMs在量化敏感性上存在根本差异，直接移植LLM优化技术会导致次优结果，因为SLMs具有独特的架构特征和训练动态。

Conclusion: SLMQuant为在边缘应用中推进高效SLM部署建立了基础框架，并为在资源受限场景中部署轻量级语言模型提供了关键见解。

Abstract: Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.

</details>


### [486] [One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow](https://arxiv.org/abs/2511.13035)
*Zeyuan Wang,Da Li,Yulin Chen,Ye Shi,Liang Bai,Tianyuan Yu,Yanwei Fu*

Main category: cs.LG

TL;DR: 提出一种一步生成策略，通过MeanFlow的残差重构实现噪声到动作的直接映射，兼容Q学习，在离线强化学习中实现高效推理和多模态动作分布建模。


<details>
  <summary>Details</summary>
Motivation: 现有一步高斯策略推理快但难以捕捉复杂多模态动作分布，而基于流的方法表达能力更强但通常需要蒸馏和两阶段训练。需要开发既能高效推理又能表达多模态分布的单阶段训练方法。

Method: 重构MeanFlow，将速度场和噪声到动作变换集成到单一策略网络中，提出有效的残差重构形式，支持直接噪声到动作生成，实现单阶段Q学习训练。

Result: 在OGBench和D4RL基准的73个任务上进行了广泛实验，证明该方法在离线和离线到在线强化学习设置中均取得强劲性能。

Conclusion: 该方法成功结合了一步高效推理、多模态动作分布建模能力和单阶段稳定训练的优势，为离线强化学习提供了有效的解决方案。

Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.

</details>


### [487] [The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training](https://arxiv.org/abs/2511.13016)
*Subramanyam Sahoo*

Main category: cs.LG

TL;DR: 提出一个统一框架研究硬奖励、连续奖励和混合奖励结构在数学推理任务上的应用，通过自适应混合奖励调度器平衡探索和稳定性


<details>
  <summary>Details</summary>
Motivation: 奖励设计是RLHF和对齐研究的核心问题，需要研究不同奖励结构对LLM微调的影响

Method: 使用Qwen3-4B模型和LoRA微调在GSM8K数据集上，评估包含正确性、困惑度、推理质量和一致性的奖励公式，引入自适应混合奖励调度器

Result: 混合奖励结构相比纯硬奖励或连续奖励方法，提高了收敛速度和训练稳定性

Conclusion: 通过自适应奖励建模为对齐研究提供了见解，混合奖励结构在数学推理任务中表现更优

Abstract: Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.

</details>


### [488] [Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting](https://arxiv.org/abs/2511.13052)
*Yunhun Nam,Jaehyung Kim,Jongheon Jeong*

Main category: cs.LG

TL;DR: 提出LfU方法，通过正则化SFT过程来缓解语言模型在有限数据下微调时的过拟合问题，利用不良模型更新的表示对齐来提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统SFT在有限数据下容易导致语言模型过拟合，使其依赖虚假模式或损害预训练获得的通用能力。

Method: 提出LfU方法，通过一致性正则化直接对齐模型内部表示与经过不良更新后的表示，利用表示级数据增强来提升泛化。

Result: 在数学任务上比传统SFT平均提升16.8%，且对提示变化具有更好的鲁棒性，输出性能标准差降低92.1%。

Conclusion: LfU作为有效先验，在提升适应性的同时能保持预训练知识，具有广泛的应用效果。

Abstract: Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.

</details>


### [489] [The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference](https://arxiv.org/abs/2511.13018)
*Sairam S,Sara Girdhar,Shivam Soni*

Main category: cs.LG

TL;DR: R-Learner在图上应用时存在严重的"表示瓶颈"问题，图无关的最终阶段模型会导致完全失败，而端到端的Graph R-Learner能显著超越传统基线方法。


<details>
  <summary>Details</summary>
Motivation: R-Learner在处理网络数据时面临核心假设挑战，其最终阶段模型需要适应图依赖的因果异质性，但现有方法未能系统研究这一问题。

Method: 通过大规模实证研究系统分析R-Learner在图上的表现，提出端到端的Graph R-Learner，并进行"Hub-Periphery Trade-off"分析来揭示GNN过压缩问题。

Result: 统计显著证明(p<0.001)图无关最终阶段的R-Learner完全失败(MSE>4.0)，而Graph R-Learner显著优于非DML GNN T-Learner基线。发现拓扑依赖的"干扰瓶颈"与GNN过压缩相关。

Conclusion: R-Learner性能主要受最终阶段CATE估计器的归纳偏差驱动，而非干扰模型选择。揭示了关键的"最终阶段瓶颈"问题，为未来研究提供了可复现基准。

Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."

</details>


### [490] [Latency and Ordering Effects in Online Decisions](https://arxiv.org/abs/2511.13060)
*Duo Yi*

Main category: cs.LG

TL;DR: 该论文提出了一个针对延迟反馈和顺序敏感动态的在线决策系统的基准损失下界分析框架，将延迟、顺序敏感性及其相互作用量化为可解释的惩罚项。


<details>
  <summary>Details</summary>
Motivation: 在线决策系统经常面临延迟反馈和顺序敏感动态的挑战，其中动作会影响观察结果的到达顺序，传统方法难以系统分析这些复杂效应。

Method: 使用Bregman散度作为损失基准，证明超额基准损失存在结构化下界，包含延迟惩罚、顺序敏感性惩罚、几何交互惩罚和非凸性惩罚。扩展到近正则和弱凸设置，并提供通过2×2随机实验和流式诊断估计这些项的实用方法。

Result: 建立了一个统一的框架，将异构延迟、非交换性和实现差距效应打包成单个可解释的下界陈述，可在实际系统中进行压力测试和调优。

Conclusion: 该框架为在线决策系统在延迟反馈和顺序敏感动态下的性能分析提供了理论基础和实用工具，超越了传统凸优化方法的限制。

Abstract: Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Φ$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \ge L_{\mathrm{ideal}} + g_1(λ) + g_2(\varepsilon_\star) + g_{12}(λ,\varepsilon_\star) - D_{\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\mathrm{ncx}}\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.

</details>


### [491] [Learning Time-Scale Invariant Population-Level Neural Representations](https://arxiv.org/abs/2511.13022)
*Eshani Patel,Yisong Yue,Geeling Chau*

Main category: cs.LG

TL;DR: 论文提出时间尺度增强预训练(TSAP)方法，解决神经时间序列基础模型对预处理时间尺度不匹配的敏感性问题，提高模型在不同时间尺度下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经时间序列基础模型在预训练和下游任务之间存在预处理时间尺度不匹配时泛化性能下降，缺乏对时间尺度的不变性表示。

Method: 引入时间尺度增强预训练(TSAP)，通过在预训练阶段加入时间尺度增强来构建对时间尺度的不变性表示。

Result: TSAP方法在不同解码任务中一致地提高了对不同时间尺度的鲁棒性，并在表示空间中构建了不变性。

Conclusion: 处理预处理多样性是构建可泛化神经基础模型的关键步骤，TSAP为此提供了有效解决方案。

Abstract: General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.

</details>


### [492] [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)
*Vladimír Macko,Vladimír Boža*

Main category: cs.LG

TL;DR: MACKO-SpMV是一种GPU优化的稀疏矩阵向量乘法格式和内核，专门为处理大语言模型中30-90%非结构化稀疏度而设计，在50%稀疏度下实现1.5倍内存减少和1.2-1.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有SpMV方法在处理大语言模型剪枝后常见的低且非结构化稀疏度（30-90%）时性能不佳，导致非结构化剪枝只能提供有限的内存减少和加速效果。

Method: 提出MACKO-SpMV，一种与GPU执行模型兼容的GPU优化格式和内核协同设计，减少存储开销，无需专用硬件单元或格式特定的预计算。

Result: 在50%稀疏度下，MACKO首次实现显著的内存减少（1.5倍）和加速（1.2-1.5倍），相比其他SpMV基线：cuSPARSE快2.8-13.0倍，Sputnik快1.9-2.6倍，DASP快2.2-2.5倍。应用于Llama2-7B模型时，在fp16精度下实现1.5倍内存减少和1.5倍推理加速。

Conclusion: MACKO使得在50%稀疏度下的非结构化剪枝在实际大语言模型工作负载中变得合理可行。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.

</details>


### [493] [Self-Adaptive Graph Mixture of Models](https://arxiv.org/abs/2511.13062)
*Mohit Meena,Yash Punjabi,Abhishek A,Vishal Sharma,Mahesh Chandran*

Main category: cs.LG

TL;DR: 提出SAGMM框架，通过自适应选择和组合多种GNN架构来解决图神经网络模型选择难题，在多个图学习任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前GNN性能增长趋于平缓，复杂模型并不总是优于经典模型，且为特定图任务选择合适模型存在困难。

Method: SAGMM采用模块化框架，利用架构多样性和拓扑感知注意力门控机制，为每个节点自适应分配专家模型，并包含剪枝机制提高效率。

Result: 在16个基准数据集上的评估显示，SAGMM在节点分类、图分类、回归和链接预测任务中始终优于或匹配领先的GNN基线和现有混合方法。

Conclusion: SAGMM为现实世界图学习提供了鲁棒且自适应的解决方案，能够自动选择最适合的模型组合。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.

</details>


### [494] [Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning](https://arxiv.org/abs/2511.13116)
*Qipeng Song,Nan Yang,Ziqi Xu,Yue Li,Wei Shao,Feng Xia*

Main category: cs.LG

TL;DR: GFOES是一个用于机器遗忘的框架，在仅有少量保留数据且无法访问遗忘数据的情况下，通过生成最优擦除样本和两阶段微调来实现有效的类别遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法通常需要访问完整的原始训练数据集，这在现实中往往不切实际。本文针对更现实但更具挑战性的场景：few-shot zero-glance设置，即只有少量保留数据可用且遗忘数据完全无法访问。

Method: 提出GFOES框架，包含生成反馈网络(GFN)和两阶段微调过程。GFN合成最优擦除样本(OES)，这些样本在目标类别上产生高损失，使模型能够遗忘类别特定知识而无需访问原始遗忘数据。两阶段微调包括：第一阶段进行激进遗忘，第二阶段恢复模型性能。

Result: 在三个图像分类数据集上的实验表明，GFOES在logit和表示层面都能实现有效遗忘，同时仅使用5%的原始数据就能保持强大的性能。

Conclusion: GFOES为数据受限条件下的隐私保护机器学习提供了一个实用且可扩展的解决方案。

Abstract: Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.

</details>


### [495] [Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data](https://arxiv.org/abs/2511.13044)
*Rosario Napoli,Giovanni Lonia,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: Bi-View是一种新颖的混合方法，通过结合Node2Vec和GraphSAGE两种图嵌入技术来增强知识图谱中节点特征的信息内容，生成改进的图嵌入，从而提升图机器学习模型性能，无需依赖额外的合成数据。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法需要大量数据才能表现良好，限制了在稀疏或不完整场景下的应用。知识图谱由于其语义性质可能隐藏大量信息，现有图机器学习方法在处理知识图谱时面临局限性。

Method: 结合两种互补的图嵌入技术：Node2Vec（通过无监督随机游走捕获结构模式）和GraphSAGE（以监督方式聚合邻域信息）。首先计算Node2Vec嵌入表示图拓扑，然后用基于中心性的指标丰富节点特征作为GraphSAGE输入，最后通过融合层结合原始Node2Vec嵌入和GraphSAGE影响的表示。

Result: 该方法提高了下游任务性能，特别是在初始特征较差的情况下，为更准确和精确的知识图谱增强图机器学习模型奠定了基础。

Conclusion: Bi-View方法能够捕获图的拓扑和语义特性，使模型能够利用数据集中存在但未明确表示的信息特征，在不依赖额外合成数据的情况下提升图机器学习模型性能。

Abstract: Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.

</details>


### [496] [Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2511.13133)
*Shudong Wang,Xinfei Wang,Chenhao Zhang,Shanchen Pang,Haiyuan Gui,Wenhao Ji,Xiaojian Liao*

Main category: cs.LG

TL;DR: SoCo-DT是一种基于参数重要性的软冲突解决方法，通过动态调整掩码值和自适应稀疏度策略来缓解多任务强化学习中的梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中存在梯度冲突问题，现有基于掩码的方法使用粗粒度二元掩码会过度抑制关键冲突参数，阻碍任务间知识共享，且采用固定稀疏度策略无法适应不同任务的冲突程度差异。

Method: 利用Fisher信息动态调整掩码值，保留重要参数同时抑制冲突参数；引入基于四分位距的动态稀疏度调整策略，构建任务特定的阈值方案；采用非对称余弦退火调度持续更新阈值。

Result: 在Meta-World基准测试中，SoCo-DT在MT50上比最先进方法提升7.6%，在次优数据集上提升10.5%。

Conclusion: SoCo-DT能有效缓解梯度冲突，提高多任务整体性能，证明了其在多任务强化学习中的有效性。

Abstract: Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.

</details>


### [497] [ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer](https://arxiv.org/abs/2511.13198)
*Zhixin Ou,Peng Liang,Jianchen Han,Baihui Liu,Linbo Qiao*

Main category: cs.LG

TL;DR: ParaDySe是一个用于动态序列的自适应并行策略切换框架，解决了LLM训练中短序列通信并行化取消和长序列内存不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前训练框架对动态长度序列采用预定义的静态并行策略，导致短序列出现通信-并行化取消问题，长序列出现内存不足问题。

Method: 实现并行策略的模块化函数库，构建序列感知的内存和时间成本模型，通过启发式算法为动态序列选择最优层间策略。

Result: 在序列长度达624K的数据集上，ParaDySe解决了LLM训练中的内存不足和通信并行化取消瓶颈。

Conclusion: ParaDySe通过系统集成长序列优化与现有框架，实现了最优策略的无缝热切换。

Abstract: Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.

</details>


### [498] [Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2511.13053)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 通过几何分析Hopfield网络的能量景观，发现存在"优化脊"现象，在高负载和全局核条件下网络能最大化吸引子稳定性，揭示了直接驱动力与反馈力之间的反相关性机制。


<details>
  <summary>Details</summary>
Motivation: 理解基于核的学习方法如何显著提升Hopfield网络存储容量的动力学机制，这一增强背后的物理原理尚不明确。

Method: 引入"峰顶锐度"度量来量化吸引子局部稳定性，通过系统改变核宽度和存储负载，分析能量景观的几何特性，并对景观梯度进行理论分解。

Result: 发现了丰富的吸引子形状相图，核心发现是"优化脊"的出现，在该区域直接驱动力（由高存储负载放大）主导了对抗的集体反馈力。

Conclusion: 网络通过自适应地利用模式间相互作用作为协作反馈控制系统，塑造出鲁棒的能量景观，这为高容量联想记忆的稳定性提供了新的物理图像和设计原则。

Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.

</details>


### [499] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: TokenSqueeze是一种新的Long2Short方法，通过自适应选择推理深度和分布对齐的语言精炼，在保持性能的同时减少推理LLMs的token使用量。


<details>
  <summary>Details</summary>
Motivation: 现有的推理LLMs生成长链式思维痕迹导致token使用量增加，造成更高的推理延迟和内存消耗，需要平衡准确性和推理效率。

Method: 1. 自适应选择推理深度与问题复杂度匹配的样本；2. 分布对齐的语言精炼方法，在不改变推理路径的情况下优化语言表达。

Result: DeepSeek-R1-Distill-Qwen-7B使用该方法在MATH500基准上实现了50%的平均token减少，同时保持准确性。

Conclusion: TokenSqueeze仅利用模型自生成数据，无需依赖手动整理的短答案数据集，就能实现高效高保真的推理。

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [500] [Uncovering and Mitigating Transient Blindness in Multimodal Model Editing](https://arxiv.org/abs/2511.13243)
*Xiaoqi Han,Ru Li,Ran Yi,Hongye Tan,Zhuomin Liang,Víctor Gutiérrez-Basulto,Jeff Z. Pan*

Main category: cs.LG

TL;DR: 本文提出了一个全面的多模态模型编辑评估框架，解决了现有方法因依赖低相似性或随机输入而夸大成功率的问题，揭示了编辑过程中的瞬态盲现象，并提出了位置感知对抗损失来平衡跨模态表示。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本模型编辑的多模态模型编辑评估方法存在夸大成功率的问题，它们依赖低相似性或随机输入来掩盖过拟合现象，需要更全面的评估框架来准确衡量编辑效果。

Method: 提出了包含三个关键维度（随机图像位置性、无图像位置性、一致图像位置性）的全面位置性评估框架，通过七种不同的数据类型实现详细分析；引入De-VQA动态评估方法；提出位置感知对抗损失来平衡跨模态表示。

Result: 实证结果显示，该方法持续优于现有基线，平均减少17%的瞬态盲现象并改善位置性；令牌分析显示编辑对文本令牌的影响不成比例。

Conclusion: 提出的位置性评估框架和位置感知对抗损失方法有效解决了多模态模型编辑中的瞬态盲问题，显著改善了编辑的准确性和鲁棒性。

Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

</details>


### [501] [Seek and You Shall Fold](https://arxiv.org/abs/2511.13244)
*Nadav Bojan Sellam,Meital Bojan,Paul Schanda,Alex Bronstein*

Main category: cs.LG

TL;DR: 提出了一个非可微分指导框架，将连续扩散生成模型与任意黑盒目标耦合，用于在蛋白质生成模型中整合实验数据，特别是核磁共振化学位移数据。


<details>
  <summary>Details</summary>
Motivation: 准确蛋白质结构对理解生物功能至关重要，但将实验数据整合到蛋白质生成模型中仍面临挑战，特别是核磁共振化学位移等非可微分预测器难以与基于梯度的条件采样兼容。

Method: 开发了一个非可微分指导框架，将连续扩散生成模型与任意黑盒目标通过定制遗传算法耦合，支持距离约束、核奥弗豪泽效应约束和化学位移等多种模式。

Result: 证明了化学位移指导结构生成的可行性，揭示了当前预测器的关键弱点，展示了整合多样化实验信号的通用策略。

Conclusion: 该工作为实现超越可微分限制的自动化、数据条件化蛋白质建模指明了方向。

Abstract: Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.

</details>


### [502] [A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning](https://arxiv.org/abs/2511.13078)
*Liuyi Jin,Pasan Gunawardena,Amran Haroon,Runzhi Wang,Sangwoo Lee,Radu Stoleru,Michael Middleton,Zepeng Huo,Jeeeun Kim,Jason Moats*

Main category: cs.LG

TL;DR: EMSGlass是一个基于EMSNet多模态多任务模型和EMSServe低延迟服务框架的智能眼镜系统，专门为急救医疗服务设计，通过整合文本、生命体征和场景图像来提升急救人员的实时态势感知和决策效率。


<details>
  <summary>Details</summary>
Motivation: 急救医疗技术人员在高压环境下工作，需要在重认知负荷下快速做出关键决策。现有系统缺乏有效的多模态信息整合和实时推理能力。

Method: 开发了EMSNet多模态多任务模型，整合文本、生命体征和场景图像，同时支持5个关键EMS任务；构建了EMSServe低延迟服务框架，采用模态感知模型分割和特征缓存机制。

Result: EMSNet在准确性上优于最先进的单模态基线模型；EMSServe比直接PyTorch多模态推理快1.9-11.7倍；用户研究显示EMSGlass能显著提升实时态势感知、决策速度和操作效率。

Conclusion: EMSGlass成功将多模态智能与真实世界急救响应工作流程相结合，为下一代AI赋能的EMS系统提供了可行方向。

Abstract: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.

</details>


### [503] [KForge: Program Synthesis for Diverse AI Hardware Accelerators](https://arxiv.org/abs/2511.13274)
*Taras Sereda,Tom St. John,Burak Bartan,Natalie Serrino,Sachin Katti,Zain Asgar*

Main category: cs.LG

TL;DR: KForge是一个平台无关的GPU内核优化框架，使用两个协作的LLM代理：生成代理通过编译和正确性反馈迭代优化程序，性能分析代理通过分析性能数据指导优化。该架构只需单次示例即可针对新平台，支持跨平台知识迁移，并在NVIDIA CUDA和Apple Metal上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: GPU内核对机器学习性能至关重要，但在不同加速器上优化困难。现有方法难以适应多样化的硬件平台，需要一种平台无关的优化框架。

Method: 基于两个协作的LLM代理：生成代理负责程序生成和迭代优化，性能分析代理解释性能分析数据并提供优化建议。通过功能性和优化传递进行协作，支持从程序化API到GUI工具的各种性能分析数据。

Result: 实现了跨平台知识迁移，一个架构的参考实现能显著提升其他硬件目标的生成质量。在NVIDIA CUDA和Apple Metal等不同并行计算平台上验证了有效的程序合成能力。

Conclusion: KForge提供了一个平台无关的GPU内核优化解决方案，通过协作代理架构实现了跨硬件平台的程序优化，只需单次示例即可适应新平台，展示了强大的可扩展性和通用性。

Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.

</details>


### [504] [Real-time prediction of breast cancer sites using deformation-aware graph neural network](https://arxiv.org/abs/2511.13082)
*Kyunghyun Lee,Yong-Min Shin,Minwoo Shin,Jihun Kim,Sunghwan Lim,Won-Yong Shin,Kyungho Yoon*

Main category: cs.LG

TL;DR: 开发基于图神经网络的实时变形预测模型，用于乳腺癌活检中准确预测肿瘤位置变形，实现毫米级精度和4000倍计算加速。


<details>
  <summary>Details</summary>
Motivation: 解决间接MRI引导活检中实时变形乳房模型精度不足的问题，克服传统方法耗时长、成本高的限制。

Method: 结合个体特异性有限元模型和GNN，利用MRI图像结构信息处理表面位移和距离图数据，预测组织整体位移。

Result: 在幻影和真实患者数据验证中，癌症节点位移误差小于0.2mm，空间重叠DSC达0.977，计算速度比传统FE模拟快4000倍。

Conclusion: 该变形感知GNN模型为乳腺癌活检提供了高精度实时肿瘤位移预测方案，有望显著提升诊断精度和效率。

Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.

</details>


### [505] [Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning](https://arxiv.org/abs/2511.13322)
*Senne Deproost,Dennis Steckelmacher,Ann Nowé*

Main category: cs.LG

TL;DR: 提出了一种基于Voronoi分区的新方法，将状态空间划分为多个区域，在每个区域内使用可解释的线性模型来近似复杂的深度强化学习控制器，从而实现知识蒸馏并提高透明度。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习控制器虽然性能优秀但缺乏透明度，这给满足监管要求和建立信任带来挑战。需要将学习到的行为转移到人类可读的模型中，但单一简化模型在动态情况下表现不佳，需要在灵活性和复杂性之间找到平衡。

Method: 使用Voronoi分区方法将状态空间划分为多个区域，在每个区域内训练专门的线性模型来近似原始黑盒控制器的行为。该方法与模型无关，可以应用于各种控制器。

Result: 在网格世界环境和经典控制任务上的评估表明，这种局部专业化线性模型的蒸馏方法产生的策略具有可解释性，并且性能与原始黑盒策略相当甚至略有提升。

Conclusion: 提出的基于Voronoi分区的局部专业化线性模型蒸馏方法能够有效平衡模型复杂性和性能，为深度强化学习控制器提供了可解释的替代方案，同时保持或略微提升性能。

Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.

</details>


### [506] [Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning](https://arxiv.org/abs/2511.13351)
*Xinlan Wu,Bin Zhu,Feng Han,Pengkun Jiao,Jingjing Chen*

Main category: cs.LG

TL;DR: 提出了一种用于多模态食物学习的持续学习框架，通过双LoRA架构和质量增强伪回放来解决现有模型在学习新任务时的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的食物分析大模型在学习新任务时会出现灾难性遗忘，需要昂贵的从头训练，这限制了模型的实际应用。

Method: 采用双LoRA架构：专用LoRA学习任务特定知识并保持与先前任务子空间的正交性，协作LoRA通过伪回放整合跨任务共享知识。质量增强伪回放利用自一致性和语义相似性减少生成样本中的幻觉。

Result: 在Uni-Food数据集上的实验表明，该方法在减轻遗忘方面表现出色，是首个针对复杂食物任务的有效持续学习方法。

Conclusion: 提出的持续学习框架成功解决了多模态食物学习中的灾难性遗忘问题，为个性化营养和慢性病预防等健康相关任务提供了有效的解决方案。

Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.

</details>


### [507] [Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges](https://arxiv.org/abs/2511.13124)
*Changxi Chi,Yufei Huang,Jun Xia,Jiangbin Zheng,Yunfan Liu,Zelin Zang,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于薛定谔桥近似的方法，用于直接对齐控制组和扰动组的单细胞分布，解决了单细胞扰动数据不成对的问题，并在遗传和药物扰动数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞扰动结果预测对基因功能分析和药物候选选择至关重要，但由于测序的破坏性，单细胞数据通常不成对，现有方法缺乏显式条件或依赖先验空间进行间接分布对齐，限制了精确的扰动建模。

Method: 使用基于小批量最优传输的配对来近似薛定谔桥，避免双向推理和反向过程定义的不适定性，同时建模离散基因激活状态和连续表达分布。

Result: 在公共遗传和药物扰动数据集上的实验表明，该模型能有效捕捉异质性单细胞响应，并达到最先进的性能。

Conclusion: 通过直接对齐控制组和扰动组分布，该方法能够准确建模单细胞扰动并捕捉细胞异质性，为单细胞扰动预测提供了有效的解决方案。

Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

</details>


### [508] [A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs](https://arxiv.org/abs/2511.13373)
*Prakrit Timilsina,Anuj Nepal,Rajan Kadel,Robin Doss*

Main category: cs.LG

TL;DR: 本文系统评估了六种参数空间融合技术应用于两个基于Mistral-7B的医疗大语言模型，发现对于架构兼容的模型，简单的平均方法在医学基准测试中表现最佳，为资源受限的分布式医疗AI部署提供了实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在分布式医疗中面临的挑战：整合跨机构专业知识同时保护隐私、降低计算开销、防止模型更新时的灾难性遗忘。

Method: 提出了一种新颖的分层方法，结合选择性最优传输对齐注意力层和余弦相似度加权插值，并系统评估了Task Arithmetic、线性平均、DARE-TIES、DELLA、Breadcrumbs和分层方法等六种参数融合技术。

Result: 架构兼容的模型从简单平均方法中获益显著，Task Arithmetic在MedQA上达到45.80%准确率，优于复杂的剪枝方法。

Conclusion: 对于架构兼容的模型，简单平均提供了稳健且计算高效的知识整合基线，为可扩展医疗AI系统提供了实用路径。

Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.

</details>


### [509] [Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching](https://arxiv.org/abs/2511.13144)
*Jiacheng Cheng,Xu Zhang,Guanghui Qiu,Yifang Zhang,Yinchuan Li,Kaiyuan Feng*

Main category: cs.LG

TL;DR: 提出pFed1BS个性化联邦学习框架，通过一位随机草图实现极端通信压缩，解决联邦学习中的通信开销和客户端数据异构性问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临双向通信开销和客户端数据异构性的关键挑战，需要在降低通信成本的同时处理数据异构性。

Method: 使用一位随机草图进行通信压缩，引入基于符号的正则化器指导本地模型与全局共识对齐，同时保留本地数据特征，采用快速Hadamard变换提高投影效率。

Result: 理论分析证明算法收敛到全局势函数的平稳邻域，数值模拟显示pFed1BS显著降低通信成本，在性能上与先进的通信高效联邦学习算法相当。

Conclusion: pFed1BS框架成功实现了通信压缩与个性化学习的平衡，为联邦学习中的通信效率和数据异构性问题提供了有效解决方案。

Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.

</details>


### [510] [Finding Kissing Numbers with Game-theoretic Reinforcement Learning](https://arxiv.org/abs/2511.13391)
*Chengdong Ma,Théo Tao Zhaowei,Pengyu Li,Minghao Liu,Haojun Chen,Zihao Mao,Yuan Cheng,Yuan Qi,Yaodong Yang*

Main category: cs.LG

TL;DR: PackingStar使用博弈论强化学习解决高维接吻数问题，在25-31维中超越所有已知记录，发现了6000多个新结构。


<details>
  <summary>Details</summary>
Motivation: 接吻数问题是几何学、数论和信息论的基础挑战，高维几何的不规则性和指数级组合复杂性限制了现有方法的可扩展性。

Method: 将问题建模为双玩家矩阵完成博弈，一个玩家填充矩阵条目，另一个修正次优条目，合作最大化矩阵大小，对应接吻数。

Result: 在25-31维中超越所有人类已知记录，25维配置几何对应Leech格点，13维首次突破1971年的有理结构限制，发现6000多个新结构。

Conclusion: AI能够探索超越人类直觉的高维空间，为接吻数问题和更广泛的几何问题开辟了新途径。

Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.

</details>


### [511] [OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs](https://arxiv.org/abs/2511.13147)
*Shaoyuan Chen,Zhixuan Chen,Dawei Yang,Zhihang Yuan,Qiang Wu*

Main category: cs.LG

TL;DR: OTARo是一种新颖的量化方法，允许设备端LLM灵活切换量化精度，通过一次微调实现多比特宽度的性能鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法存在结构限制，无法支持设备端在不同任务需求下灵活切换精度，而实际应用中理解任务和生成任务对量化精度的容忍度不同。

Method: 提出共享指数浮点数(SEFP)量化机制，通过简单的尾数截断实现不同比特宽度；采用利用-探索比特宽度路径搜索(BPS)和低精度异步累积(LAA)策略进行学习。

Result: 在LLaMA3.2-1B、LLaMA3-8B等流行LLM上的实验表明，OTARo在所有精度下都能实现一致强大且鲁棒的性能。

Conclusion: OTARo成功解决了设备端LLM量化精度灵活切换的问题，为复杂现实场景提供了有效的解决方案。

Abstract: Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.

</details>


### [512] [PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation](https://arxiv.org/abs/2511.13414)
*Hanwen Hu,Zimo Wen,Shiyou Qian,Jian Co*

Main category: cs.LG

TL;DR: PAST网络通过主-辅助时空模式处理交通时间序列插补，在27种缺失数据条件下优于7个基线方法，RMSE提升达26.2%，MAE提升达31.6%。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以适应随机缺失位置，无法学习长期和大规模依赖关系，这在广泛缺失条件下至关重要。

Method: 提出PAST网络，包含图集成模块(GIM)和交叉门控模块(CGM)。GIM通过动态图和区间感知dropout捕获主要模式，CGM通过双向门控提取辅助模式。两个模块通过共享隐藏向量交互，在集成自监督框架下训练。

Result: 在三个数据集上的27种缺失数据条件下，PAST的插补精度优于7个最先进基线方法，RMSE提升达26.2%，MAE提升达31.6%。

Conclusion: PAST通过区分主要和辅助模式，有效处理各种缺失数据条件，显著提升了交通时间序列插补性能。

Abstract: Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.

</details>


### [513] [Warm-starting active-set solvers using graph neural networks](https://arxiv.org/abs/2511.13174)
*Ella J. Schmidtobreick,Daniel Arnström,Paul Häusner,Jens Sjölund*

Main category: cs.LG

TL;DR: 使用图神经网络预测二次规划问题中的有效集，以热启动求解器，减少迭代次数并提高实时优化效率。


<details>
  <summary>Details</summary>
Motivation: 二次规划求解器在实时控制和优化中广泛应用，但计算成本限制了其在时间关键场景中的应用。需要一种方法来加速求解过程。

Method: 采用学习优化方法，使用图神经网络将二次规划问题表示为二分图，学习预测最优有效集来热启动DAQP求解器。

Result: 图神经网络在不同问题规模下都能减少求解器迭代次数，性能与多层感知器基线相当，且能有效泛化到未见过的维度。

Conclusion: 结构感知学习在加速实时应用（如模型预测控制）中的优化过程方面具有潜力。

Abstract: Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.

</details>


### [514] [Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes](https://arxiv.org/abs/2511.13444)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.LG

TL;DR: 提出了一种基于图像卷积聚类的无监督方法，用于从单变量时间序列中发现工业操作模式，通过灰度矩阵转换、两阶段聚类策略和复合评估指标，在熔炉操作数据中识别出7种可解释的操作模式。


<details>
  <summary>Details</summary>
Motivation: 工业过程监控中传感器时间序列数据缺乏标签、高变异性且存在操作噪声，传统方法难以提取有意义的模式。现有聚类技术要么依赖固定距离度量，要么使用静态数据设计的深度模型，无法有效处理动态、非结构化的工业序列。

Method: 将原始时间序列通过重叠滑动窗口转换为灰度矩阵表示，使用深度卷积自编码器进行特征提取；集成软硬聚类输出并通过两阶段策略进行优化；使用新开发的复合评分S_eva（结合标准化轮廓系数、Calinski-Harabasz和Davies-Bouldin指数）客观评估聚类性能。

Result: 在北欧铸造厂的3900多个熔炉熔化操作中，该方法识别出7种可解释的操作模式，揭示了能耗、热动力学和生产持续时间的显著差异。相比经典和深度聚类基线，该方法实现了更优的整体性能、更强的鲁棒性和领域对齐的可解释性。

Conclusion: 该框架解决了无监督时间序列分析中的关键挑战，如序列不规则性、模式重叠和度量不一致性，为工业系统中的数据驱动诊断和能源优化提供了通用解决方案。

Abstract: Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.

</details>


### [515] [Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach](https://arxiv.org/abs/2511.13178)
*Mingxuan Tian,Haochen Mu,Donghong Ding,Mengjiao Li,Yuhan Ding,Jianping Zhao*

Main category: cs.LG

TL;DR: 提出了基于物理信息的神经算子(PINO)方法，用于金属增材制造中实时预测未来15秒的z和y方向变形场，解决了传统数值模拟计算成本高和机器学习模型难以提取时空特征的问题。


<details>
  <summary>Details</summary>
Motivation: 数字孪生和智能制造系统的发展迫切需要实时变形场预测来控制金属增材制造缺陷，但传统数值模拟方法计算成本高、运行时间长，而传统机器学习模型难以提取长时域预测的时空特征且无法解耦热力场。

Method: 采用物理信息深度算子网络-循环神经网络(PIDeepONet-RNN)，使用主干和分支网络分别处理温度历史和编码变形场，实现热力响应的解耦，并通过热传导方程作为软约束确保物理一致性。

Result: 模型在实验验证的有限元方法生成的数据集上训练测试，达到高精度、低误差累积和时间效率。z和y方向最大绝对误差分别为0.9733mm和0.2049mm，熔池区域误差较高但关键沉积区域梯度范数较低。

Conclusion: PINO代理模型在实时长时域物理场预测方面具有巨大潜力，可用于缺陷控制，其基于物理定律的基础函数为预测提供了稳健且可解释的基础。

Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.

</details>


### [516] [Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure](https://arxiv.org/abs/2511.13457)
*Bin Liu,Qinghao Zhao,Yuxi Zhou,Zhejun Sun,Kaijie Lei,Deyun Zhang,Shijia Geng,Shenda Hong*

Main category: cs.LG

TL;DR: 提出了一种基于自监督表示学习的方法，结合肺活量时间序列和人口统计学数据，用于早期检测右心衰竭，在UK Biobank数据集上表现良好，特别在高风险临床亚组中效果显著。


<details>
  <summary>Details</summary>
Motivation: 右心衰竭是一种与高发病率和死亡率相关的疾病，肺病常导致右心室负荷增加引发右心衰竭。从肺心病患者中早期筛查出可能发展为右心衰竭的患者具有重要临床意义。

Method: 采用两阶段方法：第一阶段使用变分自编码器从数据增强的无标签数据中学习肺活量时间序列的稳健低维表示；第二阶段将该表示与人口统计学信息融合，输入CatBoost分类器进行右心衰竭预测。

Result: 在UK Biobank的26,617名个体上测试，模型AUROC达到0.7501；在高风险亚组中表现更佳：慢性肾病患者测试集AUROC为0.8194，瓣膜性心脏病患者为0.8413。

Conclusion: 该研究提出的自监督表示学习方法结合肺活量时间序列和人口统计学数据，在临床实践中显示出早期检测右心衰竭的良好潜力。

Abstract: Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.

</details>


### [517] [Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction](https://arxiv.org/abs/2511.13185)
*Aishwarya Venkataramanan,Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Joachim Denzler*

Main category: cs.LG

TL;DR: 评估和比较了多种不确定性量化技术在CARS到拉曼信号重建中的应用，并证明将物理知识约束融入模型可以改善校准，为更可靠的CARS数据分析提供路径。


<details>
  <summary>Details</summary>
Motivation: CARS光谱学在医学、材料科学和化学分析中广泛应用，但其有效性受到非共振背景干扰的限制。现有的深度学习方法虽然能重建拉曼光谱，但缺乏不确定性量化能力，这在高风险应用中至关重要。

Method: 评估和比较了多种不确定性量化技术在CARS到拉曼信号重建中的表现，并将物理知识约束（如Kramers-Kronig关系和光滑性约束）以物理信息损失函数的形式整合到模型中。

Result: 研究表明，将物理知识约束整合到不确定性量化模型中能够改善模型的校准性能。

Conclusion: 这项工作为更可靠的CARS数据分析提供了一条有前景的路径，通过结合物理知识约束和不确定性量化技术，提高了模型在科学和生物医学应用中的可信度。

Abstract: Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.

</details>


### [518] [Multi-task GINN-LP for Multi-target Symbolic Regression](https://arxiv.org/abs/2511.13463)
*Hussein Rajabu,Lijun Qian,Xishuang Dong*

Main category: cs.LG

TL;DR: 提出了多任务回归GINN-LP（MTRGINN-LP），一种用于多目标符号回归的可解释神经网络，通过结合GINN-LP与多任务深度学习，解决传统符号回归在现实多输出任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 符号回归面临两个主要挑战：大多数方法在具有明确关系的科学数据集上评估，限制了泛化能力；主要针对单输出回归，而许多现实问题涉及具有相互依赖变量的多目标输出。

Method: 通过集成GINN-LP与多任务深度学习，模型结合了包含多个幂项逼近器块的共享骨干网络和任务特定的输出层，在保持可解释性的同时捕获目标间依赖关系。

Result: 在能源效率预测和可持续农业等实际多目标应用上验证了多任务GINN-LP，实验结果显示具有竞争力的预测性能和高度可解释性。

Conclusion: 该方法有效地将符号回归扩展到更广泛的现实世界多输出任务中。

Abstract: In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.

</details>


### [519] [MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing](https://arxiv.org/abs/2511.13234)
*Boris Kriuk*

Main category: cs.LG

TL;DR: MorphBoost是一种新型梯度提升框架，通过自组织树结构动态调整分裂行为，在训练过程中自适应演化分裂函数，在多个数据集上实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统梯度提升算法使用静态树结构和固定分裂标准，无法适应训练过程中梯度分布的变化和不同学习阶段的问题特性，限制了模型的适应能力。

Method: 提出自适应分裂函数，基于累积梯度统计和迭代相关学习压力演化；包括形态变化分裂准则、自动问题指纹识别、向量化树预测、交互感知特征重要性和快速模式优化。

Result: 在10个数据集上的综合基准测试显示，MorphBoost平均性能优于XGBoost 0.84%，获得4/10数据集胜利（40%胜率）和6/30前三名（20%），同时保持最低方差（σ=0.0948）和最高最小准确率。

Conclusion: MorphBoost通过动态自组织树结构实现了卓越的适应性和鲁棒性，在困难问题上表现尤为突出，展现了梯度提升算法的新发展方向。

Abstract: Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.

</details>


### [520] [Incoherent Beliefs & Inconsistent Actions in Large Language Models](https://arxiv.org/abs/2511.13240)
*Arka Pal,Teo Kitanovski,Arthur Liang,Akilesh Potti,Micah Goldblum*

Main category: cs.LG

TL;DR: LLMs在动态环境中存在信念更新不一致和行动与信念不匹配的问题，即使在静态任务中表现良好的模型也存在这些问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务与静态数据集存在差异，需要模型能够进行顺序交互、连贯更新信念并基于信念做出适当决策。预测LLMs在动态环境中的表现很重要，但从静态设置中难以确定。

Method: 通过实验检验LLMs的两个关键能力：连贯更新信念的能力，以及行动与信念一致性的程度。包括直接引出后验与正确更新先验的比较，以及在博彩市场中行动与信念的一致性分析。

Result: 发现LLMs在信念更新上存在高达30%的平均不一致性；在行动上也经常与持有的信念不一致；即使在高准确率或良好校准的强模型中，这些问题依然存在。

Conclusion: 研究结果突显了在复杂现实世界环境中预测LLM行为的困难，表明当前模型在动态交互任务中存在显著局限性。

Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.

</details>


### [521] [Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization](https://arxiv.org/abs/2511.13625)
*Kaichi Irie,Shuhei Watanabe,Masaki Onishi*

Main category: cs.LG

TL;DR: 本文提出了一种解耦拟牛顿更新的方法，通过协程在批量调用采集函数的同时保持与顺序多起点优化相同的理论收敛性，显著减少了贝叶斯优化中采集函数优化的计算时间。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中采集函数优化的主要计算瓶颈在于多起点优化，现有方法如BoTorch通过批量处理采集函数来加速，但这种方法在拟牛顿法的逆Hessian近似中存在非对角近似误差问题，导致收敛速度变慢。

Method: 提出使用协程解耦拟牛顿更新，在批量调用采集函数的同时保持独立的拟牛顿更新过程，避免了逆Hessian的非对角近似误差。

Result: 该方法不仅实现了与顺序多起点优化相同的理论收敛性，而且相比之前的方法大幅减少了实际运行时间。

Conclusion: 通过解耦拟牛顿更新和批量采集函数调用的结合，有效解决了贝叶斯优化中采集函数优化的计算效率问题，在保持收敛性能的同时显著提升了计算速度。

Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.

</details>


### [522] [Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures](https://arxiv.org/abs/2511.13640)
*Haohui Wang,Jingyuan Qi,Jianpeng Chen,Jun Wu,Lifu Huang,Lecheng Zheng,Kevin Choi,Balaji Veeramani,Edward Bowen,Alison Hu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 论文分析了混合真实和合成数据对LLM训练的影响，识别了三种缩放行为阶段，提出了针对混合数据的泛化边界理论，并开发了高效的数据估值方法。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽然具有可扩展性和成本效益，但会引入系统性分布差异，特别是在长尾知识方面代表性不足，这对混合数据集的效用评估提出了根本性挑战。

Method: 识别了三种缩放行为阶段，推导了针对真实-合成混合数据的LLM泛化边界理论，并基于理论发现提出了可扩展的高效数据估值方法。

Result: 在图像分类、情感分类、指令跟随和复杂推理四个任务上的综合实验表明，该方法在数据估值方面超越了最先进的基线方法，且计算成本显著降低。

Conclusion: 该研究为理解和评估混合真实-合成数据集的效用提供了理论框架和实用工具，有助于更有效地利用合成数据进行LLM训练。

Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.

</details>


### [523] [Weight-sparse transformers have interpretable circuits](https://arxiv.org/abs/2511.13653)
*Leo Gao,Achyuta Rajaram,Jacob Coxon,Soham V. Govande,Bowen Baker,Dan Mossing*

Main category: cs.LG

TL;DR: 通过训练权重稀疏的语言模型来寻找人类可理解的电路，在保持可解释性的同时权衡模型能力，并展示了该方法可扩展到解释现有密集模型。


<details>
  <summary>Details</summary>
Motivation: 在语言模型中寻找人类可理解的电路是机械可解释性领域的核心目标，当前需要更清晰、更易理解的神经网络连接结构。

Method: 训练权重稀疏模型，约束大多数权重为零，使每个神经元只有少量连接；通过剪枝来隔离特定任务相关的电路部分。

Result: 获得的电路包含对应自然概念的神经元和残差通道，具有少量直接可解释的连接；稀疏化在能力和可解释性之间进行权衡，模型规模扩展可改善这一平衡。

Conclusion: 该方法产生了前所未有的可理解电路，并通过严格验证，但将稀疏模型扩展到千万级参数以上同时保持可解释性仍具挑战性。

Abstract: Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.

</details>


### [524] [Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs](https://arxiv.org/abs/2511.13250)
*Aleksandar Stanković,Dejan Lisica*

Main category: cs.LG

TL;DR: 为ogbn-proteins数据集提供了可复现的边缘感知基线方法，研究了边缘特征聚合和消息传递机制，发现基于sum的边缘到节点特征聚合效果最好，并提出了改进的标准化方法和后处理技术。


<details>
  <summary>Details</summary>
Motivation: 为ogbn-proteins数据集建立可复现的基线方法，研究边缘特征如何影响图神经网络性能，特别是边缘证据聚合和消息传递中的系统选择问题。

Method: 使用PyTorch Geometric实现GraphSAGE，比较不同边缘特征聚合方法(sum/mean/max)，研究LayerNorm、BatchNorm和条件LayerNorm等标准化技术，并应用后处理技术如温度缩放和标签相关性平滑。

Result: sum聚合方法表现最佳；BatchNorm获得最高AUC；条件LayerNorm在AUC相当的情况下获得更好的阈值F1；后处理技术显著改善了micro-F1和校准误差，且对AUC影响很小。

Conclusion: 为ogbn-proteins提供了强基线方法，证明了边缘特征聚合方式、标准化技术和后处理对性能的重要影响，并发布了标准化的实验代码和脚本。

Abstract: We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.

</details>


### [525] [Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers](https://arxiv.org/abs/2511.13685)
*Disha Varshney,Samarth Garg,Sarthak Tyagi,Deeksha Varshney,Nayan Deep,Asif Ekbal*

Main category: cs.LG

TL;DR: 本文提出SSRGNet模型，结合图神经网络和语言模型，利用蛋白质残基图和序列连接来预测蛋白质二级结构，在f1分数上超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用未标记的氨基酸序列，但未能充分利用可获得的蛋白质3D结构数据，而3D结构对蛋白质功能具有决定性影响。

Method: 使用预训练的蛋白质语言模型编码氨基酸序列，采用GCN和R-GCN等消息传递机制捕获蛋白质结构的几何特征，通过堆叠卷积层学习空间图的组合信息。

Result: 在NetSurfP-2.0数据集上的实验表明，SSRGNet模型在3态和8态二级结构预测的f1分数上优于基线方法。

Conclusion: 通过结合图神经网络和语言模型，并利用蛋白质空间结构信息，能够有效提升蛋白质二级结构预测性能。

Abstract: In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.

</details>


### [526] [ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification](https://arxiv.org/abs/2511.13702)
*Luyao Niu,Nuoxian Huang*

Main category: cs.LG

TL;DR: 提出了ST-ProC框架，一种图原型多目标半监督学习方法，用于解决GPS轨迹出行模式识别中的标签稀缺问题，相比现有方法性能提升21.5%。


<details>
  <summary>Details</summary>
Motivation: GPS轨迹出行模式识别面临标注成本高导致的标签稀缺问题，现有半监督学习方法存在确认偏差且忽略数据流形结构。

Method: 结合图正则化、原型锚定和边界感知伪标签策略的核心组件，辅以对比学习和师生一致性损失的支持机制。

Result: 在真实世界稀疏标签场景中显著优于所有基线方法，相比FixMatch等最先进方法性能提升21.5%。

Conclusion: ST-ProC框架通过有效利用数据流形结构和噪声抑制策略，成功解决了出行模式识别中的标签稀缺问题。

Abstract: Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.

</details>


### [527] [Tab-PET: Graph-Based Positional Encodings for Tabular Transformers](https://arxiv.org/abs/2511.13338)
*Yunze Leng,Rohan Ghosh,Mehul Motani*

Main category: cs.LG

TL;DR: 本文发现位置编码(PEs)可以提升表格数据上Transformer模型的泛化性能，通过降低特征有效秩来简化任务。作者提出了Tab-PET框架，使用基于关联和因果的图结构来估计PEs，在50个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 表格数据缺乏结构性线索，传统Transformer模型在表格数据上通常不使用位置编码，因为缺乏先验结构信息。但作者发现结构线索特别是位置编码可以改善表格Transformer的泛化性能。

Method: 提出Tab-PET框架，通过图结构估计位置编码。探索两种图估计范式：基于关联的和基于因果关系的。将图结构信息融入Transformer的嵌入表示中。

Result: 在50个分类和回归数据集上的实验表明，图推导的位置编码显著提升了3T模型的性能。基于关联的图比因果驱动的图获得更稳定和显著的性能提升。

Conclusion: 位置编码在表格Transformer中具有意外的重要作用，可以通过降低特征有效秩来改善泛化能力。基于图结构的位置编码是提升表格Transformer性能的有效方法。

Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.

</details>


### [528] [From Black Box to Insight: Explainable AI for Extreme Event Preparedness](https://arxiv.org/abs/2511.13712)
*Kiana Vu,İsmet Selçuk Özer,Phung Lai,Zheng Wu,Thilanka Munasinghe,Jennifer Wei*

Main category: cs.LG

TL;DR: 本文研究可解释AI在极端事件预测中的作用，以野火预测为例，使用SHAP方法揭示模型决策路径和潜在偏见，提升AI系统的可信度和可操作性。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了极端事件的频率和严重性，需要准确、可解释和可操作的预测。虽然AI模型在预测方面表现出潜力，但其黑盒特性限制了在实际决策中的采用，影响了信任度和可操作性。

Method: 使用野火预测作为案例研究，评估各种AI模型，并采用SHAP方法来揭示关键特征、决策路径和模型行为中的潜在偏见。提供支持性可视化来增强XAI输出的可解释性。

Result: 分析表明XAI不仅澄清了模型推理，还支持领域专家和响应团队的关键决策。可视化增强了特征重要性和时空模式的可解释性。

Conclusion: 研究发现AI系统不仅需要准确性，还需要可解释性、可访问性和可信度，这对于灾害准备、风险缓解和气候韧性规划至关重要。

Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.

</details>


### [529] [Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model](https://arxiv.org/abs/2511.13339)
*Han Meng,Gang Mei,Hong Tian,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: 提出了一种基于表格基础模型的简单而稳健的方法，用于统计准确的岩体不连续面生成预测。该方法利用专门为小数据设计的基座模型的强大样本学习能力，在有限的测量不连续面中有效捕捉潜在的复杂分布模式。


<details>
  <summary>Details</summary>
Motivation: 岩体不连续面对岩体力学行为和稳定性至关重要，但其内部分布大多不可观测，通常通过表面暴露的不连续面使用生成预测方法进行推断。然而，表面暴露观测本质上稀疏，现有方法要么无法捕捉潜在的复杂分布模式，要么在数据稀疏条件下缺乏稳健性。

Method: 利用专门为小数据设计的表格基础模型，通过其强大的样本学习能力来捕捉有限测量不连续面中的复杂分布模式。

Result: 在十个具有不同规模和分布模式的数据集上的比较实验表明，该方法在准确性和稳健性方面优于传统统计模型和深度生成方法。

Conclusion: 这项工作推进了岩体结构的定量表征，支持更安全、更可靠的数据驱动岩土工程设计。

Abstract: Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.

</details>


### [530] [MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction](https://arxiv.org/abs/2511.13419)
*Shaheen Mohammed Saleh Ahmed,Hakan Hakan Guneyli*

Main category: cs.LG

TL;DR: 提出MMWSTM-ADRAN+双流深度学习架构，结合天气状态转换模型和异常驱动注意力机制，用于极端气温事件预测


<details>
  <summary>Details</summary>
Motivation: 准确预测极端气温事件对气候风险管理至关重要，但现有方法在短程预测中仍面临挑战

Method: 双流架构：MMWSTM流使用BiLSTM和可学习马尔可夫状态转移矩阵捕捉天气状态变化；ADRAN流使用BiGRU、多头自注意力和异常放大层增强对低概率信号的敏感性；轻量级注意力融合门自适应确定各流贡献；使用ExtremeWeatherLoss函数和时间序列数据增强优化

Result: 未在摘要中明确说明具体实验结果

Conclusion: 该模型通过耦合天气状态动态建模和异常信号增强机制，为极端气温预测提供了有效解决方案

Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data

</details>


### [531] [Hardware optimization on Android for inference of AI models](https://arxiv.org/abs/2511.13453)
*Iulius Gherasim,Carlos García Sánchez*

Main category: cs.LG

TL;DR: 研究在Android系统上AI模型的最优执行配置，重点关注目标检测(YOLO)和图像分类(ResNet)任务，评估不同量化方案和设备加速器(GPU/NPU)的使用效果。


<details>
  <summary>Details</summary>
Motivation: 移动计算中AI模型的广泛应用需要最小化延迟和高响应性，但面临实时约束和异构硬件架构的挑战，需要找到最优执行策略。

Method: 评估不同模型量化方案，利用设备加速器(GPU和NPU)，在Android系统上对YOLO目标检测和ResNet图像分类模型进行配置优化。

Result: 通过实验确定在最小精度损失和最大推理加速之间达到最佳平衡的组合配置。

Conclusion: 找到了在移动设备上部署AI模型时，量化方案与硬件加速器的最佳组合配置，实现了性能与精度的最优权衡。

Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.

</details>


### [532] [GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction](https://arxiv.org/abs/2511.13469)
*Shiyuan Luo,Chonghao Qiu,Runlong Yu,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 提出了GREAT框架，通过辅助变换增强环境建模数据，解决未监测区域预测问题，在零样本场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 环境建模面临未监测区域预测挑战，由于观测数据有限且地理分布不平衡，加上空间异质性导致模型学习虚假模式。需要保持物理关系不变性和时间一致性。

Method: GREAT框架学习神经网络多层的变换函数，增强原始环境特征和时间影响，通过双层训练过程约束增强数据保留源数据关键模式。

Result: 在美国东部六个生态多样化流域的河流温度预测实验中，GREAT在零样本场景下显著优于现有方法。

Conclusion: 这项工作为环境应用提供了实用解决方案，特别是在全面监测不可行的情况下。

Abstract: Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.

</details>


### [533] [Quantum Machine Learning via Contrastive Training](https://arxiv.org/abs/2511.13497)
*Liudmila A. Zhukas,Vivian Ni Zhang,Qiang Miao,Qingfeng Wang,Marko Cetina,Jungsang Kim,Lawrence Carin,Christopher Monroe*

Main category: cs.LG

TL;DR: 本文提出了一种量子表示的自监督预训练方法，通过在可编程离子阱量子计算机上编码图像为量子态并进行对比学习，显著提高了在有限标注数据下的图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习面临标注数据稀缺的挑战，特别是在模型规模和复杂性增加时。需要开发能够减少对标注数据依赖的方法。

Method: 在可编程离子阱量子计算机上实现自监督预训练，将图像编码为量子态，通过对比学习学习不变性，然后进行微调分类。

Result: 相比随机初始化模型，预训练模型在图像分类任务中获得了更高的平均测试准确率和更低的运行间变异性，特别是在标注数据有限的情况下性能提升显著。

Conclusion: 这项工作为量子表示学习提供了一条标注效率高的路径，对量子原生数据集具有直接相关性，并为处理更大规模经典输入提供了清晰路径。

Abstract: Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.

</details>


### [534] [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](https://arxiv.org/abs/2511.13514)
*Pragatheeswaran Vipulananthan,Kamal Premaratne,Dilip Sarkar,Manohar N. Murthi*

Main category: cs.LG

TL;DR: 提出一种基于量子物理学的白盒方法，通过将时间序列数据的核均值嵌入映射到再生核希尔伯特空间，构建张量网络启发的1D自旋链哈密顿量，并求解薛定谔方程来量化不确定性，提高模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络黑盒模型缺乏可解释性和概率白盒模型性能不足的问题，实现准确的不确定性量化和增强的可解释性。

Method: 将时间序列数据的核均值嵌入映射到再生核希尔伯特空间，构建张量网络启发的1D自旋链哈密顿量，求解薛定谔方程并应用微扰理论来量化不确定性。

Result: 在变化点检测和时间序列聚类任务中，相比最先进的白盒模型表现出有效性，为决策过程中的不确定性提供洞察。

Conclusion: 该量子物理启发的白盒方法成功结合了准确的不确定性量化和增强的可解释性，在时间序列分析任务中优于传统白盒模型。

Abstract: Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.

</details>


### [535] [Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images](https://arxiv.org/abs/2511.13527)
*Ihab Asaad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: 该论文研究高分辨率图像中的块状二元分类问题，发现肿瘤检测中存在虚假相关性（肿瘤块组织区域更大），提出使用GERNE去偏方法提升最差组准确率约7%。


<details>
  <summary>Details</summary>
Motivation: 块状多标签分类能降低标注成本、简化训练，但在肿瘤检测中发现虚假相关性：肿瘤块通常包含更大组织区域，而非肿瘤块多为背景。这种相关性会导致模型预测偏差。

Method: 采用GERNE去偏方法，该方法可适应性地最大化最差组准确率（WGA），用于缓解块组成与标签之间的虚假相关性影响。

Result: 相比经验风险最小化（ERM），GERNE方法在两个不同阈值下将最差组准确率提升了约7%，显著改善了关键少数情况（小组织肿瘤块和大组织非肿瘤块）的性能。

Conclusion: 块状分类问题中需要考虑虚假相关性感知学习，去偏策略能有效提升模型在关键少数情况下的性能，确保更可靠的肿瘤检测。

Abstract: Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.

</details>


### [536] [Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries](https://arxiv.org/abs/2511.13541)
*Yue Hou,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: BaCa是一种无需微调预训练模型的图分布外检测方法，通过双动态字典和边界感知分数校准来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图分布外检测方法通常只优化分布内特征，难以准确表示分布边界，且图数据的潜在多因素结构未被充分探索。

Method: 使用图论估计和混合策略生成边界感知拓扑，构建双动态字典捕获潜在分布内和分布外表示，进行边界感知分数校准。

Result: 在真实数据集上的大量实验表明，BaCa显著优于现有最先进的分布外检测方法。

Conclusion: BaCa通过边界感知校准有效提升了图分布外检测性能，无需微调预训练模型或使用辅助异常数据集。

Abstract: A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.

</details>


### [537] [RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise](https://arxiv.org/abs/2511.13561)
*Shihao Dong,Yue Liu,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.LG

TL;DR: 提出RAC-DMVC框架，通过可靠性图引导多源噪声环境下的鲁棒表示学习，解决多视图聚类中的缺失噪声和观测噪声问题。


<details>
  <summary>Details</summary>
Motivation: 增强多视图聚类在现实场景中的适用性，解决更具挑战性的多源噪声问题，包括缺失噪声和观测噪声。

Method: 构建可靠性图指导鲁棒表示学习：1) 交叉视图重建处理观测噪声；2) 可靠性感知噪声对比学习缓解噪声表示带来的正负对选择偏差；3) 双重注意力插补处理缺失噪声；4) 自监督聚类蒸馏模块优化表示。

Result: 在五个基准数据集上的实验表明，RAC-DMVC在多个评估指标上优于SOTA方法，并在不同噪声比例下保持优异性能。

Conclusion: RAC-DMVC框架能有效处理多源噪声环境下的多视图聚类问题，具有优越的鲁棒性和性能表现。

Abstract: Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.

</details>


### [538] [Towards Multimodal Representation Learning in Paediatric Kidney Disease](https://arxiv.org/abs/2511.13637)
*Ana Durica,John Booth,Ivana Drobnjak*

Main category: cs.LG

TL;DR: 使用电子健康记录和人口统计学信息，通过循环神经网络预测儿童未来30天内是否会出现异常血清肌酐值。


<details>
  <summary>Details</summary>
Motivation: 儿科肾脏疾病表现和进展差异大，需要持续监测肾功能。

Method: 整合纵向实验室序列和人口统计学信息的时间建模方法，使用循环神经网络训练模型。

Result: 初步证明简单的时间表示可以捕捉常规儿科数据中的有用模式。

Conclusion: 为未来使用额外临床信号和更详细肾脏结果的多模态扩展奠定了基础。

Abstract: Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.

</details>


### [539] [FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs](https://arxiv.org/abs/2511.13645)
*Aleksandar Stanković*

Main category: cs.LG

TL;DR: FuseSampleAgg是一个CUDA算子，将GraphSAGE的邻居采样和均值聚合融合为单次操作，显著提升性能并减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统GraphSAGE实现需要多次内核启动和中间结果存储，导致内存流量大、性能开销高。

Method: 通过融合邻居采样和均值聚合为单次操作，消除块物化和额外内核启动，通过索引重放保持GraphSAGE均值语义。

Result: 在多个基准测试中实现最高51倍速度提升和100倍内存减少，算子具有确定性且与PyTorch优化器兼容。

Conclusion: FuseSampleAgg通过操作融合有效解决了GraphSAGE训练中的性能瓶颈问题。

Abstract: We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.

</details>


### [540] [Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning](https://arxiv.org/abs/2511.13654)
*Pascal Zimmer,Ghassan Karame*

Main category: cs.LG

TL;DR: 本文首次系统分析了优化超参数（学习率、权重衰减、动量、批大小）对迁移攻击和查询攻击鲁棒性的影响，发现在不同攻击类型下学习率调整方向相反，并探索了同时增强两种攻击鲁棒性的超参数设计空间。


<details>
  <summary>Details</summary>
Motivation: 研究优化超参数如何影响对抗攻击鲁棒性，填补了该领域的研究空白，为实际部署中模型安全提供指导。

Method: 通过理论分析和实验验证，在集中训练、集成学习和分布式训练等多种实际部署场景下，系统测试不同优化超参数对迁移攻击和查询攻击鲁棒性的影响。

Result: 发现学习率调整对两种攻击类型产生相反效果：降低学习率可提升迁移攻击鲁棒性达64%，而提高学习率可增强查询攻击鲁棒性达28%。分布式模型通过超参数调优能最有效地同时缓解两种攻击。

Conclusion: 优化超参数是提升模型对抗攻击鲁棒性的关键因素，分布式训练模型在超参数调优下能实现最佳的双重攻击防护效果。

Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.

</details>


### [541] [Cross-Learning from Scarce Data via Multi-Task Constrained Optimization](https://arxiv.org/abs/2511.13680)
*Leopoldo Agorio,Juan Cerviño,Miguel Calvo-Fullana,Alejandro Ribeiro,Juan Andrés Bazerque*

Main category: cs.LG

TL;DR: 提出多任务交叉学习框架，通过联合估计多个相关任务的确定性参数来解决数据稀缺问题，实现从数据丰富的任务向数据稀缺任务的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 当监督数据有限时，学习模型难以泛化到未见过的案例。需要克服数据稀缺问题，特别是在参数推断对有限数据敏感的关键场景中。

Method: 将联合估计建模为约束优化问题，通过约束控制不同模型参数之间的相似性，允许参数跨任务差异但结合多个数据源的信息。

Result: 在理论框架中提供高斯数据的理论保证，并在图像分类和传染病传播等真实数据应用中展示了交叉学习方法的有效性。

Conclusion: 交叉学习框架能够从数据丰富的任务向数据稀缺任务进行知识迁移，产生更准确可靠的参数估计，为有限数据下的参数推断提供了解决方案。

Abstract: A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \emph{cross-learning} framework to overcome data scarcity by jointly estimating \emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.

</details>


### [542] [Learning stochasticity: a nonparametric framework for intrinsic noise estimation](https://arxiv.org/abs/2511.13701)
*Gianluigi Pillonetto,Alberto Giaretta,Mauro Bisiacco*

Main category: cs.LG

TL;DR: Trine是一个非参数、基于核的三阶段回归框架，用于从时间序列数据中推断状态依赖的内在噪声，特别适用于生物和生态系统中的复杂动态系统建模。


<details>
  <summary>Details</summary>
Motivation: 非线性相互作用和随机效应的不完全知识使得自下而上的建模方法常常无效，特别是在基因调控网络和信号通路等复杂系统中，内在噪声的建模对于理解系统动态行为至关重要。

Method: Trine采用三阶段算法，结合可解析求解的子问题和结构化核架构，能够同时捕捉突发的噪声驱动波动和平滑的状态依赖方差变化。

Result: 在生物和生态系统的验证中，Trine能够在不依赖预定义参数假设的情况下揭示隐藏动态，其性能可与理想观测者相媲美。

Conclusion: Trine框架为理解内在噪声如何影响复杂系统行为开辟了新途径，特别适用于缺乏强先验知识的参数模型难以处理的情况。

Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.

</details>


### [543] [Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering](https://arxiv.org/abs/2511.13705)
*Alaa Mezghiche*

Main category: cs.LG

TL;DR: 使用自编码器和聚类分析在KIRC癌症中发现了一个罕见但稳定的分子亚型，而泛癌分析主要受组织来源影响。


<details>
  <summary>Details</summary>
Motivation: 探索高维RNA-seq数据中的罕见但可重复的基因组亚型，超越标准标签的分类。

Method: 结合自编码器表示与聚类和稳定性分析，在KIRC数据集中选择高变基因，训练自编码器，运行k-means聚类，并通过稳定性标准识别罕见亚型。

Result: 在KIRC中发现了一个罕见亚型C0（占6.85%患者），具有高度稳定性（Jaccard = 0.787），而泛癌分析主要反映组织来源差异。

Conclusion: 泛癌聚类受组织来源主导，而基于稳定性的癌症内分析方法能够揭示罕见、可重复的分子亚型。

Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [544] [Neural Network-Augmented Iterative Learning Control for Friction Compensation of Motion Control Systems with Varying Disturbances](https://arxiv.org/abs/2511.11850)
*Ali Mashhadireza,Ali Sadighi*

Main category: eess.SY

TL;DR: 提出一种结合迭代学习控制和简单横向神经网络的鲁棒控制策略，用于提升线性洛伦兹力执行器在摩擦和模型不确定性下的轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 解决线性洛伦兹力执行器在摩擦和模型不确定性影响下的轨迹跟踪精度问题，传统方法难以应对时变摩擦和参考指令变化带来的挑战。

Method: 将迭代学习控制(ILC)与简单横向神经网络相结合，ILC补偿非线性摩擦效应，神经网络估计不同参考指令下的非线性ILC努力，动态调整ILC努力以适应时变摩擦。

Result: 实验结果表明该方法能有效实现精确跟踪，在多个具有不同参考轨迹的任务中表现优异，相比使用复杂神经网络的方法简化了在线训练和实现。

Conclusion: 该方法通过简化神经网络结构，使ILC与神经网络结合的控制策略更适用于实时应用，能够适应时变摩擦、减少参考变化时的误差并加速收敛。

Abstract: This paper proposes a robust control strategy that integrates Iterative Learning Control (ILC) with a simple lateral neural network to enhance the trajectory tracking performance of a linear Lorentz force actuator under friction and model uncertainties. The ILC compensates for nonlinear friction effects, while the neural network estimates the nonlinear ILC effort for varying reference commands. By dynamically adjusting the ILC effort, the method adapts to time-varying friction, reduces errors at reference changes, and accelerates convergence. Compared to previous approaches using complex neural networks, this method simplifies online training and implementation, making it practical for real-time applications. Experimental results confirm its effectiveness in achieving precise tracking across multiple tasks with different reference trajectories.

</details>


### [545] [Emulation-based Neuromorphic Control for the Stabilization of LTI Systems](https://arxiv.org/abs/2511.11875)
*Elena Petri,Koen J. A. Scheres,Erik Steur,W. P. M. H.,Heemels*

Main category: eess.SY

TL;DR: 提出了一种系统化方法，使用基于脉冲神经网络的控制器来稳定线性时不变系统，通过两步骤设计确保闭环系统的可验证稳定性。


<details>
  <summary>Details</summary>
Motivation: 神经形态技术相比传统数字时钟技术具有低延迟、低能耗和自适应控制的优势，但目前缺乏系统化的脉冲控制器设计和分析方法。

Method: 采用两步骤仿真设计：1) 建立神经元参数条件，确保神经元对生成脉冲信号能精确模拟连续时间信号；2) 提出基于特殊脉冲信号度量的积分脉冲输入到状态稳定性概念。

Result: 证明了渐近稳定的LTI系统具有iSISS特性，结合两步骤设计可以建立闭环系统的可验证实际稳定性。

Conclusion: 该方法为基于脉冲神经网络的控制器提供了系统化设计框架，并通过数值案例验证了有效性。

Abstract: Brain-inspired neuromorphic technologies can offer important advantages over classical digital clock-based technologies in various domains, including systems and control engineering. Indeed, neuromorphic engineering could provide low-latency, low-energy and adaptive control systems in the form of spiking neural networks (SNNs) exploiting spike-based control and communication. However, systematic methods for designing and analyzing neuron-inspired spiking controllers are currently lacking. This paper presents a new systematic approach for stabilizing linear time-invariant (LTI) systems using SNN-based controllers, designed as a network of integrate-and-fire neurons, whose input is the measured output from the plant and generating spiking control signals. The new approach consists of a two-step emulation-based design procedure. In the first step, we establish conditions on the neuron parameters to ensure that the spiking signal generated by a pair of neurons emulates any continuous-time signal input to the neurons with arbitrary accuracy in terms of a special metric for spiky signals. In the second step, we propose a novel stability notion, called integral spiking-input-to-state stability (iSISS) building on this special metric. We prove that an asymptotically stable LTI system has this iSISS property. By combining these steps, a certifiable practical stability property of the closed-loop system can be established. Generalizations are discussed and the effectiveness of the approach is illustrated in a numerical case study.

</details>


### [546] [Sampling-Aware Control Barrier Functions for Safety-Critical and Finite-Time Constrained Control](https://arxiv.org/abs/2511.11897)
*Shuo Liu,Wei Xiao,Calin A. Belta*

Main category: eess.SY

TL;DR: 提出采样感知控制屏障函数(SACBF)框架，解决传统高阶CBF在零阶保持控制下因采样效应导致的安全性问题，同时处理有限时间到达保持要求和多约束冲突。


<details>
  <summary>Details</summary>
Motivation: 现有CBF框架在连续时间下能保证安全，但在采样数据实现时由于采样间效应可能变得不安全，且无法显式处理有限时间到达保持要求或多约束冲突导致的可行性问题。

Method: 通过估计和整合采样时刻间屏障演化的泰勒上界，提出SACBF框架；为改进可行性，引入带松弛变量的r-SACBF变体处理通过时变CBF实现的多约束。

Result: 仿真研究表明，在传统HOCBF方法失效的场景下，SACBFs能够实现安全和可行的性能。

Conclusion: SACBFs为采样控制系统提供了统一的框架，能够保证连续时间前向不变性和有限时间到达保持集的安全性，同时通过松弛变体提高多约束处理的可行性。

Abstract: In safety-critical control systems, ensuring both safety and feasibility under sampled-data implementations is crucial for practical deployment. Existing Control Barrier Function (CBF) frameworks, such as High-Order CBFs (HOCBFs), effectively guarantee safety in continuous time but may become unsafe when executed under zero-order-hold (ZOH) controllers due to inter-sampling effects. Moreover, they do not explicitly handle finite-time reach-and-remain requirements or multiple simultaneous constraints, which often lead to conflicts between safety and reach-and-remain objectives, resulting in feasibility issues during control synthesis. This paper introduces Sampling-Aware Control Barrier Functions (SACBFs), a unified framework that accounts for sampling effects and high relative-degree constraints by estimating and incorporating Taylor-based upper bounds on barrier evolution between sampling instants. The proposed method guarantees continuous-time forward invariance of safety and finite-time reach-and-remain sets under ZOH control. To further improve feasibility, a relaxed variant (r-SACBF) introduces slack variables for handling multiple constraints realized through time-varying CBFs. Simulation studies on a unicycle robot demonstrate that SACBFs achieve safe and feasible performance in scenarios where traditional HOCBF methods fail.

</details>


### [547] [On The Detection of Minimum Forecast Horizon For Real-Time Scheduling of Energy Storage Systems in Smart Grid](https://arxiv.org/abs/2511.12029)
*Nicholas Tetteh Ofoe,Weilun Wang,Lei Wu*

Main category: eess.SY

TL;DR: 本文提出了基于轨迹对齐的最小预测时域定义和算法，用于识别储能系统实时控制中能够完全模拟全局最优控制轨迹所需的最短预测时域。


<details>
  <summary>Details</summary>
Motivation: 随着储能系统在电网中的集成度提高，需要在不确定和波动的电价下制定有效的实时控制策略。现有方法仅提供充分条件，可能忽略实际控制动作的不一致性。

Method: 引入基于轨迹对齐的最小预测时域定义，提出算法识别滚动时域控制决策与全时域全局优化完全匹配的最小规划时域。

Result: 使用丹麦Nord Pool日前市场的真实价格数据和实际ESS模型，证明60小时的预测时域能够完全模拟全局控制序列和经济结果。在某些参数配置下，不存在确保完全收敛的预测时域。

Conclusion: 研究结果为储能调度中的最小预测时域检测提供了具有操作意义的框架，并为这一重要规划指标的分析描述奠定了基础。

Abstract: The increasing integration of energy storage systems (ESSs) into power grids has necessitated effective real-time control strategies under uncertain and volatile electricity prices. An important problem of model predictive control of ESSs is identifying the minimum forecast horizon needed to exactly simulate the globally optimal control trajectory. Existing methods in the literature provide only sufficient conditions and might ignore real-world inconsistencies in control actions. In this paper, we introduce a trajectory-alignment-based definition of the minimum forecast horizon and propose an algorithm that identifies the minimum planning horizon for which all rolling-horizon control decisions match those of the full-horizon global optimization. Using real price data from the bidding zone DK1 in Denmark of the Nord Pool day-ahead market and a realistic ESS model, we illustrate that $60$ hours of forecast horizon allows us to exactly simulate the global control sequence and economic outcomes. In addition, we illustrate that under other parameter configurations, no forecast horizon ensures full convergence, demonstrating the sensitivity of the existence of a forecast horizon to various parameters. Our findings provide an operationally significant framework for minimum forecast horizon detection in storage scheduling and pave the way for the analytical description of this important planning measure.

</details>


### [548] [Real-Time Physics-Aware Battery Health Monitoring from Partial Charging Profiles via Physics-Informed Neural Networks](https://arxiv.org/abs/2511.12053)
*Xubo Gu,Xun Huan,Yao Ren,Wenqing Zhou,Weiran Jiang,Ziyou Song*

Main category: eess.SY

TL;DR: 开发了一种参数化物理信息神经网络(P-PINNSPM)，用于快速准确监测电池健康状态，在30秒内识别内部参数，比有限体积法快47倍，同时将SOH估计精度提高至少60.61%。


<details>
  <summary>Details</summary>
Motivation: 解决电池健康监测中评估速度与诊断深度之间的权衡问题，高效获取详细内部电池信息以理解各种退化机制。

Method: 基于关键老化相关参数空间开发参数化物理信息神经网络，应用于单粒子模型，预测内部电池变量并识别内部参数。

Result: 模型在30秒内准确识别内部参数，比有限体积法快47倍；参数融入使SOH估计精度提高至少60.61%；能够外推到未见SOH水平，并在不同充电曲线和操作条件下保持稳健估计。

Conclusion: 物理信息机器学习在推进实时、数据高效且物理感知的电池管理系统方面具有强大潜力。

Abstract: Monitoring battery health is essential for ensuring safe and efficient operation. However, there is an inherent trade-off between assessment speed and diagnostic depth-specifically, between rapid overall health estimation and precise identification of internal degradation states. Capturing detailed internal battery information efficiently remains a major challenge, yet such insights are key to understanding the various degradation mechanisms. To address this, we develop a parameterized physics-informed neural network (P-PINNSPM) over the key aging-related parameter space for a single particle model. The model can accurately predict internal battery variables across the parameter space and identifies internal parameters in about 30 seconds-achieving a 47x speedup over the finite volume method-while maintaining high accuracy. These parameters improve the battery state-of-health (SOH) estimation accuracy by at least 60.61%, compared to models without parameter incorporation. Moreover, they enable extrapolation to unseen SOH levels and support robust estimation across diverse charging profiles and operating conditions. Our results demonstrate the strong potential of physics-informed machine learning to advance real-time, data-efficient, and physics-aware battery management systems.

</details>


### [549] [Tight displacement-based formation control under bounded disturbances. A set-theoretic perspective](https://arxiv.org/abs/2511.12163)
*Vlad-Matei Angheluţă,Bogdan Gheorghe,Daniel Ioan,Ionela Prodan,Florin Stoican*

Main category: eess.SY

TL;DR: 提出基于集合论概念的确定性方法，用于位移编队控制中在有界扰动下的控制器合成，通过集合不变性原理保证预设性能边界


<details>
  <summary>Details</summary>
Motivation: 现有文献多采用随机框架处理测量噪声等有界扰动问题，本文旨在提供一种确定性的替代方法

Method: 利用集合不变性原理，将最终有界性理论应用于位移编队的特定动力学，通过集合论框架优化控制律参数选择

Result: 所合成的控制器能够在多障碍物环境中有效维持紧密编队，证明了方法的有效性

Conclusion: 基于集合论的确定性方法为位移编队控制提供了严格的分析框架，能够保证系统在持续扰动下的预设性能边界

Abstract: This paper investigates the synthesis of controllers for displacement-based formation control in the presence of bounded disturbances, specifically focusing on uncertainties originating from measurement noise. While the literature frequently addresses such problems using stochastic frameworks, this work proposes a deterministic methodology grounded in set-theoretic concepts. By leveraging the principles of set invariance, we adapt the theory of ultimate boundedness to the specific dynamics of displacement-based formations. This approach provides a rigorous method for analyzing the system's behavior under persistent disturbances. Furthermore, this set-theoretic framework allows for the optimized selection of the proposed control law parameters to guarantee pre-specified performance bounds. The efficacy of the synthesized controller is demonstrated in the challenging application of maintaining tight formations in a multi-obstacles environment.

</details>


### [550] [AI-Enhanced IoT Systems for Predictive Maintenance and Affordability Optimization in Smart Microgrids: A Digital Twin Approach](https://arxiv.org/abs/2511.12175)
*Koushik Ahmed Kushal,Florimond Gueniat*

Main category: eess.SY

TL;DR: 提出基于数字孪生的AI增强物联网框架，用于智能微电网的预测性维护和成本优化，通过实时数据集成和机器学习实现可靠性提升和能源效率改善。


<details>
  <summary>Details</summary>
Motivation: 解决分布式微电网环境中组件故障预测、维护成本控制和运行可靠性问题，为下一代智能能源系统提供可扩展解决方案。

Method: 采用数字孪生建模方法，集成实时传感器数据、基于机器学习的故障预测和成本感知操作分析，实现物理微电网组件与虚拟数字孪生的同步。

Result: 实验评估显示相比基准方法，预测准确性提高，运行停机时间减少，并实现可量化的成本节约。

Conclusion: 数字孪生驱动的物联网架构具有作为下一代智能经济能源系统的可扩展解决方案的潜力。

Abstract: This study presents an AI enhanced IoT framework for predictive maintenance and affordability optimization in smart microgrids using a Digital Twin modeling approach. The proposed system integrates real time sensor data, machine learning based fault prediction, and cost aware operational analytics to improve reliability and energy efficiency in distributed microgrid environments. By synchronizing physical microgrid components with a virtual Digital Twin, the framework enables early detection of component degradation, dynamic load management, and optimized maintenance scheduling. Experimental evaluations demonstrate improved predictive accuracy, reduced operational downtime, and measurable cost savings compared to baseline microgrid management methods. The findings highlight the potential of Digital Twin driven IoT architectures as a scalable solution for next generation intelligent and affordable energy systems.

</details>


### [551] [DataOps-driven CI/CD for analytics repositories](https://arxiv.org/abs/2511.12277)
*Dmytro Valiaiev*

Main category: eess.SY

TL;DR: 提出了一个基于DataOps的验证框架，通过十二个可测试的控制项和五阶段CI/CD管道来标准化SQL数据分析流程，提高数据质量和治理能力。


<details>
  <summary>Details</summary>
Motivation: SQL数据处理缺乏传统软件开发的严谨性，导致孤岛化、逻辑重复和风险增加，阻碍数据治理和验证。DataOps方法旨在解决这些问题，但缺乏标准化实施框架。

Method: 通过多源文献综述开发DataOps控制记分卡，包含12个可测试控制项，并将其映射到五阶段CI/CD管道框架：Lint、Optimize、Parse、Validate、Observe，每个阶段包含自动化检查。

Result: 创建了模块化、可扩展的验证框架，通过需求可追溯性矩阵确保高层控制与具体管道检查的对应关系，提供结构化机制来增强数据质量、治理和协作。

Conclusion: 该框架为团队提供了透明可控的分析开发扩展能力，通过标准化验证流程解决了DataOps实施中的标准化缺失问题。

Abstract: The proliferation of SQL for data processing has often occurred without the rigor of traditional software development, leading to siloed efforts, logic replication, and increased risk. This ad-hoc approach hampers data governance and makes validation nearly impossible. Organizations are adopting DataOps, a methodology combining Agile, Lean, and DevOps principles to address these challenges to treat analytics pipelines as production systems. However, a standardized framework for implementing DataOps is lacking. This perspective proposes a qualitative design for a DataOps-aligned validation framework. It introduces a DataOps Controls Scorecard, derived from a multivocal literature review, which distills key concepts into twelve testable controls. These controls are then mapped to a modular, extensible CI/CD pipeline framework designed to govern a single source of truth (SOT) SQL repository. The framework consists of five stages: Lint, Optimize, Parse, Validate, and Observe, each containing specific, automated checks. A Requirements Traceability Matrix (RTM) demonstrates how each high-level control is enforced by concrete pipeline checks, ensuring qualitative completeness. This approach provides a structured mechanism for enhancing data quality, governance, and collaboration, allowing teams to scale analytics development with transparency and control.

</details>


### [552] [Target Defense against Sequentially Arriving Intruders: Algorithm for Agents with Dubins Dynamics](https://arxiv.org/abs/2511.12329)
*Arman Pourghorban,Dipankar Maity*

Main category: eess.SY

TL;DR: 研究非完整动力学下的目标防御问题，分析防御者如何捕获连续入侵者序列，通过Dubins路径和守护弧概念量化捕获率。


<details>
  <summary>Details</summary>
Motivation: 解决单防御者面对连续非完整动力学入侵者的目标防御问题，分析防御者在有限和无限入侵序列中的捕获能力。

Method: 将入侵者-防御者交战分为部分信息和完全信息两个阶段，使用Dubins路径和守护弧概念分析可捕获性，通过蒙特卡洛实验验证理论结果。

Result: 量化了有限和无限入侵序列中的捕获百分比，理论结果通过数值实验得到验证。

Conclusion: 建立了非完整动力学下连续目标防御问题的分析框架，为防御策略设计提供了理论基础。

Abstract: We consider a variant of the target defense problem where a single defender is tasked to capture a sequence of incoming intruders. Both the defender and the intruders have non-holonomic dynamics. The intruders' objective is to breach the target perimeter without being captured by the defender, while the defender's goal is to capture as many intruders as possible. After one intruder breaches or is captured, the next appears randomly on a fixed circle surrounding the target. Therefore, the defender's final position in one game becomes its starting position for the next. We divide an intruder-defender engagement into two phases, partial information and full information, depending on the information available to the players. We address the capturability of an intruder by the defender using the notions of Dubins path and guarding arc. We quantify the percentage of capture for both finite and infinite sequences of incoming intruders. Finally, the theoretical results are verified through numerical examples using Monte-Carlo-type random trials of experiments.

</details>


### [553] [DER Day-Ahead Offering: A Neural Network Column-and-Constraint Generation Approach](https://arxiv.org/abs/2511.12384)
*Weiqi Meng,Hongyi Li,Bai Cui*

Main category: eess.SY

TL;DR: 提出了一种两阶段鲁棒自适应随机优化模型，用于解决分布式能源聚合商在日前能源市场中的报价策略问题，结合神经网络加速的列与约束生成方法，显著提高了求解效率。


<details>
  <summary>Details</summary>
Motivation: 在日前能源市场中，分布式能源聚合商需要在不确定性实现前提交价格-数量对的报价策略，需要同时处理日前价格和分布式能源发电的不确定性。

Method: 采用两阶段鲁棒自适应随机优化模型：第一阶段确定价格-数量对，第二阶段在分布式能源不确定性实现后做出运行承诺决策。使用随机规划处理日前价格不确定性，鲁棒优化处理分布式能源发电不确定性，并开发神经网络加速的列与约束生成方法。

Result: 在1028节点合成配电网络上的数值研究表明，该方法能获得高质量解，比Gurobi快100倍，比经典列与约束生成方法快33倍。

Conclusion: 所提出的方法能有效解决分布式能源聚合商的日前报价问题，在保证解质量的同时显著提高了计算效率。

Abstract: In the day-ahead energy market, the offering strategy of distributed energy resource (DER) aggregators must be submitted before the uncertainty realization in the form of price-quantity pairs. This work addresses the day-ahead offering problem through a two-stage robust adaptive stochastic optimization model, wherein the first-stage price-quantity pairs and second-stage operational commitment decisions are made before and after DER uncertainty is realized, respectively. Uncertainty in day-ahead price is addressed using a stochastic programming, while uncertainty of DER generation is handled through robust optimization. To address the max-min structure of the second-stage problem, a neural network-accelerated column-and-constraint generation method is developed. A dedicated neural network is trained to approximate the value function, while optimality is maintained by the design of the network architecture. Numerical studies indicate that the proposed method yields high-quality solutions and is up to 100 times faster than Gurobi and 33 times faster than classical column-and-constraint generation on the same 1028-node synthetic distribution network.

</details>


### [554] [Online Adaptive Probabilistic Safety Certificate with Language Guidance](https://arxiv.org/abs/2511.12431)
*Zhuoyuan Wang,Xiyu Deng,Hikaru Hoshino,Yorie Nakahira*

Main category: eess.SY

TL;DR: 提出了一种语言引导的自适应概率安全证书框架，用于在环境不确定性下保证随机系统的长期安全，同时适应不同的人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往在长期安全保证和实时控制之间进行权衡，无法适应人类偏好或风险容忍度的变化。需要一种能同时保证长期安全、适应环境变化并考虑人类偏好的方法。

Method: 将自然语言输入和贝叶斯环境估计器集成到自适应安全证书中，利用概率不变性（概率空间中的前向不变性推广）获得具有长期安全保证的近视安全条件。

Result: 通过自动驾驶车道保持的数值模拟验证，在不确定和极端道路条件下，展示了改进的安全性能权衡、对环境变化的适应性以及对不同用户偏好的个性化。

Conclusion: 该框架成功解决了在环境不确定性下保证随机系统长期安全并适应人类偏好的挑战，为自主系统在极端环境中的安全运行提供了有效解决方案。

Abstract: Achieving long-term safety in uncertain or extreme environments while accounting for human preferences remains a fundamental challenge for autonomous systems. Existing methods often trade off long-term guarantees for fast real-time control and cannot adapt to variability in human preferences or risk tolerance. To address these limitations, we propose a language-guided adaptive probabilistic safety certificate (PSC) framework that guarantees long-term safety for stochastic systems under environmental uncertainty while accommodating diverse human preferences. The proposed framework integrates natural-language inputs from users and Bayesian estimators of the environment into adaptive safety certificates that explicitly account for user preferences, system dynamics, and quantified uncertainties. Our key technical innovation leverages probabilistic invariance--a generalization of forward invariance to a probability space--to obtain myopic safety conditions with long-term safety guarantees that integrate language guidance, model information, and quantified uncertainty. We validate the framework through numerical simulations of autonomous lane-keeping with human-in-the-loop guidance under uncertain and extreme road conditions, demonstrating enhanced safety-performance trade-offs, adaptability to changing environments, and personalization to different user preferences.

</details>


### [555] [One Request, Multiple Experts: LLM Orchestrates Domain Specific Models via Adaptive Task Routing](https://arxiv.org/abs/2511.12484)
*Xu Yang,Chenhui Lin,Haotian Liu,Qi Wang,Yue Yang,Wenchuan Wu*

Main category: eess.SY

TL;DR: 提出了ADN-Agent架构，利用大语言模型协调多个领域特定模型，解决主动配电网中异构模型集成和协调的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源的大规模集成和新型市场主体的广泛参与，主动配电网运行演变为复杂的多场景、多目标问题，需要智能方法来统一协调异构的领域特定模型。

Method: 设计ADN-Agent架构，利用通用大语言模型进行自适应意图识别、任务分解和模型调用，并设计统一通信机制为异构模型提供灵活接口，对语言密集型子任务采用自动化训练管道微调小语言模型。

Result: 综合比较和消融实验验证了所提方法的有效性，ADN-Agent架构优于现有的大语言模型应用范式。

Conclusion: ADN-Agent架构能够有效统一和协调主动配电网中的异构领域特定模型，提升系统整体问题解决能力。

Abstract: With the integration of massive distributed energy resources and the widespread participation of novel market entities, the operation of active distribution networks (ADNs) is progressively evolving into a complex multi-scenario, multi-objective problem. Although expert engineers have developed numerous domain specific models (DSMs) to address distinct technical problems, mastering, integrating, and orchestrating these heterogeneous DSMs still entail considerable overhead for ADN operators. Therefore, an intelligent approach is urgently required to unify these DSMs and enable efficient coordination. To address this challenge, this paper proposes the ADN-Agent architecture, which leverages a general large language model (LLM) to coordinate multiple DSMs, enabling adaptive intent recognition, task decomposition, and DSM invocation. Within the ADN-Agent, we design a novel communication mechanism that provides a unified and flexible interface for diverse heterogeneous DSMs. Finally, for some language-intensive subtasks, we propose an automated training pipeline for fine-tuning small language models, thereby effectively enhancing the overall problem-solving capability of the system. Comprehensive comparisons and ablation experiments validate the efficacy of the proposed method and demonstrate that the ADN-Agent architecture outperforms existing LLM application paradigms.

</details>


### [556] [Density-Driven Multi-Agent Coordination for Efficient Farm Coverage and Management in Smart Agriculture](https://arxiv.org/abs/2511.12492)
*Sungjun Seo,Kooktae Lee*

Main category: eess.SY

TL;DR: 提出D2OC框架，将最优传输理论与多无人机覆盖控制相结合，实现基于虫害强度的非均匀、优先级感知的农业喷洒，减少化学品使用并提高效率。


<details>
  <summary>Details</summary>
Motivation: 现代农场规模扩大需要高效的多智能体覆盖策略，传统方法导致化学品过度使用和资源浪费，现有无人机方法受限于电池寿命、有效载荷和可扩展性，且未考虑虫害严重程度。

Method: 集成最优传输理论与多无人机覆盖控制，将无人机建模为线性时变系统以捕捉喷洒任务中的质量和惯性变化，使用拉格朗日力学推导D2OC控制律。

Result: 仿真结果表明，该方法在覆盖效率、化学品减少和操作可持续性方面优于均匀喷洒和谱多尺度覆盖方法。

Conclusion: D2OC框架为智能农业提供了可扩展的解决方案，实现了高效协调、平衡工作负载分布和改善任务持续时间。

Abstract: The growing scale of modern farms has increased the need for efficient and adaptive multi-agent coverage strategies for pest, weed, and disease management. Traditional methods such as manual inspection and blanket pesticide spraying often lead to excessive chemical use, resource waste, and environmental impact. While unmanned aerial vehicles (UAVs) offer a promising platform for precision agriculture through targeted spraying and improved operational efficiency, existing UAV-based approaches remain limited by battery life, payload capacity, and scalability, especially in large fields where single-UAV or uniformly distributed spraying is insufficient. Although multi-UAV coordination has been explored, many current frameworks still assume uniform spraying and do not account for infestation severity, UAV dynamics, non-uniform resource allocation, or energy-efficient coordination.
  To address these limitations, this paper proposes a Density-Driven Optimal Control (D2OC) framework that integrates Optimal Transport (OT) theory with multi-UAV coverage control for large-scale agricultural spraying. The method supports non-uniform, priority-aware resource allocation based on infestation intensity, reducing unnecessary chemical application. UAVs are modeled as a linear time-varying (LTV) system to capture variations in mass and inertia during spraying missions. The D2OC control law, derived using Lagrangian mechanics, enables efficient coordination, balanced workload distribution, and improved mission duration. Simulation results demonstrate that the proposed approach outperforms uniform spraying and Spectral Multiscale Coverage (SMC) in coverage efficiency, chemical reduction, and operational sustainability, providing a scalable solution for smart agriculture.

</details>


### [557] [On hyperexponential stabilization of a chain of integrators in continuous and discrete time subject to unmatched perturbations](https://arxiv.org/abs/2511.12567)
*Moussa Labbadi,Denis Efimov*

Main category: eess.SY

TL;DR: 提出了一种用于具有不匹配扰动的积分器链的递归时变状态反馈控制方法，在连续和离散时间下均能实现超指数收敛


<details>
  <summary>Details</summary>
Motivation: 针对具有不匹配扰动的积分器链系统，开发能够实现超指数收敛的控制策略，同时在连续和离散时间下保持稳定性

Method: 采用递归时变状态反馈控制，在连续时间下通过饱和增长的控制增益实现ISS特性，在离散时间下使用隐式欧拉离散化保持超指数收敛

Result: 连续时间下第一个状态变量实现超指数收敛，第二个状态保持有界，其他状态通过ISS特性保证稳定性；离散时间下成功保持了超指数收敛特性

Conclusion: 所提出的递归时变状态反馈控制方法在连续和离散时间下均能有效处理具有不匹配扰动的积分器链系统，实现了超指数收敛和稳定性

Abstract: A recursive time-varying state feedback is presented for a chain of integrators with unmatched perturbations in continuous and discrete time. In continuous time, it is shown that hyperexponential convergence is achieved for the first state variable \(x_1\), while the second state \(x_2\) remains bounded. For the other states, we establish ISS {\cb property} by saturating the growing {\cb control} gain. In discrete time, we use implicit Euler discretization to {\cb preserve} hyperexponential convergence. The main results are demonstrated through several examples of the proposed control laws, illustrating the conditions established for both continuous and discrete-time systems.

</details>


### [558] [On two-degrees-of-freedom agreement protocols](https://arxiv.org/abs/2511.12632)
*Gal Barkai,Leonid Mirkin,Daniel Zelazo*

Main category: eess.SY

TL;DR: 提出了一种分布式双自由度架构，用于驱动自主异构智能体达成一致。该架构分离了本地反馈和网络滤波，支持独立设计网络滤波器来抑制噪声，并允许控制器异构性来抑制本地扰动。


<details>
  <summary>Details</summary>
Motivation: 解决标准扩散耦合方法无法抑制激发不稳定一致极点的本地扰动的问题，为异构智能体提供更灵活的一致性控制方案。

Method: 采用分布式双自由度架构，将本地反馈与网络滤波分离，支持独立设计网络滤波器，并允许控制器异构性。

Result: 该框架能够有效抑制噪声和本地扰动，包括那些激发不稳定一致极点的扰动，并通过数值示例验证了其潜力。

Conclusion: 所提出的双自由度架构为异构智能体的一致性控制提供了更灵活和有效的解决方案，特别是在处理不稳定一致极点扰动方面具有优势。

Abstract: We propose a distributed two-degrees-of-freedom (2DOF) architecture for driving autonomous, possibly heterogeneous, agents to agreement. The scheme mirrors classical servo structures, separating local feedback from network filtering. This separation enables independent network-filter design for prescribed noise attenuation and allows controller heterogeneity to reject local disturbances, including disturbances exciting unstable agreement poles -- which is known to be impossible via standard diffusive couplings. The potential of the framework is illustrated via two numerical examples.

</details>


### [559] [Visibility-aware Satellite Selection and Resource Allocation in Multi-Orbit LEO Networks](https://arxiv.org/abs/2511.12678)
*Yingzhuo Sun,Yulan Gao,Ming Xiao,Zhu Han,Octavia A. Dobre*

Main category: eess.SY

TL;DR: 提出了一种动态可见性感知的多轨道卫星选择框架，通过马尔可夫近似和匹配博弈理论解决LEO卫星通信中的卫星选择、关联控制和资源调度联合优化问题。


<details>
  <summary>Details</summary>
Motivation: 多轨道LEO卫星通信是实现全球覆盖的关键基础设施，但现有方法在同时考虑卫星选择、关联控制和资源调度，并处理多轨道星座动态可见性方面仍存在挑战。

Method: 采用马尔可夫近似和匹配博弈理论，将问题分解为用户关联（通过匹配博弈）和功率分配（通过拉格朗日对偶规划），形成针对该问题的块坐标下降方法。

Result: 仿真结果显示算法在所有场景下都能收敛到次优解，相比四种最先进的基线方法，平均实现了约7.85%的更高总速率。

Conclusion: 所提出的动态可见性感知框架能够有效解决多轨道LEO卫星通信的联合优化问题，显著提升系统性能。

Abstract: Multi orbit low earth orbit (LEO) satellites communication is envisioned as a key infrastructure to deliver global coverage, enabling future services from space air ground integrated networks.However, the optimized design of LEO which jointly addresses satellite selection, association control, and resource scheduling while accounting for dynamic visibility in multi orbit constellations still remains open. Satellites moving along distinct orbital planes yield phase shifted ground tracks and heterogeneous, time varying coverage patterns that significantly complicate the optimization.To bridge the gap, we propose a dynamic visibility aware multi orbit satellite selection framework which can determine the optimal serving satellites across orbital layers. The framework is built upon Markov approximation and matching game theory. Specifically, we formulate a combinatorial optimization problem that maximizes the sum rate under per satellite power budgets. The problem is NP hard , combining discrete user association (UA) decisions with continuous power allocation, and an inherently non convex sum rate maximization objective. We address it through a problem specific Markov approximation. Moreover, we alternately solve UA or bandwidth allocation via a matching game and power allocation via a Lagrangian dual program, which together form a block coordinate descent method tailored to this problem. Simulation results show that the proposed algorithm converges to a suboptimal solution across all scenarios. Extensive experiments against four state of the art baselines further demonstrate that our algorithm achieves, on average, approximately 7.85% higher sum rate than the best performing baseline.

</details>


### [560] [Density-Driven Optimal Control for Non-Uniform Area Coverage in Decentralized Multi-Agent Systems Using Optimal Transport](https://arxiv.org/abs/2511.12756)
*Sungjun Seo,Kooktae Lee*

Main category: eess.SY

TL;DR: 提出D2OC框架，将最优传输理论与多智能体覆盖控制结合，解决非均匀区域覆盖问题，在考虑物理和操作约束的同时实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有均匀覆盖策略无法应对实际应用中不同区域需要不同关注度的需求，而非均匀方法要么缺乏最优性保证，要么未能考虑智能体动力学、有限操作时间、智能体数量和分散执行等现实约束。

Method: 集成最优传输理论与多智能体覆盖控制，通过求解包含物理和操作约束的优化问题，推导出最优控制输入，并开发分散数据共享机制实现无全局信息的协调。

Result: 仿真研究表明，D2OC相比现有方法显著提高了非均匀区域覆盖性能，同时保持了可扩展性和分散实现能力。

Conclusion: D2OC框架成功解决了多智能体系统中非均匀区域覆盖的关键问题，在考虑现实约束的同时提供了最优性能保证。

Abstract: This paper addresses the fundamental problem of non-uniform area coverage in multi-agent systems, where different regions require varying levels of attention due to mission-dependent priorities. Existing uniform coverage strategies are insufficient for realistic applications, and many non-uniform approaches either lack optimality guarantees or fail to incorporate crucial real-world constraints such as agent dynamics, limited operation time, the number of agents, and decentralized execution.
  To resolve these limitations, we propose a novel framework called Density-Driven Optimal Control (D2OC). The central idea of D2OC is the integration of optimal transport theory with multi-agent coverage control, enabling each agent to continuously adjust its trajectory to match a mission-specific reference density map. The proposed formulation establishes optimality by solving a constrained optimization problem that explicitly incorporates physical and operational constraints. The resulting control input is analytically derived from the Lagrangian of the objective function, yielding closed-form optimal solutions for linear systems and a generalizable structure for nonlinear systems. Furthermore, a decentralized data-sharing mechanism is developed to coordinate agents without reliance on global information.
  Comprehensive simulation studies demonstrate that D2OC achieves significantly improved non-uniform area coverage performance compared to existing methods, while maintaining scalability and decentralized implementability.

</details>


### [561] [On Boundedness of Quadratic Dynamics with Energy-Preserving Nonlinearity](https://arxiv.org/abs/2511.12758)
*Shih-Chi Liao,Maziar S. Hemati,Peter Seiler*

Main category: eess.SY

TL;DR: 本文验证了Schlegel和Noack提出的有界性必要条件的有效性：在二维系统中成立，但在三维系统中存在反例。


<details>
  <summary>Details</summary>
Motivation: 研究二次能量守恒非线性系统的有界性分析，特别是验证Schlegel和Noack提出的必要条件的适用范围。

Method: 使用独立证明验证二维系统的必要条件成立，并构造三维反例证明该条件在高维系统中不成立。

Result: 必要条件在二维系统中成立，但在三维系统中存在反例，表明该条件不能推广到高维情况。

Conclusion: 有界性分析存在理论缺口，需要进一步研究以减少保守性，特别是在高维系统中。

Abstract: Boundedness is an important property of many physical systems. This includes incompressible fluid flows, which are often modeled by quadratic dynamics with an energy-preserving nonlinearity. For such systems, Schlegel and Noack proposed a sufficient condition for boundedness utilizing quadratic Lyapunov functions. They also propose a necessary condition for boundedness aiming to provide a more complete characterization of boundedness in this class of models. The sufficient condition is based on Lyapunov theory and is true. Our paper focuses on this necessary condition. We use an independent proof to show that the condition is true for two dimensional systems. However, we provide a three dimensional counterexample to illustrate that the necessary condition fails to hold in higher dimensions. Our results highlight a theoretical gap in boundedness analysis and suggest future directions to address the conservatism.

</details>


### [562] [Discrete-Time Stability Analysis of ReLU Feedback Systems via Integral Quadratic Constraints](https://arxiv.org/abs/2511.12826)
*Sahel Vahedi Noori,Bin Hu,Geir Dullerud,Peter Seiler*

Main category: eess.SY

TL;DR: 本文分析了带有ReLU非线性的离散时间反馈系统的内部稳定性，推导了ReLU的硬积分二次约束，并通过LMI条件验证稳定性，比现有方法保守性更低。


<details>
  <summary>Details</summary>
Motivation: 研究由循环神经网络启发的带有ReLU非线性的离散时间反馈系统的内部稳定性问题。

Method: 使用有限脉冲滤波器和结构化矩阵推导标量ReLU的硬积分二次约束，结合耗散不等式得到LMI稳定性条件。

Result: 提出的动态IQCs是Zames-Falb IQCs的超集，数值结果表明比Zames-Falb乘子和静态QC方法给出更不保守的稳定性裕度。

Conclusion: 新提出的硬IQCs方法能更有效地验证带有ReLU非线性的反馈系统的内部稳定性。

Abstract: This paper analyzes internal stability of a discrete-time feedback system with a ReLU nonlinearity. This feedback system is motivated by recurrent neural networks. We first review existing static quadratic constraints (QCs) for slope-restricted nonlinearities. Next, we derive hard integral quadratic constraints (IQCs) for scalar ReLU by using finite impulse filters and structured matrices. These IQCs are combined with a dissipation inequality leading to an LMI condition that certifies internal stability. We show that our new dynamic IQCs for ReLU are a superset of the well-known Zames-Falb IQCs specified for slope-restricted nonlinearities. Numerical results show that the proposed hard IQCs give less conservative stability margins than Zames-Falb multipliers and prior static QC methods, sometimes dramatically so.

</details>


### [563] [Green Emergency Communications in RIS- and MA-Assisted Multi-UAV SAGINs: A Partially Observable Reinforcement Learning Approach](https://arxiv.org/abs/2511.12892)
*Liangshun Wu,Wen Chen,Shunqing Zhang,Yajun Wang,Kunlun Wang*

Main category: eess.SY

TL;DR: 提出一种时空A2C方法，通过传输包含局部状态、策略指纹和循环信念的决策前消息来解决灾后SAGIN中UAV通信受限的部分可观测性问题，在NLoS城市环境中提升覆盖和连接性能。


<details>
  <summary>Details</summary>
Motivation: 灾后SAGIN中地面基础设施受损，UAV需要在NLoS城市环境中快速恢复关键任务终端的连接。现有MARL方法无法有效处理通信受限的部分可观测性，非通信方法依赖不可用的全局批评者，启发式共享脆弱冗余，可学习协议丢失邻居结构并加剧非平稳性。

Method: 提出时空A2C方法，每个UAV传输包含局部状态、紧凑策略指纹和循环信念的决策前消息，按邻居编码并拼接。使用空间折扣塑造价值目标以强调局部交互，在一跳每时隙延迟下分析延迟视图的稳定训练。

Result: 实验结果显示该方法优于IA2C、ConseNet、FPrint、DIAL和CommNet，实现更快收敛、更高渐近奖励、减少TD/优势误差，以及更好的通信吞吐量-能量权衡。

Conclusion: 所提出的时空A2C方法有效解决了灾后SAGIN中UAV通信受限的部分可观测性问题，通过紧凑的消息传输和空间折扣机制显著提升了系统性能。

Abstract: In post-disaster space-air-ground integrated networks (SAGINs), terrestrial infrastructure is often impaired, and unmanned aerial vehicles (UAVs) must rapidly restore connectivity for mission-critical ground terminals in cluttered non-line-of-sight (NLoS) urban environments. To enhance coverage, UAVs employ movable antennas (MAs), while reconfigurable intelligent surfaces (RISs) on surviving high-rises redirect signals. The key challenge is communication-limited partial observability, leaving each UAV with a narrow, fast-changing neighborhood view that destabilizes value estimation. Existing multi-agent reinforcement learning (MARL) approaches are inadequate--non-communication methods rely on unavailable global critics, heuristic sharing is brittle and redundant, and learnable protocols (e.g., CommNet, DIAL) lose per-neighbor structure and aggravate non-stationarity under tight bandwidth. To address partial observability, we propose a spatiotemporal A2C where each UAV transmits prior-decision messages with local state, a compact policy fingerprint, and a recurrent belief, encoded per neighbor and concatenated. A spatial discount shapes value targets to emphasize local interactions, while analysis under one-hop-per-slot latency explains stable training with delayed views. Experimental results show our policy outperforms IA2C, ConseNet, FPrint, DIAL, and CommNet--achieving faster convergence, higher asymptotic reward, reduced Temporal-Difference(TD)/advantage errors, and a better communication throughput-energy trade-off.

</details>


### [564] [Wide-Area Feedback Control for Renewables-Heavy Power Systems: A Comparative Study of Reinforcement Learning and Lyapunov-Based Design](https://arxiv.org/abs/2511.12911)
*Muhammad Nadeem,MirSaleh Bahavarnia,Ahmad F. Taha*

Main category: eess.SY

TL;DR: 本文探讨了在可再生能源密集电网中，数据驱动的强化学习控制与基于模型的Lyapunov控制方法的对比研究。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源普及，电网动态建模日益复杂，而数据采集和实时监控能力增强，这促使从基于模型和Lyapunov的控制器设计转向模型无关方法。

Method: 针对详细的可再生能源电力系统非线性微分代数方程，分别采用完全模型无关的强化学习方法和基于Lyapunov稳定性理论的模型方法进行最优反馈控制设计。

Result: 通过理论发展和详细案例研究，对两种方法在可再生能源密集电网中的表现进行了全面分析。

Conclusion: 论文详细分析了两种方法在可再生能源密集电网中的优缺点，为选择数据驱动还是模型驱动控制方法提供了参考依据。

Abstract: As renewable energy sources become more prevalent, accurately modeling power grid dynamics is becoming increasingly more complex. Concurrently, data acquisition and realtime system state monitoring are becoming more available for control centers. This motivates shifting from \textit{model- and Lyapunov-based} feedback controller designs toward \textit{model-free} ones. Reinforcement learning (RL) has emerged as a key tool for designing model-free controllers. Various studies have been carried out to study voltage/frequency control strategies via RL. However, usually a simplified system model is used neglecting detailed dynamics of solar, wind, and composite loads -- and damping system-wide oscillations and modeling power flows are all usually ignored. To that end, we pose an optimal feedback control problem for a detailed renewables-heavy power system, defined by a set of nonlinear differential algebraic equations (NDAE). The control problem is solved using a completely model-free design via RL as well as using a model-based approach built upon the Lyapunov stability theory with guarantees. The paper in its essence seeks to explore whether data-driven feedback control should be used in power grids over its model-driven counterpart. Theoretical developments and thorough case studies are presented with an eye on this exploration. Finally, a detailed analysis is provided to delineate the strengths and weaknesses of both approaches for renewables-heavy grids.

</details>


### [565] [Cooperative ISAC for LAE: Joint Trajectory Planning, Power allocation, and Dynamic Time Division](https://arxiv.org/abs/2511.13006)
*Fangzhi Li,Zhichu Ren,Cunhua Pan,Hong Ren,Jing Jin,Qixing Wang,Jiangzhou Wang*

Main category: eess.SY

TL;DR: 提出了一种用于多无人机系统的集成感知与通信框架，通过联合优化无人机轨迹、通信感知功率分配和动态时分比，在满足感知互信息要求的同时最大化通信速率。


<details>
  <summary>Details</summary>
Motivation: 为了提升空地网络的性能，需要解决无人机系统中感知与通信之间的权衡问题，特别是在严格功率或感知约束下。

Method: 采用交替优化框架解决非凸优化问题，联合优化无人机轨迹、通信感知功率分配和动态时分比。

Result: 仿真结果表明，所提出的联合设计方案显著优于静态或部分优化的基准方案。

Conclusion: 动态轨迹和资源管理对于有效导航感知-通信权衡至关重要，特别是在严格功率或感知约束下。

Abstract: To enhance the performance of aerial-ground networks, this paper proposes an integrated sensing and communication (ISAC) framework for multi-UAV systems. In our model, ground base stations (BSs) cooperatively serve multiple unmanned aerial vehicles (UAVs), and employ a time-division strategy in which beam scanning for sensing comes before data communication in each time slot. To maximize the sum communication rate while satisfying the total sensing mutual information (MI) requirement, we jointly optimize the UAV trajectories, communication and sensing power allocation, and the dynamic time-division ratio. The resulting non-convex optimization problem is efficiently solved using an alternating optimization (AO) framework. Simulation results demonstrate that our proposed joint design significantly outperforms benchmark schemes with static or partially optimized resources. The findings also reveal the critical importance of dynamic trajectory and resource management for effectively navigating the sensing-communication trade-off, especially under stringent power or sensing constraints.

</details>


### [566] [An Online Multiobjective Policy Gradient for Long-run Average-reward Markov Decision Process](https://arxiv.org/abs/2511.13034)
*Rahul Misra,Manuela L. Bujorianu,Rafał Wisniewski*

Main category: eess.SY

TL;DR: 提出了一个基于强化学习的多目标决策框架，使用Blackwell方法定理指导的动态标量化机制，确保时间平均奖励向量收敛到预定义目标集。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法处理标量奖励，而多目标决策需要优化奖励向量，确保收敛到目标集合。

Method: 结合Blackwell方法定理的动态标量化机制，内循环在指定循环状态间应用策略梯度方法，外循环更新标量化向量。

Result: 理论证明了长期平均奖励向量收敛到目标集，并通过数值示例验证了方法的有效性。

Conclusion: 该框架为多目标强化学习提供了理论保证的收敛性，扩展了强化学习在多目标决策中的应用。

Abstract: We propose a reinforcement learning (RL) framework for multi-objective decision-making, where the agent seeks to optimize a vector of rewards rather than a single scalar value. The objective is to ensure that the time-averaged reward vector converges asymptotically to a predefined target set. Since standard RL algorithms operate on scalar rewards, we introduce a dynamic scalarization mechanism guided by Blackwell's Approachability Theorem. This theorem enables adaptive updates of the scalarization vector to guarantee convergence toward the target set. Assuming ergodicity, the Markov chain induced by the learned policies admits a stationary distribution, ensuring all states recur with finite return times. Our algorithm exploits this property by defining an inner loop that applies a policy gradient method (with baseline) between successive visits to a designated recurrent state, enforcing Blackwell's condition at each iteration. An outer loop then updates the scalarization vector after each recurrence. We establish theoretical convergence of the long-run average reward vector to the target set and validate the approach through a numerical example.

</details>


### [567] [Initial Excitation-based Adaptive Observers for Discrete-Time LTI Systems](https://arxiv.org/abs/2511.13117)
*Anchita Dey,Soutrik Bandyopadhyay,Shubhendu Bhasin*

Main category: eess.SY

TL;DR: 提出一种基于初始激励(IE)的自适应观测器，用于离散时间线性时不变系统，相比传统需要持续激励的方法更实用，采用双层滤波结构和归一化梯度下降更新律。


<details>
  <summary>Details</summary>
Motivation: 实际应用中系统参数和状态难以准确获取，连续时间系统的自适应观测器已有较多研究，但离散时间系统的对应方法相对较少，且传统方法需要无限时间的持续激励，不适用于稳定化任务。

Method: 使用双层滤波结构和归一化梯度下降更新律学习未知参数，改进回归器以增强信息提取，在初始激励条件下保证状态和参数估计的有界性和指数收敛。

Result: 理论分析证明在初始激励条件下，状态和参数估计有界且指数收敛，仿真结果验证了所提设计的有效性。

Conclusion: 提出的基于初始激励的自适应观测器相比传统方法更实用，不需要无限时间激励，适用于稳定化任务，具有理论保证和良好性能。

Abstract: In practical applications, the efficacy of a control algorithm relies critically on the accurate knowledge of the parameters and states of the underlying system. However, obtaining these quantities in practice is often challenging. Adaptive observers address this issue by performing simultaneous state and parameter estimation using only input-output measurements. While many adaptive observer designs exist for continuous-time systems, their discrete-time counterparts remain relatively unexplored. This paper proposes an initial excitation (IE)-based adaptive observer for discrete-time linear time-invariant systems. In contrast to conventional designs that rely on the persistence of excitation condition, which requires continuous excitation and infinite control effort, the proposed method does not require excitation for infinite time, thus making it more practical for stabilization tasks. We employ a two-layer filtering structure and a normalized gradient descent-based update law for learning the unknown parameters. We also propose modifying the regressors to enhance information extraction, leading to faster convergence. Rigorous theoretical analysis guarantees bounded and exponentially converging estimates of both states and parameters under the IE condition, and simulation results validate the efficacy of the proposed design.

</details>


### [568] [Carbon Reduction Potential and Sensitivity Analysis of Rural Integrated Energy System with Carbon Trading and Coordinated Electric-Thermal Demand Response](https://arxiv.org/abs/2511.13119)
*Xuxin Yang,Xue Yuan,Donghan Feng,Siru Chen,Yuanhao Feng*

Main category: eess.SY

TL;DR: 本研究构建了农村综合能源系统(RIES)的宏观和微观低碳优化框架，结合电热需求响应和碳交易机制，识别了28个碳相关参数中的高敏感性因素，提升了系统减碳潜力。


<details>
  <summary>Details</summary>
Motivation: 现有RIES减碳研究主要关注宏观层面的系统级能源设备优化运行，而需求侧柔性负荷和外部碳交易机制的协同减碳效应未被充分探索，同时微观层面设备参数的碳敏感性及其减排贡献研究不足。

Method: 宏观层面开发多能源耦合低碳优化运行框架，整合协调电热需求响应和碳交易；微观层面建立RIES组件碳排放模型，对28个碳相关参数进行敏感性分析。

Result: 基于中国北方农村典型运行数据的案例研究表明，协调电热需求响应和碳交易可实现最大减碳潜力，识别的高敏感性参数为增强RIES减碳潜力提供了重要理论指导。

Conclusion: 该研究通过宏观微观相结合的方法，有效提升了农村综合能源系统的减碳能力，为支持中国农村现代化和新型城镇化提供了低碳能源解决方案。

Abstract: Constructing clean and low-carbon rural integrated energy system (RIES) is a fundamental requirement for supporting China's rural modernization and new-type urbanization. Existing research on RIES decarbonization primarily focuses on the optimal low-carbon operation of system-level energy devices at the macro level, while the synergistic carbon-reduction effects of demand-side flexible loads and external carbon trading mechanisms have not been fully explored. Meanwhile, at the micro level, the carbon sensitivity of device parameters and their potential contribution to emission reduction remain insufficiently investigated. To address these gaps, this study integrates macro- and micro-level analyses. At the macro level, a multi-energy-coupled low-carbon optimal operation framework is developed, incorporating coordinated electric-thermal demand response (DR) and carbon trading. At the micro level, a carbon emission model for RIES components is established, and sensitivity analysis is conducted on 28 carbon-related parameters to identify highly sensitive determinants of emission reduction. Case studies based on typical operation data from a rural region in northern China demonstrate that coordinated electric-thermal DR and carbon trading can achieve maximum carbon-reduction potential. Furthermore, the identified high-sensitivity parameters provide essential theoretical guidance for enhancing the decarbonization potential of RIES.

</details>


### [569] [A Comprehensive Review of Advancements in Powering and Charging Systems for Unmanned Aerial Vehicles](https://arxiv.org/abs/2511.13122)
*Harsh Abhinandan,Aditya Dhanraj,Aryan Katoch,R. Raja Singh*

Main category: eess.SY

TL;DR: 本文综述了无人机(UAV)电源和充电技术的最新进展，包括各种能源系统、充电方法和无线电力传输技术，旨在解决无人机有限的飞行时间问题。


<details>
  <summary>Details</summary>
Motivation: 无人机的广泛应用受到机载电源有限功率预算的限制，导致飞行时间不足。需要研究新的电源和充电策略来实现长时间的自主飞行。

Method: 通过比较分析当前最先进的无人机电源和充电技术，包括传统电池、燃料电池、混合系统等能源来源，以及从简单电池更换到自动对接站、无线电力传输等多种充电方案。

Result: 综述了各种能源系统的优缺点（能量密度、重量、安全性），探讨了无线电力传输技术（近场和远场），并分析了功率电子转换器拓扑、电池管理系统和控制方法。

Conclusion: 识别了技术、经济和社会方面的主要挑战，为研究人员、工程师和政策制定者提供了增强无人机操作性能的指导，并指出了未来研究的有前景方向。

Abstract: Unmanned Aerial Vehicles (UAVs) or drones have witnessed a spectacular surge in applications for military, commercial, and civilian purposes. However, their potential for flight is always limited by the finite power budget of their onboard power supplies. The limited flight time problem has led to intensive research into new sources of power and innovative charging strategies to enable protracted, autonomous flight. This paper gives a comparative summary of the current state-of-the-art in UAV power and refuelling technology. The paper begins with an analysis of the variety of energy sources, from classical batteries to fuel cells and hybrid systems, based on their relative advantages and disadvantages in energy density, weight, and safety. Subsequently, the review explores a spectrum of replenishment options, from simple manual battery swapping to sophisticated high-tech automatic docking stations and smart contact-based charging pads. Most of the review is dedicated to the newer technology of wireless power transfer, which involves near-field (inductive, capacitive) and far-field (laser, microwave) technology. The article also delves into the most important power electronic converter topologies, battery management systems, and control approaches that form the core of these charging systems. Finally, it recapitulates the most significant challenges in technical, economic, and social aspects for promising avenues of future research. The comprehensive review is a valuable guide for researchers, engineers, and policymakers striving to enhance UAV operational performance.

</details>


### [570] [Cyber-Resilient Fault Diagnosis Methodology in Inverter-Based Resource-Dominated Microgrids with Single-Point Measurement](https://arxiv.org/abs/2511.13162)
*Yifan Wang,Yiyao Yu,Yang Xia,Yan Xu*

Main category: eess.SY

TL;DR: 提出一种基于单点测量的分数阶记忆增强攻击诊断方案(FO-MADS)，用于逆变器主导微电网的网络安全故障诊断，在四种攻击场景下达到92.8%-96.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有诊断方法需要昂贵的多点仪器或严格的建模假设，无法在单点测量约束下有效工作，而网络攻击严重威胁逆变器主导微电网的安全运行。

Method: 构建双分数阶特征库，结合Caputo和Grünwald-Letnikov导数放大信号微扰；使用两阶段分层分类器定位故障逆变器和IGBT开关；通过渐进记忆回放对抗训练增强鲁棒性。

Result: 在四逆变器测试平台上，针对1个正常类和24个故障类，在四种攻击场景下的诊断准确率分别为：偏置攻击96.6%、噪声攻击94.0%、数据替换攻击92.8%、重放攻击95.7%，无攻击条件下保持96.7%。

Conclusion: FO-MADS是一种成本效益高且易于部署的解决方案，显著增强了逆变器主导微电网的网络物理弹性。

Abstract: Cyber-attacks jeopardize the safe operation of inverter-based resource-dominated microgrids (IBR-dominated microgrids). At the same time, existing diagnostic methods either depend on expensive multi-point instrumentation or stringent modeling assumptions that are untenable under single-point measurement constraints. This paper proposes a Fractional-Order Memory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves timely fault localization and cyber-resilient fault diagnosis using only one VPQ (voltage, active power, reactive power) measurement point. FO-MADS first constructs a dual fractional-order feature library by jointly applying Caputo and Grünwald-Letnikov derivatives, thereby amplifying micro-perturbations and slow drifts in the VPQ signal. A two-stage hierarchical classifier then pinpoints the affected inverter and isolates the faulty IGBT switch, effectively alleviating class imbalance. Robustness is further strengthened through Progressive Memory-Replay Adversarial Training (PMR-AT), whose attack-aware loss is dynamically re-weighted via Online Hard Example Mining (OHEM) to prioritize the most challenging samples. Experiments on a four-inverter IBR-dominated microgrid testbed comprising 1 normal and 24 fault classes under four attack scenarios demonstrate diagnostic accuracies of 96.6% (bias), 94.0% (noise), 92.8% (data replacement), and 95.7% (replay), while sustaining 96.7% under attack-free conditions. These results establish FO-MADS as a cost-effective and readily deployable solution that markedly enhances the cyber-physical resilience of IBR-dominated microgrids.

</details>


### [571] [Event-Triggered Regulation of Mixed-Autonomy Traffic Under Varying Traffic Conditions](https://arxiv.org/abs/2511.13206)
*Yihuai Zhang,Huan Yu*

Main category: eess.SY

TL;DR: 本文开发了一种事件触发控制框架，用于缓解混合自动驾驶交通系统的拥堵，通过扩展的ARZ模型和坡道计量控制，显著减少控制更新次数，提高驾驶员舒适度和道路安全性。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的快速发展，建模和缓解由人类驾驶车辆和自动驾驶车辆组成的混合自动驾驶交通系统的拥堵问题变得越来越重要。

Method: 基于反步法设计了事件触发控制策略，结合基于观测器的ETC公式，在有限传感条件下实现实际应用。采用扩展的Aw-Rascle-Zhang模型和坡道计量作为边界执行机制。

Result: 仿真验证表明，ETC不仅稳定了混合交通流，还显著减少了控制更新，提高了驾驶员舒适度和道路安全性。更高的AV渗透率导致更长的释放时间和更少的触发事件。

Conclusion: 与连续反步控制器相比，所提出的ETC实现了近乎等效的稳定性能，但控制器更新次数少得多，从而减少了驾驶员分心，在交通管理中具有巨大应用潜力。

Abstract: Modeling and congestion mitigation of mixed-autonomy traffic systems consisting of human-driven vehicles (HVs) and autonomous vehicles (AVs) have become increasingly critical with the rapid development of autonomous driving technology. This paper develops an event-triggered control (ETC) framework for mitigating congestion in such systems, which are modeled using an extended Aw-Rascle-Zhang (ARZ) formulation consisting of coupled 4 x 4 hyperbolic partial differential equations (PDEs). Ramp metering is employed as the boundary actuation mechanism. To reduce computational and communication burdens while avoiding excessive ramp signal changes, we design the ETC strategy based on the backstepping method, together with an observer-based ETC formulation for practical implementation under limited sensing. Rigorous Lyapunov analysis ensures exponential convergence and avoidance of Zeno behavior. Extensive simulations validate the proposed approach under diverse traffic scenarios, including varying AV penetration rates, different spacing policies, multiple demand levels, and non-recurrent congestion patterns. Results show that ETC not only stabilizes mixed traffic flows but also significantly reduces control updates, improving driver comfort, and roadway safety. Higher AV penetration rates lead to longer release time and fewer triggering events, indicating the positive impact of AVs in mitigating traffic congestion while reducing computational resource usage. Compared to continuous backstepping controllers, the proposed ETC achieves near-equivalent stabilization performance with far fewer controller updates, resulting in longer signal release time that reduces driver distraction, which demonstrates great potential for ETC applications in traffic management.

</details>


### [572] [Robust Control Design Using a Hybrid-Gain Finite-Time Sliding-Mode Controller](https://arxiv.org/abs/2511.13260)
*Amit Shivam,Kiran Kumari,Fernando A. C. C. Fontes*

Main category: eess.SY

TL;DR: 提出了一种混合增益有限时间滑模控制策略，用于受扰非线性系统，结合有限时间趋近律和混合幂/指数律，在保证有限时间收敛的同时限制控制作用。


<details>
  <summary>Details</summary>
Motivation: 针对现有有限时间控制方法控制作用过大、不光滑的问题，开发一种既能保证有限时间收敛又能限制控制作用的光滑控制策略。

Method: 采用混合增益有限时间滑模控制，包含驱动滑模变量到边界层的外部有限时间趋近律和保证层内快速收敛的内部混合幂/指数律。

Result: 控制器在保持与现有有限时间方法相当的调节时间的同时，显著降低了控制作用，在二连杆机械臂轨迹跟踪中验证了鲁棒性和实用性。

Conclusion: 所提出的HG-FTSMC方法为受扰非线性系统提供了一种有效控制策略，在保证有限时间收敛和鲁棒性的同时限制控制作用，具有实际应用价值。

Abstract: This paper proposes a hybrid-gain finite-time sliding-mode control (HG-FTSMC) strategy for a class of perturbed nonlinear systems. The controller combines a finite-time reaching law that drives the sliding variable to a predefined boundary layer with an inner mixed-power or exponential law that guarantees rapid convergence within the layer while maintaining smooth and bounded control action. The resulting control design achieves finite-time convergence and robustness to matched disturbances, while explicitly limits the control effort. The control framework is first analyzed on a perturbed first-order integrator model, and then extended to Euler-Lagrange (EL) systems, representing a broad class of robotic and mechanical systems. Comparative simulations demonstrate that the proposed controller achieves settling times comparable to recent finite-time approaches [1], while substantially reducing the control effort. Finally, trajectory-tracking simulations on a two-link manipulator further validate the robustness and practical feasibility of the proposed HG-FTSMC approach.

</details>


### [573] [Beyond Energy Functions and Numerical Integration: A New Methodology to Determine Transient Stability at the Initial State](https://arxiv.org/abs/2511.13289)
*Wenhao Wu,Dan Wu,Bin Wang,Jiabing Hu*

Main category: eess.SY

TL;DR: 提出了一种新的暂态稳定性分析方法，通过轨迹依赖稳定性指标函数和策略性时间收缩映射，将TSA转化为极点配置检测问题，利用高阶导数推导有理函数近似，实现数学直接且计算高效的预测。


<details>
  <summary>Details</summary>
Motivation: 克服传统数值积分和能量函数方法的局限性，为电力系统暂态稳定性分析提供更直接有效的数学方法。

Method: 构建轨迹依赖稳定性指标函数，应用策略性时间收缩映射，将TSA转化为极点配置检测问题，利用初始状态高阶导数推导有理函数近似。

Result: 在基准系统上的数值验证表明，该方法不仅为电力系统TSA提供了直接的数学捷径，还为评估广泛非线性动力系统的暂态稳定性建立了有前景的新方法。

Conclusion: 该方法成功克服了传统TSA方法的限制，提供了一种数学直接且计算高效的暂态稳定性预测方法，具有广泛的应用潜力。

Abstract: This paper presents a novel method for transient stability analysis (TSA) that circumvents the limitations of sequential numerical integration and energy functions. The proposed method begins by constructing a trajectory-dependent stability indicator function to distinguish the system's destiny. To overcome the difficulty in analyzing the asymptotic behavior at infinite time, a strategic time contraction mapping is then applied. This allows TSA to be recast as a pole-placement detection problem for the indicator function. By leveraging high-order derivatives at the initial state, a rational function approximation is derived, yielding a mathematically direct and computationally efficient prediction. Numerical validations on benchmark systems demonstrate that the method not only provides a direct mathematical shortcut for TSA in power systems but also establishes a promising new methodology for evaluating the transient stability of a broad class of nonlinear dynamical systems.

</details>


### [574] [Event-triggered Dual Gradient Tracking for Distributed Resource Allocation](https://arxiv.org/abs/2511.13362)
*Xiayan Xu,Xiaomeng Chen,Dawei Shi,Ling Shi*

Main category: eess.SY

TL;DR: 提出了一种基于事件触发的对偶梯度跟踪算法，用于解决不平衡有向网络中的分布式资源分配问题，通过仅在局部状态偏差超过阈值时通信来显著减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统对偶梯度跟踪方法在不平衡有向图上有效，但依赖周期性通信，在资源受限网络中产生显著开销。需要减少通信频率同时保持收敛性能。

Method: 设计事件触发机制，当局部状态偏差超过预设阈值时才进行通信。结合对偶梯度跟踪算法处理不平衡有向网络问题。

Result: 证明了非凸对偶目标的次线性收敛和Polyak-Łojasiewicz条件下的线性收敛。对于强凸成本函数实现次线性收敛，对Lipschitz光滑强凸函数实现线性收敛。数值实验显示通信事件显著减少。

Conclusion: 事件触发方法在保持可比收敛性能的同时，相比周期性方案显著减少了通信事件，有效解决了资源受限网络中的通信瓶颈问题。

Abstract: High communication costs create a major bottleneck for distributed resource allocation over unbalanced directed networks. Conventional dual gradient tracking methods, while effective for problems on unbalanced digraphs, rely on periodic communication that creates significant overhead in resource-constrained networks. This paper introduces a novel event-triggered dual gradient tracking algorithm to mitigate this limitation, wherein agents communicate only when local state deviations surpass a predefined threshold. We establish comprehensive convergence guarantees for this approach. First, we prove sublinear convergence for non-convex dual objectives and linear convergence under the Polyak-Łojasiewicz condition. Building on this, we demonstrate that the proposed algorithm achieves sublinear convergence for general strongly convex cost functions and linear convergence for those that are also Lipschitz-smooth. Numerical experiments confirm that our event-triggered method significantly reduces communication events compared to periodic schemes while preserving comparable convergence performance.

</details>


### [575] [Microwave-acoustic-driven power electronics](https://arxiv.org/abs/2511.13412)
*Liyang Jin,Zichen Xi,Joseph G. Thomas,Jun Ji,Yuanzhi Zhang,Nuo Chen,Yizheng Zhu,Linbo Shao,Liyan Zhu*

Main category: eess.SY

TL;DR: 基于微波频率表面声波器件的机械隔离栅极驱动器，实现了2.75kV电气隔离和超低隔离电容，可同时传输功率和信号，在宽温度范围内工作。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以通过统一通道同时传输功率和信号，电气隔离对于确保安全和减少电磁干扰至关重要。

Method: 使用微波频率表面声波器件在铌酸锂基板上构建机械隔离栅极驱动器，利用声波传播实现电气隔离。

Result: 实现了2.75kV的电气隔离、0.032pF的超低隔离电容，在1.25mm机械传播长度上提供13.4V开路电压和44.4mA短路电流，氮化镓晶体管开启时间为108.8ns。

Conclusion: 微波频率表面声波器件提供固有的电磁干扰免疫性，可在多种半导体平台上实现异质集成，为先进电力电子提供紧凑、高性能的隔离功率和信号传输解决方案。

Abstract: Electrical isolation is critical to ensure safety and minimize electromagnetic interference (EMI), yet existing methods struggle to simultaneously transmit power and signals through a unified channel. Here we demonstrate a mechanically-isolated gate driver based on microwave-frequency surface acoustic wave (SAW) device on lithium niobate that achieves galvanic isolation of 2.75 kV with ultralow isolation capacitance (0.032 pF) over 1.25 mm mechanical propagation length, delivering 13.4 V open-circuit voltage and 44.4 mA short-circuit current. We demonstrate isolated gate driving for a gallium nitride (GaN) high-electron-mobility transistor, achieving a turn-on time of 108.8 ns comparable to commercial drivers and validate its operation in a buck converter. In addition, our SAW device operates over an ultrawide temperature range from 0.5 K (-272.6 °C) to 544 K (271 °C). The microwave-frequency SAW devices offer inherent EMI immunity and potential for heterogeneous integration on multiple semiconductor platforms, enabling compact, high-performance isolated power and signal transmission in advanced power electronics.

</details>


### [576] [High-resolution hierarchical PV system performance modeling in urban environments](https://arxiv.org/abs/2511.13424)
*Bowen Tian,Roel C. G. M. Loonen,Roland M. E. Valckenborg,Jan L. M. Hensen*

Main category: eess.SY

TL;DR: 提出了一种高分辨率分层建模框架，用于准确预测城市环境中光伏系统的性能，特别解决了复杂部分阴影问题，相比传统模型显著提高了精度。


<details>
  <summary>Details</summary>
Motivation: 城市环境中光伏系统面临复杂部分阴影问题，传统粗分辨率模型在预测实际性能时存在严重误差，需要更精确的建模方法。

Method: 采用高分辨率分层建模框架，从太阳能电池到系统级进行详细建模，通过现场测试数据进行严格验证。

Result: 模型在预测分钟级动态电气特性方面表现出高精度（R2 > 0.90），发现传统模型会高估运行功率达163%，月发电量达54%。模块级电力电子技术可将重阴影串的月发电量提高20%以上。

Conclusion: 该研究为复杂城市环境中光伏系统的可靠设计、准确功率预测和优化提供了关键工具。

Abstract: Accurate performance modeling of PV systems in urban environments is a significant challenge due to complex partial shading. This study introduces a high-resolution, hierarchical modeling framework that provides detailed insights from the solar cell to the system level. Rigorously validated against field-test data from calibrated equipment, the model demonstrates high accuracy in predicting minute-wised dynamic electrical characteristics (R2 > 0.90). A key finding is the critical shortcoming of conventional, coarser-resolution models under realistic shading; these are shown to overestimate the actual string operating power by up to 163% and the monthly energy yield by up to 54%. The proposed framework avoids these errors by precisely capturing mismatch losses and the time-varying phenomena of system components, such as bypass diode activations. Furthermore, the model accurately quantifies the effectiveness of mitigation technologies, showing that Module-Level Power Electronics (MLPEs) can increase the monthly energy yield of a heavily shaded string by over 20%. This research provides a crucial tool for reliable system design, accurate power forecasting, and the optimization of PV systems in complex urban settings.

</details>


### [577] [Handover-Aware URLLC UAV Trajectory Planning: A Continuous-Time Trajectory Optimization via Graphs of Convex Sets](https://arxiv.org/abs/2511.13429)
*Yuqi Ping,Tingting Zhang,Tianhao Liang*

Main category: eess.SY

TL;DR: 该论文研究无人机在保持与地面基站超可靠低延迟通信的同时，优化飞行轨迹和基站关联，以减少切换次数、路径长度和飞行时间。


<details>
  <summary>Details</summary>
Motivation: 长距离飞行会触发频繁的小区间切换，导致延迟和同步开销。需要同时优化轨迹和基站关联来维持URLLC连接。

Method: 将问题重新表述为基于凸集图的优化问题，构建交集图，使用贝塞尔曲线参数化轨迹，通过混合整数凸规划求解。

Result: 仿真验证该方法在保持URLLC连接的同时，实现了较少切换次数和飞行效率之间的明确权衡。

Conclusion: 提出的方法能够生成平滑、动态可行的轨迹，在通信可靠性和飞行效率之间取得良好平衡。

Abstract: In this paper, we study a cellular-connected unmanned aerial vehicle (UAV) which aims to fly between two predetermined locations while maintaining ultra-reliable low-latency communications (URLLC) for command-and-control (C2) links with terrestrial base stations (BSs). Long-range flights often trigger frequent inter-cell handovers, which may introduce delays and synchronization overhead. We jointly optimize the continuous trajectory and BS association to minimize handovers, path length, and flying time, subject to communication reliability and kinematic constraints. To address this problem, we reformulate it as an optimization based on the graph of convex sets (GCS). First, the URLLC requirement is translated into spatially feasible regions in the flight plane for each BS. And an intersection graph is constructed including the start and goal points. Each graph node is associated with a smooth and dynamically feasible trajectory segment. The trajectory is parameterized in space by Bézier curves and in time by a monotonic Bézier scaling, together with convex constraints that ensure continuity and enforce speed bounds. Next, we impose unit-flow constraints to enforce a single path, and by coupling the resulting binary edge-selection variables with the convex constraints, we obtain a mixed-integer convex program (MICP). Applying a convex relaxation and rounding to the mixed-integer convex program produces nearly globally optimal routes, and a final refinement yields smooth, dynamically feasible trajectories. Simulations verify that the method preserves URLLC connectivity while achieving a clear trade-off between fewer handovers and flight efficiency.

</details>


### [578] [The Liquid Buffer: Multi-Year Storage for Defossilization and Energy Security under Climate Uncertainty](https://arxiv.org/abs/2511.13513)
*Leonard Göke,Jan Wohland,Stefano Moret,André Bardow*

Main category: eess.SY

TL;DR: 该论文提出一个可扩展的随机模型，通过引入多年度液态碳氢化合物存储来管理可再生能源的气候不确定性，在欧洲案例中可降低系统成本4.1%，减少化石燃料进口86%，削减弃电60%。


<details>
  <summary>Details</summary>
Motivation: 应对可再生能源发电和电力需求的气候驱动不确定性对净零能源系统能源安全的挑战。

Method: 开发了一个可扩展的随机模型，隐含考虑了51,840个气候年份，识别多年度液态碳氢化合物存储作为管理气候不确定性的关键方案。

Result: 在欧洲，多年度存储使系统成本降低4.1%，化石燃料进口减少86%，弃电减少60%。所需液态碳氢化合物储能容量为525TWh，氢储能容量为116TWh。

Conclusion: 多年度存储能有效管理气候不确定性，确保能源安全，供应安全保持高水平，未满足能源仅占0.0035‰，远低于0.02‰的常见目标。

Abstract: The climate-driven uncertainty of renewable generation and electricity demand challenges energy security in net-zero energy systems. By introducing a scalable stochastic model that implicitly accounts for 51'840 climate years, this paper identifies multi-year storage of liquid hydrocarbons as a key option for managing climate uncertainty and ensuring energy security. In Europe, multi-year storage reduces system costs by 4.1%, fossil imports by 86%, and curtailment by 60%. The benefit of multi-year storage is that a renewable surplus in one year is not curtailed but converted to synthetic oil, with hydrogen as an intermediate product, and stored to balance a future deficit. We find that the required energy capacity for liquid hydrocarbons is 525 TWh, a quarter of the European Union's current oil and gas reserves, complemented by 116 TWh for hydrogen storage. Security of supply remains high and unserved energy only amounts to 0.0035 per thousand, well below the common target of 0.02 per thousand.

</details>


### [579] [On the controller form for linear hyperbolic MIMO systems with dynamic boundary conditions](https://arxiv.org/abs/2511.13546)
*Stefan Ecklebe,Frank Woittennek*

Main category: eess.SY

TL;DR: 本文为双向耦合ODE的线性双曲MIMO系统开发了代数方法获得控制器形式，提出了广义双曲控制器形式和基于平坦性的计算方案。


<details>
  <summary>Details</summary>
Motivation: 现有的SISO和MIMO ODE系统以及SISO双曲PDE系统的控制器形式方法无法直接应用于MIMO双曲系统，需要开发新的代数方法。

Method: 使用具有实指数的广义多项式描述系统中的预测和延迟，提出广义双曲控制器形式及其变体，并开发基于平坦性的计算方案。

Result: 成功开发了代数方法获得控制器形式，并将该算法应用于示例系统。

Conclusion: 提出的代数方法和基于平坦性的方案能够有效处理MIMO双曲系统的控制器形式问题，为这类系统的控制设计提供了新工具。

Abstract: This contribution develops an algebraic approach to obtain a controller form for a class of linear hyperbolic MIMO systems, bidirectionally coupled with a linear ODE system at the unactuated boundary. After a short summary of established controller forms for SISO and MIMO ODE as well as SISO hyperbolic PDE systems, it is shown that the direct ap- proach to state a controller form fails already for a very simple MIMO example. Next, a generalised hyperbolic controller form with different variants is proposed and a new flatnesss-based scheme to compute said form is presented. Therein, the system is treated in an algebraic setting where generalised polynomials with real exponents are used to describe the predictions and delays in the system. The proposed algorithm is then applied to the motivating example.

</details>


### [580] [Data-driven Acceleration of MPC with Guarantees](https://arxiv.org/abs/2511.13588)
*Agustin Castellano,Shijie Pan,Enrique Mallada*

Main category: eess.SY

TL;DR: 提出一种数据驱动的MPC加速框架，用离线MPC解构建非参数策略替代在线优化，实现100-1000倍加速


<details>
  <summary>Details</summary>
Motivation: MPC虽强大但在线优化速度慢，难以满足低延迟实时控制需求

Method: 基于离线MPC解构建非参数策略，该策略相对于构造的最优成本上界是贪婪的，可作为查找规则实现

Result: 在充分数据覆盖条件下策略递归可行且具有可证明的有界最优性差距，实验显示比标准MPC快100-1000倍

Conclusion: 该方法在保持接近最优性能的同时大幅提升速度，具有实时控制应用潜力

Abstract: Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.

</details>


### [581] [Physics-Informed Neural Networks for Nonlinear Output Regulation](https://arxiv.org/abs/2511.13595)
*Sebastiano Mengozzi,Giovanni B. Esposito,Michelangelo Bin,Andrea Acquaviva,Andrea Bartolini,Lorenzo Marconi*

Main category: eess.SY

TL;DR: 本文提出了一种基于物理信息神经网络(PINN)的方法来解决非线性系统的全信息输出调节问题，通过直接近似调节器方程的解来实现精确跟踪和抑制。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要预计算轨迹或标记数据来解决调节器方程，本文旨在开发一种无需这些数据就能准确求解调节器方程的方法，并实现对外部系统变化的泛化能力。

Method: 使用物理信息神经网络(PINN)方法，通过最小化边界条件和可行性条件下的残差来直接近似π(w)和c(w)，无需预计算轨迹或标记数据。

Result: 该方法能够高保真地重建零误差流形，在外部系统参数和初始条件变化时保持调节性能，并在直升机垂直动力学与振荡平台同步任务中得到验证。

Conclusion: 基于PINN的学习求解器在非线性输出调节问题中具有广泛应用潜力，能够实时推理并泛化到不同参数的外部系统族。

Abstract: This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold π(w) and a feedforward input c(w) that render such manifold invariant. The pair (π(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates π(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.

</details>


### [582] [Scalable Iterative Algorithm for Solving Optimal Transmission Switching with De-energization](https://arxiv.org/abs/2511.13662)
*Benoît Jeanson,Mathieu Tanneau,Simon Tindemans*

Main category: eess.SY

TL;DR: 提出了一种用于考虑断电后连接性损失的最优传输切换问题的新混合整数线性规划公式和快速启发式算法，比现有求解器快100-1000倍找到高质量可行解。


<details>
  <summary>Details</summary>
Motivation: 受RTE子输电运营启发，考虑传输元件损失后可能导致的连接性损失和局部停电问题，该问题在实际运营中很重要但在文献中很少被研究。

Method: 提出新的混合整数线性规划公式，无需额外二元变量即可表示事故后连接性损失，并基于此开发了快速迭代启发式算法。

Result: 计算实验表明，最先进的优化求解器难以在合理时间内解决OTSD的扩展公式，而所提启发式算法比Gurobi快100-1000倍找到高质量可行解。

Conclusion: 该研究为考虑断电后连接性损失的最优传输切换问题提供了有效的解决方案，显著提升了求解效率。

Abstract: Transmission System Operators routinely use transmission switching as a tool to manage congestion and ensure system security. Motivated by sub-transmission operations at RTE, this paper considers the Optimal Transmission Switching with De-energization (OTSD), which captures potential loss of connectivity (and therefore localized blackout) following loss of transmission elements. While directly relevant to real-life operations, this problem has received very little attention in the literature. The paper proposes a new mixed-integer linear programming formulation for OTSD that represents post-contingency loss of connectivity without requiring additional binary variables. This new formulation provides the foundation for a fast, iterative heuristic algorithm. Computational experiments confirms that state-of-the-art optimization solvers struggle to solve the extensive formulation of OTSD, often failing to find even trivial solutions within reasonable time. In contrast, numerical results demonstrate the efficiency of the proposed heuristic, which finds high-quality feasible solutions 100-1000x faster than using Gurobi.

</details>


### [583] [Novel Stability Criteria for Discrete and Hybrid Systems via Ramanujan Inner Products](https://arxiv.org/abs/2511.13690)
*Shyam Kamal,Sunidhi Pandey,Thach Ngoc Dinh*

Main category: eess.SY

TL;DR: 提出基于拉马努金内积和范数的新框架，用于混合和离散时间系统的稳定性分析，替代传统欧几里得度量。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得度量在系统稳定性分析中存在局限性，需要探索基于数论概念的新数学框架来提供更强的鲁棒性保证。

Method: 引入拉马努金内积和相应范数，利用拉马努金求和及其与数论概念的独特关系建立新的ε-δ稳定性条件。

Result: 理论结果得到严格证明，数值模拟验证了所提方法的有效性，揭示了系统稳定性与系统动力学算术特性之间的基本联系。

Conclusion: 拉马努金内积框架为混合和离散时间系统稳定性分析提供了新的替代方法，具有增强的鲁棒性保证和深刻的数论联系。

Abstract: This paper introduces a Ramanujan inner product and its corresponding norm, establishing a novel framework for the stability analysis of hybrid and discrete-time systems as an alternative to traditional Euclidean metrics. We establish new $ε$-$δ$ stability conditions that utilize the unique properties of Ramanujan summations and their relationship with number-theoretic concepts. The proposed approach provides enhanced robustness guarantees and reveals fundamental connections between system stability and arithmetic properties of the system dynamics. Theoretical results are rigorously proven, and simulation results on numerical examples are presented to validate the efficacy of the proposed approach.

</details>


### [584] [Resilient Distribution Network Planning against Dynamic Malicious Power Injection Attacks](https://arxiv.org/abs/2511.13698)
*Hampei Sasahara,Tatsuya Yamada,Jun-ichi Imura,Henrik Sandberg*

Main category: eess.SY

TL;DR: 提出了一种基于配电网规划的电网级防御策略，通过将安全要求纳入现有规划方法，确保在终端用户节点遭受恶意功率注入时电压偏差保持在可接受范围内。


<details>
  <summary>Details</summary>
Motivation: 由于集成多种网络组件，支持可再生能源双向功率交换的主动配电网容易受到网络攻击，需要增强攻击弹性。

Method: 将原始无限维双层优化问题转化为可处理的混合整数线性规划形式，利用线性动态系统理论和图论，发现攻击严重性仅取决于变电站到目标节点的累积电抗，从而将问题简化为有限维问题。

Result: 在54节点配电网基准测试中，所提方法显著提升了29.3%的弹性，而经济成本仅增加2.1%。

Conclusion: 该方法通过将安全要求整合到配电网规划中，有效增强了网络对网络攻击的弹性，且成本增加有限。

Abstract: Active distribution networks facilitating bidirectional power exchange with renewable energy resources are susceptible to cyberattacks due to integration of a diverse array of cyber components. This study introduces a grid-level defense strategy aimed at enhancing attack resiliency based on distribution network planning. Our proposed framework imposes a security requirement into existing planning methodologies, ensuring that voltage deviation from its rated value remains within a tolerable range against dynamically and maliciously injected power at end-user nodes. Unfortunately, the formulated problem in its original form is intractable because it is an infinite-dimensional bi-level optimization problem over a function space. To address this complexity, we develop an equivalent transformation into a tractable form as mixed-integer linear program leveraging linear dynamical system theory and graph theory. Notably, our investigation reveals that the severity of potential attacks hinges solely on the cumulative reactances over the path from the substation to the targeted node, thereby reducing the problem to a finite-dimensional problem. Further, the bi-level optimization problem is reduced to a single-level optimization problem by using a technique utilized in solving the shortest path problem. Through extensive numerical simulations conducted on a 54-node distribution network benchmark, our proposed methodology exhibits a noteworthy 29.3% enhancement in the resiliency, with a mere 2.1% uptick in the economic cost.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [585] [Impact by design: translating Lead times in flux into an R handbook with code](https://arxiv.org/abs/2511.12763)
*Harrison Katz*

Main category: q-fin.ST

TL;DR: 将Lead times in flux的核心思想转化为R语言实践手册，实现端到端的预订提前期分布变化测量和预测误差分析


<details>
  <summary>Details</summary>
Motivation: 将理论方法转化为可操作的实践工具，使用R语言实现完整的分析流程，便于实际应用和复现

Method: 使用归一化L1距离测量预订提前期分布的月度变化，相对于同比和2018年固定参考点进行跟踪，并建立发散度与剩余时间范围对拾取预测相对误差的界限关系

Result: 在R中实现了端到端的分析流程，包括最小数据模式、可运行脚本、模拟示例和预设评估计划，所有结果使用合成数据确保完全可复现

Conclusion: 成功将理论方法转化为实用的R语言工具包，为预订提前期分析和预测误差评估提供了可操作的解决方案

Abstract: This commentary translates the central ideas in Lead times in flux into a practice ready handbook in R. The original article measures change in the full distribution of booking lead times with a normalized L1 distance and tracks that divergence across months relative to year over year and to a fixed 2018 reference. It also provides a bound that links divergence and remaining horizon to the relative error of pickup forecasts. We implement these ideas end to end in R, using a minimal data schema and providing runnable scripts, simulated examples, and a prespecified evaluation plan. All results use synthetic data so the exposition is fully reproducible without reference to proprietary sources.

</details>
