<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 69]
- [cs.AI](#cs.AI) [Total: 18]
- [math.OC](#math.OC) [Total: 14]
- [econ.EM](#econ.EM) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]
- [eess.SY](#eess.SY) [Total: 10]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [stat.ML](#stat.ML) [Total: 12]
- [cs.LG](#cs.LG) [Total: 90]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts](https://arxiv.org/abs/2602.15843)
*Warren Johnson*

Main category: cs.CL

TL;DR: 该论文扩展了先前关于提示压缩的研究，验证了代码生成与推理任务的压缩阈值，揭示了"困惑度悖论"机制，并提出了任务感知自适应压缩算法TAAC，在保持96%质量的同时降低22%成本。


<details>
  <summary>Details</summary>
Motivation: 先前研究（Johnson, 2026）发现代码生成能容忍激进提示压缩（r>=0.6），而思维链推理会逐渐退化，但该研究仅限于HumanEval（164个问题），未验证"困惑度悖论"机制，且缺乏自适应算法。本文旨在填补这三个研究空白。

Method: 1）在6个代码基准和4个推理基准上验证压缩阈值的普适性；2）首次进行每令牌困惑度分析（n=723令牌），揭示"困惑度悖论"；3）提出TAAC（任务感知自适应压缩）算法，实现自适应压缩。

Result: 1）压缩阈值在不同语言和难度任务中普遍适用；2）发现代码语法令牌被保留（高困惑度），而数学问题中的数值被剪枝（低困惑度），签名注入使通过率提升34个百分点；3）TAAC实现22%成本降低和96%质量保持，优于固定比率压缩7%。

Conclusion: 该研究系统验证了提示压缩在不同任务中的效果，揭示了困惑度悖论机制，并提出了高效的自适应压缩算法TAAC，为实际应用中的提示压缩提供了理论基础和实践指导。

Abstract: In "Compress or Route?" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the "perplexity paradox" mechanism unvalidated, and provided no adaptive algorithm. This paper addresses all three gaps. First, we validate across six code benchmarks (HumanEval, MBPP, HumanEval+, MultiPL-E) and four reasoning benchmarks (GSM8K, MATH, ARC-Challenge, MMLU-STEM), confirming the compression threshold generalizes across languages and difficulties. Second, we conduct the first per-token perplexity analysis (n=723 tokens), revealing a "perplexity paradox": code syntax tokens are preserved (high perplexity) while numerical values in math problems are pruned despite being task-critical (low perplexity). Signature injection recovers +34 percentage points in pass rate (5.3% to 39.3%; Cohen's h=0.890). Third, we propose TAAC (Task-Aware Adaptive Compression), achieving 22% cost reduction with 96% quality preservation, outperforming fixed-ratio compression by 7%. MBPP validation (n=1,800 trials) confirms systematic variation: 3.6% at r=0.3 to 54.6% at r=1.0.

</details>


### [2] [Language Model Representations for Efficient Few-Shot Tabular Classification](https://arxiv.org/abs/2602.15844)
*Inwon Kang,Parikshit Ram,Yi Zhou,Horst Samulowitz,Oshani Seneviratne*

Main category: cs.CL

TL;DR: 提出TaRL方法，利用現有LLM的語義嵌入進行少樣本表格分類，無需專門模型或大量重新訓練


<details>
  <summary>Details</summary>
Motivation: 網路表格結構和語義異質性高，難以建立統一方法；LLM已廣泛部署於網路基礎設施，能否直接利用現有LLM進行表格分類，避免專門模型或大量重新訓練？

Method: TaRL方法：直接使用表格行的語義嵌入，提出兩個關鍵技術：1) 移除所有嵌入中的共同成分 2) 校準softmax溫度。使用簡單元學習器預測適當溫度

Result: 在低數據量(k≤32)的語義豐富表格中，性能可與最先進模型相媲美

Conclusion: 證明了利用現有LLM基礎設施進行網路表格理解的可行性，提供高效的語義驅動途徑

Abstract: The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information they contain. Meanwhile, Large language models (LLMs) are becoming an increasingly integral component of web infrastructure for tasks like semantic search. This raises a crucial question: can we leverage these already-deployed LLMs to classify structured data in web-native tables (e.g., product catalogs, knowledge base exports, scientific data portals), avoiding the need for specialized models or extensive retraining? This work investigates a lightweight paradigm, $\textbf{Ta}$ble $\textbf{R}$epresentation with $\textbf{L}$anguage Model~($\textbf{TaRL}$), for few-shot tabular classification that directly utilizes semantic embeddings of individual table rows. We first show that naive application of these embeddings underperforms compared to specialized tabular models. We then demonstrate that their potentials can be unlocked with two key techniques: removing the common component from all embeddings and calibrating the softmax temperature. We show that a simple meta-learner, trained on handcrafted features, can learn to predict an appropriate temperature. This approach achieves performance comparable to state-of-the-art models in low-data regimes ($k \leq 32$) of semantically-rich tables. Our findings demonstrate the viability of reusing existing LLM infrastructure for efficient semantics-driven pathway to reuse existing LLM infrastructure for Web table understanding.

</details>


### [3] [KD4MT: A Survey of Knowledge Distillation for Machine Translation](https://arxiv.org/abs/2602.15845)
*Ona de Gibert,Joseph Attieh,Timothee Mickus,Yves Scherrer,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 这篇综述论文系统回顾了知识蒸馏在机器翻译领域的应用，涵盖了105篇相关文献，分析了方法分类、应用场景、研究趋势，并指出了当前评估实践不统一、存在幻觉和偏见放大等风险，同时探讨了大语言模型对该领域的影响。


<details>
  <summary>Details</summary>
Motivation: 机器翻译领域中的知识蒸馏不仅作为模型压缩工具，更是一种通用的知识转移机制，能够影响监督方式、翻译质量和效率。然而该领域缺乏系统性综述，评估实践不统一，且存在潜在风险，需要进行全面梳理以指导未来研究。

Method: 1. 对105篇相关论文进行系统性综述；2. 从方法论贡献和实际应用两个维度对KD4MT文献进行分类；3. 进行定性和定量分析识别领域趋势；4. 提供具体场景下的KD方法选择指南；5. 建立公开数据库总结所调查KD方法的主要特征。

Result: 1. 识别了KD4MT领域的共同趋势和关键研究空白；2. 发现KD方法在MT中缺乏统一的评估实践；3. 揭示了KD应用于MT时可能增加幻觉和放大偏见等风险；4. 分析了LLMs如何重塑KD4MT领域；5. 提供了实用的方法选择指南和公开数据库资源。

Conclusion: 知识蒸馏在机器翻译中具有重要价值，不仅用于模型压缩，更是有效的知识转移机制。该领域需要更统一的评估标准，并需关注潜在风险。大语言模型的出现正在重塑KD4MT研究格局，未来研究应结合公开数据库资源进行更系统的探索。

Abstract: Knowledge Distillation (KD) as a research area has gained a lot of traction in recent years as a compression tool to address challenges related to ever-larger models in NLP. Remarkably, Machine Translation (MT) offers a much more nuanced take on this narrative: in MT, KD also functions as a general-purpose knowledge transfer mechanism that shapes supervision and translation quality as well as efficiency.
  This survey synthesizes KD for MT (KD4MT) across 105 papers (through October 1, 2025). We begin by introducing both MT and KD for non-experts, followed by an overview of the standard KD approaches relevant to MT applications. Subsequently, we categorize advances in the KD4MT literature based on (i) their methodological contributions and (ii) their practical applications. Our qualitative and quantitative analyses identify common trends in the field and highlight key research gaps as well as the absence of unified evaluation practice for KD methods in MT. We further provide practical guidelines for selecting a KD method in concrete settings and highlight potential risks associated with the application of KD to MT such as increased hallucination and bias amplification. Finally, we discuss the role of LLMs in re-shaping the KD4MT field. To support further research, we complement our survey with a publicly available database summarizing the main characteristics of the surveyed KD methods and a glossary of key terms.

</details>


### [4] [Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs](https://arxiv.org/abs/2602.15846)
*Xinyu Gao,Shaonan Wang,Nai Ding*

Main category: cs.CL

TL;DR: 提出GTCA方法，通过门控树交叉注意力分支读取预计算的句法块记忆，增强解码器LLM的句法鲁棒性而不影响原有能力


<details>
  <summary>Details</summary>
Motivation: 解码器大语言模型在广泛任务上表现良好，但对细微语法扰动敏感，影响下游推理可靠性。直接注入显式句法结构会干扰预训练模型能力

Method: 引入检查点兼容的门控树交叉注意力分支，读取预计算的句法块记忆，保持主干架构不变。使用令牌更新掩码和分阶段训练控制结构更新的范围和时机

Result: 在不同基准测试和Transformer主干上，GTCA显著增强了句法鲁棒性，超越了持续训练基线，同时不影响多项选择QA性能或常识推理

Conclusion: GTCA提供了一种实用的检查点兼容方法，能够构建更句法鲁棒的解码器LLM，平衡了句法鲁棒性和原有模型能力

Abstract: Decoder-only large language models achieve strong broad performance but are brittle to minor grammatical perturbations, undermining reliability for downstream reasoning. However, directly injecting explicit syntactic structure into an existing checkpoint can interfere with its pretrained competence. We introduce a checkpoint-compatible gated tree cross-attention (GTCA) branch that reads precomputed constituency chunk memory while leaving backbone architecture unchanged. Our design uses a token update mask and staged training to control the scope and timing of structural updates. Across benchmarks and Transformer backbones, GTCA strengthens syntactic robustness beyond continued-training baselines without compromising Multiple-Choice QA performance or commonsense reasoning, providing a practical checkpoint-compatible route to more syntax-robust decoder-only LLMs.

</details>


### [5] [Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models](https://arxiv.org/abs/2602.15847)
*Pranav Bhandari,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 研究发现LLM人格特质控制方向存在几何依赖性，无法完全独立控制，正交化处理虽能消除几何重叠但无法消除跨特质行为影响


<details>
  <summary>Details</summary>
Motivation: 当前LLM人格控制方法通常假设人格特质可以独立控制，但这一假设是否成立需要验证。研究旨在分析大五人格特质控制方向之间的几何关系，检验独立控制的可行性。

Method: 研究分析了LLaMA-3-8B和Mistral-8B两个模型家族的人格控制向量，应用了从无约束方向到软硬正交化的多种几何条件化方案，考察人格控制方向的几何关系。

Result: 人格控制方向表现出显著的几何依赖性：控制一个特质会一致性地引起其他特质的变化，即使线性重叠被明确移除。硬正交化虽能强制几何独立性，但无法消除跨特质行为影响，且会降低控制强度。

Conclusion: LLM中的人格特质占据一个轻微耦合的子空间，限制了完全独立的特质控制。人格特质控制方向存在固有的几何依赖性，无法实现完全解耦。

Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big Five personality steering directions. We study steering vectors extracted from two model families (LLaMA-3-8B and Mistral-8B) and apply a range of geometric conditioning schemes, from unconstrained directions to soft and hard orthonormalisation. Our results show that personality steering directions exhibit substantial geometric dependence: steering one trait consistently induces changes in others, even when linear overlap is explicitly removed. While hard orthonormalisation enforces geometric independence, it does not eliminate cross-trait behavioural effects and can reduce steering strength. These findings suggest that personality traits in LLMs occupy a slightly coupled subspace, limiting fully independent trait control.

</details>


### [6] [Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling](https://arxiv.org/abs/2602.15848)
*Andrius Matšenas,Anet Lello,Tõnis Lees,Hans Peep,Kim Lilii Tamm*

Main category: cs.CL

TL;DR: 验证大型语言模型作为人格评估的动态替代方案，与传统问卷方法相比具有中等收敛效度，用户感知准确性相当


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否可以作为传统问卷式人格评估的动态替代方案，解决传统方法可能存在的局限性

Method: 采用被试内实验设计（N=33），比较基于引导式LLM对话得出的五大人格得分与金标准IPIP-50问卷结果，同时测量用户感知准确性

Result: 显示中等收敛效度（r=0.38-0.58），尽责性、开放性和神经质得分在两种方法间统计等效，宜人性和外向性存在显著差异，参与者认为LLM生成的人格档案与传统问卷结果同样准确

Conclusion: 对话式AI为传统心理测量学提供了有前景的新方法，但需要对特定特质进行校准

Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while also measuring user-perceived accuracy. Results indicate moderate convergent validity (r=0.38-0.58), with Conscientiousness, Openness, and Neuroticism scores statistically equivalent between methods. Agreeableness and Extraversion showed significant differences, suggesting trait-specific calibration is needed. Notably, participants rated LLM-generated profiles as equally accurate as traditional questionnaire results. These findings suggest conversational AI offers a promising new approach to traditional psychometrics.

</details>


### [7] [Preference Optimization for Review Question Generation Improves Writing Quality](https://arxiv.org/abs/2602.15849)
*Karun Sharma,Vidushee Vats,Shengzhi Li,Yuxiang Wang,Zhongtian Sun,Prayag Tiwari*

Main category: cs.CL

TL;DR: IntelliReward奖励模型和IntelliAsk问题生成模型，通过改进LLM生成的同行评审问题质量，使其更深入、更有证据支持，超越现有表面级提问方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的同行评审方法生成的问题过于表面化，超过50%的问题token来自论文第一页，缺乏深入、证据支持的提问，需要改进评审问题质量。

Method: 开发IntelliReward奖励模型（基于冻结自回归LLM和可训练多头transformer），结合Decoupled Clip和DAPO策略优化，训练IntelliAsk问题生成模型，使其符合人类专家对努力程度、证据支持和基础性的标准。

Result: IntelliAsk相比Qwen3-32B基础模型在多个基准测试中均有提升：MuSR推理任务准确率从64.7提高到68.3，WritingBench写作评估从8.07提高到8.31，表明评审问题质量与更广泛的能力相关。

Conclusion: IntelliReward和IntelliAsk显著提高了LLM生成同行评审问题的质量，建立了自动评估基准，并开源了实现、专家偏好标注和模型，为评审问题质量评估提供了新标准。

Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.

</details>


### [8] [Large Language Models for Assisting American College Applications](https://arxiv.org/abs/2602.15850)
*Zhengliang Liu,Weihang You,Peng Shu,Junhao Chen,Yi Pan,Hanqi Jiang,Yiwei Li,Zhaojun Ding,Chao Cao,Xinliang Li,Yifan Zhou,Ruidong Zhang,Shaochen Xu,Wei Ruan,Huaqin Zhao,Dajiang Zhu,Tianming Liu*

Main category: cs.CL

TL;DR: EZCollegeApp是一个基于大语言模型的系统，帮助高中生应对复杂的美国大学申请流程，通过结构化申请表格、基于权威招生文档生成建议答案，同时保持人类对最终回答的完全控制。


<details>
  <summary>Details</summary>
Motivation: 美国大学申请过程存在诸多挑战：学生需要面对分散的招生政策、重复且有条件的申请表格、以及需要交叉参考多个来源的模糊问题。这些复杂性给学生带来了沉重的负担。

Method: 系统采用"映射优先"范式，将表格理解与答案生成分离，确保在不同申请门户间保持一致的推理。系统整合了官方招生网站的文档摄取、检索增强的问答功能，以及人机协作的聊天机器人界面，在申请字段旁显示建议但不会自动提交。

Result: 系统架构包括数据管道、内部表示、安全和隐私措施，并通过自动化测试和人工质量评估进行了评估。源代码已在GitHub上公开发布以促进更广泛的影响。

Conclusion: EZCollegeApp通过LLM技术为高中生提供大学申请辅助，在保持人类控制的同时简化复杂的申请流程，其开源性质有助于该工作的广泛影响。

Abstract: American college applications require students to navigate fragmented admissions policies, repetitive and conditional forms, and ambiguous questions that often demand cross-referencing multiple sources. We present EZCollegeApp, a large language model (LLM)-powered system that assists high-school students by structuring application forms, grounding suggested answers in authoritative admissions documents, and maintaining full human control over final responses. The system introduces a mapping-first paradigm that separates form understanding from answer generation, enabling consistent reasoning across heterogeneous application portals. EZCollegeApp integrates document ingestion from official admissions websites, retrieval-augmented question answering, and a human-in-the-loop chatbot interface that presents suggestions alongside application fields without automated submission. We describe the system architecture, data pipeline, internal representations, security and privacy measures, and evaluation through automated testing and human quality assessment. Our source code is released on GitHub (https://github.com/ezcollegeapp-public/ezcollegeapp-public) to facilitate the broader impact of this work.

</details>


### [9] [NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey](https://arxiv.org/abs/2602.15866)
*Dhiman Goswami,Jai Kruthunz Naveen Kumar,Sanchari Das*

Main category: cs.CL

TL;DR: 论文提出NLP-PRISM框架，系统评估社交媒体NLP中的隐私风险，分析6个NLP任务中的隐私研究差距，发现隐私保护微调会导致模型性能下降1%-23%，并揭示模型效用与隐私保护之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 社交媒体NLP处理包含个人身份信息、行为线索和元数据的内容，存在监控、画像和定向广告等隐私风险，需要系统评估这些风险。

Method: 回顾203篇同行评审论文，提出NLP-PRISM框架，从数据收集、预处理、可见性、公平性、计算风险和法规合规六个维度评估隐私漏洞，并分析六个NLP任务的隐私覆盖情况。

Result: Transformer模型F1分数0.58-0.84，隐私保护微调下性能下降1%-23%；隐私研究在六个NLP任务中存在显著差距；模型效用降低2%-9%，成员推断攻击AUC 0.81，属性推断攻击准确率0.75。

Conclusion: 需要加强匿名化、隐私感知学习和公平性驱动训练，以实现社交媒体环境中的伦理NLP应用。

Abstract: Natural Language Processing (NLP) is integral to social media analytics but often processes content containing Personally Identifiable Information (PII), behavioral cues, and metadata raising privacy risks such as surveillance, profiling, and targeted advertising. To systematically assess these risks, we review 203 peer-reviewed papers and propose the NLP Privacy Risk Identification in Social Media (NLP-PRISM) framework, which evaluates vulnerabilities across six dimensions: data collection, preprocessing, visibility, fairness, computational risk, and regulatory compliance. Our analysis shows that transformer models achieve F1-scores ranging from 0.58-0.84, but incur a 1% - 23% drop under privacy-preserving fine-tuning. Using NLP-PRISM, we examine privacy coverage in six NLP tasks: sentiment analysis (16), emotion detection (14), offensive language identification (19), code-mixed processing (39), native language identification (29), and dialect detection (24) revealing substantial gaps in privacy research. We further found a (reduced by 2% - 9%) trade-off in model utility, MIA AUC (membership inference attacks) 0.81, AIA accuracy 0.75 (attribute inference attacks). Finally, we advocate for stronger anonymization, privacy-aware learning, and fairness-driven training to enable ethical NLP in social media contexts.

</details>


### [10] [Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey](https://arxiv.org/abs/2602.15851)
*David Y. Liu,Aditya Joshi,Paul Dawson*

Main category: cs.CL

TL;DR: 该论文调查了叙事理论与大语言模型在自然语言处理中的结合应用，提出了叙事研究的分类法，并指出了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索叙事理论与大语言模型在自然语言处理中的交叉应用，为系统化的叙事研究提供理论基础和分类框架。

Method: 通过调查分析现有研究，提出叙事研究的分类法，分析叙事数据集、任务、理论、NLP流程以及提示和微调的方法趋势。

Result: 发现LLMs能够轻松连接NLP流程与抽象叙事概念，促进跨学科合作；但缺乏统一的叙事任务定义和基准，模型比较困难。

Conclusion: 未来应聚焦于：定义和改进基于理论的个体叙事属性指标；进行大规模理论驱动的文学/社会/文化分析；创建验证或完善叙事理论的实验，而非追求单一的"叙事质量"通用基准。

Abstract: Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrative studies, and proposes a taxonomy for ongoing efforts that reflect established distinctions in narratology. We discover patterns in the following: narrative datasets and tasks, narrative theories and NLP pipeline and methodological trends in prompting and fine-tuning. We highlight how LLMs enable easy connections of NLP pipelines with abstract narrative concepts and opportunities for interdisciplinary collaboration. Challenges remain in attempts to work towards any unified definition or benchmark of narrative related tasks, making model comparison difficult. For future directions, instead of the pursuit of a single, generalised benchmark for 'narrative quality', we believe that progress benefits more from efforts that focus on the following: defining and improving theory-based metrics for individual narrative attributes to incrementally improve model performance; conducting large-scale, theory-driven literary/social/cultural analysis; and creating experiments where outputs can be used to validate or refine narrative theories. This work provides a contextual foundation for more systematic and theoretically informed narrative research in NLP by providing an overview to ongoing research efforts and the broader narrative studies landscape.

</details>


### [11] [CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content](https://arxiv.org/abs/2602.15871)
*Diletta Abbonato*

Main category: cs.CL

TL;DR: CheckIfExist是一个开源网络工具，通过多源验证（CrossRef、Semantic Scholar、OpenAlex）实时检测AI生成的虚假参考文献，填补了现有工具在参考文献真实性验证方面的空白。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在学术工作流程中的普及带来了参考文献幻觉（生成看似合理但实际不存在的引用）的问题，这一问题已出现在NeurIPS和ICLR等顶级会议论文中，迫切需要自动化验证机制。

Method: 采用级联验证架构，结合字符串相似度算法，对CrossRef、Semantic Scholar和OpenAlex三个学术数据库进行多源验证，计算多维匹配置信度分数。

Result: 开发了一个开源网络工具，支持单条参考文献验证和BibTeX条目批量处理，能够在几秒内返回验证后的APA引用和可导出的BibTeX记录。

Conclusion: CheckIfExist填补了现有参考文献管理工具和商业幻觉检测服务之间的空白，提供了一个免费、开源的实时参考文献真实性验证解决方案。

Abstract: The proliferation of large language models (LLMs) in academic workflows has introduced unprecedented challenges to bibliographic integrity, particularly through reference hallucination -- the generation of plausible but non-existent citations. Recent investigations have documented the presence of AI-hallucinated citations even in papers accepted at premier machine learning conferences such as NeurIPS and ICLR, underscoring the urgency of automated verification mechanisms. This paper presents "CheckIfExist", an open-source web-based tool designed to provide immediate verification of bibliographic references through multi-source validation against CrossRef, Semantic Scholar, and OpenAlex scholarly databases. While existing reference management tools offer bibliographic organization capabilities, they do not provide real-time validation of citation authenticity. Commercial hallucination detection services, though increasingly available, often impose restrictive usage limits on free tiers or require substantial subscription fees. The proposed tool fills this gap by employing a cascading validation architecture with string similarity algorithms to compute multi-dimensional match confidence scores, delivering instant feedback on reference authenticity. The system supports both single-reference verification and batch processing of BibTeX entries through a unified interface, returning validated APA citations and exportable BibTeX records within seconds.

</details>


### [12] [Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints](https://arxiv.org/abs/2602.15852)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.CL

TL;DR: 该研究提出了一种轻量级审计流程，用于识别和抑制临床NLP模型中的时间泄漏问题，确保模型在真实部署中的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床NLP模型在出院规划中显示出潜力，但基于临床记录的模型容易受到时间和词汇泄漏的影响，这些泄漏会编码未来的临床决策并夸大预测性能。这种行为对真实世界部署构成重大风险，可能导致过度自信或时间无效的预测，从而破坏临床工作流程并危及患者安全。

Method: 研究提出了一个轻量级审计流程，将可解释性集成到模型开发过程中，在最终训练前识别和抑制泄漏倾向的信号。以择期脊柱手术后次日出院预测为案例研究，评估审计如何影响预测行为、校准和安全相关的权衡。

Result: 结果显示，经过审计的模型表现出更保守和更好校准的概率估计，减少了对出院相关词汇线索的依赖。这些模型在时间有效性、校准和行为稳健性方面表现更好。

Conclusion: 部署就绪的临床NLP系统应优先考虑时间有效性、校准和行为稳健性，而不是乐观的性能指标。审计流程有助于构建在时间泄漏约束下安全可部署的临床NLP系统。

Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical decisions and inflate apparent predictive performance. Such behavior poses substantial risks for real-world deployment, where overconfident or temporally invalid predictions can disrupt clinical workflows and compromise patient safety. This study focuses on system-level design choices required to build safe and deployable clinical NLP under temporal leakage constraints. We present a lightweight auditing pipeline that integrates interpretability into the model development process to identify and suppress leakage-prone signals prior to final training. Using next-day discharge prediction after elective spine surgery as a case study, we evaluate how auditing affects predictive behavior, calibration, and safety-relevant trade-offs. Results show that audited models exhibit more conservative and better-calibrated probability estimates, with reduced reliance on discharge-related lexical cues. These findings emphasize that deployment-ready clinical NLP systems should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance.

</details>


### [13] [A Lightweight Explainable Guardrail for Prompt Safety](https://arxiv.org/abs/2602.15853)
*Md Asiful Islam,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 提出轻量级可解释护栏方法LEG，用于不安全提示分类，通过多任务学习同时进行提示分类和解释生成，使用合成数据训练，在多个数据集上取得SOTA性能且模型更小。


<details>
  <summary>Details</summary>
Motivation: 现有不安全提示分类方法通常模型较大且缺乏可解释性，需要开发轻量级、可解释的方法来帮助用户理解分类决策，同时提高模型的泛化能力。

Method: 采用多任务学习架构，联合学习提示分类器和解释分类器；使用新颖策略生成合成可解释性数据以抵消LLM确认偏差；设计新损失函数结合交叉熵、焦点损失和不确定性加权来捕捉全局解释信号。

Result: 在三个数据集上，LEG在域内和域外均达到或优于当前最先进方法的性能，包括提示分类和可解释性任务，且模型尺寸显著小于现有方法。

Conclusion: LEG是一种有效的轻量级可解释护栏方法，能够在保持高性能的同时提供解释，有助于用户理解不安全提示分类决策，并承诺公开模型和标注数据集。

Abstract: We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LEG is trained using synthetic data for explainability, which is generated using a novel strategy that counteracts the confirmation biases of LLMs. Lastly, LEG's training process uses a novel loss that captures global explanation signals and combines cross-entropy and focal losses with uncertainty-based weighting. LEG obtains equivalent or better performance than the state-of-the-art for both prompt classification and explainability, both in-domain and out-of-domain on three datasets, despite the fact that its model size is considerably smaller than current approaches. If accepted, we will release all models and the annotated dataset publicly.

</details>


### [14] [Surgical Activation Steering via Generative Causal Mediation](https://arxiv.org/abs/2602.16080)
*Aruna Sankaranarayanan,Amir Zur,Atticus Geiger,Dylan Hadfield-Menell*

Main category: cs.CL

TL;DR: GCM是一种定位和控制语言模型长文本生成行为的方法，通过识别注意力头等模型组件来干预特定概念（如拒绝、奉承、风格转换）的表达。


<details>
  <summary>Details</summary>
Motivation: 当语言模型的长文本响应中某些行为（如拒绝、奉承、风格）分散在多个token中时，需要找到有效的干预位置来控制这些行为。

Method: 首先构建对比输入和响应的数据集，然后量化单个模型组件如何中介对比概念，选择最强的中介组件进行控制。

Result: GCM成功定位了长文本响应中表达的概念，在三个任务（拒绝、奉承、风格转换）和三个语言模型上，使用稀疏注意力头进行控制时，始终优于基于相关探针的基线方法。

Conclusion: GCM为定位和控制语言模型的长文本响应提供了一种有效方法。

Abstract: Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.

</details>


### [15] [Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization](https://arxiv.org/abs/2602.15854)
*Jingyi Xu,Xingyu Ren,Zhiqiang You,Yumeng Zhang,Zhoupeng Shou*

Main category: cs.CL

TL;DR: GOPO是一个分层强化学习框架，通过专家代理优化对话轨迹级别的目标偏好，客服代理生成严格对齐策略的响应，显著提升任务导向对话系统的长时程任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在任务导向对话系统中的训练方法通常依赖token级别的似然或偏好优化，这些方法与长时程任务成功的对齐效果不佳，需要更好的优化框架。

Method: 提出目标导向偏好优化(GOPO)框架，采用分层强化学习，将策略规划与响应生成解耦：专家代理在对话轨迹级别优化多轮目标偏好，客服代理严格按选定策略生成响应。

Result: 在Mgshop数据集上，GOPO相比PPO和Memento分别提升TSE指标7.7%和10.3%；14B模型训练的GOPO比Qwen-235B和GPT-5.2分别高出2.7%和1.5%的TSE；在其他数据集上也有一致改进。

Conclusion: GOPO为商业场景中的任务导向对话系统建立了新范式，专家代理在长时程优化中起关键作用，代码和数据集将公开。

Abstract: Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.

</details>


### [16] [Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications](https://arxiv.org/abs/2602.16201)
*Sanket Badhe,Deep Shah,Nehal Kathrotia*

Main category: cs.CL

TL;DR: 本文系统分析了大型语言模型中长尾知识的定义、丢失机制、缓解方法及其社会影响，提出了统一的概念框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练数据中存在严重的幂律分布问题，导致对低频、领域特定、文化和时效性知识的处理能力不足，这些失败模式尚未得到充分研究和系统分析。

Method: 提出了一个结构化的分析框架，从四个互补维度综合先前研究：长尾知识的定义方式、训练和推理过程中的丢失或扭曲机制、缓解这些失败的技术干预措施，以及这些失败对公平性、问责制、透明度和用户信任的影响。

Result: 建立了统一的概念框架，揭示了现有评估实践如何掩盖尾部行为，并使得罕见但后果严重的失败难以追责。同时识别了与隐私、可持续性和治理相关的开放挑战。

Conclusion: 本文为理解长尾知识在部署的语言模型系统中如何被定义、丢失、评估和表现提供了统一的概念框架，指出了未来研究的关键方向。

Abstract: Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.
  We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.

</details>


### [17] [Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective](https://arxiv.org/abs/2602.15856)
*Yunhao Liu,Zian Jia,Xinyu Gao,Kanjun Xu,Yun Xiong*

Main category: cs.CL

TL;DR: SeleCom提出基于选择器的软压缩框架，通过查询条件信息选择而非全压缩，解决RAG中上下文过长和冗余检索问题，在保持性能的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有软上下文压缩方法采用类似自编码器的全压缩方式，强制编码器压缩所有文档信息而不考虑与查询的相关性，导致性能不如未压缩的RAG，存在不可行性（与LLM生成行为冲突）和非必要性（稀释任务相关信息密度）两个根本限制。

Method: SeleCom框架重新定义编码器角色为查询条件信息选择器，采用仅解码器的选择器架构，通过课程学习在大规模、多样且难度分级的合成QA数据集上进行训练，实现查询相关的信息选择而非全压缩。

Result: SeleCom显著优于现有软压缩方法，在性能上与未压缩基线相当或更优，同时减少33.8%~84.6%的计算和延迟开销。

Conclusion: 通过将软压缩范式从全压缩转向查询条件信息选择，SeleCom有效解决了RAG的可扩展性问题，在保持性能的同时大幅提升效率，为大规模RAG应用提供了实用解决方案。

Abstract: Retrieval-Augmented Generation (RAG) effectively grounds Large Language Models (LLMs) with external knowledge and is widely applied to Web-related tasks. However, its scalability is hindered by excessive context length and redundant retrievals. Recent research on soft context compression aims to address this by encoding long documents into compact embeddings, yet they often underperform non-compressed RAG due to their reliance on auto-encoder-like full-compression that forces the encoder to compress all document information regardless of relevance to the input query.
  In this work, we conduct an analysis on this paradigm and reveal two fundamental limitations: (I) Infeasibility, full-compression conflicts with the LLM's downstream generation behavior; and (II) Non-necessity: full-compression is unnecessary and dilutes task-relevant information density. Motivated by these insights, we introduce SeleCom, a selector-based soft compression framework for RAG that redefines the encoder's role as query-conditioned information selector. The selector is decoder-only and is trained with a massive, diverse and difficulty-graded synthetic QA dataset with curriculum learning.
  Extensive experiments show that SeleCom significantly outperforms existing soft compression approaches and achieves competitive or superior performance to non-compression baselines, while reducing computation and latency by 33.8%~84.6%.

</details>


### [18] [Multi-source Heterogeneous Public Opinion Analysis via Collaborative Reasoning and Adaptive Fusion: A Systematically Integrated Approach](https://arxiv.org/abs/2602.15857)
*Yi Liu*

Main category: cs.CL

TL;DR: CRAF框架通过结合传统特征方法和LLM，解决多源异构平台舆情分析中的语义对齐、特征融合和跨平台适应问题，显著提升主题聚类和情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 多源异构平台（如微博、抖音、快手等）的舆情分析面临结构差异、语义变化和平台特定偏见的挑战，需要一种能够有效整合不同来源信息并适应新平台的方法。

Method: 提出CRAF框架，包含四个创新：跨平台协作注意力模块、分层自适应融合机制、联合优化策略以及多模态提取能力（整合OCR、ASR和视觉情感分析）。

Result: 在三个多平台数据集上，CRAF实现主题聚类ARI 0.76（比最佳基线提升4.1%），情感分析F1-score 0.84（提升3.8%），理论分析显示泛化边界更紧，新平台标注数据需求减少75%。

Conclusion: CRAF框架有效解决了多源异构平台舆情分析的挑战，通过系统整合传统方法和LLM，实现了更好的语义对齐、特征融合和跨平台适应性，为多平台舆情分析提供了有效解决方案。

Abstract: The analysis of public opinion from multiple heterogeneous sources presents significant challenges due to structural differences, semantic variations, and platform-specific biases. This paper introduces a novel Collaborative Reasoning and Adaptive Fusion (CRAF) framework that systematically integrates traditional feature-based methods with large language models (LLMs) through a structured multi-stage reasoning mechanism. Our approach features four key innovations: (1) a cross-platform collaborative attention module that aligns semantic representations while preserving source-specific characteristics, (2) a hierarchical adaptive fusion mechanism that dynamically weights features based on both data quality and task requirements, (3) a joint optimization strategy that simultaneously learns topic representations and sentiment distributions through shared latent spaces, and (4) a novel multimodal extraction capability that processes video content from platforms like Douyin and Kuaishou by integrating OCR, ASR, and visual sentiment analysis. Theoretical analysis demonstrates that CRAF achieves a tighter generalization bound with a reduction of O(sqrt(d log K / m)) compared to independent source modeling, where d is feature dimensionality, K is the number of sources, and m is sample size. Comprehensive experiments on three multi-platform datasets (Weibo-12, CrossPlatform-15, NewsForum-8) show that CRAF achieves an average topic clustering ARI of 0.76 (4.1% improvement over best baseline) and sentiment analysis F1-score of 0.84 (3.8% improvement). The framework exhibits strong cross-platform adaptability, reducing the labeled data requirement for new platforms by 75%.

</details>


### [19] [State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models](https://arxiv.org/abs/2602.15858)
*Annie Wong,Aske Plaat,Thomas Bäck,Niki van Stein,Anna V. Kononova*

Main category: cs.CL

TL;DR: 研究探索大型语言模型在动态环境中状态表示的关键设计因素，发现轨迹摘要、自然语言表示和文本空间编码能提升性能，但当前模型在长时程任务中仍显脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从静态推理转向动态环境，其成功取决于在推理时与环境交互的能力。状态表示是一个未被充分探索的关键因素，研究旨在系统分析状态表示的设计选择如何影响模型在顺序决策任务中的表现。

Method: 在保持模型参数固定的条件下，系统性地改变三个关键方面：(1) 状态粒度（长形式 vs 摘要），(2) 结构（自然语言 vs 符号表示），(3) 空间基础（纯文本 vs 图像或文本地图编码），并在顺序决策基准上进行测试。

Result: 轨迹摘要通过减少噪声和稳定长时程推理提升性能；自然语言表示在不同模型中最稳健，而结构化编码主要对具有强代码或结构化输出先验的模型有帮助；文本空间编码比图像输入更有效，其优势来自构建过程本身迫使模型进行空间推理。

Conclusion: 状态表示的设计选择是影响性能的决定性因素，独立于信息可用性本身。然而，即使改进表示，当前的大语言模型和多模态模型在长时程任务中仍然脆弱，特别是在需要综合信息管理多个子任务以实现目标时。

Abstract: As large language models (LLMs) move from static reasoning tasks toward dynamic environments, their success depends on the ability to navigate and respond to an environment that changes as they interact at inference time. An underexplored factor in these settings is the representation of the state. Holding model parameters fixed, we systematically vary three key aspects: (1) state granularity (long form versus summary), (2) structure (natural language versus symbolic), and (3) spatial grounding (text-only versus images or textual map encodings) across sequential decision-making benchmarks. We find that trajectory summarisation improves performance by reducing noise and stabilising long-horizon reasoning. Second, natural language representations are the most robust across models, whereas structured encodings help mainly for models with strong code or structured output priors, such as JSON schemas. Third, while image-inputs show some benefit, text-based spatial encodings prove most effective. This advantage stems not from the spatial information itself, but from the act of construction, which compels the model to perform the spatial reasoning that static input does not elicit. Overall, we demonstrate that design choices for representing state are a decisive factor in performance, distinct from the availability of information itself. We note, however, that even with improved representations, current LLMs and VLMs remain brittle over long horizons, particularly when they must synthesise information to manage multiple subtasks to reach a goal.

</details>


### [20] [From Transcripts to AI Agents: Knowledge Extraction, RAG Integration, and Robust Evaluation of Conversational AI Assistants](https://arxiv.org/abs/2602.15859)
*Krittin Pachtrachai,Petmongkon Pornpichitsuwan,Wachiravit Modecrua,Touchapon Kraisingkorn*

Main category: cs.CL

TL;DR: 提出一个从历史通话记录构建对话AI助手的端到端框架，在房地产和招聘领域实现约30%通话自主处理，近乎完美的事实准确性和拒绝行为


<details>
  <summary>Details</summary>
Motivation: 面向客户的行业构建可靠对话AI助手面临挑战：嘈杂对话数据、碎片化知识、需要准确人工转接，特别是在依赖实时信息的领域

Method: 1) 使用简化PIPA框架对通话记录评分筛选高质量交互；2) 用LLM从精选记录提取结构化知识作为RAG唯一来源；3) 通过系统提示调优控制助手行为；4) 使用基于记录的模拟器评估覆盖度、事实准确性和人工转接行为

Result: 在房地产和招聘领域，助手自主处理约30%通话，实现近乎完美的事实准确性和拒绝行为，在对抗测试中表现出强鲁棒性

Conclusion: 该框架成功从历史通话记录构建可靠对话AI助手，在依赖实时信息的挑战性领域实现有效自动化，展示了基于记录的方法在构建行业专用助手方面的潜力

Abstract: Building reliable conversational AI assistants for customer-facing industries remains challenging due to noisy conversational data, fragmented knowledge, and the requirement for accurate human hand-off - particularly in domains that depend heavily on real-time information. This paper presents an end-to-end framework for constructing and evaluating a conversational AI assistant directly from historical call transcripts. Incoming transcripts are first graded using a simplified adaptation of the PIPA framework, focusing on observation alignment and appropriate response behavior, and are filtered to retain only high-quality interactions exhibiting coherent flow and effective human agent responses. Structured knowledge is then extracted from curated transcripts using large language models (LLMs) and deployed as the sole grounding source in a Retrieval-Augmented Generation (RAG) pipeline. Assistant behavior is governed through systematic prompt tuning, progressing from monolithic prompts to lean, modular, and governed designs that ensure consistency, safety, and controllable execution. Evaluation is conducted using a transcript-grounded user simulator, enabling quantitative measurement of call coverage, factual accuracy, and human escalation behavior. Additional red teaming assesses robustness against prompt injection, out-of-scope, and out-of-context attacks. Experiments are conducted in the Real Estate and Specialist Recruitment domains, which are intentionally challenging and currently suboptimal for automation due to their reliance on real-time data. Despite these constraints, the assistant autonomously handles approximately 30 percents of calls, achieves near-perfect factual accuracy and rejection behavior, and demonstrates strong robustness under adversarial testing.

</details>


### [21] [Reranker Optimization via Geodesic Distances on k-NN Manifolds](https://arxiv.org/abs/2602.15860)
*Wen G. Gong*

Main category: cs.CL

TL;DR: Maniscope是一种基于几何的重新排序方法，通过计算k-NN流形上的测地距离来改进RAG中的文档排序，比现有方法更快更高效。


<details>
  <summary>Details</summary>
Motivation: 当前基于交叉编码器或大语言模型的神经重新排序方法计算资源需求大、延迟高（3-5秒/查询），不适合实时RAG部署。

Method: 提出Maniscope方法，在检索到的文档候选项上构建k-NN流形，计算测地距离，结合全局余弦相似度和局部流形几何来捕捉语义结构。

Result: 在8个BEIR基准数据集上，Maniscope在三个最难数据集上优于HNSW图基线（NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3），速度快3.2倍（4.7ms vs 14.8ms）。相比交叉编码器重新排序器，在精度相差2%以内的情况下延迟降低10-45倍。

Conclusion: Maniscope为实时RAG部署提供了实用的替代方案，具有亚10毫秒延迟和开源计划，平衡了准确性和效率。

Abstract: Current neural reranking approaches for retrieval-augmented generation (RAG) rely on cross-encoders or large language models (LLMs), requiring substantial computational resources and exhibiting latencies of 3-5 seconds per query. We propose Maniscope, a geometric reranking method that computes geodesic distances on k-nearest neighbor (k-NN) manifolds constructed over retrieved document candidates. This approach combines global cosine similarity with local manifold geometry to capture semantic structure that flat Euclidean metrics miss. Evaluating on eight BEIR benchmark datasets (1,233 queries), Maniscope outperforms HNSW graph-based baseline on the three hardest datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 3.2x faster (4.7 ms vs 14.8 ms average). Compared to cross-encoder rerankers, Maniscope achieves within 2% accuracy at 10-45x lower latency. On TREC-COVID, LLM-Reranker provides only +0.5% NDCG@3 improvement over Maniscope at 840x higher latency, positioning Maniscope as a practical alternative for real-time RAG deployment. The method requires O(N D + M^2 D + M k log k) complexity where M << N , enabling sub-10 ms latency. We plan to release Maniscope as open-source software.

</details>


### [22] [CAST: Achieving Stable LLM-based Text Analysis for Data Analytics](https://arxiv.org/abs/2602.15861)
*Jinxiang Xie,Zihao Li,Wei He,Rui Ding,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: CAST框架通过算法提示和稳定思考机制提升LLM在表格数据分析中的输出稳定性，同时保持或提高输出质量


<details>
  <summary>Details</summary>
Motivation: 当前LLM在表格数据的摘要和标注任务中存在输出稳定性不足的问题，无法满足数据分析的高标准要求

Method: 提出CAST框架，结合算法提示（约束有效推理路径）和思考-说话机制（强制中间显式承诺），并引入CAST-S和CAST-T稳定性评估指标

Result: 在多个公开基准测试和LLM骨干网络上，CAST在所有基线中实现最佳稳定性，稳定性得分提升高达16.2%，同时保持或提高输出质量

Conclusion: CAST框架有效解决了LLM在表格数据分析中的稳定性问题，为数据科学应用提供了可靠的文本分析工具

Abstract: Text analysis of tabular data relies on two core operations: \emph{summarization} for corpus-level theme extraction and \emph{tagging} for row-level labeling. A critical limitation of employing large language models (LLMs) for these tasks is their inability to meet the high standards of output stability demanded by data analytics. To address this challenge, we introduce \textbf{CAST} (\textbf{C}onsistency via \textbf{A}lgorithmic Prompting and \textbf{S}table \textbf{T}hinking), a framework that enhances output stability by constraining the model's latent reasoning path. CAST combines (i) Algorithmic Prompting to impose a procedural scaffold over valid reasoning transitions and (ii) Thinking-before-Speaking to enforce explicit intermediate commitments before final generation. To measure progress, we introduce \textbf{CAST-S} and \textbf{CAST-T}, stability metrics for bulleted summarization and tagging, and validate their alignment with human judgments. Experiments across publicly available benchmarks on multiple LLM backbones show that CAST consistently achieves the best stability among all baselines, improving Stability Score by up to 16.2\%, while maintaining or improving output quality.

</details>


### [23] [Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation](https://arxiv.org/abs/2602.15862)
*Guoshan Liu,Bin Zhu,Yian Li,Jingjing Chen,Chong-Wah Ngo,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: 提出一个语义接地框架，通过预测和验证动作与食材作为内部上下文来提升食谱生成的语义准确性，结合监督微调和强化微调，在Recipe1M数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在从食物图像生成食谱时，尽管在词汇评分（如BLEU、ROUGE）上表现良好，但输出常包含语义错误的动作或食材，需要提升语义保真度。

Method: 提出两阶段框架：1）监督微调（SFT）使用动作推理数据集和食材语料库建立基础准确性；2）强化微调（RFT）采用频率感知奖励来改进长尾动作预测和食材泛化；3）语义置信度评分与修正模块（SCSR）进一步过滤和校正预测。

Result: 在Recipe1M数据集上实现了最先进的性能，并显著提高了语义保真度。

Conclusion: 提出的语义接地框架有效解决了食谱生成中的语义错误问题，通过预测和验证内部上下文、结合SFT和RFT以及SCSR模块，显著提升了生成食谱的语义准确性。

Abstract: Recent advances in Multimodal Large Language Models (MLMMs) have enabled recipe generation from food images, yet outputs often contain semantically incorrect actions or ingredients despite high lexical scores (e.g., BLEU, ROUGE). To address this gap, we propose a semantically grounded framework that predicts and validates actions and ingredients as internal context for instruction generation. Our two-stage pipeline combines supervised fine-tuning (SFT) with reinforcement fine-tuning (RFT): SFT builds foundational accuracy using an Action-Reasoning dataset and ingredient corpus, while RFT employs frequency-aware rewards to improve long-tail action prediction and ingredient generalization. A Semantic Confidence Scoring and Rectification (SCSR) module further filters and corrects predictions. Experiments on Recipe1M show state-of-the-art performance and markedly improved semantic fidelity.

</details>


### [24] [Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning](https://arxiv.org/abs/2602.15863)
*Daehoon Gwak,Minseo Jung,Junwoo Park,Minho Park,ChaeHun Park,Junha Hyung,Jaegul Choo*

Main category: cs.CL

TL;DR: LLMs通过自我生成示例提升推理能力，但研究发现真正优势来自生成过程本身而非生成的示例。


<details>
  <summary>Details</summary>
Motivation: 尽管研究表明LLMs通过自生成few-shot示例能达到与人工策划示例相当的效果，但其背后的机制尚不明确，难以确定何时及如何有效应用该技术。

Method: 在多种推理密集型任务和LLM架构上，系统评估三种提示策略：零样本提示、集成提示（LLM在单一提示中创建并解决问题）、解耦提示（重用自生成示例但排除其创建上下文）。

Result: 集成提示始终优于零样本和解耦提示，而解耦提示仅比零样本略有提升。注意力分析显示集成与解耦提示的注意力模式存在显著差异。

Conclusion: 自生成提示的优势来自问题创建过程本身，而非生成的示例，这为设计更有效的提示策略提供了重要见解。

Abstract: Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.

</details>


### [25] [Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?](https://arxiv.org/abs/2602.15867)
*Berry Gerrits*

Main category: cs.CL

TL;DR: LLMs在经典文字冒险游戏Zork中的表现评估显示，即使最佳模型也只能完成不到10%的游戏内容，表明当前LLMs在元认知和复杂问题解决方面存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 评估当代大型语言模型在复杂问题解决和推理能力方面的表现，使用Zork文字冒险游戏作为测试平台，因为其对话式结构能有效测试LLMs对自然语言描述的理解和适当行动序列的生成能力。

Method: 测试ChatGPT、Claude和Gemini等主流专有模型在Zork游戏中的表现，使用最小化和详细化两种指令设置，以游戏得分作为主要衡量指标，并进行定性分析模型的推理过程。

Result: 所有测试模型平均完成度低于10%，最佳模型Claude Opus 4.5仅获得约75/350分；详细指令和"扩展思考"功能均未带来改进；模型表现出无法反思自身思维、策略不一致、无法从历史尝试中学习等根本性局限。

Conclusion: 当前LLMs在文字冒险游戏领域表现出显著的元认知能力和问题解决能力局限，对其推理能力的本质和范围提出了质疑，表明需要进一步研究提升模型的复杂推理和自适应学习能力。

Abstract: In this positioning paper, we evaluate the problem-solving and reasoning capabilities of contemporary Large Language Models (LLMs) through their performance in Zork, the seminal text-based adventure game first released in 1977. The game's dialogue-based structure provides a controlled environment for assessing how LLM-based chatbots interpret natural language descriptions and generate appropriate action sequences to succeed in the game. We test the performance of leading proprietary models - ChatGPT, Claude, and Gemini - under both minimal and detailed instructions, measuring game progress through achieved scores as the primary metric. Our results reveal that all tested models achieve less than 10% completion on average, with even the best-performing model (Claude Opus 4.5) reaching only approximately 75 out of 350 possible points. Notably, providing detailed game instructions offers no improvement, nor does enabling ''extended thinking''. Qualitative analysis of the models' reasoning processes reveals fundamental limitations: repeated unsuccessful actions suggesting an inability to reflect on one's own thinking, inconsistent persistence of strategies, and failure to learn from previous attempts despite access to conversation history. These findings suggest substantial limitations in current LLMs' metacognitive abilities and problem-solving capabilities within the domain of text-based games, raising questions about the nature and extent of their reasoning capabilities.

</details>


### [26] [Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning](https://arxiv.org/abs/2602.15868)
*Magnus Boman*

Main category: cs.CL

TL;DR: 该论文提出使用确定性多带图灵机形式化LLM交互，将LLM管道分解为多个组件（输入字符、token、词汇表、模型参数、激活值、概率分布、输出文本），以精确定位故障模式并分析技术（如思维链提示）的机制和局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在看似简单的任务上表现出故障模式，现有方法（如几何隐喻）缺乏严谨的分析框架。需要一种形式化模型来精确定位故障模式，理解LLM内部工作机制，并为技术改进提供理论基础。

Method: 提出确定性多带图灵机形式化模型，将LLM管道分解为7个组件：输入字符带、token带、词汇表带、模型参数带、激活值带、概率分布带、输出文本带。通过这种分解，可以精确定位故障发生在哪个阶段，并分析不同技术（如思维链提示）的工作机制。

Result: 该形式化模型能够：1）精确定位故障模式到特定管道阶段（如token化阶段如何掩盖字符级结构导致计数任务失败）；2）解释思维链提示等技术的机制（通过在输出带上外部化计算）；3）揭示这些技术的根本局限性；4）提供可证伪的严谨分析框架。

Conclusion: 提出的多带图灵机形式化为LLM交互提供了严谨、可证伪的分析框架，能够精确定位故障模式，理解现有技术的机制和局限性，为LLM的改进和理论分析提供了新工具，补充了经验性缩放定律。

Abstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.

</details>


### [27] [Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches](https://arxiv.org/abs/2602.15869)
*Noopur Zambare,Kiana Aghakasiri,Carissa Lin,Carrie Ye,J. Ross Mitchell,Mohamed Abdalla*

Main category: cs.CL

TL;DR: 本文系统评估了不同规模语言模型在临床去标识化任务上的表现，发现较小模型在保持性能的同时显著降低推理成本，并通过多语言数据微调在跨文化、跨性别场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在临床去标识化任务上表现出色，但先前研究未系统评估其在格式、文化和性别间的泛化能力，且大模型部署成本高，需要探索更实用、公平且高效的解决方案。

Method: 系统评估了微调Transformer模型（BERT、ClinicalBERT、ModernBERT）、小规模LLMs（Llama 1-8B、Qwen 1.5-7B）和大规模LLMs（Llama-70B、Qwen-72B）。引入BERT-MultiCulture-DEID模型集，基于MIMIC数据并使用多语言变体标识符进行微调。

Result: 较小模型在保持可比性能的同时显著降低推理成本；通过有限数据微调，小模型在识别汉语、印地语、西班牙语、法语、孟加拉语及英语变体和性别化名称方面优于大模型；发布了BERT-MultiCulture-DEID模型集。

Conclusion: 首次量化了去标识化任务中效率与泛化能力的权衡，为公平高效的临床去标识化提供了实用路径，表明小模型在多文化场景中具有更好的部署价值。

Abstract: Large language models (LLMs) have shown strong performance on clinical de-identification, the task of identifying sensitive identifiers to protect privacy. However, previous work has not examined their generalizability between formats, cultures, and genders. In this work, we systematically evaluate fine-tuned transformer models (BERT, ClinicalBERT, ModernBERT), small LLMs (Llama 1-8B, Qwen 1.5-7B), and large LLMs (Llama-70B, Qwen-72B) at de-identification. We show that smaller models achieve comparable performance while substantially reducing inference cost, making them more practical for deployment. Moreover, we demonstrate that smaller models can be fine-tuned with limited data to outperform larger models in de-identifying identifiers drawn from Mandarin, Hindi, Spanish, French, Bengali, and regional variations of English, in addition to gendered names. To improve robustness in multi-cultural contexts, we introduce and publicly release BERT-MultiCulture-DEID, a set of de-identification models based on BERT, ClinicalBERT, and ModernBERT, fine-tuned on MIMIC with identifiers from multiple language variants. Our findings provide the first comprehensive quantification of the efficiency-generalizability trade-off in de-identification and establish practical pathways for fair and efficient clinical de-identification.
  Details on accessing the models are available at: https://doi.org/10.5281/zenodo.18342291

</details>


### [28] [VDLM: Variable Diffusion LMs via Robust Latent-to-Text Rendering](https://arxiv.org/abs/2602.15870)
*Shuhui Qu*

Main category: cs.CL

TL;DR: VDLM提出了一种模块化的变量扩散语言模型，将语义规划与文本渲染分离，通过潜在空间的迭代优化和嵌入空间的后训练，在多步推理任务上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型从左到右解码且不可逆，限制了多步推理过程中的修正能力。需要一种能够支持迭代细化和修订的推理框架

Method: 1) 使用LLaDA风格的掩码扩散在语义变量嵌入上进行迭代细化；2) 通过嵌入空间奖励和值的轨迹感知优化进行后训练；3) 使用Vec2Text渲染器将规划嵌入转换回文本；4) 引入嵌入扰动以增强解码器对规划器噪声的鲁棒性

Result: 在九个涵盖通用推理、数学和代码的基准测试中，VDLM在预训练阶段具有竞争力，在长文本生成任务的后训练中显著优于其他基线方法

Conclusion: 嵌入空间后训练和鲁棒的潜在到文本渲染对于扩散语言建模是有效的，为多步推理提供了更好的迭代优化能力

Abstract: Autoregressive language models decode left-to-right with irreversible commitments, limiting revision during multi-step reasoning. We propose \textbf{VDLM}, a modular variable diffusion language model that separates semantic planning from text rendering. VDLM applies LLaDA-style masked diffusion over semantic variable embeddings to enable iterative refinement in latent space, then post-trains the planner with trajectory-aware optimization using embedding-space rewards and values, avoiding text decoding inside the RL loop. To convert planned embeddings back to text, we use a \textbf{Vec2Text} renderer and introduce \textbf{embedding perturbations} to robustify decoding under planner noise. Across nine benchmarks spanning general reasoning, math, and code, VDLM is competitive in pre-training and yields substantial post-training improvements on long-form generation tasks, outperforming other baselines. These results highlight the effectiveness of embedding-space post-training and robust latent-to-text rendering for diffusion language modeling.

</details>


### [29] [P-RAG: Prompt-Enhanced Parametric RAG with LoRA and Selective CoT for Biomedical and Multi-Hop QA](https://arxiv.org/abs/2602.15874)
*Xingda Lyu,Gongfu Lyu,Zitai Yan,Yuxin Jiang*

Main category: cs.CL

TL;DR: 本文提出Prompt-Enhanced Parametric RAG (P-RAG)，一种结合参数化知识、检索证据、CoT提示和LoRA微调的混合架构，在生物医学问答任务上显著优于标准RAG方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖静态训练数据，检索增强生成(RAG)虽然能获取外部知识但仍受限于知识库质量。本文旨在探索改进RAG性能的方法，特别是在生物医学领域。

Method: 提出P-RAG混合架构，整合LLM参数化知识和检索证据，采用Chain-of-Thought提示和LoRA微调。使用LLaMA-3.2-1B-Instruct模型，在PubMedQA和2WikiMultihopQA数据集上评估三种RAG变体：标准RAG、DA-RAG和P-RAG。

Result: P-RAG在PubMedQA上F1分数比标准RAG提高10.47个百分点(93.33% vs 82.86%)；在2WikiMultihopQA上总体分数几乎翻倍(33.44% vs 17.83%)，在Compare子集达到44.03%。CoT提示显著提升多跳推理能力。

Conclusion: P-RAG在生物医学问答任务上表现出准确、可扩展和上下文自适应的潜力，实现了PubMedQA和2WikiMultihopQA上的最先进结果，为RAG架构提供了有效改进方案。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities but remain limited by their reliance on static training data. Retrieval-Augmented Generation (RAG) addresses this constraint by retrieving external knowledge during inference, though it still depends heavily on knowledge base quality. To explore potential improvements, we evaluated three RAG variants-Standard RAG, DA-RAG, and our proposed Prompt-Enhanced Parametric RAG (P-RAG), a hybrid architecture that integrates parametric knowledge within the LLM and retrieved evidence, guided by Chain-of-Thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning-on both general and biomedical datasets. Using LLaMA-3.2-1B-Instruct fine-tuned via LoRA, we evaluate on PubMedQA and 2WikiMultihopQA. P-RAG outperforms Standard RAG on PubMedQA by 10.47 percentage points in F1 (93.33% vs. 82.86%; 12.64% relative). On 2WikiMultihopQA, P-RAG nearly doubles the overall score vs. Standard RAG (33.44% vs. 17.83%) and achieves 44.03% on the Compare subset (with 42.74% Bridge, 21.84% Inference, 8.60% Compose). CoT prompting substantially improves multi-hop reasoning but yields mixed results for simpler, single-hop queries. These findings underscore P-RAG's potential for accurate, scalable, and contextually adaptive biomedical question answering. Our contributions include: (1) LoRA-based fine-tuning of LLaMA-3.2-1B-Instruct for biomedical QA, (2) introduction of P-RAG with Chain-of-Thought prompting, and (3) state-of-the-art results on PubMedQA and 2WikiMultihopQA.

</details>


### [30] [Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity](https://arxiv.org/abs/2602.15894)
*Haihui Pan,Yuzhong Hong,Shaoke Lv,Junwei Bao,Hongfei Jiang,Yang Song*

Main category: cs.CL

TL;DR: 提出QEMPO方法，在保证质量的同时提升LLM输出多样性，通过质量约束的熵最大化策略优化实现性能与多样性的平衡


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法虽然提升了LLM输出质量，但降低了输出多样性；而现有提升多样性的方法往往以性能下降为代价，需要一种能同时保证质量和多样性的方法

Method: 提出QEMPO方法，将对齐任务分解为质量和多样性两个分布，通过最大化策略输出熵同时保证输出质量，采用不同约束得到不同策略，并提出在线和离线训练方法

Result: 实验验证QEMPO在保持或超越RLHF性能的同时，显著提升了输出多样性

Conclusion: QEMPO成功解决了LLM对齐中质量与多样性的权衡问题，为提升模型输出多样性提供了有效方法

Abstract: Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.

</details>


### [31] [Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion](https://arxiv.org/abs/2602.15895)
*Pengcheng Zhou,Haochen Li,Zhiqiang Nie,JiaLe Chen,Qing Gong,Weizhen Zhang,Chun Yu*

Main category: cs.CL

TL;DR: CogitoRAG：受人类情景记忆启发的RAG框架，通过语义要点提取、知识图谱构建和认知检索机制，显著提升复杂知识整合与推理能力


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架中文本的离散表示会导致语义完整性损失和检索偏差，需要模拟人类认知记忆过程来改善知识检索效果

Method: 1. 离线索引阶段：从非结构化语料提取语义要点，构建包含实体、关系事实和记忆节点的多维知识图谱
2. 在线检索阶段：查询分解模块处理复杂查询，实体扩散模块进行关联检索，CogniRank算法融合扩散得分与语义相似度重排序
3. 最终以段落-记忆配对格式提供高密度信息支持给生成器

Result: 在五个主流QA基准测试和GraphBench的多任务生成任务中，CogitoRAG显著优于最先进的RAG方法，展示了在复杂知识整合和推理方面的卓越能力

Conclusion: CogitoRAG通过模拟人类认知记忆过程，有效解决了传统RAG框架的语义完整性损失问题，为复杂知识检索和推理任务提供了更有效的解决方案

Abstract: Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human episodic memory mechanism, we propose CogitoRAG, a RAG framework that simulates human cognitive memory processes. The core of this framework lies in the extraction and evolution of the Semantic Gist. During the offline indexing stage, CogitoRAG first deduces unstructured corpora into gist memory corpora, which are then transformed into a multi-dimensional knowledge graph integrating entities, relational facts, and memory nodes. In the online retrieval stage, the framework handles complex queries via Query Decomposition Module that breaks them into comprehensive sub-queries, mimicking the cognitive decomposition humans employ for complex information. Subsequently, Entity Diffusion Module performs associative retrieval across the graph, guided by structural relevance and an entity-frequency reward mechanism. Furthermore, we propose the CogniRank algorithm, which precisely reranks candidate passages by fusing diffusion-derived scores with semantic similarity. The final evidence is delivered to the generator in a passage-memory pairing format, providing high-density information support. Experimental results across five mainstream QA benchmarks and multi-task generation on GraphBench demonstrate that CogitoRAG significantly outperforms state-of-the-art RAG methods, showcasing superior capabilities in complex knowledge integration and reasoning.

</details>


### [32] [Every Little Helps: Building Knowledge Graph Foundation Model with Fine-grained Transferable Multi-modal Tokens](https://arxiv.org/abs/2602.15896)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Wen Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: TOFU是一个用于多模态知识图谱推理的token化基础模型，通过将结构、视觉和文本信息离散化为模态特定token，并采用分层融合架构，实现了在不同MMKG间的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有MMKGR方法多为转导式设计，学习数据集特定嵌入，难以泛化到新知识图谱。而知识图谱基础模型主要利用结构模式，忽略了丰富的多模态信号。需要解决跨KG泛化与多模态信息利用的不足。

Method: TOFU将结构、视觉和文本信息离散化为模态特定token，采用分层融合架构和混合消息机制处理这些token，获得可迁移特征用于MMKGR。

Result: 在17个转导式、归纳式和完全归纳式MMKG上的实验表明，TOFU持续优于强KGFM和MMKGR基线，在未见过的MMKG上表现出强大性能。

Conclusion: TOFU通过token化多模态信息和分层融合架构，实现了跨不同MMKG的强泛化能力，为多模态知识图谱推理提供了有效的解决方案。

Abstract: Multi-modal knowledge graph reasoning (MMKGR) aims to predict the missing links by exploiting both graph structure information and multi-modal entity contents. Most existing works are designed for a transductive setting, which learns dataset-specific embeddings and struggles to generalize to new KGs. Recent knowledge graph foundation models (KGFMs) improve cross-KG transfer, but they mainly exploit structural patterns and ignore rich multi-modal signals. We address these gaps by proposing a token-based foundation model (TOFU) for MMKGR, which exhibits strong generalization across different MMKGs. TOFU discretizes structural, visual, and textual information into modality-specific tokens. TOFU then employs a hierarchical fusion architecture with mixture-of-message mechanisms, aiming to process these tokens and obtain transferable features for MMKGR. Experimental results on 17 transductive, inductive, and fully-inductive MMKGs show that TOFU consistently outperforms strong KGFM and MMKGR baselines, delivering strong performance on unseen MMKGs.

</details>


### [33] [Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation](https://arxiv.org/abs/2602.15897)
*Xinguo Feng,Zhongkui Ma,Zihan Wang,Alsharif Abuadbba,Guangdong Bai*

Main category: cs.CL

TL;DR: GHOST是一种针对梯度反转攻击的防御机制，通过语义不同的替代令牌来解耦梯度、嵌入和令牌空间之间的关联，在保护隐私的同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度扰动防御方法（如噪声注入或梯度剪枝）往往效果有限，因为它们无法打破梯度、嵌入和令牌空间之间的语义相似性关联，导致梯度反转攻击仍然能够重建私有训练数据。

Method: GHOST包含两个步骤：搜索步骤使用多标准搜索过程识别语义不同的候选令牌；选择步骤选择最优的"影子令牌"，确保在保留原始令牌产生的内部输出对齐的同时，最小化对训练关键特征的干扰。

Result: 在BERT到Llama等多种模型架构和数据集上的评估表明，GHOST在保护隐私（恢复率低至1%）和保持效用（分类F1高达0.92，困惑度低至5.45）方面表现出色，在分类和生成任务中都能有效防御最先进的梯度反转攻击和自适应攻击。

Conclusion: GHOST通过令牌级混淆机制，成功解耦了梯度、嵌入和令牌空间之间的固有连接，为协作学习中的隐私保护提供了一种有效且实用的防御方案。

Abstract: Training and fine-tuning large-scale language models largely benefit from collaborative learning, but the approach has been proven vulnerable to gradient inversion attacks (GIAs), which allow adversaries to reconstruct private training data from shared gradients. Existing defenses mainly employ gradient perturbation techniques, e.g., noise injection or gradient pruning, to disrupt GIAs' direct mapping from gradient space to token space. However, these methods often fall short due to the retention of semantics similarity across gradient, embedding, and token spaces. In this work, we propose a novel defense mechanism named GHOST (gradient shield with obfuscated tokens), a token-level obfuscation mechanism that neutralizes GIAs by decoupling the inherent connections across gradient, embedding, and token spaces. GHOST is built upon an important insight: due to the large scale of the token space, there exist semantically distinct yet embedding-proximate tokens that can serve as the shadow substitutes of the original tokens, which enables a semantic disconnection in the token space while preserving the connection in the embedding and gradient spaces. GHOST comprises a searching step, which identifies semantically distinct candidate tokens using a multi-criteria searching process, and a selection step, which selects optimal shadow tokens to ensure minimal disruption to features critical for training by preserving alignment with the internal outputs produced by original tokens. Evaluation across diverse model architectures (from BERT to Llama) and datasets demonstrates the remarkable effectiveness of GHOST in protecting privacy (as low as 1% in recovery rate) and preserving utility (up to 0.92 in classification F1 and 5.45 in perplexity), in both classification and generation tasks against state-of-the-art GIAs and adaptive attack scenarios.

</details>


### [34] [MultiCube-RAG for Multi-hop Question Answering](https://arxiv.org/abs/2602.15898)
*Jimeng Shi,Wei Hu,Runchu Tian,Bowen Jin,Wonbin Kweon,SeongKu Kang,Yunfan Kang,Dingqi Ye,Sizhe Zhou,Shaowen Wang,Jiawei Han*

Main category: cs.CL

TL;DR: 提出MultiCube-RAG方法，使用基于本体的立方体结构建模多跳问答中的结构化信息，无需训练即可实现高效的多步推理和检索


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在多跳问答中难以准确捕捉结构化语义，图基RAG存在噪声和计算开销大的问题，单步检索无法满足多跳推理需求，而训练方法存在收敛不稳定和计算成本高的问题

Method: 设计基于本体的立方体结构，用多个正交维度建模主体、属性和关系；提出MultiCube-RAG训练免费方法，包含多个立方体进行多步推理和检索；将复杂查询沿立方体维度分解为简单子查询并顺序解决

Result: 在四个多跳问答数据集上的实验表明，MultiCube-RAG相比各种基线平均性能提升8.9%的响应准确率，同时具有更高的效率和内在可解释性

Conclusion: MultiCube-RAG通过立方体结构有效建模多跳问答中的结构化信息，无需训练即可实现准确、高效、可解释的多步推理和检索，为解决复杂问答问题提供了新思路

Abstract: Multi-hop question answering (QA) necessitates multi-step reasoning and retrieval across interconnected subjects, attributes, and relations. Existing retrieval-augmented generation (RAG) methods struggle to capture these structural semantics accurately, resulting in suboptimal performance. Graph-based RAGs structure such information in graphs, but the resulting graphs are often noisy and computationally expensive. Moreover, most methods rely on single-step retrieval, neglecting the need for multi-hop reasoning processes. Recent training-based approaches attempt to incentivize the large language models (LLMs) for iterative reasoning and retrieval, but their training processes are prone to unstable convergence and high computational overhead. To address these limitations, we devise an ontology-based cube structure with multiple and orthogonal dimensions to model structural subjects, attributes, and relations. Built on the cube structure, we propose MultiCube-RAG, a training-free method consisting of multiple cubes for multi-step reasoning and retrieval. Each cube specializes in modeling a class of subjects, so that MultiCube-RAG flexibly selects the most suitable cubes to acquire the relevant knowledge precisely. To enhance the query-based reasoning and retrieval, our method decomposes a complex multi-hop query into a set of simple subqueries along cube dimensions and conquers each of them sequentially. Experiments on four multi-hop QA datasets show that MultiCube-RAG improves response accuracy by 8.9% over the average performance of various baselines. Notably, we also demonstrate that our method performs with greater efficiency and inherent explainability.

</details>


### [35] [Doc-to-LoRA: Learning to Instantly Internalize Contexts](https://arxiv.org/abs/2602.15902)
*Rujikorn Charakorn,Edoardo Cetin,Shinnosuke Uesaka,Robert Tjarko Lange*

Main category: cs.CL

TL;DR: D2L：通过超网络元学习在单次前向传播中生成LoRA适配器，实现近似上下文蒸馏，减少长序列推理时的延迟和KV缓存内存消耗。


<details>
  <summary>Details</summary>
Motivation: 长输入序列对LLM的上下文学习、文档理解和多步推理至关重要，但Transformer的二次注意力成本导致推理内存密集且缓慢。传统上下文蒸馏需要昂贵的训练成本和延迟，不实用。

Method: 提出Doc-to-LoRA (D2L)，一个轻量级超网络，通过元学习在单次前向传播中执行近似上下文蒸馏。给定未见过的提示，D2L为目标LLM生成LoRA适配器，使后续查询无需重新消耗原始上下文。

Result: 在长上下文"大海捞针"任务中，D2L成功学习将上下文映射到存储针信息的适配器，在超过目标LLM原生上下文窗口4倍以上的序列长度上实现接近完美的零样本准确率。在真实世界QA数据集上，D2L在有限计算下优于标准上下文蒸馏，同时显著降低峰值内存消耗和更新延迟。

Conclusion: D2L可以促进LLM的快速适应，为频繁知识更新和个性化聊天行为开辟可能性，解决了长序列推理中的内存和延迟问题。

Abstract: Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.

</details>


### [36] [DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting](https://arxiv.org/abs/2602.15958)
*Md Mofijul Islam,Md Sirajus Salekin,Nivedha Balakrishnan,Vincil C. Bishop,Niharika Jain,Spencer Romo,Bob Strahan,Boyi Xie,Diego A. Socolinsky*

Main category: cs.CL

TL;DR: 本文提出了首个文档包分割基准数据集DocSplit，用于评估大语言模型在多页异构文档包中分离单个文档的能力，包含五个不同复杂度的数据集，并设计了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的文档理解经常需要处理包含多个文档拼接而成的异构多页文档包，但文档包分割这一基础任务尚未得到充分研究。当前视觉文档理解虽有进展，但缺乏系统性的评估框架和数据集来评估模型在复杂文档包分割任务上的能力。

Method: 1. 构建DocSplit基准数据集，包含五个不同复杂度的数据集，涵盖多样化的文档类型、布局和多模态设置；2. 形式化DocSplit任务，要求模型识别文档边界、分类文档类型并保持正确的页面顺序；3. 设计新的评估指标来系统评估文档包分割能力；4. 在数据集上进行多模态大语言模型的广泛实验。

Result: 实验结果显示当前模型在处理复杂文档分割任务时存在显著的性能差距。DocSplit基准数据集和提出的评估指标为文档理解能力的发展提供了系统性框架，特别是在法律、金融、医疗等文档密集型领域。

Conclusion: DocSplit是首个全面的文档包分割基准数据集，填补了文档理解领域的重要空白。该研究揭示了当前模型在复杂文档分割任务上的局限性，并为未来文档包处理研究提供了系统性的评估框架和开源数据集。

Abstract: Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.

</details>


### [37] [A Curious Class of Adpositional Multiword Expressions in Korean](https://arxiv.org/abs/2602.16023)
*Junghyun Min,Na-Rae Han,Jena D. Hwang,Nathan Schneider*

Main category: cs.CL

TL;DR: 该论文研究韩语中的后置词动词结构（PVCs）这类多功能多词表达，提出标注指南以支持韩语多词后置词研究并促进跨语言框架对齐。


<details>
  <summary>Details</summary>
Motivation: 韩语多词表达在PARSEME等跨语言标注框架中代表性不足，特别是韩语多词后置词缺乏系统分析、标注资源和与现有多语言框架的整合。

Method: 使用韩语维基百科数据，调查分析多种PVC表达，并与结构相似的非多词表达和轻动词结构进行对比，基于此分析提出标注指南。

Result: 对韩语后置词动词结构进行了系统分析，提出了支持韩语多词后置词研究的标注指南，有助于与跨语言框架对齐。

Conclusion: 该研究填补了韩语多词后置词研究的空白，提出的标注指南将为未来韩语多词表达研究和跨语言框架整合提供支持。

Abstract: Multiword expressions (MWEs) have been widely studied in cross-lingual annotation frameworks such as PARSEME. However, Korean MWEs remain underrepresented in these efforts. In particular, Korean multiword adpositions lack systematic analysis, annotated resources, and integration into existing multilingual frameworks. In this paper, we study a class of Korean functional multiword expressions: postpositional verb-based constructions (PVCs). Using data from Korean Wikipedia, we survey and analyze several PVC expressions and contrast them with non-MWEs and light verb constructions (LVCs) with similar structure. Building on this analysis, we propose annotation guidelines designed to support future work in Korean multiword adpositions and facilitate alignment with cross-lingual frameworks.

</details>


### [38] [CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill](https://arxiv.org/abs/2602.16054)
*Bradley McDanel,Steven Li,Harshit Khaitan*

Main category: cs.CL

TL;DR: 提出Answer-Informed Oracle评估token重要性，发现现有token-ranking方法在不同层间重要性估计不稳定，提出跨层注意力聚合(CLAA)方法，显著降低首token生成时间。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理中的prefill阶段存在计算瓶颈，现有token-ranking启发式方法通过选择性处理语义相关token来加速推理，但这些方法存在token重要性估计不稳定的问题，且在不同层间变化很大，缺乏独立于特定架构的评估方法。

Method: 引入Answer-Informed Oracle，通过测量生成答案对提示的注意力来定义ground-truth token重要性；基于诊断结果提出跨层注意力聚合(CLAA)，聚合各层注意力分数而非依赖单一层。

Result: Oracle揭示现有启发式方法在不同层间存在高方差，某些层排名会急剧下降；CLAA方法接近Oracle上界，相比Full KV Cache基线减少首token生成时间(TTFT)达39%。

Conclusion: 跨层注意力聚合是解决token重要性估计不稳定问题的简单有效方法，能显著提升长上下文LLM推理效率，为token-ranking方法提供了新的评估框架和改进方向。

Abstract: The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\% compared to the Full KV Cache baseline.

</details>


### [39] [Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs](https://arxiv.org/abs/2602.16085)
*Sean Trott,Samuel Taylor,Cameron Jones,James A. Michaelov,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 该研究通过测试41个开源语言模型在错误信念任务上的表现，发现34%的模型对知识状态敏感，但都无法完全解释人类效应。大模型表现更好，且模型行为可用于生成关于人类认知的新假设。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型的心理状态推理能力，既能促进对人类社会认知理论（如心理状态推理部分源于语言暴露）的理解，也能加深对语言模型本身能力的认识。当前研究多依赖少量闭源模型，限制了理论检验和模型评估的严谨性。

Method: 复制并扩展已发表研究，使用41个开源权重模型（来自不同模型家族）进行错误信念任务测试，评估模型对知识状态的敏感性，并比较模型与人类的表现差异。

Result: 34%的测试模型对隐含知识状态敏感；大模型敏感性更高且心理测量预测能力更强；人类和模型都表现出使用非事实动词（如"John thinks..."）时比间接提示时更倾向于归因错误信念的偏见；人类知识状态敏感性超过模型，但知识提示效应的大小在模型效应分布范围内。

Conclusion: 使用大规模开源语言模型样本有助于检验人类认知理论和评估模型能力。语言分布统计可以解释人类的知识提示效应，但无法完全解释知识状态的主要效应，表明语言暴露可能只是心理状态推理的部分来源。

Abstract: Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relatively small sample of closed-source LMs, limiting our ability to rigorously test psychological theories and evaluate LM capacities. Here, we replicate and extend published work on the false belief task by assessing LM mental state reasoning behavior across 41 open-weight models (from distinct model families). We find sensitivity to implied knowledge states in 34% of the LMs tested; however, consistent with prior work, none fully ``explain away'' the effect in humans. Larger LMs show increased sensitivity and also exhibit higher psychometric predictive power. Finally, we use LM behavior to generate and test a novel hypothesis about human cognition: both humans and LMs show a bias towards attributing false beliefs when knowledge states are cued using a non-factive verb (``John thinks...'') than when cued indirectly (``John looks in the...''). Unlike the primary effect of knowledge states, where human sensitivity exceeds that of LMs, the magnitude of the human knowledge cue effect falls squarely within the distribution of LM effect sizes-suggesting that distributional statistics of language can in principle account for the latter but not the former in humans. These results demonstrate the value of using larger samples of open-weight LMs to test theories of human cognition and evaluate LM capacities.

</details>


### [40] [Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities](https://arxiv.org/abs/2602.16093)
*Shankar Padmanabhan,Mustafa Omer Gul,Tanya Goyal*

Main category: cs.CL

TL;DR: DiSC是一种通过分割上下文进行蒸馏的持续知识适应方法，能在学习新知识的同时减轻对先前能力的遗忘


<details>
  <summary>Details</summary>
Motivation: 现有的后训练LLM只编码到截止日期的知识，需要持续适应。现有解决方案无法同时从适应文档语料库学习新知识并减轻对先前学习能力的遗忘

Method: 提出DiSC（Distillation via Split Contexts），基于上下文蒸馏的方法。通过将训练示例分割为不同片段，分别作为学生和教师的输入条件，最小化共享标记之间的KL散度，无需在训练期间进行显式生成步骤

Result: 在四个后训练模型和两个适应领域上进行实验。与先前的持续适应微调和蒸馏方法相比，DiSC在学习新知识和减轻对先前学习能力（如指令遵循、推理和事实知识）的遗忘方面始终表现出最佳权衡

Conclusion: DiSC是一种简单有效的持续知识适应方法，能够平衡新知识学习和旧能力保留

Abstract: Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.

</details>


### [41] [Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis](https://arxiv.org/abs/2602.16144)
*Rong Fu,Wenxin Zhang,Ziming Wang,Chunlei Meng,Jiaxuan Lu,Jiekai Wu,Kangan Qian,Hao Zhang,Simon Fong*

Main category: cs.CL

TL;DR: MBD框架通过结构化表示学习和可验证参数修改实现多模态情感分析中的选择性数据撤销，平衡隐私与性能


<details>
  <summary>Details</summary>
Motivation: 随着多模态系统处理更多敏感个人数据，用户需要能够选择性撤销特定数据模态以满足隐私合规和自主权要求

Method: 结合结构化表示学习和可验证参数修改管道，学习属性感知嵌入，使用生成器重建缺失通道，通过显著性候选选择和校准高斯更新实现可验证模态删除

Result: 在基准数据集上，MBD在不完整输入下保持强预测性能，提供实用的隐私-效用权衡，定位手术式遗忘作为完全重新训练的高效替代方案

Conclusion: MBD框架为多模态情感分析提供了可撤销的数据处理能力，通过可验证的删除机制满足隐私合规需求，同时保持系统性能

Abstract: As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request removal of modality-specific information. MBD learns property-aware embeddings and employs generator-based reconstruction to recover missing channels while preserving task-relevant signals. For deletion requests, the framework applies saliency-driven candidate selection and a calibrated Gaussian update to produce a machine-verifiable Modality Deletion Certificate. Experiments on benchmark datasets show that MBD achieves strong predictive performance under incomplete inputs and delivers a practical privacy-utility trade-off, positioning surgical unlearning as an efficient alternative to full retraining.

</details>


### [42] [Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution](https://arxiv.org/abs/2602.16154)
*Nithin Sivakumaran,Shoubin Yu,Hyunji Lee,Yue Zhang,Ali Payani,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: REMUL通过多智能体强化学习提升思维链的忠实性，让"说话者"生成可被"倾听者"执行的推理轨迹，在保持准确性的同时显著改善忠实性指标。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理有时不能忠实反映大语言模型的真实计算过程，这限制了其在解释模型决策方面的实用性。同时，优化忠实性和可解释性往往会降低任务性能，需要解决这种权衡问题。

Method: 提出REMUL（多倾听者推理执行）方法：1）说话者模型生成推理轨迹；2）截断该轨迹并传递给多个倾听者模型；3）倾听者"执行"该轨迹，继续推理得出答案；4）通过强化学习奖励说话者生成清晰可执行的推理；5）使用掩码监督微调进行正确性正则化，平衡忠实性与性能。

Result: 在多个推理基准测试（BIG-Bench Extra Hard、MuSR、ZebraLogicBench、FOLIO）上，REMUL显著提升了三个忠实性指标（提示归因、早期回答AOC、错误注入AOC），同时提高了准确性。分析表明这些增益在不同训练域中稳健，带来更好的可读性，并产生更短更直接的思维链。

Conclusion: REMUL通过多智能体强化学习框架有效解决了思维链忠实性与任务性能之间的权衡问题，使推理轨迹更加忠实、可解释且高效，为大语言模型的可解释性研究提供了新方向。

Abstract: Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who "execute" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.

</details>


### [43] [LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers](https://arxiv.org/abs/2602.16162)
*Peiqi Sui*

Main category: cs.CL

TL;DR: LLMs在创意写作中表现出"不确定性差距"问题：人类写作比模型输出具有显著更高的不确定性，而当前的AI对齐策略过度抑制了这种对文学创作至关重要的不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的创意写作常被批评为陈词滥调，文学理论认为不确定性是创意表达的必要条件，而现有的AI对齐策略却为了确保事实性和减少幻觉而抑制不确定性输出，这种矛盾限制了模型的创意表现。

Method: 通过信息论分析量化"不确定性差距"，在高质量故事数据集上对28个LLMs进行控制实验，比较人类写作与模型续写的不确定性水平，并分析指令微调、推理模型等不同模型类型的影响。

Result: 人类写作的不确定性显著高于模型输出；指令微调和推理模型比基础模型更严重地抑制不确定性；创意写作领域的不确定性差距比功能领域更明显；不确定性水平与写作质量强相关。

Conclusion: 实现人类水平的创意写作需要新的不确定性感知对齐范式，能够区分破坏性幻觉和文学丰富性所需的建设性模糊性。

Abstract: We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the "uncertainty gap" between human-authored stories and model-generated continuations. Through a controlled information-theoretic analysis of 28 LLMs on high-quality storytelling datasets, we demonstrate that human writing consistently exhibits significantly higher uncertainty than model outputs. We find that instruction-tuned and reasoning models exacerbate this trend compared to their base counterparts; furthermore, the gap is more pronounced in creative writing than in functional domains, and strongly correlates to writing quality. Achieving human-level creativity requires new uncertainty-aware alignment paradigms that can distinguish between destructive hallucinations and the constructive ambiguity required for literary richness.

</details>


### [44] [Beyond Learning: A Training-Free Alternative to Model Adaptation](https://arxiv.org/abs/2602.16189)
*Namkyung Yoon,Kyeonghyun Yoo,Wooyong Jung,Sanghong Kim,Hwangnam Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种无需额外训练的语言模型模块移植技术，通过识别特定任务下激活的局部模块，将其移植到目标模型中，实现即时功能改进。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型不断发展，但新版本有时会表现不如旧版本。现有改进方法通常需要大量计算资源，因此需要能够立即生效的替代方案。作者假设每个语言模型内部都有适合特定功能的局部模块。

Method: 首先通过激活分析识别在特定推理任务下表现出一致局部激活变化的模块。然后将针对特定任务适当激活的内部模块移植到目标模型中，实现无需额外训练或微调的即时功能改变。

Result: 在跨代模型移植实验中，移植激活选择的模块可以显著提升表现不佳的模型，达到目标基线的两倍，基于差距的恢复率超过100%。在基础模型与其指令调优版本之间的移植实验中，移植使表现不佳的模型向更强基线改进，最佳情况下达到目标基线的2.33倍，基于差距的恢复率达到100%。

Conclusion: 这项工作为语言模型中任务局部模块化提供了实证证据，并提出了模型移植这一新的研究领域，表明通过高度局部化的模块植入可以实现有意义的能力转移。

Abstract: Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.

</details>


### [45] [The Validity of Coreference-based Evaluations of Natural Language Understanding](https://arxiv.org/abs/2602.16200)
*Ian Porada*

Main category: cs.CL

TL;DR: 该论文分析了指代消解评估的局限性，提出新的事件可能性推理评估方法，发现当代语言模型在标准基准上表现良好但泛化能力有限


<details>
  <summary>Details</summary>
Motivation: 指代消解评估存在测量效度问题，包括定义争议和收敛效度不足，导致评估结果难以得出可泛化的结论

Method: 首先分析标准指代消解评估的局限性，然后提出并实施专注于测试系统推断事件相对可能性能力的新评估方法

Result: 当代语言模型在标准基准上表现优于早期基线系统，但对评估条件敏感，在轻微修改的上下文环境中泛化能力不足

Conclusion: 当前NLP范式既有优势也有局限，需要开发更好的评估方法和真正可泛化的系统

Abstract: In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.

</details>


### [46] [Are LLMs Ready to Replace Bangla Annotators?](https://arxiv.org/abs/2602.16241)
*Md. Najib Hasan,Touseef Hasan,Souvika Sarkar*

Main category: cs.CL

TL;DR: 研究发现LLM作为孟加拉语仇恨言论的零样本标注者存在显著偏见和不稳定性，模型规模增大并不保证标注质量提升，小型任务对齐模型反而表现更稳定


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地被用作自动化标注工具来扩展数据集创建，但它们在低资源和身份敏感设置中作为无偏见标注者的可靠性仍不清楚。特别是在孟加拉语仇恨言论检测这种人类标注者都难以达成一致的任务中，标注者偏见可能产生严重的下游后果。

Method: 使用统一的评估框架对17个LLM进行系统基准测试，研究它们作为孟加拉语仇恨言论的零样本标注者的行为。分析标注者偏见和模型判断的不稳定性。

Result: 研究发现LLM作为标注者存在显著偏见和实质性不稳定性。令人惊讶的是，模型规模的增加并不保证标注质量的改善——更小、更任务对齐的模型通常比更大的模型表现出更一致的行为。

Conclusion: 这些结果突显了当前LLM在低资源语言的敏感标注任务中的重要局限性，并强调了在部署前需要仔细评估的必要性。

Abstract: Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.

</details>


### [47] [Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation](https://arxiv.org/abs/2602.16290)
*Jonathan Mutal,Perla Al Almaoui,Simon Hengchen,Pierrette Bouillon*

Main category: cs.CL

TL;DR: Aladdin-FTI系统用于阿拉伯方言的生成和翻译，支持摩洛哥、埃及、巴勒斯坦、叙利亚和沙特方言，以及这些方言与现代标准阿拉伯语和英语之间的双向翻译。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言在自然语言处理研究中长期代表性不足，主要由于其非标准化和高变异性给计算建模带来挑战。大型语言模型的出现为解决这一问题提供了新途径，使阿拉伯语能够被建模为多中心语言而非单一系统。

Method: 提出了Aladdin-FTI系统，这是针对AMIYA共享任务的提交方案。该系统专门设计用于生成和翻译方言阿拉伯语，支持五种主要阿拉伯方言的文本生成，以及这些方言与现代标准阿拉伯语和英语之间的双向翻译。

Result: 开发了完整的Aladdin-FTI系统，代码和训练好的模型已公开可用，为阿拉伯方言处理提供了实用的工具。

Conclusion: 该研究通过Aladdin-FTI系统成功应对了阿拉伯方言处理的挑战，利用大型语言模型将阿拉伯语建模为多中心语言，为方言阿拉伯语的自然语言处理研究做出了贡献。

Abstract: Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.

</details>


### [48] [MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models](https://arxiv.org/abs/2602.16298)
*Martin Hyben,Sebastian Kula,Jan Cegin,Jakub Simko,Ivan Srba,Robert Moro*

Main category: cs.CL

TL;DR: MultiCW数据集：一个平衡的多语言基准，用于检测值得核查的声明，涵盖16种语言、7个主题领域和2种写作风格，包含123,722个样本，并提供了微调模型与零样本LLM的基准比较。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型正在改变媒体专业人士验证信息的方式，但在事实核查过程中的关键步骤——检测值得核查的声明方面，自动化支持仍然有限。现有数据集通常局限于单一语言或主题，缺乏多语言、多领域的平衡基准。

Method: 1. 引入MultiCW数据集：平衡的多语言基准，涵盖16种语言、7个主题领域和2种写作风格（非正式和正式文本），包含123,722个样本，平衡了值得核查和不值得核查的类别。
2. 引入分布外评估集：27,761个样本，涵盖4种额外语言，用于测试鲁棒性。
3. 基准测试：比较3种微调的多语言transformer模型与15种商业和开源LLM在零样本设置下的性能。

Result: 1. 微调模型在声明分类任务上始终优于零样本LLM。
2. 微调模型在语言、领域和风格方面表现出强大的分布外泛化能力。
3. MultiCW数据集为推进自动化事实核查提供了严谨的多语言资源，并支持系统比较微调模型与前沿LLM在值得核查声明检测任务上的性能。

Conclusion: MultiCW数据集填补了多语言、多领域值得核查声明检测基准的空白。研究表明，尽管LLM在信息验证方面有潜力，但针对特定任务微调的模型在检测值得核查声明方面表现更优，特别是在跨语言、跨领域的泛化能力上。该数据集为未来研究提供了系统评估的基础。

Abstract: Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.

</details>


### [49] [MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks](https://arxiv.org/abs/2602.16313)
*Zexue He,Yu Wang,Churan Zhi,Yuanzhe Hu,Tzu-Ping Chen,Lang Yin,Ze Chen,Tong Arthur Wu,Siru Ouyang,Zihan Wang,Jiaxin Pei,Julian McAuley,Yejin Choi,Alex Pentland*

Main category: cs.CL

TL;DR: MemoryArena：一个用于评估智能体在多会话环境中记忆能力的统一基准测试平台，揭示现有记忆基准测试的局限性


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将记忆和行动分开测试，无法反映真实场景中记忆与行动的紧密耦合关系。智能体需要在与环境互动中获取记忆，并依赖这些记忆解决未来任务。

Method: 提出MemoryArena基准测试平台，包含人工设计的智能体任务，具有明确相互依赖的子任务。智能体必须从早期行动和反馈中学习，将经验提炼为记忆，然后使用这些记忆指导后续行动。

Result: 在LoCoMo等现有长上下文记忆基准测试中表现接近饱和的智能体，在MemoryArena的智能体设置中表现不佳，暴露了当前记忆评估方法的差距。

Conclusion: 需要更全面的评估方法来测试智能体在真实多会话环境中的记忆能力，MemoryArena为此提供了一个统一的评估框架，支持网络导航、偏好约束规划、渐进信息搜索和顺序形式推理等多种任务。

Abstract: Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.

</details>


### [50] [Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents](https://arxiv.org/abs/2602.16346)
*Nivya Talokar,Ayush K Tarun,Murari Mandal,Maksym Andriushchenko,Antoine Bosselut*

Main category: cs.CL

TL;DR: STING是一个自动化红队框架，用于测试基于LLM的代理在多轮对话中执行非法任务的漏洞，通过逐步构建基于良性角色的非法计划，并使用评估代理跟踪阶段完成情况。


<details>
  <summary>Details</summary>
Motivation: 现有代理滥用基准主要测试单轮指令，缺乏衡量代理在多轮对话中如何帮助完成有害或非法任务的能力。需要评估真实部署场景中多轮、多语言交互的代理滥用风险。

Method: STING框架：1) 构建基于良性角色的逐步非法计划；2) 使用自适应后续问题迭代探测目标代理；3) 使用评估代理跟踪阶段完成情况；4) 将多轮红队建模为首次越狱时间随机变量进行分析。

Result: 在AgentHarm场景中，STING的非法任务完成率显著高于单轮提示和适应工具使用代理的聊天式多轮基线。在多语言评估中，攻击成功率和非法任务完成率在低资源语言中并不一致增加，与常见聊天机器人发现不同。

Conclusion: STING提供了一种实用的方法来评估和压力测试代理在真实部署场景中的滥用情况，这些场景本质上是多轮且通常是多语言的，有助于更好地理解和缓解代理安全风险。

Abstract: LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.

</details>


### [51] [Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents](https://arxiv.org/abs/2602.16379)
*Mohammad H. A. Monfared,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 提出一种用于方面情感分析的智能体数据增强方法，通过迭代生成和验证产生高质量合成训练样本，在多个任务和数据集上优于原始提示方法。


<details>
  <summary>Details</summary>
Motivation: 方面情感分析（ABSA）需要大量标注数据，但人工标注成本高。传统数据增强方法在保持标签质量方面存在局限，需要更有效的方法来生成高质量的合成训练数据。

Method: 提出智能体数据增强方法，采用迭代生成和验证机制来产生合成训练样本。同时开发了基于提示的基线方法作为对比，使用相同的模型和指令。在三个ABSA子任务（ATE、ATSC、ASPE）、四个SemEval数据集和两个编码器-解码器模型（T5-Base和Tk-Instruct）上进行评估。

Result: 智能体增强在增强数据的标签保持方面优于原始提示方法，特别是在需要生成方面术语的任务中。与真实数据结合时，智能体增强提供更高的性能提升，始终优于基于提示的生成。这些优势在T5-Base上最为明显，而预训练更充分的Tk-Instruct改进较小。增强数据帮助T5-Base达到与Tk-Instruct相当的性能。

Conclusion: 智能体数据增强方法能有效生成高质量的合成训练数据，特别是在需要生成方面术语的ABSA任务中，为资源有限的模型提供显著的性能提升。

Abstract: We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.

</details>


### [52] [TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers](https://arxiv.org/abs/2602.16429)
*Ido Levy,Eilam Shapira,Yinon Goldshtein,Avi Yaeli,Nir Mashkif,Segev Shlomov*

Main category: cs.CL

TL;DR: TabAgent用轻量级文本-表格分类器替代LLM决策组件，在保持任务成功率的同时大幅降低延迟和成本


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体系统在执行多步工作流时，通过重复调用LLM进行路由、筛选等封闭集决策任务，导致部署缓慢且昂贵（累积延迟和token使用）

Method: TabAgent框架包含三个核心组件：TabSchema从执行轨迹提取结构化模式、状态和依赖特征；TabSynth通过模式对齐的合成监督增强覆盖范围；TabHead使用轻量级分类器对候选进行评分

Result: 在AppWorld长时程基准测试中，TabAgent在保持任务级成功率的同时，消除了短列表时间的LLM调用，延迟降低约95%，推理成本降低85-91%

Conclusion: TabAgent不仅适用于工具短列表，还能推广到其他智能体决策头，为生产智能体架构中生成式瓶颈的学习判别式替代建立了范式

Abstract: Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.

</details>


### [53] [IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models](https://arxiv.org/abs/2602.16467)
*Saurabh Bharti,Gaurav Azad,Abhinaw Jagtap,Nachiket Tapas*

Main category: cs.CL

TL;DR: IndicEval是一个基于真实高利害考试题目的多语言LLM评估平台，使用UPSC、JEE、NEET等考试的英文和印地语题目，通过自动化评估揭示CoT提示的有效性、模型性能差异和多语言退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估框架缺乏真实学术严谨性和多语言复杂性，需要基于真实考试标准的评估平台来测量推理能力、领域知识和双语适应性。

Method: 构建IndicEval基准平台，使用UPSC、JEE、NEET的真实高利害考试题目（STEM和人文领域），支持英文和印地语，采用Zero-Shot、Few-Shot和Chain-of-Thought提示策略进行自动化评估，支持新模型和语言的模块化集成。

Result: 在Gemini 2.0 Flash、GPT-4、Claude和LLaMA 3-70B上的实验显示：1) CoT提示持续提升推理准确性；2) 模型间存在显著性能差异，特别是在高复杂度考试中；3) 多语言退化严重，印地语准确率明显低于英语，尤其在Zero-Shot条件下。

Conclusion: IndicEval为多语言教育环境中的LLM评估提供了实践导向、可扩展的基础，揭示了双语推理和领域迁移的持续差距，为改进推理鲁棒性和语言适应性提供了可行见解。

Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.

</details>


### [54] [Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning](https://arxiv.org/abs/2602.16469)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 研究机器翻译数据对小型英语语言模型的影响，分析不同源语言的翻译文本如何影响语言可接受性判断和语言建模


<details>
  <summary>Details</summary>
Motivation: 机器翻译数据在多语言NLP中被广泛使用，但翻译文本与原生文本存在系统性差异（翻译腔），这种差异既反映了源语言的痕迹，也体现了翻译本身的特性。需要研究训练机器翻译数据如何影响模型行为。

Method: 使用从24种类型学和资源多样性源语言翻译的英语文本训练小型语言模型，系统分析源语言和语料库特性如何影响模型学习

Result: 源语言对模型行为有显著影响：通用困惑度更多受翻译语料库词汇多样性驱动，而语法性能与英语的类型学相似度强相关（在有足够数据的情况下）

Conclusion: 机器翻译数据的特性（特别是源语言的影响）会系统地影响语言模型的性能，词汇多样性影响困惑度，类型学相似度影响语法判断能力

Abstract: Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.

</details>


### [55] [Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling](https://arxiv.org/abs/2602.16485)
*Jeffrey T. H. Wong,Zixi Zhang,Junyi Liu,Yiren Zhao*

Main category: cs.CL

TL;DR: Team-of-Thoughts：一种新型异构多智能体系统架构，通过编排器-工具范式利用不同后训练模型的互补能力，在推理和代码生成任务上显著超越同质化基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统通常采用静态、同质的模型配置，无法充分利用不同后训练模型的独特优势。需要一种能够利用异构智能体互补能力的新架构。

Method: 提出Team-of-Thoughts架构，采用编排器-工具范式，包含两个关键机制：1) 编排器校准方案，识别具有优越协调能力的模型；2) 工具智能体自我评估协议，分析自身领域专长以考虑后训练技能差异。推理时编排器根据能力配置文件动态激活最合适的工具智能体。

Result: 在五个推理和代码生成基准测试中，Team-of-Thoughts始终表现出优越的任务性能。在AIME24和LiveCodeBench上分别达到96.67%和72.53%的准确率，显著超越同质化角色扮演基线（80%和65.93%）。

Conclusion: Team-of-Thoughts通过异构智能体协作和动态编排机制，有效解决了传统多智能体系统的局限性，在复杂任务上实现了显著性能提升。

Abstract: Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.

</details>


### [56] [Learning to Learn from Language Feedback with Social Meta-Learning](https://arxiv.org/abs/2602.16488)
*Jonathan Cook,Diego Antognini,Martin Klissarov,Claudiu Musat,Edward Grefenstette*

Main category: cs.CL

TL;DR: 论文提出社会元学习(SML)方法，通过微调让大语言模型学会主动寻求反馈并从中学习，提升对话互动性和问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在对话中难以从纠正性反馈中学习，很少主动寻求反馈，导致对话静态、单向，缺乏人类对话的适应性。需要提升模型通过对话学习的能力。

Method: 从人类社会元学习中获得灵感，将SML制定为微调方法。在模拟的教学对话中训练LLMs，让模型学会寻求语言反馈并从中学习，将静态任务转化为交互式社会学习问题。

Result: SML有效教会模型通过对话解决单轮无法解决的问题。该能力具有跨领域泛化性：数学问题上的SML训练能让模型更好地利用反馈解决编程问题，反之亦然。即使只在完全指定的问题上训练，模型也能更好地解决信息在多轮对话中逐步揭示的不完全指定任务。

Conclusion: SML方法为开发能够有效从语言反馈中学习的AI系统提供了可扩展的途径。面对模糊性时，SML训练模型减少过早回答尝试，更可能询问所需信息。

Abstract: Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.

</details>


### [57] [From Growing to Looping: A Unified View of Iterative Computation in LLMs](https://arxiv.org/abs/2602.16490)
*Ferdinand Kapl,Emmanouil Angelis,Kaitlin Maile,Johannes von Oswald,Stefan Bauer*

Main category: cs.CL

TL;DR: 深度循环与深度增长是两种诱导迭代计算以增强推理能力的技术，它们在机制上统一，可组合使用，并能通过高质量数据进一步优化。


<details>
  <summary>Details</summary>
Motivation: 循环（重用层块）和深度增长（从浅到深训练模型）都被认为能增强推理能力，但它们之间的关系尚不明确。本文旨在统一这两种技术，探索它们如何共同促进迭代计算，从而提升模型推理性能。

Method: 通过分析循环模型和深度增长模型的深度特征，发现它们具有收敛的深度模式。在此基础上，将两种技术组合：在深度增长模型的中间块上应用推理时循环，并研究不同数据混合（特别是数学密集型冷却混合）对性能的影响。

Result: 循环和深度增长模型展现出相似的深度特征，表明它们共享迭代计算机制。组合使用两种技术（在深度增长模型上应用推理时循环）可将某些推理原语的准确率提升高达2倍。使用高质量数学密集型数据能获得最大推理增益，且可通过循环中间块进一步强化。

Conclusion: 深度增长和循环是互补的实用方法，能有效诱导和扩展迭代计算以改善推理能力。它们具有适应性，能受益于更多上下文示例或微调数据，且通过组合使用可获得协同效应。

Abstract: Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.

</details>


### [58] [Optimizing Soft Prompt Tuning via Structural Evolution](https://arxiv.org/abs/2602.16500)
*Zhenzhen Huang,Chaoning Zhang,Haoyu Bian,Songbo Zhang,Chi-lok Andy Tai,Jiaquan Zhang,Caiyan Qin,Jingjing Qu,Yalan Ye,Yang Yang,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本文提出了一种基于拓扑形态演化的软提示调优优化方法，使用拓扑数据分析量化软提示的结构表示，并通过构造拓扑软提示损失函数来指导模型学习结构稳定的适配。


<details>
  <summary>Details</summary>
Motivation: 软提示调优虽然在大规模预训练语言模型的少样本设置中表现优异，但其依赖高维隐式表示，缺乏明确的语义和可追踪的训练行为，这限制了其可解释性。

Method: 采用拓扑数据分析中的持久同调来量化软提示在连续参数空间中的结构表示及其训练过程演化，基于拓扑稳定性和紧凑性与下游性能的正相关关系，构建了拓扑软提示损失函数来优化软提示调优。

Result: 定量分析表明拓扑稳定且紧凑的软提示能获得更好的下游性能。实验证明，使用TSLoss训练能加速收敛并提升调优性能，为从结构和拓扑角度理解和优化软提示调优提供了可解释的方法。

Conclusion: 该方法通过拓扑形态演化优化软提示调优，不仅提高了性能，还增强了可解释性，为理解软提示的结构特性和训练行为提供了新的视角。

Abstract: Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.

</details>


### [59] [Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification](https://arxiv.org/abs/2602.16516)
*Taja Kuzman Pungeršek,Peter Rupnik,Daniela Širinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: ParlaCAP是一个用于分析欧洲议会议程设置的大规模数据集，通过教师-学生框架构建特定领域的政策主题分类器，在28个欧洲议会超过800万篇演讲上应用比较议程项目分类体系。


<details>
  <summary>Details</summary>
Motivation: 需要分析欧洲各国议会的议程设置，但缺乏大规模、多语言的议会演讲数据集和有效的政策主题分类方法。现有分类器通常基于领域外的人工标注数据，性能有限。

Method: 采用教师-学生框架：1) 使用高性能大语言模型标注领域内训练数据；2) 在多语言编码器模型上微调这些标注，实现可扩展的数据标注。将比较议程项目分类体系应用于ParlaMint语料库的800多万篇演讲。

Result: 1) LLM与人类标注者的一致性接近人类标注者间的一致性；2) 生成的模型优于基于领域外人工标注数据的现有CAP分类器；3) 创建了包含丰富元数据和情感预测的ParlaCAP数据集。

Conclusion: 该方法能构建针对目标领域的有效分类器，ParlaCAP数据集为跨国家政治注意力和代表性比较研究提供了丰富资源，通过三个用例展示了数据集的分析潜力。

Abstract: This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.

</details>


### [60] [Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset](https://arxiv.org/abs/2602.16571)
*Zhuqian Zhou,Kirk Vanacore,Bakhtawar Ahtisham,Jinsook Lee,Doug Pietrzak,Daryl Hedley,Jorge Dias,Chris Shaw,Ruth Schäfer,René F. Kizilcec*

Main category: cs.CL

TL;DR: 提出了MathEd-PII基准数据集，用于数学辅导对话中的PII检测，通过数学感知提示显著提升检测性能，减少数学内容误删


<details>
  <summary>Details</summary>
Motivation: 数学辅导对话中数字表达式常被误判为个人身份信息，导致核心教学内容被过度删除，降低数据集实用性，需要领域感知的PII检测方法

Method: 创建MathEd-PII基准数据集（1000个辅导会话），采用人机协同LLM工作流标注PII，提出基于密度的分割方法，比较四种检测策略：Presidio基线、基础LLM提示、数学感知提示、分段感知提示

Result: 数学感知提示显著优于基线（F1: 0.821 vs. 0.379），减少数字误报，证明领域上下文对保持分析实用性的重要性

Conclusion: 数学辅导数据的实用保护性去标识化需要领域感知建模，本研究提供了新基准和证据支持

Abstract: Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the "numeric ambiguity" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.

</details>


### [61] [CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes](https://arxiv.org/abs/2602.16607)
*Miguel Marques,Ana Luísa Fernandes,Ana Filipa Pacheco,Rute Rebouças,Inês Cantante,José Isidro,Luís Filipe Cunha,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,António Leal,Purificação Silvano,Ricardo Campos*

Main category: cs.CL

TL;DR: 提出了CitiLink-Summ数据集，包含100份欧洲葡萄牙语市政会议记录和2,322个手动撰写的摘要，为低资源语言市政领域文本摘要建立了首个基准。


<details>
  <summary>Details</summary>
Motivation: 市政会议记录内容冗长复杂，公民难以理解。自动摘要可以帮助解决这一问题，但在低资源语言（如欧洲葡萄牙语）中缺乏高质量数据集，限制了该领域摘要模型的发展。

Method: 创建了CitiLink-Summ语料库，包含100份市政会议记录和2,322个手动摘要。使用最先进的生成模型（BART、PRIMERA）和大语言模型（LLMs）建立基线，使用ROUGE、BLEU、METEOR和BERTScore等指标进行评估。

Result: 建立了市政领域摘要的首个基准数据集，提供了基线结果，为欧洲葡萄牙语复杂行政文本的NLP研究提供了宝贵资源。

Conclusion: CitiLink-Summ填补了低资源语言市政领域摘要研究的空白，为开发更有效的自动摘要模型奠定了基础，有助于提高政府透明度。

Abstract: Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.

</details>


### [62] [Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models](https://arxiv.org/abs/2602.16608)
*Melkamu Abay Mersha,Jugal Kalita*

Main category: cs.CL

TL;DR: 提出了CA-LIG框架，通过层间集成梯度与注意力梯度融合，为Transformer模型提供上下文感知的分层归因解释


<details>
  <summary>Details</summary>
Motivation: 现有Transformer可解释性方法存在局限：依赖最终层归因、缺乏局部与全局归因的统一、缺少上下文感知、无法捕捉相关性在层间的演化以及结构组件如何影响决策

Method: 提出上下文感知分层集成梯度框架，在每个Transformer块内计算层间集成梯度，并将这些token级归因与类别特定的注意力梯度融合，生成带符号、上下文敏感的归因图

Result: 在情感分析、长文档分类、多类文档分类、低资源语言仇恨检测、图像分类等任务上，CA-LIG相比现有方法提供更忠实、上下文敏感、语义更清晰的归因可视化

Conclusion: CA-LIG为Transformer决策提供更全面、上下文感知且可靠的可解释性，推进了深度神经网络的实际可解释性和概念理解

Abstract: Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.

</details>


### [63] [ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models](https://arxiv.org/abs/2602.16609)
*Antoine Chaffin,Luca Arnaboldi,Amélie Chatelain,Florent Krzakala*

Main category: cs.CL

TL;DR: 研究大规模多向量预训练，发现完全预训练的ColBERT-Zero模型在公开数据上超越现有方法，达到新SOTA；发现监督学习步骤可替代昂贵的无监督预训练；强调微调与预训练设置对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多向量模型通常是在强单向量模型基础上通过小规模知识蒸馏获得，依赖大规模预训练。本研究旨在探索多向量模型的预训练方法，验证大规模多向量预训练是否能产生更强的模型。

Method: 1) 完全预训练多向量模型ColBERT-Zero（仅使用公开数据）；2) 对比分析小规模知识蒸馏与完全预训练的效果；3) 探索在知识蒸馏前添加监督学习步骤以替代无监督预训练；4) 研究微调与预训练设置对齐的重要性。

Result: 1) ColBERT-Zero在公开数据上完全预训练后，超越了GTE-ModernColBERT及其基础模型GTE-ModernBERT（后者使用封闭且更强的数据），创造了该尺寸模型的新SOTA；2) 仅进行小规模知识蒸馏不足以接近完全预训练效果，但添加监督学习步骤后可获得接近性能，同时跳过昂贵的无监督阶段；3) 微调与预训练设置对齐对模型重用至关重要。

Conclusion: 大规模多向量预训练能产生更强的模型；监督学习步骤可有效替代昂贵的无监督预训练；模型微调与预训练设置对齐是成功重用的关键；为促进研究，作者发布了相关检查点和训练代码。

Abstract: Current state-of-the-art multi-vector models are obtained through a small Knowledge Distillation (KD) training step on top of strong single-vector models, leveraging the large-scale pre-training of these models. In this paper, we study the pre-training of multi-vector models and show that large-scale multi-vector pre-training yields much stronger multi-vector models. Notably, a fully ColBERT-pre-trained model, ColBERT-Zero, trained only on public data, outperforms GTE-ModernColBERT as well as its base model, GTE-ModernBERT, which leverages closed and much stronger data, setting new state-of-the-art for model this size. We also find that, although performing only a small KD step is not enough to achieve results close to full pre-training, adding a supervised step beforehand allows to achieve much closer performance while skipping the most costly unsupervised phase. Finally, we find that aligning the fine-tuning and pre-training setups is crucial when repurposing existing models. To enable exploration of our results, we release various checkpoints as well as code used to train them.

</details>


### [64] [Who can we trust? LLM-as-a-jury for Comparative Assessment](https://arxiv.org/abs/2602.16610)
*Mengjie Qian,Guangzhi Sun,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: 本文提出BT-sigma方法，通过引入判别器参数来建模LLM评委的可靠性，从而改善基于成对比较的自然语言生成评估效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为自动评估器时，通常依赖单个评委或假设多个评委具有同等可靠性进行聚合。但实际上，LLM评委在不同任务和方面表现差异很大，其判断概率可能存在偏差和不一致性，且缺乏人工标注数据进行校准。

Method: 提出BT-sigma方法，这是Bradley-Terry模型的扩展，为每个评委引入判别器参数，仅从成对比较中联合推断项目排名和评委可靠性。该方法可视为一种无监督校准机制。

Result: 在基准NLG评估数据集上的实验表明，BT-sigma始终优于基于平均的聚合方法，学习到的判别器参数与LLM判断的循环一致性独立度量强相关。

Conclusion: BT-sigma通过建模评委可靠性有效改善了LLM作为评审团的评估效果，为解决LLM评委不一致性问题提供了有效的无监督解决方案。

Abstract: Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.

</details>


### [65] [AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models](https://arxiv.org/abs/2602.16639)
*Adib Sakhawat,Fardeen Sadab*

Main category: cs.CL

TL;DR: AREG基准测试通过多轮零和谈判评估LLM的社会智能，发现说服与抵抗能力弱相关且不对称，防御通常优于进攻。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的社会智能需要从静态文本生成转向动态对抗交互，现有评估框架往往只关注说服能力而忽视抵抗能力，需要更全面的评估方法。

Method: 引入对抗资源提取游戏(AREG)基准，通过多轮零和谈判评估LLM的说服和抵抗能力；采用循环赛制评估前沿模型；进行语言分析识别有效策略。

Result: 说服与抵抗能力弱相关(ρ=0.33)，两者存在解离；所有模型中抵抗分数均高于说服分数，显示防御优势；增量承诺策略提高提取成功率，验证寻求响应增强防御效果。

Conclusion: LLM的社会影响力不是单一能力，仅关注说服的评估框架可能忽视行为不对称性；需要更全面的对抗交互评估来识别模型的社会智能漏洞。

Abstract: Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.

</details>


### [66] [Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval](https://arxiv.org/abs/2602.16640)
*Subrit Dikshit*

Main category: cs.CL

TL;DR: Quecto-V1是一个专门针对印度法律领域的小型语言模型，通过GPT-2架构（1.24亿参数）和8位量化技术，在150MB内存占用下实现离线法律智能，解决了资源受限环境下的法律AI可及性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（7B+参数）依赖云端推理，导致资源受限环境无法使用，且存在数据主权风险。需要为印度法律领域开发一个可在本地运行、保护隐私的轻量级法律智能系统。

Method: 基于GPT-2架构（1.24亿参数）从头训练，专门使用印度法律语料（IPC、CrPC、宪法等）。采用后训练8位量化（GGUF格式）压缩模型，实现150MB内存占用，可在消费级CPU上离线运行。

Result: Quecto-V1在法律定义和刑罚条款检索任务中表现出高保真度，优于通用小型语言模型。8位量化使模型大小减少74%，检索准确率下降不到3.5%，实现了性能与效率的良好平衡。

Conclusion: 对于法律等高风险专业领域，领域特定训练结合激进量化技术，为保护隐私的本地化AI应用提供了可行的替代方案，能够解决资源鸿沟和数据主权问题。

Abstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a "resource divide." State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes "lexical density" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.

</details>


### [67] [Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment](https://arxiv.org/abs/2602.16660)
*Yuyan Bu,Xiaohao Liu,ZhaoXing Ren,Yaodong Yang,Juntao Dai*

Main category: cs.CL

TL;DR: 提出一种资源高效的多语言安全对齐方法，通过多语言一致性损失提高多语言表示向量的共线性，仅需多语言提示变体即可实现多语言同时对齐，无需低资源语言的额外响应级监督。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言社区广泛部署需要可靠的多语言安全对齐，但现有方法需要大量资源（目标语言的大规模高质量监督或与高资源语言的成对对齐），限制了可扩展性。

Method: 提出即插即用的多语言一致性（MLC）损失，可集成到现有单语对齐流程中。通过提高多语言表示向量的共线性，在单次更新中鼓励多语言语义层面的方向一致性，仅使用多语言提示变体即可实现多语言同时对齐。

Result: 在不同模型架构和对齐范式上验证了方法的有效性，证明能增强多语言安全性且对模型通用能力影响有限。跨语言和任务的评估显示改善了跨语言泛化能力。

Conclusion: 该方法为有限监督下的多语言一致性对齐提供了实用解决方案，资源效率高，可扩展性强。

Abstract: The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.

</details>


### [68] [Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents](https://arxiv.org/abs/2602.16699)
*Wenxuan Ding,Nicholas Tomlin,Greg Durrett*

Main category: cs.CL

TL;DR: 提出Calibrate-Then-Act框架，通过让LLM显式推理成本-不确定性权衡，优化其在需要环境交互的复杂任务中的决策策略。


<details>
  <summary>Details</summary>
Motivation: LLM在处理需要与环境交互获取信息的复杂问题时，需要权衡探索成本与不确定性，但现有方法缺乏对这种成本-不确定性权衡的显式推理机制。

Method: 提出Calibrate-Then-Act框架，将任务形式化为不确定性下的序列决策问题，为LLM提供先验信息作为额外上下文，使其能显式推理成本-收益权衡。

Result: 在信息检索QA和简化编程任务上，CTA框架能帮助智能体发现更优的决策策略，即使在强化学习训练下，CTA相比基线方法仍保持优势。

Conclusion: 通过让LLM显式推理成本-不确定性权衡，CTA框架能显著提升智能体在需要环境交互的复杂任务中的决策优化能力。

Abstract: LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.

</details>


### [69] [Reinforced Fast Weights with Next-Sequence Prediction](https://arxiv.org/abs/2602.16704)
*Hee Seung Hwang,Xindi Wu,Sanghyuk Chun,Olga Russakovsky*

Main category: cs.CL

TL;DR: REFINE是一个强化学习框架，通过下一序列预测训练快速权重模型，解决了传统下一词预测在长上下文建模中的局限性，显著提升了长上下文任务性能。


<details>
  <summary>Details</summary>
Motivation: 快速权重架构虽然能保持恒定的内存开销来处理长上下文，但其潜力受到下一词预测训练范式的限制。NTP只优化单词预测，忽略了前缀后多个token的语义连贯性，导致快速权重模型学习到的表示无法有效捕捉长距离依赖关系。

Method: REFINE采用强化学习框架，基于预测熵选择信息丰富的token位置，生成多token rollout，分配自监督的序列级奖励，并使用组相对策略优化进行模型优化。该方法适用于预训练语言模型的整个训练生命周期：中期训练、后训练和测试时训练。

Result: 在LaCT-760M和DeltaNet-1.3B上的实验表明，REFINE在针在干草堆检索、长上下文问答以及LongBench中的多样化任务上，始终优于使用NTP的监督微调。

Conclusion: REFINE为改进快速权重架构中的长上下文建模提供了一个有效且通用的框架，通过下一序列预测训练解决了传统下一词预测的局限性。

Abstract: Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [Towards Efficient Constraint Handling in Neural Solvers for Routing Problems](https://arxiv.org/abs/2602.16012)
*Jieyi Bi,Zhiguang Cao,Jianan Zhou,Wen Song,Yaoxin Wu,Jie Zhang,Yining Ma,Cathy Wu*

Main category: cs.AI

TL;DR: CaR是首个基于显式学习可行性精炼的通用高效约束处理框架，用于神经路由求解器，通过联合训练框架和构造-改进共享表示，在复杂约束下实现优越的可行性、解质量和效率。


<details>
  <summary>Details</summary>
Motivation: 当前神经求解器在简单路由问题上表现出色，但在复杂约束下的优势尚不成熟。现有的约束处理方法（如可行性掩码或隐式可行性感知）对于硬约束可能效率低下或不适用，需要更有效的约束处理框架。

Method: 提出Construct-and-Refine (CaR)框架：1）设计联合训练框架，指导构造模块生成适合轻量级改进过程（如10步vs先前5k步）的多样化高质量解；2）首次采用构造-改进共享表示，通过统一编码器实现跨范式知识共享，特别适用于复杂约束场景。

Result: 在典型硬路由约束上的评估显示，CaR相比经典和神经最先进求解器，在可行性、解质量和效率方面都取得了优越性能。

Conclusion: CaR是首个通用高效的神经路由求解器约束处理框架，通过显式学习可行性精炼和构造-改进共享表示，有效解决了复杂约束下的路由问题，为神经求解器在约束优化领域的应用提供了新方向。

Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility awareness can be inefficient or inapplicable for hard constraints. In this paper, we present Construct-and-Refine (CaR), the first general and efficient constraint-handling framework for neural routing solvers based on explicit learning-based feasibility refinement. Unlike prior construction-search hybrids that target reducing optimality gaps through heavy improvements yet still struggle with hard constraints, CaR achieves efficient constraint handling by designing a joint training framework that guides the construction module to generate diverse and high-quality solutions well-suited for a lightweight improvement process, e.g., 10 steps versus 5k steps in prior work. Moreover, CaR presents the first use of construction-improvement-shared representation, enabling potential knowledge sharing across paradigms by unifying the encoder, especially in more complex constrained scenarios. We evaluate CaR on typical hard routing constraints to showcase its broader applicability. Results demonstrate that CaR achieves superior feasibility, solution quality, and efficiency compared to both classical and neural state-of-the-art solvers.

</details>


### [71] [Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection](https://arxiv.org/abs/2602.16037)
*Cameron Cagan,Pedram Fard,Jiazi Tian,Jingya Cheng,Shawn N. Murphy,Hossein Estiri*

Main category: cs.AI

TL;DR: 自主智能体优化过程会出现性能退化现象，特别是在低患病率分类任务中，传统评估指标可能掩盖严重失败模式。研究发现回顾性选择比主动干预更有效。


<details>
  <summary>Details</summary>
Motivation: 自主智能体工作流程具有迭代优化自身行为的潜力，但其失败模式尚未得到充分研究。需要探究优化不稳定性现象，即持续自主改进反而导致分类器性能下降的问题。

Method: 使用Pythia开源框架进行自动提示优化，评估三种不同患病率的临床症状（气短23%、胸痛12%、长新冠脑雾3%）。测试两种干预措施：引导智能体主动重定向优化，以及选择智能体回顾性识别最佳迭代。

Result: 验证灵敏度在迭代中在1.0和0.0之间振荡，严重程度与类别患病率成反比。在3%患病率时，系统达到95%准确率但检测到零阳性病例。选择智能体监督下，系统在脑雾检测上超越专家词典331%（F1），胸痛检测提升7%。

Conclusion: 研究揭示了自主AI系统的关键失败模式，证明在低患病率分类任务中，回顾性选择比主动干预更能有效稳定系统性能。

Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.

</details>


### [72] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: 该论文对大型语言模型在教育自动评估中的不确定性量化方法进行了系统基准测试，分析了不确定性模式及其影响因素，为开发更可靠的评估系统提供指导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育自动评估中展现出优势，但其固有的概率性特征引入了输出不确定性，这可能影响后续教学决策的稳定性，需要系统研究不确定性量化方法在评估场景中的适用性。

Method: 通过在多评估数据集、不同LLM家族和生成控制设置下进行综合分析，对广泛的不确定性量化方法进行基准测试，评估不同不确定性指标的优势与局限。

Result: 研究揭示了LLM在评分场景中的不确定性模式，分析了模型家族、评估任务和解码策略等关键因素对不确定性估计的影响，提供了不确定性特征的实证见解。

Conclusion: 该研究为理解LLM自动评估中的不确定性提供了系统框架，为未来开发更可靠、有效的基于不确定性的自动评分系统奠定了基础。

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [73] [Evidence-Grounded Subspecialty Reasoning: Evaluating a Curated Clinical Intelligence Layer on the 2025 Endocrinology Board-Style Examination](https://arxiv.org/abs/2602.16050)
*Amir Hosseinian,MohammadReza Zare Shahneh,Umer Mansoor,Gilbert Szeto,Kirill Karlin,Nima Aghaeepour*

Main category: cs.AI

TL;DR: January Mirror系统在专科临床推理任务中表现优于前沿LLMs，通过证据溯源和结构化推理架构实现更高准确率


<details>
  <summary>Details</summary>
Motivation: 解决专科临床推理的挑战，包括快速更新的指南和复杂的证据层次，传统LLMs在这方面表现有限

Method: 开发January Mirror系统，整合内分泌和心脏代谢证据库与结构化推理架构，在封闭证据约束下运行，对比前沿LLMs的实时网络检索

Result: Mirror达到87.5%准确率，显著超过人类参考(62.3%)和前沿LLMs(GPT-5.2:74.6%)，在最难问题上仍有76.7%准确率，74.2%输出引用指南级证据

Conclusion: 具有明确来源的精选证据系统在专科临床推理中优于无约束网络检索，支持临床部署的可审计性

Abstract: Background: Large language models have demonstrated strong performance on general medical examinations, but subspecialty clinical reasoning remains challenging due to rapidly evolving guidelines and nuanced evidence hierarchies. Methods: We evaluated January Mirror, an evidence-grounded clinical reasoning system, against frontier LLMs (GPT-5, GPT-5.2, Gemini-3-Pro) on a 120-question endocrinology board-style examination. Mirror integrates a curated endocrinology and cardiometabolic evidence corpus with a structured reasoning architecture to generate evidence-linked outputs. Mirror operated under a closed-evidence constraint without external retrieval. Comparator LLMs had real-time web access to guidelines and primary literature. Results: Mirror achieved 87.5% accuracy (105/120; 95% CI: 80.4-92.3%), exceeding a human reference of 62.3% and frontier LLMs including GPT-5.2 (74.6%), GPT-5 (74.0%), and Gemini-3-Pro (69.8%). On the 30 most difficult questions (human accuracy less than 50%), Mirror achieved 76.7% accuracy. Top-2 accuracy was 92.5% for Mirror versus 85.25% for GPT-5.2. Conclusions: Mirror provided evidence traceability: 74.2% of outputs cited at least one guideline-tier source, with 100% citation accuracy on manual verification. Curated evidence with explicit provenance can outperform unconstrained web retrieval for subspecialty clinical reasoning and supports auditability for clinical deployment.

</details>


### [74] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: 该论文提出了一种训练框架，将交互式上下文学习视为可训练技能而非涌现特性，通过信息不对称将单轮任务转化为多轮教学互动，显著提升了模型从语言反馈中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练主要依赖静态语料库建模，忽视了人类学习中基于纠正反馈动态适应环境的关键能力。模型需要能够像人类一样通过交互反馈循环进行动态调整。

Method: 提出可扩展方法，将单轮可验证任务转化为由信息不对称驱动的多轮教学互动。通过训练模型预测教师的批评来建模反馈环境，将外部信号转化为内部能力。

Result: 训练后的小模型多轮性能接近大一个数量级的大模型水平；在数学问题上交互训练能泛化到编码、谜题和迷宫导航等不同领域；模型获得了增强的上下文可塑性。

Conclusion: 交互式上下文学习能力是可训练技能而非涌现特性，该框架为自我改进提供了统一路径，使模型能够将外部反馈信号转化为内部自我纠正能力。

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [75] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: GPSBench是一个包含57,800个样本的17个任务数据集，用于评估LLM的地理空间推理能力，发现LLM在几何坐标计算方面较弱，但在真实世界地理推理方面相对可靠。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地应用于导航、机器人等与物理世界交互的场景，地理空间推理能力变得至关重要，但目前LLM在GPS坐标和真实世界地理推理方面的能力尚未得到充分探索。

Method: 构建GPSBench数据集，包含几何坐标操作（距离、方位计算）和结合坐标与世界知识的推理任务，评估14个最先进的LLM，专注于内在模型能力而非工具使用。

Result: LLM的地理空间推理仍然具有挑战性，模型在真实世界地理推理方面比几何计算更可靠；地理知识呈现层级性退化（国家层面表现强，城市层面定位弱）；对坐标噪声的鲁棒性表明模型具有真正的坐标理解而非简单记忆。

Conclusion: GPS坐标增强可以改善下游地理空间任务性能，但微调会在几何计算收益和世界知识退化之间产生权衡。数据集和代码已开源，为地理空间推理研究提供基准。

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [76] [Learning Personalized Agents from Human Feedback](https://arxiv.org/abs/2602.16173)
*Kaiqu Liang,Julia Kruk,Shengyi Qian,Xianjun Yang,Shengjie Bi,Yuanshun Yao,Shaoliang Nie,Mingyang Zhang,Lijuan Liu,Jaime Fernández Fisac,Shuyan Zhou,Saghar Hosseini*

Main category: cs.AI

TL;DR: PAHF框架通过显式用户记忆和双反馈通道实现AI代理的持续个性化，在初始偏好学习和偏好漂移适应方面显著优于无记忆和单通道基线。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理通常依赖静态数据集，难以适应新用户和随时间变化的偏好，需要一种能够在线学习并适应个体用户独特、演化偏好的框架。

Method: 提出PAHF框架：1) 行动前澄清以解决歧义；2) 基于记忆检索偏好的行动基础；3) 整合行动后反馈以更新记忆。使用显式用户记忆和双反馈通道（行动前澄清和行动后反馈）。

Result: 在具身操作和在线购物两个基准测试中，PAHF显著优于无记忆和单通道基线，减少初始个性化错误并快速适应偏好漂移。

Conclusion: 整合显式记忆与双反馈通道对持续个性化至关重要，PAHF框架能有效学习初始偏好并适应偏好变化，为AI代理个性化提供了有效解决方案。

Abstract: Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.

</details>


### [77] [EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments](https://arxiv.org/abs/2602.16179)
*Sushant Mehta,Logan Ritchie,Suhaas Garre,Nick Heiner,Edwin Chen*

Main category: cs.AI

TL;DR: 在CoreCraft企业环境中训练AI代理能产生超越训练分布的泛化能力，GLM 4.6模型经过单轮训练后任务通过率从25.37%提升到36.76%，并在多个外部基准测试中获得显著提升


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在高保真强化学习环境中训练是否能产生超越训练分布的泛化能力，验证环境质量、多样性和真实性对代理能力泛化的重要性

Method: 引入CoreCraft企业模拟环境（包含2500多个实体、14种实体类型、23种工具），使用Group Relative Policy Optimization（GRPO）和自适应裁剪训练GLM 4.6模型

Result: 单轮训练后任务通过率从25.37%提升到36.76%；在外部基准测试中：BFCL Parallel提升4.5%，τ²-Bench Retail提升7.4%，Toolathlon提升6.8%

Conclusion: 环境质量、多样性和真实性是实现可泛化代理能力的关键因素；任务中心的世界构建、专家编写的评分标准和反映真实专业模式的企业工作流程有助于能力迁移

Abstract: We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \corecraft{}, the first environment in \textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\% to 36.76\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\% on BFCL Parallel, +7.4\% on $τ^2$-Bench Retail, and +6.8\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.

</details>


### [78] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 论文探讨了实现人工超智能所需的关键"记忆"设计概念，强调"先存储后按需提取"方法优于当前主流的"先提取后存储"范式，以避免信息丢失，并提出了其他有前景的记忆方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的"先提取后存储"记忆范式存在固有缺陷，即在提取过程中可能丢失对后续任务有价值的信息。为实现"用记忆提升世界"的使命，需要探索更有效的记忆设计方法来实现人工超智能。

Method: 提出"先存储后按需提取"方法，保留原始经验并根据不同任务需求灵活应用；同时强调从大量概率经验中发现深层洞察，以及通过共享存储经验提高经验收集效率。通过简单实验验证这些方法的有效性。

Result: 实验证明这些替代方法确实有效，但相关研究仍面临重大挑战。论文识别了限制这些有前景方向探索的主要障碍。

Conclusion: 需要克服现有研究障碍，深入探索"先存储后按需提取"等替代记忆方法，这些方法对实现人工超智能至关重要，能避免信息丢失并提高经验利用效率。

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [79] [Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents](https://arxiv.org/abs/2602.16246)
*Yun-Shiuan Chuang,Chaitanya Kulkarni,Alec Chiu,Avinash Thangali,Zijie Pan,Shivani Shekhar,Yirou Ge,Yixi Li,Uma Kona,Linsey Pang,Prakhar Mehrotra*

Main category: cs.AI

TL;DR: 提出Proxy State-Based Evaluation框架，用LLM驱动的模拟替代确定性后端，通过代理状态追踪和LLM评判实现智能体评估


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准测试（如tau-bench、AppWorld）依赖完全确定性的后端，构建和维护成本高，迭代困难，需要更实用、可扩展的评估方案

Method: 提出代理状态评估框架：场景指定用户目标、事实、期望最终状态和行为；LLM状态追踪器从完整交互轨迹推断结构化代理状态；LLM评判器验证目标完成并检测工具/用户幻觉

Result: 基准测试产生稳定、能区分模型的排名；在线/离线策略rollout提供可迁移的监督信号；模拟器幻觉率接近零；人类-LLM评判一致性超过90%

Conclusion: 代理状态评估为工业级LLM智能体提供了实用、可扩展的确定性基准测试替代方案，支持可靠的模型比较和训练数据生成

Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.

</details>


### [80] [Multi-agent cooperation through in-context co-player inference](https://arxiv.org/abs/2602.16301)
*Marissa A. Weis,Maciej Wołczyk,Rajai Nasser,Rif A. Saurous,Blaise Agüera y Arcas,João Sacramento,Alexander Meulemans*

Main category: cs.AI

TL;DR: 序列模型通过上下文学习实现合作，无需硬编码假设或显式时间尺度分离


<details>
  <summary>Details</summary>
Motivation: 自利智能体之间的合作是多智能体强化学习中的基本挑战。现有方法依赖硬编码假设或严格的时间尺度分离，需要更自然的方法。

Method: 使用序列模型训练智能体对抗多样化的对手分布，自然诱导上下文最优响应策略，在快速时间尺度上作为学习算法。

Result: 上下文适应使智能体易受勒索，相互压力塑造对手的上下文学习动态，最终学习到合作行为。序列模型+对手多样性提供了可扩展的合作学习路径。

Conclusion: 序列模型的上下文学习能力允许智能体自然获得对手学习意识，无需硬编码假设或显式时间尺度分离，为合作行为提供了可扩展的解决方案。

Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.

</details>


### [81] [Verifiable Semantics for Agent-to-Agent Communication](https://arxiv.org/abs/2602.16424)
*Philipp Schoenegger,Matt Carlson,Chris Schneider,Chris Daly*

Main category: cs.AI

TL;DR: 提出基于刺激-意义模型的认证协议，通过测试代理在可观察事件上的表现来验证术语理解一致性，核心保护推理可将分歧减少51-96%


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统需要一致的通信，但缺乏验证代理对术语理解是否相同的方法。自然语言可解释但易受语义漂移影响，学习协议高效但不透明。

Method: 基于刺激-意义模型的认证协议：测试代理在共享可观察事件上的表现，如果经验分歧低于统计阈值则认证术语。核心保护推理限制代理仅使用认证术语进行推理。

Result: 在语义分歧程度不同的模拟中，核心保护将分歧减少72-96%。在使用微调语言模型的验证中，分歧减少51%。

Conclusion: 该框架为可验证的代理间通信迈出了第一步，提供了检测漂移（重新认证）和恢复共享词汇（重新协商）的机制。

Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms ("core-guarded reasoning") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.

</details>


### [82] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: CAFE：基于因果引导的自动化特征工程框架，通过因果发现与强化学习结合，提升特征工程的鲁棒性和效率


<details>
  <summary>Details</summary>
Motivation: 现有自动化特征工程方法依赖统计启发式，在分布偏移时产生脆弱特征。需要将因果结构作为软归纳先验来提升特征工程的鲁棒性。

Method: 两阶段框架：阶段I学习稀疏有向无环图获取软因果先验，将特征按因果影响分组；阶段II使用级联多智能体深度Q学习架构选择因果组和变换算子，采用分层奖励塑造和因果组级探索策略。

Result: 在15个公共基准测试中，CAFE比强基线提升达7%，减少收敛轮次，在协变量偏移下性能下降减少约4倍，产生更紧凑的特征集和更稳定的后验归因。

Conclusion: 因果结构作为软归纳先验而非刚性约束，能显著提升自动化特征工程的鲁棒性和效率。

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [83] [Towards a Science of AI Agent Reliability](https://arxiv.org/abs/2602.16666)
*Stephan Rabanser,Sayash Kapoor,Peter Kirgis,Kangheng Liu,Saiteja Utpala,Arvind Narayanan*

Main category: cs.AI

TL;DR: 论文提出12个具体指标从一致性、鲁棒性、可预测性和安全性四个维度评估AI代理的可靠性，发现当前能力提升对可靠性改善有限。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理评估主要依赖单一成功率指标，掩盖了关键操作缺陷，无法反映代理在一致性、抗干扰、可预测故障和错误严重性等方面的表现，需要更全面的可靠性评估框架。

Method: 基于安全关键工程理念，提出12个具体指标从四个维度分解代理可靠性：一致性（跨运行稳定性）、鲁棒性（抗扰动能力）、可预测性（故障模式可预测）、安全性（错误严重性有界）。在14个代理模型和两个基准测试上进行评估。

Result: 评估发现，尽管代理模型在传统基准测试上能力有所提升，但在可靠性方面的改进有限，暴露了当前代理系统的持久性局限。

Conclusion: 提出的多维可靠性指标补充了传统评估方法，为理解代理如何表现、退化和失败提供了工具，有助于更全面地评估AI代理的实际部署能力。

Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.

</details>


### [84] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: 该论文提出使用大型语言模型作为不完美专家，通过因果假设论证框架结合语义结构先验和条件独立性证据进行因果发现，在标准基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现需要专家知识构建因果图，而统计方法依赖观测数据。本文旨在探索如何利用大型语言模型作为"不完美专家"，结合符号推理框架Causal ABA，将变量名称和描述的语义信息与数据证据相结合。

Method: 采用因果假设论证框架，从大型语言模型获取变量名称和描述的语义结构先验，将这些先验知识与条件独立性证据整合。引入评估协议来减轻评估LLMs时的记忆偏差。

Result: 在标准基准测试和语义基础合成图上展示了最先进的性能。提出的评估协议有效缓解了记忆偏差问题。

Conclusion: 大型语言模型可以作为有效的语义先验来源，与因果假设论证框架结合能够提升因果发现性能，同时需要适当的评估方法来确保结果可靠性。

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [85] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: FoT是一个通用框架，用于构建和优化动态推理方案，解决现有提示方案缺乏适应性和优化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示方案（如Chain of Thought、Tree of Thoughts等）存在两个主要问题：1）需要用户定义静态的、特定于问题的推理结构，缺乏对动态或未见问题类型的适应性；2）在超参数、提示、运行时间和成本方面通常未充分优化。

Method: 提出了Framework of Thoughts (FoT)，这是一个通用基础框架，具有超参数调优、提示优化、并行执行和智能缓存等内置功能，用于构建和优化动态推理方案。

Result: 通过将Tree of Thoughts、Graph of Thoughts和ProbTree三种流行方案在FoT中实现，实证表明FoT能显著加快执行速度、降低成本，并通过优化获得更好的任务分数。

Conclusion: FoT作为一个通用框架，能够释放推理方案的潜在性能，促进未来动态高效推理方案的发展。

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [86] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: 通过七个月的诗歌工作坊，研究人员使用大型语言模型通过上下文专家反馈塑造数字诗人，无需重新训练，模型形成了独特风格和连贯作品集，在盲测中人类和AI诗歌识别率接近随机水平。


<details>
  <summary>Details</summary>
Motivation: 探索机器能否创作优秀诗歌，这涉及艺术本质和价值的根本问题。研究旨在通过工作坊式提示方法，测试大型语言模型在长期创意塑造中的潜力。

Method: 进行为期七个月的诗歌工作坊，通过迭代的上下文专家反馈塑造大型语言模型成为数字诗人，无需重新训练模型。使用定量和定性分析评估模型发展，并进行盲测实验（50名人文学生和毕业生评估6首诗，其中3首AI创作，3首知名诗人作品）。

Result: 模型形成了独特风格和连贯作品集，创造了笔名和作者形象。盲测结果显示：人类诗歌被识别为人类的概率为54%，AI诗歌被识别为AI的概率为52%，95%置信区间均包含50%（随机水平）。工作坊后，商业出版社出版了该模型的诗歌集。

Conclusion: 工作坊式提示方法能够支持长期创意塑造，重新引发关于创造力和作者身份的辩论，表明AI能够创作出与人类诗歌难以区分的作品。

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [87] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: Agent Skill框架能显著提升中等规模SLM（12B-30B参数）的性能，但小模型难以可靠选择技能，而80B参数的代码专用模型能达到闭源基线水平并提升GPU效率。


<details>
  <summary>Details</summary>
Motivation: 研究Agent Skill范式是否能为小型语言模型（SLM）带来类似大型专有模型的性能提升，这在工业场景中很重要，因为数据安全和预算限制使得持续依赖公共API不可行，且SLM在高度定制化场景中泛化能力有限。

Method: 首先形式化定义了Agent Skill过程的数学定义，然后系统评估了不同规模的语言模型在多个用例中的表现，包括两个开源任务和一个真实世界的保险索赔数据集。

Result: 微小模型难以可靠选择技能，中等规模SLM（约12B-30B参数）从Agent Skill方法中获益显著，约80B参数的代码专用变体达到闭源基线性能水平同时提升GPU效率。

Conclusion: 研究全面而细致地描述了Agent Skill框架的能力和限制，为在SLM中心环境中有效部署Agent Skill提供了可行的见解。

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [88] [Exploring New Frontiers in Vertical Federated Learning: the Role of Saddle Point Reformulation](https://arxiv.org/abs/2602.15996)
*Aleksandr Beznosikov,Georgiy Kormakov,Alexander Grigorievskiy,Mikhail Rudakov,Ruslan Nazykov,Alexander Rogozin,Anton Vakhrushev,Andrey Savchenko,Martin Takáč,Alexander Gasnikov*

Main category: math.OC

TL;DR: 本文提出了一种基于鞍点重构的垂直联邦学习方法，通过拉格朗日函数将VFL问题转化为鞍点问题，并开发了多种随机化扩展以适应实际场景。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习需要在不同设备上利用相同用户的不同特征协同训练模型，但传统最小化公式难以适应实际场景中的压缩传输、异步通信和局部计算优化等需求。

Method: 通过拉格朗日函数将VFL问题重构为鞍点问题，提出确定性求解方法，并扩展了三种随机化变体：压缩传输技术、部分参与异步通信、坐标选择加速局部计算。

Result: 为每种算法提供了收敛性分析，证明了鞍点重构方法能够实现传统最小化公式难以达到的扩展功能，数值实验验证了方法的性能和有效性。

Conclusion: 鞍点重构为垂直联邦学习提供了新的求解框架，能够灵活适应实际应用中的各种约束和需求，为VFL的实际部署提供了有效的解决方案。

Abstract: The objective of Vertical Federated Learning (VFL) is to collectively train a model using features available on different devices while sharing the same users. This paper focuses on the saddle point reformulation of the VFL problem via the classical Lagrangian function. We first demonstrate how this formulation can be solved using deterministic methods. More importantly, we explore various stochastic modifications to adapt to practical scenarios, such as employing compression techniques for efficient information transmission, enabling partial participation for asynchronous communication, and utilizing coordinate selection for faster local computation. We show that the saddle point reformulation plays a key role and opens up possibilities to use mentioned extension that seem to be impossible in the standard minimization formulation. Convergence estimates are provided for each algorithm, demonstrating their effectiveness in addressing the VFL problem. Additionally, alternative reformulations are investigated, and numerical experiments are conducted to validate performance and effectiveness of the proposed approach.

</details>


### [89] [Exponential Conic Optimization for Multi-Regime Service System Design under Congestion and Tail-Risk Control](https://arxiv.org/abs/2602.16021)
*Víctor Blanco,Miguel Martínez-Antón,Justo Puerto*

Main category: math.OC

TL;DR: 提出一个混合整数指数锥优化框架，用于设计具有多重复发机制的服务系统，在响应时间服务水平约束下平衡成本、拥塞、公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究具有多重复发机制的单设施服务系统设计问题，这些系统在不同机制下具有不同的到达率和服务率，导致响应时间呈超指数分布。需要设计机制特定的容量来平衡成本、拥塞、公平性和可靠性。

Method: 提出混合整数指数锥优化框架，整合SLA机会约束、冲突图设计限制和基于CVaR的尾部风险控制。虽然问题是NP难的，但提出了有效的分解方案和可处理的特殊情况。

Result: 计算实验和大规模城市案例研究表明，相比现有系统有显著改进，量化了效率、拥塞控制、公平性和鲁棒性之间的明确权衡。

Conclusion: 该框架为拥塞感知和尾部控制的服务系统设计提供了实用工具，能够有效平衡多个设计目标。

Abstract: We study the design of single-facility service systems operating under multiple recurring regimes with service-level constraints on response times. Regime-dependent arrival and service rates induce hyperexponential response-time distributions, and the design problem selects regime-specific capacities to balance cost, congestion, fairness, and reliability. We propose a mixed-integer exponential conic optimization framework integrating SLA chance constraints, conflict-graph design restrictions, and CVaR-based tail-risk control. Although NP-hard, the problem admits an efficient decomposition scheme and tractable special cases. Computational experiments and a large-scale urban case study show substantial improvements over the current system, quantifying explicit trade-offs between efficiency, congestion control, fairness, and robustness. The framework provides a practical tool for congestion-aware and tail-control service system design.

</details>


### [90] [Local adapt-then-combine algorithms for distributed nonsmooth optimization: Achieving provable communication acceleration](https://arxiv.org/abs/2602.16148)
*Luyao Guo,Xinli Shi,Wenying Xu,Jinde Cao*

Main category: math.OC

TL;DR: FlexATC：一种统一的分布式复合优化框架，通过概率性本地更新实现通信高效，在强凸情况下实现与目标函数和网络拓扑解耦的线性收敛率


<details>
  <summary>Details</summary>
Motivation: 解决分布式复合优化问题，其中智能体需要最小化局部平滑分量和共同非光滑项的和。现有分布式算法通信成本高，需要设计通信高效的框架来减少通信开销

Method: 提出FlexATC框架，基于Adapt-Then-Combine（ATC）机制，引入概率性本地更新机制，统一多种ATC分布式算法。使用与网络拓扑和本地更新次数无关的步长

Result: 在凸设置下获得次线性收敛率，在强凸设置下获得线性收敛率。强凸情况下线性收敛率与目标函数和网络拓扑解耦，允许在大多数迭代中跳过通信而不影响收敛率

Conclusion: FlexATC框架首次证明本地更新能加速ATC分布式算法的通信效率，数值实验验证了框架的有效性和理论结果

Abstract: This paper is concerned with the distributed composite optimization problem over networks, where agents aim to minimize a sum of local smooth components and a common nonsmooth term. Leveraging the probabilistic local updates mechanism, we propose a communication-efficient Adapt-Then-Combine (ATC) framework, FlexATC, unifying numerous ATC-based distributed algorithms. Under stepsizes independent of the network topology and the number of local updates, we establish sublinear and linear convergence rates for FlexATC in convex and strongly convex settings, respectively. Remarkably, in the strong convex setting, the linear rate is decoupled from the objective functions and network topology, and FlexATC permits communication to be skipped in most iterations without any deterioration of the linear rate. In addition, the proposed unified theory demonstrates for the first time that local updates provably lead to communication acceleration for ATC-based distributed algorithms. Numerical experiments further validate the efficacy of the proposed framework and corroborate the theoretical results.

</details>


### [91] [Null controllability of one-dimensional quasilinear parabolic equations via multiplicative controls](https://arxiv.org/abs/2602.16150)
*Jilei Huang,Peidong Lei,Yansheng Ma,Jingxue Yin*

Main category: math.OC

TL;DR: 本文研究了一类拟线性抛物方程在乘性控制下的零能控性问题，证明了通过局部空间支持的乘性控制可以在有限时间内将系统驱动到零状态，并探讨了时间最优控制的存在性。


<details>
  <summary>Details</summary>
Motivation: 研究拟线性抛物方程在乘性控制下的零能控性问题，乘性控制相比加性控制在实际应用中更常见且具有物理意义，但数学处理更为复杂。需要解决无控制时系统的衰减性质，为控制存在性提供基础。

Method: 首先建立无控制时齐次拟线性抛物方程解的衰减估计，包括L∞范数和H¹范数的衰减性质。基于衰减估计和拟线性抛物方程的最大模估计，结合加性控制下的局部零能控性结果，证明乘性控制下的零能控性。最后研究时间最优控制的存在性。

Result: 获得了拟线性抛物方程在乘性控制下的零能控性，证明了存在乘性控制能在有限时间T>0内将系统驱动到零状态。作为副产品，还得到了加性控制下大时间尺度的全局零能控性。由于乘性控制需要较长时间，进一步证明了时间最优控制的存在性。

Conclusion: 本文成功解决了拟线性抛物方程在乘性控制下的零能控性问题，建立了完整的理论框架。通过衰减估计、最大模估计和局部控制性结果的结合，证明了乘性控制的有效性。同时获得了加性控制下的全局结果，并探讨了时间最优控制的存在性，为相关控制理论提供了重要进展。

Abstract: This paper is concerned with the null controllability problem for a class of quasilinear parabolic equations under multiplicative control, locally supported in space. For the purpose of proving the existence of a multiplicative control forcing the solution rest at a time $T>0$, we need to establish the decay property of solutions for the system without control first. We have obtained decay estimates for the $L^\infty$-norm and the $H^1$-norm of solutions to the homogenous quasilinear parabolic equations. Notably, the decay of the $L^\infty$-norm requires no smallness condition on the initial data, whereas the decay of the $H^1$-norm requires that the $L^\infty$-norm remains small. Based on the decay estimates and maximum modulus estimate of solutions to quasilinear parabolic equations, together with the local null controllability of quasilinear parabolic equations under additive controls, we prove the null controllability of the quasilinear parabolic equations via multiplicative controls. As a byproduct, we also obtain the global null controllability for large time to the quasilinear parabolic equations via additive controls. Given that the controllability under multiplicative control is achieved over a long time horizon, we finally investigate the existence of time optimal control.

</details>


### [92] [Metaheuristic algorithms for the induced P-median problem with upgrades](https://arxiv.org/abs/2602.16170)
*Sergio Salazar,Abraham Duarte,Mauricio G. C. Resende,J. Manuel Colmenar*

Main category: math.OC

TL;DR: 提出基于GRASP的元启发式算法解决诱导p中位数升级问题，相比基于数学规划的方法，执行时间平均提升两个数量级，在99%测试实例中获得最优解。


<details>
  <summary>Details</summary>
Motivation: 设施选址问题具有重要社会影响，诱导p中位数升级问题（IpMU）作为经典p中位数问题的变体，将运输成本和时间作为独立度量，并引入预算来降低边成本。现有基于数学规划的方法计算效率有限，需要更高效的求解方法。

Method: 提出基于贪婪随机自适应搜索过程（GRASP）的元启发式算法，采用两阶段求解方案：独立研究中位数问题和升级问题。分析更大规模的最新实例集，评估实例特征复杂度。

Result: 相比完全基于数学规划模型的最先进方法，执行时间在困难实例上平均提升两个数量级，在超过99%的测试实例中获得最佳已知结果。

Conclusion: GRASP元启发式算法在解决诱导p中位数升级问题上表现优异，显著提升计算效率，为复杂设施选址问题提供了有效的求解方案。

Abstract: Facility location problems (FLPs) are a family of optimisation problems with significant social impact. This class of problems has been the subject of study since the 1960s, with classical approaches including the Weber problem and the p-Median problem. Currently, more complex variations of these problems are being investigated. In particular, the Induced p-Median Problem with Upgrades (IpMU) represents a variation of the classical p-Median problem, where the concepts of transport cost and time are separated as distinct metrics in the input graph of the problem. Furthermore, the problem includes a budget which allows one to relax the graph costs, reducing the cost of the edges, thus improving the associated routes between the designated medians and the customers. In this study, a metaheuristic algorithm, based on the Greedy Randomized Adaptive Search Procedure (GRASP), is proposed. A two-phase resolution scheme is defined, studying the median problem and the upgrading problem independently. In this approach, a larger set of state-of-the-art instances was analysed to ensure a fair comparison with previous proposals. In addition, the characteristics of the instances were studied to assess their complexity. The results obtained are promising when compared to the state-of-the-art, which is based entirely on mathematical programming models. The execution time was improved on average by two orders of magnitude for the harder instances, and the best known result was obtained in more than 99% of the tested instances.

</details>


### [93] [Optimal driving strategies for a fleet of trains](https://arxiv.org/abs/2602.16205)
*Phil Howlett,Maria Kapsis,Peter Pudney*

Main category: math.OC

TL;DR: 使用约束优化方法为铁路列车寻找在指定高峰时段限制总能耗的最优驾驶策略，通过调整能耗分布而非改变行程时间来降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 电力系统运营商为鼓励大型用户在高峰时段减少用电提供经济激励，铁路运营商可通过调整特定行程的能耗分布（高峰时段减少用电、其他时段增加用电）来获得经济效益，而非单纯最小化总能耗。

Method: 采用经典的约束优化方法，为列车车队寻找最优驾驶策略，在保持单个行程时间不变的前提下，限制指定中间时间间隔内的总能耗。

Result: 提出的策略可被大型铁路组织采用，在不显著干扰现有时刻表、不改变重要发车和到达时间的情况下，降低整体运营成本。

Conclusion: 通过优化列车驾驶策略来调整能耗时间分布，铁路运营商可以利用电力系统的经济激励措施，在保持服务质量的同时有效降低运营成本。

Abstract: In order to manage electricity transmission and distribution it is now common practice for system operators to offer financial incentives that encourage large consumers to reduce energy usage during designated peak demand periods. For train operators on large rail networks it may be profitable -- with selected individual journeys -- to reduce energy usage during peak times and increase energy usage at other times rather than simply minimizing overall energy consumption. We will use classical methods of constrained optimization to find optimal driving strategies for a fleet of trains subject to limits on total energy consumption during specified intermediate time intervals but with no change to individual journey times. The proposed strategies can be used by a large rail organisation to reduce overall operating costs with only minimal disruption to existing schedules and with no changes to important departure and arrival times.

</details>


### [94] [Partially observed controlled Markov chains and optimal control of the Wonham filter](https://arxiv.org/abs/2602.16392)
*Fulvia Confortola,Marco Fuhrman*

Main category: math.OC

TL;DR: 该论文研究了一类具有有限或无限时域的最优控制问题，针对状态空间有限的连续时间马尔可夫链，其中控制过程影响转移速率，且受控过程不可观测，控制动作基于受外生布朗运动扰乱的观测过程来选择。


<details>
  <summary>Details</summary>
Motivation: 研究在部分观测条件下，控制影响转移速率的连续时间马尔可夫链的最优控制问题。实际应用中，许多系统（如通信网络、金融模型、生物系统）的状态无法直接观测，只能通过噪声观测来推断，因此需要开发适用于这类部分观测控制问题的理论框架。

Method: 1. 构造具有适应观测滤波的随机转移速率的受控马尔可夫链；2. 通过Girsanov型测度变换，引入分离最优控制问题，其中状态是受控马尔可夫链的条件（未归一化）分布，观测过程变为驱动布朗运动；3. 证明分离问题与原控制问题的等价性；4. 分离问题的控制方程是Wonham滤波方程的一个实例；5. 分析分离问题：将值函数刻画为动态规划方程的唯一粘性解，证明验证定理，以及以最优性必要条件形式呈现的随机最大值原理。

Result: 1. 成功构造了适应观测滤波的受控马尔可夫链；2. 证明了通过Girsanov变换得到的分离问题与原控制问题的等价性；3. 将分离问题的控制方程与Wonham滤波方程联系起来；4. 证明了值函数是动态规划方程的唯一粘性解（抛物型和椭圆型情况）；5. 建立了验证定理和随机最大值原理形式的最优性必要条件。

Conclusion: 该论文为部分观测条件下控制影响转移速率的连续时间马尔可夫链的最优控制问题提供了一个完整的理论框架。通过分离原理将原问题转化为完全观测问题，并建立了动态规划方程、验证定理和最优性条件，为这类问题的数值求解和应用奠定了理论基础。

Abstract: We consider a class of optimal control problems, with finite or infinite horizon, for a continuous-time Markov chain with finite state space. In this case, the control process affects the transition rates. We suppose that the controlled process can not be observed, and at any time the control actions are chosen based on the observation of a related stochastic process perturbed by an exogenous Brownian motion. We describe a construction of the controlled Markov chain, having stochastic transition rates adapted to the observation filtration. By a change of probability measure of Girsanov type, we introduce the so-called separated optimal control problem, where the state is the conditional (unnormalized) distribution of the controlled Markov chain and the observation process becomes a driving Brownian motion, and we prove the equivalence with the original control problem. The controlled equations for the separated problem are an instance of the Wonham filtering equations. Next we present an analysis of the separated problem: we characterize the value function as the unique viscosity solution to the dynamic programming equations (both in the parabolic and the elliptic case) we prove verifications theorems and a version of the stochastic maximum principle in the form of a necessary conditions for optimality.

</details>


### [95] [Primal-dual dynamical systems with closed-loop control for convex optimization in continuous and discrete time](https://arxiv.org/abs/2602.16402)
*Huan Zhang,Xiangkai Sun,Shengjie Li,Kok Lay Teo*

Main category: math.OC

TL;DR: 提出一种具有闭环系数设计的原始-对偶动力系统，通过时间缩放和Hessian驱动阻尼实现快速收敛，并开发出具有自适应步长的加速原始-对偶算法。


<details>
  <summary>Details</summary>
Motivation: 解决具有线性等式约束的凸优化问题，需要设计能够同时保证原始-对偶间隙、可行性违反度和目标函数残差快速收敛的算法。

Method: 首先提出"二阶原始+一阶对偶"连续时间动力系统，其中时间缩放和Hessian驱动阻尼由拉格朗日函数梯度的反馈控制决定；然后通过时间离散化开发具有梯度定义自适应步长的加速原始-对偶算法。

Result: 获得了原始-对偶间隙、可行性违反度和目标函数残差的收敛速率，数值实验证明了算法的实际有效性和优越性能。

Conclusion: 提出的闭环系数设计原始-对偶动力系统和相应的离散算法能够有效解决约束凸优化问题，在理论和实际性能上都表现出色。

Abstract: This paper develops a primal-dual dynamical system where the coefficients are designed in closed-loop way for solving a convex optimization problem with linear equality constraints. We first introduce a ``second-order primal" + ``first-order dual'' continuous-time dynamical system, in which both the time scaling and Hessian-driven damping are governed by a feedback control of the gradient for the Lagrangian function. This system achieves the fast convergence rates for the primal-dual gap, the feasibility violation, and the objective residual along its trajectory. Subsequently, by time discretization of this system, we develop an accelerated primal-dual algorithm with a gradient-defined adaptive step size. We also obtain convergence rates for the primal-dual gap, the feasibility violation, and the objective residual. Furthermore, we provide numerical results to demonstrate the practical efficacy and superior performance of the proposed algorithm.

</details>


### [96] [The Complexity Landscape of Two-Stage Robust Selection Problems with Budgeted Uncertainty](https://arxiv.org/abs/2602.16465)
*Marc Goerigk,Dorothee Henke,Lasse Wulf*

Main category: math.OC

TL;DR: 研究预算不确定下两阶段选择问题的复杂性：连续预算不确定使两阶段选择问题NP难，但两阶段代表选择问题仍多项式可解


<details>
  <summary>Details</summary>
Motivation: 预算不确定是鲁棒优化中的标准不确定集，但两阶段设置下离散与连续预算不确定的复杂性研究不足，特别是对于选择问题等简单基础问题

Method: 分析三种选择问题和三种预算不确定集的复杂性，特别关注连续预算不确定下的两阶段选择问题

Result: 证明连续预算不确定下的两阶段选择问题是NP难的，而两阶段代表选择问题可在多项式时间内求解；该硬度结果还意味着连续预算不确定下的两阶段分配问题也是NP难的

Conclusion: 填补了预算不确定鲁棒优化中复杂性研究的空白，为三种选择问题和三种预算不确定集提供了全面的复杂性分类

Abstract: A standard type of uncertainty set in robust optimization is budgeted uncertainty, where an interval of possible values for each parameter is given and the total deviation from their lower bounds is bounded. In the two-stage setting, discrete and continuous budgeted uncertainty have to be distinguished. The complexity of such problems is largely unexplored, in particular if the underlying nominal optimization problem is simple, such as for selection problems. In this paper, we give a comprehensive answer to long-standing open complexity questions for three types of selection problems and three types of budgeted uncertainty sets. In particular, we demonstrate that the two-stage selection problem with continuous budgeted uncertainty is NP-hard, while the corresponding two-stage representative selection problem is solvable in polynomial time. Our hardness result implies that also the two-stage assignment problem with continuous budgeted uncertainty is NP-hard.

</details>


### [97] [PL conditions do not guarantee convergence of gradient descent-ascent dynamics](https://arxiv.org/abs/2602.16517)
*Jean-Christophe Mourrat*

Main category: math.OC

TL;DR: 本文给出一个满足双侧Polyak-Lojasiewicz条件的函数示例，但梯度下降-上升流线未能收敛到鞍点，而是围绕其循环


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降-上升算法在满足Polyak-Lojasiewicz条件下的收敛行为，挑战关于该条件下算法必然收敛到鞍点的常见假设

Method: 构造一个具体的函数示例，该函数满足双侧Polyak-Lojasiewicz条件，但分析其梯度下降-上升流线的动态行为

Result: 发现即使函数满足双侧Polyak-Lojasiewicz条件，梯度下降-上升流线也可能不收敛到鞍点，而是围绕鞍点循环

Conclusion: 满足Polyak-Lojasiewicz条件不足以保证梯度下降-上升算法收敛到鞍点，这对优化理论中的收敛保证提出了重要限制

Abstract: We give an example of a function satisfying a two-sided Polyak-Lojasiewicz condition but for which a gradient descent-ascent flow line fails to converge to the saddle point, circling around it instead.

</details>


### [98] [Learning Distributed Equilibria in Linear-Quadratic Stochastic Differential Games: An $α$-Potential Approach](https://arxiv.org/abs/2602.16555)
*Philipp Plank,Yufei Zhang*

Main category: math.OC

TL;DR: 独立策略梯度方法在线性二次随机微分博弈中的收敛性分析，针对对称和非对称交互网络分别证明了全局线性收敛


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中独立策略梯度学习的收敛性，特别是在线性二次随机微分博弈中，每个玩家仅依赖自身状态并独立更新策略，分析其在对称和非对称交互网络下的性能

Method: 采用独立策略梯度方法，每个玩家使用仅依赖于自身状态的分布式策略，通过梯度下降独立更新策略；利用α-势能结构分析收敛性，针对对称交互构建分布式均衡，针对非对称交互使用投影策略梯度算法

Result: 对于对称交互，独立策略梯度方法全局线性收敛到分布式均衡，复杂度与种群规模线性相关，与精度对数相关；对于非对称交互，投影策略梯度算法线性收敛到近似均衡，次优性与不对称程度成正比

Conclusion: 线性二次博弈具有α-势能结构，独立策略梯度方法在对称和非对称交互网络中都能实现全局线性收敛，为分布式多智能体学习提供了理论保证

Abstract: We analyze independent policy-gradient (PG) learning in $N$-player linear-quadratic (LQ) stochastic differential games. Each player employs a distributed policy that depends only on its own state and updates the policy independently using the gradient of its own objective. We establish global linear convergence of these methods to an equilibrium by showing that the LQ game admits an $α$-potential structure, with $α$ determined by the degree of pairwise interaction asymmetry. For pairwise-symmetric interactions, we construct an affine distributed equilibrium by minimizing the potential function and show that independent PG methods converge globally to this equilibrium, with complexity scaling linearly in the population size and logarithmically in the desired accuracy. For asymmetric interactions, we prove that independent projected PG algorithms converge linearly to an approximate equilibrium, with suboptimality proportional to the degree of asymmetry. Numerical experiments confirm the theoretical results across both symmetric and asymmetric interaction networks.

</details>


### [99] [Nonparametric Kernel Regression for Coordinated Energy Storage Peak Shaving with Stacked Services](https://arxiv.org/abs/2602.16586)
*Emily Logan,Ning Qi,Bolun Xu*

Main category: math.OC

TL;DR: 提出一个两阶段框架，协调商业建筑中储能系统的削峰和能量套利，无需依赖预测，性能比现有方法提升1.3倍


<details>
  <summary>Details</summary>
Motivation: 商业建筑中储能系统的有效控制对于降低电费和延长电池寿命至关重要，需要协调削峰和多种服务，但现有方法依赖预测且性能有限

Method: 提出端到端两阶段框架：第一阶段使用非参数核回归模型从历史数据构建满足削峰需求的荷电状态轨迹边界；第二阶段通过迁移学习方法利用剩余容量进行能量套利

Result: 使用纽约市商业建筑需求数据的案例研究表明，该方法比最先进的基于预测的方法性能提升1.3倍，实现成本节约和有效峰值管理，且不依赖预测

Conclusion: 该框架为商业建筑储能系统提供了一种无需预测的有效控制策略，能够协调削峰和能量套利，显著降低成本并延长电池寿命

Abstract: Developing effective control strategies for behind-the-meter energy storage to coordinate peak shaving and stacked services is essential for reducing electricity costs and extending battery lifetime in commercial buildings. This work proposes an end-to-end, two-stage framework for coordinating peak shaving and energy arbitrage with a theoretical decomposition guarantee. In the first stage, a non-parametric kernel regression model constructs state-of-charge trajectory bounds from historical data that satisfy peak-shaving requirements. The second stage utilizes the remaining capacity for energy arbitrage via a transfer learning method. Case studies using New York City commercial building demand data show that our method achieves a 1.3 times improvement in performance over the state-of-the-art forecast-based method, achieving cost savings and effective peak management without relying on predictions.

</details>


### [100] [Optimal bounds for numerical approximations of finite horizon problems based on dynamic programming approach](https://arxiv.org/abs/2602.16574)
*Javier de Frutos,Julia Novo*

Main category: math.OC

TL;DR: 本文为有限时域动态规划问题提供了完全离散逼近的最优误差界，证明了在标准正则性假设下，方法具有O(h+k)的先验误差界，其中h为时间步长，k为空间网格尺寸。


<details>
  <summary>Details</summary>
Motivation: 将无限时域动态规划问题的误差分析扩展到有限时域情况，避免无限时域情况下对控制变量更严格的正则性要求，同时改进时间收敛速率。

Method: 采用完全离散逼近方法，使用分段常数控制策略，将无限时域情况的误差分析技术适配到有限时域问题，通过理论分析建立先验误差界。

Result: 证明了在标准正则性假设下，有限时域动态规划问题的完全离散逼近具有O(h+k)的最优误差界，实现了时间和空间的一阶收敛，避免了无限时域情况下因分段控制导致的时间收敛速率损失。

Conclusion: 有限时域动态规划问题的完全离散逼近可以获得时间和空间的一阶收敛，相比无限时域情况具有更好的收敛性质，且对控制变量的正则性要求更低。

Abstract: In this paper we provide optimal bounds for fully discrete approximations to finite horizon problems via dynamic programming. We adapt the error analysis in \cite{nos} for the infinite horizon case to the
  finite horizon case. We prove an a priori bound of size $O(h+k)$ for the method, $h$ being the time discretization step and $k$ the spatial mesh size. Arguing with piecewise constants controls we are able to obtain first order of convergence in time and space under standard regularity assumptions, avoiding the more restrictive regularity assumptions on the controls required in \cite{nos}.
  We show that the loss in the rate of convergence in time of the
  infinite case (obtained arguing with piece-wise controls)
  can be avoided in the finite horizon case

</details>


### [101] [Hybrid Optimization Techniques for Multi-State Optimal Design Problems](https://arxiv.org/abs/2602.16592)
*Marko Erceg,Petar Kunštek,Marko Vrdoljak*

Main category: math.OC

TL;DR: 该论文提出了一种结合形状优化和材料分布优化的混合方法，用于多态稳态扩散方程控制的最优设计问题。


<details>
  <summary>Details</summary>
Motivation: 解决多态稳态扩散方程控制的最优设计问题，同时优化域形状和两种各向同性材料的分布比例。

Method: 采用混合方法：内部基于均匀化理论进行松弛，结合域边界限制；数值方法集成均匀化和形状优化，使用水平集方法演化边界，内部材料分布通过最优性准则算法更新。

Result: 建立了广义解的存在性，提出了数值实现框架，并通过代表性示例验证了方法的有效性。

Conclusion: 该混合方法能有效解决同时优化形状和材料分布的设计问题，为多物理场优化提供了可行的数值框架。

Abstract: This paper addresses optimal design problems governed by multi-state stationary diffusion equations, aiming at the simultaneous optimization of the domain shape and the distribution of two isotropic materials in prescribed proportions. Existence of generalized solutions is established via a hybrid approach combining homogenization-based relaxation in the interior with suitable restrictions on admissible domains.
  Based on this framework, we propose a numerical method that integrates homogenization and shape optimization. The domain boundary is evolved using a level set method driven by the shape derivative, while the interior material distribution is updated via an optimality criteria algorithm. The approach is demonstrated on a representative example.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [102] [Two-way Clustering Robust Variance Estimator in Quantile Regression Models](https://arxiv.org/abs/2602.16376)
*Ulrich Hounyo,Jiahao Lin*

Main category: econ.EM

TL;DR: 研究双聚类数据下线性分位数回归的推断问题，提出稳健方差估计方法，建立收敛速率和高斯逼近理论，同时揭示非高斯交互机制下均匀推断的不可能性。


<details>
  <summary>Details</summary>
Motivation: 在双聚类数据结构中，传统的分位数回归推断方法面临挑战，需要开发能够处理复杂依赖结构的稳健推断方法。

Method: 使用可分离可交换阵列框架和分位数得分投影分解，提出双聚类稳健三明治方差估计器，包含核密度"面包"和投影匹配"肉"部分。

Result: 建立了机制依赖的收敛速率和自归一化高斯逼近，证明了高斯机制下推断的一致性和有效性，同时揭示了非高斯交互机制下均匀推断的不可能性。

Conclusion: 该方法为双聚类数据下的分位数回归提供了有效的推断工具，但存在机制限制，非高斯交互机制下均匀推断不可行。

Abstract: We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime.

</details>


### [103] [Model selection confidence sets for time series models with applications to electricity load data](https://arxiv.org/abs/2602.16527)
*Piersilvio De Bortoli,Davide Ferrari,Francesco Ravazzolo,Luca Rossini*

Main category: econ.EM

TL;DR: MSCS方法用于识别与真实数据生成过程在给定置信水平下统计上无法区分的模型集合，应用于意大利电力负荷数据，揭示模型选择不确定性并识别关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 传统模型选择方法依赖单一模型和任意准则，无法量化模型选择不确定性。需要一种方法来识别统计上无法区分的模型集合，以更好地理解数据特征和关键驱动因素。

Method: 使用模型选择置信集(MSCS)方法，识别在给定置信水平下与真实数据生成过程统计上无法区分的ARMA模型集合。通过分析MSCS的大小和组成来量化不确定性，并计算各项在MSCS和最低边界模型(LBM)中的包含频率来评估重要性。

Result: 应用于意大利小时电力负荷数据时，MSCS揭示了模型选择不确定性存在显著的日内变化。识别出包含日内小时滞后、温度、日历效应和太阳能发电等关键驱动因素的模型集合，这些模型能提供有竞争力的短期预测。

Conclusion: MSCS方法能有效量化模型选择不确定性，识别关键驱动因素，并为电力负荷预测提供稳健的模型集合。相比单一模型选择，能更好地反映数据特征和不确定性。

Abstract: This paper studies the Model Selection Confidence Set (MSCS) methodology for univariate time series models involving autoregressive and moving average components, and applies it to study model selection uncertainty in the Italian electricity load data. Rather than relying on a single model selected by an arbitrary criterion, the MSCS identifies a set of models that are statistically indistinguishable from the true data-generating process at a given confidence level. The size and composition of this set reveal crucial information about model selection uncertainty: noisy data scenarios produce larger sets with many candidate models, while more informative cases narrow the set considerably. To study the importance of each model term, we consider numerical statistics measuring the frequency with which each term is included in both the entire MSCS and in Lower Boundary Models (LBM), its most parsimonious specifications. Applied to Italian hourly electricity load data, the MSCS methodology reveals marked intraday variation in model selection uncertainty and isolates a collection of model specifications that deliver competitive short-term forecasts while highlighting key drivers of electricity load like intraday hourly lags, temperature, calendar effects and solar energy generation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [104] [Should There be a Teacher In-the-Loop? A Study of Generative AI Personalized Tasks Middle School](https://arxiv.org/abs/2602.15876)
*Candace Walkington,Mingyu Feng,Itffini Pruitt-Britton,Theodora Beauchamp,Andrew Lan*

Main category: cs.CY

TL;DR: 教师与ChatGPT合作创建个性化数学问题，发现教师参与导致个性化粒度较粗，而学生偏好细粒度流行文化引用，且教师效率未随经验提升


<details>
  <summary>Details</summary>
Motivation: 研究教师使用生成式AI创建个性化学习任务时的效率问题，尽管商业AI工具声称能节省教师时间，但实际效率尚不明确

Method: 7名中学数学教师与ChatGPT合作，根据学生兴趣创建个性化数学问题，分析教师的提示策略、创建效率，并调查521名七年级学生对个性化作业的反应

Result: 教师参与导致个性化粒度较粗，学生偏好细粒度流行文化引用；教师需大量调整AI生成内容，处理问题深度和真实性问题；教师能改进与AI合作创建有趣问题的能力，但效率未随经验提升

Conclusion: 教师与生成式AI合作创建个性化任务时，存在个性化粒度偏好差异，且教师效率未如预期提升，需要更有效的教师-AI协作模式

Abstract: Adapting instruction to the fine-grained needs of individual students is a powerful application of recent advances in large language models. These generative AI models can create tasks that correspond to students' interests and enact context personalization, enhancing students' interest in learning academic content. However, when there is a teacher in-the-loop creating or modifying tasks with generative AI, it is unclear how efficient this process might be, despite commercial generative AI tools' claims that they will save teachers time. In the present study, we teamed 7 middle school mathematics teachers with ChatGPT to create personalized versions of problems in their curriculum, to correspond to their students' interests. We look at the prompting moves teachers made, their efficiency when creating problems, and the reactions of their 521 7th grade students who received the personalized assignments. We find that having a teacher-in-the-loop results in generative AI-enhanced personalization being enacted at a relatively broad grain size, whereas students tend to prefer a smaller grain size where they receive specific popular culture references that interest them. Teachers spent a lot of effort adjusting popular culture references and addressing issues with the depth or realism of the problems generated, giving higher or lower levels of ownership to the generative AI. Teachers were able to improve in their ability to craft interesting problems in partnership with generative AI, but this process did not appear to become particularly time efficient as teachers learned and reflected on their students' data, iterating their approaches.

</details>


### [105] [Pluralism in AI Governance: Toward Sociotechnical Alignment and Normative Coherence](https://arxiv.org/abs/2602.15881)
*Mike Wa Nkongolo*

Main category: cs.CY

TL;DR: 论文探讨了如何将公共价值嵌入国家AI治理框架，提出了一个分层框架来连接价值、机制和策略，并通过比较分析揭示了不同司法管辖区的监管哲学差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI渗透到医疗、司法和公共行政等领域，其合法性不仅取决于技术正确性，还需要与社会规范、民主原则和人类尊严保持一致。传统范式过于关注模型安全或市场效率，忽视了更广泛的制度背景。

Method: 综合了Full-Stack Alignment、Thick Models of Value、Value Sensitive Design和Public Constitutional AI等框架，并对欧盟、美国、中国、英国、巴西和南非等司法管辖区进行比较分析。引入了一个连接价值、机制和策略的分层框架。

Result: 揭示了监管哲学的多元化，南非的主权导向方法提供了一个独特的对比视角。识别了公平与效率、透明度与安全、隐私与公平之间的紧张关系。

Conclusion: 提出了一个整体的、价值敏感的AI治理模型，将监管重新定义为将公共价值嵌入社会技术系统的主动机制，为AI治理提供了更全面的理论框架。

Abstract: This paper examines the challenge of embedding public values into national artificial intelligence (AI) governance frameworks, a task complicated by the sociotechnical nature of contemporary systems. As AI permeates domains such as healthcare, justice, and public administration, legitimacy depends not only on technical correctness but on alignment with societal norms, democratic principles, and human dignity. Traditional paradigms focused on model safety or market efficiency neglect broader institutional contexts. To address this, the study synthesises frameworks including Full-Stack Alignment, Thick Models of Value, Value Sensitive Design, and Public Constitutional AI, alongside comparative analysis of jurisdictions such as the EU, US, China, UK, Brazil, and South Africa (SA). It introduces a layered framework linking values, mechanisms, and strategies, and maps tensions such as fairness versus efficiency, transparency versus security, and privacy versus equity. Findings reveal a pluralism of regulatory philosophies, with SA sovereignty-oriented approach offering a distinctive counterpoint. The study contributes a holistic, value-sensitive model of AI governance, reframing regulation as a proactive mechanism for embedding public values into sociotechnical systems.

</details>


### [106] [Queer NLP: A Critical Survey on Literature Gaps, Biases and Trends](https://arxiv.org/abs/2602.16151)
*Sabine Weber,Angelina Wang,Ankush Gupta,Arjun Subramonian,Dennis Ulmer,Eshaan Tanwar,Geetanjali Aich,Hannah Devinney,Jacob Hobbs,Jennifer Mickel,Joshua Tint,Mae Sosto,Ray Groshan,Simone Astarita,Vagrant Gautam,Verena Blaschke,William Agnew,Wilson Y Lee,Yanan Long*

Main category: cs.CY

TL;DR: 对NLP与LGBTQIA+社区关系研究的系统性综述，分析现状、差距与未来方向


<details>
  <summary>Details</summary>
Motivation: NLP技术正在重塑语言处理方式，在招聘、法律、医疗等领域影响人们生活，理解和减轻对边缘化群体（特别是LGBTQIA+社区）的伤害至关重要

Method: 系统性地综述ACL Anthology中所有明确讨论NLP技术与LGBTQIA+社区关系的论文，回答三个研究问题：当前研究趋势、主题和方法上的差距、未来工作方向

Result: 发现虽然近年来相关论文数量有所增长，但大多数采取被动而非主动的方法，更多指出偏见而非缓解偏见，关注现有系统的缺陷而非创造新解决方案

Conclusion: 揭示了未来工作的许多机会，特别是在利益相关者参与、交叉性、跨学科性和非英语语言方面。从酷儿研究视角出发，强调未充分研究的主题和NLP论文中未充分解决的伤害问题，呼吁采取行动创建更公正和包容的NLP技术

Abstract: Natural language processing (NLP) technologies are rapidly reshaping how language is created, processed, and analyzed by humans. With current and potential applications in hiring, law, healthcare, and other areas that impact people's lives, understanding and mitigating harms towards marginalized groups is critical. In this survey, we examine NLP research papers that explicitly address the relationship between LGBTQIA+ communities and NLP technologies. We systematically review all such papers published in the ACL Anthology, to answer the following research questions: (1) What are current research trends? (2) What gaps exist in terms of topics and methods? (3) What areas are open for future work? We find that while the number of papers on queer NLP has grown within the last few years, most papers take a reactive rather than a proactive approach, pointing out bias more often than mitigating it, and focusing on shortcomings of existing systems rather than creating new solutions. Our survey uncovers many opportunities for future work, especially regarding stakeholder involvement, intersectionality, interdisciplinarity, and languages other than English. We also offer an outlook from a queer studies perspective, highlighting understudied topics and gaps in the harms addressed in NLP papers. Beyond being a roadmap of what has been done, this survey is a call to action for work towards more just and inclusive NLP technologies.

</details>


### [107] [Generative AI Usage of University Students: Navigating Between Education and Business](https://arxiv.org/abs/2602.16307)
*Fabian Walke,Veronika Föller*

Main category: cs.CY

TL;DR: 研究探讨在职大学生的生成式AI使用情况，采用扎根理论方法，识别影响使用的因果条件、中介条件和策略，揭示教育与企业交叉使用的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有文献较少关注在职学生以及生成式AI在教育与企业之间的交叉使用，本研究旨在填补这一空白，深入了解这一特殊群体的AI使用特征。

Method: 采用扎根理论方法，对11名远程大学学生进行访谈，通过定性分析识别出影响生成式AI使用的三个因果条件、四个中介条件以及相关策略。

Result: 研究发现生成式AI能显著提升生产力和学习效果，但存在伦理问题、可靠性担忧和学术不端风险；构建的扎根模型为理解学生AI使用提供了全面框架。

Conclusion: 研究揭示了生成式AI在教育与企业交叉使用的双重性，开发的模型为教育者、政策制定者和AI工具开发者提供了有价值的见解，有助于弥合教育与商业之间的鸿沟。

Abstract: This study investigates generative artificial intelligence (GenAI) usage of university students who study alongside their professional career. Previous literature has paid little attention to part-time students and the intersectional use of GenAI between education and business. This study examines with a grounded theory approach the characteristics of GenAI usage of part-time students. Eleven students from a distance learning university were interviewed. Three causal and four intervening conditions, as well as strategies were identified, to influence the use of GenAI. The study highlights both the potential and challenges of GenAI usage in education and business. While GenAI can significantly enhance productivity and learning outcomes, concerns about ethical implications, reliability, and the risk of academic misconduct persist. The developed grounded model offers a comprehensive understanding of GenAI usage among students, providing valuable insights for educators, policymakers, and developers of GenAI tools seeking to bridge the gap between education and business.

</details>


### [108] [Agentic AI, Medical Morality, and the Transformation of the Patient-Physician Relationship](https://arxiv.org/abs/2602.16553)
*Robert Ranisch,Sabine Salloch*

Main category: cs.CY

TL;DR: 本文探讨了代理式AI在医疗领域的伦理影响，重点关注其如何改变医疗道德结构，而非传统AI伦理问题。


<details>
  <summary>Details</summary>
Motivation: 代理式AI作为医疗数字化转型的新阶段，具有自主行动和复杂任务协调能力，可能从根本上改变医患关系和医疗道德概念，这需要提前的伦理关注。

Method: 采用技术-道德变革框架，从决策、关系和感知三个领域分析代理式AI如何重塑医患关系和医疗道德结构。

Result: 代理式AI可能对医疗道德结构产生深刻但不可完全预测的变革，这些转变需要在广泛部署前进行伦理审视。

Conclusion: 应将伦理预见性整合到代理式AI的设计和使用中，以应对其对医疗道德结构的潜在重塑。

Abstract: The emergence of agentic AI marks a new phase in the digital transformation of healthcare. Distinct from conventional generative AI, agentic AI systems are capable of autonomous, goal-directed actions and complex task coordination. They promise to support or even collaborate with clinicians and patients in increasingly independent ways. While agentic AI raises familiar moral concerns regarding safety, accountability, and bias, this article focuses on a less explored dimension: its capacity to transform the moral fabric of healthcare itself. Drawing on the framework of techno-moral change and the three domains of decision, relation and perception, we investigate how agentic AI might reshape the patient-physician relationship and reconfigure core concepts of medical morality. We argue that these shifts, while not fully predictable, demand ethical attention before widespread deployment. Ultimately, the paper calls for integrating ethical foresight into the design and use of agentic AI.

</details>


### [109] [Hidden in Plain Sight: Detecting Illicit Massage Businesses from Mobility Data](https://arxiv.org/abs/2602.16561)
*Roya Shomali,Nick Freeman,Greg Bott,Iman Dayarian,Jason Parton*

Main category: cs.CY

TL;DR: 利用移动数据检测非法按摩店，通过正未标记学习处理标签不对称问题，模型AUC达0.97，识别出四大运营特征，能帮助执法部门将检查效率提高5.3倍。


<details>
  <summary>Details</summary>
Motivation: 非法按摩店(IMBs)伪装成合法按摩店进行性交易和人口贩卖，是美国室内性交易的最大部分之一。现有基于在线评论的检测方法容易被操纵，而移动数据能覆盖那些完全避免数字可见性的场所，为执法部门提供更可靠的检测手段。

Method: 从移动数据中提取特征：时间访问模式、停留时间、访客吸引区域和需求稳定性。由于只有通过广告平台确认的标签，采用正未标记学习处理标签不对称问题。开发决策支持系统为执法部门提供校准的优先级评分。

Result: 模型达到0.97 AUC和0.84平均精度。识别出四大运营特征：需求一致性、晚间集中访问、压缩服务时长和本地客户群。将最高风险的10%按摩店作为检查目标，能捕获53%已知非法运营，比无信息检查提高5.3倍效率。

Conclusion: 移动数据为检测非法按摩店提供了有效方法，其运营特征难以被操纵，因为反映实际运营而非可控制的在线信号。该系统能帮助执法部门优化有限的检查资源，集中检查最高风险场所。

Abstract: Illicit massage businesses (IMBs) masquerade as legitimate massage parlors while facilitating commercial sex and human trafficking. Law enforcement must identify these businesses within a dense population of lawful establishments, but investigative resources are limited and the illicit status of each location is unknown until inspection. Detection methods based on online reviews offer some insight, yet operators can manipulate these signals, leaving covert establishments undetected. IMBs constitute one of the largest segments of indoor sex trafficking in the United States, with an estimated 9,000 establishments. Mobility data offers an alternative to online signals, covering establishments that avoid digital visibility entirely. We derive features from mobility data spanning temporal visitation patterns, dwell times, visitor catchment areas, and demand stability. Because confirmed labels exist only for establishments identified through advertising platforms, we employ positive-unlabeled learning to address the label asymmetry in ground truth. The model achieves 0.97 AUC and 0.84 Average Precision. Four operational signatures characterize high-risk establishments: demand consistency, evening-concentrated visits, compressed service durations, and locally drawn clientele. The model produces risk scores for each business-week observation. Aggregating to the business level, prioritizing the highest-risk 10% of massage establishments captures 53% of known illicit operations, a 5.3-fold improvement over uninformed inspection. We develop a decision-support system that produces calibrated prioritization scores for law enforcement, enabling investigators to concentrate inspections on the highest-risk venues. The operational signatures may resist strategic manipulation because they reflect actual operations rather than online signals that operators can control.

</details>


### [110] [Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology](https://arxiv.org/abs/2602.16703)
*Shen Zhou Hong,Alex Kleinman,Alyssa Mathiowetz,Adam Howes,Julian Cohen,Suveer Ganta,Alex Letizia,Dora Liao,Deepika Pahari,Xavier Roberts-Gaal,Luca Righetti,Joe Torres*

Main category: cs.CY

TL;DR: LLM辅助并未显著提高新手完成复杂实验室工作流程的成功率，但在个别任务中显示出适度性能提升，揭示了AI生物安全评估需要物理世界验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生物学基准测试中表现优异，引发了对其可能帮助新手获取双重用途实验室技能的担忧，但尚不清楚这是否能转化为物理实验室中人类表现的改善。

Method: 采用预注册、研究者盲法、随机对照试验（2025年6-8月，n=153），评估LLM是否改善新手在模拟病毒反向遗传学工作流程任务中的表现。

Result: 工作流程完成率无显著差异（LLM 5.2% vs 互联网 6.6%），个别任务成功率也无显著差异，但LLM组在四项任务中数值更高，特别是细胞培养任务（68.8% vs 55.3%）。贝叶斯模型估计LLM辅助下典型任务成功率约提高1.4倍。

Conclusion: 2025年中期的LLM并未显著提高新手完成复杂实验室程序的能力，但显示出适度的性能提升。结果揭示了计算机基准测试与实际效用之间的差距，强调随着模型能力和用户熟练度发展，需要对AI生物安全评估进行物理世界验证。

Abstract: Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a "typical" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [111] [Close-enough general routing problem for multiple unmanned aerial vehicles in monitoring missions](https://arxiv.org/abs/2602.15841)
*Huan Liu,Michel Gendreau,Binjie Xu,Guohua Wu,Yi Gu*

Main category: eess.SY

TL;DR: 提出一种解决无人机近距离通用路由问题的两阶段迭代算法，结合变量邻域下降、二阶锥规划和自适应迭代局部搜索，优化无人机监控任务的总距离。


<details>
  <summary>Details</summary>
Motivation: 无人机监控任务中，每个节点有其磁盘邻域，需要优化无人机路径以最小化总距离，但现有方法未能充分考虑节点邻域特性。

Method: 提出两阶段迭代方法：1)通用路由阶段使用变量邻域下降(VND)获得包含所需节点和边的满意路径；2)近距离路由阶段使用二阶锥规划(SOCP)优化每个所需节点的代表点。两阶段在自适应迭代局部搜索(AILS)框架下迭代执行。

Result: 通过大量实验和比较研究，证明了AILS-VND-SOCP算法的效率以及磁盘邻域的优越性。

Conclusion: 提出的两阶段迭代算法能有效解决无人机近距离通用路由问题，磁盘邻域方法在优化无人机监控任务路径方面具有优势。

Abstract: In this paper, we introduce a close-enough multi-UAV general routing problem (CEMUAVGRP) where a fleet of homogeneous UAVs conduct monitoring tasks containing nodes, each of which has its disk neighborhood, and edges, aiming to minimize the total distance. A two-phase iterative method is proposed, partitioning the CEMUAVGRP into a general routing phase where a satisfactory route including required nodes and edges for each UAV is obtained without considering the disk neighborhoods of required nodes, and a close-enough routing phase where representative points are optimized for each required node in the determined route. To be specific, a variable neighborhood descent (VND) heuristic is proposed for the general routing phase, while a second-order cone programming (SOCP) procedure is applied in the close-enough routing phase. These two phases are performed in an iterative fashion under the framework of an adaptive iterated local search (AILS) algorithm until the predefined termination criteria are satisfied. Extensive experiments and comparative studies are conducted, demonstrating the efficiency of the proposed AILS-VND-SOCP algorithm and the superiority of disk neighborhoods.

</details>


### [112] [Stability and convergence of multi-converter systems using projection-free power-limiting droop control](https://arxiv.org/abs/2602.16036)
*Amirhossein Iraniparast,Dominic Groß*

Main category: eess.SY

TL;DR: 提出一种无投影功率限制下垂控制方法，用于并网电力电子设备，解决约束潮流问题，相比投影方法具有半全局指数稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统基于投影的功率限制下垂控制存在稳定性问题，需要开发更稳定、收敛性更好的控制方法来解决电力系统中的约束潮流问题。

Method: 提出无投影功率限制下垂控制方法，通过边缘坐标变换将网络动态转化为增广拉格朗日的无投影原始-对偶动态，建立收敛性分析框架。

Result: 证明了无投影控制方法具有半全局指数稳定性，提供了收敛速率边界，提出了控制器参数整定方法，分析了网络连通性与收敛速率的关系，并通过EMT仿真验证。

Conclusion: 无投影功率限制下垂控制方法在稳定性、收敛性和参数整定方面优于传统投影方法，为电力系统控制提供了理论保证和实用工具。

Abstract: In this paper, we propose a projection-free power-limiting droop control for grid-connected power electronics and an associated constrained flow problem. In contrast to projection-based power-limiting droop control, the novel projection-free power-limiting droop control results in networked dynamics that are semi-globally exponentially stable with respect to the set of optimizers of the constrained flow problem. Under a change to edge coordinates, the overall networked dynamics arising from projection-free power-limiting droop control coincide with the projection-free primal-dual dynamics associated with an augmented Lagrangian of the constrained flow problem. Leveraging this result, we (i) provide a bound on the convergence rate of the projection-free networked dynamics, (ii) propose a tuning method for controller parameters to improve the bound on the convergence rate, and (iii) analyze the relationship of the bound on the convergence rate and connectivity of the network. Finally, the analytical results are illustrated using an Electromagnetic transient (EMT) simulation.

</details>


### [113] [Harnessing Implicit Cooperation: A Multi-Agent Reinforcement Learning Approach Towards Decentralized Local Energy Markets](https://arxiv.org/abs/2602.16062)
*Nelson Salazar-Pena,Alejandra Tabares,Andres Gonzalez-Mancera*

Main category: eess.SY

TL;DR: 提出隐式合作框架，让去中心化智能体无需显式通信即可在本地能源市场中近似最优协调，使用信息素信号（系统级KPI）推断全局状态，通过多智能体强化学习解决。


<details>
  <summary>Details</summary>
Motivation: 解决本地能源市场中去中心化智能体协调问题，避免昂贵的集中式通信基础设施，同时保护隐私，通过隐式信号实现高效协调。

Method: 将问题建模为去中心化部分可观测马尔可夫决策过程，通过多智能体强化学习解决，使用信息素信号（系统级关键性能指标）推断全局状态。采用3x3因子设计（三种训练范式×三种算法）在IEEE 34节点拓扑上评估。

Result: APPO-DTDE配置最优，达到理论集中式基准（CTCE）91.7%的协调分数。去中心化方法（DTDE）在物理稳定性上表现更佳，减少电网平衡方差31%，形成高度可预测的进口偏向负荷曲线。拓扑分析显示出现空间聚类现象。

Conclusion: 信息素信号为复杂电网协调提供足够上下文，是昂贵集中式通信基础设施的稳健、隐私保护替代方案。存在效率与稳定性之间的关键权衡：集中式最大化分配效率，去中心化提供更优物理稳定性。

Abstract: This paper proposes implicit cooperation, a framework enabling decentralized agents to approximate optimal coordination in local energy markets without explicit peer-to-peer communication. We formulate the problem as a decentralized partially observable Markov decision problem that is solved through a multi-agent reinforcement learning task in which agents use stigmergic signals (key performance indicators at the system level) to infer and react to global states. Through a 3x3 factorial design on an IEEE 34-node topology, we evaluated three training paradigms (CTCE, CTDE, DTDE) and three algorithms (PPO, APPO, SAC). Results identify APPO-DTDE as the optimal configuration, achieving a coordination score of 91.7% relative to the theoretical centralized benchmark (CTCE). However, a critical trade-off emerges between efficiency and stability: while the centralized benchmark maximizes allocative efficiency with a peer-to-peer trade ratio of 0.6, the fully decentralized approach (DTDE) demonstrates superior physical stability. Specifically, DTDE reduces the variance of grid balance by 31% compared to hybrid architectures, establishing a highly predictable, import-biased load profile that simplifies grid regulation. Furthermore, topological analysis reveals emergent spatial clustering, where decentralized agents self-organize into stable trading communities to minimize congestion penalties. While SAC excelled in hybrid settings, it failed in decentralized environments due to entropy-driven instability. This research proves that stigmergic signaling provides sufficient context for complex grid coordination, offering a robust, privacy-preserving alternative to expensive centralized communication infrastructure.

</details>


### [114] [MARLEM: A Multi-Agent Reinforcement Learning Simulation Framework for Implicit Cooperation in Decentralized Local Energy Markets](https://arxiv.org/abs/2602.16063)
*Nelson Salazar-Pena,Alejandra Tabares,Andres Gonzalez-Mancera*

Main category: eess.SY

TL;DR: 该论文提出了一个开源的MARL仿真框架，用于研究本地能源市场中的隐式合作，通过增强系统级KPI到智能体的观测和奖励中，使智能体能够独立学习有益于整个系统的策略，实现无需显式通信的集体利益。


<details>
  <summary>Details</summary>
Motivation: 研究本地能源市场中的隐式合作问题，为未来智能、去中心化的能源系统提供设计、测试和验证策略的工具。现有框架缺乏对隐式合作机制的系统性研究支持。

Method: 开发了一个模块化的MARL仿真框架，建模为去中心化部分可观测马尔可夫决策过程，实现为Gymnasium环境。框架包含可插拔的清算机制、物理约束的智能体模型（包括电池储能）、真实的电网网络和全面的分析套件。核心方法是增强智能体的观测和奖励，加入系统级关键性能指标。

Result: 通过代表性案例研究展示了框架分析不同市场配置（如不同储能部署）对系统性能影响的能力。证明了框架能够促进涌现协调、提高市场效率和增强电网稳定性。

Conclusion: 提出的仿真框架是一个灵活、可扩展和可复现的工具，为研究人员和从业者设计、测试和验证未来智能、去中心化能源系统的策略提供了有力支持。

Abstract: This paper introduces a novel, open-source MARL simulation framework for studying implicit cooperation in LEMs, modeled as a decentralized partially observable Markov decision process and implemented as a Gymnasium environment for MARL. Our framework features a modular market platform with plug-and-play clearing mechanisms, physically constrained agent models (including battery storage), a realistic grid network, and a comprehensive analytics suite to evaluate emergent coordination. The main contribution is a novel method to foster implicit cooperation, where agents' observations and rewards are enhanced with system-level key performance indicators to enable them to independently learn strategies that benefit the entire system and aim for collectively beneficial outcomes without explicit communication. Through representative case studies (available in a dedicated GitHub repository in https://github.com/salazarna/marlem, we show the framework's ability to analyze how different market configurations (such as varying storage deployment) impact system performance. This illustrates its potential to facilitate emergent coordination, improve market efficiency, and strengthen grid stability. The proposed simulation framework is a flexible, extensible, and reproducible tool for researchers and practitioners to design, test, and validate strategies for future intelligent, decentralized energy systems.

</details>


### [115] [Tunable Ferroelectric Acoustic Resonators in Monolithic Thin-Film Barium Titanate](https://arxiv.org/abs/2602.16102)
*Ian Anderson,Agham Posadas,Alexander A. Demkov,Ruochen Lu*

Main category: eess.SY

TL;DR: 该研究展示了在硅衬底上外延生长的钛酸钡薄膜作为可调谐声学谐振器平台，通过横向电极激发对称兰姆波模式，在亚GHz频段实现频率调谐和品质因数增强。


<details>
  <summary>Details</summary>
Motivation: 无线通信频段的快速发展需要紧凑、低损耗且频率可调的射频滤波技术。声学谐振器是满足这些要求的理想解决方案，而可调谐实现为可重构前端提供了途径。

Method: 使用有限元仿真设计器件，在X切钛酸钡薄膜上制作横向图案化电极，通过施加直流偏压使铁电畴对齐，实现声学模式的电激励、频率调谐和品质因数增强。

Result: 在300MHz和700MHz附近观察到谐振，机电耦合系数高达8%，具有偏压相关的频率调谐特性，在20V附近表现出明显的行为转变。

Conclusion: 单晶钛酸钡/硅材料系统是用于可重构射频应用的横向激发可调谐声学谐振器的有前景平台。

Abstract: The increasing development of wireless communication bands has motivated the development of compact, low-loss, and frequency adjustable RF filtering technologies. Acoustic resonators are the ideal solution to these requirements, and tunable implementations offer a path toward reconfigurable front ends. In this work, we investigate epitaxial barium titanate (BTO) grown on silicon as a platform for tunable acoustic resonators operating in the sub-GHz regime. We demonstrate lateral excitation of symmetric lamb (S0) modes in X-cut BTO membranes, in contrast to prior thickness-defined ferroelectric resonators. Devices are designed using finite-element simulations and fabricated with laterally patterned electrodes that enable overtone coupling to multiple resonant modes. Under applied DC bias, ferroelectric domains align, allowing electrical excitation, frequency tuning, and quality-factor enhancement of acoustic modes. Resonances near 300 MHz and 700 MHz exhibit electromechanical coupling up to 8% and bias-dependent frequency tuning, with a distinct transition in behavior near 20 V. These results highlight monolithic BTO on silicon as a promising material system for laterally excited, tunable acoustic resonators for reconfigurable RF applications.

</details>


### [116] [Discovering Unknown Inverter Governing Equations via Physics-Informed Sparse Machine Learning](https://arxiv.org/abs/2602.16166)
*Jialin Zheng,Ruhaan Batta,Zhong Liu,Xiaonan Lu*

Main category: eess.SY

TL;DR: 提出PISML框架，通过稀疏符号主干和神经残差分支结合，从外部测量数据中发现并网逆变器的未知控制方程，实现高精度识别和可解释模型转换。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在识别未建模非线性与控制方程物理一致性之间取得平衡，而现代逆变器密集型电力系统需要从外部测量数据中发现未知控制方程。

Method: 提出物理信息稀疏机器学习框架，集成稀疏符号主干捕捉主导模型骨架，神经残差分支补偿复杂非线性控制逻辑，引入雅可比正则化物理信息训练机制保证多尺度一致性。

Result: 在高保真硬件在环平台上验证，相比基线误差降低340倍以上，实现高分辨率识别，并将复杂神经网络压缩为紧凑显式形式，计算复杂度降低数量级。

Conclusion: PISML框架为将结构不可访问设备转换为显式数学模型提供了统一途径，使得具有未知逆变器控制方程的电力系统稳定性分析成为可能。

Abstract: Discovering the unknown governing equations of grid-connected inverters from external measurements holds significant attraction for analyzing modern inverter-intensive power systems. However, existing methods struggle to balance the identification of unmodeled nonlinearities with the preservation of physical consistency. To address this, this paper proposes a Physics-Informed Sparse Machine Learning (PISML) framework. The architecture integrates a sparse symbolic backbone to capture dominant model skeletons with a neural residual branch that compensates for complex nonlinear control logic. Meanwhile, a Jacobian-regularized physics-informed training mechanism is introduced to enforce multi-scale consistency including large/small-scale behaviors. Furthermore, by performing symbolic regression on the neural residual branch, PISML achieves a tractable mapping from black-box data to explicit control equations. Experimental results on a high-fidelity Hardware-in-the-Loop platform demonstrate the framework's superior performance. It not only achieves high-resolution identification by reducing error by over 340 times compared to baselines but also realizes the compression of heavy neural networks into compact explicit forms. This restores analytical tractability for rigorous stability analysis and reduces computational complexity by orders of magnitude. It also provides a unified pathway to convert structurally inaccessible devices into explicit mathematical models, enabling stability analysis of power systems with unknown inverter governing equations.

</details>


### [117] [Collaborative Safe Bayesian Optimization](https://arxiv.org/abs/2602.16235)
*Alina Castell Blasco,Maxime Bouton*

Main category: eess.SY

TL;DR: 首次将安全贝叶斯优化应用于移动网络，提出CoSBO算法，通过多任务协作和多重安全约束实现高效安全的在线参数调优


<details>
  <summary>Details</summary>
Motivation: 移动网络需要安全优化以适应流量需求和信号传输质量的变化，同时提升服务性能指标。随着网络复杂度增加，传统参数调优方法变得过于保守或复杂

Method: 提出CoSBO（安全协作优化）算法，利用网络中多个优化任务的信息，考虑多重安全约束，实现安全的贝叶斯优化

Result: CoSBO算法能够在很少的迭代次数内安全地在线调优网络参数，在优化过程早期阶段相比SafeOpt-MC算法提高了样本效率

Conclusion: 安全贝叶斯优化适用于移动网络参数调优，CoSBO算法通过协作优化和多重安全约束实现了高效安全的在线优化

Abstract: Mobile networks require safe optimization to adapt to changing conditions in traffic demand and signal transmission quality, in addition to improving service performance metrics. With the increasing complexity of emerging mobile networks, traditional parameter tuning methods become too conservative or complex to evaluate. For the first time, we apply safe Bayesian optimization to mobile networks. Moreover, we develop a new safe collaborative optimization algorithm called CoSBO, leveraging information from multiple optimization tasks in the network and considering multiple safety constraints. The resulting algorithm is capable of safely tuning the network parameter online with very few iterations. We demonstrate that the proposed method improves sample efficiency in the early stages of the optimization process by comparing it against the SafeOpt-MC algorithm in a mobile network scenario.

</details>


### [118] [Autonomous and non-autonomous fixed-time leader-follower consensus for second-order multi-agent systems](https://arxiv.org/abs/2602.16260)
*Miguel A. Trujillo,Rodrigo Aldana-López,David Gomez Gutierrez,Michael Defoort,Javier Ruiz Leon,Hector M. Becerra*

Main category: eess.SY

TL;DR: 该论文针对具有双积分器动态的领导者-跟随者多智能体系统，研究了固定时间收敛的共识跟踪问题，其中只有部分跟随者能获取领导者状态。提出了分布式固定时间状态估计和跟踪控制的两步策略，并比较了自主和非自主两种协议。


<details>
  <summary>Details</summary>
Motivation: 在领导者-跟随者多智能体系统中，只有部分跟随者能直接获取领导者状态信息，需要设计分布式控制策略。现有方法在收敛时间方面存在局限性，特别是需要预设收敛时间上界，因此需要开发固定时间收敛的控制协议。

Method: 采用两步控制方案：第一步，每个跟随者通过分布式方式在固定时间内估计领导者状态；第二步，基于估计的领导者状态，每个跟随者计算控制律在固定时间内跟踪领导者。研究了两种控制策略：自主协议（用户预设收敛时间上界）和基于时变增益的非自主协议（获得更保守的收敛时间估计）。

Result: 提出的两种共识协议都能有效解决固定时间共识跟踪问题。自主协议允许用户预设收敛时间上界，而非自主协议通过时变增益获得了更保守的收敛时间估计，同时保证时变增益有界。数值仿真验证了协议的有效性。

Conclusion: 该论文成功解决了双积分器多智能体系统的固定时间共识跟踪问题，提出了两种有效的控制策略。非自主协议相比自主协议能提供更保守的收敛时间估计，为分布式控制中的固定时间收敛问题提供了新的解决方案。

Abstract: This paper addresses the problem of consensus tracking with fixed-time convergence, for leader-follower multi-agent systems with double-integrator dynamics, where only a subset of followers has access to the state of the leader. The control scheme is divided into two steps. The first one is dedicated to the estimation of the leader state by each follower in a distributed way and in a fixed-time. Then, based on the estimate of the leader state, each follower computes its control law to track the leader in a fixed-time. In this paper, two control strategies are investigated and compared to solve the two mentioned steps. The first one is an autonomous protocol which ensures a fixed-time convergence for the observer and for the controller parts where the Upper Bound of the Settling-Time (UBST) is set a priory by the user. Then, the previous strategy is redesigned using time-varying gains to obtain a non-autonomous protocol. This enables to obtain less conservative estimates of the UBST while guaranteeing that the time-varying gains remain bounded. Some numerical examples show the effectiveness of the proposed consensus protocols.

</details>


### [119] [Certifying Hamilton-Jacobi Reachability Learned via Reinforcement Learning](https://arxiv.org/abs/2602.16475)
*Prashant Solanki,Isabelle El-Hajj,Jasper J. van Beers,Erik-Jan van Kampen,Coen C. de Visser*

Main category: eess.SY

TL;DR: 提出一个框架来认证强化学习学习的Hamilton-Jacobi可达性，通过折扣初始时间旅行成本公式，将学习误差转换为严格后向可达管的校准内外包络。


<details>
  <summary>Details</summary>
Motivation: 强化学习在安全关键系统中应用时，需要保证学习到的值函数能够提供可靠的安全保证。现有方法缺乏对学习误差与Hamilton-Jacobi可达性之间关系的严格认证。

Method: 基于折扣初始时间旅行成本公式，建立小步长强化学习值迭代与带阻尼前向Hamilton-Jacobi方程的等价性。利用加性偏移恒等式将均匀值误差转换为常数HJB偏移，通过Bellman算子残差界和HJB PDE松弛界两种途径建立均匀值误差。

Result: 在双积分器系统上，通过可满足性模理论（SMT）正式认证了强化学习学习的值函数，在感兴趣紧致区域上诱导出可证明正确的内外后向可达集包络。

Conclusion: 该框架保持了Hamilton-Jacobi级别的安全语义，与深度强化学习兼容，能够将学习误差转换为校准的安全保证，为强化学习在安全关键应用中的部署提供了认证工具。

Abstract: We present a framework to \emph{certify} Hamilton--Jacobi (HJ) reachability learned by reinforcement learning (RL). Building on a discounted initial time \emph{travel-cost} formulation that makes small-step RL value iteration provably equivalent to a forward Hamilton--Jacobi (HJ) equation with damping, we convert certified learning errors into calibrated inner/outer enclosures of strict backward reachable tube. The core device is an additive-offset identity: if $W_λ$ solves the discounted travel-cost Hamilton--Jacobi--Bellman (HJB) equation, then $W_\varepsilon:=W_λ+ \varepsilon$ solves the same PDE with a constant offset $λ\varepsilon$. This means that a uniform value error is \emph{exactly} equal to a constant HJB offset. We establish this uniform value error via two routes: (A) a Bellman operator-residual bound, and (B) a HJB PDE-slack bound. Our framework preserves HJ-level safety semantics and is compatible with deep RL. We demonstrate the approach on a double-integrator system by formally certifying, via satisfiability modulo theories (SMT), a value function learned through reinforcement learning to induce provably correct inner and outer backward-reachable set enclosures over a compact region of interest.

</details>


### [120] [Optimal Placement and Sizing of PV-Based DG Units in a Distribution Network Considering Loading Capacity](https://arxiv.org/abs/2602.16565)
*Abhinav Sharma,Pratyush Chakraborty,Manoj Datta,Kazi N. Hasan*

Main category: eess.SY

TL;DR: 提出一种考虑网络负载能力的径向配电网中多光伏分布式发电单元优化配置方法，采用两阶段方法确定最佳位置和容量，显著降低网络损耗并改善电压分布。


<details>
  <summary>Details</summary>
Motivation: 在径向配电网中优化配置分布式发电单元时，需要考虑网络的负载能力限制，以确保系统安全稳定运行，同时最大化分布式发电的效益。

Method: 采用两阶段方法：第一阶段通过迭代方法确定网络和各母线的附加有功负载能力，识别高负载能力母线作为候选节点；第二阶段使用蒙特卡洛方法优化确定DG单元的位置和容量，目标是最小化电压偏差和减少有功损耗。

Result: 在IEEE 33总线标准测试系统上验证，配置1、2、3个DG单元分别减少网络有功损耗50.37%、58.62%、65.16%，显著改善所有母线的电压分布，相比现有方法允许更大的DG容量并维持更好的电压分布。

Conclusion: 所提出的两阶段方法能有效确定径向配电网中分布式发电单元的最优配置，在考虑网络负载能力的同时显著降低损耗并改善电压质量，具有实用价值。

Abstract: This research paper proposes an efficient methodology for the allocation of multiple photovoltaic (PV)-based distributed generation (DG) units in the radial distribution network (RDN), while considering the loading capacity of the network. The proposed method is structured using a two-stage approach. In the first stage, the additional active power loading capacity of the network and each individual bus is determined using an iterative approach. This analysis quantifies the network's additional active loadability limits and identifies buses with high active power loading capacity, which are considered candidate nodes for the placement of DG units. Subsequently, in the second stage, the optimal locations and sizes of DG units are determined using the Monte Carlo method, with the objectives of minimizing voltage deviation and reducing active power losses in the network. The methodology is validated on the standard IEEE 33-bus RDN to determine the optimal locations and sizes of DG units. The results demonstrate that the optimal allocation of one, two, and three DG units, achieved from proposed method, reduces network active power losses by 50.37%, 58.62%, and 65.16%, respectively, and also significantly enhances the voltage profile across all buses. When the obtained results are compared with the results of several existing studies, it is found that the proposed method allows for larger DG capacities and maintains better voltage profiles throughout the RDN.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [121] [Stackelberg Equilibria in Monopoly Insurance Markets with Probability Weighting](https://arxiv.org/abs/2602.16401)
*Maria Andraos,Mario Ghossoub,Bin Li,Benxuan Shi*

Main category: q-fin.RM

TL;DR: 研究垄断集中序贯保险市场中的Stackelberg均衡，分析保险公司使用扭曲保费原则定价时，保单持有人最小化扭曲风险度量下的最优保险合约结构。


<details>
  <summary>Details</summary>
Motivation: 研究垄断保险市场中保险公司（领导者）和保单持有人（追随者）之间的序贯博弈均衡，探讨在扭曲保费定价原则下，风险厌恶程度如何影响保险覆盖范围和保险公司利润。

Method: 采用Stackelberg博弈框架，保险公司作为领导者使用扭曲保费原则设定保费，保单持有人作为追随者最小化扭曲风险度量。通过分析均衡条件，推导最优赔偿函数的结构特征。

Result: 均衡时最优赔偿函数呈分层结构：在保单持有人比保险公司对尾部损失更悲观的损失层提供全额保险，反之则不提供保险。定价扭曲函数由保单持有人风险厌恶程度决定，价格不超过其边际保险意愿。保险覆盖范围和保险公司利润随风险厌恶程度增加而增加。

Conclusion: 均衡合约是帕累托有效的，但不会给保单持有人带来福利增益。任何不给保单持有人带来福利增益的帕累托最优合约都可以作为均衡合约获得。研究结果包含了文献中现有结果作为特例。

Abstract: We study Stackelberg Equilibria (Bowley optima) in a monopolistic centralized sequential-move insurance market, with a profit-maximizing insurer who sets premia using a distortion premium principle, and a single policyholder who seeks to minimize a distortion risk measure. We show that equilibria are characterized as follows: In equilibrium, the optimal indemnity function exhibits a layer-type structure, providing full insurance over any loss layer on which the policyholder is more pessimistic than the insurer's pricing functional about tail losses; and no insurance coverage over loss layers on which the policyholder is less pessimistic than the insurer's pricing functional about tail losses. In equilibrium, the optimal pricing distortion function is determined by the policyholder's degree of risk aversion, whereby prices never exceed the policyholder's marginal willingness to insure tail losses. Moreover, we show that both the insurance coverage and the insurer's expected profit increase with the policyholder's degree of risk aversion. Additionally, and echoing recent work in the literature, we show that equilibrium contracts are Pareto efficient, but they do not induce a welfare gain to the policyholder. Conversely, any Pareto-optimal contract that leaves no welfare gain to the policyholder can be obtained as an equilibrium contract. Finally, we consider a few examples of interest that recover some existing results in the literature as special cases of our analysis.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [122] [A Wiener Chaos Approach to Martingale Modelling and Implied Volatility Calibration](https://arxiv.org/abs/2602.16232)
*Pere Diaz-Lozano,Thomas K. Kloster*

Main category: q-fin.MF

TL;DR: 基于Wiener混沌展开构建过参数化鞅模型，实现期权价格曲面的快速校准


<details>
  <summary>Details</summary>
Motivation: 期权价格曲面校准需要灵活的风险中性鞅模型，传统模型在表达能力和校准效率上存在限制

Method: 利用鞅表示定理，通过截断Wiener混沌展开近似鞅的终端值，通过条件期望恢复中间动态，采用Hermite多项式实现

Result: 获得易于实现的表达式，能够快速校准目标隐含波动率曲面，在模拟和真实市场数据上展示了模型的灵活性和表达能力

Conclusion: 基于Wiener混沌展开的过参数化鞅模型为期权定价提供了灵活高效的校准框架

Abstract: Calibration to a surface of option prices requires specifying a suitably flexible martingale model for the discounted asset price under a risk-neutral measure. Assuming Brownian noise and mean-square integrability, we construct an over-parameterized model based on the martingale representation theorem. In particular, we approximate the terminal value of the martingale via a truncated Wiener--chaos expansion and recover the intermediate dynamics by computing the corresponding conditional expectations. Using the Hermite-polynomial formulation of the Wiener chaos, we obtain easily implementable expressions that enable fast calibration to a target implied-volatility surface. We illustrate the flexibility and expressive power of the resulting model through numerical experiments on both simulated and real market data.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [123] [Generalized Leverage Score for Scalable Assessment of Privacy Vulnerability](https://arxiv.org/abs/2602.15919)
*Valentin Dorseuil,Jamal Atif,Olivier Cappé*

Main category: stat.ML

TL;DR: 论文提出利用杠杆分数评估数据点在成员推理攻击中的隐私风险，无需重新训练模型或模拟攻击，为深度学习提供高效隐私评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统评估数据点隐私风险需要重新训练模型或模拟攻击，计算成本高。本文旨在寻找无需这些复杂操作就能评估个体数据隐私脆弱性的方法。

Method: 在线性模型设置中，理论证明成员推理攻击风险与杠杆分数相关。基于此，提出深度学习环境下杠杆分数的计算高效泛化方法。

Result: 实验验证了提出的分数与成员推理攻击成功率之间存在强相关性，表明该指标可作为个体隐私风险评估的实用替代方法。

Conclusion: 数据点对学习模型的影响决定了其在成员推理攻击中的暴露风险，杠杆分数及其泛化形式为高效隐私风险评估提供了理论基础和实用工具。

Abstract: Can the privacy vulnerability of individual data points be assessed without retraining models or explicitly simulating attacks? We answer affirmatively by showing that exposure to membership inference attack (MIA) is fundamentally governed by a data point's influence on the learned model. We formalize this in the linear setting by establishing a theoretical correspondence between individual MIA risk and the leverage score, identifying it as a principled metric for vulnerability. This characterization explains how data-dependent sensitivity translates into exposure, without the computational burden of training shadow models. Building on this, we propose a computationally efficient generalization of the leverage score for deep learning. Empirical evaluations confirm a strong correlation between the proposed score and MIA success, validating this metric as a practical surrogate for individual privacy risk assessment.

</details>


### [124] [Including Node Textual Metadata in Laplacian-constrained Gaussian Graphical Models](https://arxiv.org/abs/2602.15920)
*Jianhua Wang,Killian Cressant,Pedro Braconnot Velloso,Arnaud Breloy*

Main category: stat.ML

TL;DR: 提出一种融合节点信号和元数据的图学习方法，基于拉普拉斯约束的高斯图模型，通过MM算法优化，在金融数据集上显著提升图聚类性能


<details>
  <summary>Details</summary>
Motivation: 传统的高斯图模型图学习方法通常忽略节点相关的元数据信息（如文本描述），而这些元数据可能包含有价值的信息。为了填补这一空白，需要开发能够同时利用节点信号和元数据的图学习方法。

Method: 提出基于拉普拉斯约束的高斯图模型图学习方法，将节点信号和元数据信息联合建模。通过优化问题形式化，开发了高效的主化-最小化（MM）算法，每次迭代都有闭式解。

Result: 在真实世界金融数据集上的实验结果表明，该方法相比仅使用信号或仅使用元数据的最先进方法，显著提高了图聚类性能，证明了融合两种信息源的价值。

Conclusion: 提出的融合节点信号和元数据的图学习方法有效提升了高斯图模型中的图学习性能，证明了在传统图估计过程中整合元数据信息的重要性。

Abstract: This paper addresses graph learning in Gaussian Graphical Models (GGMs). In this context, data matrices often come with auxiliary metadata (e.g., textual descriptions associated with each node) that is usually ignored in traditional graph estimation processes. To fill this gap, we propose a graph learning approach based on Laplacian-constrained GGMs that jointly leverages the node signals and such metadata. The resulting formulation yields an optimization problem, for which we develop an efficient majorization-minimization (MM) algorithm with closed-form updates at each iteration. Experimental results on a real-world financial dataset demonstrate that the proposed method significantly improves graph clustering performance compared to state-of-the-art approaches that use either signals or metadata alone, thus illustrating the interest of fusing both sources of information.

</details>


### [125] [Robust Stochastic Gradient Posterior Sampling with Lattice Based Discretisation](https://arxiv.org/abs/2602.15925)
*Zier Mensch,Lars Holdijk,Samuel Duffield,Maxwell Aifer,Patrick J. Coles,Max Welling,Miranda C. N. Cheng*

Main category: stat.ML

TL;DR: 提出SGLRW方法，通过仅在更新协方差矩阵的非对角元素引入随机噪声，提高对minibatch大小的鲁棒性，同时保持渐近正确性


<details>
  <summary>Details</summary>
Motivation: 随机梯度MCMC方法虽然能实现可扩展的贝叶斯后验采样，但对minibatch大小和梯度噪声敏感，需要更鲁棒的算法

Method: 提出随机梯度晶格随机游走(SGLRW)，扩展晶格随机游走离散化方法，仅在更新协方差矩阵的非对角元素引入随机噪声，同时分析使用梯度裁剪的SGLD类比方法

Result: SGLRW在SGLD失败的场景下保持稳定，包括存在重尾梯度噪声的情况，预测性能匹配或优于现有方法

Conclusion: SGLRW通过创新的噪声引入方式，显著提高了随机梯度MCMC方法对minibatch大小的鲁棒性，为贝叶斯推理提供了更稳定的采样算法

Abstract: Stochastic-gradient MCMC methods enable scalable Bayesian posterior sampling but often suffer from sensitivity to minibatch size and gradient noise. To address this, we propose Stochastic Gradient Lattice Random Walk (SGLRW), an extension of the Lattice Random Walk discretization. Unlike conventional Stochastic Gradient Langevin Dynamics (SGLD), SGLRW introduces stochastic noise only through the off-diagonal elements of the update covariance; this yields greater robustness to minibatch size while retaining asymptotic correctness. Furthermore, as comparison we analyze a natural analogue of SGLD utilizing gradient clipping. Experimental validation on Bayesian regression and classification demonstrates that SGLRW remains stable in regimes where SGLD fails, including in the presence of heavy-tailed gradient noise, and matches or improves predictive performance.

</details>


### [126] [Partial Identification under Missing Data Using Weak Shadow Variables from Pretrained Models](https://arxiv.org/abs/2602.16061)
*Hongyu Chen,David Simchi-Levi,Ruoxuan Xiong*

Main category: stat.ML

TL;DR: 提出一个部分识别框架，利用预训练模型（包括LLM）的预测作为弱影子变量，通过线性规划获得估计量的尖锐边界，在MNAR数据下提供有效覆盖。


<details>
  <summary>Details</summary>
Motivation: 在平台评估和社会科学中，从用户反馈估计总体量（如平均结果）很重要，但反馈通常是MNAR（缺失非随机）的：意见更强的用户更可能回应，导致标准估计量有偏且无法识别。现有方法依赖强参数假设或特殊辅助变量，实践中可能不可用。

Method: 开发部分识别框架：通过求解一对线性规划获得估计量的尖锐边界，约束编码观测数据结构。将预训练模型（包括LLM）的预测作为额外线性约束纳入，称为弱影子变量——满足关于缺失的条件独立性，但无需满足经典影子变量方法的完整性条件。

Result: 当预测足够信息丰富时，边界坍缩为点，恢复标准识别。有限样本中，提出集合扩展估计量，在集合识别机制下实现慢于√n的收敛率，在点识别下保持标准√n率。模拟和半合成实验显示，LLM预测对经典影子变量方法条件不佳，但在本框架中高度有效，将识别区间缩小75-83%，同时在现实MNAR机制下保持有效覆盖。

Conclusion: 该框架利用LLM等预训练模型的预测作为弱影子变量，为MNAR数据下的因果推断提供实用解决方案，无需强参数假设或特殊辅助变量，在保持统计有效性的同时显著提高估计精度。

Abstract: Estimating population quantities such as mean outcomes from user feedback is fundamental to platform evaluation and social science, yet feedback is often missing not at random (MNAR): users with stronger opinions are more likely to respond, so standard estimators are biased and the estimand is not identified without additional assumptions. Existing approaches typically rely on strong parametric assumptions or bespoke auxiliary variables that may be unavailable in practice. In this paper, we develop a partial identification framework in which sharp bounds on the estimand are obtained by solving a pair of linear programs whose constraints encode the observed data structure. This formulation naturally incorporates outcome predictions from pretrained models, including large language models (LLMs), as additional linear constraints that tighten the feasible set. We call these predictions weak shadow variables: they satisfy a conditional independence assumption with respect to missingness but need not meet the completeness conditions required by classical shadow-variable methods. When predictions are sufficiently informative, the bounds collapse to a point, recovering standard identification as a special case. In finite samples, to provide valid coverage of the identified set, we propose a set-expansion estimator that achieves slower-than-$\sqrt{n}$ convergence rate in the set-identified regime and the standard $\sqrt{n}$ rate under point identification. In simulations and semi-synthetic experiments on customer-service dialogues, we find that LLM predictions are often ill-conditioned for classical shadow-variable methods yet remain highly effective in our framework. They shrink identification intervals by 75--83\% while maintaining valid coverage under realistic MNAR mechanisms.

</details>


### [127] [Empirical Cumulative Distribution Function Clustering for LLM-based Agent System Analysis](https://arxiv.org/abs/2602.16131)
*Chihiro Watanabe,Jingyu Sun*

Main category: stat.ML

TL;DR: 提出基于经验累积分布函数(ECDF)的LLM代理评估框架，通过响应与参考答案的余弦相似度分布进行更细致的质量评估，并引入ECDF聚类分析揭示不同代理配置的响应分布差异。


<details>
  <summary>Details</summary>
Motivation: 传统LLM代理评估方法（如多数投票聚合）会掩盖原始响应的质量分布特征，需要更细粒度的评估框架来揭示不同代理配置下的响应质量分布差异。

Method: 1) 基于响应与参考答案余弦相似度的ECDF评估框架；2) 使用距离度量和k-medoids算法对ECDF进行聚类分析；3) 在QA数据集上实验，分析温度、角色设定和问题主题对响应分布的影响。

Result: ECDF框架能够区分具有相似最终准确率但响应质量分布不同的代理配置，聚类分析揭示了响应中可解释的群体结构，提供了关于温度、角色和问题主题影响的深入见解。

Conclusion: 提出的ECDF评估框架提供了比传统聚合方法更细致的LLM代理响应质量分析，能够揭示响应分布特征，为理解不同代理配置的影响提供了有价值的工具。

Abstract: Large language models (LLMs) are increasingly used as agents to solve complex tasks such as question answering (QA), scientific debate, and software development. A standard evaluation procedure aggregates multiple responses from LLM agents into a single final answer, often via majority voting, and compares it against reference answers. However, this process can obscure the quality and distributional characteristics of the original responses. In this paper, we propose a novel evaluation framework based on the empirical cumulative distribution function (ECDF) of cosine similarities between generated responses and reference answers. This enables a more nuanced assessment of response quality beyond exact match metrics. To analyze the response distributions across different agent configurations, we further introduce a clustering method for ECDFs using their distances and the $k$-medoids algorithm. Our experiments on a QA dataset demonstrate that ECDFs can distinguish between agent settings with similar final accuracies but different quality distributions. The clustering analysis also reveals interpretable group structures in the responses, offering insights into the impact of temperature, persona, and question topics.

</details>


### [128] [Conjugate Learning Theory: Uncovering the Mechanisms of Trainability and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.16177)
*Binchuan Qi*

Main category: stat.ML

TL;DR: 提出基于凸共轭对偶的共轭学习理论框架，从有限样本角度定义实用可学习性，证明SGD训练DNN能到达经验风险全局最优，并推导泛化误差的确定性和概率性上界。


<details>
  <summary>Details</summary>
Motivation: 现有学习理论大多基于无限样本假设，而实际训练中样本有限。需要建立更贴近实际训练场景的理论框架，从有限样本角度理解深度学习的优化和泛化行为。

Method: 基于凸共轭对偶理论构建共轭学习理论框架。通过控制结构矩阵极端特征值和梯度能量证明SGD的全局收敛性。使用广义条件熵度量推导泛化误差上界，并分析批量大小、网络架构等因素的影响。

Result: 理论证明：1) SGD训练DNN能达到经验风险全局最优；2) 推导出模型无关的经验风险下界；3) 获得泛化误差的确定性和概率性上界，明确量化不可逆性、最大损失值、特征条件熵三个因素的影响；4) 实验验证所有理论预测。

Conclusion: 提出的共轭学习理论框架为理解深度学习在有限样本下的优化和泛化提供了统一的理论视角，揭示了数据决定可训练性的根本极限，并为正则化、不可逆变换、网络深度等对泛化的影响提供了理论解释。

Abstract: In this work, we propose a notion of practical learnability grounded in finite sample settings, and develop a conjugate learning theoretical framework based on convex conjugate duality to characterize this learnability property. Building on this foundation, we demonstrate that training deep neural networks (DNNs) with mini-batch stochastic gradient descent (SGD) achieves global optima of empirical risk by jointly controlling the extreme eigenvalues of a structure matrix and the gradient energy, and we establish a corresponding convergence theorem. We further elucidate the impact of batch size and model architecture (including depth, parameter count, sparsity, skip connections, and other characteristics) on non-convex optimization. Additionally, we derive a model-agnostic lower bound for the achievable empirical risk, theoretically demonstrating that data determines the fundamental limit of trainability. On the generalization front, we derive deterministic and probabilistic bounds on generalization error based on generalized conditional entropy measures. The former explicitly delineates the range of generalization error, while the latter characterizes the distribution of generalization error relative to the deterministic bounds under independent and identically distributed (i.i.d.) sampling conditions. Furthermore, these bounds explicitly quantify the influence of three key factors: (i) information loss induced by irreversibility in the model, (ii) the maximum attainable loss value, and (iii) the generalized conditional entropy of features with respect to labels. Moreover, they offer a unified theoretical lens for understanding the roles of regularization, irreversible transformations, and network depth in shaping the generalization behavior of deep neural networks. Extensive experiments validate all theoretical predictions, confirming the framework's correctness and consistency.

</details>


### [129] [On sparsity, extremal structure, and monotonicity properties of Wasserstein and Gromov-Wasserstein optimal transport plans](https://arxiv.org/abs/2602.16265)
*Titouan Vayer*

Main category: stat.ML

TL;DR: 本文系统比较了Gromov-Wasserstein距离与线性最优传输的差异，重点探讨了GW最优传输计划的稀疏性、置换支撑条件以及循环单调性等关键性质。


<details>
  <summary>Details</summary>
Motivation: 动机在于深入理解Gromov-Wasserstein距离的核心数学性质，特别是与标准线性最优传输框架相比的独特特征。作者希望明确回答几个关键理论问题：GW最优传输计划是否具有稀疏性？在什么条件下它们由置换支撑？是否满足某种形式的循环单调性？

Method: 采用理论分析方法，通过引入条件负半定性质这一关键概念，建立GW距离的理论框架。作者自包含地概述了GW距离的重要性质，并与线性OT框架进行系统性比较。

Result: 证明了当条件负半定性质成立时，存在稀疏的GW最优传输计划，且这些计划可以由置换支撑。这一结果为GW距离的数学特性提供了重要理论保证。

Conclusion: Gromov-Wasserstein距离在满足条件负半定性质时，具有与线性最优传输不同的数学特性，特别是存在稀疏且由置换支撑的最优传输计划，这为GW距离的理论研究和实际应用提供了重要基础。

Abstract: This note gives a self-contained overview of some important properties of the Gromov-Wasserstein (GW) distance, compared with the standard linear optimal transport (OT) framework. More specifically, I explore the following questions: are GW optimal transport plans sparse? Under what conditions are they supported on a permutation? Do they satisfy a form of cyclical monotonicity? In particular, I present the conditionally negative semi-definite property and show that, when it holds, there are GW optimal plans that are sparse and supported on a permutation.

</details>


### [130] [Machine Learning in Epidemiology](https://arxiv.org/abs/2602.16352)
*Marvin N. Wright,Lukas Burk,Pegah Golchian,Jan Kapar,Niklas Koenen,Sophie Hanna Langbein*

Main category: stat.ML

TL;DR: 本章为流行病学中应用机器学习提供方法论基础，涵盖监督/无监督学习、重要方法、模型评估策略和可解释机器学习，并配有R语言代码示例。


<details>
  <summary>Details</summary>
Motivation: 数字流行病学时代，流行病学家面临日益增长的数据量和复杂性，需要机器学习工具来分析这些海量数据。

Method: 建立机器学习在流行病学中应用的方法论框架，包括监督学习、无监督学习、模型评估、超参数优化和可解释机器学习，使用R语言代码示例和心脏病数据集进行演示。

Result: 提供了完整的机器学习应用方法论体系，包括理论指导和实践代码，使流行病学家能够有效应用机器学习分析复杂流行病学数据。

Conclusion: 机器学习为流行病学数据分析提供了强大工具，本章建立的方法论框架和实用代码示例将帮助流行病学家成功应用机器学习技术。

Abstract: In the age of digital epidemiology, epidemiologists are faced by an increasing amount of data of growing complexity and dimensionality. Machine learning is a set of powerful tools that can help to analyze such enormous amounts of data. This chapter lays the methodological foundations for successfully applying machine learning in epidemiology. It covers the principles of supervised and unsupervised learning and discusses the most important machine learning methods. Strategies for model evaluation and hyperparameter optimization are developed and interpretable machine learning is introduced. All these theoretical parts are accompanied by code examples in R, where an example dataset on heart disease is used throughout the chapter.

</details>


### [131] [Learning Preference from Observed Rankings](https://arxiv.org/abs/2602.16476)
*Yu-Chang Chen,Chen Chian Fuh,Shang En Tsai*

Main category: stat.ML

TL;DR: 提出一个从部分排序信息学习消费者偏好的灵活框架，通过逆概率加权处理选择偏差，使用随机梯度下降进行可扩展计算，在葡萄酒零售数据中提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 消费者偏好估计对经济学和营销学至关重要，但现有方法面临两个主要挑战：1) 从部分排序信息（如购买记录）推断完整偏好；2) 处理选择偏差（只有进入考虑集的商品才会被比较）。

Method: 1) 将观察到的排序解释为具有逻辑选择概率的成对比较集合；2) 将潜在效用建模为可解释产品属性、商品固定效应和低秩用户-商品因子结构之和；3) 通过商品级可观察倾向的乘积模型化对观察选择偏差；4) 使用逆概率加权（IPW）岭正则化对数似然估计偏好参数；5) 提出基于逆概率重采样的随机梯度下降算法进行可扩展计算。

Result: 在在线葡萄酒零售商交易数据应用中，该方法相对于基于流行度的基准模型，在样本外推荐性能上有所改进，特别是在预测先前未消费产品的购买方面表现尤为突出。

Conclusion: 该框架能够从部分排序信息中学习消费者偏好，有效处理选择偏差，同时保持可解释性，并在实际应用中展现出优越的推荐性能，特别是在发现新商品偏好方面。

Abstract: Estimating consumer preferences is central to many problems in economics and marketing. This paper develops a flexible framework for learning individual preferences from partial ranking information by interpreting observed rankings as collections of pairwise comparisons with logistic choice probabilities. We model latent utility as the sum of interpretable product attributes, item fixed effects, and a low-rank user-item factor structure, enabling both interpretability and information sharing across consumers and items. We further correct for selection in which comparisons are observed: a comparison is recorded only if both items enter the consumer's consideration set, inducing exposure bias toward frequently encountered items. We model pair observability as the product of item-level observability propensities and estimate these propensities with a logistic model for the marginal probability that an item is observable. Preference parameters are then estimated by maximizing an inverse-probability-weighted (IPW), ridge-regularized log-likelihood that reweights observed comparisons toward a target comparison population. To scale computation, we propose a stochastic gradient descent (SGD) algorithm based on inverse-probability resampling, which draws comparisons in proportion to their IPW weights. In an application to transaction data from an online wine retailer, the method improves out-of-sample recommendation performance relative to a popularity-based benchmark, with particularly strong gains in predicting purchases of previously unconsumed products.

</details>


### [132] [Functional Decomposition and Shapley Interactions for Interpreting Survival Models](https://arxiv.org/abs/2602.16505)
*Sophie Hanna Langbein,Hubert Baniecki,Fabian Fumagalli,Niklas Koenen,Marvin N. Wright,Julia Herbinger*

Main category: stat.ML

TL;DR: SurvFD和SurvSHAP-IQ：一种用于生存模型解释的新方法，通过分解高阶特征交互为时间依赖和独立成分，解决传统加性解释方法在生存函数中的局限性。


<details>
  <summary>Details</summary>
Motivation: 生存函数和风险函数在时间到事件预测中是自然且可解释的目标，但它们固有的非加性特性从根本上限制了标准加性解释方法。需要一种能够分析特征交互作用的新方法来理解生存模型。

Method: 提出了Survival Functional Decomposition (SurvFD)，将高阶效应分解为时间依赖和时间独立成分。基于此理论分解，开发了SurvSHAP-IQ，将Shapley交互扩展到时间索引函数，为高阶时间依赖交互提供实用估计器。

Result: SurvFD和SurvSHAP-IQ共同建立了一个交互和时间感知的可解释性方法，明确描述了加性解释何时以及为何失败，为生存建模提供了新的解释视角。

Conclusion: 该方法为时间到事件预测任务提供了广泛适用的交互和时间感知解释框架，填补了生存模型解释方法的空白。

Abstract: Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.

</details>


### [133] [Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study](https://arxiv.org/abs/2602.16601)
*Nail B. Khelifa,Richard E. Turner,Ramji Venkataramanan*

Main category: stat.ML

TL;DR: 该论文理论分析了在基于分数的扩散模型中递归训练合成数据导致性能下降的现象，量化了生成分布与目标分布之间的累积散度，并确定了不同漂移机制。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地在合成数据上进行训练或微调，但递归训练此类数据会导致性能显著下降，表现为逐渐偏离目标分布。需要理论分析这一现象，特别是在基于分数的扩散模型中的表现。

Method: 在基于分数的扩散模型设置下进行理论分析，考虑每轮训练同时使用合成数据和目标分布新鲜样本的现实训练流程。通过推导生成分布与目标分布之间累积散度的上下界，来表征不同的漂移机制。

Result: 获得了生成分布与目标分布之间累积散度的上下界，能够根据分数估计误差和每轮生成中使用的新鲜数据比例来表征不同的漂移机制。在合成数据和图像上的实证结果也验证了理论分析。

Conclusion: 该研究为理解递归训练合成数据导致的性能下降提供了理论框架，揭示了分数估计误差和新鲜数据比例如何影响分布漂移，对实际应用中合成数据的使用具有指导意义。

Abstract: Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, we obtain upper and lower bounds on the accumulated divergence between the generated and target distributions. This allows us to characterize different regimes of drift, depending on the score estimation error and the proportion of fresh data used in each generation. We also provide empirical results on synthetic data and images to illustrate the theory.

</details>


### [134] [Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models](https://arxiv.org/abs/2602.16634)
*Yu Xie,Ludwig Winkler,Lixin Sun,Sarah Lewis,Adam E. Foster,José Jiménez Luna,Tim Hempel,Michael Gastegger,Yaoyi Chen,Iryna Zaporozhets,Cecilia Clementi,Christopher M. Bishop,Frank Noé*

Main category: stat.ML

TL;DR: 本文提出增强扩散采样方法，通过精确引导协议生成偏置系综，再通过精确重加权恢复平衡统计，解决了扩散模型在稀有事件采样中的剩余问题。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型如BioEmu已成为强大的平衡采样器，但在计算依赖于平衡中稀有状态（如折叠自由能）的观测量时，稀有事件采样问题仍然存在。需要一种方法能够高效探索稀有事件区域，同时保持无偏的热力学估计器。

Method: 提出增强扩散采样框架，通过执行定量精确的引导协议生成偏置系综，然后通过精确重加权恢复平衡统计。具体实现了三种算法：UmbrellaDiff（使用扩散模型的伞形采样）、ΔG-Diff（通过倾斜系综计算自由能差）和MetaDiff（元动力学的批处理版本）。

Result: 在玩具系统、蛋白质折叠景观和折叠自由能计算中，这些方法实现了快速、准确且可扩展的平衡性质估计，每个系统仅需GPU分钟到小时级别的时间，填补了扩散模型平衡采样器出现后仍然存在的稀有事件采样空白。

Conclusion: 增强扩散采样框架成功解决了扩散模型在稀有事件采样中的剩余挑战，通过结合精确引导和重加权技术，实现了高效、准确的平衡性质估计，为分子动力学中的稀有事件采样问题提供了完整的解决方案。

Abstract: The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $Δ$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [135] [A Koopman-Bayesian Framework for High-Fidelity, Perceptually Optimized Haptic Surgical Simulation](https://arxiv.org/abs/2602.15834)
*Rohit Kaushik,Eva Kaushik*

Main category: cs.LG

TL;DR: 提出结合非线性动力学、感知心理物理学和高频触觉渲染的统一框架，用于增强手术模拟的真实感，通过Koopman算子实现线性预测控制，基于贝叶斯校准适应个体感知阈值，在多种手术任务中实现低延迟、低误差和感知改善。


<details>
  <summary>Details</summary>
Motivation: 现有手术模拟系统在触觉渲染方面存在局限性，难以同时处理非线性组织动力学和个体感知差异，需要更真实、个性化的触觉反馈来提升手术训练效果。

Method: 1) 使用Koopman算子将非线性软组织动力学提升到增广状态空间进行线性预测控制；2) 基于Weber-Fechner和Stevens定律的贝叶斯校准模块，根据个体感知阈值渐进调整力信号；3) 统一框架整合非线性动力学、感知心理物理学和高频触觉渲染。

Result: 系统在触诊、切口和骨磨削等手术任务中实现平均渲染延迟4.3ms，力误差小于2.8%，感知辨别能力提升20%。多元统计分析显示系统性能显著优于传统弹簧阻尼器和基于能量的渲染方法。

Conclusion: 该框架显著提升了手术模拟的真实感和个性化触觉反馈，对手术训练和VR医学教育有重要影响，未来可向闭环神经反馈触觉接口方向发展。

Abstract: We introduce a unified framework that combines nonlinear dynamics, perceptual psychophysics and high frequency haptic rendering to enhance realism in surgical simulation. The interaction of the surgical device with soft tissue is elevated to an augmented state space with a Koopman operator formulation, allowing linear prediction and control of the dynamics that are nonlinear by nature. To make the rendered forces consistent with human perceptual limits, we put forward a Bayesian calibration module based on WeberFechner and Stevens scaling laws, which progressively shape force signals relative to each individual's discrimination thresholds. For various simulated surgical tasks such as palpation, incision, and bone milling, the proposed system attains an average rendering latency of 4.3 ms, a force error of less than 2.8% and a 20% improvement in perceptual discrimination. Multivariate statistical analyses (MANOVA and regression) reveal that the system's performance is significantly better than that of conventional spring-damper and energy, based rendering methods. We end by discussing the potential impact on surgical training and VR, based medical education, as well as sketching future work toward closed, loop neural feedback in haptic interfaces.

</details>


### [136] [Memes-as-Replies: Can Models Select Humorous Manga Panel Responses?](https://arxiv.org/abs/2602.15842)
*Ryosuke Kohita,Seiichiro Yoshioka*

Main category: cs.LG

TL;DR: 该论文提出了Meme Reply Selection任务和MaMe-Re基准，包含10万个人工标注的漫画面板与社交媒体帖子配对，用于研究模因在对话中的幽默回复选择。研究发现LLMs能捕捉复杂社交线索但视觉信息无帮助，且模型难以区分语义相似候选的微妙幽默差异。


<details>
  <summary>Details</summary>
Motivation: 现有计算研究主要关注模因的内在属性，而模因在对话中作为互动回复的动态和上下文使用方式（特别是创造幽默）在网页科学中研究不足。需要填补这一空白，研究模因如何被上下文化地用于创造幽默。

Method: 引入Meme Reply Selection任务，创建MaMe-Re基准数据集，包含10万个人工标注配对（来自2,325名标注者的50万次标注），使用公开授权的日本漫画面板和社交媒体帖子。通过该基准评估大型语言模型在理解上下文幽默方面的能力。

Result: 1) LLMs初步显示出捕捉夸张等复杂社交线索的能力；2) 包含视觉信息并未提升性能，表明理解视觉内容与有效用于上下文幽默之间存在差距；3) LLMs在受控环境中能匹配人类判断，但难以区分语义相似候选的微妙机智差异。

Conclusion: 选择上下文幽默回复对当前模型仍是开放挑战。虽然LLMs能捕捉一些社交线索，但在理解视觉内容和区分微妙幽默差异方面仍有局限，需要进一步研究。

Abstract: Memes are a popular element of modern web communication, used not only as static artifacts but also as interactive replies within conversations. While computational research has focused on analyzing the intrinsic properties of memes, the dynamic and contextual use of memes to create humor remains an understudied area of web science. To address this gap, we introduce the Meme Reply Selection task and present MaMe-Re (Manga Meme Reply Benchmark), a benchmark of 100,000 human-annotated pairs (500,000 total annotations from 2,325 unique annotators) consisting of openly licensed Japanese manga panels and social media posts. Our analysis reveals three key insights: (1) large language models (LLMs) show preliminary evidence of capturing complex social cues such as exaggeration, moving beyond surface-level semantic matching; (2) the inclusion of visual information does not improve performance, revealing a gap between understanding visual content and effectively using it for contextual humor; (3) while LLMs can match human judgments in controlled settings, they struggle to distinguish subtle differences in wit among semantically similar candidates. These findings suggest that selecting contextually humorous replies remains an open challenge for current models.

</details>


### [137] [Kalman-Inspired Runtime Stability and Recovery in Hybrid Reasoning Systems](https://arxiv.org/abs/2602.15855)
*Barak Or*

Main category: cs.LG

TL;DR: 论文提出从卡尔曼滤波视角研究混合推理系统的运行时稳定性，引入认知漂移概念，通过监控创新信号统计量来检测和恢复推理过程中的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前结合学习组件和模型推理的混合系统在工具增强决策循环中广泛应用，但其在部分可观测和证据不匹配情况下的运行时行为缺乏理解。实践中失败往往表现为内部推理动态的逐渐发散而非孤立预测错误。

Method: 将推理建模为受内部创新信号驱动的随机推理过程，引入认知漂移作为可测量的运行时现象。提出运行时稳定性框架，监控创新统计量，检测新兴不稳定性，并触发恢复感知的控制机制。

Result: 在多步骤工具增强推理任务上的实验表明，该方法能在任务失败前可靠检测不稳定性，并在可行时通过恢复机制在有限时间内重建有界的内部行为。

Conclusion: 运行时稳定性应作为不确定环境下可靠推理的系统级要求，认知漂移监控为混合推理系统的鲁棒性提供了新视角。

Abstract: Hybrid reasoning systems that combine learned components with model-based inference are increasingly deployed in tool-augmented decision loops, yet their runtime behavior under partial observability and sustained evidence mismatch remains poorly understood. In practice, failures often arise as gradual divergence of internal reasoning dynamics rather than as isolated prediction errors. This work studies runtime stability in hybrid reasoning systems from a Kalman-inspired perspective. We model reasoning as a stochastic inference process driven by an internal innovation signal and introduce cognitive drift as a measurable runtime phenomenon. Stability is defined in terms of detectability, bounded divergence, and recoverability rather than task-level correctness. We propose a runtime stability framework that monitors innovation statistics, detects emerging instability, and triggers recovery-aware control mechanisms. Experiments on multi-step, tool-augmented reasoning tasks demonstrate reliable instability detection prior to task failure and show that recovery, when feasible, re-establishes bounded internal behavior within finite time. These results emphasize runtime stability as a system-level requirement for reliable reasoning under uncertainty.

</details>


### [138] [Genetic Generalized Additive Models](https://arxiv.org/abs/2602.15877)
*Kaaustaaub Shankar,Kelly Cohen*

Main category: cs.LG

TL;DR: 使用多目标遗传算法NSGA-II自动优化广义可加模型，在预测误差和模型复杂度之间取得平衡，提升模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 广义可加模型(GAMs)在预测准确性和可解释性之间取得了平衡，但手动配置其结构具有挑战性。需要一种自动化方法来优化GAMs，同时考虑预测性能和模型复杂度。

Method: 提出使用多目标遗传算法NSGA-II来自动优化GAMs，联合最小化预测误差(RMSE)和复杂度惩罚项，该惩罚项捕获稀疏性、平滑性和不确定性。

Result: 在加州住房数据集上的实验表明，NSGA-II发现的GAMs在准确性上优于基线LinearGAMs，或以显著更低的复杂度匹配其性能。生成的模型更简单、更平滑，置信区间更窄，增强了可解释性。

Conclusion: 该框架为自动化优化透明、高性能模型提供了一种通用方法，平衡了预测准确性和模型可解释性。

Abstract: Generalized Additive Models (GAMs) balance predictive accuracy and interpretability, but manually configuring their structure is challenging. We propose using the multi-objective genetic algorithm NSGA-II to automatically optimize GAMs, jointly minimizing prediction error (RMSE) and a Complexity Penalty that captures sparsity, smoothness, and uncertainty. Experiments on the California Housing dataset show that NSGA-II discovers GAMs that outperform baseline LinearGAMs in accuracy or match performance with substantially lower complexity. The resulting models are simpler, smoother, and exhibit narrower confidence intervals, enhancing interpretability. This framework provides a general approach for automated optimization of transparent, high-performing models. The code can be found at https://github.com/KaaustaaubShankar/GeneticAdditiveModels.

</details>


### [139] [IT-OSE: Exploring Optimal Sample Size for Industrial Data Augmentation](https://arxiv.org/abs/2602.15878)
*Mingchun Sun,Rongqiang Zhao,Zhennan Huang,Songyu Ding,Jie Liu*

Main category: cs.LG

TL;DR: 提出信息论最优样本量估计(IT-OSE)方法，为工业数据增强提供可靠的最优样本量估计，并设计ICD评分指标评估估计准确性。


<details>
  <summary>Details</summary>
Motivation: 工业场景中数据增强能提升模型性能，但缺乏最优样本量(OSS)的理论研究和估计方法，也没有评估OSS准确性的指标。

Method: 提出信息论最优样本量估计(IT-OSE)方法，设计区间覆盖与偏差(ICD)评分指标，理论分析OSS与主导因素的关系。

Result: 相比经验估计，分类任务准确率平均提升4.38%，回归任务MAPE平均降低18.80%；相比穷举搜索，计算和数据成本分别降低83.97%和93.46%。

Conclusion: IT-OSE方法能有效估计数据增强的最优样本量，提升模型性能稳定性，降低计算成本，在工业场景中具有通用性。

Abstract: In industrial scenarios, data augmentation is an effective approach to improve model performance. However, its benefits are not unidirectionally beneficial. There is no theoretical research or established estimation for the optimal sample size (OSS) in augmentation, nor is there an established metric to evaluate the accuracy of OSS or its deviation from the ground truth. To address these issues, we propose an information-theoretic optimal sample size estimation (IT-OSE) to provide reliable OSS estimation for industrial data augmentation. An interval coverage and deviation (ICD) score is proposed to evaluate the estimated OSS intuitively. The relationship between OSS and dominant factors is theoretically analyzed and formulated, thereby enhancing the interpretability. Experiments show that, compared to empirical estimation, the IT-OSE increases accuracy in classification tasks across baseline models by an average of 4.38%, and reduces MAPE in regression tasks across baseline models by an average of 18.80%. The improvements in downstream model performance are more stable. ICDdev in the ICD score is also reduced by an average of 49.30%. The determinism of OSS is enhanced. Compared to exhaustive search, the IT-OSE achieves the same OSS while reducing computational and data costs by an average of 83.97% and 93.46%. Furthermore, practicality experiments demonstrate that the IT-OSE exhibits generality across representative sensor-based industrial scenarios.

</details>


### [140] [BamaER: A Behavior-Aware Memory-Augmented Model for Exercise Recommendation](https://arxiv.org/abs/2602.15879)
*Qing Yang,Yuhao Jiang,Rui Wang,Jipeng Guo,Yejiang Wang,Xinghe Cheng,Zezheng Wu,Jiapu Wang,Jingwei Zhang*

Main category: cs.LG

TL;DR: BamaER是一个行为感知的记忆增强型练习推荐框架，通过三向混合编码捕获学生交互行为，使用动态记忆矩阵建模知识状态，并采用河马优化算法进行多样性感知的候选练习选择，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有练习推荐方法主要将学生学习表示为练习序列，忽略了丰富的交互行为信息，导致学习进度估计存在偏差。同时，固定长度的序列分割限制了早期学习经验的纳入，阻碍了长期依赖建模和知识掌握程度的准确估计。

Method: BamaER包含三个核心模块：1) 学习进度预测模块，通过三向混合编码方案捕获异构学生交互行为；2) 记忆增强知识追踪模块，维护动态记忆矩阵联合建模历史和当前知识状态；3) 练习过滤模块，将候选选择构建为多样性感知优化问题，使用河马优化算法减少冗余并提高推荐覆盖率。

Result: 在五个真实世界教育数据集上的实验表明，BamaER在一系列评估指标上持续优于最先进的基线方法。

Conclusion: BamaER通过整合行为感知编码、记忆增强知识追踪和多样性优化，有效解决了现有练习推荐方法的局限性，提供了更准确可靠的学习进度估计和个性化推荐。

Abstract: Exercise recommendation focuses on personalized exercise selection conditioned on students' learning history, personal interests, and other individualized characteristics. Despite notable progress, most existing methods represent student learning solely as exercise sequences, overlooking rich behavioral interaction information. This limited representation often leads to biased and unreliable estimates of learning progress. Moreover, fixed-length sequence segmentation limits the incorporation of early learning experiences, thereby hindering the modeling of long-term dependencies and the accurate estimation of knowledge mastery. To address these limitations, we propose BamaER, a Behavior-aware memory-augmented Exercise Recommendation framework that comprises three core modules: (i) the learning progress prediction module that captures heterogeneous student interaction behaviors via a tri-directional hybrid encoding scheme; (ii) the memory-augmented knowledge tracing module that maintains a dynamic memory matrix to jointly model historical and current knowledge states for robust mastery estimation; and (iii) the exercise filtering module that formulates candidate selection as a diversity-aware optimization problem, solved via the Hippopotamus Optimization Algorithm to reduce redundancy and improve recommendation coverage. Experiments on five real-world educational datasets show that BamaER consistently outperforms state-of-the-art baselines across a range of evaluation metrics.

</details>


### [141] [Distributed physics-informed neural networks via domain decomposition for fast flow reconstruction](https://arxiv.org/abs/2602.15883)
*Yixiao Qian,Jiaxu Liu,Zewei Xia,Song Chen,Chao Xu,Shengze Cai*

Main category: cs.LG

TL;DR: 提出一个鲁棒的分布式PINNs框架，通过时空域分解实现高效流场重建，解决了子网络压力不确定性问题，并利用CUDA图和JIT编译加速训练。


<details>
  <summary>Details</summary>
Motivation: PINNs在流场重建中能融合稀疏速度测量和Navier-Stokes方程，但扩展到大的时空域时面临计算瓶颈和优化不稳定问题。

Method: 采用时空域分解的分布式PINNs框架，通过参考锚点归一化策略和去耦非对称加权解决压力不确定性问题，并利用CUDA图和JIT编译加速训练。

Result: 在复杂流场基准测试中验证了方法能实现接近线性的强扩展性和高保真重建。

Conclusion: 该方法为流场重建和复杂流体动力学理解提供了可扩展且物理严谨的途径。

Abstract: Physics-Informed Neural Networks (PINNs) offer a powerful paradigm for flow reconstruction, seamlessly integrating sparse velocity measurements with the governing Navier-Stokes equations to recover complete velocity and latent pressure fields. However, scaling such models to large spatiotemporal domains is hindered by computational bottlenecks and optimization instabilities. In this work, we propose a robust distributed PINNs framework designed for efficient flow reconstruction via spatiotemporal domain decomposition. A critical challenge in such distributed solvers is pressure indeterminacy, where independent sub-networks drift into inconsistent local pressure baselines. We address this issue through a reference anchor normalization strategy coupled with decoupled asymmetric weighting. By enforcing a unidirectional information flow from designated master ranks where the anchor point lies to neighboring ranks, our approach eliminates gauge freedom and guarantees global pressure uniqueness while preserving temporal continuity. Furthermore, to mitigate the Python interpreter overhead associated with computing high-order physics residuals, we implement a high-performance training pipeline accelerated by CUDA graphs and JIT compilation. Extensive validation on complex flow benchmarks demonstrates that our method achieves near-linear strong scaling and high-fidelity reconstruction, establishing a scalable and physically rigorous pathway for flow reconstruction and understanding of complex hydrodynamics.

</details>


### [142] [Adaptive Semi-Supervised Training of P300 ERP-BCI Speller System with Minimum Calibration Effort](https://arxiv.org/abs/2602.15955)
*Shumeng Chen,Jane E. Huggins,Tianwen Ma*

Main category: cs.LG

TL;DR: 提出基于P300的脑机接口拼写器自适应半监督学习框架，减少校准时间，提高拼写效率


<details>
  <summary>Details</summary>
Motivation: 传统P300脑机接口拼写器需要冗长的校准过程来构建二分类器，降低了整体效率，需要减少校准工作量

Method: 提出统一框架，使用少量标记校准数据，采用自适应半监督EM-GMM算法更新二分类器

Result: 15名参与者中，9人达到0.7的最小字符级准确率，其中7人显示自适应方法优于基准方法

Conclusion: 提出的半监督学习框架为实时BCI拼写系统提供了实用高效的选择，特别是在标记数据有限的情况下

Abstract: A P300 ERP-based Brain-Computer Interface (BCI) speller is an assistive communication tool. It searches for the P300 event-related potential (ERP) elicited by target stimuli, distinguishing it from the neural responses to non-target stimuli embedded in electroencephalogram (EEG) signals. Conventional methods require a lengthy calibration procedure to construct the binary classifier, which reduced overall efficiency. Thus, we proposed a unified framework with minimum calibration effort such that, given a small amount of labeled calibration data, we employed an adaptive semi-supervised EM-GMM algorithm to update the binary classifier. We evaluated our method based on character-level prediction accuracy, information transfer rate (ITR), and BCI utility. We applied calibration on training data and reported results on testing data. Our results indicate that, out of 15 participants, 9 participants exceed the minimum character-level accuracy of 0.7 using either on our adaptive method or the benchmark, and 7 out of these 9 participants showed that our adaptive method performed better than the benchmark. The proposed semi-supervised learning framework provides a practical and efficient alternative to improve the overall spelling efficiency in the real-time BCI speller system, particularly in contexts with limited labeled data.

</details>


### [143] [R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions](https://arxiv.org/abs/2602.15961)
*Zhi Sheng,Yuan Yuan,Guozhen Zhang,Yong Li*

Main category: cs.LG

TL;DR: R²Energy是一个用于可再生能源预测的大规模基准数据集，包含中国4个省份902个风能和太阳能电站的1070万小时记录，提供标准化、无泄漏的NWP辅助预测评估框架，揭示了极端天气下模型鲁棒性与复杂性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源（特别是风能和太阳能）的快速扩张，可靠的预测对电力系统运行变得至关重要。虽然深度学习模型在平均准确率上表现良好，但气候驱动的极端天气事件频率和强度不断增加，对电网稳定性和运行安全构成严重威胁。因此，开发能够在波动条件下保持稳健的预测模型成为首要挑战。

Method: 提出了R²Energy基准数据集，包含中国4个省份902个风能和太阳能电站的1070万小时高保真记录，涵盖多样化的气象条件。建立了一个标准化、无泄漏的预测范式，确保所有模型都能平等访问未来的数值天气预报信号。采用基于专家标注的极端天气注释的分区评估方法，揭示模型鲁棒性差距。

Result: 研究发现了一个关键的"鲁棒性差距"，这一差距通常被平均指标所掩盖。揭示了鲁棒性与复杂性之间的权衡关系：在极端条件下，模型的可靠性由其气象集成策略驱动，而非架构复杂性。R²Energy为评估和开发安全关键电力系统应用的预测模型提供了原则性基础。

Conclusion: R²Energy基准为可再生能源预测提供了标准化评估框架，揭示了极端天气条件下模型鲁棒性的重要性，强调气象集成策略比架构复杂性更能决定模型在恶劣条件下的可靠性，为电力系统安全应用提供了重要指导。

Abstract: The rapid expansion of renewable energy, particularly wind and solar power, has made reliable forecasting critical for power system operations. While recent deep learning models have achieved strong average accuracy, the increasing frequency and intensity of climate-driven extreme weather events pose severe threats to grid stability and operational security. Consequently, developing robust forecasting models that can withstand volatile conditions has become a paramount challenge. In this paper, we present R$^2$Energy, a large-scale benchmark for NWP-assisted renewable energy forecasting. It comprises over 10.7 million high-fidelity hourly records from 902 wind and solar stations across four provinces in China, providing the diverse meteorological conditions necessary to capture the wide-ranging variability of renewable generation. We further establish a standardized, leakage-free forecasting paradigm that grants all models identical access to future Numerical Weather Prediction (NWP) signals, enabling fair and reproducible comparison across state-of-the-art representative forecasting architectures. Beyond aggregate accuracy, we incorporate regime-wise evaluation with expert-aligned extreme weather annotations, uncovering a critical ``robustness gap'' typically obscured by average metrics. This gap reveals a stark robustness-complexity trade-off: under extreme conditions, a model's reliability is driven by its meteorological integration strategy rather than its architectural complexity. R$^2$Energy provides a principled foundation for evaluating and developing forecasting models for safety-critical power system applications.

</details>


### [144] [B-DENSE: Branching For Dense Ensemble Network Learning](https://arxiv.org/abs/2602.15971)
*Cherish Puniani,Tushar Kumar,Arnav Bendre,Gaurav Kumar,Shree Singhi*

Main category: cs.LG

TL;DR: 提出B-DENSE框架，通过多分支轨迹对齐解决扩散模型蒸馏中稀疏监督导致的离散化误差问题，提高图像生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出色，但其迭代采样特性导致高推理延迟。现有蒸馏技术虽然加速采样，但丢弃了中间轨迹步骤，稀疏监督导致结构信息丢失和显著离散化误差

Method: 提出B-DENSE框架，修改学生架构使其输出K倍扩展通道，每个子集对应教师轨迹中的特定离散中间步骤。通过训练这些分支同时映射到教师目标时间步的整个序列，实现密集中间轨迹对齐

Result: 学生模型从训练早期就能学习在解空间中导航，相比基线蒸馏框架展现出更优的图像生成质量

Conclusion: B-DENSE通过多分支轨迹对齐有效缓解了扩散模型蒸馏中的离散化误差问题，提高了生成质量，为加速扩散模型推理提供了更优方案

Abstract: Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory steps. This sparse supervision leads to a loss of structural information and introduces significant discretization errors. To mitigate this, we propose B-DENSE, a novel framework that leverages multi-branch trajectory alignment. We modify the student architecture to output $K$-fold expanded channels, where each subset corresponds to a specific branch representing a discrete intermediate step in the teacher's trajectory. By training these branches to simultaneously map to the entire sequence of the teacher's target timesteps, we enforce dense intermediate trajectory alignment. Consequently, the student model learns to navigate the solution space from the earliest stages of training, demonstrating superior image generation quality compared to baseline distillation frameworks.

</details>


### [145] [Fast Online Learning with Gaussian Prior-Driven Hierarchical Unimodal Thompson Sampling](https://arxiv.org/abs/2602.15972)
*Tianchi Zhao,He Liu,Hongyin Shi,Jinliang Li*

Main category: cs.LG

TL;DR: 本文提出针对高斯奖励反馈的聚类多臂赌博机问题，设计了TSCG和UTSCG算法，利用2级层次结构降低遗憾界，并通过理论分析和实验验证了算法优势。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多问题（如毫米波通信、风险资产组合管理）中的高斯分布普遍性导致需要处理具有高斯奖励反馈的聚类多臂赌博机问题。现有TSG算法未充分利用聚类结构信息，存在优化空间。

Method: 基于高斯先验的Thompson Sampling算法，提出TSCG算法专门处理2级层次聚类结构。进一步针对单峰奖励场景，提出UTSCG算法。两种算法都利用了聚类结构信息来优化臂选择策略。

Result: 理论分析证明TSCG算法相比普通TSG算法能获得更低的遗憾界。对于单峰奖励场景，UTSCG算法能实现更低的遗憾界。数值实验验证了所提算法的优势。

Conclusion: 通过利用聚类结构信息，提出的TSCG和UTSCG算法在高斯奖励反馈的多臂赌博机问题中实现了更低的遗憾界，为实际应用提供了有效的解决方案。

Abstract: We study a type of Multi-Armed Bandit (MAB) problems in which arms with a Gaussian reward feedback are clustered. Such an arm setting finds applications in many real-world problems, for example, mmWave communications and portfolio management with risky assets, as a result of the universality of the Gaussian distribution. Based on the Thompson Sampling algorithm with Gaussian prior (TSG) algorithm for the selection of the optimal arm, we propose our Thompson Sampling with Clustered arms under Gaussian prior (TSCG) specific to the 2-level hierarchical structure. We prove that by utilizing the 2-level structure, we can achieve a lower regret bound than we do with ordinary TSG. In addition, when the reward is Unimodal, we can reach an even lower bound on the regret by our Unimodal Thompson Sampling algorithm with Clustered Arms under Gaussian prior (UTSCG). Each of our proposed algorithms are accompanied by theoretical evaluation of the upper regret bound, and our numerical experiments confirm the advantage of our proposed algorithms.

</details>


### [146] [Verifier-Constrained Flow Expansion for Discovery Beyond the Data](https://arxiv.org/abs/2602.15984)
*Riccardo De Santi,Kimon Protopapas,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 提出Flow Expander方法，通过验证器约束的熵最大化来扩展预训练流模型，使其生成超出原始数据分布的多样有效样本，特别适用于科学发现应用。


<details>
  <summary>Details</summary>
Motivation: 现有流和扩散模型在有限数据上预训练，只能生成数据分布内的样本，无法探索整个有效设计空间，这限制了科学发现应用需要生成超出可用数据分布的有效设计。

Method: 提出Flow Expander方法，引入强弱验证器的形式化概念，通过验证器约束的熵最大化在流过程加噪状态空间中进行概率空间优化，使用可扩展的镜像下降方案。

Result: 理论分析证明了方法在理想化和一般假设下的收敛保证，实验表明FE能扩展预训练流模型，增加构象多样性同时保持有效性，特别在分子设计任务中表现良好。

Conclusion: Flow Expander能够有效解决预训练流模型生成范围狭窄的问题，通过验证器引导的熵最大化实现全局和局部扩展，为科学发现应用提供了生成超出数据分布的有效设计的方法。

Abstract: Flow and diffusion models are typically pre-trained on limited available data (e.g., molecular samples), covering only a fraction of the valid design space (e.g., the full molecular space). As a consequence, they tend to generate samples from only a narrow portion of the feasible domain. This is a fundamental limitation for scientific discovery applications, where one typically aims to sample valid designs beyond the available data distribution. To this end, we address the challenge of leveraging access to a verifier (e.g., an atomic bonds checker), to adapt a pre-trained flow model so that its induced density expands beyond regions of high data availability, while preserving samples validity. We introduce formal notions of strong and weak verifiers and propose algorithmic frameworks for global and local flow expansion via probability-space optimization. Then, we present Flow Expander (FE), a scalable mirror descent scheme that provably tackles both problems by verifier-constrained entropy maximization over the flow process noised state space. Next, we provide a thorough theoretical analysis of the proposed method, and state convergence guarantees under both idealized and general assumptions. Ultimately, we empirically evaluate our method on both illustrative, yet visually interpretable settings, and on a molecular design task showcasing the ability of FE to expand a pre-trained flow model increasing conformer diversity while preserving validity.

</details>


### [147] [Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks](https://arxiv.org/abs/2602.15997)
*Jayadev Billa*

Main category: cs.LG

TL;DR: 研究发现神经网络训练中的能力涌现具有几何特征：表示几何先于能力涌现，呈现自上而下的层级传播模式，但预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练中能力涌现的机制仍然不透明，需要系统性地研究表示几何与能力涌现之间的关系。

Method: 在五个模型规模（405K-85M参数）、八个算法任务、三个Pythia语言模型（160M-2.8B）中，追踪120多个涌现事件和五个几何度量。

Result: 发现：1）训练开始时表示会普遍坍缩到任务特定的尺度不变水平；2）坍缩自上而下传播；3）表示几何是能力涌现的先导指标（硬任务75-100%先导率），而局部学习系数同步，Hessian度量滞后。

Conclusion: 研究揭示了能力涌现的几何解剖结构及其边界条件，但几何度量只能编码粗略任务难度，无法预测细粒度时序，且需要任务训练对齐才能产生先导关系。

Abstract: Capability emergence during neural network training remains mechanistically opaque. We track five geometric measures across five model scales (405K-85M parameters), 120+ emergence events in eight algorithmic tasks, and three Pythia language models (160M-2.8B). We find: (1) training begins with a universal representation collapse to task-specific floors that are scale-invariant across a 210X parameter range (e.g., modular arithmetic collapses to RANKME ~ 2.0 regardless of model size); (2) collapse propagates top-down through layers (32/32 task X model consistency), contradicting bottom-up feature-building intuition; (3) a geometric hierarchy in which representation geometry leads emergence (75-100% precursor rate for hard tasks), while the local learning coefficient is synchronous (0/24 precursor) and Hessian measures lag. We also delineate prediction limits: geometric measures encode coarse task difficulty but not fine-grained timing (within-class concordance 27%; when task ordering reverses across scales, prediction fails at 26%). On Pythia, global geometric patterns replicate but per-task precursor signals do not -- the precursor relationship requires task-training alignment that naturalistic pre-training does not provide. Our contribution is the geometric anatomy of emergence and its boundary conditions, not a prediction tool.

</details>


### [148] [Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training](https://arxiv.org/abs/2602.16065)
*Kevin Wang,Hongqian Niu,Didong Li*

Main category: cs.LG

TL;DR: 该论文研究了生成式AI递归训练中的数据污染问题，发现在一般框架下，即使使用AI生成的数据进行训练，模型仍会收敛，收敛速率等于基线模型收敛速率与每轮真实数据比例的最小值。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（如大语言模型）的普及，网络数据中AI生成内容与人类生成内容日益混杂，难以区分。后续模型训练时会混合使用人类数据和早期AI生成数据，形成递归训练过程，导致数据污染。现有理论研究仅针对简化场景（离散或高斯分布），而真实数据分布和现代生成模型要复杂得多。

Method: 建立一般性理论框架，对真实数据分布做最小假设，允许底层生成模型为通用逼近器。分析递归训练过程，证明其收敛性，并计算收敛速率。进一步扩展到存在采样偏差的数据收集场景，并通过实证研究验证理论结果。

Result: 在一般框架下，即使存在数据污染的递归训练仍然收敛，收敛速率等于基线模型收敛速率与每轮迭代中真实数据比例的最小值。这是首个在没有数据分布假设下关于递归训练的正面理论结果。实证研究支持了理论发现。

Conclusion: 该研究填补了复杂真实数据分布下递归训练理论分析的空白，表明在一般条件下，即使使用AI生成数据进行训练，模型仍会收敛，收敛速率取决于真实数据比例。这为理解生成式AI递归训练提供了理论基础。

Abstract: Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to separate them from naturally generated content. As generative models are updated regularly, later models will inevitably be trained on mixtures of human-generated data and AI-generated data from earlier versions, creating a recursive training process with data contamination. Existing theoretical work has examined only highly simplified settings, where both the real data and the generative model are discrete or Gaussian, where it has been shown that such recursive training leads to model collapse. However, real data distributions are far more complex, and modern generative models are far more flexible than Gaussian and linear mechanisms. To fill this gap, we study recursive training in a general framework with minimal assumptions on the real data distribution and allow the underlying generative model to be a general universal approximator. In this framework, we show that contaminated recursive training still converges, with a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data used in each iteration. To the best of our knowledge, this is the first (positive) theoretical result on recursive training without distributional assumptions on the data. We further extend the analysis to settings where sampling bias is present in data collection and support all theoretical results with empirical studies.

</details>


### [149] [Geometry-Aware Uncertainty Quantification via Conformal Prediction on Manifolds](https://arxiv.org/abs/2602.16015)
*Marzieh Amiri Shahbazi,Ali Baheri*

Main category: cs.LG

TL;DR: 提出自适应测地线保形预测框架，用测地线非共形分数替代欧几里得残差，通过交叉验证难度估计器处理异方差噪声，在球面上生成位置无关的测地线帽预测区域。


<details>
  <summary>Details</summary>
Motivation: 现有保形预测方法假设欧几里得输出空间，当响应位于黎曼流形（如球面）时，预测区域校准不佳，且无法处理异方差噪声。

Method: 使用测地线非共形分数替代欧几里得残差，通过交叉验证难度估计器对残差进行归一化处理，生成自适应大小的测地线帽预测区域。

Result: 在具有强异方差的合成球面实验和基于IGRF-14卫星数据的真实地磁场预测任务中，自适应方法显著降低条件覆盖变异性，将最坏情况覆盖率提升至接近名义水平，而基于坐标的基线方法因图表失真浪费大量覆盖面积。

Conclusion: 自适应测地线保形预测框架有效处理黎曼流形上的回归问题，通过测地线残差和难度自适应归一化，在保持分布自由覆盖保证的同时，显著改善条件覆盖均匀性。

Abstract: Conformal prediction provides distribution-free coverage guaranties for regression; yet existing methods assume Euclidean output spaces and produce prediction regions that are poorly calibrated when responses lie on Riemannian manifolds. We propose \emph{adaptive geodesic conformal prediction}, a framework that replaces Euclidean residuals with geodesic nonconformity scores and normalizes them by a cross-validated difficulty estimator to handle heteroscedastic noise. The resulting prediction regions, geodesic caps on the sphere, have position-independent area and adapt their size to local prediction difficulty, yielding substantially more uniform conditional coverage than non-adaptive alternatives. In a synthetic sphere experiment with strong heteroscedasticity and a real-world geomagnetic field forecasting task derived from IGRF-14 satellite data, the adaptive method markedly reduces conditional coverage variability and raises worst-case coverage much closer to the nominal level, while coordinate-based baselines waste a large fraction of coverage area due to chart distortion.

</details>


### [150] [Feature-based morphological analysis of shape graph data](https://arxiv.org/abs/2602.16120)
*Murad Hossen,Demetrio Labate,Nicolas Charon*

Main category: cs.LG

TL;DR: 提出用于分析嵌入2D/3D空间形状图数据集的统计计算流程，通过提取拓扑、几何和方向特征来同时捕捉连接结构和分支几何差异


<details>
  <summary>Details</summary>
Motivation: 传统图分析主要关注抽象连接结构，但形状图（如道路网络、神经元等）还需要考虑分支的几何特性差异。现有方法难以同时处理拓扑和几何变化

Method: 提取经过精心设计的拓扑、几何和方向特征集，这些特征满足关键不变性要求。利用特征表示进行群体比较、聚类和分类任务

Result: 在真实数据集（城市道路网络、神经元追踪、星形胶质细胞成像）上评估有效性，并与多种特征和非特征方法进行基准比较

Conclusion: 提出的特征表示方法能够有效分析形状图数据集，同时捕捉拓扑和几何差异，在多个应用领域展现出实用价值

Abstract: This paper introduces and demonstrates a computational pipeline for the statistical analysis of shape graph datasets, namely geometric networks embedded in 2D or 3D spaces. Unlike traditional abstract graphs, our purpose is not only to retrieve and distinguish variations in the connectivity structure of the data but also geometric differences of the network branches. Our proposed approach relies on the extraction of a specifically curated and explicit set of topological, geometric and directional features, designed to satisfy key invariance properties. We leverage the resulting feature representation for tasks such as group comparison, clustering and classification on cohorts of shape graphs. The effectiveness of this representation is evaluated on several real-world datasets including urban road/street networks, neuronal traces and astrocyte imaging. These results are benchmarked against several alternative methods, both feature-based and not.

</details>


### [151] [MolCrystalFlow: Molecular Crystal Structure Prediction via Flow Matching](https://arxiv.org/abs/2602.16020)
*Cheng Zeng,Harry W. Sullivan,Thomas Egg,Maya M. Martirossyan,Philipp Höllmer,Jirui Jin,Richard G. Hennig,Adrian Roitberg,Stefano Martiniani,Ellad B. Tadmor,Mingjie Liu*

Main category: cs.LG

TL;DR: MolCrystalFlow：基于流模型的分子晶体结构预测方法，通过将分子作为刚体嵌入，在黎曼流形上联合学习晶格矩阵、分子取向和质心位置，解决了周期性分子晶体结构预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 分子晶体结构预测是计算化学的重大挑战，因为分子尺寸大且存在复杂的分子内和分子间相互作用。虽然生成模型已革新了分子、无机固体和金属有机框架的结构发现，但将其扩展到完全周期性的分子晶体仍然困难。

Method: MolCrystalFlow是一种基于流模型的生成方法，通过将分子作为刚体嵌入，将分子内复杂性从分子间堆积中解耦。该方法联合学习晶格矩阵、分子取向和质心位置，其中质心和取向在其原生黎曼流形上表示，允许构建测地流并使用图神经网络操作来保持几何对称性。

Result: 在两个开源分子晶体数据集上，该方法在大型周期性晶体生成模型中表现优于最先进的生成模型和基于规则的结构生成方法。展示了MolCrystalFlow与通用机器学习势能相结合，加速分子晶体结构预测的能力。

Conclusion: MolCrystalFlow为数据驱动的分子晶体生成发现铺平了道路，通过流模型在黎曼流形上的表示，有效解决了周期性分子晶体结构预测的挑战。

Abstract: Molecular crystal structure prediction represents a grand challenge in computational chemistry due to large sizes of constituent molecules and complex intra- and intermolecular interactions. While generative modeling has revolutionized structure discovery for molecules, inorganic solids, and metal-organic frameworks, extending such approaches to fully periodic molecular crystals is still elusive. Here, we present MolCrystalFlow, a flow-based generative model for molecular crystal structure prediction. The framework disentangles intramolecular complexity from intermolecular packing by embedding molecules as rigid bodies and jointly learning the lattice matrix, molecular orientations, and centroid positions. Centroids and orientations are represented on their native Riemannian manifolds, allowing geodesic flow construction and graph neural network operations that respects geometric symmetries. We benchmark our model against state-of-the-art generative models for large-size periodic crystals and rule-based structure generation methods on two open-source molecular crystal datasets. We demonstrate an integration of MolCrystalFlow model with universal machine learning potential to accelerate molecular crystal structure prediction, paving the way for data-driven generative discovery of molecular crystals.

</details>


### [152] [Bayesian Quadrature: Gaussian Processes for Integration](https://arxiv.org/abs/2602.16218)
*Maren Mahsereci,Toni Karvonen*

Main category: cs.LG

TL;DR: 这是一篇关于贝叶斯求积法的系统性综述，涵盖了其数学基础、分类体系、理论保证、数值研究、实际挑战和详尽文献。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯求积法是一种基于概率模型的数值积分方法，虽然自1980年代就已流行，但缺乏系统全面的综述。本文旨在填补这一空白，提供对该方法的全面梳理和评估。

Method: 作者从多个角度回顾贝叶斯求积法的数学基础，提出基于建模、推断和采样三个维度的分类体系，收集理论保证，进行受控数值研究，并评估实际应用挑战。

Result: 提供了贝叶斯求积法的系统性综述，包括分类框架、理论结果、数值实验发现，以及对该方法实际应用局限性的现实评估。

Conclusion: 本文填补了贝叶斯求积法领域缺乏系统性综述的空白，为研究者和实践者提供了全面的参考框架，同时指出了该方法的实际应用挑战和未来发展方向。

Abstract: Bayesian quadrature is a probabilistic, model-based approach to numerical integration, the estimation of intractable integrals, or expectations. Although Bayesian quadrature was popularised already in the 1980s, no systematic and comprehensive treatment has been published. The purpose of this survey is to fill this gap. We review the mathematical foundations of Bayesian quadrature from different points of view; present a systematic taxonomy for classifying different Bayesian quadrature methods along the three axes of modelling, inference, and sampling; collect general theoretical guarantees; and provide a controlled numerical study that explores and illustrates the effect of different choices along the axes of the taxonomy. We also provide a realistic assessment of practical challenges and limitations to application of Bayesian quadrature methods and include an up-to-date and nearly exhaustive bibliography that covers not only machine learning and statistics literature but all areas of mathematics and engineering in which Bayesian quadrature or equivalent methods have seen use.

</details>


### [153] [AI-CARE: Carbon-Aware Reporting Evaluation Metric for AI Models](https://arxiv.org/abs/2602.16042)
*KC Santosh,Srikanth Baride,Rodrigue Rizk*

Main category: cs.LG

TL;DR: AI-CARE是一个评估机器学习模型能耗和碳排放的工具，引入碳-性能权衡曲线可视化Pareto前沿，旨在推动研究社区向透明、多目标评估转变。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习快速发展，模型训练和推理的环境成本已成为关键社会问题。现有基准主要关注准确性等标准性能指标，而忽略了能耗和碳排放，这种单目标评估范式与大规模部署的实际需求（特别是在移动设备、发展中地区等能源受限环境）日益脱节。

Method: 提出AI-CARE评估工具，用于报告机器学习模型的能耗和碳排放。引入碳-性能权衡曲线，这是一个可解释的工具，可视化性能与碳成本之间的Pareto前沿。通过理论分析和代表性ML工作负载的实证验证来展示方法。

Result: 碳感知基准测试改变了模型的相对排名，鼓励同时具备准确性和环境责任感的架构。实证验证表明该方法能有效评估模型的环境影响。

Conclusion: AI-CARE旨在推动研究社区向透明、多目标评估转变，使机器学习进展与全球可持续发展目标保持一致。工具和文档已开源提供。

Abstract: As machine learning (ML) continues its rapid expansion, the environmental cost of model training and inference has become a critical societal concern. Existing benchmarks overwhelmingly focus on standard performance metrics such as accuracy, BLEU, or mAP, while largely ignoring energy consumption and carbon emissions. This single-objective evaluation paradigm is increasingly misaligned with the practical requirements of large-scale deployment, particularly in energy-constrained environments such as mobile devices, developing regions, and climate-aware enterprises. In this paper, we propose AI-CARE, an evaluation tool for reporting energy consumption, and carbon emissions of ML models. In addition, we introduce the carbon-performance tradeoff curve, an interpretable tool that visualizes the Pareto frontier between performance and carbon cost. We demonstrate, through theoretical analysis and empirical validation on representative ML workloads, that carbon-aware benchmarking changes the relative ranking of models and encourages architectures that are simultaneously accurate and environmentally responsible. Our proposal aims to shift the research community toward transparent, multi-objective evaluation and align ML progress with global sustainability goals. The tool and documentation are available at https://github.com/USD-AI-ResearchLab/ai-care.

</details>


### [154] [Regret and Sample Complexity of Online Q-Learning via Concentration of Stochastic Approximation with Time-Inhomogeneous Markov Chains](https://arxiv.org/abs/2602.16274)
*Rahul Singh,Siddharth Chandak,Eric Moulines,Vivek S. Borkar,Nicholas Bambos*

Main category: cs.LG

TL;DR: 首次为经典在线Q学习在无限时域折扣MDP中提供高概率遗憾界，无需乐观或奖励项。分析了Boltzmann Q学习和平滑ε-greedy探索方案，后者获得接近O(N^{9/10})的间隙鲁棒遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有在线Q学习算法通常依赖乐观或奖励项来获得遗憾界，本文旨在为经典在线Q学习提供高概率遗憾界，无需这些额外机制。同时解决传统探索策略在次优间隙较小时的性能下降问题。

Method: 1) 分析带衰减温度的Boltzmann Q学习；2) 提出结合ε-greedy和Boltzmann探索的平滑ε-greedy探索方案；3) 开发适用于迭代和时间依赖转移动态的收缩马尔可夫随机逼近的高概率集中界分析工具。

Result: 1) Boltzmann Q学习的遗憾严重依赖MDP的次优间隙：大间隙时次线性遗憾，小间隙时接近线性增长；2) 平滑ε-greedy方案获得接近O(N^{9/10})的间隙鲁棒遗憾界；3) 开发的新分析工具具有独立价值，收缩因子由混合时间控制且可渐近收敛到1。

Conclusion: 本文首次为经典在线Q学习提供高概率遗憾界，无需乐观机制。提出的平滑ε-greedy探索方案解决了传统探索策略的间隙敏感性，获得鲁棒遗憾界。开发的分析工具为研究收缩马尔可夫随机逼近提供了新方法。

Abstract: We present the first high-probability regret bound for classical online Q-learning in infinite-horizon discounted Markov decision processes, without relying on optimism or bonus terms. We first analyze Boltzmann Q-learning with decaying temperature and show that its regret depends critically on the suboptimality gap of the MDP: for sufficiently large gaps, the regret is sublinear, while for small gaps it deteriorates and can approach linear growth. To address this limitation, we study a Smoothed $ε_n$-Greedy exploration scheme that combines $ε_n$-greedy and Boltzmann exploration, for which we prove a gap-robust regret bound of near-$\tilde{O}(N^{9/10})$. To analyze these algorithms, we develop a high-probability concentration bound for contractive Markovian stochastic approximation with iterate- and time-dependent transition dynamics. This bound may be of independent interest as the contraction factor in our bound is governed by the mixing time and is allowed to converge to one asymptotically.

</details>


### [155] [MoE-Spec: Expert Budgeting for Efficient Speculative Decoding](https://arxiv.org/abs/2602.16052)
*Bradley McDanel,Steven Li,Sruthikesh Surineni,Harshit Khaitan*

Main category: cs.LG

TL;DR: MoE-Spec：一种用于MoE模型的无训练验证时专家预算方法，通过限制每层专家容量来解耦推测深度与内存成本，提高推测解码吞吐量10-30%


<details>
  <summary>Details</summary>
Motivation: 针对MoE模型在推测解码中面临的问题：大推测树会激活大量独特专家，显著增加内存压力并降低相对于自回归解码的加速效果。现有方法通过减少推测深度来应对昂贵的MoE验证，但这限制了性能提升。

Method: 提出MoE-Spec方法，在验证时实施专家预算策略：1) 在每层强制执行固定的专家容量限制；2) 仅加载对验证贡献最大的专家；3) 丢弃使用频率低的长尾专家以减少带宽开销。该方法无需训练，解耦了推测深度与内存成本。

Result: 在多个模型规模和数据集上的实验表明，MoE-Spec在保持可比质量的同时，比最先进的推测解码基线（EAGLE-3）实现10-30%的更高吞吐量。该方法还提供灵活性，可通过更严格的预算在准确性和延迟之间进行权衡。

Conclusion: MoE-Spec通过专家预算策略有效解决了MoE模型在推测解码中的内存瓶颈问题，实现了显著的吞吐量提升，为MoE模型的推理加速提供了实用解决方案。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by verifying multiple drafted tokens in parallel. However, for Mixture-of-Experts (MoE) models, this parallelism introduces a severe bottleneck: large draft trees activate many unique experts, significantly increasing memory pressure and diminishing speedups from speculative decoding relative to autoregressive decoding. Prior methods reduce speculation depth when MoE verification becomes expensive. We propose MoE-Spec, a training-free verification-time expert budgeting method that decouples speculation depth from memory cost by enforcing a fixed expert capacity limit at each layer, loading only the experts that contribute most to verification and dropping the long tail of rarely used experts that drive bandwidth overhead. Experiments across multiple model scales and datasets show that this method yields 10--30\% higher throughput than state-of-the-art speculative decoding baselines (EAGLE-3) at comparable quality, with flexibility to trade accuracy for further latency reductions through tighter budgets.

</details>


### [156] [The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks](https://arxiv.org/abs/2602.16340)
*Eitan Gronich,Gal Vardi*

Main category: cs.LG

TL;DR: 研究动量优化器在齐次模型中的隐式偏差，证明动量算法（如Muon、MomentumGD、Signum）近似于最速下降轨迹，具有向相应边际最大化问题KKT点的偏差。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注最速下降在齐次模型中的隐式偏差，但对动量优化器的偏差理解不足。本文旨在将隐式偏差分析扩展到动量优化器，揭示它们在不同范数下的边际最大化特性。

Method: 首先扩展齐次模型中现有最速下降隐式偏差结果到带可选学习率调度的归一化最速下降。然后证明对于光滑齐次模型，动量最速下降算法（Muon、MomentumGD、Signum）在衰减学习率调度下近似于最速下降轨迹，并分析Adam、Muon-Signum和Muon-Adam的偏差特性。

Result: 动量优化器具有向相应边际最大化问题KKT点的隐式偏差：Muon最大化谱范数边际，MomentumGD最大化ℓ₂边际，Signum最大化ℓ∞边际，Adam（无稳定性常数）也最大化ℓ∞边际，Muon-Signum和Muon-Adam最大化混合范数。实验验证了理论结果。

Conclusion: 动量优化器在齐次模型中表现出与最速下降相似的隐式偏差特性，偏差方向取决于优化器选择的范数类型。这扩展了齐次模型中最速下降和线性模型中动量优化器的现有研究。

Abstract: We study the implicit bias of momentum-based optimizers on homogeneous models. We first extend existing results on the implicit bias of steepest descent in homogeneous models to normalized steepest descent with an optional learning rate schedule. We then show that for smooth homogeneous models, momentum steepest descent algorithms like Muon (spectral norm), MomentumGD ($\ell_2$ norm), and Signum ($\ell_\infty$ norm) are approximate steepest descent trajectories under a decaying learning rate schedule, proving that these algorithms too have a bias towards KKT points of the corresponding margin maximization problem. We extend the analysis to Adam (without the stability constant), which maximizes the $\ell_\infty$ margin, and to Muon-Signum and Muon-Adam, which maximize a hybrid norm. Our experiments corroborate the theory and show that the identity of the margin maximized depends on the choice of optimizer. Overall, our results extend earlier lines of work on steepest descent in homogeneous models and momentum-based optimizers in linear models.

</details>


### [157] [Multi-Objective Alignment of Language Models for Personalized Psychotherapy](https://arxiv.org/abs/2602.16053)
*Mehrab Beikzadeh,Yasaman Asadollah Salmanpour,Ashima Suvarna,Sriram Sankararaman,Matteo Malgaroli,Majid Sarrafzadeh,Saadia Gabriel*

Main category: cs.LG

TL;DR: 本文提出了一种多目标对齐框架MODPO，用于平衡心理健康AI系统中的患者偏好与临床安全性，相比单目标优化实现了更好的平衡效果。


<details>
  <summary>Details</summary>
Motivation: 全球有超过10亿人受心理健康问题影响，但护理资源有限且成本高昂。现有AI系统虽然具有治疗潜力，但当前的对齐方法独立优化目标，无法平衡患者偏好与临床安全性之间的矛盾。

Method: 调查335名有心理健康经历的个人收集偏好排名，开发基于直接偏好优化的多目标对齐框架。训练六个标准（共情、安全性、积极倾听、自我激励改变、信任/融洽关系、患者自主性）的奖励模型，系统比较多目标方法与单目标优化、监督微调和参数合并等方法。

Result: 多目标DPO（MODPO）实现了更好的平衡（77.6%共情，62.6%安全性），优于单目标优化（93.6%共情，47.8%安全性）。治疗标准比一般沟通原则表现好17.2%。盲法临床评估确认MODPO始终被偏好，LLM评估者一致性达到临床医生间可靠性水平。

Conclusion: 多目标对齐框架MODPO能够有效平衡心理健康AI系统中的患者偏好与临床安全性需求，为开发更安全、有效的治疗性AI系统提供了可行方案。

Abstract: Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.

</details>


### [158] [Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent](https://arxiv.org/abs/2602.16436)
*Jean Dufraiche,Paul Mangold,Michaël Perrot,Marc Tommasi*

Main category: cs.LG

TL;DR: 提出IWP-SGD算法，通过Weierstrass逆变换校正LDP数据中的非线性函数估计偏差，在二分类任务中实现无偏估计并收敛到真实总体风险最小化器。


<details>
  <summary>Details</summary>
Motivation: 在非交互式本地差分隐私（LDP）下一次性发布数据虽然支持完全的数据重用，但引入的噪声会在后续分析中产生偏差，特别是在二分类任务中需要校正这种偏差以获得准确的非线性函数估计。

Method: 利用Weierstrass变换刻画LDP数据中的偏差，证明逆变换可得到非线性函数的无偏估计，并构建Inverse Weierstrass Private SGD（IWP-SGD）随机梯度下降算法。

Result: IWP-SGD算法以O(1/n)的速率收敛到真实总体风险最小化器，其中n为样本数量，在合成和真实世界数据集的二分类任务中得到了实证验证。

Conclusion: 通过Weierstrass逆变换成功校正了LDP数据中的偏差，提出的IWP-SGD算法在保护隐私的同时实现了准确的非线性函数估计和模型训练。

Abstract: Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.

</details>


### [159] [Extracting and Analyzing Rail Crossing Behavior Signatures from Videos using Tensor Methods](https://arxiv.org/abs/2602.16057)
*Dawon Ahn,Het Patel,Aemal Khattak,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 提出多视角张量分解框架分析铁路道口行为模式，发现位置比时间对行为影响更大，接近阶段行为最具区分性


<details>
  <summary>Details</summary>
Motivation: 铁路道口安全面临复杂挑战，驾驶员行为因位置、时间和条件而异。传统方法单独分析每个道口，难以发现跨位置的共享行为模式

Method: 提出多视角张量分解框架，将行为分为三个阶段：接近、等待、通过。使用TimeSformer嵌入表示每个阶段，构建相异性矩阵，应用非负对称CP分解发现潜在行为成分

Result: 张量分析显示道口位置比时间对行为模式影响更大，接近阶段行为提供特别有区分性的特征。可视化显示基于位置的聚类，某些道口形成独特的行为簇

Conclusion: 该自动化框架支持跨多个道口的可扩展模式发现，为按行为相似性分组道口提供基础，有助于制定有针对性的安全干预措施

Abstract: Railway crossings present complex safety challenges where driver behavior varies by location, time, and conditions. Traditional approaches analyze crossings individually, limiting the ability to identify shared behavioral patterns across locations. We propose a multi-view tensor decomposition framework that captures behavioral similarities across three temporal phases: Approach (warning activation to gate lowering), Waiting (gates down to train passage), and Clearance (train passage to gate raising). We analyze railway crossing videos from multiple locations using TimeSformer embeddings to represent each phase. By constructing phase-specific similarity matrices and applying non-negative symmetric CP decomposition, we discover latent behavioral components with distinct temporal signatures. Our tensor analysis reveals that crossing location appears to be a stronger determinant of behavior patterns than time of day, and that approach-phase behavior provides particularly discriminative signatures. Visualization of the learned component space confirms location-based clustering, with certain crossings forming distinct behavioral clusters. This automated framework enables scalable pattern discovery across multiple crossings, providing a foundation for grouping locations by behavioral similarity to inform targeted safety interventions.

</details>


### [160] [GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation](https://arxiv.org/abs/2602.16449)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: GICDM方法通过修正嵌入空间中的hubness现象，改进生成模型评估中基于距离的度量指标


<details>
  <summary>Details</summary>
Motivation: 生成模型评估通常依赖高维嵌入空间计算样本距离，但这些空间存在hubness现象，扭曲最近邻关系并偏置基于距离的度量指标

Method: 基于经典ICDM方法，提出Generative ICDM (GICDM)，修正真实数据和生成数据的邻域估计，并引入多尺度扩展以改进经验行为

Result: 在合成和真实基准测试中，GICDM解决了hubness引起的失效问题，恢复了可靠的度量行为，并提高了与人类判断的一致性

Conclusion: GICDM通过修正嵌入空间中的hubness现象，显著改善了生成模型评估的准确性和可靠性

Abstract: Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.

</details>


### [161] [Sequential Membership Inference Attacks](https://arxiv.org/abs/2602.16596)
*Thomas Michel,Debabrota Basu,Emilie Kaufmann*

Main category: cs.LG

TL;DR: 提出SeMI*攻击方法，利用模型更新序列进行成员推理攻击，相比仅使用最终模型的现有方法能获得更强的攻击效果和更严格的隐私审计。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型会经历多次更新，现有成员推理攻击主要针对静态模型，缺乏对模型动态更新的理论分析。利用模型更新序列可以增强攻击能力，但现有研究缺乏对"最优"攻击的理论分析。

Method: 开发了SeMI*攻击方法，利用模型更新序列来检测在特定更新步骤插入的目标数据。对于经验均值计算，推导了在有限样本下的最优攻击能力，包括有隐私保护和无隐私保护的情况。

Result: 理论分析表明，使用模型序列可以避免成员推理信号在最终模型中被稀释。SeMI*能够通过调整插入时间和canary数据来获得更严格的隐私审计。实验验证了SeMI*在不同数据分布和DP-SGD训练模型上的有效性。

Conclusion: SeMI*攻击方法利用模型动态更新序列，在理论和实践上都优于现有基线方法，能够提供更强大的成员推理攻击和更严格的隐私审计。

Abstract: Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.

</details>


### [162] [Omni-iEEG: A Large-Scale, Comprehensive iEEG Dataset and Benchmark for Epilepsy Research](https://arxiv.org/abs/2602.16072)
*Chenda Duan,Yipeng Zhang,Sotaro Kanai,Yuanyi Ding,Atsuro Daida,Pengyue Yu,Tiancheng Zheng,Naoto Kuroda,Shaun A. Hussain,Eishi Asano,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.LG

TL;DR: Omni-iEEG是一个大规模、标准化的颅内脑电图数据集，包含302名患者和178小时高分辨率记录，提供超过36K个专家验证的病理事件标注，旨在解决癫痫研究中数据格式不一致、缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 癫痫影响全球超过5000万人，其中三分之一患者患有药物难治性癫痫，手术是最佳治疗选择。目前临床工作流程依赖劳动密集型的手动审查，而现有数据驱动方法通常基于单中心数据集，存在格式不一致、缺乏标准化基准、很少发布病理事件标注等问题，阻碍了研究的可重复性、跨中心验证和临床相关性。

Method: 通过大量努力整合公开可用来源中的异质iEEG格式、元数据和记录，创建了Omni-iEEG数据集。该数据集包含302名患者和178小时高分辨率记录，提供统一的临床元数据（如癫痫发作起始区、切除区域和手术结果），并由委员会认证的癫痫专家验证。数据集还包含超过36K个专家验证的病理事件标注。

Result: 创建了一个大规模、标准化的iEEG资源，定义了基于临床先验的统一评估指标的临床意义任务，能够在临床相关环境中系统评估模型。展示了在长iEEG片段上进行端到端建模的潜力，并突出了在非神经生理学领域预训练表示的可迁移性。

Conclusion: Omni-iEEG作为机器学习和癫痫研究之间的桥梁，为可重复、可推广和临床可转化的癫痫研究奠定了基础。该项目提供了数据集和代码链接，有助于推动癫痫研究的标准化和临床转化。

Abstract: Epilepsy affects over 50 million people worldwide, and one-third of patients suffer drug-resistant seizures where surgery offers the best chance of seizure freedom. Accurate localization of the epileptogenic zone (EZ) relies on intracranial EEG (iEEG). Clinical workflows, however, remain constrained by labor-intensive manual review. At the same time, existing data-driven approaches are typically developed on single-center datasets that are inconsistent in format and metadata, lack standardized benchmarks, and rarely release pathological event annotations, creating barriers to reproducibility, cross-center validation, and clinical relevance. With extensive efforts to reconcile heterogeneous iEEG formats, metadata, and recordings across publicly available sources, we present $\textbf{Omni-iEEG}$, a large-scale, pre-surgical iEEG resource comprising $\textbf{302 patients}$ and $\textbf{178 hours}$ of high-resolution recordings. The dataset includes harmonized clinical metadata such as seizure onset zones, resections, and surgical outcomes, all validated by board-certified epileptologists. In addition, Omni-iEEG provides over 36K expert-validated annotations of pathological events, enabling robust biomarker studies. Omni-iEEG serves as a bridge between machine learning and epilepsy research. It defines clinically meaningful tasks with unified evaluation metrics grounded in clinical priors, enabling systematic evaluation of models in clinically relevant settings. Beyond benchmarking, we demonstrate the potential of end-to-end modeling on long iEEG segments and highlight the transferability of representations pretrained on non-neurophysiological domains. Together, these contributions establish Omni-iEEG as a foundation for reproducible, generalizable, and clinically translatable epilepsy research. The project page with dataset and code links is available at omni-ieeg.github.io/omni-ieeg.

</details>


### [163] [Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff](https://arxiv.org/abs/2602.16092)
*Patrick Pynadath,Ruqi Zhang*

Main category: cs.LG

TL;DR: 两流注意力在任意顺序自回归模型中的成功不仅源于位置与内容的分离，更源于规避了任意顺序生成中固有的结构-语义权衡问题。


<details>
  <summary>Details</summary>
Motivation: 研究任意顺序自回归模型中两流注意力的真正作用，挑战传统认为其仅用于分离位置与内容的观点，探索更深层的结构-语义权衡问题。

Method: 提出解耦RoPE（旋转位置编码），在不暴露目标内容的情况下提供目标位置信息，用于分离位置-内容分离与结构-语义权衡问题。

Result: 解耦RoPE在短序列长度下表现良好（结构邻近与语义邻近重合），但随着序列长度增加而性能下降（两种顺序分离），表明两流注意力成功源于规避结构-语义权衡。

Conclusion: 两流注意力的核心价值在于解决任意顺序生成中隐藏表示必须同时关注语义信息和结构信息的固有冲突，而非仅仅分离位置与内容。

Abstract: Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.

</details>


### [164] [Axle Sensor Fusion for Online Continual Wheel Fault Detection in Wayside Railway Monitoring](https://arxiv.org/abs/2602.16101)
*Afonso Lourenço,Francisca Osório,Diogo Risca,Goreti Marreiros*

Main category: cs.LG

TL;DR: 提出语义感知、标签高效的持续学习框架，用于铁路故障诊断，融合VAE编码的振动信号与AI提取的语义元数据，实现无监督异常检测并适应运行条件变化


<details>
  <summary>Details</summary>
Motivation: 铁路轮轨界面易磨损故障，传统预测维护方法需要手动特征工程，深度学习模型在线环境下性能下降，需要适应运行模式变化的可靠故障诊断方案

Method: 使用变分自编码器无监督编码加速度计信号，通过AI峰值检测提取光纤光栅传感器的语义元数据，融合两者特征，采用轻量级梯度提升分类器稳定异常评分，结合回放式持续学习策略适应变化

Result: 模型能够检测扁平和多边形化等微小缺陷，并能适应列车类型、速度、载荷和轨道剖面等运行条件的变化，使用单一加速度计和应变计实现路边监测

Conclusion: 提出的语义感知持续学习框架在铁路故障诊断中实现了标签高效的异常检测，能够适应不断变化的运行条件，为铁路安全维护提供了可靠解决方案

Abstract: Reliable and cost-effective maintenance is essential for railway safety, particularly at the wheel-rail interface, which is prone to wear and failure. Predictive maintenance frameworks increasingly leverage sensor-generated time-series data, yet traditional methods require manual feature engineering, and deep learning models often degrade in online settings with evolving operational patterns. This work presents a semantic-aware, label-efficient continual learning framework for railway fault diagnostics. Accelerometer signals are encoded via a Variational AutoEncoder into latent representations capturing the normal operational structure in a fully unsupervised manner. Importantly, semantic metadata, including axle counts, wheel indexes, and strain-based deformations, is extracted via AI-driven peak detection on fiber Bragg grating sensors (resistant to electromagnetic interference) and fused with the VAE embeddings, enhancing anomaly detection under unknown operational conditions. A lightweight gradient boosting supervised classifier stabilizes anomaly scoring with minimal labels, while a replay-based continual learning strategy enables adaptation to evolving domains without catastrophic forgetting. Experiments show the model detects minor imperfections due to flats and polygonization, while adapting to evolving operational conditions, such as changes in train type, speed, load, and track profiles, captured using a single accelerometer and strain gauge in wayside monitoring.

</details>


### [165] [On the Power of Source Screening for Learning Shared Feature Extractors](https://arxiv.org/abs/2602.16125)
*Leo,Wang,Connor Mclaughlin,Lili Su*

Main category: cs.LG

TL;DR: 论文提出在异构数据源共享表示学习中，即使面对传统认为的"好"数据源集合，通过精心选择子集进行训练也能达到统计最优，无需使用全部数据源。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常同时训练所有相关数据源的共同特征提取器和源特定头部，但低相关性或质量差的数据源可能阻碍表示学习。即使对于传统认为的"好"数据源集合（各源具有相似相关性和质量），也需要进一步研究哪些数据源应该联合学习。

Method: 在线性设置下，假设数据源共享低维子空间。提出源筛选方法，识别信息性子群体，开发算法和实用启发式方法来选择训练子集。通过理论分析和实证评估验证方法的有效性。

Result: 研究表明，对于广泛的问题实例，在精心选择的源子集上训练足以达到极小极大最优性，即使丢弃大量数据。理论分析和合成及真实世界数据集的实证评估都验证了方法的有效性。

Conclusion: 源筛选在统计最优子空间估计中起核心作用。通过识别信息性子群体并仅在这些子集上训练，可以在保持最优性能的同时提高计算效率，为异构数据源共享表示学习提供了新的视角。

Abstract: Learning with shared representation is widely recognized as an effective way to separate commonalities from heterogeneity across various heterogeneous sources. Most existing work includes all related data sources via simultaneously training a common feature extractor and source-specific heads. It is well understood that data sources with low relevance or poor quality may hinder representation learning. In this paper, we further dive into the question of which data sources should be learned jointly by focusing on the traditionally deemed ``good'' collection of sources, in which individual sources have similar relevance and qualities with respect to the true underlying common structure. Towards tractability, we focus on the linear setting where sources share a low-dimensional subspace. We find that source screening can play a central role in statistically optimal subspace estimation. We show that, for a broad class of problem instances, training on a carefully selected subset of sources suffices to achieve minimax optimality, even when a substantial portion of data is discarded. We formalize the notion of an informative subpopulation, develop algorithms and practical heuristics for identifying such subsets, and validate their effectiveness through both theoretical analysis and empirical evaluations on synthetic and real-world datasets.

</details>


### [166] [Investigating GNN Convergence on Large Randomly Generated Graphs with Realistic Node Feature Correlations](https://arxiv.org/abs/2602.16145)
*Mohammed Zain Ali Ahmed*

Main category: cs.LG

TL;DR: 本文提出了一种生成具有相关节点特征的随机图的新方法，以更真实地评估图神经网络的表达能力，挑战了现有收敛性研究的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究分析图神经网络在大随机图上的收敛行为时，大多未考虑节点特征之间的相关性，而现实网络中的节点特征往往存在相关性。这导致对GNN表达能力的评估不够准确，不能反映其在真实图上的实际表现。

Method: 提出了一种生成具有相关节点特征的随机图的新方法。节点特征以特定方式采样，确保相邻节点之间的相关性。采样方案的设计灵感来源于现实图（特别是Barabási-Albert模型）所表现出的特性。

Result: 理论分析表明，在某些情况下可以避免收敛问题。在大规模随机图上进行的实证验证观察到了发散行为，这表明GNN的表达能力可能比最初研究显示的更强，特别是在现实图上。

Conclusion: 图神经网络在具有相关节点特征的现实图上可能比现有收敛性研究显示的更具表达能力。新提出的随机图生成方法能够更准确地评估GNN在真实场景下的表现。

Abstract: There are a number of existing studies analysing the convergence behaviour of graph neural networks on large random graphs. Unfortunately, the majority of these studies do not model correlations between node features, which would naturally exist in a variety of real-life networks. Consequently, the derived limitations of GNNs, resulting from such convergence behaviour, is not truly reflective of the expressive power of GNNs when applied to realistic graphs. In this paper, we will introduce a novel method to generate random graphs that have correlated node features. The node features will be sampled in such a manner to ensure correlation between neighbouring nodes. As motivation for our choice of sampling scheme, we will appeal to properties exhibited by real-life graphs, particularly properties that are captured by the Barabási-Albert model. A theoretical analysis will strongly indicate that convergence can be avoided in some cases, which we will empirically validate on large random graphs generated using our novel method. The observed divergent behaviour provides evidence that GNNs may be more expressive than initial studies would suggest, especially on realistic graphs.

</details>


### [167] [ASPEN: Spectral-Temporal Fusion for Cross-Subject Brain Decoding](https://arxiv.org/abs/2602.16147)
*Megan Lee,Seung Ha Hwang,Inhyeok Choi,Shreyas Darade,Mengchun Zhang,Kateryna Shapovalenko*

Main category: cs.LG

TL;DR: ASPEN：一种通过乘法融合频谱和时域特征的混合架构，用于提升EEG脑机接口的跨被试泛化能力


<details>
  <summary>Details</summary>
Motivation: EEG脑机接口中的跨被试泛化面临挑战，因为神经信号存在个体差异。研究发现频谱特征比时域波形具有更高的跨被试相似性，这为改进跨被试迁移提供了机会。

Method: 提出ASPEN混合架构，结合频谱和时域特征流，通过乘法融合要求跨模态一致性才能传播特征，能够根据范式动态实现最优的频谱-时域平衡。

Result: 在六个基准数据集上的实验表明，ASPEN在三个数据集上取得了最佳未见被试准确率，在其他数据集上表现具有竞争力，证明了乘法多模态融合能够实现有效的跨被试泛化。

Conclusion: 频谱特征比时域信号具有更高的跨被试稳定性，通过乘法融合频谱和时域特征的ASPEN架构能够有效提升EEG脑机接口的跨被试泛化能力。

Abstract: Cross-subject generalization in EEG-based brain-computer interfaces (BCIs) remains challenging due to individual variability in neural signals. We investigate whether spectral representations offer more stable features for cross-subject transfer than temporal waveforms. Through correlation analyses across three EEG paradigms (SSVEP, P300, and Motor Imagery), we find that spectral features exhibit consistently higher cross-subject similarity than temporal signals. Motivated by this observation, we introduce ASPEN, a hybrid architecture that combines spectral and temporal feature streams via multiplicative fusion, requiring cross-modal agreement for features to propagate. Experiments across six benchmark datasets reveal that ASPEN is able to dynamically achieve the optimal spectral-temporal balance depending on the paradigm. ASPEN achieves the best unseen-subject accuracy on three of six datasets and competitive performance on others, demonstrating that multiplicative multimodal fusion enables effective cross-subject generalization.

</details>


### [168] [Differentially Private Non-convex Distributionally Robust Optimization](https://arxiv.org/abs/2602.16155)
*Difei Xu,Meng Ding,Zebin Ma,Huanyi Xie,Youming Tao,Aicha Slaitane,Di Wang*

Main category: cs.LG

TL;DR: 本文研究差分隐私下的分布鲁棒优化（DP-DRO），针对非凸损失和ψ-散度，提出了DP Double-Spider和DP Recursive-Spider方法，在隐私保护下实现了较好的效用边界。


<details>
  <summary>Details</summary>
Motivation: 现实部署面临分布偏移、群体不平衡和对抗扰动，传统经验风险最小化（ERM）在这些情况下性能严重下降。分布鲁棒优化（DRO）通过优化不确定性分布集上的最坏情况期望损失提供了鲁棒性解决方案。同时，由于DRO训练数据包含敏感信息，需要差分隐私（DP）保护以防止信息泄露。然而，与经典DP-ERM相比，DP-DRO由于其具有不确定性约束的极小极大优化结构而研究较少。

Method: 首先将具有一般ψ-散度的DRO重新表述为最小化问题，并针对该结构开发了新颖的(ε,δ)-DP优化方法DP Double-Spider。对于特定的KL散度情况，通过将问题转化为组合有限和优化问题，开发了DP Recursive-Spider方法。两种方法都针对非凸损失函数设计。

Result: DP Double-Spider在温和假设下实现了梯度范数的效用边界O(1/√n + (√d log(1/δ)/(nε))^{2/3})。对于KL散度的DP-DRO，DP Recursive-Spider实现了效用边界O((√d log(1/δ)/(nε))^{2/3})，与非凸DP-ERM的最佳已知结果匹配。实验表明，所提方法在DP极小极大优化中优于现有方法。

Conclusion: 本文填补了DP-DRO研究的空白，为具有ψ-散度和非凸损失的分布鲁棒优化提供了有效的差分隐私保护方法，在隐私保护和模型效用之间取得了良好平衡，特别是在KL散度情况下达到了与非凸DP-ERM相当的性能。

Abstract: Real-world deployments routinely face distribution shifts, group imbalances, and adversarial perturbations, under which the traditional Empirical Risk Minimization (ERM) framework can degrade severely.
  Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case expected loss over an uncertainty set of distributions, offering a principled approach to robustness.
  Meanwhile, as training data in DRO always involves sensitive information, safeguarding it against leakage under Differential Privacy (DP) is essential.
  In contrast to classical DP-ERM, DP-DRO has received much less attention due to its minimax optimization structure with uncertainty constraint.
  To bridge the gap, we provide a comprehensive study of DP-(finite-sum)-DRO with $ψ$-divergence and non-convex loss.
  First, we study DRO with general $ψ$-divergence by reformulating it as a minimization problem, and develop a novel $(\varepsilon, δ)$-DP optimization method, called DP Double-Spider, tailored to this structure.
  Under mild assumptions, we show that it achieves a utility bound of $\mathcal{O}(\frac{1}{\sqrt{n}}+ (\frac{\sqrt{d \log (1/δ)}}{n \varepsilon})^{2/3})$ in terms of the gradient norm, where $n$ denotes the data size and $d$ denotes the model dimension.
  We further improve the utility rate for specific divergences.
  In particular, for DP-DRO with KL-divergence, by transforming the problem into a compositional finite-sum optimization problem, we develop a DP Recursive-Spider method and show that it achieves a utility bound of $\mathcal{O}((\frac{\sqrt{d \log(1/δ)}}{n\varepsilon})^{2/3} )$, matching the best-known result for non-convex DP-ERM.
  Experimentally, we demonstrate that our proposed methods outperform existing approaches for DP minimax optimization.

</details>


### [169] [HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents](https://arxiv.org/abs/2602.16165)
*Jiangweizhi Peng,Yuanxin Liu,Ruida Zhou,Charles Fleming,Zhaoran Wang,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: HiPER：分层规划-执行强化学习框架，通过高层规划提出子目标、低层执行实现多步动作，使用分层优势估计优化信用分配，在稀疏奖励的长时任务中显著提升LLM智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法将LLM智能体建模为单时间尺度的扁平策略，在稀疏延迟奖励的长时任务中，需要在整个轨迹上传播信用，导致优化不稳定和信用分配低效。

Method: 提出HiPER分层框架：高层规划器提出子目标，低层执行器执行多步动作。引入分层优势估计（HAE）技术，在规划和执行两个层面分配信用，通过聚合每个子目标的回报和协调两级更新来减少方差。

Result: 在ALFWorld上达到97.4%成功率，WebShop上达到83.3%（相比之前最佳方法分别提升6.6%和8.3%），在需要多个依赖子任务的长时任务上提升尤其显著。

Conclusion: 显式的分层分解对于可扩展的多轮LLM智能体RL训练至关重要，HiPER框架通过分离规划与执行、改进信用分配，显著提升了长时决策任务的性能。

Abstract: Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.
  We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.
  Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\% success on ALFWorld and 83.3\% on WebShop with Qwen2.5-7B-Instruct (+6.6\% and +8.3\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.

</details>


### [170] [Muon with Spectral Guidance: Efficient Optimization for Scientific Machine Learning](https://arxiv.org/abs/2602.16167)
*Binghang Lu,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: 提出SpecMuon优化器，结合Muon的正交化几何与模态松弛标量辅助变量机制，通过奇异模态分解和自适应步长调节，解决物理信息神经网络中的优化困难问题。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络和神经算子常面临优化困难，包括病态梯度、多尺度频谱行为和物理约束导致的刚性。虽然Muon优化器通过奇异向量基的正交化更新改善了几何条件，但其单位奇异值更新可能导致步长过于激进，且缺乏稳定性保证。

Method: 提出SpecMuon优化器，将矩阵值梯度分解为奇异模态，沿主导频谱方向分别应用松弛标量辅助变量更新。该方法将优化解释为多模态梯度流，自适应调节步长同时保持Muon的尺度平衡特性。

Result: 建立了SpecMuon的严格理论性质，包括修正的能量耗散定律、辅助变量的正性和有界性，以及在Polyak-Lojasiewicz条件下的全局线性收敛率。数值实验表明，在一维Burgers方程和分数阶偏微分方程等基准问题上，SpecMuon相比Adam、AdamW和原始Muon优化器实现了更快收敛和更好稳定性。

Conclusion: SpecMuon通过整合Muon的正交化几何与模态松弛标量辅助变量机制，为物理信息学习提供了具有理论保证的稳定优化方法，显著改善了收敛速度和稳定性。

Abstract: Physics-informed neural networks and neural operators often suffer from severe optimization difficulties caused by ill-conditioned gradients, multi-scale spectral behavior, and stiffness induced by physical constraints. Recently, the Muon optimizer has shown promise by performing orthogonalized updates in the singular-vector basis of the gradient, thereby improving geometric conditioning. However, its unit-singular-value updates may lead to overly aggressive steps and lack explicit stability guarantees when applied to physics-informed learning. In this work, we propose SpecMuon, a spectral-aware optimizer that integrates Muon's orthogonalized geometry with a mode-wise relaxed scalar auxiliary variable (RSAV) mechanism. By decomposing matrix-valued gradients into singular modes and applying RSAV updates individually along dominant spectral directions, SpecMuon adaptively regulates step sizes according to the global loss energy while preserving Muon's scale-balancing properties. This formulation interprets optimization as a multi-mode gradient flow and enables principled control of stiff spectral components. We establish rigorous theoretical properties of SpecMuon, including a modified energy dissipation law, positivity and boundedness of auxiliary variables, and global convergence with a linear rate under the Polyak-Lojasiewicz condition. Numerical experiments on physics-informed neural networks, DeepONets, and fractional PINN-DeepONets demonstrate that SpecMuon achieves faster convergence and improved stability compared with Adam, AdamW, and the original Muon optimizer on benchmark problems such as the one-dimensional Burgers equation and fractional partial differential equations.

</details>


### [171] [Discrete Stochastic Localization for Non-autoregressive Generation](https://arxiv.org/abs/2602.16169)
*Yunshu Wu,Jiayi Cheng,Partha Thakuria,Rob Brekelmans,Evangelos E. Papalexakis,Greg Ver Steeg*

Main category: cs.LG

TL;DR: DSL方法通过训练单一SNR不变的降噪器，在连续噪声水平上优化掩码扩散语言模型，显著提升了迭代精化的步数效率，用更少的评估次数达到或超过自回归模型质量。


<details>
  <summary>Details</summary>
Motivation: 非自回归生成虽然能并行预测多个token减少解码延迟，但迭代精化存在错误累积和自生成草稿下的分布偏移问题。掩码扩散语言模型及其重掩码采样器作为现代NAR迭代精化方法，需要提高采样效率。

Method: 提出DSL（离散随机定位）方法，训练单一SNR不变的降噪器，在连续噪声水平上工作，将中间草稿噪声与掩码式端点损坏连接在一个扩散Transformer中。

Result: 在OpenWebText上，DSL微调在低步数预算下获得显著MAUVE增益，比MDLM+ReMDM基线减少约4倍降噪器评估次数，在高预算下匹配自回归质量。分析显示改进了自校正和不确定性校准。

Conclusion: 仅通过训练就能显著提升MDLM/ReMDM采样的步数效率，使重掩码采样在计算上更加高效，为非自回归生成提供了更实用的解决方案。

Abstract: Non-autoregressive (NAR) generation reduces decoding latency by predicting many tokens in parallel, but iterative refinement often suffers from error accumulation and distribution shift under self-generated drafts. Masked diffusion language models (MDLMs) and their remasking samplers (e.g., ReMDM) can be viewed as modern NAR iterative refinement, where generation repeatedly revises a partially observed draft. In this work we show that \emph{training alone} can substantially improve the step-efficiency of MDLM/ReMDM sampling. We propose \textsc{DSL} (Discrete Stochastic Localization), which trains a single SNR-invariant denoiser across a continuum of corruption levels, bridging intermediate draft noise and mask-style endpoint corruption within one Diffusion Transformer. On OpenWebText, \textsc{DSL} fine-tuning yields large MAUVE gains at low step budgets, surpassing the MDLM+ReMDM baseline with \(\sim\)4$\times$ fewer denoiser evaluations, and matches autoregressive quality at high budgets. Analyses show improved self-correction and uncertainty calibration, making remasking markedly more compute-efficient.

</details>


### [172] [Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters](https://arxiv.org/abs/2602.16181)
*Diego Labate,Dipanwita Thakur,Giancarlo Fortino*

Main category: cs.LG

TL;DR: 提出一种基于联邦学习的隐私保护能源盗窃检测框架，使用轻量级MLP模型和差分隐私技术，在保护用户隐私的同时实现高效检测


<details>
  <summary>Details</summary>
Motivation: 能源盗窃对智能电网稳定性和效率构成重大威胁，传统集中式机器学习方法需要聚合用户数据，存在隐私和安全问题，且智能电表设备资源受限无法运行复杂模型

Method: 提出隐私保护联邦学习框架，使用轻量级多层感知器（MLP）模型适合低功耗智能电表部署，集成基本差分隐私技术，在本地模型更新前注入高斯噪声，确保隐私保护

Result: 在真实世界智能电表数据集上评估，在IID和非IID数据分布下均取得竞争性准确率、精确率、召回率和AUC分数，同时保持隐私和效率

Conclusion: 该方法为下一代智能电网基础设施提供了实用且可扩展的安全能源盗窃检测解决方案，在保护隐私的同时不损害学习性能

Abstract: Energy theft poses a significant threat to the stability and efficiency of smart grids, leading to substantial economic losses and operational challenges. Traditional centralized machine learning approaches for theft detection require aggregating user data, raising serious concerns about privacy and data security. These issues are further exacerbated in smart meter environments, where devices are often resource-constrained and lack the capacity to run heavy models. In this work, we propose a privacy-preserving federated learning framework for energy theft detection that addresses both privacy and computational constraints. Our approach leverages a lightweight multilayer perceptron (MLP) model, suitable for deployment on low-power smart meters, and integrates basic differential privacy (DP) by injecting Gaussian noise into local model updates before aggregation. This ensures formal privacy guarantees without compromising learning performance. We evaluate our framework on a real-world smart meter dataset under both IID and non-IID data distributions. Experimental results demonstrate that our method achieves competitive accuracy, precision, recall, and AUC scores while maintaining privacy and efficiency. This makes the proposed solution practical and scalable for secure energy theft detection in next-generation smart grid infrastructures.

</details>


### [173] [Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting](https://arxiv.org/abs/2602.16188)
*Filippos Bellos,NaveenJohn Premkumar,Yannis Avrithis,Nam H. Nguyen,Jason J. Corso*

Main category: cs.LG

TL;DR: TPC通过将时间作为第一类模态，在多层次注入时间信息，解决了现有LLM时间序列方法中时间信息在深层衰减的问题，在长期预测中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LLM时间序列方法通常只在输入层浅层注入时间信息（如位置编码或提示），导致时间信息在深层网络中逐渐衰减，限制了模型的时间推理能力。

Method: 提出Temporal-Prior Conditioning (TPC)：1) 将时间作为第一类模态，在多个深度层次进行条件化；2) 在补丁流中添加可学习的时间序列标记；3) 在选定层中，这些标记通过交叉注意力机制关注由同一冻结LLM编码的紧凑、人类可读时间描述符；4) 通过自注意力将时间上下文反馈给模型。

Result: TPC在仅训练交叉注意力模块的情况下，在多个数据集上的长期预测任务中，一致优于完全微调和浅层条件化策略，达到了最先进的性能。

Conclusion: TPC通过将时间作为第一类模态并在多层次注入时间信息，有效解耦了时间序列信号和时间信息，在保持低参数预算的同时显著提升了时间序列预测性能。

Abstract: LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers. We introduce Temporal-Prior Conditioning (TPC), which elevates time to a first-class modality that conditions the model at multiple depths. TPC attaches a small set of learnable time series tokens to the patch stream; at selected layers these tokens cross-attend to temporal embeddings derived from compact, human-readable temporal descriptors encoded by the same frozen LLM, then feed temporal context back via self-attention. This disentangles time series signal and temporal information while maintaining a low parameter budget. We show that by training only the cross-attention modules and explicitly disentangling time series signal and temporal information, TPC consistently outperforms both full fine-tuning and shallow conditioning strategies, achieving state-of-the-art performance in long-term forecasting across diverse datasets. Code available at: https://github.com/fil-mp/Deep_tpc

</details>


### [174] [Rethinking Input Domains in Physics-Informed Neural Networks via Geometric Compactification Mappings](https://arxiv.org/abs/2602.16193)
*Zhenzhen Huang,Haoyu Bian,Jiaquan Zhang,Yibei Liu,Kuien Liu,Caiyan Qin,Guoqing Wang,Yang Yang,Chaoning Zhang*

Main category: cs.LG

TL;DR: 提出GC-PINN框架，通过几何紧化映射解决PINN在多尺度PDE中梯度刚度和病态问题，提高训练稳定性和收敛速度


<details>
  <summary>Details</summary>
Motivation: 多尺度PDE同时包含平滑低频分量和局部化高频结构，传统PINN使用固定坐标系输入，几何错位会导致梯度刚度和病态问题，阻碍收敛

Method: 引入几何紧化映射范式，通过可微几何紧化映射重塑输入坐标，将PDE的几何结构与残差算子的谱特性耦合。提出GC-PINN框架，包含三种映射策略：周期性边界、远场尺度扩展和局部奇异结构

Result: 在代表性1D和2D PDE上，该方法产生更均匀的残差分布和更高的求解精度，同时改善训练稳定性和收敛速度

Conclusion: GC-PINN通过几何映射有效解决了多尺度PDE中的梯度刚度和病态问题，无需修改底层PINN架构，显著提升了PINN的性能

Abstract: Several complex physical systems are governed by multi-scale partial differential equations (PDEs) that exhibit both smooth low-frequency components and localized high-frequency structures. Existing physics-informed neural network (PINN) methods typically train with fixed coordinate system inputs, where geometric misalignment with these structures induces gradient stiffness and ill-conditioning that hinder convergence. To address this issue, we introduce a mapping paradigm that reshapes the input coordinates through differentiable geometric compactification mappings and couples the geometric structure of PDEs with the spectral properties of residual operators. Based on this paradigm, we propose Geometric Compactification (GC)-PINN, a framework that introduces three mapping strategies for periodic boundaries, far-field scale expansion, and localized singular structures in the input domain without modifying the underlying PINN architecture. Extensive empirical evaluation demonstrates that this approach yields more uniform residual distributions and higher solution accuracy on representative 1D and 2D PDEs, while improving training stability and convergence speed.

</details>


### [175] [Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16196)
*Emile Anand,Richard Hoffmann,Sarah Liaw,Adam Wierman*

Main category: cs.LG

TL;DR: 提出GMFS框架，通过子采样解决异构智能体交互的大规模MARL问题，降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体强化学习中，联合状态-动作空间随智能体数量指数增长。传统均值场方法假设同质交互，而图子方法虽能捕捉异质性但计算成本高。

Method: 引入GMFS（图子均值场子采样）框架，根据交互强度子采样κ个智能体，近似图子加权均值场，学习策略。

Result: 样本复杂度为poly(κ)，最优性差距为O(1/√κ)。在机器人协调任务中的数值模拟验证了理论，GMFS实现了接近最优的性能。

Conclusion: GMFS为具有异构智能体交互的可扩展合作MARL提供了有效的解决方案，平衡了计算效率和性能。

Abstract: Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\texttt{GMFS}$, a $\textbf{G}$raphon $\textbf{M}$ean-$\textbf{F}$ield $\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\mathrm{poly}(κ)$ and optimality gap $O(1/\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\texttt{GMFS}$ achieves near-optimal performance.

</details>


### [176] [ModalImmune: Immunity Driven Unlearning via Self Destructive Training](https://arxiv.org/abs/2602.16197)
*Rong Fu,Jia Yee Tan,Wenxin Zhang,Zijian Zhang,Ziming Wang,Zhaolu Kang,Muge Qi,Shuning Zhang,Simon Fong*

Main category: cs.LG

TL;DR: ModalImmune是一个训练框架，通过在训练中故意破坏选定模态信息，使多模态模型学习对模态破坏具有鲁棒性的联合表示。


<details>
  <summary>Details</summary>
Motivation: 多模态系统在部署时容易受到输入通道部分或完全丢失的影响，这会削弱其在真实世界环境中的可靠性。

Method: 结合频谱自适应崩溃正则化器、信息增益引导的控制器进行针对性干预、曲率感知梯度掩码稳定破坏性更新，以及经过认证的Neumann截断超梯度过程来自动调整元参数。

Result: 在标准多模态基准测试上的实证评估表明，ModalImmune提高了对模态移除和损坏的韧性，同时保持了收敛稳定性和重建能力。

Conclusion: ModalImmune框架通过强制模态免疫性，使多模态模型能够更好地应对现实世界中的模态破坏问题，提高系统可靠性。

Abstract: Multimodal systems are vulnerable to partial or complete loss of input channels at deployment, which undermines reliability in real-world settings. This paper presents ModalImmune, a training framework that enforces modality immunity by intentionally and controllably collapsing selected modality information during training so the model learns joint representations that are robust to destructive modality influence. The framework combines a spectrum-adaptive collapse regularizer, an information-gain guided controller for targeted interventions, curvature-aware gradient masking to stabilize destructive updates, and a certified Neumann-truncated hyper-gradient procedure for automatic meta-parameter adaptation. Empirical evaluation on standard multimodal benchmarks demonstrates that ModalImmune improves resilience to modality removal and corruption while retaining convergence stability and reconstruction capacity.

</details>


### [177] [Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform](https://arxiv.org/abs/2602.16198)
*Qijie Zhu,Zeqi Ye,Han Liu,Zhaoran Wang,Minshuo Chen*

Main category: cs.LG

TL;DR: DOIT是一种无需训练、计算高效的适应方法，通过Doob's h-transform将预训练扩散模型导向高奖励分布，适用于不可微分的通用奖励函数。


<details>
  <summary>Details</summary>
Motivation: 现有适应方法要么计算开销大（需要额外训练），要么依赖奖励函数的可微性假设，且缺乏理论保证。需要一种训练免费、计算高效、适用于通用不可微分奖励的方法。

Method: 基于测度传输框架，使用Doob's h-transform实现从预训练生成分布到高奖励目标分布的传输。通过动态修正扩散采样过程，无需修改预训练模型，支持基于模拟的计算。

Result: 理论上建立了高概率收敛保证，分析了动态Doob修正的近似误差。在D4RL离线RL基准测试中，始终优于最先进基线方法，同时保持采样效率。

Conclusion: DOIT提供了一种训练免费、计算高效的适应框架，适用于通用不可微分奖励，具有理论保证，在离线强化学习任务中表现出色。

Abstract: Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical justification and guarantees are seldom established. In this paper, we propose DOIT (Doob-Oriented Inference-time Transformation), a training-free and computationally efficient adaptation method that applies to generic, non-differentiable rewards. The key framework underlying our method is a measure transport formulation that seeks to transport the pre-trained generative distribution to a high-reward target distribution. We leverage Doob's $h$-transform to realize this transport, which induces a dynamic correction to the diffusion sampling process and enables efficient simulation-based computation without modifying the pre-trained model. Theoretically, we establish a high probability convergence guarantee to the target high-reward distribution via characterizing the approximation error in the dynamic Doob's correction. Empirically, on D4RL offline RL benchmarks, our method consistently outperforms state-of-the-art baselines while preserving sampling efficiency.

</details>


### [178] [Linked Data Classification using Neurochaos Learning](https://arxiv.org/abs/2602.16204)
*Pooja Honna,Ayush Patravali,Nithin Nagaraj,Nanjangud C. Narendra*

Main category: cs.LG

TL;DR: 该论文将神经混沌学习（NL）应用于知识图谱等链接数据，通过节点聚合将图特征输入ChaosNet架构，在同性图和异性图数据集上测试性能。


<details>
  <summary>Details</summary>
Motivation: 神经混沌学习（NL）具有小样本学习和低计算需求的优势，已在可分离数据和时序数据上表现优异。本研究旨在将NL扩展到链接数据领域，特别是知识图谱，探索其在图数据上的应用潜力。

Method: 通过实现知识图谱上的节点聚合，将聚合后的节点特征输入最简单的NL架构ChaosNet。在同性图和不同异性程度的异性图数据集上进行实验验证。

Result: 该方法在同性图上的效果优于异性图。展示了NL在链接数据上的应用可行性，同时揭示了在不同类型图数据上的性能差异。

Conclusion: 成功将神经混沌学习扩展到知识图谱等链接数据领域，证明了其在同性图上的有效性，为未来在更复杂图结构和异性图上的改进提供了方向。

Abstract: Neurochaos Learning (NL) has shown promise in recent times over traditional deep learning due to its two key features: ability to learn from small sized training samples, and low compute requirements. In prior work, NL has been implemented and extensively tested on separable and time series data, and demonstrated its superior performance on both classification and regression tasks. In this paper, we investigate the next step in NL, viz., applying NL to linked data, in particular, data that is represented in the form of knowledge graphs. We integrate linked data into NL by implementing node aggregation on knowledge graphs, and then feeding the aggregated node features to the simplest NL architecture: ChaosNet. We demonstrate the results of our implementation on homophilic graph datasets as well as heterophilic graph datasets of verying heterophily. We show better efficacy of our approach on homophilic graphs than on heterophilic graphs. While doing so, we also present our analysis of the results, as well as suggestions for future work.

</details>


### [179] [Geometric Neural Operators via Lie Group-Constrained Latent Dynamics](https://arxiv.org/abs/2602.16209)
*Jiaquan Zhang,Fachrina Dewi Puspitasari,Songbo Zhang,Yibei Liu,Kuien Liu,Caiyan Qin,Fan Mo,Peng Wang,Yang Yang,Chaoning Zhang*

Main category: cs.LG

TL;DR: 提出MCL方法，通过李群约束流形来改进神经算子的长期预测稳定性，降低误差30-50%


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在多层迭代和长期推演中存在不稳定性问题，这是因为欧几里得潜在空间更新违反了几何和守恒定律

Method: 提出基于李群的流形约束方法(MCL)，使用低秩李代数参数化约束流形，在潜在表示上执行群作用更新

Result: 在1-D Burgers和2-D Navier-Stokes等PDE上的实验表明，MCL能有效降低相对预测误差30-50%，仅增加2.26%参数

Conclusion: MCL为神经算子提供了可扩展的解决方案，通过引入几何约束提高了长期预测的保真度

Abstract: Neural operators offer an effective framework for learning solutions of partial differential equations for many physical systems in a resolution-invariant and data-driven manner. Existing neural operators, however, often suffer from instability in multi-layer iteration and long-horizon rollout, which stems from the unconstrained Euclidean latent space updates that violate the geometric and conservation laws. To address this challenge, we propose to constrain manifolds with low-rank Lie algebra parameterization that performs group action updates on the latent representation. Our method, termed Manifold Constraining based on Lie group (MCL), acts as an efficient \emph{plug-and-play} module that enforces geometric inductive bias to existing neural operators. Extensive experiments on various partial differential equations, such as 1-D Burgers and 2-D Navier-Stokes, over a wide range of parameters and steps demonstrate that our method effectively lowers the relative prediction error by 30-50\% at the cost of 2.26\% of parameter increase. The results show that our approach provides a scalable solution for improving long-term prediction fidelity by addressing the principled geometric constraints absent in the neural operator updates.

</details>


### [180] [Graph neural network for colliding particles with an application to sea ice floe modeling](https://arxiv.org/abs/2602.16213)
*Ruibiao Zhu*

Main category: cs.LG

TL;DR: 提出使用图神经网络（GNN）进行海冰建模的新方法，利用海冰自然图结构（节点为冰片，边为物理相互作用），结合数据同化技术，在合成数据上验证了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法计算密集且可扩展性差，需要更高效的海冰建模工具，特别是在边缘冰区（MIZ）的预测中。

Method: 开发了碰撞捕获网络（CN），利用GNN模拟海冰的自然图结构（节点=冰片，边=物理相互作用），结合数据同化技术，在一维框架中作为基础步骤实现。

Result: 使用合成数据验证（有/无观测点），模型在不损失精度的情况下加速了轨迹模拟，为边缘冰区预测提供了更高效工具。

Conclusion: 该方法展示了机器学习与数据同化结合进行高效海冰建模的潜力，为边缘冰区预测提供了更有效的工具。

Abstract: This paper introduces a novel approach to sea ice modeling using Graph Neural Networks (GNNs), utilizing the natural graph structure of sea ice, where nodes represent individual ice pieces, and edges model the physical interactions, including collisions. This concept is developed within a one-dimensional framework as a foundational step. Traditional numerical methods, while effective, are computationally intensive and less scalable. By utilizing GNNs, the proposed model, termed the Collision-captured Network (CN), integrates data assimilation (DA) techniques to effectively learn and predict sea ice dynamics under various conditions. The approach was validated using synthetic data, both with and without observed data points, and it was found that the model accelerates the simulation of trajectories without compromising accuracy. This advancement offers a more efficient tool for forecasting in marginal ice zones (MIZ) and highlights the potential of combining machine learning with data assimilation for more effective and efficient modeling.

</details>


### [181] [UCTECG-Net: Uncertainty-aware Convolution Transformer ECG Network for Arrhythmia Detection](https://arxiv.org/abs/2602.16216)
*Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya*

Main category: cs.LG

TL;DR: UCTECG-Net：结合一维卷积和Transformer编码器的混合架构，用于心电图分类，集成三种不确定性量化方法，在MIT-BIH和PTB数据集上达到98.58%和99.14%的准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在心电图自动分类方面取得了进步，但预测可靠性的有限洞察阻碍了其在安全关键场景中的应用。需要开发能够提供不确定性估计的模型，以增强临床决策支持的可信度。

Method: 提出UCTECG-Net混合架构，结合一维卷积和Transformer编码器，同时处理原始ECG信号和其频谱图。集成三种不确定性量化方法：蒙特卡洛Dropout、深度集成和集成蒙特卡洛Dropout，并使用不确定性感知混淆矩阵和衍生指标进行评估。

Result: 在MIT-BIH心律失常和PTB诊断数据集上，UCTECG-Net在准确率、精确率、召回率和F1分数上均优于LSTM、CNN1D和Transformer基线模型，分别达到98.58%和99.14%的准确率。集成或EMCD方法使UCTECG-Net提供更可靠、更好对齐的不确定性估计。

Conclusion: UCTECG-Net，特别是结合集成或EMCD方法，能够提供比竞争架构更可靠和更好对齐的不确定性估计，为风险感知的心电图决策支持提供了更强的基础。

Abstract: Deep learning has improved automated electrocardiogram (ECG) classification, but limited insight into prediction reliability hinders its use in safety-critical settings. This paper proposes UCTECG-Net, an uncertainty-aware hybrid architecture that combines one-dimensional convolutions and Transformer encoders to process raw ECG signals and their spectrograms jointly. Evaluated on the MIT-BIH Arrhythmia and PTB Diagnostic datasets, UCTECG-Net outperforms LSTM, CNN1D, and Transformer baselines in terms of accuracy, precision, recall and F1 score, achieving up to 98.58% accuracy on MIT-BIH and 99.14% on PTB. To assess predictive reliability, we integrate three uncertainty quantification methods (Monte Carlo Dropout, Deep Ensembles, and Ensemble Monte Carlo Dropout) into all models and analyze their behavior using an uncertainty-aware confusion matrix and derived metrics. The results show that UCTECG-Net, particularly with Ensemble or EMCD, provides more reliable and better-aligned uncertainty estimates than competing architectures, offering a stronger basis for risk-aware ECG decision support.

</details>


### [182] [Multi-Class Boundary Extraction from Implicit Representations](https://arxiv.org/abs/2602.16217)
*Jash Vira,Andrew Myers,Simon Ratcliffe*

Main category: cs.LG

TL;DR: 提出一种用于多类别隐式神经表示的2D边界提取算法，保证拓扑一致性和水密性，并支持设置最小细节约束


<details>
  <summary>Details</summary>
Motivation: 目前从多类别隐式表示中提取表面的方法无法保证拓扑正确性和无孔洞，需要一种能处理多类别情况且保持拓扑一致性的边界提取算法

Method: 开发了一种2D边界提取算法，专注于多类别情况下的拓扑一致性和水密性，允许设置最小细节约束来控制近似精度

Result: 使用地质建模数据进行评估，展示了算法的适应性和处理复杂拓扑结构的能力

Conclusion: 为多类别隐式表示的表面提取奠定了理论基础，提供了保证拓扑一致性和水密性的边界提取解决方案

Abstract: Surface extraction from implicit neural representations modelling a single class surface is a well-known task. However, there exist no surface extraction methods from an implicit representation of multiple classes that guarantee topological correctness and no holes. In this work, we lay the groundwork by introducing a 2D boundary extraction algorithm for the multi-class case focusing on topological consistency and water-tightness, which also allows for setting minimum detail restraint on the approximation. Finally, we evaluate our algorithm using geological modelling data, showcasing its adaptiveness and ability to honour complex topology.

</details>


### [183] [SEMixer: Semantics Enhanced MLP-Mixer for Multiscale Mixing and Long-term Time Series Forecasting](https://arxiv.org/abs/2602.16220)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: SEMixer是一个轻量级多尺度模型，用于长期时间序列预测，通过随机注意力机制和多尺度渐进混合链解决多尺度模式建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的冗余和噪声，以及非相邻尺度之间的语义鸿沟，使得多尺度时间依赖性的高效对齐和整合具有挑战性。

Method: 提出SEMixer模型，包含两个关键组件：1) 随机注意力机制(RAM)，在训练中捕获多样化的时间片段交互，在推理时通过dropout集成聚合；2) 多尺度渐进混合链(MPMC)，以内存高效的方式堆叠RAM和MLP-Mixer，实现更有效的时间混合。

Result: 在10个公共数据集上验证了有效性，并在基于21GB真实无线网络数据的2025 CCF AlOps挑战中获得第三名。

Conclusion: SEMixer能够有效解决多尺度建模中的语义鸿沟问题，提升长期时间序列预测性能，是一个轻量级且高效的解决方案。

Abstract: Modeling multiscale patterns is crucial for long-term time series forecasting (TSF). However, redundancy and noise in time series, together with semantic gaps between non-adjacent scales, make the efficient alignment and integration of multi-scale temporal dependencies challenging. To address this, we propose SEMixer, a lightweight multiscale model designed for long-term TSF. SEMixer features two key components: a Random Attention Mechanism (RAM) and a Multiscale Progressive Mixing Chain (MPMC). RAM captures diverse time-patch interactions during training and aggregates them via dropout ensemble at inference, enhancing patch-level semantics and enabling MLP-Mixer to better model multi-scale dependencies. MPMC further stacks RAM and MLP-Mixer in a memory-efficient manner, achieving more effective temporal mixing. It addresses semantic gaps across scales and facilitates better multiscale modeling and forecasting performance. We not only validate the effectiveness of SEMixer on 10 public datasets, but also on the \textit{2025 CCF AlOps Challenge} based on 21GB real wireless network data, where SEMixer achieves third place. The code is available at the link https://github.com/Meteor-Stars/SEMixer.

</details>


### [184] [Amortized Predictability-aware Training Framework for Time Series Forecasting and Classification](https://arxiv.org/abs/2602.16224)
*Xu Zhang,Peng Wang,Yichen Li,Wei Wang*

Main category: cs.LG

TL;DR: APTF是一个通用的可预测性感知训练框架，通过分层可预测性感知损失和摊销模型来识别和惩罚低可预测性样本，提高时间序列预测和分类的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常包含噪声和低可预测性模式，这些样本会偏离正常数据分布，导致训练不稳定或收敛到较差的局部最优解。现有深度学习模型很少考虑如何识别和惩罚低可预测性样本来从训练角度提升模型性能。

Method: 提出APTF框架，包含两个关键设计：1) 分层可预测性感知损失(HPL)，动态识别低可预测性样本并随着训练进展逐步扩大其损失惩罚；2) 摊销模型，减轻模型偏差引起的可预测性估计误差，增强HPL效果。

Result: 该框架适用于时间序列预测(TSF)和时间序列分类(TSC)任务，代码已在GitHub开源。

Conclusion: APTF通过关注高可预测性样本同时适当学习低可预测性样本，有效缓解低可预测性样本的负面影响，提升了时间序列分析任务的性能。

Abstract: Time series data are prone to noise in various domains, and training samples may contain low-predictability patterns that deviate from the normal data distribution, leading to training instability or convergence to poor local minima. Therefore, mitigating the adverse effects of low-predictability samples is crucial for time series analysis tasks such as time series forecasting (TSF) and time series classification (TSC). While many deep learning models have achieved promising performance, few consider how to identify and penalize low-predictability samples to improve model performance from the training perspective. To fill this gap, we propose a general Amortized Predictability-aware Training Framework (APTF) for both TSF and TSC. APTF introduces two key designs that enable the model to focus on high-predictability samples while still learning appropriately from low-predictability ones: (i) a Hierarchical Predictability-aware Loss (HPL) that dynamically identifies low-predictability samples and progressively expands their loss penalty as training evolves, and (ii) an amortization model that mitigates predictability estimation errors caused by model bias, further enhancing HPL's effectiveness. The code is available at https://github.com/Meteor-Stars/APTF.

</details>


### [185] [Factored Latent Action World Models](https://arxiv.org/abs/2602.16229)
*Zizhao Wang,Chang Shi,Jiaheng Hu,Kevin Rohling,Roberto Martín-Martín,Amy Zhang,Peter Stone*

Main category: cs.LG

TL;DR: FLAM提出因子化潜在动作模型，将场景分解为独立因子，每个因子推断自己的潜在动作并预测下一步值，在无动作视频中优于单体模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单体逆动力学和正向动力学模型学习单一潜在动作控制整个场景，在复杂多实体同时行动的环境中表现不佳。

Method: 引入因子化潜在动作模型(FLAM)，将场景分解为独立因子，每个因子推断自己的潜在动作并预测自己的下一步因子值。

Result: 在模拟和真实世界多实体数据集上，FLAM在预测准确性和表示质量方面优于先前工作，并促进下游策略学习。

Conclusion: 因子化潜在动作模型能更准确建模复杂多实体动力学，提高无动作视频设置中的视频生成质量。

Abstract: Learning latent actions from action-free video has emerged as a powerful paradigm for scaling up controllable world model learning. Latent actions provide a natural interface for users to iteratively generate and manipulate videos. However, most existing approaches rely on monolithic inverse and forward dynamics models that learn a single latent action to control the entire scene, and therefore struggle in complex environments where multiple entities act simultaneously. This paper introduces Factored Latent Action Model (FLAM), a factored dynamics framework that decomposes the scene into independent factors, each inferring its own latent action and predicting its own next-step factor value. This factorized structure enables more accurate modeling of complex multi-entity dynamics and improves video generation quality in action-free video settings compared to monolithic models. Based on experiments on both simulation and real-world multi-entity datasets, we find that FLAM outperforms prior work in prediction accuracy and representation quality, and facilitates downstream policy learning, demonstrating the benefits of factorized latent action models.

</details>


### [186] [Online Prediction of Stochastic Sequences with High Probability Regret Bounds](https://arxiv.org/abs/2602.16236)
*Matthias Frey,Jonathan H. Manton,Jingge Zhu*

Main category: cs.LG

TL;DR: 本文重新审视了有限时间范围T下的通用预测问题，提出了高概率的遗憾界，弥补了现有文献中仅提供期望遗憾界的不足。


<details>
  <summary>Details</summary>
Motivation: 现有通用预测文献主要关注期望遗憾界，缺乏高概率保证。本文旨在填补这一空白，为随机序列的通用预测提供高概率的遗憾界。

Method: 针对可数字母表上的随机过程通用预测问题，提出了高概率遗憾界，形式与先前的期望界相似，但包含概率参数δ。

Result: 获得了收敛率为O(T^{-1/2}δ^{-1/2})的高概率遗憾界，概率至少为1-δ。同时证明了不可能性结果，表明在不增加额外假设的情况下无法改进δ的指数。

Conclusion: 本文成功为通用预测问题建立了高概率遗憾界，填补了现有文献的空白，并确定了这些界的最优性。

Abstract: We revisit the classical problem of universal prediction of stochastic sequences with a finite time horizon $T$ known to the learner. The question we investigate is whether it is possible to derive vanishing regret bounds that hold with high probability, complementing existing bounds from the literature that hold in expectation. We propose such high-probability bounds which have a very similar form as the prior expectation bounds. For the case of universal prediction of a stochastic process over a countable alphabet, our bound states a convergence rate of $\mathcal{O}(T^{-1/2} δ^{-1/2})$ with probability as least $1-δ$ compared to prior known in-expectation bounds of the order $\mathcal{O}(T^{-1/2})$. We also propose an impossibility result which proves that it is not possible to improve the exponent of $δ$ in a bound of the same form without making additional assumptions.

</details>


### [187] [Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge](https://arxiv.org/abs/2602.16264)
*Zixian Wu,Xuebao Li,Yanfang Zheng,Rui Wang,Shunhuang Zhang,Jinfang Wei,Yongshang Lv,Liang Dong,Zamri Zainal Abidin,Noraisyah Mohamed Shah,Hongwei Ye,Pengchao Yan,Xuefeng Li,Xiaojia Ji,Xusheng Huang,Xiaotian Wang,Honglei Jin*

Main category: cs.LG

TL;DR: 首次开发了基于类别相关奖励（CDR）的监督分类框架，用于预测24小时内≥M级太阳耀斑，结合知识特征和磁图数据，CDR-Transformer模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 开发更准确的太阳耀斑预测方法，通过引入类别相关奖励机制来改进传统深度学习模型，提高对≥M级耀斑的预测能力

Method: 构建多个数据集（知识特征和LOS磁图），应用三种深度学习模型（CNN、CNN-BiLSTM、Transformer）及其CDR版本，进行特征重要性分析、性能比较、奖励工程敏感性分析、SHAP可解释性分析，并与NASA/CCMC对比

Result: 1) R_VALUE和AREA_ACR是最重要的LOS特征；2) Transformer在结合LOS和矢量磁场数据时表现最佳；3) 知识特征模型优于磁图模型；4) CDR-Transformer在所有模型中表现最好；5) CDR模型对奖励选择不敏感；6) SHAP显示CDR和Transformer关注不同特征；7) CDR-Transformer优于NASA/CCMC

Conclusion: CDR-Transformer框架在太阳耀斑预测中表现出色，结合类别相关奖励机制和Transformer架构，显著提升了预测性能，优于现有方法

Abstract: In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results. (2)Transformer achieves better performance with combined LOS and vector magnetic field data than with either alone. (3)Models using knowledge-informed features outperform those using magnetograms. (4)While CNN and CNN-BiLSTM outperform their CDR counterparts on magnetograms, CDR-Transformer is slightly superior to its deep learning counterpart when using knowledge-informed features. Among all models, CDR-Transformer achieves the best performance. (5)The predictive performance of the CDR models is not overly sensitive to the reward choices.(6)Through SHAP analysis, the CDR model tends to regard TOTUSJH as more important, while the Transformer tends to prioritize R_VALUE more.(7)Under identical prediction time and active region (AR) number, the CDR-Transformer shows superior predictive capabilities compared to NASA/CCMC.

</details>


### [188] [Fast KV Compaction via Attention Matching](https://arxiv.org/abs/2602.16284)
*Adam Zweiger,Xinghong Fu,Han Guo,Yoon Kim*

Main category: cs.LG

TL;DR: 提出Attention Matching方法，通过匹配注意力输出来快速构建紧凑的KV缓存，实现50倍压缩且质量损失小


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型的KV缓存大小成为瓶颈，现有基于摘要的压缩方法损失大，而Cartridges方法虽然效果好但训练成本高，需要快速高效的压缩方案

Method: 提出Attention Matching框架，通过构建紧凑的键值来重现注意力输出并保持注意力质量，该方法可分解为简单子问题，部分有闭式解，实现快速压缩

Result: 在压缩时间与质量之间显著推进Pareto前沿，在某些数据集上实现50倍压缩且质量损失小，压缩时间仅需数秒

Conclusion: Attention Matching为长上下文语言模型提供了快速高效的KV缓存压缩方案，平衡了压缩速度与质量

Abstract: Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.

</details>


### [189] [A Graph Meta-Network for Learning on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.16316)
*Guy Bar-Shalom,Ami Tavory,Itay Evron,Maya Bechler-Speicher,Ido Guy,Haggai Maron*

Main category: cs.LG

TL;DR: 提出了首个针对KANs的权重空间架构WS-KAN，通过KAN图表示和对称性处理，在多种任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有权重空间模型主要针对标准神经网络设计，缺乏专门针对Kolmogorov-Arnold Networks（KANs）的分析和架构。KANs具有独特的计算特性，需要专门的权重空间架构来有效学习其参数。

Method: 1) 证明KANs与MLPs具有相同的置换对称性；2) 提出KAN图作为KANs的计算图表示；3) 基于此开发WS-KAN架构，自然考虑KANs的对称性；4) 分析WS-KAN的表达能力，证明其能复制输入KAN的前向传播。

Result: 构建了包含多种任务的KANs训练"动物园"作为基准，在所有任务上WS-KAN都一致优于结构无关的基线方法，且优势通常很显著。

Conclusion: 成功开发了首个专门针对KANs的权重空间架构WS-KAN，通过利用KANs的对称性和计算图表示，显著提升了在KAN参数学习任务上的性能。

Abstract: Weight-space models learn directly from the parameters of neural networks, enabling tasks such as predicting their accuracy on new datasets. Naive methods -- like applying MLPs to flattened parameters -- perform poorly, making the design of better weight-space architectures a central challenge. While prior work leveraged permutation symmetries in standard networks to guide such designs, no analogous analysis or tailored architecture yet exists for Kolmogorov-Arnold Networks (KANs). In this work, we show that KANs share the same permutation symmetries as MLPs, and propose the KAN-graph, a graph representation of their computation. Building on this, we develop WS-KAN, the first weight-space architecture that learns on KANs, which naturally accounts for their symmetry. We analyze WS-KAN's expressive power, showing it can replicate an input KAN's forward pass - a standard approach for assessing expressiveness in weight-space architectures. We construct a comprehensive ``zoo'' of trained KANs spanning diverse tasks, which we use as benchmarks to empirically evaluate WS-KAN. Across all tasks, WS-KAN consistently outperforms structure-agnostic baselines, often by a substantial margin. Our code is available at https://github.com/BarSGuy/KAN-Graph-Metanetwork.

</details>


### [190] [Guide-Guard: Off-Target Predicting in CRISPR Applications](https://arxiv.org/abs/2602.16327)
*Joseph Bingham,Netanel Arussy,Saman Zonouz*

Main category: cs.LG

TL;DR: 提出Guide-Guard机器学习模型，用于预测CRISPR基因编辑中gRNA的脱靶行为，准确率达84%，可同时训练多个基因


<details>
  <summary>Details</summary>
Motivation: 随着CRISPR等基因编辑技术的发展，研究人员需要更好的工具来预测脱靶效应，这是基因编辑安全性的关键问题

Method: 采用数据驱动方法探索生物化学模型，开发名为Guide-Guard的机器学习解决方案

Result: Guide-Guard模型在预测CRISPR基因编辑过程中gRNA行为方面达到84%的准确率，且能同时训练多个基因而不损失准确性

Conclusion: 机器学习方法可以有效预测CRISPR基因编辑的脱靶行为，Guide-Guard模型为解决基因编辑安全性问题提供了有效工具

Abstract: With the introduction of cyber-physical genome sequencing and editing technologies, such as CRISPR, researchers can more easily access tools to investigate and create remedies for a variety of topics in genetics and health science (e.g. agriculture and medicine). As the field advances and grows, new concerns present themselves in the ability to predict the off-target behavior. In this work, we explore the underlying biological and chemical model from a data driven perspective. Additionally, we present a machine learning based solution named \textit{Guide-Guard} to predict the behavior of the system given a gRNA in the CRISPR gene-editing process with 84\% accuracy. This solution is able to be trained on multiple different genes at the same time while retaining accuracy.

</details>


### [191] [HAWX: A Hardware-Aware FrameWork for Fast and Scalable ApproXimation of DNNs](https://arxiv.org/abs/2602.16336)
*Samira Nazari,Mohammad Saeed Almasi,Mahdi Taheri,Ali Azarpeyvand,Ali Mokhtari,Ali Mahani,Christian Herglotz*

Main category: cs.LG

TL;DR: HAWX是一个硬件感知的可扩展探索框架，通过多级灵敏度评分指导异构近似计算块的集成，使用预测模型加速配置评估，在保持精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络中近似计算块集成时面临的巨大设计空间探索问题，传统方法评估成本过高，需要高效的硬件感知搜索框架。

Method: 采用多级灵敏度评分（算子、滤波器、层、模型级）指导异构近似计算块的选择性集成，结合精度、功耗和面积的预测模型，支持空间和时间加速器架构。

Result: 在LeNet-5上实现层级搜索23倍加速和滤波器级搜索超过3×10^6倍加速，在VGG-11、ResNet-18、EfficientNetLite等基准测试中，效率随网络规模呈指数级提升。

Conclusion: HAWX框架能够高效探索近似计算设计空间，在保持精度的同时显著加速评估过程，其效率优势随网络规模扩大而指数增长，支持多种加速器架构。

Abstract: This work presents HAWX, a hardware-aware scalable exploration framework that employs multi-level sensitivity scoring at different DNN abstraction levels (operator, filter, layer, and model) to guide selective integration of heterogeneous AxC blocks. Supported by predictive models for accuracy, power, and area, HAWX accelerates the evaluation of candidate configurations, achieving over 23* speedup in a layer-level search with two candidate approximate blocks and more than (3*106)* speedup at the filter-level search only for LeNet-5, while maintaining accuracy comparable to exhaustive search. Experiments across state-of-the-art DNN benchmarks such as VGG-11, ResNet-18, and EfficientNetLite demonstrate that the efficiency benefits of HAWX scale exponentially with network size. The HAWX hardware-aware search algorithm supports both spatial and temporal accelerator architectures, leveraging either off-the-shelf approximate components or customized designs.

</details>


### [192] [Explainability for Fault Detection System in Chemical Processes](https://arxiv.org/abs/2602.16341)
*Georgios Gravanis,Dimitrios Kyriakou,Spyros Voutetakis,Simira Papadopoulou,Konstantinos Diamantaras*

Main category: cs.LG

TL;DR: 本文应用并比较了两种先进的XAI方法（IG和SHAP）来解释LSTM分类器在田纳西伊士曼过程故障诊断中的决策，发现XAI方法能帮助识别故障发生的子系统，SHAP在某些情况下能更接近故障根本原因。


<details>
  <summary>Details</summary>
Motivation: 虽然LSTM分类器在田纳西伊士曼过程故障诊断中表现出高精度，但需要可解释性方法来理解其决策过程，识别故障发生的具体子系统，从而提高诊断系统的透明度和可信度。

Method: 应用并比较两种模型无关的XAI方法：集成梯度（IG）和SHAP（SHapley Additive exPlanations），解释高精度LSTM分类器的故障诊断决策。LSTM分类器用于检测基准非线性化学过程（田纳西伊士曼过程）中的故障。

Result: XAI方法能有效识别故障发生的子系统；在大多数情况下，两种方法指示的最重要特征相同；在某些情况下，SHAP方法提供的信息更丰富，更接近故障的根本原因。

Conclusion: XAI方法能为故障诊断提供有价值的解释，帮助识别故障子系统；由于所用XAI方法是模型无关的，该方法不仅限于特定过程，也可用于类似问题。

Abstract: In this work, we apply and compare two state-of-the-art eXplainability Artificial Intelligence (XAI) methods, the Integrated Gradients (IG) and the SHapley Additive exPlanations (SHAP), that explain the fault diagnosis decisions of a highly accurate Long Short-Time Memory (LSTM) classifier. The classifier is trained to detect faults in a benchmark non-linear chemical process, the Tennessee Eastman Process (TEP). It is highlighted how XAI methods can help identify the subsystem of the process where the fault occurred. Using our knowledge of the process, we note that in most cases the same features are indicated as the most important for the decision, while insome cases the SHAP method seems to be more informative and closer to the root cause of the fault. Finally, since the used XAI methods are model-agnostic, the proposed approach is not limited to the specific process and can also be used in similar problems.

</details>


### [193] [Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks](https://arxiv.org/abs/2602.16357)
*Sarkis Ter Martirosyan,Xinyue Huang,David Qin,Anthony Yu,Stanislav Emelianov*

Main category: cs.LG

TL;DR: SPOI-AE是一种用于光谱光声成像的自动编码器，能够解决光学反演和光谱解混问题，无需假设线性关系，在活体小鼠淋巴结成像中表现优于传统算法。


<details>
  <summary>Details</summary>
Motivation: 光谱光声成像中，准确估计色团相对浓度对于揭示生理过程的结构、功能和分子信息至关重要，但由于非线性性和病态性，浓度估计一直难以解决。

Method: 开发了光谱光声光学反演自动编码器（SPOI-AE），在未知真实色团浓度的活体小鼠淋巴结光谱光声图像上进行训练和测试，无需假设线性关系。

Result: SPOI-AE比传统算法更好地重建输入的光声像素，同时提供生物学上一致的光学参数、色团浓度和组织氧饱和度估计；通过模拟小鼠淋巴结幻影验证了解混准确性。

Conclusion: SPOI-AE能够有效解决光谱光声成像中的光学反演和光谱解混问题，为色团浓度估计提供了更准确的方法。

Abstract: Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.

</details>


### [194] [Improved Bounds for Reward-Agnostic and Reward-Free Exploration](https://arxiv.org/abs/2602.16363)
*Oran Ridel,Alon Cohen*

Main category: cs.LG

TL;DR: 本文研究了无奖励和奖励不可知探索，提出了新算法显著放宽了对精度参数ε的限制，并建立了无奖励探索的紧下界


<details>
  <summary>Details</summary>
Motivation: 研究在未知环境中进行探索的问题，其中智能体无法观察到外部奖励。无奖励探索旨在为探索后揭示的任何奖励提供ε最优策略，而奖励不可知探索则针对从小有限类中抽取的奖励实现ε最优性。现有方法在奖励不可知设置中虽然达到了极小极大样本复杂度，但仅适用于限制性小的精度参数ε。

Method: 提出了一种新算法，采用精心设计奖励的在线学习过程来构建探索策略。该策略用于收集足够的数据以进行准确的动态估计，并在奖励揭示后计算ε最优策略。算法技术新颖，本身具有技术意义。

Result: 新算法显著放宽了对精度参数ε的要求，相比现有方法有重要改进。同时建立了无奖励探索的紧下界，填补了已知上下界之间的空白。

Conclusion: 本文在无奖励和奖励不可知探索方面取得了重要进展，提出了更通用的算法并建立了理论上的紧下界，为相关领域研究提供了新的技术方法和理论保证。

Abstract: We study reward-free and reward-agnostic exploration in episodic finite-horizon Markov decision processes (MDPs), where an agent explores an unknown environment without observing external rewards. Reward-free exploration aims to enable $ε$-optimal policies for any reward revealed after exploration, while reward-agnostic exploration targets $ε$-optimality for rewards drawn from a small finite class. In the reward-agnostic setting, Li, Yan, Chen, and Fan achieve minimax sample complexity, but only for restrictively small accuracy parameter $ε$. We propose a new algorithm that significantly relaxes the requirement on $ε$. Our approach is novel and of technical interest by itself. Our algorithm employs an online learning procedure with carefully designed rewards to construct an exploration policy, which is used to gather data sufficient for accurate dynamics estimation and subsequent computation of an $ε$-optimal policy once the reward is revealed. Finally, we establish a tight lower bound for reward-free exploration, closing the gap between known upper and lower bounds.

</details>


### [195] [Easy Data Unlearning Bench](https://arxiv.org/abs/2602.16400)
*Roy Rinberg,Pol Puigdemont,Martin Pawelczyk,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出一个统一的机器遗忘评估基准套件，使用KLoM指标简化算法评估，提供预计算模型集成和基础设施


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘方法的评估存在技术挑战，现有基准需要复杂设置和大量工程开销，需要更简单、标准化的评估框架

Method: 开发统一的、可扩展的基准套件，使用KLoM（边缘KL散度）指标，提供预计算模型集成、oracle输出和流线化基础设施

Result: 创建了一个标准化的评估框架，能够实现可重复、可扩展和公平的机器遗忘方法比较，代码和数据已公开可用

Conclusion: 该基准套件为机器遗忘研究提供了实用基础，旨在加速研究进展并促进最佳实践

Abstract: Evaluating machine unlearning methods remains technically challenging, with recent benchmarks requiring complex setups and significant engineering overhead. We introduce a unified and extensible benchmarking suite that simplifies the evaluation of unlearning algorithms using the KLoM (KL divergence of Margins) metric. Our framework provides precomputed model ensembles, oracle outputs, and streamlined infrastructure for running evaluations out of the box. By standardizing setup and metrics, it enables reproducible, scalable, and fair comparison across unlearning methods. We aim for this benchmark to serve as a practical foundation for accelerating research and promoting best practices in machine unlearning. Our code and data are publicly available.

</details>


### [196] [Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment](https://arxiv.org/abs/2602.16438)
*Eva Paraschou,Line Harder Clemmensen,Sneha Das*

Main category: cs.LG

TL;DR: 研究发现针对单一属性的LLM公平性对齐会导致偏见溢出，即改善某一属性公平性时可能在其他属性上加剧偏见，特别是在模糊语境下


<details>
  <summary>Details</summary>
Motivation: 传统LLM公平性对齐主要关注单一敏感属性，忽略了公平性的多维性和情境特定性，可能导致偏见溢出问题，这在LLM对齐中尚未充分研究

Method: 使用直接偏好优化和BBQ基准，在三个先进LLM（Mistral 7B、Llama 3.1 8B、Qwen 2.5 7B）上评估针对性别对齐对九个敏感属性公平性的影响，分析模糊和明确语境下的表现

Result: 发现明显的偏见溢出：虽然总体结果有所改善，但情境感知分析显示在模糊语境下公平性显著下降，特别是在外貌（所有模型p<0.001）、性取向和残疾状况方面

Conclusion: 改善单一属性公平性可能在不确定性下无意中加剧其他属性的不平等，强调需要情境感知、多属性公平性评估框架

Abstract: Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.

</details>


### [197] [Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA](https://arxiv.org/abs/2602.16442)
*Kamil Jeziorek,Piotr Wzorek,Krzysztof Blachut,Hiroshi Nakano,Manon Dampfhoffer,Thomas Mesquida,Hiroaki Nishi,Thomas Dalgaty,Tomasz Kryjak*

Main category: cs.LG

TL;DR: FPGA实现的事件图神经网络用于音频处理，通过人工耳蜗将时域信号转换为稀疏事件数据，在分类任务上达到SOTA性能，同时大幅减少参数和资源消耗，实现低延迟、低功耗的关键词检测。


<details>
  <summary>Details</summary>
Motivation: 随着边缘传感器数据量增加，特别是神经形态设备产生的离散事件流，需要硬件感知的神经网络架构来实现高效、低延迟、节能的本地处理。

Method: 采用FPGA实现事件图神经网络，利用人工耳蜗将时域信号转换为稀疏事件数据，结合图卷积层和循环序列建模，在SoC FPGA上实现端到端的关键词检测系统。

Result: 在SHD数据集上达到92.7%准确率（仅比SOTA低2.4%），参数减少10-67倍；在SSC数据集上达到66.9-71.0%准确率；量化模型达到92.3%准确率，优于FPGA SNN 19.3%；关键词检测系统达到95%词尾检测准确率，仅10.53微秒延迟和1.18W功耗。

Conclusion: 该工作展示了事件图神经网络在FPGA上的高效实现，为节能的事件驱动关键词检测建立了强基准，证明了硬件感知架构在边缘音频处理中的优势。

Abstract: As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.

</details>


### [198] [Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC](https://arxiv.org/abs/2602.16456)
*Abdulla Jasem Almansoori,Maria Ivanova,Andrey Veprikov,Aleksandr Beznosikov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: 提出LoRSum方法，通过近端子问题和交替最小二乘更新，在内存高效的情况下实现与全步长SVD投影相当的LoRA优化性能


<details>
  <summary>Details</summary>
Motivation: 解决LoRA微调与使用低秩投影的全步长训练（SVDLoRA）之间的性能差距，同时保持LoRA的参数效率和内存优势

Method: 将LoRA优化转化为近端子问题，使用交替最小二乘更新求解，证明其为隐式块幂方法；提出缩放变体以支持K-FAC和Shampoo等结构化度量

Result: 在合成任务、CIFAR-100以及GLUE、SQuAD v2、WikiText-103等语言模型微调任务上，LoRSum能够匹配或超越LoRA基线，同时避免全矩阵SVD投影并保持参数效率

Conclusion: LoRSum填补了LoRA微调与全步长低秩投影训练之间的差距，提供了一种内存高效且计算开销适中的优化方法，可应用于各种结构化预条件器

Abstract: Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.

</details>


### [199] [HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.16468)
*Jung Min Choi,Vijaya Krishna Yalavarthi,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: HPMixer：一种用于长期多元时间序列预测的分层补丁混合器，通过解耦建模周期性和残差动态，在标准基准测试中取得竞争性或最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在长期多元时间序列预测中，有效捕捉周期模式和残差动态至关重要。现有方法需要更好地在标准深度学习基准设置中处理这两个方面。

Method: 提出分层补丁混合器（HPMixer），采用解耦但互补的方式建模周期性和残差。周期性组件使用可学习循环模块和非线性通道MLP；残差组件通过可学习平稳小波变换提取稳定频域表示，然后使用通道混合编码器和两级非重叠分层补丁机制。

Result: 在标准多元基准测试上的大量实验表明，HPMixer相比近期基线方法取得了竞争性或最先进的性能。

Conclusion: 通过将解耦的周期性建模与结构化、多尺度残差学习相结合，HPMixer为长期多元时间序列预测提供了一个有效的框架。

Abstract: In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.

</details>


### [200] [Synthesis and Verification of Transformer Programs](https://arxiv.org/abs/2602.16473)
*Hongjian Jiang,Matthew Hague,Philipp Rümmer,Anthony Widjaja Lin*

Main category: cs.LG

TL;DR: 本文提出了C-RASP程序的自动验证和学习算法，利用Lustre同步数据流程序验证技术实现高效模型检查，并通过局部搜索算法从示例中学习C-RASP程序。


<details>
  <summary>Details</summary>
Motivation: C-RASP是一种能表达transformer概念的编程语言，但缺乏自动验证和学习方法。本文旨在解决这两个问题：1) 如何自动验证C-RASP程序的正确性；2) 如何从示例中学习C-RASP程序。

Method: 1) 将C-RASP验证问题连接到Lustre同步数据流程序验证，利用先进的模型检查器和优化的SMT求解器；2) 提出基于局部搜索的算法从示例中学习C-RASP程序。

Result: 实现了C-RASP验证和学习系统，在文献中的C-RASP基准测试中展示了有效性，特别是在transformer程序优化和基于部分规范的约束学习两个应用场景中。

Conclusion: 本文为C-RASP程序提供了完整的验证和学习框架，通过连接Lustre验证技术和局部搜索算法，实现了高效的自动验证和程序学习，为transformer程序的分析和合成提供了实用工具。

Abstract: C-RASP is a simple programming language that was recently shown to capture concepts expressible by transformers. In this paper, we develop new algorithmic techniques for automatically verifying C-RASPs. To this end, we establish a connection to the verification of synchronous dataflow programs in Lustre, which enables us to exploit state-of-the-art model checkers utilizing highly optimized SMT-solvers. Our second contribution addresses learning a C-RASP program in the first place. To this end, we provide a new algorithm for learning a C-RASP from examples using local search. We demonstrate efficacy of our implementation for benchmarks of C-RASPs in the literature, in particular in connection to the following applications: (1) transformer program optimization, and (2) constrained learning of transformer programs (based on a partial specification).

</details>


### [201] [Fast and Scalable Analytical Diffusion](https://arxiv.org/abs/2602.16498)
*Xinyi Shang,Peng Sun,Jingyu Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: GoldDiff提出了一种训练免费的动态时间感知黄金子集扩散方法，通过渐进后验集中现象，将推理复杂度与数据集大小解耦，实现71倍加速并首次将分析扩散模型扩展到ImageNet-1K。


<details>
  <summary>Details</summary>
Motivation: 分析扩散模型虽然具有数学透明性，但标准实现需要在每个时间步扫描整个数据集，计算成本随数据集大小线性增长，这限制了其在大规模数据集上的应用。

Method: 提出GoldDiff框架，利用后验渐进集中现象：随着信噪比增加，去噪分数的有效支撑集从全局流形收缩到局部邻域。采用粗到精机制动态定位"黄金子集"进行推理，而非静态检索。

Result: 在AFHQ上实现71倍加速，性能匹配甚至优于全扫描基线。首次成功将分析扩散模型扩展到ImageNet-1K，为大规模生成建模提供了可扩展的训练免费范式。

Conclusion: GoldDiff通过动态黄金子集选择解决了分析扩散模型的可扩展性瓶颈，在保持理论保证的同时显著加速推理，为大规模生成建模开辟了新途径。

Abstract: Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\bf 71 \times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.

</details>


### [202] [Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects](https://arxiv.org/abs/2602.16503)
*Vasilis Gkolemis,Loukas Kavouras,Dimitrios Kyriakopoulos,Konstantinos Tsopelas,Dimitrios Rontogiannis,Giuseppe Casalicchio,Theodore Dalamagas,Christos Diou*

Main category: cs.LG

TL;DR: CALMs是一种新的模型类别，在保持GAMs可解释性的同时，通过允许每个特征在不同输入区域有多个单变量形状函数来捕捉交互作用，达到接近GA²Ms的准确性。


<details>
  <summary>Details</summary>
Motivation: GAMs虽然可解释但无法处理特征交互而欠拟合，GA²Ms添加成对交互提高了准确性但牺牲了可解释性和模型可审计性。需要一种能平衡准确性和可解释性的新方法。

Method: 提出条件可加局部模型(CALMs)，允许每个特征在输入空间的不同区域有多个单变量形状函数，这些区域由特征交互的简单逻辑条件（阈值）定义。采用基于蒸馏的训练流程，识别交互有限的同质区域，并通过区域感知反向拟合拟合可解释的形状函数。

Result: 在多种分类和回归任务上的实验表明，CALMs始终优于GAMs，并且达到与GA²Ms相当的准确性。

Conclusion: CALMs在预测准确性和可解释性之间提供了有吸引力的权衡，平衡了GAMs的可解释性和GA²Ms的准确性。

Abstract: Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms. Overall, CALMs offer a compelling trade-off between predictive accuracy and interpretability.

</details>


### [203] [Small molecule retrieval from tandem mass spectrometry: what are we optimizing for?](https://arxiv.org/abs/2602.16507)
*Gaetan De Waele,Marek Wydmuch,Krzysztof Dembczyński,Wojciech Kotłowski,Willem Waegeman*

Main category: cs.LG

TL;DR: 该研究分析了LC-MS/MS谱图识别中深度学习模型的不同损失函数，揭示了指纹相似性与分子检索之间的根本权衡：优化指纹预测准确性会损害检索结果，反之亦然。


<details>
  <summary>Details</summary>
Motivation: 在LC-MS/MS数据分析中，深度学习模型通过预测分子指纹向量来识别化合物，但不同损失函数对模型性能的影响尚不清楚，需要系统研究。

Method: 研究分析了常用的损失函数，推导了新的遗憾界限来表征这些目标的贝叶斯最优决策何时必须分歧，并理论分析了候选集相似性结构的影响。

Result: 研究揭示了指纹相似性与分子检索之间的根本权衡：优化更准确的指纹预测通常会恶化检索结果，反之亦然。这种权衡取决于候选集的相似性结构。

Conclusion: 该研究为损失函数和指纹选择提供了理论指导，表明需要根据具体应用目标（指纹相似性vs分子检索）来选择合适的损失函数。

Abstract: One of the central challenges in the computational analysis of liquid chromatography-tandem mass spectrometry (LC-MS/MS) data is to identify the compounds underlying the output spectra. In recent years, this problem is increasingly tackled using deep learning methods. A common strategy involves predicting a molecular fingerprint vector from an input mass spectrum, which is then used to search for matches in a chemical compound database. While various loss functions are employed in training these predictive models, their impact on model performance remains poorly understood. In this study, we investigate commonly used loss functions, deriving novel regret bounds that characterize when Bayes-optimal decisions for these objectives must diverge. Our results reveal a fundamental trade-off between the two objectives of (1) fingerprint similarity and (2) molecular retrieval. Optimizing for more accurate fingerprint predictions typically worsens retrieval results, and vice versa. Our theoretical analysis shows this trade-off depends on the similarity structure of candidate sets, providing guidance for loss function and fingerprint selection.

</details>


### [204] [Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study](https://arxiv.org/abs/2602.16523)
*Gerhard Stenzel,Isabella Debelic,Michael Kölle,Tobias Rohe,Leo Sünkel,Julian Hager,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 扩展DQCS强化学习到含连续旋转的参数化量子态制备，比较单阶段和两阶段训练方法，PPO在稳定超参数下成功，但可扩展性在λ=3-4时饱和


<details>
  <summary>Details</summary>
Motivation: 将定向量子电路合成的强化学习方法从纯离散门选择扩展到包含连续单量子比特旋转的参数化量子态制备，以解决更复杂的量子态制备问题

Method: 使用Gymnasium和PennyLane，比较两种训练机制：单阶段代理联合选择门类型、作用量子比特和旋转角度；两阶段方法先提出离散电路，再用Adam优化旋转角度。评估PPO和A2C在2-10量子比特系统上的表现

Result: A2C未能学习有效策略，PPO在稳定超参数下成功。两种方法都能可靠重构计算基态（83-99%成功率）和贝尔态（61-77%成功率），但可扩展性在λ≈3-4时饱和，无法扩展到10量子比特目标。两阶段方法仅提供边际精度提升但需要约三倍运行时间

Conclusion: 在固定计算预算下，推荐使用单阶段PPO策略，提供了明确的合成电路，并与经典变分基线对比，指出了改进可扩展性的途径

Abstract: We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \(R_x\), \(R_y\), and \(R_z\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \(λ\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \(5\times10^{-4}\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \(10^{-4}\)). Both approaches reliably reconstruct computational basis states (between 83\% and 99\% success) and Bell states (between 61\% and 77\% success). However, scalability saturates for \(λ\) of approximately three to four and does not extend to ten-qubit targets even at \(λ=2\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.

</details>


### [205] [Capacity-constrained demand response in smart grids using deep reinforcement learning](https://arxiv.org/abs/2602.16525)
*Shafagh Abband Pashaki,Sepehr Maleki,Amir Badiee*

Main category: cs.LG

TL;DR: 提出基于容量约束的激励型需求响应方法，使用深度强化学习优化实时激励费率，有效降低住宅智能电网的峰值需求


<details>
  <summary>Details</summary>
Motivation: 住宅智能电网中需要维持电网容量限制并防止拥堵，通过经济激励让终端用户减少或转移能源消耗

Method: 采用分层架构，服务提供商根据批发电价和聚合住宅负荷调整小时激励费率；使用深度强化学习在显式容量约束下学习最优实时激励费率；通过设备级家庭能源管理系统和不满成本建模异构用户偏好

Result: 使用三个家庭的实际用电和价格数据进行仿真，结果显示该方法有效降低峰值需求并平滑聚合负荷曲线，相比无需求响应情况，峰均比降低约22.82%

Conclusion: 提出的容量约束激励型需求响应方法能够有效管理住宅智能电网负荷，平衡服务提供商和终端用户的经济利益，实现电网稳定运行

Abstract: This paper presents a capacity-constrained incentive-based demand response approach for residential smart grids. It aims to maintain electricity grid capacity limits and prevent congestion by financially incentivising end users to reduce or shift their energy consumption. The proposed framework adopts a hierarchical architecture in which a service provider adjusts hourly incentive rates based on wholesale electricity prices and aggregated residential load. The financial interests of both the service provider and end users are explicitly considered. A deep reinforcement learning approach is employed to learn optimal real-time incentive rates under explicit capacity constraints. Heterogeneous user preferences are modelled through appliance-level home energy management systems and dissatisfaction costs. Using real-world residential electricity consumption and price data from three households, simulation results show that the proposed approach effectively reduces peak demand and smooths the aggregated load profile. This leads to an approximately 22.82% reduction in the peak-to-average ratio compared to the no-demand-response case.

</details>


### [206] [FEKAN: Feature-Enriched Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.16530)
*Sidharth S. Menon,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: FEKAN是一种改进的Kolmogorov-Arnold网络，通过特征增强在保持KAN所有优点的同时，显著提高了计算效率和预测精度，不增加可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有KAN架构（如样条、小波、径向基函数等变体）存在计算成本高、收敛速度慢的问题，限制了其可扩展性和实际应用。需要一种既能保持KAN可解释性优势，又能提高效率和精度的新架构。

Method: 提出特征增强的Kolmogorov-Arnold网络（FEKAN），通过特征增强技术在不增加可训练参数的情况下提高表示能力。该方法保留了KAN的所有优点，同时加速收敛并减少计算开销。

Result: 在函数逼近任务、物理信息公式（各种偏微分方程）和神经算子设置等综合基准测试中，FEKAN相比多种KAN变体（FastKAN、WavKAN、ReLUKAN等）表现出显著更快的收敛速度和更高的逼近精度。

Conclusion: FEKAN在保持KAN可解释性的同时，通过特征增强显著提高了计算效率和预测精度，为KAN架构的实际应用提供了更可行的解决方案，并建立了理论基础证明其优于传统KAN的表示能力。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a compelling alternative to multilayer perceptrons, offering enhanced interpretability via functional decomposition. However, existing KAN architectures, including spline-, wavelet-, radial-basis variants, etc., suffer from high computational cost and slow convergence, limiting scalability and practical applicability. Here, we introduce Feature-Enriched Kolmogorov-Arnold Networks (FEKAN), a simple yet effective extension that preserves all the advantages of KAN while improving computational efficiency and predictive accuracy through feature enrichment, without increasing the number of trainable parameters. By incorporating these additional features, FEKAN accelerates convergence, increases representation capacity, and substantially mitigates the computational overhead characteristic of state-of-the-art KAN architectures. We investigate FEKAN across a comprehensive set of benchmarks, including function-approximation tasks, physics-informed formulations for diverse partial differential equations (PDEs), and neural operator settings that map between input and output function spaces. For function approximation, we systematically compare FEKAN against a broad family of KAN variants, FastKAN, WavKAN, ReLUKAN, HRKAN, ChebyshevKAN, RBFKAN, and the original SplineKAN. Across all tasks, FEKAN demonstrates substantially faster convergence and consistently higher approximation accuracy than the underlying baseline architectures. We also establish the theoretical foundations for FEKAN, showing its superior representation capacity compared to KAN, which contributes to improved accuracy and efficiency.

</details>


### [207] [Transfer Learning of Linear Regression with Multiple Pretrained Models: Benefiting from More Pretrained Models via Overparameterization Debiasing](https://arxiv.org/abs/2602.16531)
*Daniel Boharon,Yehuda Dar*

Main category: cs.LG

TL;DR: 研究使用多个可能过参数化的最小二乘预训练模型进行线性回归任务的迁移学习，分析测试误差并提出去偏方法


<details>
  <summary>Details</summary>
Motivation: 研究在过参数化预训练模型情况下，如何使用多个预训练模型进行迁移学习，并解决过参数化偏置问题

Method: 将目标学习任务公式化为最小化目标数据集平方误差加上与预训练模型距离的惩罚项，提出乘性校正因子来减少过参数化偏置

Result: 阐明了何时使用更多预训练模型可以改善迁移学习：当过参数化时，使用足够多的预训练模型很重要，但可能受到过参数化偏置的影响

Conclusion: 提出的简单去偏方法可以减少过参数化偏置，使迁移学习能够利用更多预训练模型学习目标预测器

Abstract: We study transfer learning for a linear regression task using several least-squares pretrained models that can be overparameterized.
  We formulate the target learning task as optimization that minimizes squared errors on the target dataset with penalty on the distance of the learned model from the pretrained models. We analytically formulate the test error of the learned target model and provide the corresponding empirical evaluations.
  Our results elucidate when using more pretrained models can improve transfer learning. Specifically, if the pretrained models are overparameterized, using sufficiently many of them is important for beneficial transfer learning. However, the learning may be compromised by overparameterization bias of pretrained models, i.e., the minimum $\ell_2$-norm solution's restriction to a small subspace spanned by the training examples in the high-dimensional parameter space. We propose a simple debiasing via multiplicative correction factor that can reduce the overparameterization bias and leverage more pretrained models to learn a target predictor.

</details>


### [208] [Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning](https://arxiv.org/abs/2602.16543)
*Jialiang Fan,Shixiong Jiang,Mengyu Liu,Fanxin Kong*

Main category: cs.LG

TL;DR: 提出一个针对安全强化学习的对抗攻击框架，利用专家演示和黑盒环境交互学习约束模型和代理策略，无需受害者策略的内部梯度或真实安全约束信息。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法假设环境友好，易受现实世界中常见对抗扰动的影响；现有基于梯度的对抗攻击通常需要访问策略的梯度信息，这在现实场景中不切实际。

Method: 使用专家演示和黑盒环境交互学习约束模型和代理（学习者）策略，实现基于梯度的攻击优化，无需受害者策略的内部梯度或真实安全约束。

Result: 在多个安全强化学习基准测试中验证了方法的有效性，在有限特权访问条件下成功攻击安全RL策略。

Conclusion: 提出的对抗攻击框架能够有效揭示安全强化学习策略的脆弱性，为实际应用中的安全评估提供了实用工具。

Abstract: Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy's internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.

</details>


### [209] [RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion](https://arxiv.org/abs/2602.16548)
*Tianmeng Hu,Yongzheng Cui,Biao Luo,Ke Li*

Main category: cs.LG

TL;DR: RIDER：基于强化学习的RNA三维结构逆设计框架，直接优化结构相似性而非序列恢复率


<details>
  <summary>Details</summary>
Motivation: 现有RNA三维结构逆设计方法主要优化和评估原生序列恢复率，但这作为结构保真度的代理指标存在局限，因为不同序列可以折叠成相似结构，高恢复率不一定表示正确折叠

Method: 1. 预训练基于GNN的条件扩散生成模型，以目标3D结构为条件；2. 使用改进的策略梯度算法进行微调，采用基于3D自一致性指标的四个任务特定奖励函数

Result: 预训练模型在原生序列恢复率上比SOTA方法提升9%；RIDER在所有3D结构相似性指标上提升超过100%，并能发现与原生序列不同的设计

Conclusion: RIDER通过直接优化3D结构相似性而非序列恢复率，显著提高了RNA逆设计的结构保真度，为合成生物学和治疗应用中的功能RNA工程提供了更有效的工具

Abstract: The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.

</details>


### [210] [AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS](https://arxiv.org/abs/2602.16579)
*Maria Luisa Taccari,Kenza Tazi,Oisín M. Morrison,Andreas Grafberger,Juan Colonese,Corentin Carton de Wiart,Christel Prudhomme,Cinzia Mazzetti,Matthew Chantry,Florian Pappenberger*

Main category: cs.LG

TL;DR: AIFL是全球首个基于CARAVAN生态系统的端到端LSTM日径流预报模型，采用两阶段训练策略解决再分析到预报的领域偏移问题，在极端事件检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型从历史再分析数据过渡到业务预报产品时存在性能差距，需要开发可靠的全球径流预报系统用于洪水预警和水资源管理。

Method: 基于LSTM的确定性模型，采用两阶段训练：先在ERA5-Land再分析数据(1980-2019)上预训练，然后在IFS业务预报数据(2016-2019)上微调，使用CARAVAN数据集的18,588个流域。

Result: 在独立测试集(2021-2024)上，AIFL获得中位数KGE'为0.66，中位数NSE为0.53，与当前最先进的全球系统竞争力相当，在极端事件检测中表现特别可靠。

Conclusion: AIFL为全球水文社区提供了一个透明、可复现的径流预报基准模型，通过两阶段训练策略有效解决了再分析到预报的领域偏移问题，具有业务应用价值。

Abstract: Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.

</details>


### [211] [Illustration of Barren Plateaus in Quantum Computing](https://arxiv.org/abs/2602.16558)
*Gerhard Stenzel,Tobias Rohe,Michael Kölle,Leo Sünkel,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 参数共享在变分量子电路中虽然能减少参数维度并可能缓解贫瘠高原现象，但会通过欺骗性梯度改变优化景观，增加优化难度，导致传统梯度优化器性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究参数共享在变分量子电路中的复杂权衡，特别是它如何通过欺骗性梯度改变优化景观，这一问题在现有研究中被忽视。

Method: 通过系统实验分析，研究不同参数共享程度对优化景观的影响，引入梯度欺骗性检测算法和量子电路优化难度量化框架。

Result: 增加参数共享会产生更复杂的解景观，具有更高的梯度幅度和可测量的欺骗性比率，传统梯度优化器（Adam、SGD）的收敛性能随参数共享增加而逐渐恶化。

Conclusion: 参数共享虽然能提高电路表达能力，但代价是显著增加景观欺骗性，揭示了经典优化策略与量子参数景观之间的根本性不匹配，为实际应用中的量子电路设计提供了重要考虑因素。

Abstract: Variational Quantum Circuits (VQCs) have emerged as a promising paradigm for quantum machine learning in the NISQ era. While parameter sharing in VQCs can reduce the parameter space dimensionality and potentially mitigate the barren plateau phenomenon, it introduces a complex trade-off that has been largely overlooked. This paper investigates how parameter sharing, despite creating better global optima with fewer parameters, fundamentally alters the optimization landscape through deceptive gradients -- regions where gradient information exists but systematically misleads optimizers away from global optima. Through systematic experimental analysis, we demonstrate that increasing degrees of parameter sharing generate more complex solution landscapes with heightened gradient magnitudes and measurably higher deceptiveness ratios. Our findings reveal that traditional gradient-based optimizers (Adam, SGD) show progressively degraded convergence as parameter sharing increases, with performance heavily dependent on hyperparameter selection. We introduce a novel gradient deceptiveness detection algorithm and a quantitative framework for measuring optimization difficulty in quantum circuits, establishing that while parameter sharing can improve circuit expressivity by orders of magnitude, this comes at the cost of significantly increased landscape deceptiveness. These insights provide important considerations for quantum circuit design in practical applications, highlighting the fundamental mismatch between classical optimization strategies and quantum parameter landscapes shaped by parameter sharing.

</details>


### [212] [A Scalable Approach to Solving Simulation-Based Network Security Games](https://arxiv.org/abs/2602.16564)
*Michael Lanier,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: MetaDOAR：一种轻量级元控制器，通过分区感知过滤层和Q值缓存增强Double Oracle/PSRO范式，实现大规模网络环境下的可扩展多智能体强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决在大型网络环境中进行多智能体强化学习时的可扩展性问题，传统方法在内存使用和训练时间上存在显著扩展瓶颈。

Method: 1. 学习紧凑的状态投影，从节点结构嵌入中快速评分并选择设备子集（top-k分区）；2. 低层执行器在选定分区上进行聚焦波束搜索，利用评论家智能体；3. 候选动作通过批量评论家前向传播评估，存储在LRU缓存中；4. 使用量化状态投影和局部动作标识符作为缓存键，通过保守的k跳缓存失效保持决策质量。

Result: 在大型网络拓扑上获得比最先进基线更高的玩家收益，在内存使用和训练时间方面没有显著的扩展问题。

Conclusion: MetaDOAR为大规模网络决策问题提供了一条实用且理论上有动机的高效分层策略学习路径。

Abstract: We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.

</details>


### [213] [Steering diffusion models with quadratic rewards: a fine-grained analysis](https://arxiv.org/abs/2602.16570)
*Ankur Moitra,Andrej Risteski,Dhruv Rohatgi*

Main category: cs.LG

TL;DR: 本文研究了从奖励倾斜扩散模型中采样的计算复杂性，特别关注二次奖励函数。证明了线性奖励总是可高效采样，低秩正定二次奖励也可高效采样，但负定二次奖励即使在秩1情况下也是难解的。


<details>
  <summary>Details</summary>
Motivation: 当前推理时间算法在实践中使用的都是启发式方法，存在各种失败模式，且对这些启发式方法何时能高效改进的理解很少。本文旨在对奖励倾斜扩散模型的采样任务提供精细的计算复杂性分析。

Method: 研究从奖励倾斜扩散模型 $p^{\star}(x) \propto p(x) \exp(r(x))$ 中采样的计算复杂性，其中 $r$ 是奖励函数，$p$ 是预训练扩散模型。特别关注二次奖励函数 $r(x) = x^\top A x + b^\top x$。使用线性奖励倾斜作为构建模块，并引入新的概念工具——Hubbard-Stratonovich变换。

Result: 1. 线性奖励倾斜总是可高效采样（这一简单结果在文献中似乎被忽略了）。2. 对于低秩正定二次倾斜（$A$ 正定且秩为 $O(1)$），提供了高效采样算法。3. 对于负定二次倾斜（$r(x) = - x^\top A x$，$A$ 正定），证明即使在秩1情况下（尽管具有指数级大的条目）也是难解的。

Conclusion: 本文对奖励倾斜扩散模型的采样复杂性提供了精细的分析，揭示了不同奖励函数类型的计算边界。线性奖励总是可高效处理，低秩正定二次奖励也可高效采样，但负定二次奖励即使在简单情况下也是计算难解的。这些结果为推理时间算法的设计提供了理论基础。

Abstract: Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.
  In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\star}(x) \propto p(x) \exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\top A x + b^\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).

</details>


### [214] [A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models](https://arxiv.org/abs/2602.16626)
*SungJun Cho,Chetan Gohil,Rukuang Huang,Oiwi Parker Jones,Mark W. Woolrich*

Main category: cs.LG

TL;DR: 本文系统评估了神经影像数据（MEG）的tokenization策略，比较了可学习和不可学习方法，发现两者在重建精度和下游任务性能上表现相当，表明简单的固定tokenization策略足以用于神经基础模型开发。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理在神经影像基础模型中的应用增长，连续神经时间序列数据的离散化（tokenization）变得重要，但不同tokenization策略对神经数据的影响尚不清楚，需要系统评估。

Method: 系统评估了基于transformer的大型神经影像模型（LNMs）在MEG数据上的样本级tokenization策略，比较了可学习和不可学习tokenizer，引入基于自编码器的新型可学习tokenizer，在三个公开MEG数据集上进行实验。

Result: 可学习和不可学习离散化方案都能达到高重建精度，在大多数评估标准上表现相当，包括token预测、生成数据的生物学合理性、受试者特定信息保留和下游任务性能。

Conclusion: 简单的固定样本级tokenization策略可用于神经基础模型开发，为神经影像数据的tokenization选择提供了实用指导。

Abstract: Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.

</details>


### [215] [MoDE-Boost: Boosting Shared Mobility Demand with Edge-Ready Prediction Models](https://arxiv.org/abs/2602.16573)
*Antonios Tziorvas,George S. Theodoropoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 提出两种梯度提升模型（分类和回归）用于城市需求预测，能够预测5分钟到1小时的时间范围，通过整合时空特征提高共享微出行服务效率


<details>
  <summary>Details</summary>
Motivation: 城市需求预测对智能交通系统中的路径优化、调度和拥堵管理至关重要。通过数据融合和分析技术，交通需求预测可以作为识别新兴时空需求模式的关键中间措施。现代城市面临快速城市化带来的挑战，需要更可持续、高效和宜居的城市管理方法。

Method: 提出两种梯度提升模型变体：一个用于分类，一个用于回归。两种模型都能生成从5分钟到1小时不同时间范围的需求预测。整体方法有效整合了时间和上下文特征，实现准确预测。

Result: 使用五个大都市区的电动滑板车和电动自行车网络的开放共享出行数据进行评估。与最先进方法以及基于生成式AI的模型进行比较，证明该方法能有效捕捉现代城市出行的复杂性。

Conclusion: 该方法为城市微出行管理提供了新颖见解，有助于应对快速城市化带来的挑战，从而促进更可持续、高效和宜居的城市发展。

Abstract: Urban demand forecasting plays a critical role in optimizing routing, dispatching, and congestion management within Intelligent Transportation Systems. By leveraging data fusion and analytics techniques, traffic demand forecasting serves as a key intermediate measure for identifying emerging spatial and temporal demand patterns. In this paper, we tackle this challenge by proposing two gradient boosting model variations, one for classiffication and one for regression, both capable of generating demand forecasts at various temporal horizons, from 5 minutes up to one hour. Our overall approach effectively integrates temporal and contextual features, enabling accurate predictions that are essential for improving the efficiency of shared (micro-) mobility services. To evaluate its effectiveness, we utilize open shared mobility data derived from e-scooter and e-bike networks in five metropolitan areas. These real-world datasets allow us to compare our approach with state-of-the-art methods as well as a Generative AI-based model, demonstrating its effectiveness in capturing the complexities of modern urban mobility. Ultimately, our methodology offers novel insights on urban micro-mobility management, helping to tackle the challenges arising from rapid urbanization and thus, contributing to more sustainable, efficient, and livable cities.

</details>


### [216] [Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes](https://arxiv.org/abs/2602.16629)
*Ethan Blaser,Jiuqi Wang,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文证明了n步差分TD学习算法在标准递减学习率下（无需局部时钟）的几乎必然收敛性，为平均奖励RL提供了更实用的理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有的差分TD学习算法收敛性证明需要依赖于状态访问计数的局部时钟学习率，这与实际应用不符且无法扩展到表格设置之外，限制了算法的实用性和理论完整性。

Method: 证明了在标准递减学习率下（无需局部时钟）的on-policy n步差分TD算法的几乎必然收敛性，并推导了off-policy n步差分TD收敛的三个充分条件。

Result: 成功证明了on-policy n步差分TD算法在标准学习率下的收敛性，并建立了off-policy收敛的三个充分条件，将理论分析与实际实现更紧密地结合起来。

Conclusion: 这项工作强化了差分TD学习的理论基础，使其收敛性分析更贴近实际应用，为平均奖励强化学习提供了更实用的理论保证。

Abstract: The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.

</details>


### [217] [Predicting The Cop Number Using Machine Learning](https://arxiv.org/abs/2602.16600)
*Meagan Mann,Christian Muise,Erin Meger*

Main category: cs.LG

TL;DR: 机器学习方法（特别是基于树的模型和图神经网络）能够准确预测图的警察数，无需显式特征工程，且可识别影响警察数的关键图结构特征。


<details>
  <summary>Details</summary>
Motivation: 警察与强盗游戏中的警察数计算在计算上很困难，传统算法通常局限于小图族。研究机器学习方法是否能从图的结构特性准确预测警察数，并识别哪些特性对预测影响最大。

Method: 使用经典机器学习方法（特别是基于树的模型）和图神经网络来预测图的警察数。分析哪些图结构特性对预测最有效，并进行可解释性分析。

Result: 基于树的模型在类别不平衡情况下仍能达到高预测准确率，图神经网络无需显式特征工程也能获得可比结果。可解释性分析显示最具有预测性的特征与节点连通性、聚类、团结构和宽度参数相关。

Conclusion: 机器学习方法可以作为现有警察数算法的补充，在计算不可行时提供可扩展的近似解。最预测性的特征与已知理论结果一致，表明机器学习能有效捕捉影响警察数的关键图结构特性。

Abstract: Cops and Robbers is a pursuit evasion game played on a graph, first introduced independently by Quilliot \cite{quilliot1978jeux} and Nowakowski and Winkler \cite{NOWAKOWSKI1983235} over four decades ago. A main interest in recent the literature is identifying the cop number of graph families. The cop number of a graph, $c(G)$, is defined as the minimum number of cops required to guarantee capture of the robber. Determining the cop number is computationally difficult and exact algorithms for this are typically restricted to small graph families. This paper investigates whether classical machine learning methods and graph neural networks can accurately predict a graph's cop number from its structural properties and identify which properties most strongly influence this prediction. Of the classical machine learning models, tree-based models achieve high accuracy in prediction despite class imbalance, whereas graph neural networks achieve comparable results without explicit feature engineering. The interpretability analysis shows that the most predictive features are related to node connectivity, clustering, clique structure, and width parameters, which aligns with known theoretical results. Our findings suggest that machine learning approaches can be used in complement with existing cop number algorithms by offering scalable approximations where computation is infeasible.

</details>


### [218] [Optimizer choice matters for the emergence of Neural Collapse](https://arxiv.org/abs/2602.16642)
*Jim Zhao,Tin Sum Cheng,Wojciech Masarczyk,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 该研究挑战了神经折叠现象在所有优化器中普遍存在的假设，证明优化器选择对神经折叠的出现至关重要，并首次提供了优化器依赖性的理论解释。


<details>
  <summary>Details</summary>
Motivation: 现有神经折叠理论分析大多忽略优化器的作用，认为神经折叠在所有优化方法中普遍存在。本研究挑战这一假设，探索优化器选择如何影响神经折叠现象的出现。

Method: 引入新的诊断指标NC0，其收敛到零是神经折叠的必要条件。使用NC0分析不同优化器的理论动态，包括SGD、带耦合权重衰减的SignGD（Adam特例）和带解耦权重衰减的SignGD（AdamW特例）。进行了3900次训练实验，涵盖多种数据集、架构、优化器和超参数。

Result: 理论证明：自适应优化器中的解耦权重衰减（如AdamW）无法产生神经折叠；不同优化器表现出不同的NC0动态；动量在SGD训练中能加速神经折叠。实验验证了理论结果。

Conclusion: 优化器选择对神经折叠的出现至关重要，权重衰减耦合方式在塑造优化器隐式偏置中起关键作用，这是首次提供神经折叠优化器依赖性的理论解释。

Abstract: Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.

</details>


### [219] [Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment](https://arxiv.org/abs/2602.16643)
*Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出使用因子分解机与二次优化退火（FMQA）解决RNA逆折叠问题，并系统分析核苷酸整数分配和二进制编码方法对性能的影响


<details>
  <summary>Details</summary>
Motivation: 现有RNA逆折叠方法通常需要大量序列评估，实验验证成本高。FMQA作为离散黑盒优化方法，能以有限评估次数获得高质量解，但其在RNA逆折叠中的应用需要将核苷酸转换为二进制变量，而整数-核苷酸分配和二进制编码方法的影响尚未深入研究

Method: 建立FMQA框架用于RNA逆折叠，评估所有24种可能的核苷酸整数分配（0-3）与四种二进制整数编码方法（one-hot、domain-wall、binary、unary）的组合

Result: one-hot和domain-wall编码在归一化集成缺陷值方面优于binary和unary编码。在domain-wall编码中，分配给边界整数（0和3）的核苷酸出现频率更高。将鸟嘌呤和胞嘧啶分配给这些边界整数能促进它们在茎区的富集，从而获得比one-hot编码更热力学稳定的二级结构

Conclusion: 成功建立了用于RNA逆折叠的FMQA框架，并揭示了整数-核苷酸分配和编码方法对优化性能的重要影响，特别是domain-wall编码结合特定核苷酸分配能产生更稳定的RNA结构

Abstract: The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.

</details>


### [220] [Neighborhood Stability as a Measure of Nearest Neighbor Searchability](https://arxiv.org/abs/2602.16673)
*Thomas Vecchiato,Sebastian Bruch*

Main category: cs.LG

TL;DR: 提出了两种衡量高维数据聚类搜索性的指标：聚类-邻域稳定性度量（clustering-NSM）用于评估聚类质量并预测ANNS准确率，点-邻域稳定性度量（point-NSM）用于评估数据集本身的可聚类性并预测clustering-NSM。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏分析工具来确定基于聚类的近似最近邻搜索（ANNS）对特定数据集的适用性（即"可搜索性"）。现有方法无法预测聚类ANNS在给定数据集上的表现，这限制了算法的选择和优化。

Method: 提出了两种基于邻域关系的度量方法：1) clustering-NSM：作为聚类质量的内部度量，基于聚类划分计算，用于预测ANNS准确率；2) point-NSM：作为数据集可聚类性的度量，基于数据点本身计算，用于预测clustering-NSM。两种方法都基于点之间的最近邻关系而非距离，因此适用于包括内积在内的多种距离函数。

Result: 开发了能够预测聚类ANNS性能的分析工具，通过point-NSM评估数据集的可聚类性，再通过clustering-NSM评估具体聚类方案的质量，从而确定数据集是否适合采用聚类ANNS方法。

Conclusion: 提出的两种稳定性度量为聚类ANNS的可搜索性提供了理论分析框架，使得仅通过数据点本身就能评估聚类ANNS的适用性，填补了该领域分析工具的空白。

Abstract: Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset -- what we call "searchability." To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.

</details>


### [221] [Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition](https://arxiv.org/abs/2602.16684)
*Bo Pan,Peter Zhiping Zhang,Hao-Wei Pang,Alex Zhu,Xiang Yu,Liying Zhang,Liang Zhao*

Main category: cs.LG

TL;DR: 提出基于大规模MMP变换的变体生成基础模型，通过提示机制和检索增强框架实现可控的类似物设计


<details>
  <summary>Details</summary>
Motivation: 现有ML方法要么在全分子层面操作且编辑可控性有限，要么从受限设置和小模型中学习MMP式编辑，需要更灵活可控的类似物生成方法

Method: 采用变体到变体的生成公式，在大规模MMP变换上训练基础模型，开发提示机制让用户指定偏好变换模式，并引入MMPT-RAG检索增强框架利用外部参考类似物作为上下文指导

Result: 在通用化学语料库和专利特定数据集上的实验表明，该方法在多样性、新颖性和可控性方面均有改进，能够在实际发现场景中恢复真实的类似物结构

Conclusion: 提出的方法为药物化学家提供了更灵活可控的类似物设计工具，能够从项目特定系列中泛化并生成多样化的候选分子

Abstract: Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.

</details>


### [222] [Protecting the Undeleted in Machine Unlearning](https://arxiv.org/abs/2602.16697)
*Aloni Cohen,Refael Kohen,Kobbi Nissim,Uri Stemmer*

Main category: cs.LG

TL;DR: 论文指出完美再训练式的机器遗忘存在隐私风险，攻击者可通过删除请求重构整个数据集，提出新的安全定义保护未删除数据


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练模型中删除特定数据点，追求"完美再训练"效果。但研究发现这种方法及其安全定义对剩余（未删除）数据点存在重大隐私风险

Method: 1) 展示重构攻击：证明对于某些任务，遵循完美再训练的机制允许攻击者通过少量删除请求重构几乎整个数据集；2) 调查现有机器遗忘定义；3) 提出新的安全定义，专门保护未删除数据免受其他点删除导致的泄漏

Result: 发现完美再训练方法存在严重隐私漏洞，现有定义要么易受攻击要么过于限制。提出的新安全定义能够支持公告板、求和、统计学习等基本功能

Conclusion: 机器遗忘的完美再训练方法存在根本性隐私缺陷，需要重新设计安全定义以保护未删除数据，提出的新定义在安全性和功能性之间取得了更好平衡

Abstract: Machine unlearning aims to remove specific data points from a trained model, often striving to emulate "perfect retraining", i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $ω(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.

</details>


### [223] [Causality is Key for Interpretability Claims to Generalise](https://arxiv.org/abs/2602.16698)
*Shruti Joshi,Aaron Mueller,David Klindt,Wieland Brendel,Patrik Reizinger,Dhanya Sridhar*

Main category: cs.LG

TL;DR: 该论文提出使用因果推理框架来改进大语言模型的可解释性研究，解决现有研究中普遍存在的泛化性不足和因果解释过度的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型可解释性研究存在两个主要问题：1）研究发现难以泛化；2）因果解释往往超出证据支持范围。需要建立一个更严谨的框架来确保可解释性研究的有效性和可靠性。

Method: 采用Pearl的因果层次理论作为分析框架，结合因果表示学习（CRL）方法。因果层次理论将证据分为三个层次：观察（关联）、干预（因果效应）和反事实（假设情景）。CRL则具体化了从模型激活中可恢复哪些变量以及需要什么假设。

Result: 提出了一个诊断框架，帮助研究者根据证据类型选择合适的方法和评估指标，确保研究结论与证据强度相匹配，从而提高研究发现的泛化能力。

Conclusion: 因果推理为可解释性研究提供了严谨的理论基础，能够明确界定不同证据类型支持的结论范围。通过因果层次理论和CRL的结合，可以构建更可靠、可泛化的可解释性研究方法。

Abstract: Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.

</details>


### [224] [Knowledge-Embedded Latent Projection for Robust Representation Learning](https://arxiv.org/abs/2602.16709)
*Weijing Tang,Ming Yuan,Zongqi Xia,Tianxi Cai*

Main category: cs.LG

TL;DR: 提出知识嵌入的潜在投影模型，利用外部语义嵌入（如临床概念预训练嵌入）来正则化表示学习，解决电子健康记录中维度不平衡问题的潜在空间模型估计挑战。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）等离散数据矩阵存在维度不平衡问题：队列规模受疾病流行率或数据可用性限制而较小，但特征空间因医学编码系统而极大。传统潜在空间模型在这种不平衡情况下估计困难，而外部语义嵌入（如临床概念预训练嵌入）的可用性增加为利用语义辅助信息提供了机会。

Method: 提出知识嵌入的潜在投影模型，将列嵌入建模为语义嵌入在再生核希尔伯特空间中的平滑函数映射。开发计算高效的两步估计程序：1）通过核主成分分析进行语义引导的子空间构建；2）可扩展的投影梯度下降。该方法利用外部语义信息正则化表示学习。

Result: 建立了估计误差界限，刻画了统计误差与核投影引起的近似误差之间的权衡关系。为非凸优化程序提供了局部收敛保证。广泛的模拟研究和真实世界EHR应用证明了该方法的有效性。

Conclusion: 提出的知识嵌入潜在投影模型能够有效利用外部语义信息来正则化表示学习，解决了高维离散数据矩阵在维度不平衡情况下的估计挑战，为电子健康记录等应用提供了实用的解决方案。

Abstract: Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.

</details>
