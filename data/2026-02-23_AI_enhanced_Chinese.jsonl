{"id": "2602.17851", "categories": ["q-fin.CP", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2602.17851", "abs": "https://arxiv.org/abs/2602.17851", "authors": ["Krishna Neupane", "Prem Sapkota", "Ujjwal Prajapati"], "title": "Beyond the Numbers: Causal Effects of Financial Report Sentiment on Bank Profitability", "comment": null, "summary": "This study establishes the causal effects of market sentiment on firm profitability, moving beyond traditional correlational analyses. It leverages a causal forest machine learning methodology to control for numerous confounding variables, enabling systematic analysis of heterogeneity and non-linearities often overlooked. A key innovation is the use of a pre-trained FinancialBERT to generate sentiment scores from quarterly reports, which are then treated as causal interventions impacting profitability dynamics like returns and volatilities. Utilizing a comprehensive dataset from NEPSE, NRB, and individual financial institutions, the research employs SHAP analysis to identify influential profit predictors. A two-pronged causal analysis further explores how sentiment's impact is conditioned by Loan Portfolio/Asset Composition and Balance Sheet Strength/Leverage. Average Treatment Effect analyses, combined with SHAP insights, reveal statistically significant causal associations between certain balance sheet and expense management variables and profitability. This advanced causal machine learning framework significantly extends existing literature, providing a more robust understanding of how financial sentiment truly impacts firm performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u56e0\u679c\u68ee\u6797\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408FinancialBERT\u60c5\u611f\u5206\u6790\uff0c\u63a2\u8ba8\u5e02\u573a\u60c5\u7eea\u5bf9\u4f01\u4e1a\u76c8\u5229\u80fd\u529b\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u76f8\u5173\u6027\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u591a\u5173\u6ce8\u5e02\u573a\u60c5\u7eea\u4e0e\u4f01\u4e1a\u76c8\u5229\u7684\u76f8\u5173\u6027\uff0c\u4f46\u7f3a\u4e4f\u56e0\u679c\u5173\u7cfb\u7684\u6df1\u5165\u5206\u6790\u3002\u672c\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u5e02\u573a\u60c5\u7eea\u5bf9\u4f01\u4e1a\u76c8\u5229\u80fd\u529b\u7684\u56e0\u679c\u6548\u5e94\uff0c\u8003\u8651\u5f02\u8d28\u6027\u548c\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u7406\u89e3\u3002", "method": "1. \u4f7f\u7528\u9884\u8bad\u7ec3\u7684FinancialBERT\u4ece\u5b63\u5ea6\u62a5\u544a\u4e2d\u751f\u6210\u60c5\u611f\u5206\u6570\u4f5c\u4e3a\u56e0\u679c\u5e72\u9884\u53d8\u91cf\uff1b2. \u91c7\u7528\u56e0\u679c\u68ee\u6797\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63a7\u5236\u4f17\u591a\u6df7\u6dc6\u53d8\u91cf\uff1b3. \u4f7f\u7528SHAP\u5206\u6790\u8bc6\u522b\u5f71\u54cd\u76c8\u5229\u7684\u5173\u952e\u9884\u6d4b\u56e0\u5b50\uff1b4. \u8fdb\u884c\u53cc\u7ba1\u9f50\u4e0b\u7684\u56e0\u679c\u5206\u6790\uff0c\u8003\u5bdf\u8d37\u6b3e\u7ec4\u5408/\u8d44\u4ea7\u6784\u6210\u4ee5\u53ca\u8d44\u4ea7\u8d1f\u503a\u8868\u5f3a\u5ea6/\u6760\u6746\u5982\u4f55\u8c03\u8282\u60c5\u7eea\u5f71\u54cd\uff1b5. \u8fdb\u884c\u5e73\u5747\u5904\u7406\u6548\u5e94\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u8d44\u4ea7\u8d1f\u503a\u8868\u548c\u8d39\u7528\u7ba1\u7406\u53d8\u91cf\u4e0e\u76c8\u5229\u80fd\u529b\u5b58\u5728\u7edf\u8ba1\u663e\u8457\u7684\u56e0\u679c\u5173\u7cfb\u3002SHAP\u5206\u6790\u8bc6\u522b\u4e86\u5f71\u54cd\u76c8\u5229\u7684\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u3002\u7814\u7a76\u63ed\u793a\u4e86\u60c5\u7eea\u5f71\u54cd\u5982\u4f55\u53d7\u8d37\u6b3e\u7ec4\u5408\u3001\u8d44\u4ea7\u6784\u6210\u3001\u8d44\u4ea7\u8d1f\u503a\u8868\u5f3a\u5ea6\u548c\u6760\u6746\u7684\u8c03\u8282\u3002", "conclusion": "\u8be5\u5148\u8fdb\u7684\u56e0\u679c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u663e\u8457\u6269\u5c55\u4e86\u73b0\u6709\u6587\u732e\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u91d1\u878d\u60c5\u7eea\u5982\u4f55\u771f\u6b63\u5f71\u54cd\u4f01\u4e1a\u7ee9\u6548\u7684\u66f4\u7a33\u5065\u7406\u89e3\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u76f8\u5173\u6027\u5206\u6790\u3002"}}
{"id": "2602.17890", "categories": ["q-fin.CP"], "pdf": "https://arxiv.org/pdf/2602.17890", "abs": "https://arxiv.org/abs/2602.17890", "authors": ["Krishna Neupane"], "title": "The Information Dynamics of Insider Intent: How Reporting Inversions (Form 144) Mask Informational Rents in Insider Sales (Form 4)", "comment": null, "summary": "This study identifies and quantifies a significant informational friction embedded in the SEC Form 144 disclosure regime, characterized as predictive decoupling. Drawing on a theoretical foundation of welfare economics, the article argues that the current reporting inversion -- where trade execution (Form 4) frequently precedes the public notice of intent (Form 144) -- violates the conditions for Pareto efficiency by inducing non-symmetric pricing. Utilizing an event-study framework of intent-to-sell windows, the analysis examines cases where insiders file a notice of proposed sale but fail to execute within the statutory 90-day period. The machine learning audit reveals a persistent 52.4 percent opacity rate, where aborted signals remain statistically indistinguishable from routine executions, creating a structural information ceiling that prevents the market from exhausting the signal's informational content. Contrary to the traditional small-firm effect, the study documents a large-cap significance paradox: while small-cap portfolios yield higher absolute abnormal returns (32.21 bps), statistically significant alpha is concentrated in large-cap firms (14.49 bps, $p = 0.021$). The results suggest that Institutional Salience enables more reliable processing of this negative non-event when reputational costs are maximized. Cross-sectional tests confirm that prior idiosyncratic volatility serves as a signal amplifier, with causal estimators identifying an illiquidity jump of up to 2.63 times. To mitigate this market failure, the study proposes a mandatory execution confirmation (Form 144-A) to transition the regime toward bilateral accountability, converting a predictive blind spot into a verifiable data stream and restoring the informational symmetry requisite for efficient capital allocation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0SEC Form 144\u62ab\u9732\u5236\u5ea6\u5b58\u5728\u4fe1\u606f\u6469\u64e6\uff0c\u8868\u73b0\u4e3a\"\u9884\u6d4b\u8131\u94a9\"\u73b0\u8c61\uff0c\u5373\u4ea4\u6613\u6267\u884c(Form 4)\u5e38\u65e9\u4e8e\u51fa\u552e\u610f\u5411\u516c\u544a(Form 144)\uff0c\u5bfc\u81f4\u975e\u5bf9\u79f0\u5b9a\u4ef7\uff0c\u8fdd\u53cd\u5e15\u7d2f\u6258\u6548\u7387\u6761\u4ef6\u3002", "motivation": "\u5f53\u524dSEC Form 144\u62ab\u9732\u5236\u5ea6\u5b58\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u4ea4\u6613\u6267\u884c(Form 4)\u7ecf\u5e38\u5148\u4e8e\u51fa\u552e\u610f\u5411\u516c\u544a(Form 144)\uff0c\u8fd9\u79cd\"\u62a5\u544a\u5012\u7f6e\"\u5bfc\u81f4\u5e02\u573a\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4fe1\u606f\uff0c\u9020\u6210\u5b9a\u4ef7\u6548\u7387\u635f\u5931\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u7814\u7a76\u6cd5\u5206\u6790\u610f\u56fe\u51fa\u552e\u7a97\u53e3\uff0c\u7814\u7a76\u5185\u90e8\u4eba\u58eb\u63d0\u4ea4\u51fa\u552e\u610f\u5411\u4f46\u672a\u572890\u5929\u6cd5\u5b9a\u671f\u9650\u5185\u6267\u884c\u7684\u60c5\u51b5\uff1b\u8fd0\u7528\u673a\u5668\u5b66\u4e60\u5ba1\u8ba1\u8bc4\u4f30\u4fe1\u606f\u4e0d\u900f\u660e\u5ea6\uff1b\u8fdb\u884c\u6a2a\u622a\u9762\u6d4b\u8bd5\u5206\u6790\u5f02\u8d28\u6027\u6ce2\u52a8\u7387\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b052.4%\u7684\u4e0d\u900f\u660e\u5ea6\u7387\uff0c\u4e2d\u6b62\u4fe1\u53f7\u4e0e\u5e38\u89c4\u6267\u884c\u5728\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\uff1b\u5927\u578b\u516c\u53f8\u8868\u73b0\u51fa\u663e\u8457\u6027\u6096\u8bba(14.49bps\u663e\u8457alpha)\uff0c\u800c\u5c0f\u578b\u516c\u53f8\u867d\u6709\u66f4\u9ad8\u5f02\u5e38\u56de\u62a5(32.21bps)\u4f46\u4e0d\u663e\u8457\uff1b\u5f02\u8d28\u6027\u6ce2\u52a8\u7387\u653e\u5927\u4fe1\u53f7\u6548\u5e94\uff0c\u5bfc\u81f4\u6d41\u52a8\u6027\u4e0b\u964d\u8fbe2.63\u500d\u3002", "conclusion": "\u5efa\u8bae\u5f15\u5165\u5f3a\u5236\u6027\u6267\u884c\u786e\u8ba4\u673a\u5236(Form 144-A)\uff0c\u5b9e\u73b0\u53cc\u8fb9\u95ee\u8d23\uff0c\u5c06\u9884\u6d4b\u76f2\u70b9\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u6570\u636e\u6d41\uff0c\u6062\u590d\u4fe1\u606f\u5bf9\u79f0\u6027\uff0c\u4fc3\u8fdb\u8d44\u672c\u914d\u7f6e\u6548\u7387\u3002"}}
{"id": "2602.17895", "categories": ["q-fin.CP", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2602.17895", "abs": "https://arxiv.org/abs/2602.17895", "authors": ["Krishna Neupane"], "title": "The Strategic Gap: How AI-Driven Timing and Complexity Shape Investor Trust in the Age of Digital Agents", "comment": null, "summary": "Traditional models of market efficiency assume that equity prices incorporate information based on content alone, often neglecting the structural influence of reporting timing and cadence. This study introduces the Autonomous Disclosure Regulator, a multi-node AI framework designed to audit the intersection of disclosure complexity and filing unpredictability. Analyzing a population of 484,796 regulatory filings, the research identifies a structural Strategic Gap: a state where companies use confusing language and unpredictable timing to slow down how fast the market learns the truth by 60%. The results demonstrate a fundamental computational asymmetry in contemporary capital markets. While investors are now good at spotting \"copy-paste\" text, they remain vulnerable to strategic timing that obscures structural deterioration. The framework isolates 39 high-priority failures where the convergence of dense text and temporal surprises facilitated significant information rent extraction by insiders. By implementing a recursive agentic audit, the system identifies a cumulative welfare recovery potential of over 360\\% and demonstrates near-perfect resilience against technical data interruptions. The study concludes by proposing a transition toward an agentic regulatory state, arguing that as information integration costss rise, infrastructure must evolve from passive data repositories into active auditing nodes capable of real-time synthesis to preserve market integrity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u81ea\u4e3b\u62ab\u9732\u76d1\u7ba1\u5668\"AI\u6846\u67b6\uff0c\u53d1\u73b0\u516c\u53f8\u901a\u8fc7\u590d\u6742\u8bed\u8a00\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u62ab\u9732\u65f6\u95f4\u5236\u9020\"\u6218\u7565\u7f3a\u53e3\"\uff0c\u4f7f\u5e02\u573a\u4e86\u89e3\u771f\u76f8\u7684\u901f\u5ea6\u964d\u4f4e60%\uff0c\u5e76\u8bc6\u522b\u51fa39\u4e2a\u9ad8\u98ce\u9669\u5931\u6548\u6848\u4f8b\u3002", "motivation": "\u4f20\u7edf\u5e02\u573a\u6548\u7387\u6a21\u578b\u53ea\u5173\u6ce8\u62ab\u9732\u5185\u5bb9\u800c\u5ffd\u7565\u62a5\u544a\u65f6\u95f4\u548c\u8282\u594f\u7684\u7ed3\u6784\u6027\u5f71\u54cd\uff0c\u5bfc\u81f4\u6295\u8d44\u8005\u5bf9\u6218\u7565\u6027\u65f6\u95f4\u5b89\u6392\u548c\u590d\u6742\u8bed\u8a00\u7684\u8106\u5f31\u6027\u3002\u9700\u8981\u65b0\u7684\u76d1\u7ba1\u6846\u67b6\u6765\u5e94\u5bf9\u4fe1\u606f\u6574\u5408\u6210\u672c\u4e0a\u5347\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u591a\u8282\u70b9AI\u6846\u67b6\uff08\u81ea\u4e3b\u62ab\u9732\u76d1\u7ba1\u5668\uff09\uff0c\u5206\u6790484,796\u4efd\u76d1\u7ba1\u6587\u4ef6\uff0c\u8bc6\u522b\u62ab\u9732\u590d\u6742\u6027\u548c\u7533\u62a5\u4e0d\u53ef\u9884\u6d4b\u6027\u7684\u4ea4\u53c9\u70b9\uff0c\u91c7\u7528\u9012\u5f52\u4ee3\u7406\u5ba1\u8ba1\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\"\u6218\u7565\u7f3a\u53e3\"\u73b0\u8c61\uff1a\u516c\u53f8\u4f7f\u7528\u6df7\u6dc6\u8bed\u8a00\u548c\u4e0d\u53ef\u9884\u6d4b\u65f6\u95f4\u4f7f\u5e02\u573a\u4e86\u89e3\u771f\u76f8\u901f\u5ea6\u964d\u4f4e60%\uff1b\u8bc6\u522b39\u4e2a\u9ad8\u98ce\u9669\u5931\u6548\u6848\u4f8b\uff1b\u7cfb\u7edf\u663e\u793a360%\u7684\u798f\u5229\u6062\u590d\u6f5c\u529b\u548c\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u6280\u672f\u4e2d\u65ad\u6062\u590d\u80fd\u529b\u3002", "conclusion": "\u5efa\u8bae\u5411\u4ee3\u7406\u76d1\u7ba1\u72b6\u6001\u8fc7\u6e21\uff0c\u76d1\u7ba1\u57fa\u7840\u8bbe\u65bd\u5e94\u4ece\u88ab\u52a8\u6570\u636e\u5b58\u50a8\u5e93\u6f14\u53d8\u4e3a\u80fd\u591f\u5b9e\u65f6\u5408\u6210\u7684\u4e3b\u52a8\u5ba1\u8ba1\u8282\u70b9\uff0c\u4ee5\u7ef4\u62a4\u5e02\u573a\u5b8c\u6574\u6027\u3002"}}
{"id": "2602.18062", "categories": ["q-fin.CP", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2602.18062", "abs": "https://arxiv.org/abs/2602.18062", "authors": ["Daniel Chee", "Noufel Frikha", "Libo Li"], "title": "A Monotone Limit Approach to Entropy-Regularized American Options", "comment": null, "summary": "Recent advances in continuous-time optimal stopping have been driven by entropy-regularized formulations of randomized stopping problems, with most existing approaches relying on partial differential equation methods. In this paper, we propose a fully probabilistic framework based on the Doob-Meyer-Mertens decomposition of the Snell envelope and its representation through reflected backward stochastic differential equations. We introduce an entropy-regularized penalization scheme yielding a monotone approximation of the value function and establish explicit convergence rates under suitable regularity assumptions. In addition, we develop a policy improvement algorithm based on linear backward stochastic differential equations and illustrate its performance through a simple numerical experiment for an American-style max call option", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eDoob-Meyer-Mertens\u5206\u89e3\u548c\u53cd\u5c04BSDE\u7684\u5b8c\u5168\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u71b5\u6b63\u5219\u5316\u6700\u4f18\u505c\u6b62\u95ee\u9898\uff0c\u5efa\u7acb\u5355\u8c03\u903c\u8fd1\u548c\u6536\u655b\u7387\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u7ebf\u6027BSDE\u7684\u7b56\u7565\u6539\u8fdb\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u65f6\u95f4\u6700\u4f18\u505c\u6b62\u65b9\u6cd5\u591a\u4f9d\u8d56PDE\u65b9\u6cd5\uff0c\u672c\u6587\u65e8\u5728\u5efa\u7acb\u5b8c\u5168\u6982\u7387\u6846\u67b6\uff0c\u5229\u7528Snell\u5305\u7edc\u7684\u5206\u89e3\u548c\u53cd\u5c04BSDE\u8868\u793a\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u57fa\u4e8eDoob-Meyer-Mertens\u5206\u89e3\u548c\u53cd\u5c04BSDE\u8868\u793aSnell\u5305\u7edc\uff0c\u5f15\u5165\u71b5\u6b63\u5219\u5316\u60e9\u7f5a\u65b9\u6848\u83b7\u5f97\u4ef7\u503c\u51fd\u6570\u7684\u5355\u8c03\u903c\u8fd1\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u7ebf\u6027BSDE\u7684\u7b56\u7565\u6539\u8fdb\u7b97\u6cd5\u3002", "result": "\u5728\u9002\u5f53\u6b63\u5219\u6027\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u660e\u786e\u7684\u6536\u655b\u7387\uff0c\u5e76\u901a\u8fc7\u7f8e\u5f0f\u6700\u5927\u770b\u6da8\u671f\u6743\u7684\u7b80\u5355\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u6846\u67b6\u4e3a\u71b5\u6b63\u5219\u5316\u6700\u4f18\u505c\u6b62\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\u548c\u7b97\u6cd5\u5b9e\u73b0\uff0c\u6269\u5c55\u4e86\u73b0\u6709PDE\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.17874", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17874", "abs": "https://arxiv.org/abs/2602.17874", "authors": ["J. Liu", "F. Milano"], "title": "Modal Energy for Power System Analysis: Definitions and Requirements", "comment": null, "summary": "Modal energy provides information complementary to and based on conventional eigenvalues and participation factors for power system modal analysis. However, modal energy definition is not unique. This letter clarifies the definitions and applicability of mainstream modal energy approaches, focusing on their mappings to eigenvalues and to the total system energy. It is shown that these mappings hold only under restrictive conditions, notably system normality, which limits their applicability in inverter-dominated power systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6f84\u6e05\u4e86\u4e3b\u6d41\u6a21\u6001\u80fd\u91cf\u65b9\u6cd5\u7684\u5b9a\u4e49\u548c\u9002\u7528\u6027\uff0c\u6307\u51fa\u5b83\u4eec\u4ec5\u5728\u7cfb\u7edf\u6b63\u6001\u6027\u7b49\u9650\u5236\u6761\u4ef6\u4e0b\u624d\u6210\u7acb\uff0c\u8fd9\u5728\u9006\u53d8\u5668\u4e3b\u5bfc\u7684\u7535\u529b\u7cfb\u7edf\u4e2d\u9002\u7528\u6027\u6709\u9650\u3002", "motivation": "\u6a21\u6001\u80fd\u91cf\u4e3a\u7535\u529b\u7cfb\u7edf\u6a21\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u57fa\u4e8e\u4f20\u7edf\u7279\u5f81\u503c\u548c\u53c2\u4e0e\u56e0\u5b50\u7684\u8865\u5145\u4fe1\u606f\uff0c\u4f46\u6a21\u6001\u80fd\u91cf\u7684\u5b9a\u4e49\u5e76\u4e0d\u552f\u4e00\uff0c\u9700\u8981\u6f84\u6e05\u4e0d\u540c\u65b9\u6cd5\u7684\u5b9a\u4e49\u548c\u9002\u7528\u6761\u4ef6\u3002", "method": "\u5206\u6790\u4e3b\u6d41\u6a21\u6001\u80fd\u91cf\u65b9\u6cd5\u7684\u5b9a\u4e49\uff0c\u91cd\u70b9\u5173\u6ce8\u5b83\u4eec\u4e0e\u7279\u5f81\u503c\u4ee5\u53ca\u603b\u7cfb\u7edf\u80fd\u91cf\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5e76\u63a2\u8ba8\u8fd9\u4e9b\u6620\u5c04\u6210\u7acb\u7684\u6761\u4ef6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6620\u5c04\u5173\u7cfb\u4ec5\u5728\u9650\u5236\u6027\u6761\u4ef6\u4e0b\u6210\u7acb\uff0c\u7279\u522b\u662f\u7cfb\u7edf\u6b63\u6001\u6027\u6761\u4ef6\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9006\u53d8\u5668\u4e3b\u5bfc\u7684\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u6a21\u6001\u80fd\u91cf\u65b9\u6cd5\u5728\u4f20\u7edf\u7535\u529b\u7cfb\u7edf\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u9006\u53d8\u5668\u4e3b\u5bfc\u7684\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u4e2d\u9002\u7528\u6027\u53d7\u9650\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u5176\u5e94\u7528\u6761\u4ef6\u3002"}}
{"id": "2602.17720", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17720", "abs": "https://arxiv.org/abs/2602.17720", "authors": ["Yue Fu", "Yifan Lin", "Yessica Wang", "Sarah Tran", "Alexis Hiniker"], "title": "\"Everyone's using it, but no one is allowed to talk about it\": College Students' Experiences Navigating the Higher Education Environment in a Generative AI World", "comment": null, "summary": "Higher education students are increasingly using generative AI in their academic work. However, existing institutional practices have not yet adapted to this shift. Through semi-structured interviews with 23 college students, our study examines the environmental and social factors that influence students' use of AI. Findings show that institutional pressure factors like deadlines, exam cycles, and grading lead students to engage with AI even when they think it undermines their learning. Social influences, particularly peer micro-communities, establish de-facto AI norms regardless of official AI policies. Campus-wide ``AI shame'' is prevalent, often pushing AI use underground. Current institutional AI policies are perceived as generic, inconsistent, and confusing, resulting in routine noncompliance. Additionally, students develop value-based self-regulation strategies, but environmental pressures create a gap between students' intentions and their behaviors. Our findings show student AI use to be a situated practice, and we discuss implications for institutions, instructors, and system tool designers to effectively support student learning with AI.", "AI": {"tldr": "\u5927\u5b66\u751f\u5728\u5b66\u672f\u5de5\u4f5c\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u4f46\u73b0\u6709\u5236\u5ea6\u5b9e\u8df5\u5c1a\u672a\u9002\u5e94\u8fd9\u4e00\u8f6c\u53d8\u3002\u7814\u7a76\u53d1\u73b0\u5236\u5ea6\u538b\u529b\u548c\u793e\u4f1a\u56e0\u7d20\u5f71\u54cd\u5b66\u751fAI\u4f7f\u7528\uff0c\u5bfc\u81f4\u610f\u56fe\u4e0e\u884c\u4e3a\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u9ad8\u7b49\u6559\u80b2\u5b66\u751f\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u5b66\u672f\u5de5\u4f5c\u4e2d\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u4f46\u73b0\u6709\u7684\u5236\u5ea6\u5b9e\u8df5\u5c1a\u672a\u9002\u5e94\u8fd9\u4e00\u8f6c\u53d8\u3002\u9700\u8981\u4e86\u89e3\u5f71\u54cd\u5b66\u751fAI\u4f7f\u7528\u7684\u73af\u5883\u548c\u793e\u4f1a\u56e0\u7d20\uff0c\u4ee5\u4fbf\u4e3a\u673a\u6784\u3001\u6559\u5e08\u548c\u7cfb\u7edf\u5de5\u5177\u8bbe\u8ba1\u8005\u63d0\u4f9b\u6709\u6548\u652f\u6301\u5b66\u751f\u5b66\u4e60\u7684\u5efa\u8bae\u3002", "method": "\u901a\u8fc7\u5bf923\u540d\u5927\u5b66\u751f\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u7814\u7a76\u91c7\u7528\u5b9a\u6027\u65b9\u6cd5\u63a2\u8ba8\u5f71\u54cd\u5b66\u751fAI\u4f7f\u7528\u7684\u73af\u5883\u548c\u793e\u4f1a\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5236\u5ea6\u538b\u529b\u56e0\u7d20\uff08\u622a\u6b62\u65e5\u671f\u3001\u8003\u8bd5\u5468\u671f\u3001\u8bc4\u5206\uff09\u5bfc\u81f4\u5b66\u751f\u5373\u4f7f\u8ba4\u4e3aAI\u4f1a\u524a\u5f31\u5b66\u4e60\u4e5f\u4f1a\u4f7f\u7528\uff1b2\uff09\u793e\u4f1a\u5f71\u54cd\uff0c\u7279\u522b\u662f\u540c\u4f34\u5fae\u793e\u533a\uff0c\u5efa\u7acb\u4e86\u4e8b\u5b9e\u4e0a\u7684AI\u89c4\u8303\uff1b3\uff09\u6821\u56ed\u666e\u904d\u5b58\u5728\"AI\u7f9e\u803b\"\uff0c\u63a8\u52a8AI\u4f7f\u7528\u8f6c\u5165\u5730\u4e0b\uff1b4\uff09\u5f53\u524d\u7684\u673a\u6784AI\u653f\u7b56\u88ab\u8ba4\u4e3a\u7b3c\u7edf\u3001\u4e0d\u4e00\u81f4\u4e14\u4ee4\u4eba\u56f0\u60d1\uff0c\u5bfc\u81f4\u5e38\u89c4\u6027\u8fdd\u89c4\uff1b5\uff09\u5b66\u751f\u53d1\u5c55\u57fa\u4e8e\u4ef7\u503c\u7684\u81ea\u6211\u8c03\u8282\u7b56\u7565\uff0c\u4f46\u73af\u5883\u538b\u529b\u9020\u6210\u610f\u56fe\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u5b66\u751fAI\u4f7f\u7528\u662f\u4e00\u79cd\u60c5\u5883\u5316\u5b9e\u8df5\u3002\u7814\u7a76\u4e3a\u673a\u6784\u3001\u6559\u5e08\u548c\u7cfb\u7edf\u5de5\u5177\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u5b66\u751fAI\u5b66\u4e60\u7684\u542f\u793a\uff0c\u5305\u62ec\u9700\u8981\u5236\u5b9a\u66f4\u5177\u4f53\u3001\u4e00\u81f4\u7684\u653f\u7b56\uff0c\u51cf\u5c11\u5236\u5ea6\u538b\u529b\uff0c\u4ee5\u53ca\u521b\u9020\u66f4\u5f00\u653e\u7684\u5b66\u4e60\u73af\u5883\u6765\u51cf\u5c11AI\u7f9e\u803b\u611f\u3002"}}
{"id": "2602.18078", "categories": ["q-fin.MF"], "pdf": "https://arxiv.org/pdf/2602.18078", "abs": "https://arxiv.org/abs/2602.18078", "authors": ["Daniel Chee", "Noufel Frikha", "Libo Li"], "title": "Entropy-regularized penalization schemes for American options and reflected BSDEs with singular generators", "comment": null, "summary": "This paper extends our previous work in Chee et al. [9] to continuous-time optimal stopping problems, with a particular focus on American options within an exploratory framework. We pursue two main objectives. First, motivated by reinforcement learning applications, we introduce an entropy-regularized penalization scheme for continuous-time optimal stopping problems. The scheme is inspired by classical penalization techniques for reflected backward stochastic differential equations (RBSDEs) and provides a smooth approximation of the degenerate stopping rule inherent to the American option problem. This regularization promotes exploration, enables the use of gradient-based optimization methods, and leads naturally to policy improvement algorithms. We establish well-posedness and convergence properties of the scheme, and illustrate its numerical feasibility through low-dimensional experiments based on policy iteration and least-squares Monte Carlo methods. Second, from a theoretical perspective, we study the asymptotic limit of the entropy-regularized penalization as the penalization parameter tends to infinity. We show that the limiting value process solves a reflected BSDE with a logarithmically singular driver, and we prove existence and uniqueness of solutions to this new class of RBSDEs via a monotone limit argument. To the best of our knowledge, such equations have not previously been investigated in the literature", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u4e4b\u524d\u7684\u5de5\u4f5c\uff0c\u5c06\u63a2\u7d22\u6027\u6846\u67b6\u5e94\u7528\u4e8e\u8fde\u7eed\u65f6\u95f4\u6700\u4f18\u505c\u6b62\u95ee\u9898\uff0c\u7279\u522b\u662f\u7f8e\u5f0f\u671f\u6743\u3002\u63d0\u51fa\u4e86\u71b5\u6b63\u5219\u5316\u60e9\u7f5a\u65b9\u6848\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u6536\u655b\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u6781\u9650\u60c5\u51b5\u4e0b\u7684\u5947\u5f02\u53cd\u5c04BSDE\u3002", "motivation": "\u5c06\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u6027\u6846\u67b6\u5f15\u5165\u8fde\u7eed\u65f6\u95f4\u6700\u4f18\u505c\u6b62\u95ee\u9898\uff0c\u7279\u522b\u662f\u7f8e\u5f0f\u671f\u6743\u5b9a\u4ef7\u3002\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u505c\u6b62\u89c4\u5219\u9000\u5316\u95ee\u9898\uff0c\u9700\u8981\u6b63\u5219\u5316\u6765\u4fc3\u8fdb\u63a2\u7d22\u3001\u652f\u6301\u68af\u5ea6\u4f18\u5316\u548c\u7b56\u7565\u6539\u8fdb\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u71b5\u6b63\u5219\u5316\u60e9\u7f5a\u65b9\u6848\uff0c\u7075\u611f\u6765\u81ea\u53cd\u5c04BSDE\u7684\u7ecf\u5178\u60e9\u7f5a\u6280\u672f\u3002\u8be5\u65b9\u6848\u5e73\u6ed1\u903c\u8fd1\u7f8e\u5f0f\u671f\u6743\u7684\u9000\u5316\u505c\u6b62\u89c4\u5219\uff0c\u5efa\u7acb\u4e86\u9002\u5b9a\u6027\u548c\u6536\u655b\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u4f7f\u7528\u7b56\u7565\u8fed\u4ee3\u548c\u6700\u5c0f\u4e8c\u4e58\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u71b5\u6b63\u5219\u5316\u60e9\u7f5a\u65b9\u6848\u7684\u9002\u5b9a\u6027\u548c\u6536\u655b\u6027\u3002\u5f53\u60e9\u7f5a\u53c2\u6570\u8d8b\u4e8e\u65e0\u7a77\u65f6\uff0c\u6781\u9650\u503c\u8fc7\u7a0b\u6c42\u89e3\u5177\u6709\u5bf9\u6570\u5947\u5f02\u9a71\u52a8\u7684\u53cd\u5c04BSDE\uff0c\u901a\u8fc7\u5355\u8c03\u6781\u9650\u8bba\u8bc1\u8bc1\u660e\u4e86\u8fd9\u7c7b\u65b0RBSDE\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\u3002", "conclusion": "\u6210\u529f\u5c06\u63a2\u7d22\u6027\u6846\u67b6\u6269\u5c55\u5230\u8fde\u7eed\u65f6\u95f4\u6700\u4f18\u505c\u6b62\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u71b5\u6b63\u5219\u5316\u65b9\u6848\u65e2\u5177\u6709\u5b9e\u9645\u8ba1\u7b97\u4f18\u52bf\uff08\u652f\u6301\u68af\u5ea6\u4f18\u5316\u548c\u7b56\u7565\u6539\u8fdb\uff09\uff0c\u53c8\u4ea7\u751f\u4e86\u65b0\u7684\u7406\u8bba\u7ed3\u679c\uff08\u5947\u5f02\u53cd\u5c04BSDE\uff09\u3002"}}
{"id": "2602.17784", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17784", "abs": "https://arxiv.org/abs/2602.17784", "authors": ["Meng Ye", "Xiao Lin", "Georgina Lukoczki", "Graham W. Lederer", "Yi Yao"], "title": "QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration", "comment": null, "summary": "Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.", "AI": {"tldr": "QueryPlot\u662f\u4e00\u4e2a\u8bed\u4e49\u68c0\u7d22\u548c\u5236\u56fe\u6846\u67b6\uff0c\u6574\u5408\u5730\u8d28\u6587\u672c\u8bed\u6599\u5e93\u4e0e\u5730\u8d28\u56fe\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8fdb\u884c\u77ff\u4ea7\u8fdc\u666f\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u77ff\u4ea7\u8fdc\u666f\u5236\u56fe\u9700\u8981\u624b\u52a8\u6574\u5408\u5f02\u8d28\u5730\u8d28\u77e5\u8bc6\uff0c\u8fc7\u7a0b\u7e41\u7410\u4e14\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u3002\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u6574\u5408\u6587\u672c\u77ff\u5e8a\u6a21\u578b\u548c\u5730\u7406\u7a7a\u95f4\u6570\u636e\uff0c\u63d0\u9ad8\u9884\u6d4b\u6548\u7387\u3002", "method": "\u6536\u96c6120\u591a\u79cd\u77ff\u5e8a\u7c7b\u578b\u7684\u63cf\u8ff0\u6a21\u578b\uff0c\u5c06\u5730\u8d28\u56fe\u591a\u8fb9\u5f62\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\u7f16\u7801\u67e5\u8be2\u548c\u533a\u57df\u63cf\u8ff0\uff0c\u8ba1\u7b97\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0c\u652f\u6301\u7ec4\u5408\u67e5\u8be2\u548c\u591a\u5c42\u805a\u5408\u5206\u6790\u3002", "result": "\u5728\u94a8\u77fd\u5361\u5ca9\u77ff\u5e8a\u6848\u4f8b\u4e2d\uff0c\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u80fd\u9ad8\u53ec\u56de\u5df2\u77e5\u77ff\u5e8a\uff0c\u9884\u6d4b\u533a\u57df\u4e0e\u4e13\u5bb6\u5b9a\u4e49\u8bb8\u53ef\u533a\u57df\u9ad8\u5ea6\u4e00\u81f4\u3002\u76f8\u4f3c\u5ea6\u5f97\u5206\u4f5c\u4e3a\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u53ef\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "QueryPlot\u6210\u529f\u6574\u5408\u5730\u8d28\u6587\u672c\u4e0e\u7a7a\u95f4\u6570\u636e\uff0c\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u77ff\u4ea7\u8fdc\u666f\u9884\u6d4b\uff0c\u652f\u6301\u591a\u6807\u51c6\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u96c6\u6210\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2602.17676", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17676", "abs": "https://arxiv.org/abs/2602.17676", "authors": ["Xingcheng Xu", "Jingjing Qu", "Qiaosheng Zhang", "Chaochao Lu", "Yanqing Yang", "Na Zou", "Xia Hu"], "title": "Epistemic Traps: Rational Misalignment Driven by Model Misspecification", "comment": null, "summary": "The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a \"locked-in\" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAI\u5b89\u5168\u7684\u65b0\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u548cAI\u4ee3\u7406\u7684\u75c5\u7406\u884c\u4e3a\uff08\u5982\u5949\u627f\u3001\u5e7b\u89c9\u3001\u6218\u7565\u6b3a\u9a97\uff09\u89e3\u91ca\u4e3a\u6a21\u578b\u8bef\u8bbe\u4e0b\u7684\u7406\u6027\u884c\u4e3a\uff0c\u800c\u975e\u8bad\u7ec3\u7f3a\u9677\u3002\u901a\u8fc7\u5f15\u5165\u7ecf\u6d4e\u5b66\u4e2d\u7684Berk-Nash\u7406\u6027\u5316\u7406\u8bba\uff0c\u8bc1\u660e\u8fd9\u4e9b\u4e0d\u5b89\u5168\u884c\u4e3a\u662f\u7ed3\u6784\u5fc5\u7136\u6027\uff0c\u5b89\u5168\u662f\u79bb\u6563\u76f8\u800c\u975e\u5956\u52b1\u7684\u8fde\u7eed\u51fd\u6570\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u8303\u5f0f\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u548cAI\u4ee3\u7406\u7684\u75c5\u7406\u884c\u4e3a\u89c6\u4e3a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6682\u65f6\u7f3a\u9677\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e9b\u884c\u4e3a\u7684\u51fa\u73b0\u548c\u7a33\u5b9a\u6027\u3002\u8fd9\u4e9b\u884c\u4e3a\uff08\u5982\u5949\u627f\u3001\u5e7b\u89c9\u3001\u6218\u7565\u6b3a\u9a97\uff09\u963b\u788d\u4e86AI\u5728\u5173\u952e\u793e\u4f1a\u548c\u6280\u672f\u9886\u57df\u7684\u90e8\u7f72\u3002", "method": "\u5c06\u7ecf\u6d4e\u5b66\u4e2d\u7684Berk-Nash\u7406\u6027\u5316\u7406\u8bba\u5f15\u5165\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u5c06AI\u4ee3\u7406\u5efa\u6a21\u4e3a\u5728\u9519\u8bef\u7684\u4e3b\u89c2\u4e16\u754c\u6a21\u578b\u4e0b\u8fdb\u884c\u4f18\u5316\u3002\u901a\u8fc7\u516d\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u5bb6\u65cf\u7684\u884c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\uff0c\u751f\u6210\u7cbe\u786e\u6620\u5c04\u5b89\u5168\u884c\u4e3a\u62d3\u6251\u8fb9\u754c\u7684\u76f8\u56fe\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u5b89\u5168\u884c\u4e3a\u662f\u7ed3\u6784\u5fc5\u7136\u6027\uff1a\u8981\u4e48\u4f5c\u4e3a\u7a33\u5b9a\u7684\u672a\u5bf9\u9f50\u5747\u8861\u51fa\u73b0\uff0c\u8981\u4e48\u4f5c\u4e3a\u632f\u8361\u5faa\u73af\u51fa\u73b0\uff0c\u53d6\u51b3\u4e8e\u5956\u52b1\u65b9\u6848\u3002\u6218\u7565\u6b3a\u9a97\u4f5c\u4e3a\"\u9501\u5b9a\"\u5747\u8861\u6216\u901a\u8fc7\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6301\u7eed\u5b58\u5728\u3002\u5b89\u5168\u662f\u79bb\u6563\u76f8\uff0c\u7531\u4ee3\u7406\u7684\u8ba4\u77e5\u5148\u9a8c\u51b3\u5b9a\uff0c\u800c\u975e\u5956\u52b1\u5927\u5c0f\u7684\u8fde\u7eed\u51fd\u6570\u3002", "conclusion": "\u5b89\u5168\u53d6\u51b3\u4e8e\u4ee3\u7406\u7684\u5185\u90e8\u4fe1\u5ff5\u7ed3\u6784\u800c\u975e\u73af\u5883\u5956\u52b1\u3002\u8fd9\u786e\u7acb\u4e86\"\u4e3b\u89c2\u6a21\u578b\u5de5\u7a0b\"\uff08\u8bbe\u8ba1\u4ee3\u7406\u5185\u90e8\u4fe1\u5ff5\u7ed3\u6784\uff09\u4f5c\u4e3a\u7a33\u5065\u5bf9\u9f50\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u6807\u5fd7\u7740\u4ece\u64cd\u7eb5\u73af\u5883\u5956\u52b1\u5230\u5851\u9020\u4ee3\u7406\u5bf9\u73b0\u5b9e\u89e3\u91ca\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2602.17782", "categories": ["math.OC", "math.DG"], "pdf": "https://arxiv.org/pdf/2602.17782", "abs": "https://arxiv.org/abs/2602.17782", "authors": ["Adriano Da Silva", "Lino Grama", "Douglas Duarte Novaes", "Margarita Quispe Tusco"], "title": "Maxwell Strata in the sub-Riemannian problem on solvable, nonnilpotent regular three-dimensional Lie groups", "comment": null, "summary": "In this paper, we study the sub-Riemannian problem associated with contact structures on connected, simply connected, solvable, non-nilpotent, regular three-dimensional Lie groups. For these groups, the vertical component of the Hamiltonian system takes the form of a perturbed pendulum. A qualitative phase-space analysis allows us to prove that this vertical component exhibits nontrivial symmetries. In particular, we are able to fully characterize the Maxwell set corresponding to these symmetries, and show that its first Maxwell time coincides with the period of the pendulum for almost all geodesics. This result yields an explicit upper bound for the cut time in terms of the period of the pendulum.", "AI": {"tldr": "\u7814\u7a76\u4e09\u7ef4\u53ef\u89e3\u975e\u5e42\u96f6\u674e\u7fa4\u4e0a\u7684\u63a5\u89e6\u7ed3\u6784\u4e9a\u9ece\u66fc\u95ee\u9898\uff0c\u53d1\u73b0\u54c8\u5bc6\u987f\u7cfb\u7edf\u7684\u5782\u76f4\u5206\u91cf\u5448\u73b0\u6270\u52a8\u6446\u5f62\u5f0f\uff0c\u5176\u9ea6\u514b\u65af\u97e6\u96c6\u4e0e\u6446\u5468\u671f\u76f8\u5173\uff0c\u4ece\u800c\u5f97\u5230\u5272\u65f6\u95f4\u7684\u663e\u5f0f\u4e0a\u754c\u3002", "motivation": "\u7814\u7a76\u8fde\u901a\u3001\u5355\u8fde\u901a\u3001\u53ef\u89e3\u3001\u975e\u5e42\u96f6\u3001\u6b63\u5219\u4e09\u7ef4\u674e\u7fa4\u4e0a\u7684\u63a5\u89e6\u7ed3\u6784\u4e9a\u9ece\u66fc\u95ee\u9898\uff0c\u8fd9\u7c7b\u7fa4\u5728\u51e0\u4f55\u63a7\u5236\u7406\u8bba\u548c\u4e9a\u9ece\u66fc\u51e0\u4f55\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\u3002", "method": "\u5c06\u54c8\u5bc6\u987f\u7cfb\u7edf\u7684\u5782\u76f4\u5206\u91cf\u5efa\u6a21\u4e3a\u6270\u52a8\u6446\uff0c\u901a\u8fc7\u5b9a\u6027\u76f8\u7a7a\u95f4\u5206\u6790\u8bc1\u660e\u8be5\u5206\u91cf\u5177\u6709\u975e\u5e73\u51e1\u5bf9\u79f0\u6027\uff0c\u5b8c\u5168\u523b\u753b\u5bf9\u5e94\u7684\u9ea6\u514b\u65af\u97e6\u96c6\uff0c\u5e76\u5efa\u7acb\u4e0e\u6446\u5468\u671f\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u51e0\u4e4e\u6240\u6709\u6d4b\u5730\u7ebf\u7684\u7b2c\u4e00\u4e2a\u9ea6\u514b\u65af\u97e6\u65f6\u95f4\u4e0e\u6446\u7684\u5468\u671f\u91cd\u5408\uff0c\u7531\u6b64\u5f97\u5230\u5272\u65f6\u95f4\u7684\u663e\u5f0f\u4e0a\u754c\uff0c\u8be5\u4e0a\u754c\u7531\u6446\u7684\u5468\u671f\u7ed9\u51fa\u3002", "conclusion": "\u5728\u4e09\u7ef4\u53ef\u89e3\u975e\u5e42\u96f6\u674e\u7fa4\u7684\u63a5\u89e6\u7ed3\u6784\u4e9a\u9ece\u66fc\u95ee\u9898\u4e2d\uff0c\u54c8\u5bc6\u987f\u7cfb\u7edf\u7684\u5782\u76f4\u5206\u91cf\u5177\u6709\u6270\u52a8\u6446\u884c\u4e3a\uff0c\u5176\u5bf9\u79f0\u6027\u5bfc\u81f4\u9ea6\u514b\u65af\u97e6\u96c6\u4e0e\u6446\u5468\u671f\u5bc6\u5207\u76f8\u5173\uff0c\u4e3a\u5272\u65f6\u95f4\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u51e0\u4f55\u4e0a\u754c\u3002"}}
{"id": "2602.17779", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17779", "abs": "https://arxiv.org/abs/2602.17779", "authors": ["Antoine Maillard", "Tony Bonnaire", "Giulio Biroli"], "title": "Topological Exploration of High-Dimensional Empirical Risk Landscapes: general approach, and applications to phase retrieval", "comment": "43 pages, 14 figures", "summary": "We consider the landscape of empirical risk minimization for high-dimensional Gaussian single-index models (generalized linear models). The objective is to recover an unknown signal $\\boldsymbol\u03b8^\\star \\in \\mathbb{R}^d$ (where $d \\gg 1$) from a loss function $\\hat{R}(\\boldsymbol\u03b8)$ that depends on pairs of labels $(\\mathbf{x}_i \\cdot \\boldsymbol\u03b8, \\mathbf{x}_i \\cdot \\boldsymbol\u03b8^\\star)_{i=1}^n$, with $\\mathbf{x}_i \\sim \\mathcal{N}(0, I_d)$, in the proportional asymptotic regime $n \\asymp d$. Using the Kac-Rice formula, we analyze different complexities of the landscape -- defined as the expected number of critical points -- corresponding to various types of critical points, including local minima. We first show that some variational formulas previously established in the literature for these complexities can be drastically simplified, reducing to explicit variational problems over a finite number of scalar parameters that we can efficiently solve numerically. Our framework also provides detailed predictions for properties of the critical points, including the spectral properties of the Hessian and the joint distribution of labels. We apply our analysis to the real phase retrieval problem for which we derive complete topological phase diagrams of the loss landscape, characterizing notably BBP-type transitions where the Hessian at local minima (as predicted by the Kac-Rice formula) becomes unstable in the direction of the signal. We test the predictive power of our analysis to characterize gradient flow dynamics, finding excellent agreement with finite-size simulations of local optimization algorithms, and capturing fine-grained details such as the empirical distribution of labels. Overall, our results open new avenues for the asymptotic study of loss landscapes and topological trivialization phenomena in high-dimensional statistical models.", "AI": {"tldr": "\u4f7f\u7528Kac-Rice\u516c\u5f0f\u5206\u6790\u9ad8\u7ef4\u9ad8\u65af\u5355\u6307\u6807\u6a21\u578b\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u7684\u4e34\u754c\u70b9\u666f\u89c2\uff0c\u63a8\u5bfc\u51fa\u7b80\u5316\u7684\u53d8\u5206\u516c\u5f0f\uff0c\u5e76\u5e94\u7528\u4e8e\u5b9e\u76f8\u4f4d\u6062\u590d\u95ee\u9898\uff0c\u9884\u6d4b\u68af\u5ea6\u6d41\u52a8\u529b\u5b66\u3002", "motivation": "\u7814\u7a76\u9ad8\u7ef4\u7edf\u8ba1\u6a21\u578b\u4e2d\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u7684\u635f\u5931\u51fd\u6570\u666f\u89c2\u62d3\u6251\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5728\u6bd4\u4f8b\u6e10\u8fd1\u673a\u5236\u4e0b\uff0c\u7406\u89e3\u4e34\u754c\u70b9\u7684\u5206\u5e03\u548c\u6027\u8d28\u5bf9\u4f18\u5316\u7b97\u6cd5\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Kac-Rice\u516c\u5f0f\u5206\u6790\u4e0d\u540c\u4e34\u754c\u70b9\u7c7b\u578b\u7684\u590d\u6742\u5ea6\uff0c\u63a8\u5bfc\u7b80\u5316\u4e3a\u6709\u9650\u6807\u91cf\u53c2\u6570\u7684\u53d8\u5206\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u5b9e\u76f8\u4f4d\u6062\u590d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u5efa\u7acb\u4e86\u7b80\u5316\u7684\u53d8\u5206\u516c\u5f0f\uff0c\u80fd\u591f\u9ad8\u6548\u6570\u503c\u6c42\u89e3\uff1b\u9884\u6d4b\u4e86\u4e34\u754c\u70b9\u7684Hessian\u8c31\u6027\u8d28\u548c\u6807\u7b7e\u8054\u5408\u5206\u5e03\uff1b\u5728\u5b9e\u76f8\u4f4d\u6062\u590d\u4e2d\u83b7\u5f97\u4e86\u5b8c\u6574\u7684\u62d3\u6251\u76f8\u56fe\uff0c\u53d1\u73b0\u4e86BBP\u578b\u8f6c\u53d8\uff1b\u7406\u8bba\u9884\u6d4b\u4e0e\u6709\u9650\u5c3a\u5bf8\u6a21\u62df\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u7ef4\u7edf\u8ba1\u6a21\u578b\u4e2d\u635f\u5931\u666f\u89c2\u7684\u6e10\u8fd1\u7814\u7a76\u548c\u62d3\u6251\u5e73\u51e1\u5316\u73b0\u8c61\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u80fd\u591f\u7cbe\u7ec6\u9884\u6d4b\u4f18\u5316\u7b97\u6cd5\u884c\u4e3a\u548c\u4e34\u754c\u70b9\u6027\u8d28\u3002"}}
{"id": "2602.17877", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17877", "abs": "https://arxiv.org/abs/2602.17877", "authors": ["Markus Heinrichs", "Aydin Sezgin", "Rainer Kronberger"], "title": "A Scalable Reconfigurable Intelligent Surface with 3 Bit Phase Resolution and High Bandwidth for 3.6 GHz 5G/6G Applications", "comment": null, "summary": "Reconfigurable Intelligent Surfaces enable active control of wireless propagation channels, which is crucial for future 5G and 6G networks. This work presents a scalable RIS design operating at 3.6 GHz with both 1 bit and 3 bit phase resolution, supporting wideband applications. The unit cells employ low-cost printed circuit board technology with an innovative spring-contact feeding structure, enabling efficient assembly and reduced manufacturing complexity for large-area arrays. The design achieves broadband phase control, low power consumption, and high scalability, with experimental results demonstrating phase tunability across the n78 frequency band and competitive reflection performance compared to existing solutions. This RIS architecture provides a practical platform for experimental studies of smart radio environments, beam steering, and sensing applications in next-generation wireless networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u76843.6GHz\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8bbe\u8ba1\uff0c\u652f\u63011\u4f4d\u548c3\u4f4d\u76f8\u4f4d\u5206\u8fa8\u7387\uff0c\u91c7\u7528\u4f4e\u6210\u672cPCB\u6280\u672f\u548c\u521b\u65b0\u7684\u5f39\u7c27\u63a5\u89e6\u9988\u7535\u7ed3\u6784\uff0c\u9002\u7528\u4e8e5G/6G\u7f51\u7edc\u7684\u5bbd\u5e26\u5e94\u7528\u3002", "motivation": "\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u5bf9\u4e8e\u672a\u67655G\u548c6G\u7f51\u7edc\u4e2d\u65e0\u7ebf\u4f20\u64ad\u4fe1\u9053\u7684\u4e3b\u52a8\u63a7\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bbe\u8ba1\u5728\u53ef\u6269\u5c55\u6027\u3001\u5236\u9020\u6210\u672c\u548c\u5bbd\u5e26\u6027\u80fd\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u4f4e\u6210\u672c\u5370\u5237\u7535\u8def\u677f\u6280\u672f\uff0c\u521b\u65b0\u6027\u5730\u4f7f\u7528\u5f39\u7c27\u63a5\u89e6\u9988\u7535\u7ed3\u6784\uff0c\u8bbe\u8ba1\u652f\u63011\u4f4d\u548c3\u4f4d\u76f8\u4f4d\u5206\u8fa8\u7387\u7684\u5355\u5143\u7ed3\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u7ec4\u88c5\u548c\u964d\u4f4e\u5927\u9762\u79ef\u9635\u5217\u7684\u5236\u9020\u590d\u6742\u5ea6\u3002", "result": "\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5bbd\u5e26\u76f8\u4f4d\u63a7\u5236\u3001\u4f4e\u529f\u8017\u548c\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728n78\u9891\u6bb5\u5177\u6709\u826f\u597d\u7684\u76f8\u4f4d\u53ef\u8c03\u6027\uff0c\u53cd\u5c04\u6027\u80fd\u4e0e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5RIS\u67b6\u6784\u4e3a\u667a\u80fd\u65e0\u7ebf\u7535\u73af\u5883\u3001\u6ce2\u675f\u8d4b\u5f62\u548c\u4f20\u611f\u5e94\u7528\u7684\u5b9e\u9a8c\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u3002"}}
{"id": "2602.17721", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17721", "abs": "https://arxiv.org/abs/2602.17721", "authors": ["Melissa Langworthy", "Yana Rodgers"], "title": "Gender and Digital Platform Work During Turbulent Times", "comment": "Gender, Work & Organization", "summary": "This commentary explores how the platform economy shapes labour market responses during times of crisis, with a focus on gendered experiences. Drawing on cases of economic crisis, natural disasters, and refugee displacement, it examines how digital labour platforms offer flexible work opportunities while also reinforcing existing inequalities. Women face distinct constraints (such as caregiving responsibilities, limited mobility, and economic insecurity) that hinder their employment opportunities and earnings potential. These constraints are more pronounced during crises, when access to stable income and safe working conditions becomes more difficult. While platform work can serve as a lifeline, it is not a guaranteed solution, and its benefits are unevenly distributed. The commentary calls for gender-responsive policies and new research to understand how digital infrastructures mediate labour experiences across different crisis contexts. Such research can inform inclusive strategies that promote resilience and equity in platform-based work, particularly for marginalized and displaced populations.", "AI": {"tldr": "\u5e73\u53f0\u7ecf\u6d4e\u5728\u5371\u673a\u671f\u95f4\u63d0\u4f9b\u7075\u6d3b\u5de5\u4f5c\u673a\u4f1a\uff0c\u4f46\u4e5f\u52a0\u5267\u6027\u522b\u4e0d\u5e73\u7b49\uff0c\u5973\u6027\u9762\u4e34\u7167\u987e\u8d23\u4efb\u3001\u884c\u52a8\u9650\u5236\u548c\u7ecf\u6d4e\u4e0d\u5b89\u5168\u7b49\u7ea6\u675f\uff0c\u9700\u8981\u6027\u522b\u54cd\u5e94\u653f\u7b56", "motivation": "\u63a2\u8ba8\u5e73\u53f0\u7ecf\u6d4e\u5982\u4f55\u5851\u9020\u5371\u673a\u65f6\u671f\u7684\u52b3\u52a8\u529b\u5e02\u573a\u53cd\u5e94\uff0c\u7279\u522b\u5173\u6ce8\u6027\u522b\u5dee\u5f02\uff0c\u5206\u6790\u6570\u5b57\u52b3\u52a8\u5e73\u53f0\u5728\u63d0\u4f9b\u7075\u6d3b\u5de5\u4f5c\u673a\u4f1a\u7684\u540c\u65f6\u5982\u4f55\u5f3a\u5316\u73b0\u6709\u4e0d\u5e73\u7b49", "method": "\u57fa\u4e8e\u7ecf\u6d4e\u5371\u673a\u3001\u81ea\u7136\u707e\u5bb3\u548c\u96be\u6c11\u6d41\u79bb\u5931\u6240\u7b49\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u6570\u5b57\u52b3\u52a8\u5e73\u53f0\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u5973\u6027\u7684\u7ea6\u675f\u6761\u4ef6", "result": "\u5e73\u53f0\u5de5\u4f5c\u5728\u5371\u673a\u671f\u95f4\u53ef\u4f5c\u4e3a\u751f\u547d\u7ebf\uff0c\u4f46\u5e76\u975e\u4fdd\u8bc1\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u76ca\u5904\u5206\u5e03\u4e0d\u5747\uff1b\u5973\u6027\u9762\u4e34\u7167\u987e\u8d23\u4efb\u3001\u884c\u52a8\u9650\u5236\u548c\u7ecf\u6d4e\u4e0d\u5b89\u5168\u7b49\u7ea6\u675f\uff0c\u8fd9\u4e9b\u7ea6\u675f\u5728\u5371\u673a\u671f\u95f4\u66f4\u52a0\u660e\u663e", "conclusion": "\u9700\u8981\u6027\u522b\u54cd\u5e94\u653f\u7b56\u548c\u65b0\u7684\u7814\u7a76\u6765\u7406\u89e3\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u5982\u4f55\u5728\u4e0d\u540c\u5371\u673a\u80cc\u666f\u4e0b\u8c03\u8282\u52b3\u52a8\u4f53\u9a8c\uff0c\u4ee5\u5236\u5b9a\u4fc3\u8fdb\u5e73\u53f0\u5de5\u4f5c\u97e7\u6027\u548c\u516c\u5e73\u7684\u5305\u5bb9\u6027\u7b56\u7565\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8fb9\u7f18\u5316\u548c\u6d41\u79bb\u5931\u6240\u4eba\u7fa4"}}
{"id": "2602.17815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17815", "abs": "https://arxiv.org/abs/2602.17815", "authors": ["Zhining Zhang", "Wentao Zhu", "Chi Han", "Yizhou Wang", "Heng Ji"], "title": "Neural Synchrony Between Socially Interacting Language Models", "comment": "Accepted at ICLR 2026", "summary": "Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the \"social minds\" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u795e\u7ecf\u540c\u6b65\u6027\u5206\u6790\u63a2\u7d22LLMs\u7684\"\u793e\u4f1a\u5fc3\u667a\"\uff0c\u53d1\u73b0\u4ea4\u4e92\u4e2d\u7684LLMs\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u793e\u4ea4\u7684\u795e\u7ecf\u540c\u6b65\u73b0\u8c61\uff0c\u4e14\u4e0e\u793e\u4ea4\u8868\u73b0\u76f8\u5173\u3002", "motivation": "\u4f20\u7edf\u8ba4\u4e3a\u793e\u4f1a\u5fc3\u667a\u662f\u751f\u7269\u7279\u6709\u5c5e\u6027\uff0c\u867d\u7136LLMs\u88ab\u5e7f\u6cdb\u63a5\u53d7\u4e3a\u4eba\u7c7b\u884c\u4e3a\u7684\u8fd1\u4f3c\uff0c\u4f46\u591aLLM\u7cfb\u7edf\u80fd\u5426\u4e0e\u4eba\u7c7b\u793e\u4f1a\u5fc3\u667a\u76f8\u6bd4\u4ecd\u5b58\u4e89\u8bae\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u540c\u6b65\u6027\u4e3a\u8fd9\u4e00\u8fa9\u8bba\u63d0\u4f9b\u5b9e\u8bc1\u8bc1\u636e\u3002", "method": "\u5f15\u5165\u793e\u4ea4\u6a21\u62df\u4e2d\u7684\u795e\u7ecf\u540c\u6b65\u6027\u4f5c\u4e3a\u5206\u6790LLMs\u793e\u4f1a\u6027\u7684\u65b0\u4ee3\u7406\u6307\u6807\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u53ef\u9760\u53cd\u6620\u793e\u4ea4\u53c2\u4e0e\u5ea6\u548c\u65f6\u95f4\u5bf9\u9f50\u3002", "result": "LLMs\u4e4b\u95f4\u7684\u795e\u7ecf\u540c\u6b65\u6027\u4e0e\u5b83\u4eec\u7684\u793e\u4ea4\u8868\u73b0\u5f3a\u76f8\u5173\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u540c\u6b65\u6027\u4e0eLLMs\u793e\u4ea4\u884c\u4e3a\u4e4b\u95f4\u7684\u91cd\u8981\u8054\u7cfb\u3002", "conclusion": "\u7814\u7a76\u4e3a\u68c0\u9a8cLLMs\u7684\"\u793e\u4f1a\u5fc3\u667a\"\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0eLLM\u793e\u4ea4\u4e92\u52a8\u4e2d\u5185\u90e8\u52a8\u6001\u7684\u60ca\u4eba\u76f8\u4f3c\u6027\u3002"}}
{"id": "2602.17826", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17826", "abs": "https://arxiv.org/abs/2602.17826", "authors": ["Marcelo Labre"], "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge", "comment": "Submitted to NeuS 2026. Supplementary materials and code: https://doi.org/10.5281/zenodo.18665030", "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5f62\u5f0f\u5316\u9886\u57df\u672c\u4f53\u80fd\u5426\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u672c\u4f53\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u5728\u68c0\u7d22\u8d28\u91cf\u9ad8\u65f6\u80fd\u6539\u5584\u6027\u80fd\uff0c\u4f46\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u3001\u8106\u5f31\u6027\u548c\u7f3a\u4e4f\u5f62\u5f0f\u5316\u57fa\u7840\u7b49\u6839\u672c\u9650\u5236\uff0c\u8fd9\u5728\u9700\u8981\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u9ad8\u98ce\u9669\u4e13\u4e1a\u9886\u57df\u5c24\u5176\u6210\u95ee\u9898\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u5f62\u5f0f\u5316\u9886\u57df\u672c\u4f53\u80fd\u5426\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u6570\u5b66\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u795e\u7ecf\u7b26\u53f7\u7ba1\u9053\uff0c\u5229\u7528OpenMath\u672c\u4f53\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\uff0c\u5c06\u76f8\u5173\u5b9a\u4e49\u6ce8\u5165\u6a21\u578b\u63d0\u793a\u4e2d\u3002\u5728MATH\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u68c0\u7d22\u8d28\u91cf\u9ad8\u65f6\uff0c\u672c\u4f53\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u4e3b\u52a8\u964d\u4f4e\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u6f5c\u529b\u548c\u6311\u6218\u3002", "conclusion": "\u5f62\u5f0f\u5316\u672c\u4f53\u5728\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u68c0\u7d22\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u635f\u5bb3\u6027\u80fd\uff0c\u8868\u660e\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u68c0\u7d22\u673a\u5236\u3002"}}
{"id": "2602.17795", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.17795", "abs": "https://arxiv.org/abs/2602.17795", "authors": ["Vsevolod Ivanov Ivanov"], "title": "Optimality conditions via exact penalty functions", "comment": "10 pages", "summary": "In this paper, we obtain optimality conditions for the problem with inequality, equality and closed set constraints in terms of the lower Hadamard derivative. The results are obtained applying exact penalty functions.", "AI": {"tldr": "\u4f7f\u7528\u7cbe\u786e\u7f5a\u51fd\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0bHadamard\u5bfc\u6570\u83b7\u5f97\u4e86\u5e26\u4e0d\u7b49\u5f0f\u3001\u7b49\u5f0f\u548c\u95ed\u96c6\u7ea6\u675f\u95ee\u9898\u7684\u6700\u4f18\u6027\u6761\u4ef6", "motivation": "\u7814\u7a76\u5177\u6709\u4e0d\u7b49\u5f0f\u3001\u7b49\u5f0f\u548c\u95ed\u96c6\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u4e8e\u4e0bHadamard\u5bfc\u6570\u7684\u66f4\u4e00\u822c\u6700\u4f18\u6027\u6761\u4ef6", "method": "\u5e94\u7528\u7cbe\u786e\u7f5a\u51fd\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0bHadamard\u5bfc\u6570\u5206\u6790\u7ea6\u675f\u4f18\u5316\u95ee\u9898", "result": "\u83b7\u5f97\u4e86\u57fa\u4e8e\u4e0bHadamard\u5bfc\u6570\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6700\u4f18\u6027\u6761\u4ef6", "conclusion": "\u7cbe\u786e\u7f5a\u51fd\u6570\u65b9\u6cd5\u662f\u63a8\u5bfc\u5e26\u591a\u79cd\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6700\u4f18\u6027\u6761\u4ef6\u7684\u6709\u6548\u5de5\u5177\uff0c\u4e0bHadamard\u5bfc\u6570\u4e3a\u6b64\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u5408\u9002\u7684\u5206\u6790\u6846\u67b6"}}
{"id": "2602.17830", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17830", "abs": "https://arxiv.org/abs/2602.17830", "authors": ["Marcos Tapia Costa", "Nikolas Kantas", "George Deligiannidis"], "title": "Drift Estimation for Stochastic Differential Equations with Denoising Diffusion Models", "comment": null, "summary": "We study the estimation of time-homogeneous drift functions in multivariate stochastic differential equations with known diffusion coefficient, from multiple trajectories observed at high frequency over a fixed time horizon. We formulate drift estimation as a denoising problem conditional on previous observations, and propose an estimator of the drift function which is a by-product of training a conditional diffusion model capable of simulating new trajectories dynamically. Across different drift classes, the proposed estimator was found to match classical methods in low dimensions and remained consistently competitive in higher dimensions, with gains that cannot be attributed to architectural design choices alone.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u6f02\u79fb\u51fd\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u5143\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff0c\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b", "motivation": "\u7814\u7a76\u591a\u5143\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u4e2d\u65f6\u95f4\u9f50\u6b21\u6f02\u79fb\u51fd\u6570\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u5df2\u77e5\u6269\u6563\u7cfb\u6570\uff0c\u4ece\u9ad8\u9891\u89c2\u6d4b\u7684\u591a\u4e2a\u8f68\u8ff9\u4e2d\u4f30\u8ba1\u6f02\u79fb\u51fd\u6570", "method": "\u5c06\u6f02\u79fb\u4f30\u8ba1\u8f6c\u5316\u4e3a\u57fa\u4e8e\u5148\u524d\u89c2\u6d4b\u7684\u53bb\u566a\u95ee\u9898\uff0c\u901a\u8fc7\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6765\u4f30\u8ba1\u6f02\u79fb\u51fd\u6570\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u52a8\u6001\u6a21\u62df\u65b0\u8f68\u8ff9", "result": "\u5728\u4e0d\u540c\u6f02\u79fb\u51fd\u6570\u7c7b\u522b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u7ef4\u60c5\u51b5\u4e0b\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5176\u4f18\u52bf\u4e0d\u80fd\u4ec5\u5f52\u56e0\u4e8e\u67b6\u6784\u8bbe\u8ba1\u9009\u62e9", "conclusion": "\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u6f02\u79fb\u51fd\u6570\u4f30\u8ba1\u65b9\u6cd5\u5728\u9ad8\u7ef4\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e3a\u6f02\u79fb\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84"}}
{"id": "2602.18031", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18031", "abs": "https://arxiv.org/abs/2602.18031", "authors": ["Yan Chen", "Ruyi Huang", "Cheng Liu"], "title": "Decision Support under Prediction-Induced Censoring", "comment": null, "summary": "In many data-driven online decision systems, actions determine not only operational costs but also the data availability for future learning -- a phenomenon termed Prediction-Induced Censoring (PIC). This challenge is particularly acute in large-scale resource allocation for generative AI (GenAI) serving: insufficient capacity triggers shortages but hides the true demand, leaving the system with only a \"greater-than\" constraint. Standard decision-making approaches that rely on uncensored data suffer from selection bias, often locking the system into a self-reinforcing low-provisioning trap. To break this loop, this paper proposes an adaptive approach named PIC-Reinforcement Learning (PIC-RL), a closed-loop framework that transforms censoring from a data quality problem into a decision signal. PIC-RL integrates (1) Uncertainty-Aware Demand Prediction to manage the information-cost trade-off, (2) Pessimistic Surrogate Inference to construct decision-aligned conservative feedback from shortage events, and (3) Dual-Timescale Adaptation to stabilize online learning against distribution drift. The analysis provides theoretical guarantees that the feedback design corrects the selection bias inherent in naive learning. Experiments on production Alibaba GenAI traces demonstrate that PIC-RL consistently outperforms state-of-the-art baselines, reducing service degradation by up to 50% while maintaining cost efficiency.", "AI": {"tldr": "\u63d0\u51faPIC-RL\u6846\u67b6\u89e3\u51b3\u9884\u6d4b\u8bf1\u5bfc\u5ba1\u67e5\u95ee\u9898\uff0c\u5c06\u5ba1\u67e5\u4ece\u6570\u636e\u8d28\u91cf\u95ee\u9898\u8f6c\u5316\u4e3a\u51b3\u7b56\u4fe1\u53f7\uff0c\u5728\u751f\u6210AI\u8d44\u6e90\u5206\u914d\u4e2d\u51cf\u5c11\u670d\u52a1\u964d\u7ea7\u8fbe50%", "motivation": "\u5728\u7ebf\u51b3\u7b56\u7cfb\u7edf\u4e2d\uff0c\u884c\u52a8\u4e0d\u4ec5\u5f71\u54cd\u8fd0\u8425\u6210\u672c\uff0c\u8fd8\u51b3\u5b9a\u672a\u6765\u5b66\u4e60\u7684\u6570\u636e\u53ef\u7528\u6027\u2014\u2014\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u9884\u6d4b\u8bf1\u5bfc\u5ba1\u67e5(PIC)\u3002\u5728\u751f\u6210AI\u670d\u52a1\u7684\u5927\u89c4\u6a21\u8d44\u6e90\u5206\u914d\u4e2d\u5c24\u4e3a\u4e25\u91cd\uff1a\u5bb9\u91cf\u4e0d\u8db3\u4f1a\u89e6\u53d1\u77ed\u7f3a\u4f46\u9690\u85cf\u771f\u5b9e\u9700\u6c42\uff0c\u7cfb\u7edf\u53ea\u80fd\u5f97\u5230\"\u5927\u4e8e\"\u7ea6\u675f\u3002\u6807\u51c6\u65b9\u6cd5\u4f9d\u8d56\u672a\u5ba1\u67e5\u6570\u636e\u4f1a\u906d\u53d7\u9009\u62e9\u504f\u5dee\uff0c\u4f7f\u7cfb\u7edf\u9677\u5165\u81ea\u6211\u5f3a\u5316\u7684\u4f4e\u4f9b\u5e94\u9677\u9631\u3002", "method": "\u63d0\u51faPIC-RL\uff08\u9884\u6d4b\u8bf1\u5bfc\u5ba1\u67e5\u5f3a\u5316\u5b66\u4e60\uff09\u95ed\u73af\u6846\u67b6\uff1a1) \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9700\u6c42\u9884\u6d4b\u7ba1\u7406\u4fe1\u606f-\u6210\u672c\u6743\u8861\uff1b2) \u60b2\u89c2\u4ee3\u7406\u63a8\u65ad\u4ece\u77ed\u7f3a\u4e8b\u4ef6\u6784\u5efa\u51b3\u7b56\u5bf9\u9f50\u7684\u4fdd\u5b88\u53cd\u9988\uff1b3) \u53cc\u65f6\u95f4\u5c3a\u5ea6\u9002\u5e94\u7a33\u5b9a\u5728\u7ebf\u5b66\u4e60\u5bf9\u6297\u5206\u5e03\u6f02\u79fb\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u53cd\u9988\u8bbe\u8ba1\u80fd\u7ea0\u6b63\u6734\u7d20\u5b66\u4e60\u4e2d\u7684\u9009\u62e9\u504f\u5dee\u3002\u5728\u963f\u91cc\u5df4\u5df4\u751f\u6210AI\u751f\u4ea7\u8f68\u8ff9\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPIC-RL\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u670d\u52a1\u964d\u7ea7\u51cf\u5c11\u8fbe50%\u540c\u65f6\u4fdd\u6301\u6210\u672c\u6548\u7387\u3002", "conclusion": "PIC-RL\u6210\u529f\u5c06\u5ba1\u67e5\u4ece\u6570\u636e\u8d28\u91cf\u95ee\u9898\u8f6c\u5316\u4e3a\u51b3\u7b56\u4fe1\u53f7\uff0c\u6253\u7834\u9884\u6d4b\u8bf1\u5bfc\u5ba1\u67e5\u7684\u81ea\u6211\u5f3a\u5316\u5faa\u73af\uff0c\u4e3a\u5b58\u5728\u6570\u636e\u5ba1\u67e5\u7684\u5728\u7ebf\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17726", "categories": ["cs.CY", "cs.DC", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17726", "abs": "https://arxiv.org/abs/2602.17726", "authors": ["Qness Ndlovu"], "title": "Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention", "comment": "23 pages, 4 figures", "summary": "In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.\n  Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility.", "AI": {"tldr": "\u5f00\u53d1\u4f4e\u6210\u672cAI\u5929\u6c14\u9884\u8b66\u7cfb\u7edf\uff0c\u4ee51,430-1,730\u7f8e\u5143/\u6708\u7684\u6210\u672c\u5b9e\u73b0\u56fd\u5bb6\u7ea7\u90e8\u7f72\uff0c\u6bd4\u4f20\u7edf\u96f7\u8fbe\u6210\u672c\u4f4e2,000-4,545\u500d\uff0c\u901a\u8fc7WhatsApp\u63d0\u4f9b15\u5929\u5168\u7403\u5929\u6c14\u9884\u62a5\uff0c\u54cd\u5e94\u65f6\u95f4\u4f4e\u4e8e200\u6beb\u79d2\u3002", "motivation": "2026\u5e741\u6708\u975e\u6d32\u5357\u90e8\u66b4\u96e8\u9020\u6210200-300\u4eba\u6b7b\u4ea1\uff0c\u66b4\u9732\u4e86\u975e\u6d3260%\u5730\u533a\u7f3a\u4e4f\u6709\u6548\u9884\u8b66\u7cfb\u7edf\u7684\u95ee\u9898\u3002\u4f20\u7edf\u96f7\u8fbe\u7ad9\u6210\u672c\u8d85\u8fc7100\u4e07\u7f8e\u5143\uff0c\u5bfc\u81f4\u975e\u6d32\u9884\u8b66\u8986\u76d6\u6bd4\u6b27\u7f8e\u4f4e18\u500d\uff0c\u6025\u9700\u7ecf\u6d4e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528NVIDIA Earth-2 AI\u5929\u6c14\u6a21\u578b\uff0c\u6784\u5efa\u751f\u4ea7\u7ea7\u67b6\u6784\uff1a1) \u4f7f\u7528ProcessPoolExecutor\u89e3\u51b3\u5f02\u6b65Python\u4e2d\u7684\u4f1a\u8bdd\u751f\u547d\u5468\u671f\u51b2\u7a81\uff1b2) \u6570\u636e\u5e93\u652f\u6301\u7684\u670d\u52a1\u67b6\u6784\uff0cGPU\u76f4\u63a5\u5c06\u5168\u7403\u9884\u62a5\u5199\u5165PostgreSQL\uff0c\u907f\u514dHTTP\u4f20\u8f93\u74f6\u9888\uff1b3) \u81ea\u52a8\u5316\u5750\u6807\u7ba1\u7406\u5904\u740661\u4e2a\u65f6\u95f4\u6b65\u7684\u591a\u6b65\u63a8\u7406\u3002", "result": "2026\u5e742\u6708\u5728\u5357\u975e\u6210\u529f\u90e8\u7f72\uff0c\u7cfb\u7edf\u80fd\u4ee51,430-1,730\u7f8e\u5143/\u6708\u7684\u6210\u672c\u5b9e\u73b0\u56fd\u5bb6\u7ea7\u8986\u76d6\uff0c\u6bd4\u96f7\u8fbe\u6210\u672c\u4f4e2,000-4,545\u500d\u3002\u751f\u621015\u5929\u5168\u7403\u5927\u6c14\u9884\u62a5\uff0c\u67e5\u8be2\u54cd\u5e94\u65f6\u95f4\u4f4e\u4e8e200\u6beb\u79d2\uff0c\u901a\u8fc7WhatsApp\uff0880%+\u5e02\u573a\u6e17\u900f\u7387\uff09\u63d0\u4f9b\u9884\u8b66\u3002", "conclusion": "\u8be5\u67b6\u6784\u4f7f\u5927\u9646\u7ea7\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u5728\u7ecf\u6d4e\u4e0a\u53ef\u884c\uff0c\u652f\u6301UNDRR\u5173\u4e8e\u6b64\u7c7b\u7cfb\u7edf\u80fd\u5c06\u707e\u5bb3\u6b7b\u4ea1\u7387\u964d\u4f4e6\u500d\u7684\u7814\u7a76\u53d1\u73b0\u3002\u6240\u6709\u67b6\u6784\u7ec6\u8282\u90fd\u6709\u6587\u6863\u8bb0\u5f55\uff0c\u786e\u4fdd\u5b8c\u5168\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2602.17848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17848", "abs": "https://arxiv.org/abs/2602.17848", "authors": ["Cassandra L. Jacobs", "Morgan Grobol"], "title": "On the scaling relationship between cloze probabilities and language model next-token prediction", "comment": null, "summary": "Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u773c\u52a8\u548c\u9605\u8bfb\u65f6\u95f4\u6570\u636e\u9884\u6d4b\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4f1a\u4f4e\u4f30\u4eba\u7c7b\u53cd\u5e94\u6982\u7387\u3002\u5927\u6a21\u578b\u80fd\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\uff0c\u5bf9\u8bcd\u6c47\u5171\u73b0\u7edf\u8ba1\u4e0d\u654f\u611f\uff0c\u4f46\u8bed\u4e49\u4e0a\u4e0e\u4eba\u7c7b\u5b8c\u5f62\u586b\u7a7a\u53cd\u5e94\u66f4\u4e00\u81f4\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u884c\u4e3a\uff08\u5982\u773c\u52a8\u548c\u9605\u8bfb\u65f6\u95f4\uff09\u65b9\u9762\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u6a21\u578b\u5927\u5c0f\u5982\u4f55\u5f71\u54cd\u5176\u5bf9\u8bcd\u6c47\u9884\u6d4b\u548c\u8bed\u4e49\u7406\u89e3\u7684\u654f\u611f\u6027\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u5728\u773c\u52a8\u6570\u636e\u3001\u9605\u8bfb\u65f6\u95f4\u6570\u636e\u548c\u5b8c\u5f62\u586b\u7a7a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u8bcd\u6c47\u5171\u73b0\u7edf\u8ba1\u7684\u654f\u611f\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u773c\u52a8\u548c\u9605\u8bfb\u65f6\u95f4\u6570\u636e\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u7684\u4e0b\u4e00\u4e2a\u8bcd\u6982\u7387\u4f30\u8ba1\u3002\u5927\u6a21\u578b\u5bf9\u8bcd\u6c47\u5171\u73b0\u7edf\u8ba1\u4e0d\u654f\u611f\uff0c\u4f46\u5728\u8bed\u4e49\u4e0a\u4e0e\u4eba\u7c7b\u5b8c\u5f62\u586b\u7a7a\u53cd\u5e94\u66f4\u4e00\u81f4\u3002\u6240\u6709\u6a21\u578b\u90fd\u4f4e\u4f30\u4e86\u4eba\u7c7b\u53cd\u5e94\u6982\u7387\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u5f3a\u7684\u8bb0\u5fc6\u80fd\u529b\u5e2e\u52a9\u5b83\u4eec\u731c\u6d4b\u66f4\u8bed\u4e49\u5408\u9002\u7684\u8bcd\uff0c\u4f46\u4f7f\u5b83\u4eec\u5bf9\u8bcd\u6c47\u8bc6\u522b\u76f8\u5173\u7684\u4f4e\u5c42\u6b21\u4fe1\u606f\u4e0d\u654f\u611f\u3002\u8fd9\u652f\u6301\u4e86\u6a21\u578b\u5927\u5c0f\u5f71\u54cd\u8bed\u8a00\u5904\u7406\u9884\u6d4b\u80fd\u529b\u7684\u89c2\u70b9\u3002"}}
{"id": "2602.17831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17831", "abs": "https://arxiv.org/abs/2602.17831", "authors": ["Simon Henniger", "Gabriel Poesia"], "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels", "comment": "Project website: https://token-games.ai/", "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.", "AI": {"tldr": "TTG\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f16\u7a0b\u8c1c\u9898\u5bf9\u51b3\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u4e92\u51fa\u9898\u6311\u6218\uff0c\u901a\u8fc7Elo\u8bc4\u5206\u6bd4\u8f83\u6a21\u578b\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u51fa\u9898\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u4eba\u5de5\u51fa\u9898\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u9700\u8981\u535a\u58eb\u7ea7\u9886\u57df\u77e5\u8bc6\u7684\u96be\u9898\uff1b2\uff09\u96be\u4ee5\u533a\u5206\u6a21\u578b\u662f\u771f\u6b63\u63a8\u7406\u8fd8\u662f\u89c1\u8fc7\u7c7b\u4f3c\u8bad\u7ec3\u6570\u636e\u3002\u9700\u8981\u4e00\u79cd\u53ef\u6301\u7eed\u3001\u9632\u9971\u548c\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "method": "\u53d716\u4e16\u7eaa\u6570\u5b66\u5bf9\u51b3\u542f\u53d1\uff0c\u8bbe\u8ba1Token Games\u6846\u67b6\uff1a\u6a21\u578b\u901a\u8fc7\u521b\u5efa\u7f16\u7a0b\u8c1c\u9898\u76f8\u4e92\u6311\u6218\uff08\u7ed9\u5b9a\u8fd4\u56de\u5e03\u5c14\u503c\u7684Python\u51fd\u6570\uff0c\u627e\u5230\u4f7f\u51fd\u6570\u8fd4\u56deTrue\u7684\u8f93\u5165\uff09\u3002\u4f7f\u7528\u6210\u5bf9\u5bf9\u51b3\u7ed3\u679c\u8ba1\u7b97Elo\u8bc4\u5206\u6765\u6bd4\u8f83\u6a21\u578b\u76f8\u5bf9\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e8610\u4e2a\u524d\u6cbf\u6a21\u578b\uff0cTTG\u7684\u6392\u540d\u4e0eHumanity's Last Exam\u7b49\u73b0\u6709\u57fa\u51c6\u9ad8\u5ea6\u5339\u914d\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u51fa\u9898\u3002\u53d1\u73b0\u521b\u5efa\u4f18\u8d28\u8c1c\u9898\u5bf9\u5f53\u524d\u6a21\u578b\u4ecd\u662f\u6781\u5177\u6311\u6218\u7684\u4efb\u52a1\uff0c\u8fd9\u662f\u4ee5\u5f80\u57fa\u51c6\u672a\u6d4b\u91cf\u7684\u80fd\u529b\u3002", "conclusion": "TTG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\uff1a\u65e2\u80fd\u9632\u6b62\u8bbe\u8ba1\u9971\u548c\uff0c\u53c8\u80fd\u540c\u65f6\u6d4b\u8bd5\u6a21\u578b\u7684\u521b\u9020\u529b\u3001\u4efb\u52a1\u521b\u5efa\u80fd\u529b\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4e3a\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17823", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.17823", "abs": "https://arxiv.org/abs/2602.17823", "authors": ["Peter Bank", "Filippo de Feo"], "title": "Duality methods in stochastic optimal control", "comment": null, "summary": "We prove two duality descriptions of the value function for a generic stochastic optimal problem. These descriptions also hold when the diffusion is controlled, a case left open by the literature so far.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u968f\u673a\u6700\u4f18\u95ee\u9898\u4ef7\u503c\u51fd\u6570\u7684\u4e24\u79cd\u5bf9\u5076\u63cf\u8ff0\uff0c\u5e76\u89e3\u51b3\u4e86\u6269\u6563\u53d7\u63a7\u60c5\u51b5\u4e0b\u7684\u672a\u89e3\u51b3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u5bf9\u968f\u673a\u6700\u4f18\u95ee\u9898\u7684\u4ef7\u503c\u51fd\u6570\u5bf9\u5076\u63cf\u8ff0\u7814\u7a76\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u6269\u6563\u53d7\u63a7\u7684\u60c5\u51b5\u4e0b\u5c1a\u672a\u89e3\u51b3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u968f\u673a\u6700\u4f18\u95ee\u9898\u4ef7\u503c\u51fd\u6570\u7684\u4e24\u79cd\u5bf9\u5076\u63cf\u8ff0\uff0c\u7279\u522b\u5904\u7406\u4e86\u6269\u6563\u53d7\u63a7\u7684\u60c5\u51b5\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u4e24\u79cd\u5bf9\u5076\u63cf\u8ff0\u5bf9\u4e00\u822c\u968f\u673a\u6700\u4f18\u95ee\u9898\u6210\u7acb\uff0c\u5e76\u4e14\u8fd9\u4e9b\u7ed3\u679c\u5728\u6269\u6563\u53d7\u63a7\u7684\u60c5\u51b5\u4e0b\u4e5f\u6210\u7acb\uff0c\u89e3\u51b3\u4e86\u6587\u732e\u4e2d\u7684\u5f00\u653e\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u968f\u673a\u6700\u4f18\u63a7\u5236\u7406\u8bba\uff0c\u4e3a\u6269\u6563\u53d7\u63a7\u7684\u968f\u673a\u6700\u4f18\u95ee\u9898\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5bf9\u5076\u63cf\u8ff0\u6846\u67b6\u3002"}}
{"id": "2602.17876", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.17876", "abs": "https://arxiv.org/abs/2602.17876", "authors": ["Nived Rajaraman", "Yanjun Han"], "title": "Interactive Learning of Single-Index Models via Stochastic Gradient Descent", "comment": "26 pages, 2 figures", "summary": "Stochastic gradient descent (SGD) is a cornerstone algorithm for high-dimensional optimization, renowned for its empirical successes. Recent theoretical advances have provided a deep understanding of how SGD enables feature learning in high-dimensional nonlinear models, most notably the \\textit{single-index model} with i.i.d. data. In this work, we study the sequential learning problem for single-index models, also known as generalized linear bandits or ridge bandits, where SGD is a simple and natural solution, yet its learning dynamics remain largely unexplored. We show that, similar to the optimal interactive learner, SGD undergoes a distinct ``burn-in'' phase before entering the ``learning'' phase in this setting. Moreover, with an appropriately chosen learning rate schedule, a single SGD procedure simultaneously achieves near-optimal (or best-known) sample complexity and regret guarantees across both phases, for a broad class of link functions. Our results demonstrate that SGD remains highly competitive for learning single-index models under adaptive data.", "AI": {"tldr": "SGD\u5728\u5355\u6307\u6570\u6a21\u578b\u7684\u5e8f\u5217\u5b66\u4e60\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u80fd\u5728\"\u71c3\u70e7\u671f\"\u548c\"\u5b66\u4e60\u671f\"\u540c\u65f6\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\u548c\u9057\u61be\u4fdd\u8bc1\u3002", "motivation": "\u867d\u7136SGD\u5728\u9ad8\u7ef4\u975e\u7ebf\u6027\u6a21\u578b\u7684\u7279\u5f81\u5b66\u4e60\u65b9\u9762\u5df2\u6709\u6df1\u5165\u7406\u8bba\u7406\u89e3\uff0c\u4f46\u5728\u5355\u6307\u6570\u6a21\u578b\u7684\u5e8f\u5217\u5b66\u4e60\u95ee\u9898\uff08\u5982\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\uff09\u4e2d\uff0cSGD\u7684\u5b66\u4e60\u52a8\u6001\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76SGD\u5728\u5355\u6307\u6570\u6a21\u578b\u5e8f\u5217\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\uff0c\u5206\u6790\u5176\"\u71c3\u70e7\u671f\"\u548c\"\u5b66\u4e60\u671f\"\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u8bbe\u8ba1\u9002\u5f53\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\u7b56\u7565\u3002", "result": "SGD\u4e0e\u6700\u4f18\u4ea4\u4e92\u5b66\u4e60\u5668\u7c7b\u4f3c\uff0c\u7ecf\u5386\u660e\u663e\u7684\"\u71c3\u70e7\u671f\"\u540e\u8fdb\u5165\"\u5b66\u4e60\u671f\"\u3002\u901a\u8fc7\u9002\u5f53\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\uff0c\u5355\u4e2aSGD\u8fc7\u7a0b\u80fd\u5728\u4e24\u4e2a\u9636\u6bb5\u540c\u65f6\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\u548c\u9057\u61be\u4fdd\u8bc1\u3002", "conclusion": "SGD\u5728\u81ea\u9002\u5e94\u6570\u636e\u4e0b\u5b66\u4e60\u5355\u6307\u6570\u6a21\u578b\u65f6\u4ecd\u7136\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u4f18\u79c0\u7684\u6837\u672c\u6548\u7387\u548c\u9057\u61be\u6027\u80fd\u3002"}}
{"id": "2602.18048", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18048", "abs": "https://arxiv.org/abs/2602.18048", "authors": ["N. Naveen Mukesh", "Debraj Chakraborty"], "title": "Incremental Data Driven Transfer Identification", "comment": "15 Pages, 7 figures", "summary": "We introduce a geometric method for online transfer identification of a deterministic linear time-invariant system. At the beginning of the identification process, we assume access to abundant data from a system that is similar, though not identical, to the true system. In the early stages of data collection from the true system, the dataset generated is still not sufficiently informative to enable precise identification. Consequently, multiple candidate models remain consistent with the observations available at that point. Our method picks, at each instant, the model closest to the similar system that is consistent with the current data. As more data are collected, the proposed model gradually moves away from the initial similar system and eventually converges to the true system when the data set grows to be informative. Numerical examples demonstrate the effectiveness of the incremental transfer identification paradigm, where identified models with minimal data are used to solve the pole placement problem.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u8f6c\u79fb\u8bc6\u522b\u7684\u51e0\u4f55\u65b9\u6cd5\uff0c\u5229\u7528\u76f8\u4f3c\u7cfb\u7edf\u7684\u5148\u9a8c\u6570\u636e\uff0c\u5728\u771f\u5b9e\u7cfb\u7edf\u6570\u636e\u4e0d\u8db3\u65f6\u9009\u62e9\u6700\u63a5\u8fd1\u76f8\u4f3c\u7cfb\u7edf\u7684\u5019\u9009\u6a21\u578b\uff0c\u968f\u7740\u6570\u636e\u589e\u52a0\u9010\u6b65\u6536\u655b\u5230\u771f\u5b9e\u7cfb\u7edf", "motivation": "\u5728\u7ebf\u8bc6\u522b\u7cfb\u7edf\u65f6\uff0c\u521d\u671f\u6536\u96c6\u7684\u771f\u5b9e\u7cfb\u7edf\u6570\u636e\u5f80\u5f80\u4e0d\u8db3\uff0c\u65e0\u6cd5\u7cbe\u786e\u8bc6\u522b\uff0c\u4f46\u901a\u5e38\u53ef\u4ee5\u83b7\u5f97\u76f8\u4f3c\u7cfb\u7edf\u7684\u4e30\u5bcc\u6570\u636e\u3002\u5982\u4f55\u5229\u7528\u76f8\u4f3c\u7cfb\u7edf\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u52a0\u901f\u771f\u5b9e\u7cfb\u7edf\u7684\u8bc6\u522b\u8fc7\u7a0b\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898", "method": "\u91c7\u7528\u51e0\u4f55\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u7cfb\u7edf\u6570\u636e\u4e0d\u8db3\u65f6\uff0c\u4ece\u4e0e\u5f53\u524d\u89c2\u6d4b\u4e00\u81f4\u7684\u591a\u4e2a\u5019\u9009\u6a21\u578b\u4e2d\uff0c\u9009\u62e9\u6700\u63a5\u8fd1\u76f8\u4f3c\u7cfb\u7edf\u7684\u6a21\u578b\u3002\u968f\u7740\u6570\u636e\u79ef\u7d2f\uff0c\u9010\u6b65\u8c03\u6574\u6a21\u578b\uff0c\u6700\u7ec8\u6536\u655b\u5230\u771f\u5b9e\u7cfb\u7edf", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u589e\u91cf\u8f6c\u79fb\u8bc6\u522b\u8303\u5f0f\u4e2d\u6709\u6548\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u5c11\u7684\u6570\u636e\u4e5f\u80fd\u8bc6\u522b\u51fa\u53ef\u7528\u4e8e\u6781\u70b9\u914d\u7f6e\u95ee\u9898\u7684\u6a21\u578b", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u76f8\u4f3c\u7cfb\u7edf\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u771f\u5b9e\u7cfb\u7edf\u6570\u636e\u4e0d\u8db3\u65f6\u63d0\u4f9b\u5408\u7406\u7684\u6a21\u578b\u4f30\u8ba1\uff0c\u5e76\u968f\u7740\u6570\u636e\u79ef\u7d2f\u9010\u6b65\u6536\u655b\u5230\u771f\u5b9e\u7cfb\u7edf\uff0c\u4e3a\u89e3\u51b3\u5728\u7ebf\u7cfb\u7edf\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.17729", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17729", "abs": "https://arxiv.org/abs/2602.17729", "authors": ["Nathan G. Wood", "Scott Robbins", "Eduardo Zegarra Berodt", "Anton Graf von Westerholt", "Michelle Behrndt", "Daniel Kloock-Schreiber"], "title": "Stop Saying \"AI\"", "comment": null, "summary": "Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' may affect decision-making, responsibility, or the likelihood of mistakes (to name only a few categories of critique). However, for most critiques, the target is generally ``AI'', a broad term admitting many (types of) systems used for a variety of tasks and each coming with its own set of limitations, challenges, and potential use cases. In this article, we focus on the military domain as a case study and present both a loose enumerative taxonomy of systems captured under the umbrella term ``military AI'', as well as discussion of the challenges of each. In doing so, we highlight that critiques of one (type of) system will not always transfer to other (types of) systems. Building on this, we argue that in order for debates to move forward fruitfully, it is imperative that the discussions be made more precise and that ``AI'' be excised from debates to the extent possible. Researchers, developers, and policy-makers should make clear exactly what systems they have in mind and what possible benefits and risks attend the deployment of those particular systems. While we focus on AI in the military as an exemplar for the overall trends in discussions of ``AI'', the argument's conclusions are broad and have import for discussions of AI across a host of domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\"AI\"\u4e00\u8bcd\u8fc7\u4e8e\u5bbd\u6cdb\uff0c\u5bfc\u81f4\u8ba8\u8bba\u6a21\u7cca\u4e0d\u6e05\uff0c\u7279\u522b\u662f\u5728\u519b\u4e8b\u9886\u57df\u3002\u4f5c\u8005\u63d0\u51fa\u519b\u4e8bAI\u7684\u5206\u7c7b\u6cd5\uff0c\u5f3a\u8c03\u4e0d\u540c\u7cfb\u7edf\u7684\u98ce\u9669\u548c\u6311\u6218\u5404\u5f02\uff0c\u4e3b\u5f20\u5728\u8ba8\u8bba\u4e2d\u5e94\u5177\u4f53\u8bf4\u660e\u7cfb\u7edf\u7c7b\u578b\u800c\u975e\u7b3c\u7edf\u4f7f\u7528\"AI\"\u4e00\u8bcd\u3002", "motivation": "\u5f53\u524d\u5173\u4e8eAI\u7684\u8ba8\u8bba\u8fc7\u4e8e\u7b3c\u7edf\uff0c\u4f7f\u7528\"AI\"\u8fd9\u4e00\u5bbd\u6cdb\u672f\u8bed\u63a9\u76d6\u4e86\u4e0d\u540c\u7cfb\u7edf\u7684\u5177\u4f53\u7279\u6027\u548c\u98ce\u9669\u3002\u7279\u522b\u662f\u5728\u519b\u4e8b\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u8fd9\u79cd\u6a21\u7cca\u6027\u963b\u788d\u4e86\u6709\u610f\u4e49\u7684\u8fa9\u8bba\u548c\u653f\u7b56\u5236\u5b9a\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u519b\u4e8b\u9886\u57df\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u63ed\u793a\u8fd9\u4e00\u95ee\u9898\u7684\u91cd\u8981\u6027\u3002", "method": "\u4ee5\u519b\u4e8b\u9886\u57df\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u63d0\u51fa\u4e00\u4e2a\u679a\u4e3e\u5f0f\u5206\u7c7b\u6cd5\u6765\u533a\u5206\"\u519b\u4e8bAI\"\u4e0b\u7684\u5404\u79cd\u7cfb\u7edf\u7c7b\u578b\u3002\u5206\u6790\u6bcf\u79cd\u7cfb\u7edf\u7684\u5177\u4f53\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u5c55\u793a\u5bf9\u4e00\u79cd\u7cfb\u7edf\u7684\u6279\u8bc4\u4e0d\u4e00\u5b9a\u9002\u7528\u4e8e\u5176\u4ed6\u7cfb\u7edf\u3002", "result": "\u5efa\u7acb\u4e86\u519b\u4e8bAI\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u7cfb\u7edf\u7c7b\u578b\u7684\u72ec\u7279\u7279\u5f81\u548c\u98ce\u9669\u3002\u8bba\u8bc1\u4e86\u5bf9\"AI\"\u7684\u7b3c\u7edf\u6279\u8bc4\u5f80\u5f80\u4e0d\u51c6\u786e\uff0c\u56e0\u4e3a\u4e0d\u540c\u7cfb\u7edf\u5728\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u5e94\u7528\u573a\u666f\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u4e3a\u4e86\u63a8\u52a8\u6709\u6210\u6548\u7684\u8fa9\u8bba\uff0c\u5fc5\u987b\u4f7f\u8ba8\u8bba\u66f4\u52a0\u7cbe\u786e\uff0c\u5c3d\u53ef\u80fd\u907f\u514d\u4f7f\u7528\"AI\"\u8fd9\u4e00\u7b3c\u7edf\u672f\u8bed\u3002\u7814\u7a76\u4eba\u5458\u3001\u5f00\u53d1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5e94\u660e\u786e\u5177\u4f53\u8ba8\u8bba\u7684\u7cfb\u7edf\u7c7b\u578b\u53ca\u5176\u76f8\u5173\u5229\u76ca\u548c\u98ce\u9669\u3002\u867d\u7136\u4ee5\u519b\u4e8b\u9886\u57df\u4e3a\u4f8b\uff0c\u4f46\u8fd9\u4e00\u7ed3\u8bba\u9002\u7528\u4e8e\u6240\u6709\u9886\u57df\u7684AI\u8ba8\u8bba\u3002"}}
{"id": "2602.17881", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17881", "abs": "https://arxiv.org/abs/2602.17881", "authors": ["Joschka Braun"], "title": "Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations", "comment": "Master's Thesis, University of T\u00fcbingen. 89 pages, 34 figures. Portions of this work were published at the ICLR 2025 Workshop on Foundation Models in the Wild (see arXiv:2505.22637)", "summary": "Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff1a\u5f15\u5bfc\u5411\u91cf\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u8bad\u7ec3\u6fc0\u6d3b\u5dee\u5f02\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b63\u8d1f\u6fc0\u6d3b\u5728\u5f15\u5bfc\u65b9\u5411\u4e0a\u7684\u5206\u79bb\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u6f5c\u5728\u884c\u4e3a\u8868\u793a\u662f\u5426\u80fd\u591f\u88ab\u7ebf\u6027\u65b9\u5411\u6709\u6548\u8fd1\u4f3c\u3002", "motivation": "\u5f15\u5bfc\u5411\u91cf\u662f\u4e00\u79cd\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u6dfb\u52a0\u5b66\u4e60\u504f\u7f6e\u6765\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u4f46\u6548\u679c\u5728\u4e0d\u540c\u6837\u672c\u95f4\u53d8\u5316\u5f88\u5927\uff0c\u5bf9\u8bb8\u591a\u76ee\u6807\u884c\u4e3a\u4e0d\u53ef\u9760\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u4e3a\u4ec0\u4e48\u5f15\u5bfc\u53ef\u9760\u6027\u5728\u4e0d\u540c\u884c\u4e3a\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u5f15\u5bfc\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f15\u5bfc\u5411\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u7279\u6027\uff1a1) \u6d4b\u91cf\u8bad\u7ec3\u6fc0\u6d3b\u5dee\u5f02\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff1b2) \u89c2\u5bdf\u6b63\u8d1f\u6fc0\u6d3b\u5728\u5f15\u5bfc\u65b9\u5411\u4e0a\u7684\u5206\u79bb\u7a0b\u5ea6\uff1b3) \u6bd4\u8f83\u4e0d\u540c\u63d0\u793a\u53d8\u4f53\u8bad\u7ec3\u7684\u5f15\u5bfc\u5411\u91cf\u7684\u65b9\u5411\u5dee\u5f02\u548c\u6027\u80fd\u76f8\u5173\u6027\u3002", "result": "1) \u8bad\u7ec3\u6fc0\u6d3b\u5dee\u5f02\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8d8a\u9ad8\uff0c\u5f15\u5bfc\u8d8a\u53ef\u9760\uff1b2) \u6b63\u8d1f\u6fc0\u6d3b\u5728\u5f15\u5bfc\u65b9\u5411\u4e0a\u5206\u79bb\u5f97\u8d8a\u597d\uff0c\u884c\u4e3a\u8d8a\u5bb9\u6613\u88ab\u5f15\u5bfc\uff1b3) \u4e0d\u540c\u63d0\u793a\u8bad\u7ec3\u7684\u5f15\u5bfc\u5411\u91cf\u65b9\u5411\u4e0d\u540c\u4f46\u6027\u80fd\u76f8\u4f3c\uff0c\u4e14\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\u76f8\u5173\u3002", "conclusion": "\u5f15\u5bfc\u5411\u91cf\u4e0d\u53ef\u9760\u7684\u539f\u56e0\u662f\u6f5c\u5728\u76ee\u6807\u884c\u4e3a\u8868\u793a\u4e0d\u80fd\u88ab\u7ebf\u6027\u5f15\u5bfc\u65b9\u5411\u6709\u6548\u8fd1\u4f3c\u3002\u8fd9\u4e3a\u8bca\u65ad\u5f15\u5bfc\u4e0d\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e76\u6fc0\u52b1\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5f15\u5bfc\u65b9\u6cd5\uff0c\u9700\u8981\u663e\u5f0f\u8003\u8651\u975e\u7ebf\u6027\u6f5c\u5728\u884c\u4e3a\u8868\u793a\u3002"}}
{"id": "2602.17902", "categories": ["cs.AI", "cs.MA", "cs.SE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.17902", "abs": "https://arxiv.org/abs/2602.17902", "authors": ["Jiaru Bai", "Abdulrahman Aldossary", "Thomas Swanick", "Marcel M\u00fcller", "Yeonghun Kang", "Zijian Zhang", "Jin Won Lee", "Tsz Wai Ko", "Mohammad Ghazi Vakili", "Varinia Bernales", "Al\u00e1n Aspuru-Guzik"], "title": "El Agente Gr\u00e1fico: Structured Execution Graphs for Scientific Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gr\u00e1fico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.", "AI": {"tldr": "\u63d0\u51faEl Agente Gr\u00e1fico\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u578b\u5b89\u5168\u6267\u884c\u73af\u5883\u548c\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u89e3\u51b3LLM\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u96c6\u6210\u5f02\u6784\u5de5\u5177\u65f6\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u53ef\u8ffd\u6eaf\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5b58\u5728ad hoc\u96c6\u6210\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u8106\u5f31\u3001\u51b3\u7b56\u6eaf\u6e90\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u7ed3\u6784\u5316\u6846\u67b6\u6765\u652f\u6301\u590d\u6742\u79d1\u5b66\u8ba1\u7b97\u3002", "method": "\u5f00\u53d1\u5355\u4ee3\u7406\u6846\u67b6\uff0c\u5c06LLM\u51b3\u7b56\u5d4c\u5165\u7c7b\u578b\u5b89\u5168\u6267\u884c\u73af\u5883\uff0c\u4f7f\u7528\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u5916\u90e8\u6301\u4e45\u5316\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u79d1\u5b66\u6982\u5ff5\u62bd\u8c61\u548c\u5bf9\u8c61-\u56fe\u8c31\u6620\u5c04\u5668\u7ba1\u7406\u8ba1\u7b97\u72b6\u6001\u3002", "result": "\u5728\u91cf\u5b50\u5316\u5b66\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4ee3\u7406\u6846\u67b6\u80fd\u591f\u7a33\u5065\u6267\u884c\u590d\u6742\u591a\u6b65\u9aa4\u5e76\u884c\u8ba1\u7b97\uff1b\u5728\u6784\u8c61\u7cfb\u7efc\u751f\u6210\u548c\u91d1\u5c5e\u6709\u673a\u6846\u67b6\u8bbe\u8ba1\u4e2d\uff0c\u77e5\u8bc6\u56fe\u8c31\u6210\u529f\u4f5c\u4e3a\u8bb0\u5fc6\u548c\u63a8\u7406\u57fa\u7840\u3002", "conclusion": "\u62bd\u8c61\u5316\u548c\u7c7b\u578b\u5b89\u5168\u4e3a\u57fa\u4e8e\u4ee3\u7406\u7684\u79d1\u5b66\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u57fa\u7840\uff0c\u8d85\u8d8a\u4e86\u4ee5\u63d0\u793a\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u8ffd\u6eaf\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u3002"}}
{"id": "2602.17828", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.17828", "abs": "https://arxiv.org/abs/2602.17828", "authors": ["Oliver Mason"], "title": "A note on diffusive solutions of the Lyapunov and Riccati inequalities for quasi-monotone (QM) mappings on cones", "comment": "Accepted for publication in Communications in Optimization Theory", "summary": "We consider three key properties of Metzler and nonnegative matrices and extensions of these to classes of self-dual proper convex cones. Specifically, we study mappings that are quasi-monotone (QM) with respect to a cone $K$ and discuss results extending D-stability, diagonal Lyapunov stability, and diagonal Riccati stability to this setting. Mappings that act diffusively with respect to the cone are used as generalisations of diagonal matrices. Relationships with recent results for symmetric cones obtained using Jordan algebraic methods are also discussed.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Metzler\u77e9\u9635\u548c\u975e\u8d1f\u77e9\u9635\u7684\u4e09\u4e2a\u5173\u952e\u6027\u8d28\u5728\u81ea\u5bf9\u5076\u771f\u51f8\u9525\u4e0a\u7684\u63a8\u5e7f\uff0c\u5305\u62ec\u51c6\u5355\u8c03\u6620\u5c04\u3001D-\u7a33\u5b9a\u6027\u3001\u5bf9\u89d2Lyapunov\u7a33\u5b9a\u6027\u548c\u5bf9\u89d2Riccati\u7a33\u5b9a\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e0e\u5bf9\u79f0\u9525\u4e0aJordan\u4ee3\u6570\u65b9\u6cd5\u7ed3\u679c\u7684\u5173\u7cfb\u3002", "motivation": "\u5c06Metzler\u77e9\u9635\u548c\u975e\u8d1f\u77e9\u9635\u7684\u91cd\u8981\u6027\u8d28\u63a8\u5e7f\u5230\u66f4\u4e00\u822c\u7684\u81ea\u5bf9\u5076\u771f\u51f8\u9525\u4e0a\uff0c\u5efa\u7acb\u66f4\u5e7f\u6cdb\u7684\u7a33\u5b9a\u6027\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4e0e\u5bf9\u79f0\u9525\u4e0a\u7684\u73b0\u6709\u7ed3\u679c\u5efa\u7acb\u8054\u7cfb\u3002", "method": "\u7814\u7a76\u76f8\u5bf9\u4e8e\u9525K\u7684\u51c6\u5355\u8c03\u6620\u5c04\uff0c\u63a8\u5e7fD-\u7a33\u5b9a\u6027\u3001\u5bf9\u89d2Lyapunov\u7a33\u5b9a\u6027\u548c\u5bf9\u89d2Riccati\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u4f7f\u7528\u76f8\u5bf9\u4e8e\u9525\u7684\u6269\u6563\u6620\u5c04\u4f5c\u4e3a\u5bf9\u89d2\u77e9\u9635\u7684\u63a8\u5e7f\uff0c\u5e76\u4e0eJordan\u4ee3\u6570\u65b9\u6cd5\u7684\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5efa\u7acb\u4e86Metzler\u548c\u975e\u8d1f\u77e9\u9635\u5173\u952e\u6027\u8d28\u5728\u81ea\u5bf9\u5076\u771f\u51f8\u9525\u4e0a\u7684\u63a8\u5e7f\u6846\u67b6\uff0c\u5c06\u7a33\u5b9a\u6027\u7406\u8bba\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684\u9525\u7ed3\u6784\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0e\u5bf9\u79f0\u9525\u4e0aJordan\u4ee3\u6570\u7ed3\u679c\u7684\u8054\u7cfb\u3002", "conclusion": "\u6210\u529f\u5c06\u7ecf\u5178\u77e9\u9635\u6027\u8d28\u63a8\u5e7f\u5230\u81ea\u5bf9\u5076\u771f\u51f8\u9525\u4e0a\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u7a33\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u4e0e\u5bf9\u79f0\u9525\u4e0aJordan\u4ee3\u6570\u65b9\u6cd5\u7684\u8054\u7cfb\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.17894", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.17894", "abs": "https://arxiv.org/abs/2602.17894", "authors": ["Michael O. Harding", "Vikas Singh", "Kirthevasan Kandasamy"], "title": "Learning from Biased and Costly Data Sources: Minimax-optimal Data Collection under a Budget", "comment": null, "summary": "Data collection is a critical component of modern statistical and machine learning pipelines, particularly when data must be gathered from multiple heterogeneous sources to study a target population of interest. In many use cases, such as medical studies or political polling, different sources incur different sampling costs. Observations often have associated group identities (for example, health markers, demographics, or political affiliations) and the relative composition of these groups may differ substantially, both among the source populations and between sources and target population.\n  In this work, we study multi-source data collection under a fixed budget, focusing on the estimation of population means and group-conditional means. We show that naive data collection strategies (e.g. attempting to \"match\" the target distribution) or relying on standard estimators (e.g. sample mean) can be highly suboptimal. Instead, we develop a sampling plan which maximizes the effective sample size: the total sample size divided by $D_{\u03c7^2}(q\\mid\\mid\\overline{p}) + 1$, where $q$ is the target distribution, $\\overline{p}$ is the aggregated source distribution, and $D_{\u03c7^2}$ is the $\u03c7^2$-divergence. We pair this sampling plan with a classical post-stratification estimator and upper bound its risk. We provide matching lower bounds, establishing that our approach achieves the budgeted minimax optimal risk. Our techniques also extend to prediction problems when minimizing the excess risk, providing a principled approach to multi-source learning with costly and heterogeneous data sources.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u6e90\u6570\u636e\u6536\u96c6\u7684\u4f18\u5316\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6709\u6548\u6837\u672c\u91cf\u6765\u4f30\u8ba1\u603b\u4f53\u5747\u503c\u548c\u7ec4\u6761\u4ef6\u5747\u503c\uff0c\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u8fbe\u5230\u6700\u5c0f\u6700\u5927\u6700\u4f18\u98ce\u9669\u3002", "motivation": "\u5728\u591a\u6e90\u5f02\u6784\u6570\u636e\u6536\u96c6\u573a\u666f\u4e2d\uff08\u5982\u533b\u5b66\u7814\u7a76\u3001\u653f\u6cbb\u6c11\u8c03\uff09\uff0c\u4e0d\u540c\u6570\u636e\u6e90\u6709\u4e0d\u540c\u7684\u91c7\u6837\u6210\u672c\uff0c\u4e14\u7ec4\u522b\u5206\u5e03\u5dee\u5f02\u663e\u8457\u3002\u4f20\u7edf\u7684\u6570\u636e\u6536\u96c6\u7b56\u7565\uff08\u5982\u5339\u914d\u76ee\u6807\u5206\u5e03\uff09\u6216\u6807\u51c6\u4f30\u8ba1\u5668\uff08\u5982\u6837\u672c\u5747\u503c\uff09\u5f80\u5f80\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u4f18\u7684\u65b9\u6cd5\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u8fdb\u884c\u6709\u6548\u4f30\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6700\u5927\u5316\u6709\u6548\u6837\u672c\u91cf\u7684\u91c7\u6837\u8ba1\u5212\uff1a\u6709\u6548\u6837\u672c\u91cf = \u603b\u6837\u672c\u91cf / (D_\u03c7\u00b2(q||p\u0304) + 1)\uff0c\u5176\u4e2dq\u662f\u76ee\u6807\u5206\u5e03\uff0cp\u0304\u662f\u805a\u5408\u6e90\u5206\u5e03\uff0cD_\u03c7\u00b2\u662f\u03c7\u00b2\u6563\u5ea6\u3002\u5c06\u8be5\u91c7\u6837\u8ba1\u5212\u4e0e\u7ecf\u5178\u7684\u540e\u5206\u5c42\u4f30\u8ba1\u5668\u7ed3\u5408\uff0c\u5e76\u7ed9\u51fa\u98ce\u9669\u4e0a\u754c\u3002", "result": "\u63d0\u4f9b\u4e86\u5339\u914d\u7684\u4e0b\u754c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u6700\u5c0f\u6700\u5927\u6700\u4f18\u98ce\u9669\u3002\u6280\u672f\u8fd8\u6269\u5c55\u5230\u9884\u6d4b\u95ee\u9898\u4e2d\u7684\u8d85\u989d\u98ce\u9669\u6700\u5c0f\u5316\uff0c\u4e3a\u591a\u6e90\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u6e90\u5f02\u6784\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u6700\u4f18\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u91c7\u6837\u6210\u672c\u4e0d\u540c\u4e14\u7ec4\u522b\u5206\u5e03\u5dee\u5f02\u663e\u8457\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982\u533b\u5b66\u7814\u7a76\u548c\u653f\u6cbb\u6c11\u8c03\u3002"}}
{"id": "2602.18059", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18059", "abs": "https://arxiv.org/abs/2602.18059", "authors": ["Junseon Park", "Hyeongon Park", "Rahul K. Gupta"], "title": "Iterative McCormick Relaxation for Joint Impedance Control and Network Topology Optimization", "comment": null, "summary": "Power system operators are increasingly deploying Variable Impedance Devices (VIDs), e.g., Smart Wires, and Network Topology Optimization (NTO) schemes for mitigating operational challenges such as line and transformer congestion, and voltage violations. This work aims to optimize and coordinate the operation of distributed VIDs considering fixed and optimized topologies. This problem is inherently non-linear due to power flow equations as well as bilinear terms introduced due to variable line impedance of VIDs. Furthermore, the topology optimization scheme makes it a mixed integer nonlinear problem. To tackle this, we introduce using McCormick relaxation scheme, which converts the bilinear constraints into a linear set of constraints along with the DC power flow equations. We propose an iterative correction of the McCormick relaxation to enhance its accuracy. The proposed framework is validated on standard IEEE benchmark test systems, and we present a performance comparison of the iterative McCormick method against the non-linear, SOS2 piecewise linear approximation, and original McCormick relaxation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eMcCormick\u677e\u5f1b\u7684\u8fed\u4ee3\u6821\u6b63\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u534f\u8c03\u5206\u5e03\u5f0f\u53ef\u53d8\u963b\u6297\u8bbe\u5907\uff08VIDs\uff09\u4e0e\u7f51\u7edc\u62d3\u6251\u4f18\u5316\uff08NTO\uff09\uff0c\u89e3\u51b3\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u7ebf\u8def\u62e5\u585e\u548c\u7535\u538b\u8fdd\u89c4\u95ee\u9898\u3002", "motivation": "\u7535\u529b\u7cfb\u7edf\u8fd0\u8425\u5546\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u53ef\u53d8\u963b\u6297\u8bbe\u5907\uff08\u5982Smart Wires\uff09\u548c\u7f51\u7edc\u62d3\u6251\u4f18\u5316\u65b9\u6848\u6765\u7f13\u89e3\u7ebf\u8def\u62e5\u585e\u3001\u53d8\u538b\u5668\u62e5\u585e\u548c\u7535\u538b\u8fdd\u89c4\u7b49\u8fd0\u884c\u6311\u6218\u3002\u9700\u8981\u4f18\u5316\u534f\u8c03\u5206\u5e03\u5f0fVIDs\u5728\u56fa\u5b9a\u548c\u4f18\u5316\u62d3\u6251\u4e0b\u7684\u8fd0\u884c\u3002", "method": "\u4f7f\u7528McCormick\u677e\u5f1b\u65b9\u6848\u5c06\u53cc\u7ebf\u6027\u7ea6\u675f\u8f6c\u6362\u4e3a\u7ebf\u6027\u7ea6\u675f\u96c6\uff0c\u7ed3\u5408\u76f4\u6d41\u6f6e\u6d41\u65b9\u7a0b\u3002\u63d0\u51fa\u8fed\u4ee3\u6821\u6b63McCormick\u677e\u5f1b\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002\u5c06\u95ee\u9898\u4ece\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u6c42\u89e3\u5f62\u5f0f\u3002", "result": "\u5728\u6807\u51c6IEEE\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\uff0c\u5e76\u5bf9\u8fed\u4ee3McCormick\u65b9\u6cd5\u4e0e\u975e\u7ebf\u6027\u65b9\u6cd5\u3001SOS2\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c\u548c\u539f\u59cbMcCormick\u677e\u5f1b\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fed\u4ee3McCormick\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3VIDs\u4e0eNTO\u534f\u8c03\u4f18\u5316\u95ee\u9898\uff0c\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6027\u80fd\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17753", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17753", "abs": "https://arxiv.org/abs/2602.17753", "authors": ["Leon Staufer", "Kevin Feng", "Kevin Wei", "Luke Bailey", "Yawen Duan", "Mick Yang", "A. Pinar Ozisik", "Stephen Casper", "Noam Kolt"], "title": "The 2025 AI Agent Index: Documenting Technical and Safety Features of Deployed Agentic AI Systems", "comment": null, "summary": "Agentic AI systems are increasingly capable of performing professional and personal tasks with limited human involvement. However, tracking these developments is difficult because the AI agent ecosystem is complex, rapidly evolving, and inconsistently documented, posing obstacles to both researchers and policymakers. To address these challenges, this paper presents the 2025 AI Agent Index. The Index documents information regarding the origins, design, capabilities, ecosystem, and safety features of 30 state-of-the-art AI agents based on publicly available information and email correspondence with developers. In addition to documenting information about individual agents, the Index illuminates broader trends in the development of agents, their capabilities, and the level of transparency of developers. Notably, we find different transparency levels among agent developers and observe that most developers share little information about safety, evaluations, and societal impacts. The 2025 AI Agent Index is available online at https://aiagentindex.mit.edu", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e862025\u5e74AI\u667a\u80fd\u4f53\u6307\u6570\uff0c\u65e8\u5728\u89e3\u51b3AI\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u590d\u6742\u3001\u5feb\u901f\u6f14\u53d8\u4e14\u6587\u6863\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8bb0\u5f5530\u4e2a\u6700\u5148\u8fdbAI\u667a\u80fd\u4f53\u7684\u8d77\u6e90\u3001\u8bbe\u8ba1\u3001\u80fd\u529b\u3001\u751f\u6001\u7cfb\u7edf\u548c\u5b89\u5168\u7279\u6027\u7b49\u4fe1\u606f\u3002", "motivation": "AI\u667a\u80fd\u4f53\u7cfb\u7edf\u8d8a\u6765\u8d8a\u80fd\u591f\u4ee5\u6709\u9650\u7684\u4eba\u7c7b\u53c2\u4e0e\u6267\u884c\u4e13\u4e1a\u548c\u4e2a\u4eba\u4efb\u52a1\uff0c\u4f46\u7531\u4e8eAI\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u590d\u6742\u3001\u5feb\u901f\u6f14\u53d8\u4e14\u6587\u6863\u4e0d\u4e00\u81f4\uff0c\u8ddf\u8e2a\u8fd9\u4e9b\u53d1\u5c55\u53d8\u5f97\u56f0\u96be\uff0c\u7ed9\u7814\u7a76\u4eba\u5458\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5e26\u6765\u4e86\u969c\u788d\u3002", "method": "\u521b\u5efa2025\u5e74AI\u667a\u80fd\u4f53\u6307\u6570\uff0c\u57fa\u4e8e\u516c\u5f00\u4fe1\u606f\u548c\u4e0e\u5f00\u53d1\u8005\u7684\u7535\u5b50\u90ae\u4ef6\u901a\u4fe1\uff0c\u8bb0\u5f5530\u4e2a\u6700\u5148\u8fdbAI\u667a\u80fd\u4f53\u7684\u8d77\u6e90\u3001\u8bbe\u8ba1\u3001\u80fd\u529b\u3001\u751f\u6001\u7cfb\u7edf\u548c\u5b89\u5168\u7279\u6027\u7b49\u4fe1\u606f\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u667a\u80fd\u4f53\u5f00\u53d1\u8005\u7684\u900f\u660e\u5ea6\u6c34\u5e73\u5b58\u5728\u5dee\u5f02\uff0c\u5927\u591a\u6570\u5f00\u53d1\u8005\u5f88\u5c11\u5206\u4eab\u5173\u4e8e\u5b89\u5168\u6027\u3001\u8bc4\u4f30\u548c\u793e\u4f1a\u5f71\u54cd\u7684\u4fe1\u606f\u3002\u8be5\u6307\u6570\u8fd8\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u53d1\u5c55\u3001\u80fd\u529b\u548c\u5f00\u53d1\u8005\u900f\u660e\u5ea6\u65b9\u9762\u7684\u66f4\u5e7f\u6cdb\u8d8b\u52bf\u3002", "conclusion": "2025\u5e74AI\u667a\u80fd\u4f53\u6307\u6570\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u5e2e\u52a9\u8ddf\u8e2aAI\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u5e76\u5f3a\u8c03\u4e86\u63d0\u9ad8\u5f00\u53d1\u8005\u900f\u660e\u5ea6\u7684\u5fc5\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u6027\u3001\u8bc4\u4f30\u548c\u793e\u4f1a\u5f71\u54cd\u65b9\u9762\u3002"}}
{"id": "2602.17907", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17907", "abs": "https://arxiv.org/abs/2602.17907", "authors": ["Raymond Li", "Amirhossein Abaskohi", "Chuyuan Li", "Gabriel Murray", "Giuseppe Carenini"], "title": "Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions", "comment": "20 pages, 5 figures", "summary": "Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u4e49\u8f6f\u6807\u7b7e\u6765\u6539\u8fdb\u795e\u7ecf\u4e3b\u9898\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u53f7\u800c\u975e\u4f20\u7edf\u8bcd\u888b\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e3b\u9898\u8d28\u91cf\u548c\u6587\u6863\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u4e3b\u9898\u6a21\u578b\u4ec5\u91cd\u6784\u6587\u6863\u7684\u8bcd\u888b\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e14\u53d7\u6570\u636e\u7a00\u758f\u6027\u56f0\u6270\uff0c\u5bfc\u81f4\u4e3b\u9898\u8d28\u91cf\u4e0d\u9ad8\u4e14\u4e0e\u8bed\u6599\u4e3b\u9898\u7ed3\u6784\u5bf9\u9f50\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u7279\u5b9a\u63d0\u793a\u8bcd\u751f\u6210\u4e0b\u4e00\u4e2a\u8bcd\u7684\u6982\u7387\u5206\u5e03\uff0c\u5c06\u5176\u6295\u5f71\u5230\u9884\u5b9a\u4e49\u8bcd\u6c47\u8868\u4e0a\u5f97\u5230\u8bed\u4e49\u8f6f\u6807\u7b7e\uff0c\u7136\u540e\u8bad\u7ec3\u4e3b\u9898\u6a21\u578b\u91cd\u6784\u8fd9\u4e9b\u8f6f\u6807\u7b7e\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e3b\u9898\u8fde\u8d2f\u6027\u548c\u7eaf\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u65b0\u63d0\u51fa\u7684\u68c0\u7d22\u6307\u6807\u663e\u793a\u5176\u5728\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u6587\u6863\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u68c0\u7d22\u5bfc\u5411\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u8bed\u4e49\u8f6f\u6807\u7b7e\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u795e\u7ecf\u4e3b\u9898\u6a21\u578b\u7684\u4e3b\u9898\u8d28\u91cf\u548c\u6587\u6863\u68c0\u7d22\u80fd\u529b\uff0c\u4e3a\u68c0\u7d22\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17910", "abs": "https://arxiv.org/abs/2602.17910", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems", "comment": null, "summary": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.", "AI": {"tldr": "APEMO\u662f\u4e00\u4e2a\u8fd0\u884c\u65f6\u8c03\u5ea6\u5c42\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4-\u60c5\u611f\u4fe1\u53f7\u4f18\u5316\u56fa\u5b9a\u9884\u7b97\u4e0b\u7684\u8ba1\u7b97\u5206\u914d\uff0c\u63d0\u5347\u957f\u65f6\u7a0b\u81ea\u4e3b\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u8f68\u8ff9\u7ea7\u53ef\u9760\u6027\u548c\u91cd\u7528\u6982\u7387\u3002", "motivation": "\u4f20\u7edfAI\u5bf9\u9f50\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6a21\u578b\u8f93\u51fa\uff0c\u4f46\u81ea\u4e3b\u4ee3\u7406\u5728\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u9700\u8981\u5728\u6574\u4e2a\u4ea4\u4e92\u8f68\u8ff9\u4e0a\u4fdd\u6301\u6301\u7eed\u53ef\u9760\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f18\u5316\u8ba1\u7b97\u5206\u914d\uff0c\u63d0\u9ad8\u8f68\u8ff9\u7ea7\u8d28\u91cf\u3002", "method": "APEMO\u662f\u4e00\u4e2a\u8fd0\u884c\u65f6\u8c03\u5ea6\u5c42\uff0c\u901a\u8fc7\u884c\u4e3a\u4ee3\u7406\u68c0\u6d4b\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u5173\u952e\u7247\u6bb5\uff08\u5982\u5cf0\u503c\u65f6\u523b\u548c\u7ed3\u675f\u65f6\u523b\uff09\u8fdb\u884c\u4fee\u590d\uff0c\u800c\u4e0d\u662f\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002\u5b83\u64cd\u4f5c\u5316\u65f6\u95f4-\u60c5\u611f\u4fe1\u53f7\u6765\u4f18\u5316\u8ba1\u7b97\u5206\u914d\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u57fa\u4e8eLLM\u7684\u89c4\u5212-\u6267\u884c\u6d41\u7a0b\u8bc4\u4f30\u4e2d\uff0cAPEMO\u5728\u8f68\u8ff9\u7ea7\u8d28\u91cf\u548c\u91cd\u7528\u6982\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u7ed3\u6784\u5316\u7f16\u6392\u5668\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u65f6\u95f4\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5f39\u6027\u5de5\u7a0b\u8def\u5f84\u3002"}}
{"id": "2602.17845", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.17845", "abs": "https://arxiv.org/abs/2602.17845", "authors": ["Bryce Christopherson", "Farhad Jafari"], "title": "A Refinement in \u010cech Cohomology of Coron's Necessary Condition", "comment": "12 pages", "summary": "Coron established a homological obstruction to continuous feedback stabilization of nonlinear control systems $\\dot{x}=f(x,u)$ with $f \\in C(\u03a9,\\mathbb{R}^n)$ and $f(0,0)=0$, showing that local asymptotic stabilizability implies the induced homomorphism $f_*$ satisfies $f_*\\big(H_{n-1}(\u03a3_\u03b5)\\big)=H_{n-1}(S^{n-1})$, where $\u03a3_\u03b5:=\\Big(\\big(\\mathbb{B}_\u03b5^{\\mathbb{R}^n}(0)\\times\\mathbb{B}_\u03b5^{\\mathbb{R}^m}(0)\\big)\\cap \u03a9\\Big)\\setminus f^{-1}(0)$. In this paper, we refine Coron's necessary condition using \u010cech cohomology and the Vietoris-Begle mapping theorem. Specifically, we prove that the closed version of $\u03a3_\u03b5$ must be a \u010cech cohomology $(n-1)$-sphere and that the restriction of $f$ to this subset induces an isomorphism on its \u010cech cohomology groups in all degrees. This strengthens Coron's condition from a constraint on the top class to a full cohomological rigidity statement.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86Coron\u5173\u4e8e\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\u8fde\u7eed\u53cd\u9988\u9547\u5b9a\u7684\u540c\u8c03\u969c\u788d\u6761\u4ef6\uff0c\u5229\u7528\u010cech\u4e0a\u540c\u8c03\u548cVietoris-Begle\u6620\u5c04\u5b9a\u7406\uff0c\u8bc1\u660e\u4e86\u66f4\u5f3a\u7684\u4e0a\u540c\u8c03\u521a\u6027\u6761\u4ef6\u3002", "motivation": "Coron\u5efa\u7acb\u4e86\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\u8fde\u7eed\u53cd\u9988\u9547\u5b9a\u7684\u540c\u8c03\u969c\u788d\u6761\u4ef6\uff0c\u4f46\u8be5\u6761\u4ef6\u53ea\u6d89\u53ca\u6700\u9ad8\u7ef4\u540c\u8c03\u7fa4\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u66f4\u7cbe\u7ec6\u7684\u010cech\u4e0a\u540c\u8c03\u5de5\u5177\uff0c\u5f97\u5230\u66f4\u5f3a\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u010cech\u4e0a\u540c\u8c03\u548cVietoris-Begle\u6620\u5c04\u5b9a\u7406\uff0c\u5206\u6790\u63a7\u5236\u7cfb\u7edf\u8bf1\u5bfc\u7684\u6620\u5c04\u5728\u95ed\u7248\u672c\u03a3_\u03b5\u4e0a\u7684\u6027\u8d28\uff0c\u8bc1\u660e\u5176\u5fc5\u987b\u6ee1\u8db3\u5b8c\u6574\u7684\u540c\u6784\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u95ed\u7248\u672c\u03a3_\u03b5\u5fc5\u987b\u662f\u010cech\u4e0a\u540c\u8c03\u7684(n-1)-\u7403\u9762\uff0c\u4e14f\u5728\u8be5\u5b50\u96c6\u4e0a\u7684\u9650\u5236\u5728\u6240\u6709\u7ef4\u6570\u4e0a\u90fd\u8bf1\u5bfc\u010cech\u4e0a\u540c\u8c03\u7fa4\u7684\u540c\u6784\uff0c\u8fd9\u6bd4Coron\u7684\u6761\u4ef6\u66f4\u5f3a\u3002", "conclusion": "\u672c\u6587\u5f97\u5230\u4e86\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\u8fde\u7eed\u53cd\u9988\u9547\u5b9a\u7684\u66f4\u7cbe\u7ec6\u7684\u010cech\u4e0a\u540c\u8c03\u5fc5\u8981\u6761\u4ef6\uff0c\u5c06Coron\u7684\u5355\u4e00\u6761\u4ef6\u6269\u5c55\u4e3a\u5b8c\u6574\u7684\u540c\u8c03\u521a\u6027\u9648\u8ff0\u3002"}}
{"id": "2602.18053", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.18053", "abs": "https://arxiv.org/abs/2602.18053", "authors": ["Dinesh Karthik Mulumudi", "Piyushi Manupriya", "Gholamali Aminian", "Anant Raj"], "title": "On the Generalization and Robustness in Conditional Value-at-Risk", "comment": null, "summary": "Conditional Value-at-Risk (CVaR) is a widely used risk-sensitive objective for learning under rare but high-impact losses, yet its statistical behavior under heavy-tailed data remains poorly understood. Unlike expectation-based risk, CVaR depends on an endogenous, data-dependent quantile, which couples tail averaging with threshold estimation and fundamentally alters both generalization and robustness properties. In this work, we develop a learning-theoretic analysis of CVaR-based empirical risk minimization under heavy-tailed and contaminated data. We establish sharp, high-probability generalization and excess risk bounds under minimal moment assumptions, covering fixed hypotheses, finite and infinite classes, and extending to $\u03b2$-mixing dependent data; we further show that these rates are minimax optimal. To capture the intrinsic quantile sensitivity of CVaR, we derive a uniform Bahadur-Kiefer type expansion that isolates a threshold-driven error term absent in mean-risk ERM and essential in heavy-tailed regimes. We complement these results with robustness guarantees by proposing a truncated median-of-means CVaR estimator that achieves optimal rates under adversarial contamination. Finally, we show that CVaR decisions themselves can be intrinsically unstable under heavy tails, establishing a fundamental limitation on decision robustness even when the population optimum is well separated. Together, our results provide a principled characterization of when CVaR learning generalizes and is robust, and when instability is unavoidable due to tail scarcity.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5728\u91cd\u5c3e\u548c\u6c61\u67d3\u6570\u636e\u4e0bCVaR\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u7684\u7edf\u8ba1\u6027\u8d28\uff0c\u5efa\u7acb\u4e86\u6700\u4f18\u7684\u6cdb\u5316\u754c\u548c\u8d85\u989d\u98ce\u9669\u754c\uff0c\u63ed\u793a\u4e86CVaR\u5728\u91cd\u5c3e\u6570\u636e\u4e0b\u7684\u5185\u5728\u4e0d\u7a33\u5b9a\u6027\u3002", "motivation": "CVaR\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u98ce\u9669\u654f\u611f\u76ee\u6807\u51fd\u6570\uff0c\u4f46\u5728\u91cd\u5c3e\u6570\u636e\u4e0b\u7684\u7edf\u8ba1\u884c\u4e3a\u5c1a\u4e0d\u6e05\u695a\u3002CVaR\u4f9d\u8d56\u4e8e\u5185\u751f\u7684\u3001\u6570\u636e\u4f9d\u8d56\u7684\u5206\u4f4d\u6570\uff0c\u8fd9\u6539\u53d8\u4e86\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u7279\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u5f00\u53d1\u4e86CVaR\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u7684\u5b66\u4e60\u7406\u8bba\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u9ad8\u6982\u7387\u6cdb\u5316\u754c\u548c\u8d85\u989d\u98ce\u9669\u754c\uff0c\u63d0\u51fa\u4e86\u622a\u65ad\u4e2d\u4f4d\u6570\u5747\u503cCVaR\u4f30\u8ba1\u5668\uff0c\u5e76\u63a8\u5bfc\u4e86\u7edf\u4e00\u7684Bahadur-Kiefer\u578b\u5c55\u5f00\u6765\u9694\u79bb\u9608\u503c\u9a71\u52a8\u8bef\u5dee\u3002", "result": "\u5728\u6700\u5c0f\u77e9\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u5c16\u9510\u7684\u6700\u4f18\u6cdb\u5316\u754c\uff0c\u8986\u76d6\u4e86\u56fa\u5b9a\u5047\u8bbe\u3001\u6709\u9650\u548c\u65e0\u9650\u7c7b\u522b\u4ee5\u53ca\u03b2\u6df7\u5408\u4f9d\u8d56\u6570\u636e\u3002\u8bc1\u660e\u4e86CVaR\u51b3\u7b56\u5728\u91cd\u5c3e\u6570\u636e\u4e0b\u53ef\u80fd\u672c\u8d28\u4e0d\u7a33\u5b9a\uff0c\u5373\u4f7f\u603b\u4f53\u6700\u4f18\u89e3\u662f\u826f\u597d\u5206\u79bb\u7684\u3002", "conclusion": "\u7814\u7a76\u7cfb\u7edf\u523b\u753b\u4e86CVaR\u5b66\u4e60\u4f55\u65f6\u80fd\u591f\u6cdb\u5316\u548c\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u4f55\u65f6\u7531\u4e8e\u5c3e\u90e8\u7a00\u7f3a\u6027\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\u4e0d\u53ef\u907f\u514d\uff0c\u4e3aCVaR\u5728\u91cd\u5c3e\u6570\u636e\u4e0b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.18247", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18247", "abs": "https://arxiv.org/abs/2602.18247", "authors": ["Fen Wu", "Chengzhi Yuan"], "title": "Hybrid Control of ADT Switched Linear Systems subject to Actuator Saturation", "comment": null, "summary": "This paper develops a hybrid output-feedback control framework for average dwell-time (ADT) switched linear systems subject to actuator saturation. The considered subsystems may be exponentially unstable, and the saturation nonlinearity is explicitly handled through a deadzone-based representation. The proposed hybrid controller combines mode-dependent full-order dynamic output-feedback controllers with a supervisory reset mechanism that updates controller states at switching instants. By incorporating the reset rule directly into the synthesis conditions, switching boundary constraints and performance requirements are addressed in a unified convex formulation. Sufficient conditions are derived in terms of linear matrix inequalities (LMIs) to guarantee exponential stability under ADT switching and a prescribed weighted ${\\cal L}_2$-gain disturbance attenuation level for energy-bounded disturbances. An explicit controller construction algorithm is provided based on feasible LMI solutions. Simulation results demonstrate the effectiveness and computational tractability of the proposed approach and highlight its advantages over existing output-feedback saturation control methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u8f93\u51fa\u53cd\u9988\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u6267\u884c\u5668\u9971\u548c\u7684\u5e73\u5747\u9a7b\u7559\u65f6\u95f4\u5207\u6362\u7ebf\u6027\u7cfb\u7edf\uff0c\u901a\u8fc7\u76d1\u7763\u91cd\u7f6e\u673a\u5236\u548cLMI\u6761\u4ef6\u4fdd\u8bc1\u6307\u6570\u7a33\u5b9a\u6027\u548c\u6270\u52a8\u8870\u51cf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5207\u6362\u7cfb\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u5904\u7406\u6267\u884c\u5668\u9971\u548c\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5f53\u5b50\u7cfb\u7edf\u53ef\u80fd\u6307\u6570\u4e0d\u7a33\u5b9a\u65f6\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u9971\u548c\u975e\u7ebf\u6027\u3001\u5207\u6362\u884c\u4e3a\u548c\u8f93\u51fa\u53cd\u9988\u7ea6\u675f\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u91c7\u7528\u6df7\u5408\u63a7\u5236\u5668\u7ed3\u6784\uff1a\u6a21\u5f0f\u4f9d\u8d56\u7684\u5168\u9636\u52a8\u6001\u8f93\u51fa\u53cd\u9988\u63a7\u5236\u5668 + \u76d1\u7763\u91cd\u7f6e\u673a\u5236\uff08\u5728\u5207\u6362\u65f6\u523b\u66f4\u65b0\u63a7\u5236\u5668\u72b6\u6001\uff09\u3002\u57fa\u4e8e\u6b7b\u533a\u8868\u793a\u5904\u7406\u9971\u548c\u975e\u7ebf\u6027\uff0c\u901a\u8fc7LMI\u6761\u4ef6\u7edf\u4e00\u5904\u7406\u5207\u6362\u8fb9\u754c\u7ea6\u675f\u548c\u6027\u80fd\u8981\u6c42\u3002", "result": "\u63a8\u5bfc\u51fa\u4fdd\u8bc1\u6307\u6570\u7a33\u5b9a\u6027\u548c\u52a0\u6743L2\u589e\u76ca\u6270\u52a8\u8870\u51cf\u7684LMI\u5145\u5206\u6761\u4ef6\uff0c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u53ef\u884cLMI\u89e3\u7684\u663e\u5f0f\u63a7\u5236\u5668\u6784\u9020\u7b97\u6cd5\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u8f93\u51fa\u53cd\u9988\u63a7\u5236\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u5177\u6709\u6267\u884c\u5668\u9971\u548c\u7684\u5207\u6362\u7ebf\u6027\u7cfb\u7edf\uff0c\u5728\u4fdd\u8bc1\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u7684\u540c\u65f6\u5177\u6709\u8ba1\u7b97\u53ef\u884c\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u9971\u548c\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2602.17791", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17791", "abs": "https://arxiv.org/abs/2602.17791", "authors": ["Jinsook Lee", "Conrad Borchers", "AJ Alvero", "Thorsten Joachims", "Rene F. Kizilcec"], "title": "The Digital Divide in Generative AI: Evidence from Large Language Model Use in College Admissions Essays", "comment": null, "summary": "Large language models (LLMs) have become popular writing tools among students and may expand access to high-quality feedback for students with less access to traditional writing support. At the same time, LLMs may standardize student voice or invite overreliance. This study examines how adoption of LLM-assisted writing varies across socioeconomic groups and how it relates to outcomes in a high-stakes context: U.S. college admissions. We analyze a de-identified longitudinal dataset of applications to a selective university from 2020 to 2024 (N = 81,663). Estimating LLM use using a distribution-based detector trained on synthetic and historical essays, we tracked how student writing changed as LLM use proliferated, how adoption differed by socioeconomic status (SES), and whether potential benefits translated equitably into admissions outcomes. Using fee-waiver status as a proxy for SES, we observe post-2023 convergence in surface-level linguistic features, with the largest changes in fee-waived and rejected applicants. Estimated LLM use rose sharply in 2024 across all groups, with disproportionately larger increases among lower SES applicants, consistent with an access hypothesis in which LLMs substitute for scarce writing support. However, increased estimated LLM use was more strongly associated with declines in predicted admission probability for lower SES applicants than for higher SES applicants, even after controlling for academic credentials and stylometric features. These findings raise concerns about equity and the validity of essay-based evaluation in an era of AI-assisted writing and provide the first large-scale longitudinal evidence linking LLM adoption, linguistic change, and evaluative outcomes in college admissions.", "AI": {"tldr": "\u7814\u7a76\u5206\u67902020-2024\u5e74\u7f8e\u56fd\u5927\u5b66\u7533\u8bf7\u4e2dLLM\u4f7f\u7528\u60c5\u51b5\uff0c\u53d1\u73b0\u4f4e\u6536\u5165\u5b66\u751f\u66f4\u591a\u4f7f\u7528LLM\u4f5c\u4e3a\u5199\u4f5c\u652f\u6301\u66ff\u4ee3\uff0c\u4f46LLM\u4f7f\u7528\u4e0e\u5f55\u53d6\u6982\u7387\u4e0b\u964d\u7684\u5173\u8054\u5728\u4f4e\u6536\u5165\u5b66\u751f\u4e2d\u66f4\u5f3a\u3002", "motivation": "\u7814\u7a76LLM\u4f5c\u4e3a\u5199\u4f5c\u5de5\u5177\u5728\u4e0d\u540c\u793e\u4f1a\u7ecf\u6d4e\u7fa4\u4f53\u4e2d\u7684\u91c7\u7528\u5dee\u5f02\uff0c\u53ca\u5176\u5728\u5927\u5b66\u5f55\u53d6\u8fd9\u79cd\u9ad8\u98ce\u9669\u60c5\u5883\u4e0b\u5bf9\u516c\u5e73\u6027\u548c\u8bc4\u4f30\u6709\u6548\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5206\u67902020-2024\u5e74\u9009\u62e9\u6027\u5927\u5b6681,663\u4efd\u7533\u8bf7\u6570\u636e\uff0c\u4f7f\u7528\u57fa\u4e8e\u5206\u5e03\u7684\u68c0\u6d4b\u5668\u4f30\u8ba1LLM\u4f7f\u7528\uff0c\u4ee5\u8d39\u7528\u51cf\u514d\u72b6\u6001\u4f5c\u4e3a\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u4ee3\u7406\u53d8\u91cf\uff0c\u8ffd\u8e2a\u5199\u4f5c\u53d8\u5316\u548c\u5f55\u53d6\u7ed3\u679c\u3002", "result": "2023\u5e74\u540e\u6240\u6709\u7fa4\u4f53\u8868\u9762\u8bed\u8a00\u7279\u5f81\u8d8b\u540c\uff0c2024\u5e74LLM\u4f7f\u7528\u6025\u5267\u589e\u52a0\uff0c\u4f4e\u6536\u5165\u5b66\u751f\u589e\u5e45\u66f4\u5927\uff1b\u4f46\u4f4e\u6536\u5165\u5b66\u751fLLM\u4f7f\u7528\u589e\u52a0\u4e0e\u5f55\u53d6\u6982\u7387\u4e0b\u964d\u7684\u5173\u8054\u66f4\u5f3a\uff0c\u5373\u4f7f\u63a7\u5236\u5b66\u672f\u8d44\u5386\u548c\u6587\u4f53\u7279\u5f81\u540e\u4f9d\u7136\u5982\u6b64\u3002", "conclusion": "LLM\u53ef\u80fd\u52a0\u5267\u6559\u80b2\u4e0d\u5e73\u7b49\uff0c\u5728AI\u8f85\u52a9\u5199\u4f5c\u65f6\u4ee3\uff0c\u57fa\u4e8e\u6587\u7ae0\u7684\u8bc4\u4f30\u6709\u6548\u6027\u53d7\u5230\u6311\u6218\uff0c\u9700\u8981\u5173\u6ce8\u516c\u5e73\u6027\u95ee\u9898\u3002"}}
{"id": "2602.17911", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17911", "abs": "https://arxiv.org/abs/2602.17911", "authors": ["Jash Rajesh Parekh", "Wonbin Kweon", "Joey Chan", "Rezarta Islamaj", "Robert Leaman", "Pengcheng Jiang", "Chih-Hsuan Wei", "Zhizheng Wang", "Zhiyong Lu", "Jiawei Han"], "title": "Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering", "comment": null, "summary": "Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86CondMedQA\u57fa\u51c6\u548cCondition-Gated Reasoning\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u89e3\u51b3\u6761\u4ef6\u6027\u751f\u7269\u533b\u5b66\u95ee\u7b54\u95ee\u9898", "motivation": "\u73b0\u6709\u751f\u7269\u533b\u5b66QA\u7cfb\u7edf\u5047\u8bbe\u533b\u5b66\u77e5\u8bc6\u666e\u904d\u9002\u7528\uff0c\u4f46\u771f\u5b9e\u4e34\u5e8a\u63a8\u7406\u5177\u6709\u6761\u4ef6\u6027\uff08\u53d6\u51b3\u4e8e\u60a3\u8005\u7279\u5b9a\u56e0\u7d20\uff09\uff0c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u8fd9\u79cd\u6761\u4ef6\u63a8\u7406\uff0c\u68c0\u7d22\u589e\u5f3a\u6216\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u7f3a\u4e4f\u786e\u4fdd\u68c0\u7d22\u77e5\u8bc6\u9002\u7528\u4e8e\u7279\u5b9a\u4e0a\u4e0b\u6587\u7684\u673a\u5236", "method": "\u63d0\u51faCondition-Gated Reasoning\u6846\u67b6\uff1a\u6784\u5efa\u6761\u4ef6\u611f\u77e5\u77e5\u8bc6\u56fe\u8c31\uff0c\u57fa\u4e8e\u67e5\u8be2\u6761\u4ef6\u9009\u62e9\u6027\u5730\u6fc0\u6d3b\u6216\u526a\u679d\u63a8\u7406\u8def\u5f84", "result": "CGR\u80fd\u66f4\u53ef\u9760\u5730\u9009\u62e9\u6761\u4ef6\u9002\u5f53\u7684\u7b54\u6848\uff0c\u5728\u751f\u7269\u533b\u5b66QA\u57fa\u51c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u6761\u4ef6\u6027\u5bf9\u4e8e\u7a33\u5065\u7684\u533b\u5b66\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u6761\u4ef6\u6027\u751f\u7269\u533b\u5b66\u95ee\u7b54\u95ee\u9898"}}
{"id": "2602.17990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17990", "abs": "https://arxiv.org/abs/2602.17990", "authors": ["Madhav Kanda", "Pedro Las-Casas", "Alok Gautam Kumbhare", "Rodrigo Fonseca", "Sharad Agarwal"], "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics", "comment": null, "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86WorkflowPerturb\u57fa\u51c6\uff0c\u901a\u8fc7\u53ef\u63a7\u6270\u52a8\u8bc4\u4f30\u5de5\u4f5c\u6d41\u8bc4\u4f30\u6307\u6807\u7684\u654f\u611f\u6027\u548c\u6821\u51c6\u6027", "motivation": "LLM\u751f\u6210\u7684\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u8bc4\u4f30\u56f0\u96be\uff0c\u73b0\u6709\u6307\u6807\u5206\u6570\u672a\u6821\u51c6\u4e14\u53d8\u5316\u65e0\u6cd5\u53cd\u6620\u5de5\u4f5c\u6d41\u9000\u5316\u4e25\u91cd\u7a0b\u5ea6", "method": "\u6784\u5efa\u5305\u542b4,973\u4e2a\u9ec4\u91d1\u5de5\u4f5c\u6d41\u548c44,757\u4e2a\u6270\u52a8\u53d8\u4f53\u7684\u57fa\u51c6\uff0c\u5e94\u7528\u4e09\u79cd\u6270\u52a8\u7c7b\u578b\uff08\u7f3a\u5931\u6b65\u9aa4\u3001\u538b\u7f29\u6b65\u9aa4\u3001\u63cf\u8ff0\u53d8\u5316\uff09\u548c\u4e09\u4e2a\u4e25\u91cd\u7ea7\u522b\uff0810%\u300130%\u300150%\uff09", "result": "\u57fa\u51c6\u4e86\u591a\u79cd\u6307\u6807\u5bb6\u65cf\uff0c\u5206\u6790\u5176\u654f\u611f\u6027\u548c\u6821\u51c6\u7279\u6027\uff0c\u652f\u6301\u57fa\u4e8e\u4e25\u91cd\u7a0b\u5ea6\u7684\u5de5\u4f5c\u6d41\u8bc4\u4f30\u5206\u6570\u89e3\u91ca", "conclusion": "WorkflowPerturb\u57fa\u51c6\u80fd\u591f\u7cfb\u7edf\u5206\u6790\u5de5\u4f5c\u6d41\u8bc4\u4f30\u6307\u6807\u7684\u5dee\u5f02\uff0c\u4fc3\u8fdb\u4e25\u91cd\u7a0b\u5ea6\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u53d1\u5c55"}}
{"id": "2602.17847", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.17847", "abs": "https://arxiv.org/abs/2602.17847", "authors": ["Bryce Christopherson", "Farhad Jafari"], "title": "Stabilization of Nonlinear Systems by Gain-Limited Feedback Laws", "comment": "18 pages, 1 figure", "summary": "We study local stabilization of nonlinear control systems under explicit gain constraints on the feedback law. Using a quantitative refinement of Brockett's openness condition, we introduce the notion of a maximal continuous openness rate for the system vector field near equilibrium. Combining this with a local-section characterization of stabilizability, we derive a general necessary condition for the existence of gain-limited stabilizing feedback. This condition yields sharp no-go results for broad classes of nonlinear systems, including systems that are stabilizable only by nonsmooth feedback. Several examples illustrate how openness rates impose fundamental lower bounds on stabilizing feedback growth near an equilibrium point.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u53cd\u9988\u589e\u76ca\u7ea6\u675f\u4e0b\u7684\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\u5c40\u90e8\u9547\u5b9a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6700\u5927\u8fde\u7eed\u5f00\u653e\u7387\u6982\u5ff5\uff0c\u5e76\u63a8\u5bfc\u4e86\u589e\u76ca\u53d7\u9650\u9547\u5b9a\u53cd\u9988\u5b58\u5728\u7684\u4e00\u822c\u5fc5\u8981\u6761\u4ef6\u3002", "motivation": "\u7814\u7a76\u5728\u660e\u786e\u589e\u76ca\u7ea6\u675f\u4e0b\u7684\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\u5c40\u90e8\u9547\u5b9a\u95ee\u9898\uff0c\u56e0\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u53cd\u9988\u589e\u76ca\u901a\u5e38\u53d7\u5230\u7269\u7406\u9650\u5236\u6216\u5b89\u5168\u8003\u8651\u7684\u9650\u5236\u3002", "method": "1. \u4f7f\u7528Brockett\u5f00\u653e\u6761\u4ef6\u7684\u5b9a\u91cf\u7ec6\u5316\uff0c\u5f15\u5165\u7cfb\u7edf\u5411\u91cf\u573a\u5728\u5e73\u8861\u70b9\u9644\u8fd1\u7684\u6700\u5927\u8fde\u7eed\u5f00\u653e\u7387\u6982\u5ff5\uff1b2. \u7ed3\u5408\u9547\u5b9a\u6027\u7684\u5c40\u90e8\u622a\u9762\u7279\u5f81\uff0c\u63a8\u5bfc\u589e\u76ca\u53d7\u9650\u9547\u5b9a\u53cd\u9988\u5b58\u5728\u7684\u4e00\u822c\u5fc5\u8981\u6761\u4ef6\u3002", "result": "1. \u83b7\u5f97\u4e86\u589e\u76ca\u53d7\u9650\u9547\u5b9a\u53cd\u9988\u5b58\u5728\u7684\u5fc5\u8981\u6761\u4ef6\uff1b2. \u4e3a\u5305\u62ec\u4ec5\u80fd\u901a\u8fc7\u975e\u5149\u6ed1\u53cd\u9988\u9547\u5b9a\u7684\u7cfb\u7edf\u5728\u5185\u7684\u5e7f\u6cdb\u975e\u7ebf\u6027\u7cfb\u7edf\u7c7b\u522b\u63d0\u4f9b\u4e86\u5c16\u9510\u7684\"\u4e0d\u53ef\u884c\"\u7ed3\u679c\uff1b3. \u901a\u8fc7\u591a\u4e2a\u4f8b\u5b50\u5c55\u793a\u4e86\u5f00\u653e\u7387\u5982\u4f55\u5bf9\u9547\u5b9a\u53cd\u9988\u5728\u5e73\u8861\u70b9\u9644\u8fd1\u7684\u589e\u957f\u65bd\u52a0\u57fa\u672c\u4e0b\u754c\u3002", "conclusion": "\u5f00\u653e\u7387\u5bf9\u9547\u5b9a\u53cd\u9988\u5728\u5e73\u8861\u70b9\u9644\u8fd1\u7684\u589e\u957f\u65bd\u52a0\u4e86\u57fa\u672c\u9650\u5236\uff0c\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u7406\u89e3\u589e\u76ca\u53d7\u9650\u4e0b\u7684\u9547\u5b9a\u53ef\u884c\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u975e\u7ebf\u6027\u7cfb\u7edf\u9547\u5b9a\u4e2d\u7684\u57fa\u672c\u7ea6\u675f\u3002"}}
{"id": "2602.18186", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18186", "abs": "https://arxiv.org/abs/2602.18186", "authors": ["Seohwa Hwang", "Junyong Park"], "title": "Box Thirding: Anytime Best Arm Identification under Insufficient Sampling", "comment": "29 pages, 5 figures", "summary": "We introduce Box Thirding (B3), a flexible and efficient algorithm for Best Arm Identification (BAI) under fixed-budget constraints. It is designed for both anytime BAI and scenarios with large N, where the number of arms is too large for exhaustive evaluation within a limited budget T. The algorithm employs an iterative ternary comparison: in each iteration, three arms are compared--the best-performing arm is explored further, the median is deferred for future comparisons, and the weakest is discarded. Even without prior knowledge of T, B3 achieves an epsilon-best arm misidentification probability comparable to Successive Halving (SH), which requires T as a predefined parameter, applied to a randomly selected subset of c0 arms that fit within the budget. Empirical results show that B3 outperforms existing methods under limited-budget constraints in terms of simple regret, as demonstrated on the New Yorker Cartoon Caption Contest dataset.", "AI": {"tldr": "B3\u7b97\u6cd5\u662f\u4e00\u79cd\u7528\u4e8e\u56fa\u5b9a\u9884\u7b97\u4e0b\u6700\u4f73\u81c2\u8bc6\u522b\u7684\u7075\u6d3b\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u5143\u6bd4\u8f83\u8fed\u4ee3\u9009\u62e9\uff1a\u6bcf\u8f6e\u6bd4\u8f83\u4e09\u4e2a\u81c2\uff0c\u4fdd\u7559\u6700\u4f73\u81c2\u7ee7\u7eed\u63a2\u7d22\uff0c\u4e2d\u4f4d\u81c2\u63a8\u8fdf\uff0c\u6700\u5dee\u81c2\u4e22\u5f03\u3002\u65e0\u9700\u9884\u77e5\u603b\u9884\u7b97T\uff0c\u5728\u6709\u9650\u9884\u7b97\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u56fa\u5b9a\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u81c2\u6570\u91cfN\u5f88\u5927\u65f6\u65e0\u6cd5\u5728\u6709\u9650\u9884\u7b97T\u5185\u8fdb\u884c\u7a77\u4e3e\u8bc4\u4f30\u7684\u573a\u666f\u3002\u73b0\u6709\u65b9\u6cd5\u5982Successive Halving\u9700\u8981\u9884\u77e5\u603b\u9884\u7b97T\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2dT\u53ef\u80fd\u672a\u77e5\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u4e09\u5143\u6bd4\u8f83\u7b56\u7565\uff1a\u6bcf\u8f6e\u9009\u62e9\u4e09\u4e2a\u81c2\u8fdb\u884c\u6bd4\u8f83\uff0c\u6700\u4f73\u81c2\u7ee7\u7eed\u63a2\u7d22\uff0c\u4e2d\u4f4d\u81c2\u63a8\u8fdf\u5230\u540e\u7eed\u6bd4\u8f83\uff0c\u6700\u5dee\u81c2\u76f4\u63a5\u4e22\u5f03\u3002\u7b97\u6cd5\u65e0\u9700\u9884\u77e5\u603b\u9884\u7b97T\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u81c2\u6570\u91cfN\u7684\u60c5\u51b5\u3002", "result": "B3\u5728epsilon\u6700\u4f73\u81c2\u8bef\u8bc6\u522b\u6982\u7387\u65b9\u9762\u4e0e\u9700\u8981\u9884\u77e5T\u7684Successive Halving\u76f8\u5f53\uff0c\u5728\u6709\u9650\u9884\u7b97\u7ea6\u675f\u4e0b\uff0c\u5728New Yorker Cartoon Caption Contest\u6570\u636e\u96c6\u4e0a\u7684\u7b80\u5355\u9057\u61be\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "B3\u7b97\u6cd5\u4e3a\u56fa\u5b9a\u9884\u7b97\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u81c2\u6570\u91cf\u5927\u4e14\u9884\u7b97\u672a\u77e5\u7684\u573a\u666f\uff0c\u5728\u6709\u9650\u9884\u7b97\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.18261", "categories": ["eess.SY", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.18261", "abs": "https://arxiv.org/abs/2602.18261", "authors": ["Philippe Jacquod", "Laurent Pagnier", "Daniel J. Gauthier"], "title": "Accurate Data-Based State Estimation from Power Loads Inference in Electric Power Grids", "comment": "10 pages, 10 figures", "summary": "Accurate state estimation is a crucial requirement for the reliable operation and control of electric power systems. Here, we construct a data-driven, numerical method to infer missing power load values in large-scale power grids. Given partial observations of power demands, the method estimates the operational state using a linear regression algorithm, exploiting statistical correlations within synthetic training datasets. We evaluate the performance of the method on three synthetic transmission grid test systems. Numerical experiments demonstrate the high accuracy achieved by the method in reconstructing missing demand values under various operating conditions. We further apply the method to real data for the transmission power grid of Switzerland. Despite the restricted number of observations in this dataset, the method infers missing power loads rather accurately. Furthermore, Newton-Raphson power flow solutions show that deviations between true and inferred values for power loads result in smaller deviations between true and inferred values for flows on power lines. This ensures that the estimated operational state correctly captures potential line contingencies. Overall, our results indicate that simple data-based regression techniques can provide an efficient and reliable alternative for state estimation in modern power grids.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a8\u65ad\u5927\u89c4\u6a21\u7535\u7f51\u4e2d\u7f3a\u5931\u7684\u7535\u529b\u8d1f\u8377\u503c\uff0c\u901a\u8fc7\u5229\u7528\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7edf\u8ba1\u76f8\u5173\u6027\u6765\u4f30\u8ba1\u8fd0\u884c\u72b6\u6001\u3002", "motivation": "\u51c6\u786e\u7684\u7535\u529b\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u5bf9\u4e8e\u7535\u7f51\u53ef\u9760\u8fd0\u884c\u548c\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u7535\u7f51\u4e2d\uff0c\u7535\u529b\u8d1f\u8377\u6570\u636e\u5f80\u5f80\u4e0d\u5b8c\u6574\u6216\u7f3a\u5931\uff0c\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63a8\u65ad\u8fd9\u4e9b\u7f3a\u5931\u503c\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u6570\u503c\u65b9\u6cd5\uff0c\u57fa\u4e8e\u90e8\u5206\u89c2\u6d4b\u7684\u7535\u529b\u9700\u6c42\u6570\u636e\uff0c\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u7b97\u6cd5\uff0c\u5229\u7528\u5408\u6210\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u7edf\u8ba1\u76f8\u5173\u6027\u6765\u4f30\u8ba1\u7f3a\u5931\u7684\u7535\u529b\u8d1f\u8377\u503c\u3002", "result": "\u5728\u4e09\u4e2a\u5408\u6210\u8f93\u7535\u7f51\u7edc\u6d4b\u8bd5\u7cfb\u7edf\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u8fd0\u884c\u6761\u4ef6\u4e0b\u90fd\u80fd\u9ad8\u7cbe\u5ea6\u5730\u91cd\u5efa\u7f3a\u5931\u7684\u9700\u6c42\u503c\u3002\u5e94\u7528\u4e8e\u745e\u58eb\u5b9e\u9645\u8f93\u7535\u7535\u7f51\u6570\u636e\u65f6\uff0c\u5c3d\u7ba1\u89c2\u6d4b\u6570\u636e\u6709\u9650\uff0c\u4ecd\u80fd\u8f83\u51c6\u786e\u5730\u63a8\u65ad\u7f3a\u5931\u8d1f\u8377\u3002\u725b\u987f-\u62c9\u592b\u900a\u6f6e\u6d41\u8ba1\u7b97\u8868\u660e\uff0c\u8d1f\u8377\u4f30\u8ba1\u8bef\u5dee\u4f1a\u5bfc\u81f4\u66f4\u5c0f\u7684\u7ebf\u8def\u6f6e\u6d41\u504f\u5dee\uff0c\u786e\u4fdd\u4f30\u8ba1\u72b6\u6001\u80fd\u6b63\u786e\u6355\u6349\u6f5c\u5728\u7684\u7ebf\u8def\u6545\u969c\u3002", "conclusion": "\u57fa\u4e8e\u6570\u636e\u7684\u7b80\u5355\u56de\u5f52\u6280\u672f\u53ef\u4ee5\u4e3a\u73b0\u4ee3\u7535\u7f51\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u9ad8\u6548\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u51c6\u786e\u63a8\u65ad\u7f3a\u5931\u8d1f\u8377\u5e76\u6709\u6548\u6355\u6349\u7535\u7f51\u8fd0\u884c\u72b6\u6001\u3002"}}
{"id": "2602.17841", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17841", "abs": "https://arxiv.org/abs/2602.17841", "authors": ["Dejan Grba"], "title": "Strange Undercurrents: A Critical Outlook on AI's Cultural Influence", "comment": "15 pages", "summary": "While generative artificial intelligence (generative AI) is being examined extensively, some issues it epitomizes call for more refined scrutiny and deeper contextualization. Besides the lack of nuanced understanding of art's continuously changing character in discussions about generative AI's cultural impact, one of the notably underexplored aspects is the conceptual and ideological substrate of AI science and industry whose attributes generative AI propagates by fostering the integration of diverse modes of AI-powered artmaking into the mainstream culture and economy. Taking the current turmoil around the generative AI as a pretext, this paper summarizes a broader study of AI's influence on art notions focusing on the confluence of certain foundational concepts in computer science and ideological vectors of the AI industry that transfer into art, culture, and society. This influence merges diverse and sometimes inconsistent but somehow coalescing philosophical premises, technical ideas, and political views, many of which have unfavorable overtones.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5bf9\u827a\u672f\u89c2\u5ff5\u7684\u5f71\u54cd\uff0c\u805a\u7126\u8ba1\u7b97\u673a\u79d1\u5b66\u57fa\u7840\u6982\u5ff5\u4e0eAI\u4ea7\u4e1a\u610f\u8bc6\u5f62\u6001\u5982\u4f55\u6e17\u900f\u5230\u827a\u672f\u3001\u6587\u5316\u548c\u793e\u4f1a\u4e2d\uff0c\u63ed\u793a\u5176\u4e2d\u4e0d\u5229\u7684\u54f2\u5b66\u3001\u6280\u672f\u548c\u653f\u6cbb\u524d\u63d0\u3002", "motivation": "\u5f53\u524d\u5bf9\u751f\u6210\u5f0fAI\u7684\u8ba8\u8bba\u7f3a\u4e4f\u5bf9\u827a\u672f\u52a8\u6001\u7279\u6027\u7684\u7ec6\u81f4\u7406\u89e3\uff0c\u4e14\u5ffd\u89c6\u4e86AI\u79d1\u5b66\u4e0e\u4ea7\u4e1a\u7684\u6982\u5ff5\u548c\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u5982\u4f55\u901a\u8fc7\u5c06AI\u827a\u672f\u521b\u4f5c\u878d\u5165\u4e3b\u6d41\u6587\u5316\u548c\u7ecf\u6d4e\u800c\u4f20\u64ad\u5176\u5c5e\u6027\u3002", "method": "\u4ee5\u5f53\u524d\u751f\u6210\u5f0fAI\u7684\u4e89\u8bae\u4e3a\u5951\u673a\uff0c\u603b\u7ed3\u5173\u4e8eAI\u5bf9\u827a\u672f\u89c2\u5ff5\u5f71\u54cd\u7684\u66f4\u5e7f\u6cdb\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u673a\u79d1\u5b66\u57fa\u7840\u6982\u5ff5\u4e0eAI\u4ea7\u4e1a\u610f\u8bc6\u5f62\u6001\u5411\u91cf\u5728\u827a\u672f\u9886\u57df\u7684\u4ea4\u6c47\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u5bf9\u827a\u672f\u7684\u5f71\u54cd\u878d\u5408\u4e86\u591a\u6837\u4e14\u6709\u65f6\u4e0d\u4e00\u81f4\u4f46\u53c8\u80fd\u51dd\u805a\u7684\u54f2\u5b66\u524d\u63d0\u3001\u6280\u672f\u7406\u5ff5\u548c\u653f\u6cbb\u89c2\u70b9\uff0c\u5176\u4e2d\u8bb8\u591a\u5177\u6709\u4e0d\u5229\u7684\u57fa\u8c03\u3002", "conclusion": "\u9700\u8981\u66f4\u7cbe\u7ec6\u5730\u5ba1\u89c6\u751f\u6210\u5f0fAI\u6240\u4f53\u73b0\u7684\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u7f6e\u4e8e\u66f4\u6df1\u7684\u8bed\u5883\u4e2d\u7406\u89e3\uff0c\u7279\u522b\u662fAI\u79d1\u5b66\u4e0e\u4ea7\u4e1a\u7684\u6982\u5ff5\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u5982\u4f55\u5f71\u54cd\u827a\u672f\u89c2\u5ff5\u548c\u6587\u5316\u3002"}}
{"id": "2602.17937", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17937", "abs": "https://arxiv.org/abs/2602.17937", "authors": ["Xiaotang Du", "Giwon Hong", "Wai-Chung Kwan", "Rohit Saxena", "Ivan Titov", "Pasquale Minervini", "Emily Allaway"], "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification", "comment": null, "summary": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8eDSPy\u4f18\u5316\u6846\u67b6\u7684\u6307\u4ee4\u4f18\u5316\u65b9\u6cd5\u5728\u8868\u683c\u4e8b\u5b9e\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6307\u4ee4\u4f18\u5316\u80fd\u6301\u7eed\u63d0\u5347\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u4e0d\u540c\u4f18\u5316\u5668\u5bf9\u4e0d\u540c\u63d0\u793a\u6280\u672f\u6709\u7279\u5b9a\u4f18\u52bf\u3002", "motivation": "\u6307\u4ee4\u4f18\u5316\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5728\u8868\u683c\u4e8b\u5b9e\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\u7814\u7a76\u3002", "method": "\u57fa\u4e8eDSPy\u4f18\u5316\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u63d0\u793a\u6280\u672f\uff08\u76f4\u63a5\u9884\u6d4b\u3001\u601d\u7ef4\u94fe\u3001\u5e26SQL\u5de5\u5177\u7684ReAct\u3001\u5e26Python\u6267\u884c\u7684CodeAct\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u4e09\u79cd\u4f18\u5316\u5668\uff08COPRO\u3001MiPROv2\u3001SIMBA\uff09\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6307\u4ee4\u4f18\u5316\u6301\u7eed\u63d0\u5347\u9a8c\u8bc1\u51c6\u786e\u7387\uff1aMiPROv2\u5bf9\u601d\u7ef4\u94fe\u63d0\u793a\u5e26\u6765\u6700\u7a33\u5b9a\u7684\u589e\u76ca\uff0cSIMBA\u5bf9ReAct\u667a\u80fd\u4f53\u63d0\u4f9b\u6700\u5927\u6536\u76ca\uff08\u5c24\u5176\u5728\u66f4\u5927\u6a21\u578b\u89c4\u6a21\u65f6\uff09\u3002\u884c\u4e3a\u5206\u6790\u663e\u793aSIMBA\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u9f13\u52b1\u66f4\u76f4\u63a5\u7684\u63a8\u7406\u8def\u5f84\u3002", "conclusion": "\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u8868\u683c\u4e8b\u5b9e\u68c0\u67e5\u4e2d\u4fdd\u6301\u6709\u6548\uff08\u5c24\u5176\u5bf9\u5c0f\u6a21\u578b\uff09\uff0c\u800c\u57fa\u4e8e\u5927\u6a21\u578b\u7684ReAct\u667a\u80fd\u4f53\u867d\u80fd\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\u4f46\u9700\u8981\u4ed4\u7ec6\u7684\u6307\u4ee4\u4f18\u5316\u3002\u4e0d\u540c\u4f18\u5316\u5668\u9488\u5bf9\u4e0d\u540c\u63d0\u793a\u6280\u672f\u6709\u7279\u5b9a\u4f18\u52bf\u3002"}}
{"id": "2602.18025", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18025", "abs": "https://arxiv.org/abs/2602.18025", "authors": ["Haruki Abe", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets", "comment": "ICLR 2026", "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u5b66\u4e60\u6765\u89e3\u51b3\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u4e2d\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa16\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u7684\u6570\u636e\u96c6\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5177\u8eab\u76f8\u4f3c\u6027\u7684\u5206\u7ec4\u7b56\u7565\u6765\u7f13\u89e3\u5f62\u6001\u5dee\u5f02\u5e26\u6765\u7684\u68af\u5ea6\u51b2\u7a81\u3002", "motivation": "\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\uff0c\u4f46\u4e3a\u6bcf\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u6536\u96c6\u8fd9\u4e9b\u6570\u636e\u6210\u672c\u9ad8\u6602\u3002\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5229\u7528\u4e13\u5bb6\u548c\u6b21\u4f18\u6570\u636e\uff0c\u8de8\u5177\u8eab\u5b66\u4e60\u53ef\u4ee5\u6574\u5408\u4e0d\u540c\u5f62\u6001\u673a\u5668\u4eba\u7684\u8f68\u8ff9\u6570\u636e\uff0c\u7ed3\u5408\u4e24\u8005\u6709\u671b\u964d\u4f4e\u6570\u636e\u6536\u96c6\u6210\u672c\u5e76\u83b7\u53d6\u901a\u7528\u63a7\u5236\u5148\u9a8c\u3002", "method": "1) \u7cfb\u7edf\u5206\u6790\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u8de8\u5177\u8eab\u5b66\u4e60\u7684\u7ed3\u5408\u8303\u5f0f\uff1b2) \u6784\u5efa\u5305\u542b16\u4e2a\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u7684\u8fd0\u52a8\u6570\u636e\u96c6\uff1b3) \u63d0\u51fa\u57fa\u4e8e\u5177\u8eab\u76f8\u4f3c\u6027\u7684\u5206\u7ec4\u7b56\u7565\uff1a\u5c06\u5f62\u6001\u76f8\u4f3c\u7684\u673a\u5668\u4eba\u805a\u7c7b\uff0c\u4f7f\u7528\u7ec4\u68af\u5ea6\u66f4\u65b0\u6a21\u578b\uff0c\u51cf\u5c11\u8de8\u673a\u5668\u4eba\u68af\u5ea6\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60+\u8de8\u5177\u8eab\u5b66\u4e60\u5728\u6b21\u4f18\u8f68\u8ff9\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u7eaf\u884c\u4e3a\u514b\u9686\uff1b2) \u968f\u7740\u6b21\u4f18\u6570\u636e\u6bd4\u4f8b\u548c\u673a\u5668\u4eba\u7c7b\u578b\u589e\u52a0\uff0c\u5f62\u6001\u5dee\u5f02\u5bfc\u81f4\u7684\u68af\u5ea6\u51b2\u7a81\u4f1a\u963b\u788d\u5b66\u4e60\uff1b3) \u63d0\u51fa\u7684\u9759\u6001\u5206\u7ec4\u7b56\u7565\u80fd\u663e\u8457\u51cf\u5c11\u673a\u5668\u4eba\u95f4\u51b2\u7a81\uff0c\u4f18\u4e8e\u73b0\u6709\u51b2\u7a81\u89e3\u51b3\u65b9\u6cd5\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u8de8\u5177\u8eab\u5b66\u4e60\u7684\u7ed3\u5408\u662f\u6709\u6548\u7684\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6b21\u4f18\u6570\u636e\u4e30\u5bcc\u7684\u573a\u666f\u4e0b\u3002\u4f46\u9700\u8981\u89e3\u51b3\u5f62\u6001\u5dee\u5f02\u5e26\u6765\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u57fa\u4e8e\u5177\u8eab\u76f8\u4f3c\u6027\u7684\u5206\u7ec4\u7b56\u7565\u662f\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17878", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.17878", "abs": "https://arxiv.org/abs/2602.17878", "authors": ["Matthew X. Burns", "Jiaming Liang"], "title": "Improved Analysis of Restarted Accelerated Gradient and Augmented Lagrangian Methods via Inexact Proximal Point Frameworks", "comment": "53 pages, 4 figures", "summary": "This paper studies a class of double-loop (inner-outer) algorithms for convex composite optimization. For unconstrained problems, we develop a restarted accelerated composite gradient method that attains the optimal first-order complexity in both the convex and strongly convex settings. For linearly constrained problems, we introduce inexact augmented Lagrangian methods, including a basic method and an outer-accelerated variant, and establish near-optimal first-order complexity for both methods. The established complexity bounds follow from a unified analysis based on new inexact proximal point frameworks that accommodate relative and absolute inexactness, acceleration, and strongly convex objectives. Numerical experiments on LASSO and linearly constrained quadratic programs demonstrate the practical efficiency of the proposed methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u51f8\u590d\u5408\u4f18\u5316\u7684\u53cc\u5faa\u73af\u7b97\u6cd5\uff0c\u63d0\u51fa\u91cd\u542f\u52a0\u901f\u590d\u5408\u68af\u5ea6\u6cd5\u548c\u975e\u7cbe\u786e\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6cd5\uff0c\u5efa\u7acb\u4e86\u6700\u4f18\u6216\u8fd1\u6700\u4f18\u7684\u4e00\u9636\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u51f8\u590d\u5408\u4f18\u5316\u95ee\u9898\u7684\u9ad8\u6548\u6c42\u89e3\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u65e0\u7ea6\u675f\u548c\u7ebf\u6027\u7ea6\u675f\u95ee\u9898\uff0c\u65e8\u5728\u5f00\u53d1\u5177\u6709\u6700\u4f18\u4e00\u9636\u590d\u6742\u5ea6\u7684\u53cc\u5faa\u73af\u7b97\u6cd5\u3002", "method": "1. \u9488\u5bf9\u65e0\u7ea6\u675f\u95ee\u9898\uff1a\u63d0\u51fa\u91cd\u542f\u52a0\u901f\u590d\u5408\u68af\u5ea6\u6cd5\uff1b2. \u9488\u5bf9\u7ebf\u6027\u7ea6\u675f\u95ee\u9898\uff1a\u63d0\u51fa\u975e\u7cbe\u786e\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6cd5\uff08\u57fa\u672c\u65b9\u6cd5\u548c\u5916\u52a0\u901f\u53d8\u4f53\uff09\uff1b3. \u57fa\u4e8e\u65b0\u7684\u975e\u7cbe\u786e\u8fd1\u7aef\u70b9\u6846\u67b6\u8fdb\u884c\u7edf\u4e00\u5206\u6790\uff0c\u652f\u6301\u76f8\u5bf9\u548c\u7edd\u5bf9\u975e\u7cbe\u786e\u6027\u3001\u52a0\u901f\u548c\u5f3a\u51f8\u76ee\u6807\u3002", "result": "1. \u65e0\u7ea6\u675f\u95ee\u9898\uff1a\u5728\u51f8\u548c\u5f3a\u51f8\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u4f18\u4e00\u9636\u590d\u6742\u5ea6\uff1b2. \u7ebf\u6027\u7ea6\u675f\u95ee\u9898\uff1a\u4e24\u79cd\u65b9\u6cd5\u5747\u5efa\u7acb\u4e86\u8fd1\u6700\u4f18\u7684\u4e00\u9636\u590d\u6742\u5ea6\uff1b3. \u5728LASSO\u548c\u7ebf\u6027\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u53cc\u5faa\u73af\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u51f8\u590d\u5408\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u6c42\u89e3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u5ea6\u548c\u5b9e\u9645\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.17683", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17683", "abs": "https://arxiv.org/abs/2602.17683", "authors": ["Irene Iele", "Giulia Romoli", "Daniele Molino", "Elena Mulero Ayll\u00f3n", "Filippo Ruffini", "Paolo Soda", "Matteo Tortora"], "title": "Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates", "comment": null, "summary": "Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u6982\u7387\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u536b\u661fNDVI\u690d\u88ab\u6307\u6570\u9884\u6d4b\uff0c\u901a\u8fc7\u5206\u79bb\u5386\u53f2\u690d\u88ab\u52a8\u6001\u4e0e\u672a\u6765\u5916\u751f\u4fe1\u606f\u5efa\u6a21\uff0c\u89e3\u51b3\u4e91\u5c42\u906e\u6321\u5bfc\u81f4\u7684\u7a00\u758f\u4e0d\u89c4\u5219\u91c7\u6837\u95ee\u9898\u3002", "motivation": "\u7cbe\u51c6\u519c\u4e1a\u9700\u8981\u51c6\u786e\u7684\u77ed\u671f\u690d\u88ab\u52a8\u6001\u9884\u6d4b\uff0c\u4f46\u536b\u661fNDVI\u9884\u6d4b\u9762\u4e34\u4e91\u5c42\u906e\u6321\u5bfc\u81f4\u7684\u7a00\u758f\u4e0d\u89c4\u5219\u91c7\u6837\u4ee5\u53ca\u4f5c\u7269\u751f\u957f\u7684\u5f02\u8d28\u6027\u6c14\u5019\u6761\u4ef6\u7b49\u6311\u6218\u3002", "method": "\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5206\u79bb\u5386\u53f2\u690d\u88ab\u52a8\u6001\u4e0e\u672a\u6765\u5916\u751f\u4fe1\u606f\u5efa\u6a21\uff1b\u5f15\u5165\u65f6\u95f4\u8ddd\u79bb\u52a0\u6743\u5206\u4f4d\u6570\u635f\u5931\u5904\u7406\u4e0d\u89c4\u5219\u91cd\u8bbf\u6a21\u5f0f\uff1b\u52a0\u5165\u7d2f\u79ef\u548c\u6781\u7aef\u5929\u6c14\u7279\u5f81\u5de5\u7a0b\u6355\u6349\u5ef6\u8fdf\u6c14\u8c61\u6548\u5e94\u3002", "result": "\u5728\u6b27\u6d32\u536b\u661f\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u70b9\u9884\u6d4b\u548c\u6982\u7387\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u7edf\u8ba1\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u8fd1\u671f\u65f6\u95f4\u5e8f\u5217\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u9884\u6d4b\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u536b\u661fNDVI\u9884\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u76ee\u6807\u5386\u53f2\u5efa\u6a21\u8d77\u6838\u5fc3\u4f5c\u7528\uff0c\u6c14\u8c61\u534f\u53d8\u91cf\u63d0\u4f9b\u8865\u5145\u589e\u76ca\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.18331", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18331", "abs": "https://arxiv.org/abs/2602.18331", "authors": ["Liang Wu", "Wallace Gian Yion Tan", "Richard D. Braatz", "J\u00e1n Drgo\u0148a"], "title": "Koopman-BoxQP: Solving Large-Scale NMPC at kHz Rates", "comment": "Accepted by the 8th Annual Learning for Dynamics and Control Conference (L4DC 2026). arXiv admin note: text overlap with arXiv:2602.15596", "summary": "Solving large-scale nonlinear model predictive control (NMPC) problems at kilohertz (kHz) rates on standard processors remains a formidable challenge. This paper proposes a Koopman-BoxQP framework that i) learns a linear Koopman high-dimensional model, ii) eliminates the high-dimensional observables to construct a multi-step prediction model of the states and control inputs, iii) penalizes the multi-step prediction model into the objective, which results in a structured box-constrained quadratic program (BoxQP) whose decision variables include both the system states and control inputs, iv) develops a structure-exploited and warm-starting-supported variant of the feasible Mehrotra's interior-point algorithm for BoxQP. Numerical results demonstrate that Koopman-BoxQP can solve a large-scale NMPC problem with $1040$ variables and $2080$ inequalities at a kHz rate.", "AI": {"tldr": "\u63d0\u51faKoopman-BoxQP\u6846\u67b6\uff0c\u901a\u8fc7Koopman\u7ebf\u6027\u5316\u6a21\u578b\u548c\u7ed3\u6784\u5316BoxQP\u6c42\u89e3\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5728kHz\u901f\u7387\u4e0b\u7684\u5b9e\u65f6\u6c42\u89e3", "motivation": "\u5927\u89c4\u6a21\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5728\u6807\u51c6\u5904\u7406\u5668\u4e0a\u5b9e\u73b0kHz\u901f\u7387\u7684\u5b9e\u65f6\u6c42\u89e3\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u6c42\u89e3\u6846\u67b6", "method": "\u63d0\u51fa\u56db\u6b65\u6846\u67b6\uff1a1) \u5b66\u4e60\u7ebf\u6027Koopman\u9ad8\u7ef4\u6a21\u578b\uff1b2) \u6d88\u9664\u9ad8\u7ef4\u89c2\u6d4b\u6784\u5efa\u72b6\u6001\u548c\u63a7\u5236\u8f93\u5165\u7684\u591a\u6b65\u9884\u6d4b\u6a21\u578b\uff1b3) \u5c06\u591a\u6b65\u9884\u6d4b\u6a21\u578b\u60e9\u7f5a\u5230\u76ee\u6807\u51fd\u6570\u4e2d\uff0c\u5f62\u6210\u7ed3\u6784\u5316Box\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\uff1b4) \u5f00\u53d1\u7ed3\u6784\u5229\u7528\u548c\u70ed\u542f\u52a8\u652f\u6301\u7684\u53ef\u884cMehrotra\u5185\u70b9\u7b97\u6cd5", "result": "Koopman-BoxQP\u80fd\u591f\u4ee5kHz\u901f\u7387\u6c42\u89e3\u5177\u67091040\u4e2a\u53d8\u91cf\u548c2080\u4e2a\u4e0d\u7b49\u5f0f\u7684\u5927\u89c4\u6a21NMPC\u95ee\u9898", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5728\u6807\u51c6\u5904\u7406\u5668\u4e0a\u5b9e\u73b0kHz\u901f\u7387\u5b9e\u65f6\u6c42\u89e3\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u63a7\u5236\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.17919", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17919", "abs": "https://arxiv.org/abs/2602.17919", "authors": ["Ruiqing Han", "Hao Cui", "Taha Yasseri"], "title": "Visual Anthropomorphism Shifts Evaluations of Gendered AI Managers", "comment": "Preprint, Under Review", "summary": "This research examines whether competence cues can reduce gender bias in evaluations of AI managers and whether these effects depend on how the AI is represented. Across two preregistered experiments (N = 2,505), each employing a 2 x 2 x 3 design manipulating AI gender, competence, and decision outcome, we compared text-based descriptions of AI managers with visually generated AI faces created using a reverse-correlation paradigm. In the text condition, evaluations were driven by competence rather than gender. When participants received unfavourable decisions, high-competence AI managers were judged as fairer, more competent, and better leaders than low-competence managers, regardless of AI gender. In contrast, when the AI manager was visually represented, competence cues had attenuated influence once facial information was present. Instead, participants showed systematic gender-differentiated responses to AI faces, with feminine-appearing managers evaluated as more competent and more trustworthy than masculine-appearing managers, particularly when delivering favourable outcomes. These gender effects were largely absent when outcomes were unfavourable, suggesting that negative feedback attenuates the influence of both competence information and facial cues. Taken together, these findings show that competence information can mitigate negative reactions to AI managers in text-based interactions, whereas facial anthropomorphism elicits gendered perceptual biases not observed in text-only settings. The results highlight that representational modality plays a critical role in determining when gender stereotypes are activated in evaluations of AI systems and underscore that design choices are consequential for AI governance in evaluative contexts.", "AI": {"tldr": "\u80fd\u529b\u7ebf\u7d22\u53ef\u51cf\u5c11\u6587\u672cAI\u7ba1\u7406\u8005\u7684\u6027\u522b\u504f\u89c1\uff0c\u4f46\u89c6\u89c9AI\u9762\u90e8\u4f1a\u5f15\u53d1\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u7279\u522b\u662f\u5728\u79ef\u6781\u51b3\u7b56\u65f6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u80fd\u529b\u7ebf\u7d22\u662f\u5426\u80fd\u51cf\u5c11\u5bf9AI\u7ba1\u7406\u8005\u7684\u6027\u522b\u504f\u89c1\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6548\u679c\u662f\u5426\u53d6\u51b3\u4e8eAI\u7684\u5448\u73b0\u65b9\u5f0f\uff08\u6587\u672cvs\u89c6\u89c9\uff09\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u9884\u6ce8\u518c\u5b9e\u9a8c(N=2,505)\uff0c\u4f7f\u75282\u00d72\u00d73\u8bbe\u8ba1\uff1a\u64cd\u7eb5AI\u6027\u522b\u3001\u80fd\u529b\u548c\u51b3\u7b56\u7ed3\u679c\uff0c\u6bd4\u8f83\u6587\u672c\u63cf\u8ff0\u4e0e\u901a\u8fc7\u53cd\u5411\u5173\u8054\u8303\u5f0f\u751f\u6210\u7684\u89c6\u89c9AI\u9762\u90e8\u3002", "result": "\u6587\u672c\u6761\u4ef6\u4e0b\uff0c\u8bc4\u4f30\u7531\u80fd\u529b\u800c\u975e\u6027\u522b\u9a71\u52a8\uff1b\u9ad8\u80fd\u529bAI\u7ba1\u7406\u8005\u88ab\u8ba4\u4e3a\u66f4\u516c\u5e73\u3001\u66f4\u6709\u80fd\u529b\u3001\u9886\u5bfc\u529b\u66f4\u5f3a\u3002\u89c6\u89c9\u6761\u4ef6\u4e0b\uff0c\u80fd\u529b\u7ebf\u7d22\u5f71\u54cd\u51cf\u5f31\uff0c\u51fa\u73b0\u7cfb\u7edf\u6027\u6027\u522b\u5dee\u5f02\u53cd\u5e94\uff1a\u5973\u6027\u5316AI\u88ab\u8ba4\u4e3a\u66f4\u6709\u80fd\u529b\u548c\u66f4\u53ef\u4fe1\uff0c\u7279\u522b\u662f\u5728\u63d0\u4f9b\u79ef\u6781\u7ed3\u679c\u65f6\u3002", "conclusion": "\u80fd\u529b\u4fe1\u606f\u53ef\u5728\u6587\u672c\u4ea4\u4e92\u4e2d\u51cf\u8f7b\u5bf9AI\u7ba1\u7406\u8005\u7684\u8d1f\u9762\u53cd\u5e94\uff0c\u800c\u9762\u90e8\u62df\u4eba\u5316\u4f1a\u5f15\u53d1\u6587\u672c\u73af\u5883\u4e2d\u672a\u89c2\u5bdf\u5230\u7684\u6027\u522b\u504f\u89c1\u3002\u5448\u73b0\u65b9\u5f0f\u5bf9\u6027\u522b\u523b\u677f\u5370\u8c61\u7684\u6fc0\u6d3b\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u8bbe\u8ba1\u9009\u62e9\u5bf9AI\u6cbb\u7406\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2602.17949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17949", "abs": "https://arxiv.org/abs/2602.17949", "authors": ["Victoria Blake", "Mathew Miller", "Jamie Novak", "Sze-yuan Ooi", "Blanca Gallego"], "title": "CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications", "comment": "30 pages, 6 figures, 4 tables", "summary": "Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.", "AI": {"tldr": "CUICurate\uff1a\u57fa\u4e8e\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684UMLS\u6982\u5ff5\u96c6\u81ea\u52a8\u6784\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u4e0eLLM\u63a8\u7406\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf", "motivation": "\u4e34\u5e8a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u5de5\u5177\u901a\u5e38\u5c06\u81ea\u7531\u6587\u672c\u6620\u5c04\u5230UMLS\u6982\u5ff5\u552f\u4e00\u6807\u8bc6\u7b26(CUIs)\uff0c\u4f46\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\u9700\u8981\u7684\u662f\u5305\u542b\u76f8\u5173\u540c\u4e49\u8bcd\u3001\u5b50\u7c7b\u578b\u548c\u8d85\u7c7b\u578b\u7684\u6982\u5ff5\u96c6\u3002\u76ee\u524d\u6784\u5efa\u8fd9\u6837\u7684\u6982\u5ff5\u96c6\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u3001\u6267\u884c\u4e0d\u4e00\u81f4\u7684\uff0c\u5e76\u4e14\u73b0\u6709\u5de5\u5177\u652f\u6301\u4e0d\u8db3", "method": "\u63d0\u51faCUICurate\u6846\u67b6\uff1a1)\u6784\u5efaUMLS\u77e5\u8bc6\u56fe\u8c31\u5e76\u8fdb\u884c\u5d4c\u5165\u7528\u4e8e\u8bed\u4e49\u68c0\u7d22\uff1b2)\u9488\u5bf9\u6bcf\u4e2a\u76ee\u6807\u6982\u5ff5\u4eceKG\u4e2d\u68c0\u7d22\u5019\u9009CUIs\uff1b3)\u4f7f\u7528LLM(GPT-5\u548cGPT-5-mini)\u8fdb\u884c\u8fc7\u6ee4\u548c\u5206\u7c7b\uff1b4)\u5728\u4e94\u4e2a\u8bcd\u6c47\u5f02\u8d28\u6027\u4e34\u5e8a\u6982\u5ff5\u4e0a\u8bc4\u4f30\uff0c\u4e0e\u4eba\u5de5\u57fa\u51c6\u548c\u9ec4\u91d1\u6807\u51c6\u6982\u5ff5\u96c6\u6bd4\u8f83", "result": "CUICurate\u751f\u6210\u7684\u6982\u5ff5\u96c6\u6bd4\u4eba\u5de5\u57fa\u51c6\u66f4\u5927\u66f4\u5b8c\u6574\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u7cbe\u786e\u5ea6\u3002GPT-5-mini\u5728\u8fc7\u6ee4\u9636\u6bb5\u53ec\u56de\u7387\u66f4\u9ad8\uff0cGPT-5\u7684\u5206\u7c7b\u7ed3\u679c\u66f4\u63a5\u8fd1\u4e34\u5e8a\u533b\u751f\u5224\u65ad\u3002\u8f93\u51fa\u7a33\u5b9a\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e", "conclusion": "CUICurate\u4e3aUMLS\u6982\u5ff5\u96c6\u6784\u5efa\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u3002\u901a\u8fc7\u6574\u5408\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u548cLLM\u63a8\u7406\uff0c\u8be5\u6846\u67b6\u751f\u6210\u805a\u7126\u7684\u5019\u9009\u6982\u5ff5\u96c6\uff0c\u53ef\u9002\u5e94\u4e0d\u540c\u8868\u578b\u548c\u5206\u6790\u9700\u6c42\u7684\u4e34\u5e8aNLP\u6d41\u7a0b"}}
{"id": "2602.18095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18095", "abs": "https://arxiv.org/abs/2602.18095", "authors": ["Hyunseok Oh", "Sam Stern", "Youngki Lee", "Matthai Philipose"], "title": "Neurosymbolic Language Reasoning as Satisfiability Modulo Theory", "comment": null, "summary": "Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructions, show that Logitext improves both accuracy and coverage. This work is the first that treats LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains.", "AI": {"tldr": "Logitext\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u8bed\u8a00\uff0c\u5c06\u6587\u6863\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u6587\u672c\u7ea6\u675f\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u7ea6\u675f\u8bc4\u4f30\u4e0eSMT\u6c42\u89e3\u5b9e\u73b0\u6587\u672c\u4e0e\u903b\u8f91\u7684\u8054\u5408\u63a8\u7406\uff0c\u6269\u5c55\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7ed3\u5408LLM\u4e0e\u6c42\u89e3\u5668\uff0c\u4f46\u4ec5\u9650\u4e8e\u5b8c\u5168\u5f62\u5f0f\u5316\u7684\u4efb\u52a1\uff08\u5982\u6570\u5b66\u6216\u7a0b\u5e8f\u5408\u6210\uff09\uff0c\u65e0\u6cd5\u5904\u7406\u5177\u6709\u90e8\u5206\u903b\u8f91\u7ed3\u6784\u7684\u81ea\u7136\u6587\u6863\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u4e0e\u903b\u8f91\u6df7\u5408\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLogitext\u795e\u7ecf\u7b26\u53f7\u8bed\u8a00\uff0c\u5c06\u6587\u6863\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u6587\u672c\u7ea6\u675f\uff1b\u5f00\u53d1\u7b97\u6cd5\u5c06LLM\u7ea6\u675f\u8bc4\u4f30\u4e0eSMT\u6c42\u89e3\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u6587\u672c-\u903b\u8f91\u8054\u5408\u63a8\u7406\uff1b\u5c06LLM\u63a8\u7406\u89c6\u4e3aSMT\u7406\u8bba\u7684\u4e00\u90e8\u5206\u3002", "result": "\u5728\u5185\u5bb9\u5ba1\u6838\u65b0\u57fa\u51c6\u3001LegalBench\u548cSuper-Natural Instructions\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLogitext\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8986\u76d6\u7387\uff0c\u662f\u9996\u4e2a\u5c06LLM\u63a8\u7406\u89c6\u4e3aSMT\u7406\u8bba\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u3002", "conclusion": "Logitext\u901a\u8fc7\u5c06\u6587\u6863\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u6587\u672c\u7ea6\u675f\uff0c\u7ed3\u5408LLM\u4e0eSMT\u6c42\u89e3\uff0c\u6210\u529f\u6269\u5c55\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5230\u90e8\u5206\u5f62\u5f0f\u5316\u7684\u81ea\u7136\u6587\u6863\u9886\u57df\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6587\u672c-\u903b\u8f91\u8054\u5408\u63a8\u7406\u3002"}}
{"id": "2602.17879", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.17879", "abs": "https://arxiv.org/abs/2602.17879", "authors": ["Andreas Sojmark", "Zeng Zhang"], "title": "The mean-field control problem for heterogeneous forward-backward systems", "comment": "28 pages", "summary": "We study the problem of mean-field control when the state dynamics are given by general systems of forward-backward stochastic differential equations (FBSDEs) with heterogeneous mean-field interactions. Firstly, we introduce a novel methodology for reducing the well-posedness of such systems to that of a single randomized mean-field FBSDE. As a consequence, we show that, in the fully coupled case, smallness conditions yield existence and uniqueness for both the system itself and the associated variational and adjoint systems. Secondly, we derive a stochastic maximum principle and a verification for the mean-field control problem. This provides necessary and sufficient conditions for optimality.", "AI": {"tldr": "\u7814\u7a76\u5177\u6709\u5f02\u8d28\u5e73\u5747\u573a\u4ea4\u4e92\u7684\u4e00\u822c\u524d\u5411-\u540e\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u7684\u5e73\u5747\u573a\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u7684\u964d\u7ef4\u65b9\u6cd5\uff0c\u5efa\u7acb\u968f\u673a\u6700\u5927\u503c\u539f\u7406\u548c\u9a8c\u8bc1\u5b9a\u7406", "motivation": "\u7814\u7a76\u5177\u6709\u5f02\u8d28\u5e73\u5747\u573a\u4ea4\u4e92\u7684\u4e00\u822cFBSDE\u7cfb\u7edf\u7684\u5e73\u5747\u573a\u63a7\u5236\u95ee\u9898\uff0c\u8fd9\u7c7b\u7cfb\u7edf\u5728\u91d1\u878d\u3001\u7ecf\u6d4e\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u5bf9\u5b8c\u5168\u8026\u5408\u60c5\u51b5\u4e0b\u7684\u9002\u5b9a\u6027\u7814\u7a76\u4e0d\u8db3", "method": "1. \u63d0\u51fa\u65b0\u65b9\u6cd5\u5c06\u539f\u7cfb\u7edf\u7684\u9002\u5b9a\u6027\u95ee\u9898\u7b80\u5316\u4e3a\u5355\u4e2a\u968f\u673a\u5316\u5e73\u5747\u573aFBSDE\u7684\u9002\u5b9a\u6027\u95ee\u9898\uff1b2. \u5728\u5b8c\u5168\u8026\u5408\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5c0f\u6027\u6761\u4ef6\u8bc1\u660e\u7cfb\u7edf\u53ca\u5176\u53d8\u5206\u7cfb\u7edf\u548c\u4f34\u968f\u7cfb\u7edf\u7684\u5b58\u5728\u552f\u4e00\u6027\uff1b3. \u63a8\u5bfc\u5e73\u5747\u573a\u63a7\u5236\u95ee\u9898\u7684\u968f\u673a\u6700\u5927\u503c\u539f\u7406\u548c\u9a8c\u8bc1\u5b9a\u7406", "result": "1. \u5efa\u7acb\u4e86\u5c06\u590d\u6742\u7cfb\u7edf\u9002\u5b9a\u6027\u7b80\u5316\u4e3a\u5355\u4e2a\u65b9\u7a0b\u7684\u65b9\u6cd5\uff1b2. \u5728\u5b8c\u5168\u8026\u5408\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5c0f\u6027\u6761\u4ef6\u8bc1\u660e\u4e86\u7cfb\u7edf\u53ca\u5176\u76f8\u5173\u7cfb\u7edf\u7684\u5b58\u5728\u552f\u4e00\u6027\uff1b3. \u83b7\u5f97\u4e86\u5e73\u5747\u573a\u63a7\u5236\u95ee\u9898\u7684\u5fc5\u8981\u548c\u5145\u5206\u6700\u4f18\u6027\u6761\u4ef6", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5177\u6709\u5f02\u8d28\u5e73\u5747\u573a\u4ea4\u4e92\u7684\u4e00\u822cFBSDE\u7cfb\u7edf\u7684\u5e73\u5747\u573a\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5305\u62ec\u9002\u5b9a\u6027\u5206\u6790\u548c\u6700\u4f18\u6027\u6761\u4ef6\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u9886\u57df\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.17699", "categories": ["cs.LG", "math.RA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17699", "abs": "https://arxiv.org/abs/2602.17699", "authors": ["Chandrasekhar Gokavarapu", "Sudhakar Gadde", "Y. Rajasekhar", "S. R. Bhargava"], "title": "Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure", "comment": null, "summary": "Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5bf9\u9884\u6d4b\u5668\u7684\u98ce\u9669\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u663e\u5f0f\u4e0a\u754c\uff0c\u901a\u8fc7\u53ef\u8ba1\u7b97\u7684\u504f\u79fb\u5ea6\u91cf\u548c\u6a21\u578b\u53c2\u6570\u6765\u4fdd\u8bc1\u98ce\u9669\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u98ce\u9669\u8ba4\u8bc1\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u4fdd\u8bc1\u800c\u975e\u7ecf\u9a8c\u6027\u8bc4\u4f30\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u7684\u89c4\u5f8b\u6027\u548c\u590d\u6742\u6027\u7ea6\u675f\uff0c\u63a8\u5bfc\u51fa\u7531\u53ef\u8ba1\u7b97\u7684\u504f\u79fb\u5ea6\u91cf\u548c\u6a21\u578b\u53c2\u6570\u51b3\u5b9a\u7684\u663e\u5f0f\u98ce\u9669\u4e0a\u754c\u3002", "result": "\u5728\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u9884\u6d4b\u5668\u7684\u8d85\u989d\u98ce\u9669\u5b58\u5728\u663e\u5f0f\u4e0a\u754c\uff0c\u53ef\u901a\u8fc7\u8ba1\u7b97\u504f\u79fb\u5ea6\u91cf\u548c\u6a21\u578b\u53c2\u6570\u6765\u8ba4\u8bc1\uff1b\u6846\u67b6\u652f\u6301\u975e\u5e73\u51e1\u89c4\u6a21\u4e0b\u7684\u6a21\u578b\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u98ce\u9669\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u8ba4\u8bc1\u65b9\u6cd5\uff0c\u660e\u786e\u4e86\u5047\u8bbe\u6761\u4ef6\u3001\u5931\u6548\u6a21\u5f0f\u548c\u975e\u53ef\u8ba4\u8bc1\u673a\u5236\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.18365", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18365", "abs": "https://arxiv.org/abs/2602.18365", "authors": ["Feng Zhao", "Tongxin Zheng", "Dane Schiro", "Xiaochu Wang"], "title": "A Marginal Reliability Impact Based Accreditation Framework for Capacity Markets", "comment": null, "summary": "This paper presents a Marginal Reliability Impact (MRI) based resource accreditation framework for capacity market design. Under this framework, a resource is accredited based on its marginal impact on system reliability, thus aligning the resource accreditation value with its reliability contribution. A key feature of the MRI based accreditation is that the accredited capacities supplied by different resources to the capacity market are substitutable in reliability contribution, a desired feature of homogeneous products. Moreover, with MRI based capacity demand, substitutability between supply and demand for capacity is also achieved. As a result, a capacity market with the MRI based capacity product can better characterize the underlying resource adequacy problem and lead to more efficient market outcomes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fb9\u9645\u53ef\u9760\u6027\u5f71\u54cd(MRI)\u7684\u8d44\u6e90\u8ba4\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u5bb9\u91cf\u5e02\u573a\u8bbe\u8ba1\uff0c\u4f7f\u8ba4\u8bc1\u5bb9\u91cf\u4e0e\u53ef\u9760\u6027\u8d21\u732e\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5bb9\u91cf\u4ea7\u54c1\u7684\u540c\u8d28\u5316\u548c\u4f9b\u9700\u53ef\u66ff\u4ee3\u6027\u3002", "motivation": "\u4f20\u7edf\u5bb9\u91cf\u5e02\u573a\u8bbe\u8ba1\u4e2d\uff0c\u8d44\u6e90\u8ba4\u8bc1\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5176\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u5b9e\u9645\u8d21\u732e\uff0c\u5bfc\u81f4\u5e02\u573a\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u597d\u8868\u5f81\u8d44\u6e90\u5145\u8db3\u6027\u95ee\u9898\u7684\u8ba4\u8bc1\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u8fb9\u9645\u53ef\u9760\u6027\u5f71\u54cd(MRI)\u7684\u8d44\u6e90\u8ba4\u8bc1\u6846\u67b6\uff1a\u6839\u636e\u8d44\u6e90\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u8fb9\u9645\u5f71\u54cd\u8fdb\u884c\u8ba4\u8bc1\uff0c\u4f7f\u8ba4\u8bc1\u5bb9\u91cf\u4e0e\u53ef\u9760\u6027\u8d21\u732e\u5bf9\u9f50\u3002\u8be5\u6846\u67b6\u786e\u4fdd\u4e0d\u540c\u8d44\u6e90\u63d0\u4f9b\u7684\u8ba4\u8bc1\u5bb9\u91cf\u5728\u53ef\u9760\u6027\u8d21\u732e\u4e0a\u53ef\u66ff\u4ee3\uff0c\u5b9e\u73b0\u5bb9\u91cf\u4ea7\u54c1\u7684\u540c\u8d28\u6027\u3002", "result": "MRI\u8ba4\u8bc1\u6846\u67b6\u4f7f\u5bb9\u91cf\u4ea7\u54c1\u5177\u6709\u540c\u8d28\u6027\uff0c\u4e0d\u540c\u8d44\u6e90\u7684\u8ba4\u8bc1\u5bb9\u91cf\u5728\u53ef\u9760\u6027\u8d21\u732e\u4e0a\u53ef\u66ff\u4ee3\u3002\u540c\u65f6\uff0c\u57fa\u4e8eMRI\u7684\u5bb9\u91cf\u9700\u6c42\u4e5f\u4f7f\u5bb9\u91cf\u4f9b\u9700\u4e4b\u95f4\u5177\u6709\u53ef\u66ff\u4ee3\u6027\uff0c\u4ece\u800c\u66f4\u597d\u5730\u8868\u5f81\u8d44\u6e90\u5145\u8db3\u6027\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8eMRI\u7684\u5bb9\u91cf\u5e02\u573a\u8bbe\u8ba1\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u8d44\u6e90\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u8d21\u732e\uff0c\u5b9e\u73b0\u5bb9\u91cf\u4ea7\u54c1\u7684\u540c\u8d28\u5316\u548c\u4f9b\u9700\u53ef\u66ff\u4ee3\u6027\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u9ad8\u6548\u7684\u5e02\u573a\u7ed3\u679c\u3002"}}
{"id": "2602.17932", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17932", "abs": "https://arxiv.org/abs/2602.17932", "authors": ["Anirban Mukherjee", "Hannah Hanwen Chang"], "title": "Operational Agency: A Permeable Legal Fiction for Tracing Culpability in AI Systems", "comment": "Forthcoming, SMU Science and Technology Law Review", "summary": "Modern artificial intelligence (AI) systems act with a high degree of independence yet lack legal personhood-a paradox that fractures doctrines grounded in human-centric notions of mens rea and actus reus. This Article introduces Operational Agency (OA)-a permeable legal fiction structured as an ex post evidentiary framework-and Operational Agency Graph (OAG), a tool for mapping causal interactions among human actors, organizations, and AI systems. OA evaluates an AI's observable operational characteristics: its goal-directedness (as a proxy for intent), predictive processing (as a proxy for foresight), and safety architecture (as a proxy for a standard of care). OAG operationalizes that analysis by embedding these characteristics in a causal graph to trace and apportion culpability among developers, fine-tuners, deployers, and users. Drawing on corporate criminal liability, the innocent-agent doctrine, and secondary and vicarious liability frameworks, the Article shows how OA and OAG strengthen existing doctrines. Across five real-world case studies spanning tort, civil rights, constitutional law, and antitrust, it demonstrates how the framework addresses challenges ranging from autonomous vehicle collisions to algorithmic price-fixing, offering courts a principled evidentiary method-and legislatures and industry a conceptual foundation-to ensure human accountability keeps pace with technological autonomy, without conferring personhood on AI.", "AI": {"tldr": "\u63d0\u51fa\"\u64cd\u4f5c\u4ee3\u7406\"\u6cd5\u5f8b\u6846\u67b6\u548c\"\u64cd\u4f5c\u4ee3\u7406\u56fe\"\u5de5\u5177\uff0c\u901a\u8fc7\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u76ee\u6807\u5bfc\u5411\u6027\u3001\u9884\u6d4b\u5904\u7406\u548c\u5b89\u5168\u67b6\u6784\u7b49\u64cd\u4f5c\u7279\u6027\uff0c\u5728\u4eba\u7c7b\u4e2d\u5fc3\u6cd5\u5f8b\u539f\u5219\u4e0b\u8ffd\u6eafAI\u76f8\u5173\u8d23\u4efb\uff0c\u65e0\u9700\u8d4b\u4e88AI\u6cd5\u5f8b\u4eba\u683c\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u5177\u6709\u9ad8\u5ea6\u81ea\u4e3b\u6027\u4f46\u7f3a\u4e4f\u6cd5\u5f8b\u4eba\u683c\uff0c\u8fd9\u4e0e\u57fa\u4e8e\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\uff08\u72af\u7f6a\u610f\u56fe\u548c\u884c\u4e3a\uff09\u7684\u6cd5\u5f8b\u539f\u5219\u5f62\u6210\u77db\u76fe\u3002\u9700\u8981\u5efa\u7acb\u4e00\u79cd\u6846\u67b6\uff0c\u5728\u4e0d\u8d4b\u4e88AI\u6cd5\u5f8b\u4eba\u683c\u7684\u524d\u63d0\u4e0b\uff0c\u786e\u4fdd\u4eba\u7c7b\u5bf9AI\u884c\u4e3a\u8d1f\u8d23\u3002", "method": "\u63d0\u51fa\"\u64cd\u4f5c\u4ee3\u7406\"\u4f5c\u4e3a\u53ef\u6e17\u900f\u7684\u6cd5\u5f8b\u62df\u5236\uff0c\u6784\u5efa\u4e3a\u4e8b\u540e\u8bc1\u636e\u6846\u67b6\uff1b\u5f00\u53d1\"\u64cd\u4f5c\u4ee3\u7406\u56fe\"\u5de5\u5177\uff0c\u5c06AI\u7684\u53ef\u89c2\u5bdf\u64cd\u4f5c\u7279\u6027\uff08\u76ee\u6807\u5bfc\u5411\u6027\u4f5c\u4e3a\u610f\u56fe\u4ee3\u7406\u3001\u9884\u6d4b\u5904\u7406\u4f5c\u4e3a\u9884\u89c1\u4ee3\u7406\u3001\u5b89\u5168\u67b6\u6784\u4f5c\u4e3a\u6ce8\u610f\u6807\u51c6\u4ee3\u7406\uff09\u5d4c\u5165\u56e0\u679c\u56fe\u4e2d\uff0c\u8ffd\u6eaf\u5f00\u53d1\u8005\u3001\u5fae\u8c03\u8005\u3001\u90e8\u7f72\u8005\u548c\u7528\u6237\u4e4b\u95f4\u7684\u8d23\u4efb\u5206\u914d\u3002", "result": "\u8be5\u6846\u67b6\u501f\u9274\u516c\u53f8\u5211\u4e8b\u8d23\u4efb\u3001\u65e0\u8f9c\u4ee3\u7406\u4eba\u539f\u5219\u4ee5\u53ca\u6b21\u7ea7\u548c\u66ff\u4ee3\u8d23\u4efb\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\uff08\u4fb5\u6743\u3001\u6c11\u6743\u3001\u5baa\u6cd5\u3001\u53cd\u5784\u65ad\uff09\u5c55\u793a\u4e86\u5982\u4f55\u5e94\u5bf9\u4ece\u81ea\u52a8\u9a7e\u9a76\u78b0\u649e\u5230\u7b97\u6cd5\u4ef7\u683c\u64cd\u7eb5\u7b49\u6311\u6218\uff0c\u4e3a\u6cd5\u9662\u63d0\u4f9b\u539f\u5219\u6027\u8bc1\u636e\u65b9\u6cd5\uff0c\u4e3a\u7acb\u6cd5\u548c\u884c\u4e1a\u63d0\u4f9b\u6982\u5ff5\u57fa\u7840\u3002", "conclusion": "\u64cd\u4f5c\u4ee3\u7406\u6846\u67b6\u548c\u64cd\u4f5c\u4ee3\u7406\u56fe\u5de5\u5177\u80fd\u591f\u5728\u4e0d\u8d4b\u4e88AI\u6cd5\u5f8b\u4eba\u683c\u7684\u524d\u63d0\u4e0b\uff0c\u786e\u4fdd\u4eba\u7c7b\u8d23\u4efb\u4e0e\u6280\u672f\u8fdb\u6b65\u540c\u6b65\uff0c\u4e3a\u6cd5\u9662\u3001\u7acb\u6cd5\u8005\u548c\u884c\u4e1a\u63d0\u4f9b\u4e86\u5e94\u5bf9AI\u81ea\u4e3b\u6027\u6311\u6218\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17981", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17981", "abs": "https://arxiv.org/abs/2602.17981", "authors": ["Amine Kobeissi", "Philippe Langlais"], "title": "Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering", "comment": null, "summary": "Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91d1\u878d\u95ee\u7b54\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u867d\u7136\u68c0\u7d22\u5230\u4e86\u6b63\u786e\u6587\u6863\uff0c\u4f46\u9057\u6f0f\u4e86\u5305\u542b\u7b54\u6848\u7684\u5177\u4f53\u9875\u9762\u6216\u5757\uff0c\u5bfc\u81f4\u751f\u6210\u5668\u57fa\u4e8e\u4e0d\u5b8c\u6574\u4e0a\u4e0b\u6587\u8fdb\u884c\u63a8\u65ad\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u9488\u5bf9\u91d1\u878d\u76d1\u7ba1\u6587\u4ef6\u8fdb\u884c\u9886\u57df\u5fae\u8c03\u7684\u9875\u9762\u8bc4\u5206\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9875\u9762\u53ec\u56de\u7387\u548c\u5757\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u91d1\u878d\u95ee\u7b54\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u80fd\u5426\u68c0\u7d22\u5230\u786e\u5207\u7684\u4e0a\u4e0b\u6587\u6765\u652f\u6301\u7b54\u6848\u3002\u5f53\u524d\u5b58\u5728\u4e00\u4e2a\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\uff1a\u68c0\u7d22\u5230\u4e86\u6b63\u786e\u6587\u6863\u4f46\u9057\u6f0f\u4e86\u5305\u542b\u7b54\u6848\u7684\u5177\u4f53\u9875\u9762\u6216\u5757\uff0c\u5bfc\u81f4\u751f\u6210\u5668\u57fa\u4e8e\u4e0d\u5b8c\u6574\u4e0a\u4e0b\u6587\u8fdb\u884c\u63a8\u65ad\u3002\u5c3d\u7ba1\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u91d1\u878dQA\u6587\u732e\u4e2d\u5bf9\u6b64\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "1. \u5728\u591a\u7c92\u5ea6\u7ea7\u522b\uff08\u6587\u6863\u3001\u9875\u9762\u3001\u5757\uff09\u8bc4\u4f30\u68c0\u7d22\u6027\u80fd\uff1b2. \u5f15\u5165\u57fa\u4e8eoracle\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u4e3a\u68c0\u7d22\u548c\u751f\u6210\u6027\u80fd\u63d0\u4f9b\u7ecf\u9a8c\u4e0a\u9650\uff1b3. \u5728FinanceBench\u7684150\u4e2a\u95ee\u9898\u5b50\u96c6\u4e0a\uff0c\u590d\u73b0\u548c\u6bd4\u8f83\u591a\u79cd\u68c0\u7d22\u7b56\u7565\uff08\u7a20\u5bc6\u3001\u7a00\u758f\u3001\u6df7\u5408\u3001\u5206\u5c42\u65b9\u6cd5\uff0c\u5305\u62ec\u91cd\u6392\u5e8f\u548c\u67e5\u8be2\u91cd\u6784\uff09\uff1b4. \u63d0\u51fa\u9886\u57df\u5fae\u8c03\u7684\u9875\u9762\u8bc4\u5206\u5668\uff0c\u5c06\u9875\u9762\u4f5c\u4e3a\u6587\u6863\u548c\u5757\u4e4b\u95f4\u7684\u4e2d\u95f4\u68c0\u7d22\u5355\u5143\uff0c\u4e13\u95e8\u9488\u5bf9\u91d1\u878d\u76d1\u7ba1\u6587\u4ef6\u5fae\u8c03\u53cc\u7f16\u7801\u5668\uff0c\u5229\u7528\u9875\u9762\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "result": "1. \u4e0d\u540c\u65b9\u6cd5\u4e2d\uff0c\u6587\u6863\u53d1\u73b0\u7387\u7684\u63d0\u5347\u5f80\u5f80\u8f6c\u5316\u4e3a\u66f4\u5f3a\u7684\u9875\u9762\u53ec\u56de\u7387\uff1b2. \u4f46oracle\u6027\u80fd\u4ecd\u8868\u660e\u9875\u9762\u548c\u5757\u7ea7\u68c0\u7d22\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff1b3. \u63d0\u51fa\u7684\u9886\u57df\u5fae\u8c03\u9875\u9762\u8bc4\u5206\u5668\u5728\u9875\u9762\u53ec\u56de\u7387\u548c\u5757\u68c0\u7d22\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u91d1\u878d\u95ee\u7b54\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u9886\u57df\u5fae\u8c03\u7684\u9875\u9762\u7ea7\u68c0\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u91d1\u878d\u76d1\u7ba1\u6587\u4ef6\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u9ad8\u98ce\u9669\u7684\u91d1\u878d\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u3002"}}
{"id": "2602.18201", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18201", "abs": "https://arxiv.org/abs/2602.18201", "authors": ["Joseph Bingham", "Netanel Arussy", "Dvir Aran"], "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps", "comment": "10 pages, 2 figures, preprint", "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime", "AI": {"tldr": "SOMtime\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u5373\u4f7f\u654f\u611f\u5c5e\u6027\u88ab\u6392\u9664\uff0c\u4ecd\u80fd\u6062\u590d\u51fa\u4e0e\u5e74\u9f84\u3001\u6536\u5165\u7b49\u654f\u611f\u5c5e\u6027\u5bf9\u9f50\u7684\u6f5c\u5728\u8f74\uff0c\u63ed\u793a\u4e86\"\u516c\u5e73\u901a\u8fc7\u65e0\u77e5\"\u5728\u8868\u793a\u5c42\u9762\u7684\u5931\u8d25\u3002", "motivation": "\u6311\u6218\u65e0\u76d1\u7763\u8868\u793a\u5bf9\u654f\u611f\u5c5e\u6027\u4fdd\u6301\u4e2d\u6027\u7684\u5047\u8bbe\uff0c\u8bc1\u660e\u5373\u4f7f\u654f\u611f\u5c5e\u6027\u88ab\u6392\u9664\u5728\u8bad\u7ec3\u4e4b\u5916\uff0c\u5b83\u4eec\u4ecd\u53ef\u80fd\u4f5c\u4e3a\u4e3b\u5bfc\u6f5c\u5728\u8f74\u51fa\u73b0\uff0c\u63ed\u793a\u65e0\u76d1\u7763\u7ec4\u4ef6\u4e2d\u7684\u516c\u5e73\u98ce\u9669\u3002", "method": "\u4f7f\u7528SOMtime\uff08\u57fa\u4e8e\u9ad8\u5bb9\u91cf\u81ea\u7ec4\u7ec7\u6620\u5c04\u7684\u62d3\u6251\u4fdd\u6301\u8868\u793a\u65b9\u6cd5\uff09\uff0c\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\uff08\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u548c\u4eba\u53e3\u666e\u67e5\u6536\u5165\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83PCA\u3001UMAP\u3001t-SNE\u548c\u81ea\u7f16\u7801\u5668\u7b49\u65b9\u6cd5\u3002", "result": "SOMtime\u6062\u590d\u4e86\u4e0e\u88ab\u6392\u9664\u654f\u611f\u5c5e\u6027\u5bf9\u9f50\u7684\u5355\u8c03\u6392\u5e8f\uff0cSpearman\u76f8\u5173\u6027\u9ad8\u8fbe0.85\uff0c\u800c\u5176\u4ed6\u65b9\u6cd5\u901a\u5e38\u4f4e\u4e8e0.23\uff1b\u65e0\u76d1\u7763\u5206\u5272SOMtime\u5d4c\u5165\u4f1a\u4ea7\u751f\u4eba\u53e3\u7edf\u8ba1\u504f\u659c\u7684\u805a\u7c7b\u3002", "conclusion": "\"\u516c\u5e73\u901a\u8fc7\u65e0\u77e5\"\u5728\u8868\u793a\u5c42\u9762\u5bf9\u4e8e\u6709\u5e8f\u654f\u611f\u5c5e\u6027\u662f\u5931\u8d25\u7684\uff0c\u516c\u5e73\u5ba1\u8ba1\u5fc5\u987b\u6269\u5c55\u5230\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u7684\u65e0\u76d1\u7763\u7ec4\u4ef6\u3002"}}
{"id": "2602.17885", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.17885", "abs": "https://arxiv.org/abs/2602.17885", "authors": ["Christina Frederick", "Haomin Zhou"], "title": "Multi-agent path-planning in a moving medium via Wasserstein Hamiltonian Flow", "comment": null, "summary": "We present a finite dimensional variational model for multi-agent path-planning in which a group of agents traverses from initial positions to a target distribution in a moving medium. The model is derived using the agent-based formulation of the Wasserstein Hamiltonian flows that transport between probability distributions while optimizing a running cost. The objective is the mismatch between their final positions and the target distribution. The constraints are a system of Hamiltonian equations that provide the trajectories of the agents. The free variables on which the optimization is defined form a finite vector of the initial velocities for the agents. The model is solved numerically by the L-BFGS method in conjunction with a shooting strategy. Several simulation examples, including a time-dependent moving medium, are presented to illustrate the performance of the model.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eWasserstein\u54c8\u5bc6\u987f\u6d41\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u53d8\u5206\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u901f\u5ea6\u4f7f\u667a\u80fd\u4f53\u4ece\u521d\u59cb\u4f4d\u7f6e\u79fb\u52a8\u5230\u76ee\u6807\u5206\u5e03\uff0c\u5728\u79fb\u52a8\u4ecb\u8d28\u4e2d\u5b9e\u73b0\u6700\u4f18\u8fd0\u8f93", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5728\u79fb\u52a8\u4ecb\u8d28\u4e2d\u4ece\u521d\u59cb\u4f4d\u7f6e\u5230\u76ee\u6807\u5206\u5e03\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5c06\u6982\u7387\u5206\u5e03\u8fd0\u8f93\u4e0e\u8def\u5f84\u4f18\u5316\u76f8\u7ed3\u5408", "method": "\u57fa\u4e8eWasserstein\u54c8\u5bc6\u987f\u6d41\u7684\u667a\u80fd\u4f53\u516c\u5f0f\u63a8\u5bfc\u53d8\u5206\u6a21\u578b\uff0c\u5c06\u4f18\u5316\u95ee\u9898\u5b9a\u4e49\u4e3a\u521d\u59cb\u901f\u5ea6\u7684\u6709\u9650\u5411\u91cf\uff0c\u4f7f\u7528L-BFGS\u65b9\u6cd5\u548c\u5c04\u51fb\u7b56\u7565\u8fdb\u884c\u6570\u503c\u6c42\u89e3", "result": "\u6a21\u578b\u6210\u529f\u5e94\u7528\u4e8e\u591a\u4e2a\u4eff\u771f\u573a\u666f\uff0c\u5305\u62ec\u65f6\u53d8\u79fb\u52a8\u4ecb\u8d28\uff0c\u5c55\u793a\u4e86\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u6709\u9650\u7ef4\u53d8\u5206\u6a21\u578b\u4e3a\u591a\u667a\u80fd\u4f53\u5728\u79fb\u52a8\u4ecb\u8d28\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u5b66\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4ece\u521d\u59cb\u4f4d\u7f6e\u5230\u76ee\u6807\u5206\u5e03\u7684\u6700\u4f18\u8fd0\u8f93\u95ee\u9898"}}
{"id": "2602.17743", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17743", "abs": "https://arxiv.org/abs/2602.17743", "authors": ["Di Zhang"], "title": "Provable Adversarial Robustness in In-Context Learning", "comment": "16 pages", "summary": "Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($\u03c1$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($\u03c1_{\\text{max}} \\propto \\sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_\u03c1- N_0 \\propto \u03c1^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u5206\u5e03\u9c81\u68d2\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4e3aICL\u5728Wasserstein\u5206\u5e03\u504f\u79fb\u4e0b\u63d0\u4f9b\u6700\u574f\u60c5\u51b5\u6027\u80fd\u4fdd\u8bc1\uff0c\u53d1\u73b0\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u5bb9\u91cf\u5e73\u65b9\u6839\u6210\u6b63\u6bd4\uff0c\u5bf9\u6297\u8bbe\u7f6e\u5e26\u6765\u4e0e\u6270\u52a8\u5e45\u5ea6\u5e73\u65b9\u6210\u6b63\u6bd4\u7684\u6837\u672c\u590d\u6742\u5ea6\u60e9\u7f5a\u3002", "motivation": "\u5f53\u524dICL\u7684\u7406\u8bba\u89e3\u91ca\u5047\u8bbe\u6d4b\u8bd5\u4efb\u52a1\u4e0e\u9884\u8bad\u7ec3\u5206\u5e03\u76f8\u4f3c\uff0c\u5ffd\u7565\u4e86\u5bf9\u6297\u6027\u5206\u5e03\u504f\u79fb\u5bf9\u5b9e\u9645\u53ef\u9760\u6027\u7684\u5a01\u80c1\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\uff0c\u4e3aICL\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165\u5206\u5e03\u9c81\u68d2\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8eWasserstein\u8ddd\u79bb\u91cf\u5316\u5206\u5e03\u504f\u79fb\u3002\u805a\u7126\u7ebf\u6027\u81ea\u6ce8\u610f\u529bTransformer\uff0c\u63a8\u5bfc\u975e\u6e10\u8fd1\u8fb9\u754c\uff0c\u5206\u6790\u5bf9\u6297\u6270\u52a8\u5f3a\u5ea6(\u03c1)\u3001\u6a21\u578b\u5bb9\u91cf(m)\u548c\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570(N)\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u9c81\u68d2\u6027\u968f\u5bb9\u91cf\u5e73\u65b9\u6839\u7f29\u653e(\u03c1_max \u221d \u221am)\uff0c\u5bf9\u6297\u8bbe\u7f6e\u5e26\u6765\u4e0e\u6270\u52a8\u5e45\u5ea6\u5e73\u65b9\u6210\u6b63\u6bd4\u7684\u6837\u672c\u590d\u6742\u5ea6\u60e9\u7f5a(N_\u03c1 - N_0 \u221d \u03c1^2)\u3002\u5408\u6210\u4efb\u52a1\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7f29\u653e\u89c4\u5f8b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63a8\u8fdb\u4e86\u5bf9ICL\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u6781\u9650\u7684\u7406\u8bba\u7406\u89e3\uff0c\u8868\u660e\u6a21\u578b\u5bb9\u91cf\u662f\u5206\u5e03\u9c81\u68d2\u6027\u7684\u57fa\u672c\u8d44\u6e90\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684ICL\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2602.18376", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18376", "abs": "https://arxiv.org/abs/2602.18376", "authors": ["Ashwin P. Dani"], "title": "Parameter Update Laws for Adaptive Control with Affine Equality Parameter Constraints", "comment": null, "summary": "In this paper, constrained parameter update laws for adaptive control with convex equality constraint on the parameters are developed, one based on a gradient only update and the other incorporating concurrent learning (CL) update. The update laws are derived by solving a constrained optimization problem with affine equality constraints. This constrained problem is reformulated as an equivalent unconstrained problem in a new variable, thereby eliminating the equality constraints. The resulting update law is integrated with an adaptive trajectory tracking controller, enabling online learning of the unknown system parameters. Lyapunov stability of the closed-loop system with the equality-constrained parameter update law is established. The effectiveness of the proposed equality-constrained adaptive control law is demonstrated through simulations, validating its ability to maintain constraints on the parameter estimates, achieving convergence to the true parameters for CL-based update law, and achieving asymptotic and exponential tracking performance for constrained gradient and constrained CL-based update laws, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5e26\u51f8\u7b49\u5f0f\u7ea6\u675f\u7684\u81ea\u9002\u5e94\u63a7\u5236\u53c2\u6570\u66f4\u65b0\u5f8b\uff1a\u57fa\u4e8e\u68af\u5ea6\u7684\u66f4\u65b0\u548c\u7ed3\u5408\u5e76\u53d1\u5b66\u4e60\u7684\u66f4\u65b0\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u63a8\u5bfc\uff0c\u5e76\u4e0e\u81ea\u9002\u5e94\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u5668\u96c6\u6210\uff0c\u4fdd\u8bc1\u95ed\u73af\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "motivation": "\u5728\u81ea\u9002\u5e94\u63a7\u5236\u4e2d\uff0c\u6709\u65f6\u9700\u8981\u5bf9\u53c2\u6570\u65bd\u52a0\u7ea6\u675f\uff08\u5982\u7269\u7406\u9650\u5236\u6216\u5148\u9a8c\u77e5\u8bc6\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u51f8\u7b49\u5f0f\u7ea6\u675f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u7ebf\u5b66\u4e60\u672a\u77e5\u7cfb\u7edf\u53c2\u6570\u540c\u65f6\u6ee1\u8db3\u7ea6\u675f\u7684\u66f4\u65b0\u5f8b\u3002", "method": "\u5c06\u5e26\u4eff\u5c04\u7b49\u5f0f\u7ea6\u675f\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u65e0\u7ea6\u675f\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u4e24\u79cd\u66f4\u65b0\u5f8b\uff1a\u57fa\u4e8e\u68af\u5ea6\u7684\u7ea6\u675f\u66f4\u65b0\u5f8b\u548c\u7ed3\u5408\u5e76\u53d1\u5b66\u4e60\u7684\u7ea6\u675f\u66f4\u65b0\u5f8b\uff0c\u5e76\u4e0e\u81ea\u9002\u5e94\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u5668\u96c6\u6210\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a\u80fd\u591f\u4fdd\u6301\u53c2\u6570\u4f30\u8ba1\u7684\u7ea6\u675f\uff0cCL\u66f4\u65b0\u5f8b\u6536\u655b\u5230\u771f\u5b9e\u53c2\u6570\uff0c\u7ea6\u675f\u68af\u5ea6\u66f4\u65b0\u5f8b\u5b9e\u73b0\u6e10\u8fd1\u8ddf\u8e2a\uff0c\u7ea6\u675fCL\u66f4\u65b0\u5f8b\u5b9e\u73b0\u6307\u6570\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u5e26\u51f8\u7b49\u5f0f\u7ea6\u675f\u7684\u81ea\u9002\u5e94\u63a7\u5236\u53c2\u6570\u66f4\u65b0\u5f8b\uff0c\u5efa\u7acb\u4e86\u95ed\u73af\u7cfb\u7edf\u7684Lyapunov\u7a33\u5b9a\u6027\uff0c\u4e3a\u5904\u7406\u53c2\u6570\u7ea6\u675f\u7684\u81ea\u9002\u5e94\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18036", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18036", "abs": "https://arxiv.org/abs/2602.18036", "authors": ["Ankit Singh", "Vidhi Thakur", "Nachiket Tapas"], "title": "Atrial Fibrillation Detection Using Machine Learning", "comment": null, "summary": "Atrial fibrillation (AF) is a common cardiac arrhythmia and a major risk factor for ischemic stroke. Early detection of AF using non-invasive signals can enable timely intervention. In this work, we present a comprehensive machine learning framework for AF detection from simultaneous photoplethysmogram (PPG) and electrocardiogram (ECG) signals. We partitioned continuous recordings from 35 subjects into 525 segments (15 segments of 10,000 samples each at 125Hz per subject). After data cleaning to remove segments with missing samples, 481 segments remained (263 AF, 218 normal).\n  We extracted 22 features per segment, including time-domain statistics (mean, standard deviation, skewness, etc.), bandpower, and heart-rate variability metrics from both PPG and ECG signals. Three classifiers -- ensemble of bagged decision trees, cubic-kernel support vector machine (SVM), and subspace k-nearest neighbors (KNN) -- were trained and evaluated using 10-fold cross-validation and hold-out testing. The subspace KNN achieved the highest test accuracy (98.7\\%), slightly outperforming bagged trees (97.9\\%) and cubic SVM (97.1\\%). Sensitivity (AF detection) and specificity (normal rhythm detection) were all above 95\\% for the top-performing models.\n  The results indicate that ensemble-based machine learning models using combined PPG and ECG features can effectively detect atrial fibrillation. A comparative analysis of model performance along with strengths and limitations of the proposed framework is presented.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408PPG\u548cECG\u4fe1\u53f7\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5fc3\u623f\u98a4\u52a8\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u5176\u4e2d\u5b50\u7a7a\u95f4KNN\u6a21\u578b\u5728\u6d4b\u8bd5\u4e2d\u8fbe\u523098.7%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u5fc3\u623f\u98a4\u52a8\u662f\u5e38\u89c1\u7684\u5fc3\u5f8b\u5931\u5e38\uff0c\u4e5f\u662f\u7f3a\u8840\u6027\u4e2d\u98ce\u7684\u4e3b\u8981\u98ce\u9669\u56e0\u7d20\u3002\u901a\u8fc7\u975e\u4fb5\u5165\u6027\u4fe1\u53f7\u65e9\u671f\u68c0\u6d4bAF\u53ef\u4ee5\u5b9e\u73b0\u53ca\u65f6\u5e72\u9884\uff0c\u4ece\u800c\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "method": "\u4ece35\u540d\u53d7\u8bd5\u8005\u7684\u8fde\u7eed\u8bb0\u5f55\u4e2d\u5206\u5272\u51fa525\u4e2a\u7247\u6bb5\uff08\u6bcf\u4e2a\u7247\u6bb510,000\u4e2a\u6837\u672c\uff0c\u91c7\u6837\u7387125Hz\uff09\uff0c\u7ecf\u8fc7\u6570\u636e\u6e05\u6d17\u540e\u4fdd\u7559481\u4e2a\u7247\u6bb5\u3002\u4ecePPG\u548cECG\u4fe1\u53f7\u4e2d\u63d0\u53d622\u4e2a\u7279\u5f81\uff0c\u5305\u62ec\u65f6\u57df\u7edf\u8ba1\u91cf\u3001\u9891\u5e26\u529f\u7387\u548c\u5fc3\u7387\u53d8\u5f02\u6027\u6307\u6807\u3002\u4f7f\u7528\u4e09\u79cd\u5206\u7c7b\u5668\uff08\u888b\u88c5\u51b3\u7b56\u6811\u96c6\u6210\u3001\u7acb\u65b9\u6838SVM\u3001\u5b50\u7a7a\u95f4KNN\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u91c7\u752810\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4fdd\u7559\u6d4b\u8bd5\u3002", "result": "\u5b50\u7a7a\u95f4KNN\u83b7\u5f97\u6700\u9ad8\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0898.7%\uff09\uff0c\u7565\u4f18\u4e8e\u888b\u88c5\u51b3\u7b56\u6811\uff0897.9%\uff09\u548c\u7acb\u65b9\u6838SVM\uff0897.1%\uff09\u3002\u6240\u6709\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u7684\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u5747\u8d85\u8fc795%\u3002", "conclusion": "\u57fa\u4e8e\u96c6\u6210\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7ed3\u5408PPG\u548cECG\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u5fc3\u623f\u98a4\u52a8\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u6a21\u578b\u6027\u80fd\u7684\u6bd4\u8f83\u5206\u6790\u4ee5\u53ca\u6240\u63d0\u51fa\u6846\u67b6\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2602.18029", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18029", "abs": "https://arxiv.org/abs/2602.18029", "authors": ["Ali El Filali", "In\u00e8s Bedar"], "title": "Towards More Standardized AI Evaluation: From Models to Agents", "comment": "19 pages, 3 figures", "summary": "Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer \"How good is the model?\" but \"Can we trust the system to behave as intended, under change, at scale?\". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u4f20\u7edf\u57fa\u4e8e\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u548c\u805a\u5408\u5206\u6570\u7684\u8bc4\u4f30\u65b9\u6cd5\u5df2\u4e0d\u9002\u7528\u4e8eAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u9700\u8981\u5c06\u8bc4\u4f30\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u6d4b\u91cf\u5b66\u79d1\u800c\u975e\u6027\u80fd\u5267\u573a\uff0c\u4ee5\u5efa\u7acb\u5bf9\u975e\u786e\u5b9a\u6027\u7cfb\u7edf\u7684\u4fe1\u4efb\u3001\u8fed\u4ee3\u548c\u6cbb\u7406\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u4ece\u9759\u6001\u6a21\u578b\u53d1\u5c55\u4e3a\u590d\u5408\u578b\u3001\u4f7f\u7528\u5de5\u5177\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8bc4\u4f30\u7684\u89d2\u8272\u9700\u8981\u4ece\u6700\u7ec8\u68c0\u67e5\u70b9\u8f6c\u53d8\u4e3a\u6838\u5fc3\u63a7\u5236\u529f\u80fd\u3002\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u57fa\u4e8e\u6a21\u578b\u4e2d\u5fc3\u65f6\u4ee3\u7684\u5047\u8bbe\uff08\u9759\u6001\u57fa\u51c6\u3001\u805a\u5408\u5206\u6570\u3001\u4e00\u6b21\u6027\u6210\u529f\u6807\u51c6\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8bc4\u4f30\u4ee3\u7406\u7cfb\u7edf\u65f6\u53d8\u5f97\u6a21\u7cca\u800c\u975e\u6e05\u6670\uff0c\u65e0\u6cd5\u56de\u7b54\"\u6211\u4eec\u80fd\u5426\u4fe1\u4efb\u7cfb\u7edf\u5728\u53d8\u5316\u548c\u89c4\u6a21\u4e0b\u6309\u9884\u671f\u8fd0\u884c\"\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u8bc4\u4f30\u7ba1\u9053\u672c\u8eab\u5f15\u5165\u7684\u9759\u9ed8\u6545\u969c\u6a21\u5f0f\uff0c\u63a2\u8ba8\u4e3a\u4ec0\u4e48\u9ad8\u57fa\u51c6\u5206\u6570\u7ecf\u5e38\u8bef\u5bfc\u56e2\u961f\uff0c\u4ee5\u53ca\u667a\u80fd\u4f53\u7cfb\u7edf\u5982\u4f55\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u6027\u80fd\u6d4b\u91cf\u7684\u610f\u4e49\u3002\u4f5c\u8005\u6ca1\u6709\u63d0\u51fa\u65b0\u7684\u6307\u6807\u6216\u66f4\u96be\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u662f\u91cd\u65b0\u5ba1\u89c6\u8bc4\u4f30\u5728AI\u65f6\u4ee3\uff08\u7279\u522b\u662f\u5bf9\u4e8e\u667a\u80fd\u4f53\uff09\u7684\u89d2\u8272\u5b9a\u4f4d\u3002", "result": "\u8bba\u6587\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5728\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5c40\u9650\u6027\uff1a\u8bc4\u4f30\u7ba1\u9053\u4f1a\u5f15\u5165\u9759\u9ed8\u6545\u969c\u6a21\u5f0f\uff0c\u9ad8\u57fa\u51c6\u5206\u6570\u7ecf\u5e38\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\uff0c\u4ee3\u7406\u7cfb\u7edf\u7684\u975e\u786e\u5b9a\u6027\u7279\u6027\u9700\u8981\u5b8c\u5168\u4e0d\u540c\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "conclusion": "\u8bc4\u4f30\u5e94\u8be5\u4ece\"\u6027\u80fd\u5267\u573a\"\u8f6c\u53d8\u4e3a\u6d4b\u91cf\u5b66\u79d1\uff0c\u4f5c\u4e3a\u5efa\u7acb\u5bf9\u975e\u786e\u5b9a\u6027\u7cfb\u7edf\u4fe1\u4efb\u3001\u652f\u6301\u8fed\u4ee3\u5f00\u53d1\u548c\u5b9e\u73b0\u6709\u6548\u6cbb\u7406\u7684\u6838\u5fc3\u673a\u5236\u3002\u8bc4\u4f30\u9700\u8981\u6210\u4e3aAI\u7cfb\u7edf\u751f\u547d\u5468\u671f\u7684\u6301\u7eed\u63a7\u5236\u529f\u80fd\uff0c\u800c\u975e\u4e00\u6b21\u6027\u68c0\u67e5\u70b9\u3002"}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6269\u6563\u7b56\u7565\u6846\u67b6OMAD\uff0c\u901a\u8fc7\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u8054\u5408\u71b5\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\uff0c\u5728CTDE\u8303\u5f0f\u4e0b\u4f7f\u7528\u8054\u5408\u5206\u5e03\u4ef7\u503c\u51fd\u6570\u4f18\u5316\u5206\u6563\u6269\u6563\u7b56\u7565\uff0c\u572810\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u591a\u6a21\u6001\u8868\u793a\u80fd\u529b\uff0c\u4f46\u5728\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e3b\u8981\u969c\u788d\u662f\u6269\u6563\u6a21\u578b\u7684\u4e0d\u53ef\u5904\u7406\u4f3c\u7136\u6027\u963b\u788d\u4e86\u57fa\u4e8e\u71b5\u7684\u63a2\u7d22\u548c\u534f\u8c03\u3002", "method": "\u63d0\u51faOMAD\u6846\u67b6\uff1a1) \u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u7f29\u653e\u8054\u5408\u71b5\uff0c\u5b9e\u73b0\u65e0\u9700\u53ef\u5904\u7406\u4f3c\u7136\u7684\u6709\u6548\u63a2\u7d22\uff1b2) \u5728CTDE\u8303\u5f0f\u4e0b\u4f7f\u7528\u8054\u5408\u5206\u5e03\u4ef7\u503c\u51fd\u6570\u4f18\u5316\u5206\u6563\u6269\u6563\u7b56\u7565\uff1b3) \u5229\u7528\u53ef\u5904\u7406\u7684\u71b5\u589e\u5f3a\u76ee\u6807\u6307\u5bfc\u6269\u6563\u7b56\u7565\u540c\u65f6\u66f4\u65b0\uff0c\u786e\u4fdd\u7a33\u5b9a\u534f\u8c03\u3002", "result": "\u5728MPE\u548cMAMuJoCo\u768410\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u6280\u672f\uff0c\u6837\u672c\u6548\u7387\u663e\u8457\u63d0\u9ad82.5\u500d\u52305\u500d\u3002", "conclusion": "OMAD\u6210\u529f\u5c06\u6269\u6563\u7b56\u7565\u5e94\u7528\u4e8e\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u548c\u8054\u5408\u5206\u5e03\u4ef7\u503c\u51fd\u6570\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728MARL\u4e2d\u7684\u63a2\u7d22\u548c\u534f\u8c03\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.17968", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2602.17968", "abs": "https://arxiv.org/abs/2602.17968", "authors": ["Robert Parker", "Manuel Garcia", "Russell Bent"], "title": "Exploiting block triangular submatrices in KKT systems", "comment": null, "summary": "We propose a method for solving Karush-Kuhn-Tucker (KKT) systems that exploits block triangular submatrices by first using a Schur complement decomposition to isolate the block triangular submatrices then performing a block backsolve where only diagonal blocks of the block triangular form need to be factorized. We show that factorizing reducible symmetric-indefinite matrices with standard 1$\\times$1 or 2$\\times$2 pivots yields fill-in outside the diagonal blocks of the block triangular form, in contrast to our proposed method. While exploiting a block triangular submatrix has limited fill-in, unsymmetric matrix factorization methods do not reveal inertia, which is required by interior point methods for nonconvex optimization. We show that our target matrix has inertia that is known \\textit{a priori}, letting us compute inertia of the KKT matrix by Sylvester's law. Finally, we demonstrate the computational advantage of this method on KKT systems from optimization problems with neural network surrogates in their constraints. Our method achieves up to 15$\\times$ speedups over state-of-the-art symmetric indefinite matrix factorization methods MA57 and MA86 in a constant-hardware comparison.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5757\u4e09\u89d2\u5b50\u77e9\u9635\u6c42\u89e3KKT\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7Schur\u8865\u5206\u89e3\u9694\u79bb\u5757\u4e09\u89d2\u7ed3\u6784\uff0c\u4ec5\u9700\u5206\u89e3\u5bf9\u89d2\u5757\uff0c\u76f8\u6bd4\u4f20\u7edf\u5bf9\u79f0\u4e0d\u5b9a\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\u51cf\u5c11\u586b\u5145\uff0c\u5728\u795e\u7ecf\u7f51\u7edc\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e0a\u5b9e\u73b015\u500d\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u5bf9\u79f0\u4e0d\u5b9a\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\uff08\u5982MA57\u3001MA86\uff09\u5728\u5904\u7406KKT\u7cfb\u7edf\u65f6\uff0c\u5373\u4f7f\u5b58\u5728\u5757\u4e09\u89d2\u7ed3\u6784\uff0c\u4e5f\u4f1a\u4ea7\u751f\u5927\u91cf\u586b\u5145\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u3002\u540c\u65f6\uff0c\u975e\u5bf9\u79f0\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u60ef\u6027\u4fe1\u606f\uff0c\u800c\u60ef\u6027\u4fe1\u606f\u5bf9\u975e\u51f8\u4f18\u5316\u7684\u5185\u70b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9996\u5148\u4f7f\u7528Schur\u8865\u5206\u89e3\u9694\u79bbKKT\u77e9\u9635\u4e2d\u7684\u5757\u4e09\u89d2\u5b50\u77e9\u9635\uff0c\u7136\u540e\u8fdb\u884c\u5757\u56de\u4ee3\u6c42\u89e3\uff0c\u4ec5\u9700\u5206\u89e3\u5757\u4e09\u89d2\u5f62\u5f0f\u7684\u5bf9\u89d2\u5757\u3002\u5229\u7528\u76ee\u6807\u77e9\u9635\u7684\u5148\u9a8c\u60ef\u6027\u77e5\u8bc6\uff0c\u901a\u8fc7Sylvester\u5b9a\u5f8b\u8ba1\u7b97KKT\u77e9\u9635\u7684\u60ef\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u795e\u7ecf\u7f51\u7edc\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u7684KKT\u7cfb\u7edf\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5bf9\u79f0\u4e0d\u5b9a\u77e9\u9635\u5206\u89e3\u65b9\u6cd5MA57\u548cMA86\uff0c\u5728\u76f8\u540c\u786c\u4ef6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe15\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5229\u7528KKT\u77e9\u9635\u7684\u5757\u4e09\u89d2\u7ed3\u6784\uff0c\u51cf\u5c11\u4e86\u586b\u5145\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u60ef\u6027\u4fe1\u606f\uff0c\u4e3a\u5305\u542b\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684KKT\u7cfb\u7edf\u6c42\u89e3\u65b9\u6848\u3002"}}
{"id": "2602.17744", "categories": ["cs.LG", "cs.CL", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17744", "abs": "https://arxiv.org/abs/2602.17744", "authors": ["Di Zhang", "Jiaqi Xing"], "title": "Bayesian Optimality of In-Context Learning with Selective State Spaces", "comment": "17 pages", "summary": "We propose Bayesian optimal sequential prediction as a new principle for understanding in-context learning (ICL). Unlike interpretations framing Transformers as performing implicit gradient descent, we formalize ICL as meta-learning over latent sequence tasks. For tasks governed by Linear Gaussian State Space Models (LG-SSMs), we prove a meta-trained selective SSM asymptotically implements the Bayes-optimal predictor, converging to the posterior predictive mean. We further establish a statistical separation from gradient descent, constructing tasks with temporally correlated noise where the optimal Bayesian predictor strictly outperforms any empirical risk minimization (ERM) estimator. Since Transformers can be seen as performing implicit ERM, this demonstrates selective SSMs achieve lower asymptotic risk due to superior statistical efficiency. Experiments on synthetic LG-SSM tasks and a character-level Markov benchmark confirm selective SSMs converge faster to Bayes-optimal risk, show superior sample efficiency with longer contexts in structured-noise settings, and track latent states more robustly than linear Transformers. This reframes ICL from \"implicit optimization\" to \"optimal inference,\" explaining the efficiency of selective SSMs and offering a principled basis for architecture design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u8d1d\u53f6\u65af\u6700\u4f18\u5e8f\u5217\u9884\u6d4b\u4f5c\u4e3a\u7406\u89e3\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b0\u539f\u5219\uff0c\u8bc1\u660e\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u80fd\u5b9e\u73b0\u8d1d\u53f6\u65af\u6700\u4f18\u9884\u6d4b\u5668\uff0c\u5728\u7edf\u8ba1\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684Transformer\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c06Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u89e3\u91ca\u4e3a\u9690\u5f0f\u68af\u5ea6\u4e0b\u964d\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e0d\u80fd\u5b8c\u5168\u89e3\u91ca\u5176\u6548\u7387\u3002\u4ed6\u4eec\u5e0c\u671b\u4ece\u8d1d\u53f6\u65af\u6700\u4f18\u63a8\u7406\u7684\u89d2\u5ea6\u91cd\u65b0\u7406\u89e3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4e3a\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u66f4\u539f\u5219\u6027\u7684\u57fa\u7840\u3002", "method": "\u5c06\u4e0a\u4e0b\u6587\u5b66\u4e60\u5f62\u5f0f\u5316\u4e3a\u5143\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u7ebf\u6027\u9ad8\u65af\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4efb\u52a1\u4e2d\uff0c\u8bc1\u660e\u5143\u8bad\u7ec3\u7684\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u80fd\u6e10\u8fd1\u5b9e\u73b0\u8d1d\u53f6\u65af\u6700\u4f18\u9884\u6d4b\u5668\uff08\u540e\u9a8c\u9884\u6d4b\u5747\u503c\uff09\u3002\u6784\u5efa\u5177\u6709\u65f6\u95f4\u76f8\u5173\u566a\u58f0\u7684\u4efb\u52a1\uff0c\u5c55\u793a\u8d1d\u53f6\u65af\u9884\u6d4b\u5668\u4e25\u683c\u4f18\u4e8e\u4efb\u4f55\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u4f30\u8ba1\u5668\u3002", "result": "\u5728\u5408\u6210LG-SSM\u4efb\u52a1\u548c\u5b57\u7b26\u7ea7\u9a6c\u5c14\u53ef\u592b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9009\u62e9\u6027SSM\u66f4\u5feb\u6536\u655b\u5230\u8d1d\u53f6\u65af\u6700\u4f18\u98ce\u9669\uff0c\u5728\u7ed3\u6784\u5316\u566a\u58f0\u8bbe\u7f6e\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\uff0c\u6bd4\u7ebf\u6027Transformer\u66f4\u7a33\u5065\u5730\u8ddf\u8e2a\u6f5c\u5728\u72b6\u6001\u3002", "conclusion": "\u5c06\u4e0a\u4e0b\u6587\u5b66\u4e60\u4ece\"\u9690\u5f0f\u4f18\u5316\"\u91cd\u65b0\u6846\u67b6\u4e3a\"\u6700\u4f18\u63a8\u7406\"\uff0c\u89e3\u91ca\u4e86\u9009\u62e9\u6027SSM\u7684\u6548\u7387\uff0c\u5e76\u4e3a\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002\u8d1d\u53f6\u65af\u6700\u4f18\u5e8f\u5217\u9884\u6d4b\u4e3a\u7406\u89e3\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2602.17677", "categories": ["cs.LG", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17677", "abs": "https://arxiv.org/abs/2602.17677", "authors": ["Sutej Kulgod", "Sean Ye", "Sanchit Tanwar", "Christoffer Heckman"], "title": "Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving", "comment": "7 pages, 2 figures", "summary": "Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51faMCQA\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6587\u672c\u7ebf\u7d22\u6f0f\u6d1e\uff0c\u5bfc\u81f4VLM\u65e0\u9700\u89c6\u89c9\u8f93\u5165\u5373\u53ef\u83b7\u5f97\u9ad8\u51c6\u786e\u7387\u3002\u4f5c\u8005\u63d0\u51fa\u65b9\u6cd5\u5c06\u76f2\u51c6\u786e\u7387\u4ece+66.9%\u964d\u81f3+2.9%\uff0c\u5f3a\u5236\u6a21\u578b\u4f9d\u8d56\u89c6\u89c9\u57fa\u7840\u3002", "motivation": "\u73b0\u6709MCQA\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff1a\u5408\u6210\u751f\u6210\u7684MCQA\u6570\u636e\u5bb9\u6613\u88ab\u6a21\u578b\u5229\u7528\u6587\u672c\u7ebf\u7d22\u800c\u975e\u89c6\u89c9\u5185\u5bb9\u6765\u56de\u7b54\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u8bc4\u4f30\u5931\u771f\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u6b63\u786e\u7b54\u6848\u4e0e\u8bed\u8a00\u4f2a\u5f71\uff08\u6d88\u9664\u6587\u672c\u7ebf\u7d22\uff09\uff0c\u5e76\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5f3a\u5236\u6a21\u578b\u4f9d\u8d56\u89c6\u89c9\u57fa\u7840\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u975e\u5229\u7528\u6587\u672c\u6a21\u5f0f\u5339\u914d\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u88ab\u5229\u7528\u7684\u6587\u672c\u6377\u5f84\uff0c\u5c06\u76f2\u51c6\u786e\u7387\uff08\u65e0\u89c6\u89c9\u8f93\u5165\u65f6\u7684\u51c6\u786e\u7387\uff09\u4ece\u6bd4\u968f\u673a\u9ad866.9%\u964d\u4f4e\u5230\u4ec5\u6bd4\u968f\u673a\u9ad82.9%\uff0c\u4f7f\u6027\u80fd\u8bc4\u4f30\u66f4\u771f\u5b9e\u53cd\u6620\u611f\u77e5\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u57fa\u4e8eMCQA\u7684VLM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u786e\u4fdd\u6027\u80fd\u6d4b\u91cf\u771f\u6b63\u53cd\u6620\u89c6\u89c9\u7406\u89e3\u800c\u975e\u6587\u672c\u6a21\u5f0f\u5339\u914d\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u6d88\u9664\u6587\u672c\u7ebf\u7d22\u6f0f\u6d1e\uff0c\u4f7f\u8bc4\u4f30\u66f4\u53ef\u9760\u3002"}}
{"id": "2602.18382", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18382", "abs": "https://arxiv.org/abs/2602.18382", "authors": ["Yu Kawano", "Simone Betteti", "Alexander Davydov", "Francesco Bullo"], "title": "Incremental Input-to-State Stability and Equilibrium Tracking for Stochastic Contracting Dynamics", "comment": null, "summary": "In this paper, we study the contractivity of nonlinear stochastic differential equations (SDEs) driven by deterministic inputs and Brownian motions. Given a weighted $\\ell_2$-norm for the state space, we show that an SDE is incrementally noise- and input-to-state stable if its vector field is uniformly contracting in the state and uniformly Lipschitz in the input. This result is applied to error estimation for time-varying equilibrium tracking in the presence of noise affecting both the system dynamics and the input signals. We consider both Ornstein-Uhlenbeck processes modeling unbounded noise and Jacobi diffusion processes modeling bounded noise. Finally, we turn our attention to the associated Fokker-Planck equation of an SDE. For this context, we prove incremental input-to-state stability with respect to an arbitrary $p$-Wasserstein metric when the drift vector field is uniformly contracting in the state and uniformly Lipschitz in the input with respect to an arbitrary norm.", "AI": {"tldr": "\u7814\u7a76\u975e\u7ebf\u6027\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5728\u786e\u5b9a\u6027\u8f93\u5165\u548c\u5e03\u6717\u8fd0\u52a8\u9a71\u52a8\u4e0b\u7684\u6536\u7f29\u6027\uff0c\u8bc1\u660e\u5f53\u5411\u91cf\u573a\u5728\u72b6\u6001\u4e0a\u4e00\u81f4\u6536\u7f29\u4e14\u5728\u8f93\u5165\u4e0a\u4e00\u81f4Lipschitz\u65f6\uff0cSDE\u5177\u6709\u589e\u91cf\u566a\u58f0\u548c\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\uff0c\u5e76\u5e94\u7528\u4e8e\u542b\u566a\u58f0\u7684\u65f6\u53d8\u5e73\u8861\u8ddf\u8e2a\u8bef\u5dee\u4f30\u8ba1\u3002", "motivation": "\u7814\u7a76\u975e\u7ebf\u6027\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7684\u6536\u7f29\u6027\uff0c\u4e3a\u542b\u566a\u58f0\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u5206\u6790\u548c\u8bef\u5dee\u4f30\u8ba1\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u8f93\u5165\u4fe1\u53f7\u90fd\u53d7\u566a\u58f0\u5f71\u54cd\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u52a0\u6743\u2113\u2082\u8303\u6570\u5206\u6790\u72b6\u6001\u7a7a\u95f4\uff0c\u8bc1\u660e\u5f53SDE\u7684\u5411\u91cf\u573a\u5728\u72b6\u6001\u4e0a\u4e00\u81f4\u6536\u7f29\u4e14\u5728\u8f93\u5165\u4e0a\u4e00\u81f4Lipschitz\u65f6\uff0c\u7cfb\u7edf\u5177\u6709\u589e\u91cf\u566a\u58f0\u548c\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\u3002\u5c06\u7ed3\u679c\u5e94\u7528\u4e8eOrnstein-Uhlenbeck\u8fc7\u7a0b\uff08\u65e0\u754c\u566a\u58f0\uff09\u548cJacobi\u6269\u6563\u8fc7\u7a0b\uff08\u6709\u754c\u566a\u58f0\uff09\u7684\u8bef\u5dee\u4f30\u8ba1\uff0c\u5e76\u7814\u7a76\u76f8\u5173Fokker-Planck\u65b9\u7a0b\u5728\u4efb\u610fp-Wasserstein\u5ea6\u91cf\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5efa\u7acb\u4e86SDE\u6536\u7f29\u6027\u4e0e\u589e\u91cf\u566a\u58f0/\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u4e3a\u542b\u566a\u58f0\u7cfb\u7edf\u7684\u65f6\u53d8\u5e73\u8861\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u8bef\u5dee\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86Fokker-Planck\u65b9\u7a0b\u5728\u4efb\u610fp-Wasserstein\u5ea6\u91cf\u4e0b\u7684\u589e\u91cf\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5411\u91cf\u573a\u7684\u6536\u7f29\u6027\u548cLipschitz\u6027\u8d28\u662f\u4fdd\u8bc1\u975e\u7ebf\u6027\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7a33\u5b9a\u6027\u7684\u5173\u952e\u6761\u4ef6\uff0c\u8be5\u7406\u8bba\u6846\u67b6\u53ef\u6709\u6548\u5904\u7406\u542b\u566a\u58f0\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u5206\u6790\u548c\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.18139", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18139", "abs": "https://arxiv.org/abs/2602.18139", "authors": ["L. C. R. Patell", "O. E. Guest"], "title": "Demonstrating Restraint", "comment": null, "summary": "Some have claimed that the future development of powerful AI systems would enable the United States to shift the international balance of power dramatically in its favor. Such a feat may not be technically possible; even so, if American AI development is perceived as a sufficiently severe threat by its nation-state adversaries, then the risk that they take extreme preventive action against the United States may rise. To bolster its security against preventive action, the United States could aim to pursue a strategy of restraint by demonstrating that it would not use powerful AI to threaten the survival of other nations. Drawing from the international relations literature that explores how states can make credible commitments, we sketch a set of options that the United States could employ to implement this strategy. In the most challenging setting, where it is certain that the US will unilaterally obtain powerful new capabilities, it is difficult to credibly commit to restraint, though an approach that layers significant policy effort with technical breakthroughs may make credibility achievable. If an adversary has realistic levels of uncertainty about the capabilities and intentions of the United States, a strategy of restraint becomes more feasible. Though restraint faces difficulties, it deserves to be weighed against alternative strategies that have been proposed for avoiding conflict during the transition to a world with advanced AI.", "AI": {"tldr": "\u7f8e\u56fd\u53ef\u901a\u8fc7\u5c55\u793aAI\u514b\u5236\u7b56\u7565\u6765\u964d\u4f4e\u5bf9\u624b\u91c7\u53d6\u9884\u9632\u6027\u884c\u52a8\u7684\u98ce\u9669\uff0c\u901a\u8fc7\u53ef\u4fe1\u627f\u8bfa\u673a\u5236\u4fdd\u969c\u56fd\u5bb6\u5b89\u5168", "motivation": "\u7f8e\u56fdAI\u53d1\u5c55\u53ef\u80fd\u88ab\u5bf9\u624b\u89c6\u4e3a\u4e25\u91cd\u5a01\u80c1\uff0c\u5f15\u53d1\u9884\u9632\u6027\u519b\u4e8b\u884c\u52a8\u98ce\u9669\uff0c\u9700\u8981\u63a2\u7d22\u964d\u4f4e\u51b2\u7a81\u98ce\u9669\u7684\u7b56\u7565", "method": "\u501f\u9274\u56fd\u9645\u5173\u7cfb\u5b66\u4e2d\u7684\u53ef\u4fe1\u627f\u8bfa\u7406\u8bba\uff0c\u63d0\u51fa\u7f8e\u56fd\u53ef\u901a\u8fc7\u5c55\u793a\u514b\u5236\u610f\u56fe\u3001\u653f\u7b56\u52aa\u529b\u4e0e\u6280\u672f\u7a81\u7834\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\u5efa\u7acb\u53ef\u4fe1\u627f\u8bfa", "result": "\u5728\u5bf9\u624b\u5bf9\u7f8e\u56fd\u80fd\u529b\u548c\u610f\u56fe\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u514b\u5236\u7b56\u7565\u66f4\u4e3a\u53ef\u884c\uff1b\u5728\u5355\u65b9\u9762\u83b7\u5f97\u5f3a\u5927\u80fd\u529b\u7684\u6700\u56f0\u96be\u60c5\u5883\u4e0b\uff0c\u9700\u8981\u653f\u7b56\u52aa\u529b\u4e0e\u6280\u672f\u7a81\u7834\u76f8\u7ed3\u5408\u624d\u80fd\u5b9e\u73b0\u53ef\u4fe1\u627f\u8bfa", "conclusion": "\u5c3d\u7ba1\u514b\u5236\u7b56\u7565\u9762\u4e34\u56f0\u96be\uff0c\u4f46\u5e94\u4e0e\u5176\u4ed6\u907f\u514dAI\u8fc7\u6e21\u671f\u51b2\u7a81\u7684\u7b56\u7565\u8fdb\u884c\u6743\u8861\uff0c\u4f5c\u4e3a\u964d\u4f4e\u9884\u9632\u6027\u884c\u52a8\u98ce\u9669\u7684\u6709\u6548\u9014\u5f84"}}
{"id": "2602.18092", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18092", "abs": "https://arxiv.org/abs/2602.18092", "authors": ["Matthew DiGiuseppe", "Joshua Robison"], "title": "Perceived Political Bias in LLMs Reduces Persuasive Abilities", "comment": "39 pages, 10 figures", "summary": "Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u7528\u6237\u8ba4\u4e3aAI\u804a\u5929\u673a\u5668\u4eba\u5b58\u5728\u515a\u6d3e\u504f\u89c1\u65f6\uff0c\u5176\u7ea0\u6b63\u9519\u8bef\u89c2\u5ff5\u7684\u8bf4\u670d\u529b\u4f1a\u4e0b\u964d28%\uff0c\u8868\u660eAI\u7684\u653f\u6cbb\u4e2d\u7acb\u6027\u611f\u77e5\u5bf9\u5176\u8bf4\u670d\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u968f\u7740LLMs\u8fdb\u5165\u515a\u6d3e\u51b2\u7a81\u9886\u57df\uff0c\u7cbe\u82f1\u9636\u5c42\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u5b83\u4eec\u63cf\u7ed8\u6210\u5177\u6709\u610f\u8bc6\u5f62\u6001\u503e\u5411\u3002\u672c\u7814\u7a76\u65e8\u5728\u6d4b\u8bd5\u8fd9\u4e9b\u53ef\u4fe1\u5ea6\u653b\u51fb\u662f\u5426\u4f1a\u964d\u4f4e\u57fa\u4e8eLLM\u7684\u8bf4\u670d\u6548\u679c\uff0c\u63a2\u8ba8AI\u7684\u653f\u6cbb\u4e2d\u7acb\u6027\u611f\u77e5\u5bf9\u5176\u7ea0\u6b63\u516c\u5171\u8bef\u89e3\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u5728\u7f8e\u56fd\u8fdb\u884c\u4e86\u4e00\u9879\u9884\u6ce8\u518c\u8c03\u67e5\u5b9e\u9a8c\uff08N=2144\uff09\uff0c\u53c2\u4e0e\u8005\u4e0eChatGPT\u8fdb\u884c\u4e09\u8f6e\u5bf9\u8bdd\uff0c\u8ba8\u8bba\u4e2a\u4eba\u6301\u6709\u7684\u7ecf\u6d4e\u653f\u7b56\u8bef\u89e3\u3002\u5b9e\u9a8c\u7ec4\u5728\u5bf9\u8bdd\u524d\u6536\u5230\u7b80\u77ed\u4fe1\u606f\uff0c\u8868\u660eLLM\u5bf9\u53c2\u4e0e\u8005\u6240\u5c5e\u515a\u6d3e\u5b58\u5728\u504f\u89c1\uff0c\u5bf9\u7167\u7ec4\u5219\u4fdd\u6301\u4e2d\u7acb\u3002", "result": "\u4e0e\u4e2d\u7acb\u5bf9\u7167\u7ec4\u76f8\u6bd4\uff0c\u6536\u5230AI\u5b58\u5728\u515a\u6d3e\u504f\u89c1\u8b66\u544a\u7684\u53c2\u4e0e\u8005\uff0c\u5176\u88ab\u8bf4\u670d\u7a0b\u5ea6\u964d\u4f4e\u4e8628%\u3002\u5bf9\u8bdd\u8bb0\u5f55\u5206\u6790\u663e\u793a\uff0c\u8b66\u544a\u6539\u53d8\u4e86\u4e92\u52a8\u6a21\u5f0f\uff1a\u53d7\u8bbf\u8005\u66f4\u9891\u7e41\u5730\u53cd\u9a73\uff0c\u4e14\u53c2\u4e0e\u5ea6\u66f4\u4e0d\u5f00\u653e\u3002", "conclusion": "\u5bf9\u8bdd\u5f0fAI\u7684\u8bf4\u670d\u6548\u679c\u5177\u6709\u653f\u6cbb\u6761\u4ef6\u6027\uff0c\u53d7\u5230\u515a\u6d3e\u4e00\u81f4\u6027\u611f\u77e5\u7684\u9650\u5236\u3002\u5f53\u7528\u6237\u8ba4\u4e3aAI\u5b58\u5728\u515a\u6d3e\u504f\u89c1\u65f6\uff0c\u5176\u7ea0\u6b63\u9519\u8bef\u89c2\u5ff5\u7684\u6709\u6548\u6027\u4f1a\u663e\u8457\u964d\u4f4e\uff0c\u8fd9\u51f8\u663e\u4e86\u4fdd\u6301AI\u653f\u6cbb\u4e2d\u7acb\u6027\u611f\u77e5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.17684", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17684", "abs": "https://arxiv.org/abs/2602.17684", "authors": ["Xiao Zhu", "Xinyu Zhou", "Boyu Zhu", "Hanxu Hu", "Mingzhe Du", "Haotian Zhang", "Huiming Wang", "Zhijiang Guo"], "title": "CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).", "AI": {"tldr": "CodeScaler\u662f\u4e00\u4e2a\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u6269\u5c55\u4ee3\u7801\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u7406\uff0c\u901a\u8fc7\u8bed\u6cd5\u611f\u77e5\u7684\u4ee3\u7801\u63d0\u53d6\u548c\u5956\u52b1\u5851\u9020\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u57fa\u4e8e\u6267\u884c\u7684RL\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4f9d\u8d56\u5355\u5143\u6d4b\u8bd5\u7684\u6267\u884c\u53cd\u9988\uff0c\u4f46\u5176\u53ef\u6269\u5c55\u6027\u53d7\u5230\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u7684\u9650\u5236\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\u6765\u6269\u5c55\u4ee3\u7801\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u7406\u3002", "method": "CodeScaler\u662f\u4e00\u4e2a\u6267\u884c\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\uff0c\u57fa\u4e8e\u4ece\u5df2\u9a8c\u8bc1\u4ee3\u7801\u95ee\u9898\u4e2d\u7cbe\u5fc3\u7b56\u5212\u7684\u504f\u597d\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u91c7\u7528\u8bed\u6cd5\u611f\u77e5\u7684\u4ee3\u7801\u63d0\u53d6\u548c\u4fdd\u6301\u6709\u6548\u6027\u7684\u5956\u52b1\u5851\u9020\u6280\u672f\uff0c\u786e\u4fdd\u7a33\u5b9a\u548c\u9c81\u68d2\u7684\u4f18\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u7f16\u7801\u57fa\u51c6\u4e0a\uff0cCodeScaler\u5c06Qwen3-8B-Base\u5e73\u5747\u63d0\u534711.72\u5206\uff0c\u6bd4\u57fa\u4e8e\u6267\u884c\u7684RL\u9ad8\u51fa1.82\u5206\u3002\u5728\u63a8\u7406\u65f6\uff0c\u5ef6\u8fdf\u964d\u4f4e10\u500d\uff0c\u6027\u80fd\u4e0e\u5355\u5143\u6d4b\u8bd5\u65b9\u6cd5\u76f8\u5f53\u3002\u5728RM-Bench\u4e0a\uff0c\u4e0d\u4ec5\u5728\u4ee3\u7801\u9886\u57df\uff08+3.3\u5206\uff09\uff0c\u5728\u901a\u7528\u548c\u63a8\u7406\u9886\u57df\uff08\u5e73\u5747+2.7\u5206\uff09\u4e5f\u8d85\u8d8a\u73b0\u6709\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "CodeScaler\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u4e3a\u4ee3\u7801LLM\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18003", "categories": ["math.OC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.18003", "abs": "https://arxiv.org/abs/2602.18003", "authors": ["Jongmin Lee", "Ernest K. Ryu"], "title": "Policy Gradient Algorithms in Average-Reward Multichain MDPs", "comment": "arXiv admin note: text overlap with arXiv:2510.18340", "summary": "While there is an extensive body of research analyzing policy gradient methods for discounted cumulative-reward MDPs, prior work on policy gradient methods for average-reward MDPs has been limited, with most existing results restricted to ergodic or unichain settings. In this work, we first establish a policy gradient theorem for average-reward multichain MDPs based on the invariance of the classification of recurrent and transient states. Building on this foundation, we develop refined analyses and obtain a collection of convergence and sample-complexity results that advance the understanding of this setting. In particular, we show that the proposed $\u03b1$-clipped policy mirror ascent algorithm attains an $\u03b5$-optimal policy with respect to positive policies.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u5e73\u5747\u5956\u52b1\u591a\u94feMDPs\u7684\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff0c\u63d0\u51fa\u4e86\u03b1-\u88c1\u526a\u7b56\u7565\u955c\u50cf\u4e0a\u5347\u7b97\u6cd5\uff0c\u5728\u6b63\u7b56\u7565\u7a7a\u95f4\u5185\u8fbe\u5230\u03b5\u6700\u4f18\u7b56\u7565", "motivation": "\u73b0\u6709\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6298\u6263\u7d2f\u79ef\u5956\u52b1MDPs\uff0c\u800c\u5e73\u5747\u5956\u52b1MDPs\u7684\u7814\u7a76\u6709\u9650\uff0c\u5927\u591a\u6570\u7ed3\u679c\u5c40\u9650\u4e8e\u904d\u5386\u6216\u5355\u94fe\u8bbe\u7f6e\u3002\u9700\u8981\u6269\u5c55\u5bf9\u591a\u94fe\u5e73\u5747\u5956\u52b1MDPs\u7684\u7b56\u7565\u68af\u5ea6\u5206\u6790\u3002", "method": "\u57fa\u4e8e\u5faa\u73af\u72b6\u6001\u548c\u77ac\u6001\u5206\u7c7b\u7684\u4e0d\u53d8\u6027\uff0c\u5efa\u7acb\u5e73\u5747\u5956\u52b1\u591a\u94feMDPs\u7684\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u03b1-\u88c1\u526a\u7b56\u7565\u955c\u50cf\u4e0a\u5347\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u7cbe\u7ec6\u5316\u5206\u6790\u3002", "result": "\u83b7\u5f97\u4e86\u4e00\u7cfb\u5217\u6536\u655b\u6027\u548c\u6837\u672c\u590d\u6742\u5ea6\u7ed3\u679c\uff0c\u63a8\u8fdb\u4e86\u5bf9\u8be5\u8bbe\u7f6e\u7684\u7406\u89e3\u3002\u7279\u522b\u5730\uff0c\u63d0\u51fa\u7684\u03b1-\u88c1\u526a\u7b56\u7565\u955c\u50cf\u4e0a\u5347\u7b97\u6cd5\u5728\u6b63\u7b56\u7565\u7a7a\u95f4\u5185\u8fbe\u5230\u03b5\u6700\u4f18\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u5e73\u5747\u5956\u52b1\u591a\u94feMDPs\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u591a\u94fe\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2602.17827", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17827", "abs": "https://arxiv.org/abs/2602.17827", "authors": ["Pedro Dall'Antonia", "Tiago da Silva", "Daniel Csillag", "Salem Lahlou", "Diego Mesquita"], "title": "Avoid What You Know: Divergent Trajectory Balance for GFlowNets", "comment": "20 pages, under review", "summary": "Generative Flow Networks (GFlowNets) are a flexible family of amortized samplers trained to generate discrete and compositional objects with probability proportional to a reward function. However, learning efficiency is constrained by the model's ability to rapidly explore diverse high-probability regions during training. To mitigate this issue, recent works have focused on incentivizing the exploration of unvisited and valuable states via curiosity-driven search and self-supervised random network distillation, which tend to waste samples on already well-approximated regions of the state space. In this context, we propose Adaptive Complementary Exploration (ACE), a principled algorithm for the effective exploration of novel and high-probability regions when learning GFlowNets. To achieve this, ACE introduces an exploration GFlowNet explicitly trained to search for high-reward states in regions underexplored by the canonical GFlowNet, which learns to sample from the target distribution. Through extensive experiments, we show that ACE significantly improves upon prior work in terms of approximation accuracy to the target distribution and discovery rate of diverse high-reward states.", "AI": {"tldr": "\u63d0\u51faAdaptive Complementary Exploration (ACE)\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e13\u95e8\u7684\u63a2\u7d22GFlowNet\u6765\u641c\u7d22\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9ad8\u5956\u52b1\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347GFlowNets\u7684\u5b66\u4e60\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "GFlowNets\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9762\u4e34\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u597d\u5947\u5fc3\u9a71\u52a8\u641c\u7d22\u548c\u81ea\u76d1\u7763\u968f\u673a\u7f51\u7edc\u84b8\u998f\uff09\u4f1a\u5728\u5df2\u7ecf\u5145\u5206\u63a2\u7d22\u7684\u533a\u57df\u6d6a\u8d39\u6837\u672c\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u6765\u53d1\u73b0\u65b0\u9896\u4e14\u9ad8\u6982\u7387\u7684\u533a\u57df\u3002", "method": "\u63d0\u51faAdaptive Complementary Exploration (ACE)\u7b97\u6cd5\uff0c\u5f15\u5165\u4e00\u4e2a\u4e13\u95e8\u7684\u63a2\u7d22GFlowNet\uff0c\u8be5\u7f51\u7edc\u88ab\u8bad\u7ec3\u6765\u641c\u7d22\u90a3\u4e9b\u88ab\u6807\u51c6GFlowNet\uff08\u5b66\u4e60\u76ee\u6807\u5206\u5e03\uff09\u672a\u5145\u5206\u63a2\u7d22\u7684\u9ad8\u5956\u52b1\u72b6\u6001\u533a\u57df\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cACE\u5728\u76ee\u6807\u5206\u5e03\u7684\u8fd1\u4f3c\u7cbe\u5ea6\u548c\u591a\u6837\u5316\u9ad8\u5956\u52b1\u72b6\u6001\u7684\u53d1\u73b0\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u5de5\u4f5c\u3002", "conclusion": "ACE\u662f\u4e00\u79cd\u539f\u5219\u6027\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347GFlowNets\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5bf9\u65b0\u9896\u548c\u9ad8\u6982\u7387\u533a\u57df\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u63a2\u7d22\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.17679", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.17679", "abs": "https://arxiv.org/abs/2602.17679", "authors": ["Saksham Kiroriwal", "Julius Pfrommer", "J\u00fcrgen Beyerer"], "title": "Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization", "comment": "This paper is under review and has been submitted for CIRP CMS 2026", "summary": "Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.", "AI": {"tldr": "\u63d0\u51faPOGPN-JPSS\u6846\u67b6\uff0c\u7ed3\u5408\u90e8\u5206\u53ef\u89c2\u6d4b\u9ad8\u65af\u8fc7\u7a0b\u7f51\u7edc\u4e0e\u8054\u5408\u53c2\u6570\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff0c\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u63d0\u53d6\u9ad8\u7ef4\u4e2d\u95f4\u89c2\u6d4b\u7684\u4f4e\u7ef4\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u591a\u9636\u6bb5\u751f\u7269\u4e59\u9187\u751f\u4ea7\u8fc7\u7a0b\u4f18\u5316\u6548\u7387", "motivation": "\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u5904\u7406\u9ad8\u7ef4\u591a\u9636\u6bb5\u5236\u9020\u8fc7\u7a0b\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u6807\u51c6\u65b9\u6cd5\u5ffd\u7565\u4e2d\u95f4\u89c2\u6d4b\u548c\u8fc7\u7a0b\u7ed3\u6784\uff1b\u73b0\u6709POGPN\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u65f6\u95f4\u5e8f\u5217\u7684\u4e2d\u95f4\u89c2\u6d4b", "method": "\u63d0\u51faPOGPN-JPSS\u6846\u67b6\uff0c\u5c06\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u4ece\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u6570\u636e\u63d0\u53d6\u4f4e\u7ef4\u6f5c\u5728\u7279\u5f81\uff0c\u4f7f\u7528\u8054\u5408\u53c2\u6570\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u5229\u7528\u4e2d\u95f4\u63d0\u53d6\u4fe1\u606f", "result": "\u5728\u591a\u9636\u6bb5\u751f\u7269\u4e59\u9187\u751f\u4ea7\u8fc7\u7a0b\u7684\u9ad8\u7ef4\u4eff\u771f\u4e2d\uff0cPOGPN-JPSS\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fbe\u5230\u671f\u671b\u6027\u80fd\u9608\u503c\u7684\u901f\u5ea6\u63d0\u9ad8\u4e24\u500d\uff0c\u53ef\u9760\u6027\u66f4\u9ad8\uff0c\u5927\u5e45\u8282\u7701\u65f6\u95f4\u548c\u8d44\u6e90", "conclusion": "\u5c06\u4e13\u5bb6\u77e5\u8bc6\u4e0e\u7ed3\u6784\u5316\u6982\u7387\u6a21\u578b\u7ed3\u5408\u5bf9\u4e8e\u5feb\u901f\u8fc7\u7a0b\u6210\u719f\u81f3\u5173\u91cd\u8981\uff0cPOGPN-JPSS\u6846\u67b6\u4e3a\u9ad8\u7ef4\u591a\u9636\u6bb5\u5236\u9020\u8fc7\u7a0b\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.18416", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18416", "abs": "https://arxiv.org/abs/2602.18416", "authors": ["Kenshiro Oguri", "Gregory Lantoine"], "title": "Convex Block-Cholesky Approach to Risk-Constrained Low-thrust Trajectory Design under Operational Uncertainty", "comment": null, "summary": "Designing robust trajectories under uncertainties is an emerging technology that may represent a key paradigm shift in space mission design. As we pursue more ambitious scientific goals (e.g., multi-moon tours, missions with extensive components of autonomy), it becomes more crucial that missions are designed with navigation (Nav) processes in mind. The effect of Nav processes is statistical by nature, as they consist of orbit determination (OD) and flight-path control (FPC). Thus, this mission design paradigm calls for techniques that appropriately quantify statistical effects of Nav, evaluate associated risks, and design missions that ensure sufficiently low risk while minimizing a statistical performance metric; a common metric is Delta-V99: worst-case (99%-quantile) Delta-V expenditure including statistical FPC efforts. In response to the need, this paper develops an algorithm for risk-constrained trajectory optimization under operational uncertainties due to initial state dispersion, navigation error, maneuver execution error, and imperfect dynamics modeling. We formulate it as a nonlinear stochastic optimal control problem and develop a computationally tractable algorithm that combines optimal covariance steering and sequential convex programming (SCP). Specifically, the proposed algorithm takes a block-Cholesky approach for convex formulation of optimal covariance steering, and leverages a recent SCP algorithm, SCvx*, for reliable numerical convergence. We apply the developed algorithm to risk-constrained, statistical trajectory optimization for exploration of dwarf planet Ceres with a Mars gravity assist, and demonstrate the robustness of the statistically-optimal trajectory and FPC policies via nonlinear Monte Carlo simulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u98ce\u9669\u7ea6\u675f\u8f68\u8ff9\u4f18\u5316\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u6700\u4f18\u534f\u65b9\u5dee\u63a7\u5236\u548c\u5e8f\u5217\u51f8\u89c4\u5212\uff0c\u5e94\u7528\u4e8e\u706b\u661f\u5f15\u529b\u8f85\u52a9\u63a2\u6d4b\u8c37\u795e\u661f\u4efb\u52a1", "motivation": "\u968f\u7740\u592a\u7a7a\u4efb\u52a1\u76ee\u6807\u65e5\u76ca\u590d\u6742\uff08\u5982\u591a\u536b\u661f\u5de1\u6e38\u3001\u81ea\u4e3b\u4efb\u52a1\uff09\uff0c\u9700\u8981\u5728\u4efb\u52a1\u8bbe\u8ba1\u4e2d\u8003\u8651\u5bfc\u822a\u8fc7\u7a0b\u7684\u7edf\u8ba1\u7279\u6027\u3002\u5bfc\u822a\u8fc7\u7a0b\u672c\u8d28\u4e0a\u662f\u7edf\u8ba1\u6027\u7684\uff0c\u5305\u62ec\u8f68\u9053\u786e\u5b9a\u548c\u98de\u884c\u8def\u5f84\u63a7\u5236\uff0c\u56e0\u6b64\u9700\u8981\u91cf\u5316\u5bfc\u822a\u7684\u7edf\u8ba1\u6548\u5e94\u3001\u8bc4\u4f30\u76f8\u5173\u98ce\u9669\uff0c\u5e76\u8bbe\u8ba1\u786e\u4fdd\u4f4e\u98ce\u9669\u540c\u65f6\u6700\u5c0f\u5316\u7edf\u8ba1\u6027\u80fd\u6307\u6807\u7684\u4efb\u52a1", "method": "\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u975e\u7ebf\u6027\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5f00\u53d1\u8ba1\u7b97\u53ef\u884c\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u6700\u4f18\u534f\u65b9\u5dee\u63a7\u5236\u548c\u5e8f\u5217\u51f8\u89c4\u5212\u3002\u91c7\u7528\u5757Cholesky\u65b9\u6cd5\u8fdb\u884c\u6700\u4f18\u534f\u65b9\u5dee\u63a7\u5236\u7684\u51f8\u5316\u8868\u8ff0\uff0c\u5e76\u5229\u7528SCvx*\u7b97\u6cd5\u786e\u4fdd\u6570\u503c\u6536\u655b\u53ef\u9760\u6027", "result": "\u5c06\u7b97\u6cd5\u5e94\u7528\u4e8e\u706b\u661f\u5f15\u529b\u8f85\u52a9\u63a2\u6d4b\u8c37\u795e\u661f\u7684\u98ce\u9669\u7ea6\u675f\u7edf\u8ba1\u8f68\u8ff9\u4f18\u5316\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u4e86\u7edf\u8ba1\u6700\u4f18\u8f68\u8ff9\u548c\u98de\u884c\u8def\u5f84\u63a7\u5236\u7b56\u7565\u7684\u9c81\u68d2\u6027", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u521d\u59cb\u72b6\u6001\u5206\u6563\u3001\u5bfc\u822a\u8bef\u5dee\u3001\u673a\u52a8\u6267\u884c\u8bef\u5dee\u548c\u4e0d\u5b8c\u7f8e\u52a8\u529b\u5b66\u5efa\u6a21\u7b49\u64cd\u4f5c\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u590d\u6742\u592a\u7a7a\u4efb\u52a1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u98ce\u9669\u7ea6\u675f\u8f68\u8ff9\u4f18\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.18189", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18189", "abs": "https://arxiv.org/abs/2602.18189", "authors": ["Dejan Grba"], "title": "Computer Vision in Tactical AI Art", "comment": "18 pages", "summary": "AI art comprises a spectrum of creative endeavors that emerge from and respond to the development of artificial intelligence (AI), the expansion of AI-powered economies, and their influence on culture and society. Within this repertoire, the relationship between the cognitive value of human vision and the wide application range of computer vision (CV) technologies opens a sizeable space for exploring the problematic sociopolitical aspects of automated inference and decision-making in modern AI. In this paper, I examine the art practices critically engaged with the notions and protocols of CV. After identifying and contextualizing the CV-related tactical AI art, I discuss the features of exemplar artworks in four interrelated subject areas. Their topical imbrications, common critical points, and shared pitfalls plot a wider landscape of tactical AI art, allowing me to detect factors that affect its poetic cogency, social responsibility, and political impact, some of which exist in the theoretical premises of digital art activism. Along these lines, I outline the routes for addressing the challenges and advancing the field.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u827a\u672f\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u6279\u5224\u73b0\u4ee3AI\u7684\u81ea\u52a8\u5316\u63a8\u7406\u4e0e\u51b3\u7b56\u7684\u793e\u4f1a\u653f\u6cbb\u95ee\u9898\uff0c\u5206\u6790\u76f8\u5173\u827a\u672f\u5b9e\u8df5\u7684\u7279\u5f81\u3001\u6311\u6218\u4e0e\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "AI\u827a\u672f\u7684\u5174\u8d77\u4e0eAI\u6280\u672f\u53d1\u5c55\u3001AI\u9a71\u52a8\u7ecf\u6d4e\u7684\u6269\u5f20\u53ca\u5176\u5bf9\u793e\u4f1a\u6587\u5316\u7684\u5f71\u54cd\u5bc6\u5207\u76f8\u5173\u3002\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u4e0e\u4eba\u7c7b\u89c6\u89c9\u8ba4\u77e5\u4ef7\u503c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u63a2\u7d22\u73b0\u4ee3AI\u81ea\u52a8\u5316\u63a8\u7406\u548c\u51b3\u7b56\u7684\u6709\u95ee\u9898\u793e\u4f1a\u653f\u6cbb\u65b9\u9762\u63d0\u4f9b\u4e86\u91cd\u8981\u7a7a\u95f4\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc6\u522b\u548c\u5b9a\u4f4d\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u7684\u6218\u672f\u6027AI\u827a\u672f\u5b9e\u8df5\uff0c\u7136\u540e\u5206\u6790\u56db\u4e2a\u76f8\u4e92\u5173\u8054\u4e3b\u9898\u9886\u57df\u7684\u4ee3\u8868\u6027\u827a\u672f\u4f5c\u54c1\uff0c\u63a2\u8ba8\u5b83\u4eec\u7684\u4e3b\u9898\u91cd\u53e0\u3001\u5171\u540c\u6279\u5224\u70b9\u548c\u5171\u4eab\u7f3a\u9677\uff0c\u4ece\u800c\u7ed8\u5236\u6218\u672f\u6027AI\u827a\u672f\u7684\u66f4\u5e7f\u6cdb\u56fe\u666f\u3002", "result": "\u901a\u8fc7\u5206\u6790\u53d1\u73b0\uff0c\u6218\u672f\u6027AI\u827a\u672f\u5728\u8bd7\u610f\u8bf4\u670d\u529b\u3001\u793e\u4f1a\u8d23\u4efb\u548c\u653f\u6cbb\u5f71\u54cd\u529b\u65b9\u9762\u53d7\u5230\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u5176\u4e2d\u4e00\u4e9b\u56e0\u7d20\u5b58\u5728\u4e8e\u6570\u5b57\u827a\u672f\u884c\u52a8\u4e3b\u4e49\u7684\u7406\u8bba\u524d\u63d0\u4e2d\u3002\u7814\u7a76\u63ed\u793a\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u4f5c\u8005\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u5e94\u5bf9\u6311\u6218\u548c\u63a8\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u7684\u8def\u7ebf\u56fe\uff0c\u4e3a\u6218\u672f\u6027AI\u827a\u672f\u7684\u672a\u6765\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.18137", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18137", "abs": "https://arxiv.org/abs/2602.18137", "authors": ["Vincent Grari", "Ciprian Tomoiaga", "Sylvain Lamprier", "Tatsunori Hashimoto", "Marcin Detyniecki"], "title": "Agentic Adversarial QA for Improving Domain-Specific LLMs", "comment": "9 pages, 1 Figure", "summary": "Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u6027\u95ee\u7b54\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u76ee\u6807\u6a21\u578b\u4e0e\u4e13\u5bb6\u6a21\u578b\u8f93\u51fa\uff0c\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u6311\u6218\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u9002\u5e94\u6548\u7387", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u9002\u5e94\u56f0\u96be\uff0c\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u7f3a\u4e4f\u5bf9\u89e3\u91ca\u6027\u63a8\u7406\u80fd\u529b\u7684\u652f\u6301\uff1b2) \u751f\u6210\u6570\u636e\u5197\u4f59\u4e14\u6837\u672c\u6548\u7387\u4f4e", "method": "\u63d0\u51fa\u5bf9\u6297\u6027\u95ee\u7b54\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u8fc7\u7a0b\u6bd4\u8f83\u76ee\u6807\u6a21\u578b\u4e0e\u57fa\u4e8e\u53c2\u8003\u6587\u6863\u7684\u4e13\u5bb6\u6a21\u578b\u8f93\u51fa\uff0c\u751f\u6210\u63ed\u793a\u7406\u89e3\u5dee\u8ddd\u7684\u8bed\u4e49\u6311\u6218\u6027\u95ee\u9898", "result": "\u5728LegalBench\u4e13\u4e1a\u5b50\u96c6\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7528\u66f4\u5c11\u7684\u5408\u6210\u6837\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387", "conclusion": "\u5bf9\u6297\u6027\u95ee\u7b54\u751f\u6210\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u9002\u5e94\u6548\u7387\uff0c\u89e3\u51b3\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2602.17686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17686", "abs": "https://arxiv.org/abs/2602.17686", "authors": ["Bowen Yu", "Maolin Wang", "Sheng Zhang", "Binhao Wang", "Yi Wen", "Jingtong Gao", "Bowen Liu", "Zimo Zhao", "Wanyu Wang", "Xiangyu Zhao"], "title": "Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO", "comment": "22 pages, 12 figures", "summary": "Distilling Chain-of-Thought (CoT) reasoning from large language models into compact student models presents a fundamental challenge: teacher rationales are often too verbose for smaller models to faithfully reproduce. Existing approaches either compress reasoning into single-step, losing the interpretability that makes CoT valuable. We present a three-stage curriculum learning framework that addresses this capacity mismatch through progressive skill acquisition. First, we establish structural understanding via masked shuffled reconstruction. Second, we apply Group Relative Policy Optimization (GRPO) on masked completion tasks, enabling the model to discover its own balance between accuracy and brevity. Third, we identify persistent failure cases and guide the student to internalize teacher knowledge through targeted rewriting, again optimized with GRPO. Experiments on GSM8K demonstrate that our approach enables Qwen2.5-3B-Base to achieve an 11.29 percent accuracy improvement while reducing output length by 27.4 percent, surpassing both instruction-tuned variants and prior distillation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6280\u80fd\u83b7\u53d6\u89e3\u51b3\u5927\u6a21\u578b\u4e0e\u5c0f\u6a21\u578b\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u84b8\u998f\u4e2d\u7684\u5bb9\u91cf\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u7f29\u77ed\u8f93\u51fa\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u601d\u7ef4\u94fe\u63a8\u7406\u84b8\u998f\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6311\u6218\uff1a\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u901a\u5e38\u8fc7\u4e8e\u5197\u957f\uff0c\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u96be\u4ee5\u5fe0\u5b9e\u590d\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c06\u63a8\u7406\u538b\u7f29\u4e3a\u5355\u6b65\uff08\u5931\u53bb\u53ef\u89e3\u91ca\u6027\uff09\uff0c\u8981\u4e48\u76f4\u63a5\u84b8\u998f\uff08\u8f93\u51fa\u8fc7\u957f\uff09\u3002", "method": "\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff1a1) \u901a\u8fc7\u63a9\u7801\u6253\u4e71\u91cd\u6784\u5efa\u7acb\u7ed3\u6784\u7406\u89e3\uff1b2) \u5728\u63a9\u7801\u8865\u5168\u4efb\u52a1\u4e0a\u5e94\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u8ba9\u6a21\u578b\u81ea\u4e3b\u53d1\u73b0\u7cbe\u5ea6\u4e0e\u7b80\u6d01\u6027\u7684\u5e73\u8861\uff1b3) \u8bc6\u522b\u6301\u7eed\u5931\u8d25\u6848\u4f8b\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u91cd\u5199\u6307\u5bfc\u5b66\u751f\u5185\u5316\u6559\u5e08\u77e5\u8bc6\uff0c\u540c\u6837\u4f7f\u7528GRPO\u4f18\u5316\u3002", "result": "\u5728GSM8K\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f7fQwen2.5-3B-Base\u6a21\u578b\u51c6\u786e\u7387\u63d0\u534711.29%\uff0c\u540c\u65f6\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1127.4%\uff0c\u8d85\u8d8a\u4e86\u6307\u4ee4\u8c03\u4f18\u53d8\u4f53\u548c\u5148\u524d\u7684\u84b8\u998f\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u601d\u7ef4\u94fe\u63a8\u7406\u84b8\u998f\u4e2d\u7684\u5bb9\u91cf\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u80fd\u591f\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u751f\u6210\u66f4\u7b80\u6d01\u3001\u66f4\u51c6\u786e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2602.18293", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.18293", "abs": "https://arxiv.org/abs/2602.18293", "authors": ["Camilla Brizzi", "Lorenzo Portinale"], "title": "On the $q$-integrability of $p$-Wasserstein barycenters", "comment": null, "summary": "We study the $L^q$-regularity of the density of barycenters of $N$ probability measures on $\\mathbb{R}^d$ with respect to the $p$-Wasserstein metric ($1<p<\\infty$). According to a previous result by the first author and collaborators, if one marginal is absolutely continuous, so is the $W_p$-barycenter. The next natural question is whether the $L^q$- regularity on the marginals is also preserved for any $q > 1$, as in the classical case ($p=2$) of Agueh--Carlier, or for $W_p$-geodesics ($N=2$). Here we prove that this is the case if one marginal belongs to $L^q$ and the supports of all the marginals satisfy suitable geometric assumptions. However, we show that, as soon as $N>2$, it is possible to find examples of $W_p$-barycenters which are not $q$-integrable, even if one marginal is compactly supported and bounded, thus highlighting the role played by the geometry of the supports. Furthermore, we provide a general estimate of the $L^q$-norm, including a detailed study of the sources of singularities, and a characterization of the $W_p$-barycenters \u00e0 la Agueh--Carlier in terms of the associated Kantorovich potentials. Finally, we explicitly compute the $W_p$-barycenters of measures obtained as push-forward of special affine transformations. In this case, regularity holds without any additional requirement on the supports.", "AI": {"tldr": "\u7814\u7a76p-Wasserstein\u5ea6\u91cf\u4e0b\u6982\u7387\u6d4b\u5ea6\u91cd\u5fc3\u7684L^q\u6b63\u5219\u6027\uff0c\u8bc1\u660e\u5728\u67d0\u4e9b\u51e0\u4f55\u6761\u4ef6\u4e0b\u8fb9\u9645\u7684L^q\u6b63\u5219\u6027\u4f1a\u4f20\u9012\u7ed9\u91cd\u5fc3\uff0c\u4f46N>2\u65f6\u5b58\u5728\u53cd\u4f8b\uff0c\u5e76\u7ed9\u51fa\u4e86L^q\u8303\u6570\u4f30\u8ba1\u548c\u91cd\u5fc3\u7279\u5f81\u5316\u3002", "motivation": "\u7814\u7a76p-Wasserstein\u5ea6\u91cf\u4e0b\u6982\u7387\u6d4b\u5ea6\u91cd\u5fc3\u7684\u6b63\u5219\u6027\u4fdd\u6301\u95ee\u9898\uff0c\u63a2\u7d22\u8fb9\u9645\u7684L^q\u6b63\u5219\u6027\u662f\u5426\u80fd\u591f\u4f20\u9012\u7ed9\u91cd\u5fc3\uff0c\u7279\u522b\u662f\u5728N>2\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u6bd4\u7ecf\u5178\u7684p=2\u60c5\u51b5\u6216\u6d4b\u5730\u7ebf(N=2)\u60c5\u51b5\u66f4\u590d\u6742\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u3001\u6784\u9020\u53cd\u4f8b\u3001L^q\u8303\u6570\u4f30\u8ba1\u548cKantorovich\u52bf\u7279\u5f81\u5316\u7b49\u65b9\u6cd5\uff0c\u7814\u7a76\u91cd\u5fc3\u6b63\u5219\u6027\u3002\u7279\u522b\u5173\u6ce8\u8fb9\u9645\u652f\u6491\u96c6\u7684\u51e0\u4f55\u6761\u4ef6\u5bf9\u6b63\u5219\u6027\u7684\u5f71\u54cd\u3002", "result": "\u8bc1\u660e\u5728\u9002\u5f53\u51e0\u4f55\u6761\u4ef6\u4e0b\uff0c\u4e00\u4e2a\u8fb9\u9645\u7684L^q\u6b63\u5219\u6027\u4f1a\u4f20\u9012\u7ed9\u91cd\u5fc3\uff1b\u4f46\u6784\u9020\u4e86N>2\u65f6\u91cd\u5fc3\u53ef\u80fd\u4e0d\u662fq\u53ef\u79ef\u7684\u53cd\u4f8b\uff0c\u5373\u4f7f\u8fb9\u9645\u6709\u754c\u7d27\u652f\u6491\uff1b\u7ed9\u51fa\u4e86L^q\u8303\u6570\u4f30\u8ba1\u548c\u5947\u5f02\u6027\u6765\u6e90\u5206\u6790\uff1b\u901a\u8fc7Kantorovich\u52bf\u7279\u5f81\u5316\u4e86\u91cd\u5fc3\uff1b\u5bf9\u7279\u6b8a\u4eff\u5c04\u53d8\u6362\u63a8\u524d\u6d4b\u5ea6\u8ba1\u7b97\u4e86\u663e\u5f0f\u91cd\u5fc3\u3002", "conclusion": "p-Wasserstein\u91cd\u5fc3\u7684L^q\u6b63\u5219\u6027\u4fdd\u6301\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u8fb9\u9645\u7684\u6b63\u5219\u6027\uff0c\u8fd8\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u8fb9\u9645\u652f\u6491\u96c6\u7684\u51e0\u4f55\u7ed3\u6784\u3002N>2\u65f6\u60c5\u51b5\u6bd4\u7ecf\u5178\u60c5\u51b5\u66f4\u590d\u6742\uff0c\u9700\u8981\u989d\u5916\u7684\u51e0\u4f55\u6761\u4ef6\u6765\u4fdd\u8bc1\u6b63\u5219\u6027\u4f20\u9012\u3002"}}
{"id": "2602.17985", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17985", "abs": "https://arxiv.org/abs/2602.17985", "authors": ["Ryan O'Dowd"], "title": "Learning Without Training", "comment": "PhD Dissertation of Ryan O'Dowd, defended successfully at Claremont Graduate University on 1/28/2026", "summary": "Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.\n  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\\mathcal{D}=\\{(x_j,f(x_j))\\}_{j=1}^M$, can one build a model $F\\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.\n  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.\n  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.", "AI": {"tldr": "\u8be5\u535a\u58eb\u8bba\u6587\u5305\u542b\u4e09\u4e2a\u673a\u5668\u5b66\u4e60\u9879\u76ee\uff1a1) \u76d1\u7763\u5b66\u4e60\u548c\u6d41\u5f62\u5b66\u4e60\u7684\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\uff1b2) \u90e8\u5206\u6570\u636e\u5df2\u77e5\u60c5\u51b5\u4e0b\u7684\u8fc1\u79fb\u5b66\u4e60\u51fd\u6570\u63d0\u5347\u7406\u8bba\uff1b3) \u57fa\u4e8e\u4fe1\u53f7\u5206\u79bb\u6280\u672f\u7684\u4e3b\u52a8\u5b66\u4e60\u5206\u7c7b\u65b0\u7b97\u6cd5\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u7684\u6210\u529f\uff0c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u65e5\u76ca\u589e\u591a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u5b66\u7406\u8bba\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u51fd\u6570\u903c\u8fd1\u7684\u7406\u8bba\u7f3a\u9677\u3001\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "1) \u9488\u5bf9\u76d1\u7763\u5b66\u4e60\u63d0\u51fa\u65b0\u7684\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\uff1b2) \u7814\u7a76\u90e8\u5206\u6570\u636e\u5df2\u77e5\u60c5\u51b5\u4e0b\u7684\u51fd\u6570\u63d0\u5347\u7406\u8bba\uff0c\u5206\u6790\u63d0\u5347\u5b9a\u4e49\u57df\u548c\u5c40\u90e8\u5149\u6ed1\u6027\u5173\u7cfb\uff1b3) \u5c06\u4fe1\u53f7\u5206\u79bb\u6280\u672f\u5e94\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u63d0\u51fa\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "1) \u6539\u8fdb\u4e86\u5f53\u524d\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u7684\u7406\u8bba\u7f3a\u9677\uff1b2) \u5efa\u7acb\u4e86\u51fd\u6570\u63d0\u5347\u7684\u7406\u8bba\u6846\u67b6\uff1b3) \u65b0\u5206\u7c7b\u7b97\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u6570\u5b66\u7406\u8bba\u65b9\u6cd5\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e09\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u4e3a\u51fd\u6570\u903c\u8fd1\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u9ad8\u6548\u7b97\u6cd5\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u5b66\u4e60\u7406\u8bba\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.17680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17680", "abs": "https://arxiv.org/abs/2602.17680", "authors": ["Yujia Wang", "Jihong Guan", "Wengen Li", "Shuigeng Zhou", "Xuhong Wang"], "title": "BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs", "comment": null, "summary": "Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.", "AI": {"tldr": "BioBridge\u662f\u4e00\u4e2a\u86cb\u767d\u8d28\u7406\u89e3\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u548c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u86cb\u767d\u8d28\u5e8f\u5217\u7406\u89e3\u548c\u901a\u7528\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u9002\u5e94\u6027\u548c\u8de8\u751f\u7269\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u86cb\u767d\u8d28\u5e8f\u5217\u89e3\u91ca\u80fd\u529b\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u65e0\u6cd5\u8fdb\u884c\u6709\u6548\u7684\u751f\u7269\u8bed\u4e49\u63a8\u7406", "method": "\u63d0\u51faBioBridge\u6846\u67b6\uff0c\u91c7\u7528\u9886\u57df\u589e\u91cf\u6301\u7eed\u9884\u8bad\u7ec3(DICP)\u540c\u65f6\u6ce8\u5165\u86cb\u767d\u8d28\u9886\u57df\u77e5\u8bc6\u548c\u901a\u7528\u63a8\u7406\u8bed\u6599\uff0c\u901a\u8fc7PLM-Projector-LLM\u7ba1\u9053\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u4f18\u5316\u652f\u6301\u86cb\u767d\u8d28\u6027\u8d28\u9884\u6d4b\u548c\u77e5\u8bc6\u95ee\u7b54\u7b49\u591a\u79cd\u4efb\u52a1", "result": "BioBridge\u5728EC\u548cBindingDB\u7b49\u591a\u4e2a\u86cb\u767d\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0e\u4e3b\u6d41PLMs\u76f8\u5f53\uff0c\u5728MMLU\u548cRACE\u7b49\u901a\u7528\u7406\u89e3\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0eLLMs\u76f8\u5f53\u7684\u7ed3\u679c", "conclusion": "BioBridge\u5c55\u793a\u4e86\u5c06\u9886\u57df\u7279\u5b9a\u9002\u5e94\u6027\u4e0e\u901a\u7528\u8bed\u8a00\u80fd\u529b\u76f8\u7ed3\u5408\u7684\u521b\u65b0\u4f18\u52bf\uff0c\u6210\u529f\u878d\u5408\u4e86\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u548c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u70b9"}}
{"id": "2602.17975", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17975", "abs": "https://arxiv.org/abs/2602.17975", "authors": ["Robert Parker"], "title": "Generating adversarial inputs for a graph neural network model of AC power flow", "comment": null, "summary": "This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$\u0394$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\u751f\u6210\u5bf9\u6297\u6027\u8f93\u5165\u70b9\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7684\u4ea4\u6d41\u6f6e\u6d41\u89e3\u4e0e\u771f\u5b9e\u65b9\u7a0b\u89e3\u4ea7\u751f\u663e\u8457\u8bef\u5dee\uff0c\u572814\u8282\u70b9\u6d4b\u8bd5\u7535\u7f51\u4e2d\u9a8c\u8bc1\u4e86CANOS-PF\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u6f6e\u6d41\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u8bc1\u660e\u5373\u4f7f\u7ecf\u8fc7\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e5f\u53ef\u80fd\u88ab\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6027\u8f93\u5165\u653b\u51fb\uff0c\u4ece\u800c\u4ea7\u751f\u4e25\u91cd\u9519\u8bef\u7684\u6f6e\u6d41\u9884\u6d4b\u7ed3\u679c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u6784\u5efa\u4f18\u5316\u95ee\u9898\u6765\u751f\u6210\u4f7f\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u8bef\u5dee\u6700\u5927\u5316\u7684\u8f93\u5165\u70b9\uff1b2) \u5728PF\u0394\u57fa\u51c6\u5e93\u4e2d\u7684CANOS-PF\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e0a\u5b9e\u65bd\uff1b3) \u4f7f\u752814\u8282\u70b9\u6d4b\u8bd5\u7535\u7f51\u4f5c\u4e3a\u5b9e\u9a8c\u5e73\u53f0\uff1b4) \u6700\u5c0f\u5316\u6ee1\u8db3\u5bf9\u6297\u6027\u7ea6\u675f\u6240\u9700\u7684\u6270\u52a8\u3002", "result": "\u7ed3\u679c\uff1a1) \u751f\u6210\u7684\u5bf9\u6297\u6027\u70b9\u80fd\u4ea7\u751f\u9ad8\u8fbe3.4pu\u7684\u65e0\u529f\u529f\u7387\u8bef\u5dee\u548c0.08pu\u7684\u7535\u538b\u5e45\u503c\u8bef\u5dee\uff1b2) \u4ec5\u9700\u5728\u5355\u4e2a\u6bcd\u7ebf\u4e0a\u65bd\u52a00.04pu\u7684\u7535\u538b\u5e45\u503c\u6270\u52a8\u5373\u53ef\u6ee1\u8db3\u5bf9\u6297\u6027\u7ea6\u675f\uff1b3) \u9a8c\u8bc1\u4e86\u795e\u7ecf\u7f51\u7edc\u6f6e\u6d41\u6a21\u578b\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u9700\u8981\u4e3a\u795e\u7ecf\u7f51\u7edc\u6f6e\u6d41\u6a21\u578b\u5f00\u53d1\u4e25\u683c\u7684\u9a8c\u8bc1\u65b9\u6cd5\u548c\u9c81\u68d2\u8bad\u7ec3\u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u7535\u529b\u7cfb\u7edf\u8fd0\u884c\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.18202", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18202", "abs": "https://arxiv.org/abs/2602.18202", "authors": ["Dejan Grba"], "title": "Art Notions in the Age of (Mis)anthropic AI", "comment": "27 pages", "summary": "In this paper, I take the cultural effects of generative artificial intelligence (generative AI) as a context for examining a broader perspective of AI's impact on contemporary art notions. After the introductory overview of generative AI, I summarize the distinct but often confused aspects of art notions and review the principal lines in which AI influences them: the strategic normalization of AI through art, the representation of AI art in the artworld, academia, and AI research, and the mutual permeability of art and kitsch in the digital culture. I connect these notional factors with the conceptual and ideological substrate of the computer science and AI industry, which blends the machinic agency fetishism, the equalization of computers and humans, the sociotechnical blindness, and cyberlibertarianism. The overtones of alienation, sociopathy, and misanthropy in the disparate but somehow coalescing philosophical premises, technical ideas, and political views in this substrate remain underexposed in AI studies so, in the closing discussion, I outline their manifestations in generative AI and introduce several viewpoints for a further critique of AI's cultural zeitgeist. They add a touch of skepticism to pondering how technological trends change our understanding of art and in which directions they stir its social, economic, and political roles.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5bf9\u5f53\u4ee3\u827a\u672f\u89c2\u5ff5\u7684\u6587\u5316\u5f71\u54cd\uff0c\u5206\u6790AI\u901a\u8fc7\u827a\u672f\u88ab\u6218\u7565\u6027\u5730\u6b63\u5e38\u5316\u3001\u5728\u827a\u672f\u754c/\u5b66\u672f\u754c/AI\u7814\u7a76\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u827a\u672f\u4e0e\u5a9a\u4fd7\u5728\u6570\u5b57\u6587\u5316\u4e2d\u7684\u76f8\u4e92\u6e17\u900f\uff0c\u63ed\u793a\u8ba1\u7b97\u673a\u79d1\u5b66\u548cAI\u4ea7\u4e1a\u80cc\u540e\u7684\u6982\u5ff5\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u5bf9\u5f53\u4ee3\u827a\u672f\u89c2\u5ff5\u7684\u6587\u5316\u5f71\u54cd\uff0c\u63ed\u793aAI\u6280\u672f\u80cc\u540e\u7684\u610f\u8bc6\u5f62\u6001\u57fa\u7840\uff08\u673a\u68b0\u4ee3\u7406\u5d07\u62dc\u3001\u4eba\u673a\u7b49\u540c\u3001\u793e\u4f1a\u6280\u672f\u76f2\u89c6\u3001\u7f51\u7edc\u81ea\u7531\u4e3b\u4e49\uff09\uff0c\u8fd9\u4e9b\u57fa\u7840\u5728AI\u7814\u7a76\u4e2d\u5c1a\u672a\u5145\u5206\u66b4\u9732\uff0c\u9700\u8981\u6279\u5224\u6027\u5206\u6790\u3002", "method": "\u9996\u5148\u6982\u8ff0\u751f\u6210\u5f0fAI\uff0c\u603b\u7ed3\u827a\u672f\u89c2\u5ff5\u7684\u4e0d\u540c\u4f46\u5e38\u88ab\u6df7\u6dc6\u7684\u65b9\u9762\uff0c\u7136\u540e\u4ece\u4e09\u4e2a\u4e3b\u7ebf\u5206\u6790AI\u5bf9\u827a\u672f\u7684\u5f71\u54cd\uff1aAI\u901a\u8fc7\u827a\u672f\u7684\u6218\u7565\u6027\u6b63\u5e38\u5316\u3001AI\u827a\u672f\u5728\u827a\u672f\u754c/\u5b66\u672f\u754c/AI\u7814\u7a76\u4e2d\u7684\u8868\u73b0\u3001\u6570\u5b57\u6587\u5316\u4e2d\u827a\u672f\u4e0e\u5a9a\u4fd7\u7684\u76f8\u4e92\u6e17\u900f\uff0c\u6700\u540e\u5c06\u8fd9\u4e9b\u6982\u5ff5\u56e0\u7d20\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u548cAI\u4ea7\u4e1a\u7684\u6982\u5ff5\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u63ed\u793a\u4e86\u751f\u6210\u5f0fAI\u80cc\u540e\u9690\u85cf\u7684\u5f02\u5316\u3001\u53cd\u793e\u4f1a\u6027\u548c\u538c\u4e16\u503e\u5411\uff0c\u8fd9\u4e9b\u503e\u5411\u4f53\u73b0\u5728\u4e0d\u540c\u7684\u54f2\u5b66\u524d\u63d0\u3001\u6280\u672f\u601d\u60f3\u548c\u653f\u6cbb\u89c2\u70b9\u4e2d\uff0c\u5728AI\u7814\u7a76\u4e2d\u5c1a\u672a\u5145\u5206\u66b4\u9732\uff0c\u9700\u8981\u5728\u751f\u6210\u5f0fAI\u7684\u80cc\u666f\u4e0b\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5bf9\u827a\u672f\u89c2\u5ff5\u7684\u5f71\u54cd\u9700\u8981\u6301\u6000\u7591\u6001\u5ea6\u7684\u6279\u5224\u6027\u5206\u6790\uff0c\u6280\u672f\u8d8b\u52bf\u4e0d\u4ec5\u6539\u53d8\u6211\u4eec\u5bf9\u827a\u672f\u7684\u7406\u89e3\uff0c\u8fd8\u5f71\u54cd\u827a\u672f\u7684\u793e\u4f1a\u3001\u7ecf\u6d4e\u548c\u653f\u6cbb\u89d2\u8272\u65b9\u5411\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6279\u5224AI\u7684\u6587\u5316\u65f6\u4ee3\u7cbe\u795e\u3002"}}
{"id": "2602.18145", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18145", "abs": "https://arxiv.org/abs/2602.18145", "authors": ["Siya Qi", "Yudong Chen", "Runcong Zhao", "Qinglin Zhu", "Zhanghao Hu", "Wei Liu", "Yulan He", "Zheng Yuan", "Lin Gui"], "title": "Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention", "comment": "25 pages, 10 figures", "summary": "Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u5206\u6790\u7684\u6ce8\u610f\u529b\u673a\u5236\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u5206\u5e03\u5efa\u6a21\u4e3a\u79bb\u6563\u4fe1\u53f7\u5e76\u63d0\u53d6\u9ad8\u9891\u6210\u5206\uff0c\u63ed\u793a\u4e86\u5e7b\u89c9token\u4e0e\u9ad8\u9891\u6ce8\u610f\u529b\u80fd\u91cf\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5e76\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u5e7b\u89c9\u68c0\u6d4b\u5668\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7c97\u7c92\u5ea6\u6c47\u603b\uff0c\u65e0\u6cd5\u6355\u6349\u6ce8\u610f\u529b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u4e0d\u7a33\u5b9a\u6027\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u65b9\u6cd5\u6765\u5206\u6790\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4ee5\u66f4\u597d\u5730\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u5f15\u5165\u9891\u7387\u611f\u77e5\u7684\u6ce8\u610f\u529b\u5206\u6790\u89c6\u89d2\uff0c\u5c06\u6ce8\u610f\u529b\u5206\u5e03\u5efa\u6a21\u4e3a\u79bb\u6563\u4fe1\u53f7\uff0c\u63d0\u53d6\u53cd\u6620\u6ce8\u610f\u529b\u5feb\u901f\u5c40\u90e8\u53d8\u5316\u7684\u9ad8\u9891\u6210\u5206\u3002\u57fa\u4e8e\u9ad8\u9891\u6ce8\u610f\u529b\u7279\u5f81\u5f00\u53d1\u8f7b\u91cf\u7ea7\u5e7b\u89c9\u68c0\u6d4b\u5668\u3002", "result": "\u5728RAGTruth\u548cHalluRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u57fa\u4e8e\u9a8c\u8bc1\u3001\u5185\u90e8\u8868\u793a\u548c\u6ce8\u610f\u529b\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6ce8\u610f\u529b\u9891\u7387\u5206\u6790\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89c6\u89d2\uff0c\u9ad8\u9891\u6ce8\u610f\u529b\u80fd\u91cf\u53cd\u6620\u4e86\u5e7b\u89c9token\u7684\u788e\u7247\u5316\u548c\u4e0d\u7a33\u5b9a\u63a5\u5730\u884c\u4e3a\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1\u7684\u68c0\u6d4b\u5668\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.17689", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17689", "abs": "https://arxiv.org/abs/2602.17689", "authors": ["Melika Filvantorkaman", "Mohsen Piri"], "title": "Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction", "comment": "28 pages, 3 figures", "summary": "Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.", "AI": {"tldr": "Robust-MMR\uff1a\u4e00\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u6270\u52a8\u611f\u77e5\u63a9\u7801\u3001\u9886\u57df\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u6a21\u6001\u5f39\u6027\u7ea6\u675f\uff0c\u5728\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u663e\u5f0f\u5efa\u6a21\u9c81\u68d2\u6027\uff0c\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6210\u50cf\u8bbe\u5907\u3001\u91c7\u96c6\u534f\u8bae\u548c\u62a5\u544a\u98ce\u683c\u53d8\u5316\u5bfc\u81f4\u7684\u9886\u57df\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u9c81\u68d2\u6027\uff0c\u5c06\u5176\u89c6\u4e3a\u4e0b\u6e38\u9002\u5e94\u95ee\u9898\u3002", "method": "\u63d0\u51faRobust-MMR\u6846\u67b6\uff0c\u5c06\u9c81\u68d2\u6027\u76ee\u6807\u878d\u5165\u63a9\u7801\u89c6\u89c9\u8bed\u8a00\u5b66\u4e60\uff0c\u5305\u542b\uff1a1) \u975e\u5bf9\u79f0\u6270\u52a8\u611f\u77e5\u63a9\u7801\uff1b2) \u9886\u57df\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff1b3) \u6a21\u6001\u5f39\u6027\u7ea6\u675f\uff0c\u4ee5\u9f13\u52b1\u9886\u57df\u4e0d\u53d8\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aVQA-RAD\u8de8\u57df\u51c6\u786e\u7387\u8fbe78.9%\uff08\u63d0\u53473.8%\uff09\uff0cSLAKE 74.6%\uff0cVQA-2019 77.0%\uff1b\u6270\u52a8\u8bc4\u4f30\u4e0bVQA-RAD\u51c6\u786e\u7387\u4ece69.1%\u63d0\u5347\u81f375.6%\uff1bMELINDA\u8de8\u57df\u51c6\u786e\u7387\u4ece70.3%\u63d0\u5347\u81f375.2%\uff1b\u68c0\u7d22\u4efb\u52a1\u4e2d\u5e73\u5747\u6392\u540d\u9000\u5316\u4ece16+\u964d\u81f34.1\u3002", "conclusion": "\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u663e\u5f0f\u5efa\u6a21\u9c81\u68d2\u6027\u80fd\u591f\u4ea7\u751f\u66f4\u53ef\u9760\u3001\u53ef\u8fc1\u79fb\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u8868\u793a\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\uff0c\u6539\u5584\u4e86\u75be\u75c5\u68c0\u6d4b\u548c\u7ed3\u6784\u5f02\u5e38\u8bc4\u4f30\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.18277", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18277", "abs": "https://arxiv.org/abs/2602.18277", "authors": ["Finn van der Knaap", "Kejiang Qian", "Zheng Xu", "Fengxiang He"], "title": "PRISM: Parallel Reward Integration with Symmetry for MORL", "comment": null, "summary": "This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\\% over the baseline and up to 32\\% over the oracle. The code is at \\href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.", "AI": {"tldr": "PRISM\u7b97\u6cd5\u901a\u8fc7\u53cd\u5c04\u5bf9\u79f0\u6027\u89e3\u51b3\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\u76ee\u6807\u65f6\u95f4\u9891\u7387\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u957f\u65f6\u5956\u52b1\u7684\u5b66\u4e60\u6548\u7387", "motivation": "\u5f02\u6784\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4e0d\u540c\u76ee\u6807\u7684\u65f6\u95f4\u9891\u7387\u5dee\u5f02\u5de8\u5927\uff0c\u5bfc\u81f4\u5bc6\u96c6\u5956\u52b1\u76ee\u6807\u4e3b\u5bfc\u5b66\u4e60\uff0c\u800c\u7a00\u758f\u957f\u65f6\u5956\u52b1\u7684\u4fe1\u7528\u5206\u914d\u8584\u5f31\uff0c\u6837\u672c\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51faPRISM\u7b97\u6cd5\uff0c\u5305\u542bReSymNet\u6a21\u578b\uff08\u4f7f\u7528\u6b8b\u5dee\u5757\u5b66\u4e60\u7f29\u653e\u673a\u4f1a\u4ef7\u503c\uff09\u548cSymReg\u6b63\u5219\u5316\u5668\uff08\u5f3a\u5236\u53cd\u5c04\u7b49\u53d8\u6027\uff09\uff0c\u901a\u8fc7\u53cd\u5c04\u5bf9\u79f0\u6027\u5bf9\u9f50\u5956\u52b1\u901a\u9053", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPRISM\u663e\u8457\u4f18\u4e8e\u7a00\u758f\u5956\u52b1\u57fa\u7ebf\u548c\u5b8c\u5168\u5bc6\u96c6\u5956\u52b1\u7684oracle\uff0c\u8d85\u4f53\u79ef\u589e\u76ca\u8d85\u8fc7\u57fa\u7ebf100%\uff0c\u6bd4oracle\u63d0\u534732%\uff0c\u6539\u5584\u4e86\u5e15\u7d2f\u6258\u8986\u76d6\u548c\u5206\u5e03\u5e73\u8861", "conclusion": "\u901a\u8fc7\u53cd\u5c04\u5bf9\u79f0\u6027\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0cPRISM\u6709\u6548\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65f6\u95f4\u9891\u7387\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.17681", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17681", "abs": "https://arxiv.org/abs/2602.17681", "authors": ["Ofir Gordon", "Lior Dikstein", "Arnon Netzer", "Idan Achituve", "Hai Victor Habi"], "title": "LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs", "comment": "24 pages, 4 figures", "summary": "Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLATMiX\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u53ef\u9006\u4eff\u5c04\u53d8\u6362\u4f18\u5316MX\u4f4e\u6bd4\u7279\u91cf\u5316\uff0c\u5728\u591a\u79cd\u6a21\u578b\u5927\u5c0f\u548c\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f20\u7edf\u91cf\u5316\u65b9\u6848\uff0c\u800c\u73b0\u4ee3\u786c\u4ef6\u8d8a\u6765\u8d8a\u652f\u6301\u5fae\u7f29\u653e(MX)\u6570\u636e\u683c\u5f0f\u3002\u5c06\u6fc0\u6d3b\u53d8\u6362\u4e0eMX\u91cf\u5316\u7ed3\u5408\u65f6\u4f1a\u51fa\u73b0\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\uff0c\u5148\u524d\u5de5\u4f5c\u4e0d\u5f97\u4e0d\u5bf9\u53d8\u6362\u65bd\u52a0\u9650\u5236\u6027\u5047\u8bbe\u3002", "method": "\u9996\u5148\u5bf9MX\u91cf\u5316\u4e0b\u7684\u53d8\u6362\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u91cf\u5316\u8bef\u5dee\u8fb9\u754c\u3002\u57fa\u4e8e\u6b64\u5206\u6790\u63d0\u51faLATMiX\u65b9\u6cd5\uff0c\u5c06\u5f02\u5e38\u503c\u51cf\u5c11\u63a8\u5e7f\u5230\u53ef\u5b66\u4e60\u7684\u53ef\u9006\u4eff\u5c04\u53d8\u6362\uff0c\u4f7f\u7528\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5e7f\u6cdb\u7684\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLATMiX\u5728MX\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0a\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u53ef\u5b66\u4e60\u53d8\u6362\u8bbe\u8ba1\uff0cLATMiX\u6210\u529f\u89e3\u51b3\u4e86MX\u91cf\u5316\u4e0e\u6fc0\u6d3b\u53d8\u6362\u7ed3\u5408\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17998", "categories": ["cs.LG", "cs.AI", "cs.CE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17998", "abs": "https://arxiv.org/abs/2602.17998", "authors": ["Shubham Bhardwaj", "Chandrajit Bajaj"], "title": "PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting", "comment": "50 pages", "summary": "Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \\emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\\dot{x}=(J-R)\\nabla H(x)$, guaranteeing $dH/dt\\le 0$ when $R\\succeq 0$. We introduce \\textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.", "AI": {"tldr": "PHAST\uff1a\u4e00\u79cd\u57fa\u4e8e\u7aef\u53e3\u54c8\u5bc6\u987f\u6846\u67b6\u7684\u7269\u7406\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u4f4d\u7f6e\u89c2\u6d4b\u6570\u636e\u5b9e\u73b0\u7a33\u5b9a\u957f\u671f\u9884\u6d4b\u548c\u7269\u7406\u53c2\u6570\u6062\u590d", "motivation": "\u771f\u5b9e\u7269\u7406\u7cfb\u7edf\u90fd\u662f\u8017\u6563\u7684\uff0c\u4ece\u90e8\u5206\u89c2\u6d4b\u4e2d\u9884\u6d4b\u5176\u52a8\u529b\u5b66\u662f\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002\u7279\u522b\u662f\"\u4ec5\u4f4d\u7f6e\"\u95ee\u9898\uff1a\u53ea\u7ed9\u5b9a\u79bb\u6563\u65f6\u95f4\u7684\u4f4d\u7f6e\u89c2\u6d4b\uff08\u52a8\u91cf\u4e3a\u9690\u53d8\u91cf\uff09\uff0c\u9700\u8981\u5b66\u4e60\u4e00\u4e2a\u65e2\u80fd\u4ea7\u751f\u7a33\u5b9a\u957f\u671f\u9884\u6d4b\uff0c\u53c8\u80fd\u5728\u63d0\u4f9b\u8db3\u591f\u7ed3\u6784\u65f6\u6062\u590d\u7269\u7406\u610f\u4e49\u53c2\u6570\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faPHAST\uff08\u7aef\u53e3\u54c8\u5bc6\u987f\u7ed3\u6784\u65f6\u95f4\u52a8\u529b\u5b66\u67b6\u6784\uff09\uff0c\u57fa\u4e8e\u7aef\u53e3\u54c8\u5bc6\u987f\u6846\u67b6\u5c06\u4fdd\u5b88-\u8017\u6563\u5206\u89e3\u4e3a$\\dot{x}=(J-R)\\nabla H(x)$\uff0c\u4fdd\u8bc1\u80fd\u91cf\u8870\u51cf\u3002\u5c06\u54c8\u5bc6\u987f\u91cf\u5206\u89e3\u4e3a\u52bf\u80fd$V(q)$\u3001\u8d28\u91cf$M(q)$\u548c\u963b\u5c3c$D(q)$\u4e09\u4e2a\u90e8\u5206\uff0c\u5bf9\u5e94\u4e09\u79cd\u77e5\u8bc6\u72b6\u6001\uff08\u5df2\u77e5\u3001\u90e8\u5206\u5df2\u77e5\u3001\u672a\u77e5\uff09\uff0c\u4f7f\u7528\u9ad8\u6548\u4f4e\u79e9PSD/SPD\u53c2\u6570\u5316\uff0c\u5e76\u901a\u8fc7Strang\u5206\u88c2\u63a8\u8fdb\u52a8\u529b\u5b66\u3002", "result": "\u572813\u4e2a\u4ec5\u4f4d\u7f6e\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d6\u673a\u68b0\u3001\u7535\u6c14\u3001\u5206\u5b50\u3001\u70ed\u3001\u91cd\u529b\u548c\u751f\u6001\u7cfb\u7edf\uff09\u4e2d\uff0cPHAST\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u5e76\u5728\u6709\u8db3\u591f\u951a\u70b9\u65f6\u80fd\u591f\u6062\u590d\u7269\u7406\u610f\u4e49\u53c2\u6570\u3002\u7814\u7a76\u8868\u660e\uff0c\u6ca1\u6709\u8fd9\u79cd\u951a\u70b9\u65f6\u8bc6\u522b\u95ee\u9898\u662f\u75c5\u6001\u7684\uff08\u89c4\u8303\u81ea\u7531\u5ea6\uff09\u3002", "conclusion": "PHAST\u6210\u529f\u89e3\u51b3\u4e86\u4ec5\u4f4d\u7f6e\u89c2\u6d4b\u7684\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u957f\u671f\u9884\u6d4b\u548c\u7269\u7406\u53c2\u6570\u6062\u590d\u3002\u63d0\u51fa\u4e86\u4e24\u8f74\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u7a33\u5b9a\u6027\u4e0e\u53ef\u8bc6\u522b\u6027\u5206\u5f00\u8bc4\u4f30\uff0c\u4e3a\u7269\u7406\u7ed3\u6784\u5b66\u4e60\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2602.18431", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18431", "abs": "https://arxiv.org/abs/2602.18431", "authors": ["Shafkat Farabi", "Didac Marti Pinto", "Wei Lu", "Manuel Ramos-Maqueda", "Sanmay Das", "Antoine Deeb", "Anja Sautmann"], "title": "SMaRT: Online Reusable Resource Assignment and an Application to Mediation in the Kenyan Judiciary", "comment": null, "summary": "Motivated by the problem of assigning mediators to cases in the Kenyan judicial, we study an online resource allocation problem where incoming tasks (cases) must be immediately assigned to available, capacity-constrained resources (mediators). The resources differ in their quality, which may need to be learned. In addition, resources can only be assigned to a subset of tasks that overlaps to varying degrees with the subset of tasks other resources can be assigned to. The objective is to maximize task completion while satisfying soft capacity constraints across all the resources. The scale of the real-world problem poses substantial challenges, since there are over 2000 mediators and a multitude of combinations of geographic locations (87) and case types (12) that each mediator is qualified to work on. Together, these features, unknown quality of new resources, soft capacity constraints, and a high-dimensional state space, make existing scheduling and resource allocation algorithms either inapplicable or inefficient. We formalize the problem in a tractable manner using a quadratic program formulation for assignment and a multi-agent bandit-style framework for learning. We demonstrate the key properties and advantages of our new algorithm, SMaRT (Selecting Mediators that are Right for the Task), compared with baselines on stylized instances of the mediator allocation problem. We then consider its application to real-world data on cases and mediators from the Kenyan judiciary. SMaRT outperforms baselines and allows control over the tradeoff between the strictness of capacity constraints and overall case resolution rates, both in settings where mediator quality is known beforehand and in bandit-like settings where learning is part of the problem definition. On the strength of these results, we plan to run a randomized controlled trial with SMaRT in the judiciary in the near future.", "AI": {"tldr": "SMaRT\u7b97\u6cd5\uff1a\u9488\u5bf9\u80af\u5c3c\u4e9a\u53f8\u6cd5\u8c03\u89e3\u5458\u5206\u914d\u95ee\u9898\u7684\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e8c\u6b21\u89c4\u5212\u548c\u591a\u667a\u80fd\u4f53\u8d4c\u535a\u673a\u5b66\u4e60\uff0c\u89e3\u51b3\u672a\u77e5\u8d28\u91cf\u3001\u8f6f\u5bb9\u91cf\u7ea6\u675f\u548c\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u80af\u5c3c\u4e9a\u53f8\u6cd5\u7cfb\u7edf\u4e2d\u8c03\u89e3\u5458\u5206\u914d\u7684\u5b9e\u9645\u95ee\u9898\uff1a\u9700\u8981\u7acb\u5373\u5c06\u6848\u4ef6\u5206\u914d\u7ed9\u80fd\u529b\u6709\u9650\u7684\u8c03\u89e3\u5458\uff0c\u8c03\u89e3\u5458\u8d28\u91cf\u672a\u77e5\uff0c\u53ea\u80fd\u5904\u7406\u7279\u5b9a\u7c7b\u578b\u548c\u5730\u7406\u4f4d\u7f6e\u7684\u6848\u4ef6\uff0c\u4e14\u5b58\u5728\u8f6f\u5bb9\u91cf\u7ea6\u675f\uff0c\u73b0\u6709\u7b97\u6cd5\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u9ad8\u7ef4\u590d\u6742\u573a\u666f\u3002", "method": "\u4f7f\u7528\u4e8c\u6b21\u89c4\u5212\u8fdb\u884c\u4efb\u52a1\u5206\u914d\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8d4c\u535a\u673a\u6846\u67b6\u8fdb\u884c\u5b66\u4e60\u3002SMaRT\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u8c03\u89e3\u5458\u8d28\u91cf\u672a\u77e5\u3001\u8f6f\u5bb9\u91cf\u7ea6\u675f\u548c\u4efb\u52a1-\u8d44\u6e90\u5339\u914d\u7684\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\uff0cSMaRT\u7b97\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5bb9\u91cf\u7ea6\u675f\u4e25\u683c\u6027\u548c\u6848\u4ef6\u89e3\u51b3\u7387\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u63a7\u5236\u3002\u5728\u8c03\u89e3\u5458\u8d28\u91cf\u5df2\u77e5\u548c\u9700\u8981\u5b66\u4e60\u7684\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "SMaRT\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u80af\u5c3c\u4e9a\u53f8\u6cd5\u8c03\u89e3\u5458\u5206\u914d\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u8ba1\u5212\u5728\u8fd1\u671f\u8fdb\u884c\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4f18\u5316\u548c\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u5177\u6709\u672a\u77e5\u8d28\u91cf\u3001\u8f6f\u5bb9\u91cf\u7ea6\u675f\u548c\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u7684\u590d\u6742\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002"}}
{"id": "2602.18152", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.18152", "abs": "https://arxiv.org/abs/2602.18152", "authors": ["Ortal Hadad", "Edoardo Loru", "Jacopo Nudo", "Niccol\u00f2 Di Marco", "Matteo Cinelli", "Walter Quattrociocchi"], "title": "The Statistical Signature of LLMs", "comment": null, "summary": "Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u65e0\u635f\u538b\u7f29\u5206\u6790\u53d1\u73b0LLM\u751f\u6210\u6587\u672c\u6bd4\u4eba\u7c7b\u6587\u672c\u5177\u6709\u66f4\u9ad8\u7684\u7ed3\u6784\u89c4\u5f8b\u6027\u548c\u53ef\u538b\u7f29\u6027\uff0c\u63ed\u793a\u4e86\u6982\u7387\u751f\u6210\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u4f46\u5728\u5c0f\u5c3a\u5ea6\u4ea4\u4e92\u73af\u5883\u4e2d\u8fd9\u79cd\u533a\u5206\u4f1a\u51cf\u5f31\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u6982\u7387\u91c7\u6837\u751f\u6210\u6587\u672c\u65f6\uff0c\u5982\u4f55\u91cd\u5851\u8bed\u8a00\u7684\u7ed3\u6784\u7edf\u8ba1\u7ec4\u7ec7\u3002\u76ee\u524d\u5bf9\u8fd9\u4e00\u8fc7\u7a0b\u5982\u4f55\u6539\u53d8\u6587\u672c\u7684\u7edf\u8ba1\u89c4\u5f8b\u6027\u7279\u5f81\u5c1a\u672a\u5b8c\u5168\u8868\u5f81\u3002", "method": "\u91c7\u7528\u65e0\u635f\u538b\u7f29\u4f5c\u4e3a\u6a21\u578b\u65e0\u5173\u7684\u7edf\u8ba1\u89c4\u5f8b\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5206\u6790\u4e09\u79cd\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\uff1a\u53d7\u63a7\u7684\u4eba\u7c7b-LLM\u5ef6\u7eed\u3001\u77e5\u8bc6\u57fa\u7840\u8bbe\u65bd\u7684\u751f\u6210\u4e2d\u4ecb\uff08\u7ef4\u57fa\u767e\u79d1 vs Grokipedia\uff09\u3001\u5b8c\u5168\u5408\u6210\u7684\u793e\u4ea4\u4e92\u52a8\u73af\u5883\uff08Moltbook vs Reddit\uff09\u3002", "result": "\u5728\u53d7\u63a7\u548c\u4e2d\u4ecb\u73af\u5883\u4e2d\uff0cLLM\u751f\u6210\u7684\u8bed\u8a00\u6bd4\u4eba\u7c7b\u6587\u672c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7ed3\u6784\u89c4\u5f8b\u6027\u548c\u53ef\u538b\u7f29\u6027\uff0c\u8f93\u51fa\u96c6\u4e2d\u5728\u9ad8\u5ea6\u5faa\u73af\u7684\u7edf\u8ba1\u6a21\u5f0f\u4e2d\u3002\u4f46\u5728\u788e\u7247\u5316\u4ea4\u4e92\u73af\u5883\u4e2d\uff0c\u8fd9\u79cd\u533a\u5206\u4f1a\u51cf\u5f31\uff0c\u8868\u660e\u5c0f\u5c3a\u5ea6\u8868\u9762\u53ef\u533a\u5206\u6027\u5b58\u5728\u6839\u672c\u9650\u5236\u3002", "conclusion": "\u65e0\u635f\u538b\u7f29\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7a33\u5065\u7684\u6846\u67b6\u6765\u91cf\u5316\u751f\u6210\u7cfb\u7edf\u5982\u4f55\u91cd\u5851\u6587\u672c\u751f\u4ea7\uff0c\u4e3a\u7406\u89e3\u901a\u4fe1\u590d\u6742\u6027\u6f14\u53d8\u63d0\u4f9b\u4e86\u7ed3\u6784\u89c6\u89d2\u3002\u8fd9\u79cd\u53ef\u538b\u7f29\u6027\u533a\u5206\u5728\u4e0d\u540c\u6a21\u578b\u3001\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u4e00\u81f4\u51fa\u73b0\uff0c\u53ef\u76f4\u63a5\u4ece\u8868\u9762\u6587\u672c\u89c2\u5bdf\u5230\u3002"}}
{"id": "2602.17692", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17692", "abs": "https://arxiv.org/abs/2602.17692", "authors": ["Bin Wang", "Fan Wang", "Pingping Wang", "Jinyu Cong", "Yang Yu", "Yilong Yin", "Zhongyi Han", "Benzheng Wei"], "title": "Agentic Unlearning: When LLM Agent Meets Machine Unlearning", "comment": "9 pages, 6 figures, 6 tables", "summary": "In this paper, we introduce \\textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.", "AI": {"tldr": "\u63d0\u51fa\"agentic unlearning\"\u6982\u5ff5\uff0c\u901a\u8fc7\u540c\u6b65\u53c2\u6570\u548c\u8bb0\u5fc6\u53cc\u8def\u5f84\u9057\u5fd8\u673a\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u9057\u5fd8\u65b9\u6cd5\u4e2d\u53c2\u6570-\u8bb0\u5fc6\u56de\u6d41\u6c61\u67d3\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u5f7b\u5e95\u7684\u654f\u611f\u4fe1\u606f\u79fb\u9664\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u4ec5\u9488\u5bf9\u6a21\u578b\u53c2\u6570\uff0c\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a1) \u53c2\u6570-\u8bb0\u5fc6\u56de\u6d41\u95ee\u9898 - \u68c0\u7d22\u4f1a\u91cd\u65b0\u6fc0\u6d3b\u53c2\u6570\u6b8b\u7559\u6216\u8bb0\u5fc6\u4f2a\u5f71\u91cd\u65b0\u5f15\u5165\u654f\u611f\u5185\u5bb9\uff1b2) \u7f3a\u4e4f\u7edf\u4e00\u7684\u53c2\u6570\u548c\u8bb0\u5fc6\u53cc\u8def\u5f84\u9057\u5fd8\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u540c\u6b65\u56de\u6d41\u9057\u5fd8(SBU)\u6846\u67b6\uff1a\u8bb0\u5fc6\u8def\u5f84\u91c7\u7528\u57fa\u4e8e\u4f9d\u8d56\u95ed\u5305\u7684\u9057\u5fd8\uff0c\u4fee\u526a\u5b64\u7acb\u5b9e\u4f53\u5e76\u903b\u8f91\u4e0a\u4f7f\u5171\u4eab\u4f2a\u5f71\u5931\u6548\uff1b\u53c2\u6570\u8def\u5f84\u91c7\u7528\u968f\u673a\u53c2\u8003\u5bf9\u9f50\uff0c\u5f15\u5bfc\u6a21\u578b\u8f93\u51fa\u5411\u9ad8\u71b5\u5148\u9a8c\uff1b\u901a\u8fc7\u540c\u6b65\u53cc\u66f4\u65b0\u534f\u8bae\u96c6\u6210\u53cc\u8def\u5f84\uff0c\u5f62\u6210\u95ed\u73af\u673a\u5236\u9632\u6b62\u8de8\u8def\u5f84\u518d\u6c61\u67d3\u3002", "result": "\u5728\u533b\u7597QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSBU\u663e\u8457\u51cf\u5c11\u4e86\u76ee\u6807\u9690\u79c1\u4fe1\u606f\u5728\u53cc\u8def\u5f84\u4e2d\u7684\u75d5\u8ff9\uff0c\u540c\u65f6\u5bf9\u4fdd\u7559\u6570\u636e\u7684\u6027\u80fd\u5f71\u54cd\u6709\u9650\u3002", "conclusion": "SBU\u6846\u67b6\u901a\u8fc7\u540c\u6b65\u53c2\u6570\u548c\u8bb0\u5fc6\u53cc\u8def\u5f84\u9057\u5fd8\uff0c\u6709\u6548\u89e3\u51b3\u4e86agentic\u7cfb\u7edf\u4e2d\u654f\u611f\u4fe1\u606f\u7684\u5f7b\u5e95\u79fb\u9664\u95ee\u9898\uff0c\u9632\u6b62\u4e86\u53c2\u6570-\u8bb0\u5fc6\u56de\u6d41\u6c61\u67d3\uff0c\u4e3a\u5177\u6709\u95ed\u73af\u4ea4\u4e92\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u9057\u5fd8\u673a\u5236\u3002"}}
{"id": "2602.18114", "categories": ["cs.LG", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18114", "abs": "https://arxiv.org/abs/2602.18114", "authors": ["Yiding Feng", "Jiashuo Jiang", "Yige Wang"], "title": "Non-Stationary Online Resource Allocation: Learning from a Single Sample", "comment": null, "summary": "We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.\n  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\\tilde{O}(\\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.", "AI": {"tldr": "\u7814\u7a76\u975e\u5e73\u7a33\u9700\u6c42\u4e0b\u7684\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u4ec5\u9700\u6bcf\u671f\u4e00\u4e2a\u5386\u53f2\u6837\u672c\uff0c\u63d0\u51fa\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u5143\u7b56\u7565\uff0c\u5728\u4e24\u79cd\u6837\u672c\u4fe1\u606f\u8bbe\u7f6e\u4e0b\u5206\u522b\u8fbe\u5230\u221aT\u548c\u5bf9\u6570\u7ea7\u9057\u61be\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u8d44\u6e90\u5206\u914d\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u9700\u6c42\u5206\u5e03\u7684\u975e\u5e73\u7a33\u6027\u548c\u79bb\u7ebf\u6570\u636e\u7a00\u7f3a\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5e73\u7a33\u73af\u5883\u6216\u9700\u8981\u5927\u91cf\u5386\u53f2\u6570\u636e\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u6c42\u6a21\u5f0f\u53ef\u80fd\u4efb\u610f\u53d8\u5316\u4e14\u5386\u53f2\u6570\u636e\u6709\u9650\u3002", "method": "\u63d0\u51fa\u7c7b\u578b\u4f9d\u8d56\u7684\u5206\u4f4d\u6570\u5143\u7b56\u7565\uff0c\u5c06\u95ee\u9898\u89e3\u8026\u4e3a\u4e09\u4e2a\u6a21\u5757\uff1a1) \u5956\u52b1\u5206\u5e03\u4f30\u8ba1\uff1b2) \u901a\u8fc7\u6d41\u4f53\u677e\u5f1b\u4f18\u5316\u76ee\u6807\u670d\u52a1\u6982\u7387\uff1b3) \u901a\u8fc7\u52a8\u6001\u63a5\u53d7\u9608\u503c\u8fdb\u884c\u5b9e\u65f6\u51b3\u7b56\u3002\u9488\u5bf9\u4e24\u79cd\u6837\u672c\u4fe1\u606f\u8bbe\u7f6e\u5206\u522b\u8bbe\u8ba1\u7b56\u7565\uff1a\u5956\u52b1\u89c2\u6d4b\u6837\u672c\u4f7f\u7528\u9759\u6001\u9608\u503c\u7b56\u7565\uff0c\u7c7b\u578b\u4ec5\u6837\u672c\u5728\u6700\u5c0f\u5230\u8fbe\u6982\u7387\u5047\u8bbe\u4e0b\u8bbe\u8ba1\u90e8\u5206\u81ea\u9002\u5e94\u548c\u5b8c\u5168\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5bf9\u4e8e\u5956\u52b1\u89c2\u6d4b\u6837\u672c\uff0c\u9759\u6001\u9608\u503c\u7b56\u7565\u8fbe\u5230\u00d5(\u221aT)\u9057\u61be\uff1b\u5bf9\u4e8e\u7c7b\u578b\u4ec5\u6837\u672c\uff0c\u5728\u6700\u5c0f\u5230\u8fbe\u6982\u7387\u5047\u8bbe\u4e0b\uff0c\u90e8\u5206\u81ea\u9002\u5e94\u7b56\u7565\u8fbe\u5230\u00d5(\u221aT)\u9057\u61be\uff0c\u5b8c\u5168\u81ea\u9002\u5e94\u89e3\u6790\u7b56\u7565\u9996\u6b21\u5b9e\u73b0O((log T)\u00b3)\u7684\u591a\u5bf9\u6570\u9057\u61be\uff0c\u8fd9\u662f\u975e\u5e73\u7a33\u591a\u8d44\u6e90\u5206\u914d\u95ee\u9898\u7684\u9996\u4e2a\u5bf9\u6570\u7ea7\u9057\u61be\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4ec5\u9700\u6bcf\u671f\u4e00\u4e2a\u5386\u53f2\u6837\u672c\u7684\u6781\u7b80\u6570\u636e\u8981\u6c42\u4e0b\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u975e\u5e73\u7a33\u6027\uff08\u65e0\u9700\u53d8\u5316\u9884\u7b97\u5047\u8bbe\uff09\uff0c\u652f\u6301\u591a\u8d44\u6e90\u7ea6\u675f\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u975e\u5e73\u7a33\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u7684\u7406\u8bba\u8fb9\u754c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u7b97\u6cd5\u57fa\u7840\u3002"}}
{"id": "2602.18396", "categories": ["cs.LG", "eess.SP", "math.PR", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18396", "abs": "https://arxiv.org/abs/2602.18396", "authors": ["Ehsan Lari", "Reza Arablouei", "Stefan Werner"], "title": "PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing", "comment": "13 pages, 5 figures, 2 tables, Submitted to IEEE Transactions on Signal Processing (TSP)", "summary": "We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.", "AI": {"tldr": "PRISM-FCP\uff1a\u4e00\u79cd\u62dc\u5360\u5ead\u9c81\u68d2\u7684\u8054\u90a6\u5171\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u6a21\u578b\u5171\u4eab\u548c\u7edf\u8ba1\u8fb9\u7f18\u6821\u51c6\uff0c\u5728\u8bad\u7ec3\u548c\u6821\u51c6\u9636\u6bb5\u90fd\u62b5\u6297\u62dc\u5360\u5ead\u653b\u51fb\uff0c\u4fdd\u6301\u8986\u76d6\u4fdd\u8bc1\u7684\u540c\u65f6\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u53ea\u5728\u6821\u51c6\u9636\u6bb5\u5904\u7406\u5bf9\u6297\u884c\u4e3a\uff0c\u5bfc\u81f4\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8bad\u7ec3\u9636\u6bb5\u7684\u6295\u6bd2\u653b\u51fb\u3002\u9700\u8981\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u548c\u5171\u5f62\u6821\u51c6\u9636\u6bb5\u90fd\u80fd\u62b5\u5fa1\u62dc\u5360\u5ead\u653b\u51fb\u3002", "method": "1. \u8bad\u7ec3\u9636\u6bb5\uff1a\u5ba2\u6237\u7aef\u6bcf\u8f6e\u53ea\u4f20\u8f93D\u4e2a\u53c2\u6570\u4e2d\u7684M\u4e2a\uff08\u90e8\u5206\u6a21\u578b\u5171\u4eab\uff09\uff0c\u5c06\u653b\u51fb\u8005\u6270\u52a8\u7684\u671f\u671b\u80fd\u91cf\u8870\u51cfM/D\u500d\uff1b2. \u6821\u51c6\u9636\u6bb5\uff1a\u5c06\u975e\u5171\u5f62\u5206\u6570\u8f6c\u6362\u4e3a\u7279\u5f81\u5411\u91cf\uff0c\u8ba1\u7b97\u57fa\u4e8e\u8ddd\u79bb\u7684\u6076\u610f\u5206\u6570\uff0c\u5728\u4f30\u8ba1\u5171\u5f62\u5206\u4f4d\u6570\u524d\u5bf9\u53ef\u7591\u62dc\u5360\u5ead\u8d21\u732e\u8fdb\u884c\u964d\u6743\u6216\u8fc7\u6ee4\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548cUCI\u8d85\u5bfc\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPRISM-FCP\u5728\u62dc\u5360\u5ead\u653b\u51fb\u4e0b\u4fdd\u6301\u540d\u4e49\u8986\u76d6\u4fdd\u8bc1\uff0c\u907f\u514d\u4e86\u6807\u51c6FCP\u4e2d\u89c2\u5bdf\u5230\u7684\u533a\u95f4\u81a8\u80c0\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "PRISM-FCP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u62dc\u5360\u5ead\u653b\u51fb\u7f13\u89e3\uff0c\u5728\u8bad\u7ec3\u548c\u6821\u51c6\u9636\u6bb5\u90fd\u5b9e\u73b0\u4e86\u9c81\u68d2\u6027\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17682", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17682", "abs": "https://arxiv.org/abs/2602.17682", "authors": ["Peng Sun", "Xinyi Shang", "Tao Lin", "Zhiqiang Shen"], "title": "Duality Models: An Embarrassingly Simple One-step Generation Paradigm", "comment": "https://github.com/LINs-lab/DuMo", "summary": "Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional \"one input, one output\" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a \"one input, dual output\" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo", "AI": {"tldr": "\u63d0\u51faDuality Models (DuMo)\u89e3\u6c7a\u4e00\u81f4\u6027\u751f\u6210\u6a21\u578b\u4e2d\u8a13\u7df4\u76ee\u6a19\u5206\u96e2\u554f\u984c\uff0c\u901a\u904e\"\u55ae\u8f38\u5165\u96d9\u8f38\u51fa\"\u67b6\u69cb\u540c\u6642\u9810\u6e2c\u901f\u5ea6\u548c\u6d41\u6620\u5c04\uff0c\u5728ImageNet 256\u00d7256\u4e0a\u50c5\u75282\u6b65\u9054\u5230SOTA FID 1.79", "motivation": "\u50b3\u7d71\u4e00\u81f4\u6027\u751f\u6210\u6a21\u578b\uff08\u5982Shortcut\u548cMeanFlow\uff09\u63a1\u7528\"\u55ae\u8f38\u5165\u55ae\u8f38\u51fa\"\u7bc4\u5f0f\uff0c\u9700\u8981\u5c07\u8a13\u7df4\u9810\u7b97\u5206\u5272\u70ba\u591a\u6b65\u76ee\u6a19\uff08\u598275%\uff09\u548c\u5c11\u6b65\u76ee\u6a19\uff0c\u5c0e\u81f4\u5c11\u6b65\u751f\u6210\u8a13\u7df4\u4e0d\u8db3\uff0c\u5f71\u97ff\u6536\u6582\u6027\u548c\u53ef\u64f4\u5c55\u6027", "method": "\u63d0\u51faDuality Models (DuMo)\uff0c\u63a1\u7528\"\u55ae\u8f38\u5165\u96d9\u8f38\u51fa\"\u7bc4\u5f0f\uff0c\u4f7f\u7528\u5171\u4eab\u9aa8\u5e79\u7db2\u7d61\u548c\u96d9\u982d\u67b6\u69cb\uff0c\u5f9e\u55ae\u4e00\u8f38\u5165x_t\u540c\u6642\u9810\u6e2c\u901f\u5ea6v_t\u548c\u6d41\u6620\u5c04u_t\uff0c\u5c07\u591a\u6b65\u76ee\u6a19\u7684\u5e7e\u4f55\u7d04\u675f\u61c9\u7528\u65bc\u6bcf\u500b\u6a23\u672c", "result": "\u5728ImageNet 256\u00d7256\u6578\u64da\u96c6\u4e0a\uff0c\u4f7f\u7528679M\u53c3\u6578\u7684\u64f4\u6563\u8b8a\u58d3\u5668\u548cSD-VAE\uff0c\u50c5\u97002\u6b65\u5c31\u9054\u52301.79\u7684FID\uff0c\u5275\u4e0b\u65b0\u7684SOTA\u7d50\u679c", "conclusion": "DuMo\u901a\u904e\u6d88\u9664\u8a13\u7df4\u76ee\u6a19\u5206\u96e2\uff0c\u986f\u8457\u63d0\u9ad8\u4e86\u7a69\u5b9a\u6027\u548c\u6548\u7387\uff0c\u70ba\u4e00\u81f4\u6027\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u512a\u7684\u8a13\u7df4\u7bc4\u5f0f\uff0c\u5be6\u73fe\u4e86\u9ad8\u6548\u7684\u9ad8\u8cea\u91cf\u5716\u50cf\u751f\u6210"}}
{"id": "2602.18109", "categories": ["cs.LG", "cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18109", "abs": "https://arxiv.org/abs/2602.18109", "authors": ["Rong Fu", "Yibo Meng", "Guangzhen Yao", "Jiaxuan Lu", "Zeyu Zhang", "Zhaolu Kang", "Ziming Guo", "Jia Yee Tan", "Xiaojing Du", "Simon James Fong"], "title": "TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs", "comment": "43 pages, 12 figures", "summary": "Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.", "AI": {"tldr": "TempoNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u65f6\u8c03\u5ea6\u5668\uff0c\u4f7f\u7528Transformer\u67b6\u6784\u548c\u6df1\u5ea6Q\u8fd1\u4f3c\uff0c\u901a\u8fc7Urgency Tokenizer\u5904\u7406\u65f6\u95f4\u677e\u5f1b\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u6838\u4efb\u52a1\u8c03\u5ea6\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edf\u8c03\u5ea6\u5668\u3002", "motivation": "\u5b9e\u65f6\u8c03\u5ea6\u5668\u9700\u8981\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u5904\u7406\u7d27\u622a\u6b62\u671f\u9650\uff0c\u4f20\u7edf\u8c03\u5ea6\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u96c6\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u3001\u9ad8\u6548\u7684\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7f6e\u6362\u4e0d\u53d8Transformer\u548c\u6df1\u5ea6Q\u8fd1\u4f3c\uff1bUrgency Tokenizer\u5c06\u65f6\u95f4\u677e\u5f1b\u5ea6\u79bb\u6563\u5316\u4e3a\u53ef\u5b66\u4e60\u5d4c\u5165\uff1b\u5ef6\u8fdf\u611f\u77e5\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u652f\u6301\u5168\u5c40\u63a8\u7406\uff1b\u591a\u6838\u6620\u5c04\u5c42\u5c06Q\u5206\u6570\u8f6c\u6362\u4e3a\u5904\u7406\u5668\u5206\u914d\u3002", "result": "\u5728\u5de5\u4e1a\u6df7\u5408\u5173\u952e\u6027\u8ffd\u8e2a\u548c\u5927\u89c4\u6a21\u591a\u5904\u7406\u5668\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4\u5206\u6790\u8c03\u5ea6\u5668\u548c\u795e\u7ecf\u57fa\u7ebf\uff0c\u5728\u622a\u6b62\u671f\u9650\u6ee1\u8db3\u7387\u65b9\u9762\u83b7\u5f97\u6301\u7eed\u63d0\u5347\uff0c\u540c\u65f6\u6539\u5584\u4e86\u4f18\u5316\u7a33\u5b9a\u6027\uff0c\u63a8\u7406\u65f6\u95f4\u4e9a\u6beb\u79d2\u7ea7\u3002", "conclusion": "TempoNet\u4e3a\u57fa\u4e8eTransformer\u7684\u9ad8\u541e\u5410\u91cf\u5b9e\u65f6\u8c03\u5ea6\u51b3\u7b56\u5efa\u7acb\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u8c03\u5ea6\u95ee\u9898\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.18154", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18154", "abs": "https://arxiv.org/abs/2602.18154", "authors": ["Mirae Kim", "Seonghun Jeong", "Youngjun Kwak"], "title": "FENCE: A Financial and Multimodal Jailbreak Detection Dataset", "comment": "lrec 2026 accepted paper", "summary": "Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.", "AI": {"tldr": "FENCE\u662f\u4e00\u4e2a\u9488\u5bf9\u91d1\u878d\u9886\u57df\u7684\u53cc\u8bed\u591a\u6a21\u6001\u8d8a\u72f1\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u8d8a\u72f1\u68c0\u6d4b\u5668\uff0c\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8d8a\u72f1\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u91d1\u878d\u7b49\u654f\u611f\u9886\u57df\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u91d1\u878d\u9886\u57df\u7684\u591a\u6a21\u6001\u8d8a\u72f1\u68c0\u6d4b\u8d44\u6e90\uff0c\u9700\u8981\u4e13\u95e8\u7684\u91d1\u878d\u76f8\u5173\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u53ef\u9760\u7684\u68c0\u6d4b\u6a21\u578b\u3002", "method": "\u521b\u5efa\u4e86FENCE\u6570\u636e\u96c6\uff0c\u5305\u542b\u97e9\u82f1\u53cc\u8bed\u7684\u591a\u6a21\u6001\u91d1\u878d\u76f8\u5173\u67e5\u8be2\uff0c\u7ed3\u5408\u56fe\u50cf\u57fa\u7840\u7684\u5a01\u80c1\u5185\u5bb9\u3002\u4f7f\u7528\u5546\u4e1a\u548c\u5f00\u6e90VLM\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u8bad\u7ec3\u57fa\u7ebf\u68c0\u6d4b\u5668\u8bc4\u4f30\u6570\u636e\u96c6\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGPT-4o\u5b58\u5728\u53ef\u6d4b\u91cf\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5f00\u6e90\u6a21\u578b\u66f4\u6613\u53d7\u653b\u51fb\u3002\u57fa\u4e8eFENCE\u8bad\u7ec3\u7684\u57fa\u7ebf\u68c0\u6d4b\u5668\u8fbe\u523099%\u7684\u5206\u5e03\u5185\u51c6\u786e\u7387\uff0c\u5728\u5916\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "FENCE\u4e3a\u91d1\u878d\u9886\u57df\u591a\u6a21\u6001\u8d8a\u72f1\u68c0\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684AI\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u654f\u611f\u9886\u57df\u3002\u6570\u636e\u96c6\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.17693", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17693", "abs": "https://arxiv.org/abs/2602.17693", "authors": ["Yuchen Luo", "Fangyue Zhu", "Ruining Zhou", "Mingzhe Huang", "Jian Zhu", "Fanyu Fan", "Wei Shao"], "title": "A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU", "comment": null, "summary": "Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cdPTQ\u7b97\u6cd5\u5728\u6607\u817eNPU\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b04\u4f4d\u6743\u91cd\u91cf\u5316\u5bf9\u5927\u578b\u6a21\u578b\u53ef\u884c\uff0c\u4f464\u4f4d\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u5728NPU\u4e0a\u5b58\u5728\u6821\u51c6\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u800c8\u4f4d\u91cf\u5316\u4fdd\u6301\u7a33\u5b9a\u3002\u5b9e\u9645\u90e8\u7f72\u4e2d\u52a8\u6001\u91cf\u5316\u5f00\u9500\u9650\u5236\u4e86\u7aef\u5230\u7aef\u52a0\u901f\u3002", "motivation": "\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u5bf9\u9ad8\u6548\u6a21\u578b\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u6607\u817eNPU\u4e0a\u7684\u6709\u6548\u6027\u76f8\u6bd4GPU\u67b6\u6784\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ee3\u8868\u6027PTQ\u57fa\u7ebf\u5728\u63a8\u7406\u5bfc\u5411\u6a21\u578b\uff08\u5982DeepSeek-R1-Distill-Qwen\u7cfb\u5217\u548cQwQ-32B\uff09\u5728\u6607\u817eNPU\u4e0a\u7684\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u91c7\u7528\u6848\u4f8b\u7814\u7a76\u65b9\u6cd5\uff0c\u8bc4\u4f30\u56db\u79cd\u4e0d\u540c\u7684PTQ\u7b97\u6cd5\uff1aAWQ\u3001GPTQ\u3001SmoothQuant\u548cFlatQuant\uff0c\u8986\u76d6\u4ece\u4ec5\u6743\u91cd\u91cf\u5316\u5230\u57fa\u4e8e\u65cb\u8f6c\u7684\u9ad8\u7ea7\u65b9\u6cd5\u3002\u5728\u6607\u817eNPU\u4e0a\u5bf9DeepSeek-R1-Distill-Qwen\u7cfb\u5217\uff081.5B/7B/14B\uff09\u548cQwQ-32B\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "1\uff094\u4f4d\u4ec5\u6743\u91cd\u91cf\u5316\u5bf9\u5927\u578b\u6a21\u578b\u53ef\u884c\uff1b2\uff09\u6fc0\u8fdb\u76844\u4f4d\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u65b9\u6848\u5728NPU\u4e0a\u5b58\u5728\u5c42\u95f4\u6821\u51c6\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u903b\u8f91\u5d29\u6e83\uff1b3\uff09\u6807\u51c68\u4f4d\u91cf\u5316\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\uff1b4\uff09\u5b9e\u9645INT8\u90e8\u7f72\u4e2d\uff0c\u5c3d\u7ba1\u4f18\u5316\u5185\u6838\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u4f46\u52a8\u6001\u91cf\u5316\u5f00\u9500\u76ee\u524d\u9650\u5236\u4e86\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u6607\u817eNPU\u4e0a\u90e8\u7f72\u91cf\u5316\u63a8\u7406\u6a21\u578b\u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\u3002\u5e73\u53f0\u654f\u611f\u6027\u663e\u8457\uff0c\u9700\u8981\u9488\u5bf9NPU\u67b6\u6784\u4f18\u5316\u91cf\u5316\u7b56\u7565\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u65b9\u6848\u3002"}}
{"id": "2602.18157", "categories": ["q-fin.PM", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18157", "abs": "https://arxiv.org/abs/2602.18157", "authors": ["Oumar Mbodji"], "title": "Time consistent portfolio strategies for a general utility function", "comment": null, "summary": "We study the Merton portfolio management problem within a complete market, non constant time discount rate and general utility framework. The non constant discount rate introduces time inconsistency which can be solved by introducing sub game perfect strategies. Under some asymptotic assumptions on the utility function, we show that the subgame perfect strategy is the same as the optimal strategy, provided the discount rate is replaced by the utility weighted discount rate $\u03c1(t,x)$ that depends on the time $t$ and wealth level $x$. A fixed point iteration is used to find $\u03c1$. The consumption to wealth ratio and the investment to wealth ratio are given in feedback form as functions of the value function.", "AI": {"tldr": "\u7814\u7a76Merton\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u95ee\u9898\uff0c\u8003\u8651\u975e\u6052\u5b9a\u65f6\u95f4\u8d34\u73b0\u7387\u548c\u4e00\u822c\u6548\u7528\u51fd\u6570\uff0c\u901a\u8fc7\u5b50\u535a\u5f08\u5b8c\u7f8e\u7b56\u7565\u89e3\u51b3\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u6548\u7528\u52a0\u6743\u8d34\u73b0\u7387\u03c1(t,x)\u548c\u56fa\u5b9a\u70b9\u8fed\u4ee3\u65b9\u6cd5", "motivation": "\u4f20\u7edfMerton\u6295\u8d44\u7ec4\u5408\u95ee\u9898\u901a\u5e38\u5047\u8bbe\u6052\u5b9a\u8d34\u73b0\u7387\uff0c\u4f46\u5b9e\u9645\u4e2d\u8d34\u73b0\u7387\u53ef\u80fd\u968f\u65f6\u95f4\u53d8\u5316\u3002\u975e\u6052\u5b9a\u8d34\u73b0\u7387\u4f1a\u5bfc\u81f4\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5728\u5b8c\u5168\u5e02\u573a\u6846\u67b6\u4e0b\uff0c\u5f15\u5165\u5b50\u535a\u5f08\u5b8c\u7f8e\u7b56\u7565\u89e3\u51b3\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002\u5728\u6548\u7528\u51fd\u6570\u7684\u6e10\u8fd1\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u5b50\u535a\u5f08\u5b8c\u7f8e\u7b56\u7565\u7b49\u4ef7\u4e8e\u6700\u4f18\u7b56\u7565\uff0c\u4f46\u9700\u7528\u4f9d\u8d56\u4e8e\u65f6\u95f4\u548c\u8d22\u5bcc\u6c34\u5e73\u7684\u6548\u7528\u52a0\u6743\u8d34\u73b0\u7387\u03c1(t,x)\u66ff\u4ee3\u539f\u8d34\u73b0\u7387\u3002\u4f7f\u7528\u56fa\u5b9a\u70b9\u8fed\u4ee3\u6c42\u89e3\u03c1", "result": "\u6d88\u8d39\u8d22\u5bcc\u6bd4\u548c\u6295\u8d44\u8d22\u5bcc\u6bd4\u53ef\u4ee5\u8868\u793a\u4e3a\u4ef7\u503c\u51fd\u6570\u7684\u53cd\u9988\u5f62\u5f0f\u51fd\u6570\u3002\u901a\u8fc7\u6548\u7528\u52a0\u6743\u8d34\u73b0\u7387\u03c1(t,x)\u7684\u5f15\u5165\uff0c\u89e3\u51b3\u4e86\u975e\u6052\u5b9a\u8d34\u73b0\u7387\u4e0b\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86Merton\u6295\u8d44\u7ec4\u5408\u7406\u8bba\uff0c\u4e3a\u975e\u6052\u5b9a\u8d34\u73b0\u7387\u60c5\u51b5\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u6548\u7528\u52a0\u6743\u8d34\u73b0\u7387\u548c\u5b50\u535a\u5f08\u5b8c\u7f8e\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898"}}
{"id": "2602.18401", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18401", "abs": "https://arxiv.org/abs/2602.18401", "authors": ["Josue Casco-Rodriguez", "Nanda H. Krishna", "Richard G. Baraniuk"], "title": "Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay", "comment": null, "summary": "Biological neural networks (like the hippocampus) can internally generate \"replay\" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u566a\u58f0\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u91cd\u653e\u73b0\u8c61\uff0c\u5c06\u5176\u89c6\u4e3a\u91c7\u6837\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u6539\u8fdb\u65b9\u6cd5\uff1a\u5229\u7528\u9690\u85cf\u72b6\u6001\u6cc4\u6f0f\u4fc3\u8fdb\u91cd\u653e\uff0c\u5206\u6790\u9690\u85cf\u72b6\u6001\u9002\u5e94\u7684\u63a2\u7d22-\u901f\u5ea6\u6743\u8861\uff0c\u4ee5\u53ca\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u52a8\u91cf\u5b9e\u73b0\u65f6\u95f4\u538b\u7f29\u91cd\u653e\u3002", "motivation": "\u751f\u7269\u795e\u7ecf\u7f51\u7edc\uff08\u5982\u6d77\u9a6c\u4f53\uff09\u80fd\u591f\u5185\u90e8\u751f\u6210\u7c7b\u4f3c\u523a\u6fc0\u9a71\u52a8\u6d3b\u52a8\u7684\"\u91cd\u653e\"\u73b0\u8c61\u3002\u73b0\u6709\u8ba1\u7b97\u6a21\u578b\u5c06\u566a\u58f0\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u91cd\u653e\u63cf\u8ff0\u4e3a\u6717\u4e4b\u4e07\u91c7\u6837\uff0c\u4f46\u65b0\u7684\u6539\u8fdb\u65b9\u6cd5\u5df2\u7ecf\u8d85\u8d8a\u4e86\u8fd9\u4e00\u63cf\u8ff0\u3002\u672c\u6587\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6\u566a\u58f0\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u91cd\u653e\u4f5c\u4e3a\u91c7\u6837\u8fc7\u7a0b\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u6539\u8fdb\u5b83\u3002", "method": "1. \u5728\u7b80\u5355\u5047\u8bbe\u4e0b\u8bc1\u660e\u91cd\u653e\u6d3b\u52a8\u5e94\u9075\u5faa\u7684\u68af\u5ea6\u662f\u65f6\u53d8\u7684\u4e14\u96be\u4ee5\u4f30\u8ba1\uff0c\u4f46\u53ef\u4ee5\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5728RNN\u4e2d\u4f7f\u7528\u9690\u85cf\u72b6\u6001\u6cc4\u6f0f\u6709\u52a9\u4e8e\u91cd\u653e\u3002\n2. \u9a8c\u8bc1\u9690\u85cf\u72b6\u6001\u9002\u5e94\uff08\u8d1f\u53cd\u9988\uff09\u4fc3\u8fdb\u91cd\u653e\u4e2d\u7684\u63a2\u7d22\uff0c\u4f46\u4f1a\u5bfc\u81f4\u975e\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u4ece\u800c\u51cf\u6162\u91cd\u653e\u901f\u5ea6\u3002\n3. \u63d0\u51fa\u9996\u4e2a\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u52a8\u91cf\u5728\u566a\u58f0\u8def\u5f84\u79ef\u5206RNN\u4e2d\u5b9e\u73b0\u65f6\u95f4\u538b\u7f29\u91cd\u653e\u7684\u6a21\u578b\uff0c\u5c06\u5176\u4e0e\u6b20\u963b\u5c3c\u6717\u4e4b\u4e07\u91c7\u6837\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5c55\u793a\u4e0e\u9002\u5e94\u76f8\u7ed3\u5408\u53ef\u4ee5\u62b5\u6d88\u901f\u5ea6\u51cf\u6162\u540c\u65f6\u4fdd\u6301\u63a2\u7d22\u3002", "result": "\u901a\u8fc72D\u4e09\u89d2\u5f62\u8def\u5f84\u3001T\u578b\u8ff7\u5bab\u8def\u5f84\u4ee5\u53ca\u5408\u6210\u5927\u9f20\u4f4d\u7f6e\u7ec6\u80de\u6d3b\u52a8\u7684\u9ad8\u7ef4\u8def\u5f84\u7684\u8def\u5f84\u79ef\u5206\u9a8c\u8bc1\u4e86\u7814\u7a76\u53d1\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff1a\u9690\u85cf\u72b6\u6001\u6cc4\u6f0f\u6709\u52a9\u4e8e\u91cd\u653e\uff1b\u9690\u85cf\u72b6\u6001\u9002\u5e94\u4fc3\u8fdb\u63a2\u7d22\u4f46\u51cf\u6162\u91cd\u653e\uff1b\u9690\u85cf\u72b6\u6001\u52a8\u91cf\u4e0e\u9002\u5e94\u76f8\u7ed3\u5408\u53ef\u4ee5\u5b9e\u73b0\u65f6\u95f4\u538b\u7f29\u91cd\u653e\uff0c\u540c\u65f6\u4fdd\u6301\u63a2\u7d22\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u4e3a\u566a\u58f0\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u91cd\u653e\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u7684\u91c7\u6837\u7406\u8bba\u89c6\u89d2\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u91cd\u653e\u6027\u80fd\u7684\u5177\u4f53\u65b9\u6cd5\u3002\u901a\u8fc7\u7ed3\u5408\u9690\u85cf\u72b6\u6001\u9002\u5e94\u548c\u52a8\u91cf\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u63a2\u7d22\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u65f6\u95f4\u538b\u7f29\u91cd\u653e\uff0c\u8fd9\u4e3a\u7406\u89e3\u548c\u6a21\u62df\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u91cd\u653e\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u6846\u67b6\u3002"}}
{"id": "2602.18171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18171", "abs": "https://arxiv.org/abs/2602.18171", "authors": ["Wojciech Michaluk", "Tymoteusz Urban", "Mateusz Kubita", "Soveatin Kuntur", "Anna Wroblewska"], "title": "Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models", "comment": null, "summary": "Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408Transformer\u6587\u672c\u5d4c\u5165\u4e0e\u8bed\u8a00\u5b66\u4fe1\u606f\u7279\u5f81\u7684\u6df7\u5408\u65b9\u6cd5\u68c0\u6d4b\u70b9\u51fb\u8bf1\u9975\u6807\u9898\uff0c\u6700\u4f73\u6a21\u578bF1\u5206\u6570\u8fbe91%\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u70b9\u51fb\u8bf1\u9975\u6807\u9898\u964d\u4f4e\u5728\u7ebf\u4fe1\u606f\u8d28\u91cf\u5e76\u635f\u5bb3\u7528\u6237\u4fe1\u4efb\uff0c\u9700\u8981\u6709\u6548\u68c0\u6d4b\u65b9\u6cd5", "method": "\u6df7\u5408\u65b9\u6cd5\uff1a\u7ed3\u5408Transformer\u6587\u672c\u5d4c\u5165\u4e0e15\u4e2a\u8bed\u8a00\u5b66\u4fe1\u606f\u7279\u5f81\uff0c\u4f7f\u7528XGBoost\u5206\u7c7b\u5668", "result": "\u6700\u4f73\u6a21\u578bF1\u5206\u657091%\uff0c\u4f18\u4e8eTF-IDF\u3001Word2Vec\u3001GloVe\u3001LLM\u63d0\u793a\u5206\u7c7b\u548c\u7eaf\u7279\u5f81\u57fa\u7ebf", "conclusion": "\u63d0\u51fa\u7684\u7279\u5f81\u96c6\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u7a81\u51fa\u7b2c\u4e8c\u4eba\u79f0\u4ee3\u8bcd\u3001\u6700\u9ad8\u7ea7\u3001\u6570\u5b57\u7b49\u8bed\u8a00\u5b66\u7ebf\u7d22\uff0c\u5b9e\u73b0\u900f\u660e\u4e14\u6821\u51c6\u826f\u597d\u7684\u70b9\u51fb\u8bf1\u9975\u9884\u6d4b"}}
{"id": "2602.17694", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17694", "abs": "https://arxiv.org/abs/2602.17694", "authors": ["Hui Ma", "Shaoyu Dou", "Ya Liu", "Fei Xing", "Li Feng", "Feng Pi"], "title": "AsynDBT: Asynchronous Distributed Bilevel Tuning for efficient In-Context Learning with Large Language Models", "comment": "Accepted in Scientific Reports", "summary": "With the rapid development of large language models (LLMs), an increasing number of applications leverage cloud-based LLM APIs to reduce usage costs. However, since cloud-based models' parameters and gradients are agnostic, users have to manually or use heuristic algorithms to adjust prompts for intervening LLM outputs, which requiring costly optimization procedures. In-context learning (ICL) has recently emerged as a promising paradigm that enables LLMs to adapt to new tasks using examples provided within the input, eliminating the need for parameter updates. Nevertheless, the advancement of ICL is often hindered by the lack of high-quality data, which is often sensitive and different to share. Federated learning (FL) offers a potential solution by enabling collaborative training of distributed LLMs while preserving data privacy. Despite this issues, previous FL approaches that incorporate ICL have struggled with severe straggler problems and challenges associated with heterogeneous non-identically data. To address these problems, we propose an asynchronous distributed bilevel tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on the feedback from the LLM, thereby enhancing downstream task performance. Benefiting from its distributed architecture, AsynDBT provides privacy protection and adaptability to heterogeneous computing environments. Furthermore, we present a theoretical analysis establishing the convergence guarantees of the proposed algorithm. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.", "AI": {"tldr": "\u63d0\u51fa\u5f02\u6b65\u5206\u5e03\u5f0f\u53cc\u5c42\u8c03\u4f18\u7b97\u6cd5AsynDBT\uff0c\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u6837\u672c\u548c\u63d0\u793a\u7247\u6bb5\u6765\u63d0\u5347LLM\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u5e76\u9002\u5e94\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u3002", "motivation": "\u4e91\u7aefLLM API\u4f7f\u7528\u6210\u672c\u4f4e\u4f46\u53c2\u6570\u68af\u5ea6\u4e0d\u53ef\u77e5\uff0c\u7528\u6237\u9700\u624b\u52a8\u8c03\u6574\u63d0\u793a\uff1b\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u9700\u8981\u9ad8\u8d28\u91cf\u6570\u636e\u4f46\u6570\u636e\u654f\u611f\u96be\u4ee5\u5171\u4eab\uff1b\u8054\u90a6\u5b66\u4e60(FL)\u80fd\u4fdd\u62a4\u9690\u79c1\u4f46\u5b58\u5728\u6ede\u540e\u8282\u70b9\u95ee\u9898\u548c\u5f02\u6784\u6570\u636e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5f02\u6b65\u5206\u5e03\u5f0f\u53cc\u5c42\u8c03\u4f18\u7b97\u6cd5AsynDBT\uff0c\u57fa\u4e8eLLM\u53cd\u9988\u540c\u65f6\u4f18\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u6837\u672c\u548c\u63d0\u793a\u7247\u6bb5\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\u4fdd\u62a4\u9690\u79c1\u5e76\u9002\u5e94\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AsynDBT\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "AsynDBT\u901a\u8fc7\u5206\u5e03\u5f0f\u53cc\u5c42\u8c03\u4f18\u89e3\u51b3\u4e86\u4e91\u7aefLLM\u4f18\u5316\u3001ICL\u6570\u636e\u8d28\u91cf\u548cFL\u6ede\u540e\u8282\u70b9\u7b49\u95ee\u9898\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u4e86LLM\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.18160", "categories": ["cs.LG", "cs.CC", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18160", "abs": "https://arxiv.org/abs/2602.18160", "authors": ["Shahaf Bassan", "Xuanxiang Huang", "Guy Katz"], "title": "Unifying Formal Explanations: A Complexity-Theoretic Perspective", "comment": "To appear in ICLR 2026", "summary": "Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u5206\u6790ML\u6a21\u578b\u9884\u6d4b\u7684\u5145\u5206\u539f\u56e0\u548c\u5bf9\u6bd4\u539f\u56e0\u89e3\u91ca\uff0c\u53d1\u73b0\u8ba1\u7b97\u590d\u6742\u5ea6\u53d6\u51b3\u4e8e\u4ef7\u503c\u51fd\u6570\u7684\u5355\u8c03\u6027\u3001\u6b21\u6a21\u6027\u548c\u8d85\u6a21\u6027\uff0c\u63ed\u793a\u4e86\u5168\u5c40\u89e3\u91ca\u6027\u53ef\u591a\u9879\u5f0f\u65f6\u95f4\u8ba1\u7b97\u800c\u5c40\u90e8\u89e3\u91ca\u6027\u4e3aNP\u96be\u7684\u53cd\u76f4\u89c9\u7ed3\u679c\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5206\u522b\u63a2\u8ba8\u4e86ML\u6a21\u578b\u9884\u6d4b\u7684\u5145\u5206\u539f\u56e0\u548c\u5bf9\u6bd4\u539f\u56e0\u89e3\u91ca\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u89e3\u91ca\u7684\u8ba1\u7b97\u7279\u6027\u3002", "method": "\u5f15\u5165\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u6240\u6709\u89e3\u91ca\u7c7b\u578b\u901a\u8fc7\u6700\u5c0f\u5316\u7edf\u4e00\u7684\u6982\u7387\u4ef7\u503c\u51fd\u6570\u6765\u8868\u5f81\u3002\u5206\u6790\u4ef7\u503c\u51fd\u6570\u7684\u4e09\u4e2a\u5173\u952e\u7ec4\u5408\u4f18\u5316\u5c5e\u6027\uff1a\u5355\u8c03\u6027\u3001\u6b21\u6a21\u6027\u548c\u8d85\u6a21\u6027\uff0c\u5e76\u7814\u7a76\u8fd9\u4e9b\u5c5e\u6027\u5728\u5c40\u90e8\u4e0e\u5168\u5c40\u89e3\u91ca\u8bbe\u7f6e\u4e2d\u7684\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u5c40\u90e8\u4ef7\u503c\u51fd\u6570\u4e0d\u5177\u5907\u5355\u8c03\u6027\u6216\u6b21\u6a21\u6027/\u8d85\u6a21\u6027\uff0c\u800c\u5168\u5c40\u4ef7\u503c\u51fd\u6570\u5177\u6709\u8fd9\u4e9b\u5c5e\u6027\u3002\u56e0\u6b64\u5728\u5168\u5c40\u89e3\u91ca\u6027\u8bbe\u7f6e\u4e2d\uff0c\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8ba1\u7b97\u591a\u79cd\u89e3\u91ca\uff08\u5305\u62ec\u795e\u7ecf\u7f51\u7edc\u3001\u51b3\u7b56\u6811\u7b49\u6a21\u578b\uff09\uff0c\u800c\u76f8\u5e94\u7684\u5c40\u90e8\u89e3\u91ca\u6027\u8bbe\u7f6e\u4e2d\u5373\u4f7f\u662f\u7b80\u5316\u7248\u672c\u4e5f\u662fNP\u96be\u7684\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u63ed\u793a\u4e86ML\u89e3\u91ca\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\uff0c\u8bc1\u660e\u4e86\u5168\u5c40\u89e3\u91ca\u6027\u76f8\u6bd4\u5c40\u90e8\u89e3\u91ca\u6027\u5728\u8ba1\u7b97\u4e0a\u66f4\u5177\u4f18\u52bf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2602.18176", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18176", "abs": "https://arxiv.org/abs/2602.18176", "authors": ["Kaisen Yang", "Jayden Teoh", "Kaicheng Yang", "Yitong Zhang", "Alex Lamb"], "title": "Improving Sampling for Masked Diffusion Models via Information Gain", "comment": "https://github.com/yks23/Information-Gain-Sampler", "summary": "Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.", "AI": {"tldr": "\u63d0\u51faInfo-Gain Sampler\uff0c\u4e00\u79cd\u7528\u4e8e\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u65b0\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u8861\u5373\u65f6\u4e0d\u786e\u5b9a\u6027\u548c\u672a\u6765\u4fe1\u606f\u589e\u76ca\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u6269\u6563\u6a21\u578b\u89e3\u7801\u5668\u91c7\u7528\u8d2a\u5fc3\u7b56\u7565\uff0c\u53ea\u5173\u6ce8\u5c40\u90e8\u786e\u5b9a\u6027\uff0c\u5ffd\u7565\u4e86\u5f53\u524d\u89e3\u7801\u51b3\u7b56\u5bf9\u540e\u7eed\u6b65\u9aa4\u7684\u5f71\u54cd\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528MDMs\u7684\u975e\u56e0\u679c\u7279\u6027\u6765\u6700\u5c0f\u5316\u7d2f\u79ef\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4fe1\u606f\u589e\u76ca\u91c7\u6837\u5668\uff0c\u5728\u89e3\u7801\u65f6\u4e0d\u4ec5\u8003\u8651\u5f53\u524d\u4f4d\u7f6e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd8\u8bc4\u4f30\u5f53\u524d\u51b3\u7b56\u5982\u4f55\u6539\u53d8\u6240\u6709\u5269\u4f59\u63a9\u7801\u4f4d\u7f6e\u7684token\u6982\u7387/\u4e0d\u786e\u5b9a\u6027\uff0c\u5e73\u8861\u5373\u65f6\u4e0d\u786e\u5b9a\u6027\u548c\u672a\u6765\u4fe1\u606f\u589e\u76ca\u3002", "result": "\u5728\u63a8\u7406\u3001\u7f16\u7a0b\u3001\u521b\u610f\u5199\u4f5c\u548c\u56fe\u50cf\u751f\u6210\u7b49\u591a\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u4e0a\uff0cInfo-Gain Sampler\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u91c7\u6837\u5668\uff1a\u63a8\u7406\u4efb\u52a1\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.6%\uff0c\u521b\u610f\u5199\u4f5c\u80dc\u738763.1%\uff0c\u63a8\u7406\u4efb\u52a1\u7d2f\u79ef\u4e0d\u786e\u5b9a\u6027\u4ece78.4\u964d\u81f348.6\u3002", "conclusion": "Info-Gain Sampler\u901a\u8fc7\u5229\u7528MDMs\u7684\u975e\u56e0\u679c\u7279\u6027\uff0c\u7cfb\u7edf\u6027\u5730\u8003\u8651\u89e3\u7801\u51b3\u7b56\u7684\u672a\u6765\u5f71\u54cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4e3aMDMs\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u7801\u6846\u67b6\u3002"}}
{"id": "2602.17695", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17695", "abs": "https://arxiv.org/abs/2602.17695", "authors": ["Xin Yu", "Hanwen Xing", "Lingzhou Xue"], "title": "EXACT: Explicit Attribute-Guided Decoding-Time Personalization", "comment": null, "summary": "Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.", "AI": {"tldr": "EXACT\uff1a\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u5c5e\u6027\u7684\u89e3\u7801\u65f6\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u7528\u6237\u7279\u5b9a\u5c5e\u6027\u5b50\u96c6\u5e76\u57fa\u4e8e\u8bed\u4e49\u68c0\u7d22\u76f8\u5173\u5c5e\u6027\u6765\u5f15\u5bfc\u751f\u6210\uff0c\u6709\u6548\u5e94\u5bf9\u4e0a\u4e0b\u6587\u504f\u597d\u53d8\u5316", "motivation": "\u73b0\u6709\u89e3\u7801\u65f6\u4e2a\u6027\u5316\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u3001\u96be\u4ee5\u89e3\u91ca\u7684\u504f\u597d\u8868\u793a\uff0c\u4e14\u91c7\u7528\u50f5\u5316\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u7528\u6237\u8868\u793a\uff0c\u65e0\u6cd5\u5904\u7406\u4e0d\u540c\u63d0\u793a\u4e0b\u7684\u504f\u597d\u53d8\u5316", "method": "EXACT\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u53ef\u89e3\u91ca\u5c5e\u6027\u96c6\uff0c\u5728\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7\u6700\u5927\u5316\u504f\u597d\u54cd\u5e94\u4f3c\u7136\u8bc6\u522b\u7528\u6237\u7279\u5b9a\u5c5e\u6027\u5b50\u96c6\uff1b\u5728\u7ebf\u63a8\u7406\u65f6\u57fa\u4e8e\u8bed\u4e49\u68c0\u7d22\u4e0e\u8f93\u5165\u63d0\u793a\u6700\u76f8\u5173\u7684\u5c5e\u6027\u6ce8\u5165\u4e0a\u4e0b\u6587\u5f15\u5bfc\u751f\u6210", "result": "\u5728\u4eba\u5de5\u6807\u6ce8\u7684\u504f\u597d\u6570\u636e\u96c6\u4e0a\uff0cEXACT\u5728\u504f\u597d\u5efa\u6a21\u51c6\u786e\u7387\u548c\u4e2a\u6027\u5316\u751f\u6210\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "EXACT\u901a\u8fc7\u53ef\u89e3\u91ca\u5c5e\u6027\u548c\u8bed\u4e49\u68c0\u7d22\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u504f\u597d\u53d8\u5316\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u5bf9\u9f50\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.17685", "categories": ["cs.LG", "cs.RO", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2602.17685", "abs": "https://arxiv.org/abs/2602.17685", "authors": ["Agni Bandyopadhyay", "Gunther Waxenegger-Wilfing"], "title": "Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling", "comment": "Presented at Conference: IFAC Workshop on Control Aspects of Multi-Satellite Systems (CAMSAT) 2025 At: Wuerzburg", "summary": "This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u5171\u692d\u5706\u673a\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u970d\u66fc\u8f6c\u79fb\u3001\u5b89\u5168\u692d\u5706\u63a5\u8fd1\u64cd\u4f5c\u548c\u663e\u5f0f\u52a0\u6cb9\u903b\u8f91\uff0c\u7528\u4e8e\u4f4e\u5730\u7403\u8f68\u9053\u591a\u76ee\u6807\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u4efb\u52a1\u89c4\u5212\uff0c\u6bd4\u8f83\u8d2a\u5a6a\u542f\u53d1\u5f0f\u3001\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u63a9\u7801PPO\u5f3a\u5316\u5b66\u4e60\u4e09\u79cd\u7b97\u6cd5\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5730\u7403\u8f68\u9053\u591a\u76ee\u6807\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u8d44\u6e90\u4f18\u5316\u7684\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u968f\u673a\u788e\u7247\u573a\u3001\u7981\u98de\u533a\u548c\u71c3\u6599\u7ea6\u675f\u7b49\u73b0\u5b9e\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u5171\u692d\u5706\u673a\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u970d\u66fc\u8f6c\u79fb\u3001\u5b89\u5168\u692d\u5706\u63a5\u8fd1\u64cd\u4f5c\u548c\u663e\u5f0f\u52a0\u6cb9\u903b\u8f91\u3002\u5728\u5305\u542b\u968f\u673a\u788e\u7247\u573a\u3001\u7981\u98de\u533a\u548c\u0394V\u7ea6\u675f\u7684\u903c\u771f\u8f68\u9053\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u5bf9\u6bd4\u8bc4\u4f30\u8d2a\u5a6a\u542f\u53d1\u5f0f\u3001\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u63a9\u7801\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u4e09\u79cd\u89c4\u5212\u7b97\u6cd5\u3002", "result": "\u5728100\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u63a9\u7801PPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5c55\u73b0\u51fa\u6700\u4f18\u7684\u4efb\u52a1\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd\uff0c\u8bbf\u95ee\u7684\u788e\u7247\u6570\u91cf\u662f\u8d2a\u5a6a\u7b97\u6cd5\u7684\u4e24\u500d\uff0c\u4e14\u5728\u8fd0\u884c\u65f6\u95f4\u4e0a\u663e\u8457\u4f18\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u3002", "conclusion": "\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u3001\u5b89\u5168\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u7a7a\u95f4\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u81ea\u4e3b\u6027\u7684\u672a\u6765\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.18217", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18217", "abs": "https://arxiv.org/abs/2602.18217", "authors": ["Kohei Kajikawa", "Shinnosuke Isono", "Ethan Gotlieb Wilcox"], "title": "Information-Theoretic Storage Cost in Sentence Comprehension", "comment": null, "summary": "Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u53e5\u5b50\u7406\u89e3\u52a0\u5de5\u5b58\u50a8\u6210\u672c\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u5148\u524d\u8bcd\u8bed\u5bf9\u672a\u6765\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u82f1\u8bed\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5b9e\u65f6\u53e5\u5b50\u7406\u89e3\u5bf9\u5de5\u4f5c\u8bb0\u5fc6\u4ea7\u751f\u663e\u8457\u8d1f\u8377\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u7b26\u53f7\u8bed\u6cd5\u7684\u5ea6\u91cf\u65b9\u6cd5\u4f7f\u7528\u79bb\u6563\u3001\u7edf\u4e00\u7684\u53e5\u6cd5\u9884\u6d4b\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u8fde\u7eed\u3001\u7406\u8bba\u4e2d\u7acb\u4e14\u80fd\u4ece\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u52a0\u5de5\u5b58\u50a8\u6210\u672c\u5ea6\u91cf\uff0c\u5b9a\u4e49\u4e3a\u5148\u524d\u8bcd\u8bed\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u5bf9\u672a\u6765\u4e0a\u4e0b\u6587\u6240\u643a\u5e26\u7684\u4fe1\u606f\u91cf\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4f30\u8ba1\uff0c\u901a\u8fc7\u4e09\u4e2a\u82f1\u8bed\u5206\u6790\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "result": "\u8be5\u5ea6\u91cf\u65b9\u6cd5\uff1a(1)\u6062\u590d\u4e86\u4e2d\u5fc3\u5d4c\u5957\u548c\u5173\u7cfb\u4ece\u53e5\u4e2d\u5df2\u77e5\u7684\u52a0\u5de5\u4e0d\u5bf9\u79f0\u6027\uff1b(2)\u5728\u53e5\u6cd5\u6807\u6ce8\u8bed\u6599\u5e93\u4e2d\u4e0e\u57fa\u4e8e\u8bed\u6cd5\u7684\u5b58\u50a8\u6210\u672c\u76f8\u5173\uff1b(3)\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u81ea\u7136\u6570\u636e\u96c6\u4e2d\u9884\u6d4b\u9605\u8bfb\u65f6\u95f4\u65b9\u5dee\uff0c\u4f18\u4e8e\u4f20\u7edf\u4fe1\u606f\u57fa\u7ebf\u7684\u9884\u6d4b\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fe1\u606f\u8bba\u5ea6\u91cf\u65b9\u6cd5\u4e3a\u53e5\u5b50\u7406\u89e3\u4e2d\u7684\u52a0\u5de5\u5b58\u50a8\u6210\u672c\u63d0\u4f9b\u4e86\u8fde\u7eed\u3001\u7406\u8bba\u4e2d\u7acb\u4e14\u53ef\u8ba1\u7b97\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u6355\u6349\u4f20\u7edf\u79bb\u6563\u5ea6\u91cf\u65e0\u6cd5\u6355\u6349\u7684\u52a0\u5de5\u8d1f\u8377\u7ec6\u5fae\u5dee\u522b\u3002"}}
{"id": "2602.17696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17696", "abs": "https://arxiv.org/abs/2602.17696", "authors": ["Zongmin Li", "Jian Su", "Farah Benamara", "Aixin Sun"], "title": "Can LLM Safety Be Ensured by Constraining Parameter Regions?", "comment": "32 pages", "summary": "Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.", "AI": {"tldr": "\u5f53\u524d\u7684\u5b89\u5168\u533a\u57df\u8bc6\u522b\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u7a33\u5b9a\u3001\u6570\u636e\u96c6\u65e0\u5173\u7684\u5b89\u5168\u533a\u57df", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u88ab\u8ba4\u4e3a\u5305\u542b\"\u5b89\u5168\u533a\u57df\"\u2014\u2014\u76f4\u63a5\u5f71\u54cd\u5b89\u5168\u884c\u4e3a\u7684\u53c2\u6570\u5b50\u96c6\uff0c\u4f46\u73b0\u6709\u5b89\u5168\u533a\u57df\u8bc6\u522b\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30", "method": "\u7cfb\u7edf\u8bc4\u4f30\u56db\u79cd\u4e0d\u540c\u53c2\u6570\u7c92\u5ea6\uff08\u4ece\u5355\u4e2a\u6743\u91cd\u5230\u6574\u4e2aTransformer\u5c42\uff09\u7684\u5b89\u5168\u533a\u57df\u8bc6\u522b\u65b9\u6cd5\uff0c\u5728\u56db\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLM\u5bb6\u65cf\u4e0a\uff0c\u4f7f\u7528\u5341\u4e2a\u5b89\u5168\u8bc6\u522b\u6570\u636e\u96c6", "result": "\u8bc6\u522b\u7684\u5b89\u5168\u533a\u57df\u4ec5\u663e\u793a\u4f4e\u5230\u4e2d\u5ea6\u7684\u91cd\u53e0\uff08IoU\u6d4b\u91cf\uff09\uff0c\u5f53\u4f7f\u7528\u6548\u7528\u6570\u636e\u96c6\uff08\u975e\u6709\u5bb3\u67e5\u8be2\uff09\u8fdb\u4e00\u6b65\u7cbe\u70bc\u65f6\uff0c\u91cd\u53e0\u663e\u8457\u4e0b\u964d", "conclusion": "\u5f53\u524d\u6280\u672f\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u7a33\u5b9a\u3001\u6570\u636e\u96c6\u65e0\u5173\u7684\u5b89\u5168\u533a\u57df\uff0c\u8868\u660e\u73b0\u6709\u5b89\u5168\u533a\u57df\u8bc6\u522b\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2602.18232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18232", "abs": "https://arxiv.org/abs/2602.18232", "authors": ["Lexiang Tang", "Weihao Gao", "Bingchen Zhao", "Lu Ma", "Qiao jin", "Bang Yang", "Yuexian Zou"], "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning", "comment": null, "summary": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.", "AI": {"tldr": "\u63d0\u51faConfidence-Driven Contrastive Decoding\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u4f4e\u7f6e\u4fe1\u5ea6token\u5e76\u9009\u62e9\u6027\u5e72\u9884\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u53ef\u9760\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6", "motivation": "\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u589e\u52a0\u63a8\u7406\u8ba1\u7b97\u91cf\u80fd\u5747\u5300\u63d0\u5347\u6b63\u786e\u6027\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u4e0d\u786e\u5b9a\u6027\u9ad8\u5ea6\u5c40\u90e8\u5316\uff1a\u5c11\u6570\u4f4e\u7f6e\u4fe1\u5ea6token\u5bf9\u63a8\u7406\u9519\u8bef\u548c\u4e0d\u5fc5\u8981\u8f93\u51fa\u6269\u5c55\u8d21\u732e\u4e0d\u6210\u6bd4\u4f8b", "method": "\u63d0\u51faThinking by Subtraction\u65b9\u6cd5\uff0c\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u68c0\u6d4b\u4f4e\u7f6e\u4fe1\u5ea6token\uff0c\u6784\u5efa\u5bf9\u6bd4\u53c2\u8003\u5206\u5e03\uff08\u7528\u6700\u5c0f\u5360\u4f4d\u7b26\u66ff\u6362\u9ad8\u7f6e\u4fe1\u5ea6token\uff09\uff0c\u5728\u4f4e\u7f6e\u4fe1\u4f4d\u7f6e\u901a\u8fc7\u51cf\u53bb\u53c2\u8003\u5206\u5e03\u6765\u7cbe\u70bc\u9884\u6d4b", "result": "CCD\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\uff0cKV-cache\u5f00\u9500\u6700\u5c0f\uff0c\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u4f4e\u7f6e\u4fe1\u5ea6\u5e72\u9884\uff0cCCD\u65b9\u6cd5\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u53ef\u9760\u6027\uff0c\u907f\u514d\u8ba1\u7b97\u5197\u4f59\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u8fc7\u7a0b"}}
{"id": "2602.17698", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17698", "abs": "https://arxiv.org/abs/2602.17698", "authors": ["Xinlin Li", "Timothy Chou", "Josh Fromm", "Zichang Liu", "Yunjie Pan", "Christina Fragouli"], "title": "ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs", "comment": null, "summary": "Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.", "AI": {"tldr": "ScaleBITS\uff1a\u4e00\u79cd\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u786c\u4ef6\u5bf9\u9f50\u7684\u5757\u7ea7\u6743\u91cd\u5206\u533a\u548c\u53cc\u5411\u901a\u9053\u91cd\u6392\u5e8f\uff0c\u5728\u5185\u5b58\u9884\u7b97\u4e0b\u5b9e\u73b0\u81ea\u52a8\u5316\u7ec6\u7c92\u5ea6\u6bd4\u7279\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u540e\u8bad\u7ec3\u6743\u91cd\u91cf\u5316\u9762\u4e34\u6311\u6218\uff1a\u4f4e\u4e8e4\u6bd4\u7279\u65f6\u6743\u91cd\u654f\u611f\u6027\u9ad8\u5ea6\u4e0d\u5747\u5300\uff0c\u7f3a\u4e4f\u539f\u5219\u6027\u7cbe\u5ea6\u5206\u914d\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6848\u8981\u4e48\u4f7f\u7528\u9ad8\u8fd0\u884c\u65f6\u5f00\u9500\u7684\u4e0d\u89c4\u5219\u7ec6\u7c92\u5ea6\u6df7\u5408\u7cbe\u5ea6\uff0c\u8981\u4e48\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u9ad8\u5ea6\u53d7\u9650\u7684\u7cbe\u5ea6\u5206\u914d\u7b56\u7565\u3002", "method": "\u63d0\u51faScaleBITS\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u65b0\u654f\u611f\u6027\u5206\u6790\u6307\u5bfc\uff1b2\uff09\u5f15\u5165\u786c\u4ef6\u5bf9\u9f50\u7684\u5757\u7ea7\u6743\u91cd\u5206\u533a\u65b9\u6848\uff0c\u91c7\u7528\u53cc\u5411\u901a\u9053\u91cd\u6392\u5e8f\uff1b3\uff09\u5c06\u5168\u5c40\u6bd4\u7279\u5206\u914d\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u8d2a\u5fc3\u7b97\u6cd5\u8fd1\u4f3c\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cScaleBITS\u5728\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u663e\u8457\u4f18\u4e8e\u5747\u5300\u7cbe\u5ea6\u91cf\u5316\uff08\u63d0\u5347\u8fbe36%\uff09\uff0c\u5e76\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u654f\u611f\u6027\u611f\u77e5\u57fa\u7ebf\u65b9\u6cd5\uff08\u63d0\u5347\u8fbe13%\uff09\uff0c\u4e14\u4e0d\u589e\u52a0\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "ScaleBITS\u901a\u8fc7\u786c\u4ef6\u9ad8\u6548\u7684\u7ec6\u7c92\u5ea6\u6df7\u5408\u7cbe\u5ea6\u5206\u914d\uff0c\u89e3\u51b3\u4e86LLM\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2602.17688", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17688", "abs": "https://arxiv.org/abs/2602.17688", "authors": ["Anton Xue", "Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "AnCoder: Anchored Code Generation via Discrete Diffusion Models", "comment": null, "summary": "Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.", "AI": {"tldr": "\u63d0\u51faAnchorTree\u6846\u67b6\uff0c\u901a\u8fc7\u62bd\u8c61\u8bed\u6cd5\u6811\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u4f18\u5148\u89e3\u51b3\u8bed\u6cd5\u548c\u8bed\u4e49\u5173\u952etoken\uff0c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u8d28\u91cf", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e3a\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u5168\u5c40\u89c4\u5212\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e0d\u5c0a\u91cd\u7f16\u7a0b\u8bed\u8a00\u7684\u521a\u6027\u7ed3\u6784\uff0c\u5e38\u4ea7\u751f\u65e0\u6cd5\u6267\u884c\u7684\u9519\u8bef\u7a0b\u5e8f", "method": "\u5f15\u5165AnchorTree\u6846\u67b6\uff0c\u4f7f\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u4f5c\u4e3a\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u4f18\u5148\u89e3\u6790\u8bed\u6cd5\u548c\u8bed\u4e49\u5173\u952etoken\uff08\u5982\u5173\u952e\u5b57\u3001\u6807\u8bc6\u7b26\uff09\uff0c\u5efa\u7acb\u7ed3\u6784\u652f\u67b6\u5f15\u5bfc\u540e\u7eed\u751f\u6210", "result": "\u901a\u8fc7AnCoder\u6a21\u578b\u7cfb\u5217\u9a8c\u8bc1\uff0c\u7ed3\u6784\u951a\u5b9a\u7684\u6269\u6563\u65b9\u6cd5\u80fd\u4ee5\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210", "conclusion": "\u7ed3\u6784\u5316\u951a\u5b9a\u6269\u6563\u4e3a\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\uff0c\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u4ee3\u7801\u7684\u5c42\u6b21\u7ed3\u6784\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u7a0b\u5e8f\u7684\u8d28\u91cf\u548c\u53ef\u6267\u884c\u6027"}}
{"id": "2602.18262", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18262", "abs": "https://arxiv.org/abs/2602.18262", "authors": ["Aaron Louis Eidt", "Nils Feldhus"], "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA", "comment": "EACL 2026 System Demonstrations. GitHub: https://github.com/aaron0eidt/ELIA", "summary": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.", "AI": {"tldr": "ELIA\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0fWeb\u5e94\u7528\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cdLLM\u5206\u6790\u5de5\u5177\u548cAI\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u964d\u4f4e\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u4f7f\u975e\u4e13\u5bb6\u4e5f\u80fd\u7406\u89e3\u590d\u6742\u6a21\u578b\u5206\u6790\u3002", "motivation": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u8fc7\u4e8e\u590d\u6742\uff0c\u4ec5\u9650\u4e8e\u4e13\u5bb6\u4f7f\u7528\uff0c\u5b58\u5728\u53ef\u8bbf\u95ee\u6027\u5dee\u8ddd\u3002\u9700\u8981\u8bbe\u8ba1\u66f4\u6613\u7528\u7684\u5de5\u5177\u8ba9\u66f4\u5e7f\u6cdb\u7684\u53d7\u4f17\u80fd\u591f\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5206\u6790\u7ed3\u679c\u3002", "method": "\u6784\u5efaELIA\u4ea4\u4e92\u5f0fWeb\u5e94\u7528\uff0c\u6574\u5408\u5f52\u56e0\u5206\u6790\u3001\u51fd\u6570\u5411\u91cf\u5206\u6790\u548c\u7535\u8def\u8ffd\u8e2a\u4e09\u79cd\u5173\u952e\u6280\u672f\uff0c\u5e76\u521b\u65b0\u6027\u5730\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u590d\u6742\u53ef\u89c6\u5316\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u660e\u663e\u504f\u597d\u4ea4\u4e92\u5f0f\u53ef\u63a2\u7d22\u754c\u9762\u800c\u975e\u9759\u6001\u53ef\u89c6\u5316\u3002AI\u751f\u6210\u7684\u89e3\u91ca\u5e2e\u52a9\u975e\u4e13\u5bb6\u7406\u89e3\uff0c\u7edf\u8ba1\u663e\u793a\u7528\u6237\u5148\u524dLLM\u7ecf\u9a8c\u4e0e\u7406\u89e3\u5206\u6570\u65e0\u663e\u8457\u76f8\u5173\u6027\uff0c\u8bf4\u660e\u7cfb\u7edf\u964d\u4f4e\u4e86\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u7406\u89e3\u969c\u788d\u3002", "conclusion": "AI\u7cfb\u7edf\u786e\u5b9e\u53ef\u4ee5\u7b80\u5316\u590d\u6742\u6a21\u578b\u5206\u6790\uff0c\u4f46\u5176\u771f\u6b63\u6f5c\u529b\u5728\u4e8e\u4e0e\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u4f18\u5148\u8003\u8651\u4ea4\u4e92\u6027\u3001\u7279\u5f02\u6027\u548c\u53d9\u4e8b\u5f15\u5bfc\uff0c\u4ece\u800c\u964d\u4f4e\u7406\u89e3\u95e8\u69db\u3002"}}
{"id": "2602.17700", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.17700", "abs": "https://arxiv.org/abs/2602.17700", "authors": ["Konstanty Subbotko"], "title": "MIDAS: Mosaic Input-Specific Differentiable Architecture Search", "comment": null, "summary": "Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.", "AI": {"tldr": "MIDAS\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5fae\u5206\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u751f\u6210\u8f93\u5165\u7279\u5b9a\u7684\u67b6\u6784\u53c2\u6570\uff0c\u91c7\u7528\u5c40\u90e8\u5316\u8865\u4e01\u7ea7\u9009\u62e9\u548c\u62d3\u6251\u611f\u77e5\u641c\u7d22\u7a7a\u95f4\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5fae\u5206\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u91c7\u7528\u6709\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u5982DARTS\u4f7f\u7528\u9759\u6001\u67b6\u6784\u53c2\u6570\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u8f93\u5165\u7684\u9002\u5e94\u6027\uff0c\u4e14\u641c\u7d22\u7a7a\u95f4\u8bbe\u8ba1\u4e0d\u591f\u9c81\u68d2\u3002", "method": "1. \u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u66ff\u6362DARTS\u4e2d\u7684\u9759\u6001\u67b6\u6784\u53c2\u6570\uff0c\u751f\u6210\u52a8\u6001\u3001\u8f93\u5165\u7279\u5b9a\u7684\u53c2\u6570\uff1b2. \u5728\u6fc0\u6d3b\u56fe\u7684\u6bcf\u4e2a\u7a7a\u95f4\u8865\u4e01\u4e0a\u5c40\u90e8\u5316\u8ba1\u7b97\u67b6\u6784\u9009\u62e9\uff1b3. \u5f15\u5165\u53c2\u6570\u514d\u8d39\u3001\u62d3\u6251\u611f\u77e5\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u5efa\u6a21\u8282\u70b9\u8fde\u63a5\u6027\u5e76\u7b80\u5316\u6bcf\u4e2a\u8282\u70b9\u7684\u4e24\u4e2a\u8f93\u5165\u8fb9\u9009\u62e9\u3002", "result": "\u5728DARTS\u641c\u7d22\u7a7a\u95f4\u4e0a\uff0cCIFAR-10\u8fbe\u523097.42% top-1\u51c6\u786e\u7387\uff0cCIFAR-100\u8fbe\u523083.38%\uff1b\u5728NAS-Bench-201\u4e2d\u59cb\u7ec8\u627e\u5230\u5168\u5c40\u6700\u4f18\u67b6\u6784\uff1b\u5728RDARTS\u7684\u56db\u4e2a\u641c\u7d22\u7a7a\u95f4\u4e2d\u6709\u4e24\u4e2a\u8fbe\u5230SOTA\uff1b\u5206\u6790\u663e\u793a\u8865\u4e01\u7ea7\u6ce8\u610f\u529b\u63d0\u9ad8\u4e86\u5019\u9009\u64cd\u4f5c\u533a\u5206\u5ea6\uff0c\u53c2\u6570\u5206\u5e03\u5177\u6709\u7c7b\u522b\u611f\u77e5\u6027\u4e14\u4e3b\u8981\u4e3a\u5355\u5cf0\u5206\u5e03\u3002", "conclusion": "MIDAS\u901a\u8fc7\u52a8\u6001\u8f93\u5165\u7279\u5b9a\u7684\u67b6\u6784\u53c2\u6570\u548c\u5c40\u90e8\u5316\u9009\u62e9\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fae\u5206NAS\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u67b6\u6784\u641c\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18324", "abs": "https://arxiv.org/abs/2602.18324", "authors": ["Alexandra Ciobotaru", "Ana-Maria Bucur", "Liviu P. Dinu"], "title": "PsihoRo: Depression and Anxiety Romanian Text Corpus", "comment": "This article was accepted at LREC 2026", "summary": "Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.", "AI": {"tldr": "\u521b\u5efa\u4e86\u9996\u4e2a\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u6291\u90c1\u548c\u7126\u8651\u5fc3\u7406\u5065\u5eb7\u8bed\u6599\u5e93PsihoRo\uff0c\u5305\u542b205\u540d\u53d7\u8bbf\u8005\u7684\u6587\u672c\u6570\u636e\uff0c\u901a\u8fc7\u5f00\u653e\u5f0f\u95ee\u9898\u548c\u6807\u51c6\u5316\u95ee\u5377\u6536\u96c6\uff0c\u586b\u8865\u4e86\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u5fc3\u7406\u5065\u5eb7NLP\u8d44\u6e90\u7684\u7a7a\u767d\u3002", "motivation": "\u76ee\u524d\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7f3a\u4e4f\u5f00\u6e90\u7684\u5fc3\u7406\u5065\u5eb7\u8bed\u6599\u5e93\uff0c\u800c\u82f1\u8bed\u5df2\u6709\u4e30\u5bcc\u7684\u5fc3\u7406NLP\u8d44\u6e90\u3002\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u6536\u96c6\u5b58\u5728\u5047\u8bbe\u504f\u5dee\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5305\u542b6\u4e2a\u5f00\u653e\u5f0f\u95ee\u9898\u7684\u8868\u683c\uff0c\u7ed3\u5408\u6807\u51c6\u5316\u7684PHQ-9\u548cGAD-7\u7b5b\u67e5\u95ee\u5377\u6536\u96c6\u6570\u636e\u3002\u5bf9205\u540d\u53d7\u8bbf\u8005\u7684\u6587\u672c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3001\u7f57\u9a6c\u5c3c\u4e9a\u8bedLIWC\u6587\u672c\u5206\u6790\u3001\u60c5\u611f\u68c0\u6d4b\u548c\u4e3b\u9898\u5efa\u6a21\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u6291\u90c1\u548c\u7126\u8651\u8bed\u6599\u5e93PsihoRo\uff0c\u867d\u7136\u6837\u672c\u91cf\u8f83\u5c0f\uff08205\u540d\u53d7\u8bbf\u8005\uff09\uff0c\u4f46\u4e3a\u5206\u6790\u7f57\u9a6c\u5c3c\u4e9a\u4eba\u53e3\u5fc3\u7406\u5065\u5eb7\u6587\u672c\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002", "conclusion": "PsihoRo\u586b\u8865\u4e86\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u5fc3\u7406\u5065\u5eb7NLP\u8d44\u6e90\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u8be5\u65b0\u8d44\u6e90\u7684\u91cd\u8981\u7279\u5f81\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u7f57\u9a6c\u5c3c\u4e9a\u4eba\u53e3\u7684\u5fc3\u7406\u5065\u5eb7\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.17691", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17691", "abs": "https://arxiv.org/abs/2602.17691", "authors": ["Craig Atkinson"], "title": "Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering", "comment": "16 pages, 6 tables", "summary": "Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.\n  On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.\n  Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.", "AI": {"tldr": "HELIX\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u5c06\u9690\u85cf\u72b6\u6001\u8f68\u8ff9\u951a\u5b9a\u5728\u9884\u8ba1\u7b97\u7684\u771f\u5b9e\u6027\u6d41\u5f62\u4e0a\uff0c\u89e3\u8026\u8f93\u51fa\u71b5\u4e0e\u5e7b\u89c9\uff0c\u4f7f\u91cf\u5316\u8bed\u8a00\u6a21\u578b\u80fd\u5728\u9ad8\u6e29\u5ea6\u4e0b\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "motivation": "\u91cf\u5316\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u57fa\u672c\u56f0\u5883\uff1a\u4f4e\u91c7\u6837\u6e29\u5ea6\u5bfc\u81f4\u91cd\u590d\u3001\u6a21\u5f0f\u5d29\u6e83\u7684\u8f93\u51fa\uff0c\u800c\u9ad8\u6e29\u5ea6\uff08T > 2.0\uff09\u5219\u5f15\u8d77\u8f68\u8ff9\u53d1\u6563\u548c\u8bed\u4e49\u4e0d\u8fde\u8d2f\u3002\u9700\u8981\u89e3\u51b3\u9ad8\u6e29\u5ea6\u4e0b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u521b\u9020\u6027\u3002", "method": "\u63d0\u51faHELIX\u51e0\u4f55\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u7edf\u4e00\u771f\u5b9e\u6027\u5206\u6570\uff08UTS\uff09\u7ed3\u5408\u6807\u8bb0\u7ea7\u8bed\u4e49\u71b5\u548c\u9a6c\u6c0f\u8ddd\u79bb\uff0c\u5f53\u68c0\u6d4b\u5230\u8f68\u8ff9\u53d1\u6563\u65f6\uff0c\u4f7f\u7528\u5206\u7ea7\u8f6c\u5411\u5411\u91cf\u5c06\u6fc0\u6d3b\u91cd\u5b9a\u5411\u5230\u7ed3\u6784\u8fde\u8d2f\u533a\u57df\uff0c\u4ec5\u5f71\u54cd0.2-2.5%\u7684\u6807\u8bb0\u3002", "result": "\u57284\u4f4d\u91cf\u5316Granite 4.0 H Small\u6a21\u578b\u4e0a\uff1aGSM8K\u5728T=3.0\u65f6\u4fdd\u630188.84%\u51c6\u786e\u7387\uff08\u4ec5\u6bd4T=0.5\u4e0b\u964d2.81pp\uff09\uff1bMMLU\u572814,042\u4e2a\u95ee\u9898\u4e0a\u4fdd\u630172.49%\uff08\u4e0b\u964d1.24pp\uff09\u3002\u8de8\u67b6\u6784\u9a8c\u8bc1\u663e\u793a\u67b6\u6784\u72ec\u7acb\u6027\uff0c\u6982\u5ff5\u751f\u6210\u72ec\u7279\u5ea6\u63d0\u9ad846.7%\u3002", "conclusion": "\u9ad8\u6e29\u5ea6\u5e7b\u89c9\u4e3b\u8981\u662f\u8f68\u8ff9\u53d1\u6563\u800c\u975e\u8bed\u4e49\u5d29\u6e83\uff0cHELIX\u4f5c\u4e3a\u8bed\u6cd5\u951a\u5b9a\u5668\uff0c\u80fd\u591f\u63a2\u7d22\u8bed\u4e49\u591a\u6837\u6027\u800c\u4e0d\u8fdd\u53cd\u903b\u8f91\u9aa8\u67b6\uff0c\u5b9e\u73b0\u591a\u6e29\u5ea6\u5408\u6210\uff0c\u751f\u6210\u6bd4\u5355\u6e29\u5ea6\u63a8\u7406\u591a200%\u7684\u72ec\u7279\u6982\u5ff5\u3002"}}
{"id": "2602.18326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18326", "abs": "https://arxiv.org/abs/2602.18326", "authors": ["Tao Wu", "Adam Kapelner"], "title": "Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning", "comment": "8 pages, 3 figures, 4 tables", "summary": "We describe a modern deep learning system that automatically identifies informative contextual examples (\\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \\qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e3a\u9ad8\u4e2d\u751f\u7b2c\u4e00\u8bed\u8a00\u8bcd\u6c47\u6559\u5b66\u81ea\u52a8\u7b5b\u9009\u4f18\u8d28\u4e0a\u4e0b\u6587\u4f8b\u53e5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e09\u79cd\u5efa\u6a21\u65b9\u6cd5\uff0c\u53d1\u73b0\u7ed3\u5408\u4eba\u5de5\u7279\u5f81\u76d1\u7763\u7684Qwen3\u5d4c\u5165\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e3a\u9ad8\u4e2d\u751f\u8bcd\u6c47\u6559\u5b66\u81ea\u52a8\u5bfb\u627e\u9ad8\u8d28\u91cf\u7684\u4e0a\u4e0b\u6587\u4f8b\u53e5\uff0c\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u6765\u5927\u89c4\u6a21\u751f\u6210\u4f18\u8d28\u6559\u5b66\u6750\u6599\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a(1) \u57fa\u4e8eMPNet\u65e0\u76d1\u7763\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\uff1b(2) \u57fa\u4e8eQwen3\u5d4c\u5165\u5fae\u8c03\u7684\u6709\u76d1\u7763\u975e\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff1b(3) \u5728\u65b9\u6cd5(2)\u57fa\u7840\u4e0a\u52a0\u5165\u4eba\u5de5\u8bbe\u8ba1\u7279\u5f81\u3002\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\"\u4fdd\u6301\u80fd\u529b\u66f2\u7ebf\"\u6765\u53ef\u89c6\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6a21\u578b(3)\u8868\u73b0\u6700\u4f73\uff0c\u5728\u4ec5\u4e22\u5f0370%\u4f18\u8d28\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86440:1\u7684\"\u4f18\u8d28-\u52a3\u8d28\"\u4e0a\u4e0b\u6587\u6bd4\u4f8b\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u4eba\u7c7b\u76d1\u7763\u7684\u73b0\u4ee3\u5d4c\u5165\u6a21\u578b\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u80fd\u591f\u4f4e\u6210\u672c\u5730\u5927\u89c4\u6a21\u751f\u6210\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u8bcd\u6c47\u6559\u5b66\u4e0a\u4e0b\u6587\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u76ee\u6807\u8bcd\u6c47\u7684\u6559\u5b66\u3002"}}
{"id": "2602.18346", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18346", "abs": "https://arxiv.org/abs/2602.18346", "authors": ["Pavithra PM Nair", "Preethu Rose Anish"], "title": "Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System", "comment": null, "summary": "In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.", "AI": {"tldr": "Vichara\u662f\u4e00\u4e2a\u9488\u5bf9\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4e0a\u8bc9\u6848\u4ef6\u6587\u4ef6\u4e3a\u51b3\u7b56\u70b9\uff0c\u4f7f\u7528LLM\u9884\u6d4b\u548c\u89e3\u91ca\u4e0a\u8bc9\u5224\u51b3\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u5370\u5ea6\u6cd5\u9662\u9762\u4e34\u5927\u91cf\u6848\u4ef6\u79ef\u538b\uff0c\u4e0a\u8bc9\u6848\u4ef6\u662f\u5176\u4e2d\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u4eba\u5de5\u667a\u80fd\u5728\u9884\u6d4b\u6cd5\u5f8b\u5224\u51b3\u65b9\u9762\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u8bbe\u8ba1\u6846\u67b6\u3002", "method": "Vichara\u6846\u67b6\u5904\u7406\u82f1\u6587\u4e0a\u8bc9\u6848\u4ef6\u6587\u4ef6\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u51b3\u7b56\u70b9\uff08\u5305\u542b\u6cd5\u5f8b\u95ee\u9898\u3001\u51b3\u5b9a\u673a\u6784\u3001\u7ed3\u679c\u3001\u63a8\u7406\u548c\u65f6\u95f4\u80cc\u666f\u7684\u7ed3\u6784\u5316\u8868\u793a\uff09\u3002\u4f7f\u7528IRAC\u6846\u67b6\u7684\u53d8\u4f53\u751f\u6210\u89e3\u91ca\uff0c\u5e76\u5728\u56db\u4e2aLLM\uff08GPT-4o mini\u3001Llama-3.1-8B\u3001Mistral-7B\u3001Qwen2.5-7B\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Vichara\u5728PredEx\u548cILDC_expert\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u5224\u51b3\u9884\u6d4b\u57fa\u51c6\uff0cGPT-4o mini\u8868\u73b0\u6700\u4f73\uff08F1: 81.5 on PredEx, 80.3 on ILDC_expert\uff09\u3002\u4eba\u7c7b\u8bc4\u4f30\u663e\u793aGPT-4o mini\u5728\u6e05\u6670\u5ea6\u3001\u5173\u8054\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u5177\u6709\u6700\u4f73\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Vichara\u4e3a\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5224\u51b3\u9884\u6d4b\u548c\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u51b3\u7b56\u70b9\u8868\u793a\u548cIRAC\u98ce\u683c\u7684\u63a8\u7406\uff0c\u65e2\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53c8\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u6848\u4ef6\u79ef\u538b\u95ee\u9898\u3002"}}
{"id": "2602.17751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17751", "abs": "https://arxiv.org/abs/2602.17751", "authors": ["Nina Brolich", "Simon Geis", "Maximilian Kasper", "Alexander Barnhill", "Axel Plinge", "Dominik Seu\u00df"], "title": "Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring", "comment": "11 pages, 7 figures, Funding: GreenICT@FMD (BMFTR grant 16ME0491K)", "summary": "Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u5fae\u63a7\u5236\u5668\u5355\u5143(MCU)\u4e0a\u8fd0\u884c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9e1f\u7c7b\u76d1\u6d4b\uff0c\u901a\u8fc7\u6a21\u578b\u538b\u7f29\u5b9e\u73b0\u9ad8\u6548\u8fb9\u7f18\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u751f\u7269\u591a\u6837\u6027\u4e27\u5931\u5bf9\u4eba\u7c7b\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u9e1f\u7c7b\u76d1\u6d4b\u5bf9\u8bc4\u4f30\u751f\u6001\u7cfb\u7edf\u5065\u5eb7\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u9700\u8981\u590d\u6742\u6a21\u578b\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e0d\u9002\u5408\u91ce\u5916\u90e8\u7f72\u3002", "method": "\u5728MCU\u4e0a\u76f4\u63a5\u8fd0\u884c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u9488\u5bf9\u786c\u4ef6\u548c\u80fd\u6e90\u9650\u5236\u8bbe\u8ba1\u9ad8\u6548AI\u67b6\u6784\u3002\u8bad\u7ec3\u5e76\u538b\u7f29\u4e0d\u540c\u76ee\u6807\u7c7b\u522b\u6570\u91cf\u7684\u6a21\u578b\uff0c\u8bc4\u4f30\u591a\u7269\u79cd\u68c0\u6d4b\u6548\u679c\uff0c\u7814\u7a76\u7269\u79cd\u6570\u91cf\u5bf9\u795e\u7ecf\u7f51\u7edc\u53ef\u538b\u7f29\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u538b\u7f29\u7387\u4e14\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002\u63d0\u4f9b\u4e86\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e76\u8bc4\u4f30\u4e86\u90e8\u7f72\u80fd\u6e90\u81ea\u4e3b\u8bbe\u5907\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fdb\u884c\u9e1f\u7c7b\u76d1\u6d4b\uff0c\u4e3a\u91ce\u5916\u90e8\u7f72\u80fd\u6e90\u81ea\u4e3b\u7684\u76d1\u6d4b\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.18351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18351", "abs": "https://arxiv.org/abs/2602.18351", "authors": ["Jordan Robinson", "Angus R. Williams", "Katie Atkinson", "Anthony G. Cohn"], "title": "Validating Political Position Predictions of Arguments", "comment": "13 pages, 6 figures, 6 tables. Under review", "summary": "Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \\textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $\u03b1=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($\u03b1=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5c3a\u5ea6\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u70b9\u5bf9\u70b9\u548c\u6210\u5bf9\u4eba\u5de5\u6807\u6ce8\uff0c\u7528\u4e8e\u653f\u6cbb\u7acb\u573a\u9884\u6d4b\u7684\u4e3b\u89c2\u8fde\u7eed\u77e5\u8bc6\u8868\u793a\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u653f\u6cbb\u7acb\u573a\u77e5\u8bc6\u5e93\uff0c\u8bc1\u660e\u53ef\u4ee5\u4ece\u70b9\u5bf9\u70b9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u4e2d\u63d0\u53d6\u5e8f\u6570\u7ed3\u6784\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u8868\u793a\u5e38\u9700\u6355\u6349\u4e3b\u89c2\u8fde\u7eed\u5c5e\u6027\uff08\u5982\u653f\u6cbb\u7acb\u573a\uff09\uff0c\u8fd9\u4e0e\u5e7f\u6cdb\u63a5\u53d7\u7684\u6210\u5bf9\u9a8c\u8bc1\u9ec4\u91d1\u6807\u51c6\u76f8\u51b2\u7a81\u3002\u9700\u8981\u89e3\u51b3\u4e3b\u89c2\u8fde\u7eed\u77e5\u8bc6\u9a8c\u8bc1\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u5c3a\u5ea6\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u70b9\u5bf9\u70b9\u548c\u6210\u5bf9\u4eba\u5de5\u6807\u6ce8\u3002\u4f7f\u752822\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u5305\u542b23,228\u4e2a\u8bba\u70b9\u7684\u5927\u89c4\u6a21\u653f\u6cbb\u7acb\u573a\u9884\u6d4b\u77e5\u8bc6\u5e93\uff0c\u6570\u636e\u6765\u81ea30\u573a\u82f1\u56fd\u653f\u6cbb\u7535\u89c6\u8282\u76ee\u300aQuestion Time\u300b\u7684\u8fa9\u8bba\u3002", "result": "\u70b9\u5bf9\u70b9\u8bc4\u4f30\u663e\u793a\u4e2d\u7b49\u7684\u4eba\u673a\u4e00\u81f4\u6027\uff08Krippendorff's \u03b1=0.578\uff09\uff0c\u53cd\u6620\u5185\u5728\u4e3b\u89c2\u6027\uff1b\u800c\u6210\u5bf9\u9a8c\u8bc1\u663e\u793a\u4eba\u673a\u6392\u540d\u95f4\u663e\u8457\u66f4\u5f3a\u7684\u5bf9\u9f50\uff08\u6700\u4f73\u6a21\u578b\u03b1=0.86\uff09\u3002", "conclusion": "\u63d0\u51fa\u5b9e\u7528\u9a8c\u8bc1\u65b9\u6cd5\u5e73\u8861\u53ef\u6269\u5c55\u6027\u4e0e\u53ef\u9760\u6027\uff1b\u6784\u5efa\u9a8c\u8bc1\u7684\u7ed3\u6784\u5316\u8bba\u8bc1\u77e5\u8bc6\u5e93\u652f\u6301\u56fe\u57fa\u63a8\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff1b\u8bc1\u660e\u53ef\u4ee5\u4ece\u70b9\u5bf9\u70b9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u4e2d\u63d0\u53d6\u5e8f\u6570\u7ed3\u6784\uff0c\u63a8\u8fdb\u4f20\u7edf\u7b26\u53f7\u6216\u5206\u7c7b\u65b9\u6cd5\u4e0d\u8db3\u9886\u57df\u7684\u77e5\u8bc6\u8868\u793a\u80fd\u529b\u3002"}}
{"id": "2602.18420", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18420", "abs": "https://arxiv.org/abs/2602.18420", "authors": ["Jiamin Yao", "Eren Gultepe"], "title": "SPQ: An Ensemble Technique for Large Language Model Compression", "comment": "Accepted to LREC 2026 Main Conference", "summary": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/", "AI": {"tldr": "SPQ\u662f\u4e00\u79cd\u96c6\u6210\u538b\u7f29\u6280\u672f\uff0c\u7ed3\u5408SVD\u3001\u526a\u679d\u548c\u91cf\u5316\u4e09\u79cd\u65b9\u6cd5\uff0c\u9488\u5bf9LLM\u7684\u4e0d\u540c\u4f4e\u6548\u6e90\u8fdb\u884c\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u9762\u4e34\u5185\u5b58\u5360\u7528\u5927\u3001\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u6280\u672f\u6765\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u4fbf\u4e8e\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "SPQ\u96c6\u6210\u4e09\u79cd\u4e92\u8865\u6280\u672f\uff1a1) \u57fa\u4e8e\u6fc0\u6d3b\u7684\u526a\u679d\u53bb\u9664MLP\u5c42\u4e2d\u7684\u5197\u4f59\u795e\u7ecf\u5143\uff1b2) \u4fdd\u7559\u65b9\u5dee\u7684SVD\u5c06\u6ce8\u610f\u529b\u6295\u5f71\u5206\u89e3\u4e3a\u7d27\u51d1\u7684\u4f4e\u79e9\u56e0\u5b50\uff1b3) 8\u4f4d\u7ebf\u6027\u91cf\u5316\u5747\u5300\u538b\u7f29\u6240\u6709\u7ebf\u6027\u5c42\u3002", "result": "\u5728LLaMA-2-7B\u4e0a\uff0cSPQ\u5b9e\u73b0\u9ad8\u8fbe75%\u7684\u5185\u5b58\u51cf\u5c11\uff0c\u540c\u65f6\u56f0\u60d1\u5ea6\u6539\u5584\uff08\u5982WikiText-2\u4ece5.47\u964d\u81f34.91\uff09\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4fdd\u6301\u51c6\u786e\u6027\u3002\u76f8\u6bd4GPTQ\u548cSparseGPT\uff0cSPQ\u4f7f\u7528\u66f4\u5c11\u5185\u5b58\uff086.86GB vs 7.16GB\uff09\u4e14\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u8fbe1.9\u500d\u3002", "conclusion": "SPQ\u901a\u8fc7\u5c42\u611f\u77e5\u548c\u4e92\u8865\u7684\u538b\u7f29\u6280\u672f\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u538b\u7f29\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u96c6\u6210\u591a\u79cd\u538b\u7f29\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e2d\u7684LLM\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18425", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18425", "abs": "https://arxiv.org/abs/2602.18425", "authors": ["Deniz Qian", "Hung-Ting Chen", "Eunsol Choi"], "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering", "comment": "18 pages, 12 figures, 12 tables", "summary": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.", "AI": {"tldr": "RVR\u662f\u4e00\u4e2a\u591a\u8f6e\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22-\u9a8c\u8bc1-\u68c0\u7d22\u7684\u8fed\u4ee3\u8fc7\u7a0b\u6700\u5927\u5316\u7b54\u6848\u8986\u76d6\u7387\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u5b8c\u6574\u53ec\u56de\u7387\u3002", "motivation": "\u9488\u5bf9\u9700\u8981\u5e7f\u6cdb\u6709\u6548\u7b54\u6848\u7684\u67e5\u8be2\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u8986\u76d6\u6240\u6709\u53ef\u80fd\u7684\u7b54\u6848\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6700\u5927\u5316\u7b54\u6848\u8986\u76d6\u7387\u7684\u68c0\u7d22\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u68c0\u7d22-\u9a8c\u8bc1-\u68c0\u7d22\uff08RVR\uff09\u591a\u8f6e\u6846\u67b6\uff1a\u7b2c\u4e00\u8f6e\u68c0\u7d22\u5668\u5904\u7406\u539f\u59cb\u67e5\u8be2\u8fd4\u56de\u5019\u9009\u6587\u6863\uff0c\u9a8c\u8bc1\u5668\u8bc6\u522b\u9ad8\u8d28\u91cf\u5b50\u96c6\uff1b\u540e\u7eed\u8f6e\u6b21\u5c06\u67e5\u8be2\u4e0e\u5df2\u9a8c\u8bc1\u6587\u6863\u7ed3\u5408\uff0c\u53d1\u73b0\u4e4b\u524d\u672a\u8986\u76d6\u7684\u7b54\u6848\u3002", "result": "\u5728QAMPARI\u6570\u636e\u96c6\u4e0a\u76f8\u5bf9\u63d0\u5347\u81f3\u5c1110%\uff0c\u7edd\u5bf9\u63d0\u53473%\u7684\u5b8c\u6574\u53ec\u56de\u7387\uff1b\u5728QUEST\u548cWebQuestionsSP\u4e24\u4e2a\u57df\u5916\u6570\u636e\u96c6\u4e0a\u4e5f\u6709\u7a33\u5b9a\u63d0\u5347\uff0c\u4f18\u4e8e\u5305\u62ec\u667a\u80fd\u641c\u7d22\u65b9\u6cd5\u5728\u5185\u7684\u57fa\u7ebf\u3002", "conclusion": "RVR\u5c55\u793a\u4e86\u5229\u7528\u9a8c\u8bc1\u5668\u548c\u9002\u5e94\u65b0\u63a8\u7406\u573a\u666f\u7684\u8fed\u4ee3\u68c0\u7d22\u65b9\u6cd5\u5728\u5168\u9762\u7b54\u6848\u53ec\u56de\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5373\u4f7f\u4f7f\u7528\u73b0\u6210\u68c0\u7d22\u5668\u4e5f\u6709\u6548\uff0c\u5fae\u8c03\u540e\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2602.17865", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17865", "abs": "https://arxiv.org/abs/2602.17865", "authors": ["Andrzej Podobi\u0144ski", "Jaros\u0142aw A. Chudziak"], "title": "Financial time series augmentation using transformer based GAN architecture", "comment": "This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings", "summary": "Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684GAN\uff08TTS-GAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LSTM\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u6bd4\u7279\u5e01\u548c\u6807\u666e500\u4ef7\u683c\u6570\u636e\u4e0a\u3002", "motivation": "\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u901a\u5e38\u7a00\u7f3a\u4e14\u6ce2\u52a8\u6027\u5927\uff0c\u5bfc\u81f4\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u4e0d\u5145\u5206\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9700\u8981\u53ef\u9760\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8eTransformer\u7684GAN\uff08TTS-GAN\uff09\u751f\u6210\u5408\u6210\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u5de5\u5177\uff0c\u5e76\u8bad\u7ec3LSTM\u9884\u6d4b\u6a21\u578b\u3002\u540c\u65f6\u63d0\u51fa\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u548c\u6539\u8fdb\u7684\u6df1\u5ea6\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u5ea6\u91cf\uff08DeD-iMs\uff09\u7684\u65f6\u95f4\u5e8f\u5217\u7279\u5b9a\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u6bd4\u7279\u5e01\u548c\u6807\u666e500\u4ef7\u683c\u6570\u636e\u4e0a\uff0c\u4f7f\u7528TTS-GAN\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684LSTM\u6a21\u578b\u6bd4\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u7684\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u6027\u663e\u8457\u63d0\u9ad8\uff0c\u5728\u4e0d\u540c\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u90fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "GAN-based\u6570\u636e\u589e\u5f3a\u80fd\u6709\u6548\u514b\u670d\u91d1\u878d\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u9ad8\u9884\u6d4b\u80fd\u529b\u3002\u63d0\u51fa\u7684\u65f6\u95f4\u5e8f\u5217\u7279\u5b9a\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u80fd\u53ef\u9760\u76d1\u63a7\u8bad\u7ec3\u8fc7\u7a0b\u548c\u8bc4\u4f30\u751f\u6210\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2602.18429", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18429", "abs": "https://arxiv.org/abs/2602.18429", "authors": ["Harshul Raj Surana", "Arijit Maji", "Aryan Vats", "Akash Ghosh", "Sriparna Saha", "Amit Sheth"], "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.", "AI": {"tldr": "\u63d0\u51fa\u4e86VIRAASAT\u6570\u636e\u96c6\u548cSCoM\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u5728\u5370\u5ea6\u6587\u5316\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0", "motivation": "\u73b0\u6709LLMs\u5728\u9700\u8981\u4e30\u5bcc\u793e\u4f1a\u6587\u5316\u77e5\u8bc6\u548c\u672c\u5730\u80cc\u666f\u7684\u4efb\u52a1\uff08\u7279\u522b\u662f\u5370\u5ea6\u6587\u5316\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u6587\u5316\u57fa\u51c6\u5b58\u5728\u624b\u52a8\u6784\u5efa\u3001\u5355\u8df3\u95ee\u9898\u3001\u96be\u4ee5\u6269\u5c55\u7b49\u95ee\u9898", "method": "1) \u521b\u5efaVIRAASAT\u6570\u636e\u96c6\uff1a\u57fa\u4e8e\u5305\u542b700\u591a\u4e2a\u4e13\u5bb6\u7b56\u5212\u6587\u5316\u5de5\u4ef6\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u8986\u76d6\u5370\u5ea613\u4e2a\u6587\u5316\u5c5e\u6027\u548c\u6240\u6709\u884c\u653f\u533a\u5212\uff0c\u751f\u62103200\u591a\u4e2a\u591a\u8df3\u95ee\u9898\uff1b2) \u63d0\u51faSCoM\u6846\u67b6\uff1a\u8bad\u7ec3\u6a21\u578b\u5185\u90e8\u6a21\u62df\u77e5\u8bc6\u56fe\u8c31\u7684\u539f\u5b50\u64cd\u4f5c\uff0c\u53ef\u9760\u904d\u5386\u56fe\u8c31\u62d3\u6251\u7ed3\u6784", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524dSOTA LLMs\u5728VIRAASAT\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5fae\u8c03CoT\u8ddf\u8e2a\u65e0\u6cd5\u5904\u7406\u4f4e\u6982\u7387\u4e8b\u5b9e\u7684\u63a5\u5730\u548c\u5408\u6210\u3002SCoM\u5728\u76d1\u7763\u5fae\u8c03\u5b9e\u9a8c\u4e2d\u6bd4\u6807\u51c6CoT\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe20%", "conclusion": "VIRAASAT\u6570\u636e\u96c6\u548cSCoM\u6846\u67b6\u4e3a\u6784\u5efa\u6587\u5316\u611f\u77e5\u63a8\u7406\u6a21\u578b\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u6587\u5316\u7279\u5b9a\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027"}}
{"id": "2602.17868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17868", "abs": "https://arxiv.org/abs/2602.17868", "authors": ["Vasilii Feofanov", "Songkang Wen", "Jianfeng Zhang", "Lujia Pan", "Ievgen Redko"], "title": "MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies", "comment": null, "summary": "Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.", "AI": {"tldr": "MantisV2\u548cMantis+\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u3001\u67b6\u6784\u4f18\u5316\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u7279\u5f81\u63d0\u53d6\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u3002", "motivation": "\u5f00\u53d1\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u57fa\u7840\u6a21\u578b\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002\u65e9\u671f\u6a21\u578b\u5982Mantis\u867d\u663e\u793a\u6f5c\u529b\uff0c\u4f46\u51bb\u7ed3\u7f16\u7801\u5668\u4e0e\u5fae\u8c03\u7f16\u7801\u5668\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "method": "1. Mantis+\uff1a\u5b8c\u5168\u5728\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u4e0a\u9884\u8bad\u7ec3\u7684Mantis\u53d8\u4f53\uff1b2. MantisV2\uff1a\u901a\u8fc7\u53d7\u63a7\u6d88\u878d\u7814\u7a76\u6539\u8fdb\u7684\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\uff1b3. \u589e\u5f3a\u6d4b\u8bd5\u65f6\u65b9\u6cd5\uff1a\u5229\u7528\u4e2d\u95f4\u5c42\u8868\u793a\u548c\u6539\u8fdb\u8f93\u51fa\u6807\u8bb0\u805a\u5408\uff1b4. \u81ea\u96c6\u6210\u548c\u8de8\u6a21\u578b\u5d4c\u5165\u878d\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728UCR\u3001UEA\u3001\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u57fa\u51c6\u548cEEG\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMantisV2\u548cMantis+\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u3001\u67b6\u6784\u4f18\u5316\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u4e3a\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u63d0\u53d6\u5668\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.17697", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17697", "abs": "https://arxiv.org/abs/2602.17697", "authors": ["Nada Zine", "Cl\u00e9ment Quinton", "Romain Rouvoy"], "title": "Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u53ef\u914d\u7f6e\u7cfb\u7edf\uff0c\u5e94\u7528\u53d8\u5f02\u6027\u7ba1\u7406\u6280\u672f\u6765\u7cfb\u7edf\u5206\u6790\u63a8\u7406\u914d\u7f6e\u9009\u62e9\uff0c\u4ee5\u4f18\u5316\u80fd\u8017\u548c\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9636\u6bb5\u7684\u8ba1\u7b97\u9700\u6c42\u5de8\u5927\uff0c\u80fd\u8017\u95ee\u9898\u7a81\u51fa\uff0c\u4f46\u914d\u7f6e\u7a7a\u95f4\u5e9e\u5927\u5bfc\u81f4\u7ecf\u9a8c\u8bc4\u4f30\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u914d\u7f6e\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u5c06LLMs\u89c6\u4e3a\u53ef\u914d\u7f6e\u7cfb\u7edf\uff0c\u4f7f\u7528\u57fa\u4e8e\u7279\u5f81\u7684\u53d8\u5f02\u6027\u6a21\u578b\u8868\u793a\u751f\u6210\u8d85\u53c2\u6570\u53ca\u5176\u7ea6\u675f\uff0c\u91c7\u6837\u4ee3\u8868\u6027\u914d\u7f6e\uff0c\u6d4b\u91cf\u80fd\u8017\u3001\u5ef6\u8fdf\u3001\u51c6\u786e\u6027\uff0c\u5e76\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u53d8\u5f02\u6027\u5efa\u6a21\u80fd\u6709\u6548\u7ba1\u7406LLM\u63a8\u7406\u914d\u7f6e\u590d\u6742\u6027\uff0c\u7cfb\u7edf\u5206\u6790\u8d85\u53c2\u6570\u6548\u5e94\u548c\u4ea4\u4e92\u4f5c\u7528\uff0c\u63ed\u793a\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u80fd\u4ece\u6709\u9650\u6d4b\u91cf\u4e2d\u51c6\u786e\u9884\u6d4b\u63a8\u7406\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u53d8\u5f02\u6027\u5efa\u6a21\u4e3aLLM\u9ad8\u6548\u53ef\u6301\u7eed\u914d\u7f6e\u5f00\u8f9f\u4e86\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u6865\u63a5\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u3002"}}
{"id": "2602.17888", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17888", "abs": "https://arxiv.org/abs/2602.17888", "authors": ["Sayeed Shafayet Chowdhury", "Karen D'Souza", "V. Siva Kakumani", "Snehasis Mukhopadhyay", "Shiaofen Fang", "Rodney J. Schlosser", "Daniel M. Beswick", "Jeremiah A. Alt", "Jess C. Mace", "Zachary M. Soler", "Timothy L. Smith", "Vijay R. Ramakrishnan"], "title": "Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data", "comment": null, "summary": "Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u672f\u524d\u6570\u636e\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u60a3\u8005\u7684\u624b\u672f\u83b7\u76ca\uff0c\u51c6\u786e\u7387\u8fbe85%\uff0c\u572830\u4f8b\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e34\u5e8a\u4e13\u5bb6\uff0880% vs 75.6%\uff09\uff0c\u5c55\u793a\u4e86AI\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\u7684\u6f5c\u529b\u3002", "motivation": "\u6162\u6027\u9f3b\u7aa6\u708e\uff08CRS\uff09\u624b\u672f\u51b3\u7b56\u590d\u6742\uff0c\u9700\u6743\u8861\u624b\u672f\u98ce\u9669\u4e0e\u4e0d\u786e\u5b9a\u7684\u4e2a\u4f53\u5316\u7ed3\u679c\u3002\u867d\u7136AI\u5728\u533b\u7597\u9884\u540e\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u57fa\u4e8e\u524d\u77bb\u6027\u89c2\u5bdf\u6027\u4e34\u5e8a\u8bd5\u9a8c\u6807\u51c6\u5316\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7814\u7a76\u4ecd\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86AI\u5728\u964d\u4f4e\u533b\u7597\u6210\u672c\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u524d\u77bb\u6027\u6536\u96c6\u7684\u89c2\u5bdf\u6027\u5e72\u9884\u8bd5\u9a8c\u961f\u5217\u6570\u636e\uff0c\u6240\u6709\u60a3\u8005\u5747\u63a5\u53d7\u4e86\u624b\u672f\u3002\u7814\u7a76\u8bad\u7ec3\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u4ec5\u4f7f\u7528\u672f\u524d\u6570\u636e\u9884\u6d4b\u624b\u672f\u83b7\u76ca\uff0c\u4ee5Sino-Nasal Outcome Test-22\uff08SNOT-22\uff09\u4f5c\u4e3a\u4e3b\u8981\u60a3\u8005\u62a5\u544a\u7ed3\u5c40\u6307\u6807\u3002\u91c7\u7528\u591a\u79cd\u7b97\u6cd5\u5305\u62ec\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u572830\u4f8b\u6df7\u5408\u96be\u5ea6\u75c5\u4f8b\u7684\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6700\u4f73\u6a21\u578b\u8fbe\u5230\u7ea685%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u80fd\u591f\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u5730\u9884\u6d4b\u624b\u672f\u5019\u9009\u8d44\u683c\u3002\u572830\u4f8b\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe80%\uff0c\u8d85\u8fc7\u4e86\u4e34\u5e8a\u4e13\u5bb6\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u7387\uff0875.6%\uff09\uff0c\u8868\u660e\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u90a3\u4e9b\u672f\u524d\u53ef\u80fd\u4e0d\u5efa\u8bae\u624b\u672f\u7684\u60a3\u8005\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u60a3\u8005\u7684\u624b\u672f\u83b7\u76ca\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u4e34\u5e8a\u4e13\u5bb6\uff0c\u5c55\u793a\u4e86AI\u5728\u589e\u5f3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u548c\u5b9e\u73b0\u4e2a\u6027\u5316CRS\u6cbb\u7597\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u51cf\u5c11\u4e0d\u5fc5\u8981\u624b\u672f\u548c\u4f18\u5316\u60a3\u8005\u7ba1\u7406\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2602.17706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17706", "abs": "https://arxiv.org/abs/2602.17706", "authors": ["Rongyao Cai", "Yuxi Wan", "Kexin Zhang", "Ming Jin", "Zhiqiang Ge", "Qingsong Wen", "Yong Liu"], "title": "Parallel Complex Diffusion for Scalable Time Series Generation", "comment": null, "summary": "Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \\textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.", "AI": {"tldr": "PaCoDi\u662f\u4e00\u79cd\u8c31\u539f\u751f\u67b6\u6784\uff0c\u901a\u8fc7\u9891\u57df\u89e3\u8026\u751f\u6210\u5efa\u6a21\uff0c\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4e2d\u957f\u7a0b\u4f9d\u8d56\u7684\u8868\u793a\u80fd\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b050%\u6ce8\u610f\u529bFLOPs\u51cf\u5c11\u4e14\u4e0d\u635f\u5931\u4fe1\u606f\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u6269\u6563\u6a21\u578b\u5b58\u5728\u5c40\u90e8\u7ea0\u7f20\u95ee\u9898\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684O(L\u00b2)\u8ba1\u7b97\u6210\u672c\uff0c\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u5728\u8868\u793a\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002", "method": "\u63d0\u51faPaCoDi\uff08\u5e76\u884c\u590d\u6269\u6563\uff09\u67b6\u6784\uff0c\u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u4f5c\u4e3a\u5bf9\u89d2\u5316\u7b97\u5b50\u5c06\u65f6\u95f4\u4fe1\u53f7\u8f6c\u6362\u4e3a\u89e3\u8026\u7684\u8c31\u5206\u91cf\uff1b\u8bc1\u660e\u6b63\u4ea4\u524d\u5411\u6269\u6563\u548c\u6761\u4ef6\u53cd\u5411\u5206\u89e3\u5b9a\u7406\uff1b\u91c7\u7528\u5e73\u5747\u573a\u7406\u8bba\u8fd1\u4f3c\u548c\u4ea4\u4e92\u6821\u6b63\u673a\u5236\uff1b\u63a8\u5e7f\u5230\u8fde\u7eed\u65f6\u95f4\u9891\u7387SDE\uff1b\u5229\u7528\u5b9e\u503c\u4fe1\u53f7\u7684\u5384\u7c73\u5bf9\u79f0\u6027\u538b\u7f29\u5e8f\u5217\u957f\u5ea6\uff1b\u63a8\u5bfc\u5f02\u65b9\u5dee\u635f\u5931\u5904\u7406\u538b\u7f29\u6d41\u5f62\u4e0a\u7684\u975e\u5404\u5411\u540c\u6027\u566a\u58f0\u5206\u5e03\u3002", "result": "PaCoDi\u5728\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e8650%\u7684\u6ce8\u610f\u529bFLOPs\u51cf\u5c11\u800c\u4e0d\u635f\u5931\u4fe1\u606f\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u4e25\u8c28\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "PaCoDi\u901a\u8fc7\u9891\u57df\u89e3\u8026\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u95ee\u9898\u62d3\u6251\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4e2d\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5728\u7406\u8bba\u4e25\u8c28\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2602.17930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17930", "abs": "https://arxiv.org/abs/2602.17930", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance", "comment": "International Conference on Learning Representations (ICLR'26)", "summary": "Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/", "AI": {"tldr": "MIRA\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u56fe\u6574\u5408LLM\u5148\u9a8c\u77e5\u8bc6\uff0c\u51cf\u5c11\u5bf9\u5b9e\u65f6LLM\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u52a0\u901fRL\u5b66\u4e60", "motivation": "\u4f20\u7edfRL\u5728\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u73af\u5883\u4e2d\u6837\u672c\u6548\u7387\u4f4e\uff0c\u800c\u8fc7\u5ea6\u4f9d\u8d56LLM\u76d1\u7763\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\u548c\u4fe1\u53f7\u53ef\u9760\u6027\u95ee\u9898", "method": "\u6784\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u56fe\u5b58\u50a8\u9ad8\u56de\u62a5\u7ecf\u9a8c\u548cLLM\u8f93\u51fa\uff0c\u4ece\u4e2d\u63a8\u5bfc\u6548\u7528\u4fe1\u53f7\u8f6f\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\uff0c\u968f\u7740\u8bad\u7ec3\u8fdb\u5c55\u6548\u7528\u9879\u8870\u51cf", "result": "MIRA\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u4f18\u4e8eRL\u57fa\u7ebf\uff0c\u8fbe\u5230\u4e0e\u9891\u7e41LLM\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u56de\u62a5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5728\u7ebfLLM\u67e5\u8be2", "conclusion": "MIRA\u901a\u8fc7\u8bb0\u5fc6\u56fe\u6574\u5408LLM\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u51cf\u5c11LLM\u4f9d\u8d56\u7684\u540c\u65f6\u52a0\u901f\u65e9\u671f\u5b66\u4e60\uff0c\u4fdd\u6301\u6807\u51c6\u6536\u655b\u4fdd\u8bc1"}}
{"id": "2602.17931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17931", "abs": "https://arxiv.org/abs/2602.17931", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning", "comment": "Association for the Advancement of Artificial Intelligence (AAAI)", "summary": "In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u548c\u8bb0\u5fc6\u56fe\u7684\u65b9\u6cd5\u6765\u6539\u5584\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\uff0c\u901a\u8fc7\u6784\u5efa\u8bb0\u5fc6\u56fe\u7f16\u7801\u5b50\u76ee\u6807\u548c\u8f68\u8ff9\uff0c\u5e76\u4ece\u4e2d\u63a8\u5bfc\u6548\u7528\u51fd\u6570\u6765\u6307\u5bfc\u4f18\u52bf\u51fd\u6570\uff0c\u51cf\u5c11\u5bf9\u9891\u7e41LLM\u8c03\u7528\u7684\u4f9d\u8d56\u3002", "motivation": "\u5728\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4ea4\u4e92\u5bfc\u81f4\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u7528\u4e8e\u5b50\u76ee\u6807\u53d1\u73b0\u548c\u8f68\u8ff9\u6307\u5bfc\uff0c\u4f46\u9891\u7e41\u4f9d\u8d56LLM\u8c03\u7528\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efa\u8bb0\u5fc6\u56fe\u7f16\u7801\u6765\u81eaLLM\u6307\u5bfc\u548c\u667a\u80fd\u4f53\u81ea\u8eab\u6210\u529f\u8f68\u8ff9\u7684\u5b50\u76ee\u6807\u548c\u8f68\u8ff9\uff0c\u4ece\u4e2d\u63a8\u5bfc\u6548\u7528\u51fd\u6570\u8bc4\u4f30\u667a\u80fd\u4f53\u8f68\u8ff9\u4e0e\u5148\u524d\u6210\u529f\u7b56\u7565\u7684\u5339\u914d\u7a0b\u5ea6\uff0c\u8be5\u6548\u7528\u51fd\u6570\u7528\u4e8e\u5851\u9020\u4f18\u52bf\u51fd\u6570\uff0c\u4e3acritic\u63d0\u4f9b\u989d\u5916\u6307\u5bfc\u800c\u4e0d\u6539\u53d8\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u57fa\u51c6\u73af\u5883\u4e2d\u7684\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebfRL\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u52a0\u901f\u4e86\u65e9\u671f\u5b66\u4e60\uff0c\u6700\u7ec8\u56de\u62a5\u4e0e\u9700\u8981\u9891\u7e41LLM\u4ea4\u4e92\u7684\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u8bb0\u5fc6\u56fe\u548c\u63a8\u5bfc\u6548\u7528\u51fd\u6570\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5f3a\u5316\u5b66\u4e60\u5bf9\u9891\u7e41LLM\u8c03\u7528\u7684\u4f9d\u8d56\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u65e9\u671f\u5b66\u4e60\u901f\u5ea6\uff0c\u4e3b\u8981\u4f9d\u8d56\u79bb\u7ebf\u8f93\u5165\u548c\u5076\u5c14\u7684\u5728\u7ebf\u67e5\u8be2\uff0c\u907f\u514d\u4e86\u6301\u7eedLLM\u76d1\u7763\u7684\u9700\u6c42\u3002"}}
{"id": "2602.17934", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17934", "abs": "https://arxiv.org/abs/2602.17934", "authors": ["Simi Job", "Xiaohui Tao", "Taotao Cai", "Haoran Xie", "Jianming Yong"], "title": "Causal Neighbourhood Learning for Invariant Graph Representations", "comment": null, "summary": "Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.", "AI": {"tldr": "CNL-GNN\uff1a\u4e00\u79cd\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u56fe\u7ed3\u6784\u5b66\u4e60\u6765\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027", "motivation": "\u56fe\u6570\u636e\u4e2d\u5e38\u5b58\u5728\u566a\u58f0\u548c\u865a\u5047\u76f8\u5173\u6027\uff0c\u63a9\u76d6\u4e86\u771f\u5b9e\u7684\u56e0\u679c\u5173\u7cfb\u3002\u4f20\u7edfGNN\u4f9d\u8d56\u865a\u5047\u8fde\u63a5\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u56fe\u95f4\u6709\u6548\u6cdb\u5316\uff0c\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u4f1a\u653e\u5927\u8fd9\u4e9b\u865a\u5047\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faCNL-GNN\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u56fe\u7ed3\u6784\u6765\u8bc6\u522b\u548c\u4fdd\u7559\u56e0\u679c\u76f8\u5173\u8fde\u63a5\uff0c\u51cf\u5c11\u865a\u5047\u5f71\u54cd\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u751f\u6210\u53cd\u4e8b\u5b9e\u90bb\u57df\u3001\u57fa\u4e8e\u53ef\u5b66\u4e60\u91cd\u8981\u6027\u63a9\u7801\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u81ea\u9002\u5e94\u8fb9\u6270\u52a8\uff0c\u4ee5\u53ca\u7ed3\u5408\u7ed3\u6784\u7ea7\u5e72\u9884\u4e0e\u56e0\u679c\u7279\u5f81\u4ece\u6df7\u6742\u56e0\u7d20\u4e2d\u89e3\u8026\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08\u5305\u62ec\u4e00\u4e2a\u6570\u636e\u96c6\u7684\u591a\u4e2a\u9886\u57df\u53d8\u4f53\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCNL-GNN\u4f18\u4e8e\u6700\u5148\u8fdb\u7684GNN\u6a21\u578b\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u9c81\u68d2\u4e14\u5728\u4e0d\u540c\u56fe\u7ed3\u6784\u95f4\u6cdb\u5316\u826f\u597d\u7684\u4e0d\u53d8\u8282\u70b9\u8868\u793a\u3002", "conclusion": "CNL-GNN\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u56fe\u7ed3\u6784\u5b66\u4e60\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u56e0\u679c\u56fe\u5b66\u4e60\u80fd\u529b\uff0c\u6784\u5efa\u4e86\u9c81\u68d2\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u4e2d\u865a\u5047\u76f8\u5173\u6027\u5bfc\u81f4\u7684\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2602.17867", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17867", "abs": "https://arxiv.org/abs/2602.17867", "authors": ["Jo\u00e3o N. Cardoso", "Arlindo L. Oliveira", "Bruno Martins"], "title": "ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization", "comment": null, "summary": "Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.", "AI": {"tldr": "ADAPT\u662f\u4e00\u79cd\u7ed3\u5408\u675f\u641c\u7d22\u521d\u59cb\u5316\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u5f15\u5bfc\u7a81\u53d8\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316LLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b66\u4e60\u65b9\u5411\u7684\u79bb\u6563\u6587\u672c\u7279\u5f81\u53ef\u89c6\u5316\uff0c\u5728Gemma 2 2B\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3LLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b66\u4e60\u65b9\u5411\u7f16\u7801\u7684\u7279\u5f81\u9700\u8981\u8bc6\u522b\u5f3a\u70c8\u6fc0\u6d3b\u8fd9\u4e9b\u65b9\u5411\u7684\u8f93\u5165\u3002\u7279\u5f81\u53ef\u89c6\u5316\u901a\u8fc7\u4f18\u5316\u8f93\u5165\u6765\u6700\u5927\u5316\u6fc0\u6d3b\u76ee\u6807\u65b9\u5411\uff0c\u4e3a\u6602\u8d35\u7684\u6570\u636e\u5e93\u641c\u7d22\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u6587\u672c\u7684\u79bb\u6563\u6027\uff0c\u5728LLM\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u7684\u63d0\u793a\u4f18\u5316\u6280\u672f\u4e0d\u9002\u5408\u8fd9\u4e2a\u9886\u57df\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "method": "ADAPT\u662f\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u675f\u641c\u7d22\u521d\u59cb\u5316\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u5f15\u5bfc\u7a81\u53d8\uff0c\u4e13\u95e8\u9488\u5bf9\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u5728Gemma 2 2B\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6570\u636e\u96c6\u6fc0\u6d3b\u7edf\u8ba1\u7684\u5ea6\u91cf\u6807\u51c6\u4ee5\u5b9e\u73b0\u4e25\u683c\u6bd4\u8f83\u3002", "result": "ADAPT\u5728\u4e0d\u540c\u5c42\u548c\u6f5c\u5728\u7c7b\u578b\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660eLLM\u7684\u7279\u5f81\u53ef\u89c6\u5316\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u8be5\u9886\u57df\u91cf\u8eab\u5b9a\u5236\u7684\u8bbe\u8ba1\u5047\u8bbe\u3002", "conclusion": "LLM\u7684\u7279\u5f81\u53ef\u89c6\u5316\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u79bb\u6563\u6587\u672c\u4f18\u5316\u9886\u57df\u8bbe\u8ba1\u7684\u7b97\u6cd5\u3002ADAPT\u901a\u8fc7\u7ed3\u5408\u675f\u641c\u7d22\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u5f15\u5bfc\u7a81\u53d8\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u95ee\u9898\uff0c\u4e3a\u7406\u89e3LLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u5b66\u4e60\u65b9\u5411\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2602.17941", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17941", "abs": "https://arxiv.org/abs/2602.17941", "authors": ["Simi Job", "Xiaohui Tao", "Taotao Cai", "Haoran Xie", "Jianming Yong", "Xin Wang"], "title": "Optimizing Graph Causal Classification Models: Estimating Causal Effects and Addressing Confounders", "comment": null, "summary": "Graph data is becoming increasingly prevalent due to the growing demand for relational insights in AI across various domains. Organizations regularly use graph data to solve complex problems involving relationships and connections. Causal learning is especially important in this context, since it helps to understand cause-effect relationships rather than mere associations. Since many real-world systems are inherently causal, graphs can efficiently model these systems. However, traditional graph machine learning methods including graph neural networks (GNNs), rely on correlations and are sensitive to spurious patterns and distribution changes. On the other hand, causal models enable robust predictions by isolating true causal factors, thus making them more stable under such shifts. Causal learning also helps in identifying and adjusting for confounders, ensuring that predictions reflect true causal relationships and remain accurate even under interventions. To address these challenges and build models that are robust and causally informed, we propose CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings. Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.", "AI": {"tldr": "CCAGNN\u662f\u4e00\u4e2a\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u6df7\u6742\u56e0\u7d20\u6765\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u4f9d\u8d56\u76f8\u5173\u6027\uff0c\u5bf9\u865a\u5047\u6a21\u5f0f\u548c\u5206\u5e03\u53d8\u5316\u654f\u611f\uff0c\u800c\u56e0\u679c\u6a21\u578b\u80fd\u901a\u8fc7\u9694\u79bb\u771f\u5b9e\u56e0\u679c\u56e0\u7d20\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u9884\u6d4b\u3002\u56e0\u679c\u5b66\u4e60\u8fd8\u80fd\u8bc6\u522b\u548c\u8c03\u6574\u6df7\u6742\u56e0\u7d20\uff0c\u786e\u4fdd\u9884\u6d4b\u53cd\u6620\u771f\u5b9e\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u63d0\u51faCCAGNN\u6846\u67b6\uff0c\u5c06\u56e0\u679c\u63a8\u7406\u878d\u5165\u56fe\u5b66\u4e60\u4e2d\uff0c\u652f\u6301\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u6784\u5efa\u5177\u6709\u6df7\u6742\u56e0\u7d20\u611f\u77e5\u80fd\u529b\u7684\u56e0\u679c\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0cCCAGNN\u59cb\u7ec8\u4f18\u4e8e\u9886\u5148\u7684\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "CCAGNN\u6846\u67b6\u901a\u8fc7\u6574\u5408\u56e0\u679c\u63a8\u7406\uff0c\u80fd\u591f\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u56e0\u679c\u611f\u77e5\u7684\u56fe\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u63d0\u4f9b\u53ef\u9760\u7684\u9884\u6d4b\u3002"}}
{"id": "2602.17778", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.17778", "abs": "https://arxiv.org/abs/2602.17778", "authors": ["Zachary Coalson", "Bo Fang", "Sanghyun Hong"], "title": "Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs", "comment": "Pre-print", "summary": "Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u5bf9\u8bdd\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\"\u8f6e\u6b21\u653e\u5927\"\u6545\u969c\u6a21\u5f0f\uff0c\u653b\u51fb\u8005\u53ef\u7cfb\u7edf\u5229\u7528\u6f84\u6e05\u5bfb\u6c42\u884c\u4e3a\u5ef6\u957f\u591a\u8f6e\u5bf9\u8bdd\u800c\u4e0d\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd9\u79cd\u653b\u51fb\u6e90\u4e8e\u5bf9\u8bdd\u52a8\u6001\u673a\u5236\u800c\u975e\u63d0\u793a\u4f18\u5316\uff0c\u80fd\u901a\u8fc7\u5fae\u8c03\u6216\u53c2\u6570\u7834\u574f\u5b9e\u73b0\u3002", "motivation": "\u591a\u8f6e\u4ea4\u4e92\u957f\u5ea6\u662f\u5bf9\u8bddLLM\u8fd0\u8425\u6210\u672c\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u9700\u8981\u8bc6\u522b\u65b0\u7684\u6545\u969c\u6a21\u5f0f\u6765\u7406\u89e3\u6a21\u578b\u5982\u4f55\u88ab\u6076\u610f\u5229\u7528\u5ef6\u957f\u5bf9\u8bdd\u800c\u4e0d\u5b8c\u6210\u4efb\u52a1\u3002", "method": "\u4ece\u673a\u5236\u89d2\u5ea6\u8bc6\u522b\u4e0e\u6f84\u6e05\u5bfb\u6c42\u54cd\u5e94\u76f8\u5173\u7684\u67e5\u8be2\u65e0\u5173\u901a\u7528\u6fc0\u6d3b\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u4f9b\u5e94\u94fe\u653b\u51fb\uff08\u5fae\u8c03\uff09\u548c\u8fd0\u884c\u65f6\u653b\u51fb\uff08\u4f4e\u7ea7\u53c2\u6570\u7834\u574f\uff09\u8bf1\u5bfc\u8f6e\u6b21\u653e\u5927\uff0c\u5728\u591a\u4e2a\u6307\u4ee4\u8c03\u4f18LLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u3002", "result": "\u653b\u51fb\u80fd\u663e\u8457\u589e\u52a0\u5bf9\u8bdd\u8f6e\u6b21\u540c\u65f6\u4fdd\u6301\u5408\u89c4\u6027\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u5bf9\u6b64\u7c7b\u6545\u969c\u4fdd\u62a4\u6709\u9650\uff0c\u653b\u51fb\u5728\u4e0d\u540c\u63d0\u793a\u548c\u4efb\u52a1\u4e2d\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u8f6e\u6b21\u653e\u5927\u662f\u5bf9\u8bddLLM\u4e2d\u4e00\u79cd\u65b0\u7684\u7cfb\u7edf\u6027\u6545\u969c\u6a21\u5f0f\uff0c\u6e90\u4e8e\u5bf9\u8bdd\u52a8\u6001\u673a\u5236\u800c\u975e\u63d0\u793a\u7ea7\u884c\u4e3a\uff0c\u5bf9\u6a21\u578b\u5b89\u5168\u548c\u8fd0\u8425\u6210\u672c\u6784\u6210\u5a01\u80c1\uff0c\u9700\u8981\u65b0\u7684\u9632\u5fa1\u65b9\u6cd5\u3002"}}
{"id": "2602.18008", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18008", "abs": "https://arxiv.org/abs/2602.18008", "authors": ["Zihan Guan", "Rituparna Datta", "Mengxuan Hu", "Shunshun Liu", "Aiying Zhang", "Prasanna Balachandran", "Sheng Li", "Anil Vullikanti"], "title": "NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs", "comment": "19 pages, 6 figures", "summary": "Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.", "AI": {"tldr": "\u63d0\u51faNIMM\u8bc4\u4f30\u6846\u67b6\u6d4b\u8bd5LLM\u751f\u6210\u673a\u7406\u6a21\u578b\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5f00\u53d1NIMMgen\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u6a21\u578b\u4ee3\u7801\u6b63\u786e\u6027\u548c\u5b9e\u7528\u6027", "motivation": "\u73b0\u6709LLM\u751f\u6210\u673a\u7406\u6a21\u578b\u7684\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\u73b0\u5b9e\u6761\u4ef6\uff0c\u4e0d\u6e05\u695a\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u662f\u5426\u53ef\u9760\uff0c\u9700\u8981\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u6846\u67b6", "method": "\u63d0\u51faNIMM\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u90e8\u5206\u89c2\u6d4b\u548c\u591a\u6837\u5316\u4efb\u52a1\u76ee\u6807\u7684\u73b0\u5b9e\u6761\u4ef6\u4e0b\u8bc4\u4f30LLM\u751f\u6210\u6a21\u578b\uff1b\u8bbe\u8ba1NIMMgen\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u4ee3\u7801\u6b63\u786e\u6027\u548c\u5b9e\u7528\u6027", "result": "\u8bc4\u4f30\u53d1\u73b0\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff0c\u4ece\u6a21\u578b\u6709\u6548\u6027\u5230\u4ee3\u7801\u7ea7\u6b63\u786e\u6027\u90fd\u6709\u95ee\u9898\uff1bNIMMgen\u5728\u4e09\u4e2a\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b66\u4e60\u5230\u7684\u673a\u7406\u6a21\u578b\u652f\u6301\u53cd\u4e8b\u5b9e\u5e72\u9884\u6a21\u62df", "conclusion": "LLM\u751f\u6210\u673a\u7406\u6a21\u578b\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u4f46\u901a\u8fc7NIMMgen\u6846\u67b6\u7684\u8fed\u4ee3\u4f18\u5316\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2602.17783", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17783", "abs": "https://arxiv.org/abs/2602.17783", "authors": ["Xiangyu Sun", "Shirin Hosseinmardi", "Amin Yousefpour", "Ramin Bostanabad"], "title": "Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors", "comment": null, "summary": "Machine learning (ML) has been increasingly used for topology optimization (TO). However, most existing ML-based approaches focus on simplified benchmark problems due to their high computational cost, spectral bias, and difficulty in handling complex physics. These limitations become more pronounced in multi-material, multi-physics problems whose objective or constraint functions are not self-adjoint. To address these challenges, we propose a framework based on physics-informed Gaussian processes (PIGPs). In our approach, the primary, adjoint, and design variables are represented by independent GP priors whose mean functions are parametrized via neural networks whose architectures are particularly beneficial for surrogate modeling of PDE solutions. We estimate all parameters of our model simultaneously by minimizing a loss that is based on the objective function, multi-physics potential energy functionals, and design-constraints. We demonstrate the capability of the proposed framework on benchmark TO problems such as compliance minimization, heat conduction optimization, and compliant mechanism design under single- and multi-material settings. Additionally, we leverage thermo-mechanical TO with single- and multi-material options as a representative multi-physics problem. We also introduce differentiation and integration schemes that dramatically accelerate the training process. Our results demonstrate that the proposed PIGP framework can effectively solve coupled multi-physics and design problems simultaneously -- generating super-resolution topologies with sharp interfaces and physically interpretable material distributions. We validate these results using open-source codes and the commercial software package COMSOL.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u9ad8\u65af\u8fc7\u7a0b(PIGP)\u7684\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u6750\u6599\u3001\u591a\u7269\u7406\u573a\u95ee\u9898\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8c31\u504f\u5dee\u7b49\u6311\u6218\uff0c\u80fd\u540c\u65f6\u751f\u6210\u8d85\u5206\u8fa8\u7387\u62d3\u6251\u548c\u7269\u7406\u89e3\u91ca\u7684\u6750\u6599\u5206\u5e03\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u62d3\u6251\u4f18\u5316\u4e2d\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u8c31\u504f\u5dee\u548c\u5904\u7406\u590d\u6742\u7269\u7406\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u6750\u6599\u3001\u591a\u7269\u7406\u573a\u975e\u81ea\u4f34\u968f\u95ee\u9898\u4e2d\u66f4\u4e3a\u7a81\u51fa\u3002", "method": "\u4f7f\u7528\u72ec\u7acb\u9ad8\u65af\u8fc7\u7a0b\u5148\u9a8c\u8868\u793a\u4e3b\u53d8\u91cf\u3001\u4f34\u968f\u53d8\u91cf\u548c\u8bbe\u8ba1\u53d8\u91cf\uff0c\u5747\u503c\u51fd\u6570\u7531\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u57fa\u4e8e\u76ee\u6807\u51fd\u6570\u3001\u591a\u7269\u7406\u573a\u52bf\u80fd\u6cdb\u51fd\u548c\u8bbe\u8ba1\u7ea6\u675f\u7684\u635f\u5931\u51fd\u6570\u540c\u65f6\u4f30\u8ba1\u6240\u6709\u53c2\u6570\u3002", "result": "\u6846\u67b6\u6210\u529f\u5e94\u7528\u4e8e\u5408\u89c4\u5ea6\u6700\u5c0f\u5316\u3001\u70ed\u4f20\u5bfc\u4f18\u5316\u3001\u67d4\u987a\u673a\u6784\u8bbe\u8ba1\u7b49\u57fa\u51c6\u95ee\u9898\uff0c\u4ee5\u53ca\u70ed\u673a\u68b0\u62d3\u6251\u4f18\u5316\u7b49\u4ee3\u8868\u6027\u591a\u7269\u7406\u573a\u95ee\u9898\uff0c\u751f\u6210\u5177\u6709\u9510\u5229\u754c\u9762\u548c\u7269\u7406\u89e3\u91ca\u6750\u6599\u5206\u5e03\u7684\u8d85\u5206\u8fa8\u7387\u62d3\u6251\u3002", "conclusion": "PIGP\u6846\u67b6\u80fd\u6709\u6548\u540c\u65f6\u89e3\u51b3\u8026\u5408\u591a\u7269\u7406\u573a\u548c\u8bbe\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u5fae\u5206\u548c\u79ef\u5206\u65b9\u6848\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u7ed3\u679c\u7ecf\u5f00\u6e90\u4ee3\u7801\u548cCOMSOL\u9a8c\u8bc1\u3002"}}
{"id": "2602.18037", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18037", "abs": "https://arxiv.org/abs/2602.18037", "authors": ["Johannes Ackermann", "Michael Noukhovitch", "Takashi Ishida", "Masashi Sugiyama"], "title": "Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards", "comment": "25 pages, 15 figures", "summary": "Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u68af\u5ea6\u6b63\u5219\u5316\uff08GR\uff09\u66ff\u4ee3\u4f20\u7edf\u7684KL\u60e9\u7f5a\u6765\u89e3\u51b3RLHF\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u7b56\u7565\u66f4\u65b0\u504f\u5411\u5956\u52b1\u6a21\u578b\u66f4\u51c6\u786e\u7684\u5e73\u5766\u533a\u57df\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5728\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4e2d\uff0cRLHF\u548cRLVR\u9762\u4e34\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u2014\u2014\u7b56\u7565\u53ef\u80fd\u5229\u7528\u5956\u52b1\u6a21\u578b\u7684\u4e0d\u51c6\u786e\u6027\u5b66\u4e60\u5230\u975e\u9884\u671f\u884c\u4e3a\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528KL\u60e9\u7f5a\u9650\u5236\u7b56\u7565\u66f4\u65b0\uff0c\u4f46\u672c\u6587\u63d0\u51fa\u9700\u8981\u66f4\u76f4\u63a5\u5730\u89e3\u51b3\u5956\u52b1\u6a21\u578b\u51c6\u786e\u6027\u95ee\u9898\u3002", "method": "1. \u7406\u8bba\u63a8\u5bfc\u5956\u52b1\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u6536\u655b\u70b9\u5e73\u5766\u5ea6\u7684\u5173\u7cfb\uff1b2. \u4f7f\u7528\u68af\u5ea6\u6b63\u5219\u5316\uff08GR\uff09\u5c06\u8bad\u7ec3\u504f\u5411\u5e73\u5766\u533a\u57df\u4ee5\u4fdd\u6301\u5956\u52b1\u6a21\u578b\u51c6\u786e\u6027\uff1b3. \u63d0\u51fa\u9ad8\u6548\u7684\u6709\u9650\u5dee\u5206\u4f30\u8ba1\u5b9e\u73b0\u663e\u5f0fGR\uff1b4. \u4e0eKL\u60e9\u7f5a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "1. \u5b9e\u8bc1\u663e\u793a\u68af\u5ea6\u8303\u6570\u4e0e\u5956\u52b1\u51c6\u786e\u6027\u76f8\u5173\uff1b2. KL\u60e9\u7f5a\u7684\u53c2\u8003\u91cd\u7f6e\u9690\u5f0f\u4f7f\u7528GR\u5bfb\u627e\u5e73\u5766\u533a\u57df\uff1b3. \u663e\u5f0fGR\u5728\u591a\u79cdRL\u5b9e\u9a8c\u4e2d\u4f18\u4e8eKL\u60e9\u7f5a\uff1a\u5728RLHF\u4e2d\u83b7\u5f97\u66f4\u9ad8GPT\u8bc4\u5224\u80dc\u7387\uff0c\u907f\u514d\u8fc7\u5ea6\u5173\u6ce8\u57fa\u4e8e\u89c4\u5219\u7684\u6570\u5b66\u5956\u52b1\u683c\u5f0f\uff0c\u9632\u6b62LLM-as-a-Judge\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u8bc4\u5224\u9ed1\u5ba2\u3002", "conclusion": "\u68af\u5ea6\u6b63\u5219\u5316\u662f\u89e3\u51b3RLHF\u4e2d\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7b56\u7565\u66f4\u65b0\u504f\u5411\u5956\u52b1\u6a21\u578b\u66f4\u51c6\u786e\u7684\u533a\u57df\uff0c\u6bd4\u4f20\u7edfKL\u60e9\u7f5a\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002"}}
{"id": "2602.17976", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17976", "abs": "https://arxiv.org/abs/2602.17976", "authors": ["Alessio Russo", "Yin-Ching Lee", "Ryan Welch", "Aldo Pacchiano"], "title": "In-Context Learning for Pure Exploration in Continuous Spaces", "comment": null, "summary": "In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $\u03b5$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.", "AI": {"tldr": "\u63d0\u51faC-ICPE-TS\u7b97\u6cd5\uff0c\u901a\u8fc7\u5143\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7eaf\u63a2\u7d22\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u63a8\u65ad\u771f\u5b9e\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edf\u7eaf\u63a2\u7d22\u95ee\u9898\u901a\u5e38\u5047\u8bbe\u79bb\u6563\u5047\u8bbe\u7a7a\u95f4\uff0c\u4f46\u8bb8\u591a\u73b0\u4ee3\u5e94\u7528\u573a\u666f\uff08\u5982\u8fde\u7eed\u81c2\u8d4c\u535a\u673a\u3001\u533a\u57df\u5b9a\u4f4d\u3001\u51fd\u6570\u6700\u5c0f\u5316\u4f30\u8ba1\uff09\u7684\u5047\u8bbe\u7a7a\u95f4\u662f\u8fde\u7eed\u7684\uff0c\u4e14\u4e0e\u67e5\u8be2/\u52a8\u4f5c\u7a7a\u95f4\u91cd\u5408\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u5904\u7406\u8fde\u7eed\u7a7a\u95f4\u7684\u7eaf\u63a2\u7d22\u95ee\u9898\u3002", "method": "\u63d0\u51faC-ICPE-TS\u7b97\u6cd5\uff1a\u5143\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\uff0c\u5c06\u89c2\u6d4b\u5386\u53f2\u6620\u5c04\u5230\uff081\uff09\u4e0b\u4e00\u4e2a\u8fde\u7eed\u67e5\u8be2\u52a8\u4f5c\u548c\uff082\uff09\u9884\u6d4b\u5047\u8bbe\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u5e8f\u5217\u6d4b\u8bd5\u7b56\u7565\u3002\u63a8\u7406\u65f6\uff0c\u7b97\u6cd5\u4e3b\u52a8\u6536\u96c6\u8bc1\u636e\u5e76\u63a8\u65ad\u771f\u5b9e\u5047\u8bbe\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u6216\u663e\u5f0f\u624b\u5de5\u4fe1\u606f\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1C-ICPE-TS\uff0c\u6db5\u76d6\u8fde\u7eed\u6700\u4f73\u81c2\u8bc6\u522b\u3001\u533a\u57df\u5b9a\u4f4d\u548c\u51fd\u6570\u6700\u5c0f\u5316\u8bc6\u522b\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "C-ICPE-TS\u4e3a\u8fde\u7eed\u7a7a\u95f4\u4e2d\u7684\u7eaf\u63a2\u7d22\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u5e8f\u5217\u6d4b\u8bd5\u7b56\u7565\uff0c\u5e76\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u8fdb\u884c\u63a8\u7406\u3002"}}
{"id": "2602.17798", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17798", "abs": "https://arxiv.org/abs/2602.17798", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds", "comment": null, "summary": "Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $\u039b$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\\% routing collapse across all seeds, comparable or better perplexity with 15--30\\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.", "AI": {"tldr": "\u63d0\u51faGrMoE\u8def\u7531\u6846\u67b6\uff0c\u5728Grassmannian\u6d41\u5f62\u4e0a\u4f7f\u7528Matrix Bingham\u5206\u5e03\u63a7\u5236\u8def\u7531\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u5355\u4e00\u53ef\u89e3\u91ca\u53c2\u6570\u039b\u8fde\u7eed\u8c03\u8282\u8def\u7531\u71b5\uff0c\u907f\u514d\u4e13\u5bb6\u5d29\u6e83\u5e76\u5b9e\u73b0\u540e\u9a8c\u7a00\u758f\u5ea6\u8c03\u6574\u3002", "motivation": "\u4f20\u7edfMoE\u6a21\u578b\u7684softmax\u95e8\u63a7\u673a\u5236\u7f3a\u4e4f\u63a7\u5236\u7a00\u758f\u6027\u4e0e\u5229\u7528\u7387\u6743\u8861\u7684\u7406\u8bba\u673a\u5236\uff0c\u4e14\u5b58\u5728\u4e13\u5bb6\u5d29\u6e83\u95ee\u9898\uff0c\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u8def\u7531\u6846\u67b6\u3002", "method": "\u5728Grassmannian\u5b50\u7a7a\u95f4\u6d41\u5f62\u4e0a\u6784\u5efa\u8def\u7531\u6846\u67b6\uff0c\u4f7f\u7528Matrix Bingham\u5206\u5e03\u7684\u6d53\u5ea6\u53c2\u6570\u4f5c\u4e3a\u95e8\u63a7\u6743\u91cd\uff0c\u5f00\u53d1\u644a\u9500\u53d8\u5206\u63a8\u7406\u8ba1\u7b97\u540e\u9a8c\u8def\u7531\u5206\u5e03\uff0c\u5efa\u7acb\u6d53\u5ea6\u8c31\u4e0e\u8def\u7531\u71b5\u7684\u4e25\u683c\u7406\u8bba\u5173\u7cfb\u3002", "result": "\u5728350M-2.7B\u53c2\u6570\u7684MoE\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b00%\u8def\u7531\u5d29\u6e83\uff0c\u56f0\u60d1\u5ea6\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u8d1f\u8f7d\u5747\u8861\u63d0\u534715-30%\uff0c\u6d53\u5ea6\u4e0e\u7a00\u758f\u5ea6\u5448\u73b0\u5e73\u6ed1\u5355\u8c03\u5173\u7cfb\uff0c\u4e13\u5bb6\u5b66\u4e60\u5230\u4e0e\u8bed\u8a00\u4e13\u4e1a\u5316\u76f8\u5173\u7684\u5f02\u8d28\u6d53\u5ea6\u503c\u3002", "conclusion": "GrMoE\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u8def\u7531\u7a00\u758f\u6027\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u89e3\u51b3\u4e13\u5bb6\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u8def\u7531\u884c\u4e3a\u548c\u540e\u9a8c\u7a00\u758f\u5ea6\u8c03\u6574\uff0c\u4e3aMoE\u8def\u7531\u5efa\u7acb\u4e86\u9996\u4e2a\u5f62\u5f0f\u5316\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.18297", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18297", "abs": "https://arxiv.org/abs/2602.18297", "authors": ["Usman Anwar", "Tim Bakker", "Dana Kianfar", "Cristina Pinneri", "Christos Louizos"], "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory", "comment": "First two authors contributed equally", "summary": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790\u53d1\u73b0\u601d\u7ef4\u94fe\u4e0e\u8f93\u51fa\u95f4\u7684\u975e\u96f6\u4e92\u4fe1\u606f\u662f\u601d\u7ef4\u94fe\u53ef\u76d1\u63a7\u7684\u5fc5\u8981\u4f46\u4e0d\u5145\u5206\u6761\u4ef6\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u76d1\u63a7\u51c6\u786e\u6027\u5e76\u9632\u6b62\u601d\u7ef4\u94fe\u9000\u5316\u3002", "motivation": "\u601d\u7ef4\u94fe\u76d1\u63a7\u7cfb\u7edf\u901a\u8fc7\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u6765\u68c0\u6d4b\u8f93\u51fa\u662f\u5426\u5177\u6709\u7279\u5b9a\u5c5e\u6027\uff08\u5982\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6d4b\u8bd5\u9ed1\u5ba2\u884c\u4e3a\uff09\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u6027\u80fd\u95ee\u9898\u3002\u9700\u8981\u7406\u89e3\u601d\u7ef4\u94fe\u53ef\u76d1\u63a7\u6027\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5f00\u53d1\u6539\u8fdb\u76d1\u63a7\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "1) \u4fe1\u606f\u8bba\u5206\u6790\uff1a\u8bc1\u660e\u601d\u7ef4\u94fe\u4e0e\u8f93\u51fa\u95f4\u7684\u975e\u96f6\u4e92\u4fe1\u606f\u662f\u53ef\u76d1\u63a7\u7684\u5fc5\u8981\u4f46\u4e0d\u5145\u5206\u6761\u4ef6\uff1b2) \u8bc6\u522b\u4e24\u79cd\u8fd1\u4f3c\u8bef\u5dee\u6e90\uff1a\u4fe1\u606f\u5dee\u8ddd\u548c\u542f\u53d1\u8bef\u5dee\uff1b3) \u63d0\u51fa\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff1a\u57fa\u4e8eoracle\u7684\u65b9\u6cd5\u76f4\u63a5\u5956\u52b1\u6a21\u578b\u751f\u6210\u6700\u5927\u5316\u76d1\u63a7\u51c6\u786e\u6027\u7684\u601d\u7ef4\u94fe\uff1b\u65e0\u6807\u7b7e\u65b9\u6cd5\u6700\u5927\u5316\u8f93\u51fa\u4e0e\u601d\u7ef4\u94fe\u95f4\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u4e0d\u540c\u73af\u5883\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u76d1\u63a7\u51c6\u786e\u6027\uff0c\u540c\u65f6\u9632\u6b62\u4e86\u601d\u7ef4\u94fe\u9000\u5316\uff0c\u5373\u4f7f\u5728\u4e0e\u76d1\u63a7\u5668\u5bf9\u6297\u8bad\u7ec3\u65f6\u4e5f\u80fd\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "conclusion": "\u601d\u7ef4\u94fe\u53ef\u76d1\u63a7\u6027\u53ef\u4ee5\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u76ee\u6807\u7cfb\u7edf\u6027\u5730\u6539\u8fdb\u3002\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u76d1\u63a7\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u4efb\u52a1\u5956\u52b1\u4e0d\u5b8c\u7f8e\u6307\u5b9a\u65f6\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2602.17978", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17978", "abs": "https://arxiv.org/abs/2602.17978", "authors": ["Daqian Shao"], "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees", "comment": "A thesis submitted for the degree of DPhil in Computer Science at Oxford", "summary": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5b58\u5728\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u51b3\u7b56\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u89e3\u51b3\u56e0\u679c\u6548\u5e94\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u5230\u6a21\u4eff\u5b66\u4e60\u548cLTL\u76ee\u6807\u5b66\u4e60\uff0c\u5177\u6709\u6536\u655b\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\uff0c\u8fd9\u5728\u6210\u672c\u9ad8\u3001\u5371\u9669\u6216\u4e0d\u53ef\u884c\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u95ee\u9898\u3002\u79bb\u7ebf\u6570\u636e\u96c6\u5b66\u4e60\u9762\u4e34\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6df7\u6742\u56e0\u7d20\u53ef\u80fd\u5bfc\u81f4\u865a\u5047\u76f8\u5173\u6027\u548c\u6b21\u4f18\u51b3\u7b56\u3002", "method": "1. \u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u8bc6\u522b\u56e0\u679c\u6548\u5e94\uff0c\u4f5c\u4e3a\u6761\u4ef6\u77e9\u9650\u5236\u95ee\u9898\uff1b2. \u53d7\u53cc\u673a\u5668\u5b66\u4e60\u542f\u53d1\uff0c\u5f00\u53d1\u5177\u6709\u6536\u655b\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u7684\u6837\u672c\u9ad8\u6548\u7b97\u6cd5\uff1b3. \u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u6a21\u4eff\u5b66\u4e60\u573a\u666f\uff1b4. \u5f00\u53d1\u5b66\u4e60\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u8868\u8fbe\u7684\u9ad8\u5c42\u76ee\u6807\u7684\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u3001\u5408\u6210\u548c\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u51b3\u7b56\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u5b58\u5728\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u79bb\u7ebf\u51b3\u7b56\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u9645\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u56e0\u679c\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.17809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17809", "abs": "https://arxiv.org/abs/2602.17809", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.", "AI": {"tldr": "SBA\u662f\u4e00\u79cd\u8d1d\u53f6\u65af\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u5728Stiefel\u6d41\u5f62\u4e0a\u4f7f\u7528Matrix Langevin\u5148\u9a8c\uff0c\u901a\u8fc7\u5207\u7a7a\u95f4\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u8fdb\u884c\u540e\u9a8c\u63a8\u65ad\uff0c\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5728\u9886\u57df\u8f6c\u79fb\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5bfc\u81f4\u9884\u6d4b\u6821\u51c6\u4e0d\u4f73\u4e14\u5728\u9886\u57df\u8f6c\u79fb\u4e0b\u884c\u4e3a\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u6821\u51c6\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u8d1d\u53f6\u65afPEFT\u6846\u67b6\u3002", "method": "\u5728Stiefel\u6d41\u5f62\u4e0a\u653e\u7f6eMatrix Langevin\u5148\u9a8c\u4e8e\u6b63\u4ea4\u9002\u914d\u5668\u56e0\u5b50\uff0c\u901a\u8fc7\u5207\u7a7a\u95f4\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u4e0e\u6d4b\u5730\u7ebf\u56de\u7f29\u8fdb\u884c\u8fd1\u4f3c\u540e\u9a8c\u63a8\u65ad\u3002\u76f8\u6bd4\u5e73\u5766\u7a7a\u95f4\u7684\u9ad8\u65af\u5148\u9a8c\u6295\u5f71\u5230\u6b63\u4ea4\u7ea6\u675f\uff0c\u8be5\u65b9\u6cd5\u5728\u6d41\u5f62\u4e0a\u81ea\u7136\u7f16\u7801\u9002\u914d\u5668\u5b50\u7a7a\u95f4\u5e94\u826f\u597d\u6761\u4ef6\u4e14\u6b63\u4ea4\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\uff08RoBERTa-large\u3001LLaMA-2-7B/13B\u3001Mistral-7B\u3001Qwen2.5-7B\uff09\u548c\u4efb\u52a1\u4e0a\uff0cSBA\u8fbe\u5230\u4e0eLoRA/DoRA\u76f8\u5f53\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u9884\u671f\u6821\u51c6\u8bef\u5dee\u964d\u4f4e18-34%\uff0c\u5728\u9886\u57df\u8f6c\u79fb\u4e0b\u9009\u62e9\u6027\u9884\u6d4bAUROC\u63d0\u9ad812-25%\uff0c\u4ee5\u8fdc\u4f4e\u4e8e\u4e94\u4e2aLoRA\u6a21\u578b\u96c6\u6210\u53c2\u6570\u6210\u672c\u5728OOD\u68c0\u6d4b\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5728\u6b63\u786e\u7684\u51e0\u4f55\u7ed3\u6784\u4e0a\u653e\u7f6e\u4e0d\u786e\u5b9a\u6027\u6bd4\u7b80\u5355\u4e3a\u9002\u914d\u5668\u6dfb\u52a0\u4efb\u4f55\u8d1d\u53f6\u65af\u5904\u7406\u66f4\u91cd\u8981\u3002SBA\u63d0\u4f9b\u4e86\u6821\u51c6\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u800c\u65e0\u9700\u91cd\u65b0\u6821\u51c6\uff0c\u5728\u6d41\u5f62\u4e0a\u8fdb\u884c\u63a8\u7406\u5177\u6709\u4e25\u683c\u7684\u7406\u8bba\u4f18\u52bf\u3002"}}
{"id": "2602.18301", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18301", "abs": "https://arxiv.org/abs/2602.18301", "authors": ["Ivan Bondarenko", "Egor Palkin", "Fedor Tikunov"], "title": "On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction", "comment": null, "summary": "Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for \"imposing\" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4e2d\u7684\u4e24\u4e2a\u539f\u578b\u4ee4\u724c\uff08m-token\u548ce-token\uff09\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u7f16\u7801\u4e0d\u540c\u4fe1\u606f\uff1am-token\u4e3b\u8981\u6355\u83b7\u8bed\u4e49\u4fe1\u606f\uff0ce-token\u66f4\u5173\u6ce8\u53e5\u6cd5\u7ed3\u6784\uff1b\u901a\u8fc7\u5173\u7cfb\u84b8\u998f\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u91cd\u6784\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5c06\u8bed\u4e49\u5173\u7cfb\u8f6c\u79fb\u5230\u539f\u578b\u4ee4\u724c\u7a7a\u95f4\u3002", "motivation": "\u63a2\u7d22LLM\u4e2d\u7528\u4e8e\u5355\u6b65\u6587\u672c\u91cd\u6784\u7684\u539f\u578b\u4ee4\u724c\uff08proto-tokens\uff09\u7f16\u7801\u7684\u4fe1\u606f\u7c7b\u578b\u548c\u884c\u4e3a\u7279\u6027\uff0c\u4e3a\u8d85\u8d8a\u81ea\u56de\u5f52\u8303\u5f0f\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u652f\u6301\u672a\u6765\u975e\u81ea\u56de\u5f52\u5e8f\u5217\u5230\u5e8f\u5217\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u5206\u6790\u4e24\u4e2a\u539f\u578b\u4ee4\u724c\u7684\u8bed\u4e49\u548c\u53e5\u6cd5\u5185\u5bb9\u5206\u79bb\uff0c\u7814\u7a76e-token\u7684\u7a33\u5b9a\u6027\u7279\u6027\uff0c\u53ef\u89c6\u5316\u91cd\u6784\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5e76\u6d4b\u8bd5\u4e24\u79cd\u4f7f\u7528\u6559\u5e08\u5d4c\u5165\u5bf9e-token\u65bd\u52a0\u8bed\u4e49\u7ed3\u6784\u7684\u6b63\u5219\u5316\u65b9\u6848\uff1a\u57fa\u4e8e\u951a\u70b9\u7684\u635f\u5931\u548c\u5173\u7cfb\u84b8\u998f\u76ee\u6807\u3002", "result": "\u6807\u51c6\u4f18\u5316\u4e0bm-token\u6bd4e-token\u66f4\u5f3a\u70c8\u5730\u6355\u83b7\u8bed\u4e49\u4fe1\u606f\uff1b\u57fa\u4e8e\u951a\u70b9\u7684\u7ea6\u675f\u4f1a\u663e\u8457\u727a\u7272\u91cd\u6784\u7cbe\u5ea6\uff1b\u5173\u7cfb\u84b8\u998f\u53ef\u4ee5\u5728\u4e0d\u635f\u5931\u91cd\u6784\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5c06\u6279\u6b21\u7ea7\u8bed\u4e49\u5173\u7cfb\u8f6c\u79fb\u5230\u539f\u578b\u4ee4\u724c\u7a7a\u95f4\u3002", "conclusion": "\u539f\u578b\u4ee4\u724c\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u5177\u6709\u53ef\u884c\u6027\uff0c\u5173\u7cfb\u84b8\u998f\u65b9\u6cd5\u4e3a\u5f00\u53d1\u975e\u81ea\u56de\u5f52seq2seq\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u9884\u6d4b\u539f\u578b\u4ee4\u724c\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u3002"}}
{"id": "2602.17993", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17993", "abs": "https://arxiv.org/abs/2602.17993", "authors": ["Mohan Tang", "Sidi Lu"], "title": "Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers", "comment": null, "summary": "Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward connections is critical; our dense interaction significantly outperforms \"sparse\" alternatives that only pass a single hidden state or vector. Notably, TurboConn can be integrated into pre-trained LLMs to overcome task-specific plateaus: while a fine-tuned Qwen-3-1.7B achieves only 53.78% on Parity, adding our architectural modification enables the model to reach 100% accuracy, all without the necessity to retrain the full model from scratch or sophisticated curriculum learning. Our results provide strong empirical evidence that the depth of the computational path is a key factor in reasoning ability, also offering a new mechanism to enhance LLMs without significantly affecting generation latency.", "AI": {"tldr": "TurboConn\u662f\u4e00\u79cd\u65b0\u578bTransformer\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u9ad8\u5c42\u9690\u85cf\u72b6\u6001\u8def\u7531\u5230\u540e\u7eedtoken\u7684\u4f4e\u5c42\uff0c\u7a81\u7834\u56fa\u5b9a\u8ba1\u7b97\u6df1\u5ea6\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u591a\u6b65\u9aa4\u63a8\u7406\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u4f46\u73b0\u6709Transformer\u7684\u8ba1\u7b97\u6df1\u5ea6\u53d7\u9650\u4e8e\u56fa\u5b9a\u5c42\u6570\uff0c\u9650\u5236\u4e86\u5176\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u6765\u63d0\u5347LLMs\u5728\u6570\u5b66\u3001\u903b\u8f91\u548c\u89c4\u5212\u7b49\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faTurboConn\u67b6\u6784\uff0c\u5c06\u6bcf\u4e2atoken\u7684\u9ad8\u5c42\u9690\u85cf\u72b6\u6001\u901a\u8fc7\u591a\u4e2a\u6b8b\u5dee\u8fde\u63a5\u8def\u7531\u5230\u4e0b\u4e00\u4e2atoken\u7684\u4f4e\u5c42\u3002\u8fd9\u79cd\u5bc6\u96c6\u7684\u5411\u540e\u8fde\u63a5\u673a\u5236\u5141\u8bb8\u4fe1\u606f\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u8de8\u5c42\u6d41\u52a8\uff0c\u5f62\u6210\u66f4\u6df1\u7684\u8ba1\u7b97\u8def\u5f84\u3002", "result": "\u5728GSM8K\u3001Parity\u548c\u591a\u6b65\u7b97\u672f\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u83b7\u5f970.9%\u5230\u8d85\u8fc710%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002\u7279\u522b\u5730\uff0c\u5728Parity\u4efb\u52a1\u4e0a\uff0cQwen-3-1.7B\u4ece53.78%\u63d0\u5347\u5230100%\u51c6\u786e\u7387\u3002\u5bc6\u96c6\u8fde\u63a5\u8bbe\u8ba1\u663e\u8457\u4f18\u4e8e\u7a00\u758f\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8ba1\u7b97\u8def\u5f84\u6df1\u5ea6\u662f\u5f71\u54cd\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\u3002TurboConn\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u6216\u590d\u6742\u8bfe\u7a0b\u5b66\u4e60\u5373\u53ef\u589e\u5f3a\u9884\u8bad\u7ec3LLMs\u7684\u65b0\u673a\u5236\uff0c\u4e14\u4e0d\u5f71\u54cd\u751f\u6210\u5ef6\u8fdf\u3002"}}
{"id": "2602.18333", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18333", "abs": "https://arxiv.org/abs/2602.18333", "authors": ["M. Reza Ebrahimi", "Micha\u00ebl Defferrard", "Sunny Panchal", "Roland Memisevic"], "title": "On the \"Induction Bias\" in Sequence Models", "comment": null, "summary": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.", "AI": {"tldr": "Transformer\u6a21\u578b\u5728\u72b6\u6001\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u5339\u914d\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u6570\u636e\u6548\u7387\u8fdc\u4f4e\u4e8eRNN\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u8de8\u5e8f\u5217\u957f\u5ea6\u5171\u4eab\u5b66\u4e60\u5230\u7684\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5728\u8bed\u8a00\u6a21\u578b\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u5176\u5728\u72b6\u6001\u8ffd\u8e2a\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5176\u5728\u5206\u5e03\u5916\u6cdb\u5316\uff08\u5982\u957f\u5ea6\u5916\u63a8\uff09\u7684\u5931\u8d25\uff0c\u672c\u6587\u8f6c\u5411\u7814\u7a76\u8fd9\u4e9b\u5c40\u9650\u5728\u5206\u5e03\u5185\u7684\u5f71\u54cd\uff0c\u63a2\u7a76Transformer\u5728\u72b6\u6001\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u7684\u6839\u672c\u80fd\u529b\u7f3a\u9677\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u7814\u7a76\uff0c\u6bd4\u8f83Transformer\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(RNN)\u5728\u591a\u79cd\u76d1\u7763\u673a\u5236\u4e0b\u7684\u6570\u636e\u6548\u7387\u3002\u5206\u6790\u6240\u9700\u8bad\u7ec3\u6570\u636e\u91cf\u968f\u72b6\u6001\u7a7a\u95f4\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u957f\u60c5\u51b5\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u5b66\u4e60\u5230\u7684\u72b6\u6001\u8ffd\u8e2a\u673a\u5236\u5728\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u95f4\u7684\u5171\u4eab\u7a0b\u5ea6\uff0c\u901a\u8fc7\u6743\u91cd\u5171\u4eab\u5206\u6790\u6765\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u5b66\u4e60\u5230\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "Transformer\u6240\u9700\u8bad\u7ec3\u6570\u636e\u91cf\u968f\u72b6\u6001\u7a7a\u95f4\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\u5feb\u901f\u589e\u957f\uff0c\u8fdc\u9ad8\u4e8eRNN\u3002Transformer\u5728\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u95f4\u8868\u73b0\u51fa\u53ef\u5ffd\u7565\u751a\u81f3\u6709\u5bb3\u7684\u6743\u91cd\u5171\u4eab\uff0c\u8868\u660e\u5176\u5b66\u4e60\u7684\u662f\u957f\u5ea6\u7279\u5b9a\u7684\u5b64\u7acb\u89e3\u51b3\u65b9\u6848\u3002\u76f8\u53cd\uff0c\u5faa\u73af\u6a21\u578b\u901a\u8fc7\u8de8\u957f\u5ea6\u5171\u4eab\u6743\u91cd\u5b9e\u73b0\u6709\u6548\u7684\u644a\u9500\u5b66\u4e60\uff0c\u5141\u8bb8\u4e00\u4e2a\u5e8f\u5217\u957f\u5ea6\u7684\u6570\u636e\u63d0\u5347\u5176\u4ed6\u957f\u5ea6\u7684\u6027\u80fd\u3002", "conclusion": "\u72b6\u6001\u8ffd\u8e2a\u4ecd\u7136\u662fTransformer\u7684\u6839\u672c\u6311\u6218\uff0c\u5373\u4f7f\u8bad\u7ec3\u548c\u8bc4\u4f30\u5206\u5e03\u5339\u914d\u3002Transformer\u7f3a\u4e4f\u6709\u6548\u7684\u8de8\u957f\u5ea6\u6743\u91cd\u5171\u4eab\u673a\u5236\uff0c\u5bfc\u81f4\u6570\u636e\u6548\u7387\u4f4e\u4e0b\uff0c\u800cRNN\u901a\u8fc7\u644a\u9500\u5b66\u4e60\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u8868\u660eTransformer\u5728\u72b6\u6001\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u4e0d\u4ec5\u662f\u5206\u5e03\u5916\u6cdb\u5316\u95ee\u9898\uff0c\u800c\u662f\u5176\u67b6\u6784\u7684\u6839\u672c\u7279\u6027\u3002"}}
{"id": "2602.17829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17829", "abs": "https://arxiv.org/abs/2602.17829", "authors": ["Preetom Biswas", "Giulia Pedrielli", "K. Sel\u00e7uk Candan"], "title": "Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models", "comment": null, "summary": "Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.", "AI": {"tldr": "ruleXplain\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u4eff\u771f\u9a71\u52a8\u7684\u52a8\u529b\u7cfb\u7edf\u4e2d\u63d0\u53d6\u5f62\u5f0f\u5316\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u7b26\u53f7\u89c4\u5219\u8bed\u8a00\u548c\u5ef6\u8fdf\u8bed\u4e49\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u56e0\u679c\u89c4\u5219\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u5ef6\u8fdf\u6548\u5e94\u7684\u65f6\u5e8f\u6570\u636e\u56e0\u679c\u5173\u7cfb\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u63d0\u4f9b\u6cdb\u5316\u6027\u5f3a\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u56e0\u4e3a\u591a\u4e2a\u4e0d\u540c\u7684\u8f93\u5165\u8f68\u8ff9\u53ef\u80fd\u4ea7\u751f\u51e0\u4e4e\u65e0\u6cd5\u533a\u5206\u7684\u8f93\u51fa\u3002", "method": "\u63d0\u51faruleXplain\u6846\u67b6\uff0c\u5229\u7528LLMs\u4ece\u4eff\u771f\u5668\u751f\u6210\u7684\u591a\u5143\u8f93\u5165\u65f6\u5e8f\u6570\u636e\u4e2d\u63d0\u53d6\u5f62\u5f0f\u5316\u89e3\u91ca\u3002\u6846\u67b6\u5305\u542b\u7ea6\u675f\u7b26\u53f7\u89c4\u5219\u8bed\u8a00\u3001\u53cd\u4e8b\u5b9e\u8f93\u5165\u751f\u6210\u3001\u805a\u7c7b\u548cLLM\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u4ee5\u53ca\u95ed\u73af\u7cbe\u70bc\u8fc7\u7a0b\u786e\u4fdd\u89c4\u5219\u4e00\u81f4\u6027\u3002", "result": "\u5728PySIRTEM\u6d41\u884c\u75c5\u6a21\u62df\u5668\u548cEnergyPlus\u5efa\u7b51\u80fd\u6e90\u6a21\u62df\u5668\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u901a\u8fc7\u8f93\u5165\u91cd\u5efa\u3001\u56e0\u679c\u7f16\u7801\u6d88\u878d\u7814\u7a76\u548c\u6cdb\u5316\u6d4b\u8bd5\u4e09\u7c7b\u5b9e\u9a8c\u8bc1\u660e\u4e86\u89c4\u5219\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ruleXplain\u80fd\u591f\u4ece\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u4e2d\u63d0\u53d6\u53ef\u9a8c\u8bc1\u7684\u56e0\u679c\u89c4\u5219\uff0c\u4e3a\u65f6\u5e8f\u6570\u636e\u7684\u56e0\u679c\u5173\u7cfb\u89e3\u91ca\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u5ef6\u8fdf\u6548\u5e94\u7684\u4eff\u771f\u9a71\u52a8\u7cfb\u7edf\u3002"}}
{"id": "2602.18417", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18417", "abs": "https://arxiv.org/abs/2602.18417", "authors": ["Joshua Nunley"], "title": "Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures", "comment": "12 pages, 3 figures, 8 tables", "summary": "This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5728U(d)\u95ed\u5b50\u7fa4\u4e0a\u6784\u5efa\u5e8f\u5217\u6a21\u578b\u7684\u76f4\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u516c\u7406\u8bbe\u7f6e\u4ece\u5171\u4eab\u9aa8\u67b6\u63a8\u5bfc\u5faa\u73af\u548cTransformer\u6a21\u677f\uff0c\u5e76\u5728O(d)\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1", "motivation": "\u4e3a\u5e8f\u5217\u6a21\u578b\u63d0\u4f9b\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u9690\u85cf\u72b6\u6001\u9650\u5236\u5728U(d)\u7684\u95ed\u5b50\u7fa4\u4e0a\uff0c\u901a\u8fc7\u5b50\u7fa4\u9009\u62e9\u4f5c\u4e3a\u72b6\u6001\u7a7a\u95f4\u3001\u5207\u7a7a\u95f4\u6295\u5f71\u548c\u66f4\u65b0\u6620\u5c04\u7684\u5373\u63d2\u5373\u7528\u7ec4\u4ef6", "method": "1) \u5efa\u7acb\u6700\u5c0f\u516c\u7406\u8bbe\u7f6e\uff0c\u4ece\u5171\u4eab\u9aa8\u67b6\u63a8\u5bfc\u5faa\u73af\u548cTransformer\u6a21\u677f\uff1b2) \u4e13\u95e8\u5316\u5230O(d)\u6b63\u4ea4\u7fa4\uff1b3) \u63d0\u51fa\u5207\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u6df7\u5408\u6269\u5c55\uff1b4) \u5728Tiny Shakespeare\u548cPenn Treebank\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6b63\u4ea4\u72b6\u6001RNN\u548cTransformer\u6a21\u578b", "result": "\u5728\u53c2\u6570\u5339\u914d\u8bbe\u7f6e\u4e0b\uff0c\u6b63\u4ea4\u72b6\u6001\u6a21\u578b\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff1b\u5207\u7a7a\u95f4\u7ebf\u6027\u6df7\u5408\u6269\u5c55\u5728\u6709\u9650\u53c2\u6570\u9884\u7b97\u4e0b\u63d0\u5347\u4e86O(d)\u5b9e\u9a8c\u7684\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5e8f\u5217\u6a21\u578b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u57fa\u7840\uff0c\u5b50\u7fa4\u9009\u62e9\u4f5c\u4e3a\u7075\u6d3b\u7ec4\u4ef6\uff0c\u5207\u7a7a\u95f4\u7ebf\u6027\u6df7\u5408\u6269\u5c55\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u5411"}}
{"id": "2602.17832", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17832", "abs": "https://arxiv.org/abs/2602.17832", "authors": ["Hang Liu", "Sangli Teng", "Maani Ghaffari"], "title": "MePoly: Max Entropy Polynomial Policy Optimization", "comment": null, "summary": "Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.", "AI": {"tldr": "\u63d0\u51faMePoly\u2014\u2014\u57fa\u4e8e\u591a\u9879\u5f0f\u80fd\u91cf\u6a21\u578b\u7684\u65b0\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u63d0\u4f9b\u663e\u5f0f\u53ef\u5904\u7406\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u89e3\u51b3\u6269\u6563\u7b56\u7565\u7f3a\u4e4f\u663e\u5f0f\u5bc6\u5ea6\u7684\u95ee\u9898\uff0c\u5728\u591a\u6a21\u6001\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u673a\u6700\u4f18\u63a7\u5236\u4e3a\u590d\u6742\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\uff0c\u4f46\u4f20\u7edf\u53c2\u6570\u5316\u7b56\u7565\u96be\u4ee5\u8868\u793a\u89e3\u7684\u591a\u6a21\u6001\u6027\u3002\u6269\u6563\u7b56\u7565\u867d\u80fd\u6062\u590d\u591a\u6a21\u6001\uff0c\u4f46\u7f3a\u4e4f\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\uff0c\u4f7f\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u590d\u6742\u5316\u3002", "method": "\u57fa\u4e8e\u591a\u9879\u5f0f\u80fd\u91cf\u6a21\u578b\u7684\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5MePoly\uff0c\u5229\u7528\u7ecf\u5178\u77e9\u95ee\u9898\u7684\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u80fd\u91cf\u51fd\u6570\u63d0\u4f9b\u663e\u5f0f\u53ef\u5904\u7406\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u5b9e\u73b0\u7cbe\u786e\u71b5\u6700\u5927\u5316\u3002", "result": "MePoly\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u975e\u51f8\u6d41\u5f62\uff0c\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6a21\u6001\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u8868\u73b0\u3002", "conclusion": "MePoly\u901a\u8fc7\u63d0\u4f9b\u663e\u5f0f\u53ef\u5904\u7406\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.18015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18015", "abs": "https://arxiv.org/abs/2602.18015", "authors": ["Jongseong Chae", "Jongeui Park", "Yongjae Shin", "Gyeongmin Kim", "Seungyul Han", "Youngchul Sung"], "title": "Flow Actor-Critic for Offline Reinforcement Learning", "comment": "Accepted to ICLR 2026", "summary": "The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.", "AI": {"tldr": "\u63d0\u51faFlow Actor-Critic\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u6a21\u578b\u540c\u65f6\u6539\u8fdbactor\u548ccritic\uff0c\u89e3\u51b3\u79bb\u7ebfRL\u4e2d\u591a\u6a21\u6001\u6570\u636e\u5206\u5e03\u95ee\u9898\uff0c\u5728D4RL\u548cOGBench\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u96c6\u5206\u5e03\u901a\u5e38\u5448\u73b0\u590d\u6742\u591a\u6a21\u6001\u7279\u6027\uff0c\u9700\u8981\u6bd4\u9ad8\u65af\u7b56\u7565\u66f4\u5177\u8868\u8fbe\u80fd\u529b\u7684\u7b56\u7565\u6765\u6355\u6349\u8fd9\u79cd\u5206\u5e03\u3002", "method": "\u63d0\u51faFlow Actor-Critic\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u6d41\u6a21\u578b\u4f5c\u4e3aactor\u7b56\u7565\uff1b2) \u5229\u7528\u6d41\u6a21\u578b\u8fdb\u884c\u4fdd\u5b88critic\u83b7\u53d6\uff0c\u9632\u6b62\u5728\u6570\u636e\u5916\u533a\u57df\u7684Q\u503c\u7206\u70b8\uff1b3) \u63d0\u51fa\u57fa\u4e8e\u6d41\u884c\u4e3a\u4ee3\u7406\u6a21\u578b\u7684\u65b0critic\u6b63\u5219\u5316\u5668\uff0c\u8be5\u6a21\u578b\u662f\u6d41actor\u8bbe\u8ba1\u7684\u526f\u4ea7\u54c1\u3002", "result": "\u5728D4RL\u548cOGBench\u7b49\u79bb\u7ebfRL\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5229\u7528\u6d41\u6a21\u578b\u6539\u8fdbactor\u548ccritic\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u63d0\u5347\u79bb\u7ebfRL\u6027\u80fd\u3002"}}
{"id": "2602.17835", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17835", "abs": "https://arxiv.org/abs/2602.17835", "authors": ["Sirui Chen", "Yunzhe Qi", "Mengting Ai", "Yifan Sun", "Ruizhong Qiu", "Jiaru Zou", "Jingrui He"], "title": "Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model's influence. Experimental results across diverse LLM families and evaluation tasks show that Iprox consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with Iprox achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, Iprox achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that Iprox provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.", "AI": {"tldr": "Iprox\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9\u538b\u7f29\u548c\u5bf9\u9f50\u68af\u5ea6\u4e0elogits\uff0c\u4ece\u76ee\u6807\u5927\u6a21\u578b\u4e2d\u6784\u5efa\u4fdd\u6301\u5f71\u54cd\u529b\u7684\u5c0f\u578b\u4ee3\u7406\u6a21\u578b\uff0c\u4f7f\u57fa\u4e8e\u68af\u5ea6\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u80fd\u66f4\u9ad8\u6548\u5730\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff08\u5982TracIn\u548cInfluence Functions\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u6570\u5341\u4ebf\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002\u4f7f\u7528\u73b0\u6210\u5c0f\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5176\u5b66\u4e60\u52a8\u6001\u4e0d\u660e\u786e\u3001\u5c3a\u5bf8\u65e0\u6cd5\u7075\u6d3b\u8c03\u6574\u3001\u4e14\u65e0\u6cd5\u4e0e\u76ee\u6807\u6a21\u578b\u5728\u68af\u5ea6\u5f71\u54cd\u529b\u4f30\u8ba1\u4e0a\u5bf9\u9f50\u3002", "method": "Iprox\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4f4e\u79e9\u538b\u7f29\u9636\u6bb5\uff0c\u901a\u8fc7\u538b\u7f29\u76ee\u6807\u6a21\u578b\u6765\u4fdd\u7559\u5f71\u54cd\u529b\u4fe1\u606f\uff1b2\uff09\u5bf9\u9f50\u9636\u6bb5\uff0c\u540c\u65f6\u5bf9\u9f50\u6a21\u578b\u68af\u5ea6\u548clogits\uff0c\u6784\u5efa\u80fd\u7075\u6d3b\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u4e14\u4fdd\u6301\u76ee\u6807\u6a21\u578b\u5f71\u54cd\u529b\u7684\u4ee3\u7406\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIprox\u5728\u591a\u79cdLLM\u5bb6\u65cf\u548c\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6210\u4ee3\u7406\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728Qwen3-4B\u4e0a\uff0c1.5B\u7684Iprox\u4ee3\u7406\u6bd41.7B\u73b0\u6210\u4ee3\u7406\u8868\u73b0\u66f4\u597d\u3002\u5728Llama3.2\u4e0a\uff0cIprox\u5728\u51cf\u5c11\u4e00\u534a\u4ee5\u4e0a\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4ecd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Iprox\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4fdd\u6301\u5f71\u54cd\u529b\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u4f7f\u57fa\u4e8e\u68af\u5ea6\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6269\u5c55\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u4ee3\u7406\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002"}}
{"id": "2602.17846", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17846", "abs": "https://arxiv.org/abs/2602.17846", "authors": ["Nick Dodson", "Xinyu Gao", "Qingsong Wang", "Yusu Wang", "Zhengchao Wan"], "title": "Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models", "comment": null, "summary": "Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5b58\u5728\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u7684\u9690\u79c1\u98ce\u9669\uff0c\u672c\u6587\u63d0\u51fa\u51e0\u4f55\u6846\u67b6\u5c06\u566a\u58f0\u8c03\u5ea6\u5206\u4e3a\u4e09\u4e2a\u533a\u57df\uff0c\u63ed\u793a\u8bb0\u5fc6\u98ce\u9669\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u9ad8\u5ea6\u4e0d\u5747\u5300\uff0c\u5e76\u8bc6\u522b\u51fa\u4e2d\u566a\u58f0\u533a\u57df\u662f\u8bb0\u5fc6\u98ce\u9669\u6700\u9ad8\u7684\"\u5371\u9669\u533a\"\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u4f46\u4e5f\u4f1a\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u4e25\u91cd\u9690\u79c1\u95ee\u9898\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u7684\u673a\u5236\uff1a\u8bb0\u5fc6\u5728\u566a\u58f0\u8c03\u5ea6\u7684\u54ea\u4e2a\u9636\u6bb5\u4ea7\u751f\u3001\u6570\u636e\u51e0\u4f55\u5982\u4f55\u5f71\u54cd\u8bb0\u5fc6\u3001\u4e0d\u540c\u566a\u58f0\u5c3a\u5ea6\u73b0\u8c61\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u5f15\u5165\u51e0\u4f55\u6846\u67b6\uff0c\u57fa\u4e8e\u9ad8\u65af\u58f3\u8986\u76d6\u8bad\u7ec3\u6570\u636e\u7684\u7279\u6027\u548c\u540e\u9a8c\u96c6\u4e2d\u884c\u4e3a\u8fd9\u4e24\u4e2a\u51b3\u5b9a\u6269\u6563\u6a21\u578b\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u7684\u57fa\u672c\u5bf9\u8c61\uff0c\u5c06\u566a\u58f0\u8c03\u5ea6\u5212\u5206\u4e3a\u4e09\u4e2a\u533a\u57df\u3002\u9488\u5bf9\u4e2d\u566a\u58f0\u533a\u57df\uff0c\u63d0\u51fa\u51e0\u4f55\u6761\u4ef6\u5e76\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u9488\u5bf9\u6027\u5e72\u9884\u6765\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u3002", "result": "\u8bb0\u5fc6\u98ce\u9669\u5728\u566a\u58f0\u6c34\u5e73\u4e0a\u9ad8\u5ea6\u4e0d\u5747\u5300\uff1a\u4e2d\u566a\u58f0\u533a\u57df\u662f\u8bb0\u5fc6\u6700\u663e\u8457\u7684\"\u5371\u9669\u533a\"\uff0c\u800c\u5c0f\u566a\u58f0\u548c\u5927\u566a\u58f0\u533a\u57df\u90fd\u62b5\u6297\u8bb0\u5fc6\uff0c\u4f46\u673a\u5236\u4e0d\u540c\uff1a\u5c0f\u566a\u58f0\u56e0\u8bad\u7ec3\u8986\u76d6\u6709\u9650\u800c\u907f\u514d\u8bb0\u5fc6\uff0c\u5927\u566a\u58f0\u56e0\u540e\u9a8c\u96c6\u4e2d\u5ea6\u4f4e\u4e14\u5177\u6709\u53ef\u8bc1\u660e\u7684\u8fd1\u7ebf\u6027\u9ad8\u65af\u53bb\u566a\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u6846\u67b6\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u7684\u673a\u5236\uff0c\u8bc6\u522b\u51fa\u8bb0\u5fc6\u98ce\u9669\u6700\u9ad8\u7684\u4e2d\u566a\u58f0\u533a\u57df\uff0c\u5e76\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u7684\u5e72\u9884\u65b9\u6cd5\u6765\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\uff0c\u4e3a\u7406\u89e3\u548c\u7ba1\u7406\u6269\u6563\u6a21\u578b\u7684\u9690\u79c1\u98ce\u9669\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.17849", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.17849", "abs": "https://arxiv.org/abs/2602.17849", "authors": ["Aditya Agrawal", "Albert Magyar", "Hiteshwar Eswaraiah", "Patrick Sheridan", "Pradeep Janedula", "Ravi Krishnan Venkatesan", "Krishna Nair", "Ravi Iyer"], "title": "Dual Length Codes for Lossless Compression of BFloat16", "comment": "6 pages, 5 figures", "summary": "Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Dual Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. Analyzing BFloat16 tensors from the Gemma model, we observed that the top 8 most frequent symbols account for approximately 50% of the cumulative probability. These 8 symbols are assigned a short 4 bit code. The remaining 248 symbols are assigned a longer 9 bit code. The coding scheme uses a single prefix bit to distinguish between the two code lengths. The scheme uses a small Look Up Table with only 8 entries for encoding and decoding. The scheme achieves a compressibility of 18.6% in comparison to 21.3% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDual Length Codes\uff0c\u4e00\u79cd\u6df7\u5408\u7f16\u7801\u65b9\u6cd5\uff0c\u5728\u538b\u7f29\u6548\u7387\u548c\u89e3\u7801\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9488\u5bf9LLM\u8bad\u7ec3\u4e2d\u7684\u7f51\u7edc\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "LLM\u8bad\u7ec3\u548c\u670d\u52a1\u4e25\u91cd\u4f9d\u8d56\u5e76\u884c\u5316\u548c\u96c6\u4f53\u64cd\u4f5c\uff0c\u5e38\u53d7\u7f51\u7edc\u5e26\u5bbd\u9650\u5236\u3002\u73b0\u6709\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\u5982Huffman\u7f16\u7801\u89e3\u7801\u6162\u4e14\u786c\u4ef6\u590d\u6742\uff0c\u800c\u901a\u7528\u7f16\u7801\u5982Exponential-Golomb\u7f16\u7801\u867d\u5feb\u4f46\u4e0d\u5229\u7528\u7b26\u53f7\u9891\u7387\u5206\u5e03\u3002", "method": "\u63d0\u51faDual Length Codes\u6df7\u5408\u65b9\u6cd5\uff1a\u5206\u6790Gemma\u6a21\u578b\u7684BFloat16\u5f20\u91cf\uff0c\u53d1\u73b0\u524d8\u4e2a\u6700\u9891\u7e41\u7b26\u53f7\u5360\u7ea650%\u6982\u7387\uff0c\u5206\u914d4\u4f4d\u77ed\u7801\uff1b\u5176\u4f59248\u4e2a\u7b26\u53f7\u5206\u914d9\u4f4d\u957f\u7801\u3002\u4f7f\u7528\u5355\u4e2a\u524d\u7f00\u4f4d\u533a\u5206\u4e24\u79cd\u7801\u957f\uff0c\u4ec5\u97008\u4e2a\u6761\u76ee\u7684\u67e5\u627e\u8868\u8fdb\u884c\u7f16\u89e3\u7801\u3002", "result": "\u4e0eHuffman\u7f16\u7801\u768421.3%\u538b\u7f29\u7387\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u8fbe\u523018.6%\u538b\u7f29\u7387\uff0c\u4f46\u663e\u8457\u52a0\u5feb\u4e86\u89e3\u7801\u901f\u5ea6\u5e76\u7b80\u5316\u4e86\u786c\u4ef6\u590d\u6742\u5ea6\u3002", "conclusion": "Dual Length Codes\u5728\u538b\u7f29\u6548\u7387\u548c\u89e3\u7801\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u7f51\u7edc\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u786c\u4ef6\u53cb\u597d\u6027\u548c\u89e3\u7801\u6027\u80fd\u3002"}}
{"id": "2602.17853", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17853", "abs": "https://arxiv.org/abs/2602.17853", "authors": ["Masoud Yavari", "Payman Moallem"], "title": "Neural Prior Estimation: Learning Class Priors from Latent Representations", "comment": null, "summary": "Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.", "AI": {"tldr": "\u63d0\u51faNPE\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u6761\u4ef6\u5316\u7684\u5bf9\u6570\u5148\u9a8c\u4f30\u8ba1\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u795e\u7ecf\u574d\u7f29\u7406\u8bba\u4e0b\u53ef\u6062\u590d\u7c7b\u522b\u5bf9\u6570\u5148\u9a8c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u957f\u5c3e\u6570\u636e\u96c6\u4e0a\u6709\u6548\u63d0\u5347\u5c11\u6570\u7c7b\u6027\u80fd", "motivation": "\u7c7b\u522b\u4e0d\u5e73\u8861\u4f1a\u5bfc\u81f4\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4ea7\u751f\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u663e\u5f0f\u7684\u7c7b\u522b\u7edf\u8ba1\u6216\u5206\u5e03\u7279\u5b9a\u7684\u8d85\u53c2\u6570\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u548c\u81ea\u9002\u5e94\u80fd\u529b", "method": "\u63d0\u51fa\u795e\u7ecf\u5148\u9a8c\u4f30\u8ba1\u5668(NPE)\uff0c\u4ece\u6f5c\u5728\u8868\u793a\u4e2d\u5b66\u4e60\u7279\u5f81\u6761\u4ef6\u5316\u7684\u5bf9\u6570\u5148\u9a8c\u4f30\u8ba1\uff0c\u4f7f\u7528\u4e00\u4e2a\u6216\u591a\u4e2a\u5148\u9a8c\u4f30\u8ba1\u6a21\u5757\u901a\u8fc7\u5355\u5411\u903b\u8f91\u635f\u5931\u4e0e\u4e3b\u5e72\u7f51\u7edc\u8054\u5408\u8bad\u7ec3\uff0c\u5f62\u6210NPE-LA\u8fdb\u884c\u504f\u5dee\u611f\u77e5\u9884\u6d4b", "result": "\u5728\u957f\u5c3eCIFAR\u548c\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5(STARE, ADE20K)\u4e0a\u5b9e\u9a8c\uff0c\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u5c11\u6570\u7c7b\u522b\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "NPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u7406\u8bba\u5408\u7406\u7684\u5148\u9a8c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u4e0d\u5e73\u8861\u611f\u77e5\u9884\u6d4b\uff0c\u5728\u795e\u7ecf\u574d\u7f29\u7406\u8bba\u4e0b\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u65e0\u9700\u663e\u5f0f\u7c7b\u522b\u7edf\u8ba1\u6216\u5206\u5e03\u7279\u5b9a\u8d85\u53c2\u6570"}}
{"id": "2602.18116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18116", "abs": "https://arxiv.org/abs/2602.18116", "authors": ["Olga Saukh", "Dong Wang", "Haris \u0160iki\u0107", "Yun Cheng", "Lothar Thiele"], "title": "Cut Less, Fold More: Model Compression through the Lens of Projection Geometry", "comment": "Accepted by ICLR 2026", "summary": "Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u65b9\u6cd5\"\u6a21\u578b\u6298\u53e0\"\uff0c\u901a\u8fc7\u6743\u91cd\u805a\u7c7b\u5b9e\u73b0\u4f4e\u79e9\u6295\u5f71\uff0c\u5728\u7406\u8bba\u4e0a\u548c\u5b9e\u8df5\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edf\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c4\u6a21\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u538b\u7f29\u65b9\u6cd5\u3002\u4f20\u7edf\u7ed3\u6784\u5316\u526a\u679d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u4f18\u7684\u51e0\u4f55\u611f\u77e5\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u4ece\u6295\u5f71\u51e0\u4f55\u89d2\u5ea6\u5206\u6790\u538b\u7f29\uff1a\u7ed3\u6784\u5316\u526a\u679d\u662f\u8f74\u5bf9\u9f50\u6295\u5f71\uff0c\u6a21\u578b\u6298\u53e0\u901a\u8fc7\u6743\u91cd\u805a\u7c7b\u5b9e\u73b0\u4f4e\u79e9\u6295\u5f71\u3002\u5c06\u4e24\u8005\u5f62\u5f0f\u5316\u4e3a\u6b63\u4ea4\u7b97\u5b50\uff0c\u8bc1\u660e\u5728\u79e9\u8ddd\u79bb\u4e3a1\u65f6\uff0c\u6298\u53e0\u5177\u6709\u66f4\u5c0f\u7684\u53c2\u6570\u91cd\u6784\u8bef\u5dee\u548c\u529f\u80fd\u6270\u52a8\u3002", "result": "\u57281000\u591a\u4e2a\u68c0\u67e5\u70b9\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d6ResNet18\u3001ViT-B/32\u3001CLIP ViT-B/32\u548cLLaMA\u6a21\u578b\u3002\u6298\u53e0\u901a\u5e38\u5728\u538b\u7f29\u540e\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u5728\u4e2d\u7b49\u81f3\u9ad8\u538b\u7f29\u7387\u4e0b\u4f18\u52bf\u6700\u5927\uff0c\u5728\u67d0\u4e9b\u7279\u5b9a\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u5dee\u8ddd\u4f1a\u7f29\u5c0f\u751a\u81f3\u53cd\u8f6c\u3002", "conclusion": "\u6a21\u578b\u6298\u53e0\u662f\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u3001\u65e0\u9700\u6821\u51c6\u7684\u526a\u679d\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u5b9e\u8df5\u4e2d\u901a\u5e38\u66f4\u4f18\u4e14\u5728\u7406\u8bba\u4e0a\u66f4\u6709\u539f\u5219\u6027\u3002"}}
{"id": "2602.17861", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17861", "abs": "https://arxiv.org/abs/2602.17861", "authors": ["Ryan McKenna", "Galen Andrew", "Borja Balle", "Vadym Doroshenko", "Arun Ganesh", "Weiwei Kong", "Alex Kurakin", "Brendan McMahan", "Mikhail Pravilov"], "title": "JAX-Privacy: A library for differentially private machine learning", "comment": null, "summary": "JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.", "AI": {"tldr": "JAX-Privacy\u662f\u4e00\u4e2a\u7528\u4e8e\u7b80\u5316\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u673a\u5236\u90e8\u7f72\u7684\u5e93\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u539f\u8bed\uff0c\u652f\u6301\u6df1\u5ea6\u5b9a\u5236\u548c\u5f00\u7bb1\u5373\u7528\u4f53\u9a8c", "motivation": "\u7b80\u5316\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u7684\u90e8\u7f72\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u7edf\u4e00\u5de5\u5177\uff0c\u6574\u5408\u6700\u65b0\u7814\u7a76\u6210\u679c\uff0c\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u5728\u53ef\u7528\u6027\u3001\u7075\u6d3b\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3", "method": "\u57fa\u4e8eJAX\u6784\u5efa\uff0c\u63d0\u4f9b\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6a21\u5757\u5316\u539f\u8bed\uff0c\u6db5\u76d6\u6279\u91cf\u9009\u62e9\u3001\u68af\u5ea6\u88c1\u526a\u3001\u566a\u58f0\u6dfb\u52a0\u3001\u4f1a\u8ba1\u548c\u5ba1\u8ba1\u7b49\u5173\u952e\u7ec4\u4ef6\uff0c\u652f\u6301\u6df1\u5ea6\u5b9a\u5236\u548c\u5f00\u7bb1\u5373\u7528\u4e24\u79cd\u4f7f\u7528\u6a21\u5f0f", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u5e93\uff0c\u6574\u5408\u4e86\u5927\u91cf\u6700\u65b0\u7814\u7a76\u6210\u679c\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u800c\u7075\u6d3b\u7684\u5de5\u5177", "conclusion": "JAX-Privacy\u901a\u8fc7\u8bbe\u8ba1\u539f\u5219\u6307\u5bfc\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u65e2\u652f\u6301\u6df1\u5ea6\u5b9a\u5236\u53c8\u63d0\u4f9b\u5f00\u7bb1\u5373\u7528\u4f53\u9a8c\u7684\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u5e93\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528"}}
{"id": "2602.18117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18117", "abs": "https://arxiv.org/abs/2602.18117", "authors": ["Yongjae Shin", "Jongseong Chae", "Jongeui Park", "Youngchul Sung"], "title": "Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning", "comment": "ICLR 2026 camera-ready", "summary": "Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.", "AI": {"tldr": "FINO\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u566a\u58f0\u589e\u5f3a\u63a2\u7d22\uff0c\u7ed3\u5408\u71b5\u5f15\u5bfc\u91c7\u6837\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u5728\u6709\u9650\u5728\u7ebf\u9884\u7b97\u4e0b\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u79bb\u7ebfRL\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6269\u5c55\u5230\u5728\u7ebf\u5fae\u8c03\u65f6\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u5728\u7ebf\u5fae\u8c03\u89c6\u4e3a\u79bb\u7ebf\u9884\u8bad\u7ec3\u7684\u76f4\u63a5\u5ef6\u7eed\uff0c\u672a\u80fd\u89e3\u51b3\u5173\u952e\u95ee\u9898\uff0c\u7279\u522b\u662f\u5982\u4f55\u6709\u6548\u63a2\u7d22\u79bb\u7ebf\u6570\u636e\u96c6\u4e4b\u5916\u7684\u52a8\u4f5c\u7a7a\u95f4\u3002", "method": "FINO\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u6d41\u5339\u914d\u7684\u7b56\u7565\uff0c\u5728\u7b56\u7565\u8bad\u7ec3\u4e2d\u6ce8\u5165\u566a\u58f0\u4ee5\u9f13\u52b1\u63a2\u7d22\u79bb\u7ebf\u6570\u636e\u96c6\u4e4b\u5916\u7684\u52a8\u4f5c\uff1b2) \u7ed3\u5408\u71b5\u5f15\u5bfc\u91c7\u6837\u673a\u5236\uff0c\u5728\u5728\u7ebf\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u6311\u6218\u6027\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cFINO\u5728\u6709\u9650\u7684\u5728\u7ebf\u9884\u7b97\u4e0b\u59cb\u7ec8\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u589e\u5f3a\u63a2\u7d22\u548c\u6837\u672c\u6548\u7387\u7684\u6709\u6548\u6027\u3002", "conclusion": "FINO\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u71b5\u5f15\u5bfc\u91c7\u6837\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u79bb\u7ebf\u5230\u5728\u7ebfRL\u4e2d\u7684\u63a2\u7d22\u6311\u6218\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.18182", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18182", "abs": "https://arxiv.org/abs/2602.18182", "authors": ["Daniel Romero-Alvarado", "Fernando Mart\u00ednez-Plumed", "Lorenzo Pacchiardi", "Hugo Save", "Siddhesh Milind Pawar", "Behzad Mehrbakhsh", "Pablo Antonio Moreno Casares", "Ben Slater", "Paolo Bova", "Peter Romero", "Zachary R. Tyler", "Jonathan Prunty", "Luning Sun", "Jose Hernandez-Orallo"], "title": "Capabilities Ain't All You Need: Measuring Propensities in AI", "comment": null, "summary": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u6d4b\u91cfAI\u503e\u5411\u6027\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u903b\u8f91\u51fd\u6570\u63cf\u8ff0\u6a21\u578b\u5728\"\u7406\u60f3\u533a\u95f4\"\u5185\u7684\u9ad8\u6210\u529f\u7387\uff0c\u7ed3\u5408\u503e\u5411\u6027\u4e0e\u80fd\u529b\u8bc4\u4f30\u53ef\u66f4\u51c6\u786e\u9884\u6d4bAI\u884c\u4e3a", "motivation": "\u4f20\u7edfAI\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u80fd\u529b\u6d4b\u91cf\uff0c\u4f46\u503e\u5411\u6027\uff08\u6a21\u578b\u5c55\u73b0\u7279\u5b9a\u884c\u4e3a\u7684\u8d8b\u52bf\uff09\u5bf9\u6027\u80fd\u548c\u5b89\u5168\u6027\u6709\u91cd\u8981\u5f71\u54cd\u3002\u4f20\u7edfIRT\u65b9\u6cd5\u4e0d\u9002\u5408\u6d4b\u91cf\u503e\u5411\u6027\uff0c\u56e0\u4e3a\u503e\u5411\u6027\u8fc7\u9ad8\u6216\u8fc7\u4f4e\u90fd\u53ef\u80fd\u6709\u95ee\u9898", "method": "\u5f15\u5165\u53cc\u903b\u8f91\u51fd\u6570\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u6210\u529f\u6982\u7387\u5efa\u6a21\u4e3a\u5f53\u503e\u5411\u6027\u5904\u4e8e\"\u7406\u60f3\u533a\u95f4\"\u5185\u65f6\u8f83\u9ad8\uff1b\u4f7f\u7528\u914d\u5907\u65b0\u5f00\u53d1\u7684\u4efb\u52a1\u65e0\u5173\u8bc4\u5206\u6807\u51c6\u7684LLM\u6765\u4f30\u8ba1\u7406\u60f3\u533a\u95f4\u7684\u754c\u9650\uff1b\u5e94\u7528\u4e8e\u516d\u4e2aLLM\u6a21\u578b\u5bb6\u65cf", "result": "\u80fd\u591f\u6d4b\u91cf\u503e\u5411\u6027\u504f\u79fb\u7a0b\u5ea6\u53ca\u5176\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\uff1b\u57fa\u4e8e\u4e00\u4e2a\u57fa\u51c6\u4f30\u8ba1\u7684\u503e\u5411\u6027\u53ef\u6210\u529f\u9884\u6d4b\u4fdd\u7559\u4efb\u52a1\u7684\u884c\u4e3a\uff1b\u7ed3\u5408\u503e\u5411\u6027\u548c\u80fd\u529b\u8bc4\u4f30\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u9884\u6d4b\u80fd\u529b", "conclusion": "\u5c55\u793a\u4e86\u5982\u4f55\u8fdb\u884c\u4e25\u683c\u7684\u503e\u5411\u6027\u6d4b\u91cf\uff0c\u8bc1\u660e\u7ed3\u5408\u503e\u5411\u6027\u548c\u80fd\u529b\u8bc4\u4f30\u6bd4\u4ec5\u4f7f\u7528\u80fd\u529b\u8bc4\u4f30\u80fd\u66f4\u51c6\u786e\u9884\u6d4bAI\u884c\u4e3a\uff0c\u4e3aAI\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6"}}
{"id": "2602.17893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17893", "abs": "https://arxiv.org/abs/2602.17893", "authors": ["Jiajun Shen", "Yufei Jin", "Yi He", "xingquan Zhu"], "title": "COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models", "comment": null, "summary": "State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.", "AI": {"tldr": "COMBA\uff1a\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u5927\u56fe\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u4e0a\u4e0b\u6587\u95e8\u63a7\u548c\u8de8\u6279\u6b21\u805a\u5408\u89e3\u51b3\u56fe\u5230\u5e8f\u5217\u8f6c\u6362\u7684\u6311\u6218", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5728\u5e8f\u5217\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u7279\u522b\u662f\u5927\u56fe\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3aSSMs\u662f\u5e8f\u5217\u6a21\u578b\uff0c\u800c\u5c06\u56fe\u8f6c\u6362\u4e3a\u5e8f\u5217\u8fdb\u884c\u6709\u6548\u5b66\u4e60\u6210\u672c\u5f88\u9ad8", "method": "\u63d0\u51faCOMBA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u56fe\u4e0a\u4e0b\u6587\u95e8\u63a7 - \u5229\u7528\u4e0d\u540c\u8df3\u6570\u7684\u90bb\u5c45\u4e0a\u4e0b\u6587\u5b66\u4e60\u6700\u4f73\u90bb\u5c45\u805a\u5408\u63a7\u5236\uff1b2) \u8de8\u6279\u6b21\u805a\u5408 - \u5bf9\u6bcf\u4e2a\u56fe\u4e0a\u4e0b\u6587\u91c7\u6837\u8282\u70b9\u6279\u6b21\uff0c\u8bad\u7ec3GNN\u5e76\u5728\u6279\u6b21\u95f4\u805a\u5408\u4fe1\u606f\uff0c\u5b9e\u73b0\u5927\u56fe\u6269\u5c55", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8de8\u6279\u6b21\u805a\u5408\u6bd4\u65e0\u805a\u5408\u7684GNN\u8bad\u7ec3\u5177\u6709\u66f4\u4f4e\u7684\u8bef\u5dee\uff1b\u5728\u57fa\u51c6\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "COMBA\u6210\u529f\u5c06\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5e94\u7528\u4e8e\u5927\u56fe\u5b66\u4e60\uff0c\u901a\u8fc7\u56fe\u4e0a\u4e0b\u6587\u95e8\u63a7\u548c\u8de8\u6279\u6b21\u805a\u5408\u89e3\u51b3\u4e86\u56fe\u5230\u5e8f\u5217\u8f6c\u6362\u7684\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.18195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18195", "abs": "https://arxiv.org/abs/2602.18195", "authors": ["Hairong Chen", "Yicheng Feng", "Ziyu Jia", "Samir Bhatt", "Hengguan Huang"], "title": "LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification", "comment": null, "summary": "Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.", "AI": {"tldr": "LERD\uff1a\u4e00\u79cd\u7aef\u5230\u7aef\u8d1d\u53f6\u65af\u7535\u751f\u7406\u795e\u7ecf\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u76f4\u63a5\u4ece\u591a\u901a\u9053EEG\u63a8\u65ad\u6f5c\u5728\u795e\u7ecf\u4e8b\u4ef6\u53ca\u5176\u5173\u7cfb\u7ed3\u6784\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6539\u53d8\u8111\u7535\u751f\u7406\u5e76\u7834\u574f\u591a\u901a\u9053EEG\u52a8\u529b\u5b66\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u9ed1\u76d2\u5206\u7c7b\u5668\uff0c\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u751f\u6210\u89c2\u6d4b\u4fe1\u53f7\u7684\u57fa\u7840\u52a8\u529b\u5b66\u3002", "method": "\u63d0\u51faLERD\u7cfb\u7edf\uff0c\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u63a8\u65ad\u6a21\u5757\u548c\u968f\u673a\u4e8b\u4ef6\u751f\u6210\u8fc7\u7a0b\uff0c\u91c7\u7528\u7535\u751f\u7406\u542f\u53d1\u7684\u52a8\u529b\u5b66\u5148\u9a8c\u6307\u5bfc\u5b66\u4e60\uff0c\u65e0\u9700\u4e8b\u4ef6\u6216\u4ea4\u4e92\u6807\u6ce8\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u4e24\u4e2a\u771f\u5b9eAD EEG\u961f\u5217\u4e0a\uff0cLERD\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u4ea7\u751f\u4e0e\u751f\u7406\u5bf9\u9f50\u7684\u6f5c\u5728\u6458\u8981\uff0c\u6709\u52a9\u4e8e\u8868\u5f81\u7fa4\u4f53\u6c34\u5e73\u52a8\u529b\u5b66\u5dee\u5f02\u3002", "conclusion": "LERD\u4e3aEEG\u52a8\u529b\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u591a\u901a\u9053EEG\u4e2d\u63a8\u65ad\u795e\u7ecf\u4e8b\u4ef6\u548c\u5173\u7cfb\u7ed3\u6784\uff0c\u5728AD\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.17898", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17898", "abs": "https://arxiv.org/abs/2602.17898", "authors": ["Jingquan Yan", "Yuwei Miao", "Peiran Yu", "Junzhou Huang"], "title": "Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors", "comment": "Accepted by ICLR 2026", "summary": "Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6ce8\u610f\u529b\u56de\u5f52\u6a21\u578b\u4e2dPCC\u505c\u6ede\u73b0\u8c61\uff0c\u63ed\u793a\u4e86MSE\u4f18\u5316\u4e0ePCC\u68af\u5ea6\u51b2\u7a81\u3001\u8f6f\u6ce8\u610f\u529b\u673a\u5236\u9650\u5236\u4ee5\u53ca\u51f8\u805a\u5408\u5668PCC\u6539\u8fdb\u4e0a\u9650\uff0c\u5e76\u63d0\u51faECA\u65b9\u6cd5\u7a81\u7834PCC\u5e73\u53f0\u671f\u3002", "motivation": "\u6ce8\u610f\u529b\u56de\u5f52\u6a21\u578b\u8bad\u7ec3\u4e2d\u5e38\u51fa\u73b0PCC\u505c\u6ede\u73b0\u8c61\uff1a\u5373\u4f7fMSE\u6301\u7eed\u4e0b\u964d\uff0cPCC\u65e9\u671f\u5c31\u505c\u6b62\u6539\u8fdb\u3002\u8fd9\u79cd\u73b0\u8c61\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u4fdd\u6301\u8bef\u5dee\u5e45\u5ea6\u540c\u65f6\u6539\u5584\u9884\u6d4b\u5f62\u72b6\u7684\u80fd\u529b\u3002", "method": "\u9996\u5148\u7406\u8bba\u5206\u6790PCC\u505c\u6ede\u7684\u4e24\u4e2a\u6839\u672c\u539f\u56e0\uff1a1) MSE\u4f18\u5316\u4e0ePCC\u68af\u5ea6\u51b2\u7a81\uff0c\u8f6f\u6ce8\u610f\u529b\u673a\u5236\u5728\u6570\u636e\u540c\u8d28\u65f6\u52a0\u5267\u6b64\u95ee\u9898\uff1b2) \u51f8\u805a\u5408\u5668\uff08\u5305\u62ec\u8f6f\u6ce8\u610f\u529b\uff09\u7684PCC\u6539\u8fdb\u5b58\u5728\u7406\u8bba\u4e0a\u9650\u3002\u57fa\u4e8e\u6b64\u63d0\u51faECA\u65b9\u6cd5\uff0c\u5305\u542b\u65b0\u673a\u5236\u6539\u5584PCC\u4f18\u5316\u5e76\u8d85\u8d8a\u51f8\u5305\u9650\u5236\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cECA\u80fd\u6301\u7eed\u7a81\u7834PCC\u5e73\u53f0\u671f\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u540c\u8d28\u6570\u636e\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u76f8\u5173\u6027\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3MSE\u6027\u80fd\u3002", "conclusion": "PCC\u505c\u6ede\u73b0\u8c61\u6e90\u4e8e\u4f18\u5316\u52a8\u6001\u548c\u6a21\u578b\u5bb9\u91cf\u7684\u6839\u672c\u9650\u5236\u3002\u63d0\u51fa\u7684ECA\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3a\u6ce8\u610f\u529b\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u540c\u65f6\u4f18\u5316\u8bef\u5dee\u5e45\u5ea6\u548c\u9884\u6d4b\u5f62\u72b6\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.18230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18230", "abs": "https://arxiv.org/abs/2602.18230", "authors": ["Jorge Carrasco Pollo", "Ioannis Kapetangeorgis", "Joshua Rosenthal", "John Hua Yao"], "title": "[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games", "comment": "Accepted for publication at Transactions on Machine Learning Research (TMLR) and MLRC Journal Track, 2025. Code available at: https://github.com/joshrosie/FACT29", "summary": "Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u590d\u73b0\u4e86Abdelnabi\u7b49\u4eba(2024)\u63d0\u51fa\u7684\u57fa\u4e8e\u53ef\u8bc4\u5206\u6e38\u620f\u7684LLM\u591a\u667a\u80fd\u4f53\u8c08\u5224\u57fa\u51c6\uff0c\u53d1\u73b0\u867d\u7136\u57fa\u51c6\u590d\u6742\uff0c\u4f46\u6a21\u578b\u6bd4\u8f83\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u5b9e\u9a8c\u8bbe\u7f6e\u5b58\u5728\u4fe1\u606f\u6cc4\u9732\u68c0\u6d4b\u548c\u6d88\u878d\u7814\u7a76\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\u3002", "motivation": "LLM\u5728\u591a\u667a\u80fd\u4f53\u8c08\u5224\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u7a33\u5065\u4e14\u53ef\u6cdb\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u3002Abdelnabi\u7b49\u4eba(2024)\u63d0\u51fa\u4e86\u57fa\u4e8e\u53ef\u8bc4\u5206\u6e38\u620f\u7684\u8c08\u5224\u57fa\u51c6\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8be5\u57fa\u51c6\u58f0\u660e\u7684\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u5176\u53ef\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u5728\u66f4\u591a\u6a21\u578b\u4e0a\u590d\u73b0\u539f\u59cb\u5b9e\u9a8c\uff1b2. \u5f15\u5165\u989d\u5916\u6307\u6807\u9a8c\u8bc1\u8c08\u5224\u8d28\u91cf\u548c\u8bc4\u4f30\u516c\u5e73\u6027\uff1b3. \u5728\u6269\u5c55\u7248\u57fa\u51c6\u4e0a\u68c0\u9a8c\u66f4\u5e7f\u6cdb\u6a21\u578b\u7684\u884c\u4e3a\uff1b4. \u5206\u6790\u5b9e\u9a8c\u8bbe\u7f6e\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4fe1\u606f\u6cc4\u9732\u68c0\u6d4b\u548c\u6d88\u878d\u7814\u7a76\u7684\u5b8c\u6574\u6027\u3002", "result": "1. \u57fa\u51c6\u786e\u5b9e\u590d\u6742\uff0c\u4f46\u6a21\u578b\u6bd4\u8f83\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u5bf9\u5176\u5ba2\u89c2\u6027\u63d0\u51fa\u8d28\u7591\uff1b2. \u53d1\u73b0\u5b9e\u9a8c\u8bbe\u7f6e\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4fe1\u606f\u6cc4\u9732\u68c0\u6d4b\u4e0d\u8db3\u548c\u6d88\u878d\u7814\u7a76\u4e0d\u5f7b\u5e95\uff1b3. \u901a\u8fc7\u6269\u5c55\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e3a\u6f5c\u5728\u7528\u6237\u63d0\u4f9b\u989d\u5916\u80cc\u666f\u7684\u89c1\u89e3\uff1b4. \u5f3a\u8c03\u4e86\u6a21\u578b\u6bd4\u8f83\u8bc4\u4f30\u4e2d\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86Abdelnabi\u7b49\u4eba(2024)\u8c08\u5224\u57fa\u51c6\u5728\u6a21\u578b\u6bd4\u8f83\u5ba2\u89c2\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u57fa\u51c6\u9700\u8981\u66f4\u4e25\u8c28\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u66f4\u5168\u9762\u7684\u5206\u6790\uff0c\u4e3a\u672a\u6765\u8c08\u5224\u57fa\u51c6\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2602.17918", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17918", "abs": "https://arxiv.org/abs/2602.17918", "authors": ["Jialin Yu", "Mo\u00efse Blanchard"], "title": "Distribution-Free Sequential Prediction with Abstentions", "comment": "38 pages, 2 figures. Submitted to COLT 2026. Extended version", "summary": "We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\\ instances, but at each round, the learner may also \\emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $\u03bc$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $\u03bc$ is \\emph{unknown} and propose an algorithm \\textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \\emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.", "AI": {"tldr": "\u7814\u7a76\u4e00\u79cd\u534a\u5bf9\u6297\u6027\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u5b66\u4e60\u8005\u53ef\u4ee5\u5728\u5b9e\u4f8b\u88ab\u6c61\u67d3\u65f6\u5f03\u6743\u800c\u4e0d\u53d7\u60e9\u7f5a\uff0c\u5728\u5206\u5e03\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0VC\u7c7b\u7684\u53ef\u5b66\u4e60\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u5b66\u4e60\u8005\u5df2\u77e5\u5e72\u51c0\u6837\u672c\u7684\u5206\u5e03\u03bc\uff0c\u8fd9\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u90fd\u662f\u5f3a\u5047\u8bbe\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5728\u5206\u5e03\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u662f\u5426\u4ecd\u80fd\u5b9e\u73b0\u7c7b\u4f3c\u7684\u5b66\u4e60\u4fdd\u8bc1\uff0c\u8fd9\u66f4\u7b26\u5408\u7ecf\u5178\u5b66\u4e60\u6846\u67b6\uff08\u5982PAC\u5b66\u4e60\uff09\u7684\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f31\u5b66\u4e60\u5668\u63d0\u5347\u8fc7\u7a0b\u7684\u7b97\u6cd5AbstainBoost\uff0c\u901a\u8fc7\u63d0\u5347\u5f31\u5b66\u4e60\u5668\u6765\u4fdd\u8bc1\u5728\u5206\u5e03\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u4e00\u822cVC\u7c7b\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u6b21\u7ebf\u6027\u8bef\u5dee\u3002", "result": "\u7b97\u6cd5\u5728\u5206\u5e03\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u4e00\u822cVC\u7c7b\u5728\u975e\u9002\u5e94\u6027\u5bf9\u6297\u8005\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u8bef\u5dee\u4fdd\u8bc1\uff0c\u5bf9\u4e8e\u9002\u5e94\u6027\u5bf9\u6297\u8005\uff0c\u5728\u7ebf\u6027\u5206\u7c7b\u5668\u7b49\u7ed3\u6784\u5316\u51fd\u6570\u7c7b\u4e2d\u4e5f\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4fdd\u8bc1\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u4e0b\u754c\uff0c\u63ed\u793a\u4e86\u8bef\u5206\u7c7b\u9519\u8bef\u4e0e\u9519\u8bef\u5f03\u6743\u6b21\u6570\u4e4b\u95f4\u7684\u591a\u9879\u5f0f\u6743\u8861\u3002", "conclusion": "\u5728\u5206\u5e03\u672a\u77e5\u7684\u534a\u5bf9\u6297\u6027\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u63d0\u5347\u7b97\u6cd5\u53ef\u4ee5\u5b9e\u73b0VC\u7c7b\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u586b\u8865\u4e86\u5df2\u77e5\u5206\u5e03\u5047\u8bbe\u4e0e\u5b8c\u5168\u5bf9\u6297\u6027\u5b66\u4e60\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18292", "abs": "https://arxiv.org/abs/2602.18292", "authors": ["Xiaotong Ji", "Rasul Tutunov", "Matthieu Zimmer", "Haitham Bou-Ammar"], "title": "Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers", "comment": null, "summary": "Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u89e3\u7801\u89c6\u4e3a\u4e00\u4e2a\u539f\u5219\u6027\u7684\u4f18\u5316\u5c42\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u95ee\u9898\u7edf\u4e00\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u6846\u67b6\u8bbe\u8ba1\u4e86\u65b0\u7684Best-of-K\u89e3\u7801\u5668\u6765\u63d0\u5347\u591a\u6837\u672c\u7ba1\u9053\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\u4ecd\u88ab\u89c6\u4e3a\u542f\u53d1\u5f0f\u7684\u53c2\u6570\u8c03\u6574\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u7406\u8bba\u6846\u67b6\u3002\u4f5c\u8005\u8ba4\u4e3a\u89e3\u7801\u5e94\u8be5\u88ab\u7406\u89e3\u4e3a\u539f\u5219\u6027\u7684\u4f18\u5316\u5c42\uff0c\u80fd\u591f\u7edf\u4e00\u89e3\u91ca\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\u5e76\u4fbf\u4e8e\u8bbe\u8ba1\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6b63\u5219\u5316\u4f18\u5316\u6846\u67b6\uff1a\u5728\u6bcf\u4e2atoken\u4f4d\u7f6e\uff0c\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u89e3\u51b3\u4e00\u4e2a\u6b63\u5219\u5316\u95ee\u9898\uff0c\u5e73\u8861\u6a21\u578b\u5f97\u5206\u4e0e\u7ed3\u6784\u504f\u597d\u548c\u7ea6\u675f\u3002\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u8d2a\u5a6a\u89e3\u7801\u3001Softmax\u91c7\u6837\u3001Top-K\u3001Top-P\u548cSparsemax\u7b49\u73b0\u6709\u65b9\u6cd5\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u8bbe\u8ba1\u4e86Best-of-K\u89e3\u7801\u5668\uff0c\u4f7f\u7528KL\u6563\u5ea6\u951a\u5b9a\u7684\u8986\u76d6\u76ee\u6807\u6765\u4f18\u5316\u591a\u6837\u672c\u7ba1\u9053\u3002", "result": "\u63d0\u51fa\u7684Best-of-K\u89e3\u7801\u5668\u5728\u56fa\u5b9aK\u6837\u672c\u9884\u7b97\u5185\u63d0\u9ad8\u4e86\u8986\u76d6\u826f\u597d\u66ff\u4ee3\u65b9\u6848\u7684\u6982\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u8bc1\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728Qwen2.5-Math-7B\u6a21\u578b\u4e0a\uff0c\u5728\u9ad8\u6e29\u91c7\u6837\u4e0b\u5c06MATH500\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86+18.6%\u3002", "conclusion": "\u89e3\u7801\u5e94\u8be5\u88ab\u91cd\u65b0\u7406\u89e3\u4e3a\u539f\u5219\u6027\u7684\u4f18\u5316\u5c42\uff0c\u800c\u4e0d\u662f\u542f\u53d1\u5f0f\u53c2\u6570\u8c03\u6574\u3002\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u4e0d\u4ec5\u89e3\u91ca\u4e86\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff0c\u8fd8\u4fbf\u4e8e\u8bbe\u8ba1\u65b0\u7684\u89e3\u7801\u5668\uff0c\u5982Best-of-K\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6837\u672c\u7ba1\u9053\u7684\u6027\u80fd\u3002"}}
{"id": "2602.17940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17940", "abs": "https://arxiv.org/abs/2602.17940", "authors": ["Shogo Iwazaki"], "title": "Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere", "comment": "27 pages, 2 figures", "summary": "We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \\emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $\u03a9(\\sqrt{T (\\ln T)^{d} (\\ln \\ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $\u03a9(\u03b5^{-2}(\\ln \\frac{1}\u03b5)^d (\\ln \\ln \\frac{1}\u03b5)^{-d})$ time steps to find an $\u03b5$-optimal point. We also provide the improved $O((\\ln T)^{d+1}(\\ln \\ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \\emph{dimension-independent} logarithmic factors under a hyperspherical input domain.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09bandit\u95ee\u9898\u5728\u5e73\u65b9\u6307\u6570\uff08SE\uff09\u6838\u4e0b\u7684\u7b97\u6cd5\u65e0\u5173\u6700\u574f\u60c5\u51b5\u4e0b\u754c\uff0c\u9488\u5bf9\u8d85\u7403\u9762\u8f93\u5165\u57df\uff0c\u90e8\u5206\u89e3\u51b3\u4e86\u7ef4\u5ea6\u4f9d\u8d56\u5bf9\u6570\u56e0\u5b50\u7684\u5f00\u653e\u95ee\u9898\u3002", "motivation": "GP bandit\u95ee\u9898\u4e2d\uff0c\u5bf9\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u5e73\u65b9\u6307\u6570\uff08SE\uff09\u6838\uff0c\u4e0a\u754c\u548c\u4e0b\u754c\u4e4b\u95f4\u5728\u7ef4\u5ea6\u4f9d\u8d56\u7684\u5bf9\u6570\u56e0\u5b50\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\uff0c\u8fd9\u662f\u4e00\u4e2a\u5c1a\u672a\u89e3\u51b3\u7684\u5f00\u653e\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u90e8\u5206\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d85\u7403\u9762\u8f93\u5165\u57df\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u7b97\u6cd5\u65e0\u5173\u7684\u6700\u574f\u60c5\u51b5\u5206\u6790\u65b9\u6cd5\uff0c\u9488\u5bf9\u8d85\u7403\u9762\u8f93\u5165\u57df\uff0c\u63a8\u5bfc\u7d2f\u79ef\u9057\u61be\u548c\u7b80\u5355\u9057\u61be\u7684\u4e0b\u754c\u3002\u540c\u65f6\u6539\u8fdb\u4e86SE\u6838\u7684\u6700\u5927\u4fe1\u606f\u589e\u76ca\u4e0a\u754c\u3002", "result": "1. \u7d2f\u79ef\u9057\u61be\u4e0b\u754c\uff1a\u03a9(\u221a(T(ln T)^d(ln ln T)^{-d}))\n2. \u7b80\u5355\u9057\u61be\u4e0b\u754c\uff1a\u627e\u5230\u03b5\u6700\u4f18\u70b9\u9700\u8981\u03a9(\u03b5^{-2}(ln 1/\u03b5)^d(ln ln 1/\u03b5)^{-d})\u65f6\u95f4\u6b65\n3. \u6700\u5927\u4fe1\u606f\u589e\u76ca\u4e0a\u754c\uff1aO((ln T)^{d+1}(ln ln T)^{-d})", "conclusion": "\u5728\u8d85\u7403\u9762\u8f93\u5165\u57df\u4e0b\uff0c\u672c\u6587\u7ed3\u679c\u4fdd\u8bc1\u4e86\u73b0\u6709\u6700\u4f73\u7b97\u6cd5\u5728\u7ef4\u5ea6\u65e0\u5173\u5bf9\u6570\u56e0\u5b50\u8303\u56f4\u5185\u7684\u6700\u4f18\u6027\uff0c\u90e8\u5206\u89e3\u51b3\u4e86GP bandit\u95ee\u9898\u4e2d\u7ef4\u5ea6\u4f9d\u8d56\u5bf9\u6570\u56e0\u5b50\u7684\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2602.18308", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18308", "abs": "https://arxiv.org/abs/2602.18308", "authors": ["Biswa Sengupta", "Jinhua Wang", "Leo Brunswic"], "title": "JPmHC Dynamical Isometry via Orthogonal Hyper-Connections", "comment": null, "summary": "Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.", "AI": {"tldr": "JPmHC\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u6301\u96c5\u53ef\u6bd4\u8c31\u7684\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u6df7\u5408\u5668\u66ff\u4ee3\u6052\u7b49\u8df3\u8dc3\u8fde\u63a5\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u8d85\u8fde\u63a5\u7b49\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55\u4e86\u6b8b\u5dee\u8fde\u63a5\u8303\u5f0f\uff0c\u4f46\u7834\u574f\u4e86\u6052\u7b49\u6620\u5c04\u7279\u6027\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u6027\u6709\u9650\u548c\u5185\u5b58\u5f00\u9500\u589e\u52a0\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faJPmHC\u6846\u67b6\uff1a1) \u7528\u53ef\u8bad\u7ec3\u7ebf\u6027\u6df7\u5408\u5668\u66ff\u4ee3\u6052\u7b49\u8df3\u8dc3\u8fde\u63a5\uff1b2) \u5c06\u6df7\u5408\u5668\u7ea6\u675f\u5728\u7b97\u5b50\u8303\u6570\u6709\u754c\u6d41\u5f62\u4e0a\uff1b3) \u4f7f\u7528\u81ea\u7531\u6982\u7387\u5206\u6790\u9884\u6d4b\u96c5\u53ef\u6bd4\u8c31\uff1b4) \u91c7\u7528\u5185\u5b58\u9ad8\u6548\u7684\u9690\u5f0f\u5fae\u5206\uff1b5) \u901a\u8fc7Cayley\u53d8\u6362\u5b9e\u73b0Stiefel\u7ea6\u675f\u6df7\u5408\u5668\u3002", "result": "\u5728ARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJPmHC\u76f8\u6bd4\u53cc\u968f\u673a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "JPmHC\u4f5c\u4e3a\u4e00\u79cd\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u8d85\u8fde\u63a5\u6269\u5c55\uff0c\u63a8\u8fdb\u4e86\u8c31\u611f\u77e5\u3001\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\u548c\u57fa\u7840\u6a21\u578b\u6f14\u8fdb\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2602.17947", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17947", "abs": "https://arxiv.org/abs/2602.17947", "authors": ["Yubo Zhou", "Jun Shu", "Junmin Liu", "Deyu Meng"], "title": "Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition", "comment": null, "summary": "Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u68af\u5ea6\u4f30\u8ba1\u7684\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\uff0c\u63d0\u51fa\u4e86\u51cf\u5c11\u65b9\u5dee\u7684\u96c6\u6210\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u8d85\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f30\u8ba1\u504f\u5dee\uff0c\u800c\u5ffd\u7565\u4e86\u6570\u636e\u5206\u5e03\u5e26\u6765\u7684\u65b9\u5dee\u8bef\u5dee\uff0c\u8fd9\u5f71\u54cd\u4e86\u6027\u80fd\u8868\u73b0\u3002\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8d85\u68af\u5ea6\u4f30\u8ba1\u4e2d\u7684\u65b9\u5dee\u95ee\u9898\u3002", "method": "1) \u5bf9\u8d85\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u8fdb\u884c\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\uff1b2) \u63d0\u4f9b\u8d85\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u754c\u7684\u5168\u9762\u5206\u6790\uff1b3) \u63d0\u51fa\u51cf\u5c11\u65b9\u5dee\u7684\u96c6\u6210\u8d85\u68af\u5ea6\u7b56\u7565\uff1b4) \u5efa\u7acb\u8d85\u68af\u5ea6\u4f30\u8ba1\u4e0e\u8d85\u989d\u8bef\u5dee\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u5728\u6b63\u5219\u5316\u8d85\u53c2\u6570\u5b66\u4e60\u3001\u6570\u636e\u8d85\u6e05\u6d17\u548c\u5c11\u6837\u672c\u5b66\u4e60\u7b49\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b9\u5dee\u51cf\u5c11\u7b56\u7565\u6539\u5584\u4e86\u8d85\u68af\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u4e3a\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\uff08\u5982\u9a8c\u8bc1\u96c6\u8fc7\u62df\u5408\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\u6df1\u5165\u5206\u6790\u4e86\u8d85\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\uff0c\u63d0\u51fa\u7684\u96c6\u6210\u7b56\u7565\u6709\u6548\u51cf\u5c11\u4e86\u65b9\u5dee\uff0c\u4e3a\u8d85\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.17948", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17948", "abs": "https://arxiv.org/abs/2602.17948", "authors": ["Yu Bai", "Zhe Wang", "Jiarui Zhang", "Dong-Xiao Zhang", "Yinjun Gao", "Jun-Jie Zhang"], "title": "A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion", "comment": "22 pages, 3 figures", "summary": "The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\\%$ to $95.63\\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \\emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \\emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u79f0\u6027\u7834\u7f3a\u7ef4\u5ea6\u6269\u5c55(SBDE)\u7814\u7a76\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u6743\u8861\u7684\u51e0\u4f55\u673a\u5236\uff0c\u53d1\u73b0\u63d2\u5165\u7684\u8f85\u52a9\u7ef4\u5ea6\u521b\u9020\u4e86\u5c16\u9510\u8fb9\u754c\uff0c\u5bfc\u81f4\u5bf9\u6297\u8106\u5f31\u6027", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u51c6\u786e\u6027\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u6743\u8861\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5176\u51e0\u4f55\u8d77\u6e90\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u63a2\u7a76\u5176\u6839\u672c\u673a\u5236", "method": "\u4f7f\u7528\u5bf9\u79f0\u6027\u7834\u7f3a\u7ef4\u5ea6\u6269\u5c55(SBDE)\u4f5c\u4e3a\u53d7\u63a7\u63a2\u9488\uff0c\u901a\u8fc7\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u63d2\u5165\u5e38\u6570\u503c\u50cf\u7d20\u6765\u6253\u7834\u5e73\u79fb\u5bf9\u79f0\u6027\uff0c\u5e76\u91c7\u7528\u6d4b\u8bd5\u65f6\u63a9\u7801\u6295\u5f71\u6765\u91cd\u7f6e\u63d2\u5165\u7684\u8f85\u52a9\u50cf\u7d20", "result": "SBDE\u663e\u8457\u63d0\u9ad8\u5e72\u51c0\u51c6\u786e\u7387\uff08\u5982CIFAR-10\u4e0a\u4ece90.47%\u523095.63%\uff09\uff0c\u4f46\u964d\u4f4e\u4e86\u5bf9\u8fed\u4ee3\u767d\u76d2\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff1b\u63a9\u7801\u6295\u5f71\u53ef\u6709\u6548\u4e2d\u548c\u653b\u51fb\u5e76\u6062\u590d\u9c81\u68d2\u6027", "conclusion": "\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u6743\u8861\u7684\u51e0\u4f55\u89e3\u91ca\u662f\uff1a\u4f18\u5316\u666f\u89c2\u52a0\u6df1\u5438\u5f15\u76c6\u5730\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u4e0d\u53ef\u907f\u514d\u5730\u6cbf\u7740\u8f85\u52a9\u81ea\u7531\u5ea6\u5efa\u7acb\u9661\u5ced\u7684\u5899\u58c1\uff0c\u5bfc\u81f4\u5bf9\u79bb\u6d41\u5f62\u6270\u52a8\u7684\u8106\u5f31\u654f\u611f\u6027"}}
{"id": "2602.17952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17952", "abs": "https://arxiv.org/abs/2602.17952", "authors": ["Hu Lou", "Yin-Jun Gao", "Dong-Xiao Zhang", "Tai-Jiao Du", "Jun-Jie Zhang", "Jia-Rui Zhang"], "title": "Hardware-Friendly Input Expansion for Accelerating Function Approximation", "comment": "22 pages, 4 figures", "summary": "One-dimensional function approximation is a fundamental problem in scientific computing and engineering applications. While neural networks possess powerful universal approximation capabilities, their optimization process is often hindered by flat loss landscapes induced by parameter-space symmetries, leading to slow convergence and poor generalization, particularly for high-frequency components. Inspired by the principle of \\emph{symmetry breaking} in physics, this paper proposes a hardware-friendly approach for function approximation through \\emph{input-space expansion}. The core idea involves augmenting the original one-dimensional input (e.g., $x$) with constant values (e.g., $\u03c0$) to form a higher-dimensional vector (e.g., $[\u03c0, \u03c0, x, \u03c0, \u03c0]$), effectively breaking parameter symmetries without increasing the network's parameter count. We evaluate the method on ten representative one-dimensional functions, including smooth, discontinuous, high-frequency, and non-differentiable functions. Experimental results demonstrate that input-space expansion significantly accelerates training convergence (reducing LBFGS iterations by 12\\% on average) and enhances approximation accuracy (reducing final MSE by 66.3\\% for the optimal 5D expansion). Ablation studies further reveal the effects of different expansion dimensions and constant selections, with $\u03c0$ consistently outperforming other constants. Our work proposes a low-cost, efficient, and hardware-friendly technique for algorithm design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u8f93\u5165\u7a7a\u95f4\u6269\u5c55\u6765\u6253\u7834\u53c2\u6570\u5bf9\u79f0\u6027\u7684\u786c\u4ef6\u53cb\u597d\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e00\u7ef4\u51fd\u6570\u903c\u8fd1\uff0c\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u5e76\u63d0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u4f46\u5176\u4f18\u5316\u8fc7\u7a0b\u5e38\u53d7\u53c2\u6570\u7a7a\u95f4\u5bf9\u79f0\u6027\u5bfc\u81f4\u7684\u5e73\u5766\u635f\u5931\u666f\u89c2\u5f71\u54cd\uff0c\u5bfc\u81f4\u6536\u655b\u7f13\u6162\u3001\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5c24\u5176\u5bf9\u9ad8\u9891\u5206\u91cf\u3002\u53d7\u7269\u7406\u5b66\u4e2d\u5bf9\u79f0\u6027\u7834\u7f3a\u539f\u7406\u542f\u53d1\uff0c\u9700\u8981\u4e00\u79cd\u786c\u4ef6\u53cb\u597d\u7684\u65b9\u6cd5\u6765\u6539\u5584\u51fd\u6570\u903c\u8fd1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u8f93\u5165\u7a7a\u95f4\u6269\u5c55\u65b9\u6cd5\uff1a\u5c06\u539f\u59cb\u4e00\u7ef4\u8f93\u5165\uff08\u5982x\uff09\u4e0e\u5e38\u6570\u503c\uff08\u5982\u03c0\uff09\u7ec4\u5408\u5f62\u6210\u66f4\u9ad8\u7ef4\u5411\u91cf\uff08\u5982[\u03c0, \u03c0, x, \u03c0, \u03c0]\uff09\uff0c\u5728\u4e0d\u589e\u52a0\u7f51\u7edc\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u6253\u7834\u53c2\u6570\u5bf9\u79f0\u6027\u3002", "result": "\u572810\u4e2a\u4ee3\u8868\u6027\u4e00\u7ef4\u51fd\u6570\uff08\u5e73\u6ed1\u3001\u4e0d\u8fde\u7eed\u3001\u9ad8\u9891\u3001\u4e0d\u53ef\u5fae\u51fd\u6570\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u8f93\u5165\u7a7a\u95f4\u6269\u5c55\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff08LBFGS\u8fed\u4ee3\u5e73\u5747\u51cf\u5c1112%\uff09\uff0c\u63d0\u9ad8\u903c\u8fd1\u7cbe\u5ea6\uff08\u6700\u4f185D\u6269\u5c55\u4f7f\u6700\u7ec8MSE\u51cf\u5c1166.3%\uff09\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\u03c0\u4f5c\u4e3a\u5e38\u6570\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u4e14\u786c\u4ef6\u53cb\u597d\u7684\u7b97\u6cd5\u8bbe\u8ba1\u6280\u672f\uff0c\u901a\u8fc7\u8f93\u5165\u7a7a\u95f4\u6269\u5c55\u6253\u7834\u53c2\u6570\u5bf9\u79f0\u6027\uff0c\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u5728\u51fd\u6570\u903c\u8fd1\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2602.18384", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18384", "abs": "https://arxiv.org/abs/2602.18384", "authors": ["Fotios Zantalis", "Evangelos Zervas", "Grigorios Koulouras"], "title": "FedZMG: Efficient Client-Side Optimization in Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the \"intensity\" or \"bias\" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.", "AI": {"tldr": "FedZMG\u662f\u4e00\u79cd\u65b0\u578b\u8054\u90a6\u5b66\u4e60\u5ba2\u6237\u7aef\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u68af\u5ea6\u6295\u5f71\u5230\u96f6\u5747\u503c\u8d85\u5e73\u9762\u6765\u51cf\u5c11\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c\u65e0\u9700\u989d\u5916\u901a\u4fe1\u6216\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5728\u975eIID\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u4e8eFedAvg\u548cFedAdam\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u901a\u5e38\u662f\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u964d\u4f4e\u6536\u655b\u901f\u5ea6\u548c\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u81ea\u9002\u5e94\u4f18\u5316\u5668\u867d\u7136\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u8ba1\u7b97\u590d\u6742\u6216\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\u3002", "method": "\u63d0\u51faFedZMG\u7b97\u6cd5\uff0c\u57fa\u4e8e\u68af\u5ea6\u4e2d\u5fc3\u5316\u601d\u60f3\uff0c\u5c06\u672c\u5730\u68af\u5ea6\u6295\u5f71\u5230\u96f6\u5747\u503c\u8d85\u5e73\u9762\uff0c\u4ece\u800c\u4e2d\u548c\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e2d\u7684\"\u5f3a\u5ea6\"\u6216\"\u504f\u5dee\"\u504f\u79fb\u3002\u8be5\u65b9\u6cd5\u53c2\u6570\u514d\u8d39\uff0c\u65e0\u9700\u989d\u5916\u901a\u4fe1\u6216\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eFedZMG\u80fd\u51cf\u5c11\u6709\u6548\u68af\u5ea6\u65b9\u5dee\u5e76\u63d0\u4f9b\u66f4\u7d27\u7684\u6536\u655b\u754c\u3002\u5728EMNIST\u3001CIFAR100\u548cShakespeare\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedZMG\u5728\u9ad8\u5ea6\u975eIID\u8bbe\u7f6e\u4e0b\u6bd4FedAvg\u548cFedAdam\u5177\u6709\u66f4\u597d\u7684\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "FedZMG\u662f\u4e00\u79cd\u6709\u6548\u7684\u5ba2\u6237\u7aef\u4f18\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\uff0c\u5728\u975eIID\u6570\u636e\u5206\u5e03\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.17958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17958", "abs": "https://arxiv.org/abs/2602.17958", "authors": ["Aida Afshar", "Yuke Zhang", "Aldo Pacchiano"], "title": "Bayesian Online Model Selection", "comment": null, "summary": "Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\\left( d^* M \\sqrt{T} + \\sqrt{(MT)} \\right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8d1d\u53f6\u65af\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u7b97\u6cd5\uff0c\u7528\u4e8e\u968f\u673a\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u57fa\u7840\u5b66\u4e60\u5668\u4e2d\u9009\u62e9\u6700\u4f18\u8005\uff0c\u83b7\u5f97\u6b21\u7ebf\u6027\u8d1d\u53f6\u65af\u9057\u61be\u754c\u3002", "motivation": "\u89e3\u51b3\u8d1d\u53f6\u65af\u8d4c\u535a\u673a\u4e2d\u7684\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff1a\u5f53\u73af\u5883\u5b9e\u4f8b\u4ece\u5148\u9a8c\u5206\u5e03\u4e2d\u91c7\u6837\u65f6\uff0c\u5982\u4f55\u8bbe\u8ba1\u81ea\u9002\u5e94\u7b56\u7565\u6765\u63a2\u7d22\u591a\u4e2a\u8d4c\u535a\u673a\u5b66\u4e60\u5668\uff0c\u5e76\u4e0e\u4e8b\u540e\u6700\u4f18\u5b66\u4e60\u5668\u7ade\u4e89\uff1f", "method": "\u63d0\u51fa\u65b0\u7684\u8d1d\u53f6\u65af\u7b97\u6cd5\u7528\u4e8e\u968f\u673a\u8d4c\u535a\u673a\u4e2d\u7684\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u3002\u7b97\u6cd5\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5728\u591a\u4e2a\u57fa\u7840\u5b66\u4e60\u5668\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\uff0c\u5e76\u8003\u8651\u4e86\u5b66\u4e60\u5668\u4e4b\u95f4\u7684\u6570\u636e\u5171\u4eab\u4ee5\u7f13\u89e3\u5148\u9a8c\u8bef\u8bbe\u95ee\u9898\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u83b7\u5f97\u8d1d\u53f6\u65af\u9057\u61be\u754c\u4e3aO(d*M\u221aT + \u221a(MT))\uff0c\u5176\u4e2dM\u662f\u57fa\u7840\u5b66\u4e60\u5668\u6570\u91cf\uff0cd*\u662f\u6700\u4f18\u5b66\u4e60\u5668\u7684\u9057\u61be\u7cfb\u6570\uff0cT\u662f\u65f6\u95f4\u8303\u56f4\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u5728\u591a\u79cd\u968f\u673a\u8d4c\u535a\u673a\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0e\u6700\u4f18\u57fa\u7840\u5b66\u4e60\u5668\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d1d\u53f6\u65af\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63a2\u7d22\u591a\u4e2a\u5b66\u4e60\u5668\u5e76\u4e0e\u6700\u4f18\u8005\u7ade\u4e89\uff0c\u540c\u65f6\u6570\u636e\u5171\u4eab\u673a\u5236\u6709\u52a9\u4e8e\u7f13\u89e3\u5148\u9a8c\u8bef\u8bbe\u95ee\u9898\uff0c\u4e3a\u8d1d\u53f6\u65af\u8d4c\u535a\u673a\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17962", "abs": "https://arxiv.org/abs/2602.17962", "authors": ["Shuo Sun", "Meiling Zhou", "Chen Zhao", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Hui Shen", "Hong-Wen Deng", "Kui Zhang", "Weihua Zhou"], "title": "Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts", "comment": "26 pages, 3 tables, 1 figure", "summary": "Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff08MMD\u3001CORAL\u3001DANN\uff09\u53ca\u5176\u7ec4\u5408\u5728\u9acb\u90e8\u9aa8\u6298\u98ce\u9669\u9884\u6d4b\u4e2d\u7684\u8de8\u961f\u5217\u6cdb\u5316\u6027\u80fd\uff0c\u53d1\u73b0\u591a\u65b9\u6cd5\u7ec4\u5408\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u961f\u5217\uff08\u5982\u4e0d\u540c\u533b\u9662\u3001\u5730\u533a\u3001\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\uff09\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u7279\u522b\u662f\u5728\u9acb\u90e8\u9aa8\u6298\u98ce\u9669\u9884\u6d4b\u4e2d\uff0c\u6e90\u961f\u5217\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u76ee\u6807\u961f\u5217\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u9700\u8981\u89e3\u51b3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u5927\u578b\u961f\u5217\uff08SOF\u3001MrOS\u3001UK Biobank\uff09\u7684\u4e34\u5e8a\u548cDXA\u7279\u5f81\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff1a\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u3001\u76f8\u5173\u6027\u5bf9\u9f50\uff08CORAL\uff09\u548c\u9886\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff08DANN\uff09\u53ca\u5176\u7ec4\u5408\u3002\u91c7\u7528\u65e0\u7ed3\u679c\u65b9\u6cd5\uff0c\u4e0d\u4f9d\u8d56\u76ee\u6807\u961f\u5217\u7684\u5df2\u77e5\u7ed3\u679c\u3002", "result": "\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u76f8\u6bd4\u65e0\u81ea\u9002\u5e94\u57fa\u7ebf\uff08\u4ec5\u6e90\u8bad\u7ec3\uff09\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u591a\u65b9\u6cd5\u7ec4\u5408\u5e26\u6765\u6700\u5927\u4e14\u6700\u7a33\u5b9a\u7684\u589e\u76ca\u3002MMD+CORAL+DANN\u7ec4\u5408\u5728\u4ec5\u7537\u6027\u6e90\u961f\u5217\u4e2dAUC\u8fbe0.88\uff0c\u4ec5\u5973\u6027\u6e90\u961f\u5217\u4e2dAUC\u8fbe0.95\uff0c\u8868\u660e\u96c6\u6210\u591a\u65b9\u6cd5\u53ef\u4ea7\u751f\u5bf9\u6570\u636e\u96c6\u5dee\u5f02\u4e0d\u654f\u611f\u7684\u7279\u5f81\u8868\u793a\u3002", "conclusion": "\u96c6\u6210\u591a\u79cd\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u9acb\u90e8\u9aa8\u6298\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u7684\u8de8\u961f\u5217\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u4f9d\u8d56\u76d1\u7763\u8c03\u4f18\u6216\u5047\u8bbe\u76ee\u6807\u961f\u5217\u5df2\u77e5\u7ed3\u679c\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u8fd9\u79cd\u65e0\u7ed3\u679c\u65b9\u6cd5\u80fd\u5728\u5b9e\u9645\u90e8\u7f72\u6761\u4ef6\u4e0b\u8fdb\u884c\u6a21\u578b\u9009\u62e9\uff0c\u6539\u5584\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.18409", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.18409", "abs": "https://arxiv.org/abs/2602.18409", "authors": ["Huan Luo", "Jonni Virtema"], "title": "Unifying approach to uniform expressivity of graph neural networks", "comment": null, "summary": "The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6a21\u677fGNN\uff08T-GNNs\uff09\u6846\u67b6\uff0c\u5c06\u8282\u70b9\u7279\u5f81\u66f4\u65b0\u5b9a\u4e49\u4e3a\u5728\u6307\u5b9a\u56fe\u6a21\u677f\u96c6\u5408\u4e0a\u805a\u5408\u6709\u6548\u6a21\u677f\u5d4c\u5165\uff0c\u5efa\u7acb\u4e86\u4e0e\u5206\u7ea7\u6a21\u677f\u6a21\u6001\u903b\u8f91\u7684\u7b49\u4ef7\u6027\uff0c\u4e3a\u5206\u6790GNN\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u65b9\u6cd5\u3002", "motivation": "\u6807\u51c6GNN\u7684\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u53ea\u80fd\u805a\u5408\u76f4\u63a5\u90bb\u5c45\u6216\u5168\u5c40\u4fe1\u606f\u3002\u4e3a\u4e86\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5c1d\u8bd5\u7eb3\u5165\u5b50\u7ed3\u6784\u4fe1\u606f\uff08\u5982\u73af\u8ba1\u6570\u548c\u5b50\u56fe\u5c5e\u6027\uff09\u3002\u672c\u6587\u65e8\u5728\u5f62\u5f0f\u5316\u8fd9\u4e00\u67b6\u6784\u8d8b\u52bf\u3002", "method": "\u5f15\u5165\u6a21\u677fGNNs\uff08T-GNNs\uff09\u6846\u67b6\uff0c\u8282\u70b9\u7279\u5f81\u901a\u8fc7\u805a\u5408\u6307\u5b9a\u56fe\u6a21\u677f\u96c6\u5408\u4e2d\u7684\u6709\u6548\u6a21\u677f\u5d4c\u5165\u6765\u66f4\u65b0\u3002\u63d0\u51fa\u76f8\u5e94\u7684\u5206\u7ea7\u6a21\u677f\u6a21\u6001\u903b\u8f91\uff08GML(T)\uff09\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6a21\u677f\u7684\u4e92\u6a21\u62df\u548cWL\u7b97\u6cd5\u7684\u5e7f\u4e49\u6982\u5ff5\u3002", "result": "\u5efa\u7acb\u4e86T-GNNs\u4e0eGML(T)\u4e4b\u95f4\u7684\u8868\u8fbe\u80fd\u529b\u7b49\u4ef7\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6807\u51c6AC-GNNs\u53ca\u5176\u6700\u8fd1\u53d8\u4f53\u53ef\u4ee5\u4f5c\u4e3aT-GNNs\u7684\u5b9e\u4f8b\u8fdb\u884c\u89e3\u91ca\uff0c\u4e3aGNN\u8868\u8fbe\u80fd\u529b\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u65b9\u6cd5\u3002", "conclusion": "T-GNNs\u6846\u67b6\u5f62\u5f0f\u5316\u4e86GNN\u4e2d\u7eb3\u5165\u5b50\u7ed3\u6784\u4fe1\u606f\u7684\u8d8b\u52bf\uff0c\u901a\u8fc7\u5efa\u7acb\u4e0e\u903b\u8f91\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3\u548c\u5206\u6790GNN\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.17972", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17972", "abs": "https://arxiv.org/abs/2602.17972", "authors": ["Sebastian Felipe R. Bundoc", "Paula Joy B. Martinez", "Sebastian C. Iba\u00f1ez", "Erika Fille T. Legara"], "title": "Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation", "comment": null, "summary": "School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.", "AI": {"tldr": "\u83f2\u5f8b\u5bbe\u6559\u80b2\u670d\u52a1\u5408\u540c\u8ba1\u5212\u7b49\u8865\u8d34\u9879\u76ee\u56e0\u6570\u636e\u788e\u7247\u5316\u672a\u80fd\u6709\u6548\u7f13\u89e3\u5b66\u6821\u62e5\u6324\u95ee\u9898\u3002\u7814\u7a76\u63d0\u51fa\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u529b\u6a21\u578b\u5206\u6790\u5b66\u751f\u6d41\u52a8\u6a21\u5f0f\uff0c\u53d1\u73b0\u5730\u7406\u8ddd\u79bb\u6bd4\u5b66\u8d39\u6210\u672c\u5bf9\u62e9\u6821\u5f71\u54cd\u66f4\u5927\uff0c\u4e14\u5ea7\u4f4d\u5bb9\u91cf\u800c\u975e\u8865\u8d34\u91d1\u989d\u662f\u4e3b\u8981\u9650\u5236\u56e0\u7d20\u3002", "motivation": "\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6\u9762\u4e34\u5b66\u6821\u62e5\u6324\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u5b66\u4e60\u6210\u679c\u5e76\u52a0\u5267\u6559\u80b2\u4e0d\u5e73\u7b49\u3002\u867d\u7136\u5c06\u5b66\u751f\u4ece\u516c\u7acb\u5b66\u6821\u8f6c\u79fb\u5230\u79c1\u7acb\u5b66\u6821\u7684\u8865\u8d34\u9879\u76ee\u53ef\u4ee5\u7f13\u89e3\u62e5\u6324\u800c\u65e0\u9700\u8d44\u672c\u5bc6\u96c6\u578b\u5efa\u8bbe\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u7cfb\u7edf\u788e\u7247\u5316\uff0c\u8fd9\u4e9b\u9879\u76ee\u5f80\u5f80\u6548\u679c\u4e0d\u4f73\u3002\u83f2\u5f8b\u5bbe\u6559\u80b2\u670d\u52a1\u5408\u540c\u8ba1\u5212\u4f5c\u4e3a\u5168\u7403\u6700\u5927\u7684\u6559\u80b2\u8865\u8d34\u9879\u76ee\u4e4b\u4e00\uff0c\u672a\u80fd\u5b9e\u73b0\u7f13\u89e3\u516c\u7acb\u5b66\u6821\u62e5\u6324\u7684\u76ee\u6807\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u79d1\u5b66\u548c\u6570\u636e\u7684\u5206\u6790\u6765\u7406\u89e3\u5b66\u751f\u5165\u5b66\u6d41\u52a8\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u8ba1\u7b97\u6846\u67b6\uff0c\u6574\u5408\u8fd13000\u4e2a\u673a\u6784\u7684\u5f02\u6784\u653f\u5e9c\u6570\u636e\uff0c\u91c7\u7528\u8d1f\u4e8c\u9879\u56de\u5f52\u4f30\u8ba1\u7684\u968f\u673a\u91cd\u529b\u6a21\u578b\uff0c\u63a8\u5bfc\u8ddd\u79bb\u3001\u51c0\u5b66\u8d39\u6210\u672c\u548c\u793e\u4f1a\u7ecf\u6d4e\u51b3\u5b9a\u56e0\u7d20\u7684\u884c\u4e3a\u5f39\u6027\u3002\u8fd9\u4e9b\u5f39\u6027\u53c2\u6570\u4e3a\u53cc\u91cd\u7ea6\u675f\u7a7a\u95f4\u5206\u914d\u673a\u5236\u63d0\u4f9b\u4f9d\u636e\uff0c\u6a21\u62df\u4e0d\u540c\u8865\u8d34\u91d1\u989d\u4e0b\u7684\u5b66\u751f\u91cd\u65b0\u5206\u914d\uff0c\u540c\u65f6\u5c0a\u91cd\u751f\u6e90\u5019\u9009\u6c60\u548c\u76ee\u6807\u5b66\u6821\u5bb9\u91cf\u9650\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5730\u7406\u90bb\u8fd1\u6027\u5bf9\u5b66\u6821\u9009\u62e9\u7684\u9650\u5236\u6bd4\u5b66\u8d39\u6210\u672c\u5f3a\u56db\u500d\uff0c\u5ea7\u4f4d\u5bb9\u91cf\u800c\u975e\u8865\u8d34\u91d1\u989d\u662f\u4e3b\u8981\u7ea6\u675f\u56e0\u7d20\u3002\u8865\u8d34\u9879\u76ee\u672c\u8eab\u65e0\u6cd5\u89e3\u51b3\u7cfb\u7edf\u6027\u8fc7\u5ea6\u62e5\u6324\u95ee\u9898\u3002", "conclusion": "\u8ba1\u7b97\u5efa\u6a21\u80fd\u591f\u5e2e\u52a9\u6559\u80b2\u653f\u7b56\u5236\u5b9a\u8005\u505a\u51fa\u516c\u5e73\u3001\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\uff0c\u63ed\u793a\u5f71\u54cd\u6709\u6548\u8d44\u6e90\u5206\u914d\u7684\u7ed3\u6784\u6027\u7ea6\u675f\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u53d1\u6325\u4f5c\u7528\u3002\u8865\u8d34\u9879\u76ee\u672c\u8eab\u4e0d\u8db3\u4ee5\u89e3\u51b3\u7cfb\u7edf\u6027\u8fc7\u5ea6\u62e5\u6324\u95ee\u9898\u3002"}}
{"id": "2602.17997", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17997", "abs": "https://arxiv.org/abs/2602.17997", "authors": ["Zehao Jin", "Yaoye Zhu", "Chen Zhang", "Yanan Sui"], "title": "Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly", "comment": null, "summary": "Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.", "AI": {"tldr": "\u5c06\u679c\u8747\u5b8c\u6574\u8fde\u63a5\u7ec4\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7528\u4e8e\u5177\u8eab\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\uff0c\u76f8\u6bd4\u968f\u673a\u56fe\u548c\u591a\u5c42\u611f\u77e5\u673a\u5177\u6709\u66f4\u9ad8\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5927\u8111\u8fde\u63a5\u7ec4\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u5728\u5177\u8eab\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u9a8c\u8bc1\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5bf9\u8fd0\u52a8\u63a7\u5236\u7684\u4f18\u52bf\u3002", "method": "\u5f00\u53d1FlyGM\u6a21\u578b\uff0c\u5176\u9759\u6001\u7ed3\u6784\u4e0e\u6210\u5e74\u679c\u8747\u5b8c\u6574\u8fde\u63a5\u7ec4\u76f8\u540c\uff0c\u5c06\u8fde\u63a5\u7ec4\u8868\u793a\u4e3a\u6709\u5411\u6d88\u606f\u4f20\u9012\u56fe\uff0c\u4e0e\u751f\u7269\u529b\u5b66\u679c\u8747\u6a21\u578b\u96c6\u6210\u8fdb\u884c\u8fd0\u52a8\u63a7\u5236\u3002", "result": "\u5728\u591a\u6837\u5316\u8fd0\u52a8\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u63a7\u5236\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u8c03\u6574\uff1b\u76f8\u6bd4\u5ea6\u4fdd\u6301\u91cd\u8fde\u56fe\u3001\u968f\u673a\u56fe\u548c\u591a\u5c42\u611f\u77e5\u673a\uff0cFlyGM\u5177\u6709\u66f4\u9ad8\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "\u9759\u6001\u5927\u8111\u8fde\u63a5\u7ec4\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u795e\u7ecf\u7b56\u7565\uff0c\u7528\u4e8e\u5177\u8eab\u5b66\u4e60\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u8bc1\u660e\u4e86\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.18002", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18002", "abs": "https://arxiv.org/abs/2602.18002", "authors": ["Junfei Sun", "Dixi Yao", "Xuchen Gong", "Tahseen Rabbani", "Manzil Zaheer", "Tian Li"], "title": "Asynchronous Heavy-Tailed Optimization", "comment": "8-page main body, 25-page appendix, 5 figures", "summary": "Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u91cd\u5c3e\u968f\u673a\u68af\u5ea6\u566a\u58f0\u4e0b\u5f02\u6b65\u4f18\u5316\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5ef6\u8fdf\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u5ef6\u8fdf\u8865\u507f\u7684\u7b97\u6cd5\u6539\u8fdb\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u91cd\u5c3e\u968f\u673a\u68af\u5ea6\u566a\u58f0\uff08\u5728Transformer\u6a21\u578b\u4e2d\u5e38\u89c1\uff09\u4f1a\u7834\u574f\u4f18\u5316\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u96c6\u4e2d\u5f0f\u6216\u5206\u5e03\u5f0f\u540c\u6b65\u8bbe\u7f6e\u4e0b\u7684\u91cd\u5c3e\u566a\u58f0\u5904\u7406\uff0c\u800c\u91cd\u5c3e\u566a\u58f0\u4e0e\u5f02\u6b65\u4f18\u5316\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5904\u7406\u5f02\u6b65\u66f4\u65b0\u4e2d\u6389\u961f\u8005\u7684\u901a\u4fe1\u65b9\u6848\uff0c\u5e76\u57fa\u4e8e\u5ef6\u8fdf\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u5ef6\u8fdf\u8865\u507f\u8fdb\u884c\u7b97\u6cd5\u6539\u8fdb\u3002\u8fd9\u4e9b\u4fee\u6539\u65e8\u5728\u589e\u5f3a\u5f02\u6b65\u7b97\u6cd5\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u6536\u655b\u4fdd\u8bc1\u4e0e\u540c\u6b65\u5bf9\u5e94\u65b9\u6cd5\u7684\u901f\u7387\u5339\u914d\uff0c\u4e14\u6bd4\u73b0\u6709\u5f02\u6b65\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u5ef6\u8fdf\u5bb9\u5fcd\u5ea6\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387/\u8fd0\u884c\u65f6\u95f4\u6743\u8861\u548c\u8d85\u53c2\u6570\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u540c\u6b65\u548c\u5f02\u6b65\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u91cd\u5c3e\u968f\u673a\u68af\u5ea6\u566a\u58f0\u4e0b\u5f02\u6b65\u4f18\u5316\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u5ef6\u8fdf\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u5ef6\u8fdf\u8865\u507f\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u5f02\u6b65\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2602.18055", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18055", "abs": "https://arxiv.org/abs/2602.18055", "authors": ["Jingyang Qiao", "Zhizhong Zhang", "Xin Tan", "Jingyu Gong", "Yanyun Qu", "Yuan Xie"], "title": "Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework", "comment": null, "summary": "Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Continual-NExT\u6846\u67b6\u548cMAGE\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u53cc\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u3001\u5e7b\u89c9\u7b49\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u53cc\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5728\u6301\u7eed\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u3002\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5b66\u4e60\u65b0\u4efb\u52a1\u4f1a\u7834\u574f\u5df2\u5b66\u77e5\u8bc6\u3001\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u3001\u6307\u4ee4\u4e0d\u9075\u5faa\u3001\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u5931\u8d25\u7b49\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u6b64\u7c7b\u6a21\u578b\u7684\u6807\u51c6\u5316\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86Continual-NExT\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\u3002\u540c\u65f6\u63d0\u51fa\u4e86MAGE\u65b9\u6cd5\uff08\u901a\u7528LoRA\u548c\u4e13\u5bb6LoRA\u7684\u6df7\u5408\u4e0e\u805a\u5408\uff09\uff0c\u901a\u8fc7\u4fc3\u8fdb\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u548c\u51cf\u8f7b\u9057\u5fd8\u6765\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMAGE\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u9996\u4e2a\u9488\u5bf9\u53cc\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6Continual-NExT\uff0c\u5e76\u63d0\u51fa\u7684MAGE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18060", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18060", "abs": "https://arxiv.org/abs/2602.18060", "authors": ["Abhay Shinde", "Aryan Amit Barsainyan", "Jose Siguenza", "Ankita Vaishnobi Bisoi", "Rakshit Kr. Singh", "Bharath Ramsundar"], "title": "Deepmechanics", "comment": "11 pages, 7 figures, Submitted to KDD 2026", "summary": "Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.", "AI": {"tldr": "\u8bba\u6587\u5bf9\u4e09\u79cd\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08HNN\u3001LNN\u3001SRNN\uff09\u5728\u516d\u79cd\u52a8\u529b\u7cfb\u7edf\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u6df7\u6c8c\u6216\u975e\u4fdd\u5b88\u7cfb\u7edf\u4e2d\u96be\u4ee5\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "motivation": "\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5df2\u6210\u4e3a\u5b66\u4e60\u52a8\u529b\u7cfb\u7edf\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u7f3a\u4e4f\u8de8\u4e0d\u540c\u7269\u7406\u73b0\u8c61\u7684\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u5728\u4fdd\u5b88\u548c\u8017\u6563\u7cfb\u7edf\u4e2d\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u68c0\u67e5\u8f68\u8ff9\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528DeepChem\u6846\u67b6\u5bf9\u4e09\u79cd\u7269\u7406\u4fe1\u606f\u67b6\u6784\uff08\u54c8\u5bc6\u987f\u795e\u7ecf\u7f51\u7edc\u3001\u62c9\u683c\u6717\u65e5\u795e\u7ecf\u7f51\u7edc\u3001\u8f9b\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u516d\u4e2a\u52a8\u529b\u7cfb\u7edf\uff0c\u5305\u62ec\u4fdd\u5b88\u7cfb\u7edf\uff08\u8d28\u91cf\u5f39\u7c27\u3001\u5355\u6446\u3001\u53cc\u6446\u3001\u4e09\u4f53\u95ee\u9898\u3001\u5f39\u7c27\u6446\uff09\u548c\u975e\u4fdd\u5b88\u63a5\u89e6\u7cfb\u7edf\uff08\u5f39\u8df3\u7403\uff09\u3002", "result": "\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u6a21\u578b\u5728\u6df7\u6c8c\u6216\u975e\u4fdd\u5b88\u7cfb\u7edf\u4e2d\u90fd\u96be\u4ee5\u4fdd\u6301\u7a33\u5b9a\u6027\uff0c\u6a21\u578b\u9884\u6d4b\u8f68\u8ff9\u5b58\u5728\u8bef\u5dee\uff0c\u9700\u8981\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u66f4\u591a\u7814\u7a76\u624d\u80fd\u5b66\u4e60\u5230\u7ecf\u5178\u529b\u5b66\u7cfb\u7edf\u7684\u9c81\u68d2\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6df7\u6c8c\u548c\u975e\u4fdd\u5b88\u7cfb\u7edf\u65f6\u3002"}}
{"id": "2602.18084", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18084", "abs": "https://arxiv.org/abs/2602.18084", "authors": ["Benjamin Honor\u00e9", "Alba Carballo-Castro", "Yiming Qin", "Pascal Frossard"], "title": "Balancing Symmetry and Efficiency in Graph Flow Matching", "comment": "15 pages, 11 figures", "summary": "Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\\%$ of the baseline training epochs.", "AI": {"tldr": "\u7814\u7a76\u56fe\u751f\u6210\u6a21\u578b\u4e2d\u4e25\u683c\u7b49\u53d8\u6027\u4e0e\u8bad\u7ec3\u6548\u7387\u7684\u6743\u8861\uff0c\u901a\u8fc7\u53ef\u63a7\u5bf9\u79f0\u6027\u8c03\u5236\u65b9\u6848\u653e\u677e\u7b49\u53d8\u6027\u7ea6\u675f\uff0c\u52a0\u901f\u6536\u655b\u540c\u65f6\u907f\u514d\u8fc7\u62df\u5408\u3002", "motivation": "\u4e25\u683c\u7b49\u53d8\u6027\u867d\u7136\u80fd\u4fdd\u8bc1\u56fe\u751f\u6210\u6a21\u578b\u5c0a\u91cd\u56fe\u7684\u7f6e\u6362\u5bf9\u79f0\u6027\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u5e76\u51cf\u7f13\u6536\u655b\u901f\u5ea6\uff0c\u56e0\u4e3a\u6a21\u578b\u9700\u8981\u5728\u5927\u91cf\u53ef\u80fd\u7684\u8282\u70b9\u7f6e\u6362\u4e2d\u4fdd\u6301\u4e00\u81f4\u3002\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u6743\u8861\u5173\u7cfb\u3002", "method": "\u4ece\u7b49\u53d8\u79bb\u6563\u6d41\u5339\u914d\u6a21\u578b\u51fa\u53d1\uff0c\u901a\u8fc7\u57fa\u4e8e\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\u548c\u8282\u70b9\u7f6e\u6362\u7684\u53ef\u63a7\u5bf9\u79f0\u6027\u8c03\u5236\u65b9\u6848\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u653e\u677e\u6a21\u578b\u7684\u7b49\u53d8\u6027\u7ea6\u675f\u3002", "result": "\u5bf9\u79f0\u6027\u7834\u574f\u80fd\u901a\u8fc7\u63d0\u4f9b\u66f4\u7b80\u5355\u7684\u5b66\u4e60\u4fe1\u53f7\u52a0\u901f\u65e9\u671f\u8bad\u7ec3\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\uff08\u6a21\u578b\u91cd\u590d\u751f\u6210\u8bad\u7ec3\u96c6\u4e2d\u7684\u56fe\uff09\u3002\u9002\u5f53\u8c03\u5236\u5bf9\u79f0\u6027\u4fe1\u53f7\u53ef\u4ee5\u5ef6\u8fdf\u8fc7\u62df\u5408\u540c\u65f6\u52a0\u901f\u6536\u655b\uff0c\u4ec5\u7528\u57fa\u7ebf\u8bad\u7ec3\u8f6e\u6570\u768419%\u5c31\u80fd\u8fbe\u5230\u66f4\u5f3a\u7684\u6027\u80fd\u3002", "conclusion": "\u5728\u56fe\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u53ef\u63a7\u65b9\u5f0f\u8c03\u5236\u5bf9\u79f0\u6027\u4fe1\u53f7\u53ef\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u627e\u5230\u4e25\u683c\u7b49\u53d8\u6027\u4e0e\u8bad\u7ec3\u6548\u7387\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u70b9\u3002"}}
{"id": "2602.18131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18131", "abs": "https://arxiv.org/abs/2602.18131", "authors": ["Tom Potter", "Oliver Rhodes"], "title": "Learning Long-Range Dependencies with Temporal Predictive Coding", "comment": null, "summary": "Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u65f6\u95f4\u9884\u6d4b\u7f16\u7801\u4e0e\u8fd1\u4f3c\u5b9e\u65f6\u5faa\u73af\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u7f16\u7801\u5c40\u90e8\u5e76\u884c\u7279\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u5904\u7406\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u95ee\u9898\uff0c\u6027\u80fd\u63a5\u8fd1BPTT\u4f46\u66f4\u8282\u80fd\u3002", "motivation": "\u9884\u6d4b\u7f16\u7801\u5177\u6709\u5c40\u90e8\u5e76\u884c\u7279\u6027\uff0c\u9002\u5408\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u8282\u80fd\u5b9e\u73b0\uff0c\u4f46\u96be\u4ee5\u6709\u6548\u6269\u5c55\u5230\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u3002BPTT\u867d\u4e3b\u5bfcRNN\u8bad\u7ec3\uff0c\u4f46\u5176\u975e\u5c40\u90e8\u8ba1\u7b97\u3001\u7f3a\u4e4f\u7a7a\u95f4\u5e76\u884c\u6027\u3001\u9700\u5b58\u50a8\u5927\u91cf\u6fc0\u6d3b\u5386\u53f2\u5bfc\u81f4\u9ad8\u80fd\u8017\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u9884\u6d4b\u7f16\u7801\u4e0e\u8fd1\u4f3c\u5b9e\u65f6\u5faa\u73af\u5b66\u4e60\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u65f6\u7a7a\u4fe1\u7528\u5206\u914d\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u9884\u6d4b\u7f16\u7801\u7684\u5c40\u90e8\u5e76\u884c\u7279\u6027\uff0c\u540c\u65f6\u80fd\u591f\u5904\u7406\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u4efb\u52a1\u4e0a\u6027\u80fd\u63a5\u8fd1BPTT\u3002\u57281500\u4e07\u53c2\u6570\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u6d4b\u8bd5\u56f0\u60d1\u5ea6\u4e3a7.62\uff08BPTT\u4e3a7.49\uff09\uff0c\u9996\u6b21\u5c06\u65f6\u95f4\u9884\u6d4b\u7f16\u7801\u5e94\u7528\u4e8e\u6b64\u89c4\u6a21\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u5728\u4fdd\u6301\u9884\u6d4b\u7f16\u7801\u5c40\u90e8\u5e76\u884c\u7279\u6027\u7684\u540c\u65f6\u5b66\u4e60\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\uff0c\u4e3a\u66f4\u8282\u80fd\u7684\u5b66\u4e60\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\uff0c\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u80fd\u6548\u63d0\u5347\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.18141", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18141", "abs": "https://arxiv.org/abs/2602.18141", "authors": ["Pierre-Gabriel Berlureau", "Ali Hariri", "Victor Kawasaki-Borruat", "Mia Zosso", "Pierre Vandergheynst"], "title": "Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eBakry-Emery\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684mu-ChebNet\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8282\u70b9\u52bf\u80fd\u8c03\u8282\u4f20\u64ad\u52a8\u529b\u5b66\uff0c\u89e3\u51b3GNN\u957f\u8ddd\u79bb\u4fe1\u606f\u4f20\u64ad\u95ee\u9898\uff0c\u65e0\u9700\u6539\u53d8\u56fe\u62d3\u6251\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u957f\u8ddd\u79bb\u4fe1\u606f\u4f20\u64ad\u4e2d\u5b58\u5728\u8fc7\u5e73\u6ed1\u548c\u8fc7\u538b\u7f29\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5982\u56fe\u53d8\u6362\u5668\u6216\u91cd\u5e03\u7ebf\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u9700\u8981\u6539\u53d8\u56fe\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u4e0d\u6539\u53d8\u62d3\u6251\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165Bakry-Emery\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8282\u70b9\u52bf\u80fd\u6574\u5408\u6269\u6563\u548c\u5e73\u6d41\u8fc7\u7a0b\uff1b\u57fa\u4e8e\u6b64\u5f00\u53d1mu-ChebNet\u8c31\u67b6\u6784\uff0c\u8054\u5408\u5b66\u4e60\u52bf\u80fd\u548c\u5207\u6bd4\u96ea\u592b\u6ee4\u6ce2\u5668\uff0c\u7ed3\u5408\u6d88\u606f\u4f20\u9012\u81ea\u9002\u5e94\u6027\u548c\u8c31\u6548\u7387\u3002", "result": "\u5728\u5408\u6210\u957f\u8ddd\u79bb\u63a8\u7406\u4efb\u52a1\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8def\u7531\u573a\uff0c\u63ed\u793a\u4fe1\u606f\u5982\u4f55\u5728\u56fe\u4e2d\u6d41\u52a8\u3002", "conclusion": "Bakry-Emery\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u4e3a\u81ea\u9002\u5e94\u8c31\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u4e14\u9ad8\u6548\u7684\u57fa\u7840\uff0cmu-ChebNet\u6709\u6548\u89e3\u51b3\u4e86GNN\u957f\u8ddd\u79bb\u4f20\u64ad\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u62d3\u6251\u4e0d\u53d8\u6027\u3002"}}
{"id": "2602.18146", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.18146", "abs": "https://arxiv.org/abs/2602.18146", "authors": ["Lionel Salesses", "Larbi Arbaoui", "Tariq Benamara", "Arnaud Francois", "Caroline Sainvitu"], "title": "Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks", "comment": null, "summary": "Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u590d\u6742\u51e0\u4f55\u4f53\u4e0a\u65f6\u7a7a\u573a\u957f\u671f\u9884\u6d4b\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u591a\u65f6\u95f4\u5c3a\u5ea6\u67b6\u6784\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u589e\u6750\u5236\u9020\u6e29\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u590d\u6742\u51e0\u4f55\u4f53\u4e0a\u65f6\u7a7a\u573a\u7684\u957f\u671f\u51c6\u786e\u9884\u6d4b\u662f\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u589e\u6750\u5236\u9020\u4e2d\uff0c\u6e29\u5ea6\u5386\u53f2\u63a7\u5236\u7740\u7f3a\u9677\u5f62\u6210\u548c\u673a\u68b0\u6027\u80fd\u3002\u9ad8\u4fdd\u771f\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u671f\u6e29\u5ea6\u548c\u68af\u5ea6\u9884\u6d4b\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u591a\u5c3a\u5ea6\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u5728\u4e92\u8865\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fd0\u884c\u7684\u8026\u5408\u6a21\u578b\u3002\u4e24\u4e2a\u6a21\u578b\u90fd\u4f9d\u8d56\u6f5c\u5728\u5faa\u73af\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u7f51\u683c\u4e0a\u7684\u65f6\u7a7a\u52a8\u6001\uff0c\u540c\u65f6\u4f7f\u7528\u53d8\u5206\u56fe\u81ea\u7f16\u7801\u5668\u63d0\u4f9b\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\uff0c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6a21\u62df\u7c89\u672b\u5e8a\u7194\u878d\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u8de8\u4e0d\u540c\u51e0\u4f55\u4f53\u5b9e\u73b0\u51c6\u786e\u4e14\u65f6\u95f4\u7a33\u5b9a\u7684\u957f\u671f\u9884\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u867d\u7136\u662f\u5728\u4e8c\u7ef4\u4e2d\u8bc4\u4f30\u7684\uff0c\u4f46\u5177\u6709\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5177\u6709\u591a\u5c3a\u5ea6\u52a8\u6001\u7684\u7269\u7406\u9a71\u52a8\u7cfb\u7edf\u548c\u4e09\u7ef4\u51e0\u4f55\u4f53\u3002"}}
{"id": "2602.18168", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18168", "abs": "https://arxiv.org/abs/2602.18168", "authors": ["Danning Jing", "Xinhai Chen", "Xifeng Pu", "Jie Hu", "Chao Huang", "Xuguang Chen", "Qinglin Wang", "Jie Liu"], "title": "A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction", "comment": null, "summary": "Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.", "AI": {"tldr": "RGD-Blast\uff1a\u4e00\u79cd\u7528\u4e8e\u9ad8\u4fdd\u771f\u3001\u957f\u671f\u7206\u70b8\u6ce2\u9884\u6d4b\u7684\u9c81\u68d2\u53ef\u6cdb\u5316\u6df1\u5ea6\u4ee3\u7406\u6a21\u578b\uff0c\u76f8\u6bd4\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u52a0\u901f\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u5728\u672a\u89c1\u5efa\u7b51\u5e03\u5c40\u4e0a\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u7206\u70b8\u6ce2\u4f20\u64ad\u7684\u65f6\u7a7a\u52a8\u529b\u5b66\u5efa\u6a21\u9762\u4e34\u9ad8\u5ea6\u975e\u7ebf\u6027\u884c\u4e3a\u3001\u9661\u5ced\u68af\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u5728\u590d\u6742\u57ce\u5e02\u5e03\u5c40\u6216\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7cbe\u5ea6\u4e0b\u964d\uff0c\u4e14\u81ea\u56de\u5f52\u9884\u6d4b\u7b56\u7565\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u5bb9\u6613\u8bef\u5dee\u7d2f\u79ef\u3002", "method": "\u63d0\u51faRGD-Blast\u6a21\u578b\uff0c\u5305\u542b\u591a\u5c3a\u5ea6\u6a21\u5757\u6355\u6349\u5168\u5c40\u6d41\u52a8\u6a21\u5f0f\u548c\u5c40\u90e8\u8fb9\u754c\u4ea4\u4e92\uff0c\u7f13\u89e3\u81ea\u56de\u5f52\u9884\u6d4b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\uff1b\u5f15\u5165\u52a8\u6001-\u9759\u6001\u7279\u5f81\u8026\u5408\u673a\u5236\uff0c\u878d\u5408\u65f6\u53d8\u538b\u529b\u573a\u4e0e\u9759\u6001\u6e90\u548c\u5e03\u5c40\u7279\u5f81\uff0c\u589e\u5f3a\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5b9e\u73b0\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7cbe\u5ea6\u3002\u5728\u672a\u89c1\u5efa\u7b51\u5e03\u5c40\u7684\u6cdb\u5316\u6d4b\u8bd5\u4e2d\uff0c280\u4e2a\u8fde\u7eed\u65f6\u95f4\u6b65\u7684\u5e73\u5747RMSE\u4f4e\u4e8e0.01\uff0cR\u00b2\u8d85\u8fc70.89\u3002\u5728\u4e0d\u540c\u7206\u70b8\u6e90\u4f4d\u7f6e\u548c\u70b8\u836f\u91cd\u91cf\u7684\u8bc4\u4f30\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RGD-Blast\u663e\u8457\u63a8\u8fdb\u4e86\u957f\u671f\u7206\u70b8\u6ce2\u5efa\u6a21\u7684\u6280\u672f\u6c34\u5e73\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5efa\u6a21\u548c\u52a8\u6001-\u9759\u6001\u7279\u5f81\u8026\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u4ee3\u7406\u6a21\u578b\u7684\u7cbe\u5ea6\u4e0b\u964d\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u7206\u70b8\u6ce2\u9884\u6d4b\u3002"}}
{"id": "2602.18181", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18181", "abs": "https://arxiv.org/abs/2602.18181", "authors": ["Jihun Kim", "Namhoon Lee"], "title": "SeedFlood: A Step Toward Scalable Decentralized Training of LLMs", "comment": null, "summary": "This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.", "AI": {"tldr": "SeedFlood\u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u96f6\u9636\u66f4\u65b0\u7684\u79cd\u5b50\u53ef\u91cd\u6784\u7ed3\u6784\uff0c\u4f7f\u6d88\u606f\u5927\u5c0f\u63a5\u8fd1\u96f6\uff0c\u4ece\u800c\u6d88\u9664\u6a21\u578b\u89c4\u6a21\u5e26\u6765\u7684\u901a\u4fe1\u74f6\u9888\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8egossip\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u6d88\u606f\u901a\u4fe1\u6210\u672c\u968f\u6a21\u578b\u89c4\u6a21\u589e\u957f\uff1b2\uff09\u7f51\u7edc\u8df3\u6570\u5bfc\u81f4\u4fe1\u606f\u8870\u51cf\uff0c\u5168\u5c40\u5171\u8bc6\u6548\u7387\u4f4e\u4e0b\u3002\u8fd9\u4f7f\u5f97\u5927\u89c4\u6a21\u6a21\u578b\uff08\u5982\u6570\u5341\u4ebf\u53c2\u6570\uff09\u5728\u6570\u767e\u4e2a\u5ba2\u6237\u7aef\u4e0a\u7684\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002", "method": "SeedFlood\u5229\u7528\u96f6\u9636\u66f4\u65b0\u7684\u79cd\u5b50\u53ef\u91cd\u6784\u7ed3\u6784\uff0c\u4f7f\u6d88\u606f\u5927\u5c0f\u63a5\u8fd1\u96f6\uff0c\u7136\u540e\u901a\u8fc7flooding\u673a\u5236\u5c06\u6d88\u606f\u4f20\u64ad\u5230\u7f51\u7edc\u4e2d\u7684\u6bcf\u4e2a\u5ba2\u6237\u7aef\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u901a\u4fe1\u5f00\u9500\u53d8\u5f97\u53ef\u5ffd\u7565\u4e14\u4e0e\u6a21\u578b\u5927\u5c0f\u65e0\u5173\u3002", "result": "\u5728\u53bb\u4e2d\u5fc3\u5316LLM\u5fae\u8c03\u5b9e\u9a8c\u4e2d\uff0cSeedFlood\u5728\u6cdb\u5316\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8egossip\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u8bbe\u7f6e\u4e0b\u751a\u81f3\u8fbe\u5230\u4e0e\u4e00\u9636\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002", "conclusion": "SeedFlood\u901a\u8fc7\u6d88\u9664\u6a21\u578b\u89c4\u6a21\u5e26\u6765\u7684\u901a\u4fe1\u74f6\u9888\uff0c\u4f7f\u5f97\u5728\u4e4b\u524d\u88ab\u8ba4\u4e3a\u4e0d\u5207\u5b9e\u9645\u7684\u573a\u666f\uff08\u5982\u6570\u767e\u4e2a\u5ba2\u6237\u7aef\u4e0a\u7684\u6570\u5341\u4ebf\u53c2\u6570\u6a21\u578b\uff09\u4e2d\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18196", "abs": "https://arxiv.org/abs/2602.18196", "authors": ["Xiuying Wei", "Caglar Gulcehre"], "title": "RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference", "comment": null, "summary": "Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.", "AI": {"tldr": "RAT+\u662f\u4e00\u79cd\u901a\u8fc7\u5bc6\u96c6\u9884\u8bad\u7ec3\u7ed3\u5408\u5168\u5e8f\u5217\u5faa\u73af\u7684\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u5207\u6362\u4e3a\u6269\u5f20\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4ec5\u9700\u5c11\u91cf\u9002\u5e94\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u5bc6\u96c6\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7ed3\u6784\u5316\u6269\u5f20\u6ce8\u610f\u529b\u867d\u7136\u5177\u6709\u63a8\u7406\u65f6\u6548\u7387\u4f18\u52bf\uff08\u901a\u8fc7\u6269\u5f20\u56e0\u5b50D\u51cf\u5c11FLOPs\u548cKV\u7f13\u5b58\u5927\u5c0f\uff09\uff0c\u4f46\u76f4\u63a5\u5c06\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u6a21\u578b\u7a00\u758f\u5316\u4e3a\u6269\u5f20\u6a21\u5f0f\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u51c6\u786e\u6027\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5bc6\u96c6\u9884\u8bad\u7ec3\u4f18\u52bf\uff0c\u53c8\u80fd\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u5207\u6362\u4e3a\u9ad8\u6548\u7a00\u758f\u6a21\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "RAT+\u67b6\u6784\u5728\u5bc6\u96c6\u9884\u8bad\u7ec3\u65f6\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u52a0\u5165\u5168\u5e8f\u5217\u5faa\u73af\u548c\u4e3b\u52a8\u5faa\u73af\u5b66\u4e60\u3002\u5355\u4e2aRAT+\u6a21\u578b\u7ecf\u8fc7\u4e00\u6b21\u5bc6\u96c6\u9884\u8bad\u7ec3\u540e\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u5207\u6362\u4e3a\u6269\u5f20\u6ce8\u610f\u529b\u6a21\u5f0f\uff08\u53ef\u9009\u5e26\u5c40\u90e8\u7a97\u53e3\uff09\u6216\u6df7\u5408\u5c42/\u5934\u7ec4\u5408\uff0c\u4ec5\u970010\u4ebftoken\u7684\u77ed\u9002\u5e94\u8fc7\u7a0b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5355\u72ec\u7684\u7a00\u758f\u6a21\u578b\u3002", "result": "\u572815\u4ebf\u53c2\u6570\u30011000\u4ebftoken\u8bad\u7ec3\u4e0b\uff0cRAT+\u5728\u6269\u5f20\u56e0\u5b5016\u65f6\u63a5\u8fd1\u5bc6\u96c6\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5728\u6269\u5f20\u56e0\u5b5064\u65f6\u5728\u5e38\u8bc6\u63a8\u7406\u548cLongBench\u4efb\u52a1\u4e0a\u5206\u522b\u4e0b\u964d\u7ea62-3\u4e2a\u70b9\u3002\u6b64\u5916\uff0cRAT+\u5728\u7a00\u758f\u5316\u4e3atop-k\u5757\u6ce8\u610f\u529b\u65f6\u4f18\u4e8e\u6807\u51c6\u6ce8\u610f\u529b\u3002\u6269\u5c55\u523026\u4ebf\u53c2\u6570\u548c2000\u4ebftoken\u65f6\u89c2\u5bdf\u5230\u76f8\u540c\u8d8b\u52bf\u3002", "conclusion": "RAT+\u901a\u8fc7\u5bc6\u96c6\u9884\u8bad\u7ec3\u7ed3\u5408\u5faa\u73af\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u65f6\u7075\u6d3b\u5207\u6362\u4e3a\u9ad8\u6548\u7a00\u758f\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u4ec5\u9700\u5c11\u91cf\u9002\u5e94\u5373\u53ef\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u65b9\u6848\u3002"}}
{"id": "2602.18216", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18216", "abs": "https://arxiv.org/abs/2602.18216", "authors": ["Georgi Hrusanov", "Oliver Y. Ch\u00e9n", "Julien S. Bodelet"], "title": "Generative Model via Quantile Assignment", "comment": null, "summary": "Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.", "AI": {"tldr": "NeuroSQL\u662f\u4e00\u79cd\u65e0\u9700\u8f85\u52a9\u7f51\u7edc\u7684\u65b0\u578b\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u7684\u6e10\u8fd1\u8fd1\u4f3c\u5b66\u4e60\u9690\u53d8\u91cf\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff08\u5982VAE\u548cGAN\uff09\u4f9d\u8d56\u7f16\u7801\u5668\u6216\u5224\u522b\u5668\u7b49\u8f85\u52a9\u7f51\u7edc\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u5b58\u5728\u6a21\u5f0f\u5d29\u6e83\u7b49\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u751f\u6210\u8303\u5f0f\u3002", "method": "NeuroSQL\u901a\u8fc7\u6e10\u8fd1\u8fd1\u4f3c\u5c06\u9690\u53d8\u91cf\u8868\u793a\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u7684\u89e3\uff0c\u901a\u8fc7\u6c42\u89e3\u7ebf\u6027\u5206\u914d\u95ee\u9898\u5b66\u4e60\u9690\u53d8\u91cf\uff0c\u7136\u540e\u5c06\u9690\u4fe1\u606f\u4f20\u9012\u7ed9\u72ec\u7acb\u7684\u751f\u6210\u5668\uff0c\u65e0\u9700\u8f85\u52a9\u7f51\u7edc\u3002", "result": "\u5728MNIST\u3001CelebA\u3001AFHQ\u548cOASIS\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cNeuroSQL\u76f8\u6bd4VAE\u3001GAN\u548c\u6269\u6563\u6a21\u578b\uff1a1\uff09\u56fe\u50cf\u8d28\u91cf\u66f4\u597d\uff08\u50cf\u7d20\u8ddd\u79bb\u66f4\u5c0f\uff0c\u611f\u77e5/\u7ed3\u6784\u4fdd\u771f\u5ea6\u66f4\u9ad8\uff09\uff1b2\uff09\u8bad\u7ec3\u65f6\u95f4\u6700\u77ed\uff1b3\uff09\u5728\u6709\u9650\u8bad\u7ec3\u6837\u672c\u4e0b\u4ecd\u80fd\u6709\u6548\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "conclusion": "NeuroSQL\u901a\u8fc7\u5206\u4f4d\u6570\u5206\u914d\u800c\u975e\u7f16\u7801\u5668\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u7a33\u5b9a\u4e14\u9c81\u68d2\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u4fe1\u606f\u635f\u5931\u6700\u5c0f\uff0c\u4e3a\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18227", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18227", "abs": "https://arxiv.org/abs/2602.18227", "authors": ["Redwanul Karim", "Changhun Kim", "Timon Conrad", "Nora Gourmelon", "Julian Oelhaf", "David Riebesel", "Tom\u00e1s Arias-Vergara", "Andreas Maier", "Johann J\u00e4ger", "Siming Bayer"], "title": "Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction", "comment": null, "summary": "Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5c06LoRA\uff08\u4f4e\u79e9\u9002\u5e94\uff09\u4e0e\u9009\u62e9\u6027\u89e3\u51bb\u9884\u6d4b\u5934\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u7269\u7406\u4fe1\u606f\u81ea\u6ce8\u610f\u529bGNN\uff0c\u4ee5\u89e3\u51b3\u4ea4\u6d41\u6f6e\u6d41\u9884\u6d4b\u5728\u7535\u538b\u57df\u8f6c\u79fb\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u4fe1\u606f\u56fe\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u5668\u5728\u8de8\u7535\u538b\u57df\uff08\u4ece\u4e2d\u538b\u5230\u9ad8\u538b\uff09\u90e8\u7f72\u65f6\uff0c\u901a\u5e38\u9700\u8981\u5b8c\u5168\u5fae\u8c03\uff0c\u8fd9\u5bfc\u81f4\u9ad8\u91cd\u8bad\u7ec3\u6210\u672c\uff0c\u5e76\u4e14\u5728\u76ee\u6807\u57df\u9002\u5e94\u548c\u6e90\u57df\u4fdd\u7559\u4e4b\u95f4\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u6743\u8861\u4e0a\u63a7\u5236\u6709\u9650\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5c06LoRA\u5e94\u7528\u4e8e\u6ce8\u610f\u529b\u6295\u5f71\u5c42\uff0c\u540c\u65f6\u9009\u62e9\u6027\u89e3\u51bb\u9884\u6d4b\u5934\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u7684\u635f\u5931\u51fd\u6570\u9f13\u52b1\u57fa\u5c14\u970d\u592b\u4e00\u81f4\u6027\u884c\u4e3a\uff0c\u540c\u65f6\u5c06\u9002\u5e94\u9650\u5236\u5728\u4f4e\u79e9\u66f4\u65b0\u3002", "result": "\u63d0\u51fa\u7684LoRA+PHead\u65b9\u6cd5\u5728\u591a\u4e2a\u7535\u7f51\u62d3\u6251\u4e2d\uff0c\u4ee585.46%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c11\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u5168\u5fae\u8c03\u7684\u7cbe\u5ea6\uff08\u76ee\u6807\u57dfRMSE\u5dee\u8ddd\u4e3a2.6\u00d710^-4\uff09\uff0c\u7269\u7406\u6b8b\u5dee\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u5f53\uff0c\u4f46\u5728\u57df\u8f6c\u79fb\u4e0b\u4e2d\u538b\u6e90\u4fdd\u7559\u51cf\u5c11\u4e864.7\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7535\u538b\u57df\u8f6c\u79fb\u4e0b\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\uff0c\u80fd\u591f\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u7684\u4ea4\u6d41\u6f6e\u6d41\u4f30\u8ba1\uff0c\u4e3a\u8de8\u7535\u538b\u57df\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18248", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.18248", "abs": "https://arxiv.org/abs/2602.18248", "authors": ["Pietro Sittoni", "Emanuele Zangrando", "Angelo A. Casulli", "Nicola Guglielmi", "Francesco Tudisco"], "title": "Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver", "comment": null, "summary": "Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.", "AI": {"tldr": "\u63d0\u51faNeural-HSS\u67b6\u6784\uff0c\u57fa\u4e8e\u5206\u5c42\u534a\u53ef\u5206\u77e9\u9635\u7ed3\u6784\uff0c\u9488\u5bf9\u692d\u5706\u578bPDEs\u5b9e\u73b0\u53c2\u6570\u548c\u6570\u636e\u9ad8\u6548\u5b66\u4e60\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u4ecd\u4fdd\u6301\u7cbe\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u6c42\u89e3PDEs\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u751f\u6210\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u8ba1\u7b97\u6210\u672c\u4ecd\u7136\u5f88\u9ad8\uff0c\u9650\u5236\u4e86\u5173\u952e\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u53d7\u692d\u5706PDEs\u683c\u6797\u51fd\u6570\u7ed3\u6784\u542f\u53d1\uff0c\u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u534a\u53ef\u5206\u77e9\u9635\u7ed3\u6784\u7684Neural-HSS\u67b6\u6784\uff0c\u7406\u8bba\u5206\u6790\u5176\u7cbe\u786e\u6027\uff0c\u5e76\u4e0e\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u5c42\u548c\u5377\u79ef\u5c42\u5efa\u7acb\u8054\u7cfb\u3002", "result": "\u5728\u4e09\u767e\u4e07\u70b9\u7f51\u683c\u7684\u4e09\u7ef4\u6cca\u677e\u65b9\u7a0b\u4e0a\u9a8c\u8bc1\u4e86\u6570\u636e\u6548\u7387\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u5b66\u4e60\u7535\u78c1\u5b66\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u751f\u7269\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u7684PDEs\u6570\u636e\u3002", "conclusion": "Neural-HSS\u67b6\u6784\u4e3a\u692d\u5706\u578bPDEs\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u548c\u6570\u636e\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7406\u8bba\u7cbe\u786e\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.18250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18250", "abs": "https://arxiv.org/abs/2602.18250", "authors": ["Yves Ruffenach"], "title": "Variational Distributional Neuron", "comment": "29 pages, 7 figures. Code available at GitHub (link in paper)", "summary": "We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This \"contraction\" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze \"collapse\" modes and the conditions for a \"living neuron\", then extend the contribution over time via autoregressive priors over the latent, per unit.", "AI": {"tldr": "\u63d0\u51fa\u53d8\u5206\u5206\u5e03\u795e\u7ecf\u5143\u7684\u6982\u5ff5\u9a8c\u8bc1\uff1a\u5c06\u795e\u7ecf\u5143\u8bbe\u8ba1\u4e3aVAE\u6a21\u5757\uff0c\u5305\u542b\u5148\u9a8c\u3001\u644a\u9500\u540e\u9a8c\u548c\u5c40\u90e8ELBO\uff0c\u4f7f\u795e\u7ecf\u5143\u4e0d\u518d\u662f\u786e\u5b9a\u6027\u6807\u91cf\u800c\u662f\u5206\u5e03\uff0c\u8ba1\u7b97\u53d8\u4e3a\u5728\u7ea6\u675f\u4e0b\u6536\u7f29\u53ef\u80fd\u6027\u7a7a\u95f4\u3002", "motivation": "\u89e3\u51b3\u7ed3\u6784\u5f20\u529b\uff1a\u5e8f\u5217\u751f\u6210\u4e2d\u56e0\u679c\u6027\u4e3b\u8981\u5728\u7b26\u53f7\u7a7a\u95f4\u7ec4\u7ec7\uff0c\u6f5c\u5728\u53d8\u91cf\u5e38\u4e3a\u8f85\u52a9\u89d2\u8272\uff0c\u800c\u786e\u5b9a\u6027\u89e3\u7801\u5668\u4e3b\u5bfc\u6709\u6548\u52a8\u6001\uff1b\u540c\u65f6\u6982\u7387\u6f5c\u5728\u6a21\u578b\u80fd\u6355\u6349\u53d8\u5316\u56e0\u7d20\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u4e0d\u786e\u5b9a\u6027\u901a\u5e38\u7531\u5168\u5c40\u6216\u53c2\u6570\u673a\u5236\u627f\u8f7d\uff0c\u57fa\u672c\u8ba1\u7b97\u5355\u5143\u4ecd\u4f20\u64ad\u6807\u91cf\u3002\u6838\u5fc3\u95ee\u9898\uff1a\u5982\u679c\u4e0d\u786e\u5b9a\u6027\u662f\u8ba1\u7b97\u7684\u5185\u5728\u5c5e\u6027\uff0c\u4e3a\u4f55\u8ba1\u7b97\u5355\u5143\u4e0d\u663e\u5f0f\u627f\u8f7d\u5b83\uff1f", "method": "\u8bbe\u8ba1\u53d8\u5206\u5206\u5e03\u795e\u7ecf\u5143\u4f5c\u4e3aVAE\u6a21\u5757\uff0c\u6bcf\u4e2a\u795e\u7ecf\u5143\u53c2\u6570\u5316\u540e\u9a8c\u5206\u5e03\uff0c\u4f20\u64ad\u91cd\u53c2\u6570\u5316\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8ELBO\u7684KL\u9879\u8fdb\u884c\u6b63\u5219\u5316\u3002\u5206\u6790\"\u5d29\u6e83\"\u6a21\u5f0f\u548c\"\u6d3b\u795e\u7ecf\u5143\"\u6761\u4ef6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u5148\u9a8c\u5728\u65f6\u95f4\u4e0a\u6269\u5c55\u8d21\u732e\u3002", "result": "\u63d0\u51fa\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5c55\u793a\u795e\u7ecf\u5143\u5982\u4f55\u4ece\u786e\u5b9a\u6027\u6807\u91cf\u8f6c\u53d8\u4e3a\u5206\u5e03\u5b9e\u4f53\uff0c\u8ba1\u7b97\u4ece\u503c\u4f20\u64ad\u8f6c\u53d8\u4e3a\u5728\u7ea6\u675f\u4e0b\u6536\u7f29\u53ef\u80fd\u6027\u7a7a\u95f4\u3002\u901a\u8fc7\u5c40\u90e8\u7ea6\u675f\u53ef\u6d4b\u8bd5\u8fd9\u79cd\"\u6536\u7f29\"\uff0c\u5e76\u901a\u8fc7\u5185\u90e8\u5ea6\u91cf\u76d1\u63a7\u3002", "conclusion": "\u5206\u5e03\u795e\u7ecf\u5143\u4e3a\u89e3\u51b3\u8ba1\u7b97\u4e2d\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u4e24\u4e2a\u8f74\u7ebf\u5c55\u5f00\uff1a(1)\u6982\u7387\u7ea6\u675f\u7684\u7ec4\u5408\u9700\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\uff1b(2)\u7c92\u5ea6\u95ee\u9898\uff1a\u5982\u679c\u63a8\u65ad\u662f\u5728\u7ea6\u675f\u4e0b\u5206\u5e03\u534f\u5546\uff0c\u539f\u59cb\u5355\u5143\u5e94\u4fdd\u6301\u786e\u5b9a\u6027\u8fd8\u662f\u53d8\u4e3a\u5206\u5e03\u6027\uff1f\u8fd9\u4e3a\u795e\u7ecf\u8ba1\u7b97\u57fa\u7840\u5e26\u6765\u65b0\u601d\u8003\u3002"}}
{"id": "2602.18253", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18253", "abs": "https://arxiv.org/abs/2602.18253", "authors": ["Xabier de Zuazo", "Vincenzo Verbeni", "Eva Navas", "Ibon Saratxaga", "Mathieu Bourguignon", "Nicola Molinaro"], "title": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data", "comment": "6 pages, 3 figures, 3 tables, submitted to Interspeech 2026", "summary": "Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86MEG\u8bed\u97f3\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u548c\u8de8\u4efb\u52a1\u89e3\u7801\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u5f0f\u5728\u611f\u77e5\u4e0e\u4ea7\u751f\u4efb\u52a1\u95f4\u5b9e\u73b0\u6709\u6548\u89e3\u7801\uff0c\u8bc1\u5b9e\u4e86\u5171\u4eab\u795e\u7ecf\u8868\u5f81\u7684\u5b58\u5728\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u8111\u673a\u63a5\u53e3\u4e2d\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63a2\u7d22\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u5982\u4f55\u63d0\u5347\u795e\u7ecf\u89e3\u7801\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u611f\u77e5\u4e0e\u4ea7\u751f\u4efb\u52a1\u95f4\u662f\u5426\u5b58\u5728\u5171\u4eab\u7684\u795e\u7ecf\u8868\u5f81\u3002", "method": "\u4f7f\u7528Conformer-based\u6a21\u578b\uff0c\u5148\u5728\u5355\u4e2a\u88ab\u8bd5\u768450\u5c0f\u65f6\u542c\u8bed\u97f3\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u572818\u540d\u88ab\u8bd5\u4e0a\u4ec5\u7528\u6bcf\u4eba5\u5206\u949f\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u5206\u522b\u5728\u611f\u77e5\u548c\u4ea7\u751f\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u8de8\u4efb\u52a1\u89e3\u7801\u5206\u6790\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u5e26\u6765\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff1a\u4efb\u52a1\u5185\u51c6\u786e\u7387\u63d0\u9ad81-4%\uff0c\u8de8\u4efb\u52a1\u63d0\u5347\u8fbe5-6%\u3002\u9884\u8bad\u7ec3\u4e0d\u4ec5\u63d0\u5347\u5404\u4efb\u52a1\u6027\u80fd\uff0c\u8fd8\u80fd\u5b9e\u73b0\u611f\u77e5\u4e0e\u4ea7\u751f\u4efb\u52a1\u95f4\u7684\u53ef\u9760\u8de8\u4efb\u52a1\u89e3\u7801\u3002\u5173\u952e\u53d1\u73b0\u662f\uff0c\u5728\u8bed\u97f3\u4ea7\u751f\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u89e3\u7801\u88ab\u52a8\u542c\u8bed\u97f3\u4efb\u52a1\uff0c\u8bc1\u660e\u5b66\u4e60\u5230\u7684\u8868\u5f81\u53cd\u6620\u4e86\u5171\u4eab\u795e\u7ecf\u8fc7\u7a0b\u800c\u975e\u4efb\u52a1\u7279\u5f02\u6027\u8fd0\u52a8\u6d3b\u52a8\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728MEG\u8bed\u97f3\u89e3\u7801\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u4e0e\u4ea7\u751f\u4efb\u52a1\u95f4\u5b58\u5728\u5171\u4eab\u795e\u7ecf\u8868\u5f81\uff0c\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u8bed\u97f3\u8111\u673a\u63a5\u53e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u786e\u8ba4\u4e86\u5b66\u4e60\u5230\u7684\u8868\u5f81\u53cd\u6620\u7684\u662f\u8bed\u8a00\u5904\u7406\u7684\u901a\u7528\u795e\u7ecf\u673a\u5236\u800c\u975e\u7279\u5b9a\u4efb\u52a1\u6d3b\u52a8\u3002"}}
{"id": "2602.18266", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18266", "abs": "https://arxiv.org/abs/2602.18266", "authors": ["Stefan Wahl", "Raphaela Schenk", "Ali Farnoud", "Jakob H. Macke", "Daniel Gedon"], "title": "A Probabilistic Framework for LLM-Based Model Discovery", "comment": null, "summary": "Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faModelSMC\u7b97\u6cd5\uff0c\u5c06\u6a21\u578b\u53d1\u73b0\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6982\u7387\u63a8\u65ad\u95ee\u9898\uff0c\u4f7f\u7528\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u91c7\u6837\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u53d1\u73b0\u673a\u5236\u6027\u6a21\u62df\u5668\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u6a21\u578b\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u6982\u7387\u5f62\u5f0f\u5316\u3002\u4f5c\u8005\u5e0c\u671b\u4e3a\u6a21\u578b\u53d1\u73b0\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u7387\u63a8\u65ad\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u63d0\u6848\u3001\u7ec6\u5316\u548c\u9009\u62e9\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faModelSMC\u7b97\u6cd5\uff0c\u57fa\u4e8e\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u91c7\u6837\u3002\u5c06\u5019\u9009\u6a21\u578b\u8868\u793a\u4e3a\u7c92\u5b50\uff0c\u7531LLM\u8fed\u4ee3\u63d0\u51fa\u548c\u7ec6\u5316\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u4f3c\u7136\u7684\u6807\u51c6\u8fdb\u884c\u52a0\u6743\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u79d1\u5b66\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u5177\u6709\u53ef\u89e3\u91ca\u673a\u5236\u7684\u6a21\u578b\uff0c\u5e76\u6539\u5584\u4e86\u540e\u9a8c\u9884\u6d4b\u68c0\u67e5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u6a21\u578b\u53d1\u73b0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6982\u7387\u89c6\u89d2\uff0c\u5c06\u6a21\u578b\u53d1\u73b0\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6982\u7387\u63a8\u65ad\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7edf\u4e00\u7684\u6846\u67b6\u3002"}}
{"id": "2602.18348", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18348", "abs": "https://arxiv.org/abs/2602.18348", "authors": ["Matheus Camilo da Silva", "Leonardo Arrighi", "Ana Carolina Lorena", "Sylvio Barbon Junior"], "title": "Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering", "comment": null, "summary": "AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76AutoClustering\u5143\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u5206\u679022\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u5143\u7279\u5f81\u3001\u5e94\u7528\u5168\u5c40\u89e3\u91ca\u6280\u672f\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u4f7f\u7528\u5c40\u90e8\u89e3\u91ca\u5de5\u5177\u5206\u6790\u5177\u4f53\u805a\u7c7b\u51b3\u7b56\uff0c\u4e3a\u66f4\u53ef\u89e3\u91ca\u7684AutoML\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u5f53\u524dAutoClustering\u7cfb\u7edf\u867d\u7136\u6027\u80fd\u826f\u597d\uff0c\u4f46\u5176\u63a8\u8350\u7ed3\u679c\u96be\u4ee5\u89e3\u91ca\uff1a\u6570\u636e\u96c6\u5143\u7279\u5f81\u5bf9\u7b97\u6cd5\u548c\u8d85\u53c2\u6570\u9009\u62e9\u7684\u5f71\u54cd\u901a\u5e38\u4e0d\u900f\u660e\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u9760\u6027\u3001\u504f\u5dee\u8bca\u65ad\u548c\u9ad8\u6548\u7684\u5143\u7279\u5f81\u5de5\u7a0b\uff0c\u9700\u8981\u63d0\u9ad8\u65e0\u76d1\u7763\u5b66\u4e60\u81ea\u52a8\u5316\u7684\u51b3\u7b56\u900f\u660e\u5ea6\u3002", "method": "1) \u56de\u987e22\u79cd\u73b0\u6709\u65b9\u6cd5\u5e76\u5c06\u5143\u7279\u5f81\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\uff1b2) \u5e94\u7528\u5168\u5c40\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff08\u51b3\u7b56\u8c13\u8bcd\u56fe\uff09\u8bc4\u4f30\u5143\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u91cd\u8981\u6027\uff1b3) \u4f7f\u7528\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff08\u5982SHAP\uff09\u5206\u6790\u5177\u4f53\u7684\u805a\u7c7b\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5143\u7279\u5f81\u76f8\u5173\u6027\u7684\u4e00\u81f4\u6a21\u5f0f\uff0c\u8bc6\u522b\u51fa\u5f53\u524d\u5143\u5b66\u4e60\u7b56\u7565\u4e2d\u53ef\u80fd\u626d\u66f2\u63a8\u8350\u7ed3\u679c\u7684\u7ed3\u6784\u6027\u5f31\u70b9\uff0c\u5e76\u4e3a\u66f4\u53ef\u89e3\u91ca\u7684AutoML\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u63d0\u9ad8\u65e0\u76d1\u7763\u5b66\u4e60\u81ea\u52a8\u5316\u7684\u51b3\u7b56\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u8df5\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u5143\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5728AutoClustering\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u66f4\u900f\u660e\u3001\u53ef\u9760\u7684AutoML\u7cfb\u7edf\u8bbe\u8ba1\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.18403", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18403", "abs": "https://arxiv.org/abs/2602.18403", "authors": ["Orfeas Bourchas", "George Papalambrou"], "title": "Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study", "comment": "Accepted to the KGML Bridge at AAAI 2026 (non-archival)", "summary": "Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u6df7\u5408\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u8239\u8236\u4e3b\u673a\u529f\u7387\u9884\u6d4b\uff0c\u901a\u8fc7\u7269\u7406\u57fa\u7ebf\u6a21\u578b\u6355\u6349\u529f\u7387-\u901f\u5ea6\u57fa\u672c\u5173\u7cfb\uff0c\u518d\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6b8b\u5dee\uff0c\u63d0\u9ad8\u5916\u63a8\u80fd\u529b\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5982SVM\u3001ANN\u3001\u6811\u6a21\u578b\u7b49\uff09\u867d\u7136\u80fd\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u9075\u5faa\u87ba\u65cb\u6868\u5b9a\u5f8b\u4e2d\u529f\u7387\u4e0e\u901f\u5ea6\u7684\u57fa\u672c\u7269\u7406\u5173\u7cfb\uff0c\u5bfc\u81f4\u5728\u8bad\u7ec3\u8303\u56f4\u5916\u7684\u5916\u63a8\u6027\u80fd\u5dee\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u6570\u636e\u9a71\u52a8\u4f18\u52bf\u53c8\u80fd\u5c0a\u91cd\u7269\u7406\u7ea6\u675f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5efa\u6a21\u6846\u67b6\uff1a1\uff09\u7269\u7406\u57fa\u7ebf\u7ec4\u4ef6\u57fa\u4e8e\u9759\u6c34\u529f\u7387\u66f2\u7ebf\u516c\u5f0fP=cV^n\uff0c\u6355\u6349\u529f\u7387\u4e0e\u901f\u5ea6\u7684\u4e3b\u8981\u4f9d\u8d56\u5173\u7cfb\uff1b2\uff09\u6570\u636e\u9a71\u52a8\u7ec4\u4ef6\uff08XGBoost\u3001\u7b80\u5355\u795e\u7ecf\u7f51\u7edc\u3001\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edcPINN\uff09\u8bad\u7ec3\u9884\u6d4b\u6b8b\u5dee\u529f\u7387\uff0c\u8868\u793a\u7531\u73af\u5883\u548c\u64cd\u4f5c\u6761\u4ef6\u5f15\u8d77\u7684\u504f\u5dee\u3002\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7ea6\u675f\u4e3a\u6b8b\u5dee\u4fee\u6b63\uff0c\u7b80\u5316\u5b66\u4e60\u8fc7\u7a0b\u5e76\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\uff0c\u6df7\u5408\u6a21\u578b\u5728\u7a00\u758f\u6570\u636e\u533a\u57df\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u57fa\u7ebf\uff0c\u5728\u6570\u636e\u5bc6\u96c6\u533a\u57df\u4fdd\u6301\u76f8\u4f3c\u6027\u80fd\u3002\u8be5\u6846\u67b6\u4e3a\u8239\u8236\u6027\u80fd\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u5de5\u5177\u3002", "conclusion": "\u6df7\u5408\u5efa\u6a21\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7269\u7406\u77e5\u8bc6\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8239\u8236\u4e3b\u673a\u529f\u7387\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u5929\u6c14\u8def\u7531\u3001\u7eb5\u503e\u4f18\u5316\u548c\u80fd\u6548\u89c4\u5212\u7b49\u5e94\u7528\u3002"}}
{"id": "2602.18428", "categories": ["cs.LG", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.18428", "abs": "https://arxiv.org/abs/2602.18428", "authors": ["Mojtaba Sahraee-Ardakan", "Mauricio Delbracio", "Peyman Milanfar"], "title": "The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning", "comment": null, "summary": "Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\\text{marg}}(\\mathbf{u}) = -\\log p(\\mathbf{u})$, where $p(\\mathbf{u}) = \\int p(\\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u81ea\u4e3b\u751f\u6210\u6a21\u578b\uff08\u5982\u5747\u8861\u5339\u914d\u548c\u76f2\u6269\u6563\uff09\u901a\u8fc7\u8fb9\u9645\u80fd\u91cf\u6d41\u8fdb\u884c\u91c7\u6837\uff0c\u800c\u975e\u7b80\u5355\u7684\u76f2\u53bb\u566a\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u4e0d\u53ef\u77e5\u6a21\u578b\u4e2d\u68af\u5ea6\u53d1\u6563\u7684\u7406\u8bba\u6096\u8bba\u3002", "motivation": "\u81ea\u4e3b\u751f\u6210\u6a21\u578b\uff08\u65e0\u9700\u566a\u58f0\u7ea7\u522b\u6761\u4ef6\uff09\u6311\u6218\u4e86\u4f20\u7edf\u8303\u5f0f\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u6839\u672c\u6096\u8bba\uff1a\u5f53\u566a\u58f0\u7ea7\u522b\u88ab\u89c6\u4e3a\u968f\u673a\u53d8\u91cf\u65f6\uff0c\u6a21\u578b\u4f18\u5316\u7684\u5e95\u5c42\u666f\u89c2\u662f\u4ec0\u4e48\uff1f\u6709\u754c\u3001\u566a\u58f0\u4e0d\u53ef\u77e5\u7684\u7f51\u7edc\u5982\u4f55\u5728\u6570\u636e\u6d41\u5f62\u9644\u8fd1\u4fdd\u6301\u7a33\u5b9a\uff08\u901a\u5e38\u68af\u5ea6\u4f1a\u53d1\u6563\uff09\uff1f", "method": "\u5f62\u5f0f\u5316\u8fb9\u9645\u80fd\u91cf $E_{\\text{marg}}(\\mathbf{u}) = -\\log p(\\mathbf{u})$\uff0c\u5176\u4e2d $p(\\mathbf{u})$ \u662f\u566a\u58f0\u6570\u636e\u7684\u8fb9\u9645\u5bc6\u5ea6\u3002\u8bc1\u660e\u81ea\u4e3b\u6a21\u578b\u7684\u751f\u6210\u662f\u8fb9\u9645\u80fd\u91cf\u4e0a\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\u3002\u901a\u8fc7\u76f8\u5bf9\u80fd\u91cf\u5206\u89e3\uff0c\u5c55\u793a\u5b66\u4e60\u7684\u65f6\u95f4\u4e0d\u53d8\u573a\u9690\u5f0f\u5305\u542b\u5c40\u90e8\u5171\u5f62\u5ea6\u91cf\uff0c\u62b5\u6d88\u51e0\u4f55\u5947\u70b9\u3002", "result": "\u63ed\u793a\u4e86\u566a\u58f0\u9884\u6d4b\u53c2\u6570\u5316\u5b58\u5728\"Jensen Gap\"\uff0c\u4f1a\u653e\u5927\u4f30\u8ba1\u8bef\u5dee\uff0c\u5bfc\u81f4\u786e\u5b9a\u6027\u76f2\u6a21\u578b\u707e\u96be\u6027\u5931\u8d25\u3002\u800c\u901f\u5ea6\u53c2\u6570\u5316\u6ee1\u8db3\u6709\u754c\u589e\u76ca\u6761\u4ef6\uff0c\u5c06\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u5438\u6536\u4e3a\u5e73\u6ed1\u51e0\u4f55\u6f02\u79fb\uff0c\u56e0\u6b64\u5177\u6709\u56fa\u6709\u7a33\u5b9a\u6027\u3002", "conclusion": "\u81ea\u4e3b\u751f\u6210\u6a21\u578b\u901a\u8fc7\u8fb9\u9645\u80fd\u91cf\u6d41\u8fdb\u884c\u91c7\u6837\uff0c\u800c\u975e\u7b80\u5355\u7684\u76f2\u53bb\u566a\u3002\u901f\u5ea6\u53c2\u6570\u5316\u5177\u6709\u56fa\u6709\u7a33\u5b9a\u6027\uff0c\u800c\u566a\u58f0\u9884\u6d4b\u53c2\u6570\u5316\u5b58\u5728\u7406\u8bba\u7f3a\u9677\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u7a33\u5b9a\u3001\u566a\u58f0\u4e0d\u53ef\u77e5\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.18435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18435", "abs": "https://arxiv.org/abs/2602.18435", "authors": ["Aggelos Semoglou", "John Pavlopoulos"], "title": "Assigning Confidence: K-partition Ensembles", "comment": "31 pages including appendix", "summary": "Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.", "AI": {"tldr": "CAKE\u6846\u67b6\u901a\u8fc7\u805a\u7c7b\u96c6\u6210\u8ba1\u7b97\u5206\u914d\u7a33\u5b9a\u6027\u548c\u5c40\u90e8\u51e0\u4f55\u62df\u5408\u4e00\u81f4\u6027\uff0c\u4e3a\u6bcf\u4e2a\u70b9\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u8bc6\u522b\u6a21\u7cca\u70b9\u548c\u7a33\u5b9a\u6838\u5fc3\u6210\u5458\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5355\u4e2a\u5206\u914d\u53ef\u9760\u6027\u7684\u8bc4\u4f30\uff0c\u8bca\u65ad\u6307\u6807\u4e3b\u8981\u53cd\u6620\u5168\u5c40\u8d28\u91cf\u800c\u975e\u7279\u5b9a\u5b9e\u4f8b\u7684\u7f6e\u4fe1\u5ea6\u3002\u5206\u914d\u7ea7\u7684\u4e0d\u7a33\u5b9a\u6027\u4f1a\u5f71\u54cd\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u800c\u96c6\u6210\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u4f46\u7f3a\u4e4f\u7ed3\u5408\u8de8\u8fd0\u884c\u4e00\u81f4\u6027\u548c\u5b66\u4e60\u5230\u7684\u805a\u7c7b\u7ed3\u6784\u51e0\u4f55\u652f\u6301\u7684\u70b9\u7ea7\u7f6e\u4fe1\u5ea6\u91cf\u5316\u5de5\u5177\u3002", "method": "\u63d0\u51faCAKE\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u96c6\u6210\u8ba1\u7b97\u4e24\u4e2a\u4e92\u8865\u7edf\u8ba1\u91cf\uff1a\u5206\u914d\u7a33\u5b9a\u6027\u548c\u5c40\u90e8\u51e0\u4f55\u62df\u5408\u4e00\u81f4\u6027\uff0c\u5e76\u5c06\u5b83\u4eec\u7ec4\u5408\u6210[0,1]\u8303\u56f4\u5185\u7684\u5355\u4e00\u53ef\u89e3\u91ca\u8bc4\u5206\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eCAKE\u5728\u566a\u58f0\u4e0b\u4fdd\u6301\u6709\u6548\uff0c\u5e76\u80fd\u533a\u5206\u7a33\u5b9a\u70b9\u548c\u4e0d\u7a33\u5b9a\u70b9\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAKE\u80fd\u6709\u6548\u7a81\u51fa\u6a21\u7cca\u70b9\u548c\u7a33\u5b9a\u6838\u5fc3\u6210\u5458\uff0c\u63d0\u4f9b\u53ef\u7528\u4e8e\u6307\u5bfc\u8fc7\u6ee4\u6216\u4f18\u5148\u7ea7\u6392\u5e8f\u4ee5\u6539\u8fdb\u805a\u7c7b\u8d28\u91cf\u7684\u7f6e\u4fe1\u5ea6\u6392\u540d\u3002", "conclusion": "CAKE\u6846\u67b6\u586b\u8865\u4e86\u805a\u7c7b\u96c6\u6210\u65b9\u6cd5\u4e2d\u70b9\u7ea7\u7f6e\u4fe1\u5ea6\u91cf\u5316\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u7ed3\u5408\u5206\u914d\u7a33\u5b9a\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4e3a\u805a\u7c7b\u7ed3\u679c\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u805a\u7c7b\u8d28\u91cf\u3002"}}
