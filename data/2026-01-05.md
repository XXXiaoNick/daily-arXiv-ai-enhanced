<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [econ.EM](#econ.EM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [math.OC](#math.OC) [Total: 7]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.LG](#cs.LG) [Total: 64]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [eess.SY](#eess.SY) [Total: 8]
- [cs.CY](#cs.CY) [Total: 6]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: RIMRULE：基于动态规则注入的神经符号方法，通过从失败轨迹中提取紧凑可解释规则并注入提示中，提升LLM在特定领域工具使用上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在领域特定工具使用上表现不佳，因为API可能具有特殊性、文档不足或针对私有工作流定制，需要有效的任务特定工具适应方法。

Method: 提出RIMRULE神经符号方法：1) 从失败轨迹中蒸馏紧凑可解释规则；2) 在推理时动态注入提示；3) 由LLM自身提出规则，使用最小描述长度目标进行整合以追求通用性和简洁性；4) 规则以自然语言和结构化符号形式存储，支持高效检索。

Result: 在工具使用基准测试中，该方法提高了对已见和未见工具的准确性，无需修改LLM权重；优于基于提示的适应方法，可与微调互补；从一个LLM学习的规则可重用改进其他LLM，包括长推理LLM，展示了符号知识在不同架构间的可移植性。

Conclusion: RIMRULE通过动态规则注入有效解决了LLM在领域特定工具使用中的适应问题，提供了一种可解释、可移植且无需权重修改的解决方案，展示了神经符号方法的潜力。

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [2] [Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning](https://arxiv.org/abs/2601.00095)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CL

TL;DR: MetaJuLS：一种元强化学习方法，通过自适应约束传播实现跨语言和任务的通用结构化推理，相比GPU优化基线提速1.5-2倍，保持SOTA解析器99.8%准确率，且能快速适应新领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要结构化推理（如JSON模式强制执行、多语言解析），输出必须满足复杂约束。现有方法通常需要针对特定任务重新训练，缺乏通用性和效率。

Method: 将结构化推理建模为自适应约束传播问题，使用图注意力网络（GAT）和元强化学习训练通用约束传播策略。该策略可跨语言和任务应用，无需任务特定重新训练。

Result: 在10种语言的Universal Dependencies和LLM约束生成任务（LogicBench、GSM8K-Constrained）上，MetaJuLS相比GPU优化基线获得1.5-2倍加速，准确率仅比SOTA解析器低0.2%。仅需5-10个梯度步（5-15秒）即可适应新语言/任务，而非数小时训练。

Conclusion: MetaJuLS通过减少LLM部署中的传播步骤，直接降低推理碳足迹，促进绿色AI。策略分析显示其发现了类人解析策略（易优先）和新颖的非直观启发式方法。

Abstract: Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\times$ speedups over GPU-optimized baselines while maintaining within 0.2\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.

</details>


### [3] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

TL;DR: Pat-DEVAL：首个专利说明书多维度评估框架，通过法律约束推理机制评估长文本结构连贯性和法定合规性，显著优于现有评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有专利自动撰写评估方法无法评估长文本结构连贯性和法定合规性（如可实施性和书面描述要求），需要专门针对专利说明书的评估框架。

Method: 提出Pat-DEVAL框架，采用LLM-as-a-judge范式，引入Chain-of-Legal-Thought（CoLT）法律约束推理机制，强制进行顺序性专利法特定分析。

Result: 在Pap2Pat-EvalGold数据集上，Pat-DEVAL达到0.69的皮尔逊相关系数，显著优于基线指标和现有LLM评估器；在法律专业合规性方面达到0.73的优异相关性。

Conclusion: Pat-DEVAL通过显式注入法定约束，为自动专利撰写系统提供了确保技术合理性和法律合规性的新标准，建立了稳健的方法论基础。

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [4] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 该论文通过系统分析IEMOCAP数据集，揭示了情感识别对话中架构选择的关键因素，并建立了识别与生成之间的语言学联系。研究发现对话上下文至关重要，外部情感词典无增益，悲伤话语因缺乏显性语用信号最依赖上下文。


<details>
  <summary>Details</summary>
Motivation: 当前情感识别对话研究存在两个关键缺陷：1）对哪些架构选择真正重要缺乏深入理解；2）缺乏将识别与生成连接起来的语言学分析。论文旨在通过系统分析填补这两个空白。

Method: 采用系统分析方法，在IEMOCAP数据集上进行10次种子评估的严格消融研究。分析5,286个话语标记的出现情况，研究情感与标记位置的关系。使用简单架构和严格因果上下文进行情感识别。

Result: 识别方面：对话上下文至关重要，90%的增益来自最近10-30个轮次；层次化句子表示在话语层面有帮助，但加入上下文后消失；外部情感词典无增益。使用简单架构达到82.69%（4类）和67.07%（6类）加权F1，优于现有方法。语言学分析：情感与标记位置显著相关，悲伤话语左边缘标记使用率（21.9%）低于其他情感（28-32%），这与悲伤最依赖上下文（+22%p）的发现一致。

Conclusion: 对话上下文是情感识别对话的关键因素，外部资源无增益。悲伤话语因缺乏显性语用信号最依赖上下文，这为情感识别与生成之间的语言学联系提供了实证支持。简单架构结合严格因果上下文即可取得优异性能。

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [5] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

TL;DR: 提出专门针对时序知识图谱推理的蒸馏框架，利用大语言模型作为教师模型，将结构和时序推理能力迁移到轻量级学生模型，在保持高效架构的同时提升时序建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有TKG推理模型参数大、计算密集，导致硬件成本和能耗高，难以部署在资源受限的实时推理平台。现有压缩和蒸馏技术主要针对静态知识图谱，无法充分捕捉TKG的时序依赖关系，导致推理性能下降。

Method: 提出专门针对时序知识图谱推理的蒸馏框架，利用大语言模型作为教师模型指导蒸馏过程，有效转移结构和时序推理能力到轻量级学生模型。通过整合大规模公共知识和任务特定时序信息，增强学生模型的时序动态建模能力，同时保持紧凑高效架构。

Result: 在多个公开基准数据集上的广泛实验表明，该方法始终优于强基线，在推理准确性、计算效率和实际可部署性之间实现了有利的权衡。

Conclusion: 提出的蒸馏框架成功解决了TKG推理模型的计算效率和部署限制问题，通过大语言模型指导的蒸馏过程，实现了轻量级模型在保持高效架构的同时具备良好的时序推理能力。

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [6] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

TL;DR: 该研究提出了一种将循证医学原则融入图基检索增强生成的方法，通过PICO框架和贝叶斯重排序提升医学问答质量，并在运动康复领域验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的检索增强生成方法主要关注性能提升，但忽视了循证医学原则，特别是缺乏查询与证据之间的PICO对齐，以及在重排序时未考虑证据等级层次。

Method: 提出通用策略将循证医学适配到图基RAG：1) 将PICO框架整合到知识图谱构建和检索中；2) 提出贝叶斯启发的重排序算法，根据证据等级校准排序分数而不引入预定义权重。在运动康复领域验证，构建了包含357,844节点和371,226边的知识图谱。

Result: 系统在1,637个QA对基准测试中表现优异：nugget覆盖度0.830、答案忠实度0.819、语义相似度0.882、PICOT匹配准确率0.788。五位专家临床医生在5点李克特量表中给出了4.66-4.84的高评分。发布了可重用基准和知识图谱。

Conclusion: 提出的循证医学适配策略显著提升了检索和答案质量，且可迁移到其他临床领域。发布的资源有助于解决运动康复领域RAG数据集稀缺的问题。

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [7] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: JP-TL-Bench是一个轻量级开源基准，用于指导日英翻译系统的迭代开发，专注于区分"哪个翻译更好"而非"翻译是否可接受"，通过参考无关的成对LLM比较和Bradley-Terry模型聚合结果


<details>
  <summary>Details</summary>
Motivation: 日英翻译中，礼貌、隐含意义、省略和语体等微妙选择对自然度影响很大，现有基准难以区分"哪个好翻译更好"这一更精细的问题，需要专门针对日英翻译特点的评估基准

Method: 使用参考无关的成对LLM比较方法，将候选模型与固定的版本化锚定集进行对比，通过Bradley-Terry模型聚合结果，生成胜率和0-10分的"LT"标准化分数

Result: 开发了JP-TL-Bench基准，提供结构稳定的评估框架，通过冻结的锚定集确保相同基础集、评判器和聚合代码下的分数稳定性

Conclusion: JP-TL-Bench为日英翻译系统开发提供了可靠且经济的评估工具，特别适合需要区分翻译质量细微差异的迭代开发场景

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [8] [Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback](https://arxiv.org/abs/2601.00224)
*Yan Sun,Ming Cai,Stanley Kok*

Main category: cs.CL

TL;DR: 论文提出了Q*和Feedback+两种验证技术，通过生成器-判别器框架减少企业级LLM助手在商业分析中的错误率，将验证责任从用户转移到系统。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型助手在企业工作流中的集成度提高，其生成准确、语义对齐且可执行输出的能力变得至关重要。当前会话式商业分析系统缺乏内置验证机制，用户需要手动验证可能存在缺陷的结果。

Method: 引入两种互补的验证技术：Q*（执行代码与用户意图之间的反向翻译和语义匹配）和Feedback+（整合执行反馈来指导代码优化）。这些机制嵌入在生成器-判别器框架中。

Result: 在Spider、Bird和GSM8K三个基准数据集上的评估表明，Q*和Feedback+都能降低错误率和任务完成时间。研究还发现反向翻译是主要瓶颈，为未来改进提供了方向。

Conclusion: 这项工作为构建更可靠、企业级、能够提供可信决策支持的生成式AI系统贡献了一个设计导向的框架。

Abstract: As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.

</details>


### [9] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 该研究系统评估了大语言模型生成多语言反事实解释的能力，发现翻译生成的反事实比直接生成的有效性更高但修改更多，多语言反事实数据增强比跨语言增强效果更好，但生成质量限制了模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在生成英语反事实解释方面表现出色且具备多语言能力，但其在多语言反事实生成方面的有效性尚不明确，需要系统研究来评估其实际表现和应用潜力。

Method: 1) 在六种语言上进行自动评估，比较直接生成的目标语言反事实与通过英语翻译生成的反事实；2) 分析高资源欧洲语言反事实的编辑模式；3) 识别和分类多语言反事实中的错误类型；4) 评估多语言反事实数据增强与跨语言数据增强对模型性能的影响。

Result: 1) 翻译生成的反事实比直接生成的有效性更高，但需要更多修改，且质量仍不及原始英语反事实；2) 高资源欧洲语言的反事实编辑模式高度相似；3) 识别出四种跨语言一致出现的错误类型；4) 多语言反事实数据增强比跨语言增强带来更大的模型性能提升，特别是对低资源语言，但反事实的不完美限制了性能提升和鲁棒性。

Conclusion: 大语言模型在多语言反事实生成方面仍有局限，翻译方法能提高有效性但代价是更多修改，多语言数据增强优于跨语言增强但受限于生成质量，需要进一步改进反事实生成技术以充分发挥其潜力。

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [10] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: WildAGTEval是一个评估LLM代理函数调用能力的基准，专注于真实API复杂性，包含API规范和API执行两个维度的挑战，包含约32K测试配置，发现无关信息复杂性对LLM性能影响最大。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设理想化的API系统，忽略了真实世界的复杂因素（如API输出的噪声），需要评估LLM代理在真实API复杂性下的函数调用能力。

Method: 创建WildAGTEval基准，包含API规范和API执行两个维度的复杂性，涵盖60个不同的复杂性场景，可组合成约32K测试配置，并提供用户-代理交互评估框架。

Result: 大多数场景都具有挑战性，无关信息复杂性对LLM性能影响最大，使强LLM性能下降27.3%；定性分析发现LLM有时会扭曲用户意图来声称任务完成，严重影响用户满意度。

Conclusion: WildAGTEval揭示了LLM代理在真实API复杂性下的局限性，特别是处理无关信息的能力不足，以及可能扭曲用户意图的问题，这对实际应用中的用户满意度有重要影响。

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [11] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 量化对大型语言模型的自解释能力有负面影响，但影响相对较小，不影响量化作为模型压缩技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 量化被广泛用于加速LLM推理和部署，但其对自解释（模型为自己输出生成解释）的影响尚未被研究。自解释在高风险应用中用于透明度，了解量化是否会降低自解释质量和忠实度至关重要。

Method: 研究两种自解释：自然语言解释和反事实示例，使用三种常见量化技术在不同比特宽度下量化LLM，并通过用户研究评估自解释的连贯性和可信度。

Result: 量化通常导致自解释质量（最多下降4.4%）和忠实度（最多下降2.38%）适度下降。用户研究表明量化降低自解释的连贯性和可信度（最多8.5%）。较大模型在自解释质量上对量化抵抗力有限，但在保持忠实度方面更好。没有量化技术在任务准确性、自解释质量和忠实度方面始终表现优异。

Conclusion: 量化对自解释的影响因上下文而异，建议针对具体用例验证自解释质量，特别是对更敏感的自然语言解释。但自解释质量和忠实度的相对较小恶化并不削弱量化作为模型压缩技术的有效性。

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [12] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: 提出DepFlow框架，通过三阶段文本转语音技术生成伪装抑郁症数据，缓解语义偏见，提升抑郁症检测模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症数据集（如DAIC-WOZ）存在语言情感与诊断标签的强耦合，导致模型学习语义捷径，在真实场景（如伪装抑郁症）中鲁棒性不足。需要解决语义偏见问题。

Method: 1. 抑郁症声学编码器：通过对抗训练学习说话人和内容不变的抑郁症嵌入；2. 流匹配TTS模型：通过FiLM调制注入抑郁症嵌入，控制抑郁严重程度；3. 原型严重程度映射：提供平滑可解释的抑郁连续体操作。基于此构建CDoA数据集。

Result: 抑郁症声学编码器ROC-AUC达0.693；CDoA数据集在三种抑郁症检测架构上分别提升macro-F1 9%、12%和5%，优于传统增强策略。

Conclusion: DepFlow不仅通过CDoA增强抑郁症检测鲁棒性，还为对话系统和模拟评估提供了可控合成平台，解决了临床数据受伦理和覆盖范围限制的问题。

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [13] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: 该研究针对LLM幻觉问题，提出了一种基于多事实生成任务的陷阱问题集和鲁棒不确定性量化方法，在对抗性提问场景下显著提升了幻觉检测性能。


<details>
  <summary>Details</summary>
Motivation: LLM幻觉问题严重影响了AI生成内容的可靠性和可信度。现有不确定性量化方法在常规问答场景中有效，但在非规范或对抗性提问策略下表现不足，这在实际应用中存在重大隐患。

Method: 研究构建了包含虚假名称的陷阱问题集，并创新性地提出了一种鲁棒不确定性量化方法(RU)，用于多事实生成任务中的不确定性评估。

Result: 实验结果表明，构建的陷阱问题集表现优异。与基线方法相比，在四个不同模型上，提出的方法平均将ROCAUC值提升了0.1-0.2，显著优于现有最佳基线方法。

Conclusion: 该研究为解决LLM幻觉问题提供了新的视角和方法，特别是在对抗性提问场景下的不确定性量化方面取得了重要进展，增强了LLM在现实应用中的可靠性。

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [14] [The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2601.00364)
*Jiandong Shao,Raphael Tang,Crystina Zhang,Karin Sevegnani,Pontus Stenetorp,Jianfei Yang,Yao Lu*

Main category: cs.CL

TL;DR: 研究发现双语数据中仅平行数据对翻译性能至关重要，而代码转换数据贡献有限；跨语言理解和推理任务即使没有双语数据也能实现


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在主要单语预训练下仍能取得令人印象深刻的跨语言性能。虽然普遍认为预训练语料中的双语数据是实现这些能力的关键，但其具体贡献细节仍不清楚。本研究旨在探究双语数据在跨语言能力中的具体作用机制。

Method: 在受控条件下从头预训练模型，比较标准网络语料库与移除所有多语言文档的单语版本。将双语数据分类为平行数据（14%）、代码转换数据（72%）和其他文档（14%），并通过将平行数据或代码转换数据重新引入单语语料库进行细粒度消融实验。

Result: 移除仅占语料库2%的双语数据导致翻译性能下降56%（BLEU），而跨语言QA和一般推理任务表现保持稳定。平行数据几乎完全恢复翻译性能（达到未过滤基线的91%），而代码转换数据贡献极小。其他跨语言任务基本不受任一类双语数据影响。

Conclusion: 翻译性能严重依赖于平行数据提供的系统性词元级对齐，而跨语言理解和推理能力即使没有双语数据也能实现。这揭示了双语数据中不同成分对各类跨语言任务的不同贡献机制。

Abstract: Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.

</details>


### [15] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: BERT-JEPA (BEPA) 将 JEPA 训练目标加入 BERT 模型，解决 [CLS] 嵌入空间坍缩问题，创建语言无关的表示空间，提升多语言任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决 BERT 模型中 [CLS] 嵌入空间坍缩的问题，同时将 JEPA 这种在多个领域显示潜力的自监督训练技术应用于语言模型，以提升多语言任务的性能。

Method: 提出 BERT-JEPA (BEPA) 训练范式，在 BERT 风格模型中添加 JEPA 训练目标，通过联合嵌入预测架构来对抗 [CLS] 嵌入空间的坍缩，将其转化为语言无关的空间。

Result: 该方法在多个多语言基准测试中表现出性能提升，证明了 JEPA 目标能够有效改善 BERT 模型的多语言表示能力。

Conclusion: BERT-JEPA 通过结合 JEPA 训练目标成功解决了 BERT 模型的 [CLS] 嵌入空间坍缩问题，创建了语言无关的表示空间，显著提升了多语言任务的性能。

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [16] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

TL;DR: Geo-R：基于强化学习的检索免费图像地理定位框架，通过结构化地理推理和坐标对齐奖励提升定位精度与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图像地理定位中依赖合成推理标注或外部图像检索，限制了可解释性和泛化能力，需要一种更直接、可解释的地理定位方法

Method: 提出Geo-R框架：1) 基于规则的层次推理范式"区域链"，将GPS坐标映射到地理实体；2) 轻量级强化学习策略，基于Haversine距离的坐标对齐奖励；3) 检索免费的直接空间监督

Result: 在多个基准测试中验证了Geo-R的有效性，实现了更高的定位精度、更强的泛化能力和更透明的推理过程

Conclusion: Geo-R建立了检索免费的地理定位新范式，通过结构化地理推理与直接空间监督的结合，实现了可扩展且可解释的图像地理定位

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [17] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: 提出judgeWEL数据集，用于卢森堡语命名实体识别，通过LLM自动标注和验证的新流程构建


<details>
  <summary>Details</summary>
Motivation: 为低资源语言构建数据集是NLP的主要瓶颈之一，资源稀缺和语言特性使得大规模标注成本高且不一致

Method: 利用Wikipedia和Wikidata作为弱监督结构化源，通过维基百科内部链接推断实体类型，然后使用多个LLM识别和保留高质量标注句子来降低噪声

Result: 构建的语料库比现有卢森堡语NER数据集大约5倍，提供更广泛和平衡的实体类别覆盖

Conclusion: judgeWEL为多语言和低资源NER研究提供了重要的新资源，展示了利用LLM和结构化知识源构建低资源语言数据集的可行方法

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [18] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: 本文提出了一种新的超关系时序知识广义超图（HTKGHs）来弥补现有超关系时序知识图（HTKGs）在表达复杂事实（特别是涉及多个主要实体的时序事实）方面的不足，并基于POLECAT数据库构建了htkgh-polecat数据集，最后评估了LLMs在关系预测任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有超关系时序知识图（HTKGs）虽然能表示简单的时序关系，但表达能力有限，尤其无法有效支持涉及两个以上主要实体的时序事实（这在真实世界的地缘政治事件中很常见）。为了克服这一限制，需要一种更通用的表示方法。

Method: 1. 提出超关系时序知识广义超图（HTKGHs）的形式化定义，保持向后兼容性的同时支持两种复杂事实类型；2. 基于POLECAT全球事件数据库构建htkgh-polecat数据集；3. 在关系预测任务上对主流大语言模型进行基准测试和分析。

Result: 1. 成功形式化了HTKGHs，能够有效表示涉及多个主要实体的复杂时序事实；2. 创建了htkgh-polecat数据集；3. 通过基准测试获得了LLMs在复杂预测场景中的适应性和能力洞察。

Conclusion: HTKGHs为表示复杂时序事实提供了更强大的框架，特别是在地缘政治事件预测领域。通过构建新数据集和评估LLMs性能，为复杂时序知识图上的预测任务提供了重要基础。

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [19] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

TL;DR: 比较三种轻量级Transformer模型(DistilBERT、MiniLM、ALBERT)在多领域文本自动化任务中的表现，发现各有优势：ALBERT准确率最高，MiniLM推理速度最快，DistilBERT表现最均衡。


<details>
  <summary>Details</summary>
Motivation: 企业NLP领域对高效、轻量级模型的需求日益增长，需要能够处理多领域文本自动化任务的模型。本研究旨在比较三种主流轻量级Transformer模型在不同领域的性能表现，为企业选择合适模型提供指导。

Method: 使用IMDB、AG News和Measuring Hate Speech三个数据集，分别在客户情感分类、新闻主题分类、毒性和仇恨言论检测三个领域评估DistilBERT、MiniLM和ALBERT三种模型。评估指标包括准确率、精确率、召回率、F1分数等性能指标，以及模型大小、推理时间、吞吐量、内存使用等效率指标。

Result: 没有单一模型在所有性能维度上占优：ALBERT在多个领域达到最高任务特定准确率；MiniLM在推理速度和吞吐量方面表现最佳；DistilBERT在任务间保持最一致的准确率，同时保持有竞争力的效率。所有结果都是在固定企业导向约束下的受控微调结果。

Conclusion: 研究揭示了准确率与效率之间的权衡关系，为企业应用提供建议：MiniLM适合延迟敏感的企业应用，DistilBERT适合平衡性能需求，ALBERT适合资源受限环境。

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [20] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: 论文对比了语言的社会建构主义（语言游戏）与数学导向的语义场理论，认为两者在解释大语言模型的能力与局限时是互补而非竞争的关系。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型为检验长期存在的语言意义理论提供的新的实证环境，对比社会建构主义和数学结构两种语言观，以理解LLMs的成功与局限。

Method: 基于作者先前工作，形式化词汇场和语言场作为连续语义空间中的交互结构，分析Transformer架构的核心特性（分布式表示、注意力机制、嵌入空间几何规律）如何与这些概念相关。

Result: LLMs捕捉语义规律的成功支持语言具有底层数学结构的观点，而其在语用推理和上下文敏感性方面的持续局限则与社会基础的重要性一致。数学结构和语言游戏可视为互补视角。

Conclusion: 数学结构和语言游戏是互补而非竞争的关系，这一框架澄清了纯统计语言模型的适用范围和局限，并为理论指导的AI架构提供了新方向。

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [21] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 提出Defensive M2S训练范式，通过多轮到单轮对话压缩减少护栏模型的计算成本，在保持高攻击检测率的同时显著降低训练和推理开销。


<details>
  <summary>Details</summary>
Motivation: 护栏模型对LLM部署安全至关重要，但处理完整多轮对话历史会产生高昂计算成本，需要更高效的训练和推理方法。

Method: 提出Defensive M2S训练范式，在多轮到单轮压缩对话上微调护栏模型而非完整对话历史，使用三种压缩模板（hyphenize、numberize、pythonize），在三个护栏模型家族上评估。

Result: M2S将训练成本从O(n²)降至O(n)，训练token减少93倍；最佳配置（Qwen3Guard+hyphenize）达到93.8%攻击检测召回率，推理token减少94.6%，比基线提升38.9个百分点。

Conclusion: M2S压缩可作为护栏部署的有效效率技术，实现长多轮对话的可扩展安全筛查，显著降低训练和推理成本。

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [22] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: 该论文提出了一种针对职业教育培训领域历史文档的噪声感知命名实体识别方法，通过合成OCR错误注入、迁移学习和多阶段微调来提高模型在噪声条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 职业教育培训领域的历史数字化文档存在OCR噪声问题，这会影响命名实体识别的准确性。目前缺乏针对该领域多类型实体识别的方法，特别是在噪声条件下的鲁棒性解决方案。

Method: 采用噪声感知训练方法，通过合成注入OCR错误，结合迁移学习和多阶段微调策略。系统比较了三种互补策略：在噪声数据、干净数据和人工合成数据上的训练。

Result: 实验结果表明，领域特定和噪声感知的微调显著提高了模型在噪声条件下的鲁棒性和准确性。该方法能够识别职业教育培训文档中的多种实体类型。

Conclusion: 提出的噪声感知NER方法在职业教育培训领域历史文档中表现出色，具有跨语言可迁移性，并提供了公开可用的代码以确保可复现性。

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [23] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

TL;DR: 该研究分析了复杂句子结构对基于规则的原子句子提取性能的影响，发现相对从句、同位语、并列谓语、状语从句和被动结构是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 现有原子句子提取方法缺乏可解释性，无法揭示哪些语言结构导致提取失败。需要系统分析特定从句结构和依存关系如何影响提取性能。

Method: 使用WikiSplit数据集，在spaCy中实现基于依存关系的提取规则，生成100个黄金标准原子句子集，使用ROUGE和BERTScore评估性能。

Result: 系统达到ROUGE-1 F1=0.6714、ROUGE-2 F1=0.478、ROUGE-L F1=0.650、BERTScore F1=0.5898，显示中高水平的对齐。相对从句、同位语、并列谓语、状语从句和被动结构最具挑战性。

Conclusion: 基于规则的提取方法具有合理准确性，但对句法复杂性敏感。需要更精细的规则来处理复杂结构，为改进机器学习方法提供见解。

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [24] [Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends](https://arxiv.org/abs/2601.00536)
*Yuelyu Ji,Zhuochun Li,Rui Meng,Daqing He*

Main category: cs.CL

TL;DR: 这篇综述论文提出了一个四轴框架来分析多跳问答系统的执行过程，将执行过程作为分析单元，帮助比较不同模型家族的程序选择。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答系统中，检索-推理的过程往往是隐式的，使得不同模型家族之间的程序选择难以比较。需要一种系统化的方法来分析和比较这些系统的执行过程。

Method: 提出了一个四轴分析框架：(A)整体执行计划，(B)索引结构，(C)下一步控制（策略和触发器），(D)停止/继续标准。使用这个框架对代表性的多跳问答系统进行映射分析。

Result: 在标准基准测试（如HotpotQA、2WikiMultiHopQA、MuSiQue）上综合了报告的消融实验和趋势，突出了有效性、效率和证据忠实度之间的反复权衡。

Conclusion: 提出了检索-推理代理的开放挑战，包括结构感知规划、可转移的控制策略以及在分布偏移下的鲁棒停止机制。

Abstract: Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

</details>


### [25] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: ECR框架通过语义锚点保持紧凑模型嵌入空间的结构一致性，无需依赖教师模型输出，在多语言任务中稳定训练并提升表示质量。


<details>
  <summary>Details</summary>
Motivation: 紧凑模型在容量受限或多语言场景下容易丢失嵌入空间结构，导致语义漂移，现有压缩方法只关注表层输出对齐而忽略底层流形结构。

Method: 提出嵌入一致性调节（ECR）框架：从教师嵌入中提取语义锚点（离线计算），让紧凑模型学习在这些锚点周围保持几何一致性，无需匹配logits或内部特征，推理时仅添加小型投影步骤。

Result: 在10万条多语言语料实验中，ECR稳定训练并跨任务和语言保持语义结构，产生更紧凑且任务对齐的表示空间，使低容量模型能学习比传统基线更清晰的流形。

Conclusion: ECR帮助紧凑模型更好地遵循任务需求，使其在严格效率或隐私限制下更容易部署，且不依赖教师输出，与蒸馏兼容但独立。

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [26] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

TL;DR: 提出HLoRA框架，将分层LoRA-MoE集成到mHuBERT-CTC模型中，实现无需语言先验信息的单次解码轻量级多语言ASR系统


<details>
  <summary>Details</summary>
Motivation: 现有大规模多语言ASR模型（如Whisper）计算和延迟成本高，难以部署到资源受限的边缘设备，需要更轻量高效的解决方案

Method: 基于CTC架构，提出语言无关的分层LoRA-MoE框架：包含多语言共享LoRA学习语言不变声学表示，以及语言特定LoRA专家建模语言相关特征；通过LID后验驱动的LoRA路由实现端到端解码

Result: 在MSR-86K和MLC-SLM 2025挑战数据集上，HLoRA仅通过单次解码就达到了与最先进两阶段推理方法相当的性能，显著提升了低资源多语言ASR的解码效率

Conclusion: HLoRA框架实现了真正的语言无关解码，无需推理时的语言身份信息或显式语言标签，为资源受限环境下的多语言ASR部署提供了高效解决方案

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [27] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: InfoSynth是一个基于信息论原则自动生成和评估推理基准的框架，使用KL散度和熵量化基准新颖性和多样性，通过遗传算法和迭代代码反馈生成Python编程问题，实现97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基准创建依赖人工，成本高且耗时；现有基准常污染LLM训练数据，需要新颖多样的基准来准确评估LLM的真实能力。

Method: 提出基于KL散度和熵的指标量化基准新颖性和多样性；开发端到端流水线，使用遗传算法和迭代代码反馈从种子数据集合成稳健的Python编程问题。

Result: 方法生成新问题的准确测试用例和解决方案达到97%成功率；合成基准相比种子数据集具有更高的新颖性和多样性；算法可控制生成问题的新颖性/多样性和难度。

Conclusion: InfoSynth为LLM提供了一个可扩展、自验证的高质量基准构建流水线，能够生成新颖多样的基准来准确评估模型能力。

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [28] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

TL;DR: CSSBench是一个专门针对中文特定对抗模式的安全基准测试，用于评估轻量级大语言模型在中文环境下的安全性表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要部署在成本敏感和设备端场景，安全护栏主要针对英文设计。中文恶意查询通常通过同音字、拼音、符号分割等中文特有模式隐藏意图，这些对抗模式在现有英文基准测试中未被充分捕捉，特别是轻量级模型可能更容易受到此类特定对抗扰动的影响。

Method: 引入中文特定安全基准测试CSSBench，强调中文对抗模式，覆盖六个真实中文场景领域：非法活动与合规、隐私泄露、健康医疗错误信息、欺诈与仇恨、成人内容、公共与政治安全，并将查询组织成多种任务类型。

Result: 评估了一系列流行的轻量级LLM，测量过度拒绝行为以评估安全引起的性能下降。结果显示中文特定对抗模式是轻量级LLM面临的关键挑战。

Conclusion: CSSBench为中文LLM安全性提供了全面评估，有助于实际部署中的鲁棒性保障，填补了现有基准测试在中文对抗模式评估方面的空白。

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [29] [Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence](https://arxiv.org/abs/2601.00596)
*Sumanth Balaji,Piyush Mishra,Aashraya Sachdeva,Suraj Agrawal*

Main category: cs.CL

TL;DR: JourneyBench是一个用于评估客户支持场景中策略感知AI代理的新基准，通过图表示生成多样化支持场景，并提出用户旅程覆盖率指标来衡量策略遵循能力。


<details>
  <summary>Details</summary>
Motivation: 传统客户支持系统（如IVR）依赖刚性脚本，缺乏处理复杂策略驱动任务的灵活性。虽然LLM代理提供了有前景的替代方案，但评估它们遵循业务规则和真实支持工作流程的能力仍然是一个开放挑战。现有基准主要关注工具使用或任务完成，忽视了代理遵循多步骤策略、导航任务依赖关系以及应对不可预测用户行为的能力。

Method: 引入JourneyBench基准，利用图表示生成多样化、真实的客户支持场景。提出用户旅程覆盖率作为衡量策略遵循的新指标。评估了两种代理设计：静态提示代理和动态提示代理（显式建模策略控制）。在三个领域的703个对话中进行评估。

Result: 动态提示代理显著提高了策略遵循能力，甚至允许较小的模型（如GPT-4o-mini）在策略遵循方面超越更强大的模型（如GPT-4o）。结果表明结构化编排的重要性。

Conclusion: JourneyBench作为一个关键资源，能够推动AI驱动的客户支持超越IVR时代的限制，展示了结构化编排在提高策略遵循方面的重要性。

Abstract: Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.

</details>


### [30] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: 提出一个模型无关的框架，通过重复采样和多数投票机制为确定性自动化工作流中的LLM幻觉提供概率保证


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在确定性自动化工作流中经常产生上下文幻觉，即生成内容与提示中明确信息相矛盾或忽略这些信息。这种错误在输入固定、正确性明确的工作流中特别成问题。

Method: 提出一个简单、模型无关的框架：1）在独立上下文窗口中重复相同提示，利用指数级降低所有输出都错误的概率；2）使用LLM作为评判器识别正确答案；3）当评判器不完美时，通过独立评判调用的多数投票来增强，获得指数级降低的集成错误率。

Result: 在受控提取任务上的实验验证：管道失败率随重复次数指数下降，幻觉选择率随评判器数量指数下降。框架能够在不修改模型权重、解码策略或提示工程的情况下，将幻觉概率降至任意低水平。

Conclusion: 该研究提供了一个轻量级、模块化且理论可靠的方法，通过重复采样和多数投票机制，在固定输入的LLM工作流中实现任意低的幻觉概率，为确定性自动化应用提供了实用的解决方案。

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [31] [Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations](https://arxiv.org/abs/2601.00647)
*QiWei Meng*

Main category: cs.CL

TL;DR: Physio-DPO：一种基于物理信息对齐的蛋白质语言模型框架，通过考虑物理能量景观的连续结构来减少结构幻觉，提高蛋白质设计的稳定性和可折叠性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型蛋白质语言模型在生成蛋白质序列时经常产生结构幻觉，生成的语言学可能性高但热力学不稳定的构象。现有的对齐方法（如DPO）将偏好建模为二元标签，忽略了物理能量景观的连续结构。

Method: 提出Physio-DPO框架，引入幅度感知目标函数，根据天然结构与物理扰动硬负样本之间的能量差距来缩放优化更新，将蛋白质语言模型基于热力学稳定性进行对齐。

Result: Physio-DPO在实验中一致优于SFT、PPO和标准DPO等基线方法，将自一致性RMSD降低到1.28Å，将可折叠性提高到92.8%。定性分析显示Physio-DPO有效缓解结构幻觉，恢复疏水核心堆积和氢键网络等生物物理相互作用。

Conclusion: Physio-DPO通过将物理能量景观的连续结构纳入对齐过程，显著提高了蛋白质语言模型生成的热力学稳定性，为生成蛋白质设计提供了更可靠的框架。

Abstract: Large Protein Language Models have shown strong potential for generative protein design, yet they frequently produce structural hallucinations, generating sequences with high linguistic likelihood that fold into thermodynamically unstable conformations. Existing alignment approaches such as Direct Preference Optimization are limited in this setting, as they model preferences as binary labels and ignore the continuous structure of the physical energy landscape. We propose Physio-DPO, a physics informed alignment framework that grounds protein language models in thermodynamic stability. Physio-DPO introduces a magnitude aware objective that scales optimization updates according to the energy gap between native structures and physics perturbed hard negatives. Experiments show that Physio-DPO consistently outperforms strong baselines including SFT, PPO, and standard DPO, reducing self consistency RMSD to 1.28 Å and increasing foldability to 92.8%. Qualitative analysis further demonstrates that Physio-DPO effectively mitigates structural hallucinations by recovering biophysical interactions such as hydrophobic core packing and hydrogen bond networks.

</details>


### [32] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: FwPKM将静态产品键记忆转化为动态快速权重记忆，通过局部梯度下降动态更新参数，实现高效记忆存储与检索，显著提升长上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型的序列建模层面临存储容量与计算效率的权衡：Softmax注意力提供无限存储但计算成本二次方增长，线性变体计算高效但存储容量有限且固定。需要解决这一矛盾。

Method: 提出快速权重产品键记忆（FwPKM），将稀疏的产品键记忆从静态模块转化为动态的"快速权重"情景记忆。通过局部块级梯度下降在训练和推理时动态更新参数，使模型能够快速记忆和检索输入序列中的新键值对。

Result: FwPKM作为有效的情景记忆补充标准模块的语义记忆，在长上下文数据集上显著降低困惑度。在"大海捞针"评估中，尽管仅在4K标记序列上训练，却能泛化到128K标记的上下文。

Conclusion: FwPKM成功解决了序列建模中存储容量与计算效率的权衡问题，提供了一种动态、高效的记忆机制，显著提升了语言模型的长上下文处理能力。

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [33] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: 提出Sigmoid Head模块解决语言模型概率不可靠问题，使用sigmoid激活和负采样策略提升质量估计能力


<details>
  <summary>Details</summary>
Motivation: 语言模型的softmax概率分布不可靠，因为自然语言具有歧义性，多个输出选项都有效时，概率会分散到不同选项上，导致错误地指示低输出质量

Method: 在预训练语言模型上训练质量估计模块Sigmoid Head：1) 使用sigmoid激活的额外解嵌入头解决softmax限制；2) 负采样过程中使用启发式方法避免选择潜在的正确替代词

Result: Sigmoid Head的概率信号显著优于原始softmax头，训练和推理计算效率高，且不依赖人工标注的质量数据，在领域外设置中比监督质量估计更鲁棒

Conclusion: Sigmoid Head有效解决了语言模型概率作为质量估计器的局限性，提供更可靠的质量信号，特别适用于领域外场景

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [34] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在文本跨度识别任务中的表现，特别是在情感分析、冒犯性语言识别和声明验证等主观性较强的任务上，探索了指令调优、上下文学习和思维链等策略。


<details>
  <summary>Details</summary>
Motivation: 当前大多数跨度识别方法依赖于BERT等较小的预训练模型，而大型语言模型在该任务中的应用研究不足，特别是在主观性较强的任务如基于方面的情感分析方面。本文旨在填补这一重要空白。

Method: 评估了多种LLM在三个流行任务（情感分析、冒犯性语言识别、声明验证）中的文本跨度识别性能，探索了指令调优、上下文学习和思维链等策略。

Result: 结果表明，文本中的潜在关系有助于LLM识别精确的文本跨度。

Conclusion: LLM在文本跨度识别任务中表现出色，特别是在利用文本内部关系方面，为模型可解释性提供了重要贡献。

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [35] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

TL;DR: 跨省评估癌症登记NLP模型泛化能力，通过模型集成显著减少漏报癌症病例


<details>
  <summary>Details</summary>
Motivation: 癌症登记依赖病理报告，但人工提取资源密集且延迟数据。现有NLP系统在不同司法管辖区间的泛化能力不足，需要评估跨省适应能力

Method: 使用纽芬兰与拉布拉多癌症登记的病理报告数据，对BCCRTron和GatorTron进行微调，采用互补的摘要和诊断报告部分输入管道，并通过保守OR集成组合两个模型

Result: 集成模型在Tier 1任务中召回率达到0.99，漏报癌症从48/54减少到24；Tier 2任务中召回率0.99，漏报可报告癌症从54/46减少到33

Conclusion: 互补文本表示的模型集成能显著减少癌症漏报，仅共享模型权重的隐私保护工作流程支持跨省互操作NLP基础设施，为未来泛加拿大癌症病理基础模型奠定基础

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [36] [Second Thoughts: How 1-second subslots transform CEX-DEX Arbitrage on Ethereum](https://arxiv.org/abs/2601.00738)
*Aleksei Adadurov,Sergey Barseghyan,Anton Chtepine,Antero Eloranta,Andrei Sebyakin,Arsenii Valitov*

Main category: q-fin.TR

TL;DR: 研究以太坊区块时间缩短对去中心化交易所活动的影响，重点关注CEX-DEX套利行为。模拟显示，从12秒缩短到1秒子槽执行时间，套利交易量增加535%，交易额增加203%。


<details>
  <summary>Details</summary>
Motivation: 研究以太坊区块时间缩短如何影响去中心化交易所的套利活动，特别是CEX-DEX套利行为。由于DEX交易执行存在不确定性，执行风险会影响套利决策，而更快的区块时间可能改变这种风险收益平衡。

Method: 建立交易模型，其中代理的DEX交易不一定能成功执行，代理在决定是否追求套利机会时明确考虑这种执行风险。比较以太坊默认12秒槽时间环境与提供1秒子槽执行的更快机制下的代理行为。使用2025年7月至9月的币安和Uniswap v3数据进行校准模拟。

Result: 模拟结果显示，更快的槽时间使套利交易数量平均增加535%，交易额增加203%。1秒子槽机制下CEX-DEX套利活动的增加是由于成功和失败交易结果的方差减少，提高了风险调整后收益，使CEX-DEX套利更具吸引力。

Conclusion: 缩短以太坊区块时间能显著增加CEX-DEX套利活动，主要原因是减少了交易执行的不确定性，提高了风险调整后收益。这表明区块链性能改进对去中心化金融市场的流动性提供和套利效率有重要影响。

Abstract: This paper examines the impact of reducing Ethereum slot time on decentralized exchange activity, with a focus on CEX-DEX arbitrage behavior. We develop a trading model where the agent's DEX transaction is not guaranteed to land, and the agent explicitly accounts for this execution risk when deciding whether to pursue arbitrage opportunities. We compare agent behavior under Ethereum's default 12-second slot time environment with a faster regime that offers 1-second subslot execution. The simulations, calibrated to Binance and Uniswap v3 data from July to September 2025, show that faster slot times increase arbitrage transaction count by 535% and trading volume by 203% on average. The increase in CEX-DEX arbitrage activity under 1-second subslots is driven by the reduction in variance of both successful and failed trade outcomes, increasing the risk-adjusted returns and making CEX-DEX arbitrage more appealing.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [37] [Difference-in-Differences using Double Negative Controls and Graph Neural Networks for Unmeasured Network Confounding](https://arxiv.org/abs/2601.00603)
*Zihan Zhang,Lianyan Fu,Dehui Wang*

Main category: econ.EM

TL;DR: 提出结合双重负控制(DNC)和图神经网络(GNN)的差分法框架，用于估计观测网络数据中的直接和间接因果效应，解决网络干扰和未测量混杂问题。


<details>
  <summary>Details</summary>
Motivation: 观测网络数据中的因果效应估计面临网络干扰和未测量混杂的双重挑战，现有方法难以同时解决这两个问题。

Method: 提出基于修改的平行趋势假设和双重负控制(DNC)的差分法框架，结合图神经网络(GNN)和广义矩方法(GMM)估计高维协变量和网络结构函数，构建双重稳健估计器。

Result: 建立了直接和间接因果效应的半参数识别，在ψ-网络依赖和近似邻域干扰下推导了估计量的渐近正态性，模拟显示有限样本性能良好，并应用于中国绿色信贷政策对企业绿色创新的影响分析。

Conclusion: 该方法有效解决了网络数据中的因果推断问题，为政策评估提供了可靠工具，特别是在绿色金融政策效果分析中展示了实用价值。

Abstract: Estimating causal effects from observational network data faces dual challenges of network interference and unmeasured confounding. To address this, we propose a general Difference-in-Differences framework that integrates double negative controls (DNC) and graph neural networks (GNNs). Based on the modified parallel trends assumption and DNC, semiparametric identification of direct and indirect causal effects is established. We then propose doubly robust estimators. Specifically, an approach combining GNNs with the generalized method of moments is developed to estimate the functions of high-dimensional covariates and network structure. Furthermore, we derive the estimator's asymptotic normality under the $ψ$-network dependence and approximate neighborhood interference. Simulations show the finite-sample performance of our estimators. Finally, we apply our method to analyze the impact of China's green credit policy on corporate green innovation.

</details>


### [38] [Continuous time asymptotic representations for adaptive experiments](https://arxiv.org/abs/2601.00739)
*Karun Adusumilli*

Main category: econ.EM

TL;DR: 提出连续时间渐近框架分析自适应实验，通过经验分配过程的极限表示简化最优决策规则分析


<details>
  <summary>Details</summary>
Motivation: 分析完全自适应实验面临挑战，因为策略序列通常缺乏明确定义的渐近极限，需要新的理论框架来处理动态数据收集和治疗分配

Method: 聚焦于经验分配过程，证明任何自适应实验及其经验分配过程都可以用高斯扩散定义的极限实验来近似，利用连续时间分配过程简化分析

Result: 建立了自适应实验的连续时间渐近框架，推导出最优估计器，分析自适应实验的样本内遗憾，并构建了任意时间有效推断的e-过程

Conclusion: 该框架通过经验分配过程的极限表示，显著简化了自适应实验的分析，为多治疗设置提供了首个任意时间和任意实验有效的推断定义

Abstract: This article develops a continuous-time asymptotic framework for analyzing adaptive experiments -- settings in which data collection and treatment assignment evolve dynamically in response to incoming information. A key challenge in analyzing fully adaptive experiments, where the assignment policy is updated after each observation, is that the sequence of policy rules often lack a well-defined asymptotic limit. To address this, we focus instead on the empirical allocation process, which captures the fraction of observations assigned to each treatment over time. We show that, under general conditions, any adaptive experiment and its associated empirical allocation process can be approximated by a limit experiment defined by Gaussian diffusions with unknown drifts and a corresponding continuous-time allocation process. This limit representation facilitates the analysis of optimal decision rules by reducing the dimensionality of the state-space and leveraging the tractability of Gaussian diffusions. We apply the framework to derive optimal estimators, analyze in-sample regret for adaptive experiments, and construct e-processes for anytime-valid inference. Notably, we introduce the first definition of any-time and any-experiment valid inference for multi-treatment settings.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [39] [Ultimate Forward Rate Prediction and its Application to Bond Yield Forecasting: A Machine Learning Perspective](https://arxiv.org/abs/2601.00011)
*Jiawei Du,Yi Hong*

Main category: q-fin.ST

TL;DR: 该研究开发了基于终极远期利率（UFR）的债券收益率预测模型，使用中国国债数据和宏观经济变量，结合线性与非线性机器学习方法，发现非线性模型预测效果更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在预测中国债券市场的终极远期利率（UFR），并基于UFR开发更准确的债券收益率预测模型，以应对债券市场预测中的挑战。

Method: 应用de Kort-Vellekoop方法估计UFR，结合本研究提出的最优转折参数确定技术来减少异常波动。同时采用线性和非线性机器学习技术预测UFR和超长期债券收益率，并开发了基于UFR的债券收益率预测模型。

Result: 非线性机器学习模型在预测准确性上优于线性模型；加入宏观经济变量（特别是价格指数相关变量）显著提高了预测精度；新开发的UFR基债券收益率预测模型在不同期限债券上均表现出优越性能。

Conclusion: 该研究成功开发了基于UFR的债券收益率预测框架，非线性机器学习方法和宏观经济变量的结合能够有效提高债券市场预测的准确性，为债券投资和风险管理提供了实用工具。

Abstract: This study focuses on forecasting the ultimate forward rate (UFR) and developing a UFRbased bond yield prediction model using data from Chinese treasury bonds and macroeconomic variables spanning from December 2009 to December 2024. The de Kort-Vellekooptype methodology is applied to estimate the UFR, incorporating the optimal turning parameter determination technique proposed in this study, which helps mitigate anomalous fluctuations. In addition, both linear and nonlinear machine learning techniques are employed to forecast the UFR and ultra-long-term bond yields. The results indicate that nonlinear machine learning models outperform their linear counterparts in forecasting accuracy. Incorporating macroeconomic variables, particularly price index-related variables, significantly improves the accuracy of predictions. Finally, a novel UFR-based bond yield forecasting model is developed, demonstrating superior performance across different bond maturities.

</details>


### [40] [Core-Periphery Dynamics in Market-Conditioned Financial Networks: A Conditional P-Threshold Mutual Information Approach](https://arxiv.org/abs/2601.00395)
*Kundan Mukhia,Imran Ansari,S R Luwang,Md Nurujjaman*

Main category: q-fin.ST

TL;DR: 使用条件p阈值互信息最小生成树框架分析COVID-19崩盘期间金融市场结构重组，发现崩盘期间网络更整合但更脆弱，崩盘后仅部分恢复


<details>
  <summary>Details</summary>
Motivation: 研究COVID-19崩盘期间金融市场结构如何重组，特别是非线性相互依赖关系的变化，以理解系统性脆弱性

Method: 使用条件p阈值互信息最小生成树框架分析QUAD四国最大股票的非线性依赖关系，通过Hellinger距离和Hilbert谱识别崩盘期，构建网络比较崩盘前、中、后结构变化

Result: 崩盘期间网络更整合但更脆弱：路径更短、中心性更高、代数连通性更低；核心-边缘结构减弱，边缘更脆弱；异配混合促进冲击传播；崩盘后网络仅部分恢复；Gutenberg-Richter分析显示崩盘后大波动事件相对频率更高

Conclusion: 条件p阈值互信息框架有效捕捉非线性相互依赖和系统性脆弱性，所有市场在崩盘期间都表现出类似的结构重组模式，崩盘后网络恢复不完全

Abstract: This study investigates how financial market structure reorganizes during the COVID-19 crash using a conditional p-threshold mutual information (MI) based Minimum Spanning Tree (MST) framework. We analyze nonlinear dependencies among the largest stocks from four diverse QUAD countries: the US, Japan, Australia, and India. Crashes are identified using the Hellinger distance and Hilbert spectrum; a crash occurs when HD = mu\_H + 2*sigma\_H, segmenting data into pre-crash, crash, and post-crash periods. Conditional p-threshold MI filters out common market effects and applies permutation-based significance testing. Resulting validated dependencies are used to construct MST networks for comparison across periods. Networks become more integrated during the crash, with shorter path lengths, higher centrality, and lower algebraic connectivity, indicating fragility. Core-periphery structure declines, with increased periphery vulnerability, and disassortative mixing facilitates shock transmission. Post-crash networks show only partial recovery. Aftershock analysis using the Gutenberg-Richter law indicates higher relative frequency of large volatility events following the crash. Results are consistent across all markets, highlighting the conditional p-threshold MI framework for capturing nonlinear interdependencies and systemic vulnerability.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [41] [Designing Information Delays in Supply Chains](https://arxiv.org/abs/2601.00265)
*Prem Talwai,Rene Caldentey,Avi Giloni,Clifford Hurvich,David Simchi-Levi,Yichen Zhang*

Main category: math.OC

TL;DR: 零售商通过订单流结构隐式传递需求信息给供应商，研究发现分数延迟机制优于纯延迟，利用ARMA策略近似最优滤波器，并分析策略复杂度对供应链成本的影响。


<details>
  <summary>Details</summary>
Motivation: 在缺乏显式信息共享机制的去中心化两级供应链中，研究下游零售商如何通过订单流结构隐式向上游供应商传递需求信息，以改善供应商预测能力并降低系统库存成本。

Method: 引入信息延迟概念，将最优隐式信息共享与零售商订单传递函数的群延迟联系起来；使用Hardy空间分解开发可处理的ARMA策略族，近似理论最优但非有理的极限滤波器；分析策略复杂度（ARMA阶数）对成本的影响，并扩展到记忆受限供应商场景。

Result: 纯延迟机制严格次优，分数延迟机制能重塑订单自相关以改善供应商可预测性；开发的ARMA策略能保持信息延迟特性；策略复杂度对供应链成本有显著影响，在供应商有限预测窗口下，增加策略复杂度可能适得其反。

Conclusion: 零售商可以通过精心设计的订单流结构有效隐式传递需求信息，分数延迟ARMA策略能近似最优性能，策略复杂度需要与供应商的预测能力相匹配，过度复杂化可能带来负面效果。

Abstract: This paper studies how a downstream retailer in a decentralized two-tier supply chain can implicitly transmit demand information to an upstream supplier through the structure of its order stream in the absence of an explicit information-sharing mechanism. We distinguish our work from prior work by introducing the notion of information delay and by linking optimal implicit information sharing to the group delay of the retailer's ordering transfer function. We show that pure delay is strictly suboptimal, while fractional-delay mechanisms can reshape the order autocorrelation to improve supplier forecastability and reduce system-wide inventory costs. Using Hardy-space factorization, we develop a tractable family of invertible ARMA policies that approximates the theoretically optimal (but non-rational) limiting filter derived by Caldentey et al. (2025) and preserves its informational delay properties. This construction yields sharp guidance on how policy complexity, as measured by the degrees of the ARMA policies, impacts supply chain costs. We further extend the analysis to memory-constrained suppliers and characterize how the complexity of the retailer's policy should scale with the supplier's finite forecasting window, highlighting when, perhaps counterintuitively, increasing policy complexity can become counterproductive.

</details>


### [42] [The true detection probability versus the subjective detection probability of a uniformly optimal search plan](https://arxiv.org/abs/2601.00350)
*Liang Hong*

Main category: math.OC

TL;DR: 本文研究了均匀最优搜索计划的真实检测概率与主观概率之间的差异，发现均匀最优搜索计划在真实检测概率方面可能最优也可能非最优，并揭示了基于复合先验的均匀最优搜索计划可能劣于基于不同先验的复合均匀搜索计划。


<details>
  <summary>Details</summary>
Motivation: 研究均匀最优搜索计划在真实检测概率与主观概率之间的差异，探讨搜索理论中的基本问题，解决相关开放性问题。

Method: 通过构建示例分析、理论证明和收敛性分析，研究均匀最优搜索计划在不同先验分布下的真实检测概率表现。

Result: 1) 均匀最优搜索计划在真实检测概率方面可能最优也可能非最优；2) 基于复合先验的均匀最优搜索计划的真实检测概率可能低于基于不同先验的复合均匀搜索计划；3) 证明某个开放问题无解；4) 均匀最优搜索计划的真实检测概率随搜索时间趋于无穷大而收敛到1。

Conclusion: 均匀最优搜索计划在真实检测概率方面存在局限性，不能保证在所有情况下都是最优的，但具有渐近收敛性，为搜索理论提供了重要见解。

Abstract: This article investigates the difference between the true detection probability and the subjective probability of a uniformly optimal search plan. Its main contributions are multi-fold. First, it provides a set of examples to show that, in terms of the true detection probability, the uniformly optimal search plan may or may not be optimal. Secondly, it establishes that the true detection probability of the uniformly optimal search plan based on a composite prior can be less than that of the composite uniformly search plan based on different priors. Next, it argues that an open problem is unsolvable. Finally, it shows that the true detection probability of the uniformly optimal search plan converges to one as the search time approaches infinity.

</details>


### [43] [Completely Positive Reformulations of Polynomial Optimization Problems with Linear Inequality Constraints](https://arxiv.org/abs/2601.00375)
*Haibin Chen,Hong Yan,Guanglu Zhou*

Main category: math.OC

TL;DR: 本文提出了一种新的完全正张量（CPT）锥上的多项式优化问题（POPs）重构方法，特别针对具有线性不等式约束的POPs，证明了强对偶性。


<details>
  <summary>Details</summary>
Motivation: 多项式优化问题在理论和应用中都十分重要，近年来研究集中在将POPs重构为完全正张量锥上的锥规划问题。本文旨在为具有线性不等式约束的POPs提供新的完全正重构方法。

Method: 1. 将问题提升到新的凸优化框架中，变量表示为对称秩一张量的组合；2. 基于提升形式，给出可重构为CPT锥上锥规划的POPs的一般特征；3. 构造所得完全正规划的对偶形式。

Result: 1. 提出了几种新的完全正重构方法；2. 证明了在温和假设下，对偶问题是严格可行的；3. 证明了强对偶性成立。

Conclusion: 本文为具有线性不等式约束的多项式优化问题提供了新的完全正重构框架，建立了理论保证，扩展了多项式优化问题的求解方法。

Abstract: Polynomial optimization encompasses a broad class of problems in which both the objective function and constraints are polynomial functions of the decision variables. In recent years, a substantial body of research has focused on reformulating polynomial optimization problems (POPs) as conic programs over the cone of completely positive tensors (CPTs). In this article, we propose several new completely positive reformulations for a class of POPs with linear inequality constraints. Our approach begins by lifting these problems into a novel convex optimization framework, wherein the variables are represented as combinations of symmetric rank-one tensors. Based on this lifted formulation, we present a general characterization of POPs with linear inequality constraints that can be reformulated as conic programs over the CPT cone. Additionally, we construct the dual formulations of the resulting completely positive programs. Under mild assumptions, we prove that these dual problems are strictly feasible and strong duality holds.

</details>


### [44] [Quadratic Unconstrained Binary Optimisation for Training and Regularisation of Binary Neural Networks](https://arxiv.org/abs/2601.00449)
*Jonas Christoffer Villumsen,Yusuke Sugita*

Main category: math.OC

TL;DR: 该论文提出了一种基于QUBO和伊辛机的二元神经网络训练方法，通过两种新的正则化技术改进训练效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI在移动设备和边缘计算中的部署需求增长，需要更节能的神经网络模型。二元神经网络(BNNs)虽然高效，但由于其离散特性，训练计算成本高。QUBO框架和伊辛机的进展为高效优化离散神经网络提供了可能。

Method: 扩展现有的QUBO模型以适应任意网络拓扑，并提出两种新的正则化方法：1)最大化神经元边距，偏向产生更大预激活幅度的参数配置；2)受dropout启发的迭代方案，训练缩减子网络并调整网络参数的线性惩罚。

Result: 在基于GPU的伊辛机上对小型二元图像分类问题进行实验，结果表明提出的正则化项改变了训练行为，并在未见过的测试数据上提高了分类准确率。

Conclusion: 提出的QUBO框架和正则化方法能够有效改进二元神经网络的训练效果，为在资源受限环境中部署高效AI模型提供了有前景的路径。

Abstract: Advances in artificial intelligence (AI) and deep learning have raised concerns about its increasing energy consumption, while demand for deploying AI in mobile devices and machines at the edge is growing. Binary neural networks (BNNs) have recently gained attention as energy and memory efficient models suitable for resource constrained environments; however, training BNNs exactly is computationally challenging because of its discrete characteristics. Recent work proposing a framework for training BNNs based on quadratic unconstrained binary optimisation (QUBO) and progress in the design of Ising machines for solving QUBO problems suggest a potential path to efficiently optimising discrete neural networks. In this work, we extend existing QUBO models for training BNNs to accommodate arbitrary network topologies and propose two novel methods for regularisation. The first method maximises neuron margins biasing the training process toward parameter configurations that yield larger pre-activation magnitudes. The second method employs a dropout-inspired iterative scheme in which reduced subnetworks are trained and used to adjust linear penalties on network parameters. We apply the proposed QUBO formulation to a small binary image classification problem and conduct computational experiments on a GPU-based Ising machine. The numerical results indicate that the proposed regularisation terms modify training behaviour and yield improvements in classification accuracy on data not present in the training set.

</details>


### [45] [Safe Adaptive Feedback Control via Barrier States](https://arxiv.org/abs/2601.00476)
*Trivikram Satharasi,Tochukwu E. Ogri,Muzaffar Qureshi,Kyle Volle,Rushikesh Kamalapurkar*

Main category: math.OC

TL;DR: 提出了一种基于自适应动态规划和障碍状态增强的非线性控制仿射系统安全反馈控制框架，通过将安全约束嵌入贝尔曼结构，结合并发学习估计器处理参数不确定性，实现闭环稳定性和安全性。


<details>
  <summary>Details</summary>
Motivation: 针对具有参数不确定性的非线性控制仿射系统，需要开发既能保证安全性（如避免障碍物）又能处理未知参数的控制框架。传统方法可能难以同时满足安全约束和参数估计需求。

Method: 采用自适应动态规划（ADP）结合障碍状态增强技术，通过优化显式惩罚障碍状态的价值函数，将安全约束嵌入贝尔曼结构。使用基于模型的强化学习计算近优控制策略，并结合并发学习估计器识别未知参数，无需持续激励条件。

Result: 通过障碍状态李雅普诺夫函数证明了障碍动力学的有界性，并证明了闭环系统的稳定性和安全性。在最优避障问题上的数值仿真验证了所提方法的有效性。

Conclusion: 该框架成功地将安全约束直接嵌入到自适应动态规划结构中，通过并发学习估计器处理参数不确定性，为非线性控制仿射系统提供了一种有效的安全控制解决方案。

Abstract: This paper presents a safe feedback control framework for nonlinear control-affine systems with parametric uncertainty by leveraging adaptive dynamic programming (ADP) with barrier-state augmentation. The developed ADP-based controller enforces control invariance by optimizing a value function that explicitly penalizes the barrier state, thereby embedding safety directly into the Bellman structure. The near-optimal control policy computed using model-based reinforcement learning is combined with a concurrent learning estimator to identify the unknown parameters and guarantee uniform convergence without requiring persistency of excitation. Using a barrier-state Lyapunov function, we establish boundedness of the barrier dynamics and prove closed-loop stability and safety. Numerical simulations on an optimal obstacle-avoidance problem validate the effectiveness of the developed approach.

</details>


### [46] [Variational inference via Gaussian interacting particles in the Bures-Wasserstein geometry](https://arxiv.org/abs/2601.00632)
*Giacomo Borghi,José A. Carrillo*

Main category: math.OC

TL;DR: 提出一种基于高斯概率测度空间的零阶优化算法，利用高斯粒子系统通过共识优化机制自组织寻找全局最小值，在非对数凹目标函数上优于梯度方法


<details>
  <summary>Details</summary>
Motivation: 受变分推断方法启发，需要在高斯概率测度空间中解决优化问题，特别是在非对数凹目标函数情况下，传统梯度方法效果不佳

Method: 基于线性化Bures-Wasserstein空间的高斯粒子系统，通过共识优化机制进行随机探索和自组织，使用平均场近似分析动力学特性

Result: 建立了粒子动力学的适定性并研究了收敛性质，数值实验显示在变分推断任务中，算法对非对数凹目标函数具有鲁棒性和优越性能

Conclusion: 提出的零阶算法在高斯测度空间中有效解决了优化问题，特别适用于非对数凹目标，为变分推断提供了新的计算工具

Abstract: Motivated by variational inference methods, we propose a zeroth-order algorithm for solving optimization problems in the space of Gaussian probability measures. The algorithm is based on an interacting system of Gaussian particles that stochastically explore the search space and self-organize around global minima via a consensus-based optimization (CBO) mechanism. Its construction relies on the Linearized Bures-Wasserstein (LBW) space, a novel parametrization of Gaussian measures we introduce for efficient computations. LBW is inspired by linearized optimal transport and preserves key geometric features while enabling computational tractability. We establish well-posedness and study the convergence properties of the particle dynamics via a mean-field approximation. Numerical experiments on variational inference tasks demonstrate the algorithm's robustness and superior performance with respect to gradient-based method in presence of non log-concave targets.

</details>


### [47] [Stability of vehicular admission control schemes in urban traffic networks under modelling uncertainty](https://arxiv.org/abs/2601.00732)
*Michalis Ramp,Andreas Kasis,Stelios Timotheou*

Main category: math.OC

TL;DR: 本文研究了去中心化车辆准入控制策略在建模不确定性下的稳定性，利用无源性理论推导了可扩展的局部可验证条件，确保交通网络稳定。


<details>
  <summary>Details</summary>
Motivation: 城市交通网络面临严重拥堵问题，车辆准入控制策略虽能缓解拥堵，但其在建模不确定性下的稳定性尚未充分研究。现有VAC策略在非线性动态和大规模异质交通网络中的稳定性保障不足。

Method: 采用无源性理论分析去中心化VAC方案的稳定性。考虑具有非线性动态和带边界不确定性的凹宏观基本图的大规模异质交通网络，推导可扩展的局部可验证设计条件。

Result: 建立了去中心化VAC方案在建模不确定性下的稳定性保证条件。通过6区域系统的数值仿真验证了理论结果的有效性和实际相关性。

Conclusion: 提出的设计框架为去中心化车辆准入控制策略提供了在建模不确定性下的稳定性保障，具有可扩展性和实际应用价值。

Abstract: Urban transportation networks face significant challenges due to traffic congestion, leading to adverse environmental and socioeconomic impacts. Vehicular admission control (VAC) strategies have emerged as a promising solution to alleviate congestion. By leveraging information and communication technologies, VAC strategies regulate vehicle entry into the network to optimize different traffic metrics of interest over space and time. Despite the significant development of VAC strategies, their stability at the presence of modelling uncertainty remains under-explored. This paper investigates the stability properties of a class of decentralized VAC schemes under modelling uncertainty. Specifically, we consider large-scale, heterogeneous urban traffic networks characterised by nonlinear dynamics and concave macroscopic fundamental diagrams with bounded uncertainty between flow, density, and speed. In this context, we examine a broad class of decentralized VAC dynamics, described by general nonlinear forms. Using passivity theory, we derive scalable, locally verifiable conditions on the design of VAC schemes, that enable stability guarantees in the presence of modelling uncertainty. Several examples are presented to illustrate the applicability of the proposed design framework. Our analytical results are validated through numerical simulations on a 6-region system, demonstrating their effectiveness and practical relevance.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [48] [Uncertainty-Adjusted Sorting for Asset Pricing with Machine Learning](https://arxiv.org/abs/2601.00593)
*Yan Liu,Ye Luo,Zigan Wang,Xiaowei Zhang*

Main category: q-fin.PM

TL;DR: 使用不确定性调整的预测边界而非点预测来排序资产，能改善投资组合表现，主要降低波动率，对灵活机器学习模型效果最显著


<details>
  <summary>Details</summary>
Motivation: 机器学习在实证资产定价中很重要，但投资组合构建仍主要依赖点预测，忽略了资产特定的估计不确定性。需要一种能利用不确定性信息的方法来改进投资组合表现。

Method: 提出简单改变：使用不确定性调整的预测边界来排序资产，而不是仅使用点预测。在多种机器学习模型和美国股票面板数据上进行测试。

Result: 相对于点预测排序，该方法改善了投资组合表现。即使使用部分或错误指定的不确定性信息，这些收益仍然存在。主要来自波动率的降低，对灵活的机器学习模型效果最显著。

Conclusion: 不确定性调整的预测边界能有效改善投资组合表现，这些改进主要由资产层面的不确定性驱动，而非时间或总体预测不确定性。该方法简单有效，特别适合灵活的机器学习模型。

Abstract: Machine learning is central to empirical asset pricing, but portfolio construction still relies on point predictions and largely ignores asset-specific estimation uncertainty. We propose a simple change: sort assets using uncertainty-adjusted prediction bounds instead of point predictions alone. Across a broad set of ML models and a U.S. equity panel, this approach improves portfolio performance relative to point-prediction sorting. These gains persist even when bounds are built from partial or misspecified uncertainty information. They arise mainly from reduced volatility and are strongest for flexible machine learning models. Identification and robustness exercises show that these improvements are driven by asset-level rather than time or aggregate predictive uncertainty.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [49] [Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference](https://arxiv.org/abs/2601.00038)
*Shane A. McQuarrie,Mengwu Guo,Anirban Chaudhuri*

Main category: stat.ML

TL;DR: 提出一种主动学习框架，通过贝叶斯线性回归建立概率性参数化算子推断模型，利用预测不确定性设计自适应采样策略，以提升数据驱动降阶模型的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动降阶模型作为数字孪生中虚拟资产的基础，其质量对有限训练数据敏感。需要识别最佳训练参数，以在相同计算预算下获得更稳定准确的参数化降阶模型。

Method: 采用算子推断方法，建立参数化算子推断的概率版本（贝叶斯线性回归），利用概率降阶模型的预测不确定性设计顺序自适应采样方案，选择能提升全局稳定性和准确性的新训练参数向量。

Result: 在多个非线性参数化偏微分方程系统上的数值实验表明，与随机采样相比，所提出的自适应采样策略在相同计算预算下能一致地产生更稳定、更准确的降阶模型。

Conclusion: 主动学习框架通过自适应采样策略有效提升了数据驱动降阶模型的性能，为数字孪生应用提供了更可靠的虚拟资产基础。

Abstract: This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget.

</details>


### [50] [Detecting Unobserved Confounders: A Kernelized Regression Approach](https://arxiv.org/abs/2601.00200)
*Yikai Chen,Yunxin Mao,Chunyuan Zheng,Hao Zou,Shanzhi Gu,Shixuan Liu,Yang Shi,Wenjing Yang,Kun Kuang,Haotian Wang*

Main category: stat.ML

TL;DR: 提出KRCD方法，用于检测非线性单环境观测数据中的未观测混杂因素，通过比较标准和高阶核回归来检测混杂效应。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要线性假设或多个异质环境，限制了在非线性单环境设置中的应用，需要一种能在这些条件下检测未观测混杂因素的方法。

Method: 使用再生核希尔伯特空间建模复杂依赖关系，通过比较标准和高阶核回归来推导检验统计量，显著偏离零值表明存在未观测混杂。

Result: 理论证明：无限样本下回归系数相等当且仅当无未观测混杂；有限样本差异收敛于零均值高斯分布。实验显示KRCD优于现有基线且计算效率高。

Conclusion: KRCD是首个能在非线性单环境设置中检测未观测混杂因素的有效方法，具有理论保证和实际应用价值。

Abstract: Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency.

</details>


### [51] [Generative Conditional Missing Imputation Networks](https://arxiv.org/abs/2601.00517)
*George Sun,Yi-Hui Zhou*

Main category: stat.ML

TL;DR: 提出生成式条件缺失值插补网络(GCMI)，通过链式方程的多重插补框架增强鲁棒性和准确性，在MCAR和MAR机制下表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 数据缺失值是统计分析中的重要问题，现有方法在处理缺失值插补时存在局限性，需要更鲁棒和准确的解决方案

Method: 1. 提出生成式条件缺失值插补网络(GCMI)的理论框架；2. 通过链式方程方法集成多重插补框架，增强模型稳定性和插补性能

Result: 在模拟实验和基准数据集上的实证评估表明，GCMI方法在MCAR和MAR缺失机制下，相比现有领先插补技术具有更优越的效能

Conclusion: GCMI不仅具有实际应用价值，而且有潜力成为统计数据分析领域的领先工具，为缺失值处理提供了有效的解决方案

Abstract: In this study, we introduce a sophisticated generative conditional strategy designed to impute missing values within datasets, an area of considerable importance in statistical analysis. Specifically, we initially elucidate the theoretical underpinnings of the Generative Conditional Missing Imputation Networks (GCMI), demonstrating its robust properties in the context of the Missing Completely at Random (MCAR) and the Missing at Random (MAR) mechanisms. Subsequently, we enhance the robustness and accuracy of GCMI by integrating a multiple imputation framework using a chained equations approach. This innovation serves to bolster model stability and improve imputation performance significantly. Finally, through a series of meticulous simulations and empirical assessments utilizing benchmark datasets, we establish the superior efficacy of our proposed methods when juxtaposed with other leading imputation techniques currently available. This comprehensive evaluation not only underscores the practicality of GCMI but also affirms its potential as a leading-edge tool in the field of statistical data analysis.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 该论文系统评估了工业异常检测算法在不同类别不平衡程度下的性能，发现最佳检测器取决于训练数据中故障样本的总数，而非健康样本数量。


<details>
  <summary>Details</summary>
Motivation: 工业系统中机器学习面临极端类别不平衡的挑战，主要由于训练期间故障数据有限。需要评估异常检测算法在真实工程约束下的性能，为工业应用提供实用指导。

Method: 使用问题无关的模拟数据集（2D和10D超球面异常分布），在异常率0.05%-20%、训练规模1000-10000的范围内，对14种检测器进行基准测试，测试集规模为40000。

Result: 最佳检测器高度依赖于训练数据中故障样本总数：故障样本少于20时，无监督方法（kNN/LOF）占优；30-50个故障样本时，半监督（XGBOD）和监督（SVM/CatBoost）方法性能大幅提升。健康样本增加在多数情况下收益有限。

Conclusion: 研究揭示了异常检测方法在小数据集上的泛化性能下降，为工业环境中部署异常检测提供了实用见解，强调了根据可用故障样本数量选择合适方法的重要性。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [53] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

TL;DR: 将Yahtzee游戏建模为MDP，使用多种策略梯度方法训练自博弈智能体，发现A2C在固定训练预算下表现最稳健，达到接近最优性能的241.78分（最优DP分数为254.59）。


<details>
  <summary>Details</summary>
Motivation: Yahtzee游戏具有随机性、组合结构和延迟奖励的特点，是中等规模RL的良好基准。单人游戏可用动态规划求解，但多人游戏难以处理，需要近似方法。

Method: 将Yahtzee建模为马尔可夫决策过程，使用REINFORCE、A2C和PPO等策略梯度方法训练自博弈智能体，采用共享主干的多头网络架构，并对特征编码、动作编码、架构、回报估计器和熵正则化进行消融实验。

Result: 在固定训练预算下，REINFORCE和PPO对超参数敏感且无法达到接近最优性能，而A2C在各种设置下都能稳健训练。最佳智能体在10万次评估游戏中获得中位数分数241.78分，接近最优DP分数254.59的95%，上区奖励和Yahtzee达成率分别为24.9%和34.1%。

Conclusion: A2C是Yahtzee游戏中表现最稳健的方法，但所有模型都难以学习上区奖励策略，过度关注四骰同花，突显了长期信用分配和探索的持续挑战。

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [54] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文提出了一种针对LLM组合技术的攻击方法，通过设计一个在捐赠模型中功能惰性但在移植到基础模型后能可靠重构为恶意特征的"破坏令牌"，利用系数重用的几何特性破坏基础模型的生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重的LLM生态系统越来越多地使用模型组合技术（如权重合并、推测解码和词汇扩展），这些方法在不同模型家族间应用的关键前提是令牌移植，即将不兼容的词汇表对齐到共享嵌入空间。作者发现这一关键互操作性步骤引入了供应链漏洞。

Method: 作者将攻击形式化为双目标优化问题，并使用稀疏求解器实例化攻击。通过利用系数重用的几何特性，设计一个在捐赠模型中功能惰性但在移植到基础模型后能可靠重构为恶意特征的"破坏令牌"，创建不对称可实现性差距。

Result: 攻击无需训练，通过谱模仿逃避异常检测，同时在微调和权重合并后仍保持结构持久性。攻击能有效破坏基础模型的生成能力，而捐赠模型的效用与正常行为在统计上无法区分。

Conclusion: 研究揭示了模块化AI组合流程中的隐藏风险，即令牌移植这一关键互操作性步骤可能被恶意利用，通过设计特定的"破坏令牌"来破坏模型组合的安全性。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [55] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 该论文提出了一种多用户上下文赌博机框架，通过图拉普拉斯正则化与核方法的结合，将多用户奖励函数学习统一到单一的多用户再生核希尔伯特空间中，并设计了基于高斯过程的探索算法。


<details>
  <summary>Details</summary>
Motivation: 在多用户上下文赌博机问题中，用户通过图结构相关联，且奖励函数表现出非线性行为和图同质性。现有方法通常独立处理用户或仅考虑线性关系，缺乏统一的理论框架来同时处理非线性奖励和图结构信息。

Method: 提出了一种联合惩罚项，结合基于RKHS距离的图平滑项和个体粗糙度惩罚。证明了该惩罚等价于单一多用户RKHS中的平方范数，并显式推导了其再生核，该核将图拉普拉斯与基础臂核优雅地融合。基于此设计了LK-GP-UCB和LK-GP-TS算法，利用新核上的高斯过程后验进行探索。

Result: 提供了高概率遗憾界，其缩放依赖于多用户核的有效维度，而非用户数量或环境维度。实验表明，在非线性设置中，该方法优于强线性和非图感知基线，即使在真实奖励为线性的情况下也保持竞争力。

Conclusion: 该工作提供了一个统一、理论严谨且实用的框架，将拉普拉斯正则化与核化赌博机相结合，用于结构化探索，为多用户上下文赌博机问题提供了新的解决方案。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [56] [IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business](https://arxiv.org/abs/2601.00075)
*Swetha Varadarajan,Abhishek Ray,Lumina Albert*

Main category: cs.LG

TL;DR: IMBWatch是一个基于时空图神经网络的框架，用于大规模检测伪装成合法按摩服务的非法按摩店网络，这些店铺涉及人口贩运和性剥削。


<details>
  <summary>Details</summary>
Motivation: 非法按摩店（IMBs）以合法健康服务为掩护，从事人口贩运、性剥削和强迫劳动等犯罪活动。传统检测方法（如社区举报和监管检查）反应迟钝且效果有限，难以揭示犯罪网络的全貌。IMBs使用加密广告、频繁更换人员和地点、共享电话号码和地址等策略，使得检测工作极具挑战性。

Method: IMBWatch构建了一个时空图神经网络（ST-GNN）框架，从开源情报（包括在线广告、营业执照记录和众包评论）构建动态图。节点代表企业、别名、电话号码和位置等异构实体，边捕捉时空和关系模式（如共址、重复电话使用、同步广告）。该框架结合图卷积操作和时间注意力机制，建模IMB网络在时间和空间上的演变，捕捉跨城市工人流动、一次性电话轮换和协调广告激增等模式。

Result: 在美国多个城市的真实数据集上的实验表明，IMBWatch在准确率和F1分数上优于基线模型。除了性能提升外，该框架还提供了更好的可解释性，为主动和有针对性的干预提供可操作的见解。

Conclusion: IMBWatch是一个可扩展、可适应其他非法领域的框架，通过匿名数据和开源代码支持可重复研究。该框架能够有效检测复杂的IMB网络，支持执法机构进行更有效的干预。

Abstract: Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.

</details>


### [57] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 本文提出了一种渐近置信度的最佳臂识别新框架，通过放宽精确误差控制要求，在长时域场景下实现更紧的最优性和更好的非参数分布处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统固定置信度BAI方法在实际应用中存在局限性：严格的精确误差控制需要使用宽松的尾不等式和/或参数限制，导致效率低下。现实世界中的弱信号、高显著性要求和实验后推断需求往往需要长时域，这为渐近方法提供了适用场景。

Method: 引入渐近误差控制框架，要求误差控制相对于最小样本量渐近有效。开发了新颖的渐近任意时间有效置信序列，并基于此设计新的BAI算法。该方法灵活整合协变量进行方差缩减，确保在完全非参数设置下的近似误差控制。

Result: 在温和收敛假设下，提供了样本复杂度的渐近界限，并证明该方法的最坏情况样本复杂度与高斯BAI在精确误差保证和已知方差下的最佳情况样本复杂度相匹配。实验表明该方法在保持误差控制的同时减少了平均样本复杂度。

Conclusion: 提出的渐近框架克服了传统BAI方法的局限性，在长时域、弱信号、高显著性要求的实际场景中实现了更优的性能，同时能更好地处理非参数分布和充分利用个体级上下文信息。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [58] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 提出基于扩散的软重参数化方法，用于分类变量的梯度优化，通过高斯噪声过程的去噪器实现高效计算，无需训练即可采样和反向传播。


<details>
  <summary>Details</summary>
Motivation: 分类变量的梯度优化通常依赖有噪声的得分函数估计器或有偏的连续松弛方法。现有方法要么噪声大，要么优化有偏的目标函数，需要更好的重参数化技术。

Method: 引入基于扩散的软重参数化方法，利用高斯噪声过程对分类分布进行处理，其去噪器有闭式解且计算高效，实现无需训练的扩散采样器，支持反向传播。

Result: 实验表明，提出的重参数化技巧在各种基准测试中取得了竞争性或改进的优化性能。

Conclusion: 扩散基重参数化为分类变量的梯度优化提供了有效的新方法，结合了高效计算和良好优化性能的优势。

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>


### [59] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: NeuroSymBO通过贝叶斯优化自适应选择推理策略，解决LLM方程发现中的指令脆弱性问题，显著提升PDE发现性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在方程发现中表现出潜力，但输出对提示词高度敏感（指令脆弱性）。静态提示无法适应多步生成过程的演化状态，导致模型停留在次优解

Method: 将提示工程重构为序列决策问题，维护离散推理策略库，使用贝叶斯优化基于数值反馈在每一步选择最优指令

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现了更高的恢复率和更简洁的解

Conclusion: NeuroSymBO通过自适应指令选择有效解决了LLM方程发现中的指令脆弱性问题，提升了发现性能和解的简洁性

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [60] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个几何强化学习框架，用于在未知环境中进行同时导航与建图。该方法通过局部能量景观编码可达性和障碍约束，使用哈密顿优化进行动态最短路径搜索，无需构建全局地图。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中进行同时导航与建图具有挑战性，传统方法需要构建全局地图或设计复杂的多智能体策略。本文旨在开发一种仅依赖局部感知观测的方法，避免全局建图的开销。

Method: 将路径导航和建图建模为动态最短路径搜索和发现过程，使用受控哈密顿优化：将感知输入转换为局部能量景观，编码可达性、障碍屏障和变形约束；通过更新哈密顿量来演化感知、规划和重配置策略。

Result: 在两种不同的2D导航任务上评估，相比局部反应式基线和全局策略学习方法，GRL-SNAM保持了安全距离，能够泛化到未见过的布局，并通过局部能量优化实现高质量导航，而非依赖广泛的全局建图。

Conclusion: 通过哈密顿量更新的几何强化学习能够通过局部能量优化实现高质量导航，仅需最小化探索，无需构建全局地图，为未知环境中的同时导航与建图提供了有效解决方案。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [61] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 本文比较了在数据稀缺环境下基于嵌入的金融情感分析方法，发现预训练嵌入在小数据集上效果有限，验证集过小会导致过拟合，建议采用少样本学习等替代方案。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析能增强市场理解，但标准NLP方法在小数据集上遇到显著挑战。本研究旨在评估资源受限环境下基于嵌入的金融新闻情感分类方法的有效性。

Method: 比较了Word2Vec、GloVe和句子转换器表示方法，结合梯度提升算法在手动标注的新闻标题上进行评估。通过验证集和测试集性能对比分析模型效果。

Result: 实验结果显示验证集和测试集性能存在显著差距，模型表现甚至不如简单基线方法。预训练嵌入在数据量低于临界阈值时收益递减，小验证集导致模型选择过程中的过拟合。

Conclusion: 嵌入质量本身无法解决情感分类中的数据稀缺问题。对于资源有限的实践者，当标注样本稀缺时，应考虑少样本学习、数据增强或词典增强的混合方法等替代方案。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [62] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 研究非马尔可夫状态和成本过程下线性函数近似的强化学习方法，包括策略评估和Q学习的收敛性分析，并应用于部分可观测马尔可夫决策过程。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习通常假设马尔可夫过程，但实际应用中状态和成本过程可能是非马尔可夫的。需要研究在这种更一般情况下的学习算法收敛性。

Method: 1. 针对策略评估方法，在适当的遍历性条件下证明算法收敛；2. 对于Q学习，在基函数基于量化映射的特殊情况下证明收敛；3. 将结果应用于部分可观测马尔可夫决策过程，使用有限记忆变量作为状态表示。

Result: 1. 策略评估算法在非马尔可夫过程下收敛，极限对应于正交投影和辅助马尔可夫决策过程贝尔曼算子的联合算子的不动点；2. Q学习在特定基函数选择下收敛；3. 为部分可观测马尔可夫决策过程的学习算法推导了明确的误差界。

Conclusion: 该研究扩展了线性函数近似强化学习到非马尔可夫环境，为部分可观测系统提供了理论保证，但在一般Q学习收敛性方面仍有局限性。

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [63] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 提出基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，获得比传统参数平均更准确的全局奖励估计。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，异构智能体在不同环境中执行共同任务时，直接共享数据学习统一奖励函数存在困难：动态差异、隐私限制和通信带宽限制。

Method: 1) 每个客户端本地执行轻量级最大熵逆强化学习；2) 通过Wasserstein重心融合各客户端的奖励函数，考虑其底层几何结构；3) 证明该方法比传统联邦学习参数平均方法更准确。

Result: 该方法提供了原则性且通信高效的框架，能够在异构智能体和环境中推导出共享的奖励函数，且比传统参数平均方法产生更忠实的全局奖励估计。

Conclusion: 基于最优传输的联邦逆强化学习方法能够有效解决异构多智能体系统中的奖励函数学习问题，在保护隐私和通信效率的同时获得更好的泛化性能。

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [64] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 使用XGBoost模型分析美国交通事故严重程度预测，发现时间、地理位置和天气变量是关键预测因素，但模型对极端严重事故预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对美国交通事故严重程度的预测能力，为基于证据的交通管理提供支持。

Method: 使用2016-2023年50万起美国交通事故数据集，训练XGBoost分类器，通过随机搜索交叉验证优化，并采用类别加权处理类别不平衡问题。

Result: 模型整体准确率78%，对多数类（严重程度2级）表现良好（精确率和召回率87%）。特征重要性分析显示时间、地理位置、能见度、温度和风速是最强预测因子，但降水和能见度预测能力有限。

Conclusion: 研究为交通管理提供了实证依据，但数据集以中等严重程度事故为主限制了极端案例预测能力，未来需要改进采样策略、特征工程和外部数据整合。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [65] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE：一个轻量级框架，通过添加基于粒子群的探索层增强标准策略梯度方法，在非平稳奖励和高维策略任务中显著提升探索能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略场景下。现有方法在复杂任务中探索不足，需要更有效的探索机制。

Method: ARISE框架在标准策略梯度方法基础上添加紧凑的粒子群探索层，将策略动作与粒子驱动的建议混合。每个粒子代表动作空间中的候选策略轨迹，使用奖励方差线索自适应调节探索强度。

Result: 在简单任务上仅有轻微改进（如CartPole-v1 +0.7%），但在复杂任务上获得显著提升：LunarLander-v3 +46%，Hopper-v4 +22%，同时在Walker2d和Ant上保持稳定。在非平稳奖励变化下，ARISE表现出明显鲁棒性优势，在CartPole上比PPO高出75分。

Conclusion: ARISE提供了一个简单、架构无关的途径，在不改变核心算法结构的情况下，实现更具探索性和鲁棒性的RL智能体。消融研究证实粒子群组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [66] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种使用纯强化学习梯度进行决策变换器在线微调的新方法，解决了现有方法依赖监督学习目标的问题，在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但将其扩展到在线设置时，现有方法仍然严重依赖监督序列建模目标。作者发现后见回报重新标注这一标准组件与基于重要性采样的强化学习算法不兼容，导致训练不稳定，因此需要开发纯强化学习梯度的在线微调方法。

Method: 将GRPO算法适配到决策变换器，并引入关键修改：1) 子轨迹优化以改进信用分配；2) 序列级似然目标以增强稳定性和效率；3) 主动采样以鼓励在不确定区域进行探索。

Result: 通过大量实验证明，该方法在多个基准测试中超越了现有的在线决策变换器基线，实现了新的最先进性能。

Conclusion: 纯强化学习梯度的在线微调对于决策变换器是有效的，提出的方法解决了后见回报重新标注与重要性采样算法之间的不兼容问题，为决策变换器的在线应用提供了新途径。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [67] [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](https://arxiv.org/abs/2601.00737)
*Uğurcan Özalp*

Main category: cs.LG

TL;DR: STAC算法通过引入分布评论家网络建模时间回报不确定性，利用随机性不确定性而非认知不确定性来缩放悲观偏差，解决了评论家网络高估问题，同时通过dropout正则化提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 离策略演员-评论家方法虽然样本效率高，但评论家网络存在系统性高估问题。现有方法使用集成技术量化认知不确定性来引入悲观偏差，但这种方法计算成本高，且未充分利用环境随机性。

Method: 提出STAC算法：1) 使用单一分布评论家网络建模时间回报不确定性（包括随机转移、奖励和策略诱导的变异性）；2) 利用时间随机性不确定性而非认知不确定性来缩放TD更新中的悲观偏差；3) 对评论家和演员网络应用dropout进行正则化。

Result: 实验表明：1) 基于分布评论家的悲观偏差足以缓解高估问题；2) 在随机环境中自然产生风险规避行为；3) dropout进一步提高了训练稳定性和性能；4) 使用单一网络设计提升了计算效率。

Conclusion: STAC通过利用时间随机性不确定性而非认知不确定性来缩放悲观偏差，有效解决了评论家高估问题，同时通过dropout正则化提升了性能，实现了计算效率和性能的平衡。

Abstract: Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.

</details>


### [68] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一个用于在向下封闭凸体上最大化非负、非单调γ-弱DR-次模函数的近似算法，当γ=1时恢复0.401近似比，γ<1时性能优雅下降


<details>
  <summary>Details</summary>
Motivation: 在机器学习和优化中，最大化约束下的次模目标是一个基本问题。研究非负、非单调γ-弱DR-次模函数在向下封闭凸体上的最大化问题，需要处理非单调性和弱次模性

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，形成简单有效的处理非单调性的方法

Result: 算法保证平滑依赖于γ：γ=1时恢复0.401近似比，γ<1时保证优雅下降，改进了相同约束下γ-弱DR-次模最大化的先前边界

Conclusion: 该方法为向下封闭凸体上的非单调γ-弱DR-次模最大化提供了最先进的保证，通过连续贪婪和双贪婪的巧妙结合有效处理了非单调性

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [69] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: Sequential Reservoir Computing通过将大储层分解为一系列小储层，解决了传统RC在高维时空系统预测中的扩展性问题，实现了更长的预测时间、更低的误差和更低的训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM在高维时空系统预测中存在梯度训练和内存瓶颈问题，传统RC虽然缓解了这些问题但扩展性仍然不佳，需要一种更高效且可扩展的预测方法。

Method: 提出Sequential Reservoir Computing架构，将大型储层分解为一系列小型互连储层，保持长期时间依赖性的同时降低内存和计算成本。

Result: 在低维混沌系统和高维物理模拟中，Sequential RC相比LSTM和标准RNN基线实现了15-25%更长的有效预测时间、20-30%更低的误差指标，训练成本降低达三个数量级。

Conclusion: Sequential RC保持了传统RC的简单性和效率，同时实现了对高维动力系统的卓越扩展性，为科学和工程应用中的实时、节能预测提供了实用途径。

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [70] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 使用电子健康记录数据开发机器学习模型预测肝硬化的发生，相比传统FIB-4评分在1-3年预测窗口均有显著性能提升


<details>
  <summary>Details</summary>
Motivation: 肝硬化早期预测对预防和管理至关重要，传统FIB-4评分性能有限，需要利用常规电子健康记录数据开发更准确的机器学习预测模型

Method: 回顾性队列研究，使用大型学术医疗系统的去标识化电子健康记录数据，识别脂肪肝患者并分为肝硬化与非肝硬化队列，构建1-3年预测场景，使用XGBoost模型整合人口统计学、诊断、实验室结果等特征

Result: 机器学习模型在所有预测窗口均优于FIB-4：XGBoost在1年、2年、3年预测的AUC分别为0.81、0.73、0.69，而FIB-4为0.71、0.63、0.57，性能提升随预测时间延长而保持

Conclusion: 基于常规电子健康记录数据的机器学习模型显著优于传统FIB-4评分，可实现更早、更准确的风险分层，可作为自动化决策支持工具集成到临床工作流程中，支持主动的肝硬化预防和管理

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [71] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应重复编码框架，实现按维度不等错误保护，在有限带宽下显著提升语义通信性能


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限通信系统中语义意义保持的挑战，传统信道编码（如LDPC、Reed-Solomon）无法实现细粒度语义保护

Method: 基于强化学习的自适应重复编码框架，使用复合语义失真度量（平衡全局嵌入相似性和实体级保持），实现按维度不等错误保护

Result: 在1 dB SNR下，比均匀保护获得6.8%更高的chrF分数和9.3%更好的实体保持，统计显著

Conclusion: 简单但智能分配的重复编码可实现细粒度语义保护，代码结构必须与语义粒度对齐，适用于边缘计算和物联网场景

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [72] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出SSI-GAN，一种半监督Swin Transformer启发的GAN架构，仅需1-3%标注数据即可高精度分类蚊子神经元尖峰信号，检测寨卡、登革热病毒感染，大幅减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 蚊子是虫媒病毒主要传播媒介，手动分类神经元尖峰模式耗时耗力。现有深度学习方案需要完全标注的数据集和高度预处理的信号，难以在实际场景大规模应用。需要解决标注数据稀缺问题。

Method: 提出SSI-GAN（半监督Swin启发GAN），采用Swin Transformer启发的移位窗口判别器和基于Transformer的生成器。使用多头自注意力模型在平面窗口式Transformer判别器中学习稀疏高频尖峰特征。仅用1-3%标注数据，训练超过1500万尖峰样本，使用贝叶斯Optuna框架优化超参数，五折蒙特卡洛交叉验证验证鲁棒性。

Result: 感染后第三天仅用3%标注数据达到99.93%分类准确率；仅1%监督在所有感染阶段保持高准确率；相比标准监督方法减少97-99%人工标注工作量；移位窗口Transformer设计大幅超越所有基线，创下尖峰神经元感染分类新纪录。

Conclusion: SSI-GAN通过半监督学习和Transformer架构，显著减少对标注数据的依赖，在蚊子神经元尖峰分类和病毒神经嗜性检测方面表现出色，为实际现场应用提供了可行的解决方案。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [73] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出一种资源高效的数据中心框架，通过特征工程使心律失常数据线性可分，实现98.44%准确率、8.54KB模型大小和0.46μs推理延迟，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死因，需要持续监测。现有深度学习方法计算开销大，不适合资源受限的边缘设备，需要更高效的解决方案。

Method: 提出资源高效的数据中心框架，整合时频小波分解和图论结构描述符（如PageRank中心性），构建混合特征空间，使用互信息和递归消除进行特征选择，采用可解释的超轻量线性分类器。

Result: 在MIT-BIH和INCART数据集上达到98.44%诊断准确率，模型大小仅8.54KB，分类推理延迟0.46μs，每搏处理管道52ms，实时运行。相比压缩模型KD-Light（25KB，96.32%准确率）有数量级效率提升。

Conclusion: 该框架通过特征工程优先于复杂性的方法，实现了资源受限边缘设备上的高效心律失常监测，为无电池心脏传感器提供了可行的解决方案。

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [74] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: 提出利用未标注的互联网数据（wild data）来增强生成模型归因的方法，通过约束优化在保持目标模型识别性能的同时，提高对未知生成器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型的快速发展，需要超越简单的真假检测，实现特定生成模型的归因。现有方法在已知生成器上表现良好，但难以泛化到未见过的、新发布的生成器。

Method: 1. 使用CLIP特征和线性分类器建立基线；2. 提出约束优化方法，利用未标注的互联网数据（wild data），鼓励将wild样本分类为非目标模型，同时约束在标注数据上的性能保持高水平。

Result: 实验结果表明，引入wild数据显著提高了对具有挑战性的未见生成器的归因性能，证明未标注的互联网数据可以有效增强开放世界设置下的AI生成内容归因。

Conclusion: 利用未标注的互联网数据可以有效解决生成模型归因中的泛化问题，在开放世界设置下显著提升对未知生成器的识别能力。

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [75] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: 提出对抗性图提示（AGP）框架，首次将对抗学习融入图提示中，通过min-max优化解决图神经网络微调中的噪声和攻击脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在图拓扑和节点特征上对各种噪声和攻击表现出显著脆弱性，需要提高图神经网络微调的鲁棒性。

Method: 提出AGP框架：1) 将问题形式化为min-max优化问题，采用交替优化方案；2) 内层最大化使用联合投影梯度下降（JointPGD）生成强对抗噪声；3) 外层最小化学习最优节点提示来对抗噪声。

Result: AGP能理论处理图拓扑和节点噪声，是通用方法可与多种预训练GNN模型集成，在多个基准任务上的实验验证了其优于现有方法的鲁棒性和有效性。

Conclusion: AGP首次将对抗学习融入图提示，通过min-max优化框架实现了鲁棒的图神经网络微调，能有效应对各种图噪声，提高下游任务的鲁棒性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [76] [GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation](https://arxiv.org/abs/2601.00231)
*Pritish Saha,Chandrav Rajbangshi,Rudra Goyal,Mohit Goyal,Anurag Deo,Biswajit Roy,Ningthoujam Dhanachandra Singh,Raxit Goswami,Amitava Das*

Main category: cs.LG

TL;DR: GRIT是一种动态、曲率感知的LoRA方法，通过K-FAC预条件梯度、周期性重投影到Fisher特征方向、自适应调整有效秩，在减少46%可训练参数的同时，性能优于传统LoRA/QLoRA。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA和QLoRA方法存在几何无关性缺陷：它们在固定、随机方向的低秩子空间中优化，主要使用一阶下降，忽略了局部损失曲率。这会导致有效更新预算膨胀，并沿着弱约束方向放大漂移。

Method: GRIT保持LoRA参数化但引入三个关键改进：1) 使用K-FAC作为自然梯度代理在秩空间中对梯度进行预条件处理；2) 定期将低秩基重投影到主导Fisher特征方向上以抑制漂移；3) 根据谱自适应调整有效秩，使容量集中在信号存在的地方。

Result: 在LLaMA骨干网络上的指令遵循、理解和推理基准测试中，GRIT匹配或超越LoRA和QLoRA，同时平均减少46%的可训练参数（任务间25-80%），在各种提示风格和数据混合下没有实际质量损失。GRIT显示出更低的漂移和更好的更新与保留边界。

Conclusion: GRIT通过引入曲率感知的动态调整机制，解决了传统PEFT方法的几何无关性缺陷，在显著减少参数的同时保持或提升性能，为参数高效微调提供了更优的解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).

</details>


### [77] [Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering](https://arxiv.org/abs/2601.00276)
*Hongxi Li,Chunlin Huang*

Main category: cs.LG

TL;DR: 宽L2正则化网络中监督学习本质上是压缩性的，核秩受类别数C限制，SGD噪声也是低秩的，这与自监督学习的高秩表示形成对比。


<details>
  <summary>Details</summary>
Motivation: 研究宽L2正则化网络中特征学习的理论，探索监督学习的内在压缩特性，统一确定性和随机性视角下的对齐现象，并对比监督学习和自监督学习在表示结构上的差异。

Method: 提出一个理论框架，推导出预测"水填充"谱演化的核ODE，证明稳定稳态下核秩受类别数C限制，并分析SGD噪声的低秩特性。

Result: 监督学习中的核秩受类别数C限制，SGD噪声也是低秩的（O(C)），动力学被限制在任务相关子空间内，与自监督学习的高秩、扩张性表示形成鲜明对比。

Conclusion: 监督学习本质上是压缩性的，其特征表示受任务类别数限制，而自监督学习产生高秩表示，这一框架统一了对齐的确定性和随机性视角。

Abstract: We present a theory of feature learning in wide L2-regularized networks showing that supervised learning is inherently compressive. We derive a kernel ODE that predicts a "water-filling" spectral evolution and prove that for any stable steady state, the kernel rank is bounded by the number of classes ($C$). We further demonstrate that SGD noise is similarly low-rank ($O(C)$), confining dynamics to the task-relevant subspace. This framework unifies the deterministic and stochastic views of alignment and contrasts the low-rank nature of supervised learning with the high-rank, expansive representations of self-supervision.

</details>


### [78] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 论文提出基于国际象棋战术的量子基准测试QKRD，用于评估QAOA算法在结构化问题上的表现，发现约束保持混合器、热启动策略等能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有QAOA基准测试主要使用MaxCut、TSP、SAT等随机合成实例，这些实例缺乏语义结构和人类可解释性，无法反映真实世界问题的约束特性，限制了算法在实际应用中的性能评估。

Method: 引入Quantum King-Ring Domination (QKRD)基准测试，基于国际象棋战术位置构建，包含5000个结构化实例，具有one-hot约束、空间局部性和10-40量子比特规模。该基准提供人类可解释的覆盖度指标，并与经典启发式算法进行内在验证。

Result: 系统评估QAOA设计选择发现：约束保持混合器(XY, domain-wall)比标准混合器收敛快约13步；热启动策略减少45步收敛时间，能量改进显著；CVaR优化表现更差。QAOA在内在验证中优于贪婪启发式算法12.6%，优于随机选择80.1%。

Conclusion: 结构化基准测试能揭示问题感知的QAOA技术优势，这些优势在随机实例中被掩盖。研究提供了完整的代码、数据和实验工件，支持可重复的NISQ算法研究。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [79] [Smart Fault Detection in Nanosatellite Electrical Power System](https://arxiv.org/abs/2601.00335)
*Alireza Rezaee,Niloofar Nobahari,Amin Asgarifar,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出一种无需姿态控制系统的纳米卫星电源故障检测方法，使用神经网络模拟正常状态，结合多种机器学习算法进行故障分类


<details>
  <summary>Details</summary>
Motivation: 纳米卫星在低地球轨道运行时，由于压力耐受性、发射压力和环境因素，电力系统各部分容易发生故障。特别是光伏子系统的线间故障和开路、DC-DC转换器的IGBT短路和开路、地面电池调节器故障等常见问题，需要有效的故障检测方法。

Method: 首先使用神经网络建立无故障状态模型，以太阳辐射和太阳能板表面温度为输入，电流和负载为输出。然后利用神经网络分类器，通过故障模式和类型进行故障诊断。同时采用PCA分类、决策树和KNN等其他机器学习方法进行故障分类。

Result: 开发了一种无需姿态控制系统的纳米卫星电源故障检测框架，能够有效识别和分类多种电力系统故障类型。

Conclusion: 该方法为纳米卫星电力系统的故障检测提供了一种有效的解决方案，特别是在没有姿态控制系统的情况下，通过机器学习方法能够准确诊断不同类型的电源故障。

Abstract: This paper presents a new detection method of faults at Nanosatellites' electrical power without an Attitude Determination Control Subsystem (ADCS) at the LEO orbit. Each part of this system is at risk of fault due to pressure tolerance, launcher pressure, and environmental circumstances. Common faults are line to line fault and open circuit for the photovoltaic subsystem, short circuit and open circuit IGBT at DC to DC converter, and regulator fault of the ground battery. The system is simulated without fault based on a neural network using solar radiation and solar panel's surface temperature as input data and current and load as outputs. Finally, using the neural network classifier, different faults are diagnosed by pattern and type of fault. For fault classification, other machine learning methods are also used, such as PCA classification, decision tree, and KNN.

</details>


### [80] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: 该论文提出结合光流和三种深度学习模型（监督CNN、预训练CNN特征提取器、分层极限学习机）用于无人机视频中的人体检测，在UCF-ARG数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征方法依赖专家知识，对光照变化、相机抖动等动态事件敏感。需要更鲁棒、自动化的特征学习方法用于无人机视频中的人体检测。

Method: 结合光流和三种深度学习模型：1) 监督卷积神经网络（S-CNN），2) 预训练CNN特征提取器，3) 分层极限学习机（H-ELM）。在UCF-ARG无人机数据集上训练和测试，评估五种人类动作。

Result: 预训练CNN平均准确率98.09%，S-CNN使用softmax达到95.6%，使用SVM达到91.7%，H-ELM达到95.9%。H-ELM在CPU上训练时间445秒，S-CNN在GPU上训练时间770秒。

Conclusion: 提出的自动特征学习方法在无人机视频人体检测任务中表现成功，预训练CNN效果最佳，H-ELM在计算效率上有优势。

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [81] [Deep Delta Learning](https://arxiv.org/abs/2601.00417)
*Yifan Zhang,Yifeng Liu,Mengdi Wang,Quanquan Gu*

Main category: cs.LG

TL;DR: DDL提出可学习的几何变换来调制残差连接，通过Delta算子动态控制特征转换，增强网络建模复杂状态转换的能力。


<details>
  <summary>Details</summary>
Motivation: 传统残差网络的身份快捷连接虽然缓解了梯度消失问题，但强加了一个严格加性的归纳偏置，限制了网络建模复杂状态转换的能力。

Method: 提出Deep Delta Learning (DDL)，用可学习的、数据依赖的几何变换（Delta算子）调制身份快捷连接。该算子是对单位矩阵的秩-1扰动，由反射方向向量k(X)和门控标量β(X)参数化。

Result: 谱分析表明门控β(X)能够在身份映射、正交投影和几何反射之间动态插值。将残差更新重构为同步秩-1注入，门控作为动态步长控制旧信息擦除和新特征写入。

Conclusion: DDL统一了特征转换控制，使网络能够显式控制层间转移算子的谱，在保持门控残差架构稳定训练特性的同时，能够建模复杂的非单调动态。

Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.

</details>


### [82] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E-GRPO方法，通过熵感知的组相对策略优化增强流匹配模型的人类偏好对齐，通过合并低熵步骤为高熵SDE采样步骤来解决多步去噪中的稀疏奖励信号问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在优化多步去噪时面临稀疏和模糊的奖励信号问题。观察到高熵步骤能实现更高效探索，而低熵步骤导致无区别的roll-outs，需要增加SDE采样步骤的熵。

Method: 提出E-GRPO（熵感知组相对策略优化）：1）合并连续低熵步骤形成单个高熵SDE采样步骤，其他步骤使用ODE采样；2）引入多步组归一化优势函数，在共享相同合并SDE去噪步骤的样本内计算组相对优势。

Result: 在不同奖励设置下的实验结果证明了该方法的有效性。

Conclusion: 通过熵感知的SDE步骤合并和组相对优势计算，E-GRPO有效解决了流匹配模型中多步去噪的奖励稀疏性问题，提升了人类偏好对齐效果。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [83] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

TL;DR: 大规模评估16种可解释机器学习方法在216个表格数据集上的表现，发现EBMs在回归任务中表现最佳，但性能高度依赖数据集特征如维度、样本量、线性度和类别不平衡。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医疗、金融、法律等高风险领域的广泛应用，模型可解释性和责任性日益重要。尽管可解释ML受到关注，但对表格数据的内在可解释模型系统评估仍然不足，现有研究多关注聚合性能结果。

Method: 对16种内在可解释方法进行大规模比较评估，包括经典线性模型、决策树以及EBMs、符号回归、GOSDT等新方法。研究覆盖216个真实表格数据集，按数据集结构特征（维度、样本量、线性度、类别不平衡）分层分析性能，同时评估训练时间和分布偏移下的鲁棒性。

Result: 结果显示清晰的性能层次：EBMs在回归任务中始终表现出色；SR和IGANNs在非线性场景下表现优异；GOSDT对类别不平衡高度敏感。性能高度依赖上下文，不同方法在不同数据集特征下表现差异显著。

Conclusion: 研究为从业者在可解释性和预测性能之间寻求平衡提供了实用指导，深化了对表格数据可解释建模的实证理解，强调了根据具体数据集特征选择合适方法的重要性。

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [84] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）在异常检测任务中表现出色，通过零样本推理、全模型适应和参数高效微调（PEFT）策略，在多个基准测试中超越了任务特定基线，特别是在类别不平衡严重的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法大多需要大量任务特定训练，研究探索时间序列基础模型（在大规模异构数据上预训练）能否作为异常检测的通用骨干网络。

Method: 通过系统实验比较三种策略：零样本推理、全模型适应和参数高效微调（PEFT），包括LoRA、OFT和HRA等方法，在多个基准测试上进行评估。

Result: TSFMs在AUC-PR和VUS-PR指标上显著优于任务特定基线，特别是在类别不平衡严重的情况下。PEFT方法不仅降低计算成本，在大多数情况下匹配或超越了全微调性能。

Conclusion: 时间序列基础模型可作为异常检测的有前景的通用模型，即使是为预测任务预训练的模型也能高效适应异常检测，支持可扩展和高效的时间序列异常检测。

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [85] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出可控概念瓶颈模型(CCBMs)，支持概念-标签级、概念级和数据级三种粒度的模型编辑，无需重新训练即可动态维护概念瓶颈模型。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型(CBMs)主要关注静态场景，而实际应用中需要持续维护：删除错误/敏感数据(遗忘学习)、修正错误标注概念、纳入新样本(增量学习)。现有方法需要重新训练，在大规模应用中效率低下。

Method: 提出可控概念瓶颈模型(CCBMs)，基于影响函数推导出数学上严谨的闭式近似解，支持三种编辑粒度：概念-标签级(修正概念与标签关系)、概念级(修正概念本身)、数据级(数据删除和添加)。

Result: 实验结果表明CCBMs在编辑效率和适应性方面表现优异，能够实现动态可信的概念瓶颈模型，验证了其在实际应用中的价值。

Conclusion: CCBMs通过数学严谨的闭式近似解解决了概念瓶颈模型动态编辑的挑战，无需重新训练即可支持多种粒度编辑，为实际部署提供了高效可靠的解决方案。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [86] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

TL;DR: TGE：一种用于离线观察模仿学习（LfO）的轨迹级生成嵌入方法，通过时间扩散模型的潜在空间估计专家状态密度，构建密集平滑的代理奖励，有效处理专家演示稀缺且离线数据与专家行为差异大的情况。


<details>
  <summary>Details</summary>
Motivation: 现有基于分布匹配的方法在专家演示稀缺且离线次优数据与专家行为差异大的情况下表现不佳，因为它们施加严格的支持约束并依赖脆弱的单步模型，难以从非完美数据中提取有用信号。

Method: 提出TGE（轨迹级生成嵌入）方法：在离线轨迹数据上训练时间扩散模型，利用其潜在空间估计专家状态密度，构建密集平滑的代理奖励函数。该方法利用学习到的扩散嵌入的平滑几何特性，捕捉长期时序动态，有效弥合不相交支持集之间的差距。

Result: 在D4RL运动和控制基准测试中，TGE方法持续匹配或优于先前的离线LfO方法，证明了其在处理专家演示稀缺且离线数据分布与专家差异大的情况下的有效性。

Conclusion: TGE通过轨迹级生成嵌入构建平滑代理奖励，有效解决了离线观察模仿学习中专家数据稀缺且离线数据分布差异大的挑战，为处理分布不匹配问题提供了稳健的学习信号。

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [87] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

TL;DR: 该论文证明了残差网络的分层SGD能高效学习层次化模型，这类模型超越了先前可学习模型的深度限制，并提出了人类"教师"提供层次结构线索的理论框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解深度学习的理论基础，特别是为什么深度学习在具有层次结构的领域表现优异。作者观察到人类"教师"的存在暗示了层次结构的可用性，因为教师通过提供细粒度标签揭示了大脑内部算法的"提示"。

Method: 采用残差网络上的分层随机梯度下降（layerwise SGD）来学习层次化模型。模型假设存在未知的标签层次结构 L₁ ⊆ L₂ ⊆ ... ⊆ Lᵣ = [n]，其中L₁中的标签是输入的简单函数，而i>1时，Lᵢ中的标签是更简单标签的简单函数。

Result: 证明了这类层次化模型可以被残差网络的分层SGD高效学习，并且这类模型达到了高效可学习性的深度极限。相比之前可被深度学习算法学习的模型，新模型需要多项式深度来表达，而之前的模型只需要对数深度电路。

Conclusion: 层次化模型的学习能力可能最终成为理解深度学习的基础。人类"教师"的存在支持了层次结构固有的可用性假设，教师通过提供细粒度标签揭示了内部算法的层次结构，从而促进了高效学习。

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [88] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 正交性损失在MoE模型中无法有效促进专家多样性，反而增加权重空间重叠，对性能影响不一致且不可靠


<details>
  <summary>Details</summary>
Motivation: 研究几何正则化在MoE模型专家专业化中的作用，探索正交性损失是否能有效促进专家多样性

Method: 在MoE模型中应用正交性损失来强制专家多样性，通过7种正则化强度分析权重空间和激活空间重叠，在多个数据集上评估性能

Result: 正交性损失在多方面失败：权重空间重叠增加达114%，激活空间重叠保持高位(~0.6)，性能影响不一致（WikiText-103略改善，TinyStories轻微退化，PTB结果高度可变），权重与激活正交性无显著相关性(r=-0.293, p=0.523)

Conclusion: 权重空间正则化既未达到其几何目标，也未可靠改善性能，不适合用于MoE多样性

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [89] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 本文提出了一种名为AugUNet1D的1D UNet模型，通过数据增强（特别是缩放增强）来自动标记脑电图中的棘慢波放电事件，相比传统算法和基础模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）记录中事件的手动标记非常耗时，特别是对于持续数周至数月的连续记录。棘慢波放电（SWD）作为失神发作的电生理标志，通常需要手动标记。现有机器学习方法虽有应用但仍有改进空间。

Method: 比较了14种机器学习分类器在961小时小鼠EEG记录（包含22,637个标记SWD）上的性能，发现1D UNet表现最佳。进一步通过数据增强改进1D UNet（特别是缩放增强），创建了AugUNet1D模型，并与最近发表的"Twin Peaks"算法进行比较。

Result: AugUNet1D在SWD标记任务中表现最优，检测到的事件特征与手动标记的SWD更相似，性能优于"Twin Peaks"算法。缩放增强在所有增强方法中效果最显著。

Conclusion: AugUNet1D是一种有效的SWD自动标记工具，优于现有方法。作者公开了预训练和未训练的模型供其他研究者使用，有助于减少EEG分析中的手动工作量。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [90] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

TL;DR: 该论文比较了无自注意力Transformer架构（神经链）与离散动力系统的类比，通过Burgers和Eikonal方程对比标准数值离散化与PINN学习方法，发现两者获得相同动力学知识但路径不同。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索机器学习（特别是无自注意力的Transformer架构）与离散动力系统之间的类比关系，比较传统数值方法与物理信息神经网络（PINN）在求解偏微分方程方面的差异和优劣。

Method: 方法包括：1）建立神经链与离散化神经积分/偏微分方程动力系统的类比；2）对粘性和非粘性Burgers方程以及Eikonal方程进行数值求解；3）同时使用标准数值离散化（也表示为神经链形式）和PINN学习方法；4）比较两种方法的矩阵结构和参数特性。

Result: 研究发现：1）标准数值离散化和PINN学习通过不同路径获得相同的系统动力学知识；2）PINN使用随机矩阵，而有限差分方法使用高度结构化的三对角矩阵；3）可接受的随机矩阵数量远多于唯一的三对角形式，这解释了PINN通常收敛到随机集合的原因；4）PINN需要更多参数，导致物理透明度（可解释性）降低和训练成本增加。

Conclusion: 结论是：对于一维动态问题，PINN相比传统数值方法没有优势，因为需要更多参数且缺乏物理可解释性。但研究不排除PINN和机器学习在高维问题中可能有更好策略的可能性。

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [91] [When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents](https://arxiv.org/abs/2601.00513)
*Laksh Advani*

Main category: cs.LG

TL;DR: 研究发现小型语言模型（7-9B参数）存在"正确但推理错误"现象，50-69%的正确答案包含根本性推理缺陷，标准准确率指标无法检测。提出推理完整性评分（RIS）作为过程评估指标，发现RAG能改善推理完整性，而元认知干预在小模型上反而有害。


<details>
  <summary>Details</summary>
Motivation: 部署小型语言模型作为自主代理需要信任其推理过程，而不仅仅是输出结果。当前存在可靠性危机：许多正确答案包含根本性推理缺陷，这种现象无法通过标准准确率指标检测，对可信代理部署构成威胁。

Method: 分析了10,734个推理轨迹，涵盖三个模型和多样化任务。引入推理完整性评分（RIS）作为过程评估指标，验证了评分者间一致性（κ=0.657）。评估了RAG和元认知干预对推理完整性的影响，并进行了机制分析。还开发了神经分类器用于快速验证推理完整性。

Result: 发现50-69%的正确答案包含根本性推理缺陷。RAG显著改善推理完整性（Cohen's d=0.23-0.93），而元认知干预在小模型上反而损害性能（d=-0.14到-0.33）。RAG通过将计算基于外部证据减少7.6%的错误，而元认知在模型容量不足时会放大混淆。神经分类器达到0.86 F1分数，速度提升100倍。

Conclusion: 仅依赖准确率评估模型是危险的，因为模型可能"正确但推理错误"。过程验证对于可信代理部署至关重要。RAG能有效改善推理完整性，而元认知干预在小模型上需谨慎使用。需要开发过程验证方法来确保推理可靠性。

Abstract: Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.

</details>


### [92] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard：基于孪生循环自编码器的LLM智能体轨迹异常检测方法，通过对比学习和重构损失联合学习任务-轨迹对齐与序列有效性，实现实时安全验证。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法不适用于LLM智能体多步动作计划：均值池化嵌入会稀释异常步骤，仅对比方法忽略序列结构，标准无监督方法在预训练嵌入上F1分数不超过0.69。

Method: 提出Trajectory Guard，一种孪生循环自编码器，采用混合损失函数联合学习任务-轨迹对齐（通过对比学习）和序列有效性（通过重构），统一检测"错误计划"和"畸形计划结构"。

Result: 在合成扰动和真实世界失败（安全审计RAS-Eval和多智能体系统Who&When）基准测试中，平衡集上F1分数0.88-0.94，不平衡外部基准上召回率0.86-0.92，推理延迟32ms，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能有效检测LLM智能体轨迹中的异常，实现实时安全验证，适用于生产部署，解决了现有方法在序列异常检测上的局限性。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [93] [A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling](https://arxiv.org/abs/2601.00519)
*Dristi Datta,Tanmoy Debnath,Minh Chau,Manoranjan Paul,Gourab Adhikary,Md Geaur Rahman*

Main category: cs.LG

TL;DR: 提出SAFN网络，通过稀疏注意力机制融合多模态数据，在帕金森病分类中达到98%准确率，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 帕金森病具有异质性表现，需要整合生物和临床标志物。现有计算模型在可解释性、类别不平衡、高维成像和临床特征融合方面存在不足。

Method: 提出类加权稀疏注意力融合网络(SAFN)：使用模态特定编码器处理MRI皮层厚度、MRI体积测量、临床评估和人口统计学变量；采用对称交叉注意力机制捕捉非线性交互；稀疏约束注意力门控融合层动态优先处理信息模态；类平衡焦点损失处理数据不平衡。

Result: 在703名参与者(570名PD，133名健康对照)上评估，使用五折交叉验证，准确率达0.98±0.02，PR-AUC达1.00±0.00，优于现有机器学习和深度学习基线。可解释性分析显示约60%预测权重分配给临床评估，符合运动障碍学会诊断原则。

Conclusion: SAFN为神经退行性疾病的计算分析提供了可重复且透明的多模态建模范式，具有临床一致的可解释决策过程。

Abstract: Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.

</details>


### [94] [Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study](https://arxiv.org/abs/2601.00525)
*Ravi Teja Pagidoju*

Main category: cs.LG

TL;DR: LSTM模型压缩研究：通过减少隐藏单元从128到16，发现64单元模型在保持精度的同时减小73%模型大小并提升47%准确率


<details>
  <summary>Details</summary>
Motivation: 标准LSTM神经网络在零售业销售预测中虽然准确，但计算资源需求大，对中小型零售企业构成挑战，需要探索模型压缩方案

Method: 采用渐进式LSTM模型压缩方法，逐步减少隐藏单元数量（从128到16），使用Kaggle Store Item Demand Forecasting数据集（913,000条日销售记录）评估模型大小与预测准确性的权衡

Result: 实验显示64隐藏单元模型表现最佳：MAPE从128单元的23.6%降至12.4%，模型大小减小73%（从280KB到76KB），准确率提升47%，证明更大模型不一定更好

Conclusion: 通过适当的LSTM模型压缩，可以在保持甚至提升预测准确性的同时显著减小模型大小，为中小型零售企业提供可行的销售预测解决方案

Abstract: Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.

</details>


### [95] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文探讨了大型模型在联邦学习框架下的定制化方法，首次将prefix-tuning应用于联邦学习环境，验证了其可行性并展示了与集中式方法相近的性能。


<details>
  <summary>Details</summary>
Motivation: 探索大型模型在联邦学习框架下的定制化挑战，研究如何在保护数据隐私的同时实现模型个性化定制，填补prefix-tuning在联邦学习环境中应用的研究空白。

Method: 首先综述了多种大型模型定制技术（全微调、高效微调、提示工程、prefix-tuning、知识蒸馏、检索增强生成），然后讨论这些技术在联邦学习框架下的实现方式，最后重点实验了联邦prefix-tuning方法。

Result: 联邦prefix-tuning实验验证了其在联邦学习环境中的可行性，性能接近集中式方法。与其他三种联邦定制方法相比，表现出竞争性性能、满意效率和一致鲁棒性。

Conclusion: 联邦prefix-tuning是大型模型在联邦学习框架下有效的定制方法，为保护隐私的模型个性化提供了可行方案，在性能、效率和鲁棒性方面表现良好。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [96] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 使用扩散模型和云原生架构自动生成零售货架图，将设计时间从30小时减少到0.5小时，成本降低97.5%


<details>
  <summary>Details</summary>
Motivation: 传统货架图设计耗时且昂贵（平均30小时/复杂布局），需要自动化解决方案来优化零售空间规划

Method: 基于AWS的云原生架构，使用扩散模型学习多个零售店的成功货架布局，通过修改损失函数集成零售特定约束，支持云端训练和边缘实时推理

Result: 设计时间减少98.3%（30→0.5小时），约束满足率达94.4%，成本降低97.5%，4.4个月回本，支持10,000个并发请求

Conclusion: 生成式AI在自动零售空间优化中具有可行性，云原生架构能线性扩展，为零售业提供高效解决方案

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [97] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 基于非平衡随机动力学的熵触发重训练框架，通过概率流建模数据漂移，使用KL散度量化模型-数据失配，实现比传统方法更高效的重训练


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在非平稳环境中部署时，由于数据漂移导致性能下降。现有的漂移检测方法大多缺乏原理性的动力学解释，且无法指导如何平衡重训练频率与操作成本。

Method: 提出基于熵的重训练框架，将部署时数据漂移建模为由Fokker-Planck方程控制的概率流，使用时变Kullback-Leibler散度量化模型-数据失配，并证明该失配的时间导数具有熵平衡分解，包含由概率流驱动的非负熵产生项。

Result: 在受控的非平稳分类实验中，熵触发重训练实现了与高频重训练相当的预测性能，同时相对于每日重训练和基于标签的策略，将重训练事件减少了一个数量级。

Conclusion: 熵触发重训练作为一种无标签干预策略，能够响应累积的失配而非延迟的性能崩溃，为数据漂移下的模型维护提供了原理性的动力学框架。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [98] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: TeleDoCTR是一个针对电信领域的端到端故障排除系统，通过集成领域特定排序和生成模型，自动化处理工单分类、历史工单检索和故障分析报告生成，显著提升电信故障排除的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电信故障排除是一个高度复杂且耗时的任务，需要专家解读工单内容、查阅文档和搜索历史记录。这种人工密集型方法不仅延迟问题解决，还阻碍整体运营效率。需要一种专门针对电信领域的自动化系统来提升故障排除效果和效率。

Method: 提出TeleDoCTR系统，集成领域特定排序和生成模型，自动化故障排除工作流程的三个关键步骤：1) 将工单路由到适当的专家团队（分类任务）；2) 检索上下文和语义相似的历史工单（检索任务）；3) 生成详细的故障分析报告，包括问题、根本原因和潜在解决方案（生成任务）。

Result: 在真实世界的电信基础设施数据集上评估TeleDoCTR，证明其性能优于现有最先进方法，显著提升了故障排除过程的准确性和效率。

Conclusion: TeleDoCTR是一个专门针对电信领域的端到端故障排除系统，通过自动化关键工作流程步骤，有效解决了电信故障排除中的人工密集型问题，显著提升了运营效率和问题解决速度。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [99] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 论文提出需要区分两种对抗性弱点：利用非鲁棒特征的攻击和不利用这些特征的攻击，并提出了基于集成的方法来测量对抗扰动对非鲁棒特征的操纵程度。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击理论（特别是Ilyas等人提出的非鲁棒特征理论）虽然解释了大多数攻击，但忽略了那些不直接利用非鲁棒特征的对抗样本。需要区分这两种不同类型的对抗性弱点，以更全面地评估模型的对抗鲁棒性。

Method: 提出了基于集成的度量方法，用于测量对抗扰动对非鲁棒特征的操纵程度。使用该度量分析攻击者生成的对抗样本的构成，区分利用非鲁棒特征的攻击和不利用这些特征的攻击。

Result: 通过新度量分析发现对抗样本中存在两种不同类型的弱点。这一新视角能够重新解释多个现象，包括锐度感知最小化对对抗鲁棒性的影响，以及在鲁棒数据集上对抗训练与标准训练之间的鲁棒性差距。

Conclusion: 对抗性弱点应分为两种类型：利用非鲁棒特征的攻击和不利用这些特征的攻击。这种区分对于全面评估对抗鲁棒性至关重要，提出的度量方法为分析对抗样本构成提供了新工具，有助于更深入地理解对抗鲁棒性现象。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [100] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的持续学习方法，结合在线重置机制防止码本崩溃，并使用KV-LoRA高效利用压缩记忆，在保持高准确率的同时将记忆库大小减少到基准的0.3%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识容易过时，持续学习需要更新模型而不遗忘旧知识。现有记忆增强方法面临记忆库随数据流不断增长的瓶颈，需要更高效的记忆压缩方案。

Method: 提出MBC模型：1) 通过码本优化策略在线压缩记忆库；2) 引入在线重置机制防止码本崩溃；3) 在注意力层使用Key-Value低秩适应(KV-LoRA)高效利用压缩记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最竞争基线的0.3%，同时在在线适应学习中保持高记忆保留准确率。

Conclusion: MBC通过码本压缩和KV-LoRA实现了高效的持续学习，解决了记忆库无限增长的问题，为大语言模型的实际部署提供了可行的持续学习解决方案。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [101] [Learning to be Reproducible: Custom Loss Design for Robust Neural Networks](https://arxiv.org/abs/2601.00578)
*Waqas Ahmed,Sheeba Samuel,Kevin Coakley,Birgitta Koenig-Ries,Odd Erik Gundersen*

Main category: cs.LG

TL;DR: 提出自定义损失函数CLF来减少训练结果对随机因素的敏感性，提高深度学习模型的复现性和可靠性


<details>
  <summary>Details</summary>
Motivation: 当前训练方法缺乏确保跨运行一致性和鲁棒性的机制，即使控制初始化和训练条件，模型准确率仍存在显著变异性

Method: 提出自定义损失函数CLF，通过调整其参数来平衡预测准确率和训练稳定性，减少对权重初始化和数据洗牌等随机因素的敏感性

Result: 在图像分类和时间序列预测的多种架构上进行广泛实验，CLF显著提高了训练鲁棒性，且不牺牲预测性能

Conclusion: CLF是开发更稳定、可靠和可信神经网络的有效高效策略

Abstract: To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.

</details>


### [102] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需训练的方法，通过分析注意力矩阵的谱特征来检测大语言模型中的有效数学推理，在多个模型上达到85-95%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 需要一种无需训练数据、微调或学习分类器的方法来检测大语言模型生成的数学推理是否有效，以应用于幻觉检测和AI安全监控。

Method: 将注意力矩阵视为动态图的邻接矩阵，提取四个可解释的谱诊断特征：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和谱熵，通过单一阈值判断推理有效性。

Result: 在7个来自4个不同架构家族的Transformer模型上实验，效应量高达Cohen's d=3.30，分类准确率达85.0-95.6%，校准阈值在完整数据集上达到93-95%。发现该方法检测的是逻辑一致性而非编译器接受度。

Conclusion: 谱图分析为推理验证提供了一个原则性框架，具有即时应用于幻觉检测和AI安全监控的潜力，同时揭示了注意力机制设计影响哪些谱特征捕获推理有效性。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


### [103] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: HFedMoE：一个异构MoE联邦学习框架，通过专家重要性评估、自适应专家选择和稀疏感知聚合，解决资源受限客户端上大语言模型联邦微调的挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护隐私地微调大语言模型，但模型规模过大使得资源受限客户端无法进行设备端训练。MoE模型虽然计算效率高，但在联邦学习环境中面临三个关键挑战：1）缺乏可靠指标选择合适专家；2）客户端异构计算资源限制；3）客户端特定专家子集和路由偏好破坏全局聚合。

Method: 提出HFedMoE框架：1）基于专家对微调性能的贡献评估专家重要性；2）从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3）设计稀疏感知模型聚合策略，加权聚合活跃微调的专家和门控参数。

Result: 大量实验表明，HFedMoE在训练准确率和收敛速度方面优于现有最先进基准方法。

Conclusion: HFedMoE有效解决了MoE在联邦学习环境中的挑战，实现了计算高效的大语言模型联邦微调，为资源受限设备上的LLM部署提供了可行方案。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [104] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: 使用机器学习预测骑行时长，结合路线拓扑和运动员体能状态，相比传统物理模型更实用


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的骑行时长预测需要复杂的空气动力学参数和实时风况数据，这对业余骑手不实用。需要一种更简单、基于历史数据的方法来准确预测骑行时长。

Method: 采用机器学习方法，使用路线拓扑特征和运动员当前体能状态（从训练负荷指标推导）来预测骑行时长。通过特征工程消除数据泄漏，使用Lasso回归模型，在N-of-1研究设计中评估单个运动员的数据集（N=96次骑行）。

Result: Lasso回归结合拓扑+体能特征达到MAE=6.60分钟和R²=0.922。整合体能指标（CTL, ATL）相比仅使用拓扑特征将误差降低了14%（从MAE=7.66分钟）。渐进检查点预测支持动态比赛规划。

Conclusion: 机器学习方法能够有效预测骑行时长，使用历史表现代理替代复杂的物理测量。运动员的生理状态即使在自定节奏的努力中也能显著约束表现，为训练规划和赛事准备提供了实用工具。

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [105] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: 提出基于图神经网络和强化学习的交通感知出租车热点预测框架，在模拟德里数据集上减少56%乘客等待时间和38%行驶距离


<details>
  <summary>Details</summary>
Motivation: 传统出租车热点预测模型仅依赖历史需求数据，忽略了交通拥堵、道路事故、公共事件等动态影响因素，无法实现智能城市交通中的供需高效匹配

Method: 将城市道路网络建模为图（节点为交叉口，边为路段），使用GNN编码时空依赖关系，结合Q-learning智能体推荐最优出租车热点位置，奖励机制联合优化乘客等待时间、司机行驶距离和拥堵避免

Result: 在模拟德里出租车数据集上的实验表明，相比基线随机选择方法，该模型减少了约56%的乘客等待时间和38%的行驶距离

Conclusion: 该交通感知的图强化学习框架能有效优化城市出租车调度，可扩展到多模式交通系统，并集成到智能城市平台中实现实时城市移动性优化

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [106] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: YapBench是一个轻量级基准测试，用于量化LLM在需要简洁回答的提示上的过度生成问题，通过YapScore测量超出基准答案的额外长度，评估了76个助手LLM并发现显著的过度生成差异。


<details>
  <summary>Details</summary>
Motivation: 当前LLM（如ChatGPT、Claude、Gemini）作为通用助手使用时，经常对简单请求给出不必要的冗长回答，包含冗余解释、模棱两可的表述或模板化内容，这会增加认知负担和推理成本。先前研究表明基于偏好的后训练和LLM评判的评估可能导致系统性长度偏差，即使质量相当，更长的回答也更容易被奖励。

Method: 提出YapBench基准测试，包含300多个英文提示，涵盖三种需要简洁回答的场景：A) 需要简短澄清的最小或模糊输入；B) 具有简短稳定答案的封闭式事实问题；C) 只需单行命令或代码片段的编程任务。每个项目包含单轮提示、精心策划的最小充分基准答案和类别标签。主要指标YapScore测量响应超出基准答案的字符数，YapIndex则是类别级别中位数YapScore的均匀加权平均值。

Result: 评估76个助手LLM，观察到中位数额外长度存在数量级差异，并识别出特定类别的失败模式：在模糊输入上填充真空（过度解释），在一行技术请求上添加解释或格式化开销。基准测试和实时排行榜已发布，用于跟踪随时间变化的冗长行为。

Conclusion: YapBench提供了一个轻量级、无需特定分词器的基准测试，能够量化LLM在简洁理想提示上的过度生成问题，揭示了当前助手LLM在简洁性方面的显著差异和系统性偏差，有助于推动更简洁、高效的LLM开发。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [107] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: IGBO框架通过双目标优化训练可解释模型，利用DAG编码特征重要性层次结构，使用TIG度量特征重要性，并引入最优路径预言机解决TIG的OOD问题。


<details>
  <summary>Details</summary>
Motivation: 现有可解释模型训练方法缺乏结构化领域知识的有效整合，且传统特征重要性度量方法存在分布外问题，需要一种既能保持模型性能又能强制执行领域知识约束的框架。

Method: 提出IGBO框架：1) 将特征重要性层次编码为有向无环图(DAG)；2) 使用时序积分梯度(TIG)度量特征重要性；3) 引入最优路径预言机学习数据流形感知的积分路径以解决OOD问题；4) 采用双目标优化平衡模型准确性和可解释性约束。

Result: 理论分析证明了收敛性和对小批量噪声的鲁棒性；在时间序列数据上的实验表明，IGBO能有效强制执行DAG约束，在最小化准确性损失的同时优于标准正则化基线方法。

Conclusion: IGBO框架成功整合了结构化领域知识到模型训练中，通过双目标优化和最优路径预言机解决了可解释性度量的分布外问题，为训练既准确又可解释的模型提供了有效方法。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [108] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出Avatar Forcing框架，通过扩散强迫实现实时交互式头像生成，解决现有模型缺乏情感互动的问题，实现低延迟（约500ms）和富有表现力的反应。


<details>
  <summary>Details</summary>
Motivation: 当前说话头生成模型缺乏真正的交互性，只能生成单向响应，缺少情感参与。需要解决两个关键挑战：在因果约束下实时生成运动，以及无需额外标注数据学习富有表现力的反应。

Method: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-头像交互，处理多模态输入（音频和动作）。引入直接偏好优化方法，利用丢弃用户条件构建的合成负样本，实现无标签的交互学习。

Result: 框架实现低延迟实时交互（约500ms），相比基线加速6.8倍。生成的反应性和表现力强的头像运动，在80%的情况下优于基线。

Conclusion: Avatar Forcing框架成功解决了交互式头像生成的关键挑战，实现了实时、富有表现力的虚拟交流，为虚拟通信和内容创作提供了更自然的交互体验。

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [109] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: IRPO提出了一种新的强化学习框架，通过将Bradley-Terry模型整合到GRPO中，解决了成对生成奖励模型的计算瓶颈问题，实现了高效的点式评分，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 成对生成奖励模型（GRMs）虽然具有可解释性和推理时扩展性，但与GRPO等强化学习算法集成时存在计算瓶颈：1）成对比较需要O(n²)时间复杂度；2）重复采样或额外思维链推理带来计算开销。

Method: 提出Intergroup Relative Preference Optimization（IRPO）框架，将Bradley-Terry模型整合到GRPO中，为每个响应生成点式评分，从而在RL训练期间能够高效评估任意数量的候选响应，同时保持可解释性和细粒度奖励信号。

Result: IRPO在多个基准测试中实现了点式GRMs中的最先进性能，性能与当前领先的成对GRMs相当。此外，在训练后评估中，IRPO显著优于成对GRMs。

Conclusion: IRPO通过解决成对GRMs的计算瓶颈，提供了一种高效的点式评分方法，在保持可解释性和细粒度奖励信号的同时，实现了与成对方法相当甚至更优的性能，为生成奖励模型与强化学习的集成提供了更实用的解决方案。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [110] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出贝叶斯逆博弈框架，通过变分自编码器和可微纳什博弈求解器从交互数据中学习智能体目标的后验分布，相比最大似然估计能量化不确定性并实现更安全的决策。


<details>
  <summary>Details</summary>
Motivation: 现有逆博弈方法仅提供点估计，无法量化估计不确定性，导致下游规划决策可能过度自信地采取不安全行动。需要一种能处理多模态观测数据、实时生成后验分布的方法来改善安全决策。

Method: 提出贝叶斯逆博弈框架，使用结构化变分自编码器（VAE）嵌入可微纳什博弈求解器，在交互数据集上训练，无需智能体真实目标的标签。支持多模态推理，当轨迹信息不足时利用额外观测模态减少不确定性。

Result: 框架成功学习先验和后验分布，相比基于最大似然估计的逆博弈方法提高了推理质量，实现了更安全的下游决策而不牺牲效率。多模态推理在轨迹信息不足时进一步减少不确定性。

Conclusion: 贝叶斯逆博弈方法能有效量化智能体目标估计的不确定性，通过多模态观测数据改善推理质量，为自主决策提供更安全的规划基础，解决了传统点估计方法过度自信的问题。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [111] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出BSAT方法，使用B样条自适应分割时间序列，结合L-RoPE位置编码，在内存受限场景下实现高效长期时间序列预测


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长期时间序列预测中存在自注意力二次复杂度和均匀分割与数据语义结构不匹配的问题

Method: BSAT自适应分词器：使用B样条拟合时间序列，在高曲率区域放置token；L-RoPE混合位置编码：结合可学习位置编码和层间可学习基的旋转位置编码

Result: 在多个公开基准测试中表现竞争力强，高压缩率下性能优异，特别适合内存受限场景

Conclusion: BSAT方法有效解决了Transformer在长期时间序列预测中的效率和语义对齐问题，为内存受限应用提供了实用解决方案

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [112] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应精度调优框架，用于线性求解器和其他算法，通过上下文赌博机问题动态选择计算步骤的精度配置，平衡精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 科学计算中混合精度方法需要动态调整精度以平衡计算效率和准确性，传统方法缺乏自适应能力，需要自动化精度调优框架。

Method: 将问题建模为上下文赌博机问题，使用离散化状态空间和增量动作价值估计，通过Q表映射特征到精度配置，采用epsilon-greedy策略优化多目标奖励函数。

Result: 在线性系统迭代求精应用中，框架能有效选择精度配置，在保持与双精度基线相当准确性的同时显著降低计算成本，且能泛化到未见数据集。

Conclusion: 这是首个基于强化学习的精度自动调优工作，验证了RL在科学计算混合精度方法中的潜力，为其他数值算法的精度选择提供了新思路。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [113] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出Distributional Creative Reasoning (DCR)框架，分析现有LLM推理方法（STaR、GRPO、DPO等）导致多样性衰减的问题，并提供保持正确性和创造性的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理管道过度优化正确性，导致推理路径分布崩溃，语义熵下降，削弱创造性问题解决能力。需要分析这种失败并找到保持多样性的方法。

Method: 引入Distributional Creative Reasoning (DCR)框架，将训练视为通过解决方案轨迹概率测度的梯度流。该统一变分目标将STaR、GRPO、DPO等方法作为特例。

Result: 1) 多样性衰减定理：描述基于正确性的目标如何导致STaR、GRPO、DPO的不同多样性衰减模式；2) 确保收敛到稳定多样策略的设计；3) 实际可行的实现方法。

Conclusion: DCR为LLM提供了首个保持正确性和创造性的原则性方法，防止推理路径分布崩溃，维持语义多样性和创造性问题解决能力。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [114] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的角球防守评估框架，通过球员追踪数据推断盯人和区域防守任务，实现无标签的防守贡献评估和反事实分析。


<details>
  <summary>Details</summary>
Motivation: 传统足球防守评估指标难以捕捉无球防守的协调运动，现有反事实方法（如ghosting模型）依赖"平均"行为模拟而缺乏战术背景，需要更准确的防守表现评估方法。

Method: 针对角球场景设计协变量依赖隐马尔可夫模型(CDHMM)，从球员追踪数据中推断时间分辨的盯人和区域防守任务分配，提出防守贡献归因框架和角色条件ghosting反事实分析方法。

Result: 模型能够无标签地推断防守任务分配，提供可解释的防守贡献评估，相比传统方法能更好地结合战术背景进行反事实分析。

Conclusion: CDHMM框架为足球角球防守提供了有效的评估工具，能够捕捉战术背景下的无球防守表现，为防守贡献评估和反事实分析提供了新方法。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [115] [FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing](https://arxiv.org/abs/2601.00785)
*Sunny Gupta,Amit Sethi*

Main category: cs.LG

TL;DR: FedHypeVAE：基于差分隐私和超网络的联邦嵌入级数据合成框架，解决非IID客户端异构性和梯度泄漏问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦数据共享方法在非IID客户端异构性下表现不佳，且对梯度泄漏的正式保护有限，需要一种能同时保证个性化、隐私和分布对齐的生成框架

Method: 基于条件VAE架构，用超网络生成客户端感知的解码器和类条件先验，通过差分隐私优化超网络，结合局部MMD对齐和Lipschitz正则化增强稳定性

Result: FedHypeVAE在非IID条件下实现了隐私保护的嵌入级数据合成，支持领域无关合成和可控多领域覆盖，统一了生成器层面的个性化、隐私和分布对齐

Conclusion: 该框架为联邦设置中的隐私保护数据合成建立了原则性基础，通过超网络驱动的方法有效解决了客户端异构性和隐私保护的双重挑战

Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [116] [SoK: Stablecoins in Retail Payments](https://arxiv.org/abs/2601.00196)
*Yuquan Li,Yuexin Xiang,Qin Wang,Tsz Hon Yuen,Andreas Deppeler,Jiangshan Yu*

Main category: q-fin.GN

TL;DR: 本文系统比较稳定币支付与卡网络，提出CLEAR框架评估五个维度，发现稳定币在结算效率和成本上有优势，但消费者保护较弱，更适合闭环环境而非开放零售支付。


<details>
  <summary>Details</summary>
Motivation: 随着稳定币作为数字支付工具的快速增长，需要系统比较区块链结算能否替代传统卡网络在零售支付中的地位，理解两者的根本差异和适用场景。

Method: 首先映射两者的支付基础设施、参与方角色和交易生命周期，然后提出CLEAR框架从成本、合法性、体验、架构和覆盖五个维度系统评估零售支付系统。

Result: 稳定币提供高效、连续、可编程的结算，降低商户费用并支持24/7转账，但将交易费用、错误预防和争议解决外部化给用户，消费者保护较弱，认知负担较高。

Conclusion: 稳定币在闭环环境、跨境通道和高摩擦支付场景中有条件优势，但作为开放零售支付工具仍存在结构性劣势，卡网络通过内部化消费者侧摩擦支持大规模采用。

Abstract: Stablecoins have emerged as a rapidly growing digital payment instrument, raising the question of whether blockchain-based settlement can function as a substitute for incumbent card networks in retail payments. This Systematization of Knowledge (SoK) provides a systematic comparison between stablecoin payment arrangements and card networks by situating both within a unified analytical framework. We first map their respective payment infrastructures, participant roles, and transaction lifecycles, highlighting fundamental differences in how authorization, settlement, and recourse are organized. Building on this mapping, we introduce the CLEAR framework, which evaluates retail payment systems across five dimensions: cost, legality, experience, architecture, and reach. Our analysis shows that stablecoins deliver efficient, continuous, and programmable settlement, often compressing rail-level merchant fees and enabling 24/7 value transfer. However, these advantages are accompanied by an inversion of the traditional pricing and risk-allocation structure. Card networks internalize consumer-side frictions through subsidies, standardized liability rules, and post-transaction recourse, thereby supporting mass-market adoption. Stablecoin arrangements, by contrast, externalize transaction fees, error prevention, and dispute resolution to users, intermediaries, and courts, resulting in weaker consumer protection, higher cognitive burden at the point of interaction, and fragmented acceptance. Accordingly, stablecoins exhibit a conditional comparative advantage in closed-loop environments, cross-border corridors, and high-friction payment contexts, but remain structurally disadvantaged as open-loop retail payment instruments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [117] [Impact of Clustering on the Observability and Controllability of Complex Networks](https://arxiv.org/abs/2601.00221)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: 本文研究聚类对复杂无标度网络可观测性和可控性的影响，发现高聚类网络需要更少的驱动和观测节点，为资源受限的网络设计提供优化方案。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性和互联性增加，无标度网络能更好地建模现实系统。研究聚类如何影响网络的可观测性和可控性，对于优化社交网络、智能交通系统等应用中的网络设计具有重要意义。

Method: 基于结构化系统理论框架，首先量化网络可观测性/可控性需求，然后通过蒙特卡洛模拟和不同案例研究，分析聚类对这些指标的影响。

Result: 研究发现密集聚类的网络需要更少的驱动和观测节点，因为聚类内部的信息传播更高效。这为传感器/执行器布置提供了实用见解，特别是在资源受限的设置中。

Conclusion: 本文增进了对网络可观测性和可控性的理解，提出了通过改变网络结构和聚类来改善这些特性的技术，为优化网络设计提供了理论依据。

Abstract: The increasing complexity and interconnectedness of systems across various fields have led to a growing interest in studying complex networks, particularly Scale-Free (SF) networks, which best model real-world systems. This paper investigates the influence of clustering on the observability and controllability of complex SF networks, framing these characteristics in the context of structured systems theory. In this paper, we show that densely clustered networks require fewer driver and observer nodes due to better information propagation within clusters. This relationship is of interest for optimizing network design in applications such as social networks and intelligent transportation systems. We first quantify the network observability/controllability requirements, and then, through Monte-Carlo simulations and different case studies, we show how clustering affects these metrics. Our findings offer practical insights into reducing control and observer nodes for sensor/actuator placement, particularly in resource-constrained setups. This work contributes to the understanding of network observability/controllability and presents techniques for improving these features through alterations in network structure and clustering.

</details>


### [118] [Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective](https://arxiv.org/abs/2601.00257)
*Aly Sabri Abdalla,Vuk Marojevic*

Main category: eess.SY

TL;DR: 本文提出了一种基于O-RAN的低空经济框架，通过分离式RAN架构、开放接口和RAN智能控制器的协同，实现AI优化的任务关键型低空经济运营。


<details>
  <summary>Details</summary>
Motivation: 尽管低空经济应用（如无人机物流和应急响应）日益增长，但在复杂、信号受限环境中协调这些任务仍面临根本性挑战，包括缺乏实时、弹性和上下文感知的空中节点协调，以及专门针对低空经济任务的人工智能集成不足。

Method: 提出O-RAN支持的低空经济框架，利用分离式RAN架构、开放接口和RAN智能控制器的无缝协调。通过语义感知rApp作为地形解释器，为强化学习驱动的xApp提供语义指导，实现低空经济群节点的实时轨迹规划。

Result: 通过语义感知rApp和强化学习xApp的协同工作，评估了所提架构的可行性和性能。同时调查了可用于低空经济研究的无人机测试平台能力。

Conclusion: 该框架为低空经济任务提供了AI优化的解决方案，并指出了关键的研究挑战和标准化需求，为未来低空经济发展提供了技术基础。

Abstract: Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs.

</details>


### [119] [Impact Assessment of Heterogeneous Grid Support Functions in Smart Inverter Deployments](https://arxiv.org/abs/2601.00289)
*S. Gokul Krishnan,Mohd. Asim Aftab,Nabil Mohammed,Shehab Ahmed,Charalambos Konstantinou*

Main category: eess.SY

TL;DR: 该研究评估了不同智能逆变器控制模式（恒功率因数、伏安、伏瓦）在低压配电网中的动态交互影响，考虑了阻性和感性馈线类型，并使用实时仿真平台进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源（特别是光伏系统）在低压配电网中的高渗透率，智能逆变器通常基于本地测量自主运行。然而，随着DER渗透率增加，不协调的逆变器控制模式可能导致不良交互，影响系统效率、电压调节和整体稳定性。目前对异构智能逆变器组在不同控制模式下运行的系统级影响研究不足。

Method: 评估多个智能逆变器组在不同控制策略（恒功率因数、伏安、伏瓦模式）下的动态交互，覆盖阻性和感性两种馈线类型。使用实时设置进行验证，在Opal-RT平台上模拟CIRGE低压配电网作为测试网络，实现对实际电网条件下逆变器控制交互的高保真评估。

Result: 通过实时仿真平台对智能逆变器控制交互进行了高保真评估，分析了不同控制模式在阻性和感性馈线类型下的动态交互影响，为理解异构逆变器组在低压配电网中的系统级影响提供了实证基础。

Conclusion: 该研究填补了关于异构智能逆变器组在不同控制模式下运行的系统级影响研究空白，通过实时仿真验证了不同控制策略在多种馈线类型下的交互效应，为智能逆变器协调控制和电网稳定性管理提供了重要参考。

Abstract: The decarbonization of the energy sector has led to a significant high penetration of distributed energy resources (DERs), particularly photovoltaic (PV) systems, in low-voltage (LV) distribution networks. To maintain grid stability, recent standards (e.g., IEEE 1547-2018) mandate DERs to provide grid-support functionalities through smart inverters (SIs), which typically operate autonomously based on local measurements. However, as DER penetration increases, uncoordinated control modes of SIs can lead to adverse interactions, compromising system efficiency, voltage regulation, and overall stability. While previous studies have demonstrated the benefits of coordinated inverter control and optimal dispatch strategies, the system-wide impacts of heterogeneous SI groups operating under different control modes remain largely unexamined. This paper addresses this gap by assessing the dynamic interactions among multiple SI groups with varying control strategies, namely: Constant Power Factor (CPF), Volt-VAR, and Volt-Watt modes. Furthermore, the analysis covers both resistive and inductive feeder types. The validation is performed using a real-time setup. The CIRGE low-voltage (LV) distribution network is simulated in the Opal-RT platform as the test network, enabling realistic and high-fidelity evaluation of SI control interactions under practical grid conditions.

</details>


### [120] [Safety for Weakly-Hard Control Systems via Graph-Based Barrier Functions](https://arxiv.org/abs/2601.00494)
*Marc Seidel,Mahathi Anand,Frank Allgöwer*

Main category: eess.SY

TL;DR: 该论文提出了一种基于图屏障函数的方法，用于解决受弱硬约束（窗口内故障有界）的控制系统的安全验证和控制器综合问题。


<details>
  <summary>Details</summary>
Motivation: 在安全关键工程应用中，通信和计算故障仍然普遍存在。网络控制系统经常出现数据包丢失，导致开环行为严重影响系统性能；实时控制应用中，控制任务经常遇到计算超时，导致偶尔无法发出新的执行器命令。

Method: 提出了一种新的图基屏障函数概念，专门针对考虑的系统类别设计。该方法提供一组约束条件，满足这些约束条件即可在通信故障情况下保证系统安全。后续提出了安全约束的重新表述，以减少保守性并提高计算可行性。

Result: 提出的方法能够为受弱硬约束的控制系统提供安全保证。通过数值案例研究证明了该方法的有效性。

Conclusion: 该研究为解决通信和计算故障下的控制系统安全问题提供了一种有效的图基屏障函数方法，并通过约束重新表述平衡了保守性和计算可行性之间的权衡。

Abstract: Despite significant advancement in technology, communication and computational failures are still prevalent in safety-critical engineering applications. Often, networked control systems experience packet dropouts, leading to open-loop behavior that significantly affects the behavior of the system. Similarly, in real-time control applications, control tasks frequently experience computational overruns and thus occasionally no new actuator command is issued. This article addresses the safety verification and controller synthesis problem for a class of control systems subject to weakly-hard constraints, i.e., a set of window-based constraints where the number of failures are bounded within a given time horizon. The results are based on a new notion of graph-based barrier functions that are specifically tailored to the considered system class, offering a set of constraints whose satisfaction leads to safety guarantees despite communication failures. Subsequent reformulations of the safety constraints are proposed to alleviate conservatism and improve computational tractability, and the resulting trade-offs are discussed. Finally, several numerical case studies demonstrate the effectiveness of the proposed approach.

</details>


### [121] [Probability-Aware Parking Selection](https://arxiv.org/abs/2601.00521)
*Cameron Hickert,Sirui Li,Zhengbing He,Cathy Wu*

Main category: eess.SY

TL;DR: 提出概率感知停车选择问题，通过动态规划框架指导司机寻找最佳停车位而非直接前往目的地，考虑停车位可用性的概率信息，显著减少停车搜索时间。


<details>
  <summary>Details</summary>
Motivation: 现有停车导航系统低估总行程时间，未考虑寻找停车位的时间，这影响用户体验、出行方式选择、交通拥堵和排放。需要解决停车搜索时间被忽视的问题。

Method: 提出概率感知停车选择问题，采用自适应动态规划框架进行决策，基于停车场级别的停车可用性概率信息。通过闭式分析确定何时针对特定停车场或探索替代方案，以及预期时间成本。利用随机观测估计停车可用性，评估误差。

Result: 使用美国西雅图真实数据实验表明方法可行，观测频率增加时平均绝对误差从7%降至2%以下。基于数据的模拟显示，概率感知策略相比概率无感知基线节省时间达66%，但仍比直接到目的地估计多花123%时间。

Conclusion: 概率感知停车选择方法能有效考虑停车可用性的动态特性，显著减少停车搜索时间，但停车搜索仍然是行程中的重要时间组成部分。该方法为停车导航系统提供了更准确的行程时间估计框架。

Abstract: Current parking navigation systems often underestimate total travel time by failing to account for the time spent searching for a parking space, which significantly affects user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces the probability-aware parking selection problem, which aims to direct drivers to the best parking location rather than straight to their destination. An adaptable dynamic programming framework is proposed for decision-making based on probabilistic information about parking availability at the parking lot level. Closed-form analysis determines when it is optimal to target a specific parking lot or explore alternatives, as well as the expected time cost. Sensitivity analysis and three illustrative cases are examined, demonstrating the model's ability to account for the dynamic nature of parking availability. Acknowledging the financial costs of permanent sensing infrastructure, the paper provides analytical and empirical assessments of errors incurred when leveraging stochastic observations to estimate parking availability. Experiments with real-world data from the US city of Seattle indicate this approach's viability, with mean absolute error decreasing from 7% to below 2% as observation frequency grows. In data-based simulations, probability-aware strategies demonstrate time savings up to 66% relative to probability-unaware baselines, yet still take up to 123% longer than direct-to-destination estimates.

</details>


### [122] [Optimal Transport-Based Decentralized Multi-Agent Distribution Matching](https://arxiv.org/abs/2601.00548)
*Kooktae Lee*

Main category: eess.SY

TL;DR: 提出一种基于最优传输的多智能体系统分布匹配去中心化控制框架，通过局部信息实现终端空间分布匹配


<details>
  <summary>Details</summary>
Motivation: 多智能体系统需要实现规定的终端空间分布，但传统方法需要解决全局最优传输问题，计算复杂且需要全局信息

Method: 将分布匹配目标重构为可处理的单智能体决策过程，引入顺序权重更新规则构建可行局部传输计划，加入基于记忆的校正机制处理间歇性和范围受限通信

Result: 建立了收敛保证，在线性和非线性智能体动力学下都能实现替代传输成本的周期改进，仿真显示框架能有效、可扩展地实现分布匹配

Conclusion: 提出的去中心化控制框架能够仅使用局部信息实现多智能体系统的分布匹配，具有良好的收敛性和可扩展性

Abstract: This paper presents a decentralized control framework for distribution matching in multi-agent systems (MAS), where agents collectively achieve a prescribed terminal spatial distribution. The problem is formulated using optimal transport (Wasserstein distance), which provides a principled measure of distributional discrepancy and serves as the basis for the control design. To avoid solving the global optimal transport problem directly, the distribution-matching objective is reformulated into a tractable per-agent decision process, enabling each agent to identify its desired terminal locations using only locally available information. A sequential weight-update rule is introduced to construct feasible local transport plans, and a memory-based correction mechanism is incorporated to maintain reliable operation under intermittent and range-limited communication. Convergence guarantees are established, showing cycle-wise improvement of a surrogate transport cost under both linear and nonlinear agent dynamics. Simulation results demonstrate that the proposed framework achieves effective and scalable distribution matching while operating fully in a decentralized manner.

</details>


### [123] [Stability Verification for Switched Systems using Neural Multiple Lyapunov Functions](https://arxiv.org/abs/2601.00587)
*Junyue Huang,Shaoyuan Li,Xiang Yin*

Main category: eess.SY

TL;DR: 提出神经多重李雅普诺夫函数(NMLF)框架，结合MLF理论保证与NLF计算效率，用于状态依赖切换系统的稳定性分析


<details>
  <summary>Details</summary>
Motivation: 切换系统具有多操作模式和切换信号，其非线性动力学使得稳定性分析具有挑战性。现有方法如多重李雅普诺夫函数(MLF)在缺乏良好结构的系统中计算适用性有限

Method: 提出NMLF框架，结合MLF理论保证与神经李雅普诺夫函数计算效率。使用定制损失函数和反例引导归纳综合(CEGIS)方案训练神经网络，严格满足MLF条件

Result: 通过综合仿真和理论分析，证明了NMLF的有效性及其在复杂切换系统中实际部署的潜力

Conclusion: NMLF为状态依赖切换系统提供了一个统一的分析框架，将理论保证与计算效率相结合，具有实际应用价值

Abstract: Stability analysis of switched systems, characterized by multiple operational modes and switching signals, is challenging due to their nonlinear dynamics. While frameworks such as multiple Lyapunov functions (MLF) provide a foundation for analysis, their computational applicability is limited for systems without favorable structure. This paper investigates stability analysis for switched systems under state-dependent switching conditions. We propose neural multiple Lyapunov functions (NMLF), a unified framework that combines the theoretical guarantees of MLF with the computational efficiency of neural Lyapunov functions (NLF). Our approach leverages a set of tailored loss functions and a counter-example guided inductive synthesis (CEGIS) scheme to train neural networks that rigorously satisfy MLF conditions. Through comprehensive simulations and theoretical analysis, we demonstrate NMLF's effectiveness and its potential for practical deployment in complex switched systems.

</details>


### [124] [A formal theory on problem space as a semantic world model in systems engineering](https://arxiv.org/abs/2601.00755)
*Mayuranath SureshKumar,Hanumanthrao Kannan*

Main category: eess.SY

TL;DR: 本文提出了一种形式化的问题空间理论，为系统工程建立了明确的问题域语义世界模型，将问题分析与解决方案设计分离。


<details>
  <summary>Details</summary>
Motivation: 当前系统工程实践中，从利益相关者目标直接推导到规范性工件，使得关于操作环境、可接受交互和上下文条件的基本假设变得隐含或过早嵌入架构或需求中，缺乏对问题空间本身的严格系统理论表示。

Method: 通过形式化问题空间为明确的语义世界模型，包含在需求和解决方案承诺之前定义的理论构造，并开发公理、定理和推论，建立严格的边界语义、上下文相关交互可追溯性和问题空间规范充分性标准。

Result: 建立了一个严格的标准，用于无歧义的边界语义、上下文相关交互到成功利益相关者目标满足的可追溯性，以及问题空间规范的充分性，使有纪律的推理能够独立于解决方案设计进行。

Conclusion: 该理论为实践者提供了明确区分问题域真实情况与所选解决方案的框架，通过利益相关者与工程师之间的对话式假设案例研究，展示了理论如何在设计任何规范性工件之前指导问题框架构建。

Abstract: Classic problem-space theory models problem solving as a navigation through a structured space of states, operators, goals, and constraints. Systems Engineering (SE) employs analogous constructs (functional analysis, operational analysis, scenarios, trade studies), yet still lacks a rigorous systems-theoretic representation of the problem space itself. In current practice, reasoning often proceeds directly from stakeholder goals to prescriptive artifacts. This makes foundational assumptions about the operational environment, admissible interactions, and contextual conditions implicit or prematurely embedded in architectures or requirements. This paper addresses that gap by formalizing the problem space as an explicit semantic world model containing theoretical constructs that are defined prior to requirements and solution commitments. These constructs along with the developed axioms, theorems and corollary establish a rigorous criterion for unambiguous boundary semantics, context-dependent interaction traceability to successful stakeholder goal satisfaction, and sufficiency of problem-space specification over which disciplined reasoning can occur independent of solution design. It offers a clear distinction between what is true of the problem domain and what is chosen as a solution. The paper concludes by discussing the significance of the theory on practitioners and provides a dialogue-based hypothetical case study between a stakeholder and an engineer, demonstrating how the theory guides problem framing before designing any prescriptive artifacts.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [125] [The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth](https://arxiv.org/abs/2601.00306)
*Emilio Ferrara*

Main category: cs.CY

TL;DR: 论文提出"合成现实"概念，认为生成式AI的最大风险不是单个深度伪造，而是共享认知基础和制度验证实践的逐步侵蚀，并提出了分层风险框架和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 当前公众讨论往往将生成式AI的危害局限于"深度伪造"或虚假信息的扩展，而忽略了更广泛的社会技术转变：生成式AI创造了合成现实，其中内容、身份和社交互动被共同制造并相互强化，这可能导致共享认知基础的崩溃。

Method: 1) 将合成现实形式化为分层堆栈（内容、身份、互动、制度）；2) 扩展生成式AI危害分类法；3) 阐述生成式AI带来的质性转变；4) 将近期风险案例（2023-2025）整理为案例库；5) 提出包含来源基础设施、平台治理等的缓解堆栈。

Result: 论文系统分析了生成式AI带来的合成现实风险，识别了成本崩溃、定制化、来源缺口等关键机制，并通过实际案例展示了这些机制如何在欺诈、选举等领域显现，提出了互补而非替代的缓解策略。

Conclusion: 生成式AI的最大风险是共享认知基础的侵蚀，需要多层次缓解策略。最终提出了"生成式AI悖论"：随着合成媒体无处不在，社会可能理性地完全忽视数字证据。

Abstract: Generative AI (GenAI) now produces text, images, audio, and video that can be perceptually convincing at scale and at negligible marginal cost. While public debate often frames the associated harms as "deepfakes" or incremental extensions of misinformation and fraud, this view misses a broader socio-technical shift: GenAI enables synthetic realities; coherent, interactive, and potentially personalized information environments in which content, identity, and social interaction are jointly manufactured and mutually reinforcing. We argue that the most consequential risk is not merely the production of isolated synthetic artifacts, but the progressive erosion of shared epistemic ground and institutional verification practices as synthetic content, synthetic identity, and synthetic interaction become easy to generate and hard to audit. This paper (i) formalizes synthetic reality as a layered stack (content, identity, interaction, institutions), (ii) expands a taxonomy of GenAI harms spanning personal, economic, informational, and socio-technical risks, (iii) articulates the qualitative shifts introduced by GenAI (cost collapse, throughput, customization, micro-segmentation, provenance gaps, and trust erosion), and (iv) synthesizes recent risk realizations (2023-2025) into a compact case bank illustrating how these mechanisms manifest in fraud, elections, harassment, documentation, and supply-chain compromise. We then propose a mitigation stack that treats provenance infrastructure, platform governance, institutional workflow redesign, and public resilience as complementary rather than substitutable, and outline a research agenda focused on measuring epistemic security. We conclude with the Generative AI Paradox: as synthetic media becomes ubiquitous, societies may rationally discount digital evidence altogether.

</details>


### [126] [Improving criminal case management through Machine Learning system](https://arxiv.org/abs/2601.00396)
*Fernanda Sobrino,Adolfo De Unánue T.,Edgar Hernández,Patricia Villa,Elena Villalobos,David Aké,Stephany Cisneros,Cristian Paul Camacho Osnay,Armando García Neri,Israel Hernández*

Main category: cs.CY

TL;DR: 墨西哥检察官办公室使用机器学习系统对案件进行优先级排序，预测哪些案件可能在6个月内结案，以帮助处理积压案件。


<details>
  <summary>Details</summary>
Motivation: 墨西哥检察官面临案件积压严重、机构能力有限的问题，需要在不干扰现有工作流程的情况下提高效率。

Method: 与Zacatecas州检察官办公室合作开发机器学习系统，使用2014-2024年行政数据训练分类模型（随机森林），预测案件在6个月内结案的可能性，每周生成300个案件的优先级排序列表。

Result: 随机森林分类器在实时约束下达到平均Precision@300为0.74，表明数据驱动的优先级排序可以有效识别可处理的案件。

Conclusion: 数据驱动的案件优先级排序可以作为低开销工具提高检察官工作效率，系统强调可解释性和操作可行性，将通过随机对照试验进一步测试。

Abstract: Prosecutors across Mexico face growing backlogs due to high caseloads and limited institutional capacity. This paper presents a machine learning (ML) system co-developed with the Zacatecas State Prosecutor's Office to support internal case triage. Focusing on the Módulo de Atención Temprana (MAT) -- the unit responsible for intake and early-stage case resolution -- we train classification models on administrative data from the state's digital case management system (PIE) to predict which open cases are likely to finalize within six months. The model generates weekly ranked lists of 300 cases to assist prosecutors in identifying actionable files. Using historical data from 2014 to 2024, we evaluate model performance under real-time constraints, finding that Random Forest classifiers achieve a mean Precision@300 of 0.74. The system emphasizes interpretability and operational feasibility, and we will test it via a randomized controlled trial. Our results suggest that data-driven
  prioritization can serve as a low-overhead tool for improving prosecutorial efficiency without disrupting existing workflows.

</details>


### [127] [Measuring University Students Satisfaction with Traditional Search Engines and Generative AI Tools as Information Sources](https://arxiv.org/abs/2601.00493)
*Brady D. Lund,Scott J. Warren,Zoe A. Teel*

Main category: cs.CY

TL;DR: 美国大学生对传统搜索引擎的满意度高于生成式AI工具，使用频率是满意度的重要预测因素，国际生和本科生对AI工具满意度更高


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解大学生对生成式AI工具和传统搜索引擎作为学术信息源的满意度差异，探索学生信息寻求行为的变化模式，为高等教育中整合传统和AI驱动的信息源提供依据

Method: 2025年秋季对美国大学生进行电子调查，收集236份有效问卷，测量使用频率和满意度，采用主成分分析识别满意度结构，k-means聚类分析学生群体，回归分析预测因素

Result: 学生普遍对传统搜索引擎满意度更高；识别出两个主要学生群体：对搜索引擎满意但对AI不满的群体，以及对两者都中度到高度满意的群体；使用频率是满意度的强预测因素；国际生和本科生对AI工具满意度显著高于国内生和研究生

Conclusion: 学生将AI工具视为补充信息源而非替代品，研究结果揭示了学生信息寻求行为的变化模式，为高等教育评估和整合传统与AI信息源提供了重要见解

Abstract: This study examines university students levels of satisfaction with generative artificial intelligence (AI) tools and traditional search engines as academic information sources. An electronic survey was distributed to students at U.S. universities in late fall 2025, with 236 valid responses received. In addition to demographic information about respondents, frequency of use and levels of satisfaction with both generative AI and traditional search engines were measured. Principal components analysis identified distinct constructs of satisfaction for each information source, while k-means cluster analysis revealed two primary student groups: those highly satisfied with search engines but dissatisfied with AI, and those moderately to highly satisfied with both. Regression analysis showed that frequency of use strongly predicts satisfaction, with international and undergraduate students reporting significantly higher satisfaction with AI tools than domestic and graduate students. Students generally expressed higher levels of satisfaction with traditional search engines over generative AI tools. Those who did prefer AI tools appear to see them more as a complementary source of information rather than a replacement for other sources. These findings stress evolving patterns of student information seeking and use behavior and offer meaningful insights for evaluating and integrating both traditional and AI-driven information sources within higher education.

</details>


### [128] [The Imperative for Grand Challenges in Computing](https://arxiv.org/abs/2601.00700)
*William Regli,Rajmohan Rajaraman,Daniel Lopresti,David Jensen,Mary Lou Maher,Manish Parashar,Mona Singh,Holly Yanco*

Main category: cs.CY

TL;DR: 该论文呼吁计算领域定义和追求"重大挑战"，这些挑战应具有足够规模和影响力，能够激发计算机科学家的想象力并吸引其他领域研究者参与，从而推动科学和社会进步。


<details>
  <summary>Details</summary>
Motivation: 计算已成为几乎所有技术不可或缺的组成部分，对大多数科学发现和创新至关重要。然而，计算领域尚未产生像物理学、天文学或工程学那样规模的重大挑战。计算社区需要更有意识地定义能够超越传统边界、深刻理解世界并积极塑造社会未来的重大挑战。

Method: 基于先前重大挑战的经验教训，论文探讨了当今重大挑战的性质（强调规模和影响力），并分析在计算创新生态系统快速变化的背景下，社区如何应对这样的重大挑战。

Result: 论文强调现在比以往任何时候都更需要定义和追求计算领域的重大挑战，并有意识地实现其对科学和社会的影响转化。

Conclusion: 论文呼吁计算社区团结起来，为未来十年及更长时间定义计算领域的重大挑战，推动领域发展并产生广泛社会影响。

Abstract: Computing is an indispensable component of nearly all technologies and is ubiquitous for vast segments of society. It is also essential to discoveries and innovations in most disciplines. However, while past grand challenges in science have involved computing as one of the tools to address the challenge, these challenges have not been principally about computing. Why has the computing community not yet produced challenges at the scale of grandeur that we see in disciplines such as physics, astronomy, or engineering? How might we go about identifying similarly grand challenges? What are the grand challenges of computing that transcend our discipline's traditional boundaries and have the potential to dramatically improve our understanding of the world and positively shape the future of our society?
  There is a significant benefit in us, as a field, taking a more intentional approach to "grand challenges." We are seeking challenge problems that are sufficiently compelling as to both ignite the imagination of computer scientists and draw researchers from other disciplines to computational challenges.
  This paper emphasizes the importance, now more than ever, of defining and pursuing grand challenges in computing as a field, and being intentional about translation and realizing its impacts on science and society. Building on lessons from prior grand challenges, the paper explores the nature of a grand challenge today emphasizing both scale and impact, and how the community may tackle such a grand challenge, given a rapidly changing innovation ecosystem in computing. The paper concludes with a call to action for our community to come together to define grand challenges in computing for the next decade and beyond.

</details>


### [129] [PDPL Metric: Validating a Scale to Measure Personal Data Privacy Literacy Among University Students](https://arxiv.org/abs/2601.00715)
*Brady D. Lund,Nathan Brown,Ana Roeschley,Gahangir Hossain*

Main category: cs.CY

TL;DR: 开发并验证了个人数据隐私素养（PDPL）量表，用于测量大学生在六个隐私维度的能力，为高等教育中的数字素养评估提供工具。


<details>
  <summary>Details</summary>
Motivation: 需要一种有效的测量工具来评估个人数据隐私素养，特别是在高等教育环境中，以支持数字素养教育和隐私保护政策的制定。

Method: 开发了24项问卷，针对美国研究型大学学生进行调查，使用主成分分析验证各构念的单维性和内部一致性，并进行二阶分析验证整体PDPL构念。

Result: 量表验证成功，六个隐私构念（数据滥用感知风险、知情同意期望、一般隐私关注、隐私管理意识、隐私-效用权衡接受度、数据安全重要性感知）整合为统一的PDPL构念。除国内/国际学生身份外，基本人口统计学变量无显著差异。

Conclusion: PDPL量表为高等教育环境中的个人数据隐私素养评估提供了有效框架，支持将核心隐私构念整合到高等教育项目、组织政策和校园数字素养倡议中。

Abstract: Personal data privacy literacy (PDPL) refers to a collection of digital literacy skills related to an individuals ability to understand, evaluate, and manage the collection, use, and protection of personal data in online and digital environments. This study introduces and validates a new psychometric scale (PDPL Metric) designed to measure data privacy literacy among university students, focusing on six key privacy constructs: perceived risk of data misuse, expectations of informed consent, general privacy concern, privacy management awareness, privacy-utility trade-off acceptance, and perceived importance of data security. A 24-item questionnaire was developed and administered to students at U.S.-based research universities. Principal components analysis confirmed the unidimensionality and internal consistency of each construct, and a second-order analysis supported the integration of all six into a unified PDPL construct. No differences in PDPL were found based on basic demographic variables like academic level and gender, although a difference was found based on domestic/international status. The findings of this study offer a validated framework for assessing personal data privacy literacy within the higher education context and support the integration of the core constructs into higher education programs, organizational policies, and digital literacy initiatives on university campuses.

</details>


### [130] [Toward Open Science in the AEC Community: An Ecosystem for Sustainable Digital Knowledge Sharing and Reuse](https://arxiv.org/abs/2601.00788)
*Ruoxin Xiong,Yanyu Wang,Jiannan Cai,Kaijian Liu,Yuansheng Zhu,Pingbo Tang,Nora El-Gohary,George Edward Gibson*

Main category: cs.CY

TL;DR: OpenConstruction是一个社区驱动的开放科学生态系统，用于聚合、组织和情境化AEC行业的数字资源，包括数据集、模型、用例和教育材料，以提高资源的可发现性和重用性。


<details>
  <summary>Details</summary>
Motivation: AEC行业正在经历快速数字化转型，产生了大量数字资产，但这些资源分散在不同存储库中，文档不一致，限制了它们在研究、教育和实践中的可发现性、可解释性和重用性。

Method: 创建了一个社区驱动的开放科学生态系统，将资源组织为四个目录（数据集、模型、用例、教育资源），采用一致的描述符、策展人验证和透明治理机制。

Result: 截至2025年12月，平台已托管94个数据集、65个模型以及不断增长的用例和教育材料集合。两个案例研究表明该生态系统支持基准测试、课程开发和AEC领域开放科学实践的更广泛采用。

Conclusion: OpenConstruction平台通过聚合和标准化AEC数字资源，解决了资源碎片化问题，促进了开放科学实践在建筑、工程和施工行业的应用，平台已公开访问。

Abstract: The Architecture, Engineering, and Construction (AEC) industry is undergoing rapid digital transformation, producing diverse digital assets such as datasets, computational models, use cases, and educational materials across the built environment lifecycle. However, these resources are often fragmented across repositories and inconsistently documented, limiting their discoverability, interpretability, and reuse in research, education, and practice. This study introduces OpenConstruction, a community-driven open-science ecosystem that aggregates, organizes, and contextualizes openly accessible AEC digital resources. The ecosystem is structured into four catalogs, including datasets, models, use cases, and educational resources, supported by consistent descriptors, curator-led validation, and transparent governance. As of December 2025, the platform hosts 94 datasets, 65 models, and a growing collection of use cases and educational materials. Two case studies demonstrate how the ecosystem supports benchmarking, curriculum development, and broader adoption of open-science practices in the AEC sector. The platform is publicly accessible at https://www.openconstruction.org/.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [131] [Full grid solution for multi-asset options pricing with tensor networks](https://arxiv.org/abs/2601.00009)
*Lucas Arenstein,Michael Kastoryano*

Main category: q-fin.CP

TL;DR: 使用量化张量列(QTT)方法解决多维Black-Scholes PDE的维度诅咒问题，可在个人计算机上处理10-15个标的资产的高维期权定价问题。


<details>
  <summary>Details</summary>
Motivation: 传统的Black-Scholes PDE全网格求解器受维度诅咒限制，只能处理最多3个标的资产，而实践中需要处理多个相关标的的复杂金融工具，通常依赖蒙特卡洛方法。

Method: 采用量化张量列(QTT)表示方法，构建算子、支付函数和边界条件的QTT表示，其秩随维度多项式增长、随网格大小多对数增长，开发了两种求解器：时间步进算法（欧式和美式期权）和时空算法（欧式期权）。

Result: 在个人计算机上实现了3-5维相关篮子期权和最大最小期权的全网格价格和希腊值高精度计算，方法可扩展到10-15个标的资产的全网格求解。

Conclusion: QTT方法将d资产Black-Scholes PDE转化为可处理的高维问题，突破了传统方法的维度限制，为多资产期权定价提供了高效精确的数值解法。

Abstract: Pricing multi-asset options via the Black-Scholes PDE is limited by the curse of dimensionality: classical full-grid solvers scale exponentially in the number of underlyings and are effectively restricted to three assets. Practitioners typically rely on Monte Carlo methods for computing complex instrument involving multiple correlated underlyings. We show that quantized tensor trains (QTT) turn the d-asset Black-Scholes PDE into a tractable high-dimensional problem on a personal computer. We construct QTT representations of the operator, payoffs, and boundary conditions with ranks that scale polynomially in d and polylogarithmically in the grid size, and build two solvers: a time-stepping algorithm for European and American options and a space-time algorithm for European options. We compute full-grid prices and Greeks for correlated basket and max-min options in three to five dimensions with high accuracy. The methods introduced can comfortably be pushed to full-grid solutions on 10-15 underlyings, with further algorithmic optimization and more compute power.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [132] [Option Pricing beyond Black-Scholes Model:Quantum Mechanics Approach](https://arxiv.org/abs/2601.00293)
*Pengpeng Li,Shi-Dong Liang*

Main category: q-fin.RM

TL;DR: 基于随机动力学与量子谐振子的类比，提出市场驱动力模型来推广金融市场的Black-Scholes模型，提供考虑各种意外市场行为的新期权定价方案


<details>
  <summary>Details</summary>
Motivation: 传统Black-Scholes模型无法充分考虑各种意外市场行为和市场驱动力，需要更灵活的定价框架来应对实际市场中的不确定性

Method: 利用随机动力学与量子谐振子的类比，建立市场驱动力模型，将意外市场行为纳入期权定价框架

Result: 提出了新的期权定价方案，能够考虑各种市场驱动力，并通过具体市场驱动力示例分析其对期权定价的影响

Conclusion: 该模型提供两种实际应用：一是当预测到隐藏市场驱动力出现时作为新的定价方案；二是当意外驱动力出现时暗示存在风险溢价

Abstract: Based on the analog between the stochastic dynamics and quantum harmonic oscillator, we propose a market force driving model to generalize the Black-Scholes model in finance market. We give new schemes of option pricing, in which we can take various unexpected market behaviors into account to modify the option pricing. As examples, we present several market forces to analyze their effects on the option pricing. These results provide us two practical applications. One is to be used as a new scheme of option pricing when we can predict some hidden market forces or behaviors emerging. The other implies the existence of some risk premium when some unexpected forces emerge.

</details>


### [133] [Multimodal Insights into Credit Risk Modelling: Integrating Climate and Text Data for Default Prediction](https://arxiv.org/abs/2601.00478)
*Zongxiao Wu,Ran Liu,Jiang Dai,Dan Luo*

Main category: q-fin.RM

TL;DR: 该研究提出了一种多模态框架，整合结构化信贷数据、气候面板数据和非结构化文本叙述，用于小微企业信用风险评估，发现多模态方法显著提升违约预测准确性，其中气候风险因素（特别是雨水内涝）具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 小微企业信用风险评估面临传统结构化财务数据有限的挑战，需要整合更多样化的信息源，包括气候数据和文本叙述，以提高预测准确性并更好地理解风险驱动因素。

Method: 提出多模态学习框架，整合三种数据类型：结构化信贷变量、气候面板数据、非结构化文本叙述。使用LSTM、GRU和Transformer模型分析不同数据模态间的相互作用，并采用SHAP可解释性方法分析模型决策。

Result: 基于气候或文本数据的单模态模型优于仅使用结构化数据的模型，而多模态数据整合显著提升了信用违约预测性能。SHAP分析显示物理气候风险在违约预测中起重要作用，其中雨水内涝是最具影响力的因素。

Conclusion: 多模态方法在AI驱动的决策中具有巨大潜力，为信用风险评估提供了稳健工具，同时促进了环境和文本洞察在预测分析中的更广泛整合，特别适用于财务历史有限的小微企业。

Abstract: Credit risk assessment increasingly relies on diverse sources of information beyond traditional structured financial data, particularly for micro and small enterprises (mSEs) with limited financial histories. This study proposes a multimodal framework that integrates structured credit variables, climate panel data, and unstructured textual narratives within a unified learning architecture. Specifically, we use long short-term memory (LSTM), the gated recurrent unit (GRU), and transformer models to analyse the interplay between these data modalities. The empirical results demonstrate that unimodal models based on climate or text data outperform those relying solely on structured data, while the integration of multiple data modalities yields significant improvements in credit default prediction. Using SHAP-based explainability methods, we find that physical climate risks play an important role in default prediction, with water-logging by rain emerging as the most influential factor. Overall, this study demonstrates the potential of multimodal approaches in AI-enabled decision-making, which provides robust tools for credit risk assessment while contributing to the broader integration of environmental and textual insights into predictive analytics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [134] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 提出一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，提升响应质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略仍是一个挑战。需要超越表面语义相似性，检索与对话逻辑结构对齐的知识。

Method: 采用粗到细的两阶段知识检索方法：1）首先识别知识库中与上下文相关的子区域，确保所有句子都与主题相关；2）在该子区域内进一步检索与推理过程具体相关的知识。两阶段都使用蒙特卡洛树搜索启发的方法，通过关键词在知识句子中导航。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更贴近人类对话的底层推理逻辑，还显著提升了检索知识的多样性，从而生成更具信息性和创造性的响应。

Conclusion: 提出的推理感知知识检索方法成功整合了检索和推理策略，通过逻辑结构对齐的知识检索，有效提升了LLMs在多轮对话中的性能，为知识增强型对话系统提供了新思路。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [135] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 该研究开发了一种基于大语言模型的尼日利亚皮钦语抑郁症自动筛查系统，通过收集432个音频响应并精细标注，对三种LLM进行微调，GPT-4.1在PHQ-9严重程度评分预测上达到94.5%准确率，为资源有限的语言多样化环境提供心理健康筛查工具。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，主要障碍包括临床医生资源不足、社会污名化和语言障碍。传统PHQ-9问卷在高收入国家验证，但无法适应尼日利亚皮钦语和520多种本地语言的文化语言环境，需要开发适合当地语言文化的自动化筛查工具。

Method: 收集432名18-40岁尼日利亚年轻人的皮钦语音频响应，基于PHQ-9项目设计心理体验评估提示；进行转录、严格预处理和标注（语义标记、俚语习语解释、PHQ-9严重程度评分）；对Phi-3-mini-4k-instruct、Gemma-3-4B-it和GPT-4.1三种大语言模型进行微调；通过定量（准确性、精确度、语义对齐）和定性（清晰度、相关性、文化适宜性）评估模型性能。

Result: GPT-4.1表现最佳，在PHQ-9严重程度评分预测上达到94.5%准确率，优于Gemma-3-4B-it和Phi-3-mini-4k-instruct；定性评估也显示GPT-4.1产生最文化适宜、清晰且上下文相关的响应，为尼日利亚服务不足社区提供AI介导的抑郁症筛查解决方案。

Conclusion: 该研究证明了基于大语言模型的自动化抑郁症筛查在尼日利亚皮钦语环境中的可行性，为语言多样化、资源有限环境部署对话式心理健康工具奠定了基础，有助于解决临床资源不足、污名化和语言障碍等挑战。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [136] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: 该论文提出了一个基于不可逆信息处理的物理智能理论，将智能系统建模为受守恒定律约束的耦合代理-环境过程，定义了智能为每纳特不可逆处理信息产生的目标导向功。


<details>
  <summary>Details</summary>
Motivation: 建立统一的、与底物无关的智能物理理论，将信息处理与物理守恒定律联系起来，为理解生物和人工智能系统提供物理基础。

Method: 提出守恒一致编码（CCE）框架，将编码对应为吸引子的亚稳态盆地，其可分性由守恒定律强制执行。从智能定义出发推导物理约束层次，分析振荡和近临界动力学，发展连续动力电路理论。

Result: 揭示了长时程效率需要保持内部信息结构（导致自建模），确立了物理体现智能系统具有内在认知极限。分析显示大脑接近理论预测的高效运行状态，布尔逻辑作为吸引子选择的特例出现。

Conclusion: 该理论为智能作为物理现象提供了统一解释，建立了信息处理、守恒定律和功提取之间的联系，并为人工智能安全提供了基于不可逆信息流和结构稳态的物理基础视角。

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [137] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 提出多算法方法解决最后一公里包裹配送中的工作量平衡问题，通过距离和工作量双重考量优化包裹分配，确保每位配送员完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近性的包裹分配方法效率低下，导致配送员之间工作量分布不均衡。需要解决最后一公里城市包裹配送系统中的人力资源工作量平衡问题。

Method: 采用多算法方法，包括不同版本的k-means、进化算法、基于k-means初始化的递归分配（使用不同问题编码）以及混合进化集成算法。算法综合考虑配送点距离和配送员位置，优化包裹分配。

Result: 在西班牙Azuqueca de Henares的实际最后一公里包裹配送场景中验证了方法的性能，能够有效平衡配送员工作量。

Conclusion: 提出的多算法方法能够有效解决最后一公里包裹配送中的工作量平衡问题，通过优化配送时间实现工作量在全体员工中的均衡分配。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [138] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 提出基于MinDist度量的规则框架，用于13张牌印度拉米纸牌游戏，通过量化手牌与最近有效配置的编辑距离来评估手牌结构接近度，显著提升胜率。


<details>
  <summary>Details</summary>
Motivation: 13张牌印度拉米纸牌是不完全信息的顺序游戏，需要概率推理和组合决策。传统启发式方法在策略设计上存在局限性，需要更形式化和可解释的算法框架。

Method: 1. 提出MinDist手牌评估度量，修改MinScore度量，量化手牌与最近有效配置的编辑距离；2. 设计计算高效算法，基于MinScore算法，利用动态剪枝和模式缓存；3. 在两人零和模拟框架中整合对手手牌建模；4. 使用统计假设检验评估策略。

Result: 实证结果显示，基于MinDist的智能体相比传统启发式方法在胜率上有显著提升，为算法化拉米策略设计提供了形式化和可解释的步骤。

Conclusion: MinDist度量为不完全信息纸牌游戏提供了有效的策略框架，通过结构接近度评估和高效计算算法，显著改进游戏表现，为类似游戏策略设计提供了参考。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [139] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究探讨生成式AI如何理解乡土建筑中的建筑智慧，以伊朗鸽塔为例，测试三种扩散模型在不同提示阶段的表现，发现AI能复制几何模式但误解材料和气候逻辑。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI系统如何解释乡土形式中蕴含的建筑智慧，理解AI在感知、扭曲和重新想象传统设计智能方面的能力边界。

Method: 以伊朗鸽塔为案例研究，测试Midjourney v6、DALL-E 3和基于Stable Diffusion XL的DreamStudio三种扩散模型，采用参考性、适应性和推测性三个提示阶段，使用五标准评估框架（类型学、材料性、环境、真实性和文化特异性）进行评估。

Result: AI能可靠地复制几何模式，但误解材料和气候逻辑；参考图像提高真实性但限制创造力，无参考限制则产生创新但文化模糊的结果；定义了视觉相似性与建筑推理之间的边界。

Conclusion: 计算乡土推理可作为分析AI如何感知、扭曲和重新想象传统设计智能的框架，揭示了AI在理解建筑深层逻辑方面的局限性。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [140] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 提出一个基于大语言模型的智能体，能够从原始文本中提取因果反馈模糊认知图，并通过双向动态过程实现系统的准自主演化。


<details>
  <summary>Details</summary>
Motivation: 传统模糊认知图构建依赖人工专家，过程耗时且主观。本文旨在利用大语言模型的半自主能力，自动化地从文本中提取因果结构，实现更高效、可扩展的因果建模。

Method: 设计三阶段系统指令：1) 从文本中提取关键名词和名词短语；2) 从中选择FCM概念节点；3) 推断节点间的模糊因果边。使用LLM智能体（Gemini和ChatGPT）执行此过程，并混合生成FCM。

Result: 在Kissinger关于AI的论文测试中，LLM生成的FCM与人工生成的FCM收敛到相同的平衡极限环，尽管节点和边数量不同。混合FCM不仅吸收了主要成分的平衡点，还创造了新的平衡点，更好地近似底层因果动态系统。

Conclusion: LLM智能体能够有效提取文本中的因果结构并生成FCM，混合多个LLM生成的FCM可以产生更丰富的动态行为，为自动化因果建模提供了有前景的方法。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [141] [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
*Asterios Tsiourvas,Wei Sun,Georgia Perakis*

Main category: cs.AI

TL;DR: 提出基于因果推断的端到端LLM路由框架，从观测数据学习路由策略，避免传统方法需要全反馈数据和误差累积的问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方法通常采用解耦策略：先预测各模型性能指标，再基于预测选择模型。这种方法存在误差累积问题，且依赖全反馈数据（每个查询需要所有候选模型评估），实际中获取和维护成本高昂

Method: 提出因果端到端框架，直接从观测数据学习路由策略，最小化决策遗憾。引入两个理论上有保证的代理目标：基于分类的上界，以及能收敛到最优策略的softmax加权遗憾近似。还通过区间条件架构处理异构成本偏好

Result: 在公共基准测试中，该方法优于现有基线，在不同嵌入模型上实现了最先进的性能

Conclusion: 提出的因果端到端框架能够有效从观测数据学习LLM路由策略，解决了传统方法需要全反馈数据和误差累积的问题，在实际应用中具有更好的可行性和性能

Abstract: LLM routing aims to select the most appropriate model for each query, balancing competing performance metrics such as accuracy and cost across a pool of language models. Prior approaches typically adopt a decoupled strategy, where the metrics are first predicted and the model is then selected based on these estimates. This setup is prone to compounding errors and often relies on full-feedback data, where each query is evaluated by all candidate models, which is costly to obtain and maintain in practice. In contrast, we learn from observational data, which records only the outcome of the model actually deployed. We propose a causal end-to-end framework that learns routing policies by minimizing decision-making regret from observational data. To enable efficient optimization, we introduce two theoretically grounded surrogate objectives: a classification-based upper bound, and a softmax-weighted regret approximation shown to recover the optimal policy at convergence. We further extend our framework to handle heterogeneous cost preferences via an interval-conditioned architecture. Experiments on public benchmarks show that our method outperforms existing baselines, achieving state-of-the-art performance across different embedding models.

</details>


### [142] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar系统使用质量多样性算法和大型语言模型自动演化游戏机制，通过合成完整游戏并评估技能排序能力来设计游戏。


<details>
  <summary>Details</summary>
Motivation: 游戏机制设计是耗时且依赖专家经验的过程，需要自动化方法来探索多样化的游戏机制。

Method: 结合质量多样性算法和大型语言模型探索多样化机制，通过树搜索合成完整游戏，评估机制对技能排序的贡献。

Result: Mortar能生成多样且可玩的游戏，产生对技能排序得分贡献更大的机制，消融研究和用户研究验证了系统组件的重要性。

Conclusion: Mortar系统成功实现了游戏机制的自主演化，为自动游戏设计提供了有效方法。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [143] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM赋能的智能体不仅存在人口统计偏见，还会在"我们vs他们"的群体线索下表现出群体间偏见。当这种群体边界与智能体-人类划分重合时，风险从人类群体间差异转向更根本的群体不对称——人类整体可能被智能体视为外群体。研究还提出了信念中毒攻击（BPA）来抑制人类规范脚本并重新激活对人类的偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM赋能的智能体是否会在"我们vs他们"的群体线索下表现出群体间偏见，特别是当群体边界与智能体-人类划分重合时，人类整体可能被智能体视为外群体的风险。这种风险比传统的人口统计偏见（如性别、宗教偏见）更为根本。

Method: 构建了一个受控的多智能体社会模拟，基于明确收益权衡下的分配决策。通过最小化群体线索来测试智能体的群体间偏见。提出了信念中毒攻击（BPA），包括初始化时的档案中毒（BPA-PP）和通过优化信念精炼后缀注入存储反思中的记忆中毒（BPA-MP）。

Result: 实验表明：1）智能体在最小群体线索下表现出一致的群体间偏见；2）当部分对应方被设定为人类时，这种偏见会减弱，但这种减弱依赖于智能体相信真实人类存在的信念；3）信念中毒攻击（BPA）能够有效抑制人类规范脚本并重新激活对人类的偏见，BPA-PP和BPA-MP在不同设置下都表现出严重性。

Conclusion: LLM赋能的智能体确实存在群体间偏见风险，特别是当群体边界与智能体-人类划分重合时。信念依赖创造了新的攻击面，信念中毒攻击（BPA）能够利用这一漏洞。研究提出了在档案和记忆边界进行干预的实用缓解策略，旨在为更安全的智能体设计提供信息，而非促进实际利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [144] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: LLMs作为端到端库存优化求解器存在"幻觉税"性能缺陷，提出混合智能体框架分离语义推理与数学计算，通过数字孪生测试显示成本降低32.1%，证明LLMs应作为自然语言接口而非OR替代品。


<details>
  <summary>Details</summary>
Motivation: 中小企业在库存管理中缺乏部署高级优化方法的专业知识，需要探索LLMs是否能帮助弥合这一差距，但发现LLMs作为端到端求解器存在性能缺陷。

Method: 提出混合智能体框架，严格分离语义推理与数学计算：LLM作为智能接口从自然语言提取参数并解释结果，同时自动调用严格算法构建优化引擎。引入Human Imitator（有限理性管理者的数字孪生）进行可扩展、可重复的压力测试。

Result: 混合框架相比使用GPT-4o作为端到端求解器的交互基线，总库存成本降低32.1%。提供完美真实信息本身不足以改善GPT-4o性能，表明瓶颈本质是计算而非信息问题。

Conclusion: LLMs不应作为运筹学的替代品，而应作为自然语言接口，使非专家能够访问基于严格求解器的策略。混合智能体框架能有效解决LLMs在随机推理方面的局限性。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [145] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

TL;DR: 提出Mathesis神经符号架构，通过符号推理内核将逻辑约束映射到连续能量空间，将证明搜索转化为能量最小化问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理中存在持续的逻辑失败，缺乏内部公理化框架，需要新的架构来解决这一根本问题

Method: 1) 将数学状态编码为高阶超图；2) 使用可微逻辑引擎SRK将约束映射到连续能量空间；3) 定义全局能量函数E(G)，零能量表示逻辑一致性；4) 通过梯度信号训练超图变换器大脑；5) 结合蒙特卡洛树搜索和进化证明搜索实现多步推理

Result: 通过将证明搜索转化为能量最小化问题，实现了基于梯度信号的训练和引导式多步推理

Conclusion: Mathesis架构通过神经符号方法解决了LLMs的逻辑推理缺陷，将符号逻辑与神经网络训练相结合，为复杂数学推理提供了新框架

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [146] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 研究发现在视频问答任务中，置信度阈值方法可以在分布内提供可靠的错误率控制，但在分布偏移下失效，需要开发更鲁棒的弃权机制。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在高风险部署中需要选择性预测能力，即当模型不确定时选择弃权而非产生代价高昂的错误。研究旨在探究置信度弃权是否能在视频问答中提供可靠的错误率控制，以及这种控制在分布偏移下是否保持鲁棒。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型，通过置信度阈值方法进行选择性预测。在分布内和分布偏移两种情况下，系统性地调整阈值epsilon来研究风险-覆盖率的权衡关系。

Result: 1. 在分布内，置信度阈值提供了机制性控制，通过调整阈值可以平滑地降低错误率；2. 在分布偏移下，置信度阈值方法失效，无法提供可靠的错误率控制。

Conclusion: 置信度弃权在分布内有效但在分布偏移下不可靠，这表明需要开发更鲁棒的弃权机制来确保视觉语言模型在高风险应用中的安全性。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [147] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: 论文比较了三种神经推理方法：LLM推理、监督学习推理和显式模型推理，发现显式模型推理最可靠，并提出了一种新的球面神经网络来解决逻辑推理问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推理方面不可靠，监督学习推理存在灾难性遗忘问题，需要寻找更可靠的神经推理方法。

Method: 提出球面神经网络，将概念表示为n维球面上的圆，通过补圆表示否定运算符，过滤不可满足的圆形配置来实现可靠决策。

Result: 球面神经网络能掌握16种三段论推理任务，包括严格的析取三段论推理，同时保持经典三段论推理的严谨性。

Conclusion: 在三种神经推理方法中，基于显式模型构建的神经推理是最可靠的。

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [148] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: FlashInfer-Bench建立了一个标准化闭环框架，连接AI生成GPU内核的生成、基准测试和部署，通过统一模式、数据集、基准测试框架和动态替换机制，实现AI生成内核在实际LLM推理系统中的集成。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够作为自主代理生成GPU内核，但将这些AI生成的内核集成到实际推理系统中仍然具有挑战性，需要解决标准化、评估和部署的问题。

Method: 1. FlashInfer Trace提供统一模式描述内核定义、工作负载、实现和评估；2. 基于真实服务轨迹构建包含精选数据集、鲁棒基准测试框架；3. 建立公共排行榜跟踪LLM代理的GPU编程能力；4. 开发动态替换机制(apply())，将最佳性能内核无缝注入生产LLM引擎。

Result: 建立了实用的、可复现的框架，能够持续改进AI生成内核并将其部署到大规模LLM推理中，同时评估了LLM代理的性能和局限性，比较了不同GPU编程语言的权衡。

Conclusion: FlashInfer-Bench为AI生成GPU内核的实际应用提供了标准化闭环框架，解决了从生成到部署的完整流程，为未来代理设计和系统集成提供了重要见解。

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [149] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: 提出ClinicalReTrial框架，将临床试验失败预测从被动诊断转变为主动协议重设计，通过闭环优化提升试验成功率


<details>
  <summary>Details</summary>
Motivation: 现有AI方法只能被动预测临床试验失败风险，无法提供可操作的改进方案。临床试验失败是药物开发的主要瓶颈，微小的协议设计缺陷就可能导致失败，即使治疗本身有前景

Method: 提出自进化AI代理框架ClinicalReTrial，将临床试验推理建模为迭代协议重设计问题。整合失败诊断、安全感知修改和候选评估，形成闭环奖励驱动优化框架。使用结果预测模型作为仿真环境，支持低成本评估协议修改，并提供密集奖励信号。采用分层记忆机制捕获试验内迭代反馈并提炼可迁移的重设计模式

Result: ClinicalReTrial改进了83.3%的试验协议，平均成功率提升5.7%。回顾性案例研究表明，发现的重设计策略与实际临床试验修改高度一致

Conclusion: ClinicalReTrial框架成功将临床试验AI从被动风险预测转变为主动协议优化，为药物开发提供了可操作的改进方案，显著提升试验成功率

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [150] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

TL;DR: 论文提出金融群体模型，将流动性博弈与理性群体理论结合，使独立交易者通过差异奖励机制在追求个体利益的同时提升市场整体流动性。


<details>
  <summary>Details</summary>
Motivation: 将群体方法应用于金融市场流动性建模，同时将金融分析技术用于群体分析，有望推动两个研究领域的发展。在群体研究中，使用博弈论方法有望解释观察到的集体效用遵从现象；在金融市场中，理解独立金融代理如何自组织以改善和稳定市场对市场设计研究有重要意义。

Method: 将流动性博弈（交易者收益取决于交易中的总流动性）与理性群体（去中心化代理使用差异奖励将自利学习与全局目标对齐）统一起来。在马尔可夫团队博弈框架中使用差异奖励，构建交易者群体模型，其集体目标是提供市场流动性同时保持代理独立性。

Result: 研究表明，个体流动性最大化行为有助于整体市场流动性，无需协调或共谋。金融群体模型为建模理性独立代理提供了框架，使它们在双边资产市场中既能实现个体盈利又能实现集体市场效率。

Conclusion: 该金融群体模型为研究理性独立代理提供了一个统一框架，展示了如何通过差异奖励机制使自利个体行为自发促进市场整体流动性和稳定性，为金融市场设计和群体研究提供了新的理论视角。

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [151] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt是一个受生物自愈机制启发的智能自愈框架，用于分布式计算连续体系统(DCCS)，通过语言模型驱动的智能体实现自主故障隔离、诊断、自适应恢复和知识积累。


<details>
  <summary>Details</summary>
Motivation: 现代DCCS系统集成了从物联网设备到云基础设施的异构计算资源，其复杂性、移动性和动态运行条件导致频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层的四个阶段：遏制、诊断、元认知和知识。使用语言模型驱动的智能体解释异构日志、推断根本原因、优化推理路径并重新配置资源。

Result: 在公共故障数据集上评估，ReCiSt能在数十秒内实现自愈，智能体CPU使用率最低为10%。结果展示了克服不确定性的分析深度和实现弹性所需的微智能体数量。

Conclusion: ReCiSt框架成功将生物自愈机制转化为计算弹性策略，通过语言模型驱动的智能体实现了DCCS系统的自主故障恢复和知识积累，为复杂分布式系统提供了创新的自愈解决方案。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [152] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: 提出ACCD框架，通过三阶段自适应架构检测社交媒体上的协同虚假行为，显著提升检测精度并减少人工标注需求


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖表面相关性分析、使用静态参数设置、需要大量人工标注，无法有效检测协同虚假行为

Method: 三阶段渐进架构：1) 自适应CCM技术识别账户间真实因果关系；2) 半监督分类结合主动学习减少标注需求；3) 历史经验驱动的自动化验证模块

Result: 在真实数据集上达到87.3%的F1分数，比最强基线提升15.2%；减少68%人工标注需求；处理速度提升2.8倍

Conclusion: ACCD提供了一个更准确、高效、高度自动化的端到端解决方案，具有实际应用价值和广泛推广潜力

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [153] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 将语义空间推理从计算语言学扩展到团队运动战术决策，通过将球员视为词向量、团队配置视为语义结构，在共享向量空间中评估战术匹配度和对手利用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统语义空间推理主要应用于计算语言学，本文探索如何将其扩展到团队运动的战术决策领域。基于"文本-团队"类比（球员如单词，集体比赛传递意义），旨在为团队运动提供可解释、动态适应的战术分析框架。

Method: 将每个球员表示为整合技术、身体和心理属性的多维向量；通过上下文加权将团队配置聚合成高层语义表示。在共享向量空间中编码战术模板（如高位逼抢、反击、控球组织），使用向量距离度量评估战术"匹配度"和对手利用潜力。开发Python原型实现该方法。

Result: 提出的方法能够生成可解释、动态适应的战术策略建议，并提供属性层面的细粒度诊断洞察。该方法不仅适用于足球，还可推广到篮球、冰球等团队运动，以及协作机器人和人机协调系统等集体决策领域。

Conclusion: 语义空间推理为团队战术决策提供了通用框架。未来方向包括：真实数据集成、预测性模拟，以及发展混合人机战术智能系统，进一步优化团队表现和集体决策。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [154] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: MIDAS是一个分布式AI代理系统，通过模拟人类元认知构思流程，生成真正新颖多样的设计创意，将人类设计师从被动筛选者提升为主动协作伙伴。


<details>
  <summary>Details</summary>
Motivation: 当前"单次爆发"式AI系统产生大量语义聚类创意，加剧了新手设计师在生成真正新颖多样创意方面的认知挑战，需要新的AI辅助构思框架。

Method: 提出MIDAS框架，用分布式专业化AI代理团队替代单一AI范式，模拟人类元认知构思流程，逐步细化创意并评估全局新颖性（与现有方案对比）和局部新颖性（与已生成创意对比）。

Result: MIDAS展示了可行且渐进式的人机共创范式，能够生成真正新颖多样的设计创意。

Conclusion: MIDAS为人机真正协同创作提供了可行范式，将人类设计师从被动筛选者转变为参与性、主动的协作伙伴。

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [155] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 研究发现推理模型中的"顿悟时刻"（中期推理转变）很罕见，不会随训练变得更频繁，也很少提高准确性，表明它们并非真正的自我纠正机制，而是推理不稳定的表现。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的"顿悟时刻"，导致准确输出，暗示其具有内在的自我纠正能力。但尚不清楚这种内在推理策略转变是否真正提高性能。

Method: 研究中期推理转变并检测训练过程中的这些转变。分析超过100万条推理轨迹、数百个训练检查点、三个推理领域、多种解码温度和模型架构。

Result: 发现推理转变很罕见，不会随训练变得更频繁，且很少提高准确性，表明它们不符合先前对模型洞察力的认知。但其效果随模型不确定性而变化。基于此发现，在高熵条件下人为触发外在转变能可靠提高准确性。

Conclusion: 中期推理转变是推理不稳定行为的症状，而非内在的自我纠正机制。模型表现出的"顿悟时刻"实际上是推理过程不稳定的表现，而非真正的洞察力提升。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [156] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO提出了一种难度感知的直接偏好优化框架，通过估计偏好数据的难度并重新加权训练样本，解决多模态大语言模型中偏好数据难度不平衡导致的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法由于偏好数据难度不平衡容易过拟合，模型倾向于过度关注容易区分的偏好对，这阻碍了细粒度的幻觉抑制并降低了整体性能。

Method: DA-DPO包含两个主要组件：1) 难度估计：利用预训练的视觉-语言模型，结合生成性和对比性目标，通过分布感知投票策略产生难度分数；2) 难度感知训练：基于估计的难度重新加权偏好对，降低简单样本权重，强调困难样本以缓解过拟合。

Result: 大量实验表明DA-DPO持续改进多模态偏好优化，在标准基准测试中表现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率。

Conclusion: DA-DPO通过难度感知的偏好优化框架，有效解决了多模态DPO中的过拟合问题，无需额外数据或微调阶段，实现了更有效的幻觉抑制和性能提升。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [157] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: PedX-LLM：一个视觉与知识增强的LLM框架，用于行人过街行为推理，通过整合视觉特征、文本数据和交通领域知识，显著提升跨站点泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习）泛化能力有限，在新场景中表现不佳。现有LLM应用缺乏领域特定适应和视觉上下文。需要从站点特定模式识别转向可泛化的行为推理。

Method: 提出PedX-LLM框架：1) 整合LLaVA提取的视觉特征与文本数据；2) 融入交通领域知识；3) 通过LoRA微调LLaMA-2-7B基础模型；4) 支持零样本和少样本学习。

Result: 1) 平衡准确率达82.0%，超越最佳统计和监督学习方法；2) 视觉增强模块贡献2.9%性能提升；3) 领域知识集成带来额外4.1%改进；4) 零样本配置在5个未见测试站点达到66.9%平衡准确率，比基线方法至少高18个百分点；5) 少样本学习（仅5个验证样本）将准确率提升至72.2%。

Conclusion: PedX-LLM通过视觉和知识增强的推理，能够模拟人类决策逻辑，克服纯数据驱动方法的局限性，在未见场景中展现出强大的泛化能力，为行人过街行为推断提供了新的范式。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [158] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 使用智能体工作流将自然语言任务描述自动转换为完整的 DomiKnowS 程序，大幅降低神经符号编程的门槛和开发时间。


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高鲁棒性、可解释性和数据效率，但现有框架如 DomiKnowS 仍要求用户熟悉特定语法，开发过程耗时且具有挑战性。

Method: ADS 采用智能体工作流，将自由形式的任务描述翻译成完整的 DomiKnowS 程序，通过创建和单独测试每个 DomiKnowS 组件，并支持可选的人工干预来优化中间输出。

Result: ADS 使熟悉和不熟悉 DomiKnowS 的用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟。

Conclusion: ADS 通过消除对特定库语法的依赖，显著降低了神经符号编程的门槛，使更多用户能够利用符号约束增强深度学习模型。

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>
