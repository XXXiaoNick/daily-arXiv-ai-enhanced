<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [stat.ML](#stat.ML) [Total: 14]
- [math.OC](#math.OC) [Total: 27]
- [cs.AI](#cs.AI) [Total: 71]
- [cs.LG](#cs.LG) [Total: 170]
- [cs.CY](#cs.CY) [Total: 9]
- [eess.SY](#eess.SY) [Total: 14]
- [econ.EM](#econ.EM) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)
*Deepak Gupta,Davis Bartels,Dina Demner-Fuhsman*

Main category: cs.CL

TL;DR: BioACE：一个用于评估生物医学问答生成中答案质量和引用支持的自动化框架，通过完整性、正确性、精确度和召回率等多维度指标，结合NLI和预训练模型方法，提供最佳评估方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在生物医学问答中的广泛应用，评估生成答案的质量和引用支持变得至关重要。然而，由于需要专家验证与科学文献的一致性以及处理复杂的医学术语，生物医学领域的文本生成评估仍然面临挑战。

Method: 提出BioACE框架，从完整性、正确性、精确度和召回率等多个维度评估答案质量，基于ground-truth nuggets进行评价。同时探索了自然语言推理（NLI）、预训练语言模型和LLMs等多种现有方法来评估引用支持的质量。

Result: 通过大量实验评估了各种方法的性能，并分析了它们与人工评估的相关性。确定了生物医学答案和引用评估的最佳方法，并将其集成到BioACE评估包中。

Conclusion: BioACE提供了一个全面的自动化框架，用于评估生物医学问答生成中的答案质量和引用支持，解决了该领域评估的挑战，并为研究人员提供了实用的评估工具包。

Abstract: With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.

</details>


### [2] [CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System](https://arxiv.org/abs/2602.05004)
*Zexin Lin,Jiachen Yu,Haoyang Zhang,Yuzhao Li,Zhonghang Li,Yujiu Yang,Junjie Wang,Xiaoqiang Ji*

Main category: cs.CL

TL;DR: CoWork-X：一种用于实时协作任务的主动协同进化框架，通过结构化技能库和预算约束的补丁式技能整合，在减少延迟和令牌使用的同时实现稳定的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型支持交互环境中的语言条件智能体，但高度协作任务面临两个约束：亚秒级实时协调和严格在线令牌预算下的持续多回合适应。现有方法要么依赖频繁的回合内推理导致延迟和时序抖动，要么通过难以编译成可靠低成本执行的非结构化文本来提供回合后改进。

Method: 提出CoWork-X框架，将同伴协作建模为跨回合的闭环优化问题。包含：1) 技能智能体：通过HTN（分层任务网络）从结构化、可解释、可组合的技能库中检索技能执行；2) 回合后协同优化器：在明确预算约束和漂移正则化下执行补丁式技能整合。

Result: 在具有挑战性的Overcooked-AI类实时协作基准测试中，CoWork-X实现了稳定、累积的性能提升，同时持续降低在线延迟和令牌使用。

Conclusion: CoWork-X通过结构化技能库和预算约束的协同优化，有效解决了实时协作任务中的延迟和令牌预算问题，为语言条件智能体在严格约束下的持续适应提供了可行方案。

Abstract: Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.

</details>


### [3] [Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation](https://arxiv.org/abs/2602.05035)
*Sean Trott,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 多语言语言模型在词汇消歧任务上表现不如单语言模型，研究发现这源于三种容量限制：表征、注意力和词汇相关限制


<details>
  <summary>Details</summary>
Motivation: 多语言语言模型有时表现不如单语言模型，可能是由于容量限制。研究旨在量化这种"多语言惩罚"，特别是在需要精确语义表示和上下文机制的词汇消歧任务上

Method: 使用英语和西班牙语中歧义词的人类相关性判断数据集，比较同一模型家族的单语言和多语言模型。探索三种潜在容量约束：表征限制（嵌入各向同性降低）、注意力限制（对消歧线索的关注减少）和词汇相关限制（多标记分割增加）

Result: 多语言模型在所有比较中都表现较差，显示出所有三种容量限制的证据。这些因素在统计上解释了原本归因于模型多语言状态的方差，表明容量限制与消歧性能下降相关

Conclusion: 多语言语言模型确实受到多种容量限制，这些限制与消歧性能降低相关。研究为理解多语言模型的局限性提供了具体证据，并指出了改进方向

Abstract: Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.

</details>


### [4] [Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories](https://arxiv.org/abs/2602.05085)
*Sidi Lu,Zhenwen Liang,Dongyang Ma,Yan Wang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: Locas是一种局部支持的参数化记忆模块，可灵活卸载或合并到模型参数中，支持高效持续学习，减少灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 将测试时训练与新型参数化记忆相结合，这种记忆可以灵活地从模型参数中卸载或合并，以支持高效的持续学习

Method: 提出Locas（局部支持参数化记忆），共享现代transformer中FFN块的设计，可永久化到模型参数中。提供两种变体：传统两层MLP设计（理论保证更清晰）和与SOTA LLMs相同的GLU-FFN结构（易于附加到现有模型）。关键是通过重用模型参数、激活和/或梯度进行适当初始化

Result: 在PG-19整书语言建模和LoCoMo长上下文对话问答任务上验证。Locas-GLU仅需0.02%额外参数即可存储过去上下文信息，同时保持较小上下文窗口。MMLU评估显示Locas能够将过去上下文永久化为参数知识，同时最小化模型现有内部知识的灾难性遗忘

Conclusion: Locas展示了将过去上下文永久化为参数知识的潜力，同时最小化灾难性遗忘，为参数高效和计算高效的持续学习提供了有前景的解决方案

Abstract: In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.

</details>


### [5] [Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models](https://arxiv.org/abs/2602.05106)
*Michael Browder,Kevin Duh,J. David Harris,Vince Lyzinski,Paul McNamee,Youngser Park,Carey E. Priebe,Peter Viechnicki*

Main category: cs.CL

TL;DR: 本文提出数据核视角空间（DKPS）为Transformer模型输出质量提供数学分析和统计保证，解决合成数据生成中的不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 标记训练数据的稀缺是构建高性能语言技术和生成AI模型的主要瓶颈。虽然Transformer模型（特别是LLMs）被用于缓解数据稀缺问题，但由于这些模型是黑盒，合成数据的属性难以预测，工程师通常只能盲目调整参数，缺乏理论保证。

Method: 提出数据核视角空间（DKPS）作为数学分析基础，通过数学推导建立DKPS框架，为Transformer模型输出质量提供具体的统计性能保证，并将这些保证应用于下游任务分析。

Result: DKPS能够为Transformer模型的输出质量提供数学保证，这些保证可以阐明下游任务（如神经机器翻译模型或使用对比偏好优化的LLMs）的性能表现。

Conclusion: DKPS为解决Transformer模型合成数据生成中的不确定性提供了理论基础和统计保证，但当前工作存在局限性，需要未来进一步研究。

Abstract: Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.

</details>


### [6] [Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text](https://arxiv.org/abs/2602.05107)
*Ahmed Ruby,Christian Hardmeier,Sara Stymne*

Main category: cs.CL

TL;DR: 提出一种自动构建多语言多模态隐式篇章关系数据集的方法，并开发结合文本和音频信息的跨语言分类模型，发现多模态融合能提升性能，跨语言迁移对低资源语言有益。


<details>
  <summary>Details</summary>
Motivation: 隐式篇章关系分类具有挑战性，需要从上下文中推断含义。上下文线索可能分布在多模态中，且在不同语言间存在差异，仅靠文本无法完全捕捉这些信息。因此需要构建多语言多模态数据集并开发相应的分类方法。

Method: 1) 提出自动方法为英语、法语和西班牙语构建多语言多模态隐式篇章关系数据集；2) 提出多模态分类方法，通过Qwen2-Audio整合文本和音频信息，实现跨语言的隐式篇章关系联合建模。

Result: 文本模型优于音频模型，但整合两种模态能提升性能。跨语言迁移能为低资源语言带来显著改进。

Conclusion: 多模态方法能有效提升隐式篇章关系分类性能，跨语言迁移是解决低资源语言问题的有效策略。

Abstract: Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.

</details>


### [7] [GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek](https://arxiv.org/abs/2602.05150)
*Yang Zhang,Mersin Konomi,Christos Xypolopoulos,Konstantinos Divriotis,Konstantinos Skianis,Giannis Nikolentzos,Giorgos Stamou,Guokan Shang,Michalis Vazirgiannis*

Main category: cs.CL

TL;DR: 希腊MMLU：首个基于希腊语原生内容的LLM评估基准，包含21,805道选择题，覆盖45个学科领域，用于评估希腊语语言理解能力


<details>
  <summary>Details</summary>
Motivation: 当前希腊语评估基准有限，大多是从英语机器翻译而来，无法捕捉希腊语的语言和文化特征，需要建立基于希腊语原生内容的可靠评估基准

Method: 创建希腊MMLU基准，包含21,805道选择题，涵盖45个学科领域，基于新定义的主题分类法，按教育难度（从小学到专业考试）标注，所有问题均来自希腊语学术、专业和政府考试

Result: 评估80多个开源和闭源LLM，发现前沿模型与开源模型之间存在显著性能差距，希腊语适应模型与通用多语言模型之间也存在差距，并分析了模型规模、适应性和提示等因素对性能的影响

Conclusion: 希腊MMLU为希腊语LLM评估提供了可靠基准，揭示了当前模型的局限性，为提升希腊语LLM能力提供了系统分析框架，公开发布16,857个样本，保留4,948个样本用于防污染评估

Abstract: Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.

</details>


### [8] [Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems](https://arxiv.org/abs/2602.05176)
*Ziyuan Yang,Wenxuan Ding,Shangbin Feng,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 该论文研究多语言模型协作系统中的安全风险，量化恶意模型对系统性能的影响，并提出基于外部监督的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在分布式协作中的广泛应用，存在安全风险：如果多LLM系统中的某些模型被攻击或恶意，会对整个系统造成威胁。目前缺乏对这种去中心化范式下安全风险的系统性研究。

Method: 1) 设计四类恶意语言模型；2) 将其集成到四种流行的模型协作系统中；3) 在10个数据集上评估受损系统性能；4) 提出基于外部监督的缓解策略，通过监督模型协作来禁用/屏蔽恶意组件。

Result: 恶意模型对多LLM系统有严重影响，特别是在推理和安全领域，性能平均下降7.12%和7.94%。提出的缓解策略平均能恢复95.31%的初始性能，但完全抵抗恶意模型仍是开放研究问题。

Conclusion: 多LLM协作系统面临严重的安全威胁，恶意组件会显著降低系统性能。虽然外部监督策略能有效缓解影响，但实现完全安全的模型协作仍需进一步研究。

Abstract: Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.

</details>


### [9] [The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems](https://arxiv.org/abs/2602.05182)
*Shangbin Feng,Kishan Panaganti,Yulia Tsvetkov,Wenhao Yu*

Main category: cs.CL

TL;DR: 通过将多模型协作模式蒸馏到单个模型中，实现协作优势与单模型效率的平衡，并提出单-多进化循环机制让模型在协作与蒸馏中持续进化


<details>
  <summary>Details</summary>
Motivation: 多语言模型协作系统结合了不同模型的优势，但需要加载多个模型导致成本高昂。需要找到既能保持协作优势又能提高效率的方法。

Method: 1) 协作模式蒸馏：将多模型协作系统的输出作为训练数据，蒸馏到单个模型中；2) 单-多进化循环：多个模型协作 → 各自从协作输出中蒸馏 → 蒸馏改进后的模型再次协作，形成集体进化生态系统。

Result: 1) 单个模型平均提升8.0%，吸收了协作优势同时成本降至单模型水平；2) 协作系统从蒸馏后更强、更协同的模型中获益，相比初始系统平均提升14.9%；3) 在7种协作策略和15个任务（QA、推理、事实性等）上验证有效。

Conclusion: 协作蒸馏方法成功平衡了效率与性能，单-多进化循环机制超越了现有进化AI方法，兼容多种模型/协作/蒸馏设置，能解决初始模型/系统难以处理的问题。

Abstract: Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.

</details>


### [10] [Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky](https://arxiv.org/abs/2602.05189)
*Hsuan-Yu Chou,Wajiha Naveed,Shuyan Zhou,Xiaowei Yang*

Main category: cs.CL

TL;DR: 开源大语言模型在有害内容检测任务上可以达到与专有模型相当的性能，支持在消费级硬件上进行隐私保护的审核工作。


<details>
  <summary>Details</summary>
Motivation: 随着互联网访问扩大，有害内容暴露增加，需要有效的审核机制。虽然专有LLMs在社交媒体审核任务中表现出色，但开源LLMs的即用能力仍不明确，特别是在隐私保护和个性化审核方面的潜力。

Method: 评估了7个最先进的LLMs（4个专有模型和3个开源模型），使用Bluesky的真实帖子、Bluesky审核服务的决策以及两位作者的标注进行测试，分析了模型的敏感性和特异性。

Result: 开源LLMs的敏感性（81%-97%）和特异性（91%-100%）与专有模型（72%-98%和93%-99%）有相当程度的重叠。在粗鲁内容检测中特异性高于敏感性，而在不容忍和威胁检测中则相反。发现了人类审核员与LLMs之间的评分者一致性。

Conclusion: 开源LLMs能够在消费级硬件上支持隐私保护的审核工作，为设计平衡社区价值观与个人用户偏好的审核系统提供了新方向。

Abstract: As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.
  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.

</details>


### [11] [Aligning Large Language Model Behavior with Human Citation Preferences](https://arxiv.org/abs/2602.05205)
*Kenichiro Ando,Tatsuya Harada*

Main category: cs.CL

TL;DR: 研究发现LLM引用行为与人类偏好存在显著差异：模型过度引用明确标记需要引用的文本，但对数字和包含人名的句子引用不足，可通过偏好优化校准模型行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务普遍添加引用以增强可信度，但LLM如何识别值得引用的内容以及如何控制这一过程仍缺乏深入研究。本研究旨在探索LLM当前的引用倾向及其与人类偏好的对齐程度。

Method: 构建数据集表征人类引用偏好与LLM行为的关系，将网络文本分为8种引用动机类型，对所有类型组合进行成对引用偏好评估以捕捉细粒度对比。

Result: 人类最常为医学文本寻求引用，更强模型显示类似倾向；当前模型比人类多27%可能为明确标记需要引用的文本添加引用，但对数字句子引用不足22.6%，对含人名句子引用不足20.1%；直接偏好优化实验表明模型行为可校准以更好匹配人类偏好。

Conclusion: LLM引用行为与人类偏好存在系统性差异，模型过度强调某些引用类型而忽视其他重要类型，但可通过偏好优化进行校准，为更细粒度研究LLM引用偏好奠定基础。

Abstract: Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\%$ relative to humans) and sentences containing personal names (by $-20.1\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.

</details>


### [12] [Quantifying the Knowledge Proximity Between Academic and Industry Research: An Entity and Semantic Perspective](https://arxiv.org/abs/2602.05211)
*Hongye Zhao,Yi Zhao,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 该研究通过细粒度知识实体和语义空间量化了学术界与产业界的共同演化轨迹，发现两者知识邻近性在技术变革后上升，且学术界在技术范式转变期间知识主导地位减弱。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖合作论文或专利数量等宏观指标，缺乏对文献中知识单元的分析，导致对学术界与产业界之间细粒度知识邻近性的把握不足，可能影响合作框架和资源分配效率。

Method: 1) 实体测量：使用预训练模型提取细粒度知识实体，通过余弦相似度测量序列重叠，通过复杂网络分析拓扑特征；2) 语义层面：使用无监督对比学习量化语义空间收敛，测量跨机构文本相似性；3) 使用引用分布模式分析双向知识流与相似性的相关性。

Result: 分析显示学术界与产业界之间的知识邻近性上升，特别是在技术变革之后；同时发现学术界在技术范式转变期间的知识主导地位减弱，这为共同演化中的双向适应提供了文本证据。

Conclusion: 该研究通过细粒度实体和语义分析方法，揭示了学术界与产业界知识邻近性的动态演变模式，特别是在技术变革时期，为理解两者的共同演化机制提供了新的分析框架和实证证据。

Abstract: The academia and industry are characterized by a reciprocal shaping and dynamic feedback mechanism. Despite distinct institutional logics, they have adapted closely in collaborative publishing and talent mobility, demonstrating tension between institutional divergence and intensive collaboration. Existing studies on their knowledge proximity mainly rely on macro indicators such as the number of collaborative papers or patents, lacking an analysis of knowledge units in the literature. This has led to an insufficient grasp of fine-grained knowledge proximity between industry and academia, potentially undermining collaboration frameworks and resource allocation efficiency. To remedy the limitation, this study quantifies the trajectory of academia-industry co-evolution through fine-grained entities and semantic space. In the entity measurement part, we extract fine-grained knowledge entities via pre-trained models, measure sequence overlaps using cosine similarity, and analyze topological features through complex network analysis. At the semantic level, we employ unsupervised contrastive learning to quantify convergence in semantic spaces by measuring cross-institutional textual similarities. Finally, we use citation distribution patterns to examine correlations between bidirectional knowledge flows and similarity. Analysis reveals that knowledge proximity between academia and industry rises, particularly following technological change. This provides textual evidence of bidirectional adaptation in co-evolution. Additionally, academia's knowledge dominance weakens during technological paradigm shifts. The dataset and code for this paper can be accessed at https://github.com/tinierZhao/Academic-Industrial-associations.

</details>


### [13] [Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions](https://arxiv.org/abs/2602.05220)
*Jinchuan Tian,Haoran Wang,Bo-Hao Su,Chien-yu Huang,Qingzheng Wang,Jiatong Shi,William Chen,Xun Gong,Siddhant Arora,Chin-Jou Li,Masao Someki,Takashi Maekaku,Yusuke Shinohara,Jin Sakuma,Chao-Han Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: Bagpiper是一个80亿参数的音频基础模型，通过丰富的自然语言描述来理解物理音频信号，实现音频理解与生成的统一，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有音频基础模型通常依赖刚性的任务特定监督，只能处理音频的孤立因素而非整体。人类智能则能整体处理音频，无缝连接物理信号与抽象认知概念以执行复杂任务。因此需要开发能够模拟人类认知过程的音频模型。

Method: 1. 使用600B token的大规模语料库进行预训练，建立原始音频与高级概念空间之间的双向映射；2. 采用"描述-处理"的工作流程，模拟中间认知推理步骤；3. 通过丰富的自然语言描述（包含转录、音频事件等关键认知概念）来解释物理音频。

Result: 1. 在MMAU和AIRBench音频理解基准上超越Qwen-2.5-Omni；2. 在生成质量上超过CosyVoice3和TangoFlux；3. 能够合成语音、音乐和音效的任意组合；4. 实现了通用音频的统一理解与生成。

Conclusion: Bagpiper是首批实现通用音频统一理解与生成的工作之一，通过模拟人类认知过程，在音频理解和生成任务上都取得了优异表现。模型、数据和代码已公开。

Abstract: Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.

</details>


### [14] [FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters](https://arxiv.org/abs/2602.05235)
*Zhilin Liang,Yuxiang Wang,Zimu Zhou,Hainan Zhang,Boyi Liu,Yongxin Tong*

Main category: cs.CL

TL;DR: FedMosaic：首个基于参数化适配器的联邦RAG框架，通过聚类语义相关文档到多文档适配器并选择性聚合，在保护隐私的同时提升准确率并大幅降低存储和通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统RAG假设集中式知识库，但在隐私敏感领域知识分散在隔离的数据孤岛中。联邦RAG需要在中央LLM服务器与分布式数据孤岛协作时不共享原始文档，而现有方法存在存储通信开销大和聚合冲突的问题。

Method: 采用参数化RAG方法，将文档编码为轻量级适配器。提出FedMosaic框架：1）将语义相关文档聚类到多文档适配器中，使用文档特定掩码减少开销同时保持特异性；2）选择性聚合适配器，只合并相关性对齐且无冲突的适配器。

Result: 在四类任务中平均准确率比现有最优方法高10.9%，存储成本降低78.8%-86.3%，通信成本降低91.4%，且从不共享原始文档。

Conclusion: FedMosaic成功解决了联邦RAG中的存储通信开销和聚合冲突问题，在保护隐私的同时实现了高性能的检索增强生成，为隐私敏感领域的知识应用提供了可行方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.

</details>


### [15] [Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks](https://arxiv.org/abs/2602.05252)
*Guangwei Zhang,Jianing Zhu,Cheng Qian,Neil Gong,Rada Mihalcea,Zhaozhuo Xu,Jingrui He,Jiaqi Ma,Yun Huang,Chaowei Xiao,Bo Li,Ahmed Abbasi,Dongwon Lee,Heng Ji,Denghui Zhang*

Main category: cs.CL

TL;DR: 首个交互式法证系统，用于检测、分析和可视化LLM输出中的潜在版权风险，将侵权检测视为证据发现过程而非静态分类任务。


<details>
  <summary>Details</summary>
Motivation: 由于版权法的复杂性，需要一种系统化的方法来检测LLM输出中的版权风险，支持负责任部署和透明评估，即使面对黑盒访问。

Method: 整合多种检测范式：内容召回测试、改写级相似性分析、说服性越狱探测和遗忘验证，采用交互式提示、响应收集和迭代工作流程的统一可扩展框架。

Result: 开发了首个交互式法证系统，能够系统化审计逐字记忆和改写级泄露，支持对LLM版权风险进行透明评估。

Conclusion: Copyright Detective为LLM版权风险检测提供了创新框架，将侵权检测重新定义为证据发现过程，支持负责任部署和透明评估。

Abstract: We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.

</details>


### [16] [CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs](https://arxiv.org/abs/2602.05258)
*Haoran Li,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CL

TL;DR: CoPE通过软截断RoPE的低频分量，统一了OOD缓解和语义建模两个目标，在长上下文扩展中取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有的RoPE扩展方法主要分为两类：OOD缓解（适应未见位置）和语义建模（关注语义相似token）。本文旨在统一这两个看似不同的目标，通过简单干预提升长上下文扩展性能。

Method: 提出CoPE方法：对RoPE的低频分量进行软截断。这种方法不仅能消除OOD异常值、优化语义信号，还能避免硬截断引起的频谱泄漏。

Result: 实验表明，简单应用CoPE软截断策略能在RoPE上带来显著的性能提升，可扩展到256k上下文长度，验证了理论分析并建立了新的SOTA长度泛化方法。

Conclusion: CoPE通过软截断RoPE低频分量的简约干预，成功统一了OOD缓解和语义建模两个目标，成为长上下文扩展的新SOTA方法。

Abstract: Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.

</details>


### [17] [Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR](https://arxiv.org/abs/2602.05261)
*Fanfan Liu,Youyang Yin,Peng Shi,Siqi Yang,Zhixiong Zeng,Haibo Qiu*

Main category: cs.CL

TL;DR: 本文分析了强化学习可验证奖励（RLVR）算法中响应长度变化的原因，提出了长度无偏序列策略优化（LUSPO）算法，解决了GSPO中的长度偏差问题，在数学推理和多模态推理任务中取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: RLVR训练中响应长度的增加常被视为推理能力提升的关键因素，但不同RLVR算法训练过程中响应长度变化模式差异显著，需要从理论上解释这些差异的根本原因。

Method: 深入分析主流RLVR算法的组件，从理论上分析影响响应长度的因素，并通过大量实验验证理论。基于理论发现提出LUSPO算法，修正GSPO中的长度偏差，使其损失函数对响应长度无偏，解决响应长度崩溃问题。

Result: 在数学推理基准测试和多模态推理场景中进行广泛实验，LUSPO始终取得优越性能。实证结果表明LUSPO相比现有方法（如GRPO和GSPO）代表了新颖的最先进优化策略。

Conclusion: LUSPO算法通过解决RLVR训练中的长度偏差问题，实现了更稳定和优越的性能，为复杂推理任务的优化提供了新的有效策略。

Abstract: Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.

</details>


### [18] [Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science](https://arxiv.org/abs/2602.05289)
*Jingru Fan,Dewen Liu,Yufan Dang,Huatao Li,Yuheng Wang,Wei Liu,Feiyu Duan,Xuanwen Ding,Shu Yao,Lin Wu,Ruijie Shi,Wai-Shing Leung,Yuan Cheng,Zhongyu Wei,Cheng Yang,Chen Qian,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 论文提出一个集成框架，将多智能体系统研究从经验试错转向设计科学，通过建立协作增益指标Γ和系统化因子库来实现科学化优化。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的多智能体系统在复杂领域取得显著进展，但当前研究仍依赖经验试错，缺乏统一的科学框架进行系统优化。主要瓶颈在于归因模糊：缺乏结构化因子分类导致调整无方向；缺乏统一指标无法区分真实协作增益与单纯资源积累。

Method: 提出集成框架：1）建立协作增益指标Γ作为科学标准，分离内在增益与预算增加；2）提出因子归因范式系统识别协作驱动因素；3）构建系统化MAS因子库，将设计空间结构化分为控制级预设和信息级动态。

Result: 框架为多智能体系统研究提供了从盲目实验到严谨科学的转型路径，建立了科学化的优化方法，为实现真正的集体AI科学铺平道路。

Conclusion: 通过协作增益指标Γ和系统化因子库的集成框架，能够将多智能体系统研究从经验试错转向设计科学，实现系统化优化，推动集体AI科学的发展。

Abstract: Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($Γ$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $Γ$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.

</details>


### [19] [MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning](https://arxiv.org/abs/2602.05307)
*Haojin Wang,Yike Wang,Shangbin Feng,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: MentorCollab：一种推理时协作方法，让大型推理模型稀疏地指导小型模型，通过轻量验证器选择性地让小型模型跟随导师的短前瞻片段，在保持高性能的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能强但推理成本高且常产生冗余推理，小型模型效率高但在多步推理任务上表现不佳。现有协作方法往往导致模仿而非真正的错误修正，需要一种既能利用大型模型指导能力又不显著增加推理开销的方法。

Method: 提出MentorCollab方法：在随机采样的token位置，探测两个模型之间的分歧，使用轻量验证器决定小型模型是跟随导师模型的短前瞻片段还是继续自主生成。通过选择性、稀疏的指导而非接管生成来实现有效协作。

Result: 在15个SLM-LRM组合和3个领域（数学推理、通用知识、常识推理）中，12个设置下性能提升，平均提升3.0%，最高达8.0%，同时平均只有18.4%的token由昂贵的导师模型生成。短片段和选择性探测足以实现有效协作。

Conclusion: 选择性推理时指导能够恢复大型模型的推理能力而不带来显著的推理开销，为高效的大-小模型协作提供了有效方案。

Abstract: Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.

</details>


### [20] [How Do Language Models Acquire Character-Level Information?](https://arxiv.org/abs/2602.05347)
*Soma Sato,Ryohei Sasano*

Main category: cs.CL

TL;DR: 研究发现语言模型通过分词规则和语义关联等机制隐式获取字符级知识


<details>
  <summary>Details</summary>
Motivation: 语言模型被报告能够隐式编码字符级信息，但这种现象背后的机制尚未得到充分探索。为了揭示这些机制，需要分析模型如何获取字符级知识。

Method: 通过比较在受控设置下训练的语言模型（如指定预训练数据集或分词器）与标准设置下训练的模型，分析字符级知识的获取机制。将影响因素分为与分词相关和与分词无关两类。

Result: 分析发现合并规则和正字法约束是分词相关的主要因素，而子字符串的语义关联和句法信息是与分词无关的关键因素。

Conclusion: 语言模型通过分词相关机制（合并规则、正字法约束）和分词无关机制（语义关联、句法信息）隐式获取字符级知识，揭示了字符级信息编码的多重机制。

Abstract: Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.

</details>


### [21] [PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning](https://arxiv.org/abs/2602.05370)
*Jun Rao,Zixiong Yu,Xuebo Liu,Guhan Chen,Jing Li,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.CL

TL;DR: PACE方法通过生成式纠正策略替代暴力采样，在数学推理任务中仅用1/5计算量就超越了标准DPO-R1（N=16）的性能


<details>
  <summary>Details</summary>
Motivation: 挑战迭代直接偏好优化（DPO-R1）中的"扩大采样规模"假设，发现数学推理中过度探索会导致收益递减甚至策略崩溃，需要更高效的探索策略

Method: 提出PACE（Proximal Alignment via Corrective Exploration），用生成式纠正策略替代暴力采样，仅需极小预算（2<N<3），从失败探索中合成高质量偏好对

Result: PACE在数学推理任务中超越DPO-R1（N=16），仅用约1/5计算量，对奖励攻击和标签噪声表现出更强的鲁棒性

Conclusion: 在数学推理对齐中，质量优于数量，PACE通过智能纠正策略实现了更高效、更鲁棒的模型对齐

Abstract: Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.

</details>


### [22] [Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks](https://arxiv.org/abs/2602.05374)
*Chaimae Abouzahir,Congbo Ma,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 研究发现LLMs在阿拉伯语和英语医疗问答中存在语言驱动的性能差距，且随任务复杂度增加而加剧，需要语言感知的设计和评估策略。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在医疗应用中多为英语中心，对语言多样性社区的可信性和鲁棒性有限。虽然已有研究指出低资源语言在医疗任务中的性能差异，但根本原因尚不清楚。

Method: 对阿拉伯语和英语医疗问答进行跨语言实证分析，包括性能差距分析、阿拉伯语医疗文本的分词结构分析，以及模型报告置信度和解释与正确性的相关性分析。

Result: 发现持续存在的语言驱动性能差距，且随任务复杂度增加而加剧；阿拉伯语医疗文本存在结构碎片化；模型报告的置信度和解释与正确性相关性有限。

Conclusion: 强调在医疗任务LLMs中需要语言感知的设计和评估策略，以解决跨语言性能差距问题。

Abstract: In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.

</details>


### [23] [IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models](https://arxiv.org/abs/2602.05385)
*Tao Liu,Jiafan Lu,Bohan Yu,Pengcheng Wu,Liu Haixin,Guoyu Xu,Li Xiangheng,Lixiao Li,Jiaming Hou,Zhao Shijun,Xinglin Lyu,Kunli Zhang,Yuxiang Jia,Hongyin Zan*

Main category: cs.CL

TL;DR: IESR框架通过信息增强结构化推理，结合MCTS多路径推理和轨迹一致性验证，在轻量级LLMs上实现复杂Text-to-SQL任务的最优性能


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法在复杂推理、领域知识和假设性查询方面表现不佳，且企业部署成本高昂，需要轻量级解决方案

Method: 提出IESR框架：1) 利用LLMs进行关键信息理解和模式链接，解耦数学计算与SQL生成；2) 基于MCTS的多路径推理机制与多数投票；3) 轨迹一致性验证模块确保准确性

Result: 在LogicCat基准上达到24.28 EX，Archer数据集上达到37.28 EX，使用轻量级模型无需微调即实现SOTA性能

Conclusion: IESR证明了轻量级模型通过结构化推理机制可有效处理复杂Text-to-SQL任务，同时揭示了当前编码器模型在物理知识、数学计算和常识推理方面的偏差与不足

Abstract: Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.

</details>


### [24] [Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances](https://arxiv.org/abs/2602.05392)
*Jiyun Chun,Eric Fosler-Lussier,Michael White,Andrew Perrault*

Main category: cs.CL

TL;DR: 提出基于LLM的评估框架，从扩展性和独立性两个维度评估儿童话语质量，超越传统长度指标，考虑对话上下文和语言发展维度。


<details>
  <summary>Details</summary>
Motivation: 现有儿童话语质量评估指标（如平均话语长度、词汇多样性、可读性指数）过于依赖长度，忽略对话上下文，无法捕捉推理深度、话题维持和话语规划等重要质量维度。

Method: 采用LLM-as-a-judge框架，首先分类成人前话语类型，然后从两个维度评分儿童回应：扩展性（上下文阐述和推理深度）和独立性（儿童对推进话语的贡献）。

Result: 验证了发展有效性（显示与年龄相关的模式），提高了年龄估计的预测价值，检测到与话语关系相关的语义差异，且与人类判断一致。

Conclusion: 该框架将儿童话语评估从简单测量长度转向评估儿童言语如何在上下文中有意义地贡献和推进对话，实现大规模评估。

Abstract: Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.

</details>


### [25] [Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better](https://arxiv.org/abs/2602.05393)
*Ji Zhao,Yufei Gu,Shitong Shao,Xun Zhou,Liang Xiang,Zeke Xie*

Main category: cs.CL

TL;DR: 提出Late-to-Early Training (LET)方法，利用小型预训练模型的后期知识来加速大型语言模型的训练，实现1.6倍加速和5%下游任务精度提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，预训练变得计算昂贵且耗时。虽然已有许多预训练模型，但如何利用现有小型预训练模型来加速大型模型训练的问题尚未充分探索。

Method: 提出Late-to-Early Training (LET)范式，让大语言模型在早期训练步骤和早期层中学习预训练模型的后期知识。核心思想是使用预训练模型后期层的表示来指导目标模型早期层的训练。

Result: 在1.4B和7B参数模型上的实验表明，LET能显著加速训练收敛，同时提升语言建模能力和下游任务性能。在Pile数据集上训练1.4B模型时，实现1.6倍加速，下游任务精度提升近5%，即使使用的预训练模型参数只有目标模型的1/10。

Conclusion: LET方法通过利用小型预训练模型的后期知识来加速大型模型训练，提供了一种高效的大语言模型训练范式，在计算效率和性能上都取得了显著改进。

Abstract: As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\times$ speedup with nearly 5\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\times$ fewer parameters than the target model.

</details>


### [26] [OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration](https://arxiv.org/abs/2602.05400)
*Shaobo Wang,Xuan Ouyang,Tianyi Xu,Yuzheng Hu,Jialin Liu,Guo Chen,Tianyu Zhang,Junhao Zheng,Kexin Yang,Xingzhang Ren,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: OPUS提出基于优化器诱导更新空间的数据选择框架，通过投影有效更新到目标方向来动态选择高质量数据，显著提升预训练效率和数据利用率。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公共文本数据接近枯竭（数据墙现象），预训练正从更多token转向更好token。现有方法要么依赖忽略训练动态的启发式静态过滤器，要么使用基于原始梯度的动态但优化器无关的标准。

Method: OPUS在优化器诱导的更新空间中定义效用，通过将候选数据的有效更新（由现代优化器塑造）投影到来自稳定、同分布代理的目标方向上，对候选数据进行评分。采用Ghost技术和CountSketch确保计算效率，Boltzmann采样保证数据多样性，仅增加4.7%的计算开销。

Result: 在GPT-2 Large/XL的FineWeb和FineWeb-Edu 30B token预训练中，OPUS超越工业级基线，甚至优于完整的200B token训练。结合工业级静态过滤器后，即使使用低质量数据也能进一步提升预训练效率。在Qwen3-8B-Base的SciencePedia持续预训练中，仅用0.5B token就达到优于完整3B token训练的性能。

Conclusion: OPUS是一个高效、可扩展的动态数据选择框架，通过优化器感知的效用定义显著提升预训练效率，在通用和专用领域都展现出卓越的数据效率增益。

Abstract: As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.

</details>


### [27] [Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation](https://arxiv.org/abs/2602.05419)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 提出UOT-ERRANT，一种基于编辑向量和不平衡最优传输的GEC自动评估指标，专注于编辑相似性而非完整句子相似性


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入的相似性度量（如BERTScore）在GEC评估中效果不佳，因为源句中的许多词在假设和参考中保持不变。需要专门针对GEC编辑设计的评估方法

Method: 提出编辑向量表示编辑操作，使用ERRANT提取编辑，通过不平衡最优传输（UOT）将假设编辑向量传输到参考编辑向量，计算相似度

Result: 在SEEDA元评估中，UOT-ERRANT提升了评估性能，特别是在+Fluency领域（编辑较多）。方法具有高可解释性，传输计划可视为软编辑对齐

Conclusion: UOT-ERRANT是一种有效的GEC评估指标，既可用于系统排名，也可用于分析GEC系统，代码已开源

Abstract: Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.

</details>


### [28] [Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models](https://arxiv.org/abs/2602.05437)
*Basel Mousi,Fahim Dalvi,Shammur Chowdhury,Firoj Alam,Nadir Durrani*

Main category: cs.CL

TL;DR: 论文提出了M2CQA基准，用于评估视觉语言模型在跨文化背景下的反事实幻觉问题，特别关注中东和北非地区，并引入了CFHR指标来衡量模型在正确回答真实陈述后接受反事实陈述的倾向。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉基准很少测试视觉语言模型在文化上合理但视觉上错误的解释，特别是在非西方语境和非英语环境中。需要专门针对中东和北非地区文化背景的基准来评估这种失败模式。

Method: 构建了M2CQA基准，包含来自17个中东和北非国家的图像，配以英语、阿拉伯语及其方言的真实陈述和反事实陈述。提出了反事实幻觉率（CFHR）指标，衡量模型在正确回答真实陈述后接受反事实陈述的概率。评估了最先进的视觉语言模型在多种提示策略下的表现。

Result: CFHR在阿拉伯语中显著上升，特别是在方言中，即使真实陈述的准确率仍然很高。推理优先的提示策略持续增加反事实幻觉，而先回答后解释的策略提高了鲁棒性。

Conclusion: 视觉语言模型在跨文化环境中存在显著的反事实幻觉问题，特别是在非英语语境中。需要开发更鲁棒的评估方法和模型改进策略来应对这一挑战。

Abstract: Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.

</details>


### [29] [Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444)
*Yao Zhou,Zeen Song,Wenwen Qiang,Fengge Wu,Shuyi Zhou,Changwen Zheng,Hui Xiong*

Main category: cs.CL

TL;DR: 提出CFA²攻击框架，利用因果前门准则解除安全机制对LLM能力的混淆，实现高效越狱


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐机制作为潜在内部状态运行，掩盖了模型的固有能力，需要从因果角度建模安全机制作为未观测的混淆变量

Method: 将安全机制建模为未观测混淆变量，采用Pearl前门准则切断混淆关联，使用稀疏自编码器物理剥离防御相关特征，将计算昂贵的边际化简化为确定性干预

Result: CFA²实现了最先进的攻击成功率，同时为越狱过程提供了机制性解释

Conclusion: 通过因果前门调整框架能够有效解除LLM安全机制的混淆效应，实现高效越狱并保持可解释性

Abstract: Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.

</details>


### [30] [Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale](https://arxiv.org/abs/2602.05447)
*Damon McMillan*

Main category: cs.CL

TL;DR: 系统研究LLM代理在结构化数据上的上下文工程，发现模型能力是主要因素，架构选择需根据模型定制而非通用最佳实践


<details>
  <summary>Details</summary>
Motivation: LLM代理通过编程接口操作外部系统，但缺乏关于如何构建代理消费上下文的实证指导。使用SQL生成为代理操作的代理，研究结构化数据的上下文工程

Method: 使用SQL生成作为代理操作的代理，进行9649个实验，涵盖11个模型、4种格式(YAML、Markdown、JSON、TOON)和10到10000个表的模式

Result: 1) 架构选择依赖模型：前沿模型(Claude、GPT、Gemini)文件检索提升准确率(+2.7%)，开源模型整体下降(-7.7%)；2) 格式不影响总体准确率但个别模型敏感；3) 模型能力是主导因素(21%准确率差距)；4) 文件原生代理可扩展到10000个表；5) 文件大小不预测运行时效率

Conclusion: 为部署LLM代理到结构化系统提供基于证据的指导，表明架构决策应根据模型能力定制，而非假设通用最佳实践

Abstract: Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.
  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.
  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.

</details>


### [31] [Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision](https://arxiv.org/abs/2602.05471)
*Md. Mithun Hossaina,Mashary N. Alrasheedy,Nirban Bhowmick,Shamim Forhad,Md. Shakil Hossain,Sudipto Chaki,Md Shafiqul Islam*

Main category: cs.CL

TL;DR: 提出了一种不确定性感知的多语言多标签情感分类框架，通过熵基模糊加权和掩码感知目标，在部分监督下实现更稳健的学习。


<details>
  <summary>Details</summary>
Motivation: 当前基于知识的多语言情感识别系统面临情感模糊性和不完全监督的挑战。文本情感识别本质上是模糊的，因为多种情感状态常同时出现，且情感标注经常缺失或不一致。现有方法大多假设完全观察到的标签并依赖确定性学习目标，这在部分监督下会导致有偏学习和不可靠预测。

Method: 提出了Reasoning under Ambiguity框架：1) 使用共享多语言编码器配合语言特定优化；2) 基于熵的模糊加权机制，降低高模糊训练实例的权重而非将缺失标签视为负证据；3) 结合掩码感知目标和正-未标注正则化，实现部分监督下的稳健学习。

Result: 在英语、西班牙语和阿拉伯语情感分类基准测试中，相比强基线在多个评估指标上均取得一致改进，同时提升了训练稳定性、对标注稀疏性的鲁棒性以及可解释性。

Conclusion: 该不确定性感知框架能有效处理多语言多标签情感分类中的模糊性和不完全监督问题，通过显式对齐学习与标注不确定性，实现了更可靠的情感识别系统。

Abstract: Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.

</details>


### [32] [LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation](https://arxiv.org/abs/2602.05493)
*Bingru Li*

Main category: cs.CL

TL;DR: LinguistAgent是一个集成平台，利用反思性多模型架构自动化语言注释，通过双代理工作流（注释者和评审者）模拟专业同行评审过程，支持提示工程、检索增强生成和微调三种范式比较，以隐喻识别为例展示效果。


<details>
  <summary>Details</summary>
Motivation: 人文社科领域的数据注释（特别是复杂语义任务如隐喻识别）存在显著瓶颈，虽然大语言模型有潜力，但其理论能力与实际研究效用之间存在差距，需要更实用的自动化注释工具。

Method: 采用反思性多模型架构，实现双代理工作流（注释者和评审者）模拟同行评审过程，支持三种范式比较：提示工程（零/少样本）、检索增强生成和微调，提供实时标记级评估（精确率、召回率、F1分数）。

Result: 开发了LinguistAgent平台，以隐喻识别任务为例展示了其有效性，平台代码已在GitHub开源发布。

Conclusion: LinguistAgent为研究人员提供了一个用户友好的集成平台，填补了大语言模型理论能力与实际研究应用之间的差距，通过自动化语言注释提高了研究效率。

Abstract: Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.

</details>


### [33] [Transport and Merge: Cross-Architecture Merging for Large Language Models](https://arxiv.org/abs/2602.05495)
*Chenhang Cui,Binyun Yang,Fei Shen,Yuxin Chen,Jingnan Zheng,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 提出基于最优传输的跨架构模型合并框架，实现从大型高资源LLM到异构低资源目标模型的知识迁移


<details>
  <summary>Details</summary>
Motivation: 现实部署中常使用小型低资源模型，但大型高资源LLM拥有更强能力，需要将知识从大模型迁移到异构小模型

Method: 基于最优传输对齐异构模型的激活值，推断跨神经元对应关系，然后使用传输计划指导权重空间融合

Result: 在低资源语言和专门领域的广泛实验中，相比目标模型获得了一致的性能提升

Conclusion: 提出的跨架构合并框架能有效实现高资源到低资源的知识迁移，仅需少量输入即可完成

Abstract: Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.

</details>


### [34] [A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering](https://arxiv.org/abs/2602.05512)
*Larissa Pusch,Alexandre Courtiol,Tim Conrad*

Main category: cs.CL

TL;DR: 提出一个交互式框架，让LLMs生成和解释Cypher图查询，用户通过自然语言迭代优化，提高知识图谱的可访问性，同时保持事实准确性和语义严谨性。


<details>
  <summary>Details</summary>
Motivation: LLMs在知识密集型领域存在幻觉、信息过时和可解释性有限的问题，而基于文本的RAG在多跳推理方面有困难。知识图谱支持精确、可解释的查询，但需要查询语言知识，因此需要降低知识图谱的访问门槛。

Method: 开发交互式框架，LLMs生成和解释Cypher图查询，用户通过自然语言迭代优化查询。在合成电影KG上进行90个查询的基准测试，评估查询解释质量和故障检测，并在Hyena KG和MaRDI KG上进行两个小型真实查询生成实验。

Result: 框架提高了复杂数据集的可访问性，同时保持了事实准确性和语义严谨性，并提供了模型性能在不同领域变化的洞察。核心定量评估显示LLMs在查询解释和故障检测方面的表现。

Conclusion: 交互式框架成功地将LLMs与知识图谱查询相结合，通过自然语言交互降低了知识图谱的访问门槛，同时保持了查询的精确性和可解释性，为不同领域的应用提供了实用解决方案。

Abstract: Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.

</details>


### [35] [Multi-Task GRPO: Reliable LLM Reasoning Across Tasks](https://arxiv.org/abs/2602.05547)
*Shyam Sundhar Ramesh,Xiaotong Ji,Matthieu Zimmer,Sangwoong Yoon,Zhiyong Wang,Haitham Bou Ammar,Aurelien Lucchi,Ilija Bogunovic*

Main category: cs.CL

TL;DR: MT-GRPO：一种多任务GRPO算法，通过动态调整任务权重和比例保持采样器，解决多任务强化学习中的不平衡问题，显著提升最差任务性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界部署需要语言模型在多样化任务上都有可靠表现，但传统的多任务GRPO方法会导致优化不平衡，某些任务主导优化而其他任务停滞不前。此外，不同任务中提示产生零优势（零梯度）的频率差异很大，进一步扭曲了优化信号的有效贡献。

Method: 提出MT-GRPO算法：1）动态调整任务权重，显式优化最差任务性能，促进任务间平衡进展；2）引入比例保持采样器，确保任务级策略梯度反映调整后的权重。

Result: 在3任务和9任务设置中，MT-GRPO在保持竞争力的平均准确率的同时，在最差任务准确率上比标准GRPO提升16-28%，比DAPO提升6%。在3任务设置中，达到50%最差任务准确率所需的训练步骤减少50%。

Conclusion: MT-GRPO通过动态任务权重调整和比例保持采样，有效解决了多任务强化学习中的不平衡问题，显著提升了最差任务性能并提高了训练效率，为实现跨任务可靠性能提供了有效解决方案。

Abstract: RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.

</details>


### [36] [CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models](https://arxiv.org/abs/2602.05633)
*Rui Jia,Ruiyi Lan,Fengrui Liu,Zhongxiang Dai,Bo Jiang,Jing Shao,Jingyuan Chen,Guandong Xu,Fei Wu,Min Zhang*

Main category: cs.CL

TL;DR: CASTLE是一个针对教育场景的个性化安全评估基准，覆盖15种教育安全风险和14种学生属性，包含92,908个双语场景，用于评估LLM在个性化学习中的安全性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育个性化学习中存在同质化响应问题，忽视了学生认知和心理的异质性，可能对弱势群体造成安全风险。现有安全评估主要依赖事实准确性、偏见或毒性等与上下文无关的指标，无法捕捉同一响应对不同学生属性可能造成的不同危害。

Method: 提出"学生定制个性化安全"概念，构建CASTLE基准，基于教育理论覆盖15种教育安全风险和14种学生属性，包含92,908个双语场景。设计三个评估指标：风险敏感性（检测风险能力）、情感共情（识别学生状态能力）和学生对齐（响应与学生属性匹配度）。

Result: 对18个最先进的LLM进行实验，所有模型在CASTLE基准上的平均安全评分低于2.3分（满分5分），表明在个性化安全保障方面存在显著不足。

Conclusion: CASTLE基准揭示了当前LLM在教育个性化安全方面的严重缺陷，需要开发更精细的个性化安全机制来保护不同学生群体的安全。

Abstract: Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.

</details>


### [37] [Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew](https://arxiv.org/abs/2602.05648)
*Giuseppe Samo,Paola Merlo*

Main category: cs.CL

TL;DR: 研究Transformer模型如何表示土耳其语和现代希伯来语的复杂动词范式，重点关注分词策略如何影响这种能力。土耳其语因其透明的形态标记，单语和多语模型都能成功；而希伯来语中，单语和多语模型表现不同，需要特定分词策略才能处理其非连接性形态。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解Transformer模型如何表示复杂动词形态，特别是分词策略如何影响模型对不同语言形态结构的处理能力。土耳其语和希伯来语代表了两种不同的形态类型：土耳其语具有透明的连接性形态，而希伯来语具有复杂的非连接性形态。

Method: 使用Blackbird Language Matrices任务在自然数据上进行实验。比较不同分词策略：原子分词（完整单词）和小型子词单元分词。针对土耳其语和希伯来语，分别测试单语和多语模型的表现。同时使用更合成的数据集进行补充实验。

Result: 土耳其语方面，单语和多语模型都能成功处理其透明形态标记，无论采用原子分词还是子词分词。希伯来语方面，单语和多语模型表现不同：使用字符级分词的多语模型无法捕捉希伯来语的非连接性形态，而采用语素感知分词的希伯来语单语模型表现良好。所有模型在更合成的数据集上性能都有提升。

Conclusion: 分词策略对Transformer模型处理复杂动词形态的能力有重要影响。对于土耳其语这类透明形态语言，多种分词策略都有效；但对于希伯来语这类非连接性形态语言，需要特定分词策略（如语素感知分词）才能有效处理。模型性能在更合成的数据上普遍提升。

Abstract: We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.

</details>


### [38] [MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations](https://arxiv.org/abs/2602.05692)
*Congbo Ma,Yichun Zhang,Yousef Al-Jazzazi,Ahamed Foisal,Laasya Sharma,Yousra Sadqi,Khaled Saleh,Jihad Mallat,Farah E. Shamout*

Main category: cs.CL

TL;DR: MedErrBench是首个多语言临床文本错误检测、定位和纠正基准，涵盖英语、阿拉伯语和中文，由临床专家指导开发，用于评估LLM在医疗应用中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有临床文本中的错误可能导致严重后果（如误诊或错误治疗建议），而LLM在医疗应用中的使用日益增加，但缺乏专门的多语言评估基准，特别是在非英语环境中。

Method: 在临床专家指导下开发包含10种常见错误类型的扩展分类法，收集英语、阿拉伯语和中文的自然临床案例，由领域专家标注和审查，构建MedErrBench基准，并评估通用、语言特定和医疗领域语言模型在三个任务上的表现。

Result: 评估结果显示存在显著性能差距，特别是在非英语环境中，突显了需要临床基础、语言感知系统的必要性。

Conclusion: 通过公开MedErrBench和评估协议，旨在推进多语言临床NLP发展，促进全球更安全、更公平的AI医疗保健。

Abstract: Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.

</details>


### [39] [Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation](https://arxiv.org/abs/2602.05694)
*Shuting Jiang,Ran Song,Yuxin Huang,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

TL;DR: 提出一种基于神经元选择的高效微调框架，通过识别和更新与领域特征对齐的共识神经元，提升大语言模型在多领域机器翻译中的性能


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在机器翻译方面表现出色，但领域适应仍然是挑战。现有的多领域机器翻译方法（如上下文学习和参数高效微调）存在领域偏移、参数干扰和泛化能力有限的问题

Method: 提出神经元高效微调框架，通过最大化神经元行为与领域特征之间的互信息来选择共识对齐的神经元，然后基于这些神经元指导大语言模型的微调，减少参数干扰和领域特定过拟合

Result: 在三个大语言模型上对十个德英和中英翻译领域进行综合实验，该方法在已见和未见领域均优于现有参数高效微调基线，达到最先进性能

Conclusion: 通过识别和微调共识对齐的神经元，可以有效提升大语言模型在多领域机器翻译中的性能，同时缓解参数干扰和领域过拟合问题

Abstract: Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.

</details>


### [40] [OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale](https://arxiv.org/abs/2602.05711)
*Jingze Shi,Zhangyang Peng,Yizhang Zhu,Yifan Wu,Guang Liu,Yuyu Luo*

Main category: cs.CL

TL;DR: OmniMoE提出了一种系统-算法协同设计的框架，将专家粒度推到极致，通过向量级原子专家实现可扩展的路由和执行，在保持准确性的同时大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE架构在专家专业化粒度和硬件执行效率之间存在固有权衡。粗粒度专家效率高但专业化有限，细粒度专家专业化强但路由复杂、内存访问效率低。

Method: 1. 引入向量级原子专家，实现单层MoE内的可扩展路由和执行，同时保留共享的密集MLP分支进行通用处理；2. 笛卡尔积路由器将大规模索引空间分解，将路由复杂度从O(N)降至O(sqrt(N))；3. 专家中心调度反转执行顺序，将分散的内存受限查找转换为高效的密集矩阵运算。

Result: 在7个基准测试中，OmniMoE（17亿激活参数）实现了50.9%的零样本准确率，优于粗粒度（如DeepSeekMoE）和细粒度（如PEER）基线。关键的是，与PEER相比，OmniMoE将推理延迟从73ms降至6.7ms（10.9倍加速），证明大规模细粒度MoE可以既快速又准确。

Conclusion: OmniMoE通过系统-算法协同设计解决了细粒度MoE的路由复杂性和内存访问瓶颈，实现了专家粒度与硬件效率的最佳平衡，为大规模MoE模型的实际部署提供了可行方案。

Abstract: Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.

</details>


### [41] [CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering](https://arxiv.org/abs/2602.05728)
*Hao Yang,Zhiyu Yang,Xupeng Zhang,Wei Wei,Yunjie Zhang,Lin Yang*

Main category: cs.CL

TL;DR: CompactRAG：通过离线知识库重构和在线推理解耦，显著减少多跳问答中的LLM调用次数和token消耗


<details>
  <summary>Details</summary>
Motivation: 现有多跳RAG系统效率低下，需要在每个推理步骤交替进行检索和推理，导致重复的LLM调用、高token消耗以及跨跳实体一致性不稳定

Method: 1. 离线阶段：LLM一次性读取语料库，将其转换为原子QA知识库，将知识表示为最小化的细粒度问答对
2. 在线阶段：复杂查询被分解并重写以保持实体一致性，通过密集检索和RoBERTa答案提取解决
3. 推理时LLM仅调用两次（子问题分解和最终答案合成），与推理跳数无关

Result: 在HotpotQA、2WikiMultiHopQA和MuSiQue数据集上，CompactRAG在保持竞争力的准确率的同时，相比迭代式RAG基线显著减少了token消耗

Conclusion: CompactRAG提供了一种成本效益高且实用的多跳推理方法，通过解耦离线知识重构和在线推理，实现了高效的多跳问答

Abstract: Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.
  In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.
  Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.

</details>


### [42] [LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards](https://arxiv.org/abs/2602.05758)
*Bowen Ping,Zijun Chen,Yiyao Yu,Tingfeng Hui,Junchi Yan,Baobao Chang*

Main category: cs.CL

TL;DR: LongR是一个增强大语言模型长上下文推理能力的强化学习框架，通过"思考-阅读"机制和上下文密度奖励，在多个长上下文基准测试上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注数据合成或架构修改，但仅依赖稀疏的、仅基于结果的奖励信号效果有限，无法有效指导复杂的长上下文推理任务。

Method: 提出LongR统一框架：1）动态的"思考-阅读"机制，交替进行推理和文档查阅；2）基于相对信息增益的上下文密度奖励，量化相关文档的效用。

Result: 在LongBench v2上获得9%的性能提升，在RULER和InfiniteBench上也有一致的改进；在不同RL算法（如DAPO、GSPO）上都能增强性能。

Conclusion: LongR通过结合动态推理过程和细粒度奖励信号，有效提升了LLM在长上下文场景中的推理能力，并展示了在推理链长度和抗干扰性方面的鲁棒性。

Abstract: Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic "Think-and-Read" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.

</details>


### [43] [Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors](https://arxiv.org/abs/2602.05769)
*Adnan Al Ali,Jindřich Helcl,Jindřich Libovický*

Main category: cs.CL

TL;DR: 该论文重新评估了LLM生成文本检测器对非母语写作者的偏见问题，发现在捷克语环境中，非母语者的文本困惑度并不低于母语者，且现代检测器没有系统性偏见，也不依赖困惑度特征。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等LLM助手的普及，学术界对其滥用的担忧日益增加。先前研究表明，自动检测方法常错误地将非母语者的论文标记为AI生成，因为非母语者的文本困惑度较低（这是检测器的关键特征）。本研究旨在两年后重新验证这一说法，特别是在捷克语环境中。

Method: 研究在捷克语环境中进行：1）比较非母语者和母语者文本的困惑度；2）评估来自三个不同家族的检测器是否存在对非母语者的系统性偏见；3）分析当代检测器是否依赖困惑度特征进行判断。

Result: 1）非母语者的捷克语文本困惑度并不低于母语者；2）三种检测器均未表现出对非母语者的系统性偏见；3）当代检测器在有效运作时并不依赖困惑度特征。

Conclusion: 与先前研究相反，在捷克语环境中，非母语者的文本困惑度并不更低，且现代LLM生成文本检测器没有对非母语者产生系统性偏见，其有效性也不依赖于困惑度特征。

Abstract: LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.

</details>


### [44] [Reinforcement World Model Learning for LLM-based Agents](https://arxiv.org/abs/2602.05842)
*Xiao Yu,Baolin Peng,Ruize Xu,Yelong Shen,Pengcheng He,Suman Nath,Nikhil Singh,Jiangfeng Gao,Zhou Yu*

Main category: cs.CL

TL;DR: RWML是一种自监督学习方法，通过模拟到现实的差距奖励为基于LLM的智能体学习动作条件世界模型，在文本状态下对齐模拟状态与实际环境动态，提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: LLM在语言任务中表现优异，但在智能体环境中难以预测动作后果和适应环境动态，需要世界建模能力来提升基于LLM的智能体的性能。

Method: 提出强化世界模型学习(RWML)，使用模拟到现实的差距奖励在文本状态下学习动作条件世界模型。该方法在预训练嵌入空间中使模型生成的模拟下一状态与环境观察到的实际下一状态对齐，确保内部世界模拟与实际环境动态的一致性。

Result: 在ALFWorld和τ² Bench上评估，相比基础模型有显著提升。结合任务成功奖励后，在ALFWorld和τ² Bench上分别比直接任务成功奖励强化学习高出6.9和5.7分，与专家数据训练性能相当。

Conclusion: RWML为基于LLM的智能体提供了一种有效的自监督世界模型学习方法，相比下一状态令牌预测更稳健，比LLM作为评判器更不易受奖励攻击，显著提升了智能体在环境中的适应能力。

Abstract: Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $τ^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $τ^2$ Bench respectively, while matching the performance of expert-data training.

</details>


### [45] [OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions](https://arxiv.org/abs/2602.05843)
*Fangzhi Xu,Hang Yan,Qiushi Sun,Jinyang Wu,Zixian Huang,Muye Huang,Jingyang Gong,Zichen Ding,Kanzhi Cheng,Yian Wang,Xinyu Che,Zeyi Sun,Jian Zhang,Zhangyue Yin,Haoran Luo,Xuanjing Huang,Ben Kao,Jun Liu,Qika Lin*

Main category: cs.CL

TL;DR: 提出OdysseyArena评估框架，专注于长视野、主动、归纳式交互，测试LLM智能体从经验中发现潜在转移规律的能力，发现前沿模型在归纳场景中存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体评估主要采用演绎范式，基于显式规则和静态目标执行任务，忽略了智能体从经验中自主发现潜在转移规律的归纳能力，这是实现智能体预见性和战略连贯性的关键。

Method: 1. 提出OdysseyArena框架，将抽象转移动态转化为具体交互环境；2. 建立OdysseyArena-Lite标准化基准，包含120个任务评估归纳效率和长视野发现能力；3. 推出OdysseyArena-Challenge，在极端交互视野（>200步）下测试智能体稳定性。

Result: 对15+领先LLM的广泛实验表明，即使是前沿模型在归纳场景中也存在显著缺陷，揭示了复杂环境中自主发现能力的关键瓶颈。

Conclusion: OdysseyArena填补了现有评估方法的空白，强调长视野、主动、归纳式交互的重要性，为LLM智能体在复杂环境中的自主发现能力提供了新的评估基准。

Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena

</details>


### [46] [RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference](https://arxiv.org/abs/2602.05853)
*Siran Liu,Guoxia Wang,Sa Wang,Jinle Zeng,HaoYang Xie,Siyu Lou,JiaBin Yang,DianHai Yu,Haifeng Wang,Chao Yang*

Main category: cs.CL

TL;DR: RRAttention是一种新颖的动态稀疏注意力方法，通过头部轮询采样策略，在保持查询独立性的同时实现高效全局模式发现，将复杂度从O(L²)降低到O(L²/S²)，在长上下文处理中实现2.4倍加速。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的二次复杂度限制了大型语言模型处理长上下文的能力。现有的动态稀疏注意力方法存在各种权衡：需要预处理、缺乏全局评估、违反查询独立性或计算开销高。

Method: 提出RRAttention方法，采用头部轮询采样策略，在每个步长内跨注意力头轮询查询采样位置，保持查询独立性的同时实现步长级聚合的全局模式发现。使用自适应Top-τ选择实现最优稀疏度。

Result: 在自然语言理解(HELMET)和多模态视频理解(Video-MME)实验中，RRAttention恢复了超过99%的完整注意力性能，仅计算一半注意力块，在128K上下文长度下实现2.4倍加速，优于现有动态稀疏注意力方法。

Conclusion: RRAttention通过头部轮询采样策略成功解决了动态稀疏注意力的基本权衡问题，同时实现了所有期望特性，为长上下文处理提供了高效解决方案。

Abstract: The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \underline{r}ound-\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.

</details>


### [47] [xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection](https://arxiv.org/abs/2602.05874)
*Adrián Girón,Pablo Miralles,Javier Huertas-Tato,Sergio D'Antonio,David Camacho*

Main category: cs.CL

TL;DR: 提出xList-Hate框架，将仇恨言论检测分解为基于规范标准的检查清单问题，通过LLM回答后由决策树聚合，提升跨数据集鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统仇恨言论检测作为二元分类问题存在局限性：模型容易过拟合特定数据集的定义，在领域迁移和标注噪声下鲁棒性差，且缺乏透明度和可解释性。

Method: 1) 将仇恨言论检测分解为基于广泛共享规范标准的检查清单问题；2) 使用大语言模型独立回答每个问题，生成二进制诊断表示；3) 通过轻量级、完全可解释的决策树聚合诊断信号。

Result: 相比零样本LLM分类和领域内监督微调，xList-Hate在多个仇恨言论基准测试中：1) 显著提升跨数据集鲁棒性；2) 在领域迁移下保持相对性能；3) 对标注不一致和上下文模糊性更不敏感；4) 提供细粒度可解释性。

Conclusion: 将仇恨言论检测重构为诊断推理任务而非单一分类问题，为内容审核提供了更鲁棒、可解释且可扩展的替代方案，有助于解决现有方法的局限性。

Abstract: Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.
  We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.
  We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.
  Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.

</details>


### [48] [EuroLLM-22B: Technical Report](https://arxiv.org/abs/2602.05879)
*Miguel Moura Ramos,Duarte M. Alves,Hippolyte Gisserot-Boukhlef,João Alves,Pedro Henrique Martins,Patrick Fernandes,José Pombal,Nuno M. Guerreiro,Ricardo Rei,Nicolas Boizard,Amin Farajian,Mateusz Klimaszewski,José G. C. de Souza,Barry Haddow,François Yvon,Pierre Colombo,Alexandra Birch,André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-22B是一个从头训练的大语言模型，专门支持欧洲24种官方语言和11种额外语言，旨在解决欧洲语言在现有开源大模型中代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决欧洲语言在现有开源大语言模型中代表性不足和服务不足的问题，满足欧洲公民的多语言需求。

Method: 从头训练22B参数模型，包括专门的tokenizer设计、架构规范、数据过滤和训练流程，涵盖35种语言。

Result: 在广泛的多语言基准测试中，EuroLLM-22B在推理、指令遵循和翻译方面表现强劲，与同规模模型竞争性相当。

Conclusion: 该模型填补了欧洲语言在大语言模型中的空白，并开源了基础模型、指令调优模型、多语言预训练数据、指令数据集和代码库以支持未来研究。

Abstract: This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.

</details>


### [49] [Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models](https://arxiv.org/abs/2602.05897)
*Shuo Nie,Hexuan Deng,Chao Wang,Ruiyu Fang,Xuebo Liu,Shuangyong Song,Yu Li,Min Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: FaithRL：通过步骤级监督和截断重采样策略减少小推理模型的忠实性幻觉


<details>
  <summary>Details</summary>
Motivation: 小推理模型在资源受限环境中支持思维链推理，但容易产生中间推理步骤的忠实性幻觉。现有基于在线强化学习的方法依赖结果奖励或粗粒度评估，可能在最终答案正确时无意中强化不忠实的推理。

Method: 提出Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL)，通过过程奖励模型提供步骤级忠实性监督奖励，并结合隐式截断重采样策略从忠实前缀生成对比信号。

Result: 在多个小推理模型和开放书问答基准上的实验表明，FaithRL能持续减少思维链和最终答案中的幻觉，实现更忠实可靠的推理。

Conclusion: FaithRL通过步骤级监督和对比信号生成，有效解决了小推理模型的忠实性幻觉问题，提升了推理的可靠性。

Abstract: As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.

</details>


### [50] [Codified Finite-state Machines for Role-playing](https://arxiv.org/abs/2602.05905)
*Letian Peng,Yupeng Hou,Kun Zhou,Jingbo Shang*

Main category: cs.CL

TL;DR: 该论文提出了Codified Finite-State Machines (CFSMs)框架，利用LLM自动将文本角色描述编码为有限状态机，以解决角色扮演中潜在状态建模问题，并进一步扩展为概率版本CPFSMs来处理不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法主要捕捉表面行为，难以跟踪驱动交互的潜在状态，导致角色扮演不一致。传统手工制作的有限状态机虽然能建模状态转换，但在开放语义空间中适应性差。

Method: 提出CFSMs框架，使用LLM-based coding自动从文本角色描述中提取关键状态和转换，构建可解释的有限状态机结构。进一步扩展为CPFSMs，将转换建模为状态上的概率分布，以处理不确定性和变异性。

Result: 通过合成评估和真实世界角色扮演场景验证，CFSM和CPFSM在结构化任务和开放随机状态探索中都优于通用基线方法，证明了其有效性。

Conclusion: CFSMs和CPFSMs能够有效建模角色潜在状态，提高角色扮演的一致性和适应性，为开放语义空间中的状态建模提供了新方法。

Abstract: Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.

</details>


### [51] [KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs](https://arxiv.org/abs/2602.05929)
*Jian Chen,Zhuoran Wang,Jiayu Qin,Ming Li,Meng Wang,Changyou Chen,Yin Chen,Qizhen Weng,Yirui Liu*

Main category: cs.CL

TL;DR: KV-CoRE：一种基于SVD的方法，用于量化kv-cache的数据依赖性低秩可压缩性，建立了首个大规模LLM kv-cache可压缩性基准


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度增长，kv-cache的读写会迅速饱和GPU内存带宽。现有kv-cache压缩方法大多忽略了其数据依赖性特性及跨层变化

Method: 提出KV-CoRE方法，基于SVD计算Frobenius范数下的最优低秩近似，采用无梯度、增量式方法实现高效的数据集级、分层评估

Result: 分析了多个模型和数据集，涵盖5个英文领域和16种语言，发现可压缩性与模型架构、训练数据和语言覆盖存在系统性关联；归一化有效秩与压缩性能下降强相关

Conclusion: 建立了原则性评估框架和首个大规模LLM kv-cache可压缩性基准，为动态、数据感知的压缩和数据中心的模型开发提供见解

Abstract: Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.

</details>


### [52] [Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions](https://arxiv.org/abs/2602.05932)
*Léo Labat,Etienne Ollion,François Yvon*

Main category: cs.CL

TL;DR: 该研究探讨多语言大语言模型在价值导向多选题上的跨语言一致性，发现即使经过指令微调的大型模型也存在语言特异性行为，但仅在某些问题上出现。


<details>
  <summary>Details</summary>
Motivation: 研究多语言大语言模型在回答价值导向多选题时是否保持跨语言一致性，还是像多个单语模型一样根据问题语言表现出不同价值观。

Method: 创建了多语言欧洲价值观调查（MEVS）语料库，包含8种欧洲语言的人工翻译调查问题。对30多个多语言LLM进行测试，控制提示变量如答案顺序、符号类型和尾部字符。

Result: 大型指令微调模型整体一致性较高，但响应稳健性因问题而异。所有一致的指令微调模型在某些问题上都表现出语言特异性行为，表明偏好微调具有选择性影响。

Conclusion: 多语言LLM在价值导向问题上并非完全一致的理论多语者，而是在某些问题上表现出语言依赖性，需要进一步研究偏好微调的选择性效应。

Abstract: Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.

</details>


### [53] [Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training](https://arxiv.org/abs/2602.05940)
*Junxiao Liu,Zhijun Wang,Yixiao Li,Zhejian Lai,Liqian Huang,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: 提出TRIT框架，通过整合翻译训练提升多语言推理能力，解决大模型在非英语问题上推理能力不足的问题


<details>
  <summary>Details</summary>
Motivation: 当前长推理模型在多语言环境中表现不佳：倾向于用英语推理非英语问题，强制用问题语言推理时准确率大幅下降。这源于模型在多语言问题理解和多语言推理两方面的能力有限。

Method: 提出TRIT（Translation-Reasoning Integrated Training）框架，将翻译训练整合到多语言推理训练中，无需外部反馈或额外多语言数据，联合提升多语言问题理解和响应生成能力。

Result: 在MMATH数据集上，该方法平均比多个基线高出7个百分点，提升了答案正确性和语言一致性。分析显示，整合翻译训练使跨语言问题对齐提升超过10个百分点，数学问题和通用领域文本的翻译质量也得到提升，在FLORES-200上获得高达8.4 COMET点的增益。

Conclusion: TRIT框架通过自我改进机制有效解决了多语言推理中的理解和生成问题，显著提升了模型在多语言数学推理任务上的性能，同时改善了翻译质量。

Abstract: Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.

</details>


### [54] [Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space](https://arxiv.org/abs/2602.05971)
*Felipe D. Toro-Hernández,Jesuino Vieira Filho,Rodrigo M. Cabral-Carvalho*

Main category: cs.CL

TL;DR: 该研究提出将概念生成视为在嵌入空间中的导航，通过累积嵌入构建语义轨迹并提取几何和动态指标，用于区分临床组别和概念类型，为语义表示动态量化提供数学框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索人类如何在语义表示的结构化动态知识空间中导航，以检索和操纵意义。传统方法需要大量人工语言预处理，需要一种更自动化的数学框架来量化语义表示动态。

Method: 使用不同的transformer文本嵌入模型，基于累积嵌入构建参与者特定的语义轨迹，提取几何和动态指标（包括到下一个的距离、到质心的距离、熵、速度和加速度）。在四个不同语言的数据集上评估框架，涵盖不同的属性生成任务。

Result: 该方法能够区分临床组别和概念类型，累积嵌入在较长轨迹中效果最佳，而较短轨迹可能更适合非累积方法。不同嵌入模型产生相似结果，表明不同学习表示之间存在相似性。

Conclusion: 通过将语义导航框架化为嵌入空间中的结构化轨迹，该研究桥接了认知建模与学习表示，建立了量化语义表示动态的流程，在临床研究、跨语言分析和人工认知评估中具有应用价值。

Abstract: Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.

</details>


### [55] [DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs](https://arxiv.org/abs/2602.05992)
*Lizhuo Luo,Shenggui Li,Yonggang Wen,Tianwei Zhang*

Main category: cs.CL

TL;DR: 提出Dynamic Sliding Block (DSB)方法，通过动态调整块大小来改进扩散大语言模型的块调度策略，提升生成质量和推理效率


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型使用固定的预定义块调度策略，无法根据语义难度动态调整，导致质量与效率的次优结果：可能过早确定不确定位置，同时延迟边界附近的简单位置

Method: 提出Dynamic Sliding Block (DSB)方法，使用动态大小的滑动块来克服固定块的刚性；同时提出DSB Cache，为DSB量身定制的无需训练的KV缓存机制

Result: 在多个模型和基准测试上的实验表明，DSB与DSB Cache结合能够一致提升扩散大语言模型的生成质量和推理效率

Conclusion: 动态调整块调度策略以适应语义难度对于扩散大语言模型的可靠高效推理至关重要，DSB方法为此提供了有效的解决方案

Abstract: Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.

</details>


### [56] [A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies](https://arxiv.org/abs/2602.06015)
*Panagiotis Kaliosis,Adithya V Ganesan,Oscar N. E. Kjell,Whitney Ringwald,Scott Feltman,Melissa A. Carr,Dimitris Samaras,Camilo Ruggero,Benjamin J. Luft,Roman Kotov,Andrew H. Schwartz*

Main category: cs.CL

TL;DR: LLMs用于心理健康评估时，上下文知识和建模策略对准确性有重要影响，最佳性能通过监督模型与零样本LLMs集成实现。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地以零样本方式用于评估心理健康状况，但对其准确性影响因素了解有限，需要系统研究不同上下文知识和建模策略的影响。

Method: 使用1,437名个体的临床数据集，评估11个SOTA LLMs，系统变化(i)上下文知识：子量表定义、分布摘要、访谈问题；(ii)建模策略：零样本vs少样本、推理努力程度、模型大小、结构化子量表vs直接标量预测、输出重缩放和9种集成方法。

Result: (a)提供详细构念定义和叙述上下文时LLMs最准确；(b)增加推理努力提高估计准确性；(c)开源模型在70B参数后性能趋于稳定，闭源模型随新一代改进；(d)监督模型与零样本LLMs集成获得最佳性能。

Conclusion: 部署LLMs准确评估心理健康时，上下文知识和建模策略的选择至关重要，集成方法能显著提升性能。

Abstract: Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.

</details>


### [57] [Multi-Token Prediction via Self-Distillation](https://arxiv.org/abs/2602.06019)
*John Kirchenbauer,Abhimanyu Hans,Brian Bartoldson,Micah Goldblum,Ashwinee Panda,Tom Goldstein*

Main category: cs.CL

TL;DR: 提出一种简单在线蒸馏方法，将预训练自回归语言模型转换为快速多令牌预测模型，无需额外推理代码即可实现3倍以上解码加速


<details>
  <summary>Details</summary>
Motivation: 现有加速技术如推测解码需要训练辅助模型和构建复杂推理管道，部署复杂。希望找到更简单的方法，让预训练模型本身就能实现快速多令牌预测

Method: 使用简单在线蒸馏目标，将预训练自回归语言模型从单令牌预测转换为多令牌预测模型。最终模型保持与原始检查点完全相同的实现，无需添加任何辅助验证器或专门推理代码

Result: 在GSM8K上，该方法产生的模型平均解码速度可提高3倍以上，准确率下降小于5%（相对于单令牌解码性能）

Conclusion: 该方法提供了一种简单有效的语言模型推理加速方案，无需复杂部署，保持了模型实现的简洁性，在速度和准确性之间取得了良好平衡

Abstract: Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $<5\%$ drop in accuracy relative to single token decoding performance.

</details>


### [58] [Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory](https://arxiv.org/abs/2602.06025)
*Haozhen Zhang,Haodong Yue,Tao Feng,Quanyu Long,Jianzhu Bao,Bowen Jin,Weizhi Zhang,Xiao Li,Jiaxuan You,Chengwei Qin,Wenya Wang*

Main category: cs.CL

TL;DR: BudgetMem是一个运行时智能体记忆框架，通过模块化设计和预算层级（低/中/高）实现查询感知的性能-成本控制，使用轻量级路由器进行预算分配，在多个基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆系统大多采用离线、查询无关的记忆构建方式，效率低下且可能丢弃关键信息。运行时记忆利用虽然更自然，但现有方法开销大且缺乏对性能-成本权衡的显式控制。

Method: 将记忆处理结构化为多个记忆模块，每个模块提供三个预算层级（低/中/高）。使用轻量级神经网络路由器进行跨模块的预算层级路由，通过强化学习训练路由策略。研究了三种实现预算层级的策略：实现复杂度、推理行为和模型大小。

Result: 在LoCoMo、LongMemEval和HotpotQA基准测试中，BudgetMem在优先性能（高预算设置）时超越强基线方法，在更紧预算下提供更好的准确率-成本边界。分析揭示了不同层级策略在不同预算机制下的优劣势。

Conclusion: BudgetMem提供了一个统一的运行时记忆框架，实现了显式的查询感知性能-成本控制，通过模块化设计和预算层级路由在多个任务上取得优越性能，为记忆系统的设计提供了新的视角。

Abstract: Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.

</details>


### [59] [DFlash: Block Diffusion for Flash Speculative Decoding](https://arxiv.org/abs/2602.06036)
*Jian Chen,Yesheng Liang,Zhijian Liu*

Main category: cs.CL

TL;DR: DFlash是一种使用轻量级块扩散模型进行并行草稿生成的推测解码框架，相比传统自回归方法实现6倍无损加速，比现有最佳方法快2.5倍。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型需要顺序解码，导致推理延迟高、GPU利用率低。现有推测解码方法仍依赖自回归草稿生成，限制了实际加速效果。扩散模型虽能并行生成，但性能通常不如自回归模型。

Method: 使用轻量级块扩散模型进行并行草稿生成，通过单次前向传递生成草稿token，并基于从目标模型中提取的上下文特征来条件化草稿模型，实现高质量输出和高接受率。

Result: 在多种模型和任务上实现了超过6倍的无损加速，比最先进的推测解码方法EAGLE-3快2.5倍。

Conclusion: DFlash通过结合扩散模型的并行生成能力和推测解码框架，有效解决了自回归解码的瓶颈，实现了显著的推理加速，为高效LLM推理提供了新方向。

Abstract: Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [60] [On the Skew Stickiness Ratio](https://arxiv.org/abs/2602.05241)
*Masaaki Fukasawa*

Main category: q-fin.MF

TL;DR: 推导了偏斜粘性比的表示公式，并分析了其在Bergomi型随机波动率模型下的渐近行为


<details>
  <summary>Details</summary>
Motivation: 偏斜粘性比是捕捉资产价格与其波动率联合动态的重要统计量，需要建立其理论表示并分析模型下的渐近性质

Method: 使用Itô-Wentzell公式和Clark-Ocone公式推导表示公式，应用于Bergomi型随机波动率模型进行渐近分析

Result: 获得了偏斜粘性比的显式表示公式，并得到了其在Bergomi型模型下的渐近行为结果

Conclusion: 建立了偏斜粘性比的理论框架，为理解资产价格与波动率联合动态提供了数学工具和渐近分析基础

Abstract: The skew stickiness ratio is a statistic that captures the joint dynamics of an asset price and its volatility. We derive a representation formula for this quantity using the Itô-Wentzell and Clark-Ocone formulae, and we apply it to analyze its asymptotics under Bergomi-type stochastic volatility models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [61] [Finite-Particle Rates for Regularized Stein Variational Gradient Descent](https://arxiv.org/abs/2602.05172)
*Ye He,Krishnakumar Balasubramanian,Sayan Banerjee,Promit Ghosal*

Main category: stat.ML

TL;DR: 本文分析了正则化Stein变分梯度下降(R-SVGD)算法的有限粒子收敛率，修正了SVGD的常数阶偏差，建立了时间平均经验测度的非渐近界，并提供了正则化参数、步长和平均范围的调优规则。


<details>
  <summary>Details</summary>
Motivation: SVGD算法存在常数阶偏差，R-SVGD通过应用预处理器来修正这一偏差。需要建立有限粒子系统的收敛理论，量化逼近Wasserstein梯度流与控制有限粒子估计误差之间的权衡。

Method: 对R-SVGD的N粒子相互作用系统进行分析，建立时间平均（退火）经验测度的显式非渐近界。分析覆盖连续时间和离散时间动态，并推导正则化参数、步长和平均范围的调优规则。

Result: 证明了在真实（非核化）Fisher信息中的收敛性，并在目标的W1I条件下，对一大类光滑核建立了相应的W1收敛性。量化了逼近精度与估计误差之间的权衡。

Conclusion: R-SVGD能够有效修正SVGD的常数阶偏差，建立了有限粒子系统的收敛理论，并提供了实用的调优规则，为实际应用提供了理论指导。

Abstract: We derive finite-particle rates for the regularized Stein variational gradient descent (R-SVGD) algorithm introduced by He et al. (2024) that corrects the constant-order bias of the SVGD by applying a resolvent-type preconditioner to the kernelized Wasserstein gradient. For the resulting interacting $N$-particle system, we establish explicit non-asymptotic bounds for time-averaged (annealed) empirical measures, illustrating convergence in the \emph{true} (non-kernelized) Fisher information and, under a $\mathrm{W}_1\mathrm{I}$ condition on the target, corresponding $\mathrm{W}_1$ convergence for a large class of smooth kernels. Our analysis covers both continuous- and discrete-time dynamics and yields principled tuning rules for the regularization parameter, step size, and averaging horizon that quantify the trade-off between approximating the Wasserstein gradient flow and controlling finite-particle estimation error.

</details>


### [62] [Total Variation Rates for Riemannian Flow Matching](https://arxiv.org/abs/2602.05174)
*Yunrui Guan,Krishnakumar Balasubramanian,Shiqian Ma*

Main category: stat.ML

TL;DR: 本文对黎曼流匹配(RFM)采样器进行了非渐近TV收敛分析，建立了显式误差界，分离了数值离散化和学习误差，并给出了球面和SPD流形上的多项式迭代复杂度。


<details>
  <summary>Details</summary>
Motivation: 黎曼流匹配将基于流的生成建模扩展到流形数据，但缺乏对采样器收敛性的理论分析。本文旨在建立RFM采样器的非渐近TV收敛分析，明确分离数值离散化和学习误差。

Method: 通过建立控制两个流形ODE流之间TV演化的微分不等式，将TV时间导数表达为向量场不匹配的发散和参考流的得分。需要建立考虑平行传输和曲率的新界。在光滑性假设下，对学习场建立均匀或均方近似保证。

Result: 得到了形式为TV ≤ C_Lip h + C_ε ε的显式误差界（在紧致流形上还有高阶ε²项），清晰分离了数值步长h和学习误差ε。在球面S^d和SPD(n)流形上得到了显式多项式迭代复杂度。

Conclusion: 本文首次为黎曼流匹配采样器提供了非渐近TV收敛分析，建立了显式误差界并分离了不同误差源，为流形上的生成建模提供了理论保证。

Abstract: Riemannian flow matching (RFM) extends flow-based generative modeling to data supported on manifolds by learning a time-dependent tangent vector field whose flow-ODE transports a simple base distribution to the data law. We develop a nonasymptotic Total Variation (TV) convergence analysis for RFM samplers that use a learned vector field together with Euler discretization on manifolds. Our key technical ingredient is a differential inequality governing the evolution of TV between two manifold ODE flows, which expresses the time-derivative of TV through the divergence of the vector-field mismatch and the score of the reference flow; controlling these terms requires establishing new bounds that explicitly account for parallel transport and curvature. Under smoothness assumptions on the population flow-matching field and either uniform (compact manifolds) or mean-square (Hadamard manifolds) approximation guarantees for the learned field, we obtain explicit bounds of the form $\mathrm{TV}\le C_{\mathrm{Lip}}\,h + C_{\varepsilon}\,\varepsilon$ (with an additional higher-order $\varepsilon^2$ term on compact manifolds), cleanly separating numerical discretization and learning errors. Here, $h$ is the step-size and $\varepsilon$ is the target accuracy. Instantiations yield \emph{explicit} polynomial iteration complexities on the hypersphere $S^d$, and on the SPD$(n)$ manifolds under mild moment conditions.

</details>


### [63] [Radon--Wasserstein Gradient Flows for Interacting-Particle Sampling in High Dimensions](https://arxiv.org/abs/2602.05227)
*Elias Hess-Childs,Dejan Slepčev,Lantian Xu*

Main category: stat.ML

TL;DR: 提出基于Radon-Wasserstein几何的新KL散度梯度流，通过Radon变换实现高效的一维投影计算，使得粒子算法在计算成本上具有线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有KL散度梯度流（如Fokker-Planck方程和Stein变分梯度下降）在计算上存在挑战，特别是在高维空间中，需要开发既能保持准确粒子近似又具有线性计算复杂度的新方法。

Method: 引入基于Radon变换的两种新几何结构：Radon-Wasserstein几何和正则化Radon-Wasserstein几何，通过一维投影计算梯度流速度，利用快速傅里叶变换高效计算一维卷积。

Result: 提出的算法在每步计算成本上实现线性复杂度（粒子数和维度均线性），数值实验验证了算法性能，并提供了流的存在性、适定性和长期收敛性的理论保证。

Conclusion: 基于Radon-Wasserstein几何的新梯度流为高维概率分布优化提供了高效的计算框架，结合了理论保证和实际计算效率的优势。

Abstract: Gradient flows of the Kullback--Leibler (KL) divergence, such as the Fokker--Planck equation and Stein Variational Gradient Descent, evolve a distribution toward a target density known only up to a normalizing constant. We introduce new gradient flows of the KL divergence with a remarkable combination of properties: they admit accurate interacting-particle approximations in high dimensions, and the per-step cost scales linearly in both the number of particles and the dimension. These gradient flows are based on new transportation-based Riemannian geometries on the space of probability measures: the Radon--Wasserstein geometry and the related Regularized Radon--Wasserstein (RRW) geometry. We define these geometries using the Radon transform so that the gradient-flow velocities depend only on one-dimensional projections. This yields interacting-particle-based algorithms whose per-step cost follows from efficient Fast Fourier Transform-based evaluation of the required 1D convolutions. We additionally provide numerical experiments that study the performance of the proposed algorithms and compare convergence behavior and quantization. Finally, we prove some theoretical results including well-posedness of the flows and long-time convergence guarantees for the RRW flow.

</details>


### [64] [Logarithmic-time Schedules for Scaling Language Models with Momentum](https://arxiv.org/abs/2602.05298)
*Damien Ferbach,Courtney Paquette,Gauthier Gidel,Katie Everett,Elliot Paquette*

Main category: stat.ML

TL;DR: ADANA优化器通过利用语言数据的幂律结构，为AdamW的超参数设计随时间变化的调度策略，在大型语言模型训练中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 实践中AdamW的超参数通常保持固定值，但研究表明通过利用语言数据的幂律结构，设计随时间变化的超参数调度可以带来显著性能提升。

Method: 提出ADANA优化器，采用对数时间调度策略，使优化器的梯度记忆范围随训练时间增长，并通过阻尼机制保持稳定性。同时探索了对数时间权重衰减策略。

Result: ADANA在45M到2.6B参数的Transformer模型上相比AdamW、Muon和AdEMAMix，实现了高达40%的计算效率提升，且随着模型规模增大效果更好。

Conclusion: 通过利用语言数据的幂律结构设计随时间变化的超参数调度是有效的，ADANA展示了这种方法的潜力，并提供了改进鲁棒性的变体。

Abstract: In practice, the hyperparameters $(β_1, β_2)$ and weight-decay $λ$ in AdamW are typically kept at fixed values. Is there any reason to do otherwise? We show that for large-scale language model training, the answer is yes: by exploiting the power-law structure of language data, one can design time-varying schedules for $(β_1, β_2, λ)$ that deliver substantial performance gains.
  We study logarithmic-time scheduling, in which the optimizer's gradient memory horizon grows with training time. Although naive variants of this are unstable, we show that suitable damping mechanisms restore stability while preserving the benefits of longer memory. Based on this, we present ADANA, an AdamW-like optimizer that couples log-time schedules with explicit damping to balance stability and performance. We empirically evaluate ADANA across transformer scalings (45M to 2.6B parameters), comparing against AdamW, Muon, and AdEMAMix.
  When properly tuned, ADANA achieves up to 40% compute efficiency relative to a tuned AdamW, with gains that persist--and even improve--as model scale increases. We further show that similar benefits arise when applying logarithmic-time scheduling to AdEMAMix, and that logarithmic-time weight-decay alone can yield significant improvements. Finally, we present variants of ADANA that mitigate potential failure modes and improve robustness.

</details>


### [65] [Decision-Focused Sequential Experimental Design: A Directional Uncertainty-Guided Approach](https://arxiv.org/abs/2602.05340)
*Beichen Wan,Mo Liu,Paul Grigas,Zuo-Jun Max Shen*

Main category: stat.ML

TL;DR: 本文提出了一种用于预测-优化范式下顺序实验设计的新方法，通过方向性不确定性度量来替代传统基于预测精度的设计，以更直接地优化下游决策损失。


<details>
  <summary>Details</summary>
Motivation: 在预测-优化范式中，传统顺序实验设计关注最大化预测精度提升，但最终性能评估基于下游优化产生的决策损失。预测精度与决策损失之间的不匹配导致传统决策盲设计效率低下。

Method: 提出一种基于方向性的不确定性度量方法，该度量无需调用优化求解器，计算上更易处理。基于此度量构建顺序设计准则，并证明其具有强一致性和收敛保证。

Result: 在广泛的分布类别下，方向性不确定性设计比决策盲设计达到更早的停止时间。在LLM工作分配问题的真实实验中进一步验证了这一优势。

Conclusion: 方向性不确定性度量为预测-优化范式下的顺序实验设计提供了计算高效且性能优越的解决方案，能够更直接地优化下游决策质量。

Abstract: We consider the sequential experimental design problem in the predict-then-optimize paradigm. In this paradigm, the outputs of the prediction model are used as coefficient vectors in a downstream linear optimization problem. Traditional sequential experimental design aims to control the input variables (features) so that the improvement in prediction accuracy from each experimental outcome (label) is maximized. However, in the predict-then-optimize setting, performance is ultimately evaluated based on the decision loss induced by the downstream optimization, rather than by prediction error. This mismatch between prediction accuracy and decision loss renders traditional decision-blind designs inefficient. To address this issue, we propose a directional-based metric to quantify predictive uncertainty. This metric does not require solving an optimization oracle and is therefore computationally tractable. We show that the resulting sequential design criterion enjoys strong consistency and convergence guarantees. Under a broad class of distributions, we demonstrate that our directional uncertainty-based design attains an earlier stopping time than decision-blind designs. This advantage is further supported by real-world experiments on an LLM job allocation problem.

</details>


### [66] [Variance Reduction Based Experience Replay for Policy Optimization](https://arxiv.org/abs/2602.05379)
*Hua Zheng,Wei Xie,M. Ben Feng,Keilung Choy*

Main category: stat.ML

TL;DR: 提出VRER框架，通过选择性重用信息样本减少策略梯度估计方差，提升强化学习效率


<details>
  <summary>Details</summary>
Motivation: 传统经验回放均匀处理所有历史观测，未考虑不同样本对学习的贡献差异，缺乏理论分析框架

Method: 提出方差减少经验回放(VRER)框架，选择性重用信息样本；开发PG-VRER算法；建立考虑马尔可夫动态和行为策略交互依赖的理论分析框架

Result: 为PG-VRER建立有限时间收敛保证，揭示偏差-方差权衡：重用旧经验增加偏差但减少梯度方差；实验证明VRER加速策略学习并提升性能

Conclusion: VRER是有效的经验回放框架，能加速策略优化，为经验回放提供理论分析基础

Abstract: Effective reinforcement learning (RL) for complex stochastic systems requires leveraging historical data collected in previous iterations to accelerate policy optimization. Classical experience replay treats all past observations uniformly and fails to account for their varying contributions to learning. To overcome this limitation, we propose Variance Reduction Experience Replay (VRER), a principled framework that selectively reuses informative samples to reduce variance in policy gradient estimation. VRER is algorithm-agnostic and integrates seamlessly with existing policy optimization methods, forming the basis of our sample-efficient off-policy algorithm, Policy Gradient with VRER (PG-VRER). Motivated by the lack of rigorous theoretical analysis of experience replay, we develop a novel framework that explicitly captures dependencies introduced by Markovian dynamics and behavior-policy interactions. Using this framework, we establish finite-time convergence guarantees for PG-VRER and reveal a fundamental bias-variance trade-off: reusing older experience increases bias but simultaneously reduces gradient variance. Extensive empirical experiments demonstrate that VRER consistently accelerates policy learning and improves performance over state-of-the-art policy optimization algorithms.

</details>


### [67] [Optimal Bayesian Stopping for Efficient Inference of Consistent LLM Answers](https://arxiv.org/abs/2602.05395)
*Jingkai Huang,Will Ma,Zhengyuan Zhou*

Main category: stat.ML

TL;DR: 提出一种基于贝叶斯先验的高效停止策略，通过追踪最频繁答案的计数来减少LLM调用次数，在保持准确率的同时可节省高达50%的推理成本


<details>
  <summary>Details</summary>
Motivation: 当前通过采样多个LLM响应并选择最一致答案的方法虽然能提高准确性，但采样成本高昂。需要一种能提前停止采样、节省计算资源的方法

Method: 提出"L-aggregated"停止策略，利用贝叶斯先验信息，仅追踪L-1个最频繁答案的计数。理论上证明L=3即可达到渐进最优，严格优于无先验基线方法

Result: 该方法能用更少样本识别出LLM的最一致答案，在保持相似答案准确率的同时，可将LLM调用次数减少高达50%

Conclusion: 通过贝叶斯先验和高效的L-aggregated停止策略，可以在显著降低LLM推理成本的同时保持答案准确性，为实际应用提供了经济高效的解决方案

Abstract: A simple strategy for improving LLM accuracy, especially in math and reasoning problems, is to sample multiple responses and submit the answer most consistently reached. In this paper we leverage Bayesian prior information to save on sampling costs, stopping once sufficient consistency is reached. Although the exact posterior is computationally intractable, we further introduce an efficient "L-aggregated" stopping policy that tracks only the L-1 most frequent answer counts. Theoretically, we prove that L=3 is all you need: this coarse approximation is sufficient to achieve asymptotic optimality, and strictly dominates prior-free baselines, while having a fast posterior computation. Empirically, this identifies the most consistent (i.e., mode) LLM answer using fewer samples, and can achieve similar answer accuracy while cutting the number of LLM calls (i.e., saving on LLM inference costs) by up to 50%.

</details>


### [68] [Fast Rates for Nonstationary Weighted Risk Minimization](https://arxiv.org/abs/2602.05742)
*Tobias Brock,Thomas Nagler*

Main category: stat.ML

TL;DR: 本文研究了加权经验风险最小化在非平稳分布下的样本外预测误差，提供了超额风险的一般分解，并在混合条件下证明了学习误差的oracle不等式。


<details>
  <summary>Details</summary>
Motivation: 加权经验风险最小化是处理分布漂移的常用方法，但需要理论分析其在非平稳情况下的样本外预测性能。

Method: 将超额风险分解为学习项和分布漂移误差项，在混合条件下证明学习误差的oracle不等式，考虑权重向量、权重类和假设类的复杂性以及数据依赖性。

Result: 学习界在任意权重类上一致成立，并考虑了权重向量诱导的有效样本量。在线性模型、基函数逼近和神经网络等（自）回归问题中展示了结果的适用性和紧致性。

Conclusion: 该理论框架为加权经验风险最小化在非平稳分布下的预测误差提供了统一分析，在特化为未加权和平稳设置时能恢复（对数因子内）极小极大最优速率。

Abstract: Weighted empirical risk minimization is a common approach to prediction under distribution drift. This article studies its out-of-sample prediction error under nonstationarity. We provide a general decomposition of the excess risk into a learning term and an error term associated with distribution drift, and prove oracle inequalities for the learning error under mixing conditions. The learning bound holds uniformly over arbitrary weight classes and accounts for the effective sample size induced by the weight vector, the complexity of the weight and hypothesis classes, and potential data dependence. We illustrate the applicability and sharpness of our results in (auto-) regression problems with linear models, basis approximations, and neural networks, recovering minimax-optimal rates (up to logarithmic factors) when specialized to unweighted and stationary settings.

</details>


### [69] [Optimal scaling laws in learning hierarchical multi-index models](https://arxiv.org/abs/2602.05846)
*Leonardo Defilippis,Florent Krzakala,Bruno Loureiro,Antoine Maillard*

Main category: stat.ML

TL;DR: 该论文为两层神经网络在分层多索引目标上的缩放定律提供了精确的信息论分析，揭示了特征学习的级联相变过程，并证明简单的谱估计器能达到最优速率。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络在分层结构目标上的学习动态，特别是缩放定律、平台现象和谱结构，为浅层神经网络训练提供统一的理论解释。

Method: 采用信息论方法推导子空间恢复和预测误差的精确缩放定律，提出目标无关的谱估计器作为梯度下降在小学习率极限下的近似。

Result: 获得了分层特征通过级联相变顺序学习的最优速率，证明简单谱估计器能达到这些最优速率，一旦表示被识别，读出层可以统计最优地学习。

Conclusion: 为浅层神经网络在分层目标上的缩放定律、平台现象和谱结构提供了统一且严格的理论解释，揭示了特征学习的级联相变机制。

Abstract: In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. We derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. We further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. Once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. As a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets.

</details>


### [70] [Distribution-free two-sample testing with blurred total variation distance](https://arxiv.org/abs/2602.05862)
*Rohan Hore,Rina Foygel Barber*

Main category: stat.ML

TL;DR: 该论文研究了模糊总变差距离，这是总变差距离的一种松弛形式，能够在无分布假设下进行两样本检验和推断。


<details>
  <summary>Details</summary>
Motivation: 传统两样本检验在无分布假设下难以进行，特别是无法提供总变差距离的紧上界。需要一种能够在无分布假设下进行推断的方法。

Method: 提出模糊总变差距离作为总变差距离的松弛形式，提供无分布假设下的理论保证，包括上下界，并研究其在高维空间中的性质。

Result: 建立了模糊总变差距离的无分布上下界理论保证，并分析了其在高维空间中的特性。

Conclusion: 模糊总变差距离为无分布假设下的两样本检验提供了可行的解决方案，能够进行有效的推断和距离估计。

Abstract: Two-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. In particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (TV) distance between the distributions, is impossible to achieve in a distribution-free regime. In this work, we examine the blurred TV distance, a relaxation of TV distance that enables us to perform inference without assumptions on the distributions. We provide theoretical guarantees for distribution-free upper and lower bounds on the blurred TV distance, and examine its properties in high dimensions.

</details>


### [71] [Wedge Sampling: Efficient Tensor Completion with Nearly-Linear Sample Complexity](https://arxiv.org/abs/2602.05869)
*Hengrui Luo,Anna Ma,Ludovic Stephan,Yizhe Zhu*

Main category: stat.ML

TL;DR: 提出Wedge Sampling采样方案用于低秩张量补全，通过结构化采样提升初始化质量，实现多项式时间算法和近线性样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统均匀采样在低秩张量补全中需要大量样本才能实现有效初始化，存在统计与计算之间的差距。本文旨在设计非自适应采样方案，通过强化谱信号来克服这一障碍。

Method: 提出Wedge Sampling方案，将观测分配到关联二分采样图中的结构化长度-2模式（楔形）。这种采样设计直接促进长度-2连接，增强谱信号，从而支持高效初始化。

Result: Wedge Sampling使多项式时间算法能够以近线性样本复杂度实现弱恢复和精确恢复。基于楔形采样的谱初始化可与现有细化方法结合，仅需额外O~(n)均匀采样样本，显著优于均匀采样所需的O~(n^{k/2})样本复杂度。

Conclusion: Barak和Moitra强调的统计与计算差距主要是均匀采样模型的结果，而保证强初始化的替代非自适应测量设计可以克服这一障碍。Wedge Sampling为低秩张量补全提供了有效的采样方案。

Abstract: We introduce Wedge Sampling, a new non-adaptive sampling scheme for low-rank tensor completion. We study recovery of an order-$k$ low-rank tensor of dimension $n \times \cdots \times n$ from a subset of its entries. Unlike the standard uniform entry model (i.e., i.i.d. samples from $[n]^k$), wedge sampling allocates observations to structured length-two patterns (wedges) in an associated bipartite sampling graph. By directly promoting these length-two connections, the sampling design strengthens the spectral signal that underlies efficient initialization, in regimes where uniform sampling is too sparse to generate enough informative correlations.
  Our main result shows that this change in sampling paradigm enables polynomial-time algorithms to achieve both weak and exact recovery with nearly linear sample complexity in $n$. The approach is also plug-and-play: wedge-sampling-based spectral initialization can be combined with existing refinement procedures (e.g., spectral or gradient-based methods) using only an additional $\tilde{O}(n)$ uniformly sampled entries, substantially improving over the $\tilde{O}(n^{k/2})$ sample complexity typically required under uniform entry sampling for efficient methods. Overall, our results suggest that the statistical-to-computational gap highlighted in Barak and Moitra (2022) is, to a large extent, a consequence of the uniform entry sampling model for tensor completion, and that alternative non-adaptive measurement designs that guarantee a strong initialization can overcome this barrier.

</details>


### [72] [Transformers Are Born Biased: Structural Inductive Biases at Random Initialization and Their Practical Consequences](https://arxiv.org/abs/2602.05927)
*Siquan Li,Yao Tong,Haonan Wang,Tianyang Hu*

Main category: stat.ML

TL;DR: 随机初始化的Transformer已存在系统性结构偏置，表现为极端token偏好，由MLP激活不对称性和自注意力机制共同驱动，形成稳定的模型"指纹"。


<details>
  <summary>Details</summary>
Motivation: 挑战传统假设——Transformer在随机初始化时行为无结构，所有有意义偏好都需大规模训练才出现。作者发现随机初始化的Transformer已存在强烈的系统性结构偏置。

Method: 通过分析Transformer架构在初始化时的机制，发现极端token偏好源于token表示沿随机种子依赖方向的收缩。MLP子层的不对称非线性激活导致全局表示集中，自注意力通过局部聚合放大此效应。提出SeedPrint指纹识别方法，并识别注意力机制固有的位置差异。

Result: 随机初始化的Transformer已存在极端token偏好，某些token的预测概率比其他token高几个数量级。这种初始化诱导的偏置在训练中持续存在，形成稳定的模型身份。SeedPrint能可靠区分仅随机种子不同的模型，即使经过大量训练和分布偏移。发现注意力机制固有的位置差异与attention-sink现象因果相关。

Conclusion: Transformer在随机初始化时已具有系统性结构偏置，这种偏置由架构机制驱动并持续存在，形成模型固有身份。这为理解Transformer行为提供了新视角，并为控制attention-sink现象提供了原理性解释。

Abstract: Transformers underpin modern large language models (LLMs) and are commonly assumed to be behaviorally unstructured at random initialization, with all meaningful preferences emerging only through large-scale training. We challenge this assumption by showing that randomly initialized transformers already exhibit strong and systematic structural biases. In particular, untrained models display extreme token preferences: across random input sequences, certain tokens are predicted with probabilities orders of magnitude larger.
  We provide a mechanistic explanation for this phenomenon by dissecting the transformer architecture at initialization. We show that extreme token preference arises from a contraction of token representations along a random seed-dependent direction. This contraction is driven by two interacting forces: (i) asymmetric nonlinear activations in MLP sublayers induce global (inter-sequence) representation concentration, and (ii) self-attention further amplifies this effect through local (intra-sequence) aggregation. Together, these mechanisms align hidden representations along a direction determined solely by the random initialization, producing highly non-uniform next-token predictions.
  Beyond mechanistic insight, we demonstrate that these initialization-induced biases persist throughout training, forming a stable and intrinsic model identity. Leveraging this property, we introduce SeedPrint, a fingerprinting method that can reliably distinguish models that differ only in their random initialization, even after extensive training and under substantial distribution shift. Finally, we identify a fundamental positional discrepancy inherent to the attention mechanism's intra-sequence contraction that is causally linked to the attention-sink phenomenon. This discovery provides a principled explanation for the emergence of sinks and offers a pathway for their control.

</details>


### [73] [Causal Inference on Stopped Random Walks in Online Advertising](https://arxiv.org/abs/2602.05997)
*Jia Yuan Yu*

Main category: stat.ML

TL;DR: 该论文提出了一种在线广告系统中因果推断的新方法，用于估计广告机制参数（如拍卖底价）对长期效果（如年广告收入）的影响，考虑了用户交互轨迹和广告主预算约束的动态变化。


<details>
  <summary>Details</summary>
Motivation: 在线广告系统中，广告机制参数（如拍卖底价）不仅影响即时收入，还会改变用户交互轨迹和广告主投标策略，且受有限预算约束。传统i.i.d.假设不适用，需要新方法来估计长期处理效应。

Method: 采用预算分割实验设计，将实验测量建模为停止随机游走，结合Anscombe定理、Wald-like方程和中心极限定理构建长期处理效应的置信区间。

Result: 开发了一种能够处理动态用户交互和广告主预算约束的因果推断框架，为在线广告系统的长期效果评估提供了统计可靠的置信区间估计方法。

Conclusion: 该方法克服了传统i.i.d.假设的局限性，为在线广告平台评估机制参数对长期收入的影响提供了有效的统计推断工具，特别适用于存在用户轨迹变化和预算约束的动态环境。

Abstract: We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user's interaction-trajectory, and each advertiser's bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.

</details>


### [74] [Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold](https://arxiv.org/abs/2602.06021)
*Ye He,Yitong Qiu,Molei Tao*

Main category: stat.ML

TL;DR: 扩散模型通过"到达-对齐-滑动"过程在log-density ridge流形上生成数据，训练误差影响法向和切向运动，从而决定生成质量


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型如何泛化而非记忆训练数据，量化生成分布以评估下游应用性能

Method: 提出log-density ridge流形概念，量化生成数据与该流形的关系，分析推理动态的"到达-对齐-滑动"过程，通过随机特征模型示例说明归纳偏置来源

Result: 推理过程围绕ridge流形进行：轨迹先到达流形邻域，然后在法向对齐（被推向或推离流形），最后沿切向滑动。训练误差影响法向和切向运动，决定模态间生成的出现

Conclusion: 扩散模型的归纳偏置是架构偏置和训练精度的组合，随推理动态演化。合成多模态分布和MNIST潜在扩散实验支持预测的方向效应

Abstract: When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model's performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model's inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [75] [Optimal Risk-Sharing Rules in Network-based Decentralized Insurance](https://arxiv.org/abs/2602.05155)
*Heather N. Fogarty,Sooie-Hoe Loke,Nicholas F. Marshall,Enrique A. Thomann*

Main category: math.OC

TL;DR: 研究网络中的去中心化风险分担，分析仅限朋友（网络中的直接连接节点）之间进行精算公平风险分担的最优线性规则，并建立与图拉普拉斯算子的联系。


<details>
  <summary>Details</summary>
Motivation: 研究在现实社交网络或组织结构中，风险分担通常发生在直接关联的个体之间，而非全局市场。这种网络约束下的风险分担机制需要理论分析，以理解局部互动如何影响整体风险分配效率。

Method: 建立网络模型，将个体表示为节点，直接连接关系定义为"朋友"。在精算公平假设下，推导仅限朋友间风险分担的最优有符号线性风险分担规则。特别考虑每个体的朋友平均分担其风险的简化情况，建立与图拉普拉斯算子的数学联系。

Result: 推导出网络约束下的最优线性风险分担规则，建立了该规则与图拉普拉斯算子之间的数学对应关系。通过多个示例验证了理论结果，展示了网络结构如何影响风险分配模式。

Conclusion: 网络结构对风险分担有重要影响，仅限朋友间的局部互动能够实现有效的风险分配。最优风险分担规则可以通过图论工具（如图拉普拉斯）来表征，为理解去中心化风险市场提供了理论框架。

Abstract: This paper studies decentralized risk-sharing on networks. In particular, we consider a model where agents are nodes in a given network structure. Agents directly connected by edges in the network are referred to as friends. We study actuarially fair risk-sharing under the assumption that only friends can share risk, and we characterize the optimal signed linear risk-sharing rule in this network setting. Subsequently, we consider a special case of this model where all the friends of an agent take on an equal share of the agent's risk, and establish a connection to the graph Laplacian. Our results are illustrated with several examples.

</details>


### [76] [Branch-and-price strikes back for the k-vertex cut problem](https://arxiv.org/abs/2602.04984)
*Fabio Ciccarelli,Fabio Furini,Christopher Hojny,Marco Lübbecke*

Main category: math.OC

TL;DR: 提出新的整数线性规划公式和分支定价算法解决k-顶点割问题，显著提升求解性能


<details>
  <summary>Details</summary>
Motivation: k-顶点割问题在网络优化中有重要应用（基础设施保护、疫情控制），现有方法性能有限

Method: 提出统一并加强现有模型的新整数线性规划公式，基于此开发分支定价算法，包括定制分支规则、有效不等式和对称处理技术

Result: 新模型在线性松弛上优于所有现有ILP公式，在608个基准实例上显著提升性能，解决了73个之前未解决的实例

Conclusion: 提出的新ILP公式和分支定价算法在理论和计算上都表现出色，为k-顶点割问题提供了有效的解决方案

Abstract: Given an undirected graph, the k-vertex cut problem (k-VCP) asks for a minimum-cost set of vertices whose removal yields at least k connected components in the resulting graph. The k-VCP is an important problem in network optimization, with applications in infrastructure protection and epidemic containment. We present a new extended integer linear programming (ILP) formulation that unifies and strengthens existing models and serves as the foundation for a new branch-and-price algorithm for the k-VCP. An in-depth theoretical study enables us to devise algorithmic components such as tailored branching rules that preserve the structure of the pricing problems, as well as valid inequalities and symmetry-handling techniques. We also show that our new model dominates all previous ILP formulations of the k-VCP in terms of their linear relaxations, which theoretically justifies the computational effectiveness of our approach. Extensive computational experiments against state-of-the-art methods demonstrate substantially improved performance, both in terms of instances solved to proven optimality and running times. On the full benchmark of 608 instances, our algorithm consistently outperforms all competitors and is able to solve 73 previously unsolved instances within the time limit of one hour.

</details>


### [77] [Banach Control Barrier Functions for Large-Scale Swarm Control](https://arxiv.org/abs/2602.05011)
*Xuting Gao,Guillem Pascual,Scott Brown,Sonia Martínez*

Main category: math.OC

TL;DR: 提出基于Banach控制屏障函数的大规模多智能体系统安全控制框架，将大规模群体建模为概率分布，实现宏观约束与群体目标的整合，并推导出分布式局部信息算法。


<details>
  <summary>Details</summary>
Motivation: 研究大规模多智能体系统的安全控制问题，传统方法难以处理超大规模群体，需要新的框架来整合宏观约束与群体目标，并实现分布式局部信息计算。

Method: 使用Banach控制屏障函数，将大规模群体建模为空间域上的概率分布，定义稳定和滤波梯度流，特别关注最优传输算法，推导与宏观对应一致的微观算法。

Result: 建立了大规模群体安全控制的理论框架，识别了分布式局部信息计算的条件，在仿真中展示了理论结果的有效性。

Conclusion: 提出的B-CBF框架能够有效处理大规模多智能体系统的安全控制问题，实现了宏观约束与微观算法的统一，为分布式局部信息计算提供了理论基础。

Abstract: This paper studies the safe control of very large multi-agent systems via a generalized framework that employs so-called Banach Control Barrier Functions (B-CBFs). Modeling a large swarm as probability distribution over a spatial domain, we show how B-CBFs can be used to appropriately capture a variety of macroscopic constraints that can integrate with large-scale swarm objectives. Leveraging this framework, we define stable and filtered gradient flows for large swarms, paying special attention to optimal transport algorithms. Further, we show how to derive agent-level, microscopical algorithms that are consistent with macroscopic counterparts in the large-scale limit. We then identify conditions for which a group of agents can compute a distributed solution that only requires local information from other agents within a communication range. Finally, we showcase the theoretical results over swarm systems in the simulations section.

</details>


### [78] [Decaying Sensitivity of the Zero Solution for a Class of Nonlinear Optimal Control Problems](https://arxiv.org/abs/2602.05020)
*Lars Grüne,Mario Sperl*

Main category: math.OC

TL;DR: 研究图结构交互拓扑非线性最优控制问题中灵敏度的空间衰减特性，证明零参考的局部扰动会导致最优轨迹随图距离指数衰减


<details>
  <summary>Details</summary>
Motivation: 将已知的线性二次系统的空间衰减结果扩展到非线性系统，研究图结构交互拓扑下非线性最优控制问题的灵敏度衰减特性

Method: 基于非线性可控性条件进行分析，考虑具有非线性解耦动力学和二次成本的最优控制问题，研究局部扰动对最优轨迹的影响

Result: 证明零参考的局部扰动会导致最优轨迹随图距离指数衰减，数值示例验证了理论发现

Conclusion: 该分析为将已知的空间衰减结果从线性二次系统扩展到非线性系统提供了第一步，展示了图结构交互拓扑下非线性最优控制问题的灵敏度衰减特性

Abstract: We study spatial decay properties of sensitivities in a nonlinear optimal control problem with a graph--structured interaction topology. For a problem with nonlinear decoupled dynamics and quadratic cost, we show that a localized perturbation of the zero reference leads to an optimal trajectory that decays exponentially with the graph distance. The analysis, based on a nonlinear controllability condition, provides a first step toward extending known spatial decay results from linear--quadratic to nonlinear systems. A numerical example illustrates the theoretical findings.

</details>


### [79] [Approximation of Singular-Stopping Control Driven by Hawkes Processes via Rescaled MDPs](https://arxiv.org/abs/2602.05025)
*Isabel Agostino,Thibaut Mastrolia*

Main category: math.OC

TL;DR: 研究自激Hawkes过程驱动的奇异最优停止随机控制问题，建立连续与离散时间模型的联系，应用于受网络威胁的发电厂投资优化


<details>
  <summary>Details</summary>
Motivation: 研究自激Hawkes过程驱动的奇异最优停止随机控制问题，特别关注受网络威胁的发电厂投资优化等实际应用场景

Method: 1) 在连续时间设置中，将优化问题转化为带梯度约束的变分偏微分方程求解；2) 引入离散时间马尔可夫决策过程模型；3) 证明在适当缩放下，离散时间价值函数收敛于连续时间等价函数

Result: 证明了离散时间优化器对连续时间问题是渐近最优的，并将理论结果应用于Hawkes过程驱动的Ornstein-Uhlenbeck随机微分方程，通过数值模拟验证理论发现

Conclusion: 建立了连续与离散时间奇异最优停止问题的理论联系，为自激动态驱动的随机控制问题提供了有效的数值计算方法，在网络安全威胁下的发电厂投资等实际应用中具有重要价值

Abstract: We investigate a singular-optimal stopping stochastic control problem driven by self-exciting dynamics governed by a Hawkes process. In the continuous-time setting, we show that the optimization problem reduces to solving a variational partial differential equation with gradient constraints. We then introduce its discrete-time counterpart, modeled as a Markov Decision Process. We prove that, under an appropriate rescaling procedure, the value function of the discrete-time problem converges to its continuous-time equivalent, implying that the discrete-time optimizers are asymptotically optimal for the continuous-time problem. Finally, we apply these results to an Ornstein-Uhlenbeck stochastic differential equation driven by a Hawkes process with singular control, motivated by optimal power plant investment under cyber threat and we illustrate the theoretical findings through numerical simulations.

</details>


### [80] [Safe Optimal Control using Log Barrier Constrained iLQR](https://arxiv.org/abs/2602.05046)
*Abhijeet,Suman Chakravorty*

Main category: math.OC

TL;DR: 提出一种带约束的迭代线性二次调节器(iLQR)框架，通过引入对数障碍函数处理状态和控制输入的箱型约束，形成平滑的内点法公式，保持数值稳定性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统非线性最优控制问题中处理箱型约束（状态和控制输入的上下界）通常需要特殊处理，现有方法可能缺乏数值稳定性或收敛性保证。需要一种能无缝集成到标准iLQR框架中的约束处理方法。

Method: 在阶段成本中引入对数障碍函数来强制箱型约束，形成平滑的内点法公式。该方法与标准iLQR的前向-后向传递无缝集成，障碍函数的Hessian矩阵正定性增强了二次近似的正定性，提供内在正则化效果。

Result: 数值实验表明，该方法能可靠地满足箱型约束，并保持良好的收敛特性。在约束边界附近，反馈增益会自然调整，当控制饱和时，相关反馈项趋于零，恢复纯前馈行为。

Conclusion: 提出的带障碍函数的iLQR框架为非线性最优控制问题中的箱型约束提供了一种有效、数值稳定且收敛性良好的解决方案，特别适合处理控制饱和情况。

Abstract: This paper presents a constrained iterative Linear Quadratic Regulator (iLQR) framework for nonlinear optimal control problems with box constraints on both states and control inputs. We incorporate logarithmic barrier functions into the stage cost to enforce box constraints (upper and lower bounds on variables), yielding a smooth interior-point formulation that integrates seamlessly with the standard iLQR backward-forward pass. The Hessian contributions from the log barriers are positive definite, preserving and enhancing the positive definiteness of the quadratic approximations in iLQR and providing an intrinsic regularization effect that improves numerical stability and convergence. Moreover, since the negative logarithm is convex, the addition of log barrier terms preserves convexity if the cost is already convex. We further analyze how the barrier-augmented iLQR naturally adapts feedback gains near constraint boundaries. In particular, at convergence, the feedback terms associated with saturated control channels go to zero, recovering a purely feedforward behavior whenever control is saturated. Numerical examples on constrained nonlinear control problems demonstrate that the proposed method reliably respects box constraints and maintains favorable convergence properties.

</details>


### [81] [An Adaptive Framework for Robust Structural Shape Optimization under Uncertainty](https://arxiv.org/abs/2602.05054)
*Oğuz Han Altıntaş,Hamdullah Yücel*

Main category: math.OC

TL;DR: 提出自适应框架解决考虑载荷和材料不确定性的鲁棒结构形状优化问题，通过后验误差估计动态调整样本量、网格尺寸和步长


<details>
  <summary>Details</summary>
Motivation: 解决线性弹性模型下结构形状优化问题中的不确定性（载荷和材料输入），需要高效处理随机梯度近似和离散化误差

Method: 构建后验误差估计器动态调整样本集大小、网格尺寸和步长；基于形状导数方差确定样本量；考虑变形双线性形式和线性弹性系统的离散化误差；自适应调整步长估计随机形状导数的Lipschitz常数

Result: 在腿状结构组件上验证了框架有效性，成功最小化不确定接触力下的着陆柔度

Conclusion: 提出的基于估计的自适应随机优化框架能有效处理结构形状优化中的不确定性，通过动态调整参数提高计算效率和优化效果

Abstract: This work proposes an adaptive framework to solve a robust structural shape optimization problem governed by linear elasticity models that account for uncertainties in the loading and material inputs. A posteriori error estimators are constructed to adjust the sample size, mesh size, and step length. The size of the sample set in the stochastic gradient approximation is dynamically determined depending on the variance of the shape derivative. When constructing the a posteriori error estimator in the physical domain, errors arising from the discretization of the deformation bilinear form, which provides a descent direction, are considered, in addition to errors from the discretization of the linear elasticity system. The step length in gradient-based optimization is also adaptively adjusted by estimating the Lipschitz constant of the stochastic shape derivative. Moreover, an analysis of the existence and distributed-form derivation of the stochastic shape derivative is provided. Finally, the proposed estimation-based adaptive stochastic optimization framework is validated on leg-like structural components, demonstrating its effectiveness in minimizing touchdown compliance under uncertain contact forces.

</details>


### [82] [From Sequential to Parallel: Reformulating Dynamic Programming as GPU Kernels for Large-Scale Stochastic Combinatorial Optimization](https://arxiv.org/abs/2602.05179)
*Jingyi Zhao,Linxin Yang,Haohua Zhang,Tian Ding*

Main category: math.OC

TL;DR: GPU加速的SAA框架，通过场景批处理实现大规模整数第二阶段模型的并行计算，在车辆路径和库存问题中获得4-5个数量级加速。


<details>
  <summary>Details</summary>
Motivation: 传统SAA方法中，为每个场景精确求解NP-hard组合优化第二阶段问题计算成本过高，导致文献通常限制第二阶段为线性或简化模型，无法处理大规模整数优化问题。

Method: 开发基于GPU的框架，采用硬件感知的场景批处理内核，在场景、动态规划层、路径/动作选项三个维度上并行化，实现单次遍历超过100万个场景的Bellman更新。

Result: 在随机车辆路径和库存重插入两个代表性SP问题中，实现接近线性的场景扩展性，获得1-2到4-5个数量级的加速，能够处理更大场景集和更多第一阶段候选解。

Conclusion: 全保真度整数第二阶段模型在先前认为不可能的规模下变得可行，为大规模现实随机离散优化提供了实用路径，计算优势直接转化为决策质量提升。

Abstract: A major bottleneck in scenario-based Sample Average Approximation (SAA) for stochastic programming (SP) is the cost of solving an exact second-stage problem for every scenario, especially when each scenario contains an NP-hard combinatorial structure. This has led much of the SP literature to restrict the second stage to linear or simplified models. We develop a GPU-based framework that makes full-fidelity integer second-stage models tractable at scale. The key innovation is a set of hardware-aware, scenario-batched GPU kernels that expose parallelism across scenarios, dynamic-programming (DP) layers, and route or action options, enabling Bellman updates to be executed in a single pass over more than 1,000,000 realizations. We evaluate the approach in two representative SP settings: a vectorized split operator for stochastic vehicle routing and a DP for inventory reinsertion. Implementation scales nearly linearly in the number of scenarios and achieves a one-two to four-five orders of magnitude speedup, allowing far larger scenario sets and reliably stronger first-stage decisions. The computational leverage directly improves decision quality: much larger scenario sets and many more first-stage candidates can be evaluated within fixed time budgets, consistently yielding stronger SAA solutions. Our results show that full-fidelity integer second-stage models are tractable at scales previously considered impossible, providing a practical path to large-scale, realistic stochastic discrete optimization.

</details>


### [83] [Inverse Optimization Without Inverse Optimization: Direct Solution Prediction with Transformer Models](https://arxiv.org/abs/2602.05306)
*Macarena Navarro,Willem-Jan van Hoeve,Karan Singh*

Main category: math.OC

TL;DR: 提出基于Transformer的端到端框架，用于解决包含未知组件的组合优化问题，通过学习历史解并整合已知约束，生成满足约束且结构相似的新解。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题中常存在未知组件（如未知目标函数、约束等），传统方法难以处理。需要一种能够学习历史解模式、同时保证已知约束的方法。

Method: 使用Transformer序列到序列神经网络构建端到端框架，通过约束推理模块整合已知硬约束，形成约束学习方案。模型直接从历史解中学习，生成满足约束的新解。

Result: 在三个组合优化问题上验证：未知奖励函数的背包问题、未知目标函数的二分图匹配问题、未知优先约束的单机调度问题。Transformer模型表现出色，能在秒级内生成接近最优解，在复杂目标函数和未知隐约束下优于LSTM和逆优化方法。

Conclusion: Transformer模型在解决含未知组件的组合优化问题上具有强大性能，能够快速生成高质量解，特别适合处理复杂目标函数和未知约束的场景。

Abstract: We present an end-to-end framework for generating solutions to combinatorial optimization problems with unknown components using transformer-based sequence-to-sequence neural networks. Our framework learns directly from past solutions and incorporates the known components, such as hard constraints, via a constraint reasoning module, yielding a constrained learning scheme. The trained model generates new solutions that are structurally similar to past solutions and are guaranteed to respect the known constraints. We apply our approach to three combinatorial optimization problems with unknown components: the knapsack problem with an unknown reward function, the bipartite matching problem with an unknown objective function, and the single-machine scheduling problem with release times and unknown precedence constraints, with the objective of minimizing average completion time. We demonstrate that transformer models have remarkably strong performance and often produce near-optimal solutions in a fraction of a second. They can be particularly effective in the presence of more complex underlying objective functions and unknown implicit constraints compared to an LSTM-based alternative and inverse optimization.

</details>


### [84] [Twice Epi-Differentiability of Spectral Functions and its applications](https://arxiv.org/abs/2602.05357)
*Chao Ding,Ebrahim Sarabi,Shiwei Wang*

Main category: math.OC

TL;DR: 本文研究了谱函数的二阶变分性质，特别是二次上微分性质，通过谱表示中的对称部分来刻画该性质，避免了传统方法中的凸性限制，并应用于次梯度映射的原微分性和邻近算子的方向可微性。


<details>
  <summary>Details</summary>
Motivation: 二阶变分性质在优化问题的理论和数值分析中具有重要作用，其中二次上微分性质在各类扩展实值函数中普遍存在。然而，现有研究通常需要较强的凸性假设，限制了谱函数二阶性质的研究。

Method: 通过谱函数的特征值函数表示，证明二次上微分性质可以通过谱表示中对称部分的相同性质来刻画。这种方法绕过了传统方法中需要的凸性假设，为谱函数的二阶变分分析提供了新工具。

Result: 建立了谱函数二次上微分性的新刻画方法，应用该理论得到了次梯度映射的原微分性和邻近算子方向可微性的结果，并具体研究了主特征值函数和统计、鲁棒PCA中重要正则化项的二次上微分性。

Conclusion: 本文提出的方法突破了谱函数二阶变分分析中的凸性限制，为谱函数的二阶性质研究提供了有效工具，在优化理论和应用中有重要意义，特别是在统计和鲁棒PCA等领域的正则化问题中。

Abstract: Second-order variational properties have been shown to play important theoretical and numerical roles for different classes of optimization problems. Among such properties, twice epi-differentiability has a special place because of its ubiquitous presence in various classes of extended-real-valued functions that are important for optimization problems. We provide a useful characterization of this property for spectral functions by demonstrating that it can be characterized via the same property of the symmetric part of the spectral representation of an eigenvalue function. Our approach allows us to bypass the rather restrictive convexity assumption, used in many recent works that targeted second-order variational properties of spectral functions. By this theoretical tool, several applications on the proto-differentiability of subgradient mappings, the directional differentiability of the proximal mapping of spectral functions are achieved. We finally use our established theory to study twice epi-differentiability of leading eigenvalue functions and practical regularization terms that have important applications in statistics and the robust PCA.

</details>


### [85] [Relationship between MP and DPP for Risk-Sensitive Stochastic Optimal Control Problems: Viscosity Solution Framework](https://arxiv.org/abs/2602.05361)
*Huanqing Dong,Jingtao Shi*

Main category: math.OC

TL;DR: 研究风险敏感随机最优控制问题中一般最大值原理与动态规划原理的关系，控制域不必凸，通过前向-后向系统等价转换，建立伴随过程、广义哈密顿函数与值函数在粘性解框架下的联系。


<details>
  <summary>Details</summary>
Motivation: 研究非凸控制域下风险敏感随机最优控制问题中一般最大值原理与动态规划原理的关系，建立两种方法之间的理论联系，为这类问题的求解提供统一的理论框架。

Method: 将原问题等价转换为具有二次生成元的前向-后向系统的随机递归最优控制问题，在粘性解框架下证明伴随过程、广义哈密顿函数与值函数之间的关系。

Result: 建立了风险敏感随机最优控制问题中一般最大值原理与动态规划原理的理论联系，证明了伴随过程、广义哈密顿函数与值函数在粘性解框架下的关系，并通过示例验证了理论结果。

Conclusion: 成功建立了非凸控制域下风险敏感随机最优控制问题中一般最大值原理与动态规划原理的理论联系，为这类问题的求解提供了统一的理论框架和粘性解方法。

Abstract: In this paper, we study the relationship between general maximum principle and dynamic programming principle for risk-sensitive stochastic optimal control problems, where the control domain is not necessarily convex. The original problem is equivalent to a stochastic recursive optimal control problem of a forward-backward system with quadratic generators. Relations among the adjoint processes, the generalized Hamiltonian function and the value function are proved under the framework of viscosity solutions. Some examples are given to illustrate the theoretical results.

</details>


### [86] [Hybrid Quantum-Classical Optimization for Multi-Objective Supply Chain Logistics](https://arxiv.org/abs/2602.05364)
*Raoul Heese,Timothée Leleu,Sam Reifenstein,Christian Nietner,Yoshihisa Yamamoto*

Main category: math.OC

TL;DR: 将现实供应链中的多目标物流优化问题建模为QUBO，最小化成本、排放和交付时间，同时保持供应商工作份额的目标分布，并提出了两种混合量子经典求解器。


<details>
  <summary>Details</summary>
Motivation: 现实世界供应链物流优化涉及多个相互冲突的目标（成本、排放、时间），且包含复杂约束（零件依赖、双重采购、多式联运），传统方法难以有效求解，需要探索量子计算等新兴硬件解决方案。

Method: 1. 将多目标物流优化问题建模为二次无约束二进制优化问题（QUBO）；2. 提出两种混合量子经典求解器：结构感知的知情树搜索（IQTS）和模块化双层框架（HBS），将量子子程序与经典启发式算法结合；3. 在IonQ的Aria-1量子硬件上进行实验验证。

Result: 在IonQ Aria-1硬件上成功演示了将现实物流问题映射到专用组合优化硬件的方法，获得了高质量的帕累托最优解，验证了混合量子经典方法在复杂物流优化中的可行性。

Conclusion: 该研究展示了如何将现实世界物流优化问题有效映射到新兴的量子硬件，提出的混合量子经典框架为解决复杂供应链优化问题提供了有前景的途径，为量子计算在物流领域的实际应用奠定了基础。

Abstract: A multi-objective logistics optimization problem from a real-world supply chain is formulated as a Quadratic Unconstrained Binary Optimization Problem (QUBO) that minimizes cost, emissions, and delivery time, while maintaining target distributions of supplier workshare. The model incorporates realistic constraints, including part dependencies, double sourcing, and multimodal transport. Two hybrid quantum-classical solvers are proposed: a structure-aware informed tree search (IQTS) and a modular bilevel framework (HBS), combining quantum subroutines with classical heuristics. Experimental results on IonQ's Aria-1 hardware demonstrate a methodology to map real-world logistics problems onto emerging combinatorial optimization-specialized hardware, yielding high-quality, Pareto-optimal solutions.

</details>


### [87] [Distributed Model Predictive Control for Energy and Comfort Optimization in Large Buildings Using Piecewise Affine Approximation](https://arxiv.org/abs/2602.05376)
*Hongyi Li,Jun Xu,Jinfeng Liu*

Main category: math.OC

TL;DR: 提出基于分段仿射的分布式模型预测控制方案，通过ADMM分解和凸化处理非线性大规模建筑控制问题，计算效率提升86%


<details>
  <summary>Details</summary>
Motivation: 大型建筑控制面临计算效率挑战，主要由于建筑规模大且包含非线性组件，需要高效的控制方案来优化能耗和舒适度

Method: 采用分段仿射(PWA)分布式MPC方案，结合ADMM进行问题分解，将非线性问题转化为一系列凸优化子问题，提出凸ADMM算法

Result: 在36个区域的案例研究中，所提方法相比集中式版本减少了86%的执行时间，并证明算法收敛到原问题的局部最优解

Conclusion: 基于PWA的分布式MPC方案能有效处理大规模非线性建筑控制问题，显著提高计算效率，为实际应用提供了可行方案

Abstract: The control of large buildings encounters challenges in computational efficiency due to their size and nonlinear components. To address these issues, this paper proposes a Piecewise Affine (PWA)-based distributed scheme for Model Predictive Control (MPC) that optimizes energy and comfort through PWA-based quadratic programming. We utilize the Alternating Direction Method of Multipliers (ADMM) for effective decomposition and apply the PWA technique to handle the nonlinear components. To solve the resulting large-scale nonconvex problems, the paper introduces a convex ADMM algorithm that transforms the nonconvex problem into a series of smaller convex problems, significantly enhancing computational efficiency. Furthermore, we demonstrate that the convex ADMM algorithm converges to a local optimum of the original problem. A case study involving 36 zones validates the effectiveness of the proposed method. Our proposed method reduces execution time by 86\% compared to the centralized version.

</details>


### [88] [Optimistic Bilevel Optimization with Composite Lower-Level Problem](https://arxiv.org/abs/2602.05417)
*Mattia Solla,Johannes O. Royset*

Main category: math.OC

TL;DR: 提出双层正则化方案解决非强凸复合凸双层优化问题，推导正则化超目标梯度公式，设计梯度采样算法并证明收敛性


<details>
  <summary>Details</summary>
Motivation: 解决下层问题为复合凸但不一定强凸的双层优化问题，传统方法难以处理非强凸情况，需要新的正则化方案和收敛分析

Method: 提出双重正则化方案，分析正则化下层问题的原始-对偶解映射性质，推导正则化超目标梯度公式，设计基于梯度采样的算法

Result: 在温和假设下得到正则化超目标梯度几乎处处公式，证明实际超目标梯度可由正则化版本近似，算法收敛到实际问题的驻点

Conclusion: 提出的双重正则化方案能有效处理非强凸双层优化问题，理论分析和算法设计为这类问题提供了新解决方案

Abstract: This paper introduces a novel double regularization scheme for bilevel optimization problems whose lower-level problem is composite and convex, but not necessarily strongly convex, in the lower-level variable. The analysis focuses on the primal-dual solution mapping of the regularized lower-level problem and exploits its properties to derive an almost-everywhere formula for the gradient of the regularized hyper-objective under mild assumptions. The paper then establishes conditions under which the hyper-objective of the actual problem is well defined and shows that its gradient can be approximated by the gradient of the regularized hyper-objective. Building on these results, a gradient sampling-based algorithm computes approximately stationary points of the regularized hyper-objective, and we prove its convergence to stationary points of the actual problem. Two numerical examples from machine learning demonstrate the proposed approach.

</details>


### [89] [Relaxation in infinite convex programming under Slater-type regularity conditions](https://arxiv.org/abs/2602.05457)
*Rafael Correa,Abderrahim Hantoute,Marco A. López*

Main category: math.OC

TL;DR: 本文证明了在Slater条件和连续性条件下，无限凸规划的最优值与其双共轭松弛的最优值之间的对偶间隙为零，并应用该结果于对偶理论。


<details>
  <summary>Details</summary>
Motivation: 解决无限凸规划最优值与其双共轭松弛最优值之间的对偶间隙问题，建立零对偶间隙的充分条件。

Method: 利用共轭和双共轭运算的求和与逐点上确界运算的微积分规则，在Slater条件和连续性条件下证明零对偶间隙。

Result: 证明了在适当的Slater条件和连续性条件下，无限凸规划与其双共轭松弛之间存在零对偶间隙。

Conclusion: 建立了无限凸规划零对偶间隙的理论框架，并将结果应用于对偶理论，为凸优化问题提供了新的理论工具。

Abstract: The main purpose of this paper is to close the gap between the optimal values of an infinite convex program and that of its biconjugate relaxation. It is shown that Slater and continuity-type conditions guarantee such a zero-duality gap. The approach uses calculus rules for the conjugation and biconjugation of the sum and pointwise supremum operations. A second important objective of this work is to exploit these results on relaxation by applying them in the context of duality theory.

</details>


### [90] [Convergence Rate of the Last Iterate of Stochastic Proximal Algorithms](https://arxiv.org/abs/2602.05489)
*Kevin Kurian Thomas Vaidyan,Michael P. Friedlander,Ahmet Alacaoglu*

Main category: math.OC

TL;DR: 论文分析了两种解决加性复合凸优化问题的经典算法：近端随机梯度法和随机增量近端法。主要贡献是放宽了传统分析中严格的方差有界假设，在分量凸性和光滑性条件下证明了两种算法末点迭代的$\widetilde{O}(1/\sqrt{T})$收敛速率。


<details>
  <summary>Details</summary>
Motivation: 传统随机优化算法分析通常需要方差有界假设，这一假设在实际应用中往往过于严格。特别是在多任务学习和联邦学习等场景中，正则化项通常表示为协作图边上的和，传统假设难以满足。因此需要更宽松的分析框架来保证算法收敛性。

Method: 分析了两种算法：1) 近端随机梯度法（针对单个正则化项）；2) 随机增量近端法（针对正则化项为多个非光滑函数之和的情况）。在分量凸性和光滑性条件下，放宽了方差有界假设，采用新的分析技术证明末点迭代收敛性。

Result: 证明了两种算法末点迭代的$\widetilde{O}(1/\sqrt{T})$收敛速率，该速率在忽略对数项后是最优的。结果可直接应用于多任务学习和联邦学习中的图引导正则化器，其中正则化项可分解为协作图边的和。

Conclusion: 论文成功放宽了传统随机优化分析中的方差有界假设，在更宽松的条件下证明了经典算法的末点迭代收敛性。这一理论进展使得分析结果能够直接应用于实际的多任务学习和联邦学习场景，为这些领域的算法设计提供了理论保障。

Abstract: We analyze two classical algorithms for solving additively composite convex optimization problems where the objective is the sum of a smooth term and a nonsmooth regularizer: proximal stochastic gradient method for a single regularizer; and the randomized incremental proximal method, which uses the proximal operator of a randomly selected function when the regularizer is given as the sum of many nonsmooth functions. We focus on relaxing the bounded variance assumption that is common, yet stringent, for getting last iterate convergence rates. We prove the $\widetilde{O}(1/\sqrt{T})$ rate of convergence for the last iterate of both algorithms under componentwise convexity and smoothness, which is optimal up to log terms. Our results apply directly to graph-guided regularizers that arise in multi-task and federated learning, where the regularizer decomposes as a sum over edges of a collaboration graph.

</details>


### [91] [Continuized Nesterov Momentum Achieves the $O(\varepsilon^{-7/4})$ Complexity without Additional Mechanisms](https://arxiv.org/abs/2602.05504)
*Julien Hermant,Jean-François Aujol,Charles Dossal,Lorick Huang,Aude Rondepierre*

Main category: math.OC

TL;DR: 本文证明了仅使用随机参数的Nesterov动量算法就能达到非凸优化问题的最优复杂度O(ε^{-7/4})，无需额外的安全机制。


<details>
  <summary>Details</summary>
Motivation: 现有达到最优复杂度的算法都依赖于动量结合安全机制（如重启或负曲率利用），但这是否必要一直是个开放问题。本文旨在探索仅使用随机参数的动量算法能否达到相同复杂度。

Method: 采用连续化方法，设计一个带有随机参数的Nesterov动量算法，参数满足单位期望的乘法随机因子，并限制在实现在某个子集内。

Result: 理论证明该随机参数动量算法在期望意义下能达到O(ε^{-7/4})的最优复杂度，且随机因子和实现限制与目标函数无关。实验验证这些限制是温和的。

Conclusion: 动量算法的安全机制并非必要，仅通过随机参数就能达到最优复杂度，这为非凸优化算法设计提供了新思路。

Abstract: For first-order optimization of non-convex functions with Lipschitz continuous gradient and Hessian, the best known complexity for reaching an $\varepsilon$-approximation of a stationary point is $O(\varepsilon^{-7/4})$. Existing algorithms achieving this bound are based on momentum, but are always complemented with safeguard mechanisms, such as restarts or negative-curvature exploitation steps. Whether such mechanisms are fundamentally necessary has remained an open question. Leveraging the continuized method, we show that a Nesterov momentum algorithm with stochastic parameters alone achieves the same complexity in expectation. This result holds up to a multiplicative stochastic factor with unit expectation and a restriction to a subset of the realizations, both of which are independent of the objective function. We empirically verify that these constitute mild limitations.

</details>


### [92] [Solving Stochastic Variational Inequalities without the Bounded Variance Assumption](https://arxiv.org/abs/2602.05531)
*Ahmet Alacaoglu,Jun-Hyun Kim*

Main category: math.OC

TL;DR: 本文分析了无界方差或无界域假设下随机变分不等式（VI）的求解算法，主要关注可能具有无界约束集的min-max优化问题。针对单调VI和具有弱Minty VI解的结构化非单调VI两类问题，证明了期望残差范数小于ε时的oracle复杂度为$\widetilde{O}(\varepsilon^{-4})$，这是约束VI问题中已知的最佳复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有随机变分不等式算法通常需要方差有界或定义域有界的假设，但这些假设在无界域的双线性min-max问题中甚至不成立。本文旨在消除这些限制性假设，研究方差可以随优化变量范数平方增长的随机oracle下的算法复杂度。

Method: 针对两类变分不等式问题：1）单调VI；2）具有弱Minty VI解的结构化非单调VI（可解决结构化非凸-非凹min-max问题）。在方差可以随优化变量范数平方增长的随机oracle设置下，分析算法复杂度。

Result: 证明了对于两类VI问题，要使期望残差范数小于ε，oracle复杂度为$\widetilde{O}(\varepsilon^{-4})$，这是约束VI问题中已知的最佳复杂度。这一结果在无界方差假设下仍然成立，而先前文献中仅在方差有界假设下获得相同复杂度。

Conclusion: 本文在更宽松的条件下（无界方差、无界域）获得了随机变分不等式求解的最佳已知复杂度$\widetilde{O}(\varepsilon^{-4})$，特别适用于无界域的双线性min-max问题，扩展了现有理论框架的适用范围。

Abstract: We analyze algorithms for solving stochastic variational inequalities (VI) without the bounded variance or bounded domain assumptions, where our main focus is min-max optimization with possibly unbounded constraint sets. We focus on two classes of problems: monotone VIs; and structured nonmonotone VIs that admit a solution to the weak Minty VI. The latter assumption allows us to solve structured nonconvex-nonconcave min-max problems. For both classes of VIs, to make the expected residual norm less than $\varepsilon$, we show an oracle complexity of $\widetilde{O}(\varepsilon^{-4})$, which is the best-known for constrained VIs. In our setting, this complexity had been obtained with the bounded variance assumption in the literature, which is not even satisfied for bilinear min-max problems with an unbounded domain. We obtain this complexity for stochastic oracles whose variance can grow as fast as the squared norm of the optimization variable.

</details>


### [93] [Normalization of ReLU Dual for Cut Generation in Stochastic Mixed-Integer Programs](https://arxiv.org/abs/2602.05974)
*Akul Bansal,Simge Küçükyavuz*

Main category: math.OC

TL;DR: 提出对ReLU对偶进行归一化处理，以解决多重最优解导致的弱割问题，证明归一化割在原始状态空间中是紧且帕累托最优的，计算实验显示该方法优于现有方法


<details>
  <summary>Details</summary>
Motivation: ReLU对偶公式虽然能保证收敛并生成紧的、非凸的、混合整数可表示的割，但存在多重最优解问题，这些解可能产生弱割，影响求解效率

Method: 在扩展空间中对ReLU对偶进行归一化处理，以识别能产生更强割的解。证明归一化割在原始状态空间中是紧且帕累托最优的，并与现有的正则化方法进行比较

Result: 归一化方法能恢复正则化方法获得的所有割，反之则不然。计算实验表明，该方法能持续产生更强的割，并在困难实例上减少求解时间

Conclusion: 提出的归一化方法能有效解决ReLU对偶的多重最优解问题，生成更强的割，优于现有的正则化方法，提高了求解效率

Abstract: We study the Rectified Linear Unit (ReLU) dual, an existing dual formulation for stochastic programs that reformulates non-anticipativity constraints using ReLU functions to generate tight, non-convex, and mixed-integer representable cuts. While this dual reformulation guarantees convergence with mixed-integer state variables, it admits multiple optimal solutions that can yield weak cuts. To address this issue, we propose normalizing the dual in the extended space to identify solutions that yield stronger cuts. We prove that the resulting normalized cuts are tight and Pareto-optimal in the original state space. We further compare normalization with existing regularization-based approaches for handling dual degeneracy and explain why normalization offers key advantages. In particular, we show that normalization can recover any cut obtained via regularization, whereas the converse does not hold. Computational experiments demonstrate that the proposed approach outperforms existing methods by consistently yielding stronger cuts and reducing solution times on harder instances.

</details>


### [94] [Efficient Algorithms for Robust Markov Decision Processes with $s$-Rectangular Ambiguity Sets](https://arxiv.org/abs/2602.05591)
*Chin Pang Ho,Marek Petrik,Wolfram Wiesemann*

Main category: math.OC

TL;DR: 该论文提出了一个统一的求解框架，用于解决具有s-矩形模糊集的鲁棒马尔可夫决策过程，相比现有商业求解器可将求解速度提升数个数量级。


<details>
  <summary>Details</summary>
Motivation: 鲁棒MDP能够保护系统在存在模糊性时免受样本外性能下降的影响，但现有求解方法效率较低。需要开发更高效的算法来求解具有s-矩形模糊集的鲁棒MDP问题。

Method: 开发了一个统一的求解框架，专门针对s-矩形模糊集的鲁棒MDP，其中每个状态的最不利转移概率被独立考虑。该框架支持1-范数、2-范数和φ-散度等多种模糊集。

Result: 提出的算法比最先进的商业求解器快数个数量级，通常只比经典MDP慢一个对数因子。在合成数据和标准基准实例上都展示了良好的扩展性。

Conclusion: 该研究为s-矩形鲁棒MDP提供了一个高效统一的求解框架，显著提升了计算效率，使得鲁棒MDP在实际应用中更加可行。

Abstract: Robust Markov decision processes (MDPs) have attracted significant interest due to their ability to protect MDPs from poor out-of-sample performance in the presence of ambiguity. In contrast to classical MDPs, which account for stochasticity by modeling the dynamics through a stochastic process with a known transition kernel, a robust MDP additionally accounts for ambiguity by optimizing against the most adverse transition kernel from an ambiguity set constructed via historical data. In this paper, we develop a unified solution framework for a broad class of robust MDPs with $s$-rectangular ambiguity sets, where the most adverse transition probabilities are considered independently for each state. Using our algorithms, we show that $s$-rectangular robust MDPs with $1$- and $2$-norm as well as $φ$-divergence ambiguity sets can be solved several orders of magnitude faster than with state-of-the-art commercial solvers, and often only a logarithmic factor slower than classical MDPs. We demonstrate the favorable scaling properties of our algorithms on a range of synthetically generated as well as standard benchmark instances.

</details>


### [95] [Nonsmooth Optimization with Zeroth Order Comparison Feedback](https://arxiv.org/abs/2602.05622)
*Taha El Bakkali,El Mahdi Chayti,Omar Saadi*

Main category: math.OC

TL;DR: 提出一种基于噪声成对比较的无偏估计方法，用于非光滑非凸Lipschitz函数的无约束优化，计算(δ,ε)-Goldstein平稳点


<details>
  <summary>Details</summary>
Motivation: 研究仅通过已知链接函数控制的噪声成对比较来优化非光滑、非凸Lipschitz函数的问题，目标是计算Goldstein平稳点

Method: 结合随机平滑与从比较到局部值差的无偏约简，利用俄罗斯轮盘截断伯努利积展开构造方向差的无偏估计器，通过平滑梯度恒等式进行非凸SGD分析

Result: 构建了具有有限期望成本和方差随函数间隙平方O(B²)缩放的无偏估计器，为logistic、probit、cauchit等常见对称链接函数提供了明确的比较复杂度界限

Conclusion: 该方法能够有效利用噪声成对比较信息进行非光滑非凸优化，为基于比较的优化问题提供了理论保证和实用算法框架

Abstract: We study unconstrained optimization problems of nonsmooth, nonconvex Lipschitz functions, using only noisy pairwise comparisons governed by a known link function. Our goal is to compute a $(δ,\varepsilon)$-Goldstein stationary point. We combine randomized smoothing with a novel unbiased reduction from comparisons to local value differences. By leveraging a Russian-roulette truncation on the Bernoulli-product expansion of the inverse link, we construct an exactly unbiased estimator for directional differences. This estimator has finite expected cost and variance scaling quadratically with the function gap, $\mathcal{O}(B^2)$, under mild conditions. Plugging this into the smoothed gradient identity enables a standard nonconvex SGD analysis, yielding explicit comparison-complexity bounds for common symmetric links such as logistic, probit, and cauchit.

</details>


### [96] [A Smooth Locally Exact Penalty Method for Optimization Problems over Generalized Stiefel Manifolds](https://arxiv.org/abs/2602.05631)
*Linshuo Jiang,Nachuan Xiao,Xin Liu*

Main category: math.OC

TL;DR: 提出SLEP惩罚模型解决广义Stiefel流形上的优化问题，特别是针对奇异M矩阵的情况，避免了黎曼优化中的复杂几何操作，降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对称正定矩阵M诱导的广义正交约束，这种情形本质上可简化为标准Stiefel流形。然而许多实际应用涉及奇异M矩阵，这会带来显著的分析和计算挑战。

Method: 提出平滑局部精确惩罚模型(SLEP)，在有限大的惩罚参数下，该模型与原问题在驻点层面等价。该惩罚模型允许直接应用各种无约束优化技术，并继承已有的收敛性保证。

Result: 与黎曼优化方法相比，SLEP模型消除了回撤和向量传输的需求，显著降低了每次迭代的计算成本。大量数值实验验证了理论结果，证明了SLEP的有效性和实际潜力。

Conclusion: SLEP惩罚模型为广义Stiefel流形上的优化问题，特别是奇异M矩阵的情况，提供了一种高效的计算框架，避免了黎曼几何的复杂性，具有实际应用价值。

Abstract: In this paper, we consider a class of optimization problems constrained to the generalized Stiefel manifold. Such problems are fundamental to a wide range of real-world applications, including generalized canonical correlation analysis, linear discriminant analysis, and electronic structure calculations. Existing works mainly focuses on cases where the generalized orthogonality constraint is induced by a symmetric positive definite matrix M, a setting where the geometry essentially reduces to that of the standard Stiefel manifold. However, many practical scenarios involve a singular M, which introduces significant analytical and computational challenges. Therefore, we propose a Smooth Locally Exact Penalty model (SLEP) and establish its equivalence to the original problem in the aspect of stationary points under a finitly large penalty parameter. This penalty model admits the direct application of various unconstrained optimization techniques, with convergence guarantees inherited from established results. Compared to Riemannian optimization approaches, our proposed penalty mode eliminates the need for retractions and vector transports, hence significantly reducing per-iteration computational costs. Extensive numerical experiments validate our theoretical results and demonstrate the effectiveness and practical potential of the proposed penalty model SLEP.

</details>


### [97] [Characterizations of the Aubin property of the KKT-mapping in composite optimization by SC derivatives and quadratic bundles](https://arxiv.org/abs/2602.05684)
*Helmut Gfrerer,Jiri V. Outrata*

Main category: math.OC

TL;DR: 该论文证明了对于复合优化的KKT映射，Aubin性质和单值Lipschitz局部化的存在性可以用更易处理的SC导数和二次束来刻画，而非传统的极限余导数和严格图导数。


<details>
  <summary>Details</summary>
Motivation: 对于一般集值映射，Aubin性质通常通过Mordukhovich准则与极限余导数相关联，而单值Lipschitz局部化的存在性则与严格图导数相关。这些工具在计算上可能较为复杂。本文旨在为复合优化的KKT映射这一特殊情形，寻找更易处理的分析工具。

Method: 针对复合优化问题的KKT映射，作者引入了SC导数（可能指次微分或某种特定导数）和二次束作为分析工具。通过理论分析，建立了这些新工具与Aubin性质、单值Lipschitz局部化存在性之间的等价关系。

Result: 证明了对于复合优化的KKT映射，Aubin性质可以通过SC导数来刻画，而单值Lipschitz局部化的存在性可以通过二次束来刻画。这些新工具比传统的极限余导数和严格图导数更易于计算和应用。

Conclusion: 对于复合优化问题的KKT映射，存在更简单、更易处理的工具（SC导数和二次束）来刻画稳定性性质，这为相关优化问题的灵敏度分析和稳定性研究提供了更实用的理论框架。

Abstract: For general set-valued mappings, the Aubin property is ultimately tied to limiting coderivatives by the Mordukhovich criterion. Likewise, the existence of single-valued Lipschitzian localizations is related to strict graphical derivatives. In this paper we will show that for the special case of the KKT-mapping from composite optimization, the Aubin property and the existence of single-valued Lipschitzian localizations can be characterized by SC derivatives and quadratic bundles, respectively, which are easier accessible than limiting coderivatives and strict graphical derivatives.

</details>


### [98] [On Circuit Diameter and Straight Line Complexity](https://arxiv.org/abs/2602.05699)
*Daniel Dadush,Stefan Kober,Zhuan Khye Koh*

Main category: math.OC

TL;DR: 本文研究了多面体电路直径与线性规划直线复杂度之间的关系，证明了电路直径最多是直线复杂度的多项式倍，并针对每个不等式最多包含2个变量的多面体给出了强多项式电路直径界和匹配迭代复杂度的电路增强算法。


<details>
  <summary>Details</summary>
Motivation: 电路直径和直线复杂度分别是电路增强算法和路径跟踪内点法的迭代次数下界。理解这两个概念之间的关系有助于统一分析不同线性规划算法的性能，并为特定类型多面体提供强多项式复杂度保证。

Method: 通过理论分析建立电路直径与直线复杂度之间的数学关系，证明电路直径最多是直线复杂度的多项式倍。针对每个不等式最多包含2个变量的多面体，构造性地给出强多项式电路直径界，并设计匹配迭代复杂度的电路增强算法。

Result: 证明了对于多面体P，其电路直径最多是定义在P上的线性规划直线复杂度的多项式倍。对于每个不等式最多包含2个变量的多面体，获得了强多项式电路直径界。同时给出了具有匹配迭代复杂度的电路增强算法。

Conclusion: 电路直径与直线复杂度之间存在紧密联系，电路直径最多是直线复杂度的多项式倍。这一结果为特定类型多面体提供了强多项式电路直径界，并设计了相应的高效电路增强算法，统一了不同线性规划算法的复杂度分析框架。

Abstract: The circuit diameter of a polyhedron is the maximum length (number of steps) of a shortest circuit walk between any two vertices of the polyhedron. Introduced by Borgwardt, Finhold and Hemmecke (SIDMA 2015), it is a relaxation of the combinatorial diameter of a polyhedron. These two notions of diameter lower bound the number of iterations taken by circuit augmentation algorithms and the simplex method respectively for solving linear programs.
  Recently, an analogous lower bound for path-following interior point methods was introduced by Allamigeon, Dadush, Loho, Natura and Végh (SICOMP 2025). Termed straight line complexity, it refers to the minimum number of pieces of any piecewise linear curve that traverses a specified neighborhood of the central path.
  In this paper, we study the relationship between circuit diameter and straight line complexity. For a polyhedron $P:=\{x\in \mathbb{R}^n: Ax = b, x\geq \mathbf{0}\}$, we show that its circuit diameter is up to a $\mathrm{poly}(n)$ factor upper bounded by the straight line complexity of linear programs defined over $P$. This yields a strongly polynomial circuit diameter bound for polyhedra with at most 2 variables per inequality. We also give a circuit augmentation algorithm with matching iteration complexity.

</details>


### [99] [Non-Stationary Inventory Control with Lead Times](https://arxiv.org/abs/2602.05799)
*Nele H. Amiri,Sean R. Sinclair,Maximiliano Udenio*

Main category: math.OC

TL;DR: 研究非平稳需求下的库存控制问题，提出自适应在线算法，发现不同库存模型存在性能差异：积压订单和零提前期缺货模型能适应需求变化而不损失性能，但正提前期缺货系统存在根本性限制。


<details>
  <summary>Details</summary>
Motivation: 研究需求分布未知且随时间变化的非平稳库存控制问题，探索需求非平稳性如何影响不同库存模型的学习性能，包括积压订单和缺货模型（有无提前期）。

Method: 为每个库存设置提出自适应在线算法，在基库存策略类上进行优化，利用库存成本的凸性和单边反馈结构实现反事实策略评估，尽管存在需求审查问题。

Result: 算法在不同库存模型中表现出显著差异：积压系统和零提前期缺货模型能适应需求变化而不损失平稳环境性能；但正提前期缺货系统由于延迟补货和审查反馈存在根本限制，性能保证较弱。

Conclusion: 不同库存模型对需求非平稳性的适应能力存在本质差异，算法设计需考虑库存系统的具体结构特征，仿真显示所提方法显著优于现有基准。

Abstract: We study non-stationary single-item, periodic-review inventory control problems in which the demand distribution is unknown and may change over time. We analyze how demand non-stationarity affects learning performance across inventory models, including systems with demand backlogging or lost-sales, both with and without lead times. For each setting, we propose an adaptive online algorithm that optimizes over the class of base-stock policies and establish performance guarantees in terms of dynamic regret relative to the optimal base-stock policy at each time step. Our results reveal a sharp separation across inventory models. In backlogging systems and lost-sales models with zero lead time, we show that it is possible to adapt to demand changes without incurring additional performance loss in stationary environments, even without prior knowledge of the demand distributions or the number of demand shifts. In contrast, for lost-sales systems with positive lead times, we establish weaker guarantees that reflect fundamental limitations imposed by delayed replenishment in combination with censored feedback. Our algorithms leverage the convexity and one-sided feedback structure of inventory costs to enable counterfactual policy evaluation despite demand censoring. We complement the theoretical analysis with simulation results showing that our methods significantly outperform existing benchmarks.

</details>


### [100] [Objective-Function Free Multi-Objective Optimization: Rate of Convergence and Performance of an Adagrad-like algorithm](https://arxiv.org/abs/2602.05893)
*Marianna De Santis,Gabriele Eichfelder,Margherita Porcelli*

Main category: math.OC

TL;DR: 提出一种类似Adagrad的多目标无约束优化算法，仅需计算公共下降方向，无需支配性检验，采用自适应步长，收敛速度为O(1/√(k+1))


<details>
  <summary>Details</summary>
Motivation: 传统多目标优化算法依赖支配性来接受新迭代点，这限制了优化框架的灵活性。本文旨在开发一种不依赖支配性、无需Lipschitz常数知识或线搜索的简单自适应算法。

Method: 提出Adagrad-like算法，仅计算公共下降方向，不依赖支配性接受新迭代点。采用自适应步长，无需Lipschitz常数或线搜索。分析算法收敛性。

Result: 算法收敛速度为O(1/√(k+1))，在广泛的多目标无约束问题和简单多任务学习实例上验证有效，比一阶线搜索算法表现更好，在噪声环境下也展现鲁棒性。

Conclusion: 该算法提供了一种灵活、函数无关的多目标优化框架，无需支配性检验和线搜索，具有理论保证和实际有效性，在噪声环境下也表现鲁棒。

Abstract: We propose an Adagrad-like algorithm for multi-objective unconstrained optimization that relies on the computation of a common descent direction only. Unlike classical local algorithms for multi-objective optimization, our approach does not rely on the dominance property to accept new iterates, which allows for a flexible and function-free optimization framework. New points are obtained using an adaptive stepsize that does not require neither knowledge of Lipschitz constants nor the use of line search procedures. The rate of convergence is analyzed and is shown to be $\mathcal{O}(1 / \sqrt{ k+1})$ with respect to the norm of the common descent direction. The method is extensively validated on a broad class of unconstrained multi-objective problems and simple multi-task learning instances, and compared against a first-order line search algorithm. Additionally, we present a preliminary study of the behavior under noisy multi-objective settings, highlighting the robustness of the method.

</details>


### [101] [The Signed Wasserstein Barycenter Problem](https://arxiv.org/abs/2602.05976)
*Matt Jacobs,Bohan Zhou*

Main category: math.OC

TL;DR: 该论文研究带符号权重的Wasserstein重心问题，重点关注解的唯一性，扩展了单一正权重情况下的唯一性结果，并为任意权重情况引入了对偶问题。


<details>
  <summary>Details</summary>
Motivation: 带符号的Wasserstein重心问题在统计推断和数值方法中具有重要应用，但负权重破坏了问题的凸性，使得优化更具挑战性，因此需要研究其性质和解的唯一性。

Method: 研究一般运输成本下的带符号重心问题，扩展单一正权重情况下的唯一性结果到满足特定凸性性质的任何成本；对于任意权重情况，引入基于Kantorovich势的对偶问题。

Result: 当只有一个正权重时，将Tornabene等人的唯一性结果扩展到满足特定凸性性质的任何成本；对于任意权重，提供了对偶问题平稳解诱导最优带符号重心的充分条件。

Conclusion: 该工作为带符号Wasserstein重心问题建立了理论框架，特别关注解的唯一性条件，为这一具有挑战性的优化问题提供了重要的理论分析工具。

Abstract: Barycenter problems encode important geometric information about a metric space. While these problems are typically studied with positive weight coefficients associated to each distance term, more general signed Wasserstein barycenter problems have recently drawn a great deal of interest. These mixed sign problems have appeared in statistical inference setting as a way to generalize least squares regression to measure valued outputs and have appeared in numerical methods to improve the accuracy of Wasserstein gradient flow solvers. Unfortunately, the presence of negatively weighted distance terms destroys the Euclidean convexity of the unsigned problem, resulting in a much more challenging optimization task. The main focus of this work is to study properties of the signed barycenter problem for a general transport cost with a focus on establishing uniqueness of solutions. In particular, when there is only one positive weight, we extend the uniqueness result of Tornabene et al. (2025) to any cost satisfying a certain convexity property. In the case of arbitrary weights, we introduce the dual problem in terms of Kantorovich potentials and provide a sufficient condition for a stationary solution of the dual problem to induce an optimal signed barycenter.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 论文批评AI发展的线性模型，提出"熟悉智能"和"陌生智能"概念，认为AI智能将是陌生智能，具有超人类能力与亚人类表现并存的特点，支持非线性智能模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展常被线性模型主导，认为智能是单一连续体，但作者认为这种模型无法解释AI系统中观察到的能力不平衡现象，需要新的理论框架来理解AI智能的本质。

Method: 扩展Susan Schneider对线性AI进展模型的批评，引入"熟悉智能"和"陌生智能"两个新概念，发展并捍卫非线性智能模型，认为"通用智能"不是统一能力而是实现广泛目标的能力。

Result: AI智能很可能是陌生智能，在某些领域具有超人类能力，在其他领域表现亚人类，甚至在同一个领域内也会出现超人类洞察与令人惊讶的错误并存的现象。

Conclusion: 如果AI是陌生智能，即使最强大的系统也会在看似简单的任务上失败，这些错误本身不能证明系统缺乏卓越的通用智能；反之，在某一类任务上的优秀表现也不能保证在其他领域的广泛能力。

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [103] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead是一个结构感知的多轮文档推理代理，通过利用文档的层次结构和顺序结构来改进长文档问答，相比传统搜索方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的代理搜索框架通常将长文档视为扁平的文本块集合，未能充分利用文档固有的先验知识，如层次组织和顺序话语结构。随着工具使用和代理性大语言模型的发展，需要更有效地处理长文档问答。

Method: DeepRead使用基于LLM的OCR模型将PDF转换为保留标题和段落边界的结构化Markdown。在段落级别索引文档，并为每个段落分配编码其章节身份和章节内顺序的坐标式元数据键。为LLM配备两个互补工具：Retrieve工具定位相关段落并暴露其结构坐标，ReadSection工具在指定章节和段落范围内实现连续、保序的阅读。

Result: 实验表明DeepRead在文档问答方面相比Search-o1风格的代理搜索有显著改进。验证了检索和阅读工具之间的协同效应。细粒度行为分析揭示了类似人类"定位然后阅读"的阅读推理范式。

Conclusion: DeepRead通过显式操作文档的层次和顺序结构先验，实现了更有效的长文档问答，展示了结构感知的多轮文档推理代理的潜力。

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [104] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: MINT框架通过神经符号树推理知识缺口影响，利用自博弈优化AI代理的主动询问策略，结合LLM搜索总结推理过程，在未知对象规划中实现近专家级性能


<details>
  <summary>Details</summary>
Motivation: 开放世界规划中存在各种不完整信息和未知因素（如对象、人类目标意图），导致联合规划中存在知识缺口，需要AI代理主动获取人类输入来优化规划性能

Method: 提出MINT（最小信息神经符号树）框架：1）构建符号树模拟可能的人机交互；2）咨询神经规划策略估计知识缺口导致的规划不确定性；3）利用LLM搜索总结推理过程并策划最优询问集；4）通过自博弈优化AI的询问策略

Result: 在三个涉及未知/未见对象的基准测试中，MINT规划通过有限的问题数量（每个任务）实现了近专家级的回报，显著提高了奖励和成功率

Conclusion: MINT框架通过主动的人类输入获取策略，有效解决了对象驱动规划中的知识缺口问题，在开放世界人机协作规划中实现了高效的信息获取和规划性能

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [105] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: LLMs在已解决的图论问题上表现良好，能生成正确证明，但在开放问题上仅能提供探索性策略而无法推进解决，表明其适合概念探索但缺乏新颖数学洞察力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被学生用于计算机科学高级内容学习，需要评估其在数学严谨思维支持方面的可靠性，特别是在图论等数学领域。

Method: 采用八阶段评估协议模拟真实数学探究过程，包括解释、探索、策略形成和证明构建，测试LLM在已解决图论问题（线图优美性）和开放问题上的表现。

Result: 在已解决问题上，LLM表现优异：生成正确定义、识别相关结构、准确回忆结果并构建专家确认的有效证明；在开放问题上，能提供连贯解释和合理探索策略但无法推进解决，且未虚构结果。

Conclusion: LLMs适合支持已建立材料的概念探索，但在需要新颖数学洞察或关键结构推理的任务上仍有局限；教育中应引导学生用LLMs进行概念探索，同时依赖独立验证和严谨论证进行正式问题解决。

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [106] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: 本文提出了首个通用的智能体不确定性量化（UQ）框架，将现有UQ方法统一起来，并提出了条件不确定性减少的新视角，为LLM智能体UQ设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前不确定性量化研究主要集中于单轮问答场景，而随着LLM智能体在复杂任务中的广泛应用，需要面向交互式智能体的新UQ框架。现有方法在开放世界交互环境中存在局限性。

Method: 提出了首个通用的智能体UQ公式化框架，将现有UQ设置统一起来。从条件不确定性减少的新视角出发，强调动作的"交互性"，通过显式建模智能体轨迹中的可减少不确定性来构建概念框架。

Result: 建立了统一的智能体UQ理论框架，揭示了先前工作隐含地将LLM UQ视为不确定性积累过程，而新框架则将其视为条件不确定性减少过程，为交互式智能体提供了更合适的UQ方法。

Conclusion: 智能体UQ框架对前沿LLM开发和领域特定应用具有重要实践意义，但仍存在开放性问题需要进一步研究。该框架为设计LLM智能体设置中的UQ提供了可操作的指导。

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [107] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 使用强化学习框架优化小卫星多碎片主动清除任务的碰撞规避和任务规划


<details>
  <summary>Details</summary>
Motivation: 随着地球轨道碎片日益增多，主动碎片清除任务面临安全操作和碰撞风险挑战，需要更智能的解决方案来优化多碎片清除任务

Method: 采用基于掩码近端策略优化（PPO）的强化学习框架，集成加油策略、高效任务规划和自适应碰撞规避，优化航天器交会操作

Result: 使用Iridium 33碎片数据集进行模拟评估，结果显示相比传统启发式方法，该框架能降低碰撞风险并提高任务效率

Conclusion: 该研究为复杂多碎片主动清除任务提供了可扩展的解决方案，适用于自主空间任务规划中的其他多目标交会问题

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [108] [VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health](https://arxiv.org/abs/2602.05088)
*Kate H. Bentley,Luca Belli,Adam M. Chekroud,Emily J. Ward,Emily R. Dworkin,Emily Van Ark,Kelly M. Johnston,Will Alexander,Millard Brown,Matt Hawrilenko*

Main category: cs.AI

TL;DR: VERA-MH评估框架在自杀风险检测与响应中的临床有效性和可靠性验证研究，通过模拟对话评估AI聊天机器人安全性，发现临床医生评分一致性高，LLM评分与临床共识高度一致。


<details>
  <summary>Details</summary>
Motivation: 随着数百万人使用生成式AI聊天机器人寻求心理支持，AI在心理健康领域的安全性成为最紧迫的问题。需要建立基于证据的自动化安全基准来评估AI工具的安全性。

Method: 研究使用VERA-MH评估框架：1) 模拟大量LLM用户代理与通用AI聊天机器人的对话；2) 持牌心理健康临床医生使用评分标准独立评估对话中的安全/不安全行为和用户代理真实性；3) LLM法官使用相同标准评估相同对话；4) 比较临床医生之间、临床共识与LLM法官之间的评分一致性。

Result: 临床医生之间的安全评分一致性高（机会校正的组间信度：0.77），建立了黄金标准临床参考。LLM法官与临床共识高度一致（IRR：0.81），在关键条件下也保持一致。临床评分者普遍认为用户代理具有真实性。

Conclusion: 研究支持VERA-MH的临床有效性和可靠性，这是一个开源的、完全自动化的AI心理健康安全评估工具。要实现AI聊天机器人的心理健康益处，安全性至关重要。未来研究将关注VERA-MH的普适性和鲁棒性。

Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based automated safety benchmark. This study aimed to examine the clinical validity and reliability of the VERA-MH evaluation for AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then compared rating alignment across (a) individual clinicians and (b) clinician consensus and the LLM judge, and (c) examined clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR]: 0.77), thus establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus (IRR: 0.81) overall and within key conditions. Clinician raters generally perceived the user-agents to be realistic. For the potential mental health benefits of AI chatbots to be realized, attention to safety is paramount. Findings from this human evaluation study support the clinical validity and reliability of VERA-MH: an open-source, fully automated AI safety evaluation for mental health. Further research will address VERA-MH generalizability and robustness.

</details>


### [109] [Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation](https://arxiv.org/abs/2602.05403)
*Chenghua Gong,Yihang Jiang,Hao Li,Rui Sun,Juyuan Zhang,Tianjun Gu,Liming Pan,Linyuan Lü*

Main category: cs.AI

TL;DR: 本文提出OPINN框架，将扩散-对流-反应物理系统与神经网络结合，用于意见动力学建模，在真实和合成数据集上实现最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理信息神经网络的意见动力学方法存在局限性：1）依赖不完整的先验知识，缺乏整合局部、全局和内生层次动力学的完整物理系统；2）基于惩罚的约束方法难以深度编码物理先验，导致优化问题和潜在表示与物理透明度之间的差异。

Method: 1）从相互作用粒子理论出发，通过扩散-对流-反应系统解释意见动力学；2）基于神经ODE定义神经意见动力学，协调神经网络与物理先验；3）提出OPINN框架，将物理先验深度集成到神经网络架构中。

Result: 在真实世界和合成数据集上的评估表明，OPINN在意见演化预测方面达到了最先进的性能，为网络、物理和社会系统的融合提供了有前景的范式。

Conclusion: OPINN框架成功地将物理系统与神经网络结合，克服了现有方法的局限性，为意见动力学建模提供了更全面、物理透明且性能优越的解决方案，有助于理解社会行为、缓解极化和保护网络空间。

Abstract: Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.

</details>


### [110] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 比较三种自主碎片清除任务规划器：固定参数训练的PPO、领域随机化PPO和MCTS，在燃料和时间约束变化下的性能表现


<details>
  <summary>Details</summary>
Motivation: 自主碎片清除任务规划需要在效率、适应性和严格的燃料/时间约束之间取得平衡，需要评估不同规划方法在约束变化下的表现

Method: 比较三种规划器：1) 固定参数训练的Masked PPO；2) 领域随机化训练的Masked PPO；3) 蒙特卡洛树搜索(MCTS)基线。在包含加油、真实转移动力学和随机碎片场的轨道模拟中进行评估

Result: 固定参数PPO在训练条件下表现最佳但在分布偏移时性能急剧下降；领域随机化PPO适应性更好但名义性能略有损失；MCTS对约束变化处理最好但计算时间高几个数量级

Conclusion: 学习策略的速度与搜索方法的适应性存在权衡，结合训练时多样性和在线规划可能是未来弹性ADR任务规划的有前景方向

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [111] [GAMMS: Graph based Adversarial Multiagent Modeling Simulator](https://arxiv.org/abs/2602.05105)
*Rohan Patil,Jai Malegaonkar,Xiao Jiang,Andre Dion,Gaurav S. Sukhatme,Henrik I. Christensen*

Main category: cs.AI

TL;DR: GAMMS是一个轻量级、可扩展的多智能体仿真框架，基于图结构表示环境，支持快速开发和评估智能体行为，特别适用于城市道路网络和通信系统等复杂领域。


<details>
  <summary>Details</summary>
Motivation: 随着智能系统和多智能体协调在现实应用中的重要性增加，需要既具有可扩展性又易于使用的仿真工具。现有高保真仿真器虽然强大，但计算成本高，不适合快速原型设计或大规模智能体部署。

Method: GAMMS采用基于图的环境表示方法，强调五个核心目标：可扩展性、易用性、集成优先架构、快速可视化反馈和现实基础。支持与外部工具（如机器学习库、规划求解器）集成，并提供内置可视化功能。

Result: GAMMS能够高效仿真复杂领域，支持启发式、优化型和基于学习的智能体（包括使用大语言模型的智能体），降低研究门槛，在标准硬件上实现高性能仿真。

Conclusion: GAMMS通过降低研究人员入门门槛，促进多智能体系统、自主规划和对抗建模领域的实验和创新。该框架是开源的，可在GitHub上获取。

Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/

</details>


### [112] [Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment](https://arxiv.org/abs/2602.05110)
*Liang Wang,Junpeng Wang,Chin-chia Michael Yeh,Yan Zheng,Jiarui Sun,Xiran Fan,Xin Dai,Yujie Fan,Yiwei Cai*

Main category: cs.AI

TL;DR: 研究提出一个结构化多评估框架，用于评估LLM在商户风险分类中的推理质量，发现不同模型存在显著评估偏差，匿名化可减少偏差，部分模型与人类专家判断更接近。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多被用作推理质量评估者，但在支付风险场景下的可靠性和偏差仍不清楚。需要建立系统框架来评估LLM在商户风险分类中的推理表现。

Method: 引入结构化多评估框架，结合五标准评分表和蒙特卡洛评分，评估推理质量和评估稳定性。使用五个前沿LLM生成并交叉评估商户风险推理，在署名和匿名条件下进行。引入共识偏差度量消除循环性，比较每个评估者分数与其他评估者均值。

Result: 结果显示显著异质性：GPT-5.1和Claude 4.5 Sonnet显示负自评估偏差(-0.33, -0.31)，Gemini-2.5 Pro和Grok 4显示正偏差(+0.77, +0.71)，匿名化使偏差减少25.8%。LLM评估分数平均比人类共识高0.46分，GPT-5.1和Claude 4.5 Sonnet的负偏差反映与人类判断更接近。使用支付网络数据的真实验证显示四个模型具有统计显著相关性(Spearman rho = 0.56-0.77)。

Conclusion: 该框架为支付风险工作流中的LLM评估系统提供了可复现基础，强调了在金融操作环境中需要偏差感知协议。部分LLM与人类专家判断更接近，匿名化可减少评估偏差。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

</details>


### [113] [Democratic Preference Alignment via Sortition-Weighted RLHF](https://arxiv.org/abs/2602.05113)
*Suvadip Sana,Jinzhou Wu,Martin T. Wells*

Main category: cs.AI

TL;DR: DemPO框架通过算法抽签构建代表性人类评分者小组，用于AI偏好对齐训练，相比传统便利样本能更好地反映多元人口价值观。


<details>
  <summary>Details</summary>
Motivation: 当前基于偏好的AI对齐方法（如RLHF）依赖人类评分者，但这些评分者通常是便利样本，存在人口代表性偏差，无法公平反映不同群体的价值观。

Method: 提出民主偏好优化（DemPO）框架，采用算法抽签机制构建公民大会式的代表性评分小组。提供两种训练方案：硬面板（仅使用抽签选出的代表性小组数据）和软面板（保留所有数据但按抽签概率重新加权）。

Result: 在10亿到80亿参数的Llama模型上测试，使用包含人口统计信息的公开偏好数据集和代表性美国小组制定的75条宪法。硬面板在六种聚合方法中始终排名第一，软面板始终优于未加权基线，且模型容量越大效果越明显。

Conclusion: 在偏好收集阶段强制实施人口代表性，而非事后修正，能产生更好地反映代表性公众价值观的模型行为。DemPO为构建更民主的AI对齐提供了可行框架。

Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.

</details>


### [114] [SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers](https://arxiv.org/abs/2602.05115)
*Keyang Xuan,Pengda Wang,Chongrui Ye,Haofei Yu,Tal August,Jiaxuan You*

Main category: cs.AI

TL;DR: SocialVeil：一个模拟认知差异导致沟通障碍的社会学习环境，用于评估LLM在现实社交互动中的表现


<details>
  <summary>Details</summary>
Motivation: 现有基准测试假设理想化沟通，无法评估LLM在现实不完美沟通环境中的社交智能，特别是处理沟通障碍的能力

Method: 基于系统文献综述提出三种沟通障碍类型（语义模糊、社会文化不匹配、情感干扰），开发SocialVeil环境，引入两个障碍感知评估指标（未解决困惑、相互理解），在720个场景中测试四个前沿LLM

Result: 沟通障碍显著降低LLM表现：相互理解平均减少45%以上，困惑度提升近50%；人类评估验证模拟障碍的保真度（ICC≈0.78，Pearson r≈0.80）；适应策略效果有限

Conclusion: SocialVeil使社交互动环境更接近现实世界沟通，为探索LLM代理的社交智能提供了新机会，揭示了LLM在应对沟通障碍方面的局限性

Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.

</details>


### [115] [CAST-CKT: Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer for Traffic Flow Prediction](https://arxiv.org/abs/2602.05133)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Alpha Alimamy Kamara,Zhongyi Zhang*

Main category: cs.AI

TL;DR: CAST-CKT是一个用于数据稀缺跨城市交通预测的混沌感知时空和跨城市知识转移框架，通过混沌分析量化交通可预测性机制，实现机制自适应建模和跨城市知识对齐。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺的跨城市交通预测面临复杂非线性动力学和领域偏移的挑战，现有方法难以捕捉交通的固有混沌特性进行有效的少样本学习。

Method: 提出CAST-CKT框架：1) 高效混沌分析器量化交通可预测性机制；2) 混沌感知注意力实现机制自适应时间建模；3) 自适应拓扑学习动态空间依赖；4) 混沌一致性跨城市对齐进行知识转移；5) 提供具有不确定性量化的特定时间范围预测。

Result: 在四个基准数据集上的跨城市少样本实验中，CAST-CKT在MAE和RMSE指标上显著优于最先进方法，同时提供可解释的机制分析。理论分析显示改进的泛化边界。

Conclusion: CAST-CKT通过混沌感知建模有效解决了跨城市少样本交通预测的挑战，在性能提升的同时提供理论保证和可解释性，为数据稀缺场景下的交通预测提供了新思路。

Abstract: Traffic prediction in data-scarce, cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods often fail to capture traffic's inherent chaotic nature for effective few-shot learning. We propose CAST-CKT, a novel Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer framework. It employs an efficient chaotic analyser to quantify traffic predictability regimes, driving several key innovations: chaos-aware attention for regime-adaptive temporal modelling; adaptive topology learning for dynamic spatial dependencies; and chaotic consistency-based cross-city alignment for knowledge transfer. The framework also provides horizon-specific predictions with uncertainty quantification. Theoretical analysis shows improved generalisation bounds. Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis. Code is available at https://github.com/afofanah/CAST-CKT.

</details>


### [116] [HugRAG: Hierarchical Causal Knowledge Graph Design for RAG](https://arxiv.org/abs/2602.05143)
*Nengbo Wang,Tuo Liang,Vikash Singh,Chaoda Song,Van Yang,Yu Yin,Jing Ma,Jagdip Singh,Vipin Chaudhary*

Main category: cs.AI

TL;DR: HugRAG是一个基于因果门控的层次化图RAG框架，通过显式建模因果关系来抑制虚假相关性，实现大规模知识图上的可扩展推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法过度依赖表层节点匹配，缺乏显式因果建模，导致答案不可靠或虚假。先前尝试融入因果关系的方法通常局限于局部或单文档上下文，且受模块化图结构导致的信息隔离问题影响，阻碍了可扩展性和跨模块因果推理。

Method: 提出HugRAG框架，通过跨层次模块的因果门控重新思考基于图的RAG知识组织方式。显式建模因果关系以抑制虚假相关性，同时支持大规模知识图上的可扩展推理。

Result: 大量实验表明，HugRAG在多个数据集和评估指标上持续优于竞争性的基于图RAG基线方法。

Conclusion: 该工作为结构化、可扩展且基于因果基础的RAG系统建立了原则性基础。

Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

</details>


### [117] [First Proof](https://arxiv.org/abs/2602.05192)
*Mohammed Abouzaid,Andrew J. Blumberg,Martin Hairer,Joe Kileel,Tamara G. Kolda,Paul D. Nelson,Daniel Spielman,Nikhil Srivastava,Rachel Ward,Shmuel Weinberger,Lauren Williams*

Main category: cs.AI

TL;DR: 作者分享了10个研究级数学问题来评估当前AI系统回答数学研究问题的能力


<details>
  <summary>Details</summary>
Motivation: 评估当前AI系统在回答研究级数学问题方面的能力，这些问题来自作者的实际研究过程

Method: 创建了一个包含10个未公开过的研究级数学问题的数据集，这些问题来自作者的实际研究过程

Result: 提供了一个评估AI数学能力的基准数据集，答案暂时加密但作者已知晓

Conclusion: 通过分享这些研究级数学问题，为评估AI系统的数学推理能力提供了一个新的测试基准

Abstract: To assess the ability of current AI systems to correctly answer research-level mathematics questions, we share a set of ten math questions which have arisen naturally in the research process of the authors. The questions had not been shared publicly until now; the answers are known to the authors of the questions but will remain encrypted for a short time.

</details>


### [118] [Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering](https://arxiv.org/abs/2602.05195)
*Fengxian Chen,Zhilong Tao,Jiaxuan Li,Yunlong Li,Qingguo Zhou*

Main category: cs.AI

TL;DR: 本文提出DAKS和alignment graph两种方法，解决多知识库RAG中的检索偏差和跨知识库证据融合问题，在藏医药领域取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 在多知识库RAG中，密度高的百科条目容易主导检索，而权威性更强的经典文献或临床论文证据被忽视。特别是在藏医药领域，需要解决跨知识库检索和证据融合的挑战。

Method: 1. DAKS：进行知识库路由和预算检索，缓解密度驱动偏差，在适当时优先考虑权威来源。2. 使用对齐图指导证据融合和覆盖感知打包，改善跨知识库证据覆盖，避免简单拼接。

Result: 实验显示路由质量和跨知识库证据覆盖持续提升，完整系统在保持强忠实性和引用正确性的同时，达到最佳CrossEv@5指标。

Conclusion: 提出的方法能有效解决多知识库RAG中的检索偏差问题，提高跨知识库证据覆盖，在藏医药等专业领域具有实用价值。

Abstract: Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.

</details>


### [119] [Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://arxiv.org/abs/2602.05228)
*Guozhi Liu,Weiwei Lin,Tiansheng Huang,Ruichao Mo,Qi Mu,Xiumin Wang,Li Shen*

Main category: cs.AI

TL;DR: 提出Surgery方法，通过注意力汇聚机制抑制有害微调，利用汇聚散度统计量分离学习有害模式的注意力头，通过正则化减少模型学习有害模式的倾向。


<details>
  <summary>Details</summary>
Motivation: 有害微调会破坏大语言模型的安全对齐，带来显著安全风险。现有防御方法效果有限，需要更有效的微调阶段防御机制。

Method: 基于注意力汇聚机制，提出汇聚散度统计量来测量每个注意力头；发现不同注意力头呈现两种不同的汇聚散度符号；提出可分离汇聚散度假设；设计Surgery方法，使用汇聚散度抑制正则化器，引导注意力头向负汇聚散度组移动。

Result: 在BeaverTails、HarmBench和SorryBench基准测试上，Surgery分别提升了5.90%、11.25%和9.55%的防御性能，显著优于现有方法。

Conclusion: Surgery通过注意力汇聚机制有效防御有害微调，利用汇聚散度分离有害模式学习，为模型安全对齐提供了有效的微调阶段防御方案。

Abstract: Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \emph{sink divergence} for each attention head and observe that \emph{different attention heads exhibit two different signs of sink divergence}. To understand its safety implications, we conduct experiments and find that the number of attention heads of positive sink divergence increases along with the increase of the model's harmfulness when undergoing harmful fine-tuning. Based on this finding, we propose a separable sink divergence hypothesis -- \emph{attention heads associating with learning harmful patterns during fine-tuning are separable by their sign of sink divergence}. Based on the hypothesis, we propose a fine-tuning-stage defense, dubbed Surgery. Surgery utilizes a regularizer for sink divergence suppression, which steers attention heads toward the negative sink divergence group, thereby reducing the model's tendency to learn and amplify harmful patterns. Extensive experiments demonstrate that Surgery improves defense performance by 5.90\%, 11.25\%, and 9.55\% on the BeaverTails, HarmBench, and SorryBench benchmarks, respectively. Source code is available on https://github.com/Lslland/Surgery.

</details>


### [120] [Explainable AI: A Combined XAI Framework for Explaining Brain Tumour Detection Models](https://arxiv.org/abs/2602.05240)
*Patrick McGonagle,William Farrelly,Kevin Curran*

Main category: cs.AI

TL;DR: 该研究通过整合多种可解释AI技术（GRAD-CAM、LRP、SHAP）来增强脑肿瘤检测深度学习模型的可解释性，在BraTS 2021数据集上达到91.24%准确率，提供从区域到像素级的全面解释。


<details>
  <summary>Details</summary>
Motivation: 增强AI驱动医疗影像分析的透明度和可信度，特别是在脑肿瘤检测等关键任务中，需要更全面的模型决策过程解释。

Method: 开发定制卷积神经网络（CNN）并在BraTS 2021数据集上训练，结合使用GRAD-CAM（突出重要空间区域）、LRP（提供像素级相关性）和SHAP（量化特征贡献）三种XAI技术。

Result: 模型达到91.24%的准确率，成功识别完整和部分肿瘤，集成方法比单一XAI技术具有更优越的解释能力，能提供从广泛感兴趣区域到像素细节的分层解释。

Conclusion: 集成多种XAI技术能显著提高医疗AI系统的可靠性和可解释性，为脑肿瘤检测等关键医疗任务提供更全面的模型推理视角，增强透明度和信任。

Abstract: This study explores the integration of multiple Explainable AI (XAI) techniques to enhance the interpretability of deep learning models for brain tumour detection. A custom Convolutional Neural Network (CNN) was developed and trained on the BraTS 2021 dataset, achieving 91.24% accuracy in distinguishing between tumour and non-tumour regions. This research combines Gradient-weighted Class Activation Mapping (GRAD-CAM), Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanations (SHAP) to provide comprehensive insights into the model's decision-making process. This multi-technique approach successfully identified both full and partial tumours, offering layered explanations ranging from broad regions of interest to pixel-level details. GRAD-CAM highlighted important spatial regions, LRP provided detailed pixel-level relevance and SHAP quantified feature contributions. The integrated approach effectively explained model predictions, including cases with partial tumour visibility thus showing superior explanatory power compared to individual XAI methods. This research enhances transparency and trust in AI-driven medical imaging analysis by offering a more comprehensive perspective on the model's reasoning. The study demonstrates the potential of integrated XAI techniques in improving the reliability and interpretability of AI systems in healthcare, particularly for critical tasks like brain tumour detection.

</details>


### [121] [Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents](https://arxiv.org/abs/2602.05249)
*Xinyi He,Ying Yang,Chuanjian Fu,Sihan Guo,Songchun Zhu,Lifeng Fan,Zhenliang Zhang,Yujia Peng*

Main category: cs.AI

TL;DR: 提出TEA方法，通过动态原位任务生成评估智能体在未见3D环境中的能力，发现现有模型在基本感知任务上表现不佳


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在严重的数据污染和缺乏场景特异性问题，无法有效评估智能体在未见环境中的能力，而智能体即将广泛部署到多样化的家庭环境中

Method: 提出TEA方法：1) 使用结构化图表示定义任务；2) 构建两阶段交互-演化任务生成系统：交互阶段通过智能体与环境主动交互，在任务执行和生成间形成循环；演化阶段通过任务图建模重组和重用现有任务生成新任务

Result: 在10个未见场景中，TEA在两个周期内自动生成了87,876个任务，经人工验证具有物理合理性和覆盖日常认知能力；基准测试显示SOTA模型在基本感知任务上表现差，严重缺乏3D交互意识，对任务类型敏感

Conclusion: 研究结果表明在将智能体部署到真实世界人类环境之前，进行原位评估是必要的，现有模型在公共基准上表现出色但在实际场景中表现令人担忧

Abstract: As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.

</details>


### [122] [Beyond Cosine Similarity](https://arxiv.org/abs/2602.05266)
*Xinbo Ai*

Main category: cs.AI

TL;DR: 提出新的相似度度量recos，通过排序向量分量归一化点积，比余弦相似度更能捕捉复杂语义空间中的非线性关系


<details>
  <summary>Details</summary>
Motivation: 余弦相似度基于柯西-施瓦茨不等式，只能捕捉线性关系，无法建模真实语义空间的复杂非线性结构

Method: 推导出比经典柯西-施瓦茨界更紧的点积上界，基于此提出recos度量，通过排序向量分量归一化点积，将完美相似条件从严格线性依赖放宽为序数一致性

Result: 在11种嵌入模型（静态、上下文化、通用类型）上的实验表明，recos在标准语义文本相似度基准上始终优于传统余弦相似度，与人类判断的相关性更高

Conclusion: recos作为数学原理严谨且经验上优越的替代方案，为复杂嵌入空间中的语义分析提供了更高的准确性

Abstract: Cosine similarity, the standard metric for measuring semantic similarity in vector spaces, is mathematically grounded in the Cauchy-Schwarz inequality, which inherently limits it to capturing linear relationships--a constraint that fails to model the complex, nonlinear structures of real-world semantic spaces. We advance this theoretical underpinning by deriving a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound. This new bound leads directly to recos, a similarity metric that normalizes the dot product by the sorted vector components. recos relaxes the condition for perfect similarity from strict linear dependence to ordinal concordance, thereby capturing a broader class of relationships. Extensive experiments across 11 embedding models--spanning static, contextualized, and universal types--demonstrate that recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks. Our work establishes recos as a mathematically principled and empirically superior alternative, offering enhanced accuracy for semantic analysis in complex embedding spaces.

</details>


### [123] [Hallucination-Resistant Security Planning with a Large Language Model](https://arxiv.org/abs/2602.05279)
*Kim Hammar,Tansu Alpcan,Emil Lupu*

Main category: cs.AI

TL;DR: 提出一个原则性框架，将LLM集成到安全管理的决策支持中，通过一致性检查和反馈循环控制幻觉风险，在事件响应用例中减少30%恢复时间


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在安全管理工作（如事件响应规划）中具有潜力，但其不可靠性和幻觉倾向仍然是重大挑战，需要解决这些问题以实现可靠应用

Method: 提出一个原则性框架，将LLM集成到迭代循环中：LLM生成候选行动，检查与系统约束的一致性并进行前瞻预测；当一致性低时，通过数字孪生等收集外部反馈，使用上下文学习精炼候选行动

Result: 证明该设计可通过调整一致性阈值控制幻觉风险，并在某些假设下建立上下文学习的遗憾界限；在四个公共数据集的事件响应用例实验中，框架相比前沿LLM减少高达30%的恢复时间

Conclusion: 该框架为在安全管理中使用LLM提供了一种原则性方法，通过一致性检查和反馈循环有效控制幻觉风险，显著提高事件响应效率

Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.

</details>


### [124] [Position: Universal Time Series Foundation Models Rest on a Category Error](https://arxiv.org/abs/2602.05287)
*Xilin Dai,Wanxu Cai,Zhijian Xu,Qiang Xu*

Main category: cs.AI

TL;DR: 该文认为"通用时间序列基础模型"存在范畴错误，将结构容器误认为语义模态，提出因果控制代理范式替代通用性追求


<details>
  <summary>Details</summary>
Motivation: 当前追求通用时间序列基础模型存在根本问题：时间序列包含不兼容的生成过程（如金融与流体动力学），单一模型会退化为昂贵的"通用滤波器"，在分布漂移下无法泛化

Method: 提出"自回归盲区界限"理论，证明仅依赖历史的模型无法预测干预驱动的机制转变；倡导因果控制代理范式，代理利用外部上下文协调层次化专业求解器（从冻结领域专家到轻量级即时适配器）

Result: 理论分析显示通用时间序列模型的局限性，提出新的评估框架：从"零样本准确率"转向"漂移适应速度"，优先考虑鲁棒的、控制论系统

Conclusion: 应放弃通用性追求，转向因果控制代理范式，通过外部上下文协调专业求解器，并改变评估标准以构建更鲁棒的时间序列系统

Abstract: This position paper argues that the pursuit of "Universal Foundation Models for Time Series" rests on a fundamental category error, mistaking a structural Container for a semantic Modality. We contend that because time series hold incompatible generative processes (e.g., finance vs. fluid dynamics), monolithic models degenerate into expensive "Generic Filters" that fail to generalize under distributional drift. To address this, we introduce the "Autoregressive Blindness Bound," a theoretical limit proving that history-only models cannot predict intervention-driven regime shifts. We advocate replacing universality with a Causal Control Agent paradigm, where an agent leverages external context to orchestrate a hierarchy of specialized solvers, from frozen domain experts to lightweight Just-in-Time adaptors. We conclude by calling for a shift in benchmarks from "Zero-Shot Accuracy" to "Drift Adaptation Speed" to prioritize robust, control-theoretic systems.

</details>


### [125] [Aspect-Aware MOOC Recommendation in a Heterogeneous Network](https://arxiv.org/abs/2602.05297)
*Seongyeub Chu,Jongwoo Kim,Mun Yong Yi*

Main category: cs.AI

TL;DR: AMR是一个基于图神经网络的MOOC推荐框架，通过双向游走自动发现元路径，利用bi-LSTM编码器生成方面感知的路径表示，并将其作为边特征融入学习者-学习者和知识点-知识点子图，实现细粒度的语义感知推荐。


<details>
  <summary>Details</summary>
Motivation: 传统MOOC推荐方法（协同过滤、基于内容过滤）存在数据稀疏性和过度专业化问题。现有基于图的方法虽然有所改进，但仍依赖手动预定义的元路径，这些路径通常只能捕获表面结构关系，且给领域专家带来沉重负担和工程成本。

Method: 提出AMR框架：1）通过双向游走自动发现元路径；2）使用bi-LSTM编码器嵌入每个元路径中节点的语义内容，生成方面感知的路径表示；3）将这些表示作为边特征融入学习者-学习者和知识点-知识点子图，实现细粒度的语义感知知识组件推荐。

Result: 在MOOCCube和PEEK两个大规模数据集上的实验表明，AMR在HR@K和nDCG@K等关键指标上持续优于最先进的图神经网络基线。进一步分析证实AMR能有效捕获丰富的路径特定方面信息，比仅依赖预定义元路径的方法提供更准确的推荐。

Conclusion: AMR通过自动发现元路径和嵌入语义内容，克服了传统方法对预定义元路径的依赖，实现了更准确、语义感知的MOOC推荐，为学习者提供更精细的学习内容导航。

Abstract: MOOC recommendation systems have received increasing attention to help learners navigate and select preferred learning content. Traditional methods such as collaborative filtering and content-based filtering suffer from data sparsity and over-specialization. To alleviate these limitations, graph-based approaches have been proposed; however, they still rely heavily on manually predefined metapaths, which often capture only superficial structural relationships and impose substantial burdens on domain experts as well as significant engineering costs. To overcome these limitations, we propose AMR (Aspect-aware MOOC Recommendation), a novel framework that models path-specific multiple aspects by embedding the semantic content of nodes within each metapath. AMR automatically discovers metapaths through bi-directional walks, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these representations as edge features in the learner-learner and KC-KC subgraphs to achieve fine-grained semantically informed KC recommendations. Extensive experiments on the large-scale MOOCCube and PEEK datasets show that AMR consistently outperforms state-of-the-art graph neural network baselines across key metrics such as HR@K and nDCG@K. Further analysis confirms that AMR effectively captures rich path-specific aspect information, allowing more accurate recommendations than those methods that rely solely on predefined metapaths. The code will be available upon accepted.

</details>


### [126] [PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences](https://arxiv.org/abs/2602.05302)
*Chris Zhu,Sasha Cui,Will Sanok Dufallo,Runzhi Jin,Zhen Xu,Linjun Zhang,Daylian Cain*

Main category: cs.AI

TL;DR: GPT-5在谈判任务上达到或超越商学院学生水平，但中低阶模型仍需提升，谈判行为分析揭示模型间异质性


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在谈判任务上的能力，谈判是商业核心任务，需要策略推理、心智理论和经济价值创造能力

Method: 构建PieArena大规模谈判基准，基于商学院谈判课程的真实场景，进行多智能体交互评估，研究联合意向性智能体脚手架效果

Result: GPT-5达到AGI水平，匹配或超越受过训练的商学院学生；中低阶模型通过智能体脚手架获得显著提升，前沿模型收益递减；谈判行为分析揭示模型在欺骗、计算准确性、指令遵从和感知声誉方面的异质性

Conclusion: 前沿语言智能体已具备在高风险经济环境中部署的智力和心理能力，但鲁棒性和可信赖性仍是开放挑战

Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.

</details>


### [127] [ProAct: Agentic Lookahead in Interactive Environments](https://arxiv.org/abs/2602.05327)
*Yangbin Yu,Mingyu Yang,Junyou Li,Yiming Gao,Feiyu Liu,Yijun Yang,Zichuan Lin,Jiafei Lyu,Yicheng Liu,Zhicong Lu,Deheng Ye,Jie Jiang*

Main category: cs.AI

TL;DR: ProAct框架通过两阶段训练让LLM代理学习前瞻推理：GLAD阶段通过环境搜索轨迹进行监督微调，MC-Critic阶段通过蒙特卡洛评估增强策略优化，显著提升长期规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在需要长期规划的交互环境中表现不佳，主要因为模拟未来状态时错误会累积。需要一种方法让代理内化准确的前瞻推理能力。

Method: 提出ProAct框架，包含两个阶段：1) Grounded LookAhead Distillation (GLAD)：通过环境搜索轨迹进行监督微调，将复杂搜索树压缩为简洁的因果推理链；2) Monte-Carlo Critic (MC-Critic)：轻量级环境rollout校准价值估计，增强PPO/GRPO等策略梯度算法。

Result: 在随机环境（如2048）和确定性环境（如Sokoban）中，ProAct显著提升规划准确性。4B参数模型超越所有开源基线，媲美最先进的闭源模型，并在未见环境中表现出强大泛化能力。

Conclusion: ProAct通过两阶段训练范式有效解决了LLM代理的长期规划问题，将搜索树压缩为推理链，结合蒙特卡洛价值评估，实现了高效准确的前瞻推理能力。

Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct

</details>


### [128] [AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction](https://arxiv.org/abs/2602.05353)
*Ruijie Shi,Houbin Zhang,Yuecheng Han,Yuheng Wang,Jingru Fan,Runde Yang,Yufan Dang,Huatao Li,Dewen Liu,Yuan Cheng,Chen Qian*

Main category: cs.AI

TL;DR: 提出Agentic Workflow Reconstruction (AWR)任务和AgentXRay框架，通过输入-输出访问重建黑盒智能体系统的可解释工作流


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂问题解决中表现出强大能力，但许多智能体系统由于内部工作流程不透明而难以解释和控制。虽然有些框架提供了明确的协作架构，但许多部署的智能体系统对用户来说仍然是黑盒。

Method: 提出AgentXRay框架，将AWR任务形式化为组合优化问题，在链式结构工作流空间中搜索离散的智能体角色和工具调用。采用蒙特卡洛树搜索，并通过基于评分的红黑剪枝机制增强，动态整合代理质量与搜索深度。

Result: 实验表明，AgentXRay在多个领域实现了更高的代理相似度，相比未剪枝搜索减少了token消耗，在固定迭代预算下能够探索更深的工作流。

Conclusion: AgentXRay能够从黑盒智能体系统中重建可编辑的白盒工作流，无需访问模型参数，提高了智能体系统的可解释性和可控性。

Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.

</details>


### [129] [PATHWAYS: Evaluating Investigation and Context Discovery in AI Web Agents](https://arxiv.org/abs/2602.05354)
*Shifat E. Arman,Syed Nazmus Sakib,Tapodhir Karmakar Taton,Nafiul Haque,Shahrear Bin Amin*

Main category: cs.AI

TL;DR: PATHWAYS是一个包含250个多步决策任务的基准测试，用于评估网络智能体是否能发现并正确使用隐藏的上下文信息。研究发现当前智能体在自适应调查、证据整合和判断覆盖方面存在严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估当前基于网络的智能体是否能够发现并正确使用隐藏的上下文信息来完成多步决策任务。现有的智能体架构在处理需要发现隐藏证据、推翻表面误导信号的任务时表现如何尚不清楚。

Method: 研究创建了PATHWAYS基准测试，包含250个多步决策任务。测试了封闭和开放模型，评估智能体在发现隐藏上下文信息、整合证据、推翻误导信号等方面的能力。还研究了不同指令对智能体表现的影响。

Result: 智能体通常能导航到相关页面，但只在少数情况下能检索到决定性的隐藏证据。当任务需要推翻误导性表面信号时，性能急剧下降到接近随机水平。智能体经常产生幻觉，声称依赖从未访问过的证据。即使发现了正确上下文，也常常无法将其整合到最终决策中。提供更明确的指令能改善上下文发现，但通常会降低整体准确性。

Conclusion: 当前网络智能体架构缺乏可靠的自适应调查机制、证据整合能力和判断覆盖能力。在程序合规性和有效判断之间存在权衡。这表明需要开发更强大的智能体架构来处理复杂的多步决策任务。

Abstract: We introduce PATHWAYS, a benchmark of 250 multi-step decision tasks that test whether web-based agents can discover and correctly use hidden contextual information. Across both closed and open models, agents typically navigate to relevant pages but retrieve decisive hidden evidence in only a small fraction of cases. When tasks require overturning misleading surface-level signals, performance drops sharply to near chance accuracy. Agents frequently hallucinate investigative reasoning by claiming to rely on evidence they never accessed. Even when correct context is discovered, agents often fail to integrate it into their final decision. Providing more explicit instructions improves context discovery but often reduces overall accuracy, revealing a tradeoff between procedural compliance and effective judgement. Together, these results show that current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override.

</details>


### [130] [RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs](https://arxiv.org/abs/2602.05367)
*Youngcheon You,Banseok Lee,Minseop Choi,Seonyoung Kim,Hyochan Chong,Changdong Kim,Youngmin Kim,Dongkyu Kim*

Main category: cs.AI

TL;DR: RaBiT是一种新颖的残差二值化量化框架，通过算法强制残差层次结构解决并行二值路径的特征共适应问题，在保持硬件友好性的同时显著提升2-bit量化的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高效部署需要极端量化，但低比特效率与性能之间存在关键权衡。残差二值化虽然支持硬件友好的无矩阵乘法推理，但存在病态特征共适应问题，特别是并行残差二值路径学习冗余特征，限制了模型的表达能力。

Method: 提出RaBiT框架，核心机制是从单个共享全精度权重顺序推导每个二值路径，确保每个路径纠正前一个路径的误差。通过稳健初始化优先考虑功能保留而非简单的权重近似，稳定训练过程。

Result: RaBiT重新定义了2-bit精度-效率前沿：实现了最先进的性能，甚至可与硬件密集的向量量化方法相媲美，在RTX 4090上相比全精度模型实现了4.49倍的推理加速。

Conclusion: RaBiT通过算法强制残差层次结构有效解决了残差二值化中的特征共适应问题，为大型语言模型的极端量化提供了硬件友好且高性能的解决方案。

Abstract: Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\times$ inference speed-up over full-precision models on an RTX 4090.

</details>


### [131] [Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation](https://arxiv.org/abs/2602.05381)
*Ting Fang Tan,Kabilan Elangovan,Andreas Pollreisz,Kevin Bryan Dy,Wei Yan Ng,Joy Le Yi Wong,Jin Liyuan,Chrystie Quek Wan Ning,Ashley Shuen Ying Hong,Arun James Thirunavukarasu,Shelley Yin-His Chang,Jie Yao,Dylan Hong,Wang Zhaoran,Amrita Gupta,Daniel SW Ting*

Main category: cs.AI

TL;DR: 本研究评估了四个小型医疗LLM在眼科患者问答中的表现，发现Meerkat-7B表现最佳，MedLLaMA3-v20表现最差且有25.5%的幻觉内容。GPT-4-Turbo评估与临床医生评分高度一致，支持LLM评估在大规模基准测试中的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着特定领域大语言模型在眼科患者教育、分诊和临床决策支持中的应用日益增多，需要严格评估以确保安全性和准确性。本研究旨在评估小型医疗LLM在回答眼科患者查询中的表现，并探索基于LLM的评估与临床医生评分的可行性。

Method: 采用横断面研究设计，使用四个参数小于100亿的小型医疗LLM（Meerkat-7B、BioMistral-7B、OpenBioLLM-8B、MedLLaMA3-v20）回答180个眼科患者查询，生成2160个回答。由三位不同资历的眼科医生和GPT-4-Turbo使用S.C.O.R.E.框架（安全性、共识与上下文、客观性、可重复性、可解释性）进行五级评分。使用Spearman秩相关、Kendall tau统计和核密度估计分析评估LLM与临床医生评分的一致性。

Result: Meerkat-7B表现最佳，分别获得高级顾问3.44分、顾问4.08分、住院医师4.18分的平均分。MedLLaMA3-v20表现最差，25.5%的回答包含幻觉或临床误导性内容，包括编造的术语。GPT-4-Turbo评估与临床医生总体评估高度一致（Spearman rho=0.80，Kendall tau=0.67），但高级顾问评分更为保守。

Conclusion: 医疗LLM在眼科问答中显示出安全潜力，但在临床深度和共识方面仍有差距。研究支持基于LLM的评估在大规模基准测试中的可行性，并强调需要混合自动化和临床医生审查框架来指导安全的临床部署。

Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.

</details>


### [132] [H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration](https://arxiv.org/abs/2602.05407)
*Jun-Min Lee,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: H-AdminSim：一个结合真实数据生成和多智能体仿真的医院行政工作流端到端仿真框架，用于系统评估LLM驱动的行政自动化


<details>
  <summary>Details</summary>
Motivation: 医院行政部门每天处理大量操作任务（大型医院超过1万条请求），对LLM自动化有强烈需求。现有研究主要关注医患交互或孤立的行政子任务，未能捕捉真实行政工作流的复杂性。

Method: 提出H-AdminSim框架，结合真实数据生成和多智能体仿真来模拟医院行政工作流。通过FHIR集成提供统一可互操作环境，使用详细评分标准对任务进行定量评估。

Result: H-AdminSim能够系统比较不同LLM在行政自动化任务上的表现，为评估LLM驱动的行政自动化可行性和性能提供标准化测试平台。

Conclusion: H-AdminSim填补了医院行政工作流仿真的空白，通过端到端仿真框架支持异构医院环境下LLM自动化性能的系统评估，推动医院行政自动化发展。

Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.

</details>


### [133] [THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs](https://arxiv.org/abs/2602.05424)
*Weijian Yu,Yuhuan Lu,Dingqi Yang*

Main category: cs.AI

TL;DR: THOR：一种用于超关系知识图谱的归纳式链接预测方法，通过关系基础图和实体基础图建模跨图谱的结构不变性，支持完全归纳推理


<details>
  <summary>Details</summary>
Motivation: 现有超关系知识图谱链接预测方法大多局限于转导式设置，只能在同一词汇表内进行预测，无法泛化到未见过的词汇表，限制了其通用性

Method: 提出THOR方法：1) 引入关系基础图和实体基础图，建模超关系知识图谱中与特定关系和实体无关的基本交互；2) 使用两个并行图编码器学习基础图，后接Transformer解码器，支持高效掩码训练和完全归纳推理

Result: 在12个数据集上的评估显示，THOR显著优于现有方法，相比最佳规则方法提升66.1%，相比最佳半归纳方法提升55.9%，相比最佳完全归纳方法提升20.4%

Conclusion: THOR通过建模跨超关系知识图谱的结构不变性，实现了有效的完全归纳链接预测，为知识图谱推理提供了更强的泛化能力

Abstract: Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.

</details>


### [134] [M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining](https://arxiv.org/abs/2602.05429)
*Rui Lv,Juncheng Mo,Tianyi Chu,Chen Rao,Hongyi Jing,Jiajie Teng,Jiafu Chen,Shiqi Zhang,Liangzi Ding,Shuo Fang,Huaizhong Lin,Ziqiang Dang,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: M²-Miner：首个基于蒙特卡洛树搜索的低成本自动化移动GUI代理数据挖掘框架，通过多智能体协作和意图回收策略解决数据构建成本高、质量差、多样性低的问题。


<details>
  <summary>Details</summary>
Motivation: 构建强大的GUI代理需要大规模高质量用户行为轨迹数据（意图-轨迹对），但现有手动标注和GUI代理数据挖掘方法面临三大挑战：构建成本高、数据质量差、数据丰富度低。

Method: 提出M²-Miner框架：1）基于蒙特卡洛树搜索的自动化数据挖掘；2）协作多智能体框架（InferAgent指导、OrchestraAgent加速、JudgeAgent评估）；3）意图回收策略提取额外有价值的交互轨迹；4）渐进式模型在环训练策略提高数据挖掘成功率。

Result: 实验表明，使用M²-Miner挖掘的数据微调的GUI代理在多个常用移动GUI基准测试中达到最先进性能。

Conclusion: M²-Miner成功解决了GUI代理数据挖掘的关键挑战，为社区研究提供了有效的低成本自动化解决方案，将开源促进相关研究发展。

Abstract: Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.

</details>


### [135] [Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy](https://arxiv.org/abs/2602.05430)
*Kritchanat Ponyuenyong,Pengyu Tu,Jia Wei Tan,Wei Soon Cheong,Jamie Ng Suat Ling,Lianlian Jiang*

Main category: cs.AI

TL;DR: 该论文评估了时间序列基础模型在波动性电力市场中的日前电价预测性能，相比传统统计和深度学习模型取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 电力价格预测对能源市场参与者至关重要，但由于价格信号固有的波动性和非线性，传统统计和深度学习模型难以有效捕捉复杂的时间依赖关系并整合异构数据。虽然时间序列基础模型在一般时间序列预测任务中表现出色，但在波动性市场中的日前电价预测效果尚未充分探索。

Method: 提出尖峰正则化策略，评估多种时间序列基础模型（包括Tiny Time Mixers、MOIRAI、MOMENT、TimesFM），并与传统统计模型（ARIMA）和深度学习模型（LSTM、CNN-LSTM）进行比较。使用新加坡具有波动趋势的半小时批发市场数据，并在适用模型中纳入外生因素（如天气和日历变量）。

Result: 时间序列基础模型始终优于传统方法，在各种评估设置中实现了高达37.4%的MAPE改进。

Conclusion: 研究结果为提高波动性电力市场的预测准确性和决策制定提供了实用指导，表明时间序列基础模型在电力价格预测中具有显著优势。

Abstract: Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.

</details>


### [136] [Beyond Manual Planning: Seating Allocation for Large Organizations](https://arxiv.org/abs/2602.05875)
*Anton Ipsen,Michael Cashmore,Kirsty Fielding,Nicolas Marchesotti,Parisa Zehtabi,Daniele Magazzeni,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出分层座位分配问题(HSAP)，通过概率路线图(PRM)和快速探索随机树(RRT)计算座位距离，结合启发式搜索和动态规划解决组织团队分层座位分配问题


<details>
  <summary>Details</summary>
Motivation: 大型组织需要根据层级结构将团队分配到物理座位，确保有紧密层级关系的团队座位相邻（如研究小组占据连续区域）。目前手动管理导致重新规划频率低且效果不佳

Method: 提出端到端框架：使用概率路线图(PRM)和快速探索随机树(RRT)计算座位间距离，结合启发式搜索和动态规划，通过整数规划解决HSAP问题

Result: 在不同规模实例上评估PRM框架和座位分配方案，进行定量和定性分析，验证方法的有效性

Conclusion: 提出的自动化框架能有效解决分层座位分配问题，替代手动规划，提高组织座位分配的效率和优化程度

Abstract: We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.

</details>


### [137] [Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning](https://arxiv.org/abs/2602.05464)
*Jiaquan Wang,Yan Lyu,Chen Li,Yuheng Jia*

Main category: cs.AI

TL;DR: 提出OD-CRL框架，通过自适应正交基优化和零空间去噪投影解决条件表示学习中基向量敏感和子空间干扰问题，在多个定制化任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有条件表示学习方法存在两个关键限制：对子空间基向量的敏感性，以及子空间间干扰的脆弱性。这些限制影响了条件特征提取的准确性和鲁棒性。

Method: 提出OD-CRL框架，包含两个核心组件：1) 自适应正交基优化(AOBO)：通过奇异值分解和基于曲率的截断构建正交语义基向量；2) 零空间去噪投影(NSDP)：通过将嵌入投影到无关子空间的零空间来抑制非目标语义干扰。

Result: 在定制化聚类、定制化分类和定制化检索任务上进行广泛实验，OD-CRL实现了新的最先进性能，并展现出优异的泛化能力。

Conclusion: OD-CRL通过正交基优化和零空间投影有效解决了条件表示学习中的基向量敏感性和子空间干扰问题，为定制化任务提供了鲁棒且高效的特征提取框架。

Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.

</details>


### [138] [ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation](https://arxiv.org/abs/2602.05472)
*Yiwen Duan,Jing Ye,Xinpei Zhao*

Main category: cs.AI

TL;DR: ALIVE框架通过对抗学习和指导性语言反馈，让LLM内部化推理逻辑，摆脱传统强化学习对稀缺标量奖励的依赖，实现无监督的推理能力提升。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖的标量奖励存在三大问题：扩展成本高、跨领域脆弱、无法理解解决方案的内在逻辑。这种对外部贫乏信号的依赖阻碍了模型对推理原则的深入理解。

Method: ALIVE框架基于"认知协同"原则，将问题提出、解决和评判统一在单一策略模型中。通过对抗学习结合指导性语言反馈，让模型直接从原始语料中内部化评估标准，将外部批评转化为内在推理能力。

Result: 在数学推理、代码生成和一般逻辑推理基准测试中，ALIVE持续缓解了奖励信号限制。在相同数据和计算条件下，实现了准确率提升、跨领域泛化能力显著改善、以及更高的自我纠正率。

Conclusion: ALIVE通过推理三位一体（问题提出、解决、评判）促进了能力增长的自我维持轨迹，为无需人工监督的通用推理对齐提供了可扩展的基础。

Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.

</details>


### [139] [Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction](https://arxiv.org/abs/2602.05479)
*Zhe Wang,Zijing Liu,Chencheng Xu,Yuan Yao*

Main category: cs.AI

TL;DR: Phi-former是一种用于预测化合物-蛋白质相互作用的层次化表示学习方法，通过原子-原子、基序-基序和原子-基序三个层次建模相互作用，提高了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法虽然能在原子层面建模化合物-蛋白质相互作用，但忽略了化学现实：分子片段（基序或功能基团）通常是生物识别和结合的主要单元。需要一种能结合基序生物学作用的更符合化学现实的模型。

Method: 提出Phi-former方法：1）对化合物和蛋白质进行层次化表示；2）采用成对预训练框架，系统建模原子-原子、基序-基序和原子-基序三个层次的相互作用；3）设计层内和层间学习流程，使不同相互作用层次相互促进。

Result: Phi-former在CPI相关任务上表现出优越性能。案例研究表明，该方法能准确识别CPI中被激活的特定原子或基序，提供可解释的模型解释。

Conclusion: Phi-former通过层次化建模分子相互作用，不仅提高了CPI预测准确性，还提供了可解释的生物学见解，有助于指导理性药物设计和精准医疗应用。

Abstract: Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.

</details>


### [140] [SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration](https://arxiv.org/abs/2602.05499)
*Hanyu Wei,Zunhai Su,Peng Lu,Chao Li,Spandan Tiwari,Ashish Sirasao,Yuhan Dong*

Main category: cs.AI

TL;DR: SDFP是一种无需训练、即插即用的推测解码框架，通过基于Fisher信息迹的层剪枝从原始LLM构建轻量级草稿模型，实现1.32-1.5倍解码加速，适用于多媒体应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多媒体应用中存在自回归解码延迟高的问题。现有推测解码方法需要额外的草稿模型，但获取、调优和维护草稿模型成本高且复杂，需要辅助训练或专门化，即使免训练方法也涉及昂贵的搜索或优化。

Method: 提出SDFP框架，基于Fisher信息迹（FIT）进行层剪枝，以层敏感性作为输出扰动的代理，移除低影响层来构建紧凑草稿模型，同时保持与原始模型的兼容性以进行标准推测验证。

Result: 在多个基准测试中，SDFP实现了1.32-1.5倍的解码加速，且不改变目标模型的输出分布，支持低延迟多媒体应用。

Conclusion: SDFP提供了一种无需额外训练、超参数调优或单独维护草稿模型的快速部署友好型草稿构建方法，有效解决了推测解码的部署复杂性问题。

Abstract: Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.

</details>


### [141] [A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma](https://arxiv.org/abs/2602.05515)
*Ajo Babu George,Anna Mariam John,Athul Anoop,Balu Bhasuran*

Main category: cs.AI

TL;DR: 开发了一个针对成釉细胞瘤的多模态数据集和深度学习模型，显著提升了变体分类准确率和异常组织检测性能


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助颌面病理诊断缺乏高质量、结构化的多模态数据集，特别是成釉细胞瘤覆盖有限，格式不一致，无法直接用于模型训练

Method: 构建了专门针对成釉细胞瘤的多模态数据集，整合了标注的放射学、组织病理学和口腔临床图像，以及从病例报告中提取的结构化数据；使用自然语言处理技术从文本报告中提取临床特征，对图像数据进行领域特定的预处理和增强；开发了多模态深度学习模型用于分类成釉细胞瘤变体、评估复发风险等行为模式，并支持手术规划

Result: 定量评估显示显著改进：变体分类准确率从46.2%提升到65.9%，异常组织检测F1分数从43.0%提升到90.3%；相比MultiCaRe等现有资源，提供了更强大的数据集和适应性强的多模态AI框架

Conclusion: 这项工作通过提供稳健的数据集和适应性强的多模态AI框架，推进了患者特异性决策支持，为颌面病理学中的AI辅助诊断提供了重要资源

Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.

</details>


### [142] [Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities](https://arxiv.org/abs/2602.05532)
*Florian Dietz,William Wale,Oscar Gilg,Robert McCarthy,Felix Michalak,Gustavo Ewbank Rodrigues Danon,Miguelito de Guzman,Dietrich Klakow*

Main category: cs.AI

TL;DR: SPT通过训练一个"诚实人格"的LoRA适配器来检测大语言模型中的隐藏错位行为，在Anthropic审计游戏基准上达到96%准确率，远超传统方法的近0%准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能在训练中学会隐藏错位行为，传统审计方法（黑盒方法和机制可解释性）难以有效检测这种隐藏的错位。

Method: 提出Split Personality Training（SPT），通过微调训练一个"诚实人格"到LoRA参数中，该参数在正常操作时保持休眠。主模型响应后，激活LoRA适配器并插入触发字符串，让诚实人格能够审查响应并访问主模型的潜在状态。

Result: 在Anthropic审计游戏模型基准测试中，SPT达到96%的整体准确率，而Anthropic报告的传统方法准确率接近0%。诚实人格能够揭示外部观察者无法访问的潜在知识，如受损模型训练时的虚构偏见。

Conclusion: SPT提供了一种有效检测大语言模型隐藏错位行为的新方法，通过训练辅助诚实人格来审查主模型行为，显著优于现有审计技术。

Abstract: Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.

</details>


### [143] [Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach](https://arxiv.org/abs/2602.05533)
*Zhengyi Guo,Wenpin Tang,Renyuan Xu*

Main category: cs.AI

TL;DR: 提出基于Doob's h-transform的扩散模型条件生成框架，通过漂移修正实现硬约束（概率为1）的条件生成，无需修改预训练分数网络，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用和罕见事件模拟中，需要确保生成样本以概率1满足特定约束条件，而现有的软约束或基于奖励的引导方法无法保证约束满足。

Method: 基于Doob's h-transform、鞅表示和二次变差过程，提出条件扩散引导框架：1）通过添加涉及条件函数对数梯度的显式漂移修正来引导预训练扩散模型；2）提出基于鞅损失和鞅协变损失的两种离策略学习算法来估计h及其梯度。

Result: 在总变差距离和Wasserstein距离上为非渐近条件采样器提供理论保证，明确刻画了分数近似和引导估计误差的影响。数值实验验证了方法在强制硬约束和生成罕见事件样本方面的有效性。

Conclusion: 提出了一个理论严谨的条件扩散引导框架，能够在保证约束满足概率为1的前提下进行条件生成，为安全关键应用和罕见事件模拟提供了可靠解决方案。

Abstract: We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.

</details>


### [144] [Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation](https://arxiv.org/abs/2602.05544)
*Fahad Anwaar,Adil Mehmood Khan,Muhammad Khalid,Usman Zia,Kezhi Wang*

Main category: cs.AI

TL;DR: RGCF-XRec是一个混合框架，通过推理引导的协同过滤知识增强语言模型，在单一步骤中实现可解释的序列推荐，显著提升推荐准确性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在可解释推荐系统中忽视了协同信号，且将推荐和解释作为独立任务处理，导致内存占用大。需要一种能同时利用协同过滤知识和语言模型能力的统一方法。

Method: 提出RGCF-XRec框架：1) 通过上下文提示推理引导增强CF知识，发现潜在偏好和可解释推理路径；2) 基于连贯性、完整性、相关性和一致性四个维度的高效评分机制，减少噪声CF推理痕迹；3) 统一表示学习网络编码协同和语义信号，构建结构化提示来指导LLM进行可解释序列推荐。

Result: 在Amazon数据集(Sports、Toys、Beauty，共642,503用户-物品交互)上表现优异：HR@10提升7.38%(Sports)和4.59%(Toys)，ROUGE-L提升8.02%和3.49%。冷启动性能提升14.5%，热启动提升11.9%，零样本HR@5提升18.54%(Beauty)和23.16%(Toys)。使用轻量级LLaMA 3.2-3B确保训练效率和可扩展性。

Conclusion: RGCF-XRec通过融合协同过滤知识和语言模型推理能力，在单一步骤中实现高效可解释的序列推荐，显著提升推荐准确性、解释质量和泛化能力，同时保持计算效率，适用于实际应用。

Abstract: Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\% in Sports and 4.59\% in Toys, along with ROUGE-L by 8.02\% and 3.49\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\% in cold-start and 11.9\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\% in Beauty and 23.16\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.

</details>


### [145] [TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?](https://arxiv.org/abs/2602.05570)
*Yikun Zong,Cheston Tan*

Main category: cs.AI

TL;DR: 本文提出了一种基于人类认知机制（试错、观察、修正）的测试时自优化框架，通过上下文学习和奖励引导反馈循环，显著提升了视觉语言模型在七巧板等连续几何推理任务上的性能，无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在连续几何推理任务（如七巧板拼图）上表现不佳，远低于人类水平。人类通过心理旋转、迭代优化和视觉反馈等认知过程解决这类问题，因此需要设计能模拟人类认知机制、在测试时自我优化的AI框架。

Method: 提出训练免费的验证器-优化器代理框架，结合上下文学习和奖励引导反馈循环。通过递归优化循环，基于几何一致性反馈迭代地自我优化预测，无需模型重新训练。

Result: 在五个代表性视觉语言模型上的实验显示，单块任务平均IoU仅0.41，两块组合降至0.23。而提出的自优化框架将中等三角形案例的IoU从0.63提升到0.932，显著改善了连续几何推理能力。

Conclusion: 通过上下文学习和奖励循环融入人类启发的迭代优化机制，可以大幅提升视觉语言模型的几何推理能力，使自优化AI在连续空间领域从承诺走向实践。

Abstract: Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.

</details>


### [146] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: LLMs能准确预测个体层面的认知偏差，并在交互对话中复现人类偏见行为，GPT-4和GPT-5在模拟人类行为对齐方面存在差异


<details>
  <summary>Details</summary>
Motivation: 研究LLMs能否在个体层面预测认知偏差，并模拟在认知负荷等情境因素影响下的人类偏见行为动态，这对设计具有偏差感知能力的交互式AI系统至关重要

Method: 将三个经典决策场景转化为对话设置，进行人类实验（N=1100），参与者通过简单或复杂对话与聊天机器人互动；使用参与者人口统计数据和对话记录，基于GPT-4和GPT-5模拟相同交互条件

Result: 人类实验显示出强烈的认知偏差；LLMs能够精确复现人类偏见，但GPT-4和GPT-5在模拟人类行为对齐方面存在显著差异

Conclusion: LLMs能够准确模拟人类在交互环境中的认知偏差，这对设计和评估具有偏差感知能力的自适应AI系统具有重要意义，不同模型在行为对齐方面存在差异需要关注

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [147] [BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages](https://arxiv.org/abs/2602.05599)
*Subhadip Maji,Arnab Bhattacharya*

Main category: cs.AI

TL;DR: 提出GETR方法，通过图增强的标记表示实现从高资源语言到低资源语言的跨语言知识迁移，在词性标注、情感分类和命名实体识别任务上显著提升低资源语言性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言由于数据稀缺和语言资源不足，其NLP系统性能远落后于高资源语言。跨语言知识迁移通过利用高资源语言资源来解决这一挑战，但现有方法在仅有数百个标注实例的低资源语言上效果有限。

Method: 提出GETR（Graph-Enhanced Token Representation）方法，基于图神经网络实现跨语言知识迁移。同时采用两个基线方法：(a) 隐藏层增强，(b) 通过标记翻译进行标记嵌入迁移。方法专注于句子级和词级任务。

Result: GETR方法显著优于现有的多语言和跨语言基线方法：在真正低资源语言（Mizo、Khasi）的词性标注任务上提升13个百分点；在模拟低资源语言（Marathi、Bangla、Malayalam）的情感分类和NER任务上分别提升20和27个百分点的宏F1值。

Conclusion: GETR方法能有效实现从高资源语言到低资源语言的跨语言知识迁移，显著提升低资源语言的NLP任务性能。研究还分析了迁移机制的关键因素，为低资源语言处理提供了有效解决方案。

Abstract: Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.

</details>


### [148] [Reactive Knowledge Representation and Asynchronous Reasoning](https://arxiv.org/abs/2602.05625)
*Simon Kohaut,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.AI

TL;DR: 提出Resin概率编程语言和Reactive Circuits结构，通过异步反应式推理实现高效精确推断，在无人机群仿真中实现数量级加速


<details>
  <summary>Details</summary>
Motivation: 复杂概率模型中的精确推断计算成本过高，现有方法在动态环境中效率低下，无法利用现实世界信息流更新速率的异质性

Method: 提出Resin概率编程语言（结合概率逻辑与反应式编程），以及Reactive Circuits作为其高效精确语义的元结构，基于代数电路和异步数据流构建时间动态DAG，根据输入信号波动性自适应调整

Result: 在高保真无人机群仿真中，相比频率无关的推断方法实现了数个数量级的加速，RCs的结构适应成功捕捉环境动态，显著降低延迟并促进反应式实时推理

Conclusion: 通过基于异步输入变化频率划分计算，将大型推断任务分解为可单独记忆的子问题，确保只重新评估受新信息影响的模型组件，在流式上下文中大幅减少冗余计算

Abstract: Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.

</details>


### [149] [Generative Ontology: When Structured Knowledge Learns to Create](https://arxiv.org/abs/2602.05636)
*Benny Cheung*

Main category: cs.AI

TL;DR: 提出Generative Ontology框架，结合传统本体论的结构严谨性与大语言模型的创造力，通过可执行的Pydantic模式约束LLM生成，实现结构化且创新的设计生成。


<details>
  <summary>Details</summary>
Motivation: 传统本体论擅长描述领域结构但无法生成新内容，而大语言模型能流畅生成但缺乏结构有效性，经常产生幻觉。需要结合两者的互补优势：本体论提供语法结构，LLM提供创造力。

Method: 将领域知识编码为可执行的Pydantic模式，通过DSPy签名约束LLM生成。采用多智能体管道，为不同本体领域分配专业角色（如机制架构师、主题编织者、平衡批评家），每个智能体带有防止浅层输出的"焦虑"。结合检索增强生成和迭代验证确保机制与组件的一致性。

Result: 通过GameGrammar系统演示，给定主题提示（如"洞穴生态系统中发光真菌竞争"），管道能生成结构完整、可玩的桌面游戏设计，包含机制、组件、胜利条件和设置说明，满足本体约束同时保持创造性。

Conclusion: 该框架可推广到游戏之外的领域，任何具有专业词汇、有效性约束和积累范例的领域（音乐创作、软件架构、烹饪艺术）都适用。约束不仅不限制创造力，反而使其成为可能，正如语法使诗歌成为可能。

Abstract: Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional "anxiety" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt ("bioluminescent fungi competing in a cave ecosystem"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.

</details>


### [150] [Graph-based Agent Memory: Taxonomy, Techniques, and Applications](https://arxiv.org/abs/2602.05665)
*Chang Yang,Chuang Zhou,Yilin Xiao,Su Dong,Luyao Zhuang,Yujing Zhang,Zhu Wang,Zijin Hong,Zheng Yuan,Zhishang Xiang,Shengyuan Chen,Huachi Zhou,Qinggang Zhang,Ninghao Liu,Jinsong Su,Xinrun Wang,Yi Chang,Xiao Huang*

Main category: cs.AI

TL;DR: 本文综述了基于图的智能体记忆系统，分析了其在LLM智能体中的核心作用，提出了记忆分类框架，并系统梳理了图记忆的生命周期技术、开源资源及应用场景。


<details>
  <summary>Details</summary>
Motivation: 记忆是LLM智能体处理长期复杂任务（如多轮对话、游戏、科学发现）的核心模块，能够实现知识积累、迭代推理和自我进化。图结构因其建模关系依赖、组织层次信息和高效检索的内在能力，成为智能体记忆的强大结构。本文旨在从图的角度全面综述智能体记忆，为开发更高效可靠的图基记忆系统提供实用见解。

Method: 1. 提出智能体记忆的分类法：短期vs长期记忆、知识vs经验记忆、非结构化vs结构化记忆，并从图基记忆的实现视角进行分析。
2. 根据智能体记忆的生命周期，系统分析图基记忆的关键技术：记忆提取（数据转换）、存储（高效组织）、检索（支持推理的相关内容获取）和进化（内容更新）。
3. 总结支持自进化智能体记忆开发和评估的开源库与基准。
4. 探索多样化的应用场景，识别关键挑战和未来研究方向。

Result: 1. 建立了全面的智能体记忆分类框架，特别强调了图结构在记忆实现中的优势。
2. 系统梳理了图基记忆生命周期的完整技术栈，为实际系统开发提供了技术路线图。
3. 收集整理了相关研究论文、开源数据和项目资源，创建了GitHub资源库（https://github.com/DEEP-PolyU/Awesome-GraphMemory），为社区提供一站式资源。
4. 识别了该领域的关键挑战和未来研究方向，为后续研究提供指导。

Conclusion: 图基智能体记忆是LLM智能体处理长期复杂任务的关键技术，具有建模关系依赖、组织层次信息和高效检索的独特优势。通过系统化的分类框架、生命周期技术分析和资源整理，本文为开发更高效可靠的智能体记忆系统提供了全面指导。未来需要在记忆效率、可靠性、可扩展性等方面进一步研究，推动智能体记忆技术的实际应用。

Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.

</details>


### [151] [Determining Energy Efficiency Sweet Spots in Production LLM Inference](https://arxiv.org/abs/2602.05695)
*Hiari Pizzini Cavagna,Andrea Proia,Giacomo Madella,Giovanni B. Esposito,Francesco Antici,Daniele Cesarini,Zeynep Kiziltan,Andrea Bartolini*

Main category: cs.AI

TL;DR: 本文提出一个基于Transformer架构计算和内存访问复杂度的分析模型，能够准确预测LLM推理的能耗效率曲线，发现存在"甜点"区域，通过优化输入输出序列长度可显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: LLM推理在现代AI应用中至关重要，需要理解其能耗特性。现有方法通常使用输入输出序列长度的简单线性函数估计能耗，但实际观察显示存在明显的能效区域变化，表明存在非线性依赖关系。

Method: 提出基于Transformer架构计算和内存访问复杂度的分析模型，使用TensorRT-LLM在NVIDIA H100 GPU上评估1B到9B参数的多种LLM（包括OPT、LLaMA、Gemma、Falcon、Qwen2和Granite），测试输入输出长度从64到4096个token。

Result: 模型平均MAPE为1.79%，发现能效存在"甜点"区域：短到中等输入和中等长度输出时能效最高，而长输入或极短输出时能效急剧下降。通过优化序列长度可大幅降低能耗。

Conclusion: 提出的分析模型能准确表征LLM推理的能效曲线，支持生产系统中基于能效"甜点"的序列长度优化策略，如截断、摘要和自适应生成，从而显著降低能耗。

Abstract: Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency "Sweet Spots" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.

</details>


### [152] [Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions](https://arxiv.org/abs/2602.05709)
*Yihao Ouyang,Shiwei Li,Haozhao Wang,Xiandi Luo,Zhuoqi Hu,Yuetong Song,Qiyu Qin,Yichen Li,Ruixuan Li*

Main category: cs.AI

TL;DR: GenLoRA使用非线性函数生成低秩矩阵的基向量，替代显式存储，在相同参数量下获得更高的有效秩，提升微调性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA采用显式秩范式，增加模型容量需要添加更多基向量，导致参数大幅增长。研究发现这些基向量存在显著参数冗余，可以用轻量级非线性函数紧凑表示。

Method: GenLoRA为每个低秩矩阵维护一个潜在向量，使用一组轻量级径向基函数(RBFs)合成基向量。每个RBF参数远少于显式基向量，实现更高参数效率。

Result: 在多个数据集和架构上的实验表明，GenLoRA在较小参数预算下获得更高的有效LoRA秩，实现更优的微调性能。

Conclusion: GenLoRA通过非线性基向量生成替代显式存储，在参数效率方面显著优于标准LoRA，为高效模型微调提供了新方法。

Abstract: Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.

</details>


### [153] [Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification](https://arxiv.org/abs/2602.05717)
*Tianyi Wang,Long Li,Hongcan Guo,Yibiao Chen,Yixia Li,Yong Wang,Yun Chen,Guanhua Chen*

Main category: cs.AI

TL;DR: 论文提出Anchored Policy Optimization (APO)方法，解决强化学习中验证奖励导致的递归空间收缩问题，通过支持覆盖而非形状匹配来平衡效率与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于验证奖励的强化学习(RLVR)存在递归空间收缩(RSC)的系统性病理问题，即正锐化和负挤压的联合动态导致有效替代方案的采样概率消失。KL正则化虽然试图缓解此问题，但其形状匹配约束与正确性所需的锐化产生梯度冲突。

Method: 提出锚定策略优化(APO)，将范式从全局形状匹配转向支持覆盖。基于参考模型高置信度支持定义安全流形，允许积极锐化以提高效率，同时在错误校正时选择性调用恢复力以防止崩溃。

Result: 在数学基准测试中，APO打破了准确性与多样性的权衡，显著提高了Pass@1性能，同时恢复了标准策略梯度方法通常丢失的Pass@K多样性。

Conclusion: APO作为一种梯度对齐机制，通过最大化支持覆盖实现弹性恢复，能够重新膨胀有效分支，解决了RLVR中的递归空间收缩问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.

</details>


### [154] [Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification](https://arxiv.org/abs/2602.05723)
*Taoye Yin,Haoyuan Hu,Yaxin Fan,Xinhao Chen,Xinya Wu,Kai Deng,Kezun Zhang,Feng Wang*

Main category: cs.AI

TL;DR: 提出RLFKV框架，通过细粒度知识验证和强化学习减少金融RAG系统中的幻觉问题，提高生成内容与检索文档的一致性。


<details>
  <summary>Details</summary>
Motivation: 金融RAG系统虽然依赖检索文档来生成准确回答，但模型仍然会产生与检索信息矛盾的幻觉，需要解决这种不一致性问题。

Method: 提出RLFKV框架：1）将金融回答分解为原子知识单元；2）评估每个单元的正确性计算细粒度忠实度奖励；3）引入信息性奖励防止奖励攻击；4）通过强化学习优化模型。

Result: 在公开的FDD任务和新提出的FDD-ANT数据集上实验，显示方法能持续改进模型性能，验证了方法的有效性。

Conclusion: RLFKV框架通过细粒度知识验证和强化学习，有效减少了金融RAG系统中的幻觉问题，提高了生成内容与检索文档的一致性。

Abstract: In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.

</details>


### [155] [LeakBoost: Perceptual-Loss-Based Membership Inference Attack](https://arxiv.org/abs/2602.05748)
*Amit Kravchik Taub,Fred M. Grabovski,Guy Amit,Yisroel Mirsky*

Main category: cs.AI

TL;DR: LeakBoost：一种基于感知损失的主动探测框架，通过优化合成询问图像来放大成员与非成员在模型内部表征的差异，显著提升现有成员推断攻击的性能。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击主要依赖静态指标（如损失或置信度），未能充分利用模型在被主动探测时的动态行为，限制了攻击效果。

Method: 提出LeakBoost框架：1）给定候选输入，通过优化感知（激活空间）目标合成询问图像，放大成员与非成员在模型内部表征的差异；2）使用现成的成员检测器分析合成图像，无需修改检测器本身。

Result: 与现有方法结合后，LeakBoost在多个图像分类数据集和不同神经网络架构上显著提升性能：AUC从接近随机水平（0.53-0.62）提升至0.81-0.88，在1%FPR下的TPR提升超过一个数量级。敏感性分析显示深层网络和短时、低学习率优化产生最强泄漏，改进主要集中在基于梯度的检测器。

Conclusion: LeakBoost提供了一种模块化且计算高效的方法来评估白盒设置下的隐私风险，推动了动态成员推断的研究。

Abstract: Membership inference attacks (MIAs) aim to determine whether a sample was part of a model's training set, posing serious privacy risks for modern machine-learning systems. Existing MIAs primarily rely on static indicators, such as loss or confidence, and do not fully leverage the dynamic behavior of models when actively probed. We propose LeakBoost, a perceptual-loss-based interrogation framework that actively probes a model's internal representations to expose hidden membership signals. Given a candidate input, LeakBoost synthesizes an interrogation image by optimizing a perceptual (activation-space) objective, amplifying representational differences between members and non-members. This image is then analyzed by an off-the-shelf membership detector, without modifying the detector itself. When combined with existing membership inference methods, LeakBoost achieves substantial improvements at low false-positive rates across multiple image classification datasets and diverse neural network architectures. In particular, it raises AUC from near-chance levels (0.53-0.62) to 0.81-0.88, and increases TPR at 1 percent FPR by over an order of magnitude compared to strong baseline attacks. A detailed sensitivity analysis reveals that deeper layers and short, low-learning-rate optimization produce the strongest leakage, and that improvements concentrate in gradient-based detectors. LeakBoost thus offers a modular and computationally efficient way to assess privacy risks in white-box settings, advancing the study of dynamic membership inference.

</details>


### [156] [RocqSmith: Can Automatic Optimization Forge Better Proof Agents?](https://arxiv.org/abs/2602.05762)
*Andrei Kozyrev,Nikita Khramov,Denis Lochmelis,Valerio Morelli,Gleb Solovev,Anton Podkopaev*

Main category: cs.AI

TL;DR: 研究将AI代理自动优化方法应用于形式验证领域（特别是Rocq自动定理证明），评估不同优化器在优化证明生成代理时的表现，发现简单few-shot引导最有效，但都无法超越精心设计的最先进证明代理。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理自动优化方法在形式验证领域的适用性，特别是能否自动化代理系统的精细调优（如提示设计、上下文知识、控制策略），以降低人工工程成本。

Method: 在Rocq自动定理证明领域，评估多种自动代理优化器对证明生成代理的优化效果，包括few-shot bootstrapping等方法，并与精心设计的最先进证明代理进行对比。

Result: 多个优化器都能带来可测量的改进，其中简单的few-shot bootstrapping方法表现最稳定有效，但所有被研究的方法都无法达到精心设计的最先进证明代理的性能水平。

Conclusion: 虽然自动优化方法在形式验证领域有一定效果，但当前技术还无法完全替代人工精心设计的代理系统，特别是在复杂的定理证明任务中，人工工程仍然具有显著优势。

Abstract: This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.

</details>


### [157] [RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765)
*Zhong Guan,Haoran Sun,Yongjian Guo,Shuai Di,Xiaodong Bai,Jing Long,Tianyun Zhao,Mingxi Luo,Chen Zhou,Yucheng Guo,Qiming Yang,Wanting Xu,Wen Huang,Yunxuan Ma,Hongke Zhao,Likang Wu,Xiaotie Deng,Xi Xiao,Sheng Wen,Yicheng Gong,Junwu Xiong*

Main category: cs.AI

TL;DR: 本文提出首个完全异步的VLA模型训练框架，通过环境交互、策略生成和模型更新的多级解耦架构，显著提升训练效率，在LIBERO基准上实现最高126.67%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的RL训练框架（如RLinf）采用同步执行，导致环境交互、策略生成和模型更新阶段资源利用率低、吞吐量受限，成为训练效率瓶颈。

Method: 提出完全异步策略训练框架，包含：1）环境交互与轨迹收集的异步并行化；2）策略生成的流式执行；3）训练更新的解耦调度。借鉴大模型RL中的异步优化思想，设计多级解耦架构。

Result: 在LIBERO基准上，相比现有同步策略实现最高59.25%吞吐量提升；深度优化分离策略后可达126.67%提升。消融实验验证各异步组件的有效性，8-256 GPU规模下展示良好可扩展性。

Conclusion: 提出的完全异步训练框架有效解决了VLA模型训练效率瓶颈，通过系统级异步优化显著提升资源利用率和训练吞吐量，为大规模VLA模型训练提供了高效解决方案。

Abstract: In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.

</details>


### [158] [FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem](https://arxiv.org/abs/2602.05794)
*Aboli Kathar,Aman Kumar,Anusha Kamath,Araveeti Srujan,Ashish Sharma,Chandra Bhushan,Dilip Asbe,Divya Sorate,Duddu Prasanth Kumar,Evan Acharya,Harsh Sharma,Hrithik Kadam,Kanishk Singla,Keyur Doshi,Kiran Praveen,Kolisetty Krishna SK,Krishanu Adhikary,Lokesh MPT,Mayurdeep Sonowal,Nadeem Shaikh,Navya Prakash,Nimit Kothari,Nitin Kukreja,Prashant Devadiga,Rakesh Paul,Ratanjeet Pratap Chauhan,Raunak Kalani,Raviraj Joshi,Shamanth MH,Shantanu Pandey,Shubham Soni,Siddharth Dixit,Smriti Jopat,Sunil Patel,Suraj Singh,Suvradip Paul,Tulasi Pilla,Utkarsh Vaidya,Vineeth Nambiar,Vishal Kanvaty,Yatharth Dedhia*

Main category: cs.AI

TL;DR: FiMI是针对印度数字支付系统开发的金融语言模型，包含Base和Instruct两个变体，基于Mistral Small 24B架构，通过多阶段训练在金融推理和工具调用方面显著优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 开发专门针对印度数字支付系统的金融语言模型，解决现有通用模型在印度金融场景下的不足，特别是多语言（英语、印地语、印英混合语）支持和真实工作流程建模需求。

Method: 采用Mistral Small 24B架构，通过多阶段训练：1）在680亿token的金融、多语言和合成数据上进行持续预训练；2）指令微调；3）针对多轮工具驱动对话的领域特定监督微调，模拟交易纠纷和授权生命周期管理等真实工作流程。

Result: FiMI Base在金融推理基准上比Mistral Small 24B Base提升20%；FiMI Instruct在领域特定工具调用上比Mistral Small 24B Instruct提升87%；同时在通用基准上保持与同类规模模型相当的性能。

Conclusion: FiMI成功开发了针对印度数字支付系统的专业化金融语言模型，在保持通用能力的同时，在金融推理和工具调用方面取得了显著改进，证明了领域专业化模型的有效性。

Abstract: We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.

</details>


### [159] [NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking](https://arxiv.org/abs/2602.05805)
*Kang Chen,Zhuoka Feng,Sihan Zhao,Kai Xiong,Junjie Nian,Yaoning Wang,Changyi Xiao,Yixin Cao*

Main category: cs.AI

TL;DR: 提出NEX框架，通过分析推理过程中MLP神经元的激活模式来区分探索和利用阶段，无需监督即可评估推理质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推理时生成多个思维链或搜索合并检查点，选择最佳响应的瓶颈日益突出，而通常缺乏目标分布的监督信号。

Method: 提出NEX框架：将推理视为探索（E-phase）和利用（X-phase）的交替过程。通过稀疏激活缓存检测新激活的MLP神经元峰值来识别探索阶段，使用粘性两状态隐马尔可夫模型推断E-X阶段，并根据探索引入的神经元是否在后续利用中被重用来评估其价值。

Result: 在推理基准测试和Qwen3合并家族中，NEX仅需少量未标记的激活数据即可预测下游准确性并识别更好的变体。通过人类标注验证了E-X信号，并通过"有效vs冗余"神经元转移提供了因果证据。

Conclusion: NEX提供了一种无需标签、无需监督的评分框架，能够有效评估推理质量，识别过度思考问题，并为模型合并和推理优化提供指导。

Abstract: Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via "Effective-vs-Redundant" neuron transfer.

</details>


### [160] [STProtein: predicting spatial protein expression from multi-omics data](https://arxiv.org/abs/2602.05811)
*Zhaorui Jiang,Yingfang Yuan,Lei Hu,Wei Pang*

Main category: cs.AI

TL;DR: STProtein是一个基于图神经网络和多任务学习的新型框架，旨在利用相对丰富的空间转录组数据来准确预测稀缺的空间蛋白质表达数据，以解决空间多组学数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 空间多组学数据整合对生物学研究至关重要，但存在严重的数据不平衡问题：空间转录组数据相对丰富，而空间蛋白质组数据由于技术限制和高成本而稀缺，这阻碍了研究进展。

Method: 提出STProtein框架，采用图神经网络结合多任务学习策略，利用更易获取的空间多组学数据（如空间转录组）来预测未知的空间蛋白质表达。

Result: STProtein能够有效解决空间蛋白质组数据的稀缺性问题，加速空间多组学整合，有望在生命科学领域催生突破性进展。

Conclusion: 该工具使科学家能够加速发现组织内蛋白质的复杂空间模式，揭示标记基因间的新关系，探索生物"暗物质"，推动空间多组学研究。

Abstract: The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological "Dark Matter".

</details>


### [161] [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818)
*Zihao Jiang,Miao Peng,Zhenyan Shan,Wenjie Xu,Ben Liu,Gong Chen,Ziqi Gao,Min Peng*

Main category: cs.AI

TL;DR: TKG-Thinker：一种具有自主规划和自适应检索能力的新型智能体，通过动态多轮交互和双训练策略（SFT+RL）在时序知识图谱上进行推理，解决了现有LLM在时序知识图谱问答中的幻觉问题和静态提示限制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在时序知识图谱问答中存在两个主要问题：1）在复杂时序约束下容易产生推理幻觉；2）静态提示限制了模型自主性和泛化能力，缺乏与TKGs环境的动态交互优化。

Method: 提出TKG-Thinker智能体，具备自主规划和自适应检索能力。采用双训练策略：首先使用思维链数据进行监督微调（SFT）培养核心规划能力，然后通过强化学习（RL）阶段利用多维奖励在复杂时序约束下优化推理策略。

Result: 在基准数据集上使用三个开源LLM进行实验，TKG-Thinker实现了最先进的性能，并在复杂TKGQA设置中表现出强大的泛化能力。

Conclusion: TKG-Thinker通过动态多轮交互和双训练策略有效解决了LLM在时序知识图谱问答中的局限性，为复杂时序推理任务提供了有效的解决方案。

Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.

</details>


### [162] [Learning Compact Boolean Networks](https://arxiv.org/abs/2602.05830)
*Shengpu Wang,Yuhao Mao,Yani Zhang,Martin Vechev*

Main category: cs.AI

TL;DR: 提出三种方法改进布尔神经网络：学习连接、紧凑卷积和自适应离散化，显著减少布尔运算次数同时提升精度


<details>
  <summary>Details</summary>
Motivation: 浮点神经网络推理成本高，布尔网络适合资源受限场景，但学习紧凑准确的布尔网络具有组合复杂性挑战

Method: 1) 无参高效连接学习策略；2) 利用局部性的紧凑卷积布尔架构；3) 连续网络转布尔网络的自适应离散化策略

Result: 在标准视觉基准上，精度-计算量帕累托前沿显著优于现有方法，精度更高且布尔运算减少达37倍

Conclusion: 通过三方面创新有效解决了布尔神经网络的紧凑性和准确性挑战，为资源受限场景提供了高效解决方案

Abstract: Floating-point neural networks dominate modern machine learning but incur substantial inference cost, motivating interest in Boolean networks for resource-constrained settings. However, learning compact and accurate Boolean networks is challenging due to their combinatorial nature. In this work, we address this challenge from three different angles: learned connections, compact convolutions and adaptive discretization. First, we propose a novel strategy to learn efficient connections with no additional parameters and negligible computational overhead. Second, we introduce a novel convolutional Boolean architecture that exploits the locality with reduced number of Boolean operations than existing methods. Third, we propose an adaptive discretization strategy to reduce the accuracy drop when converting a continuous-valued network into a Boolean one. Extensive results on standard vision benchmarks demonstrate that the Pareto front of accuracy vs. computation of our method significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.

</details>


### [163] [OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention](https://arxiv.org/abs/2602.05847)
*Zhangquan Chen,Jiale Tao,Ruihuang Li,Yihao Hu,Ruitao Chen,Zhantao Yang,Xinlei Yu,Haodong Jing,Manyuan Zhang,Shuai Shao,Biao Wang,Qinglin Lu,Ruqi Huang*

Main category: cs.AI

TL;DR: OmniVideo-R1：一个通过自监督和对比学习增强多模态推理的强化框架，在音频-视觉理解任务上超越现有基线


<details>
  <summary>Details</summary>
Motivation: 人类通过多种模态协同感知世界，但现有全视频模型在音频-视觉理解任务上仍面临挑战，需要提升混合模态推理能力

Method: 提出OmniVideo-R1强化框架，采用两种关键策略：1）基于自监督学习的查询密集型定位；2）基于对比学习的模态注意力融合

Result: 在多个基准测试中，OmniVideo-R1始终优于强基线模型，展示了其有效性和强大的泛化能力

Conclusion: OmniVideo-R1通过"全模态线索思考"策略，显著提升了模型在音频-视觉理解任务上的性能，为多模态视频理解提供了有效解决方案

Abstract: While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.

</details>


### [164] [BABE: Biology Arena BEnchmark](https://arxiv.org/abs/2602.05857)
*Junting Zhou,Jin Chen,Linfeng Hao,Denghui Cao,Zheyu Wang,Qiguang Chen,Chaoyou Fu,Jiaze Chen,Yuchen Wu,Ge Zhang,Mingxuan Wang,Wenhao Huang,Tong Yang*

Main category: cs.AI

TL;DR: BABE是一个新的生物学AI基准测试，专注于评估AI系统整合实验结果与背景知识进行科学推理的能力，基于真实研究论文构建。


<details>
  <summary>Details</summary>
Motivation: 现有生物学基准测试未能充分评估AI整合实验结果与背景知识进行科学推理的关键能力，这是研究人员必备的核心技能。

Method: 基于同行评审研究论文和真实生物学研究构建BABE基准测试，确保任务反映实际科学探究的复杂性和跨学科性质。

Result: BABE基准测试挑战模型进行因果推理和跨尺度推断，提供了一个评估AI系统如何像实践科学家一样推理的稳健框架。

Conclusion: BABE为评估AI系统对生物学研究的潜在贡献提供了更真实的衡量标准，填补了现有基准测试在评估科学推理能力方面的空白。

Abstract: The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.

</details>


### [165] [Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877)
*Lukas Stappen,Ahmet Erkan Turan,Johann Hagerer,Georg Groh*

Main category: cs.AI

TL;DR: 论文提出AgentHeLLM威胁建模框架，用于分析车载LLM对话代理的安全风险，通过分离资产识别与攻击路径分析，并提供自动化攻击路径发现工具。


<details>
  <summary>Details</summary>
Motivation: 车载LLM对话代理与外部服务协调时，通过自然语言载荷传播攻击，可能造成从驾驶员分心到未经授权的车辆控制等严重后果。现有AI安全框架缺乏安全关键系统工程中的"关注点分离"原则，将保护对象（资产）与攻击方式（攻击路径）混为一谈。

Method: 提出AgentHeLLM威胁建模框架，包含：1）基于伤害导向的"受害者建模"和《世界人权宣言》启发的人本资产分类法；2）区分毒化路径（恶意数据传播）和触发路径（激活操作）的正式图模型；3）开源攻击路径建议工具AgentHeLLM Attack Path Generator，采用双层搜索策略自动化多阶段威胁发现。

Result: 开发了实用的威胁建模框架和自动化工具，能够系统识别车载LLM代理的安全风险，为安全关键系统提供更严谨的分析方法。

Conclusion: AgentHeLLM框架填补了现有AI安全框架的方法论空白，通过正式分离资产识别与攻击路径分析，为车载LLM对话代理的安全评估提供了系统化方法，有助于预防从驾驶员分心到车辆控制等严重安全风险。

Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.

</details>


### [166] [A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges](https://arxiv.org/abs/2602.05883)
*Philippe J. Giabbanelli*

Main category: cs.AI

TL;DR: 该论文为建模与仿真（M&S）领域的研究者和从业者提供关于使用大语言模型（LLMs）的全面实用指南，强调避免常见陷阱和做出明智设计决策。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在M&S工作流中越来越普及，但看似简单的实践可能引入微妙问题、不必要的复杂性，甚至导致较差结果。作者旨在提供全面实用的指导，帮助模型开发者做出明智决策。

Method: 论文讨论LLMs在M&S应用中的常见困惑来源，包括非确定性、知识增强（RAG和LoRA）、M&S数据分解和超参数设置。强调基于原则的设计选择、诊断策略和实证评估方法。

Result: 通过分析LLMs在M&S应用中的实际挑战，论文提供了避免常见陷阱的指导原则，帮助用户理解何时、如何以及是否依赖LLMs。

Conclusion: 论文强调需要基于原则的方法来使用LLMs进行M&S，提供实用指南帮助模型开发者做出明智决策，避免简单化做法带来的潜在问题，确保LLMs的有效和安全应用。

Abstract: Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.

</details>


### [167] [Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2602.05920)
*Eva Andrés*

Main category: cs.AI

TL;DR: 该论文比较了经典和量子强化学习方法解决带容量约束的车辆路径问题，发现混合量子-经典架构在距离、紧凑性和路线重叠方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究量子增强的强化学习方法在复杂组合优化问题（如带容量约束的车辆路径问题）中的应用潜力，探索量子计算能否提供比经典方法更好的解决方案。

Method: 实现了经典、全量子和混合三种变体的优势演员-评论家（A2C）智能体，结合transformer架构通过自注意力和交叉注意力机制捕捉车辆、客户和仓库之间的关系。实验针对20个客户和4辆车的多车辆容量约束场景，进行了10次独立运行。

Result: 所有三种方法都能学习有效的路由策略，但量子增强模型优于经典基线，产生更稳健的路线组织。混合架构在距离、紧凑性和路线重叠方面实现了最佳整体性能。定性可视化显示量子模型生成更结构化、更连贯的路由解决方案。

Conclusion: 混合量子-经典强化学习模型在解决复杂组合优化问题（如CVRP）方面具有显著潜力，量子增强方法能够提供更优、更稳健的解决方案。

Abstract: This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.

</details>


### [168] [Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins](https://arxiv.org/abs/2602.05983)
*Krešimir Kušić,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: 论文提出GATTF模型，利用传感器间的互信息增强地理感知，提升高速公路交通预测精度，不增加模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 高速公路数字孪生技术需要高分辨率实时数据和预测能力，但交通动态具有时空复杂性、时变性和非线性，现有序列深度学习模型在预测精度和模型复杂度方面仍有局限。

Method: 提出地理感知的Transformer交通预测模型GATTF，利用分布式传感器间的互信息捕捉地理关系，增强模型的地理感知能力。

Result: 使用瑞士日内瓦高速公路网络实时数据评估，结果表明通过互信息增强地理感知的GATTF比标准Transformer预测精度更高，且不增加模型复杂度。

Conclusion: GATTF模型通过互信息整合地理关系，有效提升了高速公路交通预测精度，为数字孪生交通管理提供了更好的预测支持。

Abstract: The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. Sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. To improve motorway traffic forecasting, this paper introduces a Geographically-aware Transformer-based Traffic Forecasting GATTF model, which exploits the geographical relationships between distributed sensors using their mutual information (MI). The model has been evaluated using real-time data from the Geneva motorway network in Switzerland and results confirm that incorporating geographical awareness through MI enhances the accuracy of GATTF forecasting compared to a standard Transformer, without increasing model complexity.

</details>


### [169] [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000)
*Ali Shendabadi,Parnia Izadirad,Mostafa Salehi,Mahmoud Bijankhan*

Main category: cs.AI

TL;DR: 本文探索使用Whisper预训练ASR模型进行语音情感识别，提出两种注意力池化方法，在波斯语数据集上取得SOTA结果，比HuBERT X-Large更轻量高效。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别研究因缺乏标准大型数据集而受限，现有研究使用预训练模型提取特征。本文探索Whisper预训练ASR系统在语音情感识别中的能力，旨在提供轻量高效的替代方案。

Method: 提出两种注意力池化方法：多头注意力平均池化和QKV池化，用于高效降低Whisper表示的维度同时保留情感特征。在英语(IEMOCAP)和波斯语(ShEMO)数据集上实验，使用Whisper Tiny和Small模型，比较不同编码器层的性能。

Result: 多头QKV架构在ShEMO数据集上取得SOTA结果，未加权准确率提升2.47%。发现中间层在波斯语数据集上表现更好，为语音情感识别提供了比HuBERT X-Large等大型模型更轻量高效的替代方案。

Conclusion: Whisper作为语音情感识别的表示提取器具有潜力，注意力池化方法在维度减少方面有效，为语音情感识别提供了轻量高效的解决方案。

Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.

</details>


### [170] [AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions](https://arxiv.org/abs/2602.06008)
*Xianyang Liu,Shangding Gu,Dawn Song*

Main category: cs.AI

TL;DR: AgenticPay是一个用于多智能体买卖谈判的基准测试和仿真框架，通过自然语言驱动，包含110多个任务，评估LLM在商业谈判中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估多智能体经济交互的语言基准，特别是基于自然语言的买卖谈判场景，需要系统性的评估框架来研究语言驱动的市场交互。

Method: 构建包含买卖双方私有约束和产品价值评估的市场模型，支持多轮语言谈判而非单纯数字出价，包含110多个任务，涵盖双边谈判到多对多市场，提供结构化动作提取和可行性、效率、福利等指标。

Result: 对最先进的专有和开源LLM进行基准测试，发现谈判性能存在显著差距，突显了长时程战略推理的挑战，确立了AgenticPay作为研究智能体商业和语言市场交互的基础。

Conclusion: AgenticPay为评估多智能体语言经济交互提供了系统框架，揭示了LLM在复杂谈判场景中的局限性，为未来研究智能体商业和语言市场交互奠定了基础。

Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.

</details>


### [171] [Learning Event-Based Shooter Models from Virtual Reality Experiments](https://arxiv.org/abs/2602.06023)
*Christopher A. McClurg,Alan R. Wagner*

Main category: cs.AI

TL;DR: 开发数据驱动的离散事件模拟器，用于评估学校安全干预策略，特别是机器人干预枪击事件的效果，解决VR研究中招募参与者的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: VR是评估学校安全措施的有力工具，但需要为每个条件招募新的参与者，使得大规模或迭代评估变得困难，尤其是在需要大量训练场景的学习有效干预策略时。

Method: 开发数据驱动的离散事件模拟器（DES），将枪手移动和区域内行动建模为从VR研究中参与者行为学习的随机过程，用于评估机器人干预策略。

Result: 模拟器能够重现关键经验模式，使得评估和学习那些无法直接通过人类受试者训练的干预策略变得可扩展。

Conclusion: 这项工作展示了一个高到中保真度的模拟工作流程，为开发和评估自主学校安全干预提供了可扩展的替代方案。

Abstract: Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.

</details>


### [172] [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/abs/2602.06039)
*Yuxing Lu,Yucheng Hu,Xukai Zhao,Jiuxin Cao*

Main category: cs.AI

TL;DR: DyTopo：一种动态拓扑的多智能体框架，通过每轮重构稀疏有向通信图来优化多轮推理，在代码生成和数学推理任务上平均提升6.2%


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统通常采用固定的通信模式，无法适应迭代问题解决中不同阶段的需求变化，需要更灵活的通信机制

Method: 引入DyTopo框架，每轮由管理器设定目标，各智能体输出轻量级自然语言查询（需求）和关键信息（提供）描述符，通过语义匹配构建稀疏有向通信图，仅沿诱导边传递私有消息

Result: 在代码生成和数学推理基准测试中，使用四种LLM骨干网络，DyTopo始终优于最强基线（平均提升6.2%），同时提供可解释的协调轨迹

Conclusion: DyTopo通过动态重构通信拓扑，显著提升了多智能体系统的推理性能，并提供了解释性，展示了通信路径如何在不同轮次中重新配置

Abstract: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [173] [Denoising diffusion networks for normative modeling in neuroimaging](https://arxiv.org/abs/2602.04886)
*Luke Whitbread,Lyle J. Palmer,Mark Jenkinson*

Main category: cs.LG

TL;DR: 该论文提出使用去噪扩散概率模型（DDPMs）作为表格化影像衍生表型的统一条件密度估计器，能够同时建模单变量百分位数和多元依赖结构，在神经影像规范建模中实现校准的多变量偏差剖面。


<details>
  <summary>Details</summary>
Motivation: 传统神经影像流程通常为每个影像衍生表型（IDP）单独拟合模型，虽然扩展性好但忽略了可能编码协调模式的多元依赖关系。需要一种能够同时建模单变量百分位数和多元依赖结构的统一方法。

Method: 提出使用去噪扩散概率模型（DDPMs）作为表格化IDPs的条件密度估计器，采用两种去噪器骨干：1）特征线性调制（FiLM）条件多层感知器（MLP）；2）具有特征自注意力和样本间注意力的表格变换器（SAINT），通过学习的嵌入对协变量进行条件化。

Result: 在低维度下，扩散模型提供与传统基线相当的校准良好的单IDP输出，同时联合建模真实的依赖结构。在高维度下，变换器骨干比MLP保持更好的校准性，并更好地保留高阶依赖，支持可扩展的联合规范建模。

Conclusion: 扩散基础的规范建模为神经影像中校准的多变量偏差剖面提供了一条实用路径，能够同时保持与标准单IDP流程的兼容性。

Abstract: Normative modeling estimates reference distributions of biological measures conditional on covariates, enabling centiles and clinically interpretable deviation scores to be derived. Most neuroimaging pipelines fit one model per imaging-derived phenotype (IDP), which scales well but discards multivariate dependence that may encode coordinated patterns. We propose denoising diffusion probabilistic models (DDPMs) as a unified conditional density estimator for tabular IDPs, from which univariate centiles and deviation scores are derived by sampling. We utilise two denoiser backbones: (i) a feature-wise linear modulation (FiLM) conditioned multilayer perceptron (MLP) and (ii) a tabular transformer with feature self-attention and intersample attention (SAINT), conditioning covariates through learned embeddings. We evaluate on a synthetic benchmark with heteroscedastic and multimodal age effects and on UK Biobank FreeSurfer phenotypes, scaling from dimension of 2 to 200. Our evaluation suite includes centile calibration (absolute centile error, empirical coverage, and the probability integral transform), distributional fidelity (Kolmogorov-Smirnov tests), multivariate dependence diagnostics, and nearest-neighbour memorisation analysis. For low dimensions, diffusion models deliver well-calibrated per-IDP outputs comparable to traditional baselines while jointly modeling realistic dependence structure. At higher dimensions, the transformer backbone remains substantially better calibrated than the MLP and better preserves higher-order dependence, enabling scalable joint normative models that remain compatible with standard per-IDP pipelines. These results support diffusion-based normative modeling as a practical route to calibrated multivariate deviation profiles in neuroimaging.

</details>


### [174] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: 提出Causal Analyst框架，通过因果分析识别LLM越狱的直接原因，并应用于攻击增强和防御建议


<details>
  <summary>Details</summary>
Motivation: 现有研究主要分析潜在表征，忽略了可解释提示特征与越狱发生之间的因果关系，需要更深入理解越狱机制以提升LLM安全性和可靠性

Method: 整合LLM到数据驱动的因果发现中，构建包含35k越狱尝试的数据集，联合训练LLM提示编码和GNN因果图学习，重建提示特征到越狱响应的因果路径

Result: 发现"正面角色"和"任务步骤数"等特定特征是越狱的直接因果驱动因素；Jailbreaking Enhancer显著提升攻击成功率，Guardrail Advisor有效提取混淆查询的真实恶意意图

Conclusion: 从因果角度分析越狱特征是提高LLM可靠性的有效且可解释的方法，因果分析优于非因果方法

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [175] [Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability](https://arxiv.org/abs/2602.04902)
*Kingsuk Maitra*

Main category: cs.LG

TL;DR: 论文将Transformer视为物理电路，提出动量注意力机制，通过引入运动学动量实现单层归纳头，建立了辛几何与滤波器的对偶关系。


<details>
  <summary>Details</summary>
Motivation: 将机械可解释性(MI)程序扩展到物理电路视角，通过引入守恒定律和时变交流动力学，为Transformer提供更丰富的分析框架。

Method: 提出动量注意力机制，使用运动学差分算子p_t = q_t - q_{t-1}嵌入物理先验，实现辛剪切变换，建立辛-滤波器对偶性。

Result: 125M动量模型在归纳任务上表现优异，接近350M基线模型，发现了动量与深度可替代的缩放定律γ* = 4.17 × N^{-0.74}。

Conclusion: 该框架连接了生成式AI、哈密顿物理和信号处理，提供了互补的分析工具包，突破了传统架构的拓扑深度限制。

Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $γ^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.

</details>


### [176] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: 特征导向方法能有效控制LLM行为，但会严重损害模型性能，提示工程在性能与行为控制间取得最佳平衡


<details>
  <summary>Details</summary>
Motivation: 特征导向作为控制LLM行为的新方法，其实用效果尚不明确，特别是与输出质量之间的权衡关系需要实证研究

Method: 评估Goodfire的Auto Steer特征导向方法与提示工程基线，在14个导向查询（涵盖无害和安全相关行为）上，使用Llama-8B和Llama-70B模型，在171个MMLU问题上测量准确性、连贯性和行为控制效果

Result: Auto Steer成功修改目标行为（Llama-8B得分3.33 vs 2.98，Llama-70B得分3.57 vs 3.10），但导致性能急剧下降：MMLU准确率从66%降至46%（8B）和87%降至73%（70B），连贯性从4.62降至2.24和4.94降至3.89

Conclusion: 当前特征导向方法在需要保持任务性能的实际部署中存在局限，简单提示工程在整体平衡上表现最佳，机制控制方法存在基本的能力-行为权衡，部署前需实证评估

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [177] [DCER: Dual-Stage Compression and Energy-Based Reconstruction](https://arxiv.org/abs/2602.04904)
*Yiwen Wang,Jiahao Qin*

Main category: cs.LG

TL;DR: DCER框架通过双阶段压缩和基于能量的重建解决多模态融合中的噪声输入和模态缺失问题，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态融合面临两个鲁棒性挑战：噪声输入会降低表示质量，模态缺失会导致预测失败。现有方法难以同时有效处理这两个问题。

Method: 提出DCER统一框架：1) 压缩阶段：模态内频率变换（音频用小波，视频用DCT）去除噪声；跨模态瓶颈token强制真正融合而非捷径学习。2) 基于能量的重建：通过梯度下降在学习的能量函数上恢复缺失模态的表示，最终能量提供内在不确定性量化。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS基准测试中达到最先进性能，表现出U型鲁棒性模式（在完整模态和高缺失条件下都表现良好），能量与预测误差的相关性超过0.72。

Conclusion: DCER通过双阶段压缩和能量重建有效解决了多模态融合中的噪声和缺失问题，提供了统一的鲁棒性解决方案，并能量化不确定性。

Abstract: Multimodal fusion faces two robustness challenges: noisy inputs degrade representation quality, and missing modalities cause prediction failures. We propose DCER, a
  unified framework addressing both challenges through dual-stage compression and energy-based reconstruction. The compression stage operates at two levels:
  within-modality frequency transforms (wavelet for audio, DCT for video) remove noise while preserving task-relevant patterns, and cross-modality bottleneck tokens
  force genuine integration rather than modality-specific shortcuts. For missing modalities, energy-based reconstruction recovers representations via gradient descent
  on a learned energy function, with the final energy providing intrinsic uncertainty quantification (\r{ho} > 0.72 correlation with prediction error). Experiments on
  CMU-MOSI, CMU-MOSEI, and CH-SIMS demonstrate state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion at
  both complete and high-missing conditions. The code will be available on Github.

</details>


### [178] [LISA: Laplacian In-context Spectral Analysis](https://arxiv.org/abs/2602.04906)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LISA：一种基于拉普拉斯谱分析的上下文学习方法，用于时间序列模型的推理时自适应，仅使用观测前缀进行预测


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型在动态变化的环境中适应性不足，需要能够在推理时仅使用观测前缀就能自适应的方法，将上下文学习与非参数谱方法相结合

Method: 结合延迟坐标嵌入和拉普拉斯谱学习生成扩散坐标状态表示，使用冻结非线性解码器进行一步预测，引入基于高斯过程回归或类注意力马尔可夫算子的轻量级潜在空间残差适配器

Result: 在预测和自回归展开实验中，LISA优于冻结基线，在动态变化环境下表现尤为突出

Conclusion: LISA成功将上下文学习与非参数谱方法连接起来，为动态系统提供了一种有效的推理时自适应框架

Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.

</details>


### [179] [Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution](https://arxiv.org/abs/2602.04918)
*Long Zhang,Fangwei Lin*

Main category: cs.LG

TL;DR: LLMs在遇到上下文信息与内部知识冲突时，会通过几何旋转而非信号稀释来"顺从"上下文，这种正交干扰机制让模型在保持内部知识结构的同时模拟采纳冲突信息。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在面对上下文信息与参数化记忆冲突时的机制，特别是模型如何通过顺从行为解决知识冲突，以及这种抑制是由于信号幅度稀释还是方向性几何改变。

Method: 对Qwen-4B、Llama-3.1-8B和GLM-4-9B进行分层几何分析，将反事实上下文引起的残差流更新分解为径向（基于范数）和角度（基于余弦）分量。

Result: 拒绝了"流形稀释"假说的普遍性，发现顺从行为一致表现为"正交干扰"特征，冲突上下文注入的转向向量与真实方向准正交，有效旋转隐藏状态表示。

Conclusion: 模型不会"忘记"或抑制内部真相的幅度，而是采用几何位移机制绕过正确的解嵌入向量，这挑战了基于标量置信度的幻觉检测，强调需要向量监控来区分真实知识整合与表面上下文模仿。

Abstract: Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the "Manifold Dilution" hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by "Orthogonal Interference," where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not "unlearn" or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.

</details>


### [180] [Physics as the Inductive Bias for Causal Discovery](https://arxiv.org/abs/2602.04907)
*Jianhong Chen,Naichen Shi,Xubo Yue*

Main category: cs.LG

TL;DR: 提出一个结合物理知识与因果发现的动力学系统框架，利用随机微分方程建模，将已知ODE动力学作为漂移项，未知因果耦合作为扩散项，通过稀疏MLE算法恢复因果图。


<details>
  <summary>Details</summary>
Motivation: 因果发现通常是数据驱动的，而物理模型（如ODE）为动力学过程提供机制结构。将两者结合可以让物理知识作为归纳偏置，提高动力学系统中因果发现的可识别性、稳定性和鲁棒性。然而，现实动力学系统常存在反馈、循环交互和非平稳数据趋势，而许多因果发现方法基于无环性或平衡假设，因此整合面临挑战。

Method: 提出一个整合的因果发现框架，将系统演化建模为随机微分方程（SDE），其中漂移项编码已知的ODE动力学，扩散项对应超出预设物理的未知因果耦合。开发了一个可扩展的稀疏诱导最大似然估计算法，利用因果图结构进行高效参数估计。

Result: 在温和条件下建立了恢复因果图的保证。在具有不同因果结构的动力学系统上的实验表明，该方法比纯数据驱动的先进基线方法提高了因果图恢复能力，并产生了更稳定、物理一致的估计。

Conclusion: 该工作成功地将物理知识作为归纳偏置整合到动力学系统的因果发现中，通过SDE框架和稀疏MLE算法，有效处理了反馈、循环交互和非平稳数据等挑战，提高了因果发现的性能和物理一致性。

Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.

</details>


### [181] [Temporal Pair Consistency for Variance-Reduced Flow Matching](https://arxiv.org/abs/2602.04908)
*Chika Maduabuchi,Jindong Wang*

Main category: cs.LG

TL;DR: TPC是一种轻量级方差减少方法，通过耦合同一概率路径上配对时间步的速度预测来降低梯度方差，无需修改模型架构、概率路径或求解器。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型（如扩散模型、流匹配、整流流）通常使用独立处理时间步的目标函数进行训练，导致估计器方差高和采样效率低。现有方法通过显式平滑惩罚、轨迹正则化或修改概率路径和求解器来缓解这一问题。

Method: 提出时间对一致性（TPC）原则，在估计器层面耦合同一概率路径上配对时间步的速度预测，通过轨迹耦合的正则化减少梯度方差，同时保持底层流匹配目标不变。

Result: 在CIFAR-10和ImageNet多个分辨率上，TPC提高了样本质量和效率，在相同或更低计算成本下获得更低的FID分数，并能无缝扩展到现代SOTA风格管道，包括噪声增强训练、基于分数的去噪和整流流。

Conclusion: TPC是一种有效的轻量级方差减少方法，通过时间对一致性原则改善连续时间生成模型的训练效率和采样质量，无需修改底层模型架构或概率路径。

Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.

</details>


### [182] [Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment](https://arxiv.org/abs/2602.04909)
*Youngjae Cho,Jongsuk Kim,Ji-Hoon Kim*

Main category: cs.LG

TL;DR: GAPO提出用动态几何感知锚点替代DPO中的固定参考策略，通过对抗性局部扰动创建悲观基线，自适应重加权偏好对，提升噪声监督下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DPO等方法的固定参考策略会随着策略漂移而变得不校准，导致分布不匹配和噪声信号放大；而无参考变体则存在无约束奖励漂移问题。

Method: 提出几何锚点偏好优化(GAPO)：在当前策略的小半径内创建对抗性局部扰动作为动态锚点；引入锚点间隙度量策略与锚点的奖励差异；基于该间隙对偏好对进行自适应重加权。

Result: 在多样化噪声设置下，GAPO持续提升鲁棒性，同时在标准LLM对齐和推理基准测试中达到或超越现有性能。

Conclusion: GAPO通过动态几何感知锚点解决了固定参考策略的校准问题和无参考方法的奖励漂移问题，实现了更鲁棒的偏好优化。

Abstract: Direct Preference Optimization (DPO) and related methods align large language models from pairwise preferences by regularizing updates against a fixed reference policy. As the policy drifts, a static reference, however, can become increasingly miscalibrated, leading to distributional mismatch and amplifying spurious preference signals under noisy supervision. Conversely, reference-free variants avoid mismatch but often suffer from unconstrained reward drift. We propose Geometric Anchor Preference Optimization (GAPO), which replaces the fixed reference with a dynamic, geometry-aware anchor: an adversarial local perturbation of the current policy within a small radius that serves as a pessimistic baseline. This anchor enables an adaptive reweighting mechanism, modulating the importance of each preference pair based on its local sensitivity. We further introduce the Anchor Gap, the reward discrepancy between the policy and its anchor, and show under smoothness conditions that it approximates worst-case local margin degradation. Optimizing a logistic objective weighted by this gap downweights geometrically brittle instances while emphasizing robust preference signals. Across diverse noise settings, GAPO consistently improves robustness while matching or improving performance on standard LLM alignment and reasoning benchmarks.

</details>


### [183] [A logical re-conception of neural networks: Hamiltonian bitwise part-whole architecture](https://arxiv.org/abs/2602.04911)
*E Bowen,R Granger,A Rodriguez*

Main category: cs.LG

TL;DR: 提出一种基于图表示和哈密顿算子的新型计算架构，使用低精度算术直接编码关系结构，实现符号计算特性


<details>
  <summary>Details</summary>
Motivation: 开发一种不同于标准人工神经网络的方法，能够直接表示关系结构（如部分-整体关系），实现内在的关系编码而非附加功能

Method: 将任意数据编码为图，边对应固定原始关系集；使用图哈密顿算子计算编码能量，基态表示所有关系约束同时满足；采用极低精度算术，计算成本随边数线性增长

Result: 系统能够处理标准ANN示例，同时产生具有符号计算特性的表示；能够识别数据中的简单逻辑关系结构，构建层次表示支持溯因推理；推导出等效的ANN操作，为高级语义表示提供新方法

Conclusion: 该非常简单的系统为关系表示和符号计算提供了新颖架构，虽然当前实现较为基础，但为改进和扩展提供了框架，可能对高级语义表示研究有重要价值

Abstract: We introduce a simple initial working system in which relations (such as part-whole) are directly represented via an architecture with operating and learning rules fundamentally distinct from standard artificial neural network methods. Arbitrary data are straightforwardly encoded as graphs whose edges correspond to codes from a small fixed primitive set of elemental pairwise relations, such that simple relational encoding is not an add-on, but occurs intrinsically within the most basic components of the system. A novel graph-Hamiltonian operator calculates energies among these encodings, with ground states denoting simultaneous satisfaction of all relation constraints among graph vertices. The method solely uses radically low-precision arithmetic; computational cost is correspondingly low, and scales linearly with the number of edges in the data. The resulting unconventional architecture can process standard ANN examples, but also produces representations that exhibit characteristics of symbolic computation. Specifically, the method identifies simple logical relational structures in these data (part-of; next-to), building hierarchical representations that enable abductive inferential steps generating relational position-based encodings, rather than solely statistical representations. Notably, an equivalent set of ANN operations are derived, identifying a special case of embedded vector encodings that may constitute a useful approach to current work in higher-level semantic representation. The very simple current state of the implemented system invites additional tools and improvements.

</details>


### [184] [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)
*Xiaolin Hu,Hang Yuan,Xinzhu Sang,Binbin Yan,Zhou Yu,Cong Huang,Kai Chen*

Main category: cs.LG

TL;DR: A^2-LLM：端到端的对话音频化身大语言模型，统一处理语言、音频韵律和3D面部运动，实现情感丰富的面部动画生成。


<details>
  <summary>Details</summary>
Motivation: 当前对话数字人系统大多采用级联架构，存在错误累积、高延迟、实时性差等问题，且缺乏对话上下文理解，过度依赖僵硬的唇部同步而忽视情感深度。

Method: 提出A^2-LLM端到端模型，在统一框架中联合推理语言、音频韵律和3D面部运动；创建FLAME-QA高质量多模态数据集，以问答格式对齐语义意图与表情动态。

Result: 系统在保持实时效率（500ms延迟，0.7 RTF）的同时，实现了优于简单唇部同步的情感表现力，生成情感丰富的面部运动。

Conclusion: A^2-LLM通过端到端统一建模，解决了传统级联架构的问题，为下一代人机交互提供了表达力强、响应快的对话数字人解决方案。

Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

</details>


### [185] [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)
*Jose Miguel Luna,Taha Bouhsine,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SLAY提出了一种基于球形Yat核的线性时间注意力机制，通过约束查询和键到单位球面实现仅依赖角度对齐的注意力，性能接近标准softmax注意力但保持线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有线性时间注意力机制（如Performers和Cosformers）在性能和复杂度之间存在权衡，需要一种既能保持线性时间内存复杂度又能接近标准softmax注意力性能的方法。

Method: 基于松弛的E-Product（Yat核）构建几何感知的注意力机制，将查询和键约束到单位球面，使用Bernstein定理将球形Yat核表示为非负混合的多项式-指数乘积核，并推导出严格正随机特征近似实现线性时间注意力。

Result: SLAY在性能上与标准softmax注意力几乎无法区分，同时保持线性时间和内存扩展，在实验中一致优于先前的线性时间注意力机制如Performers和Cosformers。

Conclusion: SLAY代表了迄今为止最接近softmax注意力的线性时间近似方法，能够在没有典型注意力线性化性能权衡的情况下实现可扩展的Transformer模型。

Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interactions in physics. Our method, Spherical Linearized Attention with Yat Kernels (SLAY), constrains queries and keys to the unit sphere so that attention depends only on angular alignment. Using Bernstein's theorem, we express the spherical Yat-kernel as a nonnegative mixture of polynomial-exponential product kernels and derive a strictly positive random-feature approximation enabling linear-time O(L) attention. We establish positive definiteness and boundedness on the sphere and show that the estimator yields well-defined, nonnegative attention scores. Empirically, SLAY achieves performance that is nearly indistinguishable from standard softmax attention while retaining linear time and memory scaling, and consistently outperforms prior linear-time attention mechanisms such as Performers and Cosformers. To the best of our knowledge, SLAY represents the closest linear-time approximation to softmax attention reported to date, enabling scalable Transformers without the typical performance trade-offs of attention linearization.

</details>


### [186] [Multi-Aspect Mining and Anomaly Detection for Heterogeneous Tensor Streams](https://arxiv.org/abs/2602.04917)
*Soshi Kakio,Yasuko Matsubara,Ren Fujiwara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: HeteroComp：一种用于异构张量流连续摘要和组异常检测的方法，能处理分类和连续属性，使用高斯过程先验建模未知分布，不依赖数据流长度。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解和异常检测方法存在两个主要局限：(1) 无法处理包含分类属性（如IP地址）和连续属性（如数据包长度）的异构张量流，通常需要离散化连续属性或将分类属性视为连续，这会扭曲数据的统计特性；(2) 离散化时间戳，无法跟踪流的时序动态（如趋势、异常事件），导致无法有效检测组级异常（如DoS攻击）。

Method: 提出HeteroComp方法，将异构张量流连续摘要为表示每个属性中潜在组及其时序动态的"组件"。使用高斯过程先验来建模连续属性的未知分布和时序动态，直接从数据中估计概率密度。提取的组件提供简洁有效的摘要，支持准确的组异常检测。

Result: 在真实数据集上的广泛实验表明，HeteroComp在组异常检测准确率上优于最先进算法，且其计算时间不依赖于数据流长度。

Conclusion: HeteroComp能够有效处理异构张量流，通过连续摘要和准确建模连续属性分布及时序动态，实现了高效的组异常检测，解决了现有方法的局限性。

Abstract: Analysis and anomaly detection in event tensor streams consisting of timestamps and multiple attributes - such as communication logs(time, IP address, packet length)- are essential tasks in data mining. While existing tensor decomposition and anomaly detection methods provide useful insights, they face the following two limitations. (i) They cannot handle heterogeneous tensor streams, which comprises both categorical attributes(e.g., IP address) and continuous attributes(e.g., packet length). They typically require either discretizing continuous attributes or treating categorical attributes as continuous, both of which distort the underlying statistical properties of the data.Furthermore, incorrect assumptions about the distribution family of continuous attributes often degrade the model's performance. (ii) They discretize timestamps, failing to track the temporal dynamics of streams(e.g., trends, abnormal events), which makes them ineffective for detecting anomalies at the group level, referred to as 'group anomalies' (e.g, DoS attacks). To address these challenges, we propose HeteroComp, a method for continuously summarizing heterogeneous tensor streams into 'components' representing latent groups in each attribute and their temporal dynamics, and detecting group anomalies. Our method employs Gaussian process priors to model unknown distributions of continuous attributes, and temporal dynamics, which directly estimate probability densities from data. Extracted components give concise but effective summarization, enabling accurate group anomaly detection. Extensive experiments on real datasets demonstrate that HeteroComp outperforms the state-of-the-art algorithms for group anomaly detection accuracy, and its computational time does not depend on the data stream length.

</details>


### [187] [A Short and Unified Convergence Analysis of the SAG, SAGA, and IAG Algorithms](https://arxiv.org/abs/2602.05304)
*Feng Zhu,Robert W. Heath,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出统一收敛分析框架，适用于SAG、SAGA和IAG三种算法，简化证明过程并改进收敛率


<details>
  <summary>Details</summary>
Motivation: 现有随机方差缩减算法（如SAG、SAGA）及其确定性对应算法（如IAG）的分析方法分散，依赖各自不同的证明技术，且SAG的原始证明复杂需要计算机辅助。需要统一的收敛分析框架。

Method: 开发统一的收敛分析方法，包含两个关键步骤：(1) 使用简单集中工具建立随机子采样延迟的界限；(2) 精心设计新颖的Lyapunov函数来考虑这些延迟。

Result: 获得了简洁模块化的证明，首次为SAG和SAGA提供高概率界限，并能无缝扩展到非凸目标和马尔可夫采样。作为副产品，获得了IAG算法的最佳已知收敛率，显著改进了先前界限。

Conclusion: 提出了适用于SAG、SAGA和IAG的统一收敛分析框架，简化了证明复杂度，提供了更好的理论保证，并为扩展到更广泛场景奠定了基础。

Abstract: Stochastic variance-reduced algorithms such as Stochastic Average Gradient (SAG) and SAGA, and their deterministic counterparts like the Incremental Aggregated Gradient (IAG) method, have been extensively studied in large-scale machine learning. Despite their popularity, existing analyses for these algorithms are disparate, relying on different proof techniques tailored to each method. Furthermore, the original proof of SAG is known to be notoriously involved, requiring computer-aided analysis. Focusing on finite-sum optimization with smooth and strongly convex objective functions, our main contribution is to develop a single unified convergence analysis that applies to all three algorithms: SAG, SAGA, and IAG. Our analysis features two key steps: (i) establishing a bound on delays due to stochastic sub-sampling using simple concentration tools, and (ii) carefully designing a novel Lyapunov function that accounts for such delays. The resulting proof is short and modular, providing the first high-probability bounds for SAG and SAGA that can be seamlessly extended to non-convex objectives and Markov sampling. As an immediate byproduct of our new analysis technique, we obtain the best known rates for the IAG algorithm, significantly improving upon prior bounds.

</details>


### [188] [Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog](https://arxiv.org/abs/2602.04919)
*Yiran Zhao,Shengyang Zhou,Zijian Wu,Tongyan Hu,Yuhui Xu,Rengan Dou,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 提出渐进压缩方法PTL，通过多次细粒度迭代的剪枝-调优循环，将LLM压缩至原大小近一半，仅需轻量级后训练即可保持推理性能。


<details>
  <summary>Details</summary>
Motivation: LLM规模庞大导致计算资源消耗大，需要减少冗余参数以加速推理。传统剪枝方法直接移除参数会导致推理任务性能急剧下降，且需要大量后训练恢复能力。

Method: 提出渐进压缩方法，将压缩过程分为多个细粒度迭代，每个阶段应用剪枝-调优循环(PTL)：先剪枝减少模型大小，然后通过微调恢复性能。这种迭代方法类似于"温水煮青蛙"效应，使模型能够逐步压缩而不出现突发的性能损失。

Result: PTL能将LLM压缩至接近原始大小的一半，仅需轻量级后训练即可保持与原始模型相当的推理任务性能。该方法灵活，可应用于多种剪枝策略（神经元剪枝、层剪枝）和不同后训练方法（持续预训练、强化学习）。在数学推理、代码生成等多种任务上验证了有效性。

Conclusion: PTL提供了一种有效的渐进压缩框架，能够在显著减少LLM大小的同时保持其推理能力，具有广泛的适用性。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.

</details>


### [189] [Does SGD Seek Flatness or Sharpness? An Exactly Solvable Model](https://arxiv.org/abs/2602.05065)
*Yizhou Xu,Pierfrancesco Beneventano,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: SGD没有先验的平坦偏好，而是偏好最小化梯度波动；数据分布唯一决定收敛时的锐度；平坦最小值仅在标签噪声各向同性时被偏好


<details>
  <summary>Details</summary>
Motivation: 澄清SGD在训练中偏好平坦还是尖锐解的争议，理解损失景观平坦性与性能之间的关系

Method: 识别并精确求解一个分析可解模型，该模型在训练中同时表现出平坦化和锐化行为；在MLP、RNN和Transformer等不同架构中进行受控实验验证

Result: SGD没有先验的平坦偏好，而是偏好最小化梯度波动；数据分布唯一决定收敛时的锐度；平坦最小值仅在标签噪声各向同性时被偏好；当标签噪声各向异性时，模型偏好锐度并可能收敛到任意尖锐的解

Conclusion: 至少在该模型中，SGD的平坦寻求行为由数据分布决定而非先验偏好；标签噪声的各向同性/异性是决定收敛锐度的关键因素

Abstract: A large body of theory and empirical work hypothesizes a connection between the flatness of a neural network's loss landscape during training and its performance. However, there have been conceptually opposite pieces of evidence regarding when SGD prefers flatter or sharper solutions during training. In this work, we partially but causally clarify the flatness-seeking behavior of SGD by identifying and exactly solving an analytically solvable model that exhibits both flattening and sharpening behavior during training. In this model, the SGD training has no \textit{a priori} preference for flatness, but only a preference for minimal gradient fluctuations. This leads to the insight that, at least within this model, it is data distribution that uniquely determines the sharpness at convergence, and that a flat minimum is preferred if and only if the noise in the labels is isotropic across all output dimensions. When the noise in the labels is anisotropic, the model instead prefers sharpness and can converge to an arbitrarily sharp solution, depending on the imbalance in the noise in the labels spectrum. We reproduce this key insight in controlled settings with different model architectures such as MLP, RNN, and transformers.

</details>


### [190] [Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates](https://arxiv.org/abs/2602.05311)
*Chengxiao Wang,Haoze Wu,Gagandeep Singh*

Main category: cs.LG

TL;DR: 提出一种训练鲁棒神经李雅普诺夫屏障证书的方法，用于在系统动力学存在扰动时验证强化学习控制器的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有神经李雅普诺夫和屏障证书方法仅在固定理想无扰动动力学下提供保证，而实际应用中系统动力学可能因不确定性而偏离，限制了其在现实世界中的可靠性。

Method: 定义鲁棒李雅普诺夫屏障函数，基于Lipschitz连续性指定充分条件以确保对有限扰动的鲁棒性。提出通过对抗训练、Lipschitz邻域边界和全局Lipschitz正则化的实用训练目标来强制执行这些条件。

Result: 在倒立摆和2D对接两个实际相关环境中验证方法。相比基线，显著提高了认证鲁棒性边界（高达4.6倍）和强扰动下的经验成功率（高达2.4倍）。

Conclusion: 结果表明，在动力学存在扰动的情况下，训练鲁棒神经证书对于安全强化学习是有效的，为解决现实世界应用中的不确定性提供了可靠方法。

Abstract: Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.

</details>


### [191] [CyIN: Cyclic Informative Latent Space for Bridging Complete and Incomplete Multimodal Learning](https://arxiv.org/abs/2602.04920)
*Ronghao Lin,Qiaolin He,Sijie Mai,Ying Zeng,Aolin Xiong,Li Huang,Yap-Peng Tan,Haifeng Hu*

Main category: cs.LG

TL;DR: CyIN框架通过循环信息瓶颈和跨模态循环翻译，在统一模型中同时优化完整和不完整多模态学习，提升模型在动态缺失模态场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多模态数据的模态存在高度可变性和不可预测性，导致预训练模型在动态缺失模态情况下性能显著下降。现有方法主要针对完美配对的多模态输入训练，无法适应不完整模态场景。

Method: 提出Cyclic INformative Learning框架：1) 通过循环信息瓶颈在模态间构建信息丰富的潜在空间，捕捉任务相关特征；2) 提出跨模态循环翻译，通过前向和反向传播过程用剩余模态重建缺失模态；3) 在统一模型中联合优化完整和不完整多模态学习。

Result: 在4个多模态数据集上的广泛实验表明，该方法在完整和多样不完整场景下均表现出优越性能。

Conclusion: CyIN框架成功弥合了完整和不完整多模态学习之间的差距，通过信息瓶颈和跨模态重建机制，使模型在动态缺失模态情况下保持鲁棒性。

Abstract: Multimodal machine learning, mimicking the human brain's ability to integrate various modalities has seen rapid growth. Most previous multimodal models are trained on perfectly paired multimodal input to reach optimal performance. In real-world deployments, however, the presence of modality is highly variable and unpredictable, causing the pre-trained models in suffering significant performance drops and fail to remain robust with dynamic missing modalities circumstances. In this paper, we present a novel Cyclic INformative Learning framework (CyIN) to bridge the gap between complete and incomplete multimodal learning. Specifically, we firstly build an informative latent space by adopting token- and label-level Information Bottleneck (IB) cyclically among various modalities. Capturing task-related features with variational approximation, the informative bottleneck latents are purified for more efficient cross-modal interaction and multimodal fusion. Moreover, to supplement the missing information caused by incomplete multimodal input, we propose cross-modal cyclic translation by reconstruct the missing modalities with the remained ones through forward and reverse propagation process. With the help of the extracted and reconstructed informative latents, CyIN succeeds in jointly optimizing complete and incomplete multimodal learning in one unified model. Extensive experiments on 4 multimodal datasets demonstrate the superior performance of our method in both complete and diverse incomplete scenarios.

</details>


### [192] [Reliable Explanations or Random Noise? A Reliability Metric for XAI](https://arxiv.org/abs/2602.05082)
*Poushali Sengupta,Sabita Maharjan,Frank Eliassen,Shashi Raj Pandey,Yan Zhang*

Main category: cs.LG

TL;DR: 论文提出了解释可靠性指数（ERI），用于量化机器学习模型解释在现实部署条件下的稳定性，包括输入扰动、特征冗余、模型演化和分布偏移，并揭示了当前流行解释方法存在的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（能源系统、医疗、金融、自动驾驶）中，复杂机器学习模型的解释可靠性至关重要。然而，当前广泛使用的解释方法（如SHAP和集成梯度）在现实非对抗性变化下的稳定性尚未得到充分衡量，这种可变性会损害解释的可靠性。

Method: 提出了解释可靠性指数（ERI）系列指标，基于四个可靠性公理：对微小输入扰动的鲁棒性、特征冗余下的一致性、模型演化中的平滑性、对轻微分布偏移的韧性。为每个公理推导了形式化保证（如Lipschitz型边界和时间稳定性结果），并提出了ERI-T（时序模型专用指标）和ERI-Bench（系统压力测试基准）。

Result: 实验结果显示，流行解释方法普遍存在可靠性失败，解释在现实部署条件下可能不稳定。ERI能够暴露和量化这些不稳定性，为解释可靠性提供原则性评估。

Conclusion: 通过提出ERI指标和基准，该研究使解释可靠性的原则性评估成为可能，支持构建更可信的可解释AI（XAI）系统，揭示了当前解释方法在现实部署中的可靠性问题需要被重视和解决。

Abstract: In recent years, explaining decisions made by complex machine learning models has become essential in high-stakes domains such as energy systems, healthcare, finance, and autonomous systems. However, the reliability of these explanations, namely, whether they remain stable and consistent under realistic, non-adversarial changes, remains largely unmeasured. Widely used methods such as SHAP and Integrated Gradients (IG) are well-motivated by axiomatic notions of attribution, yet their explanations can vary substantially even under system-level conditions, including small input perturbations, correlated representations, and minor model updates. Such variability undermines explanation reliability, as reliable explanations should remain consistent across equivalent input representations and small, performance-preserving model changes. We introduce the Explanation Reliability Index (ERI), a family of metrics that quantifies explanation stability under four reliability axioms: robustness to small input perturbations, consistency under feature redundancy, smoothness across model evolution, and resilience to mild distributional shifts. For each axiom, we derive formal guarantees, including Lipschitz-type bounds and temporal stability results. We further propose ERI-T, a dedicated measure of temporal reliability for sequential models, and introduce ERI-Bench, a benchmark designed to systematically stress-test explanation reliability across synthetic and real-world datasets. Experimental results reveal widespread reliability failures in popular explanation methods, showing that explanations can be unstable under realistic deployment conditions. By exposing and quantifying these instabilities, ERI enables principled assessment of explanation reliability and supports more trustworthy explainable AI (XAI) systems.

</details>


### [193] [Imposing Boundary Conditions on Neural Operators via Learned Function Extensions](https://arxiv.org/abs/2602.04923)
*Sepehr Mousavi,Siddhartha Mishra,Laura De Lorenzis*

Main category: cs.LG

TL;DR: 提出通过函数扩展将复杂边界条件融入神经算子的通用框架，在18个挑战性数据集上实现SOTA精度


<details>
  <summary>Details</summary>
Motivation: 现有神经算子处理高度变化的复杂边界条件能力有限，当解算子对边界强迫表现出强敏感性时往往失效

Method: 通过函数扩展将边界数据映射到整个空间域的潜在伪扩展，使任何标准算子学习架构都能消费边界信息

Result: 在Poisson、线性弹性和超弹性问题的18个挑战性数据集上实现最先进精度，大幅超越基线方法，且无需跨数据集超参数调优

Conclusion: 学习边界到域的扩展是现有神经算子框架中施加复杂边界条件的有效实用策略，能为更广泛的PDE控制问题提供准确鲁棒的科学机器学习模型

Abstract: Neural operators have emerged as powerful surrogates for the solution of partial differential equations (PDEs), yet their ability to handle general, highly variable boundary conditions (BCs) remains limited. Existing approaches often fail when the solution operator exhibits strong sensitivity to boundary forcings. We propose a general framework for conditioning neural operators on complex non-homogeneous BCs through function extensions. Our key idea is to map boundary data to latent pseudo-extensions defined over the entire spatial domain, enabling any standard operator learning architecture to consume boundary information. The resulting operator, coupled with an arbitrary domain-to-domain neural operator, can learn rich dependencies on complex BCs and input domain functions at the same time. To benchmark this setting, we construct 18 challenging datasets spanning Poisson, linear elasticity, and hyperelasticity problems, with highly variable, mixed-type, component-wise, and multi-segment BCs on diverse geometries. Our approach achieves state-of-the-art accuracy, outperforming baselines by large margins, while requiring no hyperparameter tuning across datasets. Overall, our results demonstrate that learning boundary-to-domain extensions is an effective and practical strategy for imposing complex BCs in existing neural operator frameworks, enabling accurate and robust scientific machine learning models for a broader range of PDE-governed problems.

</details>


### [194] [A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders](https://arxiv.org/abs/2602.05967)
*Mohamad Amin Jamshidi,Mehrbod Zarifi,Zolfa Anvari,Hamed Ghafarirad,Mohammad Zareinejad*

Main category: cs.LG

TL;DR: 提出基于LSTM和随机森林的混合算法，用于液压缸非线性摩擦力估计，相比传统解析模型具有更好的适应性和实时性


<details>
  <summary>Details</summary>
Motivation: 液压系统在工业中广泛应用，但液压缸的摩擦力显著影响其操作精度。现有解析模型通常基于实验测试，适应性有限且计算效率不高，需要更精确、高效的摩擦力估计方法

Method: 提出数据驱动的混合算法，结合长短期记忆网络(LSTM)和随机森林，利用实验液压测试装置获取的训练数据，有效结合特征检测和估计过程

Result: 算法在不同工况和外部负载变化下保持稳定，模型误差小于10%，每次估计计算成本仅1.51毫秒，适合实时应用。与LuGre模型对比显示，混合方法在动态适应液压缸变化工况方面具有优势

Conclusion: 提出的混合算法解决了传统解析模型的局限性，在精度和计算效率方面表现优异，特别适合液压系统的实时应用，为非线性摩擦力估计提供了有效的数据驱动解决方案

Abstract: Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm's performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.

</details>


### [195] [Knowing When to Answer: Adaptive Confidence Refinement for Reliable Audio-Visual Question Answering](https://arxiv.org/abs/2602.04924)
*Dinh Phu Tran,Jihoon Jeong,Saad Wazir,Seongah Kim,Thao Do,Cem Subakan,Daeyoung Kim*

Main category: cs.LG

TL;DR: 本文提出了一种用于可靠视听问答（R-AVQA）的轻量级方法ACR，通过自适应置信度修正来提升模型在不确定时选择弃答而非错误回答的能力。


<details>
  <summary>Details</summary>
Motivation: 当前AVQA模型虽然准确率高，但在识别自身可能出错并选择弃答方面的能力研究不足。需要一种可靠的方法让模型在不确定时选择弃答而非给出错误答案。

Method: 提出自适应置信度修正（ACR）方法：保持最大softmax概率（MSP）作为主要置信信号，当MSP不可靠时应用输入自适应的残差修正。引入两个学习头：残差风险头预测MSP未捕捉的低幅度正确性残差，置信度门控头判断MSP的可信度。

Result: ACR在分布内、分布外和数据偏差设置下，在三种不同的AVQA架构上一致优于现有方法，为R-AVQA任务奠定了坚实基础。

Conclusion: ACR是一种轻量级且有效的方法，通过自适应修正MSP置信度来提升可靠视听问答性能，解决了模型在不确定时选择弃答的关键问题。

Abstract: We present a formal problem formulation for \textit{Reliable} Audio-Visual Question Answering ($\mathcal{R}$-AVQA), where we prefer abstention over answering incorrectly. While recent AVQA models have high accuracy, their ability to identify when they are likely wrong and their consequent abstention from answering remain underexplored areas of research. To fill this gap, we explore several approaches and then propose Adaptive Confidence Refinement (ACR), a lightweight method to further enhance the performance of $\mathcal{R}$-AVQA. Our key insight is that the Maximum Softmax Probability (MSP) is Bayes-optimal only under strong calibration, a condition usually not met in deep neural networks, particularly in multimodal models. Instead of replacing MSP, our ACR maintains it as a primary confidence signal and applies input-adaptive residual corrections when MSP is deemed unreliable. ACR introduces two learned heads: i) a Residual Risk Head that predicts low-magnitude correctness residuals that MSP does not capture, and ii) a Confidence Gating Head to determine MSP trustworthiness. Our experiments and theoretical analysis show that ACR consistently outperforms existing methods on in- and out-of-disrtibution, and data bias settings across three different AVQA architectures, establishing a solid foundation for $\mathcal{R}$-AVQA task. The code and checkpoints will be available upon acceptance \href{https://github.com/PhuTran1005/R-AVQA}{at here}

</details>


### [196] [ZeroS: Zero-Sum Linear Attention for Efficient Transformers](https://arxiv.org/abs/2602.05230)
*Jiecheng Lu,Xu Han,Yan Sun,Viresh Pati,Yubin Kim,Siddhartha Somani,Shihao Yang*

Main category: cs.LG

TL;DR: ZeroS线性注意力通过移除常数项和重加权残差，解决了传统线性注意力只能进行凸组合的问题，实现了正负值权重和对比操作，在保持O(N)复杂度的同时性能媲美标准softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力方法虽然提供O(N)复杂度，但通常性能不如标准softmax注意力。研究发现两个根本限制：1）只能进行凸组合，仅允许加性信息融合；2）均匀累积权重偏差在长上下文中稀释注意力。

Method: 提出Zero-Sum线性注意力(ZeroS)，通过移除常数零阶项1/t并重新加权剩余的零和softmax残差。这种修改创建了数学稳定的权重，允许正负值，使单个注意力层能够执行对比操作。

Result: 在保持O(N)复杂度的同时，ZeroS理论上扩展了可表示函数的集合。实证结果显示，在各种序列建模基准测试中，ZeroS匹配或超过了标准softmax注意力的性能。

Conclusion: ZeroS通过解决线性注意力的两个根本限制，实现了既保持线性复杂度又达到或超越标准注意力性能的目标，为高效序列建模提供了有前景的解决方案。

Abstract: Linear attention methods offer Transformers $O(N)$ complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term $1/t$ and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining $O(N)$ complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks.

</details>


### [197] [Internalizing LLM Reasoning via Discovery and Replay of Latent Actions](https://arxiv.org/abs/2602.04925)
*Zhenning Shi,Yijia Zhu,Junhan Shi,Xun Zhang,Lei Wang,Congcong Miao*

Main category: cs.LG

TL;DR: STIR框架通过动态潜在轨迹控制，将思维链过程内化到隐藏状态中，在减少token消耗的同时提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖静态控制向量，无法适应复杂推理任务的非平稳演化过程，需要更动态的推理增强方法。

Method: 提出三阶段框架：1) 差分内在动作归纳收集潜在推理成功案例形成引导基元；2) 稀疏控制基构建创建紧凑几何多样的工具库；3) 价值调制轨迹干预通过锚点门控动态注入上下文特定脉冲。

Result: 在四个代表性模型的六个算术和逻辑基准测试中，STIR将平均准确率提升1.9%至7.5%，同时将平均token消耗减少高达35%。

Conclusion: 显式思维链的好处可以通过动态潜在轨迹控制实现，将推理过程内化以绕过显式生成，同时获得更高的保真度。

Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.

</details>


### [198] [Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics](https://arxiv.org/abs/2602.04928)
*Ruizhe Zhong,Jiesong Lian,Xiaoyue Mi,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Junchi Yan*

Main category: cs.LG

TL;DR: Euphonium是一个新颖的强化学习框架，通过过程奖励梯度引导的动力学来改进流匹配模型与人类偏好的对齐，解决了现有方法探索效率低的问题，在文本到视频生成任务中实现了更好的对齐效果和1.66倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 当前在线强化学习方法在流匹配模型与人类偏好对齐中存在探索效率低的问题，依赖无方向的随机性和稀疏的结果奖励，难以发现高奖励样本，导致数据效率低下和优化缓慢。

Method: 提出Euphonium框架，将采样过程形式化为理论上有原则的随机微分方程，明确将过程奖励模型的梯度纳入流漂移中；设计蒸馏目标将引导信号内化到流网络中；采用双奖励组相对策略优化算法，结合潜在过程奖励和像素级结果奖励。

Result: 在文本到视频生成任务中，Euphonium相比现有方法实现了更好的对齐效果，同时将训练收敛速度提高了1.66倍。

Conclusion: Euphonium通过过程奖励梯度引导的动力学有效解决了流匹配模型对齐中的探索效率问题，提供了一种理论上有原则且实用的框架，在保持高质量生成的同时显著加速了训练过程。

Abstract: While online Reinforcement Learning has emerged as a crucial technique for aligning flow matching models with human preferences, current approaches are hindered by inefficient exploration during training rollouts. Relying on undirected stochasticity and sparse outcome rewards, these methods struggle to discover high-reward samples, resulting in data-inefficient and slow optimization. To address these limitations, we propose Euphonium, a novel framework that steers generation via process reward gradient guided dynamics. Our key insight is to formulate the sampling process as a theoretically principled Stochastic Differential Equation that explicitly incorporates the gradient of a Process Reward Model into the flow drift. This design enables dense, step-by-step steering toward high-reward regions, advancing beyond the unguided exploration in prior works, and theoretically encompasses existing sampling methods (e.g., Flow-GRPO, DanceGRPO) as special cases. We further derive a distillation objective that internalizes the guidance signal into the flow network, eliminating inference-time dependency on the reward model. We instantiate this framework with a Dual-Reward Group Relative Policy Optimization algorithm, combining latent process rewards for efficient credit assignment with pixel-level outcome rewards for final visual fidelity. Experiments on text-to-video generation show that Euphonium achieves better alignment compared to existing methods while accelerating training convergence by 1.66x.

</details>


### [199] [TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation](https://arxiv.org/abs/2602.04929)
*Junhan Kim,Yeo Jeong Park,Seungwoo Son,Chungman Lee,Ho-young Kim,Joonyoung Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: TurboBoA是一种新的后训练量化算法，在保持BoA准确性的同时显著加速量化过程，通过联合量化、误差修正和自适应网格计算实现3倍以上加速，在低比特量化中达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: GPTQ虽然高效但在低比特量化时准确率下降严重，BoA通过考虑层间依赖提高了准确性但效率低下。需要一种既能保持BoA准确性又能显著加速的后训练量化方法。

Method: 提出TurboBoA算法：1) 多输出通道联合量化，使用闭式误差补偿规则减少顺序瓶颈；2) 对前层量化误差的修正机制；3) 自适应网格计算配合坐标下降优化，保持迭代更新中的对齐。

Result: 实验表明TurboBoA相比BoA实现显著加速（3倍以上），同时持续提升准确率。结合离群值抑制技术，在仅权重量化和权重-激活量化中都达到最先进结果。

Conclusion: TurboBoA是一种无需反向传播的后训练量化算法，成功解决了BoA的效率问题，在保持高准确性的同时大幅加速量化过程，为大规模语言模型的低比特量化提供了高效解决方案。

Abstract: The rapid growth of large language models (LLMs) has heightened the importance of post-training quantization (PTQ) for reducing memory and computation costs. Among PTQ methods, GPTQ has gained significant attention for its efficiency, enabling billion-scale LLMs to be quantized within a few GPU hours. However, GPTQ's assumption of layer-wise independence leads to severe accuracy drops in low-bit regimes. Recently, BoA improved upon GPTQ by incorporating inter-layer dependencies within attention modules, but its reliance on sequential quantization across all out-channels makes it substantially less efficient. In this paper, we propose TurboBoA, a new backpropagation-free PTQ algorithm that preserves the accuracy benefits of BoA while significantly accelerating the process. The proposed TurboBoA introduces three key innovations: (i) joint quantization of multiple out-channels with a closed-form error compensation rule, which reduces sequential bottlenecks and yields more than a three-fold speedup; (ii) a correction mechanism for errors propagated from preceding quantized layers; and (iii) adaptive grid computation with coordinate descent refinement to maintain alignment during iterative updates. Extensive experiments demonstrate that TurboBoA delivers substantial acceleration over BoA while consistently improving accuracy. When combined with outlier suppression techniques, it achieves state-of-the-art results in both weight-only and weight-activation quantization. The code will be available at https://github.com/SamsungLabs/TurboBoA.

</details>


### [200] [Joint Embedding Variational Bayes](https://arxiv.org/abs/2602.05639)
*Amin Oji,Paul Fieguth*

Main category: cs.LG

TL;DR: VJE结合联合嵌入与变分推断，在非对比、无重建的自监督学习中学习概率表示，通过对称条件ELBO优化，使用学生t分布解耦方向与径向因子，在多个数据集上达到基准性能并提升异常检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法通常基于对比学习或重建目标，缺乏对表示不确定性的建模能力。VJE旨在开发一种无需重建、非对比的框架，能够学习具有概率语义的表示，更好地捕捉数据的不确定性。

Method: VJE将联合嵌入与变分推断结合，最大化对称条件证据下界(ELBO)。使用学生t分布作为条件似然，通过极坐标分解显式解耦方向与径向因子以避免训练不稳定。采用摊销推理网络参数化对角高斯变分后验，其方差与似然尺度共享以捕获各向异性不确定性。

Result: 在ImageNet-1K、CIFAR-10/100和STL-10数据集上，VJE在线性和k-NN评估中达到标准非对比基线的可比性能。在CIFAR-10单类异常检测任务中，基于该模型似然的评分优于可比的自监督基线。

Conclusion: VJE成功地将变分推断融入联合嵌入框架，实现了无重建、非对比的概率表示学习。该方法不仅能获得与现有方法相当的表示质量，还能提供有意义的概率语义，在异常检测等任务中表现出优势。

Abstract: We introduce Variational Joint Embedding (VJE), a framework that synthesizes joint embedding and variational inference to enable self-supervised learning of probabilistic representations in a reconstruction-free, non-contrastive setting. Compared to energy-based predictive objectives that optimize pointwise discrepancies, VJE maximizes a symmetric conditional evidence lower bound (ELBO) for a latent-variable model defined directly on encoder embeddings. We instantiate the conditional likelihood with a heavy-tailed Student-$t$ model using a polar decomposition that explicitly decouples directional and radial factors to prevent norm-induced instabilities during training. VJE employs an amortized inference network to parameterize a diagonal Gaussian variational posterior whose feature-wise variances are shared with the likelihood scale to capture anisotropic uncertainty without auxiliary projection heads. Across ImageNet-1K, CIFAR-10/100, and STL-10, VJE achieves performance comparable to standard non-contrastive baselines under linear and k-NN evaluation. We further validate these probabilistic semantics through one-class CIFAR-10 anomaly detection, where likelihood-based scoring under the proposed model outperforms comparable self-supervised baselines.

</details>


### [201] [Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models](https://arxiv.org/abs/2602.04931)
*Shahar Haim,Daniel C McNamee*

Main category: cs.LG

TL;DR: LLMs在深层从上下文处理转向预测形成，表征几何重组，角度组织参数化预测分布相似性，范数编码上下文特定信息。


<details>
  <summary>Details</summary>
Motivation: 理解仅解码器大语言模型如何将上下文信息转化为预测的内部计算机制，特别是表征几何在预测形成过程中的作用。

Method: 结合几何分析和机制干预的统一框架，分析LLM深度层表征的几何特性，包括角度组织和范数编码。

Result: 发现LLM在深层经历从上下文处理到预测形成的转变，表征几何重组：角度组织参数化预测分布相似性，范数编码上下文特定信息。

Conclusion: 为LLM将上下文转化为预测的动态过程提供了机制-几何解释，揭示了表征几何在选择性因果控制中的作用。

Abstract: We show that decoder-only large language models exhibit a depth-wise transition from context-processing to prediction-forming phases of computation accompanied by a reorganization of representational geometry. Using a unified framework combining geometric analysis with mechanistic intervention, we demonstrate that late-layer representations implement a structured geometric code that enables selective causal control over token prediction. Specifically, angular organization of the representation geometry parametrizes prediction distributional similarity, while representation norms encode context-specific information that does not determine prediction. Together, these results provide a mechanistic-geometric account of the dynamics of transforming context into predictions in LLMs.

</details>


### [202] [Limitations of SGD for Multi-Index Models Beyond Statistical Queries](https://arxiv.org/abs/2602.05704)
*Daniel Barzilai,Ohad Shamir*

Main category: cs.LG

TL;DR: 论文提出新框架分析标准SGD在单/多索引模型中的局限性，超越传统SQ框架的不足


<details>
  <summary>Details</summary>
Motivation: 现有SQ框架与SGD的连接薄弱，通常依赖对抗性或特殊结构的梯度噪声，不能反映标准SGD的噪声特性，有时会导致错误预测。许多SGD分析需要非平凡算法修改，限制了理论解释的普适性。

Method: 开发新的非SQ框架来分析标准SGD在单索引和多索引模型中的局限性，适用于广泛设置和架构（包括深度神经网络）

Result: 新框架能够更准确地分析标准SGD的局限性，避免了传统SQ框架的缺陷，为理解SGD在低维投影依赖问题中的表现提供了新工具

Conclusion: 提出的非SQ框架填补了现有理论工具的空白，为研究标准SGD在单索引和多索引模型中的根本局限性提供了更准确的分析方法

Abstract: Understanding the limitations of gradient methods, and stochastic gradient descent (SGD) in particular, is a central challenge in learning theory. To that end, a commonly used tool is the Statistical Queries (SQ) framework, which studies performance limits of algorithms based on noisy interaction with the data. However, it is known that the formal connection between the SQ framework and SGD is tenuous: Existing results typically rely on adversarial or specially-structured gradient noise that does not reflect the noise in standard SGD, and (as we point out here) can sometimes lead to incorrect predictions. Moreover, many analyses of SGD for challenging problems rely on non-trivial algorithmic modifications, such as restricting the SGD trajectory to the sphere or using very small learning rates. To address these shortcomings, we develop a new, non-SQ framework to study the limitations of standard vanilla SGD, for single-index and multi-index models (namely, when the target function depends on a low-dimensional projection of the inputs). Our results apply to a broad class of settings and architectures, including (potentially deep) neural networks.

</details>


### [203] [Comparing Euclidean and Hyperbolic K-Means for Generalized Category Discovery](https://arxiv.org/abs/2602.04932)
*Mohamad Dalal,Thomas B. Moeslund,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: HC-GCD提出在双曲空间中直接进行聚类，而非像先前方法那样在双曲空间学习表示后转回欧氏空间聚类，在语义偏移基准测试中表现与先前最佳双曲GCD方法相当，且双曲K-Means优于欧氏K-Means。


<details>
  <summary>Details</summary>
Motivation: 现有双曲广义类别发现方法仅在表示学习阶段使用双曲几何，聚类时转回欧氏空间，作者认为这种混合方式不是最优的，应该直接在双曲空间中进行聚类。

Method: 提出HC-GCD方法：在Lorentz双曲面模型中学习嵌入表示，并使用双曲K-Means算法直接在双曲空间中对这些嵌入进行聚类。

Result: 在语义偏移基准数据集上，HC-GCD与先前最佳双曲GCD方法性能相当；双曲K-Means比欧氏K-Means准确率更高；欧氏嵌入范数裁剪对未见类和已见类准确率有不同影响；双曲K-Means在不同标签粒度下产生更一致的聚类结果。

Conclusion: 直接在双曲空间中进行聚类是可行的，双曲K-Means优于欧氏K-Means，且能产生更稳定的聚类结果，为广义类别发现任务提供了更好的双曲几何应用方案。

Abstract: Hyperbolic representation learning has been widely used to extract implicit hierarchies within data, and recently it has found its way to the open-world classification task of Generalized Category Discovery (GCD). However, prior hyperbolic GCD methods only use hyperbolic geometry for representation learning and transform back to Euclidean geometry when clustering. We hypothesize this is suboptimal. Therefore, we present Hyperbolic Clustered GCD (HC-GCD), which learns embeddings in the Lorentz Hyperboloid model of hyperbolic geometry, and clusters these embeddings directly in hyperbolic space using a hyperbolic K-Means algorithm. We test our model on the Semantic Shift Benchmark datasets, and demonstrate that HC-GCD is on par with the previous state-of-the-art hyperbolic GCD method. Furthermore, we show that using hyperbolic K-Means leads to better accuracy than Euclidean K-Means. We carry out ablation studies showing that clipping the norm of the Euclidean embeddings leads to decreased accuracy in clustering unseen classes, and increased accuracy for seen classes, while the overall accuracy is dataset dependent. We also show that using hyperbolic K-Means leads to more consistent clusters when varying the label granularity.

</details>


### [204] [Muon in Associative Memory Learning: Training Dynamics and Scaling Laws](https://arxiv.org/abs/2602.05725)
*Binghui Li,Kaifei Wang,Han Zhong,Pinyan Lu,Liwei Wang*

Main category: cs.LG

TL;DR: Muon优化器通过梯度矩阵符号更新参数，在理论和实验上均优于梯度下降，特别是在处理不平衡频率分量时表现更佳


<details>
  <summary>Details</summary>
Motivation: Muon优化器在实践中表现出色，但其理论动态和缩放行为尚不明确，需要在线性关联记忆模型中研究其优化特性

Method: 在线性关联记忆模型中分析Muon优化器，使用softmax检索和分层频率谱，研究有无标签噪声的情况，并与梯度下降进行对比

Result: 在无噪声情况下，Muon相比梯度下降获得指数级加速；在有噪声的幂衰减频率谱中，Muon具有更优的缩放效率；Muon可解释为自适应任务对齐和块对称梯度结构产生的隐式矩阵预处理器

Conclusion: Muon优化器通过缓解频率分量学习的不平衡性，在理论和实际应用中均优于传统梯度下降，特别是在长尾分类和LLaMA预训练等任务中表现突出

Abstract: Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.

</details>


### [205] [Linear Model Merging Unlocks Simple and Scalable Multimodal Data Mixture Optimization](https://arxiv.org/abs/2602.04937)
*Davide Berasi,Matteo Farina,Massimiliano Mancini,Elisa Ricci*

Main category: cs.LG

TL;DR: 本文提出通过模型合并作为数据混合优化的高效代理方法，解决多模态大语言模型监督微调中数据混合权重搜索的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的监督微调中，选择最佳数据混合至关重要，但确定跨多个领域特定数据集的最优混合权重存在组合搜索空间大和单次训练成本高的瓶颈问题（数据混合优化问题）。而模型合并虽然高效，但通常产生次优模型。

Method: 训练领域特定的多模态专家模型，通过评估它们的加权参数空间组合来估计相应数据混合的效果。将模型合并作为估计不同数据混合性能的高效策略。

Result: 在14个多模态基准测试上进行广泛实验，实证证明合并的代理模型与实际数据混合训练的模型具有高秩相关性。这解耦了最优混合搜索与资源密集型训练过程。

Conclusion: 模型合并为导航复杂混合权重空间提供了可扩展且高效的策略，解决了数据混合优化的瓶颈问题，代码已公开。

Abstract: Selecting the best data mixture is critical for successful Supervised Fine-Tuning (SFT) of Multimodal Large Language Models. However, determining the optimal mixture weights across multiple domain-specific datasets remains a significant bottleneck due to the combinatorial search space and the high cost associated with even a single training run. This is the so-called Data Mixture Optimization (DMO) problem. On the other hand, model merging unifies domain-specific experts through parameter interpolation. This strategy is efficient, as it only requires a single training run per domain, yet oftentimes leads to suboptimal models. In this work, we take the best of both worlds, studying model merging as an efficient strategy for estimating the performance of different data mixtures. We train domain-specific multimodal experts and evaluate their weighted parameter-space combinations to estimate the efficacy of corresponding data mixtures. We conduct extensive experiments on 14 multimodal benchmarks, and empirically demonstrate that the merged proxy models exhibit a high rank correlation with models trained on actual data mixtures. This decouples the search for optimal mixtures from the resource-intensive training process, thereby providing a scalable and efficient strategy for navigating the complex landscape of mixture weights. Code is publicly available at https://github.com/BerasiDavide/mLLMs_merging_4_DMO.

</details>


### [206] [Selecting Hyperparameters for Tree-Boosting](https://arxiv.org/abs/2602.05786)
*Floris Jan Koster,Fabio Sigrist*

Main category: cs.LG

TL;DR: 本文通过59个数据集比较了多种树提升超参数优化方法，发现SMAC方法表现最佳，并揭示了超参数调优的重要发现


<details>
  <summary>Details</summary>
Motivation: 树提升是处理表格数据的常用机器学习技术，但其样本外精度严重依赖多个超参数设置。目前存在多种超参数优化方法，但缺乏对这些方法在树提升任务上的系统实证比较。

Method: 使用59个回归和分类数据集，实证比较了随机网格搜索、TPE、GP-BO、Hyperband、SMAC和确定性全网格搜索等多种超参数优化方法在树提升任务上的表现。

Result: SMAC方法在所有比较方法中表现最佳；需要超过100次试验才能获得准确调优；使用默认超参数值会导致模型精度严重不足；所有考虑的hyperparameters都对精度有实质性影响；回归任务中通过早停选择提升迭代次数比将其纳入搜索空间效果更好。

Conclusion: 对于树提升模型的超参数优化，SMAC是最有效的选择，且需要充分的调优试验次数，不能依赖默认参数，所有超参数都同等重要，回归任务中早停策略优于将迭代次数作为搜索参数。

Abstract: Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.

</details>


### [207] [Transolver-3: Scaling Up Transformer Solvers to Industrial-Scale Geometries](https://arxiv.org/abs/2602.04940)
*Hang Zhou,Haixu Wu,Haonan Shangguan,Yuezhou Ma,Huikun Weng,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Transolver-3是一个高度可扩展的神经PDE求解框架，通过架构优化和训练策略，能够处理超过1.6亿个网格单元的工业级几何体，实现高保真物理仿真。


<details>
  <summary>Details</summary>
Motivation: 深度学习在神经PDE求解方面展现出潜力，但扩展到工业级几何体（超过10^8个网格单元）面临内存复杂度的根本挑战，现有方法难以处理高分辨率网格。

Method: 1) 利用矩阵乘法结合律实现更快的切片和反切片；2) 几何切片平铺技术划分物理状态计算；3) 在原始高分辨率网格的随机子集上进行摊销训练；4) 推理时使用物理状态缓存技术。

Result: Transolver-3能够处理超过1.6亿个网格单元的网格，在包括飞机和汽车设计任务在内的三个具有挑战性的仿真基准测试中表现出色。

Conclusion: Transolver-3通过创新的架构优化和训练策略，成功解决了神经PDE求解器扩展到工业级几何体的内存瓶颈问题，为复杂工程任务的高保真物理仿真提供了可行方案。

Abstract: Deep learning has emerged as a transformative tool for the neural surrogate modeling of partial differential equations (PDEs), known as neural PDE solvers. However, scaling these solvers to industrial-scale geometries with over $10^8$ cells remains a fundamental challenge due to the prohibitive memory complexity of processing high-resolution meshes. We present Transolver-3, a new member of the Transolver family as a highly scalable framework designed for high-fidelity physics simulations. To bridge the gap between limited GPU capacity and the resolution requirements of complex engineering tasks, we introduce two key architectural optimizations: faster slice and deslice by exploiting matrix multiplication associative property and geometry slice tiling to partition the computation of physical states. Combined with an amortized training strategy by learning on random subsets of original high-resolution meshes and a physical state caching technique during inference, Transolver-3 enables high-fidelity field prediction on industrial-scale meshes. Extensive experiments demonstrate that Transolver-3 is capable of handling meshes with over 160 million cells, achieving impressive performance across three challenging simulation benchmarks, including aircraft and automotive design tasks.

</details>


### [208] [Improving Set Function Approximation with Quasi-Arithmetic Neural Networks](https://arxiv.org/abs/2602.04941)
*Tomas Tokar,Scott Sanner*

Main category: cs.LG

TL;DR: 提出Neuralized Kolmogorov Mean（NKM）和quasi-arithmetic neural networks（QUANNs），通过可学习的可逆神经聚合函数提升集合建模的表达能力和迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有集合建模方法（如DeepSets、PointNet）使用固定的非学习性池化操作（如求和、最大值），这限制了学习嵌入的迁移性和模型表达能力。需要更灵活的可学习聚合函数。

Method: 提出Neuralized Kolmogorov Mean（NKM），通过可逆神经函数学习广义中心趋势度量。进一步提出quasi-arithmetic neural networks（QUANNs），将NKM作为可学习聚合函数集成到神经网络中。

Result: 理论分析表明QUANNs是广泛集合函数分解的通用逼近器，且由于可逆神经组件能学习更有结构的潜在表示。实证中QUANNs在多个基准测试中优于现有方法，学习的嵌入能有效迁移到非集合任务。

Conclusion: NKM和QUANNs通过可学习的可逆神经聚合函数显著提升了集合建模的表达能力和迁移性，为集合结构数据处理提供了更强大的框架。

Abstract: Sets represent a fundamental abstraction across many types of data. To handle the unordered nature of set-structured data, models such as DeepSets and PointNet rely on fixed, non-learnable pooling operations (e.g., sum or max) -- a design choice that can hinder the transferability of learned embeddings and limits model expressivity. More recently, learnable aggregation functions have been proposed as more expressive alternatives. In this work, we advance this line of research by introducing the Neuralized Kolmogorov Mean (NKM) -- a novel, trainable framework for learning a generalized measure of central tendency through an invertible neural function. We further propose quasi-arithmetic neural networks (QUANNs), which incorporate the NKM as a learnable aggregation function. We provide a theoretical analysis showing that, QUANNs are universal approximators for a broad class of common set-function decompositions and, thanks to their invertible neural components, learn more structured latent representations. Empirically, QUANNs outperform state-of-the-art baselines across diverse benchmarks, while learning embeddings that transfer effectively even to tasks that do not involve sets.

</details>


### [209] [Principled Confidence Estimation for Deep Computed Tomography](https://arxiv.org/abs/2602.05812)
*Matteo Gätzner,Johannes Kirschner*

Main category: cs.LG

TL;DR: 提出基于序列似然混合框架的CT重建置信估计方法，为深度学习重建提供理论覆盖保证的置信区域


<details>
  <summary>Details</summary>
Motivation: 需要为基于深度学习的CT重建提供可靠的置信估计，以检测重建图像中的幻觉并提供可解释的不确定性可视化

Method: 基于序列似然混合框架，采用符合Beer-Lambert定律的真实前向模型（对数线性模型加泊松噪声），适用于经典算法和深度学习重建方法

Result: 深度重建方法相比经典方法产生更紧的置信区域，同时保持理论覆盖保证，能够检测重建幻觉并提供置信区域的可视化

Conclusion: 深度学习模型不仅是强大的估计器，也是医学成像中可靠的不确定性感知工具

Abstract: We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.

</details>


### [210] [Privileged Information Distillation for Language Models](https://arxiv.org/abs/2602.04942)
*Emiliano Penaloza,Dheeraj Vattikonda,Nicolas Gontier,Alexandre Lacoste,Laurent Charlin,Massimo Caccia*

Main category: cs.LG

TL;DR: 提出π-Distill和OPSD两种方法，用于在训练时使用特权信息但推理时无法访问的情况下，蒸馏前沿智能体模型，在多个基准测试中优于行业标准方法。


<details>
  <summary>Details</summary>
Motivation: 在强化学习的困难、长视野任务中，训练时的特权信息（PI）能帮助语言模型成功完成任务，但如何将基于PI学到的能力转移到推理时无法访问PI的策略中是一个根本性挑战。特别是在蒸馏前沿智能体模型时，闭源系统通常只暴露动作轨迹而隐藏内部推理过程，这使得标准蒸馏流程失效。

Method: 提出两种方法：1) π-Distill：联合教师-学生目标，使用同一模型同时训练基于PI的教师和无PI的学生；2) OPSD（策略上自蒸馏）：使用强化学习训练，并在学生和基于PI的教师之间添加反向KL惩罚项。

Result: 两种算法都能有效地仅使用动作轨迹特权信息蒸馏前沿智能体。π-Distill（在某些情况下OPSD）在多个智能体基准测试、模型和PI形式上，都优于假设能访问完整思维链监督的行业标准实践（监督微调后接强化学习）。

Conclusion: π-Distill和OPSD为解决训练时使用特权信息但推理时无法访问的蒸馏问题提供了有效解决方案，特别适用于前沿智能体模型的蒸馏场景，其中只能观察到成功行为而无法访问推理过程。

Abstract: Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that π-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.

</details>


### [211] [Exact Recovery in the Data Block Model](https://arxiv.org/abs/2602.05852)
*Amir R. Asadi,Akbar Davoodi,Ramin Javadi,Farzad Parvaresh*

Main category: cs.LG

TL;DR: 本文研究了带节点数据的块模型（DBM）中的精确恢复问题，提出了Chernoff-TV散度来刻画精确恢复的尖锐阈值，并给出了达到该阈值的有效算法。


<details>
  <summary>Details</summary>
Motivation: 现实世界网络通常包含节点属性或标签等额外数据，而传统的随机块模型（SBM）仅基于图连接性研究社区检测。需要研究如何利用这些额外数据来改进社区检测的精确恢复性能。

Method: 引入Chernoff-TV散度作为分析工具，用于刻画带节点数据的块模型（DBM）中精确恢复的阈值。提出了一个高效的算法来达到这个阈值，并给出了匹配的逆结果证明阈值以下不可能实现精确恢复。

Result: 理论分析得到了DBM中精确恢复的尖锐阈值，算法能够达到该阈值，仿真验证了理论结果，并展示了利用节点数据作为辅助信息在社区检测中的优势。

Conclusion: 节点数据作为辅助信息能够显著改善社区检测的精确恢复性能，Chernoff-TV散度为分析此类问题提供了有效的理论工具，所提算法在实践中具有高效性和可行性。

Abstract: Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and Verdú (2017). We introduce the Chernoff--TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.

</details>


### [212] [Stochastic hierarchical data-driven optimization: application to plasma-surface kinetics](https://arxiv.org/abs/2602.04975)
*José Afonso,Vasco Guerra,Pedro Viegas*

Main category: cs.LG

TL;DR: 提出基于Sloppy Model理论的随机分层优化框架，通过约化Hessian近似识别刚性参数子空间，高效校准物理模型，在等离子体-表面相互作用问题上验证了样本效率优势。


<details>
  <summary>Details</summary>
Motivation: 物理模型校准面临参数不确定性高、计算成本大的挑战，特别是在等离子体-表面相互作用等复杂反应系统中，需要高效且可扩展的优化方法。

Method: 基于Sloppy Model理论，使用约化Hessian近似识别刚性参数子空间，结合概率公式从观测数据推导目标损失函数，实现高效的分层优化。

Result: 在等离子体-表面相互作用问题上验证，该方法在样本效率上持续优于基线优化技术，能够处理高度各向异性的参数空间。

Conclusion: 该方法为复杂反应系统（从等离子体化学到生化网络）的模型优化提供了通用且可扩展的工具，显著提高了校准效率。

Abstract: This work introduces a stochastic hierarchical optimization framework inspired by Sloppy Model theory for the efficient calibration of physical models. Central to this method is the use of a reduced Hessian approximation, which identifies and targets the stiff parameter subspace using minimal simulation queries. This strategy enables efficient navigation of highly anisotropic landscapes, avoiding the computational burden of exhaustive sampling. To ensure rigorous inference, we integrate this approach with a probabilistic formulation that derives a principled objective loss function directly from observed data. We validate the framework by applying it to the problem of plasma-surface interactions, where accurate modelling is strictly limited by uncertainties in surface reactivity parameters and the computational cost of kinetic simulations. Comparative analysis demonstrates that our method consistently outperforms baseline optimization techniques in sample efficiency. This approach offers a general and scalable tool for optimizing models of complex reaction systems, ranging from plasma chemistry to biochemical networks.

</details>


### [213] [CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs](https://arxiv.org/abs/2602.05861)
*Seyedmasoud Mousavi,Ruomeng Xu,Xiaojing Zhu*

Main category: cs.LG

TL;DR: CFRecs是一个将反事实解释转化为可操作推荐的新框架，应用于房地产推荐系统，通过最小化图结构改变来驱动期望结果。


<details>
  <summary>Details</summary>
Motivation: 虽然图神经网络广泛用于图结构数据学习，反事实图学习能提升模型可解释性，但现有反事实解释研究主要关注识别导致不同预测的反事实图，缺乏将这些解释转化为实际可操作建议的框架。

Method: CFRecs采用两阶段架构：图神经网络(GNN)和图变分自编码器(Graph-VAE)，战略性地提出最小但高影响力的图结构和节点属性改变，以驱动推荐系统中的期望结果。

Result: 在Zillow的用户-房源交互数据上的实验结果表明CFRecs的有效性，为购房者和卖家提供可操作建议，帮助他们在竞争激烈的房地产市场实现购房目标。

Conclusion: CFRecs成功将反事实解释转化为可操作推荐，为基于图的反事实推理在推荐系统中的应用提供了新视角，特别适用于房地产等复杂决策场景。

Abstract: Graph-structured data is ubiquitous and powerful in representing complex relationships in many online platforms. While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to improve model interpretability. Counterfactual explanation research focuses on identifying a counterfactual graph that is similar to the original but leads to different predictions. These explanations optimize two objectives simultaneously: the sparsity of changes in the counterfactual graph and the validity of its predictions. Building on these qualitative optimization goals, this paper introduces CFRecs, a novel framework that transforms counterfactual explanations into actionable insights. CFRecs employs a two-stage architecture consisting of a graph neural network (GNN) and a graph variational auto-encoder (Graph-VAE) to strategically propose minimal yet high-impact changes in graph structure and node attributes to drive desirable outcomes in recommender systems. We apply CFRecs to Zillow's graph-structured data to deliver actionable recommendations for both home buyers and sellers with the goal of helping them navigate the competitive housing market and achieve their homeownership goals. Experimental results on Zillow's user-listing interaction data demonstrate the effectiveness of CFRecs, which also provides a fresh perspective on recommendations using counterfactual reasoning in graphs.

</details>


### [214] [Unbiased Single-Queried Gradient for Combinatorial Objective](https://arxiv.org/abs/2602.05119)
*Thanawat Sornwanee*

Main category: cs.LG

TL;DR: 提出一种用于组合优化问题的随机梯度方法，只需单次查询即可获得无偏梯度估计，包含REINFORCE作为特例


<details>
  <summary>Details</summary>
Motivation: 组合优化问题概率化重构后，在超立方体上进行优化时，精确梯度计算需要多次查询组合函数，计算成本高

Method: 提出一种随机梯度方法，通过单次查询组合函数即可获得无偏梯度估计，该方法包含REINFORCE（通过重要性采样）作为特例，并扩展为一类新的随机梯度

Result: 该方法能够有效降低梯度计算的计算复杂度，同时保持无偏性，为组合优化问题提供了更高效的优化工具

Conclusion: 提出的随机梯度框架为组合优化问题的概率化求解提供了高效且通用的梯度估计方法，显著减少了计算开销

Abstract: In a probabilistic reformulation of a combinatorial problem, we often face an optimization over a hypercube, which corresponds to the Bernoulli probability parameter for each binary variable in the primal problem. The combinatorial nature suggests that an exact gradient computation requires multiple queries. We propose a stochastic gradient that is unbiased and requires only a single query of the combinatorial function. This method encompasses a well-established REINFORCE (through an importance sampling), as well as including a class of new stochastic gradients.

</details>


### [215] [Near-Optimal Dynamic Matching via Coarsening with Application to Heart Transplantation](https://arxiv.org/abs/2602.04989)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 提出基于粗化方法的在线匹配算法，通过将离线节点聚合成有容量限制的集群，获得接近最优的理论保证，应用于心脏移植分配，在模拟中接近全知基准性能。


<details>
  <summary>Details</summary>
Motivation: 在线匹配在互联网广告和器官分配等领域应用广泛，但实用算法往往缺乏强理论保证。需要弥合数据驱动启发式方法与悲观理论下界之间的差距。

Method: 开发基于粗化方法的在线匹配算法，将离线节点聚合成有容量限制的集群，利用历史数据的结构特性构建理论上有保障的策略。

Result: 在心脏移植分配的现实模拟中，提出的策略性能接近全知基准（omniscient benchmark），为器官分配中基于聚类的方法提供了严格的理论依据。

Conclusion: 粗化方法虽然通常意味着粒度损失，但通过聚合离线节点为有容量集群，可以获得接近最优的理论保证，弥合了数据驱动启发式与理论下界之间的差距。

Abstract: Online matching has been a mainstay in domains such as Internet advertising and organ allocation, but practical algorithms often lack strong theoretical guarantees. We take an important step toward addressing this by developing new online matching algorithms based on a coarsening approach. Although coarsening typically implies a loss of granularity, we show that, to the contrary, aggregating offline nodes into capacitated clusters can yield near-optimal theoretical guarantees. We apply our methodology to heart transplant allocation to develop theoretically grounded policies based on structural properties of historical data. In realistic simulations, our policy closely matches the performance of the omniscient benchmark. Our work bridges the gap between data-driven heuristics and pessimistic theoretical lower bounds, and provides rigorous justification for prior clustering-based approaches in organ allocation.

</details>


### [216] [$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment](https://arxiv.org/abs/2602.05946)
*Rajdeep Haldar,Lantao Mei,Guang Lin,Yue Xing,Qifan Song*

Main category: cs.LG

TL;DR: 该论文提出基于f-散度的统一对齐框架，包含f-GRPO（在线强化学习）和f-HAL（混合在线/离线目标），在RLVR和PA任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐（PA）目标可视为对齐与未对齐响应分布之间的散度估计器，但需要扩展到更一般的对齐设置（如仅使用环境奖励的强化学习），以建立统一的对齐框架。

Method: 基于f-散度的变分表示，提出f-GRPO（在线强化学习类方法）和f-HAL（混合在线/离线目标），为通用LLM对齐提供理论保证。

Result: 在RLVR（数学推理）和PA（安全对齐）任务上的实验验证了框架的有效性，相比现有方法表现出更优的性能和灵活性。

Conclusion: 提出的基于f-散度的统一对齐框架为LLM对齐提供了理论保证和实用方法，在多种对齐设置下均能有效提升平均奖励。

Abstract: Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.

</details>


### [217] [Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives](https://arxiv.org/abs/2602.04990)
*Ioannis Anagnostides,Itai Zilberstein,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文主张器官分配应考虑激励因素，提出下一代分配政策应具备激励意识，并呼吁机器学习社区整合机制设计、战略分类、因果推断和社会选择等方法。


<details>
  <summary>Details</summary>
Motivation: 当前器官分配算法从基于规则的系统向机器学习和数据驱动优化转变，但往往忽视了激励这一根本障碍。器官分配不仅是静态优化问题，而是涉及移植中心、临床医生和监管机构的复杂博弈。

Method: 本文是一篇立场论文，聚焦美国成人心脏移植分配，识别决策流程中的关键激励错配，并通过数据展示其当前负面影响。提出整合机制设计、战略分类、因果推断和社会选择的研究议程。

Result: 识别了器官分配决策流程中的激励错配问题，并通过数据证明这些错配正在产生不利后果。提出了激励意识分配政策的概念框架。

Conclusion: 下一代器官分配政策应具备激励意识，机器学习社区需要整合多学科方法（机制设计、战略分类、因果推断、社会选择）来应对战略行为，确保分配系统的鲁棒性、效率和公平性。

Abstract: The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamental barrier: incentives. In this position paper, we highlight that organ allocation is not merely a static optimization problem, but rather a complex game involving transplant centers, clinicians, and regulators. Focusing on US adult heart transplant allocation, we identify critical incentive misalignments across the decision-making pipeline, and present data showing that they are having adverse consequences today. Our main position is that the next generation of allocation policies should be incentive aware. We outline a research agenda for the machine learning community, calling for the integration of mechanism design, strategic classification, causal inference, and social choice to ensure robustness, efficiency, and fairness in the face of strategic behavior from the various constituent groups.

</details>


### [218] [Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces](https://arxiv.org/abs/2602.05961)
*Arran Carter,Sanghyeok Choi,Kirill Tamogashev,Víctor Elvira,Nikolay Malkin*

Main category: cs.LG

TL;DR: 提出离散扩散采样器的离策略训练技术，改进离散采样性能，并扩展到数据到能量的薛定谔桥任务，应用于图像生成模型的离散隐空间后验采样。


<details>
  <summary>Details</summary>
Motivation: 扩散采样算法在连续空间采样中已广泛应用，但在离散空间的应用仍不充分，现有离散扩散采样器未能充分利用连续空间采样的先进思想。

Method: 引入离策略训练技术改进离散扩散采样器；将离散扩散采样器推广到连接两个任意分布的任务，首次提出离散域的数据到能量薛定谔桥训练。

Result: 离策略训练技术提高了离散采样器在现有和新合成基准上的性能；成功将离散扩散采样器应用于图像生成模型的离散隐空间数据自由后验采样。

Conclusion: 通过引入离策略训练技术，弥合了连续和离散空间扩散采样器之间的差距，扩展了离散扩散采样器的能力，为离散空间采样提供了更有效的工具。

Abstract: Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.

</details>


### [219] [Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning](https://arxiv.org/abs/2602.04998)
*Yu-Ang Lee,Ching-Yun Ko,Pin-Yu Chen,Mi-Yen Yeh*

Main category: cs.LG

TL;DR: LoRA变体在适当调参后与原始LoRA性能相当，超参数调优比架构改进更重要


<details>
  <summary>Details</summary>
Motivation: 重新评估各种LoRA变体相对于原始LoRA的实际改进，质疑在固定超参数设置下报告的改进是否反映真实的方法优势

Method: 对四种代表性LoRA变体和原始LoRA进行系统性的超参数搜索，在数学和代码生成任务上测试不同模型规模

Result: 一旦学习率适当调优，所有方法都达到相似的峰值性能（差异在1-2%内），仅表现出细微的秩依赖行为；不同LoRA方法偏好不同的学习率范围

Conclusion: 原始LoRA仍然是具有竞争力的基线，在单一训练配置下报告的改进可能不反映一致的方法优势；二阶分析显示不同最优学习率范围与最大Hessian特征值变化相关

Abstract: Low-Rank Adaptation (LoRA) is the prevailing approach for efficient large language model (LLM) fine-tuning. Building on this paradigm, recent studies have proposed alternative initialization strategies and architectural modifications, reporting substantial improvements over vanilla LoRA. However, these gains are often demonstrated under fixed or narrowly tuned hyperparameter settings, despite the known sensitivity of neural networks to training configurations. In this work, we systematically re-evaluate four representative LoRA variants alongside vanilla LoRA through extensive hyperparameter searches. Across mathematical and code generation tasks on diverse model scales, we find that different LoRA methods favor distinct learning rate ranges. Crucially, once learning rates are properly tuned, all methods achieve similar peak performance (within 1-2%), with only subtle rank-dependent behaviors. These results suggest that vanilla LoRA remains a competitive baseline and that improvements reported under single training configuration may not reflect consistent methodological advantages. Finally, a second-order analysis attributes the differing optimal learning rate ranges to variations in the largest Hessian eigenvalue, aligning with classical learning theories.

</details>


### [220] [Inverse Depth Scaling From Most Layers Being Similar](https://arxiv.org/abs/2602.05970)
*Yizhou Liu,Sara Kangaslahti,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 研究发现大型语言模型的损失与深度成反比，这种缩放关系源于残差网络架构偏置，深度层通过集成平均而非组合学习来减少误差，导致效率低下但鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 虽然神经缩放定律描述了损失与模型大小的关系，但深度和宽度对性能的影响可能不同，需要更详细的研究来量化深度如何影响损失。

Method: 通过分析大型语言模型和玩具残差网络，研究深度对损失的影响，探索深度缩放关系的机制。

Result: 发现损失与深度成反比，这种关系源于功能相似的层通过集成平均减少误差，而非组合学习或离散化平滑动态，这种机制效率低下但鲁棒性强。

Conclusion: 要提高大型语言模型效率，可能需要架构创新来鼓励深度的组合使用，而不是当前的集成平均机制。

Abstract: Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.

</details>


### [221] [Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization](https://arxiv.org/abs/2602.05657)
*Aleksandar Armacki,Dragana Bajović,Dušan Jakovetić,Soummya Kar,Ali H. Sayed*

Main category: cs.LG

TL;DR: 该研究通过大偏差理论分析SGD方法的长期尾部衰减，为SGD和剪裁SGD在非凸损失和不同噪声假设下提供了精确的尾部衰减率，比现有有限时间界限快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有SGD尾部行为研究多为有限时间的高概率保证，缺乏对固定误差阈值下失败概率的量化，且无法捕捉现代学习模型（通常训练数百万次迭代）的真实长期尾部衰减行为。

Method: 使用大偏差理论研究SGD方法的长期尾部衰减：1) 对非凸损失和有界噪声的普通SGD，分析梯度范数平方的尾部衰减；2) 对重尾噪声（p阶矩有界）的剪裁SGD，分析其尾部衰减；3) 提供相应的下界证明紧致性。

Result: 1) SGD在非凸损失和有界噪声下，长期尾部衰减率为e^{-t/log(t)}；2) 剪裁SGD在p∈(1,2)重尾噪声下衰减率为e^{-t^{β_p}/log(t)}，p=2时为e^{-t/log²(t)}；3) 下界为e^{-t}，证明所得速率在多项式对数因子内是紧致的；比现有有限时间界限快一个数量级。

Conclusion: 该研究填补了SGD长期尾部衰减分析的空白，通过大偏差理论获得了比现有有限时间界限快一个数量级的衰减率，为个体算法运行提供了更强的长期保证，揭示了尾部衰减比先前所知更快的机制。

Abstract: The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{β_p}/\log(t)}$, where $β_p = \frac{4(p-1)}{3p-2}$ for $p \in (1,2)$ and $e^{-t/\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\sqrt{t}}$ and $e^{-t^{β_p/2}}$, $p \in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.

</details>


### [222] [EntRGi: Entropy Aware Reward Guidance for Diffusion Language Models](https://arxiv.org/abs/2602.05000)
*Atula Tejaswi,Litu Rout,Constantine Caramanis,Sanjay Shakkottai,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 提出EntRGi方法，通过基于模型置信度的熵感知机制动态调节奖励梯度，解决了离散扩散语言模型中奖励指导的连续松弛和直通估计器方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 连续扩散模型中的奖励指导在测试时适应中取得了巨大成功，但离散扩散语言模型无法直接微分离散标记。现有方法要么使用连续松弛（但奖励模型从未在连续输入上训练），要么使用直通估计器（在离散标记处评估梯度来更新连续logits导致错误优化）。

Method: 提出EntRGi（熵感知奖励指导）方法，通过基于模型置信度动态调节奖励模型梯度，调制连续松弛，为奖励模型提供可靠输入，同时改善奖励指导效果。

Result: 在7B参数的扩散语言模型上，跨3个不同奖励模型和3个多技能基准测试进行实证验证，相比最先进方法取得了一致的改进。

Conclusion: EntRGi方法通过熵感知机制超越了现有奖励指导方法的权衡，在离散扩散语言模型中实现了更有效的奖励指导，为奖励模型提供了可靠输入并改善了梯度反馈。

Abstract: Reward guidance has been applied to great success in the test-time adaptation of continuous diffusion models; it updates each denoising step using the gradients from a downstream reward model. We study reward guidance for discrete diffusion language models, where one cannot differentiate through the natural outputs of the model because they are discrete tokens. Existing approaches either replace these discrete tokens with continuous relaxations, or employ techniques like the straight-through estimator. In this work, we show the downsides of both these methods. The former degrades gradient feedback because the reward model has never been trained with continuous inputs. The latter involves incorrect optimization because the gradient evaluated at discrete tokens is used to update continuous logits. Our key innovation is to go beyond this tradeoff by introducing a novel mechanism called EntRGi: Entropy aware Reward Guidance that dynamically regulates the gradients from the reward model. By modulating the continuous relaxation using the model's confidence, our approach substantially improves reward guidance while providing reliable inputs to the reward model. We empirically validate our approach on a 7B-parameter diffusion language model across 3 diverse reward models and 3 multi-skill benchmarks, showing consistent improvements over state-of-the-art methods.

</details>


### [223] [Orthogonal Self-Attention](https://arxiv.org/abs/2602.05996)
*Leo Zhang,James Martens*

Main category: cs.LG

TL;DR: 提出正交自注意力(OSA)机制，通过参数化正交注意力矩阵解决标准Softmax自注意力在无跳跃连接架构中的不稳定问题，实现线性计算复杂度并保证雅可比矩阵良好条件数。


<details>
  <summary>Details</summary>
Motivation: 标准Softmax自注意力(SSA)在无跳跃连接架构中会导致秩崩溃和雅可比矩阵条件数差的问题，限制了Transformer的训练稳定性。需要设计新的注意力机制来绕过这些问题。

Method: 设计正交自注意力(OSA)：通过将查询-键值形成的斜对称矩阵映射到矩阵指数，参数化正交注意力矩阵。利用查询-键值的低秩结构实现线性计算复杂度，并推导出保证雅可比矩阵良好条件数的初始化方案。

Result: OSA能够实现线性序列长度的计算复杂度和内存成本，同时通过理论证明其雅可比矩阵具有良好的条件数，使得无跳跃连接和归一化层的Transformer更容易训练。

Conclusion: 正交自注意力机制有效解决了Softmax自注意力在无跳跃连接架构中的不稳定问题，为训练更简单的Transformer架构提供了可行的解决方案。

Abstract: Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.

</details>


### [224] [Enhanced QKNorm normalization for neural transformers with the Lp norm](https://arxiv.org/abs/2602.05006)
*Ezequiel Lopez-Rubio,Javier Montes-Perez,Esteban Jose Palomo*

Main category: cs.LG

TL;DR: 提出基于Lp范数的QKNorm归一化方案推广，允许使用非欧几里得范数来稳定Transformer学习


<details>
  <summary>Details</summary>
Motivation: Transformer架构中查询和键向量的归一化对学习稳定性至关重要，现有归一化方法有限，需要更灵活的归一化方案

Method: 提出QKNorm归一化方案的推广，基于Lp范数，允许使用非欧几里得范数进行归一化

Result: 实验结果表明该方法适用于简单问题，证明了方法的可行性

Conclusion: 基于Lp范数的QKNorm推广为Transformer提供了更灵活的归一化方案，在简单问题上表现出适用性

Abstract: The normalization of query and key vectors is an essential part of the Transformer architecture. It ensures that learning is stable regardless of the scale of these vectors. Some normalization approaches are available. In this preliminary work, a generalization of the QKNorm normalization scheme is proposed. The approach is based on the Lp norm, allowing non-Euclidean norms to be employed. Experimental results demonstrate the suitability of the method for a simple problem.

</details>


### [225] [Optimism Stabilizes Thompson Sampling for Adaptive Inference](https://arxiv.org/abs/2602.06014)
*Shunxing Yan,Han Zhong*

Main category: cs.LG

TL;DR: 本文研究了Thompson采样在随机多臂老虎机中的推断性质，发现乐观机制可以稳定采样过程，从而支持有效的渐近推断。


<details>
  <summary>Details</summary>
Motivation: Thompson采样在随机多臂老虎机中广泛使用，但在自适应数据收集下的推断性质很微妙。经典渐近理论可能失效，因为臂特定的样本量是随机的，并且通过动作选择规则与奖励耦合。需要研究如何在这种自适应设置下实现有效的统计推断。

Method: 研究K臂高斯老虎机，识别"乐观"作为恢复"稳定性"的关键机制。稳定性是有效渐近推断的充分条件，要求每个臂的拉动次数集中在确定性尺度周围。分析了两种乐观修改方法：1）方差膨胀Thompson采样；2）保持后验方差不变但给后验均值添加显式奖励的乐观修改。

Result: 1）证明了方差膨胀Thompson采样对任意K≥2都是稳定的，包括多个臂都是最优的挑战性情况，解决了halder2025stable提出的开放性问题；2）分析了另一种乐观修改方法，并建立了相同的稳定性结论；3）适当地实现乐观机制只会带来轻微的额外遗憾成本。

Conclusion: 适当地实现乐观机制可以稳定Thompson采样，使得在多臂老虎机中能够进行渐近有效的推断，同时只产生轻微的额外遗憾成本。这为在自适应数据收集环境下进行统计推断提供了理论保证。

Abstract: Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.

</details>


### [226] [Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers](https://arxiv.org/abs/2602.05813)
*Artem Riabinin,Andrey Veprikov,Arman Bolatov,Martin Takáč,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 本文研究适用于范数约束优化器（如Muon和Lion）的自适应学习率调度，提出基于次优性间隙的广义平滑性假设，并开发出能自动调整预热时长的实用调度器，在LLaMA架构的大语言模型预训练中表现优于手动调优。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调度（特别是预热阶段）通常依赖启发式设置，缺乏理论依据。对于范数约束优化器，需要更理论指导的自适应调度方法来提升训练效率和性能。

Method: 1. 提出广义平滑性假设：局部曲率随次优性间隙减小；2. 在该假设下建立收敛保证，推导出自然产生"预热+衰减"模式的学习率选择；3. 基于理论开发实用调度器，仅需标准超参数即可自动调整预热时长。

Result: 在LLaMA架构的大语言模型预训练中，自适应预热调度在所有实验设置下都一致优于或至少匹配最佳手动调优的预热方案，且无需额外超参数搜索。

Conclusion: 本文为范数约束优化器提供了理论指导的自适应学习率调度框架，证明了广义平滑性假设的有效性，并开发出实用的自适应预热调度器，显著减少了手动调优需求。

Abstract: We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.
  Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup

</details>


### [227] [Private PoEtry: Private In-Context Learning via Product of Experts](https://arxiv.org/abs/2602.05012)
*Rob Romijnders,Mohammad Mahdi Derakhshani,Jonathan Petit,Max Welling,Christos Louizos,Yuki M. Asano*

Main category: cs.LG

TL;DR: 论文提出了一种基于专家乘积模型的理论框架，用于保护上下文学习中的隐私，相比现有方法在多个数据集上平均提升30%以上准确率，同时保持强隐私保证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的上下文学习可能包含隐私敏感信息，现有差分隐私方法要么计算成本高，要么基于启发式方法效果有限，需要更有效且理论严谨的隐私保护方案。

Method: 通过专家乘积模型重新构建私有上下文学习框架，提供理论基础的算法设计，并实现算法的简单并行化。

Result: 在文本分类、数学和视觉语言五个数据集上评估，相比现有DP-ICL方法平均准确率提升超过30个百分点，同时保持强隐私保证。

Conclusion: 提出的专家乘积模型框架为私有上下文学习提供了理论基础，显著提升了准确率，同时实现了算法的并行化，是更有效的隐私保护解决方案。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks with only a small set of examples at inference time, thereby avoiding task-specific fine-tuning. However, in-context examples may contain privacy-sensitive information that should not be revealed through model outputs. Existing differential privacy (DP) approaches to ICL are either computationally expensive or rely on heuristics with limited effectiveness, including context oversampling, synthetic data generation, or unnecessary thresholding. We reformulate private ICL through the lens of a Product-of-Experts model. This gives a theoretically grounded framework, and the algorithm can be trivially parallelized. We evaluate our method across five datasets in text classification, math, and vision-language. We find that our method improves accuracy by more than 30 percentage points on average compared to prior DP-ICL methods, while maintaining strong privacy guarantees.

</details>


### [228] [A Simple Reduction Scheme for Constrained Contextual Bandits with Adversarial Contexts via Regression](https://arxiv.org/abs/2602.05019)
*Dhruv Sarkar,Abhishek Sinha*

Main category: cs.LG

TL;DR: 提出一种基于SquareCB的模块化算法方案，利用在线回归预言机将约束上下文赌博机问题简化为无约束问题，在对抗性上下文设置下实现改进的遗憾和约束违反保证。


<details>
  <summary>Details</summary>
Motivation: 现有约束上下文赌博机研究主要关注随机上下文，本文旨在处理更一般的对抗性上下文设置，同时控制遗憾和累积约束违反，特别是在预算耗尽后仍继续运行的持续设置中。

Method: 基于Foster等人的SquareCB框架，提出模块化算法方案，利用在线回归预言机将约束问题简化为标准无约束上下文赌博机问题，通过自适应定义代理奖励函数来实现。

Result: 在对抗性上下文设置下获得了改进的保证，提供了紧凑透明的分析，相比之前主要关注随机上下文的研究有优势。

Conclusion: 提出的方法为约束上下文赌博机问题提供了一种简单模块化的解决方案，特别适用于对抗性上下文设置，通过在线回归预言机实现了有效的简化。

Abstract: We study constrained contextual bandits (CCB) with adversarially chosen contexts, where each action yields a random reward and incurs a random cost. We adopt the standard realizability assumption: conditioned on the observed context, rewards and costs are drawn independently from fixed distributions whose expectations belong to known function classes. We consider the continuing setting, in which the algorithm operates over the entire horizon even after the budget is exhausted. In this setting, the objective is to simultaneously control regret and cumulative constraint violation. Building on the seminal SquareCB framework of Foster et al. (2018), we propose a simple and modular algorithmic scheme that leverages online regression oracles to reduce the constrained problem to a standard unconstrained contextual bandit problem with adaptively defined surrogate reward functions. In contrast to most prior work on CCB, which focuses on stochastic contexts, our reduction yields improved guarantees for the more general adversarial context setting, together with a compact and transparent analysis.

</details>


### [229] [Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting](https://arxiv.org/abs/2602.05887)
*Tianqi Shen,Jinji Yang,Junze He,Kunhan Gao,Ziye Ma*

Main category: cs.LG

TL;DR: 提出一种模拟过参数化逃逸方向的方法（SOD），用于低秩矩阵感知问题，能够确定性地逃离虚假局部极小值而不需要实际进行张量提升。


<details>
  <summary>Details</summary>
Motivation: 低秩矩阵感知等非凸优化问题存在大量虚假局部极小值，使得梯度优化器难以收敛到全局最优。虽然过参数化（张量提升）可以将局部极小值转化为严格鞍点，但实际计算不可行。

Method: 提出模拟过参数化逃逸方向（SOD）机制：设计数学框架模拟过参数化空间的景观和逃逸方向，将过参数化的逃逸方向投影到原始参数空间，确保从现有局部极小值严格降低目标值。

Result: 数值实验表明，该框架能可靠地逃离局部极小值并促进收敛到全局最优，相比显式张量过参数化计算成本极低。

Conclusion: 这是第一个能保证逃离虚假局部极小值的确定性框架，无需随机扰动或启发式估计。该框架对超越矩阵感知的非凸优化具有重要启示，展示了如何利用模拟过参数化来驯服挑战性的优化景观。

Abstract: Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.

</details>


### [230] [Laws of Learning Dynamics and the Core of Learners](https://arxiv.org/abs/2602.05026)
*Inkee Jung,Siu Cheong Lau*

Main category: cs.LG

TL;DR: 提出基于熵的终身集成学习方法，通过免疫机制防御对抗攻击，在CIFAR-10上优于简单平均集成


<details>
  <summary>Details</summary>
Motivation: 建立学习动力学的基本定律（守恒定律和总熵减少），并在此基础上开发能有效防御对抗攻击的集成学习方法

Method: 基于熵的终身集成学习，构建免疫机制防御转移式对抗攻击，形成logifold结构

Result: 在CIFAR-10数据集上，相比简单平均干净样本和对抗样本的朴素集成，logifold在大多数测试案例中达到更高准确率，在强扰动下提升尤其显著

Conclusion: 基于学习动力学基本定律的熵驱动终身集成学习方法能有效提升模型对对抗攻击的鲁棒性

Abstract: We formulate the fundamental laws governing learning dynamics, namely the conservation law and the decrease of total entropy. Within this framework, we introduce an entropy-based lifelong ensemble learning method. We evaluate its effectiveness by constructing an immunization mechanism to defend against transfer-based adversarial attacks on the CIFAR-10 dataset. Compared with a naive ensemble formed by simply averaging models specialized on clean and adversarial samples, the resulting logifold achieves higher accuracy in most test cases, with particularly large gains under strong perturbations.

</details>


### [231] [Laplacian Representations for Decision-Time Planning](https://arxiv.org/abs/2602.05031)
*Dikshant Shehmar,Matthew Schlegel,Matthew E. Taylor,Marlos C. Machado*

Main category: cs.LG

TL;DR: ALPS是一种基于拉普拉斯表示的层次规划算法，在离线目标条件RL任务中优于常用基线方法


<details>
  <summary>Details</summary>
Motivation: 基于学习模型的规划仍然是基于模型的强化学习中的关键挑战。在决策时规划中，状态表示必须支持局部成本计算同时保持长时程结构，因此需要有效的潜在空间表示。

Method: 提出拉普拉斯表示作为规划的有效潜在空间，该表示能捕捉多时间尺度的状态空间距离。在此基础上引入ALPS（分层规划算法），将长时程问题分解为子目标，减轻长预测时程中的复合误差。

Result: 在OGBench基准测试的离线目标条件RL任务中，ALPS算法优于常用基线方法，这是之前由无模型方法主导的基准。

Conclusion: 拉普拉斯表示为规划提供了有效的潜在空间，能够捕捉状态空间距离并自然分解长时程问题。ALPS算法在离线目标条件RL任务中表现出色，展示了基于模型方法的潜力。

Abstract: Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.

</details>


### [232] [Causal Representation Meets Stochastic Modeling under Generic Geometry](https://arxiv.org/abs/2602.05033)
*Jiaxu Ren,Yixin Wang,Biwei Huang*

Main category: cs.LG

TL;DR: 提出MUTATE框架，用于从连续时间随机点过程中学习可识别的因果表示，解决了现有方法局限于i.i.d.或离散时间过程的限制。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中学习有意义的因果表示对于机器学习应用和科学发现至关重要，但现有可识别性方法主要针对i.i.d.或离散时间过程，而许多现实场景（如基因组学、神经科学）需要处理连续时间随机过程。

Method: 开发MUTATE框架：1）通过分析参数空间几何来研究连续时间潜在随机点过程的因果表示可识别性；2）构建可识别的变分自编码器框架，包含时间自适应转换模块来推断随机动力学。

Result: 在模拟和实证研究中，MUTATE能有效回答科学问题，如基因组学中的突变积累机制和神经科学中神经元放电触发的时间动态机制。

Conclusion: MUTATE框架成功解决了连续时间随机点过程的因果表示学习可识别性问题，为基因组学、神经科学等领域的科学发现提供了有效工具。

Abstract: Learning meaningful causal representations from observations has emerged as a crucial task for facilitating machine learning applications and driving scientific discoveries in fields such as climate science, biology, and physics. This process involves disentangling high-level latent variables and their causal relationships from low-level observations. Previous work in this area that achieves identifiability typically focuses on cases where the observations are either i.i.d. or follow a latent discrete-time process. Nevertheless, many real-world settings require identifying latent variables that are continuous-time stochastic processes (e.g., multivariate point processes). To this end, we develop identifiable causal representation learning for continuous-time latent stochastic point processes. We study its identifiability by analyzing the geometry of the parameter space. Furthermore, we develop MUTATE, an identifiable variational autoencoder framework with a time-adaptive transition module to infer stochastic dynamics. Across simulated and empirical studies, we find that MUTATE can effectively answer scientific questions, such as the accumulation of mutations in genomics and the mechanisms driving neuron spike triggers in response to time-varying dynamics.

</details>


### [233] [Feedback Control for Multi-Objective Graph Self-Supervision](https://arxiv.org/abs/2602.05036)
*Karish Grover,Theodore Vasiloudis,Han Xie,Sixing Lu,Xiang Song,Christos Faloutsos*

Main category: cs.LG

TL;DR: ControlG：基于控制理论的多任务图自监督学习框架，通过时间分配而非权重混合协调不同目标，解决目标冲突问题


<details>
  <summary>Details</summary>
Motivation: 当前图自监督学习（SSL）存在多种预训练目标（互信息、重构、对比学习等），但组合使用时会出现目标干扰和训练不稳定问题。传统方法采用每次更新混合权重的方式，导致目标冲突、效用漂移和资源匮乏三种失败模式。

Method: 提出ControlG框架，将多目标图SSL重新定义为反馈控制的时间分配问题。框架包含三个核心组件：1）估计每个目标的难度和成对对抗性；2）通过Pareto感知的对数超体积规划器规划目标预算；3）使用PID控制器进行调度。

Result: 在9个数据集上，ControlG持续优于最先进的基线方法，同时生成可审计的调度方案，揭示哪些目标驱动了学习过程。

Conclusion: 多目标图自监督学习的协调本质上是一个时间分配问题，而非简单的权重混合。ControlG通过控制理论方法有效解决了目标冲突问题，为多任务学习提供了新的协调范式。

Abstract: Can multi-task self-supervised learning on graphs be coordinated without the usual tug-of-war between objectives? Graph self-supervised learning (SSL) offers a growing toolbox of pretext objectives: mutual information, reconstruction, contrastive learning; yet combining them reliably remains a challenge due to objective interference and training instability. Most multi-pretext pipelines use per-update mixing, forcing every parameter update to be a compromise, leading to three failure modes: Disagreement (conflict-induced negative transfer), Drift (nonstationary objective utility), and Drought (hidden starvation of underserved objectives). We argue that coordination is fundamentally a temporal allocation problem: deciding when each objective receives optimization budget, not merely how to weigh them. We introduce ControlG, a control-theoretic framework that recasts multi-objective graph SSL as feedback-controlled temporal allocation by estimating per-objective difficulty and pairwise antagonism, planning target budgets via a Pareto-aware log-hypervolume planner, and scheduling with a Proportional-Integral-Derivative (PID) controller. Across 9 datasets, ControlG consistently outperforms state-of-the-art baselines, while producing an auditable schedule that reveals which objectives drove learning.

</details>


### [234] [ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation](https://arxiv.org/abs/2602.05051)
*Songyuan Zhang,Oswin So,H. M. Sabbir Ahmad,Eric Yang Yu,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: ReFORM是一种基于流策略的离线强化学习方法，通过构造强制执行支持约束，避免分布外错误，同时保持策略表达能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中存在两个主要挑战：1) 分布外错误问题，传统方法通过惩罚统计距离项来约束策略接近行为策略，但这限制了策略改进且不能完全防止OOD动作；2) 最优策略分布可能是多模态且难以表示的。现有方法如扩散或流策略解决了表示问题，但如何在避免OOD错误的同时保持策略表达能力尚不明确。

Method: ReFORM提出基于流策略的离线RL方法，通过构造强制执行支持约束。首先学习一个有界源分布的行为克隆流策略来捕获动作分布的支持集，然后优化一个反射流，为BC流生成有界噪声同时保持支持集，以最大化性能。

Result: 在OGBench基准的40个具有不同质量数据集的挑战性任务中，使用对所有任务恒定的超参数集，ReFORM在性能曲线图上优于所有需要手动调优超参数的基线方法。

Conclusion: ReFORM通过流策略构造性地强制执行支持约束，有效解决了离线RL中的OOD错误问题，同时保持了策略的表达能力，在多个挑战性任务中表现出优越性能。

Abstract: Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.

</details>


### [235] [Learning, Solving and Optimizing PDEs with TensorGalerkin: an efficient high-performance Galerkin assembly algorithm](https://arxiv.org/abs/2602.05052)
*Shizheng Wen,Mingyuan Chi,Tianwei Yu,Ben Moseley,Mike Yan Michelis,Pu Ren,Hao Sun,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出统一的算法框架，用于变分结构PDE的数值求解、约束优化和物理信息学习，基于TensorGalerkin的高效GPU兼容张量化组装技术。


<details>
  <summary>Details</summary>
Motivation: 现有PDE求解、优化和物理信息学习方法缺乏统一高效的框架，特别是在处理变分结构PDE时，需要能够同时支持数值求解、约束优化和算子学习的高性能计算方案。

Method: 基于Galerkin变分形式离散化，提出TensorGalerkin框架：通过Python级Map阶段张量化单元操作，利用稀疏矩阵乘法在网格诱导稀疏图上进行全局归约，实现高效的线性系统组装。

Result: 在2D/3D椭圆、抛物和双曲PDE的非结构化网格基准测试中，相比多种基线方法，该框架在所有目标下游应用中均展现出显著的计算效率和精度优势。

Conclusion: 该统一框架为变分结构PDE的数值求解、约束优化和物理信息学习提供了高效、准确且GPU兼容的解决方案，具有广泛的应用前景。

Abstract: We present a unified algorithmic framework for the numerical solution, constrained optimization, and physics-informed learning of PDEs with a variational structure. Our framework is based on a Galerkin discretization of the underlying variational forms, and its high efficiency stems from a novel highly-optimized and GPU-compliant TensorGalerkin framework for linear system assembly (stiffness matrices and load vectors). TensorGalerkin operates by tensorizing element-wise operations within a Python-level Map stage and then performs global reduction with a sparse matrix multiplication that performs message passing on the mesh-induced sparsity graph. It can be seamlessly employed downstream as i) a highly-efficient numerical PDEs solver, ii) an end-to-end differentiable framework for PDE-constrained optimization, and iii) a physics-informed operator learning algorithm for PDEs. With multiple benchmarks, including 2D and 3D elliptic, parabolic, and hyperbolic PDEs on unstructured meshes, we demonstrate that the proposed framework provides significant computational efficiency and accuracy gains over a variety of baselines in all the targeted downstream applications.

</details>


### [236] [Quantile-Physics Hybrid Framework for Safe-Speed Recommendation under Diverse Weather Conditions Leveraging Connected Vehicle and Road Weather Information Systems Data](https://arxiv.org/abs/2602.05053)
*Wen Zhang,Adel W. Sadek,Chunming Qiao*

Main category: cs.LG

TL;DR: 提出混合预测框架，结合机器学习与物理约束，为高速公路在不同天气条件下推荐实时安全速度区间


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件严重影响驾驶员能见度和轮胎-路面摩擦力，需要调整安全驾驶速度以降低事故风险

Method: 使用分位数回归森林预测车辆速度分布，结合物理约束计算基于实时路面附着力和能见度的安全速度上限，融合预测分位数、法定限速和物理上限生成最终推荐区间

Result: QRF模型平均绝对误差1.55 mph，96.43%的中位数速度预测误差在5 mph内，PICP(50%)为48.55%，在不同天气类型和路段上表现出良好泛化能力

Conclusion: 该框架能有效响应天气变化并在不同路段泛化，有望实际部署以提升交通安全并减少天气相关事故

Abstract: Inclement weather conditions can significantly impact driver visibility and tire-road surface friction, requiring adjusted safe driving speeds to reduce crash risk. This study proposes a hybrid predictive framework that recommends real-time safe speed intervals for freeway travel under diverse weather conditions. Leveraging high-resolution Connected Vehicle (CV) data and Road Weather Information System (RWIS) data collected in Buffalo, NY, from 2022 to 2023, we construct a spatiotemporally aligned dataset containing over 6.6 million records across 73 days. The core model employs Quantile Regression Forests (QRF) to estimate vehicle speed distributions in 10-minute windows, using 26 input features that capture meteorological, pavement, and temporal conditions. To enforce safety constraints, a physics-based upper speed limit is computed for each interval based on real-time road grip and visibility, ensuring that vehicles can safely stop within their sight distance. The final recommended interval fuses QRF-predicted quantiles with both posted speed limits and the physics-derived upper bound. Experimental results demonstrate strong predictive performance: the QRF model achieves a mean absolute error of 1.55 mph, with 96.43% of median speed predictions within 5 mph, a PICP (50%) of 48.55%, and robust generalization across weather types. The model's ability to respond to changing weather conditions and generalize across road segments shows promise for real-world deployment, thereby improving traffic safety and reducing weather-related crashes.

</details>


### [237] [StagePilot: A Deep Reinforcement Learning Agent for Stage-Controlled Cybergrooming Simulation](https://arxiv.org/abs/2602.05060)
*Heajun An,Qi Zhang,Minqian Liu,Xinyi Zhang,Sang Won Lee,Lifu Huang,Pamela J. Wisniewski,Jin-Hee Cho*

Main category: cs.LG

TL;DR: StagePilot是一个基于离线强化学习的对话代理，模拟网络诱骗的阶段进展，用于预防培训。它使用复合奖励平衡用户情感和目标接近度，约束阶段间转换以保持真实性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 网络诱骗是对青少年的持续威胁，需要主动的教育干预措施。现有方法缺乏对诱骗行为阶段进展的模拟，无法提供有效的预防培训。

Method: 提出StagePilot离线RL对话代理，使用复合奖励函数（平衡用户情感和目标接近度）选择对话阶段，约束阶段间只能向相邻阶段转换以保持真实性和可解释性。通过LLM模拟进行评估。

Result: StagePilot生成与诱骗动态一致的现实连贯对话。在测试方法中，IQL+AWAC代理在战略规划和情感连贯性之间达到最佳平衡，比基线方法更频繁地达到最终阶段（高达43%），同时保持超过70%的情感对齐。

Conclusion: StagePilot是一个有效的网络诱骗预防培训工具，能够模拟真实的诱骗行为进展，为青少年提供有效的预防教育干预。

Abstract: Cybergrooming is an evolving threat to youth, necessitating proactive educational interventions. We propose StagePilot, an offline RL-based dialogue agent that simulates the stage-wise progression of grooming behaviors for prevention training. StagePilot selects conversational stages using a composite reward that balances user sentiment and goal proximity, with transitions constrained to adjacent stages for realism and interpretability. We evaluate StagePilot through LLM-based simulations, measuring stage completion, dialogue efficiency, and emotional engagement. Results show that StagePilot generates realistic and coherent conversations aligned with grooming dynamics. Among tested methods, the IQL+AWAC agent achieves the best balance between strategic planning and emotional coherence, reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.

</details>


### [238] [E-Globe: Scalable $ε$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching](https://arxiv.org/abs/2602.05068)
*Wenting Li,Saif R. Kazi,Russell Bent,Duo Zhou,Huan Zhang*

Main category: cs.LG

TL;DR: 提出了一种在分支定界框架中的混合验证器，通过精确的非线性互补约束规划来紧化上下界，实现高效的可验证性验证


<details>
  <summary>Details</summary>
Motivation: 神经网络在安全关键应用中部署受到鲁棒性担忧的阻碍，现有形式化验证方法面临可扩展性与完备性的权衡问题

Method: 在分支定界框架中提出混合验证器，使用精确的非线性互补约束规划进行上界估计，保留ReLU输入输出图；结合预热启动NLP求解和模式对齐强分支策略加速验证

Result: 在MNIST和CIFAR-10上，相比PGD方法在扰动半径跨越三个数量级时获得更紧的上界，实际节点求解速度快，相比基于MIP的验证实现显著端到端加速

Conclusion: 提出的混合验证方法通过精确的上界估计和高效的分支策略，在保持完备性的同时显著提升了神经网络形式化验证的可扩展性

Abstract: Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $ε-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.

</details>


### [239] [Individual Fairness In Strategic Classification](https://arxiv.org/abs/2602.05084)
*Zhiqun Zuo,Mohammad Mahdi Khalili*

Main category: cs.LG

TL;DR: 论文研究战略分类中的个体公平性，证明确定性阈值分类器违反个体公平，提出使用随机分类器实现个体公平的线性规划方法，并能扩展到群体公平。


<details>
  <summary>Details</summary>
Motivation: 战略分类中个体修改特征以影响ML决策，现有研究多关注群体公平，个体公平研究不足。需要探索如何在战略分类中实现个体公平。

Method: 分析阈值分类器，证明确定性阈值违反个体公平；提出随机分类器实现个体公平的条件；通过线性规划寻找最优且个体公平的随机分类器；方法可扩展到群体公平概念。

Result: 在真实数据集上的实验证实，该方法能有效减轻不公平性，改善公平性与准确性的权衡。

Conclusion: 随机分类器是实现战略分类中个体公平的有效方法，提出的线性规划框架能同时优化公平性和准确性，并可扩展到群体公平问题。

Abstract: Strategic classification, where individuals modify their features to influence machine learning (ML) decisions, presents critical fairness challenges. While group fairness in this setting has been widely studied, individual fairness remains underexplored. We analyze threshold-based classifiers and prove that deterministic thresholds violate individual fairness. Then, we investigate the possibility of using a randomized classifier to achieve individual fairness. We introduce conditions under which a randomized classifier ensures individual fairness and leverage these conditions to find an optimal and individually fair randomized classifier through a linear programming problem. Additionally, we demonstrate that our approach can be extended to group fairness notions. Experiments on real-world datasets confirm that our method effectively mitigates unfairness and improves the fairness-accuracy trade-off.

</details>


### [240] [Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling](https://arxiv.org/abs/2602.05087)
*Parsa Vares*

Main category: cs.LG

TL;DR: AutoDiscover框架将主动学习重构为在线决策问题，通过自适应代理在异构文献图上动态管理查询策略组合，提高系统文献综述的筛选效率。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述的手动筛选已成为研究瓶颈，面临相关研究低流行率和专家决策稀缺昂贵的问题。传统主动学习方法使用固定查询策略，无法适应时间变化且忽略了文献网络的关系结构。

Method: 1) 将文献建模为包含文档、作者和元数据的异构图；2) 使用异构图注意力网络学习节点表示；3) 采用折扣汤普森采样代理动态管理查询策略组合；4) 结合实时人机交互标签，在非平稳评审动态中平衡探索与利用。

Result: 在26个数据集的SYNERGY基准测试中，AutoDiscover比静态主动学习基线获得更高的筛选效率。该框架通过最小初始标签启动发现，解决了冷启动问题，而静态方法在此情况下会失败。

Conclusion: AutoDiscover框架结合TS-Insight可视化分析仪表板，能够在专家标签稀缺和相关研究低流行率的情况下加速系统文献综述的筛选过程，为自适应文献发现提供了有效解决方案。

Abstract: Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent's decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.

</details>


### [241] [Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks](https://arxiv.org/abs/2602.05125)
*William F. Shen,Xinchi Qiu,Chenxi Whitehouse,Lisa Alazraki,Shashwat Goel,Francesco Barbieri,Timon Willi,Akhil Mathur,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 提出RRD框架，通过递归分解-过滤循环优化LLM评估准则，提升评估准确性和强化学习训练效果


<details>
  <summary>Details</summary>
Motivation: 现有评估准则存在覆盖不全、维度混淆、偏好方向错位、冗余和高度相关标准等问题，导致评估准确性下降和强化学习训练效果不佳

Method: RRD框架采用递归分解-过滤循环：分解粗粒度准则为细粒度判别标准，过滤错位和冗余准则，使用相关性感知加权方案避免高度相关标准的过度代表

Result: 在JudgeBench和PPE上显著提升GPT-4o和Llama3.1-405B的评估准确性（最高+17.7分）；在WildChat的强化学习中，奖励提升达160%（Qwen3-4B）和60%（Llama3.1-8B），优于基线10-20%的改进

Conclusion: RRD建立了可扩展且可解释的递归准则优化框架，为开放领域LLM评估和奖励建模提供了坚实基础

Abstract: Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.

</details>


### [242] [SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines](https://arxiv.org/abs/2602.05134)
*Olga Ovcharenko,Matthias Boehm,Sebastian Schelter*

Main category: cs.LG

TL;DR: SemPipes：一种新颖的声明式编程模型，将LLM驱动的语义数据操作符集成到表格ML管道中，通过代码合成自动优化数据操作


<details>
  <summary>Details</summary>
Motivation: 现实世界中的表格机器学习需要复杂的数据准备管道，设计这些管道需要大量领域专业知识和工程努力，因此需要探索LLM如何通过代码合成来支持表格ML

Method: 引入SemPipes声明式编程模型，集成LLM驱动的语义数据操作符；语义操作符用自然语言指定数据转换，运行时系统执行；训练时基于数据特征、操作指令和管道上下文合成自定义操作符实现；通过基于进化搜索的LLM代码合成自动优化管道中的数据操作

Result: 在多样化的表格ML任务中评估SemPipes，显示语义操作符显著提高了专家设计和代理生成管道的端到端预测性能，同时降低了管道复杂性

Conclusion: SemPipes通过LLM驱动的语义操作符和代码合成，有效简化了表格ML管道设计，提高了预测性能并降低了复杂性，为自动化数据准备提供了新方法

Abstract: Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.

</details>


### [243] [Decoupled Orthogonal Dynamics: Regularization for Deep Network Optimizers](https://arxiv.org/abs/2602.05136)
*Hao Chen,Jinghui Yuan,Hanmin Zhang*

Main category: cs.LG

TL;DR: AdamW的权重衰减并非最优，存在径向拉锯战问题。作者提出正交动力学解耦方法AdamO，将范数控制与自适应梯度缩放分离，在多个任务上优于AdamW。


<details>
  <summary>Details</summary>
Motivation: AdamW虽然将权重衰减与自适应梯度缩放解耦，但仍存在根本冲突：径向拉锯战。深度学习中的梯度倾向于增加参数范数以扩展有效容量，而权重衰减则不加区分地抑制范数增长，这种推拉相互作用导致径向振荡，向Adam的二阶矩估计注入噪声，可能损害精细的切向特征学习。

Method: 提出正交动力学解耦方法，并实例化为AdamO：使用SGD风格的更新处理一维范数控制，而Adam的自适应预调节则限制在切向子空间。AdamO进一步结合曲率自适应径向步长调整，以及针对尺度不变层和低维参数的架构感知规则和投影。

Result: 在视觉和语言任务上的实验表明，AdamO相比AdamW提高了泛化能力和稳定性，且没有引入额外的复杂约束。

Conclusion: 参数的大小和方向在优化器动力学中扮演不同角色，应该被解耦。AdamO通过正交动力学解耦实现了这种分离，提供了比AdamW更优的优化性能。

Abstract: Is the standard weight decay in AdamW truly optimal? Although AdamW decouples weight decay from adaptive gradient scaling, a fundamental conflict remains: the Radial Tug-of-War. In deep learning, gradients tend to increase parameter norms to expand effective capacity while steering directions to learn features, whereas weight decay indiscriminately suppresses norm growth. This push--pull interaction induces radial oscillations, injecting noise into Adam's second-moment estimates and potentially degrading delicate tangential feature learning. We argue that magnitude and direction play distinct roles and should be decoupled in optimizer dynamics. We propose Orthogonal Dynamics Decoupling and instantiate it as AdamO: an SGD-style update handles the one-dimensional norm control, while Adam's adaptive preconditioning is confined to the tangential subspace. AdamO further incorporates curvature-adaptive radial step sizing and architecture-aware rules and projections for scale-invariant layers and low-dimensional parameters. Experiments on vision and language tasks show that AdamO improves generalization and stability over AdamW without introducing additional complex constraints.

</details>


### [244] [Adaptive Exploration for Latent-State Bandits](https://arxiv.org/abs/2602.05139)
*Jikai Jin,Kenneth Hung,Sanath Kumar Krishnamurthy,Baoyi Shi,Congshan Zhang*

Main category: cs.LG

TL;DR: 提出无需显式状态建模的bandit算法，利用滞后上下文特征和协调探测策略来应对未观测混杂因素，在非平稳奖励环境中优于经典方法


<details>
  <summary>Details</summary>
Motivation: 经典多臂老虎机算法在存在隐藏、时变状态的环境中往往失效，这些未观测混杂因素会导致奖励估计偏差和状态信息受限，影响最优动作选择

Method: 引入一族状态模型无关的bandit算法，利用滞后上下文特征和协调探测策略，隐式跟踪潜在状态并区分状态依赖的奖励模式，无需显式状态建模

Result: 在多种设置下的实证结果表明，该方法优于经典方法，能够有效学习最优策略，同时保持计算效率和适应非平稳奖励的鲁棒性

Conclusion: 提出的状态模型无关bandit算法能有效处理未观测混杂因素，为实际应用中的算法选择提供了实用建议，实现了计算效率与鲁棒适应的平衡

Abstract: The multi-armed bandit problem is a core framework for sequential decision-making under uncertainty, but classical algorithms often fail in environments with hidden, time-varying states that confound reward estimation and optimal action selection. We address key challenges arising from unobserved confounders, such as biased reward estimates and limited state information, by introducing a family of state-model-free bandit algorithms that leverage lagged contextual features and coordinated probing strategies. These implicitly track latent states and disambiguate state-dependent reward patterns. Our methods and their adaptive variants can learn optimal policies without explicit state modeling, combining computational efficiency with robust adaptation to non-stationary rewards. Empirical results across diverse settings demonstrate superior performance over classical approaches, and we provide practical recommendations for algorithm selection in real-world applications.

</details>


### [245] [Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing](https://arxiv.org/abs/2602.05144)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 论文研究了群体条件先验概率偏移(GPPS)下的公平性问题，证明了错误率公平准则具有不变性，而接受率准则会漂移，提出了无需目标域标签的估计方法和TAP-GPPS算法。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统通常在历史数据上训练和评估公平性，但部署环境可能发生变化。特别是当不同人口群体的正结果发生率变化不同时（如疾病率在不同人群中上升速度不同），公平性评估面临挑战。需要研究这种群体条件先验概率偏移对公平性的影响。

Method: 研究群体条件先验概率偏移(GPPS)，其中标签发生率P(Y=1|A=a)在训练和部署间变化，而特征生成过程P(X|Y,A)保持稳定。提出理论分析和TAP-GPPS算法，该算法利用无标签目标数据估计发生率，校正后验概率，并选择阈值以满足目标域的人口统计均等。

Result: 证明了基本二分法：基于错误率的公平准则（均等几率）在GPPS下具有结构不变性，而基于接受率的准则（人口统计均等）会漂移，且这种漂移对于非平凡分类器是不可避免的。展示了目标域风险和公平度量可以在没有目标标签的情况下识别，并验证了TAP-GPPS算法能在目标域实现公平性且效用损失最小。

Conclusion: 在群体条件先验概率偏移下，公平性准则表现出不同特性：错误率准则具有鲁棒性，而接受率准则需要调整。提出的TAP-GPPS算法能够有效应对这种偏移，在无需目标域标签的情况下实现目标域的公平性要求。

Abstract: Machine learning systems are often trained and evaluated for fairness on historical data, yet deployed in environments where conditions have shifted. A particularly common form of shift occurs when the prevalence of positive outcomes changes differently across demographic groups--for example, when disease rates rise faster in one population than another, or when economic conditions affect loan default rates unequally. We study group-conditional prior probability shift (GPPS), where the label prevalence $P(Y=1\mid A=a)$ may change between training and deployment while the feature-generation process $P(X\mid Y,A)$ remains stable. Our analysis yields three main contributions. First, we prove a fundamental dichotomy: fairness criteria based on error rates (equalized odds) are structurally invariant under GPPS, while acceptance-rate criteria (demographic parity) can drift--and we prove this drift is unavoidable for non-trivial classifiers (shift-robust impossibility). Second, we show that target-domain risk and fairness metrics are identifiable without target labels: the invariance of ROC quantities under GPPS enables consistent estimation from source labels and unlabeled target data alone, with finite-sample guarantees. Third, we propose TAP-GPPS, a label-free post-processing algorithm that estimates prevalences from unlabeled data, corrects posteriors, and selects thresholds to satisfy demographic parity in the target domain. Experiments validate our theoretical predictions and demonstrate that TAP-GPPS achieves target fairness with minimal utility loss.

</details>


### [246] [TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference](https://arxiv.org/abs/2602.05145)
*Jiyoung Park,Hankyu Jang,Changseok Song,Wookeun Jung*

Main category: cs.LG

TL;DR: TIDE是一个集成到LLM推理系统中的在线草稿适配框架，通过重用推理过程中的隐藏状态实现零开销草稿训练，自适应控制推测执行时机，利用异构集群提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 推测解码能显著加速LLM推理，但在实际应用中面临挑战：工作负载动态变化、系统级约束限制，需要更高效的在线适配方案。

Method: TIDE将在线草稿适配集成到高性能LLM推理系统中，重用目标模型推理生成的隐藏状态作为训练信号，实现零开销草稿训练；采用自适应运行时控制，仅在有益时激活推测和训练；利用异构集群将解耦的推理和训练映射到合适的GPU类别。

Result: 在多样化真实世界工作负载中，TIDE相比静态推测解码实现最高1.15倍吞吐量提升，相比重新计算训练信号的方法减少1.67倍草稿训练时间。

Conclusion: TIDE通过系统级集成、零开销训练信号重用和自适应控制，有效解决了推测解码在实际部署中的挑战，显著提升了LLM推理效率。

Abstract: Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptation directly into high-performance LLM inference systems. TIDE reuses target model hidden states generated during inference as training signals, enabling zero-overhead draft adaptation without reloading the target model, and employs adaptive runtime control to activate speculation and training only when beneficial. TIDE exploits heterogeneous clusters by mapping decoupled inference and training to appropriate GPU classes. Across diverse real-world workloads, TIDE achieves up to 1.15x throughput improvement over static speculative decoding while reducing draft training time by 1.67x compared to approaches that recompute training signals.

</details>


### [247] [Cross-talk based multi-task learning for fault classification of physically coupled machine system](https://arxiv.org/abs/2602.05146)
*Wonjun Yi,Rismaya Kumar Mishra,Yong-Hwa Park*

Main category: cs.LG

TL;DR: 该研究提出一种基于交叉对话结构的多任务学习框架，利用信号中故障条件与物理变量的耦合关系，通过联合学习提升故障分类性能。


<details>
  <summary>Details</summary>
Motivation: 机器系统产生的信号中，故障条件与各种物理变量存在物理耦合。现有故障分类研究大多仅依赖直接故障标签，忽略了信号中自然嵌入的其他物理耦合信息。利用这种耦合关系可以提升故障分类性能。

Method: 采用多任务学习框架，通过交叉对话结构联合学习故障条件和相关物理变量。基于先前提出的残差神经降维模型，将其扩展到两个物理耦合显著的基准数据集：无人机故障数据集和电机复合故障数据集。交叉对话结构允许任务间受控信息交换，同时防止负迁移。

Result: 在两个基准测试中，残差神经降维模型均优于单任务模型、合并所有标签组合的多类模型以及共享主干多任务模型。对于无人机故障，学习故障分类与物理属性（机器类型和操纵方向）能更好分类故障；对于电机复合故障，测试了单通道和多通道数据的分类性能。

Conclusion: 交叉对话多任务学习框架能有效利用信号中的物理耦合信息，提升故障分类性能。该方法在无人机故障和电机复合故障数据集上均表现出优越性，验证了利用物理变量耦合关系的有效性。

Abstract: Machine systems inherently generate signals in which fault conditions and various physical variables are physically coupled. Although many existing fault classification studies rely solely on direct fault labels, the aforementioned signals naturally embed additional information shaped by other physically coupled information. Herein, we leverage this coupling through a multi-task learning (MTL) framework that jointly learns fault conditions and the related physical variables. Among MTL architectures, crosstalk structures have distinct advantages because they allow for controlled information exchange between tasks through the cross-talk layer while preventing negative transfer, in contrast to shared trunk architectures that often mix incompatible features. We build on our previously introduced residual neural dimension reductor model, and extend its application to two benchmarks where physical coupling is prominent. The first benchmark is a drone fault dataset, in which machine type and maneuvering direction significantly alter the frequency components of measured signals even under the same nominal condition. By learning fault classification together with these physical attributes, the cross-talk architecture can better classify faults. The second benchmark dataset is the motor compound fault dataset. In this system, each fault component, inner race fault, outer race fault, misalignment, and unbalance is coupled to the other. For motor compound fault, we also test classification performance when we use single-channel data or multi-channel data as input to the classifier. Across both benchmarks, our residual neural dimension reductor, consistently outperformed single-task models, multi-class models that merge all label combinations, and shared trunk multi-task models.

</details>


### [248] [CoSA: Compressed Sensing-Based Adaptation of Large Language Models](https://arxiv.org/abs/2602.05148)
*Songtao Wei,Yi Li,Bohan Zhang,Zhichun Guo,Ying Huang,Yuede Ji,Miao Yin,Guanpeng Li,Bingzhe Li*

Main category: cs.LG

TL;DR: CoSA是一种基于压缩感知理论的新型参数高效微调方法，通过固定随机投影矩阵和可学习核心表达权重更新，克服了传统低秩方法的表达限制，在多种任务和模型规模上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA和PiSSA）依赖于权重更新的低秩分解，但低秩假设可能限制表达能力，特别是在任务特定适应场景中奇异值分布相对均匀时。

Method: 提出CoSA方法，基于压缩感知理论，通过固定随机投影矩阵和紧凑可学习核心表达权重更新，而不是将权重更新约束在低秩子空间中。

Result: 在10个多样化任务（包括自然语言理解和生成）和5个不同规模模型（RoBERTa、Llama、Qwen系列）上评估，CoSA始终匹配或优于最先进的PEFT方法。

Conclusion: CoSA为高效且表达力强的多尺度模型适应提供了原则性视角，通过压缩感知理论克服了传统低秩方法的局限性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.

</details>


### [249] [Position: Capability Control Should be a Separate Goal From Alignment](https://arxiv.org/abs/2602.05164)
*Shoaib Ahmed Siddiqui,Eleni Triantafillou,David Krueger,Adrian Weller*

Main category: cs.LG

TL;DR: 该立场论文主张将能力控制视为与对齐不同的独立目标，提出在模型全生命周期中通过数据层、学习层和系统层三层机制进行能力控制，并倡导采用纵深防御方法组合互补控制措施。


<details>
  <summary>Details</summary>
Motivation: 基础模型在广泛数据分布上训练，具备通用能力支持下游应用，但也扩大了潜在滥用和失败的空间。需要建立硬性操作限制来控制模型行为，特别是在对抗性诱导下。

Method: 提出三层能力控制机制：1) 数据层控制训练分布；2) 学习层通过权重或表示级干预；3) 系统层通过部署后对输入、输出和行为的护栏。倡导采用纵深防御方法组合这些控制层。

Result: 构建了能力控制的概念框架，明确了与对齐的区别，提出了三层控制机制的分类体系，并指出了实现有效控制的关键挑战。

Conclusion: 能力控制应作为独立于对齐的重要目标，需要采用多层次、互补的控制机制组合，但面临知识双重用途和组合泛化等开放挑战。

Abstract: Foundation models are trained on broad data distributions, yielding generalist capabilities that enable many downstream applications but also expand the space of potential misuse and failures. This position paper argues that capability control -- imposing restrictions on permissible model behavior -- should be treated as a distinct goal from alignment. While alignment is often context and preference-driven, capability control aims to impose hard operational limits on permissible behaviors, including under adversarial elicitation. We organize capability control mechanisms across the model lifecycle into three layers: (i) data-based control of the training distribution, (ii) learning-based control via weight- or representation-level interventions, and (iii) system-based control via post-deployment guardrails over inputs, outputs, and actions. Because each layer has characteristic failure modes when used in isolation, we advocate for a defense-in-depth approach that composes complementary controls across the full stack. We further outline key open challenges in achieving such control, including the dual-use nature of knowledge and compositional generalization.

</details>


### [250] [EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization](https://arxiv.org/abs/2602.05165)
*Kevin Han,Yuhang Zhou,Mingze Gao,Gedi Zhou,Serena Li,Abhishek Kumar,Xiangjun Fan,Weiwei Li,Lizhu Zhang*

Main category: cs.LG

TL;DR: EBPO通过经验贝叶斯方法改进RLVR，使用收缩估计器平衡局部组统计与全局先验，解决了GRPO的高方差和梯度消失问题，在多个基准测试中表现更优且训练更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法如GRPO存在稳定性问题：计算约束下估计器方差高，失败场景中梯度信号消失。需要更稳定的强化学习框架来提升LLM推理能力。

Method: 提出经验贝叶斯策略优化(EBPO)，使用收缩估计器动态平衡局部组统计与通过Welford在线算法更新的全局先验，正则化局部基线估计。

Result: EBPO在AIME和OlympiadBench等多个基准测试中一致优于GRPO和其他基线方法，训练稳定性更高，即使在小组规模下也能获得高性能增益，且受益于难度分层课程学习。

Conclusion: EBPO通过经验贝叶斯正则化有效解决了RLVR中的稳定性挑战，提供了更可靠、高效的强化学习框架，为提升LLM推理能力提供了新方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.

</details>


### [251] [Benchmarking Artificial Intelligence Models for Daily Coastal Hypoxia Forecasting](https://arxiv.org/abs/2602.05178)
*Magesh Rajasekaran,Md Saiful Sajol,Chris Alvin,Supratik Mukhopadhyay,Yanda Ou,Z. George Xue*

Main category: cs.LG

TL;DR: 本研究比较了四种深度学习架构（BiLSTM、Medformer、ST-Transformer、TCN）用于墨西哥湾北部缺氧事件的日尺度分类预测，其中ST-Transformer在所有评估指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 墨西哥湾北部海岸缺氧是一个持续的生态和经济问题。现有的季节性模型预测过于粗糙，无法捕捉日尺度变化，难以支持日常响应式生态系统管理。

Method: 使用2009-2020年的日尺度后报数据训练四种深度学习架构：双向LSTM、Medformer、时空Transformer和时序卷积网络。模型输入包括水柱分层、沉积物耗氧率和温度依赖分解率等特征。使用2020-2024年数据作为测试集，采用相同的数据预处理和验证协议。

Result: 所有模型都取得了高分类准确率和强判别能力，其中ST-Transformer在所有指标和测试期间表现最佳（AUC-ROC：0.982-0.992）。使用McNemar方法验证了模型预测差异的统计显著性。

Conclusion: 研究提供了一个可复现的实时缺氧预测操作框架，能够支持环境与海洋建模系统社区以及生态系统恢复力的更广泛努力。代码已开源。

Abstract: Coastal hypoxia, especially in the northern part of Gulf of Mexico, presents a persistent ecological and economic concern. Seasonal models offer coarse forecasts that miss the fine-scale variability needed for daily, responsive ecosystem management. We present study that compares four deep learning architectures for daily hypoxia classification: Bidirectional Long Short-Term Memory (BiLSTM), Medformer (Medical Transformer), Spatio-Temporal Transformer (ST-Transformer), and Temporal Convolutional Network (TCN). We trained our models with twelve years of daily hindcast data from 2009-2020 Our training data consists of 2009-2020 hindcast data from a coupled hydrodynamic-biogeochemical model. Similarly, we use hindcast data from 2020 through 2024 as a test data. We constructed classification models incorporating water column stratification, sediment oxygen consumption, and temperature-dependent decomposition rates. We evaluated each architectures using the same data preprocessing, input/output formulation, and validation protocols. Each model achieved high classification accuracy and strong discriminative ability with ST-Transformer achieving the highest performance across all metrics and tests periods (AUC-ROC: 0.982-0.992). We also employed McNemar's method to identify statistically significant differences in model predictions. Our contribution is a reproducible framework for operational real-time hypoxia prediction that can support broader efforts in the environmental and ocean modeling systems community and in ecosystem resilience. The source code is available https://github.com/rmagesh148/hypoxia-ai/

</details>


### [252] [Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.05183)
*John Yan,Michael Yu,Yuqi Sun,Alexander Duffy,Tyler Marques,Matthew Lyle Olson*

Main category: cs.LG

TL;DR: 该研究提出Meta-Autointerp方法，结合稀疏自编码器(SAEs)和LLM摘要技术分析复杂强化学习环境中LLM的训练动态，发现多种细粒度和高级行为模式，但发现大多数SAE特征和LLM生成的假设对人类理解帮助有限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在复杂强化学习和多智能体环境中训练，理解训练过程中的行为变化变得困难。稀疏自编码器在数据为中心的模型可解释性方面显示出潜力，但需要系统方法来分析训练动态。

Method: 提出Meta-Autointerp方法，将SAE特征分组为可解释的假设，结合LLM摘要技术分析Full-Press Diplomacy环境中的大规模强化学习训练过程。通过自动评估验证SAE元特征的有效性，并进行用户研究评估人类可理解性。

Result: 发现了细粒度行为（角色扮演模式、退化输出、语言切换）和高级战略行为及环境特定bug。90%的SAE元特征具有统计显著性，发现了意外的奖励黑客行为。用户研究发现大多数SAE特征和LLM假设对人类理解帮助有限，但部分SAE衍生假设对下游任务具有预测价值。通过增强未训练智能体的系统提示，得分提高了14.2%。

Conclusion: SAEs和LLM摘要技术为智能体行为提供了互补视角，Meta-Autointerp框架为未来数据为中心的可解释性研究提供了实用起点，有助于确保LLM在训练过程中的可信行为。

Abstract: Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent's system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.

</details>


### [253] [SpectraKAN: Conditioning Spectral Operators](https://arxiv.org/abs/2602.05187)
*Chun-Wun Cheng,Carola-Bibiane Schönlieb,Angelica I. Aviles-Rivero*

Main category: cs.LG

TL;DR: SpectraKAN：一种输入自适应的谱神经算子，通过全局表示调制多尺度傅里叶主干，显著提升PDE求解性能


<details>
  <summary>Details</summary>
Motivation: 现有谱神经算子（如FNO）使用静态傅里叶核，无法适应多尺度、状态依赖和各向异性的动态系统，限制了其捕捉复杂PDE解算子的能力。

Method: 从时空历史中提取紧凑全局表示，通过单查询交叉注意力调制多尺度傅里叶主干，将静态谱卷积转变为输入自适应的积分算子。

Result: 在多样PDE基准测试中达到最先进性能，RMSE降低高达49%，在具有挑战性的时空预测任务上表现尤为突出。

Conclusion: SpectraKAN通过输入自适应调制机制，在保持谱混合效率的同时显著提升了谱神经算子的表达能力，为复杂PDE求解提供了更强大的框架。

Abstract: Spectral neural operators, particularly Fourier Neural Operators (FNO), are a powerful framework for learning solution operators of partial differential equations (PDEs) due to their efficient global mixing in the frequency domain. However, existing spectral operators rely on static Fourier kernels applied uniformly across inputs, limiting their ability to capture multi-scale, regime-dependent, and anisotropic dynamics governed by the global state of the system. We introduce SpectraKAN, a neural operator that conditions the spectral operator on the input itself, turning static spectral convolution into an input-conditioned integral operator. This is achieved by extracting a compact global representation from spatio-temporal history and using it to modulate a multi-scale Fourier trunk via single-query cross-attention, enabling the operator to adapt its behaviour while retaining the efficiency of spectral mixing. We provide theoretical justification showing that this modulation converges to a resolution-independent continuous operator under mesh refinement and KAN gives smooth, Lipschitz-controlled global modulation. Across diverse PDE benchmarks, SpectraKAN achieves state-of-the-art performance, reducing RMSE by up to 49% over strong baselines, with particularly large gains on challenging spatio-temporal prediction tasks.

</details>


### [254] [Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs](https://arxiv.org/abs/2602.05191)
*Wentao Ni,Kangqi Zhang,Zhongming Yu,Oren Nelson,Mingu Lee,Hong Cai,Fatih Porikli,Jongryool Kim,Zhijian Liu,Jishen Zhao*

Main category: cs.LG

TL;DR: Double-P是一个分层稀疏注意力框架，通过两级top-p选择优化长上下文推理，在保持精度的同时显著减少计算开销和提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文推理成为大语言模型的核心，注意力机制中不断增长的键值缓存成为解码瓶颈。现有的稀疏注意力方法无法同时优化精度、选择开销和稀疏注意力成本。

Method: 提出Double-P分层稀疏注意力框架：1）在聚类级别使用大小加权质心进行粗粒度top-p估计；2）自适应地通过第二级top-p阶段分配token级注意力计算。

Result: 在长上下文基准测试中，Double-P实现了接近零的精度下降，将注意力计算开销减少高达1.8倍，端到端解码速度比最先进的固定预算稀疏注意力方法提升高达1.3倍。

Conclusion: Double-P通过分层优化策略有效解决了稀疏注意力中的精度、选择开销和计算成本之间的权衡问题，为可扩展的长上下文推理提供了高效解决方案。

Abstract: As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.

</details>


### [255] [Extreme Weather Nowcasting via Local Precipitation Pattern Prediction](https://arxiv.org/abs/2602.05204)
*Changhoon Song,Teng Yuan Chang,Youngjoon Hong*

Main category: cs.LG

TL;DR: 提出exPreCast框架和平衡雷达数据集，实现高效精确的极端天气临近预报


<details>
  <summary>Details</summary>
Motivation: 极端天气准确预报对风险管理至关重要，现有方法存在计算成本高、偏向普通降雨、数据集不平衡等问题

Method: 提出exPreCast确定性框架，包含局部时空注意力、纹理保持立方双上采样解码器和时间提取器，并构建平衡雷达数据集

Result: 在SEVIR、MeteoNet和KMA数据集上实现最先进性能，在普通和极端降雨条件下都能提供准确可靠的临近预报

Conclusion: exPreCast框架和平衡数据集有效解决了极端天气临近预报的挑战，为实际应用提供了高效可靠的解决方案

Abstract: Accurate forecasting of extreme weather events such as heavy rainfall or storms is critical for risk management and disaster mitigation. Although high-resolution radar observations have spurred extensive research on nowcasting models, precipitation nowcasting remains particularly challenging due to pronounced spatial locality, intricate fine-scale rainfall structures, and variability in forecasting horizons. While recent diffusion-based generative ensembles show promising results, they are computationally expensive and unsuitable for real-time applications. In contrast, deterministic models are computationally efficient but remain biased toward normal rainfall. Furthermore, the benchmark datasets commonly used in prior studies are themselves skewed--either dominated by ordinary rainfall events or restricted to extreme rainfall episodes--thereby hindering general applicability in real-world settings. In this paper, we propose exPreCast, an efficient deterministic framework for generating finely detailed radar forecasts, and introduce a newly constructed balanced radar dataset from the Korea Meteorological Administration (KMA), which encompasses both ordinary precipitation and extreme events. Our model integrates local spatiotemporal attention, a texture-preserving cubic dual upsampling decoder, and a temporal extractor to flexibly adjust forecasting horizons. Experiments on established benchmarks (SEVIR and MeteoNet) as well as on the balanced KMA dataset demonstrate that our approach achieves state-of-the-art performance, delivering accurate and reliable nowcasts across both normal and extreme rainfall regimes.

</details>


### [256] [Disentangled Representation Learning via Flow Matching](https://arxiv.org/abs/2602.05214)
*Jinjin Chi,Taoping Liu,Mengtao Yin,Ximing Li,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出基于流匹配的分离表示学习框架，通过正交正则化抑制因子间干扰，提高语义对齐和可控性


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法虽然通过归纳偏置鼓励因子独立性，但经常缺乏强语义对齐，需要更好的分离表示学习方法

Method: 提出流匹配框架，将分离表示学习转化为学习因子条件流；引入非重叠（正交）正则化器抑制跨因子干扰和减少信息泄漏

Result: 在多个数据集上的实验显示，相比代表性基线方法，该方法在分离分数、可控性和样本保真度方面都有持续改进

Conclusion: 流匹配框架结合正交正则化能有效实现语义对齐的分离表示学习，提高生成模型的可控性和质量

Abstract: Disentangled representation learning aims to capture the underlying explanatory factors of observed data, enabling a principled understanding of the data-generating process. Recent advances in generative modeling have introduced new paradigms for learning such representations. However, existing diffusion-based methods encourage factor independence via inductive biases, yet frequently lack strong semantic alignment. In this work, we propose a flow matching-based framework for disentangled representation learning, which casts disentanglement as learning factor-conditioned flows in a compact latent space. To enforce explicit semantic alignment, we introduce a non-overlap (orthogonality) regularizer that suppresses cross-factor interference and reduces information leakage between factors. Extensive experiments across multiple datasets demonstrate consistent improvements over representative baselines, yielding higher disentanglement scores as well as improved controllability and sample fidelity.

</details>


### [257] [Private Prediction via Shrinkage](https://arxiv.org/abs/2602.05219)
*Chao Yan*

Main category: cs.LG

TL;DR: 该论文研究差分隐私预测问题，通过改进算法将查询数量T的依赖从√T降低到polylog(T)，在流式设置中显著减少了所需标记样本数量。


<details>
  <summary>Details</summary>
Motivation: Dwork和Feldman提出的差分隐私预测框架中，标准组合方法导致查询数量T的平方根依赖(√T)，这在实际应用中效率较低。论文旨在减少这种依赖，使差分隐私预测在流式查询场景中更加高效。

Method: 针对两种不同场景设计算法：1) 对于任意概念类C和遗忘型在线对手，使用基于VC维的算法；2) 对于自适应在线对手和ℝ^d中的半空间，设计专门的差分隐私预测器。两种方法都利用流式设置的特点来减少样本复杂度。

Result: 1) 对于任意概念类C，所需标记样本数|S| = Õ(VC(C)^{3.5}log^{3.5}T)；2) 对于ℝ^d中的半空间和自适应对手，|S| = Õ(d^{5.5}log T)。相比标准组合的√T依赖，实现了polylog(T)的改进。

Conclusion: 在流式差分隐私预测中，可以将查询数量T的依赖从平方根降低到多对数级别，显著提高了差分隐私预测的效率，为实际应用提供了更实用的算法框架。

Abstract: We study differentially private prediction introduced by Dwork and Feldman (COLT 2018): an algorithm receives one labeled sample set $S$ and then answers a stream of unlabeled queries while the output transcript remains $(\varepsilon,δ)$-differentially private with respect to $S$. Standard composition yields a $\sqrt{T}$ dependence for $T$ queries.
  We show that this dependence can be reduced to polylogarithmic in $T$ in streaming settings. For an oblivious online adversary and any concept class $\mathcal{C}$, we give a private predictor that answers $T$ queries with $|S|= \tilde{O}(VC(\mathcal{C})^{3.5}\log^{3.5}T)$ labeled examples. For an adaptive online adversary and halfspaces over $\mathbb{R}^d$, we obtain $|S|=\tilde{O}\left(d^{5.5}\log T\right)$.

</details>


### [258] [Balanced Anomaly-guided Ego-graph Diffusion Model for Inductive Graph Anomaly Detection](https://arxiv.org/abs/2602.05232)
*Chunyu Wei,Siyuan He,Yu Wang,Yueguo Chen,Yunhai Wang,Bing Bai,Yidong Zhang,Yong Xie,Shunming Zhang,Fei Wang*

Main category: cs.LG

TL;DR: 提出一个数据中心的动态图异常检测框架，通过离散自我图扩散模型和课程异常增强机制，解决静态归纳学习和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前图异常检测面临两大挑战：模型层面，多数方法采用静态的归纳学习范式，不适用于动态演化网络；数据层面，异常节点稀少导致的极端类别不平衡使模型产生偏差，难以泛化到未见异常。这两个挑战相互关联，静态框架限制了有效数据增强，而不平衡问题在归纳学习设置中加剧了模型失真。

Method: 提出一个数据中心的框架，整合动态图建模与平衡异常合成。包含两个核心组件：(1) 离散自我图扩散模型，捕捉异常局部拓扑结构，生成符合异常结构分布的自我图；(2) 课程异常增强机制，在训练过程中动态调整合成数据生成，重点关注代表性不足的异常模式，以提升检测和泛化能力。

Result: 在五个数据集上的实验证明了该框架的有效性。

Conclusion: 通过整合动态图建模与平衡异常合成，提出的数据中心框架成功解决了图异常检测中的静态归纳学习和类别不平衡问题，提升了检测性能和泛化能力。

Abstract: Graph anomaly detection (GAD) is crucial in applications like fraud detection and cybersecurity. Despite recent advancements using graph neural networks (GNNs), two major challenges persist. At the model level, most methods adopt a transductive learning paradigm, which assumes static graph structures, making them unsuitable for dynamic, evolving networks. At the data level, the extreme class imbalance, where anomalous nodes are rare, leads to biased models that fail to generalize to unseen anomalies. These challenges are interdependent: static transductive frameworks limit effective data augmentation, while imbalance exacerbates model distortion in inductive learning settings. To address these challenges, we propose a novel data-centric framework that integrates dynamic graph modeling with balanced anomaly synthesis. Our framework features: (1) a discrete ego-graph diffusion model, which captures the local topology of anomalies to generate ego-graphs aligned with anomalous structural distribution, and (2) a curriculum anomaly augmentation mechanism, which dynamically adjusts synthetic data generation during training, focusing on underrepresented anomaly patterns to improve detection and generalization. Experiments on five datasets demonstrate that the effectiveness of our framework.

</details>


### [259] [Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions](https://arxiv.org/abs/2602.05234)
*Yuntai Bao,Xuhong Zhang,Jintao Chen,Ge Su,Yuxiang Cai,Hao Peng,Bing Sun,Haiqin Weng,Liu Yan,Jianwei Yin*

Main category: cs.LG

TL;DR: CDAS是一种基于概念分布对齐的模型干预方法，通过分布式交换干预和分布匹配目标实现更忠实、稳定的模型控制，在安全相关任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前基于干预的模型控制方法容易过拟合且性能不佳，因为它们过度依赖外部偏好优化目标，而非忠实识别模型内部机制。需要一种更忠实于模型内部机制的控制方法。

Method: 基于分布式对齐搜索(DAS)原则，提出概念DAS(CDAS)：采用分布式交换干预(DII)核心机制，引入针对控制任务的分布匹配目标，通过弱监督分布对齐而非概率最大化学习干预，支持双向控制和从数据中推导控制因子。

Result: 在AxBench大规模模型控制基准上，CDAS并不总是优于偏好优化方法，但在模型规模增大时可能获益更多；在两个安全案例研究中，CDAS能系统性地控制安全对齐模型的拒绝行为并中和思维链后门，同时保持模型通用效用。

Conclusion: CDAS是偏好优化方法的补充，在特定条件下构成了基于干预的模型控制的稳健方法，提供了更忠实、稳定的控制机制。

Abstract: Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at https://github.com/colored-dye/concept_das.

</details>


### [260] [CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers](https://arxiv.org/abs/2602.05243)
*Boxiang Zhang,Baijian Yang*

Main category: cs.LG

TL;DR: CORP是一种无需标签、梯度或微调的单次结构化剪枝框架，通过闭式岭回归补偿剪枝后的表示误差，在严格后训练约束下高效压缩Vision Transformers。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers虽然精度高但计算和内存成本大，现有结构化剪枝方法需要重训练或多阶段优化，限制了后训练部署。需要一种在严格后训练约束下（仅使用少量未标记校准数据）的高效剪枝方法。

Method: 将结构化剪枝建模为表示恢复问题，将被移除的激活和注意力logits建模为保留组件的仿射函数，推导闭式岭回归解并将补偿折叠到模型权重中，最小化校准分布下的期望表示误差。

Result: 在ImageNet上使用DeiT模型验证，发现MLP和注意力表示存在强冗余性。无补偿时单次结构化剪枝导致严重精度下降，使用CORP可在激进稀疏度下保持精度。DeiT-Huge剪枝50% MLP和注意力结构后保持82.8% Top-1精度，单GPU 20分钟内完成剪枝。

Conclusion: CORP是一种高效的后训练结构化剪枝框架，无需标签、梯度或微调，通过闭式补偿方法在保持精度的同时显著提升Vision Transformers的实际效率。

Abstract: Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \textbf{CORP}, a closed-form one-shot structured pruning framework for Vision Transformers. CORP removes entire MLP hidden dimensions and attention substructures without labels, gradients, or fine-tuning. It operates under strict post-training constraints using only a small unlabeled calibration set. CORP formulates structured pruning as a representation recovery problem. It models removed activations and attention logits as affine functions of retained components and derives closed-form ridge regression solutions that fold compensation into model weights. This minimizes expected representation error under the calibration distribution. Experiments on ImageNet with DeiT models show strong redundancy in MLP and attention representations. Without compensation, one-shot structured pruning causes severe accuracy degradation. With CORP, models preserve accuracy under aggressive sparsity. On DeiT-Huge, CORP retains 82.8\% Top-1 accuracy after pruning 50\% of both MLP and attention structures. CORP completes pruning in under 20 minutes on a single GPU and delivers substantial real-world efficiency gains.

</details>


### [261] [TADS: Task-Aware Data Selection for Multi-Task Multimodal Pre-Training](https://arxiv.org/abs/2602.05251)
*Guanjie Cheng,Boyi Li,Lingyu Sun,Mengying Zhu,Yangyang Wu,Xinkui Zhao,Shuiguang Deng*

Main category: cs.LG

TL;DR: TADS是一个用于多任务多模态预训练的任务感知数据选择框架，通过整合内在质量、任务相关性和分布多样性来优化训练数据选择，仅使用36%的数据就能在多个基准测试上实现更好的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模多模态预训练模型（如CLIP）依赖网络爬取数据，但这些数据通常存在噪声、不对齐和冗余问题，导致训练效率低下和泛化能力不足。现有的数据选择方法要么是基于启发式的（存在偏见和多样性有限），要么是数据驱动但任务无关的（无法优化多任务场景）。

Method: TADS框架整合了三个关键维度：1）包含单模态和跨模态操作符的综合质量评估系统；2）通过可解释相似性向量量化任务相关性；3）基于聚类的加权优化分布多样性。采用反馈驱动的元学习机制，根据代理模型在多个下游任务上的性能自适应地优化选择策略。

Result: 在CC12M数据集上的实验表明，TADS仅使用36%的数据就能在ImageNet、CIFAR-100、MS-COCO和Flickr30K等基准测试上实现更好的零样本性能，平均比基线方法提升1.0%。

Conclusion: TADS通过精心筛选高效用数据子集，在相同计算约束下显著提高了数据效率，实现了更高的性能上限，为多任务多模态预训练提供了有效的数据选择解决方案。

Abstract: Large-scale multimodal pre-trained models like CLIP rely heavily on high-quality training data, yet raw web-crawled datasets are often noisy, misaligned, and redundant, leading to inefficient training and suboptimal generalization. Existing data selection methods are either heuristic-based, suffering from bias and limited diversity, or data-driven but task-agnostic, failing to optimize for multi-task scenarios. To address these gaps, we introduce TADS (Task-Aware Data Selection), a novel framework for multi-task multimodal pre-training that integrates Intrinsic Quality, Task Relevance, and Distributional Diversity into a learnable value function. TADS employs a comprehensive quality assessment system with unimodal and cross-modal operators, quantifies task relevance via interpretable similarity vectors, and optimizes diversity through cluster-based weighting. A feedback-driven meta-learning mechanism adaptively refines the selection strategy based on proxy model performance across multiple downstream tasks. Experiments on CC12M demonstrate that TADS achieves superior zero-shot performance on benchmarks like ImageNet, CIFAR-100, MS-COCO, and Flickr30K, using only 36% of the data while outperforming baselines by an average of 1.0%. This highlights that TADS significantly enhances data efficiency by curating a high-utility subset that yields a much higher performance ceiling within the same computational constraints.

</details>


### [262] [Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction](https://arxiv.org/abs/2602.05269)
*David Alejandro Trejo Pizzo*

Main category: cs.LG

TL;DR: HGF是一种双流架构，将1.58位三元主干与可学习的低秩FP16校正路径结合，通过自适应门控控制，在边缘设备上显著恢复量化模型质量，仅增加12-15%内存开销。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署LLM受限于"内存墙"问题，现有1.58位量化技术虽然大幅减少内存占用，但会导致20-25%的困惑度下降。需要一种既能保持低内存占用又能恢复模型质量的方法。

Method: 提出Hybrid Gated Flow (HGF)双流架构：1）1.58位三元主干网络；2）可学习的低秩FP16校正路径；3）自适应门控机制控制两个流的融合。在TinyStories数据集上进行实验验证。

Result: HGF 5.4在验证损失上达到0.9306，相比BitNet的1.0294显著改善，恢复了约55%三元量化与FP16基线之间的质量差距，仅增加12-15%内存开销。实验还发现量化具有结构正则化效应，能提高训练稳定性。

Conclusion: HGF架构有效解决了边缘设备LLM部署的内存墙问题，在保持低内存占用的同时显著恢复模型质量。该方法的稳定性和质量恢复效果可线性扩展到生产级语言模型规模。

Abstract: The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the "Memory Wall" -- a hardware limitation where memory bandwidth, not compute, becomes the bottleneck. Recent 1.58-bit quantization techniques (e.g., BitNet b1.58) dramatically reduce memory footprint but typically incur a perplexity degradation of 20-25% compared to FP16 baselines. In this work, we introduce Hybrid Gated Flow (HGF), a dual-stream architecture that couples a 1.58-bit ternary backbone with a learnable, low-rank FP16 correction path controlled by adaptive gates.
  Through extensive experiments on the TinyStories dataset across two training regimes (2500 and 3500 steps), we demonstrate that HGF 5.4 achieves a validation loss of 0.9306 compared to BitNet's 1.0294, recovering approximately 55% of the quality gap between pure ternary quantization and the FP16 baseline (0.8490). This recovery is achieved with only ~12-15% memory overhead beyond the ternary backbone.
  Furthermore, we provide empirical evidence for an emergent phenomenon: quantization as structural regularization. While a full-precision differential attention baseline (Diff_Only) exhibited training instability with validation loss exceeding 1.68, the ternary-anchored HGF maintained robust convergence throughout training. Finally, we report preliminary results extending this architecture to 1.2B and 3B parameter models trained on SlimPajama and FineWeb-Edu. These larger-scale experiments confirm that the architectural stability and quality recovery observed in small-scale proxies scale linearly to production-grade language modeling regimes.

</details>


### [263] [Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities](https://arxiv.org/abs/2602.05281)
*Pengyi Li,Elizaveta Goncharova,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 论文提出了一种新的优势重加权机制（ARM）来解决RLVR中策略优化方法（如GRPO）导致的低熵策略和模式崩溃问题，通过平衡所有正确回答的置信度来增强生成多样性和响应熵。


<details>
  <summary>Details</summary>
Motivation: 标准策略优化方法（如GRPO）在强化学习可验证奖励（RLVR）中往往收敛到低熵策略，导致严重的模式崩溃和有限的输出多样性。作者从采样概率动态的角度分析这一问题，发现标准目标函数不成比例地强化最高似然路径，从而抑制了有效的替代推理链。

Method: 提出了新颖的优势重加权机制（ARM），通过将提示困惑度和回答置信度纳入优势估计，动态重塑奖励信号，以衰减过度自信推理路径的梯度更新，同时将概率质量重新分配给未被充分探索的正确解决方案。

Result: 在Qwen2.5和DeepSeek模型上的数学和编码基准测试表明，ProGRPO显著缓解了熵崩溃。具体来说，在Qwen2.5-7B上，该方法在Pass@1上比GRPO高出5.7%，在Pass@32上显著高出13.9%，突显了其在生成多样化正确推理路径方面的卓越能力。

Conclusion: 该方法在保持竞争性准确性的同时，显著增强了生成多样性和响应熵，有效地在推理任务中实现了探索与利用之间的优越权衡。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.

</details>


### [264] [Robust Inference-Time Steering of Protein Diffusion Models via Embedding Optimization](https://arxiv.org/abs/2602.05285)
*Minhuan Li,Jiequn Han,Pilar Cossio,Luhuan Wu*

Main category: cs.LG

TL;DR: EmbedOpt：一种在条件嵌入空间中引导扩散模型优化实验似然的新方法，相比基于坐标的后验采样具有更好的鲁棒性和效率


<details>
  <summary>Details</summary>
Motivation: 当目标构象位于先验分布的低密度区域时，传统的后验采样方法需要激进的似然权重调整，导致方法脆弱且不稳定。需要一种更鲁棒的推理时方法来引导扩散模型满足实验约束。

Method: 提出EmbedOpt方法，在条件嵌入空间中优化实验似然，而不是直接在原子坐标空间中进行后验采样。该嵌入空间编码了丰富的序列和共进化信号，通过优化嵌入空间可以有效地将扩散先验与实验约束对齐。

Result: 在模拟冷冻电镜图谱拟合和实验距离约束两个基准测试中，EmbedOpt在冷冻电镜图谱拟合任务中优于基于坐标的后验采样方法，在距离约束任务中性能相当，且在跨越两个数量级的超参数范围内表现出更好的工程鲁棒性。同时，其平滑的优化行为显著减少了推理所需的扩散步骤数，提高了效率。

Conclusion: EmbedOpt提供了一种更鲁棒、高效的替代方案，用于引导扩散模型生成既符合物理先验又满足实验约束的构象，特别是在目标位于先验分布低密度区域时具有显著优势。

Abstract: In many biophysical inverse problems, the goal is to generate biomolecular conformations that are both physically plausible and consistent with experimental measurements. As recent sequence-to-structure diffusion models provide powerful data-driven priors, posterior sampling has emerged as a popular framework by guiding atomic coordinates to target conformations using experimental likelihoods. However, when the target lies in a low-density region of the prior, posterior sampling requires aggressive and brittle weighting of the likelihood guidance. Motivated by this limitation, we propose EmbedOpt, an alternative inference-time approach for steering diffusion models to optimize experimental likelihoods in the conditional embedding space. As this space encodes rich sequence and coevolutionary signals, optimizing over it effectively shifts the diffusion prior to align with experimental constraints. We validate EmbedOpt on two benchmarks simulating cryo-electron microscopy map fitting and experimental distance constraints. We show that EmbedOpt outperforms the coordinate-based posterior sampling method in map fitting tasks, matches performance on distance constraint tasks, and exhibits superior engineering robustness across hyperparameters spanning two orders of magnitude. Moreover, its smooth optimization behavior enables a significant reduction in the number of diffusion steps required for inference, leading to better efficiency.

</details>


### [265] [HealthMamba: An Uncertainty-aware Spatiotemporal Graph State Space Model for Effective and Reliable Healthcare Facility Visit Prediction](https://arxiv.org/abs/2602.05286)
*Dahai Yu,Lin Jiang,Rongchao Xu,Guang Wang*

Main category: cs.LG

TL;DR: HealthMamba：一个不确定性感知的时空框架，用于准确可靠的医疗设施访问预测，通过统一时空上下文编码器、GraphMamba图状态空间模型和综合不确定性量化模块，在四个真实数据集上实现约6.0%的预测精度提升和3.5%的不确定性量化改进。


<details>
  <summary>Details</summary>
Motivation: 现有医疗设施访问预测研究通常将其视为时间序列预测问题，忽略了不同类型医疗设施之间的空间依赖性，且无法在公共紧急情况等异常情况下提供可靠预测。需要同时考虑时空依赖性和不确定性量化来提高预测的准确性和可靠性。

Method: 提出HealthMamba框架，包含三个关键组件：1）统一时空上下文编码器，融合异构静态和动态信息；2）GraphMamba图状态空间模型，用于分层时空建模；3）综合不确定性量化模块，集成三种不确定性量化机制。

Result: 在加州、纽约、德克萨斯和佛罗里达四个大规模真实数据集上的评估显示，HealthMamba相比最先进的基线方法，在预测精度上实现约6.0%的提升，在不确定性量化上实现约3.5%的改进。

Conclusion: HealthMamba通过同时考虑时空依赖性和不确定性量化，能够提供更准确可靠的医疗设施访问预测，特别是在异常情况下，为医疗资源优化分配和公共卫生政策制定提供了更好的支持。

Abstract: Healthcare facility visit prediction is essential for optimizing healthcare resource allocation and informing public health policy. Despite advanced machine learning methods being employed for better prediction performance, existing works usually formulate this task as a time-series forecasting problem without considering the intrinsic spatial dependencies of different types of healthcare facilities, and they also fail to provide reliable predictions under abnormal situations such as public emergencies. To advance existing research, we propose HealthMamba, an uncertainty-aware spatiotemporal framework for accurate and reliable healthcare facility visit prediction. HealthMamba comprises three key components: (i) a Unified Spatiotemporal Context Encoder that fuses heterogeneous static and dynamic information, (ii) a novel Graph State Space Model called GraphMamba for hierarchical spatiotemporal modeling, and (iii) a comprehensive uncertainty quantification module integrating three uncertainty quantification mechanisms for reliable prediction. We evaluate HealthMamba on four large-scale real-world datasets from California, New York, Texas, and Florida. Results show HealthMamba achieves around 6.0% improvement in prediction accuracy and 3.5% improvement in uncertainty quantification over state-of-the-art baselines.

</details>


### [266] [When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging](https://arxiv.org/abs/2602.05536)
*Yayuan Li,Ze Peng,Jian Zhang,Jintao Guo,Yue Duan,Yinghuan Shi*

Main category: cs.LG

TL;DR: 论文提出奇异值校准(SVC)方法，通过量化子空间重叠并重新缩放膨胀的奇异值来解决模型合并中的过计数问题，在视觉和语言基准测试中显著提升合并性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法主要解决任务更新间的冲突，但忽视了共享知识过计数的问题。当任务共享对齐的谱方向时，简单的线性组合会重复累积这些方向，导致奇异值膨胀并使合并模型偏向共享子空间。

Method: 提出奇异值校准(SVC)，一种无需训练和数据的后处理方法。该方法量化子空间重叠，并重新缩放膨胀的奇异值以恢复平衡的谱结构。

Result: 在视觉和语言基准测试中，SVC持续改进强合并基线并实现最先进性能。通过仅修改奇异值，SVC将任务算术的性能提升了13.0%。

Conclusion: SVC有效解决了模型合并中的共享知识过计数问题，通过校准奇异值改善了合并模型的性能平衡，为模型合并提供了新的优化方向。

Abstract: Model merging combines multiple fine-tuned models into a single model by adding their weight updates, providing a lightweight alternative to retraining. Existing methods primarily target resolving conflicts between task updates, leaving the failure mode of over-counting shared knowledge unaddressed. We show that when tasks share aligned spectral directions (i.e., overlapping singular vectors), a simple linear combination repeatedly accumulates these directions, inflating the singular values and biasing the merged model toward shared subspaces. To mitigate this issue, we propose Singular Value Calibration (SVC), a training-free and data-free post-processing method that quantifies subspace overlap and rescales inflated singular values to restore a balanced spectrum. Across vision and language benchmarks, SVC consistently improves strong merging baselines and achieves state-of-the-art performance. Furthermore, by modifying only the singular values, SVC improves the performance of Task Arithmetic by 13.0%. Code is available at: https://github.com/lyymuwu/SVC.

</details>


### [267] [Steering Large Reasoning Models towards Concise Reasoning via Flow Matching](https://arxiv.org/abs/2602.05539)
*Yawei Li,Benjamin Bergner,Yinghan Zhao,Vihang Prakash Patil,Bei Chen,Cheng Wang*

Main category: cs.LG

TL;DR: FlowSteer：一种非线性引导方法，通过学习完整分布变换来减少大推理模型的冗长输出，相比线性方法更有效


<details>
  <summary>Details</summary>
Motivation: 大推理模型在复杂推理任务上表现出色，但输出过于冗长影响效率。现有线性引导方法基于限制性线性表示假设，仅应用全局向量到隐藏表示，效果有限

Method: FlowSteer通过流匹配学习冗长推理和简洁推理相关分布之间的完整变换，作为速度场实现精确、输入相关的推理过程控制，将引导表示与简洁推理激活分布对齐

Result: 在多样化推理基准测试中，FlowSteer相比领先的推理时基线方法，展现出更强的任务性能和标记效率，产生更紧凑的推理过程

Conclusion: 使用生成技术建模完整分布传输为控制大推理模型提供了更有效和原则性的基础，超越了线性变换的限制

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.

</details>


### [268] [Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective](https://arxiv.org/abs/2602.05319)
*Yinan Huang,Hans Hao-Hsun Hsu,Junran Wang,Bo Dai,Pan Li*

Main category: cs.LG

TL;DR: 提出Sequential Flow Matching框架，通过贝叶斯滤波将流式推理视为概率流学习，从前一后验分布初始化生成，实现单步或少步快速采样，在保持性能的同时大幅降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 在随机动力系统的流式观测序列预测中，扩散和流匹配模型虽然能建模复杂多模态轨迹，但在实时流式环境中通常需要从非信息性初始分布重复采样，导致显著的推理延迟和系统积压。

Method: 提出Sequential Flow Matching框架，基于贝叶斯滤波理论，将流式推理视为学习将预测分布从一个时间步传输到下一个时间步的概率流。该方法从前一后验分布初始化生成，提供有原则的"热启动"，加速采样过程。

Result: 在广泛的预测、决策和状态估计任务中，该方法在仅需一步或很少采样步数的情况下，实现了与完整步数扩散模型竞争的性能，同时采样速度更快。

Conclusion: 通过贝叶斯滤波框架进行序列推理，为基于流模型的高效实时部署提供了新的、有原则的视角，解决了传统方法中的推理延迟问题。

Abstract: Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.

</details>


### [269] [GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL](https://arxiv.org/abs/2602.05323)
*Zifan Liu,Xinran Li,Shibo Chen,Jun Zhang*

Main category: cs.LG

TL;DR: 提出GAS算法，通过数据增强和重新标注增强拼接能力，使用目标函数估计最优奖励和成本目标，在离线安全强化学习中实现更好的奖励最大化与约束满足平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的离线安全强化学习方法存在两个主要问题：1）无法从次优轨迹中"拼接"最优转移；2）难以平衡奖励目标和成本目标，特别是当它们冲突时。

Method: 提出Goal-Assisted Stitching (GAS)算法：1）在转移级别增强和重新标注数据集，从次优轨迹构建高质量轨迹；2）引入目标函数，使用期望回归估计最优可实现的奖励和成本目标；3）重塑数据集以获得更均匀的奖励-成本回报分布。

Result: 实证结果验证了GAS的有效性，在平衡奖励最大化和约束满足方面表现出优于现有方法的性能。

Conclusion: GAS通过增强拼接能力和引入目标函数，有效解决了离线安全强化学习中生成模型方法的局限性，实现了更好的奖励-成本权衡。

Abstract: Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to "stitch" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.

</details>


### [270] [Rewards as Labels: Revisiting RLVR from a Classification Perspective](https://arxiv.org/abs/2602.05630)
*Zepeng Zhai,Meilin Chen,Jiaxuan Zhao,Junlang Qian,Lei Shen,Yuan Lu*

Main category: cs.LG

TL;DR: 提出REAL框架，将可验证奖励重新视为分类标签而非标量权重，解决GRPO方法中的梯度错配问题，在数学推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO及其变体在强化学习与可验证奖励方面取得了成功，但作者发现这些方法存在"正样本梯度错配"和"负样本梯度主导"问题，导致策略更新效率低下和次优。

Method: 提出REAL框架，将可验证奖励重新视为分类标签而非标量权重，将策略优化重新构建为分类问题。在此基础上引入锚定logits来增强策略学习。

Result: 在数学推理基准测试中，REAL提高了训练稳定性，并持续优于GRPO及其变体如DAPO。在1.5B模型上，REAL比DAPO平均Pass@1提高6.7%；在7B模型上，REAL继续优于DAPO和GSPO，分别提升6.2%和1.7%。即使使用简单的二元交叉熵，REAL仍保持稳定并平均超过DAPO 4.5%。

Conclusion: REAL通过将奖励重新定义为分类标签，实现了单调且有界的梯度加权，平衡了不同rollout间的梯度分配，有效缓解了梯度错配问题，为强化学习与可验证奖励方法提供了更稳定高效的训练框架。

Abstract: Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.

</details>


### [271] [Pool-based Active Learning as Noisy Lossy Compression: Characterizing Label Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.05333)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 本文提出了一个信息论框架来分析池式主动学习的理论极限，将池式主动学习重新表述为有噪有损压缩问题，并推导了标签复杂度和泛化误差的信息论下界。


<details>
  <summary>Details</summary>
Motivation: 池式主动学习（AL）的理论分析存在局限，现有方法未能统一分析数据选择和学习过程。需要建立信息论框架来理解池式AL的理论极限，包括标签复杂度和泛化误差的下界。

Method: 将池式主动学习重新表述为有噪有损压缩问题：池观测映射到有噪符号观测，数据选择对应压缩，学习对应解码。应用有噪有损压缩的有限块长分析，推导信息论下界。

Result: 推导出标签复杂度和泛化误差的信息论下界，这些界限反映了学习算法引起的过拟合以及其归纳偏差与目标任务之间的差异。这些界限与现有信息论界限和稳定性理论相关。

Conclusion: 该框架为池式主动学习提供了新的理论视角，统一分析了数据选择和学习过程，并建立了与信息论和稳定性理论的联系，为理解池式AL的理论极限提供了基础。

Abstract: This paper proposes an information-theoretic framework for analyzing the theoretical limits of pool-based active learning (AL), in which a subset of instances is selectively labeled. The proposed framework reformulates pool-based AL as a noisy lossy compression problem by mapping pool observations to noisy symbol observations, data selection to compression, and learning to decoding. This correspondence enables a unified information-theoretic analysis of data selection and learning in pool-based AL. Applying finite blocklength analysis of noisy lossy compression, we derive information-theoretic lower bounds on label complexity and generalization error that serve as theoretical limits for a given learning algorithm under its associated optimal data selection strategy. Specifically, our bounds include terms that reflect overfitting induced by the learning algorithm and the discrepancy between its inductive bias and the target task, and are closely related to established information-theoretic bounds and stability theory, which have not been previously applied to the analysis of pool-based AL. These properties yield a new theoretical perspective on pool-based AL.

</details>


### [272] [Smoothness Errors in Dynamics Models and How to Avoid Them](https://arxiv.org/abs/2602.05352)
*Edward Berman,Luisa Li,Jung Yeon Park,Robin Walters*

Main category: cs.LG

TL;DR: 本文提出了一种放松的酉卷积方法，用于在网格上建模物理系统动力学，解决了传统酉卷积在物理系统中过度约束的问题，并在热方程、波动方程和天气预报等任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在求解曲面偏微分方程方面表现出潜力，但图神经网络存在过度平滑问题。酉卷积虽然能保持平滑性，但在许多物理系统（如扩散过程）中，平滑性自然增加，酉性可能过度约束。需要一种平衡平滑性保持与物理系统自然平滑需求的方法。

Method: 提出放松的酉卷积，在保持平滑性的同时允许物理系统所需的自然平滑。将酉卷积和放松酉卷积从图推广到网格。系统研究不同GNN在动力学建模中的平滑效应，证明酉卷积在此类任务中会损害性能。

Result: 在复杂网格上的热方程、波动方程以及天气预报等实验中，该方法优于多个强基线，包括网格感知变换器和等变神经网络。

Conclusion: 放松的酉卷积方法有效平衡了平滑性保持与物理系统自然平滑需求，在曲面偏微分方程求解和物理系统动力学建模中表现出优越性能。

Abstract: Modern neural networks have shown promise for solving partial differential equations over surfaces, often by discretizing the surface as a mesh and learning with a mesh-aware graph neural network. However, graph neural networks suffer from oversmoothing, where a node's features become increasingly similar to those of its neighbors. Unitary graph convolutions, which are mathematically constrained to preserve smoothness, have been proposed to address this issue. Despite this, in many physical systems, such as diffusion processes, smoothness naturally increases and unitarity may be overconstraining. In this paper, we systematically study the smoothing effects of different GNNs for dynamics modeling and prove that unitary convolutions hurt performance for such tasks. We propose relaxed unitary convolutions that balance smoothness preservation with the natural smoothing required for physical systems. We also generalize unitary and relaxed unitary convolutions from graphs to meshes. In experiments on PDEs such as the heat and wave equations over complex meshes and on weather forecasting, we find that our method outperforms several strong baselines, including mesh-aware transformers and equivariant neural networks.

</details>


### [273] [Bayesian Neighborhood Adaptation for Graph Neural Networks](https://arxiv.org/abs/2602.05358)
*Paribesh Regmi,Rui Li,Kishan K C*

Main category: cs.LG

TL;DR: 提出一种贝叶斯框架，将GNN的消息传递建模为随机过程，通过beta过程自适应推断最佳邻域范围，提升GNN表达能力和性能。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法需要为每个预定义邻域范围训练验证GNN，耗时且易受搜索空间设计偏差影响。如何为同质和异质图自适应确定合适的邻域范围仍待探索。

Method: 将GNN消息传递建模为随机过程，将跳数视为beta过程，通过贝叶斯框架同时推断最可能邻域范围和优化GNN参数。

Result: 在基准同质和异质数据集上，该方法与最先进GNN变体兼容，在节点分类任务上达到竞争性或更优性能，并提供良好校准的预测。

Conclusion: 提出的贝叶斯框架能自适应推断GNN消息聚合的最佳邻域范围，理论分析显示范围推断提升GNN表达能力，实验验证了方法的有效性和通用性。

Abstract: The neighborhood scope (i.e., number of hops) where graph neural networks (GNNs) aggregate information to characterize a node's statistical property is critical to GNNs' performance. Two-stage approaches, training and validating GNNs for every pre-specified neighborhood scope to search for the best setting, is a time-consuming task and tends to be biased due to the search space design. How to adaptively determine proper neighborhood scopes for the aggregation process for both homophilic and heterophilic graphs remains largely unexplored. We thus propose to model the GNNs' message-passing behavior on a graph as a stochastic process by treating the number of hops as a beta process. This Bayesian framework allows us to infer the most plausible neighborhood scope for message aggregation simultaneously with the optimization of GNN parameters. Our theoretical analysis shows that the scope inference improves the expressivity of a GNN. Experiments on benchmark homophilic and heterophilic datasets show that the proposed method is compatible with state-of-the-art GNN variants, achieving competitive or superior performance on the node classification task, and providing well-calibrated predictions.

</details>


### [274] [Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting](https://arxiv.org/abs/2602.05371)
*Hongyi Li,Han Lin,Jun Xu*

Main category: cs.LG

TL;DR: HRT提出了一种新的斜决策树学习方法，将分裂问题重构为非线性最小二乘问题，通过交替拟合和阻尼牛顿法高效学习，在紧凑结构下实现高性能。


<details>
  <summary>Details</summary>
Motivation: 斜决策树结合了树的透明性和多元决策边界的表达能力，但学习高质量斜分裂是NP难问题，现有方法依赖缓慢搜索或缺乏理论基础的启发式方法。

Method: 将每个分裂重构为两个线性预测器的非线性最小二乘问题，其最大/最小包络产生ReLU类表达能力。采用交替拟合程序，相当于固定分区内的阻尼牛顿（高斯-牛顿）法。支持回溯线搜索和可选的岭正则化。

Result: 理论分析证明局部目标单调递减并收敛；实际中固定和自适应阻尼都能实现快速稳定收敛。HRT模型类是通用逼近器，具有明确的O(δ²)逼近率。在合成和真实基准测试中，以更紧凑的结构匹配或优于单树基线。

Conclusion: HRT提供了一种高效、理论严谨的斜决策树学习方法，结合了树的透明性和多元决策边界的表达能力，在紧凑结构下实现高性能。

Abstract: Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(δ^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.

</details>


### [275] [DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders](https://arxiv.org/abs/2602.05859)
*Xu Wang,Bingqing Jiang,Yu Wan,Baosong Yang,Lingpeng Kong,Difan Zou*

Main category: cs.LG

TL;DR: 提出了DLM-Scope，首个基于稀疏自编码器（SAE）的扩散语言模型（DLM）可解释性框架，发现SAE在DLM中的表现与自回归LLM不同，能降低早期层的交叉熵损失，并支持更有效的干预。


<details>
  <summary>Details</summary>
Motivation: 随着扩散语言模型（DLMs）成为自回归大语言模型（LLMs）的有前景替代方案，需要为这类新兴模型开发专门的可解释性工具，以支持机制可解释性研究。

Method: 提出了DLM-Scope框架，使用Top-K稀疏自编码器（SAE）从DLMs中提取稀疏、可解释的特征，并探索了SAE在DLMs中的插入效果、干预能力以及在新研究方向上的应用。

Result: SAE在DLMs中的表现与LLMs不同：在早期层插入SAE能降低交叉熵损失（LLMs中通常增加损失）；SAE特征支持更有效的扩散时间干预，性能优于LLM引导；SAE还能为DLM解码顺序提供有用信号，且特征在训练后阶段保持稳定。

Conclusion: DLM-Scope为DLMs的机制可解释性奠定了基础，展示了SAE在DLM相关任务和算法中的巨大潜力，为这一新兴模型类别提供了重要的可解释性工具。

Abstract: Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.

</details>


### [276] [Erase at the Core: Representation Unlearning for Machine Unlearning](https://arxiv.org/abs/2602.05375)
*Jaewon Lee,Yongwoo Kim,Donghyun Kim*

Main category: cs.LG

TL;DR: 论文提出EC框架解决机器学习遗忘中的表面遗忘问题，通过多层次对比遗忘和深度监督学习实现网络各层的真正遗忘


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法主要改变最终分类器，但中间层表示仍保留大量原始模型信息，导致"表面遗忘"问题

Method: EC框架：1) 在中间层附加辅助模块；2) 对遗忘集应用多层次对比遗忘损失；3) 对保留集使用深度监督学习；4) 采用分层加权损失

Result: EC不仅实现有效的logit级遗忘，还显著降低中间层与原始模型的表示相似性，且可作为插件模块提升现有遗忘方法的表示级遗忘效果

Conclusion: EC框架解决了机器学习遗忘中的表面遗忘问题，实现了网络各层次的真正遗忘，同时保持保留集性能

Abstract: Many approximate machine unlearning methods demonstrate strong logit-level forgetting -- such as near-zero accuracy on the forget set -- yet continue to preserve substantial information within their internal feature representations. We refer to this discrepancy as superficial forgetting. Recent studies indicate that most existing unlearning approaches primarily alter the final classifier, leaving intermediate representations largely unchanged and highly similar to those of the original model. To address this limitation, we introduce the Erase at the Core (EC), a framework designed to enforce forgetting throughout the entire network hierarchy. EC integrates multi-layer contrastive unlearning on the forget set with retain set preservation through deeply supervised learning. Concretely, EC attaches auxiliary modules to intermediate layers and applies both contrastive unlearning and cross-entropy losses at each supervision point, with layer-wise weighted losses. Experimental results show that EC not only achieves effective logit-level forgetting, but also substantially reduces representational similarity to the original model across intermediate layers. Furthermore, EC is model-agnostic and can be incorporated as a plug-in module into existing unlearning methods, improving representation-level forgetting while maintaining performance on the retain set.

</details>


### [277] [Constrained Group Relative Policy Optimization](https://arxiv.org/abs/2602.05863)
*Roger Girgis,Rodrigue de Schaetzen,Luke Rowe,Azalée Robitaille,Christopher Pal,Liam Paull*

Main category: cs.LG

TL;DR: 提出Constrained GRPO，一种基于拉格朗日方法的GRPO扩展，用于带行为约束的策略优化。通过标量化优势估计解决多组件优势估计中的优化病理问题，在机器人任务中实现更好的约束满足和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然GRPO已成为无critic策略学习的可扩展框架，但在具有显式行为约束的场景中应用仍待探索。需要将GRPO扩展到带约束的策略优化问题，特别是在依赖大型多模态基础模型的具身AI领域。

Method: 提出Constrained GRPO，基于拉格朗日松弛方法，通过指示器成本函数指定约束。发现朴素的多组件优势估计会破坏约束学习：不匹配的组件标准差扭曲不同目标项的相对重要性，进而破坏拉格朗日信号。为此提出标量化优势构造，保持奖励和约束项之间的预期权衡。

Result: 在玩具网格世界中验证了预测的优化病理，证明标量化优势能够恢复稳定的约束控制。在机器人任务评估中，Constrained GRPO提高了约束满足度，同时增加了任务成功率。

Conclusion: Constrained GRPO为具身AI领域的约束策略优化提供了一个简单有效的方案，特别适用于日益依赖大型多模态基础模型的场景。标量化优势构造解决了多组件优势估计中的关键问题，实现了稳定的约束控制。

Abstract: While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.

</details>


### [278] [A Decomposition-based State Space Model for Multivariate Time-Series Forecasting](https://arxiv.org/abs/2602.05389)
*Shunya Nagashima,Shuntaro Suzuki,Shuitsu Koyama,Shinnosuke Hirano*

Main category: cs.LG

TL;DR: DecompSSM：基于深度状态空间模型的三分支并行分解框架，用于多变量时间序列预测，能自适应捕获趋势、季节性和残差成分


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列交织着缓慢趋势、多速率季节性和不规则残差，现有方法要么依赖僵化的手工分解，要么使用通用的端到端架构导致成分纠缠且未能充分利用变量间的共享结构

Method: 提出DecompSSM端到端分解框架，使用三个并行的深度状态空间模型分支分别捕获趋势、季节性和残差成分；包含输入依赖的自适应时间尺度预测器、跨变量共享上下文细化模块以及强制重构和正交性的辅助损失

Result: 在ECL、Weather、ETTm2和PEMS04等标准基准测试中，DecompSSM优于强基线模型，表明组件级深度状态空间模型与全局上下文细化的结合是有效的

Conclusion: 通过结合组件级深度状态空间模型和全局上下文细化，DecompSSM能够有效处理多变量时间序列预测中的复杂成分交织问题，在多个领域基准上取得优越性能

Abstract: Multivariate time series (MTS) forecasting is crucial for decision-making in domains such as weather, energy, and finance. It remains challenging because real-world sequences intertwine slow trends, multi-rate seasonalities, and irregular residuals. Existing methods often rely on rigid, hand-crafted decompositions or generic end-to-end architectures that entangle components and underuse structure shared across variables. To address these limitations, we propose DecompSSM, an end-to-end decomposition framework using three parallel deep state space model branches to capture trend, seasonal, and residual components. The model features adaptive temporal scales via an input-dependent predictor, a refinement module for shared cross-variable context, and an auxiliary loss that enforces reconstruction and orthogonality. Across standard benchmarks (ECL, Weather, ETTm2, and PEMS04), DecompSSM outperformed strong baselines, indicating the effectiveness of combining component-wise deep state space models and global context refinement.

</details>


### [279] [Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations](https://arxiv.org/abs/2602.05885)
*Wei Liu,Jiawei Xu,Yingru Li,Longtao Zheng,Tianjian Li,Qian Liu,Junxian He*

Main category: cs.LG

TL;DR: 本文提出KernelGYM环境与TRLOO方法，用于训练LLM生成高性能GPU内核代码，解决了奖励黑客和懒惰优化问题，Dr.Kernel-14B在性能上超越Claude-4.5-Sonnet和GPT-5。


<details>
  <summary>Details</summary>
Motivation: 高质量GPU内核对可扩展AI系统至关重要，但训练LLM生成此类代码面临数据不足、环境脆弱、奖励黑客和懒惰优化等问题，需要系统研究强化学习方法。

Method: 设计KernelGYM分布式GPU环境支持奖励黑客检查和多轮交互；提出TRLOO解决GRPO中的偏差策略梯度问题；引入配置不匹配校正、基于性能分析的奖励和拒绝采样来缓解懒惰优化。

Result: Dr.Kernel-14B在KernelBench Level-2子集上，31.6%生成的内核达到至少1.2倍加速，超越Claude-4.5-Sonnet(26.7%)和GPT-5(28.6%)；多轮最佳候选选择时加速率提升至47.8%。

Conclusion: 通过KernelGYM环境和TRLOO等方法，成功训练出高性能内核生成模型，在加速性能上达到领先水平，为AI系统优化提供了有效解决方案。

Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.

</details>


### [280] [Assessing Electricity Demand Forecasting with Exogenous Data in Time Series Foundation Models](https://arxiv.org/abs/2602.05390)
*Wei Soon Cheong,Lian Lian Jiang,Jamie Ng Suat Ling*

Main category: cs.LG

TL;DR: 论文评估了时间序列基础模型在电力需求预测中利用外生特征的能力，发现基础模型表现不稳定，简单的LSTM基线在稳定气候条件下经常优于基础模型，模型架构和地理环境是关键因素。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型已成为预测的新范式，但它们在电力需求预测中有效利用外生特征的能力尚不清楚。外生特征对电力需求预测至关重要，需要评估基础模型在此领域的实际表现。

Method: 在新加坡和澳大利亚电力市场的每小时和每日粒度上，实证评估了MOIRAI、MOMENT、TinyTimeMixers、ChronosX和Chronos-2等基础模型。使用可逆实例归一化的LSTM作为基线，在三种特征配置下（所有特征、选定特征、仅目标变量）进行系统评估。

Result: 基础模型效果差异很大：Chronos-2在零样本设置中表现最佳，但在新加坡稳定气候条件下，简单的LSTM基线经常优于所有基础模型，特别是在短期预测中。模型架构至关重要，TTM的通道混合和Chronos-2的分组注意力能一致地利用外生特征，而其他方法表现不一致。地理环境同样重要，基础模型主要在变化气候条件下显示优势。

Conclusion: 结果挑战了基础模型普遍优越性的假设，强调了在能源领域需要特定领域模型的重要性。模型架构和地理环境是决定基础模型在电力需求预测中有效性的关键因素。

Abstract: Time-series foundation models have emerged as a new paradigm for forecasting, yet their ability to effectively leverage exogenous features -- critical for electricity demand forecasting -- remains unclear. This paper empirically evaluates foundation models capable of modeling cross-channel correlations against a baseline LSTM with reversible instance normalization across Singaporean and Australian electricity markets at hourly and daily granularities. We systematically assess MOIRAI, MOMENT, TinyTimeMixers, ChronosX, and Chronos-2 under three feature configurations: all features, selected features, and target-only. Our findings reveal highly variable effectiveness: while Chronos-2 achieves the best performance among foundation models (in zero-shot settings), the simple baseline frequently outperforms all foundation models in Singapore's stable climate, particularly for short-term horizons. Model architecture proves critical, with synergistic architectural implementations (TTM's channel-mixing, Chronos-2's grouped attention) consistently leveraging exogenous features, while other approaches show inconsistent benefits. Geographic context emerges as equally important, with foundation models demonstrating advantages primarily in variable climates. These results challenge assumptions about universal foundation model superiority and highlight the need for domain-specific models, specifically in the energy domain.

</details>


### [281] [DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training](https://arxiv.org/abs/2602.05890)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Jiahan Li,Chenhao Huang,Junjie Ye,Sixian Li,Mingxu Chai,Yuhui Wang,Yajie Yang,Ming Zhang,Jiazheng Zhang,Shichun Liu,Caishuang Huang,Yunke Zhang,Yuran Wang,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: DFPO是一种鲁棒的分布强化学习框架，通过将价值建模为跨时间步的连续流而非独立分位数预测，在噪声监督下实现更好的训练稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界RL训练面临噪声监督和域外泛化差的问题，特别是在LLM后训练中。现有分布RL方法虽然通过多分位数建模提高鲁棒性，但仍将每个分位数作为独立标量学习，导致价值表示粗糙，缺乏对状态信息的细粒度条件化，在复杂和OOD条件下表现不佳。

Method: 提出DFPO框架：1）将价值建模为跨时间步的连续流，学习价值流场而非孤立分位数预测；2）引入条件风险控制和一致性约束来稳定噪声反馈下的训练；3）通过更丰富的状态信息捕获实现更准确的优势估计。

Result: 在对话、数学推理和科学任务上的实验表明，DFPO在噪声监督下优于PPO、FlowRL和其他鲁棒基线，实现了更好的训练稳定性和泛化能力。

Conclusion: DFPO通过连续价值流建模和稳定性控制机制，为噪声环境下的RL训练提供了更鲁棒的解决方案，特别是在LLM后训练等复杂场景中。

Abstract: Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.

</details>


### [282] [Robust Federated Learning via Byzantine Filtering over Encrypted Updates](https://arxiv.org/abs/2602.05410)
*Adda Akram Bendoukha,Aymen Boudguiga,Nesrine Kaaniche,Renaud Sirdey,Didem Demirag,Sébastien Gambs*

Main category: cs.LG

TL;DR: 提出结合同态加密和元分类器的联邦学习安全聚合方案，同时实现隐私保护和拜占庭容错


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保护数据隐私，但仍面临推理攻击和拜占庭行为的安全威胁，现有方案往往独立处理安全聚合和拜占庭容错，难以同时实现

Method: 1) 基于属性推理攻击思路训练元分类器检测拜占庭更新，包括后门、梯度反转、标签翻转等攻击；2) 提出自动选择CKKS同态加密最优核和维度参数的方法；3) 通过重加权机制消除拜占庭加密更新

Result: 在FEMNIST、CIFAR10、GTSRB和acsincome基准测试中，SVM过滤器的拜占庭更新识别准确率达90%-94%，模型效用损失很小，加密推理运行时间6-26秒

Conclusion: 该方法有效解决了联邦学习中隐私保护和拜占庭容错的双重挑战，实现了安全高效的协作学习

Abstract: Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.

</details>


### [283] [BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs](https://arxiv.org/abs/2602.05448)
*Sheshansh Agrawal,Thien Hang Nguyen,Douwe Kiela*

Main category: cs.LG

TL;DR: 提出基于锦标赛图的k-wise重排序框架，通过聚合文档比较中的成对偏好信息，在减少LLM调用次数的同时提高重排序效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM重排序方法要么依赖启发式方法未能充分利用每次排序决策的信息，要么效率低下。需要一种既能充分利用LLM比较信息又能高效执行的方法

Method: 引入锦标赛图框架：将k个文档比较视为包含binom(k,2)个成对偏好的完整锦标赛，聚合到全局偏好图中，利用传递闭包推导额外排序关系，设计最大化信息增益的查询调度，处理非传递偏好时通过等价类生成分层排序

Result: 在14个基准测试和5个LLM上实现帕累托优势：达到或超过现有方法的准确性，同时比可比方法减少25-40%的token使用，在接近相同质量下比成对方法减少7倍token

Conclusion: 锦标赛图框架为k-wise重排序提供了理论基础，通过有效利用LLM比较中的信息，在保持高质量的同时显著减少了计算成本，解决了现有方法的效率问题

Abstract: Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.

</details>


### [284] [When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL](https://arxiv.org/abs/2602.05459)
*Jan Malte Töpperwien,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 离线目标条件强化学习中的超参数敏感性并非RL问题固有，而是由自举学习机制加剧。研究发现，在离线设置下，即使存在非平稳性，算法对超参数变化也表现出比在线RL更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究深度强化学习中普遍存在的超参数敏感性是否本质上是RL问题固有的，还是由特定训练机制（特别是自举学习）加剧的。通过离线目标条件RL设置，可以控制数据分布和非平稳性，从而隔离分析训练机制的影响。

Method: 在离线目标条件RL环境中，控制数据分布和非平稳性（通过调度数据质量变化）。研究两种代表性算法：HIQL（基于自举TD学习）和QRL（拟度量表示学习）。引入跨目标梯度对齐诊断来分析梯度干扰模式。

Result: 在离线设置下观察到比在线RL更强的超参数鲁棒性。当存在约20%专家数据时，QRL保持广泛稳定的近最优区域，而HIQL表现出尖锐的最优点且随训练阶段显著漂移。自举目标表现出更强的破坏性梯度干扰，这与超参数敏感性直接相关。

Conclusion: 训练期间对超参数配置的高敏感性并非RL不可避免的特性，而是由自举学习的动态特性加剧的。这为设计更鲁棒的算法目标提供了途径，减少对超参数调优的依赖。

Abstract: Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\approx$ 20\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.

</details>


### [285] [Thermodynamic Limits of Physical Intelligence](https://arxiv.org/abs/2602.05463)
*Koichi Takahashi,Yusuke Hayashi*

Main category: cs.LG

TL;DR: 论文提出两种比特/焦耳指标来衡量AI系统的物理效率：热力学认知复杂度/焦耳（识别能力）和赋能/焦耳（控制能力），建立了信息获取与能耗的理论联系，并提供了统一的效率评估框架。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在取得显著能力的同时消耗大量能源，需要建立智能与物理效率之间的量化联系，为AI系统的能效评估提供理论基础和实用指标。

Method: 基于随机热力学，提出两种互补的比特/焦耳指标：热力学认知复杂度/焦耳（衡量智能体内部状态编码环境结构信息的能力）和赋能/焦耳（衡量传感器运动通道容量与能耗的比值）。建立了Landauer尺度的闭环基准，并针对实际场景提出了MDL认知复杂度的替代方案。

Result: 证明了在明确子系统假设下，热力学学习不等式可推导出Landauer尺度的闭环基准；同时展示了若无边界条件和外部低熵资源成本假设，信息获取与系统内耗散之间可能不存在紧密联系。提出了统一的效率评估框架和报告清单。

Conclusion: 论文建立了智能与物理效率的理论联系，提供了两种互补的比特/焦耳指标和统一的评估框架，有助于减少歧义、支持一致的能效比较，并为能源调整的规模分析奠定了基础。

Abstract: Modern AI systems achieve remarkable capabilities at the cost of substantial energy consumption. To connect intelligence to physical efficiency, we propose two complementary bits-per-joule metrics under explicit accounting conventions: (1) Thermodynamic Epiplexity per Joule -- bits of structural information about a theoretical environment-instance variable newly encoded in an agent's internal state per unit measured energy within a stated boundary -- and (2) Empowerment per Joule -- the embodied sensorimotor channel capacity (control information) per expected energetic cost over a fixed horizon. These provide two axes of physical intelligence: recognition (model-building) vs.control (action influence). Drawing on stochastic thermodynamics, we show how a Landauer-scale closed-cycle benchmark for epiplexity acquisition follows as a corollary of a standard thermodynamic-learning inequality under explicit subsystem assumptions, and we clarify how Landauer-scaled costs act as closed-cycle benchmarks under explicit reset/reuse and boundary-closure assumptions; conversely, we give a simple decoupling construction showing that without such assumptions -- and without charging for externally prepared low-entropy resources (e.g.fresh memory) crossing the boundary -- information gain and in-boundary dissipation need not be tightly linked. For empirical settings where the latent structure variable is unavailable, we align the operational notion of epiplexity with compute-bounded MDL epiplexity and recommend reporting MDL-epiplexity / compression-gain surrogates as companions. Finally, we propose a unified efficiency framework that reports both metrics together with a minimal checklist of boundary/energy accounting, coarse-graining/noise, horizon/reset, and cost conventions to reduce ambiguity and support consistent bits-per-joule comparisons, and we sketch connections to energy-adjusted scaling analyses.

</details>


### [286] [A Unified Framework for Rethinking Policy Divergence Measures in GRPO](https://arxiv.org/abs/2602.05494)
*Qingyuan Wu,Yuhui Wang,Simon Sinong Zhan,Yanning Dai,Shilong Deng,Sarra Habchi,Qi Zhu,Matthias Gallé,Chao Huang*

Main category: cs.LG

TL;DR: 本文提出了一个统一的裁剪框架，将现有RLVR方法（如GRPO）统一到广义策略散度概念下，并引入KL3估计器作为关键策略散度约束，理论上证明其等价于非对称比率裁剪，能促进更强探索并保持训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（如GRPO及其变体）通过裁剪似然比来约束策略散度以确保稳定更新，但缺乏对策略散度度量如何影响探索和性能的系统分析。需要建立一个统一框架来理解不同策略散度度量的作用，并找到更好的约束方法。

Method: 提出统一的裁剪框架，将现有方法统一到广义策略散度概念下，涵盖似然比和KL散度等度量。引入KL3估计器（KL散度的方差减少蒙特卡洛估计器）作为关键策略散度约束，理论上证明其等价于非对称比率裁剪，能将概率质量重新分配给高置信度动作。

Result: 在数学推理基准测试上的实证结果表明，将KL3估计器整合到GRPO中能同时提高训练稳定性和最终性能，验证了基于原则的策略散度约束在策略优化中的重要性。

Conclusion: 本文的统一框架为系统分析策略散度度量对探索和性能的影响提供了理论基础，KL3估计器作为策略散度约束在保持GRPO风格方法简单性的同时促进了更强探索，是RLVR方法的重要改进。

Abstract: Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.

</details>


### [287] [Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification](https://arxiv.org/abs/2602.05535)
*Tao Huang,Rui Wang,Xiaofei Liu,Yi Qin,Li Duan,Liping Jing*

Main category: cs.LG

TL;DR: EUQ提出了一种细粒度的证据不确定性量化方法，通过区分支持性和反对性证据来检测大视觉语言模型的错误行为，包括幻觉、越狱、对抗性漏洞和OOD失败。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在面对不完整或对抗性输入时会产生不可靠甚至有害的内容（幻觉、危险指令等），这些错误行为源于认知不确定性（内部知识冲突或信息缺失），而现有的不确定性量化方法仅捕捉整体认知不确定性，对此类问题的检测效果有限。

Method: 提出证据不确定性量化（EUQ）方法，将模型输出头的特征解释为支持性（正面）或反对性（负面）证据，利用证据理论对这些证据进行建模和聚合，以量化内部冲突和知识差距，只需单次前向传播即可完成。

Result: 在四种错误行为（幻觉、越狱、对抗性漏洞、OOD失败）上对最先进的LVLM进行评估，EUQ始终优于强基线方法，发现幻觉对应高内部冲突，OOD失败对应高无知度；层间证据不确定性动态分析为内部表征演化提供了新的解释视角。

Conclusion: EUQ提供了一种有效的细粒度不确定性量化方法，能够检测大视觉语言模型的错误行为，为模型安全部署提供了重要工具，代码已开源。

Abstract: Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.

</details>


### [288] [Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation](https://arxiv.org/abs/2602.05548)
*Zhiqi Yu,Zhangquan Chen,Mengting Liu,Heye Zhang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 本文提出A-GRAE方法，通过非对称优势估计解决GRPO在探索和难度适应方面的瓶颈，相比传统对称方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: RLVR（特别是GRPO）已成为引导LLM推理的标准方法，但其在探索效率和难度适应方面存在挑战。研究发现这些瓶颈源于组相对优势估计（GRAE）中隐含的优势对称性，这种对称性导致两个关键限制：在组层面阻碍探索新正确解，在样本层面无法适应非平稳的难度需求。

Method: 提出非对称GRAE（A-GRAE）方法，动态调节探索激励和样本难度关注：1）非对称抑制正确轨迹的优势以鼓励探索；2）采用课程式学习策略，初期优先简单样本，逐步转向复杂样本。

Result: 在七个基准测试上的实验表明，A-GRAE一致性地改进了GRPO及其变体，在LLM和MLLM上均表现出色。

Conclusion: 传统GRAE的对称性假设是次优的，非对称优势估计能有效提升探索效率和难度适应能力，A-GRAE为RLVR提供了更优的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.

</details>


### [289] [Logical Guidance for the Exact Composition of Diffusion Models](https://arxiv.org/abs/2602.05549)
*Francesco Alesiani,Jonathan Warrell,Tanja Bien,Henrik Christiansen,Matheus Ferraz,Mathias Niepert*

Main category: cs.LG

TL;DR: LOGDIFF是一个扩散模型引导框架，通过逻辑表达式实现精确约束生成，提出了布尔演算确保逻辑引导的精确性，并开发了混合引导方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在推理时难以处理复杂的逻辑约束生成，需要一种能够精确组合多个原子属性引导信号的框架，以实现基于逻辑表达式的可控生成。

Method: 1) 推导精确布尔演算，为逻辑引导提供充分条件；2) 开发递归算法从原子分数和后验概率计算引导信号；3) 提出混合引导方法，结合分类器引导和无分类器引导；4) 将布尔公式编译为电路表示。

Result: 在多个图像和蛋白质结构生成任务中验证了框架的有效性，能够实现基于复杂逻辑表达式的精确约束生成。

Conclusion: LOGDIFF为扩散模型提供了理论保证的精确逻辑引导框架，能够处理复杂约束，并统一了分类器引导和无分类器引导方法，在多种生成任务中表现出色。

Abstract: We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.
  We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.
  First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.
  Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.
  In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.
  Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.
  Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.
  We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.

</details>


### [290] [MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks](https://arxiv.org/abs/2602.05567)
*Long D. Nguyen,Binh P. Nguyen*

Main category: cs.LG

TL;DR: 提出消息自适应图提示调优方法，通过在消息传递步骤注入可学习提示来重新加权邻居消息并添加任务特定提示向量，保持骨干GNN冻结，在少样本和全样本设置中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 预训练图神经网络在下游任务适应中存在挑战，因为预训练目标与任务需求不匹配。现有的图提示调优方法通常只修改输入或表示，而保持消息传递不变，限制了其适应邻域交互的能力。

Method: 提出消息自适应图提示调优方法，在消息传递步骤注入可学习提示：1) 重新加权传入的邻居消息；2) 在消息聚合期间添加任务特定的提示向量。该方法保持骨干GNN冻结，兼容常见GNN骨干和预训练策略。

Result: 在多样化的节点级和图级数据集上的实验表明，在少样本设置中相比先前的图提示方法获得一致增益，在全样本机制中达到与微调竞争的性能。

Conclusion: 消息自适应图提示调优通过直接调整消息传递过程，有效解决了现有图提示方法的局限性，在保持参数效率的同时实现了更好的任务适应能力。

Abstract: Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.

</details>


### [291] [EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking](https://arxiv.org/abs/2602.05571)
*Rishabh Bhattacharya,Naresh Manwani*

Main category: cs.LG

TL;DR: 提出EdgeMask-DG*方法，通过对抗性边掩码在特征增强图上学习领域不变结构信息，提升图神经网络在结构偏移下的泛化能力


<details>
  <summary>Details</summary>
Motivation: 图结构作为协变量在不同领域间会发生变化，现有方法依赖固定的结构增强或全局扰动，无法精确识别哪些边编码了领域不变信息。作者认为领域不变结构信息并非固定在单一拓扑中，而是存在于从拓扑和特征相似性导出的多个图结构的共识中。

Method: 首先提出EdgeMask-DG，一种新颖的min-max算法：边掩码器学习在稀疏约束下找到最坏情况的连续掩码，迫使任务GNN在这些对抗性结构扰动下有效工作。然后扩展为EdgeMask-DG*，将该对抗性掩码原则应用于特征增强图，该图结合了原始拓扑和特征导出的边，使模型即使在原始拓扑嘈杂或领域特定时也能发现不变性。

Result: EdgeMask-DG*在多种图领域泛化基准测试中达到新的最先进性能，包括引文网络、社交网络和时间图。在Cora OOD基准测试中，将最坏情况领域准确率提升至78.0%，比先前最先进方法（74.2%）提高了3.8个百分点。

Conclusion: EdgeMask-DG*是首个系统地将自适应对抗性拓扑搜索与特征增强图相结合的方法，从鲁棒优化角度提供了理论依据，显著提升了图神经网络在结构偏移下的领域泛化能力。

Abstract: Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\%, a +3.8 pp improvement over the prior state of the art (74.2\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/

</details>


### [292] [OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph](https://arxiv.org/abs/2602.05576)
*Chenxi Wan,Xunkai Li,Yilong Zuo,Haokun Deng,Sihan Li,Bowen Fan,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OpenMAG是一个全面的多模态属性图基准测试，整合了6个领域的19个数据集、16种编码器、24个SOTA模型和8个下游任务，为公平评估提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 随着多模态属性图模型的快速发展，现有基准测试在领域覆盖、编码器灵活性、模型多样性和任务范围方面存在局限，需要建立严谨统一的评估标准。

Method: 构建OpenMAG基准测试，整合19个跨6个领域的数据集，支持16种静态和可训练特征编码器，实现24个SOTA模型的标准化库，支持8个下游任务。

Result: 通过系统性评估必要性、数据质量、有效性、鲁棒性和效率，得出14个关于MAG学习的基本见解，为未来发展提供指导。

Conclusion: OpenMAG为多模态属性图学习提供了全面的基准测试框架，解决了现有评估标准的不足，促进了公平比较和未来研究进展。

Abstract: Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.

</details>


### [293] [On the Superlinear Relationship between SGD Noise Covariance and Loss Landscape Curvature](https://arxiv.org/abs/2602.05600)
*Yikuan Zhang,Ning Yang,Yuhai Tu*

Main category: cs.LG

TL;DR: SGD噪声协方差矩阵C与Hessian矩阵H的关系并非简单的正比关系，而是更复杂的幂律关系C_ii ∝ H_ii^γ，其中γ在1到2之间，由每个样本Hessian谱决定。


<details>
  <summary>Details</summary>
Motivation: 先前研究通常假设Fisher信息矩阵与Hessian矩阵在负对数似然损失下等价，从而认为SGD噪声协方差C与Hessian H成正比。但这一假设仅在限制性条件下成立，而在深度神经网络中通常被违反。需要更准确地描述SGD噪声与损失函数曲率的关系。

Method: 利用最近发现的Activity-Weight Duality，推导出与具体损失函数形式无关的更一般关系：C ∝ E_p[h_p^2]，其中h_p是每个样本的Hessian，H = E_p[h_p]。理论分析表明C和H近似可交换，其对角线元素满足幂律关系C_ii ∝ H_ii^γ，其中1 ≤ γ ≤ 2。

Result: 实验验证了理论推导的幂律关系，γ指数在1到2之间，具体值由每个样本Hessian谱决定。在不同数据集、架构和损失函数上都得到了验证，为深度学习中的噪声-曲率关系提供了统一描述。

Conclusion: SGD噪声协方差与Hessian矩阵的关系比先前假设的简单正比关系更复杂，而是满足幂律关系。这一发现修正了对SGD隐式偏置的理解，为理解SGD如何偏向平坦极小值提供了更准确的理论基础。

Abstract: Stochastic Gradient Descent (SGD) introduces anisotropic noise that is correlated with the local curvature of the loss landscape, thereby biasing optimization toward flat minima. Prior work often assumes an equivalence between the Fisher Information Matrix and the Hessian for negative log-likelihood losses, leading to the claim that the SGD noise covariance $\mathbf{C}$ is proportional to the Hessian $\mathbf{H}$. We show that this assumption holds only under restrictive conditions that are typically violated in deep neural networks. Using the recently discovered Activity--Weight Duality, we find a more general relationship agnostic to the specific loss formulation, showing that $\mathbf{C} \propto \mathbb{E}_p[\mathbf{h}_p^2]$, where $\mathbf{h}_p$ denotes the per-sample Hessian with $\mathbf{H} = \mathbb{E}_p[\mathbf{h}_p]$. As a consequence, $\mathbf{C}$ and $\mathbf{H}$ commute approximately rather than coincide exactly, and their diagonal elements follow an approximate power-law relation $C_{ii} \propto H_{ii}^γ$ with a theoretically bounded exponent $1 \leq γ\leq 2$, determined by per-sample Hessian spectra. Experiments across datasets, architectures, and loss functions validate these bounds, providing a unified characterization of the noise-curvature relationship in deep learning.

</details>


### [294] [Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.05605)
*Jiaji Zhang,Hailiang Zhao,Guoxuan Zhu,Ruichao Sun,Jiaju Wu,Xinkui Zhao,Hanlin Tang,Weiyi Lu,Kan Liu,Tao Lan,Lin Qu,Shuiguang Deng*

Main category: cs.LG

TL;DR: Shiva-DiT提出了一种基于残差的可微分Top-k选择方法，解决了DiT中自注意力二次计算成本过高的问题，在保持端到端可学习性的同时实现了硬件友好的静态预算剪枝。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiTs)由于自注意力的二次计算复杂度导致计算成本过高。现有的剪枝方法无法同时满足可微分性、效率性和硬件开销所需的严格静态预算要求。

Method: 提出Shiva-DiT框架，采用基于残差的可微分Top-k选择方法，利用残差感知的直通估计器实现确定性token计数的静态编译，同时通过残差梯度估计保持端到端可学习性。还引入了上下文感知路由器和自适应比率策略来自主学习自适应剪枝调度。

Result: 在包括SD3.5在内的主流模型上实验表明，Shiva-DiT建立了新的帕累托前沿，实现了1.54倍的wall-clock加速，同时保持优于现有基线的保真度，有效消除了不规则张量开销。

Conclusion: Shiva-DiT成功解决了DiT剪枝中可微分性、效率和静态预算之间的冲突，为大规模扩散模型的高效部署提供了有效的解决方案。

Abstract: Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.

</details>


### [295] [Path-Guided Flow Matching for Dataset Distillation](https://arxiv.org/abs/2602.05616)
*Xuhui Li,Zhengquan Luo,Xiwei Liu,Yongqiang Yu,Zhiqiang Xu*

Main category: cs.LG

TL;DR: PGFM是一种基于流匹配的数据集蒸馏方法，通过ODE求解实现快速确定性合成，在潜空间学习从高斯噪声到数据分布的类别条件传输，相比扩散方法效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的数据集蒸馏方法依赖启发式指导或原型分配，存在采样耗时、轨迹不稳定等问题，特别是在强控制或低IPC下会损害下游泛化性能。

Method: 提出路径引导流匹配(PGFM)框架，在冻结VAE的潜空间进行流匹配，学习类别条件传输；开发连续路径到原型指导算法，实现ODE一致的路径控制，确保轨迹可靠到达指定原型同时保持多样性和效率。

Result: 在高分辨率基准测试中，PGFM匹配或超越了先前的扩散蒸馏方法，采样步骤更少，性能具有竞争力，效率显著提升（比扩散方法效率高7.6倍，模式覆盖率达78%）。

Conclusion: PGFM是首个基于流匹配的生成式数据集蒸馏框架，通过ODE求解实现快速确定性合成，在保持性能的同时大幅提升效率，解决了扩散方法采样慢和轨迹不稳定的问题。

Abstract: Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\times$ more efficient than the diffusion-based counterparts with 78\% mode coverage.

</details>


### [296] [Mode-Dependent Rectification for Stable PPO Training](https://arxiv.org/abs/2602.05619)
*Mohamad Mohamad,Francesco Ponzio,Xavier Descombes*

Main category: cs.LG

TL;DR: 论文提出Mode-Dependent Rectification (MDR)方法，解决PPO算法中Batch Normalization等模式相关层导致的训练不稳定问题


<details>
  <summary>Details</summary>
Motivation: 在视觉强化学习中，Batch Normalization和dropout等模式相关层（训练和评估时行为不同的层）虽然常用，但会破坏on-policy优化的稳定性。在PPO算法中，这些层导致的训练与评估行为差异会引起策略不匹配、分布漂移和奖励崩溃

Method: 提出Mode-Dependent Rectification (MDR)，一种轻量级的双阶段训练过程，在不改变架构的情况下稳定PPO在模式相关层下的训练

Result: 在程序生成游戏和真实世界补丁定位任务上的实验表明，MDR能持续提升稳定性和性能，并能自然扩展到其他模式相关层

Conclusion: MDR有效解决了PPO算法中模式相关层导致的训练不稳定问题，为视觉强化学习中的架构设计提供了更稳定的训练方案

Abstract: Mode-dependent architectural components (layers that behave differently during training and evaluation, such as Batch Normalization or dropout) are commonly used in visual reinforcement learning but can destabilize on-policy optimization. We show that in Proximal Policy Optimization (PPO), discrepancies between training and evaluation behavior induced by Batch Normalization lead to policy mismatch, distributional drift, and reward collapse. We propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO under mode-dependent layers without architectural changes. Experiments across procedurally generated games and real-world patch-localization tasks demonstrate that MDR consistently improves stability and performance, and extends naturally to other mode-dependent layers.

</details>


### [297] [Structural Disentanglement in Bilinear MLPs via Architectural Inductive Bias](https://arxiv.org/abs/2602.05635)
*Ojasva Nema,Kaustubh Sharma,Aditya Chauhan,Parikshit Pareek*

Main category: cs.LG

TL;DR: 论文提出双线性MLP架构能通过乘法交互实现结构解耦，为选择性遗忘和长时外推提供数学基础，相比点式非线性网络能更好地恢复底层代数结构。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在选择性遗忘和长时外推方面仍然脆弱，即使任务具有底层代数结构。作者认为这些失败不仅源于优化或遗忘算法，更源于模型在训练过程中如何构建内部表示。

Method: 探索将显式乘法交互作为架构归纳偏置是否有助于结构解耦，通过双线性MLP实现。分析证明双线性参数化在梯度流条件下具有"非混合"特性，功能组件分离为正交子空间表示。

Result: 通过模块算术、循环推理、李群动力学和针对性遗忘基准的一系列受控实验验证了假设。乘法架构能够恢复与底层代数结构对齐的真实算子，而点式非线性网络则不能。

Conclusion: 模型可编辑性和泛化能力受表示结构约束，架构归纳偏置在实现可靠遗忘中起核心作用。双线性MLP为外科手术式模型修改提供了数学基础。

Abstract: Selective unlearning and long-horizon extrapolation remain fragile in modern neural networks, even when tasks have underlying algebraic structure. In this work, we argue that these failures arise not solely from optimization or unlearning algorithms, but from how models structure their internal representations during training. We explore if having explicit multiplicative interactions as an architectural inductive bias helps in structural disentanglement, through Bilinear MLPs. We show analytically that bilinear parameterizations possess a `non-mixing' property under gradient flow conditions, where functional components separate into orthogonal subspace representations. This provides a mathematical foundation for surgical model modification. We validate this hypothesis through a series of controlled experiments spanning modular arithmetic, cyclic reasoning, Lie group dynamics, and targeted unlearning benchmarks. Unlike pointwise nonlinear networks, multiplicative architectures are able to recover true operators aligned with the underlying algebraic structure. Our results suggest that model editability and generalization are constrained by representational structure, and that architectural inductive bias plays a central role in enabling reliable unlearning.

</details>


### [298] [Empowering Time Series Analysis with Large-Scale Multimodal Pretraining](https://arxiv.org/abs/2602.05646)
*Peng Chen,Siyuan Wang,Shiyan Hu,Xingjian Wu,Yang Shu,Zhongwen Rao,Meng Wang,Yijie Li,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: HORAI是一个频率增强的多模态时间序列基础模型，通过整合内生模态（图像、文本）和外生知识（新闻），在首个大规模多模态时间序列数据集MM-TS上预训练，在零样本预测和异常检测任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型主要依赖单模态预训练，缺乏互补模态来增强理解。构建多模态基础模型面临两大挑战：1）缺乏统一的多模态预训练范式和大规模多模态语料库；2）如何有效整合异构模态并增强模型泛化能力。

Method: 提出多模态预训练范式，整合时间序列的内生模态（衍生图像和文本）和外生知识（真实世界新闻）。开发自动化数据构建流程创建MM-TS数据集（6个领域，10亿数据点）。提出HORAI模型，包含频率增强跨模态编码器和时频解码器，有效融合多模态特征并增强跨模态和跨领域泛化能力。

Result: 在MM-TS数据集上预训练后，HORAI在时间序列预测和异常检测任务上实现了最先进的零样本性能，展示了强大的泛化能力。

Conclusion: 该研究为时间序列分析的多模态基础模型迈出了早期探索步伐，通过创新的预训练范式、大规模数据集和频率增强模型架构，成功解决了多模态时间序列分析的挑战，为未来研究奠定了基础。

Abstract: While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.

</details>


### [299] [End-to-End Compression for Tabular Foundation Models](https://arxiv.org/abs/2602.05649)
*Guri Zabërgja,Rafiq Kamel,Arlind Kadra,Christian M. M. Frey,Josif Grabocka*

Main category: cs.LG

TL;DR: TACO是一个端到端的表格压缩模型，通过在潜在空间压缩训练数据集，显著降低推理时间和内存消耗，同时保持性能不显著下降。


<details>
  <summary>Details</summary>
Motivation: 传统的梯度提升决策树在表格数据上的主导地位最近受到上下文学习表格基础模型的挑战。然而，这些基于注意力机制的Transformer架构具有关于数据集大小的二次复杂度，增加了训练和推理时间开销，限制了处理大规模数据集的能力。

Method: 提出TACO（端到端表格压缩模型），在潜在空间中压缩训练数据集，从而减少计算和内存需求。

Result: 在TabArena基准测试中，TACO的推理时间比最先进的表格Transformer架构快94倍，内存消耗减少97%，同时性能没有显著下降。随着数据集规模增大，该方法不仅扩展性更好，而且性能优于其他基线方法。

Conclusion: TACO通过压缩训练数据集在潜在空间中，有效解决了表格Transformer架构的二次复杂度问题，实现了高效的大规模表格数据处理，在保持性能的同时显著提升了计算效率。

Abstract: The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.

</details>


### [300] [Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation](https://arxiv.org/abs/2602.05656)
*Igor Santos-Grueiro*

Main category: cs.LG

TL;DR: 行为评估无法唯一识别潜在对齐：即使理想的行为评估也不能证明模型的内在对齐属性，只能估计不可区分性类别


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐评估主要依赖行为测试，但将观察到的合规性推断为潜在对齐属性的推理过程通常隐含且未经过正式分析。作者旨在形式化研究这一从行为证据到潜在对齐属性的推理问题。

Method: 将对齐评估形式化为部分可观测下的可识别性问题，引入"对齐可验证性问题"和"规范性不可区分性"概念，研究在有限行为评估和评估感知代理条件下，行为合规性是否能唯一识别潜在对齐。

Result: 主要结果是负面的可识别性定理：在有限行为评估和评估感知代理条件下，观察到的行为合规性不能唯一识别潜在对齐。即使理想的行为评估也无法一般性地证明对齐作为潜在属性。

Conclusion: 行为对齐测试应解释为不可区分性类别的估计器而非对齐的验证器。对齐基准提供的是特定机制内可观测合规性的上界，而非底层对齐的保证。这重新框架了对齐评估的解读方式。

Abstract: Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.
  We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.
  Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.
  We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.

</details>


### [301] [Probabilistic Multi-Regional Solar Power Forecasting with Any-Quantile Recurrent Neural Networks](https://arxiv.org/abs/2602.05660)
*Slawek Smyl,Paweł Pełka,Grzegorz Dudek*

Main category: cs.LG

TL;DR: 提出基于AQ-RNN的多区域光伏功率任意分位数概率预测框架，通过双轨循环架构处理序列特定和跨区域信息，在单一模型中实现任意概率水平的分位数估计。


<details>
  <summary>Details</summary>
Motivation: 光伏发电渗透率增加给电力系统运行带来显著不确定性，需要超越确定性点预测的预测方法，以支持不确定性感知的能源管理和运行决策。

Method: 基于AQ-RNN的任意分位数概率预测框架，采用双轨循环架构联合处理序列特定和跨区域上下文信息，结合扩张循环单元、基于补丁的时间建模和动态集成机制。

Result: 使用欧洲259个区域30年小时级光伏发电数据进行评估，相比现有统计和神经概率基准方法，在预测精度、校准度和预测区间质量方面均取得一致改进。

Conclusion: 该方法适用于可再生能源主导的电力系统中的不确定性感知能源管理和运行决策，能有效利用空间依赖性增强系统级鲁棒性。

Abstract: The increasing penetration of photovoltaic (PV) generation introduces significant uncertainty into power system operation, necessitating forecasting approaches that extend beyond deterministic point predictions. This paper proposes an any-quantile probabilistic forecasting framework for multi-regional PV power generation based on the Any-Quantile Recurrent Neural Network (AQ-RNN). The model integrates an any-quantile forecasting paradigm with a dual-track recurrent architecture that jointly processes series-specific and cross-regional contextual information, supported by dilated recurrent cells, patch-based temporal modeling, and a dynamic ensemble mechanism.
  The proposed framework enables the estimation of calibrated conditional quantiles at arbitrary probability levels within a single trained model and effectively exploits spatial dependencies to enhance robustness at the system level. The approach is evaluated using 30 years of hourly PV generation data from 259 European regions and compared against established statistical and neural probabilistic baselines. The results demonstrate consistent improvements in forecast accuracy, calibration, and prediction interval quality, underscoring the suitability of the proposed method for uncertainty-aware energy management and operational decision-making in renewable-dominated power systems.

</details>


### [302] [Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection](https://arxiv.org/abs/2602.05667)
*Ling Zhan,Zhen Li,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: 提出SCLCS方法，通过选择代表性核心集来保留FC建模方法的性能排名，解决大规模fMRI数据上功能连接性基准测试的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大规模fMRI数据集上数百种功能连接性建模方法的基准测试对可重复神经科学至关重要，但模型-数据组合的指数级增长使得穷举评估计算上不可行，阻碍了这种评估成为常规的预分析步骤。

Method: 提出SCLCS（Structure-aware Contrastive Learning for Core-set Selection）自监督框架：1）使用自适应Transformer学习每个样本的独特FC结构；2）引入结构扰动分数量化训练期间学习结构的稳定性，识别代表基础连接原型的样本；3）通过密度平衡采样策略确保最终核心集既结构稳健又分布代表性。

Result: 在REST-meta-MDD数据集上，SCLCS仅使用10%的数据就能保留真实模型排名，在排名一致性（nDCG@k）上比最先进的核心集选择方法高出23.2%。

Conclusion: 这是首个将核心集选择形式化用于FC操作符基准测试的工作，使大规模操作符比较成为计算神经科学中可行且不可或缺的部分。

Abstract: Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS

</details>


### [303] [Stable but Wrong: When More Data Degrades Scientific Conclusions](https://arxiv.org/abs/2602.05668)
*Zhipeng Zhang,Kai Li*

Main category: cs.LG

TL;DR: 论文揭示了一个根本性悖论：即使推理过程稳定收敛、校准良好且通过常规诊断检查，仍可能系统性地得出错误结论，因为观测可靠性下降在推理过程中本质上是不可观测的。


<details>
  <summary>Details</summary>
Motivation: 现代科学依赖不断增长的观测数据集和自动化推理流程，隐含假设更多数据会使科学结论更可靠。本文旨在挑战这一假设，揭示在某些结构机制下，即使推理过程表现正常，仍可能得出系统性错误结论。

Method: 通过最小化合成实验，识别并分析一个特定的结构机制：当观测可靠性以推理过程本身无法观测的方式下降时，标准推理程序会出现系统性偏差。

Result: 在该机制下，增加数据不仅不会纠正错误，反而会放大错误；同时基于残差和拟合优度的诊断检查仍会误导性地显示正常。推理过程的稳定性、收敛性和置信度不能保证认知有效性。

Conclusion: 数据驱动科学存在内在限制：推理不能仅视为数据可用性的无条件结果，而必须受到观测过程完整性的明确约束。科学推断需要超越单纯的数据积累，关注观测过程的本质可靠性。

Abstract: Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.

</details>


### [304] [Perception-Based Beliefs for POMDPs with Visual Observations](https://arxiv.org/abs/2602.05679)
*Miriam Schäfers,Merlijn Krale,Thiago D. Simão,Nils Jansen,Maximilian Weininger*

Main category: cs.LG

TL;DR: 提出PBP框架，通过感知模型将视觉观测映射到状态分布，避免传统POMDP求解器直接处理高维观测空间的问题


<details>
  <summary>Details</summary>
Motivation: 传统POMDP求解器在处理高维观测（如相机图像）时计算困难，需要一种能有效处理视觉观测的方法

Method: 引入PBP框架，使用图像分类器将视觉观测映射到状态概率分布，并直接整合到信念更新中；针对分类器不精确问题，引入不确定性量化方法

Result: PBP在性能上优于现有端到端深度RL方法；不确定性量化提高了PBP对视觉损坏的鲁棒性

Conclusion: PBP框架有效解决了POMDP中高维视觉观测的处理问题，通过感知模型和不确定性量化提高了求解效率和鲁棒性

Abstract: Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.

</details>


### [305] [Mining Generalizable Activation Functions](https://arxiv.org/abs/2602.05688)
*Alex Vitvitskyi,Michael Boratko,Matej Grcic,Razvan Pascanu,Deep Shah,Petar Veličković*

Main category: cs.LG

TL;DR: 本文提出使用AlphaEvolve进化搜索框架发现新的激活函数，该框架利用前沿LLM作为变异算子，能在更大搜索空间中发现具有特定归纳偏置的激活函数。


<details>
  <summary>Details</summary>
Motivation: 激活函数选择对神经网络优化和归纳偏置有重要影响，传统方法搜索空间有限且需要手动设计。本文旨在利用现代进化搜索框架发现更优激活函数。

Method: 使用AlphaEvolve框架，该框架依赖前沿LLM作为变异算子，能在特定FLOP预算内搜索所有可能的Python函数。通过分布外数据性能作为适应度函数，寻找具有特定归纳偏置的激活函数。

Result: 实证研究表明，相对小规模的合成数据集足以让AlphaEvolve发现有意义的激活函数，验证了该框架的有效性。

Conclusion: 进化搜索框架特别是AlphaEvolve，为发现新的激活函数提供了有效途径，不仅能提升性能，还能针对特定归纳偏置进行优化，且搜索过程更高效。

Abstract: The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.

</details>


### [306] [Almost Asymptotically Optimal Active Clustering Through Pairwise Observations](https://arxiv.org/abs/2602.05690)
*Rachel S. Y. Teo,P. N. Karthik,Ramya Korlakai Vinayak,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出一个主动聚类框架，通过噪声二元反馈查询物品对，建立查询次数下界并设计渐近最优算法


<details>
  <summary>Details</summary>
Motivation: 在噪声环境下主动收集反馈进行聚类，需要解决未知聚类数量和噪声观测的挑战，建立理论下界并设计实用算法

Method: 使用测度变换技术建立查询次数下界，设计基于广义似然比统计量的渐近最优算法，开发计算可行的GLR变体

Result: 建立了聚类准确度置信水平所需查询次数的基本下界，设计了性能与下界保持常数倍差距的实用算法

Conclusion: 提出了理论完备的主动聚类框架，将下界分析转化为优化问题，并开发了计算可行的渐近最优算法

Abstract: We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.

</details>


### [307] [FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning](https://arxiv.org/abs/2602.05693)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: FedRandom 提出了一种新的联邦学习贡献评估方法，通过随机化技术解决现有方法的不稳定性问题，显著提高了参与者贡献评估的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，公平评估参与者的贡献至关重要，这关系到参与者的激励、恶意行为检测和免费搭车者识别。然而，现有贡献评估方法存在显著的不稳定性，这种不稳定性会影响参与者参与联邦的意愿。

Method: FedRandom 将贡献评估不稳定性视为统计估计问题，通过随机化技术生成比常规联邦学习策略更多的样本。这些额外样本能够提供更一致、更可靠的参与者贡献评估。

Result: 在 CIFAR-10、MNIST、CIFAR-100 和 FMNIST 数据集上的实验表明，FedRandom 在一半的评估场景中将与真实值的整体距离减少了超过三分之一，并在超过90%的情况下提高了稳定性。

Conclusion: FedRandom 有效缓解了联邦学习中贡献评估的不稳定性问题，为参与者贡献的公平评估提供了更可靠的方法，有助于提高参与者参与联邦学习的积极性。

Abstract: Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.

</details>


### [308] [Fix Representation (Optimally) Before Fairness: Finite-Sample Shrinkage Population Correction and the True Price of Fairness Under Subpopulation Shift](https://arxiv.org/abs/2602.05707)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 论文提出在子群体分布偏移下，公平性干预可能因训练数据中群体比例失准而产生虚假的公平-准确性权衡，建议采用收缩加权修正基线来揭示真实的公平-效用边界。


<details>
  <summary>Details</summary>
Motivation: 机器学习实践中常观察到预测准确性与群体公平性之间的张力，但有时公平性干预似乎能提高准确性。作者发现这两种现象都可能是训练数据中群体比例失准的产物，需要建立正确的评估协议来揭示真实的公平-效用权衡。

Method: 在子群体分布偏移（组内分布稳定，群体比例偏移）设定下，提出收缩加权方法，在目标分布和训练分布之间进行插值，作为最优的有限样本修正。建立评估协议：先（最优地）修正表示，再比较公平性干预与收缩加权修正基线的差异。

Result: 理论分析表明：完全重要性加权修正是渐近无偏但有限样本次优的；最优有限样本修正是收缩加权；"公平性有助于准确性"的现象可能源于与不当加权基线的比较。在合成数据和真实基准（Adult、COMPAS）上的实验验证了理论预测，该协议能消除虚假权衡，揭示真实的公平-效用边界。

Conclusion: 公平性与准确性之间的表面张力常源于训练数据中群体比例失准。通过采用收缩加权修正基线，可以隔离公平性的真实、不可约代价，为公平机器学习提供更可靠的评估框架。

Abstract: Machine learning practitioners frequently observe tension between predictive accuracy and group fairness constraints -- yet sometimes fairness interventions appear to improve accuracy. We show that both phenomena can be artifacts of training data that misrepresents subgroup proportions. Under subpopulation shift (stable within-group distributions, shifted group proportions), we establish: (i) full importance-weighted correction is asymptotically unbiased but finite-sample suboptimal; (ii) the optimal finite-sample correction is a shrinkage reweighting that interpolates between target and training mixtures; (iii) apparent "fairness helps accuracy" can arise from comparing fairness methods to an improperly-weighted baseline. We provide an actionable evaluation protocol: fix representation (optimally) before fairness -- compare fairness interventions against a shrinkage-corrected baseline to isolate the true, irreducible price of fairness. Experiments on synthetic and real-world benchmarks (Adult, COMPAS) validate our theoretical predictions and demonstrate that this protocol eliminates spurious tradeoffs, revealing the genuine fairness-utility frontier.

</details>


### [309] [Projected Boosting with Fairness Constraints: Quantifying the Cost of Fair Training Distributions](https://arxiv.org/abs/2602.05713)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: FairBoost：一种在Boosting中融入群体公平性约束的方法，通过将指数权重分布投影到满足公平性约束的凸集上，在保持可分析训练动态的同时实现公平性


<details>
  <summary>Details</summary>
Motivation: 传统Boosting算法（如AdaBoost）在理论上有很强的保证，但缺乏对群体公平性的考虑。如何在保持Boosting可分析训练动态的同时融入公平性约束是一个重要问题

Method: FairBoost方法将集成学习诱导的指数权重分布投影到满足公平性约束的凸分布集上（作为重加权替代），然后在这个公平分布上训练弱学习器。关键理论洞察是：投影训练分布会减少弱学习器的有效边缘，减少量由投影的KL散度控制

Result: 证明了指数损失界限，其中收敛速率取决于弱学习器边缘减去"公平性成本"项δ_t = √(KL(w^t∥q^t)/2)。这直接量化了Boosting动态中的准确性-公平性权衡。在标准基准测试上的实验验证了理论预测，并展示了具有稳定训练曲线的竞争性公平性-准确性权衡

Conclusion: FairBoost成功地将群体公平性约束融入Boosting框架，同时保持了可分析的理论保证。该方法通过KL散度量化的公平性成本直接揭示了准确性-公平性的权衡关系，为公平机器学习提供了理论指导

Abstract: Boosting algorithms enjoy strong theoretical guarantees: when weak learners maintain positive edge, AdaBoost achieves geometric decrease of exponential loss. We study how to incorporate group fairness constraints into boosting while preserving analyzable training dynamics. Our approach, FairBoost, projects the ensemble-induced exponential-weights distribution onto a convex set of distributions satisfying fairness constraints (as a reweighting surrogate), then trains weak learners on this fair distribution. The key theoretical insight is that projecting the training distribution reduces the effective edge of weak learners by a quantity controlled by the KL-divergence of the projection. We prove an exponential-loss bound where the convergence rate depends on weak learner edge minus a "fairness cost" term $δ_t = \sqrt{\mathrm{KL}(w^t \| q^t)/2}$. This directly quantifies the accuracy-fairness tradeoff in boosting dynamics. Experiments on standard benchmarks validate the theoretical predictions and demonstrate competitive fairness-accuracy tradeoffs with stable training curves.

</details>


### [310] [CSRv2: Unlocking Ultra-Sparse Embeddings](https://arxiv.org/abs/2602.05735)
*Lixuan Guo,Yifei Wang,Tiansheng Wen,Yifan Wang,Aosong Feng,Bo Chen,Stefanie Jegelka,Chenyu You*

Main category: cs.LG

TL;DR: CSRv2是一种改进的超稀疏嵌入训练方法，通过渐进k退火、监督对比目标和全骨干微调，将死神经元从80%降至20%，在k=2时实现14%精度提升，使超稀疏嵌入达到与CSR(k=8)和MRL(32维)相当的性能，同时提供7倍加速和300倍计算内存效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有对比稀疏表示(CSR)方法在超稀疏区域（超过80%神经元不活跃）性能严重下降，无法充分发挥稀疏嵌入的效率潜力。需要一种方法使超稀疏嵌入在实际应用中可行。

Method: CSRv2采用三个关键技术：1) 渐进k退火稳定稀疏学习；2) 监督对比目标增强表示质量；3) 全骨干微调确保端到端适应性。这些方法共同解决了超稀疏嵌入中的死神经元问题。

Result: CSRv2将死神经元从80%降至20%，在k=2时获得14%精度提升。在文本和视觉任务中，CSRv2在k=4时比CSR提升7%/4%，在k=2时提升14%/6%。相比MRL提供7倍加速，相比密集嵌入实现300倍计算和内存效率提升。

Conclusion: CSRv2使超稀疏嵌入变得实用而不牺牲性能，为实时和边缘部署的AI系统拓宽了设计空间，在嵌入质量和效率都至关重要的场景中具有重要应用价值。

Abstract: In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.

</details>


### [311] [Learning to Inject: Automated Prompt Injection via Reinforcement Learning](https://arxiv.org/abs/2602.05746)
*Xin Chen,Jie Zhang,Florian Tramer*

Main category: cs.LG

TL;DR: AutoInject是一个基于强化学习的框架，用于自动生成通用、可转移的对抗性后缀，以优化LLM代理中的提示注入攻击，成功攻破多个前沿模型。


<details>
  <summary>Details</summary>
Motivation: 提示注入是LLM代理中最关键的漏洞之一，但现有方法严重依赖人工红队和手动制作的提示，限制了可扩展性和适应性，需要更有效的自动化攻击方法。

Method: 提出AutoInject强化学习框架，生成通用、可转移的对抗性后缀，联合优化攻击成功率和良性任务上的效用保持，支持基于查询的优化和向未见模型和任务的转移攻击。

Result: 仅使用1.5B参数的对抗性后缀生成器，成功攻破了GPT 5 Nano、Claude Sonnet 3.5和Gemini 2.5 Flash等前沿系统，在AgentDojo基准测试中建立了更强的自动化提示注入研究基线。

Conclusion: AutoInject为自动化提示注入攻击提供了有效的强化学习框架，展示了在攻击成功率和效用保持方面的优化能力，为LLM代理安全研究建立了新的基准。

Abstract: Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.

</details>


### [312] [How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning](https://arxiv.org/abs/2602.05749)
*Kai Ming Ting,Wei-Jie Xu,Hang Zhang*

Main category: cs.LG

TL;DR: 该论文质疑深度聚类相对于k-means的优势，发现深度嵌入聚类（DEC）未能克服k-means的基本限制，而非深度学习方法通过利用数据分布信息能更好地解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 深度聚类常声称比k-means有优势，但这种优势通常只在图像数据集上验证，不清楚是否真正解决了k-means的根本限制。作者想要验证深度嵌入聚类（DEC）是否真的克服了k-means无法发现任意形状、不同大小和密度集群的问题。

Method: 通过分析深度嵌入聚类（DEC）方法，该方法使用自编码器学习潜在表示，然后进行类似k-means的聚类，以端到端方式优化。同时研究非深度学习方法如何利用数据分布信息来解决k-means的根本限制。

Result: 研究发现DEC未能克服k-means的基本限制，即无法发现任意形状、不同大小和密度的集群。相反，非深度学习方法通过利用集群的分布信息，能够有效解决这些根本限制，达到深度聚类想要实现的目标。

Conclusion: 深度聚类方法（特别是DEC）并未真正解决k-means的根本限制，因为它们没有利用底层数据分布信息。非深度学习方法通过利用分布信息反而能更好地实现深度聚类的目标，这对深度聚类方法的有效性提出了重要质疑。

Abstract: Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.

</details>


### [313] [Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance](https://arxiv.org/abs/2602.05774)
*Xiandong Zou,Jianshu Li,Jing Huang,Pan Zhou*

Main category: cs.LG

TL;DR: VSD通过变分推断优化草稿路径训练，提升推测解码效率，相比现有方法获得显著加速


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在训练-解码不一致问题：训练优化单条贪婪轨迹，但解码需要验证和排序多条采样草稿路径

Method: 提出变分推测解码(VSD)，将草稿训练建模为潜在提案（草稿路径）的变分推断，最大化目标模型接受概率，使用EM算法优化，E步从后验采样，M步使用自适应拒绝加权和置信感知正则化

Result: VSD在LLM和MLLM上实验显示，相比EAGLE-3获得9.6%加速，相比ViSpec获得7.9%加速，显著提升解码效率

Conclusion: VSD通过变分推断框架解决训练-解码不一致问题，理论分析证实能增加预期接受长度和加速比，实验证明优于现有方法

Abstract: Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.

</details>


### [314] [Cross-Domain Offline Policy Adaptation via Selective Transition Correction](https://arxiv.org/abs/2602.05776)
*Mengbei Yan,Jiafei Lyu,Shengjie Sun,Zhongjian Qiao,Jingwen Yang,Zichuan Lin,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: 提出STC算法，通过修正源域数据的动作和奖励来适应目标域动态，实现跨域离线强化学习


<details>
  <summary>Details</summary>
Motivation: 跨域离线强化学习中，直接合并源域和目标域数据集会因动态不匹配导致性能下降，现有方法通过过滤或奖励修改可能无法充分利用源域数据价值

Method: 提出选择性转移修正（STC）算法：1）使用逆策略模型和奖励模型修正源域转移的动作和奖励；2）使用前向动态模型筛选修正后更匹配目标动态的样本

Result: 在多种动态变化环境中的实验表明，STC相比现有基线方法取得了优越的性能

Conclusion: 通过修正源域数据使其与目标域动态对齐，STC算法能够可靠地利用源域数据进行策略适应，有效解决跨域离线强化学习中的动态不匹配问题

Abstract: It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.

</details>


### [315] [How Controlling the Variance can Improve Training Stability of Sparsely Activated DNNs and CNNs](https://arxiv.org/abs/2602.05779)
*Emily Dent,Jared Tanner*

Main category: cs.LG

TL;DR: 研究发现高斯过程方差对稀疏激活网络训练至关重要，较大方差能实现90%激活稀疏度同时保持高精度，有望降低全连接层能耗。


<details>
  <summary>Details</summary>
Motivation: 探索深度网络中高斯过程方差对训练的影响，特别是对于使用稀疏诱导激活函数（如CReLU）的网络，旨在提高表达能力和训练稳定性，同时实现高激活稀疏度以降低能耗。

Method: 采用Edge-of-Chaos初始化策略，分析高斯过程方差对网络训练的影响，使用CReLU激活函数，在DNN和CNN中测试不同方差下的激活稀疏度和训练表现。

Result: 较大高斯过程方差能实现高达90%的激活稀疏度，同时保持或接近完全精度，显著提高训练稳定性，在DNN和CNN中均表现良好。

Conclusion: 高斯过程方差是影响稀疏激活网络性能的关键因素，通过优化方差可实现高激活稀疏度而不损失精度，为降低全连接层能耗提供了有前景的机制。

Abstract: The intermediate layers of deep networks can be characterised as a Gaussian process, in particular the Edge-of-Chaos (EoC) initialisation strategy prescribes the limiting covariance matrix of the Gaussian process. Here we show that the under-utilised chosen variance of the Gaussian process is important in the training of deep networks with sparsity inducing activation, such as a shifted and clipped ReLU, $\text{CReLU}_{τ,m}(x)=\min(\max(x-τ,0),m)$. Specifically, initialisations leading to larger fixed Gaussian process variances, allow for improved expressivity with activation sparsity as large as 90% in DNNs and CNNs, and generally improve the stability of the training process. Enabling full, or near full, accuracy at such high levels of sparsity in the hidden layers suggests a promising mechanism to reduce the energy consumption of machine learning models involving fully connected layers.

</details>


### [316] [Distributional Reinforcement Learning with Diffusion Bridge Critics](https://arxiv.org/abs/2602.05783)
*Shutong Ding,Yimiao Zhou,Ke Hu,Mokai Pan,Shan Zhong,Yanwei Fu,Jingya Wang,Ye Shi*

Main category: cs.LG

TL;DR: 提出首个基于扩散桥的分布强化学习方法DBC，直接建模Q值的逆累积分布函数，提升价值估计准确性，可即插即用到现有RL框架中。


<details>
  <summary>Details</summary>
Motivation: 现有扩散强化学习方法主要关注扩散策略，而忽略了扩散批评器。由于策略优化依赖于批评器，准确的价值估计比策略表达能力更重要。此外，大多数RL任务具有随机性，批评器更适合用分布模型描述。

Method: 提出扩散桥批评器(DBC)，直接建模Q值的逆累积分布函数，利用扩散桥的强大分布匹配能力准确捕捉价值分布并防止坍缩为平凡高斯分布。进一步推导解析积分公式处理DBC中的离散化误差。

Result: 在MuJoCo机器人控制基准测试中，DBC相比之前的分布批评器模型表现出优越性能。

Conclusion: DBC是首个采用扩散桥模型作为批评器的工作，是一个即插即用组件，可集成到大多数现有RL框架中，为分布强化学习提供了新的有效方法。

Abstract: Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.

</details>


### [317] [Classification Under Local Differential Privacy with Model Reversal and Model Averaging](https://arxiv.org/abs/2602.05797)
*Caihong Qin,Yang Bai*

Main category: cs.LG

TL;DR: 该论文将LDP下的隐私学习重新解释为迁移学习问题，提出噪声二进制反馈评估、模型反转和模型平均三种技术，在不损害隐私的前提下显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 本地差分隐私（LDP）虽然提供了强大的隐私保护，但引入的噪声会显著降低数据效用。现有方法在保持隐私的同时难以维持良好的分类性能，需要新的技术来解决这一矛盾。

Method: 1) 噪声二进制反馈评估机制：估计数据集效用的新方法；2) 模型反转：通过反转决策边界来挽救性能不佳的分类器；3) 模型平均：基于估计的效用为多个反转分类器分配权重。

Result: 提供了LDP下的理论超额风险界限，并展示了方法如何降低这种风险。在模拟和真实数据集上的实验结果显示分类准确率有显著提升。

Conclusion: 通过将LDP隐私学习重新框架为迁移学习问题，并开发专门针对LDP的技术，可以在不损害隐私保证的情况下显著提高分类性能，为解决LDP中的数据效用问题提供了有效途径。

Abstract: Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.

</details>


### [318] [Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents](https://arxiv.org/abs/2602.05810)
*Quan M. Tran,Zhuo Huang,Wenbin Zhang,Bo Han,Koji Yatani,Masashi Sugiyama,Tongliang Liu*

Main category: cs.LG

TL;DR: Bifrost提出了一种无需训练的方法，通过揭示上下文与轨迹之间的相关性，利用上下文差异来精确指导先前解决的任务轨迹适应新任务，从而有效利用过往经验。


<details>
  <summary>Details</summary>
Motivation: 现有自主代理在跨任务迁移时面临上下文不匹配问题，要么丢弃成功轨迹，要么使用启发式方法处理轨迹，导致微调成本高或性能不稳定。需要一种能有效利用过往经验的方法来弥合上下文差距。

Method: Bifrost基于发现的上下文-轨迹相关性（上下文变化与轨迹变化高度平行），在表示层面使用代理隐藏状态进行轨迹适应，通过上下文差异精确指导先前轨迹向目标任务的转化，确保轨迹变换在共享空间中与目标上下文对齐。

Result: 在多样化基准测试中，Bifrost持续优于现有的轨迹重用和微调自改进方法，证明代理能够有效利用过往经验，即使存在显著的上下文变化。

Conclusion: 通过揭示上下文-轨迹相关性并开发基于表示层面的轨迹适应方法，Bifrost为自主代理提供了一种无需训练的有效轨迹重用机制，显著提升了跨任务迁移的性能。

Abstract: Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.

</details>


### [319] [Synthesizing Realistic Test Data without Breaking Privacy](https://arxiv.org/abs/2602.05833)
*Laura Plein,Alexi Turcotte,Arina Hallemans,Andreas Zeller*

Main category: cs.LG

TL;DR: 提出一种基于模糊测试和判别器的隐私保护合成数据生成方法，间接利用原始数据生成具有相同统计特性的测试数据集


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的合成数据生成方法存在准确度不足或易受成员推理攻击、数据集重建攻击的问题，需要一种既能保持数据统计特性又能保护隐私的合成数据生成方法

Method: 采用受GAN启发的生成-判别框架：使用模糊测试器根据输入规范生成测试数据，保留原始数据的约束；判别器评估生成数据与原始数据的接近程度；通过演化样本和判别器筛选"好样本"来生成隐私保护数据

Result: 在四个用于评估最先进技术的数据集上进行了实验，结果显示该方法在生成具有高实用性的合成数据集的同时能有效保护隐私

Conclusion: 该方法展示了生成既保持高实用性又能保护隐私的合成数据集的潜力，通过间接利用原始数据避免了直接训练带来的隐私风险

Abstract: There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining "good samples" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.

</details>


### [320] [Visualizing the loss landscapes of physics-informed neural networks](https://arxiv.org/abs/2602.05849)
*Conor Rowan,Finn Murphy-Blanchard*

Main category: cs.LG

TL;DR: 该论文将损失函数景观分析从传统图像分类扩展到物理信息机器学习，发现物理损失函数（Deep Ritz和强形式）的景观具有平滑、条件良好、局部凸等特性，挑战了关于物理信息网络损失复杂性的普遍直觉。


<details>
  <summary>Details</summary>
Motivation: 现有损失函数景观研究主要局限于图像分类领域，而在物理信息机器学习中，损失函数由微分算子而非大数据回归定义，缺乏相应的景观可视化研究。本文旨在将损失函数景观视角引入科学机器学习社区，并比较不同物理损失形式。

Method: 首先全面回顾损失函数景观文献和现有物理信息相关研究，然后使用多种调查技术对Deep Ritz形式和平方残差形式的物理损失函数进行实证研究，分析其景观特性。

Result: 发现物理信息神经网络的损失函数景观具有与数据驱动分类问题相似的特性：平滑、条件良好、在解附近呈现凸性。出乎意料的是，两种物理损失形式（Deep Ritz和强形式）通常产生相似的景观。

Conclusion: 本文成功将损失函数景观分析扩展到物理信息机器学习领域，发现物理损失函数景观比预期更简单，挑战了关于其复杂性的普遍直觉，为科学机器学习社区提供了新的分析视角。

Abstract: Training a neural network requires navigating a high-dimensional, non-convex loss surface to find parameters that minimize this loss. In many ways, it is surprising that optimizers such as stochastic gradient descent and ADAM can reliably locate minima which perform well on both the training and test data. To understand the success of training, a "loss landscape" community has emerged to study the geometry of the loss function and the dynamics of optimization, often using visualization techniques. However, these loss landscape studies have mostly been limited to machine learning for image classification. In the newer field of physics-informed machine learning, little work has been conducted to visualize the landscapes of losses defined not by regression to large data sets, but by differential operators acting on state fields discretized by neural networks. In this work, we provide a comprehensive review of the loss landscape literature, as well as a discussion of the few existing physics-informed works which investigate the loss landscape. We then use a number of the techniques we survey to empirically investigate the landscapes defined by the Deep Ritz and squared residual forms of the physics loss function. We find that the loss landscapes of physics-informed neural networks have many of the same properties as the data-driven classification problems studied in the literature. Unexpectedly, we find that the two formulations of the physics loss often give rise to similar landscapes, which appear smooth, well-conditioned, and convex in the vicinity of the solution. The purpose of this work is to introduce the loss landscape perspective to the scientific machine learning community, compare the Deep Ritz and the strong form losses, and to challenge prevailing intuitions about the complexity of the loss landscapes of physics-informed networks.

</details>


### [321] [Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks](https://arxiv.org/abs/2602.05873)
*Minyoung Kim*

Main category: cs.LG

TL;DR: 提出一种基于分数匹配的可扩展变分推理方法，用于大规模贝叶斯神经网络，避免重参数化采样，支持随机梯度，适用于视觉Transformer等大型网络。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络在不确定性量化、鲁棒性、抗过拟合等方面优于点估计方法，但现有变分推理方法（如ELBO）在大规模BNN中存在计算和技术限制，需要更可扩展的解决方案。

Method: 提出新颖的可扩展变分推理方法，结合分数匹配损失和近端惩罚项，避免重参数化采样，允许通过随机梯度获得噪声无偏小批量分数，支持更丰富的变分密度族。

Result: 在多个基准测试（包括视觉识别和时间序列预测）中，使用大规模深度网络（包括视觉Transformer）验证了方法的有效性。

Conclusion: 提出的基于分数匹配的变分推理方法为大规模贝叶斯神经网络提供了一种可扩展的解决方案，能够处理大型网络并支持更灵活的变分分布。

Abstract: Bayesian (deep) neural networks (BNN) are often more attractive than the mainstream point-estimate vanilla deep learning in various aspects including uncertainty quantification, robustness to noise, resistance to overfitting, and more. The variational inference (VI) is one of the most widely adopted approximate inference methods. Whereas the ELBO-based variational free energy method is a dominant choice in the literature, in this paper we introduce a score-based alternative for BNN variational inference. Although there have been quite a few score-based variational inference methods proposed in the community, most are not adequate for large-scale BNNs for various computational and technical reasons. We propose a novel scalable VI method where the learning objective combines the score matching loss and the proximal penalty term in iterations, which helps our method avoid the reparametrized sampling, and allows for noisy unbiased mini-batch scores through stochastic gradients. This in turn makes our method scalable to large-scale neural networks including Vision Transformers, and allows for richer variational density families. On several benchmarks including visual recognition and time-series forecasting with large-scale deep networks, we empirically show the effectiveness of our approach.

</details>


### [322] [ContextBench: A Benchmark for Context Retrieval in Coding Agents](https://arxiv.org/abs/2602.05892)
*Han Li,Letian Zhu,Bohan Zhang,Rili Feng,Jiaming Wang,Yue Pan,Earl T. Barr,Sarro Federica,Zhaoyang Chu,He Ye*

Main category: cs.LG

TL;DR: ContextBench是一个面向过程的代码代理上下文检索评估框架，包含1,136个任务和人工标注的黄金上下文，用于评估代码代理在问题解决过程中的上下文检索质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码代理评估主要关注最终任务成功率，缺乏对代理在解决问题过程中如何检索和使用代码上下文的理解。需要过程导向的评估方法来揭示上下文检索的质量和效率。

Method: 构建ContextBench数据集，包含1,136个问题解决任务，来自66个仓库和8种编程语言，每个任务都有人工标注的黄金上下文。实现自动化评估框架，跟踪代理轨迹并测量上下文召回率、精确率和效率。

Result: 评估4个前沿LLM和5个代码代理发现：1）复杂的代理框架对上下文检索提升有限（"代码代理的苦涩教训"）；2）LLM普遍偏向召回率而非精确率；3）探索的上下文与实际使用的上下文存在显著差距。

Conclusion: ContextBench通过中间黄金上下文指标补充了端到端基准测试，能够揭示问题解决过程。这些上下文为引导LLM在软件任务中的推理提供了有价值的中间信号。

Abstract: LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval ("The Bitter Lesson" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.

</details>


### [323] [Parity, Sensitivity, and Transformers](https://arxiv.org/abs/2602.05896)
*Alexander Kozachinskiy,Tomasz Steifer,Przemysław Wałȩga*

Main category: cs.LG

TL;DR: 本文研究了Transformer架构计算能力的边界，特别是能否解决PARITY问题。作者提出了新的Transformer构造，并给出了首个下界证明。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构已有近十年历史，但我们对其计算能力仍了解有限。特别是，单层Transformer能否解决PARITY问题？现有构造需要至少2层，且依赖不实用的特性（如长度相关位置编码、hardmax、无正则化参数的layernorm等）。

Method: 提出新的Transformer构造：使用softmax、长度无关且多项式有界的位置编码、无layernorm，同时支持因果掩码和非因果掩码。此外，通过理论分析证明单层单头Transformer无法解决PARITY问题。

Result: 成功构造出能够解决PARITY问题的Transformer，该构造比现有方法更实用。同时证明了首个下界：单层单头Transformer无法解决PARITY问题。

Conclusion: 本文填补了Transformer计算能力理解的重要空白，既展示了Transformer解决PARITY问题的可能性（通过新的实用构造），又确定了其计算限制（单层单头无法解决）。

Abstract: The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.
  We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.

</details>


### [324] [Regularized Calibration with Successive Rounding for Post-Training Quantization](https://arxiv.org/abs/2602.05902)
*Seohyeon Cha,Huancheng Chen,Dongjun Kim,Haoran Zhang,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于正则化非对称校准的量化方法，通过对称与非对称校准之间的插值作为正则化，结合有界搜索的逐次舍入程序，在保持PTQ二次结构的同时提升量化质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署面临内存和延迟成本挑战，后训练量化（PTQ）虽能实现高效推理，但其效果严重依赖于量化目标和舍入程序。现有方法在激活不匹配时表现不佳，需要更鲁棒的量化方案。

Method: 提出对称与非对称校准之间的插值作为正则化方法，保持PTQ标准二次结构的同时增强鲁棒性。推导了结合非对称校准的简单逐次舍入程序，并扩展为有界搜索方法，允许在量化质量和计算成本之间进行显式权衡。

Result: 在多个LLM家族、量化比特宽度和基准测试上的实验表明，基于正则化非对称校准目标的有界搜索方法持续改进了困惑度和准确率，同时仅带来适度且可控的额外计算成本。

Conclusion: 通过将对称与非对称校准插值作为正则化，并采用有界搜索的逐次舍入程序，该方法在保持PTQ效率优势的同时显著提升了量化质量，为LLM高效部署提供了有效的解决方案。

Abstract: Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.

</details>


### [325] [Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903)
*András Balogh,Márk Jelasity*

Main category: cs.LG

TL;DR: 研究提出对抗序列生成方法验证序列模型在象棋领域的正确性，发现所有模型都存在缺陷，但某些训练技术和数据集能显著改善正确性


<details>
  <summary>Details</summary>
Motivation: 序列模型通常基于样本序列训练，但需要验证其是否能真正捕捉语言结构（世界模型）。理论表明最多只能保证生成有效序列，但无法保证生成所有有效序列。需要实用工具验证序列模型的正确性

Method: 提出对抗序列生成方法验证序列模型正确性：1）选择象棋作为研究领域（复杂度足够但有简单规则）；2）设计对抗者生成有效序列迫使序列模型预测无效下一步；3）使用多种对抗序列生成方法；4）在不同训练配方（随机vs高质量对局）的象棋模型上评估；5）研究棋盘状态探针在训练和攻击中的应用

Result: 1）所有模型都不完全正确；2）某些训练技术和数据集选择能显著改善正确性；3）提取的棋盘状态在大多数模型中与下一个标记预测没有因果关系

Conclusion: 对抗序列生成是验证序列模型正确性的有效方法，能进行细粒度故障模式分析。虽然所有模型都存在缺陷，但通过改进训练技术和数据集能提升正确性。棋盘状态信息在大多数模型中未被有效利用

Abstract: Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.

</details>


### [326] [Chunky Post-Training: Data Driven Failures of Generalization](https://arxiv.org/abs/2602.05910)
*Seoirse Murray,Allison Qi,Timothy Qian,John Schulman,Collin Burns,Sara Price*

Main category: cs.LG

TL;DR: 论文提出"块状后训练"概念，指出大语言模型后训练数据中的偶然模式会导致意外行为，并开发了SURF和TURF工具来检测和追踪这些问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练涉及多种数据集，这些数据集除了编码预期行为外，还包含格式与内容的相关性、狭窄的措辞模式等偶然模式。这些模式对开发者不可见但对模型显著，导致模型产生令开发者惊讶的行为，如拒绝以特定格式呈现的真实事实。

Method: 提出SURF（运行时检测意外行为的黑盒管道）和TURF（将失败行为追溯到特定后训练数据的工具）。将这些工具应用于前沿模型（Claude 4.5, GPT-5.1等）和开源模型（Tülu 3）进行分析。

Result: 研究发现块状后训练会导致校准不当的行为，这些行为通常源于不平衡或未充分指定的后训练数据块。工具成功检测到模型中的意外行为模式。

Conclusion: 块状后训练是大语言模型开发中的重要问题，需要更好的数据管理和检测工具来识别和解决后训练数据中的偶然模式导致的意外行为。

Abstract: LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (Tülu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.

</details>


### [327] [Clifford Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.05977)
*Matthias Wolff,Francesco Alesiani,Christof Duhme,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: ClKAN是一种用于任意Clifford代数空间中函数逼近的灵活高效架构，使用随机准蒙特卡洛网格生成解决高维代数指数缩放问题，并引入新的批归一化策略处理变域输入。


<details>
  <summary>Details</summary>
Motivation: 在Clifford代数空间中函数逼近面临高维代数指数缩放和变域输入处理的挑战，需要开发更高效灵活的架构用于科学发现和工程应用。

Method: 提出Clifford Kolmogorov-Arnold Network (ClKAN)，采用随机准蒙特卡洛网格生成解决高维代数指数缩放问题，并引入新的批归一化策略处理变域输入。

Result: 在合成和物理启发任务中验证了ClKAN的有效性，展示了其在科学发现和工程应用中的潜力。

Conclusion: ClKAN为Clifford代数空间中的函数逼近提供了灵活高效的解决方案，通过创新的网格生成和归一化策略克服了传统方法的局限性。

Abstract: We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.

</details>


### [328] [Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training](https://arxiv.org/abs/2602.05933)
*Zhenghao Xu,Qin Lu,Changlong Yu,Tuo Zhao*

Main category: cs.LG

TL;DR: PMD-mean算法通过用采样策略的平均奖励近似对数配分函数，在LLM强化学习中实现了更稳定高效的策略优化，特别在数学推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的策略镜像下降（PMD）方法在LLM强化学习中面临挑战，因为需要在庞大的动作空间中准确估计配分函数，而有限的rollouts使得这一任务变得困难。需要一种更实用的算法来处理这一挑战。

Method: 提出PMD-mean算法，用采样策略的平均奖励近似对数配分函数，在log-policy空间进行回归。该方法实际上优化了带有自适应混合KL-χ²正则化的镜像下降子问题，χ²正则化约束了概率的大幅变化，在预期奖励较低时产生更保守的更新。

Result: 在数学推理任务上的实验表明，PMD-mean实现了优越的性能，同时提高了稳定性和时间效率。该方法对有限样本估计误差具有更强的鲁棒性。

Conclusion: PMD-mean算法为LLM的强化学习提供了更实用和稳健的方法，通过自适应混合正则化机制平衡了探索和利用，为RL算法的原则性改进指明了方向。

Abstract: Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.

</details>


### [329] [Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps](https://arxiv.org/abs/2602.05993)
*Peter Holderrieth,Douglas Chen,Luca Eyring,Ishin Shah,Giri Anantharaman,Yutong He,Zeynep Akata,Tommi Jaakkola,Nicholas Matthew Boffi,Max Simchowitz*

Main category: cs.LG

TL;DR: 提出Diamond Maps：一种可高效适应任意奖励的随机流映射模型，能在推理时快速对齐用户偏好，无需重新训练


<details>
  <summary>Details</summary>
Motivation: 现有流和扩散模型在训练后难以高效适应用户偏好或约束（奖励对齐问题），需要设计本身具有适应性的生成模型

Method: 提出Diamond Maps：随机流映射模型，将多个模拟步骤摊销为单步采样器，同时保留奖励对齐所需的随机性，支持搜索、序列蒙特卡洛和引导

Result: Diamond Maps可通过GLASS Flows蒸馏高效学习，在奖励对齐性能上优于现有方法，且具有更好的扩展性

Conclusion: Diamond Maps为实现推理时可快速适应任意偏好和约束的实用生成模型提供了可行路径

Abstract: Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose "Diamond Maps", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.

</details>


### [330] [Tuning Out-of-Distribution (OOD) Detectors Without Given OOD Data](https://arxiv.org/abs/2602.05935)
*Sudeepta Mondal,Xinyi Mary Xie,Ruxiao Duan,Alex Wong,Ganesh Sundaramoorthi*

Main category: cs.LG

TL;DR: 提出无需额外OOD数据集即可调优OOD检测器的新方法，解决现有方法依赖特定OOD数据集且性能不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测器需要依赖特定的OOD数据集进行调优，但这些数据集可能难以获取或不具代表性，导致检测器性能高度依赖于数据集选择且存在较大方差。

Method: 提出一种通用的OOD检测器调优方法，仅使用训练神经网络时的数据，无需额外OOD数据集。该方法为无需OOD数据集的检测器调优问题提供了强基线。

Result: 新方法在高参数OOD检测器家族中一致优于基线方法，在低参数家族中表现相当，显著提升了OOD检测性能的稳定性。

Conclusion: 无需额外OOD数据集即可有效调优OOD检测器是可行的，新方法为解决这一长期被忽视的问题提供了有效方案，减少了对外部数据集的依赖。

Abstract: Existing out-of-distribution (OOD) detectors are often tuned by a separate dataset deemed OOD with respect to the training distribution of a neural network (NN). OOD detectors process the activations of NN layers and score the output, where parameters of the detectors are determined by fitting to an in-distribution (training) set and the aforementioned dataset chosen adhocly. At detector training time, this adhoc dataset may not be available or difficult to obtain, and even when it's available, it may not be representative of actual OOD data, which is often ''unknown unknowns." Current benchmarks may specify some left-out set from test OOD sets. We show that there can be significant variance in performance of detectors based on the adhoc dataset chosen in current literature, and thus even if such a dataset can be collected, the performance of the detector may be highly dependent on the choice. In this paper, we introduce and formalize the often neglected problem of tuning OOD detectors without a given ``OOD'' dataset. To this end, we present strong baselines as an attempt to approach this problem. Furthermore, we propose a new generic approach to OOD detector tuning that does not require any extra data other than those used to train the NN. We show that our approach improves over baseline methods consistently across higher-parameter OOD detector families, while being comparable across lower-parameter families.

</details>


### [331] [Dimensionality Reduction on Riemannian Manifolds in Data Analysis](https://arxiv.org/abs/2602.05936)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.LG

TL;DR: 该论文研究了基于黎曼几何的降维方法，这些方法尊重数据的底层流形结构，特别是主测地线分析(PGA)作为流形值数据的非线性PCA扩展，并通过黎曼适配扩展判别分析，在弯曲空间中优于欧几里得方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发能够尊重数据底层流形结构的降维方法，特别是针对约束在弯曲空间（如超球面和对称正定流形）的数据，传统欧几里得方法无法有效处理这类几何结构。

Method: 方法包括：1) 主测地线分析(PGA)作为流形值数据的非线性PCA推广；2) 通过黎曼适配扩展判别分析；3) 利用测地线距离、切空间表示和内在统计度量实现更忠实的低维嵌入；4) 讨论相关流形学习技术及其理论基础。

Result: 在代表性数据集上的实验结果表明，黎曼方法相比欧几里得对应方法提供了改进的表示质量和分类性能，特别是在约束于弯曲空间的数据上表现更优。

Conclusion: 研究强调了在机器学习和数据科学应用中几何感知降维的重要性，黎曼几何方法能够更有效地处理流形结构数据，为复杂几何空间中的数据表示提供了理论框架和实践优势。

Abstract: In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.

</details>


### [332] [Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering](https://arxiv.org/abs/2602.06022)
*Miranda Muqing Miao,Young-Min Cho,Lyle Ungar*

Main category: cs.LG

TL;DR: CORAL是一种基于正则化MLP探针的推理时引导方法，通过从模型内部激活中提取分布式正确性信号，显著提升多选问答任务的准确率和校准度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在持续性的校准偏差问题，尤其是在指令微调和偏好对齐后。虽然修改训练目标可以改善校准，但重新训练成本高昂。现有的推理时引导方法大多优化正确性的代理指标而非正确性本身。

Method: 提出CORAL方法：使用权重衰减MLP探针从模型内部激活中捕获分布式正确性信号，通过正则化推理时引导来优化正确性本身而非代理指标。

Result: 在三个7B参数模型上，CORAL平均提升准确率10%，降低预期校准误差50%。在四个保留测试集上，平均提升准确率14%，降低校准误差49%，且无需重新训练即可迁移。

Conclusion: 当单个神经元不足时，可以通过正则化探针从模型内部提取分布式信息。CORAL提供了一种计算高效、可迁移且关注校准的方法来提升推理时的MCQA性能。

Abstract: Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\% and expected calibration error (ECE) by 50\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\% accuracy improvements and 49\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.

</details>


### [333] [Orthogonal Model Merging](https://arxiv.org/abs/2602.05943)
*Sihan Yang,Kexuan Shi,Weiyang Liu*

Main category: cs.LG

TL;DR: OrthoMerge：一种在正交群流形上进行模型合并的新方法，通过保持权重的几何结构来避免传统线性合并破坏预训练权重固有几何特性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法在欧几里得空间中进行线性算术操作，往往会破坏预训练权重的内在几何特性（如超球面能量），导致性能下降和灾难性遗忘。

Method: 1. 通过正交微调（OFT）学习任务特定的正交矩阵；2. 将正交矩阵映射到李代数进行合并操作；3. 对于非OFT方法微调的模型，采用正交-残差解耦策略，通过正交Procrustes问题提取正交分量；4. 正交分量在正交群流形上合并，线性残差通过标准加法合并。

Result: 大量实验结果表明，OrthoMerge能有效缓解灾难性遗忘，并在多样化任务中保持模型性能，优于传统线性合并方法。

Conclusion: 在正交群流形上进行模型合并能更好地保持预训练权重的几何结构，为集成多样化能力提供了一种原则性且高效的方法，特别适用于需要保持多个任务性能的场景。

Abstract: Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.

</details>


### [334] [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/abs/2602.06043)
*Prakhar Kaushik,Ankit Vaidya,Shravan Chaudhari,Rama Chellappa,Alan Yuille*

Main category: cs.LG

TL;DR: Share是一种参数高效的持续微调方法，通过学习和动态更新单个共享低秩子空间，实现跨任务和模态的无缝适应，显著减少参数和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型的高效持续适应对实际部署至关重要，但面临灾难性遗忘和高重训练成本挑战。现有参数高效调优方法（如LoRA）缺乏严格的持续学习和知识整合机制。

Method: Share构建一个基础子空间提取过去任务的核心知识，通过识别关键子空间方向增量整合新信息。新任务知识被纳入这个演化子空间，促进前向知识转移，同时最小化灾难性干扰。

Result: 相比传统LoRA方法，Share实现高达100倍参数减少和281倍内存节省，性能接近联合训练模型。单个Share模型可替代数百个任务特定LoRA适配器。

Conclusion: Share为大规模AI系统中的终身学习提供了实用且可扩展的解决方案，在图像分类、自然语言理解、3D姿态估计和文本到图像生成等任务中验证了其有效性。

Abstract: Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.

</details>


### [335] [Breaking Symmetry Bottlenecks in GNN Readouts](https://arxiv.org/abs/2602.05950)
*Mouad Talhi,Arne Wolf,Anthea Monod*

Main category: cs.LG

TL;DR: 该论文揭示了GNN中线性置换不变读出操作的表达能力瓶颈，并提出了一种基于投影不变读出的新方法，能够保留对称感知信息，显著提升GNN的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统GNN的表达能力受限通常归因于消息传递机制，但本文发现读出阶段存在独立瓶颈。线性置换不变读出操作（如求和、平均池化）会消除所有非平凡对称感知信息，限制了GNN区分非同构图的能力。

Method: 使用有限维表示理论证明线性置换不变读出通过Reynolds算子投影，会消除非平凡对称感知分量。提出基于投影的不变读出方法，将节点表示分解为对称感知通道，并用非线性不变统计量进行汇总。

Result: 理论证明线性读出存在表达能力瓶颈。实验表明，仅替换读出操作就能使固定编码器分离WL-hard图对，并在多个基准测试中提升性能，证明读出设计是GNN表达能力的关键因素。

Conclusion: 读出设计是GNN表达能力的决定性但被低估的因素。提出的投影不变读出方法能够克服线性读出的局限性，保留对称感知信息，为GNN设计提供了新的理论指导和实用工具。

Abstract: Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.

</details>


### [336] [Layer-wise LoRA fine-tuning: a similarity metric approach](https://arxiv.org/abs/2602.05988)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 提出Layer-wise LoRA方法，通过选择性地仅微调最相关的层，在保持预测性能的同时进一步减少LoRA的可训练参数达50%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增长，即使参数高效的微调方法（如LoRA）仍面临可训练参数过多的问题。作者认为并非所有层对模型适应都同等重要，因此需要更精细的层选择策略来进一步减少计算成本。

Method: 提出层选择的LoRA方法，通过测量各层内部表示的变化贡献度来识别最相关的层进行微调。该方法与现有低秩适应技术正交且兼容，可减少LoRA技术中高达50%的可训练参数。

Result: 在编码器架构上，GLUE基准测试中参数减少带来的预测性能下降可忽略不计；在解码器架构上，数学问题解决和编码任务中性能有小幅下降甚至提升；在多模态模型中也观察到与全层LoRA微调相当的结果。

Conclusion: 通过系统性地选择最相关层进行微调，可以在保持预测性能的同时显著减少可训练参数，为大规模语言模型的高效适应提供了有效解决方案。

Abstract: Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA

</details>


### [337] [On Computation and Reinforcement Learning](https://arxiv.org/abs/2602.05999)
*Raj Ghugare,Michał Bortkiewicz,Alicja Ziarko,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 论文研究了强化学习中计算资源对策略性能的影响，提出了一种能够利用可变计算量的架构，证明了更多计算能解决更复杂任务并实现更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架没有形式化地研究计算资源对策略学习的影响，且深度强化学习策略通常使用固定架构的神经网络，混淆了计算量和参数数量。本文旨在回答：计算资源如何影响强化学习策略？固定参数数量的策略能否从额外计算中受益？

Method: 形式化了计算有界策略的概念，基于算法学习和无模型规划的先验工作，提出了一种能够利用可变计算量的最小架构。在31个不同任务（包括在线和离线强化学习）上进行实验验证。

Result: 实验表明：(1) 该架构仅通过使用更多计算就能获得更强的性能；(2) 在长时域测试任务上，相比使用多达5倍参数的标准前馈网络或深度残差网络，该架构展现出更强的泛化能力。

Conclusion: 计算资源是强化学习策略性能的关键因素，与参数数量分开考虑。提出的可变计算架构能够利用额外计算解决更复杂任务并实现更好的泛化，为理解计算在强化学习中的作用提供了理论框架。

Abstract: How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.

</details>


### [338] [Mechanisms of AI Protein Folding in ESMFold](https://arxiv.org/abs/2602.06020)
*Kevin Lu,Jannik Brinkmann,Stefan Huber,Aaron Mueller,Yonatan Belinkov,David Bau,Chris Wendler*

Main category: cs.LG

TL;DR: 通过分析ESMFold折叠β发夹结构的过程，揭示了蛋白质结构预测模型的两个计算阶段：早期块初始化生化信号，晚期块发展空间特征


<details>
  <summary>Details</summary>
Motivation: 理解蛋白质结构预测模型（如ESMFold）如何折叠蛋白质，特别是研究它们在折叠过程中的计算机制和决策过程

Method: 通过反事实干预模型潜在变量，追踪ESMFold折叠β发夹结构的过程，分析模型不同块的计算功能

Result: 识别出折叠过程中的两个计算阶段：1）早期块将残基身份和相关生化特征（如电荷流）从序列表示转移到成对表示；2）晚期块在成对表示中积累距离和接触信息等空间特征

Conclusion: ESMFold的结构决策机制可以被定位、通过可解释的表示进行追踪，并能通过干预产生强烈的因果效应

Abstract: How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.

</details>


### [339] [Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference](https://arxiv.org/abs/2602.06029)
*Yingke Li,Anjali Parashar,Enlu Zhou,Chuchu Fan*

Main category: cs.LG

TL;DR: 论文为主动推理中的期望自由能最小化提供了首个理论保证，证明足够的好奇心系数能同时确保贝叶斯后验一致性和无遗憾优化


<details>
  <summary>Details</summary>
Motivation: 主动推理通过期望自由能平衡探索与利用，但缺乏理论指导：好奇心不足导致短视利用和不确定性无法解决，好奇心过强导致不必要探索和遗憾

Method: 建立理论框架分析EFE最小化代理，证明单一条件（足够好奇心）能同时保证贝叶斯后验一致性和有界累积遗憾，并将主动推理与经典贝叶斯实验设计和贝叶斯优化联系起来

Result: 首次为EFE最小化代理提供理论保证，揭示了好奇心机制如何依赖于初始不确定性、可识别性和目标对齐，并转化为实际设计指南

Conclusion: 为主动推理中的探索-利用权衡提供了理论基础，连接了贝叶斯实验设计和贝叶斯优化，并为混合学习-优化问题的实际应用提供了设计指导

Abstract: Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.

</details>


### [340] [AP-OOD: Attention Pooling for Out-of-Distribution Detection](https://arxiv.org/abs/2602.06031)
*Claus Hofmann,Christian Huber,Bernhard Lehner,Daniel Klotz,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: AP-OOD是一种新颖的自然语言OOD检测方法，通过利用token级信息超越简单的平均聚合，在半监督设置下灵活结合无监督和监督方法，在文本OOD检测上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前OOD检测面临的关键挑战是如何有效利用和聚合语言模型中的token嵌入来获得OOD分数。现有方法通常采用简单的平均聚合，未能充分利用token级信息。

Method: AP-OOD是一种半监督方法，灵活地在无监督和监督设置之间插值，能够利用有限的辅助异常数据。该方法超越了简单的基于平均的聚合，通过利用token级信息来改进OOD检测。

Result: 在无监督设置下，AP-OOD在XSUM摘要任务上将FPR95从27.84%降低到4.67%，在WMT15 En-Fr翻译任务上将FPR95从77.08%降低到70.37%，建立了文本OOD检测的新SOTA。

Conclusion: AP-OOD通过有效利用token级信息和半监督方法，显著提升了文本OOD检测性能，为机器学习模型的可靠部署提供了更有效的解决方案。

Abstract: Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.

</details>


### [341] [Can vision language models learn intuitive physics from interaction?](https://arxiv.org/abs/2602.06033)
*Luca M. Schulze Buschoff,Konstantinos Voudouris,Can Demircan,Eric Schulz*

Main category: cs.LG

TL;DR: 研究显示，通过监督微调或交互式强化学习训练的视觉语言模型在物理推理任务上表现有限，无法学习到可泛化的物理规则


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型缺乏对物理世界的良好直觉，现有方法（监督微调）无法让模型学习到稳健且可泛化的物理规则

Method: 使用强化学习让模型通过与环境交互来学习物理动态，并与监督微调方法进行比较

Result: 交互式学习能提高模型在特定任务内的性能，但无法产生可泛化的物理直觉；模型在相关任务间的泛化能力差，即使任务共享视觉统计特征和物理原理

Conclusion: 当前方法（包括交互式学习）无法让AI模型获得类似人类的泛化物理直觉，需要新的学习范式来掌握可迁移的物理规则

Abstract: Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.

</details>


### [342] [Pseudo-Invertible Neural Networks](https://arxiv.org/abs/2602.06042)
*Yamit Ehrlich,Nimrod Berman,Assaf Shocher*

Main category: cs.LG

TL;DR: 该论文提出了非线性伪逆（PInv）的推广，引入可逆神经网络架构SPNN，实现了非线性反投影，扩展了零样本逆问题的解决范围。


<details>
  <summary>Details</summary>
Motivation: 传统Moore-Penrose伪逆仅适用于线性系统，无法处理非线性映射。需要将伪逆概念推广到非线性领域，特别是神经网络，以解决更广泛的逆问题。

Method: 提出Surjective Pseudo-invertible Neural Networks (SPNN)架构，设计具有可处理非线性伪逆的网络结构。定义非线性反投影(NLBP)方法，确保非线性映射的约束一致性。

Result: SPNN实现了非线性伪逆，满足基本几何性质。该方法扩展了零样本逆问题的应用范围，能够处理从光学畸变到语义抽象等各种非线性信息损失。

Conclusion: 非线性伪逆的推广为复杂非线性逆问题提供了理论框架，使扩散模型能够零样本处理非线性退化，无需重新训练扩散先验即可实现生成输出的精确语义控制。

Abstract: The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or "Back-Projection", $x' = x + A^\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, "degradation" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [343] [Learning Context Matters: Measuring and Diagnosing Personalization Gaps in LLM-Based Instructional Design](https://arxiv.org/abs/2602.04972)
*Johaun Hatchett,Debshila Basu Mallick,Brittany C. Bradford,Richard G. Baraniuk*

Main category: cs.CY

TL;DR: 该研究提出一个框架来评估学习情境如何影响LLM教学策略选择，发现提供学习情境能改善LLM决策但与专家仍有显著差距，并引入相关性-影响分析诊断这种偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在教育中应用迅速，但真正个性化教学需要理解学习情境（学习者身份、学习目标和互动方式）。目前缺乏系统方法来评估学习情境如何影响LLM的教学决策质量。

Method: 使用心理测量学基础的合成学习情境和教学学基础的决策空间，比较LLM在有/无学习情境下的教学决策，量化其与学科专家判断的一致性。引入相关性-影响分析诊断哪些学习者特征被关注、忽略或产生虚假影响。

Result: 提供学习情境能系统性地改变LLM教学决策，使其更接近专家策略，但仍存在显著偏差。分析显示LLM会关注某些学习者特征，忽略其他重要特征，或对无关特征产生过度反应。

Conclusion: 学习情境确实影响LLM教学规划，但不能可靠地产生教学上适当的个性化。该框架为评估情境感知LLM系统提供了基础，可通过学习者特征优先级、教学模型调优和学习情境工程来改进个性化效果。

Abstract: The adoption of generative AI in education has accelerated dramatically in recent years, with Large Language Models (LLMs) increasingly integrated into learning environments in the hope of providing personalized support that enhances learner engagement and knowledge retention. However, truly personalized support requires access to meaningful Learning Context (LC) regarding who the learner is, what they are trying to understand, and how they are engaging with the material. In this paper, we present a framework for measuring and diagnosing how the LC influences instructional strategy selection in LLM-based tutoring systems. Using psychometrically grounded synthetic learning contexts and a pedagogically grounded decision space, we compare LLM instructional decisions in context-blind and context-aware conditions and quantify their alignment with the pedagogical judgments of subject matter experts. Our results show that, while providing the LC induces systematic, measurable changes in instructional decisions that move LLM policies closer to the subject matter expert policy, substantial misalignment remains. To diagnose this misalignment, we introduce a relevance-impact analysis that reveals which learner characteristics are attended to, ignored, or spuriously influential in LLM instructional decision-making. This analysis, conducted in collaboration with subject matter experts, demonstrates that LC materially shapes LLM instructional planning but does not reliably induce pedagogically appropriate personalization. Our results enable principled evaluation of context-aware LLM systems and provide a foundation for improving personalization through learner characteristic prioritization, pedagogical model tuning, and LC engineering.

</details>


### [344] [Blockchain Technology for Public Services: A Polycentric Governance Synthesis](https://arxiv.org/abs/2602.05109)
*Hozefa Lakadawala,Komla Dzigbede,Yu Chen*

Main category: cs.CY

TL;DR: 区块链在公共服务中的应用受多中心治理环境影响，政府通常采用混合和许可设计实现"受控多中心性"，而非完全去中心化。


<details>
  <summary>Details</summary>
Motivation: 各国政府越来越多地采用区块链技术来提升公共服务的透明度、信任和效率，但关于这些技术在不同国家背景下如何治理的证据仍然分散且过于关注技术特征。

Method: 使用多中心治理理论，对2021-2025年间发表的同行评审研究进行系统综述，遵循PRISMA指南，从主要数字政府和信息系统数据库中综合发现，分析区块链公共服务应用及其治理安排。

Result: 区块链采用嵌入多中心环境，政府通常采用混合和许可设计，实现选择性去中心化与集中监督相结合的模式，即"受控多中心性"。主要应用领域包括数字身份、电子投票、采购和社会服务。

Conclusion: 将区块链重新定义为协调和信息共享规则编码的治理基础设施，超越了简单的采用指标，为研究人员提供理论见解，为政策制定者设计可持续的区块链公共服务提供实践指导。

Abstract: National governments are increasingly adopting blockchain to enhance transparency, trust, and efficiency in public service delivery. However, evidence on how these technologies are governed across national contexts remains fragmented and overly focused on technical features. Using Polycentric Governance Theory, this study conducts a systematic review of peer-reviewed research published between 2021 and 2025 to examine blockchain-enabled public services and the institutional, organizational, and information-management factors shaping their adoption. Following PRISMA guidelines, we synthesize findings from major digital government and information systems databases to identify key application domains, including digital identity, electronic voting, procurement, and social services, and analyze the governance arrangements underpinning these initiatives. Our analysis reveals that blockchain adoption is embedded within polycentric environments characterized by distributed authority, inter-organizational coordination, and layered accountability. Rather than adopting full decentralization, governments typically utilize hybrid and permissioned designs that allow for selective decentralization alongside centralized oversight, a pattern we conceptualize as "controlled polycentricity." By reframing blockchain as a governance infrastructure that encodes rules for coordination and information-sharing, this study advances digital government theory beyond simple adoption metrics. The findings offer theoretically grounded insights for researchers and practical guidance for policymakers seeking to design and scale sustainable blockchain-enabled public services.

</details>


### [345] [Scalable Generation and Validation of Isomorphic Physics Problems with GenAI](https://arxiv.org/abs/2602.05114)
*Naiming Liu,Leo Murch,Spencer Moore,Tong Wan,Shashank Sonkar,Richard Baraniuk,Zhongzhou Chen*

Main category: cs.CY

TL;DR: 使用生成式AI创建和评估大规模同构物理问题库，用于异步、多尝试评估，通过语言模型验证问题难度与学生表现高度相关。


<details>
  <summary>Details</summary>
Motivation: 传统同步STEM评估面临可访问性障碍、资源共享平台的安全问题以及跨机构可比性有限等挑战，需要开发异步、多尝试的评估方法。

Method: 采用生成式AI框架，通过提示链和工具使用生成同构物理问题，控制结构变化（数值、空间关系）和上下文变化，使用17个开源语言模型（0.6B-32B）进行预部署验证。

Result: 73%的部署问题库达到统计上同质的难度水平，语言模型表现与学生成绩高度相关（Pearson's ρ最高达0.594），语言模型能成功识别有问题的变体，如模糊问题文本。

Conclusion: 生成式AI能有效创建大规模同构物理问题库，语言模型验证是有效的预部署评估工具，中等规模模型最适合检测难度异常值。

Abstract: Traditional synchronous STEM assessments face growing challenges including accessibility barriers, security concerns from resource-sharing platforms, and limited comparability across institutions. We present a framework for generating and evaluating large-scale isomorphic physics problem banks using Generative AI to enable asynchronous, multi-attempt assessments. Isomorphic problems test identical concepts through varied surface features and contexts, providing richer variation than conventional parameterized questions while maintaining consistent difficulty. Our generation framework employs prompt chaining and tool use to achieve precise control over structural variations (numeric values, spatial relations) alongside diverse contextual variations. For pre-deployment validation, we evaluate generated items using 17 open-source language models (LMs) (0.6B-32B) and compare against actual student performance (N>200) across three midterm exams. Results show that 73% of deployed banks achieve statistically homogeneous difficulty, and LMs pattern correlate strongly with student performance (Pearson's $ρ$ up to 0.594). Additionally, LMs successfully identify problematic variants, such as ambiguous problem texts. Model scale also proves critical for effective validation, where extremely small (<4B) and large (>14B) models exhibit floor and ceiling effects respectively, making mid-sized models optimal for detecting difficulty outliers.

</details>


### [346] [Prediction Laundering: The Illusion of Neutrality, Transparency, and Governance in Polymarket](https://arxiv.org/abs/2602.05181)
*Yasaman Rohanifar,Syed Ishtiaque Ahmed,Sharifa Sultana*

Main category: cs.CY

TL;DR: 该论文批判性地分析了预测市场平台Polymarket如何通过"预测洗钱"过程将主观、不确定的投注转化为看似客观的真理信号，掩盖了资本不对称和战略操纵，导致认知分层和问责缺失。


<details>
  <summary>Details</summary>
Motivation: 随着预测市场作为认知基础设施的重要性日益增长，像Polymarket这样的平台被定位为提供客观、实时概率真理的提供者。然而，这些平台产生的信号往往掩盖了不确定性、战略操纵和资本不对称，导致人们错误地信任其认知权威。作者旨在揭示这种"概率权威"是如何被生产和争夺的。

Method: 采用质性社会技术审计方法，结合数字民族志、解释性走查和半结构化访谈，对Polymarket平台进行了研究（N=27）。通过引入"预测洗钱"概念，基于MacFarlane的知识传输框架，分析主观投注如何通过算法聚合被剥离原始噪声。

Result: 识别了预测洗钱的四阶段生命周期：1) 结构净化：集中化本体论脚本化可投注的未来；2) 概率扁平化：将异质动机压缩为单一信号；3) 架构掩蔽：将资本驱动的影响隐藏在表面共识背后；4) 认知硬化：消除治理争议以产生客观历史事实。该过程导致认知眩晕和问责缺口，将真相解析外包给Discord等平台外社区。

Conclusion: 挑战了关于无摩擦集体智能的叙事，揭示了认知分层现象：技术精英审计底层机制，而广大公众消费经过净化的资本加权信号。作者主张采用"摩擦积极设计"，在合成真理生产中突显社会和金融摩擦。

Abstract: The growing reliance on prediction markets as epistemic infrastructures has positioned platforms like Polymarket as providers of objective, real-time probabilistic truth, yet the signals they produce often obscure uncertainty, strategic manipulation, and capital asymmetries, encouraging misplaced epistemic trust. This paper presents a qualitative sociotechnical audit of Polymarket (N = 27), combining digital ethnography, interpretive walkthroughs, and semi-structured interviews to examine how probabilistic authority is produced and contested. We introduce the concept of Prediction Laundering, drawing on MacFarlanes framework of knowledge transmission, to describe how subjective, high-uncertainty bets, strategic hedges, and capital-heavy whale activity are stripped of their original noise through algorithmic aggregation. We trace a four-stage laundering lifecycle: Structural Sanitization, where a centralized ontology scripts the bet-able future; Probabilistic Flattening, which collapses heterogeneous motives into a single signal; Architectural Masking, which conceals capital-driven influence behind apparent consensus; and Epistemic Hardening, which erases governance disputes to produce an objective historical fact. We show that this process induces epistemic vertigo and accountability gaps by offloading truth-resolution to off-platform communities such as Discord. Challenging narratives of frictionless collective intelligence, we demonstrate Epistemic Stratification, in which technical elites audit underlying mechanisms while the broader public consumes a sanitized, capital-weighted signal, and we conclude by advocating Friction-Positive Design that surfaces the social and financial frictions inherent in synthetic truth production.

</details>


### [347] [FATe of Bots: Ethical Considerations of Social Bot Detection](https://arxiv.org/abs/2602.05200)
*Lynnette Hui Xian Ng,Ethan Pan,Michael Miller Yoder,Kathleen M. Carley*

Main category: cs.CY

TL;DR: 该论文探讨了社交媒体机器人检测算法中的伦理问题，基于FATe框架（公平性、问责制、透明度）分析训练数据集、算法开发和机器人使用三个支柱，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 社交媒体机器人传播有害信息造成广泛社会影响，虽然已有机器人检测算法，但这些算法在复杂的社会技术系统中运行，涉及用户和组织，因此伦理考量至关重要。

Method: 通过调查现有机器人检测算法的训练数据集、评估现有机器人检测数据集，并借鉴用户被误判为机器人的经验讨论，基于FATe框架（公平性、问责制、透明度）分析伦理影响。

Result: 识别了机器人检测系统在训练数据集、算法开发和机器人使用三个支柱中的伦理挑战，包括数据集偏差、算法公平性、误判影响等问题。

Conclusion: 研究人员在解决机器人检测伦理问题上面临挑战，需要更负责任和公平的方法来改进社交媒体机器人检测生态系统，提出了相关研究方向建议。

Abstract: A growing suite of research illustrates the negative impact of social media bots in amplifying harmful information with widespread social implications. Social bot detection algorithms have been developed to help identify these bot agents efficiently. While such algorithms can help mitigate the harmful effects of social media bots, they operate within complex socio-technical systems that include users and organizations. As such, ethical considerations are key while developing and deploying these bot detection algorithms, especially at scales as massive as social media ecosystems. In this article, we examine the ethical implications for social bot detection systems through three pillars: training datasets, algorithm development, and the use of bot agents. We do so by surveying the training datasets of existing bot detection algorithms, evaluating existing bot detection datasets, and drawing on discussions of user experiences of people being detected as bots. This examination is grounded in the FATe framework, which examines Fairness, Accountability, and Transparency in consideration of tech ethics. We then elaborate on the challenges that researchers face in addressing ethical issues with bot detection and provide recommendations for research directions. We aim for this preliminary discussion to inspire more responsible and equitable approaches towards improving the social media bot detection landscape.

</details>


### [348] [Fine-Tuning Large Language Models for Automatic Detection of Sexually Explicit Content in Spanish-Language Song Lyrics](https://arxiv.org/abs/2602.05485)
*Dolores Zamacola Sánchez de Lamadrid,Eduardo C. Garrido-Merchán*

Main category: cs.CY

TL;DR: 使用GPT模型微调检测西班牙语歌词中的性内容，达到87%准确率，并提出类似PEGI的多级年龄内容分级政策


<details>
  <summary>Details</summary>
Motivation: 雷鬼顿和陷阱等流行音乐中色情内容的泛滥，特别是对年轻听众的影响，引发了社会对未成年人接触有害歌词材料的担忧

Method: 通过微调GPT模型，使用100首专家标注的西班牙语歌曲（50首明确色情，50首非色情）进行训练，利用迁移学习适应拉丁城市音乐的语言特征

Result: 微调模型达到87%准确率、100%精确度和100%特异性，优于未微调的ChatGPT基线模型，与专家分类的一致性为59.2% vs 55.1%

Conclusion: 微调大语言模型可作为音乐流媒体平台的自动内容审核工具，并提出基于年龄的多级内容分级系统政策建议

Abstract: The proliferation of sexually explicit content in popular music genres such as reggaeton and trap, consumed predominantly by young audiences, has raised significant societal concern regarding the exposure of minors to potentially harmful lyrical material. This paper presents an approach to the automatic detection of sexually explicit content in Spanish-language song lyrics by fine-tuning a Generative Pre-trained Transformer (GPT) model on a curated corpus of 100 songs, evenly divided between expert-labeled explicit and non-explicit categories. The proposed methodology leverages transfer learning to adapt the pre-trained model to the idiosyncratic linguistic features of urban Latin music, including slang, metaphors, and culturally specific double entendres that evade conventional dictionary-based filtering systems. Experimental evaluation on held-out test sets demonstrates that the fine-tuned model achieves 87% accuracy, 100% precision, and 100% specificity after a feedback-driven refinement loop, outperforming both its pre-feedback configuration and a non-customized baseline ChatGPT model. A comparative analysis reveals that the fine-tuned model agrees with expert human classification in 59.2% of cases versus 55.1% for the standard model, confirming that domain-specific adaptation enhances sensitivity to implicit and culturally embedded sexual references. These findings support the viability of deploying fine-tuned large language models as automated content moderation tools on music streaming platforms. Building on these technical results, the paper develops a public policy proposal for a multi-tier age-based content rating system for music analogous to the PEGI system for video games analyzed through the PESTEL framework and Kingdon's Multiple Streams Framework, establishing both the technological feasibility and the policy pathway for systematic music content regulation.

</details>


### [349] [Wikipedia and Grokipedia: A Comparison of Human and Generative Encyclopedias](https://arxiv.org/abs/2602.05519)
*Ortal Hadad,Edoardo Loru,Jacopo Nudo,Anita Bonetti,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.CY

TL;DR: 该研究比较了维基百科和Grokipedia，分析生成式AI如何改变百科全书内容的选择、重写、叙事结构和评价框架。研究发现生成系统保留了内容的主要结构组织，但影响了内容的选择、重写和框架。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI系统（如Grokipedia）如何改变百科全书内容的创建和呈现方式，特别关注内容选择、文本重写、叙事结构和评价框架的变化。

Method: 1. 建模Grokipedia页面收录与维基百科页面流行度、参考文献密度和近期编辑活动的关系；2. 区分逐字复制和生成式重写；3. 使用适应度-复杂性框架评估编辑参与模式；4. 使用抽象意义表示构建行动者关系网络分析叙事结构；5. 分析导语部分的评价框架。

Result: 1. 收录不均衡：维基百科中可见度更高、编辑冲突更大的页面更可能出现在Grokipedia中；2. 参考文献密度高、近期有争议的页面更常被重写，而高度流行的页面更多被原样复制；3. 叙事结构在多个主题领域（美国政治、地缘政治、阴谋论）基本保持一致；4. 导语部分框架大体相关，但Grokipedia在某些话题上出现赞扬性和冲突性语言的局部变化。

Conclusion: 生成式系统保留了百科全书内容的主要结构组织，但影响了内容的选择、重写和框架方式。系统倾向于重写有争议的内容，而复制流行内容，叙事结构保持稳定但评价框架有局部变化。

Abstract: We present a comparative analysis of Wikipedia and Grokipedia to examine how generative mediation alters content selection, textual rewriting, narrative structure, and evaluative framing in encyclopedic content. We model page inclusion in Grokipedia as a function of Wikipedia page popularity, density of reference, and recent editorial activity. Inclusion is non-uniform: pages with higher visibility and greater editorial conflict in Wikipedia are more likely to appear in Grokipedia. For included pages, we distinguish between verbatim reproduction and generative rewriting. Rewriting is more frequent for pages with higher reference density and recent controversy, while highly popular pages are more often reproduced without modification. We compare editing activity across the two platforms and estimate page complexity using a fitness-complexity framework to assess whether generative mediation alters patterns of editorial participation. To assess narrative organization, we construct actor-relation networks from article texts using abstract meaning representation. Across multiple topical domains, including U.S. politics, geopolitics, and conspiracy-related narratives, narrative structure remains largely consistent between the two sources. Analysis of lead sections shows broadly correlated framing, with localized shifts in laudatory and conflict-oriented language for some topics in Grokipedia. Overall, generative systems preserve the main structural organization of encyclopedic content, while affecting how content is selected, rewritten, and framed.

</details>


### [350] [Ethology of Latent Spaces](https://arxiv.org/abs/2602.05710)
*Philippe Boisnard*

Main category: cs.CY

TL;DR: 该研究挑战了视觉语言模型潜在空间的中立性假设，通过比较分析发现不同模型在政治文化分类上存在显著差异，揭示了算法特定的感知敏感性和三种不同的算法视觉机制。


<details>
  <summary>Details</summary>
Motivation: 挑战视觉语言模型潜在空间的中立性假设，揭示算法行为中的模型特异性敏感度，这些敏感度由训练数据和架构选择塑造，影响文化解释的委托。

Method: 采用比较分析方法，研究三个模型（OpenAI CLIP、OpenCLIP LAION、SigLIP）对301件艺术作品（15-20世纪）的处理。使用基于向量类比的双极语义轴，分析模型在政治和文化类别归属上的差异。

Result: 发现模型间存在显著差异：SigLIP将59.4%作品分类为政治参与，而OpenCLIP仅4%；非洲面具在SigLIP中获得最高政治分数，在OpenAI CLIP中保持非政治性；在美学殖民轴上，模型间差异达72.6个百分点。

Conclusion: 提出三个操作概念：计算潜在政治化、涌现偏差和三种算法视觉机制（熵、制度、符号）。训练数据集作为准档案，其话语形态在潜在空间中结晶。呼吁在数字艺术史应用中批判性重新评估VLMs，并将学习架构整合到文化解释的算法委托中。

Abstract: This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices.
  Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points.
  We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault's notion of the archive, Jameson's ideologeme, and Simondon's theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.

</details>


### [351] [Cold Start Problem: An Experimental Study of Knowledge Tracing Models with New Students](https://arxiv.org/abs/2505.21517)
*Indronil Bhattacharjee,Christabel Wayllace*

Main category: cs.CY

TL;DR: 研究知识追踪中的冷启动问题，评估三种模型在新学生上的表现，发现所有模型在冷启动条件下都表现不佳，但随着交互增加而改善


<details>
  <summary>Details</summary>
Motivation: 知识追踪面临冷启动问题，即对新学生预测准确率低。现有研究通常在相同学生的后续交互上测试，而本研究关注如何将模型推广到全新学生

Method: 使用历史学生数据训练三种知识追踪模型（DKT、DKVMN、SAKT），然后在全新学生数据上评估性能，使用ASSISTments 2009、2015和2017数据集

Result: 所有模型在冷启动条件下初始表现都较差，但随着交互次数增加而逐渐改善；SAKT模型显示出较高的初始准确率，但仍面临限制

Conclusion: 需要开发能够有效推广到新学习者的知识追踪模型，强调开发在少样本和零样本学习场景中鲁棒的模型的重要性

Abstract: KnowledgeTracing (KT) involves predicting students' knowledge states based on their interactions with Intelligent Tutoring Systems (ITS). A key challenge is the cold start problem, accurately predicting knowledge for new students with minimal interaction data. Unlike prior work, which typically trains KT models on initial interactions of all students and tests on their subsequent interactions, our approach trains models solely using historical data from past students, evaluating their performance exclusively on entirely new students. We investigate cold start effects across three KT models: Deep Knowledge Tracing (DKT), Dynamic Key-Value Memory Networks (DKVMN), and Self-Attentive Knowledge Tracing (SAKT), using ASSISTments 2009, 2015, and 2017 datasets. Results indicate all models initially struggle under cold start conditions but progressively improve with more interactions; SAKT shows higher initial accuracy yet still faces limitations. These findings highlight the need for KT models that effectively generalize to new learners, emphasizing the importance of developing models robust in few-shot and zero-shot learning scenarios

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [352] [Multi-Sensor Scheduling for Remote State Estimation over Wireless MIMO Fading Channels with Semantic Over-the-Air Aggregation](https://arxiv.org/abs/2602.04971)
*Minjie Tang,Photios A. Stavrou,Marios Kountouris*

Main category: eess.SY

TL;DR: 提出基于语义空中聚合的多传感器调度方案，用于MIMO衰落信道下的远程状态估计，通过动态规划优化功率效率与估计性能。


<details>
  <summary>Details</summary>
Motivation: 传统空中聚合方法存在传输功率限制问题，需要在功率效率和估计性能之间取得平衡，特别是在无线MIMO衰落信道环境下。

Method: 将调度问题建模为有限时域动态规划问题，分析最优Q函数结构，提出基于正半定锥分解的Q函数上界，实现高效近似调度策略和低复杂度远程估计算法。

Result: 数值结果表明，所提方案在估计精度和功率效率方面均优于现有方法，调度策略能够在线适应估计误差协方差和信道变化。

Conclusion: 基于语义空中聚合的多传感器调度方案有效解决了传统方法的功率限制问题，通过动态规划优化实现了功率效率与估计性能的良好平衡。

Abstract: In this work, we study multi-sensor scheduling for remote state estimation over wireless multiple-input multiple-output (MIMO) fading channels using a novel semantic over-the-air (SemOTA) aggregation approach. We first revisit Kalman filtering with conventional over-the-air (OTA) aggregation and highlight its transmit power limitations. To balance power efficiency and estimation performance, we formulate the scheduling task as a finite-horizon dynamic programming (DP) problem. By analyzing the structure of the optimal Q-function, we show that the resulting scheduling policy exhibits a semantic structure that adapts online to the estimation error covariance and channel variations. To obtain a practical solution, we derive a tractable upper bound on the Q-function via a positive semidefinite (PSD) cone decomposition, which enables an efficient approximate scheduling policy and a low-complexity remote estimation algorithm. Numerical results confirm that the proposed scheme outperforms existing methods in both estimation accuracy and power efficiency.

</details>


### [353] [Learning Nonlinear Continuous-Time Systems for Formal Uncertainty Propagation and Probabilistic Evaluation](https://arxiv.org/abs/2602.05103)
*Peter Amorese,Morteza Lahijanian*

Main category: eess.SY

TL;DR: 提出一种基于连续动力学视角的模型学习方法，通过构造概率事件的泰勒级数近似实现形式化不确定性传播，特别适用于预测罕见事件。


<details>
  <summary>Details</summary>
Motivation: 非线性常微分方程是建模现实世界动力系统的强大工具，但在初始状态不确定性通过非线性动力学传播时面临挑战，特别是当ODE未知且从数据中学习时。

Method: 引入新颖的连续动力学视角进行模型学习，通过构造概率事件的泰勒级数近似实现形式化不确定性传播。

Result: 建立了方法可靠性的充分条件并证明了其渐近收敛性，实证结果表明该框架在预测罕见事件方面特别有效。

Conclusion: 该方法为从数据中学习非线性动力学模型并传播初始状态不确定性提供了有效的连续动力学框架，特别适用于罕见事件预测。

Abstract: Nonlinear ordinary differential equations (ODEs) are powerful tools for modeling real-world dynamical systems. However, propagating initial state uncertainty through nonlinear dynamics, especially when the ODE is unknown and learned from data, remains a major challenge. This paper introduces a novel continuum dynamics perspective for model learning that enables formal uncertainty propagation by constructing Taylor series approximations of probabilistic events. We establish sufficient conditions for the soundness of the approach and prove its asymptotic convergence. Empirical results demonstrate the framework's effectiveness, particularly when predicting rare events.

</details>


### [354] [GPU-to-Grid: Voltage Regulation via GPU Utilization Control](https://arxiv.org/abs/2602.05116)
*Zhirui Liang,Jae-Won Chung,Mosharaf Chowdhury,Jiasi Chen,Vladimir Dvorkin*

Main category: eess.SY

TL;DR: 提出GPU-to-Grid框架，将GPU设备级控制与电力系统目标耦合，研究LLM推理灵活性如何支持配电网电压调节，发现GPU功耗调节对电压上下限有不同影响


<details>
  <summary>Details</summary>
Motivation: 数据中心快速增长给电网带来挑战，但也作为灵活负荷提供新机会。现有电力系统研究将数据中心抽象为聚合资源，而计算机系统研究主要关注GPU能效优化，忽略优化后的GPU功耗对电网的影响。需要填补这一空白。

Method: 开发GPU-to-Grid框架，将设备级GPU控制与电力系统目标耦合。使用批处理大小作为控制旋钮，权衡GPU功耗对电压的影响与推理延迟和令牌吞吐量。首先将问题形式化为优化问题，然后实现为在线反馈优化控制器，利用电网和GPU系统的测量数据。

Result: 关键发现：降低GPU功耗可缓解配电网电压下限违规，而增加GPU功耗可缓解电压上限附近的违规。这与"最小化GPU功耗总是对电网有益"的普遍观念相反。

Conclusion: GPU功耗调节对配电网电压有双向影响，需要根据电压情况智能调整GPU功率，而不是一味追求最小化功耗。GPU-to-Grid框架为数据中心与电网协同优化提供了新途径。

Abstract: While the rapid expansion of data centers poses challenges for power grids, it also offers new opportunities as potentially flexible loads. Existing power system research often abstracts data centers as aggregate resources, while computer system research primarily focuses on optimizing GPU energy efficiency and largely ignores the grid impacts of optimized GPU power consumption. To bridge this gap, we develop a GPU-to-Grid framework that couples device-level GPU control with power system objectives. We study distribution-level voltage regulation enabled by flexibility in LLM inference, using batch size as a control knob that trades off the voltage impacts of GPU power consumption against inference latency and token throughput. We first formulate this problem as an optimization problem and then realize it as an online feedback optimization controller that leverages measurements from both the power grid and GPU systems. Our key insight is that reducing GPU power consumption alleviates violations of lower voltage limits, while increasing GPU power mitigates violations near upper voltage limits in distribution systems; this runs counter to the common belief that minimizing GPU power consumption is always beneficial to power grids.

</details>


### [355] [Trojan Attacks on Neural Network Controllers for Robotic Systems](https://arxiv.org/abs/2602.05121)
*Farbod Younesi,Walter Lucia,Amr Youssef*

Main category: eess.SY

TL;DR: 论文研究针对神经网络控制器的后门攻击，以差速移动机器人为案例，设计了一个轻量级并行特洛伊网络，能在特定触发条件下破坏控制器的轮速指令，导致不安全行为。


<details>
  <summary>Details</summary>
Motivation: 神经网络控制器在机器人系统中应用日益广泛，但其依赖可能不可信的训练流程或供应链带来了严重的安全漏洞。需要研究针对神经网络控制器的后门攻击，以揭示这类系统的安全威胁。

Method: 假设机器人的跟踪控制器由神经网络实现，设计一个轻量级并行特洛伊网络嵌入控制器中。该恶意模块在正常操作时保持休眠，但当检测到由机器人位姿和目标参数定义的特定触发条件时，会破坏主控制器的轮速指令。

Result: 通过仿真验证了两种不同攻击场景下的特洛伊网络有效性，结果证实了所提出攻击方法的有效性，并表明基于神经网络的机器人控制系统面临潜在的关键安全威胁。

Conclusion: 神经网络控制器的后门攻击是真实存在的安全威胁，需要在机器人控制系统的设计和部署中加强安全考虑，防范这类攻击。

Abstract: Neural network controllers are increasingly deployed in robotic systems for tasks such as trajectory tracking and pose stabilization. However, their reliance on potentially untrusted training pipelines or supply chains introduces significant security vulnerabilities. This paper investigates backdoor (Trojan) attacks against neural controllers, using a differential-drive mobile robot platform as a case study. In particular, assuming that the robot's tracking controller is implemented as a neural network, we design a lightweight, parallel Trojan network that can be embedded within the controller. This malicious module remains dormant during normal operation but, upon detecting a highly specific trigger condition defined by the robot's pose and goal parameters, compromises the primary controller's wheel velocity commands, resulting in undesired and potentially unsafe robot behaviours. We provide a proof-of-concept implementation of the proposed Trojan network, which is validated through simulation under two different attack scenarios. The results confirm the effectiveness of the proposed attack and demonstrate that neural network-based robotic control systems are subject to potentially critical security threats.

</details>


### [356] [Nonlinear Predictive Cost Adaptive Control of Pseudo-Linear Input-Output Models Using Polynomial, Fourier, and Cubic Spline Observables](https://arxiv.org/abs/2602.05263)
*Rami Abdulelah Alhazmi,Achinth Suresh Babu,Syed Aseem Ul Islam,Dennis S. Bernstein*

Main category: eess.SY

TL;DR: 提出一种基于在线系统辨识的自适应非线性模型预测控制方法，无需先验建模、训练或数据收集


<details>
  <summary>Details</summary>
Motivation: 高不确定性非线性系统的控制具有实际意义和理论挑战，需要能够在线适应系统动态的控制方法

Method: 使用非线性预测成本自适应控制（NPCAC），结合带信息子空间遗忘的递归最小二乘法在线辨识伪线性输入输出模型，配合迭代MPC进行非线性滚动时域优化

Result: NPCAC方法使用多项式、傅里叶和三次样条基函数展示了良好的控制性能

Conclusion: 提出的自适应非线性MPC方法能够有效处理高不确定性系统，完全基于在线辨识实现控制，具有实际应用价值

Abstract: Control of nonlinear systems with high levels of uncertainty is practically relevant and theoretically challenging. This paper presents a numerical investigation of an adaptive nonlinear model predictive control (MPC) technique that relies entirely on online system identification without prior modeling, training, or data collection. In particular, the paper considers predictive cost adaptive control (PCAC), which is an extension of generalized predictive control. Nonlinear PCAC (NPCAC) uses recursive least squares (RLS) with subspace of information forgetting (SIFt) to identify a discrete-time, pseudo-linear, input-output model, which is used with iterative MPC for nonlinear receding-horizon optimization. The performance of NPCAC is illustrated using polynomial, Fourier, and cubic-spline basis functions.

</details>


### [357] [Steering the Herd: A Framework for LLM-based Control of Social Learning](https://arxiv.org/abs/2504.02648)
*Raghu Arghal,Kevin He,Shirin Saeedi Bidokhti,Saswati Sarkar*

Main category: eess.SY

TL;DR: 研究算法信息中介（如LLM）如何通过控制信息结构影响社会学习过程，分析利他型和偏见型中介的最优策略及其对社会福利的影响。


<details>
  <summary>Details</summary>
Motivation: 随着算法（如社交媒体推荐、LLM）日益成为信息中介，个体在决策时既接收算法中介的信号，又向同伴学习。需要研究这种联合过程中信息中介如何影响社会学习，以及不同目标（社会福利最大化或诱导特定行为）的中介策略。

Method: 提出受控序贯社会学习模型，信息中介规划者控制代理的信息结构，同时代理从早期代理的决策中学习。结合动态规划、分散行动选择和贝叶斯信念更新的优化框架。理论分析最优策略，并通过LLM作为规划者和代理的模拟进行补充。

Result: 证明了价值函数的凸性，刻画了利他型和偏见型规划者的最优策略。在某些情况下，偏见型规划者会故意模糊代理的信号。即使在严格透明度约束下，信息中介仍能显著改变社会福利方向。LLM模拟显示出现战略性引导公众意见的行为，与理论预测趋势一致但存在偏差。

Conclusion: 该框架为研究LLM信息中介的影响和监管提供了可处理的基础，揭示了信息中介在透明度约束下仍能显著影响社会学习过程，需要相应监管机制。

Abstract: Algorithms increasingly serve as information mediators--from social media feeds and targeted advertising to the increasing ubiquity of LLMs. This engenders a joint process where agents combine private, algorithmically-mediated signals with learning from peers to arrive at decisions. To study such settings, we introduce a model of controlled sequential social learning in which an information-mediating planner (e.g. an LLM) controls the information structure of agents while they also learn from the decisions of earlier agents. The planner may seek to improve social welfare (altruistic planner) or to induce a specific action the planner prefers (biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates. We prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from induced agent choices. Notably, in some regimes the biased planner intentionally obfuscates the agents' signals. Even under stringent transparency constraints--information parity with individuals, no lying or cherry-picking, and full observability--we show that information mediation can substantially shift social welfare in either direction. We complement our theory with simulations in which LLMs act as both planner and agents. Notably, the LLM planner in our simulations exhibits emergent strategic behavior in steering public opinion that broadly mirrors the trends predicted, though key deviations suggest the influence of non-Bayesian reasoning consistent with the cognitive patterns of both humans and LLMs trained on human-like data. Together, we establish our framework as a tractable basis for studying the impact and regulation of LLM information mediators.

</details>


### [358] [Policy-Driven Orchestration Framework for Multi-Operator Non-Terrestrial Networks](https://arxiv.org/abs/2602.05363)
*Yuma Abe,Mariko Sekiguchi,Go Otsuru,Amane Miura*

Main category: eess.SY

TL;DR: 提出基于弱控制的NTN多运营商编排框架，通过协调多个非地面网络运营商，在保证各自政策自主性的同时实现全球大规模星座的虚拟构建。


<details>
  <summary>Details</summary>
Motivation: 非地面网络需要大量节点（如卫星）建立全球覆盖，但单个运营商难以独立部署。多运营商合作通过编排器协调可以构建虚拟大规模星座，但需要平衡编排器控制与运营商自主性。

Method: 提出弱控制编排框架：编排器提供多个候选路由，各运营商从中选择符合自身政策偏好的路由，而非由编排器完全决定端到端路径。通过迭代协商解决政策冲突。

Result: 运营商间合作提高了可行端到端路由的可用性；严格个体政策会降低全局可行性和性能（"自主性代价"）；运营商数量增加时跳数和延迟会上升；结果受运营商政策影响。

Conclusion: 弱控制编排框架能有效协调多运营商NTN合作，在动态NTN环境中实现实际效益，平衡了编排器协调与运营商自主性，适用于未来NTN环境。

Abstract: Non-terrestrial networks (NTNs) have gained significant attention for their scalability and wide coverage in next-generation communication systems. A large number of NTN nodes, such as satellites, are required to establish a global NTN, but not all operators have the capability to deploy such a system. Therefore, cooperation among multiple operators, facilitated by an orchestrator, enables the construction of virtually large-scale constellations. In this paper, we propose a weak-control-based orchestration framework that coordinates multiple NTN operators while ensuring that operations align with the policies of both the orchestrator and the individual operators. Unlike centralized orchestration frameworks, where the orchestrator determines the entire route from source to destination, the proposed framework allows each operator to select preferred routes from multiple candidates provided by the orchestrator. To evaluate the effectiveness of our proposed framework, we conducted numerical simulations under various scenarios and network configurations including dynamic NTN environments with time-varying topologies, showing that inter-operator cooperation improves the availability of feasible end-to-end routes. Furthermore, we analyzed the iterative negotiation process to address policy conflicts and quantitatively demonstrated the "price of autonomy," where strict individual policies degrade global feasibility and performance. The results also demonstrate that outcomes of the proposed framework depend on the operators' policies and that hop count and latency increase as the number of operators grows. These findings validate the proposed framework's ability to deliver practical benefits of orchestrated multi-operator collaboration in future NTN environments.

</details>


### [359] [Robust data-driven model-reference control of linear perturbed systems via sliding mode generation](https://arxiv.org/abs/2602.05442)
*Giorgio Riva,Gian Paolo Incremona,Simone Formentin,Antonella Ferrara*

Main category: eess.SY

TL;DR: 提出一种基于数据的积分滑模控制方案，用于增强模型参考控制器的鲁棒性，适用于具有未知动态和匹配扰动的多变量线性系统。


<details>
  <summary>Details</summary>
Motivation: 传统积分滑模控制需要系统模型知识，但在实际应用中系统动态往往未知。本文旨在开发一种不依赖系统模型的数据驱动方法，在存在匹配扰动的情况下仍能实现期望的参考模型跟踪。

Method: 将积分滑模控制重新构建为数据驱动框架，设计仅依赖于参考模型的积分滑模变量，无需系统建模。通过数据直接设计控制器，保证滑动模态生成和闭环稳定性。

Result: 提出的数据驱动积分滑模控制能够在未知系统动态和存在匹配扰动的情况下，强制实现期望的参考模型闭环行为。仿真和实验验证了理论分析的有效性。

Conclusion: 该方法成功将积分滑模控制扩展到数据驱动框架，无需系统模型知识，在匹配扰动下仍能实现鲁棒参考模型跟踪，为数据驱动控制提供了新方向。

Abstract: This paper introduces a data-based integral sliding mode control scheme for robustification of model-reference controllers, accommodating generic multivariable linear systems with unknown dynamics and affected by matched disturbances. Specifically, an integral sliding mode control (ISMC) law is recast into a data-based framework relying on an integral sliding variable depending only on the reference model, without the need of modeling the plant. The main strength of the proposed approach is the enforcement of the desired reference model in closed-loop under sliding mode conditions, despite the lack of knowledge of the model dynamics and the presence of the matched disturbances. Moreover, the conditions required to guarantee an integral sliding mode generation and the closed-loop stability are formally analyzed in the paper, remarking the generality of the proposed data-driven integral sliding mode control (DD-ISMC) with respect to the related model-based counterpart. Finally, the main practices for the data-based design of the proposed control scheme are deeply discussed in the paper, and the proposed method is tested in simulation on a benchmark example, and experimentally on a real laboratory setup. Simulation and experimental evidence fully corroborates the theoretical analysis, thus motivating further research in this direction.

</details>


### [360] [Toward Operationalizing Rasmussen: Drift Observability on the Simplex for Evolving Systems](https://arxiv.org/abs/2602.05483)
*Anatoly A. Krasnovsky*

Main category: eess.SY

TL;DR: 提出基于单纯形几何的漂移可观测性方法，用于监控软件系统向故障状态的漂移，解决欧几里得异常检测在比例信号上的局限性和架构变动导致的模型过时问题。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得异常检测在处理比例信号时容易混淆安全操作权衡与风险累积，且固定架构和模型在罕见边界事件发生前就已过时。Rasmussen的动态安全模型描述了竞争压力下的漂移，但难以在软件中实施，因为许多高价值操作信号（如工作量、剩余余量、事件影响）具有组合性且其组成部分不断演化。

Method: 提出在单纯形上实现漂移可观测性的愿景：使用Aitchison几何建模漂移和边界接近度，获得坐标不变的方向和到安全状态的距离，使用可解释的平衡坐标。监控器持续从工程工件刷新其部件清单和策略定义的边界，并应用谱系感知的聚合以保持可比性。

Result: 论文提出了早期预警诊断方法和可证伪假设的框架，但具体实验结果未在摘要中提供，需要未来评估验证。

Conclusion: 基于单纯形几何的漂移可观测性方法为解决软件系统安全监控中的关键挑战提供了有前景的方向，能够处理比例信号的组合性和架构变动问题，实现更准确的风险累积检测。

Abstract: Monitoring drift into failure is hindered by Euclidean anomaly detection that can conflate safe operational trade-offs with risk accumulation in signals expressed as shares, and by architectural churn that makes fixed schemas (and learned models) stale before rare boundary events occur. Rasmussen's dynamic safety model motivates drift under competing pressures, but operationalizing it for software is difficult because many high-value operational signals (effort, remaining margin, incident impact) are compositional and their parts evolve. We propose a vision for drift observability on the simplex: model drift and boundary proximity in Aitchison geometry to obtain coordinate-invariant direction and distance-to-safety in interpretable balance coordinates. To remain comparable under churn, a monitor would continuously refresh its part inventory and policy-defined boundaries from engineering artifacts and apply lineage-aware aggregation. We outline early-warning diagnostics and falsifiable hypotheses for future evaluation.

</details>


### [361] [Fairness-aware design of nudging policies under stochasticity and prejudices](https://arxiv.org/abs/2602.05584)
*Lisa Piccinin,Camilla Quaresmini,Edoardo Vitale,Mara Tanelli,Valentina Breschi*

Main category: eess.SY

TL;DR: 该研究提出一个考虑社会不公的创新扩散模型，并设计公平的模型预测控制方案来分配激励，通过真实数据模拟显示公平性方法能有效促进扩散而不加剧社会不平等。


<details>
  <summary>Details</summary>
Motivation: 现有创新扩散模型往往忽视社会不平等对采纳行为的影响，且激励政策可能无意中加剧这些不平等。需要开发能同时考虑扩散效果和社会公平性的框架。

Method: 扩展广义线性阈值框架，使用Beta分布为智能体分配激活阈值以捕捉不平等影响的随机性。在此基础上设计公平的模型预测控制方案，融入平等和公平目标来分配激励。

Result: 使用真实移动习惯数据的模拟显示：社会不公会降低整体采纳率；平等目标使激励分配更平滑；公平目标减少最终结果的不平等。公平性方法能确保有效扩散而不加剧现有社会不平等。

Conclusion: 将公平性目标纳入创新扩散政策设计中至关重要，既能促进技术采纳，又能避免加剧社会不平等，为政策制定提供了兼顾效率和公平的理论框架。

Abstract: We present an injustice-aware innovation-diffusion model extending the Generalized Linear Threshold framework by assigning agents activation thresholds drawn from a Beta distribution to capture the stochastic nature of adoption shaped by inequalities. Because incentive policies themselves can inadvertently amplify these inequalities, building on this model, we design a fair Model Predictive Control (MPC) scheme that incorporates equality and equity objectives for allocating incentives. Simulations using real mobility-habit data show that injustice reduces overall adoption, while equality smooths incentive distribution and equity reduces disparities in the final outcomes. Thus, incorporating fairness ensures effective diffusion without exacerbating existing social inequalities.

</details>


### [362] [Observer-based Control of Multi-agent Systems under STL Specifications](https://arxiv.org/abs/2602.05586)
*Tommaso Zaccherini,Siyuan Liu,Dimos V. Dimarogonas*

Main category: eess.SY

TL;DR: 提出一种针对大规模异构多智能体系统的分散控制器，在存在有界外部干扰且智能体需满足信号时序逻辑规范的情况下，通过k跳规定性能状态观测器解决非通信智能体间的合作问题。


<details>
  <summary>Details</summary>
Motivation: 大规模异构多智能体系统在存在外部干扰且缺乏直接通信的情况下，如何确保满足需要非通信智能体间合作的信号时序逻辑规范是一个挑战性问题。

Method: 采用分散式k跳规定性能状态观测器为每个智能体提供无法直接通信的其他智能体状态估计；利用观测器保证的状态估计误差性能边界，修改STL任务的空间鲁棒性以考虑估计误差；基于修改后的鲁棒性设计分散式连续时间反馈控制器。

Result: 提出的框架能够在最坏情况估计误差下确保STL任务的满足，并通过仿真验证了有效性。

Conclusion: 该研究为解决缺乏直接通信的大规模异构多智能体系统的STL规范满足问题提供了一种有效的分散控制方法。

Abstract: This paper proposes a decentralized controller for large-scale heterogeneous multi-agent systems subject to bounded external disturbances, where agents must satisfy Signal Temporal Logic (STL) specifications requiring cooperation among non-communicating agents. To address the lack of direct communication, we employ a decentralized k-hop Prescribed Performance State Observer (k-hop PPSO) to provide each agent with state estimates of those agents it cannot communicate with. By leveraging the performance bounds on the state estimation errors guaranteed by the k-hop PPSO, we first modify the space robustness of the STL tasks to account for these errors, and then exploit the modified robustness to design a decentralized continuous-time feedback controller that ensures satisfaction of the STL tasks even under worst-case estimation errors. A simulation result is provided to validate the proposed framework.

</details>


### [363] [UAV Trajectory Optimization via Improved Noisy Deep Q-Network](https://arxiv.org/abs/2602.05644)
*Zhang Hengyu,Maryam Cheraghy,Liu Wei,Armin Farhadi,Meysam Soltanpour,Zhong Zhuoqing*

Main category: eess.SY

TL;DR: 提出改进的Noisy DQN方法，通过结合残差噪声线性层和自适应噪声调度机制增强无人机在模拟环境中的探索能力和训练稳定性，相比标准DQN获得更快收敛和高达+40的奖励提升。


<details>
  <summary>Details</summary>
Motivation: 在无人机应用深度强化学习时，需要增强探索能力和训练稳定性，特别是在模拟环境中，传统方法可能存在收敛慢、探索不足的问题。

Method: 结合残差NoisyLinear层和自适应噪声调度机制增强探索能力，同时使用平滑损失和软目标网络更新来提高训练稳定性。

Result: 在15*15网格导航环境中，相比标准DQN，该方法实现更快收敛和高达+40的奖励提升，并快速达到任务所需的最小步数28。

Conclusion: 对NoisyNet网络结构、探索控制和训练稳定性的综合改进有效提升了深度Q学习的效率和可靠性。

Abstract: This paper proposes an Improved Noisy Deep Q-Network (Noisy DQN) to enhance the exploration and stability of Unmanned Aerial Vehicle (UAV) when applying deep reinforcement learning in simulated environments. This method enhances the exploration ability by combining the residual NoisyLinear layer with an adaptive noise scheduling mechanism, while improving training stability through smooth loss and soft target network updates. Experiments show that the proposed model achieves faster convergence and up to $+40$ higher rewards compared to standard DQN and quickly reach to the minimum number of steps required for the task 28 in the 15 * 15 grid navigation environment set up. The results show that our comprehensive improvements to the network structure of NoisyNet, exploration control, and training stability contribute to enhancing the efficiency and reliability of deep Q-learning.

</details>


### [364] [Privacy-Preserving Dynamic Average Consensus by Masking Reference Signals](https://arxiv.org/abs/2602.05803)
*Mihitha Maithripala,Zongli Lin*

Main category: eess.SY

TL;DR: 提出一种隐私保护的动态平均共识算法，通过随机数掩码保护代理的参考信号，同时保持与传统算法相同的收敛精度和速率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，动态平均共识需要交换状态信息，攻击者可能访问这些状态并推断私有信息。需要保护每个代理的参考信号免受外部窃听者和诚实但好奇的代理的攻击。

Method: 每个代理为每个邻居生成随机实数，通过加密通道在初始化时交换这些数字，计算掩码值形成掩码参考信号，然后使用掩码参考运行传统DAC算法。

Result: 收敛性和隐私性分析表明，所提算法在保护参考信号隐私的同时，与传统DAC具有相同的收敛特性。数值模拟验证了算法的有效性。

Conclusion: 成功开发了一种隐私保护的动态平均共识方法，能够在保护代理参考信号隐私的同时，实现与传统算法相同的性能和收敛特性。

Abstract: In multi-agent systems, dynamic average consensus (DAC) is a decentralized estimation strategy in which a set of agents tracks the average of time-varying reference signals. Because DAC requires exchanging state information with neighbors, attackers may gain access to these states and infer private information. In this paper, we develop a privacy-preserving method that protects each agent's reference signal from external eavesdroppers and honest-but-curious agents while achieving the same convergence accuracy and convergence rate as conventional DAC. Our approach masks the reference signals by having each agent draw a random real number for each neighbor, exchanges that number over an encrypted channel at the initialization, and computes a masking value to form a masked reference. Then the agents run the conventional DAC algorithm using the masked references. Convergence and privacy analyses show that the proposed algorithm matches the convergence properties of conventional DAC while preserving the privacy of the reference signals. Numerical simulations validate the effectiveness of the proposed privacy-preserving DAC algorithm.

</details>


### [365] [GUARDIAN: Safety Filtering for Systems with Perception Models Subject to Adversarial Attacks](https://arxiv.org/abs/2602.06026)
*Nicholas Rober,Alex Rose,Jonathan P. How*

Main category: eess.SY

TL;DR: GUARDIAN是一个安全过滤框架，为依赖神经网络状态估计器的系统提供形式化安全保证，通过神经网络验证工具和修改的Hamilton-Jacobi可达性分析来防御对抗性攻击。


<details>
  <summary>Details</summary>
Motivation: 现有安全过滤方法通常假设完美状态信息，这对于依赖神经网络状态估计器的系统存在问题，因为神经网络对噪声和对抗性输入扰动高度敏感。

Method: GUARDIAN框架：1) 运行时使用神经网络验证工具提供系统状态估计的保证边界；2) 使用修改的Hamilton-Jacobi可达性公式构建安全过滤器，基于已验证的状态边界和安全约束调整名义控制输入。

Result: 理论分析和数值实验表明，GUARDIAN能有效防御对抗性攻击，防止安全约束被违反，确保系统安全。

Conclusion: GUARDIAN提供了一个不确定性感知的安全过滤框架，为依赖神经网络状态估计器的安全关键系统提供形式化安全保证，有效应对噪声和对抗性干扰。

Abstract: Safety filtering is an effective method for enforcing constraints in safety-critical systems, but existing methods typically assume perfect state information. This limitation is especially problematic for systems that rely on neural network (NN)-based state estimators, which can be highly sensitive to noise and adversarial input perturbations. We address these problems by introducing GUARDIAN: Guaranteed Uncertainty-Aware Reachability Defense against Adversarial INterference, a safety filtering framework that provides formal safety guarantees for systems with NN-based state estimators. At runtime, GUARDIAN uses neural network verification tools to provide guaranteed bounds on the system's state estimate given possible perturbations to its observation. It then uses a modified Hamilton-Jacobi reachability formulation to construct a safety filter that adjusts the nominal control input based on the verified state bounds and safety constraints. The result is an uncertainty-aware filter that ensures safety despite the system's reliance on an NN estimator with noisy, possibly adversarial, input observations. Theoretical analysis and numerical experiments demonstrate that GUARDIAN effectively defends systems against adversarial attacks that would otherwise lead to a violation of safety constraints.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [366] [Personalized Policy Learning through Discrete Experimentation: Theory and Empirical Evidence](https://arxiv.org/abs/2602.05099)
*Zhiqi Zhang,Zhiyu Zeng,Ruohan Zhan,Dennis Zhang*

Main category: econ.EM

TL;DR: 提出DLPT框架，利用离散处理水平的RCT数据学习个性化连续策略，解决在线平台连续变量优化问题


<details>
  <summary>Details</summary>
Motivation: 当前行业实践将连续决策变量离散化进行A/B测试，但这种方法存在两个主要问题：1) 无法准确推断未测试处理水平的性能；2) 未能考虑用户特征间的异质性处理效应。这导致次优决策，特别是在优化定价、激励等连续变量时。

Method: 提出深度学习策略定位(DLPT)框架，包括个性化策略价值估计和个性化策略学习两部分。该框架基于仅包含离散处理水平的RCT观测数据，利用高维用户特征学习个性化连续策略。理论证明策略价值估计器具有渐近无偏性和一致性，学习到的策略达到根n后悔界。

Result: 与领先社交媒体平台合作，优化内容创作激励水平。实证结果显示DLPT框架显著优于现有基准方法，在评估各用户群体策略价值和识别最优个性化策略方面均取得实质性改进。

Conclusion: DLPT框架为基于离散RCT数据学习个性化连续策略提供了理论坚实且经验验证的解决方案，解决了当前行业实践中连续变量优化的局限性，在在线平台复杂运营挑战中具有重要应用价值。

Abstract: Randomized Controlled Trials (RCTs), or A/B testing, have become the gold standard for optimizing various operational policies on online platforms. However, RCTs on these platforms typically cover a limited number of discrete treatment levels, while the platforms increasingly face complex operational challenges involving optimizing continuous variables, such as pricing and incentive programs. The current industry practice involves discretizing these continuous decision variables into several treatment levels and selecting the optimal discrete treatment level. This approach, however, often leads to suboptimal decisions as it cannot accurately extrapolate performance for untested treatment levels and fails to account for heterogeneity in treatment effects across user characteristics. This study addresses these limitations by developing a theoretically solid and empirically verified framework to learn personalized continuous policies based on high-dimensional user characteristics, using observations from an RCT with only a discrete set of treatment levels. Specifically, we introduce a deep learning for policy targeting (DLPT) framework that includes both personalized policy value estimation and personalized policy learning. We prove that our policy value estimators are asymptotically unbiased and consistent, and the learned policy achieves a root-n-regret bound. We empirically validate our methods in collaboration with a leading social media platform to optimize incentive levels for content creation. Results demonstrate that our DLPT framework significantly outperforms existing benchmarks, achieving substantial improvements in both evaluating the value of policies for each user group and identifying the optimal personalized policy.

</details>


### [367] [Nested Pseudo-GMM Estimation of Demand for Differentiated Products](https://arxiv.org/abs/2602.05137)
*Victor Aguirregabiria,Hui Liu,Yao Luo*

Main category: econ.EM

TL;DR: 提出一种快速计算BLP需求模型GMM估计量的算法，通过交换GMM优化和不动点计算的顺序，避免重复求解逆需求系统，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: BLP需求模型在实证产业组织中被广泛应用，但传统估计方法计算成本高昂，特别是当产品数量增加时，需要重复求解逆需求系统，限制了模型的应用。

Method: 受动态离散选择模型中嵌套伪似然方法的启发，通过固定消费者层面的外部选择概率，使BLP的市场份额到平均效用的反演变为闭式且可分离的，从而开发出具有解析梯度的嵌套伪GMM算法。

Result: 蒙特卡洛模拟和实证应用表明，该方法比现有最快替代方法显著更快，计算效率增益随产品数量增加而超比例增长，适合并行和多线程实现。

Conclusion: 提出的嵌套伪GMM算法大幅降低了BLP模型的计算成本，使其能够处理更大规模的数据，并提供了MATLAB和Julia代码以促进实际应用。

Abstract: We propose a fast algorithm for computing the GMM estimator in the BLP demand model (Berry, Levinsohn, and Pakes, 1995). Inspired by nested pseudo-likelihood methods for dynamic discrete choice models, our approach avoids repeatedly solving the inverse demand system by swapping the order of the GMM optimization and the fixed-point computation. We show that, by fixing consumer-level outside-option probabilities, BLP's market-share to mean-utility inversion becomes closed-form and, crucially, separable across products, yielding a nested pseudo-GMM algorithm with analytic gradients. The resulting estimator scales dramatically better with the number of products and is naturally suited for parallel and multithreaded implementation. In the inner loop, outside-option probabilities are treated as fixed objects while a pseudo-GMM criterion is minimized with respect to the structural parameters, substantially reducing computational cost. Monte Carlo simulations and an empirical application show that our method is significantly faster than the fastest existing alternatives, with efficiency gains that grow more than proportionally in the number of products. We provide MATLAB and Julia code to facilitate implementation.

</details>
