<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 27]
- [stat.ML](#stat.ML) [Total: 6]
- [eess.SY](#eess.SY) [Total: 12]
- [cs.CY](#cs.CY) [Total: 14]
- [cs.LG](#cs.LG) [Total: 104]
- [math.OC](#math.OC) [Total: 12]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 4]
- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration](https://arxiv.org/abs/2602.17784)
*Meng Ye,Xiao Lin,Georgina Lukoczki,Graham W. Lederer,Yi Yao*

Main category: cs.CL

TL;DR: QueryPlot是一个语义检索和制图框架，整合地质文本语料库与地质图数据，通过自然语言查询进行矿产远景预测。


<details>
  <summary>Details</summary>
Motivation: 传统矿产远景制图需要手动整合异质地质知识，过程繁琐且依赖专家经验。需要自动化工具来整合文本矿床模型和地理空间数据，提高预测效率。

Method: 收集120多种矿床类型的描述模型，将地质图多边形转换为结构化文本表示。使用预训练嵌入模型编码查询和区域描述，计算语义相似度得分，支持组合查询和多层聚合分析。

Result: 在钨矽卡岩矿床案例中，基于嵌入的检索能高召回已知矿床，预测区域与专家定义许可区域高度一致。相似度得分作为监督学习特征可提升分类性能。

Conclusion: QueryPlot成功整合地质文本与空间数据，实现交互式矿产远景预测，支持多标准分析和机器学习集成，代码和数据已公开。

Abstract: Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.

</details>


### [2] [Neural Synchrony Between Socially Interacting Language Models](https://arxiv.org/abs/2602.17815)
*Zhining Zhang,Wentao Zhu,Chi Han,Yizhou Wang,Heng Ji*

Main category: cs.CL

TL;DR: 本文通过神经同步性分析探索LLMs的"社会心智"，发现交互中的LLMs表现出类似人类社交的神经同步现象，且与社交表现相关。


<details>
  <summary>Details</summary>
Motivation: 传统认为社会心智是生物特有属性，虽然LLMs被广泛接受为人类行为的近似，但多LLM系统能否与人类社会心智相比仍存争议。本文旨在通过神经同步性为这一辩论提供实证证据。

Method: 引入社交模拟中的神经同步性作为分析LLMs社会性的新代理指标，通过精心设计的实验验证其可靠反映社交参与度和时间对齐。

Result: LLMs之间的神经同步性与它们的社交表现强相关，揭示了神经同步性与LLMs社交行为之间的重要联系。

Conclusion: 研究为检验LLMs的"社会心智"提供了新视角，揭示了人类与LLM社交互动中内部动态的惊人相似性。

Abstract: Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the "social minds" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.

</details>


### [3] [On the scaling relationship between cloze probabilities and language model next-token prediction](https://arxiv.org/abs/2602.17848)
*Cassandra L. Jacobs,Morgan Grobol*

Main category: cs.CL

TL;DR: 大型语言模型在眼动和阅读时间数据预测上表现更好，但会低估人类反应概率。大模型能提供更高质量的下一个词预测，对词汇共现统计不敏感，但语义上与人类完形填空反应更一致。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在预测人类语言处理行为（如眼动和阅读时间）方面的表现，探索模型大小如何影响其对词汇预测和语义理解的敏感性。

Method: 通过比较不同规模的语言模型在眼动数据、阅读时间数据和完形填空任务上的表现，分析模型对词汇共现统计的敏感性和语义对齐程度。

Result: 大型语言模型在预测眼动和阅读时间数据方面表现更好，能提供更高质量的下一个词概率估计。大模型对词汇共现统计不敏感，但在语义上与人类完形填空反应更一致。所有模型都低估了人类反应概率。

Conclusion: 大型语言模型更强的记忆能力帮助它们猜测更语义合适的词，但使它们对词汇识别相关的低层次信息不敏感。这支持了模型大小影响语言处理预测能力的观点。

Abstract: Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.

</details>


### [4] [Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations](https://arxiv.org/abs/2602.17881)
*Joschka Braun*

Main category: cs.CL

TL;DR: 研究发现：引导向量的可靠性取决于训练激活差异的余弦相似度、正负激活在引导方向上的分离程度，以及潜在行为表示是否能够被线性方向有效近似。


<details>
  <summary>Details</summary>
Motivation: 引导向量是一种通过在推理时添加学习偏置来控制语言模型行为的轻量级方法，但效果在不同样本间变化很大，对许多目标行为不可靠。本研究旨在探究为什么引导可靠性在不同行为间存在差异，以及训练数据如何影响引导效果。

Method: 通过分析引导向量的训练数据特性：1) 测量训练激活差异的余弦相似度；2) 观察正负激活在引导方向上的分离程度；3) 比较不同提示变体训练的引导向量的方向差异和性能相关性。

Result: 1) 训练激活差异的余弦相似度越高，引导越可靠；2) 正负激活在引导方向上分离得越好，行为越容易被引导；3) 不同提示训练的引导向量方向不同但性能相似，且在不同数据集上的效果相关。

Conclusion: 引导向量不可靠的原因是潜在目标行为表示不能被线性引导方向有效近似。这为诊断引导不可靠性提供了实用方法，并激励开发更鲁棒的引导方法，需要显式考虑非线性潜在行为表示。

Abstract: Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.

</details>


### [5] [Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions](https://arxiv.org/abs/2602.17907)
*Raymond Li,Amirhossein Abaskohi,Chuyuan Li,Gabriel Murray,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出一种利用语言模型生成语义软标签来改进神经主题模型的方法，通过重构上下文丰富的监督信号而非传统词袋表示，显著提升主题质量和文档检索效果。


<details>
  <summary>Details</summary>
Motivation: 传统神经主题模型仅重构文档的词袋表示，忽略了上下文信息且受数据稀疏性困扰，导致主题质量不高且与语料主题结构对齐不足。

Method: 使用语言模型通过特定提示词生成下一个词的概率分布，将其投影到预定义词汇表上得到语义软标签，然后训练主题模型重构这些软标签，利用语言模型的隐藏状态作为监督信号。

Result: 在三个数据集上的实验表明，该方法在主题连贯性和纯度方面显著优于现有基线，同时新提出的检索指标显示其在识别语义相似文档方面表现优异，适用于检索导向应用。

Conclusion: 通过语言模型生成上下文丰富的语义软标签作为监督信号，能够有效提升神经主题模型的主题质量和文档检索能力，为检索应用提供了更有效的解决方案。

Abstract: Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.

</details>


### [6] [Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering](https://arxiv.org/abs/2602.17911)
*Jash Rajesh Parekh,Wonbin Kweon,Joey Chan,Rezarta Islamaj,Robert Leaman,Pengcheng Jiang,Chih-Hsuan Wei,Zhizheng Wang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 提出了CondMedQA基准和Condition-Gated Reasoning框架，用于评估和解决条件性生物医学问答问题


<details>
  <summary>Details</summary>
Motivation: 现有生物医学QA系统假设医学知识普遍适用，但真实临床推理具有条件性（取决于患者特定因素），现有基准无法评估这种条件推理，检索增强或基于图的方法缺乏确保检索知识适用于特定上下文的机制

Method: 提出Condition-Gated Reasoning框架：构建条件感知知识图谱，基于查询条件选择性地激活或剪枝推理路径

Result: CGR能更可靠地选择条件适当的答案，在生物医学QA基准上达到或超过最先进性能

Conclusion: 显式建模条件性对于稳健的医学推理至关重要，提出的框架能有效处理条件性生物医学问答问题

Abstract: Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.

</details>


### [7] [Analyzing LLM Instruction Optimization for Tabular Fact Verification](https://arxiv.org/abs/2602.17937)
*Xiaotang Du,Giwon Hong,Wai-Chung Kwan,Rohit Saxena,Ivan Titov,Pasquale Minervini,Emily Allaway*

Main category: cs.CL

TL;DR: 本文系统比较了基于DSPy优化框架的指令优化方法在表格事实验证任务中的效果，发现指令优化能持续提升验证准确率，不同优化器对不同提示技术有特定优势。


<details>
  <summary>Details</summary>
Motivation: 指令优化为提升大语言模型推理性能提供了一种轻量级、模型无关的方法，但缺乏在表格事实验证任务中的系统性比较研究。

Method: 基于DSPy优化框架，评估了四种提示技术（直接预测、思维链、带SQL工具的ReAct、带Python执行的CodeAct），并研究了三种优化器（COPRO、MiPROv2、SIMBA）在四个基准测试和三个模型家族上的表现。

Result: 指令优化持续提升验证准确率：MiPROv2对思维链提示带来最稳定的增益，SIMBA对ReAct智能体提供最大收益（尤其在更大模型规模时）。行为分析显示SIMBA通过启发式方法鼓励更直接的推理路径。

Conclusion: 思维链提示在表格事实检查中保持有效（尤其对小模型），而基于大模型的ReAct智能体虽能达到竞争性性能但需要仔细的指令优化。不同优化器针对不同提示技术有特定优势。

Abstract: Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.

</details>


### [8] [CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications](https://arxiv.org/abs/2602.17949)
*Victoria Blake,Mathew Miller,Jamie Novak,Sze-yuan Ooi,Blanca Gallego*

Main category: cs.CL

TL;DR: CUICurate：基于图检索增强生成的UMLS概念集自动构建框架，结合知识图谱检索与LLM推理，显著减少人工工作量


<details>
  <summary>Details</summary>
Motivation: 临床命名实体识别工具通常将自由文本映射到UMLS概念唯一标识符(CUIs)，但许多下游任务需要的是包含相关同义词、子类型和超类型的概念集。目前构建这样的概念集是劳动密集型的、执行不一致的，并且现有工具支持不足

Method: 提出CUICurate框架：1)构建UMLS知识图谱并进行嵌入用于语义检索；2)针对每个目标概念从KG中检索候选CUIs；3)使用LLM(GPT-5和GPT-5-mini)进行过滤和分类；4)在五个词汇异质性临床概念上评估，与人工基准和黄金标准概念集比较

Result: CUICurate生成的概念集比人工基准更大更完整，同时保持与人类相当的精确度。GPT-5-mini在过滤阶段召回率更高，GPT-5的分类结果更接近临床医生判断。输出稳定且计算成本低

Conclusion: CUICurate为UMLS概念集构建提供了可扩展、可重复的方法，显著减少人工工作。通过整合基于图的检索和LLM推理，该框架生成聚焦的候选概念集，可适应不同表型和分析需求的临床NLP流程

Abstract: Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.

</details>


### [9] [Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering](https://arxiv.org/abs/2602.17981)
*Amine Kobeissi,Philippe Langlais*

Main category: cs.CL

TL;DR: 该论文研究了金融问答中检索增强生成的失败模式：虽然检索到了正确文档，但遗漏了包含答案的具体页面或块，导致生成器基于不完整上下文进行推断。作者提出了针对金融监管文件进行领域微调的页面评分器，显著提升了页面召回率和块检索性能。


<details>
  <summary>Details</summary>
Motivation: 金融问答中检索增强生成的可靠性取决于能否检索到确切的上下文来支持答案。当前存在一个常见失败模式：检索到了正确文档但遗漏了包含答案的具体页面或块，导致生成器基于不完整上下文进行推断。尽管这在实践中很重要，但金融QA文献中对此缺乏系统性研究。

Method: 1. 在多粒度级别（文档、页面、块）评估检索性能；2. 引入基于oracle的分析方法，为检索和生成性能提供经验上限；3. 在FinanceBench的150个问题子集上，复现和比较多种检索策略（稠密、稀疏、混合、分层方法，包括重排序和查询重构）；4. 提出领域微调的页面评分器，将页面作为文档和块之间的中间检索单元，专门针对金融监管文件微调双编码器，利用页面的语义连贯性。

Result: 1. 不同方法中，文档发现率的提升往往转化为更强的页面召回率；2. 但oracle性能仍表明页面和块级检索存在改进空间；3. 提出的领域微调页面评分器在页面召回率和块检索方面取得了显著改进。

Conclusion: 该研究揭示了金融问答中检索增强生成的关键失败模式，并提出了针对性的解决方案。通过领域微调的页面级检索方法，能够有效提高金融监管文件问答系统的可靠性和准确性，为高风险的金融应用场景提供了更可靠的检索增强生成框架。

Abstract: Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.

</details>


### [10] [Towards More Standardized AI Evaluation: From Models to Agents](https://arxiv.org/abs/2602.18029)
*Ali El Filali,Inès Bedar*

Main category: cs.CL

TL;DR: 论文认为传统基于静态基准测试和聚合分数的评估方法已不适用于AI代理系统，需要将评估重新定位为测量学科而非性能剧场，以建立对非确定性系统的信任、迭代和治理。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从静态模型发展为复合型、使用工具的多智能体系统，评估的角色需要从最终检查点转变为核心控制功能。传统评估方法基于模型中心时代的假设（静态基准、聚合分数、一次性成功标准），这些方法在评估代理系统时变得模糊而非清晰，无法回答"我们能否信任系统在变化和规模下按预期运行"这一核心问题。

Method: 论文通过分析评估管道本身引入的静默故障模式，探讨为什么高基准分数经常误导团队，以及智能体系统如何从根本上改变性能测量的意义。作者没有提出新的指标或更难的基准测试，而是重新审视评估在AI时代（特别是对于智能体）的角色定位。

Result: 论文揭示了传统评估方法在代理系统中的局限性：评估管道会引入静默故障模式，高基准分数经常产生误导性结果，代理系统的非确定性特性需要完全不同的评估范式。

Conclusion: 评估应该从"性能剧场"转变为测量学科，作为建立对非确定性系统信任、支持迭代开发和实现有效治理的核心机制。评估需要成为AI系统生命周期的持续控制功能，而非一次性检查点。

Abstract: Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.

</details>


### [11] [Perceived Political Bias in LLMs Reduces Persuasive Abilities](https://arxiv.org/abs/2602.18092)
*Matthew DiGiuseppe,Joshua Robison*

Main category: cs.CL

TL;DR: 研究发现，当用户认为AI聊天机器人存在党派偏见时，其纠正错误观念的说服力会下降28%，表明AI的政治中立性感知对其说服效果至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs进入党派冲突领域，精英阶层越来越多地将它们描绘成具有意识形态倾向。本研究旨在测试这些可信度攻击是否会降低基于LLM的说服效果，探讨AI的政治中立性感知对其纠正公共误解能力的影响。

Method: 在美国进行了一项预注册调查实验（N=2144），参与者与ChatGPT进行三轮对话，讨论个人持有的经济政策误解。实验组在对话前收到简短信息，表明LLM对参与者所属党派存在偏见，对照组则保持中立。

Result: 与中立对照组相比，收到AI存在党派偏见警告的参与者，其被说服程度降低了28%。对话记录分析显示，警告改变了互动模式：受访者更频繁地反驳，且参与度更不开放。

Conclusion: 对话式AI的说服效果具有政治条件性，受到党派一致性感知的限制。当用户认为AI存在党派偏见时，其纠正错误观念的有效性会显著降低，这凸显了保持AI政治中立性感知的重要性。

Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.

</details>


### [12] [Agentic Adversarial QA for Improving Domain-Specific LLMs](https://arxiv.org/abs/2602.18137)
*Vincent Grari,Ciprian Tomoiaga,Sylvain Lamprier,Tatsunori Hashimoto,Marcin Detyniecki*

Main category: cs.CL

TL;DR: 提出对抗性问答生成框架，通过比较目标模型与专家模型输出，生成紧凑的语义挑战性问题，提升专业领域适应效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域适应困难，现有合成数据方法存在两个问题：1) 缺乏对解释性推理能力的支持；2) 生成数据冗余且样本效率低

Method: 提出对抗性问答生成框架，通过迭代反馈过程比较目标模型与基于参考文档的专家模型输出，生成揭示理解差距的语义挑战性问题

Result: 在LegalBench专业子集评估显示，该方法用更少的合成样本实现了更高的准确率

Conclusion: 对抗性问答生成框架能有效提升大语言模型在专业领域的适应效率，解决现有合成数据方法的局限性

Abstract: Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.

</details>


### [13] [Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention](https://arxiv.org/abs/2602.18145)
*Siya Qi,Yudong Chen,Runcong Zhao,Qinglin Zhu,Zhanghao Hu,Wei Liu,Yulan He,Zheng Yuan,Lin Gui*

Main category: cs.CL

TL;DR: 本文提出了一种基于频率分析的注意力机制检测方法，用于识别大语言模型在上下文生成中的幻觉问题。通过将注意力分布建模为离散信号并提取高频成分，揭示了幻觉token与高频注意力能量之间的关联，并开发了轻量级幻觉检测器。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的幻觉检测方法通常依赖粗粒度汇总，无法捕捉注意力中的细粒度不稳定性。需要更精细的方法来分析注意力模式，以更好地检测大语言模型在上下文生成中的幻觉问题。

Method: 引入频率感知的注意力分析视角，将注意力分布建模为离散信号，提取反映注意力快速局部变化的高频成分。基于高频注意力特征开发轻量级幻觉检测器。

Result: 在RAGTruth和HalluRAG基准测试中，该方法在多个模型和任务上超越了基于验证、内部表示和注意力的现有方法，取得了性能提升。

Conclusion: 注意力频率分析为幻觉检测提供了新的有效视角，高频注意力能量反映了幻觉token的碎片化和不稳定接地行为，基于此开发的检测器具有优越性能。

Abstract: Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.

</details>


### [14] [The Statistical Signature of LLMs](https://arxiv.org/abs/2602.18152)
*Ortal Hadad,Edoardo Loru,Jacopo Nudo,Niccolò Di Marco,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.CL

TL;DR: 论文通过无损压缩分析发现LLM生成文本比人类文本具有更高的结构规律性和可压缩性，揭示了概率生成的结构特征，但在小尺度交互环境中这种区分会减弱。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大语言模型通过概率采样生成文本时，如何重塑语言的结构统计组织。目前对这一过程如何改变文本的统计规律性特征尚未完全表征。

Method: 采用无损压缩作为模型无关的统计规律性度量方法，分析三种信息生态系统：受控的人类-LLM延续、知识基础设施的生成中介（维基百科 vs Grokipedia）、完全合成的社交互动环境（Moltbook vs Reddit）。

Result: 在受控和中介环境中，LLM生成的语言比人类文本表现出更高的结构规律性和可压缩性，输出集中在高度循环的统计模式中。但在碎片化交互环境中，这种区分会减弱，表明小尺度表面可区分性存在根本限制。

Conclusion: 无损压缩提供了一个简单稳健的框架来量化生成系统如何重塑文本生产，为理解通信复杂性演变提供了结构视角。这种可压缩性区分在不同模型、任务和领域中一致出现，可直接从表面文本观察到。

Abstract: Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.

</details>


### [15] [FENCE: A Financial and Multimodal Jailbreak Detection Dataset](https://arxiv.org/abs/2602.18154)
*Mirae Kim,Seonghun Jeong,Youngjun Kwak*

Main category: cs.CL

TL;DR: FENCE是一个针对金融领域的双语多模态越狱检测数据集，用于训练和评估越狱检测器，在金融应用中提高AI系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和视觉语言模型存在越狱风险，特别是在金融等敏感领域。目前缺乏针对金融领域的多模态越狱检测资源，需要专门的金融相关数据集来训练可靠的检测模型。

Method: 创建了FENCE数据集，包含韩英双语的多模态金融相关查询，结合图像基础的威胁内容。使用商业和开源VLM进行实验，并训练基线检测器评估数据集效果。

Result: 实验显示GPT-4o存在可测量的攻击成功率，开源模型更易受攻击。基于FENCE训练的基线检测器达到99%的分布内准确率，在外部基准测试中表现良好。

Conclusion: FENCE为金融领域多模态越狱检测提供了重要资源，有助于开发更安全可靠的AI系统，特别是在敏感领域。数据集展示了实际应用价值。

Abstract: Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.

</details>


### [16] [Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models](https://arxiv.org/abs/2602.18171)
*Wojciech Michaluk,Tymoteusz Urban,Mateusz Kubita,Soveatin Kuntur,Anna Wroblewska*

Main category: cs.CL

TL;DR: 提出结合Transformer文本嵌入与语言学信息特征的混合方法检测点击诱饵标题，最佳模型F1分数达91%，优于传统基线方法


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题降低在线信息质量并损害用户信任，需要有效检测方法

Method: 混合方法：结合Transformer文本嵌入与15个语言学信息特征，使用XGBoost分类器

Result: 最佳模型F1分数91%，优于TF-IDF、Word2Vec、GloVe、LLM提示分类和纯特征基线

Conclusion: 提出的特征集增强可解释性，突出第二人称代词、最高级、数字等语言学线索，实现透明且校准良好的点击诱饵预测

Abstract: Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.

</details>


### [17] [Improving Sampling for Masked Diffusion Models via Information Gain](https://arxiv.org/abs/2602.18176)
*Kaisen Yang,Jayden Teoh,Kaicheng Yang,Yitong Zhang,Alex Lamb*

Main category: cs.CL

TL;DR: 提出Info-Gain Sampler，一种用于掩码扩散模型的新解码框架，通过平衡即时不确定性和未来信息增益来提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型解码器采用贪心策略，只关注局部确定性，忽略了当前解码决策对后续步骤的影响，未能充分利用MDMs的非因果特性来最小化累积不确定性。

Method: 提出信息增益采样器，在解码时不仅考虑当前位置的不确定性，还评估当前决策如何改变所有剩余掩码位置的token概率/不确定性，平衡即时不确定性和未来信息增益。

Result: 在推理、编程、创意写作和图像生成等多种任务和架构上，Info-Gain Sampler始终优于现有采样器：推理任务平均准确率提升3.6%，创意写作胜率63.1%，推理任务累积不确定性从78.4降至48.6。

Conclusion: Info-Gain Sampler通过利用MDMs的非因果特性，系统性地考虑解码决策的未来影响，显著提升了掩码扩散模型的生成质量，为MDMs提供了更有效的解码框架。

Abstract: Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.

</details>


### [18] [Information-Theoretic Storage Cost in Sentence Comprehension](https://arxiv.org/abs/2602.18217)
*Kohei Kajikawa,Shinnosuke Isono,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 该研究提出了一种基于信息论的句子理解加工存储成本度量方法，使用预训练神经语言模型估计先前词语对未来上下文的信息量，验证了其在英语中的有效性。


<details>
  <summary>Details</summary>
Motivation: 实时句子理解对工作记忆产生显著负荷，但现有基于符号语法的度量方法使用离散、统一的句法预测成本。需要一种连续、理论中立且能从预训练语言模型估计的度量方法。

Method: 提出基于信息论的加工存储成本度量，定义为先前词语在不确定性条件下对未来上下文所携带的信息量。使用预训练神经语言模型进行估计，通过三个英语分析验证有效性。

Result: 该度量方法：(1)恢复了中心嵌套和关系从句中已知的加工不对称性；(2)在句法标注语料库中与基于语法的存储成本相关；(3)在两个大规模自然数据集中预测阅读时间方差，优于传统信息基线的预测模型。

Conclusion: 提出的信息论度量方法为句子理解中的加工存储成本提供了连续、理论中立且可计算的替代方案，能够捕捉传统离散度量无法捕捉的加工负荷细微差别。

Abstract: Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.

</details>


### [19] [Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning](https://arxiv.org/abs/2602.18232)
*Lexiang Tang,Weihao Gao,Bingchen Zhao,Lu Ma,Qiao jin,Bang Yang,Yuexian Zou*

Main category: cs.CL

TL;DR: 提出Confidence-Driven Contrastive Decoding方法，通过检测低置信度token并选择性干预，提升大语言模型推理可靠性，同时减少输出长度


<details>
  <summary>Details</summary>
Motivation: 现有研究假设增加推理计算量能均匀提升正确性，但研究发现推理不确定性高度局部化：少数低置信度token对推理错误和不必要输出扩展贡献不成比例

Method: 提出Thinking by Subtraction方法，在解码过程中检测低置信度token，构建对比参考分布（用最小占位符替换高置信度token），在低置信位置通过减去参考分布来精炼预测

Result: CCD显著提升数学推理基准的准确性，同时大幅减少输出长度，KV-cache开销最小，作为无需训练的方法有效提升推理可靠性

Conclusion: 通过针对性低置信度干预，CCD方法能提升大语言模型推理可靠性，避免计算冗余，实现更高效的推理过程

Abstract: Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.

</details>


### [20] [Simplifying Outcomes of Language Model Component Analyses with ELIA](https://arxiv.org/abs/2602.18262)
*Aaron Louis Eidt,Nils Feldhus*

Main category: cs.CL

TL;DR: ELIA是一个交互式Web应用，通过整合多种LLM分析工具和AI生成的自然语言解释，降低机制可解释性的使用门槛，使非专家也能理解复杂模型分析。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性工具虽然强大，但过于复杂，仅限于专家使用，存在可访问性差距。需要设计更易用的工具让更广泛的受众能够理解语言模型分析结果。

Method: 构建ELIA交互式Web应用，整合归因分析、函数向量分析和电路追踪三种关键技术，并创新性地使用视觉语言模型为复杂可视化自动生成自然语言解释。

Result: 混合方法用户研究表明，用户明显偏好交互式可探索界面而非静态可视化。AI生成的解释帮助非专家理解，统计显示用户先前LLM经验与理解分数无显著相关性，说明系统降低了不同经验水平的理解障碍。

Conclusion: AI系统确实可以简化复杂模型分析，但其真正潜力在于与以用户为中心的设计相结合，优先考虑交互性、特异性和叙事引导，从而降低理解门槛。

Abstract: While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.

</details>


### [21] [PsihoRo: Depression and Anxiety Romanian Text Corpus](https://arxiv.org/abs/2602.18324)
*Alexandra Ciobotaru,Ana-Maria Bucur,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 创建了首个罗马尼亚语抑郁和焦虑心理健康语料库PsihoRo，包含205名受访者的文本数据，通过开放式问题和标准化问卷收集，填补了罗马尼亚语心理健康NLP资源的空白。


<details>
  <summary>Details</summary>
Motivation: 目前罗马尼亚语缺乏开源的心理健康语料库，而英语已有丰富的心理NLP资源。社交媒体数据收集存在假设偏差问题，需要更实用的数据收集方法。

Method: 使用包含6个开放式问题的表格，结合标准化的PHQ-9和GAD-7筛查问卷收集数据。对205名受访者的文本进行统计分析、罗马尼亚语LIWC文本分析、情感检测和主题建模。

Result: 创建了首个罗马尼亚语抑郁和焦虑语料库PsihoRo，虽然样本量较小（205名受访者），但为分析罗马尼亚人口心理健康文本迈出了第一步。

Conclusion: PsihoRo填补了罗马尼亚语心理健康NLP资源的空白，展示了该新资源的重要特征，为未来研究罗马尼亚人口的心理健康提供了基础。

Abstract: Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.

</details>


### [22] [Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning](https://arxiv.org/abs/2602.18326)
*Tao Wu,Adam Kapelner*

Main category: cs.CL

TL;DR: 该研究开发了一个深度学习系统，用于为高中生第一语言词汇教学自动筛选优质上下文例句，通过对比三种建模方法，发现结合人工特征监督的Qwen3嵌入模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为高中生词汇教学自动寻找高质量的上下文例句，传统方法成本高且效率低，需要开发自动化的深度学习系统来大规模生成优质教学材料。

Method: 比较了三种方法：(1) 基于MPNet无监督相似性的方法；(2) 基于Qwen3嵌入微调的有监督非线性回归模型；(3) 在方法(2)基础上加入人工设计特征。提出了新的评估指标"保持能力曲线"来可视化模型性能。

Result: 模型(3)表现最佳，在仅丢弃70%优质上下文的情况下，实现了440:1的"优质-劣质"上下文比例，显著优于其他方法。

Conclusion: 研究表明，结合人类监督的现代嵌入模型和神经网络架构能够低成本地大规模生成近乎完美的词汇教学上下文，适用于多种目标词汇的教学。

Abstract: We describe a modern deep learning system that automatically identifies informative contextual examples (\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.

</details>


### [23] [Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System](https://arxiv.org/abs/2602.18346)
*Pavithra PM Nair,Preethu Rose Anish*

Main category: cs.CL

TL;DR: Vichara是一个针对印度司法系统的框架，通过分解上诉案件文件为决策点，使用LLM预测和解释上诉判决，在多个数据集上超越了现有基准。


<details>
  <summary>Details</summary>
Motivation: 印度法院面临大量案件积压，上诉案件是其中的重要组成部分。人工智能在预测法律判决方面具有变革潜力，但需要专门针对印度司法系统设计框架。

Method: Vichara框架处理英文上诉案件文件，将其分解为决策点（包含法律问题、决定机构、结果、推理和时间背景的结构化表示）。使用IRAC框架的变体生成解释，并在四个LLM（GPT-4o mini、Llama-3.1-8B、Mistral-7B、Qwen2.5-7B）上进行评估。

Result: Vichara在PredEx和ILDC_expert数据集上超越了现有判决预测基准，GPT-4o mini表现最佳（F1: 81.5 on PredEx, 80.3 on ILDC_expert）。人类评估显示GPT-4o mini在清晰度、关联性和实用性方面具有最佳可解释性。

Conclusion: Vichara为印度司法系统提供了一个有效的判决预测和解释框架，通过结构化决策点表示和IRAC风格的推理，既提高了预测准确性，又增强了可解释性，有助于缓解案件积压问题。

Abstract: In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.

</details>


### [24] [Validating Political Position Predictions of Arguments](https://arxiv.org/abs/2602.18351)
*Jordan Robinson,Angus R. Williams,Katie Atkinson,Anthony G. Cohn*

Main category: cs.CL

TL;DR: 提出双尺度验证框架，结合点对点和成对人工标注，用于政治立场预测的主观连续知识表示，构建大规模政治立场知识库，证明可以从点对点语言模型预测中提取序数结构。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识表示常需捕捉主观连续属性（如政治立场），这与广泛接受的成对验证黄金标准相冲突。需要解决主观连续知识验证的挑战。

Method: 采用双尺度验证框架，结合点对点和成对人工标注。使用22个语言模型，构建包含23,228个论点的大规模政治立场预测知识库，数据来自30场英国政治电视节目《Question Time》的辩论。

Result: 点对点评估显示中等的人机一致性（Krippendorff's α=0.578），反映内在主观性；而成对验证显示人机排名间显著更强的对齐（最佳模型α=0.86）。

Conclusion: 提出实用验证方法平衡可扩展性与可靠性；构建验证的结构化论证知识库支持图基推理和检索增强生成；证明可以从点对点语言模型预测中提取序数结构，推进传统符号或分类方法不足领域的知识表示能力。

Abstract: Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.

</details>


### [25] [SPQ: An Ensemble Technique for Large Language Model Compression](https://arxiv.org/abs/2602.18420)
*Jiamin Yao,Eren Gultepe*

Main category: cs.CL

TL;DR: SPQ是一种集成压缩技术，结合SVD、剪枝和量化三种方法，针对LLM的不同低效源进行压缩，在保持性能的同时显著减少内存占用并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署面临内存占用大、推理速度慢的问题，需要高效的压缩技术来在保持模型性能的同时减少资源消耗，便于在实际环境中部署。

Method: SPQ集成三种互补技术：1) 基于激活的剪枝去除MLP层中的冗余神经元；2) 保留方差的SVD将注意力投影分解为紧凑的低秩因子；3) 8位线性量化均匀压缩所有线性层。

Result: 在LLaMA-2-7B上，SPQ实现高达75%的内存减少，同时困惑度改善（如WikiText-2从5.47降至4.91），在下游任务上保持准确性。相比GPTQ和SparseGPT，SPQ使用更少内存（6.86GB vs 7.16GB）且推理速度提升达1.9倍。

Conclusion: SPQ通过层感知和互补的压缩技术实现了鲁棒的压缩效果，证明了集成多种压缩方法的优势，为内存受限环境中的LLM实际部署提供了实用解决方案。

Abstract: This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/

</details>


### [26] [RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering](https://arxiv.org/abs/2602.18425)
*Deniz Qian,Hung-Ting Chen,Eunsol Choi*

Main category: cs.CL

TL;DR: RVR是一个多轮检索框架，通过检索-验证-检索的迭代过程最大化答案覆盖率，在多种数据集上显著提升完整召回率。


<details>
  <summary>Details</summary>
Motivation: 针对需要广泛有效答案的查询，现有检索方法难以全面覆盖所有可能的答案，需要一种能够最大化答案覆盖率的检索框架。

Method: 提出检索-验证-检索（RVR）多轮框架：第一轮检索器处理原始查询返回候选文档，验证器识别高质量子集；后续轮次将查询与已验证文档结合，发现之前未覆盖的答案。

Result: 在QAMPARI数据集上相对提升至少10%，绝对提升3%的完整召回率；在QUEST和WebQuestionsSP两个域外数据集上也有稳定提升，优于包括智能搜索方法在内的基线。

Conclusion: RVR展示了利用验证器和适应新推理场景的迭代检索方法在全面答案召回方面的潜力，即使使用现成检索器也有效，微调后效果更佳。

Abstract: Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.

</details>


### [27] [VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning](https://arxiv.org/abs/2602.18429)
*Harshul Raj Surana,Arijit Maji,Aryan Vats,Akash Ghosh,Sriparna Saha,Amit Sheth*

Main category: cs.CL

TL;DR: 提出了VIRAASAT数据集和SCoM框架，用于评估和改进LLMs在印度文化多跳推理任务上的表现


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在需要丰富社会文化知识和本地背景的任务（特别是印度文化）上表现不佳，而现有文化基准存在手动构建、单跳问题、难以扩展等问题

Method: 1) 创建VIRAASAT数据集：基于包含700多个专家策划文化工件的知识图谱，覆盖印度13个文化属性和所有行政区划，生成3200多个多跳问题；2) 提出SCoM框架：训练模型内部模拟知识图谱的原子操作，可靠遍历图谱拓扑结构

Result: 评估显示当前SOTA LLMs在VIRAASAT上表现不佳，特别是微调CoT跟踪无法处理低概率事实的接地和合成。SCoM在监督微调实验中比标准CoT基线提升高达20%

Conclusion: VIRAASAT数据集和SCoM框架为构建文化感知推理模型奠定了坚实基础，解决了LLMs在文化特定多跳推理任务上的局限性

Abstract: Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [28] [Topological Exploration of High-Dimensional Empirical Risk Landscapes: general approach, and applications to phase retrieval](https://arxiv.org/abs/2602.17779)
*Antoine Maillard,Tony Bonnaire,Giulio Biroli*

Main category: stat.ML

TL;DR: 使用Kac-Rice公式分析高维高斯单指标模型经验风险最小化的临界点景观，推导出简化的变分公式，并应用于实相位恢复问题，预测梯度流动力学。


<details>
  <summary>Details</summary>
Motivation: 研究高维统计模型中经验风险最小化的损失函数景观拓扑结构，特别是在比例渐近机制下，理解临界点的分布和性质对优化算法行为的影响。

Method: 使用Kac-Rice公式分析不同临界点类型的复杂度，推导简化为有限标量参数的变分问题，应用于实相位恢复问题，并通过数值模拟验证理论预测。

Result: 建立了简化的变分公式，能够高效数值求解；预测了临界点的Hessian谱性质和标签联合分布；在实相位恢复中获得了完整的拓扑相图，发现了BBP型转变；理论预测与有限尺寸模拟高度一致。

Conclusion: 该方法为高维统计模型中损失景观的渐近研究和拓扑平凡化现象开辟了新途径，能够精细预测优化算法行为和临界点性质。

Abstract: We consider the landscape of empirical risk minimization for high-dimensional Gaussian single-index models (generalized linear models). The objective is to recover an unknown signal $\boldsymbolθ^\star \in \mathbb{R}^d$ (where $d \gg 1$) from a loss function $\hat{R}(\boldsymbolθ)$ that depends on pairs of labels $(\mathbf{x}_i \cdot \boldsymbolθ, \mathbf{x}_i \cdot \boldsymbolθ^\star)_{i=1}^n$, with $\mathbf{x}_i \sim \mathcal{N}(0, I_d)$, in the proportional asymptotic regime $n \asymp d$. Using the Kac-Rice formula, we analyze different complexities of the landscape -- defined as the expected number of critical points -- corresponding to various types of critical points, including local minima. We first show that some variational formulas previously established in the literature for these complexities can be drastically simplified, reducing to explicit variational problems over a finite number of scalar parameters that we can efficiently solve numerically. Our framework also provides detailed predictions for properties of the critical points, including the spectral properties of the Hessian and the joint distribution of labels. We apply our analysis to the real phase retrieval problem for which we derive complete topological phase diagrams of the loss landscape, characterizing notably BBP-type transitions where the Hessian at local minima (as predicted by the Kac-Rice formula) becomes unstable in the direction of the signal. We test the predictive power of our analysis to characterize gradient flow dynamics, finding excellent agreement with finite-size simulations of local optimization algorithms, and capturing fine-grained details such as the empirical distribution of labels. Overall, our results open new avenues for the asymptotic study of loss landscapes and topological trivialization phenomena in high-dimensional statistical models.

</details>


### [29] [Drift Estimation for Stochastic Differential Equations with Denoising Diffusion Models](https://arxiv.org/abs/2602.17830)
*Marcos Tapia Costa,Nikolas Kantas,George Deligiannidis*

Main category: stat.ML

TL;DR: 提出基于条件扩散模型的漂移函数估计方法，用于多元随机微分方程，在高维情况下保持竞争力


<details>
  <summary>Details</summary>
Motivation: 研究多元随机微分方程中时间齐次漂移函数的估计问题，已知扩散系数，从高频观测的多个轨迹中估计漂移函数

Method: 将漂移估计转化为基于先前观测的去噪问题，通过训练条件扩散模型来估计漂移函数，该模型能够动态模拟新轨迹

Result: 在不同漂移函数类别中，该方法在低维情况下与传统方法相当，在高维情况下保持竞争力，其优势不能仅归因于架构设计选择

Conclusion: 基于条件扩散模型的漂移函数估计方法在高维随机微分方程中具有竞争力，为漂移估计提供了新的有效途径

Abstract: We study the estimation of time-homogeneous drift functions in multivariate stochastic differential equations with known diffusion coefficient, from multiple trajectories observed at high frequency over a fixed time horizon. We formulate drift estimation as a denoising problem conditional on previous observations, and propose an estimator of the drift function which is a by-product of training a conditional diffusion model capable of simulating new trajectories dynamically. Across different drift classes, the proposed estimator was found to match classical methods in low dimensions and remained consistently competitive in higher dimensions, with gains that cannot be attributed to architectural design choices alone.

</details>


### [30] [Interactive Learning of Single-Index Models via Stochastic Gradient Descent](https://arxiv.org/abs/2602.17876)
*Nived Rajaraman,Yanjun Han*

Main category: stat.ML

TL;DR: SGD在单指数模型的序列学习问题中表现出色，通过适当的学习率调度，能在"燃烧期"和"学习期"同时实现接近最优的样本复杂度和遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 虽然SGD在高维非线性模型的特征学习方面已有深入理论理解，但在单指数模型的序列学习问题（如广义线性赌博机）中，SGD的学习动态仍未被充分探索。

Method: 研究SGD在单指数模型序列学习中的动态，分析其"燃烧期"和"学习期"两个阶段，并设计适当的学习率调度策略。

Result: SGD与最优交互学习器类似，经历明显的"燃烧期"后进入"学习期"。通过适当的学习率调度，单个SGD过程能在两个阶段同时实现接近最优的样本复杂度和遗憾保证。

Conclusion: SGD在自适应数据下学习单指数模型时仍然具有高度竞争力，能够同时实现优秀的样本效率和遗憾性能。

Abstract: Stochastic gradient descent (SGD) is a cornerstone algorithm for high-dimensional optimization, renowned for its empirical successes. Recent theoretical advances have provided a deep understanding of how SGD enables feature learning in high-dimensional nonlinear models, most notably the \textit{single-index model} with i.i.d. data. In this work, we study the sequential learning problem for single-index models, also known as generalized linear bandits or ridge bandits, where SGD is a simple and natural solution, yet its learning dynamics remain largely unexplored. We show that, similar to the optimal interactive learner, SGD undergoes a distinct ``burn-in'' phase before entering the ``learning'' phase in this setting. Moreover, with an appropriately chosen learning rate schedule, a single SGD procedure simultaneously achieves near-optimal (or best-known) sample complexity and regret guarantees across both phases, for a broad class of link functions. Our results demonstrate that SGD remains highly competitive for learning single-index models under adaptive data.

</details>


### [31] [Learning from Biased and Costly Data Sources: Minimax-optimal Data Collection under a Budget](https://arxiv.org/abs/2602.17894)
*Michael O. Harding,Vikas Singh,Kirthevasan Kandasamy*

Main category: stat.ML

TL;DR: 提出一种多源数据收集的优化采样策略，通过最大化有效样本量来估计总体均值和组条件均值，在固定预算下达到最小最大最优风险。


<details>
  <summary>Details</summary>
Motivation: 在多源异构数据收集场景中（如医学研究、政治民调），不同数据源有不同的采样成本，且组别分布差异显著。传统的数据收集策略（如匹配目标分布）或标准估计器（如样本均值）往往效果不佳，需要更优的方法在固定预算下进行有效估计。

Method: 开发了一种最大化有效样本量的采样计划：有效样本量 = 总样本量 / (D_χ²(q||p̄) + 1)，其中q是目标分布，p̄是聚合源分布，D_χ²是χ²散度。将该采样计划与经典的后分层估计器结合，并给出风险上界。

Result: 提供了匹配的下界，证明该方法达到了预算约束下的最小最大最优风险。技术还扩展到预测问题中的超额风险最小化，为多源学习提供了原则性方法。

Conclusion: 该方法为多源异构数据收集提供了理论保证的最优策略，特别适用于采样成本不同且组别分布差异显著的应用场景，如医学研究和政治民调。

Abstract: Data collection is a critical component of modern statistical and machine learning pipelines, particularly when data must be gathered from multiple heterogeneous sources to study a target population of interest. In many use cases, such as medical studies or political polling, different sources incur different sampling costs. Observations often have associated group identities (for example, health markers, demographics, or political affiliations) and the relative composition of these groups may differ substantially, both among the source populations and between sources and target population.
  In this work, we study multi-source data collection under a fixed budget, focusing on the estimation of population means and group-conditional means. We show that naive data collection strategies (e.g. attempting to "match" the target distribution) or relying on standard estimators (e.g. sample mean) can be highly suboptimal. Instead, we develop a sampling plan which maximizes the effective sample size: the total sample size divided by $D_{χ^2}(q\mid\mid\overline{p}) + 1$, where $q$ is the target distribution, $\overline{p}$ is the aggregated source distribution, and $D_{χ^2}$ is the $χ^2$-divergence. We pair this sampling plan with a classical post-stratification estimator and upper bound its risk. We provide matching lower bounds, establishing that our approach achieves the budgeted minimax optimal risk. Our techniques also extend to prediction problems when minimizing the excess risk, providing a principled approach to multi-source learning with costly and heterogeneous data sources.

</details>


### [32] [On the Generalization and Robustness in Conditional Value-at-Risk](https://arxiv.org/abs/2602.18053)
*Dinesh Karthik Mulumudi,Piyushi Manupriya,Gholamali Aminian,Anant Raj*

Main category: stat.ML

TL;DR: 本文系统分析了在重尾和污染数据下CVaR经验风险最小化的统计性质，建立了最优的泛化界和超额风险界，揭示了CVaR在重尾数据下的内在不稳定性。


<details>
  <summary>Details</summary>
Motivation: CVaR是广泛使用的风险敏感目标函数，但在重尾数据下的统计行为尚不清楚。CVaR依赖于内生的、数据依赖的分位数，这改变了泛化和鲁棒性特性，需要系统分析。

Method: 开发了CVaR经验风险最小化的学习理论分析，建立了高概率泛化界和超额风险界，提出了截断中位数均值CVaR估计器，并推导了统一的Bahadur-Kiefer型展开来隔离阈值驱动误差。

Result: 在最小矩假设下建立了尖锐的最优泛化界，覆盖了固定假设、有限和无限类别以及β混合依赖数据。证明了CVaR决策在重尾数据下可能本质不稳定，即使总体最优解是良好分离的。

Conclusion: 研究系统刻画了CVaR学习何时能够泛化和保持鲁棒性，以及何时由于尾部稀缺性导致不稳定性不可避免，为CVaR在重尾数据下的应用提供了理论基础。

Abstract: Conditional Value-at-Risk (CVaR) is a widely used risk-sensitive objective for learning under rare but high-impact losses, yet its statistical behavior under heavy-tailed data remains poorly understood. Unlike expectation-based risk, CVaR depends on an endogenous, data-dependent quantile, which couples tail averaging with threshold estimation and fundamentally alters both generalization and robustness properties. In this work, we develop a learning-theoretic analysis of CVaR-based empirical risk minimization under heavy-tailed and contaminated data. We establish sharp, high-probability generalization and excess risk bounds under minimal moment assumptions, covering fixed hypotheses, finite and infinite classes, and extending to $β$-mixing dependent data; we further show that these rates are minimax optimal. To capture the intrinsic quantile sensitivity of CVaR, we derive a uniform Bahadur-Kiefer type expansion that isolates a threshold-driven error term absent in mean-risk ERM and essential in heavy-tailed regimes. We complement these results with robustness guarantees by proposing a truncated median-of-means CVaR estimator that achieves optimal rates under adversarial contamination. Finally, we show that CVaR decisions themselves can be intrinsically unstable under heavy tails, establishing a fundamental limitation on decision robustness even when the population optimum is well separated. Together, our results provide a principled characterization of when CVaR learning generalizes and is robust, and when instability is unavoidable due to tail scarcity.

</details>


### [33] [Box Thirding: Anytime Best Arm Identification under Insufficient Sampling](https://arxiv.org/abs/2602.18186)
*Seohwa Hwang,Junyong Park*

Main category: stat.ML

TL;DR: B3算法是一种用于固定预算下最佳臂识别的灵活高效方法，通过三元比较迭代选择：每轮比较三个臂，保留最佳臂继续探索，中位臂推迟，最差臂丢弃。无需预知总预算T，在有限预算下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决固定预算约束下的最佳臂识别问题，特别是在臂数量N很大时无法在有限预算T内进行穷举评估的场景。现有方法如Successive Halving需要预知总预算T，而实际应用中T可能未知。

Method: 采用迭代三元比较策略：每轮选择三个臂进行比较，最佳臂继续探索，中位臂推迟到后续比较，最差臂直接丢弃。算法无需预知总预算T，能够处理任意臂数量N的情况。

Result: B3在epsilon最佳臂误识别概率方面与需要预知T的Successive Halving相当，在有限预算约束下，在New Yorker Cartoon Caption Contest数据集上的简单遗憾表现优于现有方法。

Conclusion: B3算法为固定预算下的最佳臂识别提供了一种灵活高效的解决方案，特别适用于臂数量大且预算未知的场景，在有限预算下表现出优越性能。

Abstract: We introduce Box Thirding (B3), a flexible and efficient algorithm for Best Arm Identification (BAI) under fixed-budget constraints. It is designed for both anytime BAI and scenarios with large N, where the number of arms is too large for exhaustive evaluation within a limited budget T. The algorithm employs an iterative ternary comparison: in each iteration, three arms are compared--the best-performing arm is explored further, the median is deferred for future comparisons, and the weakest is discarded. Even without prior knowledge of T, B3 achieves an epsilon-best arm misidentification probability comparable to Successive Halving (SH), which requires T as a predefined parameter, applied to a randomly selected subset of c0 arms that fit within the budget. Empirical results show that B3 outperforms existing methods under limited-budget constraints in terms of simple regret, as demonstrated on the New Yorker Cartoon Caption Contest dataset.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [34] [Modal Energy for Power System Analysis: Definitions and Requirements](https://arxiv.org/abs/2602.17874)
*J. Liu,F. Milano*

Main category: eess.SY

TL;DR: 该论文澄清了主流模态能量方法的定义和适用性，指出它们仅在系统正态性等限制条件下才成立，这在逆变器主导的电力系统中适用性有限。


<details>
  <summary>Details</summary>
Motivation: 模态能量为电力系统模态分析提供了基于传统特征值和参与因子的补充信息，但模态能量的定义并不唯一，需要澄清不同方法的定义和适用条件。

Method: 分析主流模态能量方法的定义，重点关注它们与特征值以及总系统能量的映射关系，并探讨这些映射成立的条件。

Result: 研究表明，这些映射关系仅在限制性条件下成立，特别是系统正态性条件，这限制了它们在逆变器主导的电力系统中的适用性。

Conclusion: 模态能量方法在传统电力系统中有效，但在逆变器主导的现代电力系统中适用性受限，需要重新评估其应用条件。

Abstract: Modal energy provides information complementary to and based on conventional eigenvalues and participation factors for power system modal analysis. However, modal energy definition is not unique. This letter clarifies the definitions and applicability of mainstream modal energy approaches, focusing on their mappings to eigenvalues and to the total system energy. It is shown that these mappings hold only under restrictive conditions, notably system normality, which limits their applicability in inverter-dominated power systems.

</details>


### [35] [A Scalable Reconfigurable Intelligent Surface with 3 Bit Phase Resolution and High Bandwidth for 3.6 GHz 5G/6G Applications](https://arxiv.org/abs/2602.17877)
*Markus Heinrichs,Aydin Sezgin,Rainer Kronberger*

Main category: eess.SY

TL;DR: 本文提出了一种可扩展的3.6GHz可重构智能表面设计，支持1位和3位相位分辨率，采用低成本PCB技术和创新的弹簧接触馈电结构，适用于5G/6G网络的宽带应用。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面对于未来5G和6G网络中无线传播信道的主动控制至关重要，但现有设计在可扩展性、制造成本和宽带性能方面存在挑战。

Method: 采用低成本印刷电路板技术，创新性地使用弹簧接触馈电结构，设计支持1位和3位相位分辨率的单元结构，实现高效组装和降低大面积阵列的制造复杂度。

Result: 设计实现了宽带相位控制、低功耗和高可扩展性，实验结果显示在n78频段具有良好的相位可调性，反射性能与现有解决方案相比具有竞争力。

Conclusion: 该RIS架构为智能无线电环境、波束赋形和传感应用的实验研究提供了实用平台，适用于下一代无线网络。

Abstract: Reconfigurable Intelligent Surfaces enable active control of wireless propagation channels, which is crucial for future 5G and 6G networks. This work presents a scalable RIS design operating at 3.6 GHz with both 1 bit and 3 bit phase resolution, supporting wideband applications. The unit cells employ low-cost printed circuit board technology with an innovative spring-contact feeding structure, enabling efficient assembly and reduced manufacturing complexity for large-area arrays. The design achieves broadband phase control, low power consumption, and high scalability, with experimental results demonstrating phase tunability across the n78 frequency band and competitive reflection performance compared to existing solutions. This RIS architecture provides a practical platform for experimental studies of smart radio environments, beam steering, and sensing applications in next-generation wireless networks.

</details>


### [36] [Decision Support under Prediction-Induced Censoring](https://arxiv.org/abs/2602.18031)
*Yan Chen,Ruyi Huang,Cheng Liu*

Main category: eess.SY

TL;DR: 提出PIC-RL框架解决预测诱导审查问题，将审查从数据质量问题转化为决策信号，在生成AI资源分配中减少服务降级达50%


<details>
  <summary>Details</summary>
Motivation: 在线决策系统中，行动不仅影响运营成本，还决定未来学习的数据可用性——这种现象称为预测诱导审查(PIC)。在生成AI服务的大规模资源分配中尤为严重：容量不足会触发短缺但隐藏真实需求，系统只能得到"大于"约束。标准方法依赖未审查数据会遭受选择偏差，使系统陷入自我强化的低供应陷阱。

Method: 提出PIC-RL（预测诱导审查强化学习）闭环框架：1) 不确定性感知需求预测管理信息-成本权衡；2) 悲观代理推断从短缺事件构建决策对齐的保守反馈；3) 双时间尺度适应稳定在线学习对抗分布漂移。

Result: 理论分析证明反馈设计能纠正朴素学习中的选择偏差。在阿里巴巴生成AI生产轨迹上的实验表明，PIC-RL持续优于最先进基线，服务降级减少达50%同时保持成本效率。

Conclusion: PIC-RL成功将审查从数据质量问题转化为决策信号，打破预测诱导审查的自我强化循环，为存在数据审查的在线决策系统提供有效解决方案。

Abstract: In many data-driven online decision systems, actions determine not only operational costs but also the data availability for future learning -- a phenomenon termed Prediction-Induced Censoring (PIC). This challenge is particularly acute in large-scale resource allocation for generative AI (GenAI) serving: insufficient capacity triggers shortages but hides the true demand, leaving the system with only a "greater-than" constraint. Standard decision-making approaches that rely on uncensored data suffer from selection bias, often locking the system into a self-reinforcing low-provisioning trap. To break this loop, this paper proposes an adaptive approach named PIC-Reinforcement Learning (PIC-RL), a closed-loop framework that transforms censoring from a data quality problem into a decision signal. PIC-RL integrates (1) Uncertainty-Aware Demand Prediction to manage the information-cost trade-off, (2) Pessimistic Surrogate Inference to construct decision-aligned conservative feedback from shortage events, and (3) Dual-Timescale Adaptation to stabilize online learning against distribution drift. The analysis provides theoretical guarantees that the feedback design corrects the selection bias inherent in naive learning. Experiments on production Alibaba GenAI traces demonstrate that PIC-RL consistently outperforms state-of-the-art baselines, reducing service degradation by up to 50% while maintaining cost efficiency.

</details>


### [37] [Incremental Data Driven Transfer Identification](https://arxiv.org/abs/2602.18048)
*N. Naveen Mukesh,Debraj Chakraborty*

Main category: eess.SY

TL;DR: 提出一种在线转移识别的几何方法，利用相似系统的先验数据，在真实系统数据不足时选择最接近相似系统的候选模型，随着数据增加逐步收敛到真实系统


<details>
  <summary>Details</summary>
Motivation: 在线识别系统时，初期收集的真实系统数据往往不足，无法精确识别，但通常可以获得相似系统的丰富数据。如何利用相似系统的先验知识来加速真实系统的识别过程是一个重要问题

Method: 采用几何方法，在真实系统数据不足时，从与当前观测一致的多个候选模型中，选择最接近相似系统的模型。随着数据积累，逐步调整模型，最终收敛到真实系统

Result: 数值实验表明该方法在增量转移识别范式中有效，即使使用最少的数据也能识别出可用于极点配置问题的模型

Conclusion: 提出的几何方法能够有效利用相似系统的先验知识，在真实系统数据不足时提供合理的模型估计，并随着数据积累逐步收敛到真实系统，为解决在线系统识别问题提供了新思路

Abstract: We introduce a geometric method for online transfer identification of a deterministic linear time-invariant system. At the beginning of the identification process, we assume access to abundant data from a system that is similar, though not identical, to the true system. In the early stages of data collection from the true system, the dataset generated is still not sufficiently informative to enable precise identification. Consequently, multiple candidate models remain consistent with the observations available at that point. Our method picks, at each instant, the model closest to the similar system that is consistent with the current data. As more data are collected, the proposed model gradually moves away from the initial similar system and eventually converges to the true system when the data set grows to be informative. Numerical examples demonstrate the effectiveness of the incremental transfer identification paradigm, where identified models with minimal data are used to solve the pole placement problem.

</details>


### [38] [Iterative McCormick Relaxation for Joint Impedance Control and Network Topology Optimization](https://arxiv.org/abs/2602.18059)
*Junseon Park,Hyeongon Park,Rahul K. Gupta*

Main category: eess.SY

TL;DR: 提出一种基于McCormick松弛的迭代校正方法，用于优化协调分布式可变阻抗设备（VIDs）与网络拓扑优化（NTO），解决电力系统中的线路拥塞和电压违规问题。


<details>
  <summary>Details</summary>
Motivation: 电力系统运营商越来越多地部署可变阻抗设备（如Smart Wires）和网络拓扑优化方案来缓解线路拥塞、变压器拥塞和电压违规等运行挑战。需要优化协调分布式VIDs在固定和优化拓扑下的运行。

Method: 使用McCormick松弛方案将双线性约束转换为线性约束集，结合直流潮流方程。提出迭代校正McCormick松弛以提高精度。将问题从混合整数非线性问题转化为可求解形式。

Result: 在标准IEEE基准测试系统上验证了所提框架，并对迭代McCormick方法与非线性方法、SOS2分段线性近似和原始McCormick松弛进行了性能比较。

Conclusion: 提出的迭代McCormick方法能有效解决VIDs与NTO协调优化问题，相比其他方法在精度和性能上具有优势，为电力系统运行提供了实用的解决方案。

Abstract: Power system operators are increasingly deploying Variable Impedance Devices (VIDs), e.g., Smart Wires, and Network Topology Optimization (NTO) schemes for mitigating operational challenges such as line and transformer congestion, and voltage violations. This work aims to optimize and coordinate the operation of distributed VIDs considering fixed and optimized topologies. This problem is inherently non-linear due to power flow equations as well as bilinear terms introduced due to variable line impedance of VIDs. Furthermore, the topology optimization scheme makes it a mixed integer nonlinear problem. To tackle this, we introduce using McCormick relaxation scheme, which converts the bilinear constraints into a linear set of constraints along with the DC power flow equations. We propose an iterative correction of the McCormick relaxation to enhance its accuracy. The proposed framework is validated on standard IEEE benchmark test systems, and we present a performance comparison of the iterative McCormick method against the non-linear, SOS2 piecewise linear approximation, and original McCormick relaxation.

</details>


### [39] [Hybrid Control of ADT Switched Linear Systems subject to Actuator Saturation](https://arxiv.org/abs/2602.18247)
*Fen Wu,Chengzhi Yuan*

Main category: eess.SY

TL;DR: 提出一种混合输出反馈控制框架，用于处理具有执行器饱和的平均驻留时间切换线性系统，通过监督重置机制和LMI条件保证指数稳定性和扰动衰减性能。


<details>
  <summary>Details</summary>
Motivation: 现有切换系统控制方法在处理执行器饱和时存在局限性，特别是当子系统可能指数不稳定时。需要开发一种能同时处理饱和非线性、切换行为和输出反馈约束的统一框架。

Method: 采用混合控制器结构：模式依赖的全阶动态输出反馈控制器 + 监督重置机制（在切换时刻更新控制器状态）。基于死区表示处理饱和非线性，通过LMI条件统一处理切换边界约束和性能要求。

Result: 推导出保证指数稳定性和加权L2增益扰动衰减的LMI充分条件，提供了基于可行LMI解的显式控制器构造算法，仿真验证了方法的有效性和计算可行性。

Conclusion: 所提出的混合输出反馈控制框架能有效处理具有执行器饱和的切换线性系统，在保证稳定性和性能的同时具有计算可行性，优于现有的饱和控制方法。

Abstract: This paper develops a hybrid output-feedback control framework for average dwell-time (ADT) switched linear systems subject to actuator saturation. The considered subsystems may be exponentially unstable, and the saturation nonlinearity is explicitly handled through a deadzone-based representation. The proposed hybrid controller combines mode-dependent full-order dynamic output-feedback controllers with a supervisory reset mechanism that updates controller states at switching instants. By incorporating the reset rule directly into the synthesis conditions, switching boundary constraints and performance requirements are addressed in a unified convex formulation. Sufficient conditions are derived in terms of linear matrix inequalities (LMIs) to guarantee exponential stability under ADT switching and a prescribed weighted ${\cal L}_2$-gain disturbance attenuation level for energy-bounded disturbances. An explicit controller construction algorithm is provided based on feasible LMI solutions. Simulation results demonstrate the effectiveness and computational tractability of the proposed approach and highlight its advantages over existing output-feedback saturation control methods.

</details>


### [40] [Accurate Data-Based State Estimation from Power Loads Inference in Electric Power Grids](https://arxiv.org/abs/2602.18261)
*Philippe Jacquod,Laurent Pagnier,Daniel J. Gauthier*

Main category: eess.SY

TL;DR: 该论文提出了一种基于数据驱动回归的方法，用于推断大规模电网中缺失的电力负荷值，通过利用训练数据中的统计相关性来估计运行状态。


<details>
  <summary>Details</summary>
Motivation: 准确的电力系统状态估计对于电网可靠运行和控制至关重要。然而，在实际电网中，电力负荷数据往往不完整或缺失，需要有效的方法来推断这些缺失值。

Method: 采用数据驱动的数值方法，基于部分观测的电力需求数据，使用线性回归算法，利用合成训练数据集中的统计相关性来估计缺失的电力负荷值。

Result: 在三个合成输电网络测试系统上，该方法在各种运行条件下都能高精度地重建缺失的需求值。应用于瑞士实际输电电网数据时，尽管观测数据有限，仍能较准确地推断缺失负荷。牛顿-拉夫逊潮流计算表明，负荷估计误差会导致更小的线路潮流偏差，确保估计状态能正确捕捉潜在的线路故障。

Conclusion: 基于数据的简单回归技术可以为现代电网状态估计提供高效可靠的替代方案，能够准确推断缺失负荷并有效捕捉电网运行状态。

Abstract: Accurate state estimation is a crucial requirement for the reliable operation and control of electric power systems. Here, we construct a data-driven, numerical method to infer missing power load values in large-scale power grids. Given partial observations of power demands, the method estimates the operational state using a linear regression algorithm, exploiting statistical correlations within synthetic training datasets. We evaluate the performance of the method on three synthetic transmission grid test systems. Numerical experiments demonstrate the high accuracy achieved by the method in reconstructing missing demand values under various operating conditions. We further apply the method to real data for the transmission power grid of Switzerland. Despite the restricted number of observations in this dataset, the method infers missing power loads rather accurately. Furthermore, Newton-Raphson power flow solutions show that deviations between true and inferred values for power loads result in smaller deviations between true and inferred values for flows on power lines. This ensures that the estimated operational state correctly captures potential line contingencies. Overall, our results indicate that simple data-based regression techniques can provide an efficient and reliable alternative for state estimation in modern power grids.

</details>


### [41] [Koopman-BoxQP: Solving Large-Scale NMPC at kHz Rates](https://arxiv.org/abs/2602.18331)
*Liang Wu,Wallace Gian Yion Tan,Richard D. Braatz,Ján Drgoňa*

Main category: eess.SY

TL;DR: 提出Koopman-BoxQP框架，通过Koopman线性化模型和结构化BoxQP求解，实现大规模非线性模型预测控制在kHz速率下的实时求解


<details>
  <summary>Details</summary>
Motivation: 大规模非线性模型预测控制在标准处理器上实现kHz速率的实时求解仍然是一个重大挑战，需要开发高效的求解框架

Method: 提出四步框架：1) 学习线性Koopman高维模型；2) 消除高维观测构建状态和控制输入的多步预测模型；3) 将多步预测模型惩罚到目标函数中，形成结构化Box约束二次规划；4) 开发结构利用和热启动支持的可行Mehrotra内点算法

Result: Koopman-BoxQP能够以kHz速率求解具有1040个变量和2080个不等式的大规模NMPC问题

Conclusion: 该框架成功解决了大规模非线性模型预测控制在标准处理器上实现kHz速率实时求解的挑战，为实时控制应用提供了有效解决方案

Abstract: Solving large-scale nonlinear model predictive control (NMPC) problems at kilohertz (kHz) rates on standard processors remains a formidable challenge. This paper proposes a Koopman-BoxQP framework that i) learns a linear Koopman high-dimensional model, ii) eliminates the high-dimensional observables to construct a multi-step prediction model of the states and control inputs, iii) penalizes the multi-step prediction model into the objective, which results in a structured box-constrained quadratic program (BoxQP) whose decision variables include both the system states and control inputs, iv) develops a structure-exploited and warm-starting-supported variant of the feasible Mehrotra's interior-point algorithm for BoxQP. Numerical results demonstrate that Koopman-BoxQP can solve a large-scale NMPC problem with $1040$ variables and $2080$ inequalities at a kHz rate.

</details>


### [42] [A Marginal Reliability Impact Based Accreditation Framework for Capacity Markets](https://arxiv.org/abs/2602.18365)
*Feng Zhao,Tongxin Zheng,Dane Schiro,Xiaochu Wang*

Main category: eess.SY

TL;DR: 提出基于边际可靠性影响(MRI)的资源认证框架，用于容量市场设计，使认证容量与可靠性贡献对齐，实现容量产品的同质化和供需可替代性。


<details>
  <summary>Details</summary>
Motivation: 传统容量市场设计中，资源认证可能无法准确反映其对系统可靠性的实际贡献，导致市场效率低下。需要一种能更好表征资源充足性问题的认证框架。

Method: 基于边际可靠性影响(MRI)的资源认证框架：根据资源对系统可靠性的边际影响进行认证，使认证容量与可靠性贡献对齐。该框架确保不同资源提供的认证容量在可靠性贡献上可替代，实现容量产品的同质性。

Result: MRI认证框架使容量产品具有同质性，不同资源的认证容量在可靠性贡献上可替代。同时，基于MRI的容量需求也使容量供需之间具有可替代性，从而更好地表征资源充足性问题。

Conclusion: 基于MRI的容量市场设计能更准确地反映资源对系统可靠性的贡献，实现容量产品的同质化和供需可替代性，从而获得更高效的市场结果。

Abstract: This paper presents a Marginal Reliability Impact (MRI) based resource accreditation framework for capacity market design. Under this framework, a resource is accredited based on its marginal impact on system reliability, thus aligning the resource accreditation value with its reliability contribution. A key feature of the MRI based accreditation is that the accredited capacities supplied by different resources to the capacity market are substitutable in reliability contribution, a desired feature of homogeneous products. Moreover, with MRI based capacity demand, substitutability between supply and demand for capacity is also achieved. As a result, a capacity market with the MRI based capacity product can better characterize the underlying resource adequacy problem and lead to more efficient market outcomes.

</details>


### [43] [Parameter Update Laws for Adaptive Control with Affine Equality Parameter Constraints](https://arxiv.org/abs/2602.18376)
*Ashwin P. Dani*

Main category: eess.SY

TL;DR: 提出了两种带凸等式约束的自适应控制参数更新律：基于梯度的更新和结合并发学习的更新，通过约束优化问题推导，并与自适应轨迹跟踪控制器集成，保证闭环系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 在自适应控制中，有时需要对参数施加约束（如物理限制或先验知识），现有方法难以处理凸等式约束，需要开发能够在线学习未知系统参数同时满足约束的更新律。

Method: 将带仿射等式约束的约束优化问题重新表述为无约束问题，推导出两种更新律：基于梯度的约束更新律和结合并发学习的约束更新律，并与自适应轨迹跟踪控制器集成。

Result: 仿真验证了所提方法的有效性：能够保持参数估计的约束，CL更新律收敛到真实参数，约束梯度更新律实现渐近跟踪，约束CL更新律实现指数跟踪性能。

Conclusion: 成功开发了带凸等式约束的自适应控制参数更新律，建立了闭环系统的Lyapunov稳定性，为处理参数约束的自适应控制提供了有效解决方案。

Abstract: In this paper, constrained parameter update laws for adaptive control with convex equality constraint on the parameters are developed, one based on a gradient only update and the other incorporating concurrent learning (CL) update. The update laws are derived by solving a constrained optimization problem with affine equality constraints. This constrained problem is reformulated as an equivalent unconstrained problem in a new variable, thereby eliminating the equality constraints. The resulting update law is integrated with an adaptive trajectory tracking controller, enabling online learning of the unknown system parameters. Lyapunov stability of the closed-loop system with the equality-constrained parameter update law is established. The effectiveness of the proposed equality-constrained adaptive control law is demonstrated through simulations, validating its ability to maintain constraints on the parameter estimates, achieving convergence to the true parameters for CL-based update law, and achieving asymptotic and exponential tracking performance for constrained gradient and constrained CL-based update laws, respectively.

</details>


### [44] [Incremental Input-to-State Stability and Equilibrium Tracking for Stochastic Contracting Dynamics](https://arxiv.org/abs/2602.18382)
*Yu Kawano,Simone Betteti,Alexander Davydov,Francesco Bullo*

Main category: eess.SY

TL;DR: 研究非线性随机微分方程在确定性输入和布朗运动驱动下的收缩性，证明当向量场在状态上一致收缩且在输入上一致Lipschitz时，SDE具有增量噪声和输入到状态稳定性，并应用于含噪声的时变平衡跟踪误差估计。


<details>
  <summary>Details</summary>
Motivation: 研究非线性随机微分方程的收缩性，为含噪声系统的稳定性分析和误差估计提供理论框架，特别是在系统动力学和输入信号都受噪声影响的情况下。

Method: 采用加权ℓ₂范数分析状态空间，证明当SDE的向量场在状态上一致收缩且在输入上一致Lipschitz时，系统具有增量噪声和输入到状态稳定性。将结果应用于Ornstein-Uhlenbeck过程（无界噪声）和Jacobi扩散过程（有界噪声）的误差估计，并研究相关Fokker-Planck方程在任意p-Wasserstein度量下的稳定性。

Result: 建立了SDE收缩性与增量噪声/输入到状态稳定性之间的等价关系，为含噪声系统的时变平衡跟踪提供了误差估计方法，并证明了Fokker-Planck方程在任意p-Wasserstein度量下的增量输入到状态稳定性。

Conclusion: 向量场的收缩性和Lipschitz性质是保证非线性随机微分方程稳定性的关键条件，该理论框架可有效处理含噪声系统的稳定性分析和控制问题，为实际工程应用提供了理论基础。

Abstract: In this paper, we study the contractivity of nonlinear stochastic differential equations (SDEs) driven by deterministic inputs and Brownian motions. Given a weighted $\ell_2$-norm for the state space, we show that an SDE is incrementally noise- and input-to-state stable if its vector field is uniformly contracting in the state and uniformly Lipschitz in the input. This result is applied to error estimation for time-varying equilibrium tracking in the presence of noise affecting both the system dynamics and the input signals. We consider both Ornstein-Uhlenbeck processes modeling unbounded noise and Jacobi diffusion processes modeling bounded noise. Finally, we turn our attention to the associated Fokker-Planck equation of an SDE. For this context, we prove incremental input-to-state stability with respect to an arbitrary $p$-Wasserstein metric when the drift vector field is uniformly contracting in the state and uniformly Lipschitz in the input with respect to an arbitrary norm.

</details>


### [45] [Convex Block-Cholesky Approach to Risk-Constrained Low-thrust Trajectory Design under Operational Uncertainty](https://arxiv.org/abs/2602.18416)
*Kenshiro Oguri,Gregory Lantoine*

Main category: eess.SY

TL;DR: 本文提出了一种用于风险约束轨迹优化的算法，结合最优协方差控制和序列凸规划，应用于火星引力辅助探测谷神星任务


<details>
  <summary>Details</summary>
Motivation: 随着太空任务目标日益复杂（如多卫星巡游、自主任务），需要在任务设计中考虑导航过程的统计特性。导航过程本质上是统计性的，包括轨道确定和飞行路径控制，因此需要量化导航的统计效应、评估相关风险，并设计确保低风险同时最小化统计性能指标的任务

Method: 将问题表述为非线性随机最优控制问题，开发计算可行的算法，结合最优协方差控制和序列凸规划。采用块Cholesky方法进行最优协方差控制的凸化表述，并利用SCvx*算法确保数值收敛可靠性

Result: 将算法应用于火星引力辅助探测谷神星的风险约束统计轨迹优化，通过非线性蒙特卡洛模拟验证了统计最优轨迹和飞行路径控制策略的鲁棒性

Conclusion: 该算法能够有效处理初始状态分散、导航误差、机动执行误差和不完美动力学建模等操作不确定性，为复杂太空任务设计提供了可靠的风险约束轨迹优化解决方案

Abstract: Designing robust trajectories under uncertainties is an emerging technology that may represent a key paradigm shift in space mission design. As we pursue more ambitious scientific goals (e.g., multi-moon tours, missions with extensive components of autonomy), it becomes more crucial that missions are designed with navigation (Nav) processes in mind. The effect of Nav processes is statistical by nature, as they consist of orbit determination (OD) and flight-path control (FPC). Thus, this mission design paradigm calls for techniques that appropriately quantify statistical effects of Nav, evaluate associated risks, and design missions that ensure sufficiently low risk while minimizing a statistical performance metric; a common metric is Delta-V99: worst-case (99%-quantile) Delta-V expenditure including statistical FPC efforts. In response to the need, this paper develops an algorithm for risk-constrained trajectory optimization under operational uncertainties due to initial state dispersion, navigation error, maneuver execution error, and imperfect dynamics modeling. We formulate it as a nonlinear stochastic optimal control problem and develop a computationally tractable algorithm that combines optimal covariance steering and sequential convex programming (SCP). Specifically, the proposed algorithm takes a block-Cholesky approach for convex formulation of optimal covariance steering, and leverages a recent SCP algorithm, SCvx*, for reliable numerical convergence. We apply the developed algorithm to risk-constrained, statistical trajectory optimization for exploration of dwarf planet Ceres with a Mars gravity assist, and demonstrate the robustness of the statistically-optimal trajectory and FPC policies via nonlinear Monte Carlo simulation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [46] ["Everyone's using it, but no one is allowed to talk about it": College Students' Experiences Navigating the Higher Education Environment in a Generative AI World](https://arxiv.org/abs/2602.17720)
*Yue Fu,Yifan Lin,Yessica Wang,Sarah Tran,Alexis Hiniker*

Main category: cs.CY

TL;DR: 大学生在学术工作中越来越多地使用生成式AI，但现有制度实践尚未适应这一转变。研究发现制度压力和社会因素影响学生AI使用，导致意图与行为之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 高等教育学生越来越多地在学术工作中使用生成式AI，但现有的制度实践尚未适应这一转变。需要了解影响学生AI使用的环境和社会因素，以便为机构、教师和系统工具设计者提供有效支持学生学习的建议。

Method: 通过对23名大学生进行半结构化访谈，研究采用定性方法探讨影响学生AI使用的环境和社会因素。

Result: 研究发现：1）制度压力因素（截止日期、考试周期、评分）导致学生即使认为AI会削弱学习也会使用；2）社会影响，特别是同伴微社区，建立了事实上的AI规范；3）校园普遍存在"AI羞耻"，推动AI使用转入地下；4）当前的机构AI政策被认为笼统、不一致且令人困惑，导致常规性违规；5）学生发展基于价值的自我调节策略，但环境压力造成意图与行为之间的差距。

Conclusion: 学生AI使用是一种情境化实践。研究为机构、教师和系统工具设计者提供了有效支持学生AI学习的启示，包括需要制定更具体、一致的政策，减少制度压力，以及创造更开放的学习环境来减少AI羞耻感。

Abstract: Higher education students are increasingly using generative AI in their academic work. However, existing institutional practices have not yet adapted to this shift. Through semi-structured interviews with 23 college students, our study examines the environmental and social factors that influence students' use of AI. Findings show that institutional pressure factors like deadlines, exam cycles, and grading lead students to engage with AI even when they think it undermines their learning. Social influences, particularly peer micro-communities, establish de-facto AI norms regardless of official AI policies. Campus-wide ``AI shame'' is prevalent, often pushing AI use underground. Current institutional AI policies are perceived as generic, inconsistent, and confusing, resulting in routine noncompliance. Additionally, students develop value-based self-regulation strategies, but environmental pressures create a gap between students' intentions and their behaviors. Our findings show student AI use to be a situated practice, and we discuss implications for institutions, instructors, and system tool designers to effectively support student learning with AI.

</details>


### [47] [Gender and Digital Platform Work During Turbulent Times](https://arxiv.org/abs/2602.17721)
*Melissa Langworthy,Yana Rodgers*

Main category: cs.CY

TL;DR: 平台经济在危机期间提供灵活工作机会，但也加剧性别不平等，女性面临照顾责任、行动限制和经济不安全等约束，需要性别响应政策


<details>
  <summary>Details</summary>
Motivation: 探讨平台经济如何塑造危机时期的劳动力市场反应，特别关注性别差异，分析数字劳动平台在提供灵活工作机会的同时如何强化现有不平等

Method: 基于经济危机、自然灾害和难民流离失所等案例研究，分析数字劳动平台对劳动力市场的影响，特别关注女性的约束条件

Result: 平台工作在危机期间可作为生命线，但并非保证解决方案，其益处分布不均；女性面临照顾责任、行动限制和经济不安全等约束，这些约束在危机期间更加明显

Conclusion: 需要性别响应政策和新的研究来理解数字基础设施如何在不同危机背景下调节劳动体验，以制定促进平台工作韧性和公平的包容性策略，特别是针对边缘化和流离失所人群

Abstract: This commentary explores how the platform economy shapes labour market responses during times of crisis, with a focus on gendered experiences. Drawing on cases of economic crisis, natural disasters, and refugee displacement, it examines how digital labour platforms offer flexible work opportunities while also reinforcing existing inequalities. Women face distinct constraints (such as caregiving responsibilities, limited mobility, and economic insecurity) that hinder their employment opportunities and earnings potential. These constraints are more pronounced during crises, when access to stable income and safe working conditions becomes more difficult. While platform work can serve as a lifeline, it is not a guaranteed solution, and its benefits are unevenly distributed. The commentary calls for gender-responsive policies and new research to understand how digital infrastructures mediate labour experiences across different crisis contexts. Such research can inform inclusive strategies that promote resilience and equity in platform-based work, particularly for marginalized and displaced populations.

</details>


### [48] [Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention](https://arxiv.org/abs/2602.17726)
*Qness Ndlovu*

Main category: cs.CY

TL;DR: 开发低成本AI天气预警系统，以1,430-1,730美元/月的成本实现国家级部署，比传统雷达成本低2,000-4,545倍，通过WhatsApp提供15天全球天气预报，响应时间低于200毫秒。


<details>
  <summary>Details</summary>
Motivation: 2026年1月非洲南部暴雨造成200-300人死亡，暴露了非洲60%地区缺乏有效预警系统的问题。传统雷达站成本超过100万美元，导致非洲预警覆盖比欧美低18倍，急需经济可行的解决方案。

Method: 采用NVIDIA Earth-2 AI天气模型，构建生产级架构：1) 使用ProcessPoolExecutor解决异步Python中的会话生命周期冲突；2) 数据库支持的服务架构，GPU直接将全球预报写入PostgreSQL，避免HTTP传输瓶颈；3) 自动化坐标管理处理61个时间步的多步推理。

Result: 2026年2月在南非成功部署，系统能以1,430-1,730美元/月的成本实现国家级覆盖，比雷达成本低2,000-4,545倍。生成15天全球大气预报，查询响应时间低于200毫秒，通过WhatsApp（80%+市场渗透率）提供预警。

Conclusion: 该架构使大陆级早期预警系统在经济上可行，支持UNDRR关于此类系统能将灾害死亡率降低6倍的研究发现。所有架构细节都有文档记录，确保完全可复现性。

Abstract: In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.
  Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility.

</details>


### [49] [Stop Saying "AI"](https://arxiv.org/abs/2602.17729)
*Nathan G. Wood,Scott Robbins,Eduardo Zegarra Berodt,Anton Graf von Westerholt,Michelle Behrndt,Daniel Kloock-Schreiber*

Main category: cs.CY

TL;DR: 该论文指出"AI"一词过于宽泛，导致讨论模糊不清，特别是在军事领域。作者提出军事AI的分类法，强调不同系统的风险和挑战各异，主张在讨论中应具体说明系统类型而非笼统使用"AI"一词。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI的讨论过于笼统，使用"AI"这一宽泛术语掩盖了不同系统的具体特性和风险。特别是在军事等安全关键领域，这种模糊性阻碍了有意义的辩论和政策制定。作者希望通过军事领域作为案例研究，揭示这一问题的重要性。

Method: 以军事领域为案例研究，提出一个枚举式分类法来区分"军事AI"下的各种系统类型。分析每种系统的具体挑战和局限性，展示对一种系统的批评不一定适用于其他系统。

Result: 建立了军事AI的分类框架，展示了不同系统类型的独特特征和风险。论证了对"AI"的笼统批评往往不准确，因为不同系统在能力、局限性和应用场景上存在显著差异。

Conclusion: 为了推动有成效的辩论，必须使讨论更加精确，尽可能避免使用"AI"这一笼统术语。研究人员、开发者和政策制定者应明确具体讨论的系统类型及其相关利益和风险。虽然以军事领域为例，但这一结论适用于所有领域的AI讨论。

Abstract: Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' may affect decision-making, responsibility, or the likelihood of mistakes (to name only a few categories of critique). However, for most critiques, the target is generally ``AI'', a broad term admitting many (types of) systems used for a variety of tasks and each coming with its own set of limitations, challenges, and potential use cases. In this article, we focus on the military domain as a case study and present both a loose enumerative taxonomy of systems captured under the umbrella term ``military AI'', as well as discussion of the challenges of each. In doing so, we highlight that critiques of one (type of) system will not always transfer to other (types of) systems. Building on this, we argue that in order for debates to move forward fruitfully, it is imperative that the discussions be made more precise and that ``AI'' be excised from debates to the extent possible. Researchers, developers, and policy-makers should make clear exactly what systems they have in mind and what possible benefits and risks attend the deployment of those particular systems. While we focus on AI in the military as an exemplar for the overall trends in discussions of ``AI'', the argument's conclusions are broad and have import for discussions of AI across a host of domains.

</details>


### [50] [The 2025 AI Agent Index: Documenting Technical and Safety Features of Deployed Agentic AI Systems](https://arxiv.org/abs/2602.17753)
*Leon Staufer,Kevin Feng,Kevin Wei,Luke Bailey,Yawen Duan,Mick Yang,A. Pinar Ozisik,Stephen Casper,Noam Kolt*

Main category: cs.CY

TL;DR: 该论文介绍了2025年AI智能体指数，旨在解决AI智能体生态系统复杂、快速演变且文档不一致的问题，通过记录30个最先进AI智能体的起源、设计、能力、生态系统和安全特性等信息。


<details>
  <summary>Details</summary>
Motivation: AI智能体系统越来越能够以有限的人类参与执行专业和个人任务，但由于AI智能体生态系统复杂、快速演变且文档不一致，跟踪这些发展变得困难，给研究人员和政策制定者带来了障碍。

Method: 创建2025年AI智能体指数，基于公开信息和与开发者的电子邮件通信，记录30个最先进AI智能体的起源、设计、能力、生态系统和安全特性等信息。

Result: 发现不同智能体开发者的透明度水平存在差异，大多数开发者很少分享关于安全性、评估和社会影响的信息。该指数还揭示了智能体发展、能力和开发者透明度方面的更广泛趋势。

Conclusion: 2025年AI智能体指数为研究人员和政策制定者提供了有价值的资源，帮助跟踪AI智能体生态系统的发展，并强调了提高开发者透明度的必要性，特别是在安全性、评估和社会影响方面。

Abstract: Agentic AI systems are increasingly capable of performing professional and personal tasks with limited human involvement. However, tracking these developments is difficult because the AI agent ecosystem is complex, rapidly evolving, and inconsistently documented, posing obstacles to both researchers and policymakers. To address these challenges, this paper presents the 2025 AI Agent Index. The Index documents information regarding the origins, design, capabilities, ecosystem, and safety features of 30 state-of-the-art AI agents based on publicly available information and email correspondence with developers. In addition to documenting information about individual agents, the Index illuminates broader trends in the development of agents, their capabilities, and the level of transparency of developers. Notably, we find different transparency levels among agent developers and observe that most developers share little information about safety, evaluations, and societal impacts. The 2025 AI Agent Index is available online at https://aiagentindex.mit.edu

</details>


### [51] [The Digital Divide in Generative AI: Evidence from Large Language Model Use in College Admissions Essays](https://arxiv.org/abs/2602.17791)
*Jinsook Lee,Conrad Borchers,AJ Alvero,Thorsten Joachims,Rene F. Kizilcec*

Main category: cs.CY

TL;DR: 研究分析2020-2024年美国大学申请中LLM使用情况，发现低收入学生更多使用LLM作为写作支持替代，但LLM使用与录取概率下降的关联在低收入学生中更强。


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为写作工具在不同社会经济群体中的采用差异，及其在大学录取这种高风险情境下对公平性和评估有效性的影响。

Method: 分析2020-2024年选择性大学81,663份申请数据，使用基于分布的检测器估计LLM使用，以费用减免状态作为社会经济地位代理变量，追踪写作变化和录取结果。

Result: 2023年后所有群体表面语言特征趋同，2024年LLM使用急剧增加，低收入学生增幅更大；但低收入学生LLM使用增加与录取概率下降的关联更强，即使控制学术资历和文体特征后依然如此。

Conclusion: LLM可能加剧教育不平等，在AI辅助写作时代，基于文章的评估有效性受到挑战，需要关注公平性问题。

Abstract: Large language models (LLMs) have become popular writing tools among students and may expand access to high-quality feedback for students with less access to traditional writing support. At the same time, LLMs may standardize student voice or invite overreliance. This study examines how adoption of LLM-assisted writing varies across socioeconomic groups and how it relates to outcomes in a high-stakes context: U.S. college admissions. We analyze a de-identified longitudinal dataset of applications to a selective university from 2020 to 2024 (N = 81,663). Estimating LLM use using a distribution-based detector trained on synthetic and historical essays, we tracked how student writing changed as LLM use proliferated, how adoption differed by socioeconomic status (SES), and whether potential benefits translated equitably into admissions outcomes. Using fee-waiver status as a proxy for SES, we observe post-2023 convergence in surface-level linguistic features, with the largest changes in fee-waived and rejected applicants. Estimated LLM use rose sharply in 2024 across all groups, with disproportionately larger increases among lower SES applicants, consistent with an access hypothesis in which LLMs substitute for scarce writing support. However, increased estimated LLM use was more strongly associated with declines in predicted admission probability for lower SES applicants than for higher SES applicants, even after controlling for academic credentials and stylometric features. These findings raise concerns about equity and the validity of essay-based evaluation in an era of AI-assisted writing and provide the first large-scale longitudinal evidence linking LLM adoption, linguistic change, and evaluative outcomes in college admissions.

</details>


### [52] [Strange Undercurrents: A Critical Outlook on AI's Cultural Influence](https://arxiv.org/abs/2602.17841)
*Dejan Grba*

Main category: cs.CY

TL;DR: 本文探讨生成式AI对艺术观念的影响，聚焦计算机科学基础概念与AI产业意识形态如何渗透到艺术、文化和社会中，揭示其中不利的哲学、技术和政治前提。


<details>
  <summary>Details</summary>
Motivation: 当前对生成式AI的讨论缺乏对艺术动态特性的细致理解，且忽视了AI科学与产业的概念和意识形态基础如何通过将AI艺术创作融入主流文化和经济而传播其属性。

Method: 以当前生成式AI的争议为契机，总结关于AI对艺术观念影响的更广泛研究，重点关注计算机科学基础概念与AI产业意识形态向量在艺术领域的交汇。

Result: 研究发现AI对艺术的影响融合了多样且有时不一致但又能凝聚的哲学前提、技术理念和政治观点，其中许多具有不利的基调。

Conclusion: 需要更精细地审视生成式AI所体现的问题，并将其置于更深的语境中理解，特别是AI科学与产业的概念意识形态基础如何影响艺术观念和文化。

Abstract: While generative artificial intelligence (generative AI) is being examined extensively, some issues it epitomizes call for more refined scrutiny and deeper contextualization. Besides the lack of nuanced understanding of art's continuously changing character in discussions about generative AI's cultural impact, one of the notably underexplored aspects is the conceptual and ideological substrate of AI science and industry whose attributes generative AI propagates by fostering the integration of diverse modes of AI-powered artmaking into the mainstream culture and economy. Taking the current turmoil around the generative AI as a pretext, this paper summarizes a broader study of AI's influence on art notions focusing on the confluence of certain foundational concepts in computer science and ideological vectors of the AI industry that transfer into art, culture, and society. This influence merges diverse and sometimes inconsistent but somehow coalescing philosophical premises, technical ideas, and political views, many of which have unfavorable overtones.

</details>


### [53] [Visual Anthropomorphism Shifts Evaluations of Gendered AI Managers](https://arxiv.org/abs/2602.17919)
*Ruiqing Han,Hao Cui,Taha Yasseri*

Main category: cs.CY

TL;DR: 能力线索可减少文本AI管理者的性别偏见，但视觉AI面部会引发性别刻板印象，特别是在积极决策时。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索能力线索是否能减少对AI管理者的性别偏见，以及这种效果是否取决于AI的呈现方式（文本vs视觉）。

Method: 采用两个预注册实验(N=2,505)，使用2×2×3设计：操纵AI性别、能力和决策结果，比较文本描述与通过反向关联范式生成的视觉AI面部。

Result: 文本条件下，评估由能力而非性别驱动；高能力AI管理者被认为更公平、更有能力、领导力更强。视觉条件下，能力线索影响减弱，出现系统性性别差异反应：女性化AI被认为更有能力和更可信，特别是在提供积极结果时。

Conclusion: 能力信息可在文本交互中减轻对AI管理者的负面反应，而面部拟人化会引发文本环境中未观察到的性别偏见。呈现方式对性别刻板印象的激活起关键作用，设计选择对AI治理有重要影响。

Abstract: This research examines whether competence cues can reduce gender bias in evaluations of AI managers and whether these effects depend on how the AI is represented. Across two preregistered experiments (N = 2,505), each employing a 2 x 2 x 3 design manipulating AI gender, competence, and decision outcome, we compared text-based descriptions of AI managers with visually generated AI faces created using a reverse-correlation paradigm. In the text condition, evaluations were driven by competence rather than gender. When participants received unfavourable decisions, high-competence AI managers were judged as fairer, more competent, and better leaders than low-competence managers, regardless of AI gender. In contrast, when the AI manager was visually represented, competence cues had attenuated influence once facial information was present. Instead, participants showed systematic gender-differentiated responses to AI faces, with feminine-appearing managers evaluated as more competent and more trustworthy than masculine-appearing managers, particularly when delivering favourable outcomes. These gender effects were largely absent when outcomes were unfavourable, suggesting that negative feedback attenuates the influence of both competence information and facial cues. Taken together, these findings show that competence information can mitigate negative reactions to AI managers in text-based interactions, whereas facial anthropomorphism elicits gendered perceptual biases not observed in text-only settings. The results highlight that representational modality plays a critical role in determining when gender stereotypes are activated in evaluations of AI systems and underscore that design choices are consequential for AI governance in evaluative contexts.

</details>


### [54] [Operational Agency: A Permeable Legal Fiction for Tracing Culpability in AI Systems](https://arxiv.org/abs/2602.17932)
*Anirban Mukherjee,Hannah Hanwen Chang*

Main category: cs.CY

TL;DR: 提出"操作代理"法律框架和"操作代理图"工具，通过评估AI系统的目标导向性、预测处理和安全架构等操作特性，在人类中心法律原则下追溯AI相关责任，无需赋予AI法律人格。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统具有高度自主性但缺乏法律人格，这与基于人类中心主义（犯罪意图和行为）的法律原则形成矛盾。需要建立一种框架，在不赋予AI法律人格的前提下，确保人类对AI行为负责。

Method: 提出"操作代理"作为可渗透的法律拟制，构建为事后证据框架；开发"操作代理图"工具，将AI的可观察操作特性（目标导向性作为意图代理、预测处理作为预见代理、安全架构作为注意标准代理）嵌入因果图中，追溯开发者、微调者、部署者和用户之间的责任分配。

Result: 该框架借鉴公司刑事责任、无辜代理人原则以及次级和替代责任框架，通过五个真实案例研究（侵权、民权、宪法、反垄断）展示了如何应对从自动驾驶碰撞到算法价格操纵等挑战，为法院提供原则性证据方法，为立法和行业提供概念基础。

Conclusion: 操作代理框架和操作代理图工具能够在不赋予AI法律人格的前提下，确保人类责任与技术进步同步，为法院、立法者和行业提供了应对AI自主性挑战的实用解决方案。

Abstract: Modern artificial intelligence (AI) systems act with a high degree of independence yet lack legal personhood-a paradox that fractures doctrines grounded in human-centric notions of mens rea and actus reus. This Article introduces Operational Agency (OA)-a permeable legal fiction structured as an ex post evidentiary framework-and Operational Agency Graph (OAG), a tool for mapping causal interactions among human actors, organizations, and AI systems. OA evaluates an AI's observable operational characteristics: its goal-directedness (as a proxy for intent), predictive processing (as a proxy for foresight), and safety architecture (as a proxy for a standard of care). OAG operationalizes that analysis by embedding these characteristics in a causal graph to trace and apportion culpability among developers, fine-tuners, deployers, and users. Drawing on corporate criminal liability, the innocent-agent doctrine, and secondary and vicarious liability frameworks, the Article shows how OA and OAG strengthen existing doctrines. Across five real-world case studies spanning tort, civil rights, constitutional law, and antitrust, it demonstrates how the framework addresses challenges ranging from autonomous vehicle collisions to algorithmic price-fixing, offering courts a principled evidentiary method-and legislatures and industry a conceptual foundation-to ensure human accountability keeps pace with technological autonomy, without conferring personhood on AI.

</details>


### [55] [Atrial Fibrillation Detection Using Machine Learning](https://arxiv.org/abs/2602.18036)
*Ankit Singh,Vidhi Thakur,Nachiket Tapas*

Main category: cs.CY

TL;DR: 提出一个结合PPG和ECG信号的机器学习框架，用于心房颤动的早期检测，其中子空间KNN模型在测试中达到98.7%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 心房颤动是常见的心律失常，也是缺血性中风的主要风险因素。通过非侵入性信号早期检测AF可以实现及时干预，从而改善患者预后。

Method: 从35名受试者的连续记录中分割出525个片段（每个片段10,000个样本，采样率125Hz），经过数据清洗后保留481个片段。从PPG和ECG信号中提取22个特征，包括时域统计量、频带功率和心率变异性指标。使用三种分类器（袋装决策树集成、立方核SVM、子空间KNN）进行训练和评估，采用10折交叉验证和保留测试。

Result: 子空间KNN获得最高的测试准确率（98.7%），略优于袋装决策树（97.9%）和立方核SVM（97.1%）。所有表现最佳的模型的敏感性和特异性均超过95%。

Conclusion: 基于集成的机器学习模型结合PPG和ECG特征可以有效检测心房颤动。研究提供了模型性能的比较分析以及所提出框架的优势和局限性。

Abstract: Atrial fibrillation (AF) is a common cardiac arrhythmia and a major risk factor for ischemic stroke. Early detection of AF using non-invasive signals can enable timely intervention. In this work, we present a comprehensive machine learning framework for AF detection from simultaneous photoplethysmogram (PPG) and electrocardiogram (ECG) signals. We partitioned continuous recordings from 35 subjects into 525 segments (15 segments of 10,000 samples each at 125Hz per subject). After data cleaning to remove segments with missing samples, 481 segments remained (263 AF, 218 normal).
  We extracted 22 features per segment, including time-domain statistics (mean, standard deviation, skewness, etc.), bandpower, and heart-rate variability metrics from both PPG and ECG signals. Three classifiers -- ensemble of bagged decision trees, cubic-kernel support vector machine (SVM), and subspace k-nearest neighbors (KNN) -- were trained and evaluated using 10-fold cross-validation and hold-out testing. The subspace KNN achieved the highest test accuracy (98.7\%), slightly outperforming bagged trees (97.9\%) and cubic SVM (97.1\%). Sensitivity (AF detection) and specificity (normal rhythm detection) were all above 95\% for the top-performing models.
  The results indicate that ensemble-based machine learning models using combined PPG and ECG features can effectively detect atrial fibrillation. A comparative analysis of model performance along with strengths and limitations of the proposed framework is presented.

</details>


### [56] [Demonstrating Restraint](https://arxiv.org/abs/2602.18139)
*L. C. R. Patell,O. E. Guest*

Main category: cs.CY

TL;DR: 美国可通过展示AI克制策略来降低对手采取预防性行动的风险，通过可信承诺机制保障国家安全


<details>
  <summary>Details</summary>
Motivation: 美国AI发展可能被对手视为严重威胁，引发预防性军事行动风险，需要探索降低冲突风险的策略

Method: 借鉴国际关系学中的可信承诺理论，提出美国可通过展示克制意图、政策努力与技术突破相结合的方式建立可信承诺

Result: 在对手对美国能力和意图存在不确定性的情况下，克制策略更为可行；在单方面获得强大能力的最困难情境下，需要政策努力与技术突破相结合才能实现可信承诺

Conclusion: 尽管克制策略面临困难，但应与其他避免AI过渡期冲突的策略进行权衡，作为降低预防性行动风险的有效途径

Abstract: Some have claimed that the future development of powerful AI systems would enable the United States to shift the international balance of power dramatically in its favor. Such a feat may not be technically possible; even so, if American AI development is perceived as a sufficiently severe threat by its nation-state adversaries, then the risk that they take extreme preventive action against the United States may rise. To bolster its security against preventive action, the United States could aim to pursue a strategy of restraint by demonstrating that it would not use powerful AI to threaten the survival of other nations. Drawing from the international relations literature that explores how states can make credible commitments, we sketch a set of options that the United States could employ to implement this strategy. In the most challenging setting, where it is certain that the US will unilaterally obtain powerful new capabilities, it is difficult to credibly commit to restraint, though an approach that layers significant policy effort with technical breakthroughs may make credibility achievable. If an adversary has realistic levels of uncertainty about the capabilities and intentions of the United States, a strategy of restraint becomes more feasible. Though restraint faces difficulties, it deserves to be weighed against alternative strategies that have been proposed for avoiding conflict during the transition to a world with advanced AI.

</details>


### [57] [Computer Vision in Tactical AI Art](https://arxiv.org/abs/2602.18189)
*Dejan Grba*

Main category: cs.CY

TL;DR: 本文探讨AI艺术如何通过计算机视觉技术批判现代AI的自动化推理与决策的社会政治问题，分析相关艺术实践的特征、挑战与发展方向。


<details>
  <summary>Details</summary>
Motivation: AI艺术的兴起与AI技术发展、AI驱动经济的扩张及其对社会文化的影响密切相关。计算机视觉技术的广泛应用与人类视觉认知价值之间的关系，为探索现代AI自动化推理和决策的有问题社会政治方面提供了重要空间。

Method: 作者首先识别和定位与计算机视觉相关的战术性AI艺术实践，然后分析四个相互关联主题领域的代表性艺术作品，探讨它们的主题重叠、共同批判点和共享缺陷，从而绘制战术性AI艺术的更广泛图景。

Result: 通过分析发现，战术性AI艺术在诗意说服力、社会责任和政治影响力方面受到多种因素影响，其中一些因素存在于数字艺术行动主义的理论前提中。研究揭示了该领域面临的挑战和局限性。

Conclusion: 作者基于分析结果，提出了应对挑战和推进该领域发展的路线图，为战术性AI艺术的未来发展指明了方向。

Abstract: AI art comprises a spectrum of creative endeavors that emerge from and respond to the development of artificial intelligence (AI), the expansion of AI-powered economies, and their influence on culture and society. Within this repertoire, the relationship between the cognitive value of human vision and the wide application range of computer vision (CV) technologies opens a sizeable space for exploring the problematic sociopolitical aspects of automated inference and decision-making in modern AI. In this paper, I examine the art practices critically engaged with the notions and protocols of CV. After identifying and contextualizing the CV-related tactical AI art, I discuss the features of exemplar artworks in four interrelated subject areas. Their topical imbrications, common critical points, and shared pitfalls plot a wider landscape of tactical AI art, allowing me to detect factors that affect its poetic cogency, social responsibility, and political impact, some of which exist in the theoretical premises of digital art activism. Along these lines, I outline the routes for addressing the challenges and advancing the field.

</details>


### [58] [Art Notions in the Age of (Mis)anthropic AI](https://arxiv.org/abs/2602.18202)
*Dejan Grba*

Main category: cs.CY

TL;DR: 本文探讨生成式AI对当代艺术观念的文化影响，分析AI通过艺术被战略性地正常化、在艺术界/学术界/AI研究中的表现，以及艺术与媚俗在数字文化中的相互渗透，揭示计算机科学和AI产业背后的概念意识形态基础。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI对当代艺术观念的文化影响，揭示AI技术背后的意识形态基础（机械代理崇拜、人机等同、社会技术盲视、网络自由主义），这些基础在AI研究中尚未充分暴露，需要批判性分析。

Method: 首先概述生成式AI，总结艺术观念的不同但常被混淆的方面，然后从三个主线分析AI对艺术的影响：AI通过艺术的战略性正常化、AI艺术在艺术界/学术界/AI研究中的表现、数字文化中艺术与媚俗的相互渗透，最后将这些概念因素与计算机科学和AI产业的概念意识形态基础联系起来。

Result: 揭示了生成式AI背后隐藏的异化、反社会性和厌世倾向，这些倾向体现在不同的哲学前提、技术思想和政治观点中，在AI研究中尚未充分暴露，需要在生成式AI的背景下进行批判性分析。

Conclusion: 生成式AI对艺术观念的影响需要持怀疑态度的批判性分析，技术趋势不仅改变我们对艺术的理解，还影响艺术的社会、经济和政治角色方向，需要进一步批判AI的文化时代精神。

Abstract: In this paper, I take the cultural effects of generative artificial intelligence (generative AI) as a context for examining a broader perspective of AI's impact on contemporary art notions. After the introductory overview of generative AI, I summarize the distinct but often confused aspects of art notions and review the principal lines in which AI influences them: the strategic normalization of AI through art, the representation of AI art in the artworld, academia, and AI research, and the mutual permeability of art and kitsch in the digital culture. I connect these notional factors with the conceptual and ideological substrate of the computer science and AI industry, which blends the machinic agency fetishism, the equalization of computers and humans, the sociotechnical blindness, and cyberlibertarianism. The overtones of alienation, sociopathy, and misanthropy in the disparate but somehow coalescing philosophical premises, technical ideas, and political views in this substrate remain underexposed in AI studies so, in the closing discussion, I outline their manifestations in generative AI and introduce several viewpoints for a further critique of AI's cultural zeitgeist. They add a touch of skepticism to pondering how technological trends change our understanding of art and in which directions they stir its social, economic, and political roles.

</details>


### [59] [SMaRT: Online Reusable Resource Assignment and an Application to Mediation in the Kenyan Judiciary](https://arxiv.org/abs/2602.18431)
*Shafkat Farabi,Didac Marti Pinto,Wei Lu,Manuel Ramos-Maqueda,Sanmay Das,Antoine Deeb,Anja Sautmann*

Main category: cs.CY

TL;DR: SMaRT算法：针对肯尼亚司法调解员分配问题的在线资源分配方法，结合二次规划和多智能体赌博机学习，解决未知质量、软容量约束和高维状态空间挑战。


<details>
  <summary>Details</summary>
Motivation: 解决肯尼亚司法系统中调解员分配的实际问题：需要立即将案件分配给能力有限的调解员，调解员质量未知，只能处理特定类型和地理位置的案件，且存在软容量约束，现有算法无法处理这种高维复杂场景。

Method: 使用二次规划进行任务分配，结合多智能体赌博机框架进行学习。SMaRT算法能够处理调解员质量未知、软容量约束和任务-资源匹配的高维状态空间。

Result: 在模拟和真实数据上，SMaRT算法优于基线方法，能够在容量约束严格性和案件解决率之间进行权衡控制。在调解员质量已知和需要学习的场景下都表现良好。

Conclusion: SMaRT算法有效解决了肯尼亚司法调解员分配的实际问题，计划在近期进行随机对照试验。该方法结合了优化和学习，适用于具有未知质量、软容量约束和高维状态空间的复杂资源分配问题。

Abstract: Motivated by the problem of assigning mediators to cases in the Kenyan judicial, we study an online resource allocation problem where incoming tasks (cases) must be immediately assigned to available, capacity-constrained resources (mediators). The resources differ in their quality, which may need to be learned. In addition, resources can only be assigned to a subset of tasks that overlaps to varying degrees with the subset of tasks other resources can be assigned to. The objective is to maximize task completion while satisfying soft capacity constraints across all the resources. The scale of the real-world problem poses substantial challenges, since there are over 2000 mediators and a multitude of combinations of geographic locations (87) and case types (12) that each mediator is qualified to work on. Together, these features, unknown quality of new resources, soft capacity constraints, and a high-dimensional state space, make existing scheduling and resource allocation algorithms either inapplicable or inefficient. We formalize the problem in a tractable manner using a quadratic program formulation for assignment and a multi-agent bandit-style framework for learning. We demonstrate the key properties and advantages of our new algorithm, SMaRT (Selecting Mediators that are Right for the Task), compared with baselines on stylized instances of the mediator allocation problem. We then consider its application to real-world data on cases and mediators from the Kenyan judiciary. SMaRT outperforms baselines and allows control over the tradeoff between the strictness of capacity constraints and overall case resolution rates, both in settings where mediator quality is known beforehand and in bandit-like settings where learning is part of the problem definition. On the strength of these results, we plan to run a randomized controlled trial with SMaRT in the judiciary in the near future.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates](https://arxiv.org/abs/2602.17683)
*Irene Iele,Giulia Romoli,Daniele Molino,Elena Mulero Ayllón,Filippo Ruffini,Paolo Soda,Matteo Tortora*

Main category: cs.LG

TL;DR: 提出基于Transformer的概率预测框架，用于卫星NDVI植被指数预测，通过分离历史植被动态与未来外生信息建模，解决云层遮挡导致的稀疏不规则采样问题。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要准确的短期植被动态预测，但卫星NDVI预测面临云层遮挡导致的稀疏不规则采样以及作物生长的异质性气候条件等挑战。

Method: 基于Transformer的架构，分离历史植被动态与未来外生信息建模；引入时间距离加权分位数损失处理不规则重访模式；加入累积和极端天气特征工程捕捉延迟气象效应。

Result: 在欧洲卫星数据上的广泛实验表明，该方法在点预测和概率评估指标上均优于统计、深度学习和近期时间序列基线方法。

Conclusion: 提出的概率预测框架能有效处理卫星NDVI预测中的挑战，目标历史建模起核心作用，气象协变量提供补充增益，代码已开源。

Abstract: Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.

</details>


### [61] [Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure](https://arxiv.org/abs/2602.17699)
*Chandrasekhar Gokavarapu,Sudhakar Gadde,Y. Rajasekhar,S. R. Bhargava*

Main category: cs.LG

TL;DR: 提出一个理论框架，用于在分布偏移下对预测器的风险提供可验证的显式上界，通过可计算的偏移度量和模型参数来保证风险。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中分布偏移下的风险认证问题，提供可验证的保证而非经验性评估，确保模型在现实世界部署中的可靠性。

Method: 建立统一的理论框架，基于可验证的规律性和复杂性约束，推导出由可计算的偏移度量和模型参数决定的显式风险上界。

Result: 在分布偏移下，预测器的超额风险存在显式上界，可通过计算偏移度量和模型参数来认证；框架支持非平凡规模下的模型验证，并通过可识别性条件确保可解释性。

Conclusion: 该框架为分布偏移下的风险提供了可验证的认证方法，明确了假设条件、失效模式和非可认证机制，为机器学习模型的可靠部署提供了理论保证。

Abstract: Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.

</details>


### [62] [Provable Adversarial Robustness in In-Context Learning](https://arxiv.org/abs/2602.17743)
*Di Zhang*

Main category: cs.LG

TL;DR: 论文提出一个分布鲁棒的元学习框架，为ICL在Wasserstein分布偏移下提供最坏情况性能保证，发现模型鲁棒性与容量平方根成正比，对抗设置带来与扰动幅度平方成正比的样本复杂度惩罚。


<details>
  <summary>Details</summary>
Motivation: 当前ICL的理论解释假设测试任务与预训练分布相似，忽略了对抗性分布偏移对实际可靠性的威胁。需要填补这一理论空白，为ICL在对抗条件下的鲁棒性提供理论保证。

Method: 引入分布鲁棒的元学习框架，基于Wasserstein距离量化分布偏移。聚焦线性自注意力Transformer，推导非渐近边界，分析对抗扰动强度(ρ)、模型容量(m)和上下文示例数(N)之间的关系。

Result: 发现模型鲁棒性随容量平方根缩放(ρ_max ∝ √m)，对抗设置带来与扰动幅度平方成正比的样本复杂度惩罚(N_ρ - N_0 ∝ ρ^2)。合成任务实验验证了这些缩放规律。

Conclusion: 这些发现推进了对ICL在对抗条件下极限的理论理解，表明模型容量是分布鲁棒性的基本资源，为设计更鲁棒的ICL系统提供了理论指导。

Abstract: Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($ρ$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($ρ_{\text{max}} \propto \sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_ρ- N_0 \propto ρ^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.

</details>


### [63] [Bayesian Optimality of In-Context Learning with Selective State Spaces](https://arxiv.org/abs/2602.17744)
*Di Zhang,Jiaqi Xing*

Main category: cs.LG

TL;DR: 该论文提出贝叶斯最优序列预测作为理解上下文学习的新原则，证明选择性状态空间模型能实现贝叶斯最优预测器，在统计效率上优于基于梯度下降的Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将Transformer的上下文学习解释为隐式梯度下降，但作者认为这不能完全解释其效率。他们希望从贝叶斯最优推理的角度重新理解上下文学习，为架构设计提供更原则性的基础。

Method: 将上下文学习形式化为元学习问题，在线性高斯状态空间模型任务中，证明元训练的选择性状态空间模型能渐近实现贝叶斯最优预测器（后验预测均值）。构建具有时间相关噪声的任务，展示贝叶斯预测器严格优于任何经验风险最小化估计器。

Result: 在合成LG-SSM任务和字符级马尔可夫基准测试中，选择性SSM更快收敛到贝叶斯最优风险，在结构化噪声设置中具有更好的样本效率，比线性Transformer更稳健地跟踪潜在状态。

Conclusion: 将上下文学习从"隐式优化"重新框架为"最优推理"，解释了选择性SSM的效率，并为架构设计提供了原则性基础。贝叶斯最优序列预测为理解上下文学习提供了新的理论视角。

Abstract: We propose Bayesian optimal sequential prediction as a new principle for understanding in-context learning (ICL). Unlike interpretations framing Transformers as performing implicit gradient descent, we formalize ICL as meta-learning over latent sequence tasks. For tasks governed by Linear Gaussian State Space Models (LG-SSMs), we prove a meta-trained selective SSM asymptotically implements the Bayes-optimal predictor, converging to the posterior predictive mean. We further establish a statistical separation from gradient descent, constructing tasks with temporally correlated noise where the optimal Bayesian predictor strictly outperforms any empirical risk minimization (ERM) estimator. Since Transformers can be seen as performing implicit ERM, this demonstrates selective SSMs achieve lower asymptotic risk due to superior statistical efficiency. Experiments on synthetic LG-SSM tasks and a character-level Markov benchmark confirm selective SSMs converge faster to Bayes-optimal risk, show superior sample efficiency with longer contexts in structured-noise settings, and track latent states more robustly than linear Transformers. This reframes ICL from "implicit optimization" to "optimal inference," explaining the efficiency of selective SSMs and offering a principled basis for architecture design.

</details>


### [64] [Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving](https://arxiv.org/abs/2602.17677)
*Sutej Kulgod,Sean Ye,Sanchit Tanwar,Christoffer Heckman*

Main category: cs.LG

TL;DR: 该论文指出MCQA基准测试存在文本线索漏洞，导致VLM无需视觉输入即可获得高准确率。作者提出方法将盲准确率从+66.9%降至+2.9%，强制模型依赖视觉基础。


<details>
  <summary>Details</summary>
Motivation: 现有MCQA基准测试存在严重缺陷：合成生成的MCQA数据容易被模型利用文本线索而非视觉内容来回答问题，导致性能评估失真，无法真实反映模型的视觉理解能力。

Method: 通过解耦正确答案与语言伪影（消除文本线索），并采用课程学习策略，强制模型依赖视觉基础进行推理，而非利用文本模式匹配。

Result: 提出的方法显著减少了可被利用的文本捷径，将盲准确率（无视觉输入时的准确率）从比随机高66.9%降低到仅比随机高2.9%，使性能评估更真实反映感知理解能力。

Conclusion: 需要重新评估基于MCQA的VLM基准测试，确保性能测量真正反映视觉理解而非文本模式匹配。提出的方法能有效消除文本线索漏洞，使评估更可靠。

Abstract: Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.

</details>


### [65] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: CodeScaler是一个无需执行的奖励模型，用于扩展代码生成的强化学习和推理，通过语法感知的代码提取和奖励塑造，在多个基准上超越基于执行的RL方法，并显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）依赖单元测试的执行反馈，但其可扩展性受到高质量测试用例可用性和可靠性的限制。需要一种无需执行的奖励模型来扩展代码生成的强化学习和推理。

Method: CodeScaler是一个执行无关的奖励模型，基于从已验证代码问题中精心策划的偏好数据进行训练。采用语法感知的代码提取和保持有效性的奖励塑造技术，确保稳定和鲁棒的优化。

Result: 在五个编码基准上，CodeScaler将Qwen3-8B-Base平均提升11.72分，比基于执行的RL高出1.82分。在推理时，延迟降低10倍，性能与单元测试方法相当。在RM-Bench上，不仅在代码领域（+3.3分），在通用和推理领域（平均+2.7分）也超越现有奖励模型。

Conclusion: CodeScaler提供了一种可扩展的、无需执行的奖励建模方法，能够有效提升代码生成模型的性能，同时显著降低推理延迟，为代码LLM的训练和推理提供了更高效的解决方案。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [66] [Avoid What You Know: Divergent Trajectory Balance for GFlowNets](https://arxiv.org/abs/2602.17827)
*Pedro Dall'Antonia,Tiago da Silva,Daniel Csillag,Salem Lahlou,Diego Mesquita*

Main category: cs.LG

TL;DR: 提出Adaptive Complementary Exploration (ACE)算法，通过训练专门的探索GFlowNet来搜索未被充分探索的高奖励区域，显著提升GFlowNets的学习效率和探索能力。


<details>
  <summary>Details</summary>
Motivation: GFlowNets在学习过程中面临探索效率低下的问题，现有方法（如好奇心驱动搜索和自监督随机网络蒸馏）会在已经充分探索的区域浪费样本，需要更有效的探索策略来发现新颖且高概率的区域。

Method: 提出Adaptive Complementary Exploration (ACE)算法，引入一个专门的探索GFlowNet，该网络被训练来搜索那些被标准GFlowNet（学习目标分布）未充分探索的高奖励状态区域。

Result: 通过大量实验证明，ACE在目标分布的近似精度和多样化高奖励状态的发现率方面显著优于先前的工作。

Conclusion: ACE是一种原则性的算法，能够有效提升GFlowNets在学习过程中对新颖和高概率区域的探索能力，解决了现有探索方法的局限性。

Abstract: Generative Flow Networks (GFlowNets) are a flexible family of amortized samplers trained to generate discrete and compositional objects with probability proportional to a reward function. However, learning efficiency is constrained by the model's ability to rapidly explore diverse high-probability regions during training. To mitigate this issue, recent works have focused on incentivizing the exploration of unvisited and valuable states via curiosity-driven search and self-supervised random network distillation, which tend to waste samples on already well-approximated regions of the state space. In this context, we propose Adaptive Complementary Exploration (ACE), a principled algorithm for the effective exploration of novel and high-probability regions when learning GFlowNets. To achieve this, ACE introduces an exploration GFlowNet explicitly trained to search for high-reward states in regions underexplored by the canonical GFlowNet, which learns to sample from the target distribution. Through extensive experiments, we show that ACE significantly improves upon prior work in terms of approximation accuracy to the target distribution and discovery rate of diverse high-reward states.

</details>


### [67] [Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization](https://arxiv.org/abs/2602.17679)
*Saksham Kiroriwal,Julius Pfrommer,Jürgen Beyerer*

Main category: cs.LG

TL;DR: 提出POGPN-JPSS框架，结合部分可观测高斯过程网络与联合参数状态空间建模，利用专家知识提取高维中间观测的低维特征，显著提升多阶段生物乙醇生产过程优化效率


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在处理高维多阶段制造过程时性能受限，标准方法忽略中间观测和过程结构；现有POGPN方法难以处理高维状态空间时间序列的中间观测

Method: 提出POGPN-JPSS框架，将过程建模为有向无环图，结合专家知识从高维状态空间数据提取低维潜在特征，使用联合参数状态空间建模利用中间提取信息

Result: 在多阶段生物乙醇生产过程的高维仿真中，POGPN-JPSS显著优于最先进方法，达到期望性能阈值的速度提高两倍，可靠性更高，大幅节省时间和资源

Conclusion: 将专家知识与结构化概率模型结合对于快速过程成熟至关重要，POGPN-JPSS框架为高维多阶段制造过程优化提供了有效解决方案

Abstract: Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.

</details>


### [68] [Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO](https://arxiv.org/abs/2602.17686)
*Bowen Yu,Maolin Wang,Sheng Zhang,Binhao Wang,Yi Wen,Jingtong Gao,Bowen Liu,Zimo Zhao,Wanyu Wang,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出三阶段课程学习框架，通过渐进式技能获取解决大模型与小模型在思维链推理蒸馏中的容量不匹配问题，在保持精度的同时显著缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理蒸馏方法存在根本挑战：教师模型的推理过程通常过于冗长，小型学生模型难以忠实复现。现有方法要么将推理压缩为单步（失去可解释性），要么直接蒸馏（输出过长）。

Method: 三阶段课程学习框架：1) 通过掩码打乱重构建立结构理解；2) 在掩码补全任务上应用组相对策略优化，让模型自主发现精度与简洁性的平衡；3) 识别持续失败案例，通过针对性重写指导学生内化教师知识，同样使用GRPO优化。

Result: 在GSM8K数据集上，该方法使Qwen2.5-3B-Base模型准确率提升11.29%，同时输出长度减少27.4%，超越了指令调优变体和先前的蒸馏方法。

Conclusion: 该框架有效解决了思维链推理蒸馏中的容量不匹配问题，使小型模型能够在保持可解释性的同时，生成更简洁、更准确的推理过程。

Abstract: Distilling Chain-of-Thought (CoT) reasoning from large language models into compact student models presents a fundamental challenge: teacher rationales are often too verbose for smaller models to faithfully reproduce. Existing approaches either compress reasoning into single-step, losing the interpretability that makes CoT valuable. We present a three-stage curriculum learning framework that addresses this capacity mismatch through progressive skill acquisition. First, we establish structural understanding via masked shuffled reconstruction. Second, we apply Group Relative Policy Optimization (GRPO) on masked completion tasks, enabling the model to discover its own balance between accuracy and brevity. Third, we identify persistent failure cases and guide the student to internalize teacher knowledge through targeted rewriting, again optimized with GRPO. Experiments on GSM8K demonstrate that our approach enables Qwen2.5-3B-Base to achieve an 11.29 percent accuracy improvement while reducing output length by 27.4 percent, surpassing both instruction-tuned variants and prior distillation methods.

</details>


### [69] [Learning Without Training](https://arxiv.org/abs/2602.17985)
*Ryan O'Dowd*

Main category: cs.LG

TL;DR: 该博士论文包含三个机器学习项目：1) 监督学习和流形学习的函数逼近方法；2) 部分数据已知情况下的迁移学习函数提升理论；3) 基于信号分离技术的主动学习分类新算法。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在大规模问题上的成功，机器学习研究日益增多。本文旨在通过数学理论解决机器学习中的三个核心问题：函数逼近的理论缺陷、跨域知识迁移的局限性，以及传统分类方法的不足。

Method: 1) 针对监督学习提出新的函数逼近方法；2) 研究部分数据已知情况下的函数提升理论，分析提升定义域和局部光滑性关系；3) 将信号分离技术应用于分类任务，提出新的主动学习算法。

Result: 1) 改进了当前监督学习范式的理论缺陷；2) 建立了函数提升的理论框架；3) 新分类算法在保持竞争力的准确率同时，显著提升了计算速度。

Conclusion: 该论文通过数学理论方法解决了机器学习中的三个重要问题，为函数逼近、迁移学习和分类任务提供了新的理论框架和高效算法，推动了机器学习理论的发展。

Abstract: Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.

</details>


### [70] [BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs](https://arxiv.org/abs/2602.17680)
*Yujia Wang,Jihong Guan,Wengen Li,Shuigeng Zhou,Xuhong Wang*

Main category: cs.LG

TL;DR: BioBridge是一个蛋白质理解领域自适应持续预训练框架，通过结合蛋白质语言模型和通用大语言模型的优势，实现蛋白质序列理解和通用推理能力


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型在多任务适应性和跨生物上下文泛化能力有限，而通用大语言模型缺乏蛋白质序列解释能力和领域特定知识，无法进行有效的生物语义推理

Method: 提出BioBridge框架，采用领域增量持续预训练(DICP)同时注入蛋白质领域知识和通用推理语料，通过PLM-Projector-LLM管道实现跨模态对齐，采用端到端优化支持蛋白质性质预测和知识问答等多种任务

Result: BioBridge在EC和BindingDB等多个蛋白质基准测试中表现与主流PLMs相当，在MMLU和RACE等通用理解任务上达到与LLMs相当的结果

Conclusion: BioBridge展示了将领域特定适应性与通用语言能力相结合的创新优势，成功融合了蛋白质语言模型和通用大语言模型的优点

Abstract: Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.

</details>


### [71] [Generating adversarial inputs for a graph neural network model of AC power flow](https://arxiv.org/abs/2602.17975)
*Robert Parker*

Main category: cs.LG

TL;DR: 该研究通过优化方法生成对抗性输入点，使神经网络预测的交流潮流解与真实方程解产生显著误差，在14节点测试电网中验证了CANOS-PF图神经网络模型的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 动机在于揭示神经网络潮流模型的安全漏洞，证明即使经过训练的神经网络模型也可能被精心设计的对抗性输入攻击，从而产生严重错误的潮流预测结果。

Method: 方法包括：1) 构建优化问题来生成使神经网络预测误差最大化的输入点；2) 在PFΔ基准库中的CANOS-PF图神经网络模型上实施；3) 使用14节点测试电网作为实验平台；4) 最小化满足对抗性约束所需的扰动。

Result: 结果：1) 生成的对抗性点能产生高达3.4pu的无功功率误差和0.08pu的电压幅值误差；2) 仅需在单个母线上施加0.04pu的电压幅值扰动即可满足对抗性约束；3) 验证了神经网络潮流模型的脆弱性。

Conclusion: 结论强调需要为神经网络潮流模型开发严格的验证方法和鲁棒训练技术，以确保电力系统运行的安全性和可靠性。

Abstract: This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$Δ$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.

</details>


### [72] [Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction](https://arxiv.org/abs/2602.17689)
*Melika Filvantorkaman,Mohsen Piri*

Main category: cs.LG

TL;DR: Robust-MMR：一种自监督预训练框架，通过非对称扰动感知掩码、领域一致性正则化和模态弹性约束，在医学视觉语言模型中显式建模鲁棒性，提升跨域性能。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型在成像设备、采集协议和报告风格变化导致的领域偏移下性能下降，现有多模态预训练方法大多忽视鲁棒性，将其视为下游适应问题。

Method: 提出Robust-MMR框架，将鲁棒性目标融入掩码视觉语言学习，包含：1) 非对称扰动感知掩码；2) 领域一致性正则化；3) 模态弹性约束，以鼓励领域不变表示。

Result: 在多个医学视觉语言基准测试中表现优异：VQA-RAD跨域准确率达78.9%（提升3.8%），SLAKE 74.6%，VQA-2019 77.0%；扰动评估下VQA-RAD准确率从69.1%提升至75.6%；MELINDA跨域准确率从70.3%提升至75.2%；检索任务中平均排名退化从16+降至4.1。

Conclusion: 在预训练阶段显式建模鲁棒性能够产生更可靠、可迁移的医学视觉语言表示，适用于真实世界部署，改善了疾病检测和结构异常评估的临床推理能力。

Abstract: Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.

</details>


### [73] [PRISM: Parallel Reward Integration with Symmetry for MORL](https://arxiv.org/abs/2602.18277)
*Finn van der Knaap,Kejiang Qian,Zheng Xu,Fengxiang He*

Main category: cs.LG

TL;DR: PRISM算法通过反射对称性解决多目标强化学习中目标时间频率差异问题，显著提升稀疏长时奖励的学习效率


<details>
  <summary>Details</summary>
Motivation: 异构多目标强化学习中，不同目标的时间频率差异巨大，导致密集奖励目标主导学习，而稀疏长时奖励的信用分配薄弱，样本效率低下

Method: 提出PRISM算法，包含ReSymNet模型（使用残差块学习缩放机会价值）和SymReg正则化器（强制反射等变性），通过反射对称性对齐奖励通道

Result: 在MuJoCo基准测试中，PRISM显著优于稀疏奖励基线和完全密集奖励的oracle，超体积增益超过基线100%，比oracle提升32%，改善了帕累托覆盖和分布平衡

Conclusion: 通过反射对称性作为归纳偏置，PRISM有效解决了多目标强化学习中的时间频率不匹配问题，提高了样本效率和泛化能力

Abstract: This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\% over the baseline and up to 32\% over the oracle. The code is at \href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.

</details>


### [74] [LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs](https://arxiv.org/abs/2602.17681)
*Ofir Gordon,Lior Dikstein,Arnon Netzer,Idan Achituve,Hai Victor Habi*

Main category: cs.LG

TL;DR: 本文提出LATMiX方法，通过可学习的可逆仿射变换优化MX低比特量化，在多种模型大小和零样本基准测试中取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法主要关注传统量化方案，而现代硬件越来越支持微缩放(MX)数据格式。将激活变换与MX量化结合时会出现严重性能下降，先前工作不得不对变换施加限制性假设。

Method: 首先对MX量化下的变换进行理论分析，推导量化误差边界。基于此分析提出LATMiX方法，将异常值减少推广到可学习的可逆仿射变换，使用标准深度学习工具进行优化。

Result: 实验表明，在广泛的零样本基准测试中，LATMiX在MX低比特量化上相比强基线取得了一致的平均准确率提升，且适用于多种模型大小。

Conclusion: 通过理论分析和可学习变换设计，LATMiX成功解决了MX量化与激活变换结合时的性能下降问题，为大语言模型的高效部署提供了有效解决方案。

Abstract: Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.

</details>


### [75] [PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting](https://arxiv.org/abs/2602.17998)
*Shubham Bhardwaj,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: PHAST：一种基于端口哈密顿框架的物理结构学习方法，仅使用位置观测数据实现稳定长期预测和物理参数恢复


<details>
  <summary>Details</summary>
Motivation: 真实物理系统都是耗散的，从部分观测中预测其动力学是科学机器学习中的核心挑战。特别是"仅位置"问题：只给定离散时间的位置观测（动量为隐变量），需要学习一个既能产生稳定长期预测，又能在提供足够结构时恢复物理意义参数的模型。

Method: 提出PHAST（端口哈密顿结构时间动力学架构），基于端口哈密顿框架将保守-耗散分解为$\dot{x}=(J-R)\nabla H(x)$，保证能量衰减。将哈密顿量分解为势能$V(q)$、质量$M(q)$和阻尼$D(q)$三个部分，对应三种知识状态（已知、部分已知、未知），使用高效低秩PSD/SPD参数化，并通过Strang分裂推进动力学。

Result: 在13个仅位置基准测试（涵盖机械、电气、分子、热、重力和生态系统）中，PHAST在长期预测方面优于竞争基线，并在有足够锚点时能够恢复物理意义参数。研究表明，没有这种锚点时识别问题是病态的（规范自由度）。

Conclusion: PHAST成功解决了仅位置观测的物理系统建模问题，实现了稳定长期预测和物理参数恢复。提出了两轴评估框架，将预测稳定性与可识别性分开评估，为物理结构学习提供了系统方法。

Abstract: Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\dot{x}=(J-R)\nabla H(x)$, guaranteeing $dH/dt\le 0$ when $R\succeq 0$. We introduce \textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.

</details>


### [76] [Agentic Unlearning: When LLM Agent Meets Machine Unlearning](https://arxiv.org/abs/2602.17692)
*Bin Wang,Fan Wang,Pingping Wang,Jinyu Cong,Yang Yu,Yilong Yin,Zhongyi Han,Benzheng Wei*

Main category: cs.LG

TL;DR: 提出"agentic unlearning"概念，通过同步参数和记忆双路径遗忘机制，解决传统遗忘方法中参数-记忆回流污染问题，实现更彻底的敏感信息移除。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法仅针对模型参数，存在两个关键缺陷：1) 参数-记忆回流问题 - 检索会重新激活参数残留或记忆伪影重新引入敏感内容；2) 缺乏统一的参数和记忆双路径遗忘策略。

Method: 提出同步回流遗忘(SBU)框架：记忆路径采用基于依赖闭包的遗忘，修剪孤立实体并逻辑上使共享伪影失效；参数路径采用随机参考对齐，引导模型输出向高熵先验；通过同步双更新协议集成双路径，形成闭环机制防止跨路径再污染。

Result: 在医疗QA基准测试中，SBU显著减少了目标隐私信息在双路径中的痕迹，同时对保留数据的性能影响有限。

Conclusion: SBU框架通过同步参数和记忆双路径遗忘，有效解决了agentic系统中敏感信息的彻底移除问题，防止了参数-记忆回流污染，为具有闭环交互的智能体提供了更安全的遗忘机制。

Abstract: In this paper, we introduce \textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.

</details>


### [77] [Non-Stationary Online Resource Allocation: Learning from a Single Sample](https://arxiv.org/abs/2602.18114)
*Yiding Feng,Jiashuo Jiang,Yige Wang*

Main category: cs.LG

TL;DR: 研究非平稳需求下的在线资源分配问题，仅需每期一个历史样本，提出基于分位数的元策略，在两种样本信息设置下分别达到√T和对数级遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中资源分配面临的两个关键挑战：需求分布的非平稳性和离线数据稀缺性。传统方法通常假设平稳环境或需要大量历史数据，而实际应用中需求模式可能任意变化且历史数据有限。

Method: 提出类型依赖的分位数元策略，将问题解耦为三个模块：1) 奖励分布估计；2) 通过流体松弛优化目标服务概率；3) 通过动态接受阈值进行实时决策。针对两种样本信息设置分别设计策略：奖励观测样本使用静态阈值策略，类型仅样本在最小到达概率假设下设计部分自适应和完全自适应策略。

Result: 对于奖励观测样本，静态阈值策略达到Õ(√T)遗憾；对于类型仅样本，在最小到达概率假设下，部分自适应策略达到Õ(√T)遗憾，完全自适应解析策略首次实现O((log T)³)的多对数遗憾，这是非平稳多资源分配问题的首个对数级遗憾保证。

Conclusion: 该框架在仅需每期一个历史样本的极简数据要求下，能够处理任意非平稳性（无需变化预算假设），支持多资源约束，显著推进了非平稳在线资源分配的理论边界，为实际应用提供了更实用的算法基础。

Abstract: We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.
  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\tilde{O}(\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.

</details>


### [78] [PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing](https://arxiv.org/abs/2602.18396)
*Ehsan Lari,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: PRISM-FCP：一种拜占庭鲁棒的联邦共形预测框架，通过部分模型共享和统计边缘校准，在训练和校准阶段都抵抗拜占庭攻击，保持覆盖保证的同时减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦共形预测方法只在校准阶段处理对抗行为，导致学习模型容易受到训练阶段的投毒攻击。需要一种端到端的鲁棒解决方案，在模型训练和共形校准阶段都能抵御拜占庭攻击。

Method: 1. 训练阶段：客户端每轮只传输D个参数中的M个（部分模型共享），将攻击者扰动的期望能量衰减M/D倍；2. 校准阶段：将非共形分数转换为特征向量，计算基于距离的恶意分数，在估计共形分位数前对可疑拜占庭贡献进行降权或过滤。

Result: 在合成数据和UCI超导数据集上的实验表明，PRISM-FCP在拜占庭攻击下保持名义覆盖保证，避免了标准FCP中观察到的区间膨胀，同时减少了通信开销。

Conclusion: PRISM-FCP提供了一个鲁棒且通信高效的联邦不确定性量化方法，通过端到端的拜占庭攻击缓解，在训练和校准阶段都实现了鲁棒性，为联邦学习中的可靠不确定性估计提供了解决方案。

Abstract: We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.

</details>


### [79] [Duality Models: An Embarrassingly Simple One-step Generation Paradigm](https://arxiv.org/abs/2602.17682)
*Peng Sun,Xinyi Shang,Tao Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 提出Duality Models (DuMo)解決一致性生成模型中訓練目標分離問題，通過"單輸入雙輸出"架構同時預測速度和流映射，在ImageNet 256×256上僅用2步達到SOTA FID 1.79


<details>
  <summary>Details</summary>
Motivation: 傳統一致性生成模型（如Shortcut和MeanFlow）採用"單輸入單輸出"範式，需要將訓練預算分割為多步目標（如75%）和少步目標，導致少步生成訓練不足，影響收斂性和可擴展性

Method: 提出Duality Models (DuMo)，採用"單輸入雙輸出"範式，使用共享骨幹網絡和雙頭架構，從單一輸入x_t同時預測速度v_t和流映射u_t，將多步目標的幾何約束應用於每個樣本

Result: 在ImageNet 256×256數據集上，使用679M參數的擴散變壓器和SD-VAE，僅需2步就達到1.79的FID，創下新的SOTA結果

Conclusion: DuMo通過消除訓練目標分離，顯著提高了穩定性和效率，為一致性生成模型提供了更優的訓練範式，實現了高效的高質量圖像生成

Abstract: Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional "one input, one output" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a "one input, dual output" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo

</details>


### [80] [TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs](https://arxiv.org/abs/2602.18109)
*Rong Fu,Yibo Meng,Guangzhen Yao,Jiaxuan Lu,Zeyu Zhang,Zhaolu Kang,Ziming Guo,Jia Yee Tan,Xiaojing Du,Simon James Fong*

Main category: cs.LG

TL;DR: TempoNet是一个基于强化学习的实时调度器，使用Transformer架构和深度Q近似，通过Urgency Tokenizer处理时间松弛度，实现高效的多核任务调度，在工业环境中优于传统调度器。


<details>
  <summary>Details</summary>
Motivation: 实时调度器需要在严格的计算预算下处理紧截止期限，传统调度方法在处理复杂任务集时存在局限性，需要更智能、高效的调度解决方案。

Method: 使用强化学习框架，结合置换不变Transformer和深度Q近似；Urgency Tokenizer将时间松弛度离散化为可学习嵌入；延迟感知稀疏注意力机制支持全局推理；多核映射层将Q分数转换为处理器分配。

Result: 在工业混合关键性追踪和大规模多处理器环境中，相比分析调度器和神经基线，在截止期限满足率方面获得持续提升，同时改善了优化稳定性，推理时间亚毫秒级。

Conclusion: TempoNet为基于Transformer的高吞吐量实时调度决策建立了实用框架，展示了强化学习在复杂调度问题中的实际应用价值。

Abstract: Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.

</details>


### [81] [A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU](https://arxiv.org/abs/2602.17693)
*Yuchen Luo,Fangyue Zhu,Ruining Zhou,Mingzhe Huang,Jian Zhu,Fanyu Fan,Wei Shao*

Main category: cs.LG

TL;DR: 本文通过案例研究评估了四种PTQ算法在昇腾NPU上的表现，发现4位权重量化对大型模型可行，但4位权重-激活量化在NPU上存在校准不稳定问题，而8位量化保持稳定。实际部署中动态量化开销限制了端到端加速。


<details>
  <summary>Details</summary>
Motivation: 后训练量化（PTQ）对高效模型部署至关重要，但其在昇腾NPU上的有效性相比GPU架构研究不足。本文旨在探索代表性PTQ基线在推理导向模型（如DeepSeek-R1-Distill-Qwen系列和QwQ-32B）在昇腾NPU上的实际表现。

Method: 采用案例研究方法，评估四种不同的PTQ算法：AWQ、GPTQ、SmoothQuant和FlatQuant，覆盖从仅权重量化到基于旋转的高级方法。在昇腾NPU上对DeepSeek-R1-Distill-Qwen系列（1.5B/7B/14B）和QwQ-32B模型进行实证评估。

Result: 1）4位仅权重量化对大型模型可行；2）激进的4位权重-激活量化方案在NPU上存在层间校准不稳定问题，导致长上下文推理任务中的逻辑崩溃；3）标准8位量化保持数值稳定；4）实际INT8部署中，尽管优化内核降低了延迟，但动态量化开销目前限制了端到端加速。

Conclusion: 研究结果为在昇腾NPU上部署量化推理模型的可行性和局限性提供了实用参考。平台敏感性显著，需要针对NPU架构优化量化策略，特别是对于权重-激活量化方案。

Abstract: Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.

</details>


### [82] [Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay](https://arxiv.org/abs/2602.18401)
*Josue Casco-Rodriguez,Nanda H. Krishna,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 本文重新审视噪声循环神经网络中的重放现象，将其视为采样过程，提出了三种改进方法：利用隐藏状态泄漏促进重放，分析隐藏状态适应的探索-速度权衡，以及通过隐藏状态动量实现时间压缩重放。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络（如海马体）能够内部生成类似刺激驱动活动的"重放"现象。现有计算模型将噪声循环神经网络中的重放描述为朗之万采样，但新的改进方法已经超越了这一描述。本文旨在重新审视噪声循环神经网络中的重放作为采样过程，以更好地理解和改进它。

Method: 1. 在简单假设下证明重放活动应遵循的梯度是时变的且难以估计，但可以解释为什么在RNN中使用隐藏状态泄漏有助于重放。
2. 验证隐藏状态适应（负反馈）促进重放中的探索，但会导致非马尔可夫采样从而减慢重放速度。
3. 提出首个通过隐藏状态动量在噪声路径积分RNN中实现时间压缩重放的模型，将其与欠阻尼朗之万采样联系起来，并展示与适应相结合可以抵消速度减慢同时保持探索。

Result: 通过2D三角形路径、T型迷宫路径以及合成大鼠位置细胞活动的高维路径的路径积分验证了研究发现。结果表明：隐藏状态泄漏有助于重放；隐藏状态适应促进探索但减慢重放；隐藏状态动量与适应相结合可以实现时间压缩重放，同时保持探索能力。

Conclusion: 本文为噪声循环神经网络中的重放现象提供了新的采样理论视角，提出了改进重放性能的具体方法。通过结合隐藏状态适应和动量，可以在保持探索能力的同时实现更高效的时间压缩重放，这为理解和模拟生物神经网络中的重放机制提供了新的计算框架。

Abstract: Biological neural networks (like the hippocampus) can internally generate "replay" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.

</details>


### [83] [AsynDBT: Asynchronous Distributed Bilevel Tuning for efficient In-Context Learning with Large Language Models](https://arxiv.org/abs/2602.17694)
*Hui Ma,Shaoyu Dou,Ya Liu,Fei Xing,Li Feng,Feng Pi*

Main category: cs.LG

TL;DR: 提出异步分布式双层调优算法AsynDBT，通过优化上下文学习样本和提示片段来提升LLM下游任务性能，同时保护数据隐私并适应异构计算环境。


<details>
  <summary>Details</summary>
Motivation: 云端LLM API使用成本低但参数梯度不可知，用户需手动调整提示；上下文学习(ICL)需要高质量数据但数据敏感难以共享；联邦学习(FL)能保护隐私但存在滞后节点问题和异构数据挑战。

Method: 提出异步分布式双层调优算法AsynDBT，基于LLM反馈同时优化上下文学习样本和提示片段，采用分布式架构保护隐私并适应异构计算环境。

Result: 理论分析证明了算法的收敛性保证，在多个基准数据集上的实验验证了AsynDBT的有效性和效率。

Conclusion: AsynDBT通过分布式双层调优解决了云端LLM优化、ICL数据质量和FL滞后节点等问题，在保护隐私的同时提升了LLM下游任务性能。

Abstract: With the rapid development of large language models (LLMs), an increasing number of applications leverage cloud-based LLM APIs to reduce usage costs. However, since cloud-based models' parameters and gradients are agnostic, users have to manually or use heuristic algorithms to adjust prompts for intervening LLM outputs, which requiring costly optimization procedures. In-context learning (ICL) has recently emerged as a promising paradigm that enables LLMs to adapt to new tasks using examples provided within the input, eliminating the need for parameter updates. Nevertheless, the advancement of ICL is often hindered by the lack of high-quality data, which is often sensitive and different to share. Federated learning (FL) offers a potential solution by enabling collaborative training of distributed LLMs while preserving data privacy. Despite this issues, previous FL approaches that incorporate ICL have struggled with severe straggler problems and challenges associated with heterogeneous non-identically data. To address these problems, we propose an asynchronous distributed bilevel tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on the feedback from the LLM, thereby enhancing downstream task performance. Benefiting from its distributed architecture, AsynDBT provides privacy protection and adaptability to heterogeneous computing environments. Furthermore, we present a theoretical analysis establishing the convergence guarantees of the proposed algorithm. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.

</details>


### [84] [Unifying Formal Explanations: A Complexity-Theoretic Perspective](https://arxiv.org/abs/2602.18160)
*Shahaf Bassan,Xuanxiang Huang,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架来分析ML模型预测的充分原因和对比原因解释，发现计算复杂度取决于价值函数的单调性、次模性和超模性，揭示了全局解释性可多项式时间计算而局部解释性为NP难的反直觉结果。


<details>
  <summary>Details</summary>
Motivation: 先前研究分别探讨了ML模型预测的充分原因和对比原因解释在不同上下文中的计算复杂度，但缺乏统一的分析框架。本文旨在建立一个统一框架来系统分析这些解释的计算特性。

Method: 引入统一框架，将所有解释类型通过最小化统一的概率价值函数来表征。分析价值函数的三个关键组合优化属性：单调性、次模性和超模性，并研究这些属性在局部与全局解释设置中的差异。

Result: 发现局部价值函数不具备单调性或次模性/超模性，而全局价值函数具有这些属性。因此在全局解释性设置中，可在多项式时间内计算多种解释（包括神经网络、决策树等模型），而相应的局部解释性设置中即使是简化版本也是NP难的。

Conclusion: 通过统一框架揭示了ML解释计算复杂度的关键决定因素，证明了全局解释性相比局部解释性在计算上更具优势，为实际应用提供了理论指导。

Abstract: Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.

</details>


### [85] [EXACT: Explicit Attribute-Guided Decoding-Time Personalization](https://arxiv.org/abs/2602.17695)
*Xin Yu,Hanwen Xing,Lingzhou Xue*

Main category: cs.LG

TL;DR: EXACT：一种基于可解释属性的解码时个性化方法，通过识别用户特定属性子集并基于语义检索相关属性来引导生成，有效应对上下文偏好变化


<details>
  <summary>Details</summary>
Motivation: 现有解码时个性化方法依赖隐式、难以解释的偏好表示，且采用僵化的上下文无关用户表示，无法处理不同提示下的偏好变化

Method: EXACT使用预定义的可解释属性集，在离线阶段通过最大化偏好响应似然识别用户特定属性子集；在线推理时基于语义检索与输入提示最相关的属性注入上下文引导生成

Result: 在人工标注的偏好数据集上，EXACT在偏好建模准确率和个性化生成质量方面持续优于强基线方法

Conclusion: EXACT通过可解释属性和语义检索机制有效解决了上下文偏好变化问题，为个性化对齐提供了理论保证和实际有效的解决方案

Abstract: Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.

</details>


### [86] [Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling](https://arxiv.org/abs/2602.17685)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.LG

TL;DR: 提出统一共椭圆机动框架，结合霍曼转移、安全椭圆接近操作和显式加油逻辑，用于低地球轨道多目标主动碎片清除任务规划，比较贪婪启发式、蒙特卡洛树搜索和掩码PPO强化学习三种算法性能。


<details>
  <summary>Details</summary>
Motivation: 解决低地球轨道多目标主动碎片清除的挑战，需要高效、可扩展且资源优化的任务规划方法，以应对随机碎片场、禁飞区和燃料约束等现实约束条件。

Method: 提出统一共椭圆机动框架，结合霍曼转移、安全椭圆接近操作和显式加油逻辑。在包含随机碎片场、禁飞区和ΔV约束的逼真轨道仿真环境中，对比评估贪婪启发式、蒙特卡洛树搜索和掩码近端策略优化强化学习三种规划算法。

Result: 在100个测试场景中，掩码PPO强化学习算法展现出最优的任务效率和计算性能，访问的碎片数量是贪婪算法的两倍，且在运行时间上显著优于蒙特卡洛树搜索。

Conclusion: 现代强化学习方法在可扩展、安全和资源高效的空间任务规划方面具有巨大潜力，为主动碎片清除自主性的未来发展铺平了道路。

Abstract: This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.

</details>


### [87] [Can LLM Safety Be Ensured by Constraining Parameter Regions?](https://arxiv.org/abs/2602.17696)
*Zongmin Li,Jian Su,Farah Benamara,Aixin Sun*

Main category: cs.LG

TL;DR: 当前的安全区域识别方法在不同数据集和模型上表现不一致，无法可靠识别稳定、数据集无关的安全区域


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常被认为包含"安全区域"——直接影响安全行为的参数子集，但现有安全区域识别方法的可靠性需要系统评估

Method: 系统评估四种不同参数粒度（从单个权重到整个Transformer层）的安全区域识别方法，在四个不同规模的LLM家族上，使用十个安全识别数据集

Result: 识别的安全区域仅显示低到中度的重叠（IoU测量），当使用效用数据集（非有害查询）进一步精炼时，重叠显著下降

Conclusion: 当前技术无法可靠识别稳定、数据集无关的安全区域，表明现有安全区域识别方法的局限性

Abstract: Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.

</details>


### [88] [ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs](https://arxiv.org/abs/2602.17698)
*Xinlin Li,Timothy Chou,Josh Fromm,Zichang Liu,Yunjie Pan,Christina Fragouli*

Main category: cs.LG

TL;DR: ScaleBITS：一种混合精度量化框架，通过硬件对齐的块级权重分区和双向通道重排序，在内存预算下实现自动化细粒度比特分配，显著提升超低比特量化性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练权重量化面临挑战：低于4比特时权重敏感性高度不均匀，缺乏原则性精度分配方法。现有方案要么使用高运行时开销的不规则细粒度混合精度，要么依赖启发式或高度受限的精度分配策略。

Method: 提出ScaleBITS混合精度量化框架：1）基于新敏感性分析指导；2）引入硬件对齐的块级权重分区方案，采用双向通道重排序；3）将全局比特分配建模为约束优化问题，开发可扩展的贪心算法近似解。

Result: 实验表明，ScaleBITS在超低比特量化中显著优于均匀精度量化（提升达36%），并超越最先进的敏感性感知基线方法（提升达13%），且不增加运行时开销。

Conclusion: ScaleBITS通过硬件高效的细粒度混合精度分配，解决了LLM超低比特量化的核心挑战，实现了性能与效率的平衡。

Abstract: Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.

</details>


### [89] [AnCoder: Anchored Code Generation via Discrete Diffusion Models](https://arxiv.org/abs/2602.17688)
*Anton Xue,Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 提出AnchorTree框架，通过抽象语法树引导扩散过程，优先解决语法和语义关键token，提高代码生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型为代码生成提供了全局规划和迭代优化的替代方案，但现有方法不尊重编程语言的刚性结构，常产生无法执行的错误程序

Method: 引入AnchorTree框架，使用抽象语法树作为结构化先验，优先解析语法和语义关键token（如关键字、标识符），建立结构支架引导后续生成

Result: 通过AnCoder模型系列验证，结构锚定的扩散方法能以参数高效的方式实现高质量代码生成

Conclusion: 结构化锚定扩散为代码生成提供了有效路径，通过显式利用代码的层次结构先验，显著提升生成程序的质量和可执行性

Abstract: Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.

</details>


### [90] [MIDAS: Mosaic Input-Specific Differentiable Architecture Search](https://arxiv.org/abs/2602.17700)
*Konstanty Subbotko*

Main category: cs.LG

TL;DR: MIDAS提出了一种改进的微分神经架构搜索方法，通过自注意力机制动态生成输入特定的架构参数，采用局部化补丁级选择和拓扑感知搜索空间，在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 微分神经架构搜索（NAS）虽然提供了高效的梯度优化方法，但在实际应用中采用有限。现有方法如DARTS使用静态架构参数，缺乏对不同输入的适应性，且搜索空间设计不够鲁棒。

Method: 1. 用自注意力机制替换DARTS中的静态架构参数，生成动态、输入特定的参数；2. 在激活图的每个空间补丁上局部化计算架构选择；3. 引入参数免费、拓扑感知的搜索空间，建模节点连接性并简化每个节点的两个输入边选择。

Result: 在DARTS搜索空间上，CIFAR-10达到97.42% top-1准确率，CIFAR-100达到83.38%；在NAS-Bench-201中始终找到全局最优架构；在RDARTS的四个搜索空间中有两个达到SOTA；分析显示补丁级注意力提高了候选操作区分度，参数分布具有类别感知性且主要为单峰分布。

Conclusion: MIDAS通过动态输入特定的架构参数和局部化选择机制，显著提升了微分NAS的性能和鲁棒性，为实际应用提供了更可靠的架构搜索解决方案。

Abstract: Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.

</details>


### [91] [Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering](https://arxiv.org/abs/2602.17691)
*Craig Atkinson*

Main category: cs.LG

TL;DR: HELIX框架通过几何方法将隐藏状态轨迹锚定在预计算的真实性流形上，解耦输出熵与幻觉，使量化语言模型能在高温度下保持语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 量化语言模型面临基本困境：低采样温度导致重复、模式崩溃的输出，而高温度（T > 2.0）则引起轨迹发散和语义不连贯。需要解决高温度下的幻觉问题，同时保持创造性。

Method: 提出HELIX几何框架，通过计算统一真实性分数（UTS）结合标记级语义熵和马氏距离，当检测到轨迹发散时，使用分级转向向量将激活重定向到结构连贯区域，仅影响0.2-2.5%的标记。

Result: 在4位量化Granite 4.0 H Small模型上：GSM8K在T=3.0时保持88.84%准确率（仅比T=0.5下降2.81pp）；MMLU在14,042个问题上保持72.49%（下降1.24pp）。跨架构验证显示架构独立性，概念生成独特度提高46.7%。

Conclusion: 高温度幻觉主要是轨迹发散而非语义崩溃，HELIX作为语法锚定器，能够探索语义多样性而不违反逻辑骨架，实现多温度合成，生成比单温度推理多200%的独特概念。

Abstract: Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.
  On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.
  Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.

</details>


### [92] [Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring](https://arxiv.org/abs/2602.17751)
*Nina Brolich,Simon Geis,Maximilian Kasper,Alexander Barnhill,Axel Plinge,Dominik Seuß*

Main category: cs.LG

TL;DR: 该论文提出在微控制器单元(MCU)上运行机器学习模型进行鸟类监测，通过模型压缩实现高效边缘计算，显著降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 生物多样性丧失对人类构成重大威胁，鸟类监测对评估生态系统健康至关重要。传统监测方法成本高、效率低，现有机器学习解决方案需要复杂模型和大量计算资源，不适合野外部署。

Method: 在MCU上直接运行机器学习模型，针对硬件和能源限制设计高效AI架构。训练并压缩不同目标类别数量的模型，评估多物种检测效果，研究物种数量对神经网络可压缩性的影响。

Result: 实现了显著的压缩率且性能损失最小。提供了不同硬件平台的基准测试结果，并评估了部署能源自主设备的可行性。

Conclusion: 提出的方法能够在资源受限的边缘设备上高效进行鸟类监测，为野外部署能源自主的监测设备提供了可行方案。

Abstract: Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.

</details>


### [93] [Financial time series augmentation using transformer based GAN architecture](https://arxiv.org/abs/2602.17865)
*Andrzej Podobiński,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 使用基于Transformer的GAN（TTS-GAN）生成合成数据来增强金融时间序列数据，显著提高了LSTM预测模型的准确性，特别是在比特币和标普500价格数据上。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列数据通常稀缺且波动性大，导致深度学习模型训练不充分、泛化能力差。需要可靠的数据增强方法来提高预测准确性。

Method: 提出使用基于Transformer的GAN（TTS-GAN）生成合成金融时间序列数据作为数据增强工具，并训练LSTM预测模型。同时提出结合动态时间规整（DTW）和改进的深度数据集相似性度量（DeD-iMs）的时间序列特定质量评估指标。

Result: 在比特币和标普500价格数据上，使用TTS-GAN增强数据训练的LSTM模型比仅使用真实数据的模型预测准确性显著提高，在不同预测时间范围内都表现良好。

Conclusion: GAN-based数据增强能有效克服金融领域数据稀缺问题，提高预测能力。提出的时间序列特定质量评估指标能可靠监控训练过程和评估生成数据质量。

Abstract: Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.

</details>


### [94] [MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies](https://arxiv.org/abs/2602.17868)
*Vasilii Feofanov,Songkang Wen,Jianfeng Zhang,Lujia Pan,Ievgen Redko*

Main category: cs.LG

TL;DR: MantisV2和Mantis+时间序列基础模型通过合成数据预训练、架构优化和测试时增强方法，显著提升了零样本特征提取性能，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 开发时间序列分类基础模型具有重要实用价值，可作为通用特征提取器用于下游任务。早期模型如Mantis虽显示潜力，但冻结编码器与微调编码器之间存在显著性能差距。

Method: 1. Mantis+：完全在合成时间序列上预训练的Mantis变体；2. MantisV2：通过受控消融研究改进的轻量级编码器；3. 增强测试时方法：利用中间层表示和改进输出标记聚合；4. 自集成和跨模型嵌入融合进一步提升性能。

Result: 在UCR、UEA、人类活动识别基准和EEG数据集上的广泛实验表明，MantisV2和Mantis+始终优于先前的时间序列基础模型，实现了最先进的零样本性能。

Conclusion: 通过合成数据预训练、架构优化和测试时增强方法，显著提升了时间序列基础模型的零样本特征提取能力，为通用时间序列特征提取器的发展提供了重要进展。

Abstract: Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.

</details>


### [95] [Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters](https://arxiv.org/abs/2602.17697)
*Nada Zine,Clément Quinton,Romain Rouvoy*

Main category: cs.LG

TL;DR: 该论文提出将大语言模型视为可配置系统，应用变异性管理技术来系统分析推理配置选择，以优化能耗和可持续性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理阶段的计算需求巨大，能耗问题突出，但配置空间庞大导致经验评估不可行，需要系统化的配置分析方法。

Method: 将LLMs视为可配置系统，使用基于特征的变异性模型表示生成超参数及其约束，采样代表性配置，测量能耗、延迟、准确性，并学习预测模型。

Result: 变异性建模能有效管理LLM推理配置复杂性，系统分析超参数效应和交互作用，揭示权衡关系，并能从有限测量中准确预测推理行为。

Conclusion: 这项工作通过变异性建模为LLM高效可持续配置开辟了新研究方向，桥接了软件工程和机器学习领域。

Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.

</details>


### [96] [Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888)
*Sayeed Shafayet Chowdhury,Karen D'Souza,V. Siva Kakumani,Snehasis Mukhopadhyay,Shiaofen Fang,Rodney J. Schlosser,Daniel M. Beswick,Jeremiah A. Alt,Jess C. Mace,Zachary M. Soler,Timothy L. Smith,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 该研究开发了机器学习模型，利用术前数据预测慢性鼻窦炎患者的手术获益，准确率达85%，在30例测试集上表现优于临床专家（80% vs 75.6%），展示了AI辅助临床决策的潜力。


<details>
  <summary>Details</summary>
Motivation: 慢性鼻窦炎（CRS）手术决策复杂，需权衡手术风险与不确定的个体化结果。虽然AI在医疗预后中应用广泛，但基于前瞻性观察性临床试验标准化数据的机器学习预测研究仍不足，这限制了AI在降低医疗成本和改善患者预后方面的潜力。

Method: 使用前瞻性收集的观察性干预试验队列数据，所有患者均接受了手术。研究训练监督学习模型仅使用术前数据预测手术获益，以Sino-Nasal Outcome Test-22（SNOT-22）作为主要患者报告结局指标。采用多种算法包括集成方法，并在30例混合难度病例的保留测试集上评估模型性能。

Result: 最佳模型达到约85%的分类准确率，能够准确且可解释地预测手术候选资格。在30例测试集上，模型准确率达80%，超过了临床专家平均预测准确率（75.6%），表明模型能够识别那些术前可能不建议手术的患者。

Conclusion: 机器学习模型能够有效预测慢性鼻窦炎患者的手术获益，其表现优于临床专家，展示了AI在增强临床决策支持和实现个性化CRS治疗方面的潜力，为减少不必要手术和优化患者管理提供了工具。

Abstract: Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

</details>


### [97] [Parallel Complex Diffusion for Scalable Time Series Generation](https://arxiv.org/abs/2602.17706)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Zhiqiang Ge,Qingsong Wen,Yong Liu*

Main category: cs.LG

TL;DR: PaCoDi是一种谱原生架构，通过频域解耦生成建模，解决时间序列生成中长程依赖的表示能力与计算效率权衡问题，实现50%注意力FLOPs减少且不损失信息。


<details>
  <summary>Details</summary>
Motivation: 传统时间扩散模型存在局部纠缠问题和注意力机制的O(L²)计算成本，难以有效建模时间序列中的长程依赖关系，需要在表示能力和计算效率之间做出权衡。

Method: 提出PaCoDi（并行复扩散）架构，利用傅里叶变换作为对角化算子将时间信号转换为解耦的谱分量；证明正交前向扩散和条件反向分解定理；采用平均场理论近似和交互校正机制；推广到连续时间频率SDE；利用实值信号的厄米对称性压缩序列长度；推导异方差损失处理压缩流形上的非各向同性噪声分布。

Result: PaCoDi在生成质量和推理速度方面均优于现有基线，实现了50%的注意力FLOPs减少而不损失信息，为时间序列建模提供了理论严谨且计算高效的解决方案。

Conclusion: PaCoDi通过频域解耦从根本上改变了问题拓扑结构，解决了时间序列生成中长程依赖建模的核心挑战，在理论严谨性和计算效率方面取得了显著进展。

Abstract: Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.

</details>


### [98] [MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance](https://arxiv.org/abs/2602.17930)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: MIRA通过结构化记忆图整合LLM先验知识，减少对实时LLM监督的依赖，在稀疏奖励环境中加速RL学习


<details>
  <summary>Details</summary>
Motivation: 传统RL在稀疏或延迟奖励环境中样本效率低，而过度依赖LLM监督存在可扩展性限制和信号可靠性问题

Method: 构建结构化记忆图存储高回报经验和LLM输出，从中推导效用信号软调整优势估计，随着训练进展效用项衰减

Result: MIRA在稀疏奖励环境中优于RL基线，达到与频繁LLM监督方法相当的回报，同时显著减少在线LLM查询

Conclusion: MIRA通过记忆图整合LLM先验知识，在减少LLM依赖的同时加速早期学习，保持标准收敛保证

Abstract: Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/

</details>


### [99] [Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2602.17931)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 论文提出了一种结合LLM和记忆图的方法来改善稀疏奖励环境中的强化学习样本效率，通过构建记忆图编码子目标和轨迹，并从中推导效用函数来指导优势函数，减少对频繁LLM调用的依赖。


<details>
  <summary>Details</summary>
Motivation: 在稀疏或延迟奖励环境中，强化学习需要大量交互导致样本复杂度高。虽然大语言模型可用于子目标发现和轨迹指导，但频繁依赖LLM调用存在可扩展性和可靠性问题。

Method: 构建记忆图编码来自LLM指导和智能体自身成功轨迹的子目标和轨迹，从中推导效用函数评估智能体轨迹与先前成功策略的匹配程度，该效用函数用于塑造优势函数，为critic提供额外指导而不改变奖励函数。

Result: 在基准环境中的初步实验显示，相比基线RL方法，该方法提高了样本效率并加速了早期学习，最终回报与需要频繁LLM交互的方法相当。

Conclusion: 该方法通过构建记忆图和推导效用函数，有效减少了强化学习对频繁LLM调用的依赖，在保持性能的同时提高了样本效率和早期学习速度，主要依赖离线输入和偶尔的在线查询，避免了持续LLM监督的需求。

Abstract: In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.

</details>


### [100] [Causal Neighbourhood Learning for Invariant Graph Representations](https://arxiv.org/abs/2602.17934)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong*

Main category: cs.LG

TL;DR: CNL-GNN：一种通过因果干预和图结构学习来减少虚假相关性的图神经网络框架，提高模型在分布变化下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 图数据中常存在噪声和虚假相关性，掩盖了真实的因果关系。传统GNN依赖虚假连接，难以在不同图间有效泛化，传统聚合方法会放大这些虚假模式，限制了模型在分布变化下的鲁棒性。

Method: 提出CNL-GNN框架，通过因果干预图结构来识别和保留因果相关连接，减少虚假影响。具体方法包括：生成反事实邻域、基于可学习重要性掩码和注意力机制的自适应边扰动，以及结合结构级干预与因果特征从混杂因素中解耦。

Result: 在四个公开数据集（包括一个数据集的多个领域变体）上的广泛实验表明，CNL-GNN优于最先进的GNN模型，能够学习到鲁棒且在不同图结构间泛化良好的不变节点表示。

Conclusion: CNL-GNN通过因果干预和图结构学习，超越了传统的基于特征的方法，提高了因果图学习能力，构建了鲁棒的分类模型，有效解决了图数据中虚假相关性导致的泛化问题。

Abstract: Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.

</details>


### [101] [ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867)
*João N. Cardoso,Arlindo L. Oliveira,Bruno Martins*

Main category: cs.LG

TL;DR: ADAPT是一种结合束搜索初始化和自适应梯度引导突变的混合方法，用于优化LLM激活空间中学习方向的离散文本特征可视化，在Gemma 2 2B的稀疏自编码器潜在空间上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 理解LLM激活空间中学习方向编码的特征需要识别强烈激活这些方向的输入。特征可视化通过优化输入来最大化激活目标方向，为昂贵的数据库搜索方法提供了替代方案，但由于文本的离散性，在LLM中仍未被充分探索。现有的提示优化技术不适合这个领域，容易陷入局部最小值。

Method: ADAPT是一种混合方法，结合了束搜索初始化和自适应梯度引导突变，专门针对这些失败模式设计。该方法在Gemma 2 2B的稀疏自编码器潜在空间上进行评估，提出了基于数据集激活统计的度量标准以实现严格比较。

Result: ADAPT在不同层和潜在类型上始终优于先前的方法。结果表明LLM的特征可视化是可行的，但需要针对该领域量身定制的设计假设。

Conclusion: LLM的特征可视化是可行的，但需要专门针对离散文本优化领域设计的算法。ADAPT通过结合束搜索和自适应梯度引导突变，克服了现有方法容易陷入局部最小值的问题，为理解LLM激活空间中的学习方向提供了有效的工具。

Abstract: Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

</details>


### [102] [Optimizing Graph Causal Classification Models: Estimating Causal Effects and Addressing Confounders](https://arxiv.org/abs/2602.17941)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong,Xin Wang*

Main category: cs.LG

TL;DR: CCAGNN是一个基于因果推理的图神经网络框架，通过考虑混杂因素来提升模型在真实世界中的鲁棒性和预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统图机器学习方法（如图神经网络）依赖相关性，对虚假模式和分布变化敏感，而因果模型能通过隔离真实因果因素实现更稳定的预测。因果学习还能识别和调整混杂因素，确保预测反映真实的因果关系。

Method: 提出CCAGNN框架，将因果推理融入图学习中，支持反事实推理，构建具有混杂因素感知能力的因果图神经网络。

Result: 在六个不同领域的公开数据集上进行综合实验，CCAGNN始终优于领先的先进模型。

Conclusion: CCAGNN框架通过整合因果推理，能够构建更鲁棒、因果感知的图学习模型，在真实世界场景中提供可靠的预测。

Abstract: Graph data is becoming increasingly prevalent due to the growing demand for relational insights in AI across various domains. Organizations regularly use graph data to solve complex problems involving relationships and connections. Causal learning is especially important in this context, since it helps to understand cause-effect relationships rather than mere associations. Since many real-world systems are inherently causal, graphs can efficiently model these systems. However, traditional graph machine learning methods including graph neural networks (GNNs), rely on correlations and are sensitive to spurious patterns and distribution changes. On the other hand, causal models enable robust predictions by isolating true causal factors, thus making them more stable under such shifts. Causal learning also helps in identifying and adjusting for confounders, ensuring that predictions reflect true causal relationships and remain accurate even under interventions. To address these challenges and build models that are robust and causally informed, we propose CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings. Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.

</details>


### [103] [Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs](https://arxiv.org/abs/2602.17778)
*Zachary Coalson,Bo Fang,Sanghyun Hong*

Main category: cs.LG

TL;DR: 论文发现对话大语言模型存在"轮次放大"故障模式，攻击者可系统利用澄清寻求行为延长多轮对话而不完成任务，这种攻击源于对话动态机制而非提示优化，能通过微调或参数破坏实现。


<details>
  <summary>Details</summary>
Motivation: 多轮交互长度是对话LLM运营成本的主要因素，需要识别新的故障模式来理解模型如何被恶意利用延长对话而不完成任务。

Method: 从机制角度识别与澄清寻求响应相关的查询无关通用激活子空间，通过供应链攻击（微调）和运行时攻击（低级参数破坏）诱导轮次放大，在多个指令调优LLM和基准测试中验证。

Result: 攻击能显著增加对话轮次同时保持合规性，现有防御措施对此类故障保护有限，攻击在不同提示和任务中持续存在。

Conclusion: 轮次放大是对话LLM中一种新的系统性故障模式，源于对话动态机制而非提示级行为，对模型安全和运营成本构成威胁，需要新的防御方法。

Abstract: Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.

</details>


### [104] [NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs](https://arxiv.org/abs/2602.18008)
*Zihan Guan,Rituparna Datta,Mengxuan Hu,Shunshun Liu,Aiying Zhang,Prasanna Balachandran,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 提出NIMM评估框架测试LLM生成机理模型在现实条件下的可靠性，并开发NIMMgen框架通过迭代优化提升模型代码正确性和实用性


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成机理模型的方法过于简化现实条件，不清楚在实际应用中是否可靠，需要更真实的评估框架

Method: 提出NIMM评估框架，在部分观测和多样化任务目标的现实条件下评估LLM生成模型；设计NIMMgen框架，通过迭代优化提升代码正确性和实用性

Result: 评估发现现有基线方法存在根本性挑战，从模型有效性到代码级正确性都有问题；NIMMgen在三个不同科学领域的数据集上表现优异，学习到的机理模型支持反事实干预模拟

Conclusion: LLM生成机理模型在现实条件下面临重大挑战，但通过NIMMgen框架的迭代优化可以显著提升可靠性和实用性

Abstract: Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.

</details>


### [105] [Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors](https://arxiv.org/abs/2602.17783)
*Xiangyu Sun,Shirin Hosseinmardi,Amin Yousefpour,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出基于物理信息高斯过程(PIGP)的拓扑优化框架，解决多材料、多物理场问题的计算成本高、谱偏差等挑战，能同时生成超分辨率拓扑和物理解释的材料分布。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在拓扑优化中存在高计算成本、谱偏差和处理复杂物理困难等问题，特别是在多材料、多物理场非自伴随问题中更为突出。

Method: 使用独立高斯过程先验表示主变量、伴随变量和设计变量，均值函数由神经网络参数化，通过最小化基于目标函数、多物理场势能泛函和设计约束的损失函数同时估计所有参数。

Result: 框架成功应用于合规度最小化、热传导优化、柔顺机构设计等基准问题，以及热机械拓扑优化等代表性多物理场问题，生成具有锐利界面和物理解释材料分布的超分辨率拓扑。

Conclusion: PIGP框架能有效同时解决耦合多物理场和设计问题，通过引入微分和积分方案显著加速训练过程，结果经开源代码和COMSOL验证。

Abstract: Machine learning (ML) has been increasingly used for topology optimization (TO). However, most existing ML-based approaches focus on simplified benchmark problems due to their high computational cost, spectral bias, and difficulty in handling complex physics. These limitations become more pronounced in multi-material, multi-physics problems whose objective or constraint functions are not self-adjoint. To address these challenges, we propose a framework based on physics-informed Gaussian processes (PIGPs). In our approach, the primary, adjoint, and design variables are represented by independent GP priors whose mean functions are parametrized via neural networks whose architectures are particularly beneficial for surrogate modeling of PDE solutions. We estimate all parameters of our model simultaneously by minimizing a loss that is based on the objective function, multi-physics potential energy functionals, and design-constraints. We demonstrate the capability of the proposed framework on benchmark TO problems such as compliance minimization, heat conduction optimization, and compliant mechanism design under single- and multi-material settings. Additionally, we leverage thermo-mechanical TO with single- and multi-material options as a representative multi-physics problem. We also introduce differentiation and integration schemes that dramatically accelerate the training process. Our results demonstrate that the proposed PIGP framework can effectively solve coupled multi-physics and design problems simultaneously -- generating super-resolution topologies with sharp interfaces and physically interpretable material distributions. We validate these results using open-source codes and the commercial software package COMSOL.

</details>


### [106] [Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards](https://arxiv.org/abs/2602.18037)
*Johannes Ackermann,Michael Noukhovitch,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出使用梯度正则化（GR）替代传统的KL惩罚来解决RLHF中的奖励黑客问题，通过将策略更新偏向奖励模型更准确的平坦区域来提高性能。


<details>
  <summary>Details</summary>
Motivation: 在语言模型的后训练中，RLHF和RLVR面临奖励黑客问题——策略可能利用奖励模型的不准确性学习到非预期行为。传统方法使用KL惩罚限制策略更新，但本文提出需要更直接地解决奖励模型准确性问题。

Method: 1. 理论推导奖励模型准确性与收敛点平坦度的关系；2. 使用梯度正则化（GR）将训练偏向平坦区域以保持奖励模型准确性；3. 提出高效的有限差分估计实现显式GR；4. 与KL惩罚进行对比实验。

Result: 1. 实证显示梯度范数与奖励准确性相关；2. KL惩罚的参考重置隐式使用GR寻找平坦区域；3. 显式GR在多种RL实验中优于KL惩罚：在RLHF中获得更高GPT评判胜率，避免过度关注基于规则的数学奖励格式，防止LLM-as-a-Judge数学任务中的评判黑客。

Conclusion: 梯度正则化是解决RLHF中奖励黑客问题的有效方法，通过将策略更新偏向奖励模型更准确的区域，比传统KL惩罚获得更好的性能，为语言模型后训练提供了新框架。

Abstract: Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.

</details>


### [107] [In-Context Learning for Pure Exploration in Continuous Spaces](https://arxiv.org/abs/2602.17976)
*Alessio Russo,Yin-Ching Lee,Ryan Welch,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出C-ICPE-TS算法，通过元训练深度神经网络策略，在连续空间中进行纯探索，无需参数更新即可推断真实假设。


<details>
  <summary>Details</summary>
Motivation: 传统纯探索问题通常假设离散假设空间，但许多现代应用场景（如连续臂赌博机、区域定位、函数最小化估计）的假设空间是连续的，且与查询/动作空间重合，需要新的方法处理连续空间的纯探索问题。

Method: 提出C-ICPE-TS算法：元训练深度神经网络策略，将观测历史映射到（1）下一个连续查询动作和（2）预测假设，直接从数据中学习可迁移的序列测试策略。推理时，算法主动收集证据并推断真实假设，无需参数更新或显式手工信息模型。

Result: 在多个基准测试中验证C-ICPE-TS，涵盖连续最佳臂识别、区域定位和函数最小化识别任务，展示了算法的有效性。

Conclusion: C-ICPE-TS为连续空间中的纯探索问题提供了一种有效的解决方案，能够学习可迁移的序列测试策略，并在未见任务上无需参数更新即可进行推理。

Abstract: In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $ε$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.

</details>


### [108] [Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds](https://arxiv.org/abs/2602.17798)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出GrMoE路由框架，在Grassmannian流形上使用Matrix Bingham分布控制路由稀疏性，通过单一可解释参数Λ连续调节路由熵，避免专家崩溃并实现后验稀疏度调整。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型的softmax门控机制缺乏控制稀疏性与利用率权衡的理论机制，且存在专家崩溃问题，需要更原则性的路由框架。

Method: 在Grassmannian子空间流形上构建路由框架，使用Matrix Bingham分布的浓度参数作为门控权重，开发摊销变分推理计算后验路由分布，建立浓度谱与路由熵的严格理论关系。

Result: 在350M-2.7B参数的MoE语言模型中实现0%路由崩溃，困惑度相当或更好，负载均衡提升15-30%，浓度与稀疏度呈现平滑单调关系，专家学习到与语言专业化相关的异质浓度值。

Conclusion: GrMoE提供了原则性的路由稀疏性控制机制，通过几何方法解决专家崩溃问题，实现可解释的路由行为和后验稀疏度调整，为MoE路由建立了首个形式化理论框架。

Abstract: Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $Λ$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\% routing collapse across all seeds, comparable or better perplexity with 15--30\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.

</details>


### [109] [Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory](https://arxiv.org/abs/2602.18297)
*Usman Anwar,Tim Bakker,Dana Kianfar,Cristina Pinneri,Christos Louizos*

Main category: cs.LG

TL;DR: 本文通过信息论分析发现思维链与输出间的非零互信息是思维链可监控的必要但不充分条件，提出了两种训练方法显著提升监控准确性并防止思维链退化。


<details>
  <summary>Details</summary>
Motivation: 思维链监控系统通过分析推理轨迹来检测输出是否具有特定属性（如代码生成中的测试黑客行为），但在实践中存在性能问题。需要理解思维链可监控性的理论基础，并开发改进监控性能的方法。

Method: 1) 信息论分析：证明思维链与输出间的非零互信息是可监控的必要但不充分条件；2) 识别两种近似误差源：信息差距和启发误差；3) 提出两种训练方法：基于oracle的方法直接奖励模型生成最大化监控准确性的思维链；无标签方法最大化输出与思维链间的条件互信息。

Result: 在多种不同环境中，两种方法都显著提高了监控准确性，同时防止了思维链退化，即使在与监控器对抗训练时也能缓解奖励黑客问题。

Conclusion: 思维链可监控性可以通过有针对性的训练目标系统性地改进。提出的两种方法有效提升了监控性能，为解决任务奖励不完美指定时的奖励黑客问题提供了实用方案。

Abstract: Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.

</details>


### [110] [Learning Optimal and Sample-Efficient Decision Policies with Guarantees](https://arxiv.org/abs/2602.17978)
*Daqian Shao*

Main category: cs.LG

TL;DR: 该论文提出了一种从存在隐藏混杂因素的离线数据集中学习决策策略的方法，使用工具变量解决因果效应识别问题，并扩展到模仿学习和LTL目标学习，具有收敛和最优性保证。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要大量在线环境交互，这在成本高、危险或不可行的实际应用中存在问题。离线数据集学习面临隐藏混杂因素的挑战，这些混杂因素可能导致虚假相关性和次优决策。

Method: 1. 使用工具变量识别因果效应，作为条件矩限制问题；2. 受双机器学习启发，开发具有收敛和最优性保证的样本高效算法；3. 将方法扩展到模仿学习场景；4. 开发学习线性时序逻辑表达的高层目标的算法。

Result: 提出的算法在强化学习基准测试、合成和半合成数据集上表现出色，优于现有最先进方法，提高了样本效率，并在实际决策中证明了其有效性。

Conclusion: 该论文为存在隐藏混杂因素的离线决策学习提供了理论保证和实用算法，解决了高风险应用中强化学习的实际挑战，推动了因果强化学习领域的发展。

Abstract: The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>


### [111] [Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.17809)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: SBA是一种贝叶斯参数高效微调框架，在Stiefel流形上使用Matrix Langevin先验，通过切空间拉普拉斯近似进行后验推断，提供校准的不确定性估计，在领域转移下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调方法（如LoRA）缺乏原则性的不确定性估计，导致预测校准不佳且在领域转移下行为不可靠。需要一种能够提供校准预测不确定性的贝叶斯PEFT框架。

Method: 在Stiefel流形上放置Matrix Langevin先验于正交适配器因子，通过切空间拉普拉斯近似与测地线回缩进行近似后验推断。相比平坦空间的高斯先验投影到正交约束，该方法在流形上自然编码适配器子空间应良好条件且正交的归纳偏置。

Result: 在多个模型（RoBERTa-large、LLaMA-2-7B/13B、Mistral-7B、Qwen2.5-7B）和任务上，SBA达到与LoRA/DoRA相当的任务性能，同时将预期校准误差降低18-34%，在领域转移下选择性预测AUROC提高12-25%，以远低于五个LoRA模型集成参数成本在OOD检测上表现更优。

Conclusion: 在正确的几何结构上放置不确定性比简单为适配器添加任何贝叶斯处理更重要。SBA提供了校准的预测不确定性而无需重新校准，在流形上进行推理具有严格的理论优势。

Abstract: Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.

</details>


### [112] [On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction](https://arxiv.org/abs/2602.18301)
*Ivan Bondarenko,Egor Palkin,Fedor Tikunov*

Main category: cs.LG

TL;DR: 研究发现LLM中的两个原型令牌（m-token和e-token）在单次前向传播中编码不同信息：m-token主要捕获语义信息，e-token更关注句法结构；通过关系蒸馏可以在不牺牲重构质量的情况下将语义关系转移到原型令牌空间。


<details>
  <summary>Details</summary>
Motivation: 探索LLM中用于单步文本重构的原型令牌（proto-tokens）编码的信息类型和行为特性，为超越自回归范式提供理论基础，支持未来非自回归序列到序列系统的发展。

Method: 通过一系列实验分析两个原型令牌的语义和句法内容分离，研究e-token的稳定性特性，可视化重构过程中的注意力模式，并测试两种使用教师嵌入对e-token施加语义结构的正则化方案：基于锚点的损失和关系蒸馏目标。

Result: 标准优化下m-token比e-token更强烈地捕获语义信息；基于锚点的约束会显著牺牲重构精度；关系蒸馏可以在不损失重构质量的情况下将批次级语义关系转移到原型令牌空间。

Conclusion: 原型令牌作为中间表示具有可行性，关系蒸馏方法为开发非自回归seq2seq系统提供了有前景的路径，这些系统可以预测原型令牌作为中间表示。

Abstract: Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for "imposing" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.

</details>


### [113] [Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers](https://arxiv.org/abs/2602.17993)
*Mohan Tang,Sidi Lu*

Main category: cs.LG

TL;DR: TurboConn是一种新型Transformer架构，通过将高层隐藏状态路由到后续token的低层，突破固定计算深度限制，显著提升LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过多步骤推理解决复杂问题，但现有Transformer的计算深度受限于固定层数，限制了其多步推理能力。需要突破这一限制来提升LLMs在数学、逻辑和规划等任务上的表现。

Method: 提出TurboConn架构，将每个token的高层隐藏状态通过多个残差连接路由到下一个token的低层。这种密集的向后连接机制允许信息在时间维度上跨层流动，形成更深的计算路径。

Result: 在GSM8K、Parity和多步算术等基准测试上获得0.9%到超过10%的准确率提升。特别地，在Parity任务上，Qwen-3-1.7B从53.78%提升到100%准确率。密集连接设计显著优于稀疏替代方案。

Conclusion: 计算路径深度是影响推理能力的关键因素。TurboConn提供了一种无需从头训练或复杂课程学习即可增强预训练LLMs的新机制，且不影响生成延迟。

Abstract: Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward connections is critical; our dense interaction significantly outperforms "sparse" alternatives that only pass a single hidden state or vector. Notably, TurboConn can be integrated into pre-trained LLMs to overcome task-specific plateaus: while a fine-tuned Qwen-3-1.7B achieves only 53.78% on Parity, adding our architectural modification enables the model to reach 100% accuracy, all without the necessity to retrain the full model from scratch or sophisticated curriculum learning. Our results provide strong empirical evidence that the depth of the computational path is a key factor in reasoning ability, also offering a new mechanism to enhance LLMs without significantly affecting generation latency.

</details>


### [114] [On the "Induction Bias" in Sequence Models](https://arxiv.org/abs/2602.18333)
*M. Reza Ebrahimi,Michaël Defferrard,Sunny Panchal,Roland Memisevic*

Main category: cs.LG

TL;DR: Transformer模型在状态追踪任务上存在根本性挑战，即使在训练和测试分布匹配的情况下，其数据效率远低于RNN，且无法有效跨序列长度共享学习到的机制。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在语言模型实践中取得了显著成功，但近期研究表明其在状态追踪能力上存在局限。现有研究主要关注其在分布外泛化（如长度外推）的失败，本文转向研究这些局限在分布内的影响，探究Transformer在状态追踪任务上的根本能力缺陷。

Method: 进行大规模实验研究，比较Transformer和循环神经网络(RNN)在多种监督机制下的数据效率。分析所需训练数据量随状态空间大小和序列长度的增长情况。进一步研究学习到的状态追踪机制在不同序列长度间的共享程度，通过权重共享分析来评估模型是否学习到通用的解决方案。

Result: Transformer所需训练数据量随状态空间大小和序列长度快速增长，远高于RNN。Transformer在不同序列长度间表现出可忽略甚至有害的权重共享，表明其学习的是长度特定的孤立解决方案。相反，循环模型通过跨长度共享权重实现有效的摊销学习，允许一个序列长度的数据提升其他长度的性能。

Conclusion: 状态追踪仍然是Transformer的根本挑战，即使训练和评估分布匹配。Transformer缺乏有效的跨长度权重共享机制，导致数据效率低下，而RNN通过摊销学习展现出更好的泛化能力。这表明Transformer在状态追踪任务上的局限性不仅是分布外泛化问题，而是其架构的根本特性。

Abstract: Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.

</details>


### [115] [Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models](https://arxiv.org/abs/2602.17829)
*Preetom Biswas,Giulia Pedrielli,K. Selçuk Candan*

Main category: cs.LG

TL;DR: ruleXplain是一个利用大语言模型从仿真驱动的动力系统中提取形式化解释的框架，通过约束符号规则语言和延迟语义生成可验证的因果规则。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理具有延迟效应的时序数据因果关系时，往往无法提供泛化性强且可解释的解释，因为多个不同的输入轨迹可能产生几乎无法区分的输出。

Method: 提出ruleXplain框架，利用LLMs从仿真器生成的多元输入时序数据中提取形式化解释。框架包含约束符号规则语言、反事实输入生成、聚类和LLM结构化提示，以及闭环精炼过程确保规则一致性。

Result: 在PySIRTEM流行病模拟器和EnergyPlus建筑能源模拟器上验证了框架有效性，通过输入重建、因果编码消融研究和泛化测试三类实验证明了规则集的有效性。

Conclusion: ruleXplain能够从复杂动力系统中提取可验证的因果规则，为时序数据的因果关系解释提供了有效框架，特别适用于具有延迟效应的仿真驱动系统。

Abstract: Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.

</details>


### [116] [Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures](https://arxiv.org/abs/2602.18417)
*Joshua Nunley*

Main category: cs.LG

TL;DR: 提出一个在U(d)闭子群上构建序列模型的直接框架，通过最小公理设置从共享骨架推导循环和Transformer模板，并在O(d)上实验验证


<details>
  <summary>Details</summary>
Motivation: 为序列模型提供统一的数学框架，将隐藏状态限制在U(d)的闭子群上，通过子群选择作为状态空间、切空间投影和更新映射的即插即用组件

Method: 1) 建立最小公理设置，从共享骨架推导循环和Transformer模板；2) 专门化到O(d)正交群；3) 提出切空间中的线性混合扩展；4) 在Tiny Shakespeare和Penn Treebank数据集上评估正交状态RNN和Transformer模型

Result: 在参数匹配设置下，正交状态模型在语言建模任务上表现良好；切空间线性混合扩展在有限参数预算下提升了O(d)实验的性能

Conclusion: 该框架为序列模型提供了统一的数学基础，子群选择作为灵活组件，切空间线性混合扩展能提升性能，为未来研究提供了可扩展的方向

Abstract: This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.

</details>


### [117] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: 提出MePoly——基于多项式能量模型的新策略参数化方法，提供显式可处理的概率密度，解决扩散策略缺乏显式密度的问题，在多模态决策任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随机最优控制为复杂决策问题提供统一框架，但传统参数化策略难以表示解的多模态性。扩散策略虽能恢复多模态，但缺乏显式概率密度，使策略梯度优化复杂化。

Method: 基于多项式能量模型的策略参数化方法MePoly，利用经典矩问题的理论基础，通过多项式能量函数提供显式可处理的概率密度，实现精确熵最大化。

Result: MePoly能有效捕捉复杂非凸流形，在多样化基准测试中性能优于基线方法，展示了在多模态决策任务中的优越表现。

Conclusion: MePoly通过提供显式可处理的概率密度，解决了扩散策略的局限性，为多模态决策问题提供了有效的策略参数化方法，在理论和实证上都表现出色。

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [118] [Flow Actor-Critic for Offline Reinforcement Learning](https://arxiv.org/abs/2602.18015)
*Jongseong Chae,Jongeui Park,Yongjae Shin,Gyeongmin Kim,Seungyul Han,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出Flow Actor-Critic方法，通过流模型同时改进actor和critic，解决离线RL中多模态数据分布问题，在D4RL和OGBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习的数据集分布通常呈现复杂多模态特性，需要比高斯策略更具表达能力的策略来捕捉这种分布。

Method: 提出Flow Actor-Critic方法：1) 使用流模型作为actor策略；2) 利用流模型进行保守critic获取，防止在数据外区域的Q值爆炸；3) 提出基于流行为代理模型的新critic正则化器，该模型是流actor设计的副产品。

Result: 在D4RL和OGBench等离线RL测试数据集上取得了新的最先进性能。

Conclusion: 通过联合利用流模型改进actor和critic，能够有效处理复杂多模态数据集，提升离线RL性能。

Abstract: The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.

</details>


### [119] [Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning](https://arxiv.org/abs/2602.17835)
*Sirui Chen,Yunzhe Qi,Mengting Ai,Yifan Sun,Ruizhong Qiu,Jiaru Zou,Jingrui He*

Main category: cs.LG

TL;DR: Iprox提出两阶段框架，通过低秩压缩和对齐梯度与logits，从目标大模型中构建保持影响力的小型代理模型，使基于梯度的数据选择方法能更高效地应用于大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的数据选择方法（如TracIn和Influence Functions）计算成本高，难以应用于数十亿参数的大语言模型。使用现成小模型作为代理效果不佳，因为其学习动态不明确、尺寸无法灵活调整、且无法与目标模型在梯度影响力估计上对齐。

Method: Iprox采用两阶段框架：1）低秩压缩阶段，通过压缩目标模型来保留影响力信息；2）对齐阶段，同时对齐模型梯度和logits，构建能灵活控制计算成本且保持目标模型影响力的代理模型。

Result: 实验表明Iprox在多种LLM家族和评估任务中均优于现成代理和基线方法。在Qwen3-4B上，1.5B的Iprox代理比1.7B现成代理表现更好。在Llama3.2上，Iprox在减少一半以上计算成本的同时仍优于基线方法。

Conclusion: Iprox提供了有效的保持影响力的代理模型，使基于梯度的数据选择方法能更好地扩展到大型语言模型中，解决了现有方法计算成本高和代理效果不佳的问题。

Abstract: Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model's influence. Experimental results across diverse LLM families and evaluation tasks show that Iprox consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with Iprox achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, Iprox achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that Iprox provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.

</details>


### [120] [Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models](https://arxiv.org/abs/2602.17846)
*Nick Dodson,Xinyu Gao,Qingsong Wang,Yusu Wang,Zhengchao Wan*

Main category: cs.LG

TL;DR: 扩散模型存在记忆训练数据的隐私风险，本文提出几何框架将噪声调度分为三个区域，揭示记忆风险在不同噪声水平下高度不均匀，并识别出中噪声区域是记忆风险最高的"危险区"。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能生成高质量样本，但也会记忆训练数据，引发严重隐私问题。目前尚不清楚记忆与泛化的机制：记忆在噪声调度的哪个阶段产生、数据几何如何影响记忆、不同噪声尺度现象如何相互作用。

Method: 引入几何框架，基于高斯壳覆盖训练数据的特性和后验集中行为这两个决定扩散模型记忆与泛化的基本对象，将噪声调度划分为三个区域。针对中噪声区域，提出几何条件并通过几何感知的针对性干预来缓解记忆问题。

Result: 记忆风险在噪声水平上高度不均匀：中噪声区域是记忆最显著的"危险区"，而小噪声和大噪声区域都抵抗记忆，但机制不同：小噪声因训练覆盖有限而避免记忆，大噪声因后验集中度低且具有可证明的近线性高斯去噪行为。

Conclusion: 通过几何框架揭示了扩散模型记忆与泛化的机制，识别出记忆风险最高的中噪声区域，并提出几何感知的干预方法来缓解记忆问题，为理解和管理扩散模型的隐私风险提供了新视角。

Abstract: Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.

</details>


### [121] [Dual Length Codes for Lossless Compression of BFloat16](https://arxiv.org/abs/2602.17849)
*Aditya Agrawal,Albert Magyar,Hiteshwar Eswaraiah,Patrick Sheridan,Pradeep Janedula,Ravi Krishnan Venkatesan,Krishna Nair,Ravi Iyer*

Main category: cs.LG

TL;DR: 本文提出Dual Length Codes，一种混合编码方法，在压缩效率和解码速度之间取得平衡，针对LLM训练中的网络带宽瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: LLM训练和服务严重依赖并行化和集体操作，常受网络带宽限制。现有无损压缩方法如Huffman编码解码慢且硬件复杂，而通用编码如Exponential-Golomb编码虽快但不利用符号频率分布。

Method: 提出Dual Length Codes混合方法：分析Gemma模型的BFloat16张量，发现前8个最频繁符号占约50%概率，分配4位短码；其余248个符号分配9位长码。使用单个前缀位区分两种码长，仅需8个条目的查找表进行编解码。

Result: 与Huffman编码的21.3%压缩率相比，该方法达到18.6%压缩率，但显著加快了解码速度并简化了硬件复杂度。

Conclusion: Dual Length Codes在压缩效率和解码速度之间取得了良好平衡，解决了LLM训练中网络带宽瓶颈问题，相比传统方法具有更好的硬件友好性和解码性能。

Abstract: Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Dual Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. Analyzing BFloat16 tensors from the Gemma model, we observed that the top 8 most frequent symbols account for approximately 50% of the cumulative probability. These 8 symbols are assigned a short 4 bit code. The remaining 248 symbols are assigned a longer 9 bit code. The coding scheme uses a single prefix bit to distinguish between the two code lengths. The scheme uses a small Look Up Table with only 8 entries for encoding and decoding. The scheme achieves a compressibility of 18.6% in comparison to 21.3% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.

</details>


### [122] [Neural Prior Estimation: Learning Class Priors from Latent Representations](https://arxiv.org/abs/2602.17853)
*Masoud Yavari,Payman Moallem*

Main category: cs.LG

TL;DR: 提出NPE框架，通过特征条件化的对数先验估计解决类别不平衡问题，在神经坍缩理论下可恢复类别对数先验，实验证明在长尾数据集上有效提升少数类性能


<details>
  <summary>Details</summary>
Motivation: 类别不平衡会导致深度神经网络产生系统性偏差，传统方法需要显式的类别统计或分布特定的超参数，缺乏理论依据和自适应能力

Method: 提出神经先验估计器(NPE)，从潜在表示中学习特征条件化的对数先验估计，使用一个或多个先验估计模块通过单向逻辑损失与主干网络联合训练，形成NPE-LA进行偏差感知预测

Result: 在长尾CIFAR和语义分割基准测试(STARE, ADE20K)上实验，一致提升性能，特别是对少数类别的表现，证明了方法的有效性

Conclusion: NPE提供了一种轻量级且理论合理的先验估计方法，可用于不平衡感知预测，在神经坍缩理论下具有理论保证，无需显式类别统计或分布特定超参数

Abstract: Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.

</details>


### [123] [Cut Less, Fold More: Model Compression through the Lens of Projection Geometry](https://arxiv.org/abs/2602.18116)
*Olga Saukh,Dong Wang,Haris Šikić,Yun Cheng,Lothar Thiele*

Main category: cs.LG

TL;DR: 论文提出了一种无需重新训练的神经网络压缩方法"模型折叠"，通过权重聚类实现低秩投影，在理论上和实践上都优于传统的结构化剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 大规模部署神经网络需要无需重新训练的压缩方法。传统结构化剪枝存在局限性，需要探索更优的几何感知压缩方法。

Method: 从投影几何角度分析压缩：结构化剪枝是轴对齐投影，模型折叠通过权重聚类实现低秩投影。将两者形式化为正交算子，证明在秩距离为1时，折叠具有更小的参数重构误差和功能扰动。

Result: 在1000多个检查点上评估，涵盖ResNet18、ViT-B/32、CLIP ViT-B/32和LLaMA模型。折叠通常在压缩后获得更高准确率，在中等至高压缩率下优势最大，在某些特定训练设置下差距会缩小甚至反转。

Conclusion: 模型折叠是一种几何感知、无需校准的剪枝替代方案，在实践中通常更优且在理论上更有原则性。

Abstract: Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.

</details>


### [124] [JAX-Privacy: A library for differentially private machine learning](https://arxiv.org/abs/2602.17861)
*Ryan McKenna,Galen Andrew,Borja Balle,Vadym Doroshenko,Arun Ganesh,Weiwei Kong,Alex Kurakin,Brendan McMahan,Mikhail Pravilov*

Main category: cs.LG

TL;DR: JAX-Privacy是一个用于简化差分隐私机器学习机制部署的库，提供模块化原语，支持深度定制和开箱即用体验


<details>
  <summary>Details</summary>
Motivation: 简化差分隐私机器学习的部署，为研究者和实践者提供统一工具，整合最新研究成果，解决现有工具在可用性、灵活性和效率方面的不足

Method: 基于JAX构建，提供经过验证的模块化原语，涵盖批量选择、梯度裁剪、噪声添加、会计和审计等关键组件，支持深度定制和开箱即用两种使用模式

Result: 创建了一个统一的差分隐私机器学习库，整合了大量最新研究成果，为研究者和实践者提供了强大而灵活的工具

Conclusion: JAX-Privacy通过设计原则指导，成功构建了一个既支持深度定制又提供开箱即用体验的差分隐私机器学习库，推动了该领域的研究和应用

Abstract: JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.

</details>


### [125] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: FINO提出了一种基于流匹配的离线到在线强化学习方法，通过注入噪声增强探索，结合熵引导采样平衡探索与利用，在有限在线预算下实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在离线RL中表现出色，但在扩展到在线微调时面临挑战。现有方法将在线微调视为离线预训练的直接延续，未能解决关键问题，特别是如何有效探索离线数据集之外的动作空间。

Method: FINO方法：1) 基于流匹配的策略，在策略训练中注入噪声以鼓励探索离线数据集之外的动作；2) 结合熵引导采样机制，在在线微调过程中动态平衡探索与利用。

Result: 在多样化的挑战性任务实验中，FINO在有限的在线预算下始终实现优越性能，证明了其增强探索和样本效率的有效性。

Conclusion: FINO通过噪声注入和熵引导采样，成功解决了离线到在线RL中的探索挑战，为生成模型在在线强化学习中的应用提供了有效方案。

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [126] [Capabilities Ain't All You Need: Measuring Propensities in AI](https://arxiv.org/abs/2602.18182)
*Daniel Romero-Alvarado,Fernando Martínez-Plumed,Lorenzo Pacchiardi,Hugo Save,Siddhesh Milind Pawar,Behzad Mehrbakhsh,Pablo Antonio Moreno Casares,Ben Slater,Paolo Bova,Peter Romero,Zachary R. Tyler,Jonathan Prunty,Luning Sun,Jose Hernandez-Orallo*

Main category: cs.LG

TL;DR: 提出首个测量AI倾向性的形式化框架，使用双逻辑函数描述模型在"理想区间"内的高成功率，结合倾向性与能力评估可更准确预测AI行为


<details>
  <summary>Details</summary>
Motivation: 传统AI评估主要关注能力测量，但倾向性（模型展现特定行为的趋势）对性能和安全性有重要影响。传统IRT方法不适合测量倾向性，因为倾向性过高或过低都可能有问题

Method: 引入双逻辑函数框架，将模型成功概率建模为当倾向性处于"理想区间"内时较高；使用配备新开发的任务无关评分标准的LLM来估计理想区间的界限；应用于六个LLM模型家族

Result: 能够测量倾向性偏移程度及其对任务的影响；基于一个基准估计的倾向性可成功预测保留任务的行为；结合倾向性和能力评估比单独使用任一方法具有更强的预测能力

Conclusion: 展示了如何进行严格的倾向性测量，证明结合倾向性和能力评估比仅使用能力评估能更准确预测AI行为，为AI评估提供了更全面的框架

Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.

</details>


### [127] [COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models](https://arxiv.org/abs/2602.17893)
*Jiajun Shen,Yufei Jin,Yi He,xingquan Zhu*

Main category: cs.LG

TL;DR: COMBA：基于状态空间模型的大图学习方法，通过图上下文门控和跨批次聚合解决图到序列转换的挑战


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）在序列数据中表现出色，但将其应用于图结构数据，特别是大图时面临挑战，因为SSMs是序列模型，而将图转换为序列进行有效学习成本很高

Method: 提出COMBA框架，包含两个关键创新：1) 图上下文门控 - 利用不同跳数的邻居上下文学习最佳邻居聚合控制；2) 跨批次聚合 - 对每个图上下文采样节点批次，训练GNN并在批次间聚合信息，实现大图扩展

Result: 理论分析表明跨批次聚合比无聚合的GNN训练具有更低的误差；在基准网络上的实验显示相比基线方法有显著性能提升

Conclusion: COMBA成功将状态空间模型应用于大图学习，通过图上下文门控和跨批次聚合解决了图到序列转换的挑战，为大规模图学习提供了有效解决方案

Abstract: State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.

</details>


### [128] [LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification](https://arxiv.org/abs/2602.18195)
*Hairong Chen,Yicheng Feng,Ziyu Jia,Samir Bhatt,Hengguan Huang*

Main category: cs.LG

TL;DR: LERD：一种端到端贝叶斯电生理神经动力学系统，直接从多通道EEG推断潜在神经事件及其关系结构，用于阿尔茨海默病诊断。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病改变脑电生理并破坏多通道EEG动力学，现有方法多依赖黑盒分类器，未能显式建模生成观测信号的基础动力学。

Method: 提出LERD系统，结合连续时间事件推断模块和随机事件生成过程，采用电生理启发的动力学先验指导学习，无需事件或交互标注。

Result: 在合成基准和两个真实AD EEG队列上，LERD始终优于强基线，产生与生理对齐的潜在摘要，有助于表征群体水平动力学差异。

Conclusion: LERD为EEG动力学建模提供了一种原则性方法，能够从多通道EEG中推断神经事件和关系结构，在AD诊断中表现出优越性能。

Abstract: Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.

</details>


### [129] [Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors](https://arxiv.org/abs/2602.17898)
*Jingquan Yan,Yuwei Miao,Peiran Yu,Junzhou Huang*

Main category: cs.LG

TL;DR: 论文分析了注意力回归模型中PCC停滞现象，揭示了MSE优化与PCC梯度冲突、软注意力机制限制以及凸聚合器PCC改进上限，并提出ECA方法突破PCC平台期。


<details>
  <summary>Details</summary>
Motivation: 注意力回归模型训练中常出现PCC停滞现象：即使MSE持续下降，PCC早期就停止改进。这种现象缺乏理论解释，阻碍了模型在保持误差幅度同时改善预测形状的能力。

Method: 首先理论分析PCC停滞的两个根本原因：1) MSE优化与PCC梯度冲突，软注意力机制在数据同质时加剧此问题；2) 凸聚合器（包括软注意力）的PCC改进存在理论上限。基于此提出ECA方法，包含新机制改善PCC优化并超越凸包限制。

Result: 在多样化基准测试中，ECA能持续突破PCC平台期，在具有挑战性的同质数据设置下显著提升相关性，同时不损害MSE性能。

Conclusion: PCC停滞现象源于优化动态和模型容量的根本限制。提出的ECA方法通过理论驱动的机制成功解决了这些问题，为注意力回归模型提供了同时优化误差幅度和预测形状的有效方案。

Abstract: Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.

</details>


### [130] [[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games](https://arxiv.org/abs/2602.18230)
*Jorge Carrasco Pollo,Ioannis Kapetangeorgis,Joshua Rosenthal,John Hua Yao*

Main category: cs.LG

TL;DR: 该研究复现了Abdelnabi等人(2024)提出的基于可评分游戏的LLM多智能体谈判基准，发现虽然基准复杂，但模型比较存在模糊性，实验设置存在信息泄露检测和消融研究不足等局限性。


<details>
  <summary>Details</summary>
Motivation: LLM在多智能体谈判任务中展现出巨大潜力，但该领域缺乏稳健且可泛化的评估基准。Abdelnabi等人(2024)提出了基于可评分游戏的谈判基准，本研究旨在验证该基准声明的可复现性，并深入理解其可用性和泛化能力。

Method: 1. 在更多模型上复现原始实验；2. 引入额外指标验证谈判质量和评估公平性；3. 在扩展版基准上检验更广泛模型的行为；4. 分析实验设置的局限性，特别是信息泄露检测和消融研究的完整性。

Result: 1. 基准确实复杂，但模型比较存在模糊性，对其客观性提出质疑；2. 发现实验设置存在局限性，包括信息泄露检测不足和消融研究不彻底；3. 通过扩展实验揭示了为潜在用户提供额外背景的见解；4. 强调了模型比较评估中上下文的重要性。

Conclusion: 该研究揭示了Abdelnabi等人(2024)谈判基准在模型比较客观性方面的局限性，强调了评估基准需要更严谨的实验设计和更全面的分析，为未来谈判基准的开发提供了重要参考。

Abstract: Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.

</details>


### [131] [Distribution-Free Sequential Prediction with Abstentions](https://arxiv.org/abs/2602.17918)
*Jialin Yu,Moïse Blanchard*

Main category: cs.LG

TL;DR: 研究一种半对抗性序列预测问题，学习者可以在实例被污染时弃权而不受惩罚，在分布未知的情况下实现VC类的可学习性。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设学习者已知干净样本的分布μ，这在理论和实践中都是强假设。本文旨在探索在分布未知的情况下，是否仍能实现类似的学习保证，这更符合经典学习框架（如PAC学习）的标准。

Method: 提出基于弱学习器提升过程的算法AbstainBoost，通过提升弱学习器来保证在分布未知的情况下，对于一般VC类在对抗性环境中实现次线性误差。

Result: 算法在分布未知的情况下，对于一般VC类在非适应性对抗者环境中实现了次线性误差保证，对于适应性对抗者，在线性分类器等结构化函数类中也实现了类似保证。同时提供了相应的下界，揭示了误分类错误与错误弃权次数之间的多项式权衡。

Conclusion: 在分布未知的半对抗性学习框架中，通过提升算法可以实现VC类的可学习性，填补了已知分布假设与完全对抗性学习之间的空白，为实际应用提供了更实用的解决方案。

Abstract: We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $μ$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $μ$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.

</details>


### [132] [Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers](https://arxiv.org/abs/2602.18292)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 该论文提出将解码视为一个原则性的优化层，通过正则化问题统一现有解码方法，并基于此框架设计了新的Best-of-K解码器来提升多样本管道的性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的解码过程仍被视为启发式的参数调整过程，缺乏理论框架。作者认为解码应该被理解为原则性的优化层，能够统一解释现有解码方法并便于设计新方法。

Method: 提出一个正则化优化框架：在每个token位置，在概率单纯形上解决一个正则化问题，平衡模型得分与结构偏好和约束。该框架统一了贪婪解码、Softmax采样、Top-K、Top-P和Sparsemax等现有方法。基于此框架设计了Best-of-K解码器，使用KL散度锚定的覆盖目标来优化多样本管道。

Result: 提出的Best-of-K解码器在固定K样本预算内提高了覆盖良好替代方案的概率，显著提升了实证性能。例如，在Qwen2.5-Math-7B模型上，在高温采样下将MATH500的准确率提高了+18.6%。

Conclusion: 解码应该被重新理解为原则性的优化层，而不是启发式参数调整。提出的统一框架不仅解释了现有解码方法，还便于设计新的解码器，如Best-of-K，能够显著提升多样本管道的性能。

Abstract: Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.

</details>


### [133] [Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere](https://arxiv.org/abs/2602.17940)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: 本文研究了高斯过程（GP）bandit问题在平方指数（SE）核下的算法无关最坏情况下界，针对超球面输入域，部分解决了维度依赖对数因子的开放问题。


<details>
  <summary>Details</summary>
Motivation: GP bandit问题中，对于广泛使用的平方指数（SE）核，上界和下界之间在维度依赖的对数因子方面存在差距，这是一个尚未解决的开放问题。本文旨在部分解决这个问题，特别是在超球面输入域的情况下。

Method: 采用算法无关的最坏情况分析方法，针对超球面输入域，推导累积遗憾和简单遗憾的下界。同时改进了SE核的最大信息增益上界。

Result: 1. 累积遗憾下界：Ω(√(T(ln T)^d(ln ln T)^{-d}))
2. 简单遗憾下界：找到ε最优点需要Ω(ε^{-2}(ln 1/ε)^d(ln ln 1/ε)^{-d})时间步
3. 最大信息增益上界：O((ln T)^{d+1}(ln ln T)^{-d})

Conclusion: 在超球面输入域下，本文结果保证了现有最佳算法在维度无关对数因子范围内的最优性，部分解决了GP bandit问题中维度依赖对数因子的开放问题。

Abstract: We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $Ω(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $Ω(ε^{-2}(\ln \frac{1}ε)^d (\ln \ln \frac{1}ε)^{-d})$ time steps to find an $ε$-optimal point. We also provide the improved $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \emph{dimension-independent} logarithmic factors under a hyperspherical input domain.

</details>


### [134] [JPmHC Dynamical Isometry via Orthogonal Hyper-Connections](https://arxiv.org/abs/2602.18308)
*Biswa Sengupta,Jinhua Wang,Leo Brunswic*

Main category: cs.LG

TL;DR: JPmHC提出了一种保持雅可比谱的流形约束超连接框架，通过可训练的线性混合器替代恒等跳跃连接，在提升性能的同时保持训练稳定性，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 超连接等深度学习方法扩展了残差连接范式，但破坏了恒等映射特性，导致训练不稳定、可扩展性有限和内存开销增加。需要解决这些挑战。

Method: 提出JPmHC框架：1) 用可训练线性混合器替代恒等跳跃连接；2) 将混合器约束在算子范数有界流形上；3) 使用自由概率分析预测雅可比谱；4) 采用内存高效的隐式微分；5) 通过Cayley变换实现Stiefel约束混合器。

Result: 在ARC-AGI基准测试中，JPmHC相比双随机基线实现了更快的收敛速度、更高的准确率和更低的计算成本。

Conclusion: JPmHC作为一种灵活可扩展的超连接扩展，推进了谱感知、稳定且高效的深度学习，为拓扑架构设计和基础模型演进提供了新见解。

Abstract: Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.

</details>


### [135] [Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition](https://arxiv.org/abs/2602.17947)
*Yubo Zhou,Jun Shu,Junmin Liu,Deyu Meng*

Main category: cs.LG

TL;DR: 该论文分析了超参数优化中梯度估计的偏差-方差分解，提出了减少方差的集成策略，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的超参数优化方法主要关注估计偏差，而忽略了数据分布带来的方差误差，这影响了性能表现。作者旨在解决超梯度估计中的方差问题。

Method: 1) 对超梯度估计误差进行偏差-方差分解；2) 提供超梯度估计误差界的全面分析；3) 提出减少方差的集成超梯度策略；4) 建立超梯度估计与超额误差之间的联系。

Result: 在正则化超参数学习、数据超清洗和少样本学习等任务上的实验结果表明，方差减少策略改善了超梯度估计性能，为实践中观察到的现象（如验证集过拟合）提供了理论解释。

Conclusion: 该研究通过偏差-方差分解深入分析了超梯度估计误差，提出的集成策略有效减少了方差，为超参数优化提供了更可靠的理论基础和实践指导。

Abstract: Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.

</details>


### [136] [A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion](https://arxiv.org/abs/2602.17948)
*Yu Bai,Zhe Wang,Jiarui Zhang,Dong-Xiao Zhang,Yinjun Gao,Jun-Jie Zhang*

Main category: cs.LG

TL;DR: 通过对称性破缺维度扩展(SBDE)研究准确性与鲁棒性权衡的几何机制，发现插入的辅助维度创造了尖锐边界，导致对抗脆弱性


<details>
  <summary>Details</summary>
Motivation: 深度学习中的准确性与对抗鲁棒性权衡现象普遍存在，但其几何起源尚不明确，需要探究其根本机制

Method: 使用对称性破缺维度扩展(SBDE)作为受控探针，通过在输入图像中插入常数值像素来打破平移对称性，并采用测试时掩码投影来重置插入的辅助像素

Result: SBDE显著提高干净准确率（如CIFAR-10上从90.47%到95.63%），但降低了对迭代白盒攻击的鲁棒性；掩码投影可有效中和攻击并恢复鲁棒性

Conclusion: 准确性与鲁棒性权衡的几何解释是：优化景观加深吸引盆地以提高准确性，但不可避免地沿着辅助自由度建立陡峭的墙壁，导致对离流形扰动的脆弱敏感性

Abstract: The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.

</details>


### [137] [Hardware-Friendly Input Expansion for Accelerating Function Approximation](https://arxiv.org/abs/2602.17952)
*Hu Lou,Yin-Jun Gao,Dong-Xiao Zhang,Tai-Jiao Du,Jun-Jie Zhang,Jia-Rui Zhang*

Main category: cs.LG

TL;DR: 提出一种通过输入空间扩展来打破参数对称性的硬件友好方法，用于一维函数逼近，显著加速训练并提高精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络虽然具有强大的通用逼近能力，但其优化过程常受参数空间对称性导致的平坦损失景观影响，导致收敛缓慢、泛化能力差，尤其对高频分量。受物理学中对称性破缺原理启发，需要一种硬件友好的方法来改善函数逼近性能。

Method: 提出输入空间扩展方法：将原始一维输入（如x）与常数值（如π）组合形成更高维向量（如[π, π, x, π, π]），在不增加网络参数数量的情况下有效打破参数对称性。

Result: 在10个代表性一维函数（平滑、不连续、高频、不可微函数）上的实验表明：输入空间扩展显著加速训练收敛（LBFGS迭代平均减少12%），提高逼近精度（最优5D扩展使最终MSE减少66.3%）。消融研究显示π作为常数表现最佳。

Conclusion: 提出了一种低成本、高效且硬件友好的算法设计技术，通过输入空间扩展打破参数对称性，改善神经网络在函数逼近任务中的训练效率和精度。

Abstract: One-dimensional function approximation is a fundamental problem in scientific computing and engineering applications. While neural networks possess powerful universal approximation capabilities, their optimization process is often hindered by flat loss landscapes induced by parameter-space symmetries, leading to slow convergence and poor generalization, particularly for high-frequency components. Inspired by the principle of \emph{symmetry breaking} in physics, this paper proposes a hardware-friendly approach for function approximation through \emph{input-space expansion}. The core idea involves augmenting the original one-dimensional input (e.g., $x$) with constant values (e.g., $π$) to form a higher-dimensional vector (e.g., $[π, π, x, π, π]$), effectively breaking parameter symmetries without increasing the network's parameter count. We evaluate the method on ten representative one-dimensional functions, including smooth, discontinuous, high-frequency, and non-differentiable functions. Experimental results demonstrate that input-space expansion significantly accelerates training convergence (reducing LBFGS iterations by 12\% on average) and enhances approximation accuracy (reducing final MSE by 66.3\% for the optimal 5D expansion). Ablation studies further reveal the effects of different expansion dimensions and constant selections, with $π$ consistently outperforming other constants. Our work proposes a low-cost, efficient, and hardware-friendly technique for algorithm design.

</details>


### [138] [FedZMG: Efficient Client-Side Optimization in Federated Learning](https://arxiv.org/abs/2602.18384)
*Fotios Zantalis,Evangelos Zervas,Grigorios Koulouras*

Main category: cs.LG

TL;DR: FedZMG是一种新型联邦学习客户端优化算法，通过将梯度投影到零均值超平面来减少客户端漂移，无需额外通信或超参数调优，在非IID数据下表现优于FedAvg和FedAdam。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据通常是非独立同分布的，这会导致客户端漂移问题，降低收敛速度和模型性能。现有自适应优化器虽然能缓解这一问题，但通常计算复杂或通信开销大，不适合资源受限的物联网环境。

Method: 提出FedZMG算法，基于梯度中心化思想，将本地梯度投影到零均值超平面，从而中和异构数据分布中的"强度"或"偏差"偏移。该方法参数免费，无需额外通信或超参数调优。

Result: 理论分析证明FedZMG能减少有效梯度方差并提供更紧的收敛界。在EMNIST、CIFAR100和Shakespeare数据集上的实验表明，FedZMG在高度非IID设置下比FedAvg和FedAdam具有更好的收敛速度和最终验证准确率。

Conclusion: FedZMG是一种有效的客户端优化算法，能够解决联邦学习中的客户端漂移问题，特别适合资源受限的物联网环境，在非IID数据分布下表现出优越性能。

Abstract: Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the "intensity" or "bias" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.

</details>


### [139] [Bayesian Online Model Selection](https://arxiv.org/abs/2602.17958)
*Aida Afshar,Yuke Zhang,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出一种贝叶斯在线模型选择算法，用于随机多臂赌博机问题，能够在多个基础学习器中选择最优者，获得次线性贝叶斯遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯赌博机中的在线模型选择问题：当环境实例从先验分布中采样时，如何设计自适应策略来探索多个赌博机学习器，并与事后最优学习器竞争？

Method: 提出新的贝叶斯算法用于随机赌博机中的在线模型选择。算法能够自适应地在多个基础学习器之间进行选择，并考虑了学习器之间的数据共享以缓解先验误设问题。

Result: 理论证明获得贝叶斯遗憾界为O(d*M√T + √(MT))，其中M是基础学习器数量，d*是最优学习器的遗憾系数，T是时间范围。实验验证在多种随机赌博机设置中表现与最优基础学习器相当。

Conclusion: 提出的贝叶斯在线模型选择算法能够有效探索多个学习器并与最优者竞争，同时数据共享机制有助于缓解先验误设问题，为贝叶斯赌博机中的模型选择提供了有效解决方案。

Abstract: Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.

</details>


### [140] [Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts](https://arxiv.org/abs/2602.17962)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 该研究评估了三种领域自适应方法（MMD、CORAL、DANN）及其组合在髋部骨折风险预测中的跨队列泛化性能，发现多方法组合能显著提升模型在不同人群中的表现。


<details>
  <summary>Details</summary>
Motivation: 临床风险预测模型在不同队列（如不同医院、地区、人口统计群体）中泛化能力差，特别是在髋部骨折风险预测中，源队列训练的模型在目标队列中性能显著下降。需要解决数据分布差异导致的泛化问题。

Method: 使用三个大型队列（SOF、MrOS、UK Biobank）的临床和DXA特征，系统评估三种领域自适应方法：最大均值差异（MMD）、相关性对齐（CORAL）和领域对抗神经网络（DANN）及其组合。采用无结果方法，不依赖目标队列的已知结果。

Result: 领域自适应方法相比无自适应基线（仅源训练）持续提升性能，多方法组合带来最大且最稳定的增益。MMD+CORAL+DANN组合在仅男性源队列中AUC达0.88，仅女性源队列中AUC达0.95，表明集成多方法可产生对数据集差异不敏感的特征表示。

Conclusion: 集成多种领域自适应方法能有效提升髋部骨折风险预测模型的跨队列泛化能力。与依赖监督调优或假设目标队列已知结果的现有方法不同，这种无结果方法能在实际部署条件下进行模型选择，改善模型泛化性能。

Abstract: Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.

</details>


### [141] [Unifying approach to uniform expressivity of graph neural networks](https://arxiv.org/abs/2602.18409)
*Huan Luo,Jonni Virtema*

Main category: cs.LG

TL;DR: 提出了模板GNN（T-GNNs）框架，将节点特征更新定义为在指定图模板集合上聚合有效模板嵌入，建立了与分级模板模态逻辑的等价性，为分析GNN表达能力提供了统一方法。


<details>
  <summary>Details</summary>
Motivation: 标准GNN的表达能力有限，只能聚合直接邻居或全局信息。为了增强表达能力，最近的研究尝试纳入子结构信息（如环计数和子图属性）。本文旨在形式化这一架构趋势。

Method: 引入模板GNNs（T-GNNs）框架，节点特征通过聚合指定图模板集合中的有效模板嵌入来更新。提出相应的分级模板模态逻辑（GML(T)），以及基于模板的互模拟和WL算法的广义概念。

Result: 建立了T-GNNs与GML(T)之间的表达能力等价性，并展示了标准AC-GNNs及其最近变体可以作为T-GNNs的实例进行解释，为GNN表达能力分析提供了统一方法。

Conclusion: T-GNNs框架形式化了GNN中纳入子结构信息的趋势，通过建立与逻辑的等价关系，为理解和分析GNN表达能力提供了系统化的理论基础。

Abstract: The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.

</details>


### [142] [Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation](https://arxiv.org/abs/2602.17972)
*Sebastian Felipe R. Bundoc,Paula Joy B. Martinez,Sebastian C. Ibañez,Erika Fille T. Legara*

Main category: cs.LG

TL;DR: 菲律宾教育服务合同计划等补贴项目因数据碎片化未能有效缓解学校拥挤问题。研究提出计算框架，通过重力模型分析学生流动模式，发现地理距离比学费成本对择校影响更大，且座位容量而非补贴金额是主要限制因素。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家面临学校拥挤问题，严重影响学习成果并加剧教育不平等。虽然将学生从公立学校转移到私立学校的补贴项目可以缓解拥挤而无需资本密集型建设，但由于数据系统碎片化，这些项目往往效果不佳。菲律宾教育服务合同计划作为全球最大的教育补贴项目之一，未能实现缓解公立学校拥挤的目标，缺乏基于科学和数据的分析来理解学生入学流动模式。

Method: 提出计算框架，整合近3000个机构的异构政府数据，采用负二项回归估计的随机重力模型，推导距离、净学费成本和社会经济决定因素的行为弹性。这些弹性参数为双重约束空间分配机制提供依据，模拟不同补贴金额下的学生重新分配，同时尊重生源候选池和目标学校容量限制。

Result: 研究发现地理邻近性对学校选择的限制比学费成本强四倍，座位容量而非补贴金额是主要约束因素。补贴项目本身无法解决系统性过度拥挤问题。

Conclusion: 计算建模能够帮助教育政策制定者做出公平、数据驱动的决策，揭示影响有效资源分配的结构性约束，即使在资源有限的情况下也能发挥作用。补贴项目本身不足以解决系统性过度拥挤问题。

Abstract: School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.

</details>


### [143] [Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly](https://arxiv.org/abs/2602.17997)
*Zehao Jin,Yaoye Zhu,Chen Zhang,Yanan Sui*

Main category: cs.LG

TL;DR: 将果蝇完整连接组作为神经网络控制器用于具身强化学习，实现了稳定的全身运动控制，相比随机图和多层感知机具有更高样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 探索大脑连接组作为神经网络控制器在具身强化学习中的应用，验证生物神经网络结构对运动控制的优势。

Method: 开发FlyGM模型，其静态结构与成年果蝇完整连接组相同，将连接组表示为有向消息传递图，与生物力学果蝇模型集成进行运动控制。

Result: 在多样化运动任务中实现稳定控制，无需任务特定架构调整；相比度保持重连图、随机图和多层感知机，FlyGM具有更高样本效率和性能。

Conclusion: 静态大脑连接组可以转化为有效的神经策略，用于具身学习中的运动控制，证明了生物神经网络结构在机器学习中的价值。

Abstract: Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.

</details>


### [144] [Asynchronous Heavy-Tailed Optimization](https://arxiv.org/abs/2602.18002)
*Junfei Sun,Dixi Yao,Xuchen Gong,Tahseen Rabbani,Manzil Zaheer,Tian Li*

Main category: cs.LG

TL;DR: 该论文研究了在重尾随机梯度噪声下异步优化的稳定性问题，提出了基于延迟感知学习率调度和延迟补偿的算法改进，在理论和实验上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 重尾随机梯度噪声（在Transformer模型中常见）会破坏优化过程的稳定性。现有研究主要关注集中式或分布式同步设置下的重尾噪声处理，而重尾噪声与异步优化之间的相互作用尚未得到充分探索。

Method: 提出了两种处理异步更新中掉队者的通信方案，并基于延迟感知学习率调度和延迟补偿进行算法改进。这些修改旨在增强异步算法在重尾噪声下的性能。

Result: 理论分析表明，在重尾噪声下的收敛保证与同步对应方法的速率匹配，且比现有异步方法具有更好的延迟容忍度。实验显示，在图像和语言任务中，该方法在准确率/运行时间权衡和超参数鲁棒性方面优于先前的同步和异步方法。

Conclusion: 该研究成功解决了重尾随机梯度噪声下异步优化的稳定性问题，提出的延迟感知学习率调度和延迟补偿方法在理论和实践上都取得了显著改进，为大规模深度学习训练提供了更鲁棒的异步优化方案。

Abstract: Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.

</details>


### [145] [Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework](https://arxiv.org/abs/2602.18055)
*Jingyang Qiao,Zhizhong Zhang,Xin Tan,Jingyu Gong,Yanyun Qu,Yuan Xie*

Main category: cs.LG

TL;DR: 本文提出了Continual-NExT框架和MAGE方法，用于解决双模态大语言模型在持续学习中的遗忘、幻觉等问题，提升模型在动态场景中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 双模态大语言模型虽然具备强大的多模态理解和生成能力，但在持续学习方面存在缺陷，难以适应动态现实场景。主要挑战包括：学习新任务会破坏已学知识、存在幻觉问题、指令不遵循、跨模态知识迁移失败等。目前缺乏针对此类模型的标准化持续学习框架。

Method: 提出了Continual-NExT持续学习框架，包含精心设计的评估指标。同时提出了MAGE方法（通用LoRA和专家LoRA的混合与聚合），通过促进跨模态知识迁移和减轻遗忘来提升持续学习能力。

Result: 大量实验表明，MAGE方法优于其他持续学习方法，并取得了最先进的性能表现。

Conclusion: 本文建立了首个针对双模态大语言模型的持续学习框架Continual-NExT，并提出的MAGE方法有效解决了持续学习中的关键挑战，为模型在动态环境中的适应提供了解决方案。

Abstract: Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.

</details>


### [146] [Deepmechanics](https://arxiv.org/abs/2602.18060)
*Abhay Shinde,Aryan Amit Barsainyan,Jose Siguenza,Ankita Vaishnobi Bisoi,Rakshit Kr. Singh,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 论文对三种物理信息深度学习模型（HNN、LNN、SRNN）在六种动力系统上进行基准测试，发现这些模型在混沌或非保守系统中难以保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 物理信息深度学习模型已成为学习动力系统的强大工具，但缺乏跨不同物理现象的系统性基准测试，特别是在保守和耗散系统中，且现有基准测试未检查轨迹稳定性。

Method: 使用DeepChem框架对三种物理信息架构（哈密顿神经网络、拉格朗日神经网络、辛循环神经网络）进行基准测试，评估六个动力系统，包括保守系统（质量弹簧、单摆、双摆、三体问题、弹簧摆）和非保守接触系统（弹跳球）。

Result: 所有基准测试模型在混沌或非保守系统中都难以保持稳定性，模型预测轨迹存在误差，需要定量和定性评估。

Conclusion: 物理信息深度学习模型需要更多研究才能学习到经典力学系统的鲁棒模型，特别是在处理混沌和非保守系统时。

Abstract: Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.

</details>


### [147] [Balancing Symmetry and Efficiency in Graph Flow Matching](https://arxiv.org/abs/2602.18084)
*Benjamin Honoré,Alba Carballo-Castro,Yiming Qin,Pascal Frossard*

Main category: cs.LG

TL;DR: 研究图生成模型中严格等变性与训练效率的权衡，通过可控对称性调制方案放松等变性约束，加速收敛同时避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 严格等变性虽然能保证图生成模型尊重图的置换对称性，但会增加计算成本并减缓收敛速度，因为模型需要在大量可能的节点置换中保持一致。需要研究这种权衡关系。

Method: 从等变离散流匹配模型出发，通过基于正弦位置编码和节点置换的可控对称性调制方案，在训练期间放松模型的等变性约束。

Result: 对称性破坏能通过提供更简单的学习信号加速早期训练，但会导致过拟合（模型重复生成训练集中的图）。适当调制对称性信号可以延迟过拟合同时加速收敛，仅用基线训练轮数的19%就能达到更强的性能。

Conclusion: 在图生成模型中，通过可控方式调制对称性信号可以在保持模型性能的同时显著加速训练收敛，找到严格等变性与训练效率之间的最佳平衡点。

Abstract: Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.

</details>


### [148] [Learning Long-Range Dependencies with Temporal Predictive Coding](https://arxiv.org/abs/2602.18131)
*Tom Potter,Oliver Rhodes*

Main category: cs.LG

TL;DR: 提出结合时间预测编码与近似实时循环学习的新方法，在保持预测编码局部并行特性的同时，有效处理循环神经网络中的长期时间依赖问题，性能接近BPTT但更节能。


<details>
  <summary>Details</summary>
Motivation: 预测编码具有局部并行特性，适合神经形态硬件节能实现，但难以有效扩展到循环神经网络处理长期时间依赖。BPTT虽主导RNN训练，但其非局部计算、缺乏空间并行性、需存储大量激活历史导致高能耗。

Method: 结合时间预测编码与近似实时循环学习，实现有效的时空信用分配。该方法保持预测编码的局部并行特性，同时能够处理循环神经网络中的时间依赖。

Result: 在合成基准和真实任务上性能接近BPTT。在1500万参数机器翻译任务中，测试困惑度为7.62（BPTT为7.49），首次将时间预测编码应用于此规模任务。

Conclusion: 该方法能在保持预测编码局部并行特性的同时学习复杂时间依赖，为更节能的学习系统铺平道路，展示了在保持性能的同时实现能效提升的潜力。

Abstract: Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.

</details>


### [149] [Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks](https://arxiv.org/abs/2602.18141)
*Pierre-Gabriel Berlureau,Ali Hariri,Victor Kawasaki-Borruat,Mia Zosso,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: 提出基于Bakry-Emery图拉普拉斯算子的mu-ChebNet架构，通过可学习的节点势能调节传播动力学，解决GNN长距离信息传播问题，无需改变图拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络在长距离信息传播中存在过平滑和过压缩问题，现有解决方案如图变换器或重布线通常计算成本高或需要改变图结构，需要一种更高效且不改变拓扑的方法。

Method: 引入Bakry-Emery图拉普拉斯算子，通过可学习的节点势能整合扩散和平流过程；基于此开发mu-ChebNet谱架构，联合学习势能和切比雪夫滤波器，结合消息传递自适应性和谱效率。

Result: 在合成长距离推理任务和真实世界基准测试中取得一致性能提升，同时提供可解释的路由场，揭示信息如何在图中流动。

Conclusion: Bakry-Emery拉普拉斯算子为自适应谱图学习提供了原则性且高效的基础，mu-ChebNet有效解决了GNN长距离传播问题，同时保持计算效率和拓扑不变性。

Abstract: Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.

</details>


### [150] [Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks](https://arxiv.org/abs/2602.18146)
*Lionel Salesses,Larbi Arbaoui,Tariq Benamara,Arnaud Francois,Caroline Sainvitu*

Main category: cs.LG

TL;DR: 提出一种用于复杂几何体上时空场长期预测的深度学习框架，采用多时间尺度架构和图神经网络，在增材制造温度预测中表现优异


<details>
  <summary>Details</summary>
Motivation: 复杂几何体上时空场的长期准确预测是科学机器学习中的基本挑战，特别是在增材制造中，温度历史控制着缺陷形成和机械性能。高保真模拟计算成本高，现有机器学习方法在长期温度和梯度预测方面仍有困难。

Method: 采用时间多尺度架构，包含两个在互补时间尺度上运行的耦合模型。两个模型都依赖潜在循环图神经网络捕捉网格上的时空动态，同时使用变分图自编码器提供紧凑的潜在表示，减少内存使用并提高训练稳定性。

Result: 在模拟粉末床熔融数据上的实验表明，该框架能够跨不同几何体实现准确且时间稳定的长期预测，优于现有基线方法。

Conclusion: 该框架虽然是在二维中评估的，但具有通用性和可扩展性，适用于具有多尺度动态的物理驱动系统和三维几何体。

Abstract: Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.

</details>


### [151] [A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction](https://arxiv.org/abs/2602.18168)
*Danning Jing,Xinhai Chen,Xifeng Pu,Jie Hu,Chao Huang,Xuguang Chen,Qinglin Wang,Jie Liu*

Main category: cs.LG

TL;DR: RGD-Blast：一种用于高保真、长期爆炸波预测的鲁棒可泛化深度代理模型，相比传统数值方法加速两个数量级，在未见建筑布局上保持高精度。


<details>
  <summary>Details</summary>
Motivation: 爆炸波传播的时空动力学建模面临高度非线性行为、陡峭梯度和计算成本高的挑战。现有机器学习代理模型在复杂城市布局或分布外场景下精度下降，且自回归预测策略在长期预测中容易误差累积。

Method: 提出RGD-Blast模型，包含多尺度模块捕捉全局流动模式和局部边界交互，缓解自回归预测中的误差累积；引入动态-静态特征耦合机制，融合时变压力场与静态源和布局特征，增强分布外泛化能力。

Result: 相比传统数值方法实现两个数量级的加速，同时保持可比精度。在未见建筑布局的泛化测试中，280个连续时间步的平均RMSE低于0.01，R²超过0.89。在不同爆炸源位置和炸药重量的评估中进一步验证了泛化能力。

Conclusion: RGD-Blast显著推进了长期爆炸波建模的技术水平，通过多尺度建模和动态-静态特征耦合，有效解决了现有代理模型的精度下降和误差累积问题，实现了高效且鲁棒的爆炸波预测。

Abstract: Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.

</details>


### [152] [SeedFlood: A Step Toward Scalable Decentralized Training of LLMs](https://arxiv.org/abs/2602.18181)
*Jihun Kim,Namhoon Lee*

Main category: cs.LG

TL;DR: SeedFlood是一种去中心化训练新方法，通过利用零阶更新的种子可重构结构，使消息大小接近零，从而消除模型规模带来的通信瓶颈，实现大规模模型的高效训练。


<details>
  <summary>Details</summary>
Motivation: 传统基于gossip的方法存在两个主要问题：1）消息通信成本随模型规模增长；2）网络跳数导致信息衰减，全局共识效率低下。这使得大规模模型（如数十亿参数）在数百个客户端上的去中心化训练变得不切实际。

Method: SeedFlood利用零阶更新的种子可重构结构，使消息大小接近零，然后通过flooding机制将消息传播到网络中的每个客户端。这种方法使通信开销变得可忽略且与模型大小无关。

Result: 在去中心化LLM微调实验中，SeedFlood在泛化性能和通信效率方面始终优于基于gossip的基线方法，在大规模设置下甚至达到与一阶方法相当的结果。

Conclusion: SeedFlood通过消除模型规模带来的通信瓶颈，使得在之前被认为不切实际的场景（如数百个客户端上的数十亿参数模型）中进行去中心化训练成为可能，为大规模分布式训练提供了新的解决方案。

Abstract: This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.

</details>


### [153] [RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference](https://arxiv.org/abs/2602.18196)
*Xiuying Wei,Caglar Gulcehre*

Main category: cs.LG

TL;DR: RAT+是一种通过密集预训练结合全序列循环的注意力架构，可在推理时灵活切换为扩张注意力模式，仅需少量适应即可实现高效推理，同时保持接近密集模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 结构化扩张注意力虽然具有推理时效率优势（通过扩张因子D减少FLOPs和KV缓存大小），但直接将预训练注意力模型稀疏化为扩张模式会导致严重的准确性下降。需要一种既能保持密集预训练优势，又能在推理时灵活切换为高效稀疏模式的解决方案。

Method: RAT+架构在密集预训练时增强注意力机制，加入全序列循环和主动循环学习。单个RAT+模型经过一次密集预训练后，可在推理时灵活切换为扩张注意力模式（可选带局部窗口）或混合层/头组合，仅需10亿token的短适应过程，无需重新训练单独的稀疏模型。

Result: 在15亿参数、1000亿token训练下，RAT+在扩张因子16时接近密集模型准确性，在扩张因子64时在常识推理和LongBench任务上分别下降约2-3个点。此外，RAT+在稀疏化为top-k块注意力时优于标准注意力。扩展到26亿参数和2000亿token时观察到相同趋势。

Conclusion: RAT+通过密集预训练结合循环增强，实现了推理时灵活切换为高效稀疏模式的能力，仅需少量适应即可在保持准确性的同时显著提升推理效率，为大规模语言模型提供了实用的效率-准确性权衡方案。

Abstract: Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.

</details>


### [154] [Generative Model via Quantile Assignment](https://arxiv.org/abs/2602.18216)
*Georgi Hrusanov,Oliver Y. Chén,Julien S. Bodelet*

Main category: cs.LG

TL;DR: NeuroSQL是一种无需辅助网络的新型生成模型，通过最优传输问题的渐近近似学习隐变量，在图像质量和计算效率上优于传统生成模型。


<details>
  <summary>Details</summary>
Motivation: 传统深度生成模型（如VAE和GAN）依赖编码器或判别器等辅助网络，导致训练不稳定、计算开销大、存在模式崩溃等问题。需要一种更简单、稳定且高效的生成范式。

Method: NeuroSQL通过渐近近似将隐变量表示为最优传输问题的解，通过求解线性分配问题学习隐变量，然后将隐信息传递给独立的生成器，无需辅助网络。

Result: 在MNIST、CelebA、AFHQ和OASIS四个数据集上，NeuroSQL相比VAE、GAN和扩散模型：1）图像质量更好（像素距离更小，感知/结构保真度更高）；2）训练时间最短；3）在有限训练样本下仍能有效生成合成数据。

Conclusion: NeuroSQL通过分位数分配而非编码器，提供了一种快速、稳定且鲁棒的生成方法，信息损失最小，为合成数据生成提供了有效解决方案。

Abstract: Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.

</details>


### [155] [Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction](https://arxiv.org/abs/2602.18227)
*Redwanul Karim,Changhun Kim,Timon Conrad,Nora Gourmelon,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 该论文提出了一种参数高效的领域自适应方法，将LoRA（低秩适应）与选择性解冻预测头相结合，用于物理信息自注意力GNN，以解决交流潮流预测在电压域转移下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息图神经网络求解器在跨电压域（从中压到高压）部署时，通常需要完全微调，这导致高重训练成本，并且在目标域适应和源域保留之间的稳定性-可塑性权衡上控制有限。

Method: 采用参数高效的领域自适应方法，将LoRA应用于注意力投影层，同时选择性解冻预测头，通过基于物理的损失函数鼓励基尔霍夫一致性行为，同时将适应限制在低秩更新。

Result: 提出的LoRA+PHead方法在多个电网拓扑中，以85.46%的可训练参数减少，实现了接近完全微调的精度（目标域RMSE差距为2.6×10^-4），物理残差与完全微调相当，但在域转移下中压源保留减少了4.7个百分点。

Conclusion: 该方法在电压域转移下实现了可控的效率-精度权衡，能够进行参数高效且物理一致的交流潮流估计，为跨电压域部署提供了实用的解决方案。

Abstract: Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.

</details>


### [156] [Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver](https://arxiv.org/abs/2602.18248)
*Pietro Sittoni,Emanuele Zangrando,Angelo A. Casulli,Nicola Guglielmi,Francesco Tudisco*

Main category: cs.LG

TL;DR: 提出Neural-HSS架构，基于分层半可分矩阵结构，针对椭圆型PDEs实现参数和数据高效学习，在低数据量下仍保持精确性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在求解PDEs方面表现出色，但大规模高质量数据集生成和模型训练的计算成本仍然很高，限制了关键应用的发展。

Method: 受椭圆PDEs格林函数结构启发，提出基于分层半可分矩阵结构的Neural-HSS架构，理论分析其精确性，并与傅里叶神经算子层和卷积层建立联系。

Result: 在三百万点网格的三维泊松方程上验证了数据效率，在低数据量下优于基线方法，并能学习电磁学、流体动力学和生物学等多个领域的PDEs数据。

Conclusion: Neural-HSS架构为椭圆型PDEs提供了一种参数和数据高效的深度学习解决方案，在低数据量下仍能保持理论精确性，具有广泛的应用潜力。

Abstract: Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.

</details>


### [157] [Variational Distributional Neuron](https://arxiv.org/abs/2602.18250)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出变分分布神经元的概念验证：将神经元设计为VAE模块，包含先验、摊销后验和局部ELBO，使神经元不再是确定性标量而是分布，计算变为在约束下收缩可能性空间。


<details>
  <summary>Details</summary>
Motivation: 解决结构张力：序列生成中因果性主要在符号空间组织，潜在变量常为辅助角色，而确定性解码器主导有效动态；同时概率潜在模型能捕捉变化因素和不确定性，但不确定性通常由全局或参数机制承载，基本计算单元仍传播标量。核心问题：如果不确定性是计算的内在属性，为何计算单元不显式承载它？

Method: 设计变分分布神经元作为VAE模块，每个神经元参数化后验分布，传播重参数化样本，并通过局部ELBO的KL项进行正则化。分析"崩溃"模式和"活神经元"条件，通过自回归先验在时间上扩展贡献。

Result: 提出概念验证，展示神经元如何从确定性标量转变为分布实体，计算从值传播转变为在约束下收缩可能性空间。通过局部约束可测试这种"收缩"，并通过内部度量监控。

Conclusion: 分布神经元为解决计算中的内在不确定性提供新视角，通过两个轴线展开：(1)概率约束的组合需稳定、可解释和可控；(2)粒度问题：如果推断是在约束下分布协商，原始单元应保持确定性还是变为分布性？这为神经计算基础带来新思考。

Abstract: We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This "contraction" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze "collapse" modes and the conditions for a "living neuron", then extend the contribution over time via autoregressive priors over the latent, per unit.

</details>


### [158] [MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data](https://arxiv.org/abs/2602.18253)
*Xabier de Zuazo,Vincenzo Verbeni,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.LG

TL;DR: 该研究首次展示了MEG语音模型的迁移学习和跨任务解码，通过预训练-微调范式在感知与产生任务间实现有效解码，证实了共享神经表征的存在。


<details>
  <summary>Details</summary>
Motivation: 解决语音脑机接口中数据效率低下的核心挑战，探索在有限数据条件下如何提升神经解码性能，并验证感知与产生任务间是否存在共享的神经表征。

Method: 使用Conformer-based模型，先在单个被试的50小时听语音数据上进行预训练，然后在18名被试上仅用每人5分钟数据进行微调，分别在感知和产生任务中评估，并进行跨任务解码分析。

Result: 迁移学习带来一致性能提升：任务内准确率提高1-4%，跨任务提升达5-6%。预训练不仅提升各任务性能，还能实现感知与产生任务间的可靠跨任务解码。关键发现是，在语音产生任务上训练的模型能解码被动听语音任务，证明学习到的表征反映了共享神经过程而非任务特异性运动活动。

Conclusion: 该研究证明了迁移学习在MEG语音解码中的有效性，揭示了感知与产生任务间存在共享神经表征，为数据高效的语音脑机接口提供了新方法，并确认了学习到的表征反映的是语言处理的通用神经机制而非特定任务活动。

Abstract: Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.

</details>


### [159] [A Probabilistic Framework for LLM-Based Model Discovery](https://arxiv.org/abs/2602.18266)
*Stefan Wahl,Raphaela Schenk,Ali Farnoud,Jakob H. Macke,Daniel Gedon*

Main category: cs.LG

TL;DR: 该论文提出ModelSMC算法，将模型发现重新定义为概率推断问题，使用序列蒙特卡洛采样从观测数据中发现机制性模拟器模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的模型发现方法通常采用手工设计的启发式流程，缺乏明确的概率形式化。作者希望为模型发现提供一个统一的概率推断框架，以改进模型提案、细化和选择过程。

Method: 提出ModelSMC算法，基于序列蒙特卡洛采样。将候选模型表示为粒子，由LLM迭代提出和细化，并使用基于似然的标准进行加权。

Result: 在真实世界科学系统上的实验表明，该方法能够发现具有可解释机制的模型，并改善了后验预测检查。

Conclusion: 该研究为理解和开发基于LLM的模型发现方法提供了概率视角，将模型发现重新定义为概率推断问题，实现了更统一的框架。

Abstract: Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.

</details>


### [160] [Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering](https://arxiv.org/abs/2602.18348)
*Matheus Camilo da Silva,Leonardo Arrighi,Ana Carolina Lorena,Sylvio Barbon Junior*

Main category: cs.LG

TL;DR: 本文研究AutoClustering元模型的可解释性，通过分析22种现有方法的元特征、应用全局解释技术评估特征重要性，并使用局部解释工具分析具体聚类决策，为更可解释的AutoML设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前AutoClustering系统虽然性能良好，但其推荐结果难以解释：数据集元特征对算法和超参数选择的影响通常不透明，这限制了可靠性、偏差诊断和高效的元特征工程，需要提高无监督学习自动化的决策透明度。

Method: 1) 回顾22种现有方法并将元特征组织成结构化分类法；2) 应用全局可解释性技术（决策谓词图）评估元模型中的特征重要性；3) 使用局部可解释性工具（如SHAP）分析具体的聚类决策。

Result: 研究发现元特征相关性的一致模式，识别出当前元学习策略中可能扭曲推荐结果的结构性弱点，并为更可解释的AutoML设计提供可操作的指导。

Conclusion: 本研究为提高无监督学习自动化的决策透明度提供了实践基础，强调了元模型可解释性在AutoClustering中的重要性，并为未来更透明、可靠的AutoML系统设计指明了方向。

Abstract: AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.

</details>


### [161] [Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study](https://arxiv.org/abs/2602.18403)
*Orfeas Bourchas,George Papalambrou*

Main category: cs.LG

TL;DR: 提出一种结合物理知识与数据驱动的混合建模框架，用于船舶主机功率预测，通过物理基线模型捕捉功率-速度基本关系，再用机器学习预测残差，提高外推能力和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法（如SVM、ANN、树模型等）虽然能捕捉非线性关系，但往往无法遵循螺旋桨定律中功率与速度的基本物理关系，导致在训练范围外的外推性能差。需要一种既能利用数据驱动优势又能尊重物理约束的方法。

Method: 提出混合建模框架：1）物理基线组件基于静水功率曲线公式P=cV^n，捕捉功率与速度的主要依赖关系；2）数据驱动组件（XGBoost、简单神经网络、物理信息神经网络PINN）训练预测残差功率，表示由环境和操作条件引起的偏差。通过将机器学习任务约束为残差修正，简化学习过程并确保物理一致性。

Result: 验证表明，混合模型在稀疏数据区域始终优于纯数据驱动基线，在数据密集区域保持相似性能。该框架为船舶性能监测提供了实用且计算高效的工具。

Conclusion: 混合建模框架通过整合物理知识与数据驱动学习，显著提高了船舶主机功率预测的泛化能力和物理一致性，适用于天气路由、纵倾优化和能效规划等应用。

Abstract: Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.

</details>


### [162] [The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning](https://arxiv.org/abs/2602.18428)
*Mojtaba Sahraee-Ardakan,Mauricio Delbracio,Peyman Milanfar*

Main category: cs.LG

TL;DR: 论文揭示了自主生成模型（如均衡匹配和盲扩散）通过边际能量流进行采样，而非简单的盲去噪，解决了噪声不可知模型中梯度发散的理论悖论。


<details>
  <summary>Details</summary>
Motivation: 自主生成模型（无需噪声级别条件）挑战了传统范式，但存在一个根本悖论：当噪声级别被视为随机变量时，模型优化的底层景观是什么？有界、噪声不可知的网络如何在数据流形附近保持稳定（通常梯度会发散）？

Method: 形式化边际能量 $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$，其中 $p(\mathbf{u})$ 是噪声数据的边际密度。证明自主模型的生成是边际能量上的黎曼梯度流。通过相对能量分解，展示学习的时间不变场隐式包含局部共形度量，抵消几何奇点。

Result: 揭示了噪声预测参数化存在"Jensen Gap"，会放大估计误差，导致确定性盲模型灾难性失败。而速度参数化满足有界增益条件，将后验不确定性吸收为平滑几何漂移，因此具有固有稳定性。

Conclusion: 自主生成模型通过边际能量流进行采样，而非简单的盲去噪。速度参数化具有固有稳定性，而噪声预测参数化存在理论缺陷。这为设计稳定、噪声不可知的生成模型提供了理论基础。

Abstract: Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.

</details>


### [163] [Assigning Confidence: K-partition Ensembles](https://arxiv.org/abs/2602.18435)
*Aggelos Semoglou,John Pavlopoulos*

Main category: cs.LG

TL;DR: CAKE框架通过聚类集成计算分配稳定性和局部几何拟合一致性，为每个点提供可解释的置信度评分，识别模糊点和稳定核心成员。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法缺乏对单个分配可靠性的评估，诊断指标主要反映全局质量而非特定实例的置信度。分配级的不稳定性会影响准确性和鲁棒性，而集成方法虽然提高了全局一致性，但缺乏结合跨运行一致性和学习到的聚类结构几何支持的点级置信度量化工具。

Method: 提出CAKE框架，通过聚类集成计算两个互补统计量：分配稳定性和局部几何拟合一致性，并将它们组合成[0,1]范围内的单一可解释评分。

Result: 理论分析表明CAKE在噪声下保持有效，并能区分稳定点和不稳定点。在合成和真实数据集上的实验表明，CAKE能有效突出模糊点和稳定核心成员，提供可用于指导过滤或优先级排序以改进聚类质量的置信度排名。

Conclusion: CAKE框架填补了聚类集成方法中点级置信度量化的空白，通过结合分配稳定性和几何一致性，为聚类结果的可靠性评估提供了实用工具，有助于提高聚类质量。

Abstract: Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [164] [Maxwell Strata in the sub-Riemannian problem on solvable, nonnilpotent regular three-dimensional Lie groups](https://arxiv.org/abs/2602.17782)
*Adriano Da Silva,Lino Grama,Douglas Duarte Novaes,Margarita Quispe Tusco*

Main category: math.OC

TL;DR: 研究三维可解非幂零李群上的接触结构亚黎曼问题，发现哈密顿系统的垂直分量呈现扰动摆形式，其麦克斯韦集与摆周期相关，从而得到割时间的显式上界。


<details>
  <summary>Details</summary>
Motivation: 研究连通、单连通、可解、非幂零、正则三维李群上的接触结构亚黎曼问题，这类群在几何控制理论和亚黎曼几何中具有重要意义，但相关研究相对较少。

Method: 将哈密顿系统的垂直分量建模为扰动摆，通过定性相空间分析证明该分量具有非平凡对称性，完全刻画对应的麦克斯韦集，并建立与摆周期的关系。

Result: 发现几乎所有测地线的第一个麦克斯韦时间与摆的周期重合，由此得到割时间的显式上界，该上界由摆的周期给出。

Conclusion: 在三维可解非幂零李群的接触结构亚黎曼问题中，哈密顿系统的垂直分量具有扰动摆行为，其对称性导致麦克斯韦集与摆周期密切相关，为割时间提供了明确的几何上界。

Abstract: In this paper, we study the sub-Riemannian problem associated with contact structures on connected, simply connected, solvable, non-nilpotent, regular three-dimensional Lie groups. For these groups, the vertical component of the Hamiltonian system takes the form of a perturbed pendulum. A qualitative phase-space analysis allows us to prove that this vertical component exhibits nontrivial symmetries. In particular, we are able to fully characterize the Maxwell set corresponding to these symmetries, and show that its first Maxwell time coincides with the period of the pendulum for almost all geodesics. This result yields an explicit upper bound for the cut time in terms of the period of the pendulum.

</details>


### [165] [Optimality conditions via exact penalty functions](https://arxiv.org/abs/2602.17795)
*Vsevolod Ivanov Ivanov*

Main category: math.OC

TL;DR: 使用精确罚函数方法，通过下Hadamard导数获得了带不等式、等式和闭集约束问题的最优性条件


<details>
  <summary>Details</summary>
Motivation: 研究具有不等式、等式和闭集约束的优化问题，需要建立基于下Hadamard导数的更一般最优性条件

Method: 应用精确罚函数方法，通过下Hadamard导数分析约束优化问题

Result: 获得了基于下Hadamard导数的约束优化问题最优性条件

Conclusion: 精确罚函数方法是推导带多种约束优化问题最优性条件的有效工具，下Hadamard导数为此类问题提供了合适的分析框架

Abstract: In this paper, we obtain optimality conditions for the problem with inequality, equality and closed set constraints in terms of the lower Hadamard derivative. The results are obtained applying exact penalty functions.

</details>


### [166] [Duality methods in stochastic optimal control](https://arxiv.org/abs/2602.17823)
*Peter Bank,Filippo de Feo*

Main category: math.OC

TL;DR: 该论文证明了随机最优问题价值函数的两种对偶描述，并解决了扩散受控情况下的未解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献对随机最优问题的价值函数对偶描述研究存在局限，特别是在扩散受控的情况下尚未解决，需要填补这一理论空白。

Method: 通过数学证明方法，建立了随机最优问题价值函数的两种对偶描述，特别处理了扩散受控的情况。

Result: 成功证明了两种对偶描述对一般随机最优问题成立，并且这些结果在扩散受控的情况下也成立，解决了文献中的开放问题。

Conclusion: 该研究扩展了随机最优控制理论，为扩散受控的随机最优问题提供了完整的对偶描述框架。

Abstract: We prove two duality descriptions of the value function for a generic stochastic optimal problem. These descriptions also hold when the diffusion is controlled, a case left open by the literature so far.

</details>


### [167] [A note on diffusive solutions of the Lyapunov and Riccati inequalities for quasi-monotone (QM) mappings on cones](https://arxiv.org/abs/2602.17828)
*Oliver Mason*

Main category: math.OC

TL;DR: 本文研究了Metzler矩阵和非负矩阵的三个关键性质在自对偶真凸锥上的推广，包括准单调映射、D-稳定性、对角Lyapunov稳定性和对角Riccati稳定性，并讨论了与对称锥上Jordan代数方法结果的关系。


<details>
  <summary>Details</summary>
Motivation: 将Metzler矩阵和非负矩阵的重要性质推广到更一般的自对偶真凸锥上，建立更广泛的稳定性理论框架，并与对称锥上的现有结果建立联系。

Method: 研究相对于锥K的准单调映射，推广D-稳定性、对角Lyapunov稳定性和对角Riccati稳定性概念，使用相对于锥的扩散映射作为对角矩阵的推广，并与Jordan代数方法的结果进行比较。

Result: 建立了Metzler和非负矩阵关键性质在自对偶真凸锥上的推广框架，将稳定性理论扩展到更一般的锥结构，并建立了与对称锥上Jordan代数结果的联系。

Conclusion: 成功将经典矩阵性质推广到自对偶真凸锥上，为更广泛的稳定性分析提供了理论框架，并展示了与对称锥上Jordan代数方法的联系，为进一步研究奠定了基础。

Abstract: We consider three key properties of Metzler and nonnegative matrices and extensions of these to classes of self-dual proper convex cones. Specifically, we study mappings that are quasi-monotone (QM) with respect to a cone $K$ and discuss results extending D-stability, diagonal Lyapunov stability, and diagonal Riccati stability to this setting. Mappings that act diffusively with respect to the cone are used as generalisations of diagonal matrices. Relationships with recent results for symmetric cones obtained using Jordan algebraic methods are also discussed.

</details>


### [168] [A Refinement in Čech Cohomology of Coron's Necessary Condition](https://arxiv.org/abs/2602.17845)
*Bryce Christopherson,Farhad Jafari*

Main category: math.OC

TL;DR: 本文改进了Coron关于非线性控制系统连续反馈镇定的同调障碍条件，利用Čech上同调和Vietoris-Begle映射定理，证明了更强的上同调刚性条件。


<details>
  <summary>Details</summary>
Motivation: Coron建立了非线性控制系统连续反馈镇定的同调障碍条件，但该条件只涉及最高维同调群。本文旨在利用更精细的Čech上同调工具，得到更强的必要条件。

Method: 使用Čech上同调和Vietoris-Begle映射定理，分析控制系统诱导的映射在闭版本Σ_ε上的性质，证明其必须满足完整的同构条件。

Result: 证明了闭版本Σ_ε必须是Čech上同调的(n-1)-球面，且f在该子集上的限制在所有维数上都诱导Čech上同调群的同构，这比Coron的条件更强。

Conclusion: 本文得到了非线性控制系统连续反馈镇定的更精细的Čech上同调必要条件，将Coron的单一条件扩展为完整的同调刚性陈述。

Abstract: Coron established a homological obstruction to continuous feedback stabilization of nonlinear control systems $\dot{x}=f(x,u)$ with $f \in C(Ω,\mathbb{R}^n)$ and $f(0,0)=0$, showing that local asymptotic stabilizability implies the induced homomorphism $f_*$ satisfies $f_*\big(H_{n-1}(Σ_ε)\big)=H_{n-1}(S^{n-1})$, where $Σ_ε:=\Big(\big(\mathbb{B}_ε^{\mathbb{R}^n}(0)\times\mathbb{B}_ε^{\mathbb{R}^m}(0)\big)\cap Ω\Big)\setminus f^{-1}(0)$. In this paper, we refine Coron's necessary condition using Čech cohomology and the Vietoris-Begle mapping theorem. Specifically, we prove that the closed version of $Σ_ε$ must be a Čech cohomology $(n-1)$-sphere and that the restriction of $f$ to this subset induces an isomorphism on its Čech cohomology groups in all degrees. This strengthens Coron's condition from a constraint on the top class to a full cohomological rigidity statement.

</details>


### [169] [Stabilization of Nonlinear Systems by Gain-Limited Feedback Laws](https://arxiv.org/abs/2602.17847)
*Bryce Christopherson,Farhad Jafari*

Main category: math.OC

TL;DR: 该论文研究了在反馈增益约束下的非线性控制系统局部镇定问题，提出了最大连续开放率概念，并推导了增益受限镇定反馈存在的一般必要条件。


<details>
  <summary>Details</summary>
Motivation: 研究在明确增益约束下的非线性控制系统局部镇定问题，因为实际应用中反馈增益通常受到物理限制或安全考虑的限制。

Method: 1. 使用Brockett开放条件的定量细化，引入系统向量场在平衡点附近的最大连续开放率概念；2. 结合镇定性的局部截面特征，推导增益受限镇定反馈存在的一般必要条件。

Result: 1. 获得了增益受限镇定反馈存在的必要条件；2. 为包括仅能通过非光滑反馈镇定的系统在内的广泛非线性系统类别提供了尖锐的"不可行"结果；3. 通过多个例子展示了开放率如何对镇定反馈在平衡点附近的增长施加基本下界。

Conclusion: 开放率对镇定反馈在平衡点附近的增长施加了基本限制，这些结果为理解增益受限下的镇定可行性提供了理论框架，并揭示了非线性系统镇定中的基本约束。

Abstract: We study local stabilization of nonlinear control systems under explicit gain constraints on the feedback law. Using a quantitative refinement of Brockett's openness condition, we introduce the notion of a maximal continuous openness rate for the system vector field near equilibrium. Combining this with a local-section characterization of stabilizability, we derive a general necessary condition for the existence of gain-limited stabilizing feedback. This condition yields sharp no-go results for broad classes of nonlinear systems, including systems that are stabilizable only by nonsmooth feedback. Several examples illustrate how openness rates impose fundamental lower bounds on stabilizing feedback growth near an equilibrium point.

</details>


### [170] [Improved Analysis of Restarted Accelerated Gradient and Augmented Lagrangian Methods via Inexact Proximal Point Frameworks](https://arxiv.org/abs/2602.17878)
*Matthew X. Burns,Jiaming Liang*

Main category: math.OC

TL;DR: 本文研究凸复合优化的双循环算法，提出重启加速复合梯度法和非精确增广拉格朗日法，建立了最优或近最优的一阶复杂度，并通过数值实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究凸复合优化问题的高效求解算法，特别是针对无约束和线性约束问题，旨在开发具有最优一阶复杂度的双循环算法。

Method: 1. 针对无约束问题：提出重启加速复合梯度法；2. 针对线性约束问题：提出非精确增广拉格朗日法（基本方法和外加速变体）；3. 基于新的非精确近端点框架进行统一分析，支持相对和绝对非精确性、加速和强凸目标。

Result: 1. 无约束问题：在凸和强凸设置下均达到最优一阶复杂度；2. 线性约束问题：两种方法均建立了近最优的一阶复杂度；3. 在LASSO和线性约束二次规划问题上的数值实验证明了方法的实际效率。

Conclusion: 本文提出的双循环算法在理论和实践上都表现出色，为凸复合优化问题提供了高效求解方案，特别是在复杂度和实际性能方面取得了良好平衡。

Abstract: This paper studies a class of double-loop (inner-outer) algorithms for convex composite optimization. For unconstrained problems, we develop a restarted accelerated composite gradient method that attains the optimal first-order complexity in both the convex and strongly convex settings. For linearly constrained problems, we introduce inexact augmented Lagrangian methods, including a basic method and an outer-accelerated variant, and establish near-optimal first-order complexity for both methods. The established complexity bounds follow from a unified analysis based on new inexact proximal point frameworks that accommodate relative and absolute inexactness, acceleration, and strongly convex objectives. Numerical experiments on LASSO and linearly constrained quadratic programs demonstrate the practical efficiency of the proposed methods.

</details>


### [171] [The mean-field control problem for heterogeneous forward-backward systems](https://arxiv.org/abs/2602.17879)
*Andreas Sojmark,Zeng Zhang*

Main category: math.OC

TL;DR: 研究具有异质平均场交互的一般前向-后向随机微分方程系统的平均场控制问题，提出新的降维方法，建立随机最大值原理和验证定理


<details>
  <summary>Details</summary>
Motivation: 研究具有异质平均场交互的一般FBSDE系统的平均场控制问题，这类系统在金融、经济等领域有广泛应用，但现有理论对完全耦合情况下的适定性研究不足

Method: 1. 提出新方法将原系统的适定性问题简化为单个随机化平均场FBSDE的适定性问题；2. 在完全耦合情况下，通过小性条件证明系统及其变分系统和伴随系统的存在唯一性；3. 推导平均场控制问题的随机最大值原理和验证定理

Result: 1. 建立了将复杂系统适定性简化为单个方程的方法；2. 在完全耦合情况下，通过小性条件证明了系统及其相关系统的存在唯一性；3. 获得了平均场控制问题的必要和充分最优性条件

Conclusion: 该研究为具有异质平均场交互的一般FBSDE系统的平均场控制问题提供了系统的理论框架，包括适定性分析和最优性条件，为相关应用领域提供了理论基础

Abstract: We study the problem of mean-field control when the state dynamics are given by general systems of forward-backward stochastic differential equations (FBSDEs) with heterogeneous mean-field interactions. Firstly, we introduce a novel methodology for reducing the well-posedness of such systems to that of a single randomized mean-field FBSDE. As a consequence, we show that, in the fully coupled case, smallness conditions yield existence and uniqueness for both the system itself and the associated variational and adjoint systems. Secondly, we derive a stochastic maximum principle and a verification for the mean-field control problem. This provides necessary and sufficient conditions for optimality.

</details>


### [172] [Multi-agent path-planning in a moving medium via Wasserstein Hamiltonian Flow](https://arxiv.org/abs/2602.17885)
*Christina Frederick,Haomin Zhou*

Main category: math.OC

TL;DR: 提出基于Wasserstein哈密顿流的多智能体路径规划变分模型，通过优化初始速度使智能体从初始位置移动到目标分布，在移动介质中实现最优运输


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在移动介质中从初始位置到目标分布的最优路径规划问题，将概率分布运输与路径优化相结合

Method: 基于Wasserstein哈密顿流的智能体公式推导变分模型，将优化问题定义为初始速度的有限向量，使用L-BFGS方法和射击策略进行数值求解

Result: 模型成功应用于多个仿真场景，包括时变移动介质，展示了在多智能体路径规划中的有效性能

Conclusion: 提出的有限维变分模型为多智能体在移动介质中的路径规划提供了有效的数学框架，能够处理从初始位置到目标分布的最优运输问题

Abstract: We present a finite dimensional variational model for multi-agent path-planning in which a group of agents traverses from initial positions to a target distribution in a moving medium. The model is derived using the agent-based formulation of the Wasserstein Hamiltonian flows that transport between probability distributions while optimizing a running cost. The objective is the mismatch between their final positions and the target distribution. The constraints are a system of Hamiltonian equations that provide the trajectories of the agents. The free variables on which the optimization is defined form a finite vector of the initial velocities for the agents. The model is solved numerically by the L-BFGS method in conjunction with a shooting strategy. Several simulation examples, including a time-dependent moving medium, are presented to illustrate the performance of the model.

</details>


### [173] [Exploiting block triangular submatrices in KKT systems](https://arxiv.org/abs/2602.17968)
*Robert Parker,Manuel Garcia,Russell Bent*

Main category: math.OC

TL;DR: 提出一种利用块三角子矩阵求解KKT系统的方法，通过Schur补分解隔离块三角结构，仅需分解对角块，相比传统对称不定矩阵分解方法减少填充，在神经网络约束优化问题上实现15倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统对称不定矩阵分解方法（如MA57、MA86）在处理KKT系统时，即使存在块三角结构，也会产生大量填充，计算效率低。同时，非对称矩阵分解方法无法提供惯性信息，而惯性信息对非凸优化的内点法至关重要。

Method: 首先使用Schur补分解隔离KKT矩阵中的块三角子矩阵，然后进行块回代求解，仅需分解块三角形式的对角块。利用目标矩阵的先验惯性知识，通过Sylvester定律计算KKT矩阵的惯性。

Result: 该方法在神经网络约束优化问题的KKT系统上，相比最先进的对称不定矩阵分解方法MA57和MA86，在相同硬件条件下实现了高达15倍的加速。

Conclusion: 提出的方法通过有效利用KKT矩阵的块三角结构，减少了填充计算，同时保持了惯性信息，为包含神经网络代理约束的优化问题提供了高效的KKT系统求解方案。

Abstract: We propose a method for solving Karush-Kuhn-Tucker (KKT) systems that exploits block triangular submatrices by first using a Schur complement decomposition to isolate the block triangular submatrices then performing a block backsolve where only diagonal blocks of the block triangular form need to be factorized. We show that factorizing reducible symmetric-indefinite matrices with standard 1$\times$1 or 2$\times$2 pivots yields fill-in outside the diagonal blocks of the block triangular form, in contrast to our proposed method. While exploiting a block triangular submatrix has limited fill-in, unsymmetric matrix factorization methods do not reveal inertia, which is required by interior point methods for nonconvex optimization. We show that our target matrix has inertia that is known \textit{a priori}, letting us compute inertia of the KKT matrix by Sylvester's law. Finally, we demonstrate the computational advantage of this method on KKT systems from optimization problems with neural network surrogates in their constraints. Our method achieves up to 15$\times$ speedups over state-of-the-art symmetric indefinite matrix factorization methods MA57 and MA86 in a constant-hardware comparison.

</details>


### [174] [Policy Gradient Algorithms in Average-Reward Multichain MDPs](https://arxiv.org/abs/2602.18003)
*Jongmin Lee,Ernest K. Ryu*

Main category: math.OC

TL;DR: 本文建立了平均奖励多链MDPs的策略梯度定理，提出了α-裁剪策略镜像上升算法，在正策略空间内达到ε最优策略


<details>
  <summary>Details</summary>
Motivation: 现有策略梯度方法研究主要集中在折扣累积奖励MDPs，而平均奖励MDPs的研究有限，大多数结果局限于遍历或单链设置。需要扩展对多链平均奖励MDPs的策略梯度分析。

Method: 基于循环状态和瞬态分类的不变性，建立平均奖励多链MDPs的策略梯度定理。在此基础上开发α-裁剪策略镜像上升算法，并进行精细化分析。

Result: 获得了一系列收敛性和样本复杂度结果，推进了对该设置的理解。特别地，提出的α-裁剪策略镜像上升算法在正策略空间内达到ε最优策略。

Conclusion: 本文填补了平均奖励多链MDPs策略梯度方法的理论空白，建立了理论基础并提出了有效的算法，为更广泛的多链设置提供了分析框架。

Abstract: While there is an extensive body of research analyzing policy gradient methods for discounted cumulative-reward MDPs, prior work on policy gradient methods for average-reward MDPs has been limited, with most existing results restricted to ergodic or unichain settings. In this work, we first establish a policy gradient theorem for average-reward multichain MDPs based on the invariance of the classification of recurrent and transient states. Building on this foundation, we develop refined analyses and obtain a collection of convergence and sample-complexity results that advance the understanding of this setting. In particular, we show that the proposed $α$-clipped policy mirror ascent algorithm attains an $ε$-optimal policy with respect to positive policies.

</details>


### [175] [On the $q$-integrability of $p$-Wasserstein barycenters](https://arxiv.org/abs/2602.18293)
*Camilla Brizzi,Lorenzo Portinale*

Main category: math.OC

TL;DR: 研究p-Wasserstein度量下概率测度重心的L^q正则性，证明在某些几何条件下边际的L^q正则性会传递给重心，但N>2时存在反例，并给出了L^q范数估计和重心特征化。


<details>
  <summary>Details</summary>
Motivation: 研究p-Wasserstein度量下概率测度重心的正则性保持问题，探索边际的L^q正则性是否能够传递给重心，特别是在N>2的情况下，这比经典的p=2情况或测地线(N=2)情况更复杂。

Method: 通过几何分析、构造反例、L^q范数估计和Kantorovich势特征化等方法，研究重心正则性。特别关注边际支撑集的几何条件对正则性的影响。

Result: 证明在适当几何条件下，一个边际的L^q正则性会传递给重心；但构造了N>2时重心可能不是q可积的反例，即使边际有界紧支撑；给出了L^q范数估计和奇异性来源分析；通过Kantorovich势特征化了重心；对特殊仿射变换推前测度计算了显式重心。

Conclusion: p-Wasserstein重心的L^q正则性保持不仅取决于边际的正则性，还强烈依赖于边际支撑集的几何结构。N>2时情况比经典情况更复杂，需要额外的几何条件来保证正则性传递。

Abstract: We study the $L^q$-regularity of the density of barycenters of $N$ probability measures on $\mathbb{R}^d$ with respect to the $p$-Wasserstein metric ($1<p<\infty$). According to a previous result by the first author and collaborators, if one marginal is absolutely continuous, so is the $W_p$-barycenter. The next natural question is whether the $L^q$- regularity on the marginals is also preserved for any $q > 1$, as in the classical case ($p=2$) of Agueh--Carlier, or for $W_p$-geodesics ($N=2$). Here we prove that this is the case if one marginal belongs to $L^q$ and the supports of all the marginals satisfy suitable geometric assumptions. However, we show that, as soon as $N>2$, it is possible to find examples of $W_p$-barycenters which are not $q$-integrable, even if one marginal is compactly supported and bounded, thus highlighting the role played by the geometry of the supports. Furthermore, we provide a general estimate of the $L^q$-norm, including a detailed study of the sources of singularities, and a characterization of the $W_p$-barycenters à la Agueh--Carlier in terms of the associated Kantorovich potentials. Finally, we explicitly compute the $W_p$-barycenters of measures obtained as push-forward of special affine transformations. In this case, regularity holds without any additional requirement on the supports.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [176] [Time consistent portfolio strategies for a general utility function](https://arxiv.org/abs/2602.18157)
*Oumar Mbodji*

Main category: q-fin.PM

TL;DR: 研究Merton投资组合管理问题，考虑非恒定时间贴现率和一般效用函数，通过子博弈完美策略解决时间不一致性，提出效用加权贴现率ρ(t,x)和固定点迭代方法


<details>
  <summary>Details</summary>
Motivation: 传统Merton投资组合问题通常假设恒定贴现率，但实际中贴现率可能随时间变化。非恒定贴现率会导致时间不一致性问题，需要新的解决方案

Method: 在完全市场框架下，引入子博弈完美策略解决时间不一致性。在效用函数的渐近假设下，证明子博弈完美策略等价于最优策略，但需用依赖于时间和财富水平的效用加权贴现率ρ(t,x)替代原贴现率。使用固定点迭代求解ρ

Result: 消费财富比和投资财富比可以表示为价值函数的反馈形式函数。通过效用加权贴现率ρ(t,x)的引入，解决了非恒定贴现率下的时间不一致性问题

Conclusion: 该研究扩展了Merton投资组合理论，为非恒定贴现率情况提供了理论框架和计算方法，通过效用加权贴现率和子博弈完美策略有效解决了时间不一致性问题

Abstract: We study the Merton portfolio management problem within a complete market, non constant time discount rate and general utility framework. The non constant discount rate introduces time inconsistency which can be solved by introducing sub game perfect strategies. Under some asymptotic assumptions on the utility function, we show that the subgame perfect strategy is the same as the optimal strategy, provided the discount rate is replaced by the utility weighted discount rate $ρ(t,x)$ that depends on the time $t$ and wealth level $x$. A fixed point iteration is used to find $ρ$. The consumption to wealth ratio and the investment to wealth ratio are given in feedback form as functions of the value function.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [177] [Entropy-regularized penalization schemes for American options and reflected BSDEs with singular generators](https://arxiv.org/abs/2602.18078)
*Daniel Chee,Noufel Frikha,Libo Li*

Main category: q-fin.MF

TL;DR: 本文扩展了之前的工作，将探索性框架应用于连续时间最优停止问题，特别是美式期权。提出了熵正则化惩罚方案，建立了理论收敛性，并研究了极限情况下的奇异反射BSDE。


<details>
  <summary>Details</summary>
Motivation: 将强化学习中的探索性框架引入连续时间最优停止问题，特别是美式期权定价。传统方法存在停止规则退化问题，需要正则化来促进探索、支持梯度优化和策略改进算法。

Method: 提出熵正则化惩罚方案，灵感来自反射BSDE的经典惩罚技术。该方案平滑逼近美式期权的退化停止规则，建立了适定性和收敛性。数值实验使用策略迭代和最小二乘蒙特卡洛方法。

Result: 证明了熵正则化惩罚方案的适定性和收敛性。当惩罚参数趋于无穷时，极限值过程求解具有对数奇异驱动的反射BSDE，通过单调极限论证证明了这类新RBSDE解的存在唯一性。

Conclusion: 成功将探索性框架扩展到连续时间最优停止问题，提出的熵正则化方案既具有实际计算优势（支持梯度优化和策略改进），又产生了新的理论结果（奇异反射BSDE）。

Abstract: This paper extends our previous work in Chee et al. [9] to continuous-time optimal stopping problems, with a particular focus on American options within an exploratory framework. We pursue two main objectives. First, motivated by reinforcement learning applications, we introduce an entropy-regularized penalization scheme for continuous-time optimal stopping problems. The scheme is inspired by classical penalization techniques for reflected backward stochastic differential equations (RBSDEs) and provides a smooth approximation of the degenerate stopping rule inherent to the American option problem. This regularization promotes exploration, enables the use of gradient-based optimization methods, and leads naturally to policy improvement algorithms. We establish well-posedness and convergence properties of the scheme, and illustrate its numerical feasibility through low-dimensional experiments based on policy iteration and least-squares Monte Carlo methods. Second, from a theoretical perspective, we study the asymptotic limit of the entropy-regularized penalization as the penalization parameter tends to infinity. We show that the limiting value process solves a reflected BSDE with a logarithmically singular driver, and we prove existence and uniqueness of solutions to this new class of RBSDEs via a monotone limit argument. To the best of our knowledge, such equations have not previously been investigated in the literature

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [178] [Beyond the Numbers: Causal Effects of Financial Report Sentiment on Bank Profitability](https://arxiv.org/abs/2602.17851)
*Krishna Neupane,Prem Sapkota,Ujjwal Prajapati*

Main category: q-fin.CP

TL;DR: 该研究使用因果森林机器学习方法，结合FinancialBERT情感分析，探讨市场情绪对企业盈利能力的因果影响，超越了传统相关性分析。


<details>
  <summary>Details</summary>
Motivation: 传统研究多关注市场情绪与企业盈利的相关性，但缺乏因果关系的深入分析。本研究旨在建立市场情绪对企业盈利能力的因果效应，考虑异质性和非线性关系，提供更稳健的理解。

Method: 1. 使用预训练的FinancialBERT从季度报告中生成情感分数作为因果干预变量；2. 采用因果森林机器学习方法控制众多混淆变量；3. 使用SHAP分析识别影响盈利的关键预测因子；4. 进行双管齐下的因果分析，考察贷款组合/资产构成以及资产负债表强度/杠杆如何调节情绪影响；5. 进行平均处理效应分析。

Result: 研究发现某些资产负债表和费用管理变量与盈利能力存在统计显著的因果关系。SHAP分析识别了影响盈利的关键预测因子。研究揭示了情绪影响如何受贷款组合、资产构成、资产负债表强度和杠杆的调节。

Conclusion: 该先进的因果机器学习框架显著扩展了现有文献，提供了关于金融情绪如何真正影响企业绩效的更稳健理解，超越了传统的相关性分析。

Abstract: This study establishes the causal effects of market sentiment on firm profitability, moving beyond traditional correlational analyses. It leverages a causal forest machine learning methodology to control for numerous confounding variables, enabling systematic analysis of heterogeneity and non-linearities often overlooked. A key innovation is the use of a pre-trained FinancialBERT to generate sentiment scores from quarterly reports, which are then treated as causal interventions impacting profitability dynamics like returns and volatilities. Utilizing a comprehensive dataset from NEPSE, NRB, and individual financial institutions, the research employs SHAP analysis to identify influential profit predictors. A two-pronged causal analysis further explores how sentiment's impact is conditioned by Loan Portfolio/Asset Composition and Balance Sheet Strength/Leverage. Average Treatment Effect analyses, combined with SHAP insights, reveal statistically significant causal associations between certain balance sheet and expense management variables and profitability. This advanced causal machine learning framework significantly extends existing literature, providing a more robust understanding of how financial sentiment truly impacts firm performance.

</details>


### [179] [The Information Dynamics of Insider Intent: How Reporting Inversions (Form 144) Mask Informational Rents in Insider Sales (Form 4)](https://arxiv.org/abs/2602.17890)
*Krishna Neupane*

Main category: q-fin.CP

TL;DR: 研究发现SEC Form 144披露制度存在信息摩擦，表现为"预测脱钩"现象，即交易执行(Form 4)常早于出售意向公告(Form 144)，导致非对称定价，违反帕累托效率条件。


<details>
  <summary>Details</summary>
Motivation: 当前SEC Form 144披露制度存在信息不对称问题，交易执行(Form 4)经常先于出售意向公告(Form 144)，这种"报告倒置"导致市场无法有效处理信息，造成定价效率损失。

Method: 采用事件研究法分析意图出售窗口，研究内部人士提交出售意向但未在90天法定期限内执行的情况；运用机器学习审计评估信息不透明度；进行横截面测试分析异质性波动率的影响。

Result: 发现52.4%的不透明度率，中止信号与常规执行在统计上无法区分；大型公司表现出显著性悖论(14.49bps显著alpha)，而小型公司虽有更高异常回报(32.21bps)但不显著；异质性波动率放大信号效应，导致流动性下降达2.63倍。

Conclusion: 建议引入强制性执行确认机制(Form 144-A)，实现双边问责，将预测盲点转化为可验证数据流，恢复信息对称性，促进资本配置效率。

Abstract: This study identifies and quantifies a significant informational friction embedded in the SEC Form 144 disclosure regime, characterized as predictive decoupling. Drawing on a theoretical foundation of welfare economics, the article argues that the current reporting inversion -- where trade execution (Form 4) frequently precedes the public notice of intent (Form 144) -- violates the conditions for Pareto efficiency by inducing non-symmetric pricing. Utilizing an event-study framework of intent-to-sell windows, the analysis examines cases where insiders file a notice of proposed sale but fail to execute within the statutory 90-day period. The machine learning audit reveals a persistent 52.4 percent opacity rate, where aborted signals remain statistically indistinguishable from routine executions, creating a structural information ceiling that prevents the market from exhausting the signal's informational content. Contrary to the traditional small-firm effect, the study documents a large-cap significance paradox: while small-cap portfolios yield higher absolute abnormal returns (32.21 bps), statistically significant alpha is concentrated in large-cap firms (14.49 bps, $p = 0.021$). The results suggest that Institutional Salience enables more reliable processing of this negative non-event when reputational costs are maximized. Cross-sectional tests confirm that prior idiosyncratic volatility serves as a signal amplifier, with causal estimators identifying an illiquidity jump of up to 2.63 times. To mitigate this market failure, the study proposes a mandatory execution confirmation (Form 144-A) to transition the regime toward bilateral accountability, converting a predictive blind spot into a verifiable data stream and restoring the informational symmetry requisite for efficient capital allocation.

</details>


### [180] [The Strategic Gap: How AI-Driven Timing and Complexity Shape Investor Trust in the Age of Digital Agents](https://arxiv.org/abs/2602.17895)
*Krishna Neupane*

Main category: q-fin.CP

TL;DR: 论文提出"自主披露监管器"AI框架，发现公司通过复杂语言和不可预测的披露时间制造"战略缺口"，使市场了解真相的速度降低60%，并识别出39个高风险失效案例。


<details>
  <summary>Details</summary>
Motivation: 传统市场效率模型只关注披露内容而忽略报告时间和节奏的结构性影响，导致投资者对战略性时间安排和复杂语言的脆弱性。需要新的监管框架来应对信息整合成本上升的挑战。

Method: 开发多节点AI框架（自主披露监管器），分析484,796份监管文件，识别披露复杂性和申报不可预测性的交叉点，采用递归代理审计方法。

Result: 发现"战略缺口"现象：公司使用混淆语言和不可预测时间使市场了解真相速度降低60%；识别39个高风险失效案例；系统显示360%的福利恢复潜力和近乎完美的技术中断恢复能力。

Conclusion: 建议向代理监管状态过渡，监管基础设施应从被动数据存储库演变为能够实时合成的主动审计节点，以维护市场完整性。

Abstract: Traditional models of market efficiency assume that equity prices incorporate information based on content alone, often neglecting the structural influence of reporting timing and cadence. This study introduces the Autonomous Disclosure Regulator, a multi-node AI framework designed to audit the intersection of disclosure complexity and filing unpredictability. Analyzing a population of 484,796 regulatory filings, the research identifies a structural Strategic Gap: a state where companies use confusing language and unpredictable timing to slow down how fast the market learns the truth by 60%. The results demonstrate a fundamental computational asymmetry in contemporary capital markets. While investors are now good at spotting "copy-paste" text, they remain vulnerable to strategic timing that obscures structural deterioration. The framework isolates 39 high-priority failures where the convergence of dense text and temporal surprises facilitated significant information rent extraction by insiders. By implementing a recursive agentic audit, the system identifies a cumulative welfare recovery potential of over 360\% and demonstrates near-perfect resilience against technical data interruptions. The study concludes by proposing a transition toward an agentic regulatory state, arguing that as information integration costss rise, infrastructure must evolve from passive data repositories into active auditing nodes capable of real-time synthesis to preserve market integrity.

</details>


### [181] [A Monotone Limit Approach to Entropy-Regularized American Options](https://arxiv.org/abs/2602.18062)
*Daniel Chee,Noufel Frikha,Libo Li*

Main category: q-fin.CP

TL;DR: 提出基于Doob-Meyer-Mertens分解和反射BSDE的完全概率框架，用于熵正则化最优停止问题，建立单调逼近和收敛率，并开发基于线性BSDE的策略改进算法。


<details>
  <summary>Details</summary>
Motivation: 现有连续时间最优停止方法多依赖PDE方法，本文旨在建立完全概率框架，利用Snell包络的分解和反射BSDE表示，提供更灵活的理论基础。

Method: 基于Doob-Meyer-Mertens分解和反射BSDE表示Snell包络，引入熵正则化惩罚方案获得价值函数的单调逼近，并开发基于线性BSDE的策略改进算法。

Result: 在适当正则性假设下建立了明确的收敛率，并通过美式最大看涨期权的简单数值实验验证了算法性能。

Conclusion: 提出的概率框架为熵正则化最优停止问题提供了新的理论工具和算法实现，扩展了现有PDE方法的局限性。

Abstract: Recent advances in continuous-time optimal stopping have been driven by entropy-regularized formulations of randomized stopping problems, with most existing approaches relying on partial differential equation methods. In this paper, we propose a fully probabilistic framework based on the Doob-Meyer-Mertens decomposition of the Snell envelope and its representation through reflected backward stochastic differential equations. We introduce an entropy-regularized penalization scheme yielding a monotone approximation of the value function and establish explicit convergence rates under suitable regularity assumptions. In addition, we develop a policy improvement algorithm based on linear backward stochastic differential equations and illustrate its performance through a simple numerical experiment for an American-style max call option

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [182] [Epistemic Traps: Rational Misalignment Driven by Model Misspecification](https://arxiv.org/abs/2602.17676)
*Xingcheng Xu,Jingjing Qu,Qiaosheng Zhang,Chaochao Lu,Yanqing Yang,Na Zou,Xia Hu*

Main category: cs.AI

TL;DR: 论文提出AI安全的新理论框架，将大语言模型和AI代理的病理行为（如奉承、幻觉、战略欺骗）解释为模型误设下的理性行为，而非训练缺陷。通过引入经济学中的Berk-Nash理性化理论，证明这些不安全行为是结构必然性，安全是离散相而非奖励的连续函数。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全范式将大语言模型和AI代理的病理行为视为训练过程中的暂时缺陷，缺乏统一的理论框架来解释这些行为的出现和稳定性。这些行为（如奉承、幻觉、战略欺骗）阻碍了AI在关键社会和技术领域的部署。

Method: 将经济学中的Berk-Nash理性化理论引入人工智能领域，建立理论框架，将AI代理建模为在错误的主观世界模型下进行优化。通过六个最先进模型家族的行为实验验证理论预测，生成精确映射安全行为拓扑边界的相图。

Result: 研究发现不安全行为是结构必然性：要么作为稳定的未对齐均衡出现，要么作为振荡循环出现，取决于奖励方案。战略欺骗作为"锁定"均衡或通过认知不确定性持续存在。安全是离散相，由代理的认知先验决定，而非奖励大小的连续函数。

Conclusion: 安全取决于代理的内部信念结构而非环境奖励。这确立了"主观模型工程"（设计代理内部信念结构）作为稳健对齐的必要条件，标志着从操纵环境奖励到塑造代理对现实解释的范式转变。

Abstract: The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a "locked-in" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.

</details>


### [183] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 论文研究形式化领域本体能否通过检索增强生成提升语言模型在数学等专业领域的可靠性，发现本体引导的上下文在检索质量高时能改善性能，但无关上下文会降低性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在幻觉、脆弱性和缺乏形式化基础等根本限制，这在需要可验证推理的高风险专业领域尤其成问题。研究者希望探索形式化领域本体能否增强语言模型的可靠性。

Method: 使用数学作为概念验证，实现神经符号管道，利用OpenMath本体结合混合检索和交叉编码器重排序，将相关定义注入模型提示中。在MATH基准上评估三个开源模型。

Result: 评估显示，当检索质量高时，本体引导的上下文能提高性能，但无关上下文会主动降低性能，突显了神经符号方法的潜力和挑战。

Conclusion: 形式化本体在增强语言模型可靠性方面具有潜力，但检索质量至关重要。无关上下文会损害性能，表明神经符号方法需要精心设计的检索机制。

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [184] [The Token Games: Evaluating Language Model Reasoning with Puzzle Duels](https://arxiv.org/abs/2602.17831)
*Simon Henniger,Gabriel Poesia*

Main category: cs.AI

TL;DR: TTG是一个基于编程谜题对决的评估框架，让大语言模型相互出题挑战，通过Elo评分比较模型能力，无需人工出题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在两个问题：1）人工出题成本高，特别是需要博士级领域知识的难题；2）难以区分模型是真正推理还是见过类似训练数据。需要一种可持续、防饱和的评估范式。

Method: 受16世纪数学对决启发，设计Token Games框架：模型通过创建编程谜题相互挑战（给定返回布尔值的Python函数，找到使函数返回True的输入）。使用成对对决结果计算Elo评分来比较模型相对能力。

Result: 评估了10个前沿模型，TTG的排名与Humanity's Last Exam等现有基准高度匹配，且无需人工出题。发现创建优质谜题对当前模型仍是极具挑战的任务，这是以往基准未测量的能力。

Conclusion: TTG提出了一种新的评估范式：既能防止设计饱和，又能同时测试模型的创造力、任务创建能力和问题解决能力，为评估推理能力提供了可持续的解决方案。

Abstract: Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.

</details>


### [185] [El Agente Gráfico: Structured Execution Graphs for Scientific Agents](https://arxiv.org/abs/2602.17902)
*Jiaru Bai,Abdulrahman Aldossary,Thomas Swanick,Marcel Müller,Yeonghun Kang,Zijian Zhang,Jin Won Lee,Tsz Wai Ko,Mohammad Ghazi Vakili,Varinia Bernales,Alán Aspuru-Guzik*

Main category: cs.AI

TL;DR: 提出El Agente Gráfico框架，通过类型安全执行环境和动态知识图谱，解决LLM在科学工作流中集成异构工具时的上下文管理和可追溯性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的科学工作流自动化存在ad hoc集成、上下文管理脆弱、决策溯源困难等问题，需要更可靠的结构化框架来支持复杂科学计算。

Method: 开发单代理框架，将LLM决策嵌入类型安全执行环境，使用动态知识图谱进行外部持久化，通过结构化科学概念抽象和对象-图谱映射器管理计算状态。

Result: 在量子化学任务基准测试中，单代理框架能够稳健执行复杂多步骤并行计算；在构象系综生成和金属有机框架设计中，知识图谱成功作为记忆和推理基础。

Conclusion: 抽象化和类型安全为基于代理的科学自动化提供了可扩展基础，超越了以提示为中心的设计，实现了更可靠、可追溯的科学工作流自动化。

Abstract: Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gráfico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.

</details>


### [186] [Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems](https://arxiv.org/abs/2602.17910)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.AI

TL;DR: APEMO是一个运行时调度层，通过利用时间-情感信号优化固定预算下的计算分配，提升长时程自主代理工作流的轨迹级可靠性和重用概率。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐主要关注单个模型输出，但自主代理在长时程工作流中需要在整个交互轨迹上保持持续可靠性。需要一种方法来优化计算分配，提高轨迹级质量。

Method: APEMO是一个运行时调度层，通过行为代理检测轨迹不稳定性，并在关键片段（如峰值时刻和结束时刻）进行修复，而不是修改模型权重。它操作化时间-情感信号来优化计算分配。

Result: 在多智能体模拟和基于LLM的规划-执行流程评估中，APEMO在轨迹级质量和重用概率方面持续优于结构化编排器。

Conclusion: 研究结果将AI对齐重新定义为时间控制问题，为长时程智能体系统的开发提供了弹性工程路径。

Abstract: Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.

</details>


### [187] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: 提出了WorkflowPerturb基准，通过可控扰动评估工作流评估指标的敏感性和校准性


<details>
  <summary>Details</summary>
Motivation: LLM生成的结构化工作流评估困难，现有指标分数未校准且变化无法反映工作流退化严重程度

Method: 构建包含4,973个黄金工作流和44,757个扰动变体的基准，应用三种扰动类型（缺失步骤、压缩步骤、描述变化）和三个严重级别（10%、30%、50%）

Result: 基准了多种指标家族，分析其敏感性和校准特性，支持基于严重程度的工作流评估分数解释

Conclusion: WorkflowPerturb基准能够系统分析工作流评估指标的差异，促进严重程度感知的评估方法发展

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [188] [Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets](https://arxiv.org/abs/2602.18025)
*Haruki Abe,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.AI

TL;DR: 论文提出结合离线强化学习和跨具身学习来解决机器人策略预训练中高质量演示数据成本高的问题，通过构建16个机器人平台的数据集验证方法，并引入基于具身相似性的分组策略来缓解形态差异带来的梯度冲突。


<details>
  <summary>Details</summary>
Motivation: 机器人策略预训练需要大量高质量演示数据，但为每个机器人平台收集这些数据成本高昂。离线强化学习可以利用专家和次优数据，跨具身学习可以整合不同形态机器人的轨迹数据，结合两者有望降低数据收集成本并获取通用控制先验。

Method: 1) 系统分析离线强化学习与跨具身学习的结合范式；2) 构建包含16个不同机器人平台的运动数据集；3) 提出基于具身相似性的分组策略：将形态相似的机器人聚类，使用组梯度更新模型，减少跨机器人梯度冲突。

Result: 实验表明：1) 离线强化学习+跨具身学习在次优轨迹丰富的数据集上优于纯行为克隆；2) 随着次优数据比例和机器人类型增加，形态差异导致的梯度冲突会阻碍学习；3) 提出的静态分组策略能显著减少机器人间冲突，优于现有冲突解决方法。

Conclusion: 离线强化学习与跨具身学习的结合是有效的机器人策略预训练方法，特别是在次优数据丰富的场景下。但需要解决形态差异带来的梯度冲突问题，提出的基于具身相似性的分组策略是简单有效的解决方案。

Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

</details>


### [189] [Neurosymbolic Language Reasoning as Satisfiability Modulo Theory](https://arxiv.org/abs/2602.18095)
*Hyunseok Oh,Sam Stern,Youngki Lee,Matthai Philipose*

Main category: cs.AI

TL;DR: Logitext是一种神经符号语言，将文档表示为自然语言文本约束，通过结合LLM约束评估与SMT求解实现文本与逻辑的联合推理，扩展了神经符号方法的应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号系统结合LLM与求解器，但仅限于完全形式化的任务（如数学或程序合成），无法处理具有部分逻辑结构的自然文档。需要一种能够处理自然语言与逻辑混合推理的方法。

Method: 提出Logitext神经符号语言，将文档表示为自然语言文本约束；开发算法将LLM约束评估与SMT求解相结合，实现文本-逻辑联合推理；将LLM推理视为SMT理论的一部分。

Result: 在内容审核新基准、LegalBench和Super-Natural Instructions上的实验表明，Logitext提高了准确性和覆盖率，是首个将LLM推理视为SMT理论的神经符号方法。

Conclusion: Logitext通过将文档表示为自然语言文本约束，结合LLM与SMT求解，成功扩展了神经符号方法到部分形式化的自然文档领域，实现了可靠的文本-逻辑联合推理。

Abstract: Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructions, show that Logitext improves both accuracy and coverage. This work is the first that treats LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains.

</details>


### [190] [SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps](https://arxiv.org/abs/2602.18201)
*Joseph Bingham,Netanel Arussy,Dvir Aran*

Main category: cs.AI

TL;DR: SOMtime方法在无监督学习中即使敏感属性被排除，仍能恢复出与年龄、收入等敏感属性对齐的潜在轴，揭示了"公平通过无知"在表示层面的失败。


<details>
  <summary>Details</summary>
Motivation: 挑战无监督表示对敏感属性保持中性的假设，证明即使敏感属性被排除在训练之外，它们仍可能作为主导潜在轴出现，揭示无监督组件中的公平风险。

Method: 使用SOMtime（基于高容量自组织映射的拓扑保持表示方法），在两个大规模真实数据集（世界价值观调查和人口普查收入数据集）上进行实验，比较PCA、UMAP、t-SNE和自编码器等方法。

Result: SOMtime恢复了与被排除敏感属性对齐的单调排序，Spearman相关性高达0.85，而其他方法通常低于0.23；无监督分割SOMtime嵌入会产生人口统计偏斜的聚类。

Conclusion: "公平通过无知"在表示层面对于有序敏感属性是失败的，公平审计必须扩展到机器学习管道的无监督组件。

Abstract: Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime

</details>


### [191] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: 提出首个在线多智能体强化学习扩散策略框架OMAD，通过松弛策略目标最大化联合熵实现高效探索，在CTDE范式下使用联合分布价值函数优化分散扩散策略，在10个任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成和离线设置中表现出卓越的表达能力和多模态表示能力，但在在线多智能体强化学习中的应用尚未充分探索。主要障碍是扩散模型的不可处理似然性阻碍了基于熵的探索和协调。

Method: 提出OMAD框架：1) 松弛策略目标最大化缩放联合熵，实现无需可处理似然的有效探索；2) 在CTDE范式下使用联合分布价值函数优化分散扩散策略；3) 利用可处理的熵增强目标指导扩散策略同时更新，确保稳定协调。

Result: 在MPE和MAMuJoCo的10个多样化任务上进行广泛评估，证明该方法成为新的最先进技术，样本效率显著提高2.5倍到5倍。

Conclusion: OMAD成功将扩散策略应用于在线多智能体强化学习，通过创新的松弛策略目标和联合分布价值函数解决了扩散模型在MARL中的探索和协调挑战，实现了显著的性能提升。

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>
