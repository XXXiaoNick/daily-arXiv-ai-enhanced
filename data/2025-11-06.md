<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 72]
- [econ.EM](#econ.EM) [Total: 8]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.AI](#cs.AI) [Total: 16]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [math.OC](#math.OC) [Total: 17]
- [eess.SY](#eess.SY) [Total: 17]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Cache Mechanism for Agent RAG Systems](https://arxiv.org/abs/2511.02919)
*Shuhang Lin,Zhencan Peng,Lingyao Li,Xiao Lin,Xi Zhu,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ARC是一种新颖的代理RAG缓存机制，通过动态管理小型高价值语料库，将存储需求降低到原始语料的0.015%，同时提供高达79.8%的命中率和80%的平均检索延迟减少。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在提升代理性能方面取得成功，但代理级别的缓存管理（特别是为每个代理动态构建、维护和更新紧凑相关语料库）仍然研究不足。

Method: ARC通过综合历史查询分布模式和嵌入空间中缓存项目的内在几何结构，自动维护高相关性缓存，无需人工标注。

Result: 在三个检索数据集上的实验表明，ARC将存储需求降至原始语料的0.015%，提供高达79.8%的命中率，平均检索延迟减少80%。

Conclusion: ARC能够显著提升RAG驱动的LLM代理的效率和有效性。

Abstract: Recent advances in Large Language Model (LLM)-based agents have been
propelled by Retrieval-Augmented Generation (RAG), which grants the models
access to vast external knowledge bases. Despite RAG's success in improving
agent performance, agent-level cache management, particularly constructing,
maintaining, and updating a compact, relevant corpus dynamically tailored to
each agent's need, remains underexplored. Therefore, we introduce ARC (Agent
RAG Cache Mechanism), a novel, annotation-free caching framework that
dynamically manages small, high-value corpora for each agent. By synthesizing
historical query distribution patterns with the intrinsic geometry of cached
items in the embedding space, ARC automatically maintains a high-relevance
cache. With comprehensive experiments on three retrieval datasets, our
experimental results demonstrate that ARC reduces storage requirements to
0.015% of the original corpus while offering up to 79.8% has-answer rate and
reducing average retrieval latency by 80%. Our results demonstrate that ARC can
drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

</details>


### [2] [Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model](https://arxiv.org/abs/2511.02958)
*Cristian García-Romero,Miquel Esplà-Gomis,Felipe Sánchez-Martínez*

Main category: cs.CL

TL;DR: 提出一种利用多语言机器翻译模型内部表示来区分人工翻译和机器翻译句子的新方法，在非英语语言对上比现有技术准确率提升至少5个百分点。


<details>
  <summary>Details</summary>
Motivation: 现代机器翻译系统依赖从互联网收集的大规模平行语料，但其中大量文本是机器生成的翻译，过度依赖此类合成内容会显著降低翻译质量，因此过滤非人工翻译成为构建高质量MT系统的关键预处理步骤。

Method: 利用代理多语言机器翻译模型的内部表示来直接区分人工翻译和机器翻译句子。

Result: 实验结果显示该方法优于当前最先进技术，特别是在非英语语言对上，准确率提升至少5个百分点。

Conclusion: 该方法能有效识别机器翻译内容，为构建高质量机器翻译系统提供了重要的预处理工具。

Abstract: Modern machine translation (MT) systems depend on large parallel corpora,
often collected from the Internet. However, recent evidence indicates that (i)
a substantial portion of these texts are machine-generated translations, and
(ii) an overreliance on such synthetic content in training data can
significantly degrade translation quality. As a result, filtering out non-human
translations is becoming an essential pre-processing step in building
high-quality MT systems. In this work, we propose a novel approach that
directly exploits the internal representations of a surrogate multilingual MT
model to distinguish between human and machine-translated sentences.
Experimental results show that our method outperforms current state-of-the-art
techniques, particularly for non-English language pairs, achieving gains of at
least 5 percentage points of accuracy.

</details>


### [3] [LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation](https://arxiv.org/abs/2511.03001)
*Gyeom Hwangbo,Hyungjoo Chae,Minseok Kang,Hyeonjong Ju,Soohyun Oh,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出了LEGO-Eval评估框架和LEGO-Bench基准，用于评估细粒度指令与生成的3D场景之间的对齐程度，解决了现有方法在3D场景理解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的3D场景缺乏真实世界的空间布局和物体属性，这会导致训练出的具身智能体学习到与现实世界物理和语义不符的先验知识。现有评估方法如CLIPScore和VLMs无法可靠评估细粒度指令与场景的对齐。

Method: 开发了LEGO-Eval评估框架，配备多样化工具来显式地锚定场景组件，实现更准确的对齐评估。同时提出了LEGO-Bench基准，包含详细指令来指定真实世界环境的复杂布局和属性。

Result: 实验显示LEGO-Eval在评估场景-指令对齐方面比VLM-as-a-judge高出0.41 F1分数。使用LEGO-Bench基准测试发现当前生成方法存在显著局限性，所有评估方法的成功率最多只有10%。

Conclusion: LEGO-Eval能够更准确地评估3D场景生成与细粒度指令的对齐程度，揭示了当前生成方法在创建真实世界场景方面的严重不足。

Abstract: Despite recent progress in using Large Language Models (LLMs) for
automatically generating 3D scenes, generated scenes often lack realistic
spatial layouts and object attributes found in real-world environments. As this
problem stems from insufficiently detailed, coarse-grained instructions,
advancing 3D scene synthesis guided by more detailed, fine-grained instructions
that reflect real-world environments becomes crucial. Without such realistic
scenes, training embodied agents in unrealistic environments can lead them to
learn priors that diverge significantly from real-world physics and semantics,
degrading their performance when deployed. Thus, verifying the alignment
between the fine-grained instruction and the generated scene is essential for
effective learning. However, current evaluation methods, such as CLIPScore and
vision-language models (VLMs), often fail to reliably assess such alignment.
This shortcoming arises primarily from their shallow understanding of 3D
scenes, which often leads to improperly grounded scene components. To address
this, we introduce LEGO-Eval, an evaluation framework equipped with diverse
tools designed to explicitly ground scene components, enabling more accurate
alignment assessments. We also present LEGO-Bench, a benchmark of detailed
instructions that specify complex layouts and attributes of real-world
environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge
by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with
LEGO-Bench reveals significant limitations in current generation methods.
Across all evaluated approaches, success rates reached at most 10% in
generating scenes that fully align with fine-grained instructions.

</details>


### [4] [Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT](https://arxiv.org/abs/2511.03005)
*Hee-Jin Lee,Zhen Guo,Luchao Jin,Morteza Moazami Goudarzi*

Main category: cs.CL

TL;DR: ARF管道通过分析-修订-微调流程，使小型开源语言模型在客服摘要任务中超越大型专有模型，提升成本效率和数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决大型专有模型成本高、数据隐私问题，同时提升小型开源模型在客服摘要任务中的性能。

Method: 分析GPT-3.5摘要错误，使用Llama 3.1 70B进行针对性修订生成高质量训练数据，然后微调Llama 3.1 8B模型。

Result: 微调后的8B模型在摘要任务中表现优于GPT-3.5，实现了成本效率和数据隐私的平衡。

Conclusion: ARF管道为增强开源语言模型在各种下游应用中的性能提供了一个可推广的框架。

Abstract: We introduce an Analyze-Revise-Finetune (ARF) pipeline that enables smaller
open-source language models (LLMs) to surpass substantially larger proprietary
models in customer service summarization tasks. The pipeline first analyzes and
categorizes common errors in summaries produced by a teacher model (GPT-3.5),
then performs a targeted revision using a compact editor model (Llama 3.1 70B)
to generate high-quality, refined training data. Fine-tuning a smaller student
model (Llama 3.1 8B) on this refined data resulted in superior summarization
performance compared to GPT-3.5. The ARF pipeline improves cost efficiency and
data privacy while maintaining competitive accuracy, illustrating a
generalizable framework for enhancing open-source LLMs across diverse
downstream applications.

</details>


### [5] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taškova*

Main category: cs.CL

TL;DR: 提出了FTS-OBP评估方法解决ABSA任务中边界变化问题，研究了小型生成式语言模型在教育评论ABSA中的应用，并发布了首个教育评论ABSA资源集。


<details>
  <summary>Details</summary>
Motivation: ABSA研究集中在商业领域，而教育和医疗等高需求低资源领域缺乏研究；传统评估方法对边界变化过于严格；现有方法依赖资源密集型知识注入。

Method: 1) 提出FTS-OBP评估方法；2) 研究小型解码器生成语言模型，探索无数据和少数据微调方法，提出多任务微调策略；3) 发布教育评论ABSA资源集。

Result: FTS-OBP与传统指标强相关且提供细粒度诊断；1.5-3.8B参数模型在单GPU上仅用200-1000个样本即可超越专有大模型并接近基准结果。

Conclusion: 该方法解决了低资源领域ABSA的挑战，为教育和医疗等领域提供了可行的解决方案，推动了ABSA在非商业领域的应用。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [6] [ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment](https://arxiv.org/abs/2511.03048)
*Anthony Hevia,Sanjana Chintalapati,Veronica Ka Wai Lai,Thanh Tam Nguyen,Wai-Tat Wong,Terry Klassen,Lucy Lu Wang*

Main category: cs.CL

TL;DR: ROBOTO2是一个基于网络的开源平台，利用大语言模型辅助进行临床试验偏倚风险评估，通过交互式界面结合PDF解析、检索增强的LLM提示和人工审核，简化了传统的ROB2标注流程。


<details>
  <summary>Details</summary>
Motivation: 传统的ROB2风险评估过程劳动密集且耗时，需要自动化工具来简化这一关键的系统综述环节。

Method: 开发了交互式平台，结合PDF解析、检索增强的LLM提示和人工在环审核，用户可以上传临床试验报告，接收初步答案和证据支持，并提供实时反馈。

Result: 构建并发布了包含521份儿科临床试验报告的数据集（8954个信号问题，1202个证据段落），使用该数据集对4个LLM进行了ROB2性能基准测试。

Conclusion: ROBOTO2平台公开可用，代码和数据已发布以促进可重复性和采用，同时分析了当前模型能力和自动化这一关键系统综述环节的持续挑战。

Abstract: We present ROBOTO2, an open-source, web-based platform for large language
model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2
streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process
via an interactive interface that combines PDF parsing, retrieval-augmented LLM
prompting, and human-in-the-loop review. Users can upload clinical trial
reports, receive preliminary answers and supporting evidence for ROB2 signaling
questions, and provide real-time feedback or corrections to system suggestions.
ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and
data released to foster reproducibility and adoption. We construct and release
a dataset of 521 pediatric clinical trial reports (8954 signaling questions
with 1202 evidence passages), annotated using both manually and LLM-assisted
methods, serving as a benchmark and enabling future research. Using this
dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into
current model capabilities and ongoing challenges in automating this critical
aspect of systematic review.

</details>


### [7] [Reading Between the Lines: The One-Sided Conversation Problem](https://arxiv.org/abs/2511.03056)
*Victoria Ebert,Rishabh Singh,Tuochao Chen,Noah A. Smith,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 论文研究了单边对话问题(1SC)，提出了从单边对话记录中重建缺失对话和生成摘要的方法，在多个数据集上验证了提示和微调模型的效果。


<details>
  <summary>Details</summary>
Motivation: 在现实场景如远程医疗、呼叫中心和智能眼镜中，通常只能记录对话的一方，这限制了对话AI的应用。

Method: 使用提示和微调模型，探索利用未来对话轮次和话语长度信息来重建缺失对话，以及使用占位符提示来减少幻觉。

Result: 大型模型通过提示能生成有前景的重建结果，小型模型需要微调；高质量摘要可以在不重建缺失对话的情况下生成。

Conclusion: 1SC是一个新的挑战，研究结果标志着向隐私感知对话AI迈出了重要一步。

Abstract: Conversational AI is constrained in many real-world settings where only one
side of a dialogue can be recorded, such as telemedicine, call centers, and
smart glasses. We formalize this as the one-sided conversation problem (1SC):
inferring and learning from one side of a conversation. We study two tasks: (1)
reconstructing the missing speaker's turns for real-time use cases, and (2)
generating summaries from one-sided transcripts. Evaluating prompting and
finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B
testing and LLM-as-a-judge metrics, we find that access to one future turn and
information about utterance length improves reconstruction, placeholder
prompting helps to mitigate hallucination, and while large models generate
promising reconstructions with prompting, smaller models require finetuning.
Further, high-quality summaries can be generated without reconstructing missing
turns. We present 1SC as a novel challenge and report promising results that
mark a step toward privacy-aware conversational AI.

</details>


### [8] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: PolyNorm是一种基于提示的大语言模型文本归一化方法，通过自动数据管理和评估管道，在八种语言上相比生产级系统持续降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 传统文本归一化系统虽然准确率高，但需要大量工程工作、难以扩展、语言覆盖有限，特别是在低资源环境中。

Method: 使用基于提示的大语言模型方法，结合语言无关的自动数据管理和评估管道，减少对手动编写规则的依赖。

Result: 在八种语言的实验中，相比生产级系统持续降低了词错误率，并发布了多语言数据集PolyNorm-Benchmark。

Conclusion: PolyNorm提供了一种减少人工干预、具有更广泛语言适用性的文本归一化解决方案。

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [9] [A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures](https://arxiv.org/abs/2511.03089)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: cs.CL

TL;DR: 该研究使用计算语言学方法（惊奇度和语义连贯性）来量化精神分裂症患者的语言障碍，分析这些指标如何区分患者与健康对照组，以及如何随症状严重程度变化。


<details>
  <summary>Details</summary>
Motivation: 精神分裂症患者的语言障碍（如言语混乱和话语连贯性受损）反映了潜在的认知障碍，有潜力作为症状严重程度和诊断的客观标记。

Method: 通过计算模型计算语言中的惊奇度和语义连贯性这两个计算语言学指标，比较精神分裂症患者与健康对照组在这些指标上的差异。

Result: 研究发现惊奇度和语义连贯性能够区分精神分裂症患者与健康对照组，并且这些语言障碍指标随症状严重程度而变化。

Conclusion: 计算语言学指标（惊奇度和语义连贯性）可以作为量化精神分裂症语言障碍的有效工具，为症状评估和诊断提供客观依据。

Abstract: Language disruptions are one of the well-known effects of schizophrenia
symptoms. They are often manifested as disorganized speech and impaired
discourse coherence. These abnormalities in spontaneous language production
reflect underlying cognitive disturbances and have the potential to serve as
objective markers for symptom severity and diagnosis of schizophrenia. This
study focuses on how these language disruptions can be characterized in terms
of two computational linguistic measures: surprisal and semantic coherence. By
computing surprisal and semantic coherence of language using computational
models, this study investigates how they differ between subjects with
schizophrenia and healthy controls. Furthermore, this study provides further
insight into how language disruptions in terms of these linguistic measures
change with varying degrees of schizophrenia symptom severity.

</details>


### [10] [CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic](https://arxiv.org/abs/2511.03102)
*Saad Mankarious,Ayah Zirikly*

Main category: cs.CL

TL;DR: CARMA是首个自动标注的大规模阿拉伯语Reddit帖子数据集，涵盖六种心理健康状况，为阿拉伯语心理健康检测提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语人群的心理健康检测资源有限，且存在文化污名化问题，而现有研究主要集中在英语，阿拉伯语领域研究不足。

Method: 构建CARMA数据集，包含六种心理健康状况和对照组，进行定性和定量分析，并使用从浅层分类器到大语言模型的各种模型进行分类实验。

Result: CARMA在规模和多样性上超过现有资源，分类实验结果显示在阿拉伯语心理健康检测方面具有良好潜力。

Conclusion: 该研究展示了在阿拉伯语等代表性不足语言中推进心理健康检测的前景，为未来研究提供了重要数据集基础。

Abstract: Mental health disorders affect millions worldwide, yet early detection
remains a major challenge, particularly for Arabic-speaking populations where
resources are limited and mental health discourse is often discouraged due to
cultural stigma. While substantial research has focused on English-language
mental health detection, Arabic remains significantly underexplored, partly due
to the scarcity of annotated datasets. We present CARMA, the first
automatically annotated large-scale dataset of Arabic Reddit posts. The dataset
encompasses six mental health conditions, such as Anxiety, Autism, and
Depression, and a control group. CARMA surpasses existing resources in both
scale and diversity. We conduct qualitative and quantitative analyses of
lexical and semantic differences between users, providing insights into the
linguistic markers of specific mental health conditions. To demonstrate the
dataset's potential for further mental health analysis, we perform
classification experiments using a range of models, from shallow classifiers to
large language models. Our results highlight the promise of advancing mental
health detection in underrepresented languages such as Arabic.

</details>


### [11] [Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification](https://arxiv.org/abs/2511.03217)
*Shaghayegh Kolli,Richard Rosenbaum,Timo Cavelius,Lasse Strothe,Andrii Lata,Jana Diesner*

Main category: cs.CL

TL;DR: 提出了一种结合大语言模型、知识图谱和实时搜索代理的混合事实核查方法，在FEVER基准测试中达到0.93的F1分数，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成流畅但缺乏可靠的信息基础，而基于知识图谱的事实核查器虽然精确可解释，但覆盖范围有限或延迟较高。通过整合两者的优势，构建更可靠的事实核查系统。

Method: 采用三步自主流程：1) 知识图谱检索用于DBpedia中的快速单跳查找；2) 基于任务特定标签提示的LM分类，产生具有内部基于规则逻辑的输出；3) 当KG覆盖不足时调用网络搜索代理。

Result: 在FEVER基准测试的Supported/Refuted分割上达到0.93的F1分数，无需任务特定微调。重新标注研究表明，该方法经常为原本标记为"信息不足"的声明找到有效证据。

Conclusion: 提出了一个模块化、开源的事实核查管道，具有回退策略和跨数据集的泛化能力，有效结合了LLM、知识图谱和搜索代理的优势。

Abstract: Large language models (LLMs) excel in generating fluent utterances but can
lack reliable grounding in verified information. At the same time,
knowledge-graph-based fact-checkers deliver precise and interpretable evidence,
yet suffer from limited coverage or latency. By integrating LLMs with knowledge
graphs and real-time search agents, we introduce a hybrid fact-checking
approach that leverages the individual strengths of each component. Our system
comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid
one-hop lookups in DBpedia, 2) an LM-based classification guided by a
task-specific labeling prompt, producing outputs with internal rule-based
logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient.
Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the
Supported/Refuted split without task-specific fine-tuning. To address Not
enough information cases, we conduct a targeted reannotation study showing that
our approach frequently uncovers valid evidence for claims originally labeled
as Not Enough Information (NEI), as confirmed by both expert annotators and LLM
reviewers. With this paper, we present a modular, opensource fact-checking
pipeline with fallback strategies and generalization across datasets.

</details>


### [12] [Control Barrier Function for Aligning Large Language Models](https://arxiv.org/abs/2511.03121)
*Yuya Miyaoka,Masaki Inoue*

Main category: cs.CL

TL;DR: 提出了一个基于控制屏障函数的LLM对齐框架，通过安全过滤器干预文本生成，无需微调基线模型即可实现用户期望的文本生成。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在文本生成过程中可能产生不符合用户期望内容的问题，需要一种无需重新训练模型就能实现安全对齐的方法。

Method: 使用控制屏障函数(CBF)作为安全过滤器，对基线LLM生成的预测token进行干预，该过滤器是附加型设计，可直接应用于现有模型。

Result: 基于开源语言模型实现了整体文本生成系统，成功生成正面文本，验证了框架的有效性。

Conclusion: 提出的CBF安全过滤框架为LLM对齐提供了一种灵活有效的解决方案，无需模型微调即可确保生成用户期望的文本内容。

Abstract: This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.

</details>


### [13] [Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models](https://arxiv.org/abs/2511.03699)
*Francesco Corso,Francesco Pierri,Gianmarco De Francisci Morales*

Main category: cs.CL

TL;DR: 研究发现LLMs表现出部分阴谋论倾向，存在社会人口统计偏见，且容易通过提示词被操纵转向阴谋论观点。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否再现了人类高阶心理结构如阴谋论思维，这对理解LLMs的社会保真度和潜在风险至关重要。

Method: 使用经过验证的心理测量调查问卷，在不同提示和条件策略下对多个模型进行测试。

Result: LLMs对阴谋论元素表现出部分认同，社会人口属性条件化产生不均匀效应，暴露潜在人口偏见，且目标提示能轻易使模型转向阴谋论方向。

Conclusion: 需要批判性评估LLMs中嵌入的心理维度，以推进计算社会科学并制定针对有害使用的缓解策略。

Abstract: In this paper, we investigate whether Large Language Models (LLMs) exhibit
conspiratorial tendencies, whether they display sociodemographic biases in this
domain, and how easily they can be conditioned into adopting conspiratorial
perspectives. Conspiracy beliefs play a central role in the spread of
misinformation and in shaping distrust toward institutions, making them a
critical testbed for evaluating the social fidelity of LLMs. LLMs are
increasingly used as proxies for studying human behavior, yet little is known
about whether they reproduce higher-order psychological constructs such as a
conspiratorial mindset. To bridge this research gap, we administer validated
psychometric surveys measuring conspiracy mindset to multiple models under
different prompting and conditioning strategies. Our findings reveal that LLMs
show partial agreement with elements of conspiracy belief, and conditioning
with socio-demographic attributes produces uneven effects, exposing latent
demographic biases. Moreover, targeted prompts can easily shift model responses
toward conspiratorial directions, underscoring both the susceptibility of LLMs
to manipulation and the potential risks of their deployment in sensitive
contexts. These results highlight the importance of critically evaluating the
psychological dimensions embedded in LLMs, both to advance computational social
science and to inform possible mitigation strategies against harmful uses.

</details>


### [14] [MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity](https://arxiv.org/abs/2511.03146)
*Kaiyuan Zhang,Chenghao Yang,Zhoufutu Wen,Sihang Yuan,Qiuyue Wang,Chaoyi Huang,Guosheng Zhu,He Wang,Huawenyu Lu,Jianing Wen,Jianpeng Jiao,Lishu Luo,Longxiang Liu,Sijin Wu,Xiaolei Zhu,Xuanliang Zhang,Ge Zhang,Yi Lin,Guang Shi,Chaoyou Fu,Wenhao Huang*

Main category: cs.CL

TL;DR: MME-CC是一个多模态认知能力评估基准，专注于视觉中心认知行为，包含空间、几何和知识推理三大类11个任务，评估了16个主流MLLM的认知能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准过度强调文本推理或未能系统评估视觉中心认知行为，导致MLLM的认知能力评估不足。

Method: 构建MME-CC基准，将11个代表性推理任务分为空间、几何和知识推理三类，对16个代表性MLLM进行广泛实验。

Result: 闭源模型整体领先（如Gemini-2.5-Pro得42.66 vs GLM-4.5V得30.45），空间和几何推理普遍较弱（≤30%），识别出方向错误、跨视图身份持续性差等常见错误模式。

Conclusion: 希望推动将MLLM认知能力作为评估和模型设计的核心考量。

Abstract: As reasoning models scale rapidly, the essential role of multimodality in
human cognition has come into sharp relief, driving a growing need to probe
vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either
overemphasize textual reasoning or fall short of systematically capturing
vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs
insufficiently assessed. To address this limitation, we introduce MME-CC
(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded
benchmark that organizes 11 representative reasoning tasks into three
fundamental categories of visual information: spatial, geometric, and
knowledge-based reasoning, and provides fine-grained analyses of MLLMs'
cognitive capacity across these dimensions. Based on MME-CC, we conduct
extensive experiments over 16 representative MLLMs. Our study reveals that
closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.
30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak
(less than or equal to 30%). We further identify common error patterns,
including orientation mistakes, fragile cross-view identity persistence, and
poor adherence to counterfactual instructions, and observe that
Chain-of-Thought typically follows a three-stage process (extract -> reason ->
verify) with heavy reliance on visual extraction. We hope this work catalyzes a
shift toward treating the cognitive capacity of MLLMs as central to both
evaluation and model design.

</details>


### [15] [Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment](https://arxiv.org/abs/2511.03152)
*Srishti Yadav,Jasmina Gajcin,Erik Miehling,Elizabeth Daly*

Main category: cs.CL

TL;DR: 提出一个使用LLMs作为评判者来预测和解释AI系统风险的框架，通过Risk Atlas Nexus和GloVE解释方法生成利益相关者特定的可解释政策，展示不同利益相关者对相同风险的分歧。


<details>
  <summary>Details</summary>
Motivation: 理解不同利益相关者对AI系统风险的认知对于负责任部署至关重要，需要使LLM评估更加透明、可解释并与以人为中心的AI治理目标保持一致。

Method: 使用LLMs作为评判者预测风险，结合Risk Atlas Nexus和GloVE解释方法，生成利益相关者特定的可解释政策，并通过交互式可视化展示冲突推理。

Result: 在医疗AI、自动驾驶和欺诈检测三个真实用例中验证，结果显示利益相关者视角显著影响风险认知和冲突模式。

Conclusion: 强调利益相关者感知解释的重要性，使LLM评估更加透明、可解释，并与以人为中心的AI治理目标保持一致。

Abstract: Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.

</details>


### [16] [Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks](https://arxiv.org/abs/2511.03166)
*Kevin Wang,Subre Abdoul Moktar,Jia Li,Kangshuo Li,Feng Chen*

Main category: cs.CL

TL;DR: 对12种不确定性估计方法在LLM问答任务中的实证研究，发现在分布内数据中信息类方法表现最佳，分布外数据中密度方法和P(True)指标更有效，语义一致性方法在各种场景下表现稳定。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各行业的广泛应用，确保其输出的可信度至关重要，不确定性估计在其中扮演关键角色。需要系统评估不同UE方法在LLM中的鲁棒性和有效性。

Method: 使用12种不同的不确定性估计方法和4种生成质量指标（包括LLMScore），在问答任务的分布内和分布外数据集上评估LLM生成答案的不确定性。

Result: 信息类方法（基于token和序列概率）在分布内设置中表现优异，密度方法和P(True)指标在分布外场景中效果更好，语义一致性方法在不同数据集和生成指标下表现可靠。

Conclusion: 不同不确定性估计方法各有优势：信息类方法适合分布内数据，密度方法和P(True)适合分布外场景，语义一致性方法具有较好的通用性，但没有单一方法在所有情况下都是最优的。

Abstract: Large Language Models (LLMs) have become increasingly pervasive, finding
applications across many industries and disciplines. Ensuring the
trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE)
plays a key role. In this work, a comprehensive empirical study is conducted to
examine the robustness and effectiveness of diverse UE measures regarding
aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE
methods and four generation quality metrics including LLMScore from LLM
criticizers to evaluate the uncertainty of LLM-generated answers in
Question-Answering (QA) tasks on both in-distribution (ID) and
out-of-distribution (OOD) datasets. Our analysis reveals that information-based
methods, which leverage token and sequence probabilities, perform exceptionally
well in ID settings due to their alignment with the model's understanding of
the data. Conversely, density-based methods and the P(True) metric exhibit
superior performance in OOD contexts, highlighting their effectiveness in
capturing the model's epistemic uncertainty. Semantic consistency methods,
which assess variability in generated answers, show reliable performance across
different datasets and generation metrics. These methods generally perform well
but may not be optimal for every situation.

</details>


### [17] [Silenced Biases: The Dark Side LLMs Learned to Refuse](https://arxiv.org/abs/2511.03369)
*Rom Himelstein,Amit LeVi,Brit Youngmann,Yaniv Nemcovsky,Avi Mendelson*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Safety-aligned large language models (LLMs) are becoming increasingly
widespread, especially in sensitive applications where fairness is essential
and biased outputs can cause significant harm. However, evaluating the fairness
of models is a complex challenge, and approaches that do so typically utilize
standard question-answer (QA) styled schemes. Such methods often overlook
deeper issues by interpreting the model's refusal responses as positive
fairness measurements, which creates a false sense of fairness. In this work,
we introduce the concept of silenced biases, which are unfair preferences
encoded within models' latent space and are effectively concealed by
safety-alignment. Previous approaches that considered similar indirect biases
often relied on prompt manipulation or handcrafted implicit queries, which
present limited scalability and risk contaminating the evaluation process with
additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to
uncover these biases by employing activation steering to reduce model refusals
during QA. SBB supports easy expansion to new demographic groups and subjects,
presenting a fairness evaluation framework that encourages the future
development of fair models and tools beyond the masking effects of alignment
training. We demonstrate our approach over multiple LLMs, where our findings
expose an alarming distinction between models' direct responses and their
underlying fairness issues.

</details>


### [18] [BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture](https://arxiv.org/abs/2511.03180)
*Shahriyar Zaman Ridoy,Azmine Toushik Wasi,Koushik Ahamed Tonmoy*

Main category: cs.CL

TL;DR: BengaliMoralBench是首个针对孟加拉语的大规模伦理基准，涵盖5个道德领域和50个文化相关子主题，用于评估多语言大语言模型在孟加拉文化背景下的伦理对齐表现。


<details>
  <summary>Details</summary>
Motivation: 现有伦理基准主要基于英语和西方框架，忽视了南亚地区特别是孟加拉语（全球第六大语言，2.85亿使用者）的文化细微差别，导致多语言LLMs在实际部署时缺乏文化适应性。

Method: 构建包含日常活动、习惯、育儿、家庭关系和宗教活动5个道德领域的基准，每个场景通过母语者共识使用美德伦理、常识伦理和正义伦理三个伦理视角进行标注，并对主流多语言LLMs进行零样本评估。

Result: 多语言LLMs表现差异显著（准确率50-91%），定性分析显示在文化基础、常识推理和道德公平性方面存在一致弱点。

Conclusion: BengaliMoralBench为负责任的本土化提供了基础，支持在低资源多语言环境中部署伦理稳健的AI系统。

Abstract: As multilingual Large Language Models (LLMs) gain traction across South Asia,
their alignment with local ethical norms, particularly for Bengali, which is
spoken by over 285 million people and ranked 6th globally, remains
underexplored. Existing ethics benchmarks are largely English-centric and
shaped by Western frameworks, overlooking cultural nuances critical for
real-world deployment. To address this, we introduce BengaliMoralBench, the
first large-scale ethics benchmark for the Bengali language and socio-cultural
contexts. It covers five moral domains, Daily Activities, Habits, Parenting,
Family Relationships, and Religious Activities, subdivided into 50 culturally
relevant subtopics. Each scenario is annotated via native-speaker consensus
using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct
systematic zero-shot evaluation of prominent multilingual LLMs, including
Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and
standard metrics. Performance varies widely (50-91% accuracy), with qualitative
analysis revealing consistent weaknesses in cultural grounding, commonsense
reasoning, and moral fairness. BengaliMoralBench provides a foundation for
responsible localization, enabling culturally aligned evaluation and supporting
the deployment of ethically robust AI in diverse, low-resource multilingual
settings such as Bangladesh.

</details>


### [19] [LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval](https://arxiv.org/abs/2511.03214)
*Wenchang Lei,Ping Zou,Yue Wang,Feng Sun,Lei Zhao*

Main category: cs.CL

TL;DR: 提出语言图模型(LGM)来解决LLM在处理模糊或概念不对齐术语时的困难，通过提取元关系并验证，结合概念迭代检索算法，提升LLM的概念理解和响应准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语义理解方面表现强大，但当用户指令涉及模糊或概念不对齐的术语时表现不佳，需要增强概念清晰度。

Method: 从自然语言中提取继承、别名和组合等元关系，采用反射机制验证这些关系，结合概念迭代检索算法动态向LLM提供关系和描述。

Result: 在标准基准测试中，LGM始终优于现有的RAG基线方法。

Conclusion: LGM方法使大型语言模型能够处理任意长度的文本而无需截断，显著提升了概念解释和响应生成的准确性。

Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.

</details>


### [20] [Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval](https://arxiv.org/abs/2511.03228)
*Shantanu Agarwal,Joel Barry,Elizabeth Boschee,Scott Miller*

Main category: cs.CL

TL;DR: SARAL系统在MATERIAL项目的第三阶段评估中，在6个评估条件中的5个超越了其他团队，重点开发了面向文档集而非仅排序列表的跨语言信息检索方法。


<details>
  <summary>Details</summary>
Motivation: MATERIAL是IARPA旨在推进跨语言信息检索(CLIR)技术的倡议，需要开发能够检索查询相关文档集而不仅仅是排序文档列表的新方法。

Method: 开发了SARAL系统，采用新颖的跨语言信息检索方法，特别强调检索查询相关的文档集而非单一排序列表。

Result: 在MATERIAL第三阶段评估中，SARAL在涉及三种语言(波斯语、哈萨克语、格鲁吉亚语)的6个评估条件中的5个表现优于其他团队。

Conclusion: SARAL系统成功展示了其跨语言信息检索方法的有效性，特别是在检索相关文档集方面表现出色。

Abstract: Machine Translation for English Retrieval of Information in Any Language
(MATERIAL) is an IARPA initiative targeted to advance the state of
cross-lingual information retrieval (CLIR). This report provides a detailed
description of Information Sciences Institute's (ISI's) Summarization and
domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.
Specifically, we outline our team's novel approach to handle CLIR with emphasis
in developing an approach amenable to retrieve a query-relevant document
\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3
evaluations, SARAL exceeded the performance of other teams in five out of six
evaluation conditions spanning three different languages (Farsi, Kazakh, and
Georgian).

</details>


### [21] [IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs](https://arxiv.org/abs/2511.03237)
*Souvik Rana,Arul Menezes,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: IndicSuperTokenizer是一种针对印度多语言大模型的新型分词器，结合子词和多词分词方法，在22种印度语言上实现了最先进的生育率分数，推理吞吐量比LLaMA4提高44%。


<details>
  <summary>Details</summary>
Motivation: 为多语言大模型设计有效的分词器具有挑战性，特别是在印度语言这种脚本多样、形态变化丰富的场景中。现有子词方法在 multilingual 设置下的效果尚未充分探索。

Method: 结合子词和多词分词方法，采用语言特定的预分词策略，生成更符合语言学的分词结果。

Result: 在英语、22种印度语言和代码数据上评估，平均生育率分数比LLaMA4提高39.5%，比当前最佳模型Sutra提高18%，推理吞吐量比LLaMA4提高44%。

Conclusion: IndicSuperTokenizer通过创新的分词设计在多语言场景下取得了显著性能提升，证明了结合子词和多词分词方法的有效性。

Abstract: Tokenizers play a crucial role in determining the performance, training
efficiency, and the inference cost of Large Language Models (LLMs). Designing
effective tokenizers for multilingual LLMs is particularly challenging due to
diverse scripts and rich morphological variation. While subword methods such as
Byte Pair Encoding (BPE) are widely adopted, their effectiveness in
multilingual settings remains underexplored. We present IndicSuperTokenizer, a
tokenizer for Indic multilingual LLMs, that combines both subword and
multi-word tokenization, along with language-specific pre-tokenization, leading
to more linguistically aligned tokens and achieving a new state-of-the-art in
fertility score. Evaluated across English, 22 Indian languages and code data,
our tokenizer improves the average fertility score by 39.5% over LLaMA4 and by
18% over Sutra (the current best). This translates to 44% improvement in
inference throughput over LLaMA4 while maintaining comparable performance on
English and Indic benchmarks. We also present detailed ablations across
tokenizer training data size, vocabulary size, merging techniques, and
pre-tokenization strategies, demonstrating the robustness of our design
choices.

</details>


### [22] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: 本研究比较了四种开源LLM（Mistral-7b-instruct、LLaMa2-7b-chat、Falcon-7b-instruct、Orca-mini-v3-7b）和GPT-3.5在RAG增强的计算机科学文献问答任务中的表现。GPT-3.5表现最佳，而Mistral-7b-instruct在开源模型中表现最好。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术的重要性日益增加，需要比较不同LLM在问答任务中的性能表现，特别是在计算机科学文献领域。

Method: 使用RAG技术增强LLM，在计算机科学文献问答任务中评估四种开源LLM和GPT-3.5。评估指标包括准确率、精确度（二元问题）、专家排名、Gemini排名和余弦相似度（长答案问题）。

Result: GPT-3.5在RAG支持下表现最佳；在开源LLM中，Mistral-7b-instruct表现最好；Orca-mini-v3-7b响应延迟最短，LLaMa2-7b-chat延迟最长。

Conclusion: 开源LLM在适当基础设施支持下可以与GPT-3.5等专有模型相媲美，Mistral-7b-instruct是表现最好的开源模型。

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [23] [SCALE: Upscaled Continual Learning of Large Language Models](https://arxiv.org/abs/2511.03270)
*Jin-woo Lee,Junhwa Choi,Bongkyu Hwang,Jinho Choo,Bogun Kim,JeongSeon Yi,Joonseok Lee,DongYoung Jung,Jaeseon Park,Kyoungwon Park,Suk-hoon Jung*

Main category: cs.CL

TL;DR: SCALE是一种宽度扩展架构，通过向线性模块插入轻量级扩展来增加模型容量，同时冻结所有预训练参数，保持残差和注意力拓扑结构不变。


<details>
  <summary>Details</summary>
Motivation: 当前持续预训练的进展更多地依赖于扩展正确的结构而非仅仅扩展参数数量，需要一种既能增加容量又不干扰基础模型原始功能的方法。

Method: 引入SCALE架构，采用持久性保持原则（冻结预训练权重）和协作适应原则（选择性训练扩展组件），包括SCALE-Preserve、SCALE-Adapt和带路由扩展的SCALE-Route三种变体。

Result: 在合成传记基准测试中，SCALE缓解了深度扩展中观察到的严重遗忘问题；在韩语语料库的持续预训练中，SCALE变体在英语评估中表现出较少遗忘，在韩语基准测试中取得有竞争力的提升，提供了最佳的稳定性-可塑性权衡。

Conclusion: SCALE架构通过保持与适应的交互作用稳定了优化过程，相比标准持续学习设置具有更好的性能表现。

Abstract: We revisit continual pre-training for large language models and argue that
progress now depends more on scaling the right structure than on scaling
parameters alone. We introduce SCALE, a width upscaling architecture that
inserts lightweight expansion into linear modules while freezing all
pre-trained parameters. This preserves the residual and attention topologies
and increases capacity without perturbing the base model's original
functionality. SCALE is guided by two principles: Persistent Preservation,
which maintains the base model's behavior via preservation-oriented
initialization and freezing of the pre-trained weights, and Collaborative
Adaptation, which selectively trains a subset of expansion components to
acquire new knowledge with minimal interference. We instantiate these ideas as
SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and
SCALE-Route, an optional routing extension that performs token-level routing
between preservation and adaptation heads. On a controlled synthetic biography
benchmark, SCALE mitigates the severe forgetting observed with depth expansion
while still acquiring new knowledge. In continual pre-training on a Korean
corpus, SCALE variants achieve less forgetting on English evaluations and
competitive gains on Korean benchmarks, with these variants offering the best
overall stability-plasticity trade-off. Accompanying analysis clarifies when
preservation provably holds and why the interplay between preservation and
adaptation stabilizes optimization compared to standard continual learning
setups.

</details>


### [24] [How to Evaluate Speech Translation with Source-Aware Neural MT Metrics](https://arxiv.org/abs/2511.03295)
*Mauro Cettolo,Marco Gaido,Matteo Negri,Sara Papi,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本研究首次系统性地研究了语音翻译中的源感知评估指标，提出了两种生成音频文本代理的策略（ASR转录和回译），并引入了跨语言重分段算法来解决对齐问题。


<details>
  <summary>Details</summary>
Motivation: 传统的语音翻译评估仅比较翻译假设与参考译文，忽略了源音频中的有价值信息。机器翻译领域已证明包含源文本的神经指标与人类判断相关性更强，但将此扩展到语音翻译面临挑战，因为源是音频而非文本。

Method: 探索两种生成音频文本代理的策略：自动语音识别转录和参考译文的回译；提出新颖的两步跨语言重分段算法来解决合成源与参考译文之间的对齐不匹配问题。

Result: 在79种语言对和6种不同架构的语音翻译系统上的实验表明：当词错误率低于20%时，ASR转录比回译更可靠；回译始终是计算成本更低但仍有效的替代方案；跨语言重分段算法能够稳健地在语音翻译评估中使用源感知机器翻译指标。

Conclusion: 该研究为语音翻译评估提供了更准确和原则性的方法，通过源感知指标和跨语言重分段技术，显著提升了评估质量。

Abstract: Automatic evaluation of speech-to-text translation (ST) systems is typically
performed by comparing translation hypotheses with one or more reference
translations. While effective to some extent, this approach inherits the
limitation of reference-based evaluation that ignores valuable information from
the source input. In machine translation (MT), recent progress has shown that
neural metrics incorporating the source text achieve stronger correlation with
human judgments. Extending this idea to ST, however, is not trivial because the
source is audio rather than text, and reliable transcripts or alignments
between source and references are often unavailable. In this work, we conduct
the first systematic study of source-aware metrics for ST, with a particular
focus on real-world operating conditions where source transcripts are not
available. We explore two complementary strategies for generating textual
proxies of the input audio, automatic speech recognition (ASR) transcripts, and
back-translations of the reference translation, and introduce a novel two-step
cross-lingual re-segmentation algorithm to address the alignment mismatch
between synthetic sources and reference translations. Our experiments, carried
out on two ST benchmarks covering 79 language pairs and six ST systems with
diverse architectures and performance levels, show that ASR transcripts
constitute a more reliable synthetic source than back-translations when word
error rate is below 20%, while back-translations always represent a
computationally cheaper but still effective alternative. Furthermore, our
cross-lingual re-segmentation algorithm enables robust use of source-aware MT
metrics in ST evaluation, paving the way toward more accurate and principled
evaluation methodologies for speech translation.

</details>


### [25] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: 对两种领先的多模态大语言模型（Seed1.5-VL和Gemini-2.5-Flash）在医学应用中的"思考模式"能力进行评估，发现在大多数任务中，激活思考模式相比标准非思考模式的改进有限。


<details>
  <summary>Details</summary>
Motivation: 随着"双状态"多模态大语言模型的快速发展和采用，本研究旨在严格评估这些模型增强的推理过程如何影响临床任务中的模型性能和可靠性。

Method: 使用VQA-RAD和ROCOv2数据集，在四个视觉医学任务上评估两种领先MLLMs的主动"思考模式"能力。

Result: 研究发现，在大多数任务中，激活思考模式相比标准非思考模式的改进很小。在开放式VQA和医学图像解释等复杂医学任务上的表现仍然不理想。

Conclusion: 医学领域需要特定领域的医学数据和更先进的医学知识整合方法，以提升MLLMs在复杂医学任务中的表现。

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [26] [Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances](https://arxiv.org/abs/2511.03354)
*Riasad Alvi,Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Md Rafi Ur Rashid,Md Rafiqul Islam,Yakub Sebastian,Sami Azam*

Main category: cs.CL

TL;DR: 这篇综述系统评估了生成式AI在生物信息学中的进展，提出了六个研究问题，涵盖应用领域、模型架构、性能优势、技术改进、局限性和数据集支持等方面。


<details>
  <summary>Details</summary>
Motivation: 系统识别和评估生成式AI在生物信息学中的快速发展，评估其在方法论进步、预测性能和专业化方面的策略，并识别用于高级建模、数据密集型发现和整合生物学分析的有前景方法。

Method: 采用系统综述和元分析方法，提出六个研究问题(RQ1-RQ6)，涵盖应用多样性、模型架构比较、领域优势、技术改进、局限性和数据集支持等方面。

Result: 生成式AI在序列分析、分子设计和整合数据建模中表现优于传统方法；专业化模型架构优于通用模型；在分子分析和数据整合方面显著提升准确性；在结构建模、功能预测和合成数据生成方面取得改进；但存在可扩展性不足和数据偏见等限制。

Conclusion: 生成式AI已成为生物信息学的变革性方法，在多个子领域展现卓越性能，未来需关注稳健评估和生物学基础建模，以解决当前局限并推动进一步发展。

Abstract: Generative artificial intelligence (GenAI) has become a transformative
approach in bioinformatics that often enables advancements in genomics,
proteomics, transcriptomics, structural biology, and drug discovery. To
systematically identify and evaluate these growing developments, this review
proposed six research questions (RQs), according to the preferred reporting
items for systematic reviews and meta-analysis methods. The objective is to
evaluate impactful GenAI strategies in methodological advancement, predictive
performance, and specialization, and to identify promising approaches for
advanced modeling, data-intensive discovery, and integrative biological
analysis. RQ1 highlights diverse applications across multiple bioinformatics
subfields (sequence analysis, molecular design, and integrative data modeling),
which demonstrate superior performance over traditional methods through pattern
recognition and output generation. RQ2 reveals that adapted specialized model
architectures outperformed general-purpose models, an advantage attributed to
targeted pretraining and context-aware strategies. RQ3 identifies significant
benefits in the bioinformatics domains, focusing on molecular analysis and data
integration, which improves accuracy and reduces errors in complex analysis.
RQ4 indicates improvements in structural modeling, functional prediction, and
synthetic data generation, validated by established benchmarks. RQ5 suggests
the main constraints, such as the lack of scalability and biases in data that
impact generalizability, and proposes future directions focused on robust
evaluation and biologically grounded modeling. RQ6 examines that molecular
datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as
CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly
support the training and generalization of GenAI models.

</details>


### [27] [EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation](https://arxiv.org/abs/2511.03370)
*Yunbo Long,Yuhan Liu,Alexandra Brintrup*

Main category: cs.CL

TL;DR: EQ-Negotiator是一个新颖框架，通过情感角色建模使小型语言模型在信用谈判中达到甚至超越大型语言模型的性能，同时满足隐私保护和边缘计算需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动谈判中性能优异但计算成本高且需要数据隐私保护，不适合移动助手、嵌入式AI代理等隐私敏感场景。小型语言模型虽然实用，但在处理情感化复杂角色时存在性能差距。

Method: EQ-Negotiator框架整合了博弈论和隐马尔可夫模型，在线学习和跟踪债务人情感状态，无需预训练。该推理系统使小型语言模型具备战略智能，能够应对操纵行为、缓解冲突并维护道德标准。

Result: 通过多种信用谈判场景的广泛代理间模拟，包括对抗性债务人策略（如欺骗、威胁、扮演受害者），配备EQ-Negotiator的70亿参数语言模型在债务回收和谈判效率方面优于规模大10倍以上的基线大型语言模型。

Conclusion: 该研究将角色建模从描述性特征档案推进到动态情感架构，证明战略情商而非原始模型规模是自动谈判成功的关键因素，为在边缘设备上运行的有效、道德且保护隐私的AI谈判者铺平了道路。

Abstract: The deployment of large language models (LLMs) in automated negotiation has
set a high performance benchmark, but their computational cost and data privacy
requirements render them unsuitable for many privacy-sensitive, on-device
applications such as mobile assistants, embodied AI agents or private client
interactions. While small language models (SLMs) offer a practical alternative,
they suffer from a significant performance gap compared to LLMs in playing
emotionally charged complex personas, especially for credit negotiation. This
paper introduces EQ-Negotiator, a novel framework that bridges this capability
gap using emotional personas. Its core is a reasoning system that integrates
game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional
states online, without pre-training. This allows EQ-Negotiator to equip SLMs
with the strategic intelligence to counter manipulation while de-escalating
conflict and upholding ethical standards. Through extensive agent-to-agent
simulations across diverse credit negotiation scenarios, including adversarial
debtor strategies like cheating, threatening, and playing the victim, we show
that a 7B parameter language model with EQ-Negotiator achieves better debt
recovery and negotiation efficiency than baseline LLMs more than 10 times its
size. This work advances persona modeling from descriptive character profiles
to dynamic emotional architectures that operate within privacy constraints.
Besides, this paper establishes that strategic emotional intelligence, not raw
model scale, is the critical factor for success in automated negotiation,
paving the way for effective, ethical, and privacy-preserving AI negotiators
that can operate on the edge.

</details>


### [28] [LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning](https://arxiv.org/abs/2511.03372)
*Shenghao Li*

Main category: cs.CL

TL;DR: LFC-DA是一个符号逻辑控制的数据增强管道，通过将逻辑文本映射为命题表达式，编译规则库，并通过有界状态空间搜索发现有效公式，再将其语言化为自然语言问题，确保在命题逻辑下的多样性和逻辑严谨性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂逻辑数据增强中过度依赖人工标注成本高，以及直接使用大语言模型生成不可解释且逻辑同质化的问题。

Method: 提出LFC-DA符号逻辑控制管道：将逻辑文本映射为命题表达式，编译紧凑规则库，通过有界状态空间搜索系统性地发现有效公式，然后将其语言化为自然语言问题。

Result: 在ReClor和LogiQA数据集上的实验表明，预训练模型的逻辑推理准确性显著提高。

Conclusion: LFC-DA对于LLM引导的逻辑数据增强是有效的，能够确保逻辑多样性和严谨性。

Abstract: For complex logical data augmentation, heavy reliance on human annotation is
costly, whereas direct generation with large language models yields
uninterpretable and logically homogeneous examples. To address this, we present
LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to
propositional expressions, a compact rule library is compiled, and a bounded
state-space search systematically discovers valid formulas that are then
verbalized back into natural-language questions, ensuring both diversity and
logical rigor under propositional logic. Experiments on ReClor and LogiQA show
significant improvements in the logical-reasoning accuracy of pretrained
models, confirming the effectiveness of LFC-DA for LLM-guided logical data
augmentation.

</details>


### [29] [Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance](https://arxiv.org/abs/2511.03383)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: 研究发现对称BPE分词在机器翻译中并非最优，提出非对称BPE方法，在低资源场景下显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译研究通常使用对称的BPE分词方法，但这种方法在不同语言对和数据规模下可能不是最优选择。

Method: 通过在不同数据规模和语言对上测试BPE分词策略，比较对称和非对称BPE的性能。非对称BPE为源语言和目标语言设置不同的合并操作次数。

Result: 非对称BPE在低资源设置下显著优于对称BPE，在英语-印地语翻译中平均提升5.32、4.46和0.7 CHRF++。在6个额外语言对的12个系统中，10个系统观察到统计显著改进。

Conclusion: 源语言使用高NMO（4K-32K）和目标语言使用低NMO（0.5K-2K）的非对称BPE配置在低资源机器翻译中效果最佳。

Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set
of hyperparameters for word segmentation models, symmetric Byte Pair Encoding
(BPE), which applies the same number of merge operations (NMO) to train
tokenizers for both source and target languages. However, we demonstrate that
this uniform approach doesn't guarantee optimal MT performance across different
language pairs and data sizes. This work investigates BPE segmentation recipes
across various data volumes and language pairs to evaluate MT system
performance. We find that utilizing asymmetric BPE, where the source and target
languages have different NMOs, significantly improves results over the
symmetric approach, especially in low-resource settings (50K, 100K, and 500K
sentence pairs). Specifically, asymmetric BPE yield statistically significant
($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in
low-resource setups. We validated this trend across six additional language
pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut),
observing statistically significant improvement in 10 out of 12 systems
compared to symmetric BPE. Our findings indicate a high NMO for the source (4K
to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results,
particularly benefiting low-resource MT.

</details>


### [30] [Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties](https://arxiv.org/abs/2511.03407)
*Célian Ringwald,Fabien Gandon,Catherine Faron,Franck Michel,Hanna Abi Akl*

Main category: cs.CL

TL;DR: 本文研究了小语言模型在关系抽取中处理数据属性和对象属性的能力，发现稀有属性的长尾分布是关键瓶颈，并评估了多种策略来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究小语言模型在完整RDF图抽取中如何处理数据属性和对象属性，特别是解决稀有属性长尾分布带来的性能瓶颈问题。

Method: 评估了四种策略：分层抽样、加权损失、数据集缩放和基于模板的合成数据增强，以解决稀有属性分布不平衡问题。

Result: 最佳策略是构建一个训练集，其中每个属性的出现次数超过给定阈值，这样可以在不平衡的目标属性上实现均衡性能。

Conclusion: 研究结果为训练形状感知的小语言模型提供了实用指导，并为语义关系抽取的未来工作指明了有前景的方向。

Abstract: Small language models (SLMs) have shown promises for relation extraction (RE)
when extracting RDF triples guided by SHACL shapes focused on common datatype
properties. This paper investigates how SLMs handle both datatype and object
properties for a complete RDF graph extraction. We show that the key bottleneck
is related to long-tail distribution of rare properties. To solve this issue,
we evaluate several strategies: stratified sampling, weighted loss, dataset
scaling, and template-based synthetic data augmentation. We show that the best
strategy to perform equally well over unbalanced target properties is to build
a training set where the number of occurrences of each property exceeds a given
threshold. To enable reproducibility, we publicly released our datasets,
experimental results and code. Our findings offer practical guidance for
training shape-aware SLMs and highlight promising directions for future work in
semantic RE.

</details>


### [31] [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)
*Canhui Wu,Qiong Cao,Chao Xue,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 3TF框架通过思想训练和免思想推理，让模型在内部进行丰富推理的同时保持外部输出简洁，无需显式生成推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要压缩冗长的推理输出，但仍依赖显式推理。3TF旨在让模型内部化结构化推理，在推理时无需显式生成推理步骤。

Method: 首先训练混合模型支持推理和非推理模式，然后在CoT标注数据上进一步训练以内部化结构化推理，推理时使用非推理模式强制生成简洁输出。

Result: 3TF训练模型在免思想推理下，在推理基准测试中获得显著提升，证明高质量推理可以隐式学习和执行。

Conclusion: 3TF框架表明模型能够学习和执行隐式推理，无需显式逐步生成，为高效推理提供了新视角。

Abstract: Recent advances in large language models (LLMs) have leveraged explicit
Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most
existing methods primarily compress verbose reasoning outputs. These
Long-to-Short transformations aim to improve efficiency, but still rely on
explicit reasoning during inference. In this work, we introduce \textbf{3TF}
(\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree
inference), a framework for efficient reasoning that takes a Short-to-Long
perspective. We first train a hybrid model that can operate in both reasoning
and non-reasoning modes, and then further train it on CoT-annotated data to
internalize structured reasoning, while enforcing concise, thought-free outputs
at inference time using the no-reasoning mode. Unlike compression-based
approaches, 3TF improves the reasoning quality of non-reasoning outputs,
enabling models to perform rich internal reasoning implicitly while keeping
external outputs short. Empirically, 3TF-trained models obtain large
improvements on reasoning benchmarks under thought-free inference,
demonstrating that high quality reasoning can be learned and executed
implicitly without explicit step-by-step generation.

</details>


### [32] [Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG](https://arxiv.org/abs/2511.03410)
*Longpeng Qiu,Ting Li,Shuai Mao,Nan Yang,Xiaohui Yan*

Main category: cs.CL

TL;DR: QuestionRAG是一个解决问答系统中输入错误问题的框架，通过知识增强和强化学习对齐来提升LLMs的问题修正能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在处理有错误的输入问题时经常出现误解用户意图或过度修正的问题。

Method: 使用外部知识（如搜索结果、相关实体）增强输入以解决误解问题；使用强化学习对齐模型目标，确保精确修正而非简单改写。

Result: 知识增强对理解错误问题至关重要；基于RL的对齐方法比传统监督微调更有效，显著提升模型的指令遵循和泛化能力。

Conclusion: 通过整合知识增强和强化学习对齐，QuestionRAG充分释放了LLMs在问题修正任务中的潜力。

Abstract: Input errors in question-answering (QA) systems often lead to incorrect
responses. Large language models (LLMs) struggle with this task, frequently
failing to interpret user intent (misinterpretation) or unnecessarily altering
the original question's structure (over-correction). We propose QuestionRAG, a
framework that tackles these problems. To address misinterpretation, it
enriches the input with external knowledge (e.g., search results, related
entities). To prevent over-correction, it uses reinforcement learning (RL) to
align the model's objective with precise correction, not just paraphrasing. Our
results demonstrate that knowledge augmentation is critical for understanding
faulty questions. Furthermore, RL-based alignment proves significantly more
effective than traditional supervised fine-tuning (SFT), boosting the model's
ability to follow instructions and generalize. By integrating these two
strategies, QuestionRAG unlocks the full potential of LLMs for the question
correction task.

</details>


### [33] [CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field](https://arxiv.org/abs/2511.03441)
*Doria Bonzi,Alexandre Guiggi,Frédéric Béchet,Carlos Ramisch,Benoit Favre*

Main category: cs.CL

TL;DR: CareMedEval是一个专门用于评估LLMs在生物医学领域批判性评估和推理能力的数据集，包含534个基于37篇科学文章的问题，揭示了当前模型在批判性推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域需要批判性评估科学文献的能力，但现有LLMs在专业领域的批判性推理可靠性有限，需要专门的评估工具。

Method: 从法国医学生真实考试中提取数据构建CareMedEval数据集，包含534个基于37篇科学文章的问题，专门评估基于科学论文的批判性阅读和推理能力。

Result: 测试显示最先进的通用和专业生物医学LLMs表现不佳，准确率不超过0.5，特别是在研究局限性和统计分析问题上表现更差，尽管生成中间推理token能改善结果。

Conclusion: CareMedEval为基于证据的推理提供了具有挑战性的基准，暴露了当前LLMs的局限性，为未来开发自动化批判性评估支持工具铺平了道路。

Abstract: Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.

</details>


### [34] [Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction](https://arxiv.org/abs/2511.03466)
*Ringwald Celian,Gandon Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: Kastor框架通过RDF模式提取优化小语言模型，将单SHACL形状验证扩展到所有可能属性组合评估，通过迭代学习提升知识库完善能力。


<details>
  <summary>Details</summary>
Motivation: 满足专业领域知识库完善和精炼的需求，提升小语言模型在有限文本和RDF数据上的关系抽取性能。

Method: 将传统单SHACL形状验证重新表述为评估从形状派生的所有可能属性组合，为每个训练示例选择最优组合，并采用迭代学习过程精炼噪声知识库。

Result: 显著增强模型泛化能力和性能，能够创建能够发现新相关事实的鲁棒模型。

Conclusion: Kastor框架通过RDF模式提取方法有效提升了小语言模型在知识库完善任务中的表现，为专业领域知识管理提供了实用解决方案。

Abstract: RDF pattern-based extraction is a compelling approach for fine-tuning small
language models (SLMs) by focusing a relation extraction task on a specified
SHACL shape. This technique enables the development of efficient models trained
on limited text and RDF data. In this article, we introduce Kastor, a framework
that advances this approach to meet the demands for completing and refining
knowledge bases in specialized domains. Kastor reformulates the traditional
validation task, shifting from single SHACL shape validation to evaluating all
possible combinations of properties derived from the shape. By selecting the
optimal combination for each training example, the framework significantly
enhances model generalization and performance. Additionally, Kastor employs an
iterative learning process to refine noisy knowledge bases, enabling the
creation of robust models capable of uncovering new, relevant facts

</details>


### [35] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: 提出了BanglaSTEM数据集和基于T5的翻译模型，专门用于提高孟加拉语技术内容的英语翻译质量，特别是在STEM领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语技术问题解决上表现良好，但在孟加拉语问题上表现不佳。现有的孟加拉语-英语翻译系统在处理技术术语时存在困难，经常误译专业词汇，导致问题含义改变和错误答案。

Method: 创建了包含5,000个精心挑选的孟加拉语-英语句子对的BanglaSTEM数据集，涵盖计算机科学、数学、物理、化学和生物学等STEM领域。使用语言模型生成超过12,000个翻译，然后通过人工评估选择最高质量的翻译对。基于该数据集训练T5翻译模型。

Result: 在代码生成和数学问题解决两个任务上的测试结果显示，技术内容的翻译准确性显著提高，使孟加拉语使用者能够更有效地使用英语导向的语言模型。

Conclusion: BanglaSTEM数据集和训练好的翻译模型公开可用，显著改善了孟加拉语技术内容的翻译质量，为孟加拉语使用者提供了更好的技术问题解决工具。

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [36] [HaluMem: Evaluating Hallucinations in Memory Systems of Agents](https://arxiv.org/abs/2511.03506)
*Ding Chen,Simin Niu,Kehang Li,Peng Liu,Xiangping Zheng,Bo Tang,Xinchi Li,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 提出了HaluMem基准，这是首个针对记忆系统的操作级幻觉评估基准，包含三个评估任务，用于全面揭示不同操作阶段的幻觉行为。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统在存储和检索过程中经常出现记忆幻觉，包括捏造、错误、冲突和遗漏，但现有评估主要是端到端的问答，难以定位幻觉产生的具体操作阶段。

Method: 定义了三个评估任务（记忆提取、记忆更新和记忆问答），构建了用户中心的多轮人机交互数据集HaluMem-Medium和HaluMem-Long，包含约15k记忆点和3.5k多类型问题。

Result: 基于HaluMem的实证研究表明，现有记忆系统在提取和更新阶段倾向于产生和积累幻觉，这些错误随后会传播到问答阶段。

Conclusion: 未来研究应专注于开发可解释和受约束的记忆操作机制，系统性地抑制幻觉并提高记忆可靠性。

Abstract: Memory systems are key components that enable AI systems such as LLMs and AI
agents to achieve long-term learning and sustained interaction. However, during
memory storage and retrieval, these systems frequently exhibit memory
hallucinations, including fabrication, errors, conflicts, and omissions.
Existing evaluations of memory hallucinations are primarily end-to-end question
answering, which makes it difficult to localize the operational stage within
the memory system where hallucinations arise. To address this, we introduce the
Hallucination in Memory Benchmark (HaluMem), the first operation level
hallucination evaluation benchmark tailored to memory systems. HaluMem defines
three evaluation tasks (memory extraction, memory updating, and memory question
answering) to comprehensively reveal hallucination behaviors across different
operational stages of interaction. To support evaluation, we construct
user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and
HaluMem-Long. Both include about 15k memory points and 3.5k multi-type
questions. The average dialogue length per user reaches 1.5k and 2.6k turns,
with context lengths exceeding 1M tokens, enabling evaluation of hallucinations
across different context scales and task complexities. Empirical studies based
on HaluMem show that existing memory systems tend to generate and accumulate
hallucinations during the extraction and updating stages, which subsequently
propagate errors to the question answering stage. Future research should focus
on developing interpretable and constrained memory operation mechanisms that
systematically suppress hallucinations and improve memory reliability.

</details>


### [37] [One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework](https://arxiv.org/abs/2511.03508)
*Qi Jia,Kaiwei Zhang,Xiujie Song,Ye Shen,Xiangyang Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 提出了一个可扩展的多轮指令跟随评估框架EvolIF，通过三层机制解耦语言表面形式和用户意图模拟，动态构建包含状态变化和回溯的对话基准，评估LLM在多话题对话中的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常限制固定的对话轮数，容易饱和且无法考虑用户的交互体验，需要评估LLM在多轮对话中持续跟随用户指令的能力。

Method: 采用三层机制跟踪约束、指令和话题，模拟用户-LLM交互，动态构建基准测试，在模拟用户耐心耗尽时终止对话，并定义了一套交互过程质量指标。

Result: GPT-5表现出优越的指令跟随性能，平均维持18.54个对话轮次，稳健性达70.31%，显著优于Gemini-2.5-Pro 11.41%，其他模型表现差距较大。

Conclusion: 提出的框架能有效评估LLM的多轮指令跟随能力，GPT-5在该任务上表现最佳，所有数据和代码将公开。

Abstract: Understanding how well large language models can follow users' instructions
throughout a dialogue spanning multiple topics is of great importance for
data-intensive conversational applications. Existing benchmarks are often
limited to a fixed number of turns, making them susceptible to saturation and
failing to account for the user's interactive experience. In this work, we
propose an extensible framework for assessing multi-turn instruction-following
ability. At its core, our framework decouples linguistic surface forms from
user intent simulation through a three-layer mechanism that tracks constraints,
instructions, and topics. This framework mimics User-LLM interaction by
enabling the dynamic construction of benchmarks with state changes and
tracebacks, terminating a conversation only when the model exhausts a simulated
user's patience. We define a suite of metrics capturing the quality of the
interaction process. Using this framework, we construct EvolIF, an evolving
instruction-following benchmark incorporating nine distinct constraint types.
Our results indicate that GPT-5 exhibits superior instruction-following
performance. It sustains an average of 18.54 conversational turns and
demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant
margin of 11.41%, while other models lag far behind. All of the data and code
will be made publicly available online.

</details>


### [38] [SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties](https://arxiv.org/abs/2511.03542)
*Roberta Di Marino,Giovanni Dioguardi,Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Flora Amato,Vincenzo Moscato*

Main category: cs.CL

TL;DR: SOLVE-Med是一个多智能体架构，结合领域专业化的小语言模型来处理复杂医疗查询，在意大利医疗论坛数据上表现优异，超越单独大模型，同时支持本地部署。


<details>
  <summary>Details</summary>
Motivation: 解决医疗问答系统面临的幻觉、偏见、计算需求、隐私问题以及跨领域专业知识需求等部署挑战。

Method: 采用多智能体架构，包括路由代理动态选择专家、十个专业模型（各10亿参数）在特定医疗领域微调，以及协调代理合成响应。

Result: 在十个医疗专业领域的意大利医疗论坛数据评估中，SOLVE-Med取得ROUGE-1为0.301和BERTScore F1为0.697的优异表现，超越参数高达140亿的独立模型，同时支持本地部署。

Conclusion: SOLVE-Med通过多智能体专业化方法有效解决了医疗问答系统的关键挑战，在保持高性能的同时实现了本地部署的可行性。

Abstract: Medical question answering systems face deployment challenges including
hallucinations, bias, computational demands, privacy concerns, and the need for
specialized expertise across diverse domains. Here, we present SOLVE-Med, a
multi-agent architecture combining domain-specialized small language models for
complex medical queries. The system employs a Router Agent for dynamic
specialist selection, ten specialized models (1B parameters each) fine-tuned on
specific medical domains, and an Orchestrator Agent that synthesizes responses.
Evaluated on Italian medical forum data across ten specialties, SOLVE-Med
achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,
outperforming standalone models up to 14B parameters while enabling local
deployment. Our code is publicly available on GitHub:
https://github.com/PRAISELab-PicusLab/SOLVE-Med.

</details>


### [39] [Bearing Syntactic Fruit with Stack-Augmented Neural Networks](https://arxiv.org/abs/2511.03547)
*Brian DuSell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 本文展示了堆栈增强神经网络能够在无需语法监督、大规模预训练或过度训练的情况下，像人类一样进行层次化泛化。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络是否具有与人类儿童学习语言时相似的层次化语法规则偏好，而无需消歧示例。

Method: 测试了三种基础架构（transformer、简单RNN、LSTM）与两种堆栈风格（Joulin & Mikolov的叠加堆栈和DuSell & Chiang的非确定性泛化）的组合，并在经典问题形成任务上进行评估。

Result: 带有非确定性堆栈的transformer在这些架构中泛化效果最好，同时提出了改进堆栈RNN架构的方法以增强层次化泛化能力。

Conclusion: 堆栈增强神经网络可能是比标准架构更准确的人类语言习得模型，可作为心理语言学研究的有效工具。

Abstract: Any finite set of training data is consistent with an infinite number of
hypothetical algorithms that could have generated it. Studies have shown that
when human children learn language, they consistently favor hypotheses based on
hierarchical syntactic rules without ever encountering disambiguating examples.
A recent line of work has inquired as to whether common neural network
architectures share this bias, finding that they do so only under special
conditions: when syntactically supervised, when pre-trained on massive corpora,
or when trained long past convergence. In this paper, we demonstrate, for the
first time, neural network architectures that are able to generalize in
human-like fashion without any of the aforementioned requirements:
stack-augmented neural networks. We test three base architectures (transformer,
simple RNN, LSTM) augmented with two styles of stack: the superposition stack
of Joulin & Mikolov (2015) and a nondeterministic generalization of it proposed
by DuSell & Chiang (2023). We find that transformers with nondeterministic
stacks generalize best out of these architectures on a classical question
formation task. We also propose a modification to the stack RNN architecture
that improves hierarchical generalization. These results suggest that
stack-augmented neural networks may be more accurate models of human language
acquisition than standard architectures, serving as useful objects of
psycholinguistic study. Our code is publicly available.

</details>


### [40] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: 本文创建了多语言逻辑推理基准数据集MultiZebraLogic，包含9种日耳曼语言的斑马谜题，通过调整谜题大小、线索类型和干扰项来测试不同能力LLM的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 需要创建大规模高质量的多语言数据集来全面评估大型语言模型的逻辑推理能力，特别是在不同语言和难度级别下的表现。

Method: 生成多语言、多主题、多尺寸的斑马谜题，包含14种线索类型和8种干扰项类型，通过调整谜题大小和干扰项数量来控制难度。

Result: 发现2x3尺寸谜题对GPT-4o mini具有挑战性，4x5尺寸对o3-mini具有挑战性；5个干扰项使o3-mini在4x5谜题上的准确率下降15±7%；语言和主题对性能无显著影响；线索类型与难度无相关性。

Conclusion: 成功创建了可扩展的多语言逻辑推理基准数据集，为评估不同能力LLM的逻辑推理技能提供了有效工具。

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


### [41] [AILA--First Experiments with Localist Language Models](https://arxiv.org/abs/2511.03559)
*Joachim Diederich*

Main category: cs.CL

TL;DR: 本文首次实证展示了Transformer语言模型中的可控局部性，通过可调节的局部性参数实现表示局部化程度的连续控制，在可解释的局部编码和高效的分布式表示之间动态插值。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型仅依赖分布式表示，缺乏可解释性。本文旨在开发一种能够在高度可解释的局部编码和高效分布式表示之间动态切换的框架，满足监管领域对透明性和性能的双重需求。

Method: 使用两层Transformer架构在WikiText语料库上进行实验，系统性地改变局部性参数λ（从1.0完全局部到0.0完全分布式），通过注意力熵和指针保真度等指标评估模型性能。

Result: 局部配置显著降低注意力熵（λ=1.0时为5.36比特，λ=0.0时为7.18比特），同时保持更高的指针保真度。中间局部性值（λ=0.6）在可解释性和性能之间达到最优平衡，测试困惑度为4.65，准确率为84.7%。

Conclusion: 局部语言模型为需要透明性和能力的监管领域应用提供了实用框架，通过明确的惩罚阈值和信息论设计原则，实现了对可解释性-性能谱系的精确数学控制。

Abstract: This paper presents the first empirical demonstration of controllable
locality in transformer language models, a novel architectural framework that
enables continuous control over the degree of representation localization
through a tunable locality dial parameter. Unlike traditional language models
that rely exclusively on distributed representations, our approach allows
dynamic interpolation between highly interpretable localist encodings and
efficient distributed representations without requiring model retraining. We
conducted experiments on the WikiText corpus using a two-layer transformer
architecture, systematically varying the locality parameter {\lambda} across
the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our
results demonstrate that localist configurations achieve dramatically lower
attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18
bits at {\lambda} = 0.0, while maintaining substantially higher pointer
fidelity scores reflecting stronger alignment with rule-specified targets.
Prediction experiments reveal that intermediate locality values optimize the
tradeoff between interpretability and performance, with {\lambda} = 0.6
achieving test perplexity of 4.65 and accuracy of 84.7%. These findings
establish that localist language models provide a practical framework for
applications in regulated domains requiring both transparency and capability,
offering precise mathematical control over the interpretability-performance
spectrum through explicit penalty thresholds and information-theoretic design
principles.

</details>


### [42] [ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation](https://arxiv.org/abs/2511.03563)
*One Octadion,Bondan Sapta Prakoso,Nanang Yudi Setiawan,Novanto Yudistira*

Main category: cs.CL

TL;DR: 通过微调大语言模型并集成检索增强生成技术，开发了一个专门支持政策制定者理解和起草法律规章的工具。


<details>
  <summary>Details</summary>
Motivation: 帮助政策制定者更好地理解、分析和制定法律规章，满足法律领域对深度理解法律文本的特定需求。

Method: 1) 针对法律领域需求构建监督数据集进行模型微调；2) 集成检索增强生成方法，使模型能够访问和整合最新的外部法律知识。

Result: 该方法显著提升了法律研究和规章制定的效率，为政策制定者提供了有力的辅助工具。

Conclusion: 结合微调和RAG增强的方法能够有效支持法律领域工作，为不断发展的法律领域提供宝贵资源。

Abstract: In this study, we explore the fine-tuning of Large Language Models (LLMs) to
better support policymakers in their crucial work of understanding, analyzing,
and crafting legal regulations. To equip the model with a deep understanding of
legal texts, we curated a supervised dataset tailored to the specific needs of
the legal domain. Additionally, we integrated the Retrieval-Augmented
Generation (RAG) method, enabling the LLM to access and incorporate up-to-date
legal knowledge from external sources. This combination of fine-tuning and
RAG-based augmentation results in a tool that not only processes legal
information but actively assists policymakers in interpreting regulations and
drafting new ones that align with current needs. The results demonstrate that
this approach can significantly enhance the effectiveness of legal research and
regulation development, offering a valuable resource in the ever-evolving field
of law.

</details>


### [43] [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601)
*Chao Yan,Boyong Wu,Peng Yang,Pengfei Tan,Guoqiang Hu,Yuxin Zhang,Xiangyu,Zhang,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: Step-Audio-EditX是首个开源基于LLM的音频模型，擅长表达性和迭代式音频编辑，涵盖情感、说话风格和副语言特征，同时具备强大的零样本文本转语音能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要关注表示级解缠，需要嵌入先验或辅助模块，限制了音频编辑的表达性和迭代控制能力。

Method: 采用仅使用大边界合成数据的创新方法，无需嵌入先验或辅助模块，通过大边界学习实现迭代控制和高表达性。

Result: 评估结果显示，Step-Audio-EditX在情感编辑和其他细粒度控制任务上超越了MiniMax-2.6-hd和Doubao-Seed-TTS-2.0。

Conclusion: 大边界学习方法代表了从传统表示级解缠向更有效音频编辑范式的根本转变，实现了更好的表达性和控制能力。

Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

</details>


### [44] [A systematic review of relation extraction task since the emergence of Transformers](https://arxiv.org/abs/2511.03610)
*Ringwald Celian,Gandon,Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: 对2019-2024年间Transformer模型在关系抽取领域的系统综述，分析了34篇综述、64个数据集和104个模型，总结了方法进展、基准资源和语义网技术整合。


<details>
  <summary>Details</summary>
Motivation: 系统梳理Transformer时代关系抽取研究的发展脉络，为研究者和从业者提供全面的参考框架，理解该领域的演进和未来方向。

Method: 使用自动化框架收集和标注文献，对2019-2024年间的相关出版物进行系统性分析。

Result: 识别了当前趋势、局限性和开放挑战，整合了多维度结果，提供了关系抽取领域的全面概览。

Conclusion: 该研究为理解关系抽取的演变和未来方向提供了有价值的参考，突出了方法学进展和语义网技术的整合。

Abstract: This article presents a systematic review of relation extraction (RE)
research since the advent of Transformer-based models. Using an automated
framework to collect and annotate publications, we analyze 34 surveys, 64
datasets, and 104 models published between 2019 and 2024. The review highlights
methodological advances, benchmark resources, and the integration of semantic
web technologies. By consolidating results across multiple dimensions, the
study identifies current trends, limitations, and open challenges, offering
researchers and practitioners a comprehensive reference for understanding the
evolution and future directions of RE.

</details>


### [45] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: 提出了IRIS框架，一种可解释的零样本立场检测方法，通过隐式和显式推理来理解文本对未见目标的立场，无需真实推理标签即可提供内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有零样本立场检测方法在泛化性、文本与目标连贯性方面的不足，以及过度依赖显式推理、缺乏细微解释、难以解释模型预测的问题。

Method: 将立场检测视为信息检索排序任务，基于文本中的序列（隐式推理）和语言度量（显式推理）来理解输入对目标的态度，无需真实推理标签。

Result: 在VAST、EZ-STANCE、P-Stance和RFD基准数据集上使用50%、30%甚至10%训练数据进行的广泛实验证明了模型的泛化能力。

Conclusion: IRIS框架通过提出的架构和可解释设计，在零样本立场检测任务中展现出良好的泛化性能，提供了内在的可解释性。

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


### [46] [ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation](https://arxiv.org/abs/2511.03656)
*Jing Gao,Shutiao Luo,Yumeng Liu,Yuanming Li,Hongji Zeng*

Main category: cs.CL

TL;DR: 提出了中文多文档问答数据集ChiMDQA，涵盖6个领域的6,068个高质量问答对，支持文档理解、知识提取和智能问答等NLP任务。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理技术的发展，对高质量中文文档问答数据集的需求日益增长，需要专门针对下游业务场景的多领域数据集。

Method: 通过细致的文档筛选和系统化的问题设计方法构建数据集，涵盖学术、教育、金融、法律、医疗和新闻等六个领域，并将问答对进一步细分为十个精细类别。

Result: 创建了包含6,068个高质量问答对的ChiMDQA数据集，保证了数据的多样性和高质量，适用于多种NLP任务。

Conclusion: ChiMDQA数据集为中文问答系统的未来研究和实际应用提供了重要基础，代码和数据已公开。

Abstract: With the rapid advancement of natural language processing (NLP) technologies,
the demand for high-quality Chinese document question-answering datasets is
steadily growing. To address this issue, we present the Chinese Multi-Document
Question Answering Dataset(ChiMDQA), specifically designed for downstream
business scenarios across prevalent domains including academic, education,
finance, law, medical treatment, and news. ChiMDQA encompasses long-form
documents from six distinct fields, consisting of 6,068 rigorously curated,
high-quality question-answer (QA) pairs further classified into ten
fine-grained categories. Through meticulous document screening and a systematic
question-design methodology, the dataset guarantees both diversity and high
quality, rendering it applicable to various NLP tasks such as document
comprehension, knowledge extraction, and intelligent QA systems. Additionally,
this paper offers a comprehensive overview of the dataset's design objectives,
construction methodologies, and fine-grained evaluation system, supplying a
substantial foundation for future research and practical applications in
Chinese QA. The code and data are available at:
https://anonymous.4open.science/r/Foxit-CHiMDQA/.

</details>


### [47] [Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask](https://arxiv.org/abs/2511.03718)
*Nan Li,Albert Gatt,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文提出了一个视角主义标注方案，用于分析协作对话中的指称误解问题，通过分别捕捉说话者和听话者的理解来追踪理解的形成、分歧和修复过程。


<details>
  <summary>Details</summary>
Motivation: 在非对称协作对话中，参与者可能认为自己达成共识但实际上指称不同实体，需要研究这种误解如何产生和修复。

Method: 使用方案约束的LLM标注流程对HCRC MapTask语料库进行标注，获得13k个标注的指称表达，并分析理解状态。

Result: 结果显示，一旦统一词汇变体，完全误解很少见，但多重性差异会系统性地导致分歧，表明表面上的共识可能掩盖指称错位。

Conclusion: 该框架为研究基础误解和评估(V)LLMs在协作对话中建模视角依赖基础的能力提供了资源和分析视角。

Abstract: Collaborative dialogue relies on participants incrementally establishing
common ground, yet in asymmetric settings they may believe they agree while
referring to different entities. We introduce a perspectivist annotation scheme
for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures
speaker and addressee grounded interpretations for each reference expression,
enabling us to trace how understanding emerges, diverges, and repairs over
time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k
annotated reference expressions with reliability estimates and analyze the
resulting understanding states. The results show that full misunderstandings
are rare once lexical variants are unified, but multiplicity discrepancies
systematically induce divergences, revealing how apparent grounding can mask
referential misalignment. Our framework provides both a resource and an
analytic lens for studying grounded misunderstanding and for evaluating
(V)LLMs' capacity to model perspective-dependent grounding in collaborative
dialogue.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [48] [LiveTradeBench: Seeking Real-World Alpha with Large Language Models](https://arxiv.org/abs/2511.03628)
*Haofei Yu,Fenghai Li,Jiaxuan You*

Main category: q-fin.TR

TL;DR: LiveTradeBench是一个实时交易环境，用于在动态市场条件下评估LLM代理的决策能力，填补了静态基准测试与真实世界能力之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要在静态设置中进行，缺乏真实动态性和不确定性，无法评估在不确定性下的决策能力。

Method: 设计LiveTradeBench遵循三个原则：实时数据流、投资组合管理抽象、多市场评估（美股和预测市场），让LLM代理在实时市场环境中进行投资决策。

Result: 对21个LLM进行50天实时评估发现：(1)高LMArena分数不意味着更好的交易结果；(2)模型展现出不同的投资组合风格；(3)部分LLM能有效利用实时信号调整决策。

Conclusion: 静态评估与真实世界能力存在差距，需要测试顺序决策和在实时不确定性下一致性的基准。

Abstract: Large language models (LLMs) achieve strong performance across
benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but
these tests occur in static settings, lacking real dynamics and uncertainty.
Consequently, they evaluate isolated reasoning or problem-solving rather than
decision-making under uncertainty. To address this, we introduce
LiveTradeBench, a live trading environment for evaluating LLM agents in
realistic and evolving markets. LiveTradeBench follows three design principles:
(i) Live data streaming of market prices and news, eliminating dependence on
offline backtesting and preventing information leakage while capturing
real-time uncertainty; (ii) a portfolio-management abstraction that extends
control from single-asset actions to multi-asset allocation, integrating risk
management and cross-asset reasoning; and (iii) multi-market evaluation across
structurally distinct environments--U.S. stocks and Polymarket prediction
markets--differing in volatility, liquidity, and information flow. At each
step, an agent observes prices, news, and its portfolio, then outputs
percentage allocations that balance risk and return. Using LiveTradeBench, we
run 50-day live evaluations of 21 LLMs across families. Results show that (1)
high LMArena scores do not imply superior trading outcomes; (2) models display
distinct portfolio styles reflecting risk appetite and reasoning dynamics; and
(3) some LLMs effectively leverage live signals to adapt decisions. These
findings expose a gap between static evaluation and real-world competence,
motivating benchmarks that test sequential decision making and consistency
under live uncertainty.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: FATE是一个新的形式代数定理评估基准系列，包含FATE-H和FATE-X两个组件，每个包含100个抽象代数和交换代数问题，难度从本科练习到超过博士资格考试水平。评估显示当前最先进的LLM证明器在该基准上表现很差，揭示了自然语言推理与形式化推理之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基于竞赛的数学基准（如IMO）无法反映现代数学研究的深度、广度和抽象性，需要建立能评估高级数学推理能力的基准。

Method: 引入FATE基准系列，包含FATE-H和FATE-X两个组件，每个包含100个问题，涵盖抽象代数和交换代数。采用两阶段评估方法，分别评估自然语言推理能力和形式化推理能力，并系统分类形式化过程中的常见错误。

Result: 最佳模型在FATE-H上仅达到3%（pass@64）准确率，在FATE-X上为0%。研究发现模型的自然语言推理明显比形式化推理更准确，专门化证明器在自然语言阶段的反思能力可能不如通用模型。

Conclusion: FATE提供了一个稳健且具有挑战性的基准，为研究级形式数学推理的发展路径建立了重要检查点。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [50] [Stochastic Deep Graph Clustering for Practical Group Formation](https://arxiv.org/abs/2511.02879)
*Junhyung Park,Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.LG

TL;DR: DeepForm是一个用于动态群组推荐的框架，通过随机深度图聚类解决实时群组形成问题，能够自适应调整群组数量并提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的群组推荐系统主要关注推荐准确性，但假设群组是静态或预定义的，不适合动态现实场景。因此需要解决群组形成这一核心挑战。

Method: 采用轻量级GCN架构捕捉高阶用户信息，使用随机聚类学习实现无需重新训练的自适应群组重构，并通过对比学习在动态条件下优化群组。

Result: 在多个数据集上的实验表明，DeepForm在群组形成质量、效率和推荐准确性方面均优于各种基线方法。

Conclusion: DeepForm成功解决了动态群组推荐中的群组形成问题，满足了实时性、自适应性和高阶信息整合等关键需求。

Abstract: While prior work on group recommender systems (GRSs) has primarily focused on
improving recommendation accuracy, most approaches assume static or predefined
groups, making them unsuitable for dynamic, real-world scenarios. We reframe
group formation as a core challenge in GRSs and propose DeepForm (Stochastic
Deep Graph Clustering for Practical Group Formation), a framework designed to
meet three key operational requirements: (1) the incorporation of high-order
user information, (2) real-time group formation, and (3) dynamic adjustment of
the number of groups. DeepForm employs a lightweight GCN architecture that
effectively captures high-order structural signals. Stochastic cluster learning
enables adaptive group reconfiguration without retraining, while contrastive
learning refines groups under dynamic conditions. Experiments on multiple
datasets demonstrate that DeepForm achieves superior group formation quality,
efficiency, and recommendation accuracy compared with various baselines.

</details>


### [51] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: 通过从在公开ARC任务上预训练的微小递归模型出发，可以在比赛允许的计算限制内高效地对竞赛任务进行微调，最终在半私有评估任务上达到6.67%的分数。


<details>
  <summary>Details</summary>
Motivation: 在2025年ARC Prize竞赛结束前，领先的开源方法TRM虽然能在公开ARC AGI II评估集上获得约7.8%的分数，但其计算需求远超比赛允许的限制。

Method: 首先在1,280个公开任务上预训练一个微小递归模型（700k+优化器步骤，48小时，4xH100 SXM GPU），然后在竞赛期间仅用12,500次梯度步骤进行后训练，采用完全微调而非LoRA微调或仅微调任务嵌入。

Result: 预训练模型在公开评估集上获得约10%的分数，后训练后在半私有评估任务上达到6.67%的分数。

Conclusion: 从预训练的微小递归模型出发进行高效微调，可以在计算限制内实现有竞争力的性能，证明了该方法在资源受限环境下的有效性。

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [52] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: 提出了一个AI辅助框架，利用海面温度和叶绿素浓度等海洋参数预测北印度洋的潜在渔区，以帮助渔民提高捕鱼效率。


<details>
  <summary>Details</summary>
Motivation: 北印度洋是沿海社区重要的生计来源，但渔民在寻找高产渔场时面临不确定性，需要更准确的渔区预测方法。

Method: 使用AI框架分析海洋参数（海面温度和叶绿素浓度）来识别潜在渔区，提供区域特定的可持续捕鱼实践洞察。

Result: 初步结果显示该框架能够支持渔民减少搜索时间、降低燃料消耗，并促进资源的高效利用。

Conclusion: 该AI辅助框架有望通过提高渔区识别准确性来支持可持续渔业发展，为渔民提供实用的决策支持。

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [53] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 提出使用大语言模型进行零样本、单样本和少样本学习，通过角色扮演和逐步推理来检测和净化可穿戴物联网系统中的人类活动识别数据中毒攻击。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备在物联网中的广泛应用，人类活动识别系统面临数据中毒攻击的威胁，传统防御方法需要大量标注数据和特定任务训练，难以适应动态的物联网环境。

Method: 采用大语言模型框架，结合角色扮演提示（让LLM扮演专家角色）和逐步推理策略，从原始传感器数据中推断中毒指标并生成干净的替代数据。

Result: 通过广泛评估验证了框架在检测精度、净化质量、延迟和通信成本方面的有效性，证明了大语言模型在提升可穿戴物联网系统安全性和可靠性方面的实用性。

Conclusion: 大语言模型能够减少对大量数据集的依赖，为可穿戴物联网系统提供实时、鲁棒且适应性强的防御机制，显著提升系统安全性和可靠性。

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [54] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: 使用Llama 3.1-405B大语言模型自动生成基因组数据集在科学文献中的使用案例标签，避免人工标注和传统机器学习训练数据开发的高成本。


<details>
  <summary>Details</summary>
Motivation: 近年来识别特定数据集与引用它们的科学文献之间关联的需求增加，需要探索数据在文献中的具体使用方式，而大语言模型提供了规模化描述数据使用案例的潜力。

Method: 应用开源LLM Llama 3.1-405B对已知引用特定基因组数据集的文献生成结构化数据使用案例标签，并引入新的评估框架来验证方法效果。

Result: 零样本数据引用分类任务中，基础模型达到F1分数0.674，无需预定义类别，结果有前景但受限于数据可用性、提示过拟合、计算基础设施和负责任性能评估的成本等障碍。

Conclusion: 大语言模型在自动标注科学文献数据使用案例方面具有潜力，但实际应用仍面临数据可用性、计算成本和评估方法等方面的挑战。

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [55] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文针对ROGUE多臂老虎机框架开发了ROGUE-TS算法，通过概率裁剪平衡个性化推荐与群体效应学习，在微随机试验中实现低遗憾和高统计功效。


<details>
  <summary>Details</summary>
Motivation: 现有算法在ROGUE框架下过度强调利用而忽视探索，限制了群体效应估计能力，这在微随机试验中尤为重要。

Method: 开发ROGUE-TS汤普森采样算法，引入概率裁剪程序来平衡个性化与群体学习，提供遗憾与最小探索概率的量化权衡。

Result: 在两个微随机试验数据集上的验证显示，该方法比现有方法获得更低遗憾，通过裁剪程序保持高统计功效且不显著增加遗憾。

Conclusion: 该框架为微随机试验设计者提供了平衡个性化与统计有效性的实用指导，能够可靠检测治疗效果同时考虑个体行为动态。

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [56] [Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks](https://arxiv.org/abs/2511.02957)
*Mohsin Mahmud Topu,Mahfuz Ahmed Anik,Azmine Toushik Wasi,Md Manjurul Ahsan*

Main category: cs.LG

TL;DR: 提出了一个结合数字孪生和图神经网络的统一框架，用于可扩展的、数据驱动的路面健康监测和预测性维护。


<details>
  <summary>Details</summary>
Motivation: 传统路面管理系统主要是被动的，缺乏实时智能来预防故障和优化维护计划。路面基础设施监测面临复杂的空间依赖性、变化的环境条件以及道路网络中的非线性劣化挑战。

Method: 将路面段和空间关系建模为图节点和边，实时无人机、传感器和激光雷达数据流入数字孪生。归纳式图神经网络从图结构输入中学习劣化模式，以预测路面损坏并实现主动干预。

Result: 在具有路段属性和动态连接性的真实世界启发数据集上训练，模型达到R²为0.3798，优于基线回归器，有效捕捉非线性退化。还开发了交互式仪表板和强化学习模块用于模拟、可视化和自适应维护规划。

Conclusion: 数字孪生与图神经网络的集成提高了预测精度，并建立了持续改进的闭环反馈循环，为主动、智能和可持续的路面管理奠定了基础。

Abstract: Pavement infrastructure monitoring is challenged by complex spatial
dependencies, changing environmental conditions, and non-linear deterioration
across road networks. Traditional Pavement Management Systems (PMS) remain
largely reactive, lacking real-time intelligence for failure prevention and
optimal maintenance planning. To address this, we propose a unified Digital
Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven
pavement health monitoring and predictive maintenance. Pavement segments and
spatial relations are modeled as graph nodes and edges, while real-time UAV,
sensor, and LiDAR data stream into the DT. The inductive GNN learns
deterioration patterns from graph-structured inputs to forecast distress and
enable proactive interventions. Trained on a real-world-inspired dataset with
segment attributes and dynamic connectivity, our model achieves an R2 of
0.3798, outperforming baseline regressors and effectively capturing non-linear
degradation. We also develop an interactive dashboard and reinforcement
learning module for simulation, visualization, and adaptive maintenance
planning. This DT-GNN integration enhances forecasting precision and
establishes a closed feedback loop for continuous improvement, positioning the
approach as a foundation for proactive, intelligent, and sustainable pavement
management, with future extensions toward real-world deployment, multi-agent
coordination, and smart-city integration.

</details>


### [57] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru Pădurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: 提出UserAlign方法，通过少量成对响应比较来获取用户偏好，在推理时实现个性化对齐，基于逻辑赌博机的最佳臂识别理论框架。


<details>
  <summary>Details</summary>
Motivation: 现有个性化对齐方法需要大量用户偏好查询或要求将偏好明确指定为文本输入，存在效率和使用便捷性问题。

Method: 基于逻辑赌博机的最佳臂识别理论框架，将用户反馈视为一致且无噪声，从固定响应池中选择个性化响应。

Result: 在个性化文本和图像生成任务上的实验结果表明，UserAlign在实现个性化对齐方面具有有效性。

Conclusion: UserAlign通过少量成对比较就能有效实现生成模型响应与用户偏好的个性化对齐。

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [58] [Value of Information-Enhanced Exploration in Bootstrapped DQN](https://arxiv.org/abs/2511.02969)
*Stergios Plataniotis,Charilaos Akasiadis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 该论文将期望信息价值(EVOI)概念整合到Bootstrapped DQN框架中，开发了两种新算法来增强深度强化学习中的探索能力，在稀疏奖励的复杂Atari游戏中表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统探索策略如ε-greedy和Boltzmann方法在高维状态和稀疏奖励环境中难以有效平衡探索与利用，需要更高效的深度探索方法。

Method: 将期望信息价值(EVOI)整合到Bootstrapped DQN中，利用不同网络头之间的意见差异来估计信息价值，引导探索到最有潜力的区域。

Result: 在复杂稀疏奖励的Atari游戏中，新算法表现出更高的性能，更好地利用了随机网络初始化产生的不确定性，且无需引入额外超参数。

Conclusion: 基于期望信息价值的探索方法能够有效增强深度强化学习算法的探索能力，在复杂环境中实现更好的性能表现。

Abstract: Efficient exploration in deep reinforcement learning remains a fundamental
challenge, especially in environments characterized by high-dimensional states
and sparse rewards. Traditional exploration strategies that rely on random
local policy noise, such as $\epsilon$-greedy and Boltzmann exploration
methods, often struggle to efficiently balance exploration and exploitation. In
this paper, we integrate the notion of (expected) value of information (EVOI)
within the well-known Bootstrapped DQN algorithmic framework, to enhance the
algorithm's deep exploration ability. Specifically, we develop two novel
algorithms that incorporate the expected gain from learning the value of
information into Bootstrapped DQN. Our methods use value of information
estimates to measure the discrepancies of opinions among distinct network
heads, and drive exploration towards areas with the most potential. We evaluate
our algorithms with respect to performance and their ability to exploit
inherent uncertainty arising from random network initialization. Our
experiments in complex, sparse-reward Atari games demonstrate increased
performance, all the while making better use of uncertainty, and, importantly,
without introducing extra hyperparameters.

</details>


### [59] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan Günnemann*

Main category: cs.LG

TL;DR: GraphBSI是一种基于贝叶斯样本推理(BSI)的一次性图生成模型，通过在分布参数的连续空间中迭代优化图结构的信念来处理离散结构，在分子和合成图生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 图结构数据在分子生成、知识图谱和网络分析中很重要，但其离散、无序的特性使得传统生成模型难以处理，因此需要离散扩散和流匹配模型。

Method: GraphBSI基于贝叶斯样本推理(BSI)，在分布参数的连续空间中迭代优化图结构的信念，而不是直接演化样本。将BSI表述为随机微分方程(SDE)，并推导出通过分数函数近似保持边缘分布的可控噪声SDE族。

Result: 在分子和合成图生成的标准基准测试Moses和GuacaMol上，GraphBSI表现优于现有的一次性图生成模型，达到最先进性能。

Conclusion: GraphBSI通过贝叶斯样本推理方法有效处理离散图结构生成问题，理论分析揭示了与贝叶斯流网络和扩散模型的联系，并在多个基准测试中验证了其优越性能。

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [60] [Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation](https://arxiv.org/abs/2511.03012)
*Hongrui Chen,Liwei Wang,Levent Burak Kara*

Main category: cs.LG

TL;DR: 提出基于神经网络的超材料设计框架，通过连续双尺度表示解决异构单元设计中的兼容性问题，无需预定义数据集即可生成高分辨率结构。


<details>
  <summary>Details</summary>
Motivation: 传统异构超材料设计方法面临巨大设计空间和相邻单元兼容性挑战，现有方法要么计算昂贵，要么受限于固定数据集且需要后处理来确保连接性。

Method: 使用多尺度神经表示，神经网络同时输入全局和局部坐标，输出表示多尺度结构的隐式场，通过兼容性损失项训练确保相邻单元连接性。

Result: 训练后的网络可生成任意高分辨率的超材料设计，实现无限上采样，在力学超材料设计、负泊松比和力学隐身问题上验证了有效性。

Conclusion: 该框架成功解决了异构超材料设计中的兼容性问题，为机器人、生物工程和航空航天等领域的应用提供了有效解决方案。

Abstract: Metamaterials are engineered materials composed of specially designed unit
cells that exhibit extraordinary properties beyond those of natural materials.
Complex engineering tasks often require heterogeneous unit cells to accommodate
spatially varying property requirements. However, designing heterogeneous
metamaterials poses significant challenges due to the enormous design space and
strict compatibility requirements between neighboring cells. Traditional
concurrent multiscale design methods require solving an expensive optimization
problem for each unit cell and often suffer from discontinuities at cell
boundaries. On the other hand, data-driven approaches that assemble structures
from a fixed library of microstructures are limited by the dataset and require
additional post-processing to ensure seamless connections. In this work, we
propose a neural network-based metamaterial design framework that learns a
continuous two-scale representation of the structure, thereby jointly
addressing these challenges. Central to our framework is a multiscale neural
representation in which the neural network takes both global (macroscale) and
local (microscale) coordinates as inputs, outputting an implicit field that
represents multiscale structures with compatible unit cell geometries across
the domain, without the need for a predefined dataset. We use a compatibility
loss term during training to enforce connectivity between adjacent unit cells.
Once trained, the network can produce metamaterial designs at arbitrarily high
resolution, hence enabling infinite upsampling for fabrication or simulation.
We demonstrate the effectiveness of the proposed approach on mechanical
metamaterial design, negative Poisson's ratio, and mechanical cloaking problems
with potential applications in robotics, bioengineering, and aerospace.

</details>


### [61] [Cross-Modal Alignment via Variational Copula Modelling](https://arxiv.org/abs/2511.03196)
*Feng Wu,Tsai Hor Chan,Fuying Wang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出了一种基于copula的多模态学习框架，通过建模不同模态间的复杂交互来学习联合分布，能够有效处理缺失模态问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中存在多种数据模态，需要开发多模态学习方法。现有方法主要依赖拼接或Kronecker积，过度简化了模态间的交互结构，需要建模更复杂的交互。

Method: 提出copula驱动的多模态学习框架，将copula模型解释为对齐模态边缘分布的工具。假设每个模态服从高斯混合分布，在联合分布上应用copula模型。

Result: 在公开MIMIC数据集上的大量实验表明，该模型优于其他竞争方法。

Conclusion: 该copula驱动的多模态学习框架能够有效捕捉模态间的复杂交互，并为缺失模态生成准确表示。

Abstract: Various data modalities are common in real-world applications (e.g.,
electronic health records, medical images and clinical notes in healthcare). It
is essential to develop multimodal learning methods to aggregate various
information from multiple modalities. The main challenge is how to
appropriately align and fuse the representations of different modalities into a
joint distribution. Existing methods mainly rely on concatenation or the
Kronecker product, oversimplifying the interaction structure between modalities
and indicating a need to model more complex interactions. Additionally, the
joint distribution of latent representations with higher-order interactions is
underexplored. Copula is a powerful statistical structure for modelling the
interactions among variables, as it naturally bridges the joint distribution
and marginal distributions of multiple variables. We propose a novel
copula-driven multimodal learning framework, which focuses on learning the
joint distribution of various modalities to capture the complex interactions
among them. The key idea is to interpret the copula model as a tool to align
the marginal distributions of the modalities efficiently. By assuming a
Gaussian mixture distribution for each modality and a copula model on the joint
distribution, our model can generate accurate representations for missing
modalities. Extensive experiments on public MIMIC datasets demonstrate the
superior performance of our model over other competitors. The code is available
at https://github.com/HKU-MedAI/CMCM.

</details>


### [62] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: 提出了一种自适应无传感器监测方法，通过残差校正框架修正无传感器模型的系统性偏差，显著提高了集装箱内部温湿度的预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统无传感器监测方法不包含遥测信息且无法修正系统性误差，导致预测结果与实时数据差异显著，给用户带来困惑。

Method: 引入残差校正方法，在观测到实时遥测数据后修正无传感器模型的系统性偏差，构建自适应无传感器监测模型。

Result: 在348万数据点上评估，自适应无传感器模型相比基线模型显著改善：温度MAE从2.43°C降至2.24-2.31°C，湿度MAE从7.99%降至5.72-7.09%；温度RMSE从3.38°C降至3.19-3.26°C，湿度RMSE从10.0%降至7.70-9.12%。

Conclusion: 自适应无传感器模型能够实现更准确的货物监测、早期风险检测，并减少全球航运中对完全连接性的依赖。

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [63] [Decoupled Entropy Minimization](https://arxiv.org/abs/2511.03256)
*Jing Ma,Hanlin Li,Xiang Xiang*

Main category: cs.LG

TL;DR: 本文揭示了经典熵最小化(EM)的内在机制，将其解耦为两个相反作用的部分，并提出自适应解耦熵最小化(AdaDEM)来解决经典EM的局限性。


<details>
  <summary>Details</summary>
Motivation: 经典熵最小化虽然能减少类别重叠、缩小领域差距和限制不确定性，但其潜力有限。需要研究EM的内在机制并解决其耦合公式带来的局限性。

Method: 将经典EM解耦为聚类聚合驱动因子(CADF)和梯度缓解校准器(GMC)，然后提出AdaDEM方法，对CADF的奖励进行归一化，并用边际熵校准器(MEC)替代GMC。

Result: AdaDEM在噪声和动态环境中的各种不完善监督学习任务中表现优于DEM*(经典EM的上界变体)，实现了更优越的性能。

Conclusion: 通过解耦经典EM并引入自适应机制，AdaDEM有效解决了奖励崩溃和易分类偏差问题，在不完善监督学习任务中表现出色。

Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
domain gap, and restricting uncertainty for various tasks in machine learning,
yet its potential is limited. To study the internal mechanism of EM, we
reformulate and decouple the classical EM into two parts with opposite effects:
cluster aggregation driving factor (CADF) rewards dominant classes and prompts
a peaked output distribution, while gradient mitigation calibrator (GMC)
penalizes high-confidence classes based on predicted probabilities.
Furthermore, we reveal the limitations of classical EM caused by its coupled
formulation: 1) reward collapse impedes the contribution of high-certainty
samples in the learning process, and 2) easy-class bias induces misalignment
between output distribution and label distribution. To address these issues, we
propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
reward brought from CADF and employs a marginal entropy calibrator (MEC) to
replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
and achieves superior performance across various imperfectly supervised
learning tasks in noisy and dynamic environments.

</details>


### [64] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: 提出了DADO算法，一种能够利用设计变量可分解性的分布优化方法，通过图消息传递协调优化过程


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的科学工程中，需要根据用户指定属性设计离散对象，但现有分布优化算法无法利用设计变量的可分解性结构

Method: 使用软因子化的搜索分布作为生成模型，通过图消息传递协调链接因子间的优化

Result: DADO算法能够更有效地利用设计变量的可分解性进行优化

Conclusion: DADO算法为可分解设计空间的分布优化提供了更高效的解决方案

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [65] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: 使用Vision Transformer (ViT)架构从隐含波动率表面预测资产未来30天的已实现波动率，探索了Transformer模型在期权数据应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 金融机器学习中深度学习方法的复杂性优势已被证实，但Transformer架构在期权数据预测中的应用仍较少探索。

Method: 将Vision Transformer (ViT)架构应用于期权数据，从单日的隐含波动率表面（增强日期信息）预测未来30天的已实现波动率。

Result: ViT能够从IV表面学习季节性模式和非线性特征，显示出良好的模型开发前景。

Conclusion: ViT在期权数据预测中表现出潜力，为Transformer模型在金融领域的应用提供了有希望的研究方向。

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


### [66] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: 提出首个针对目标驱动交互的无监督评估指标，利用未标注交互数据的统计特性和微调LLM来适应分布变化，无需依赖人工标注的理想响应。


<details>
  <summary>Details</summary>
Motivation: 企业应用中LLM代理与人类的目标驱动交互系统难以评估：数据复杂且无标注、人工标注不具可扩展性、自定义指标无法检测未知错误、LLM评估结果不可靠。

Method: 利用未标注交互数据的统计特性，通过微调LLM来适应分布变化，开发了用户目标标注、目标完成度测量和LLM不确定性量化的指标。

Result: 在开放领域和任务特定交互数据上验证了方法的有效性。

Conclusion: 该方法为LLM驱动的目标驱动交互系统提供了可靠的无监督评估框架，解决了传统评估方法的局限性。

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [67] [Why Less is More (Sometimes): A Theory of Data Curation](https://arxiv.org/abs/2511.03492)
*Elvis Dohmatob,Mohammad Pezeshki,Reyhane Askari-Hemmat*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来解决机器学习中的核心悖论：何时使用更少数据反而更好。研究表明，在某些条件下，经过精心筛选的小数据集可以超越完整数据集，并提供了数据大小和质量相关的相变曲线。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习中的核心悖论：挑战传统"越多越好"的扩展定律，解释为何在某些情况下使用更少数据反而能获得更好性能。

Method: 研究数据筛选策略，其中不完美的预言机根据样本难度和正确性选择训练样本。推导了在标签无关和标签感知筛选规则下测试误差的精确扩展定律曲线。

Result: 理论分析表明，在某些条件下，小型筛选数据集可以超越完整数据集。在ImageNet上的实证结果验证了理论预测，显示筛选何时能提高准确性甚至缓解模型崩溃。

Conclusion: 该框架为最近在LLM数学推理中观察到的矛盾筛选策略提供了原则性解释，揭示了数据筛选改善泛化的条件和机制。

Abstract: This paper introduces a theoretical framework to resolve a central paradox in
modern machine learning: When is it better to use less data? This question has
become critical as classical scaling laws suggesting ``more is more'' (Sun et
al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et
al., 2025; Muenighoff et al., 2025), which achieve superior performance with
small, aggressively curated datasets. Here, we study data curation strategies
where an imperfect oracle selects the training examples according to their
difficulty and correctness. Our results provide exact scaling law curves for
test error under both label-agnostic and label-aware curation rules, revealing
when and why keeping only a subset of data can improve generalization. In
contrast to classical scaling laws, we show that under certain conditions,
small curated datasets can outperform full datasets, and we provide analytical
conditions for this by deriving precise phase transition curves tied to data
size and quality. We validate these theoretical claims with empirical results
on ImageNet, confirming our predictions about when curation improves accuracy
and can even mitigate model collapse. Furthermore, our framework provides a
principled explanation for the contradictory curation strategies recently
observed in LLM mathematical reasoning.

</details>


### [68] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: 该论文提出了一个将Transformer语言模型与广义相对论类比的理论框架，将注意力机制视为在弯曲流形上的平行传输，并通过实验验证了嵌入空间曲率的存在和影响。


<details>
  <summary>Details</summary>
Motivation: 旨在为Transformer模型提供一个几何理解框架，类比广义相对论来解释注意力机制中查询、键和值向量的相互作用，以及表征在层间的演化过程。

Method: 设计了三组实验：(i)可视化整个段落的曲率景观，(ii)通过模拟验证尖锐/平坦角度和长度-弦长比异常，(iii)受爱因斯坦日食实验启发，通过控制上下文编辑探测嵌入轨迹的偏转。

Result: 实验证实了注意力诱导的曲率存在：局部转向角度在token和层间变化，尖锐/平坦角度和长度-弦长比异常无法用维度或偶然性解释，上下文编辑导致可测量的、意义一致的嵌入轨迹弯曲。

Conclusion: Transformer语言模型确实在弯曲的嵌入空间中运行，注意力机制实现了在该流形上的平行传输，这为理解模型内部工作机制提供了新的几何视角。

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [69] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: 使用Lean 4定理证明器基于Mathlib库形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性


<details>
  <summary>Details</summary>
Motivation: Q学习和线性TD学习是最早且最有影响力的强化学习算法，研究其收敛性质不仅是RL领域早期发展的主要研究课题，如今也受到越来越多的关注

Method: 基于Robbins-Siegmund定理的统一框架，使用Lean 4定理证明器进行形式化验证

Result: 成功形式化验证了Q学习和线性TD学习的几乎必然收敛性

Conclusion: 这项工作为完全形式化收敛RL结果迈出了重要一步，所开发的框架可以轻松扩展到收敛速率和其他收敛模式

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [70] [Homomorphism distortion: A metric to distinguish them all and in the latent space bind them](https://arxiv.org/abs/2511.03068)
*Martin Carrasco,Olga Zaghen,Erik Bekkers,Bastian Rieck*

Main category: cs.LG

TL;DR: 本文提出了一种基于图同态失真的新方法来测量顶点属性图之间的相似性，该方法能够完全表征图结构，是一种完整的图嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 长期以来，图神经网络的表达能力仅通过组合性质来衡量，本文旨在打破这一传统，提供一种原则性的方法来测量顶点属性图之间的相似性。

Method: 提出图同态失真度量，通过采样方法高效计算该度量，并证明该度量可以转化为度量空间。

Result: 实验验证表明，图同态失真方法能够完全区分BREC数据集中4-WL无法区分的图，并在ZINC-12k数据集上优于先前基于同态的方法。

Conclusion: 该理论结果为未来图表征研究开辟了新途径，将图论传统扩展到新的前沿。

Abstract: For far too long, expressivity of graph neural networks has been measured
\emph{only} in terms of combinatorial properties. In this work we stray away
from this tradition and provide a principled way to measure similarity between
vertex attributed graphs. We denote this measure as the \emph{graph
homomorphism distortion}. We show it can \emph{completely characterize} graphs
and thus is also a \emph{complete graph embedding}. However, somewhere along
the road, we run into the graph canonization problem. To circumvent this
obstacle, we devise to efficiently compute this measure via sampling, which in
expectation ensures \emph{completeness}. Additionally, we also discovered that
we can obtain a metric from this measure. We validate our claims empirically
and find that the \emph{graph homomorphism distortion}: (1.) fully
distinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishable
graphs, and (2.) \emph{outperforms} previous methods inspired in homomorphisms
under the \texttt{ZINC-12k} dataset.
  These theoretical results, (and their empirical validation), pave the way for
future characterization of graphs, extending the graph theoretic tradition to
new frontiers.

</details>


### [71] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: 提出MSUCB算法，通过中位数均值估计器在在线学习排序中实现鲁棒性，在无污染时达到最优对数遗憾，在存在点击欺诈时遗憾仅随总污染量线性增加。


<details>
  <summary>Details</summary>
Motivation: 在线学习排序系统容易受到点击欺诈等操纵行为的影响，这些污染反馈会误导学习过程并降低用户体验，需要开发鲁棒算法来应对这种挑战。

Method: 提出MSUCB算法，采用新颖的中位数均值估计器，在无污染时表现如标准均值，在存在污染时通过中位数步骤过滤异常值和污染样本。

Result: 在真实世界数据集上的实验表明，MSUCB始终优于现有方法，相比两种最先进方法分别实现了97.35%和91.60%的遗憾改进。

Conclusion: MSUCB算法在无污染时达到最优性能，在存在污染时保持强鲁棒性，为在线学习排序中的污染问题提供了有效解决方案。

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [72] [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)
*Gaia Grosso,Sai Sumedh R. Hindupur,Thomas Fel,Samuel Bright-Thonney,Philip Harris,Demba Ba*

Main category: cs.LG

TL;DR: 本文提出SparKer方法，通过稀疏、局部性和竞争性三个原则构建自组织局部核，在最小先验信息下进行异常检测，能够在高维表示空间中有效识别统计显著的异常位置。


<details>
  <summary>Details</summary>
Motivation: 现代AI提取的数据表示虽然丰富，但其统计特性难以控制，导致传统异常检测方法在弱信号或罕见信号面前失效。需要一种在最小先验信息下仍能有效检测异常的方法。

Method: 提出SparKer方法，使用稀疏高斯核集合，在半监督Neyman-Pearson框架下训练，局部建模可能包含异常的样本与正常参考样本之间的似然比。

Result: 仅需少量核的集成就能在数千维表示空间中识别统计显著的异常位置，证明了方法的可解释性、效率和可扩展性。

Conclusion: 基于稀疏、局部性和竞争性原则的自组织局部核方法为高维表示空间中的异常检测提供了有效解决方案，在科学发现、开放世界新颖性检测等多个领域具有应用价值。

Abstract: Modern artificial intelligence has revolutionized our ability to extract rich
and versatile data representations across scientific disciplines. Yet, the
statistical properties of these representations remain poorly controlled,
causing misspecified anomaly detection (AD) methods to falter. Weak or rare
signals can remain hidden within the apparent regularity of normal data,
creating a gap in our ability to detect and interpret anomalies. We examine
this gap and identify a set of structural desiderata for detection methods
operating under minimal prior information: sparsity, to enforce parsimony;
locality, to preserve geometric sensitivity; and competition, to promote
efficient allocation of model capacity. These principles define a class of
self-organizing local kernels that adaptively partition the representation
space around regions of statistical imbalance. As an instantiation of these
principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained
within a semi-supervised Neyman--Pearson framework to locally model the
likelihood ratio between a sample that may contain anomalies and a nominal,
anomaly-free reference. We provide theoretical insights into the mechanisms
that drive detection and self-organization in the proposed model, and
demonstrate the effectiveness of this approach on realistic high-dimensional
problems of scientific discovery, open-world novelty detection, intrusion
detection, and generative-model validation. Our applications span both the
natural- and computer-science domains. We demonstrate that ensembles containing
only a handful of kernels can identify statistically significant anomalous
locations within representation spaces of thousands of dimensions, underscoring
both the interpretability, efficiency and scalability of the proposed approach.

</details>


### [73] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: 提出了DiCoDe框架，通过投影通用引导和评论家蒸馏机制，解决了多智能体环境协同设计中的可扩展性和样本效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前协同设计方法难以应对高维环境设计空间，且在联合优化中面临移动目标导致的样本效率低下问题。

Method: DiCoDe框架包含两个核心创新：投影通用引导(PUG)采样技术和评论家蒸馏机制，前者探索奖励最大化环境分布并满足硬约束，后者确保扩散模型适应演化的智能体策略。

Result: 在仓库自动化、多智能体路径规划和风电场优化等基准测试中表现优异，例如在仓库设置中实现了39%的奖励提升和66%的模拟样本减少。

Conclusion: 为智能体-环境协同设计设立了新标准，是向现实世界应用迈出的重要一步。

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [74] [Tensor-Efficient High-Dimensional Q-learning](https://arxiv.org/abs/2511.03595)
*Junyi Wu,Dan Li*

Main category: cs.LG

TL;DR: 提出Tensor-Efficient Q-Learning (TEQL)，通过改进的低秩张量分解和探索机制，解决高维强化学习中的维度灾难和样本效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 高维强化学习面临计算复杂和样本效率低的挑战，特别是在大状态-动作空间中。现有Q学习算法受维度灾难影响，而神经网络方法虽然有效但参数效率不高。

Method: 在现有张量方法基础上，TEQL通过改进的块坐标下降法在离散化状态-动作空间上进行低秩张量分解，结合新的探索策略（近似误差+访问计数上置信界）和正则化机制（频率惩罚项）。

Result: 在经典控制任务上的实验结果表明，TEQL在样本效率和总奖励方面优于传统矩阵方法和深度强化学习方法。

Conclusion: TEQL适用于资源受限的应用场景（如航天和医疗领域），其中采样成本较高，能够有效提升强化学习的效率和性能。

Abstract: High-dimensional reinforcement learning faces challenges with complex
calculations and low sample efficiency in large state-action spaces. Q-learning
algorithms struggle particularly with the curse of dimensionality, where the
number of state-action pairs grows exponentially with problem size. While
neural network-based approaches like Deep Q-Networks have shown success, recent
tensor-based methods using low-rank decomposition offer more
parameter-efficient alternatives. Building upon existing tensor-based methods,
we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor
decomposition via improved block coordinate descent on discretized state-action
spaces, incorporating novel exploration and regularization mechanisms. The key
innovation is an exploration strategy that combines approximation error with
visit count-based upper confidence bound to prioritize actions with high
uncertainty, avoiding wasteful random exploration. Additionally, we incorporate
a frequency-based penalty term in the objective function to encourage
exploration of less-visited state-action pairs and reduce overfitting to
frequently visited regions. Empirical results on classic control tasks
demonstrate that TEQL outperforms conventional matrix-based methods and deep RL
approaches in both sample efficiency and total rewards, making it suitable for
resource-constrained applications, such as space and healthcare where sampling
costs are high.

</details>


### [75] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: 提出了改进的CTF-IDF算法和IRLBA降维方法，在文本分析中替代深度学习，显著降低计算复杂度和碳足迹，同时保持较高准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在文本分析领域的广泛应用导致计算资源需求和碳足迹急剧增加，需要寻找更高效、环保的替代方案。

Method: 改进传统TF-IDF算法为CTF-IDF，结合IRLBA算法进行降维处理，构建基于经典机器学习方法的文本分析流程。

Result: 相比深度学习方法，在碳足迹方面更高效、计算速度更快、计算强度更低，时间复杂度显著降低，模型准确率有所提升。

Conclusion: 经典机器学习方法结合CTF-IDF和IRLBA算法在文本分析中能够有效平衡计算效率和准确性，为减少AI碳足迹提供了可行方案。

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [76] [Towards Scalable Backpropagation-Free Gradient Estimation](https://arxiv.org/abs/2511.03110)
*Daniel Wang,Evan Markou,Dylan Campbell*

Main category: cs.LG

TL;DR: 提出了一种新的梯度估计方法，通过操纵上游雅可比矩阵来减少偏差和方差，在更宽的网络中表现更好，有望扩展到大型网络。


<details>
  <summary>Details</summary>
Motivation: 反向传播需要两次前向传播和存储中间激活值，而现有的前向模式自动微分方法由于估计方差高难以扩展到小型网络之外，且现有减少方差的方法会引入显著偏差。

Method: 通过操纵上游雅可比矩阵来计算猜测方向，从而同时减少梯度估计的偏差和方差。

Result: 该方法显示出有希望的结果，随着网络宽度的增加性能更好，有望扩展到更大的网络。

Conclusion: 该方法通过分析偏差和方差及其与神经网络梯度低维结构的联系，提供了一种有前景的梯度估计替代方案。

Abstract: While backpropagation--reverse-mode automatic differentiation--has been
extraordinarily successful in deep learning, it requires two passes (forward
and backward) through the neural network and the storage of intermediate
activations. Existing gradient estimation methods that instead use forward-mode
automatic differentiation struggle to scale beyond small networks due to the
high variance of the estimates. Efforts to mitigate this have so far introduced
significant bias to the estimates, reducing their utility. We introduce a
gradient estimation approach that reduces both bias and variance by
manipulating upstream Jacobian matrices when computing guess directions. It
shows promising results and has the potential to scale to larger networks,
indeed performing better as the network width is increased. Our understanding
of this method is facilitated by analyses of bias and variance, and their
connection to the low-dimensional structure of neural network gradients.

</details>


### [77] [FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation](https://arxiv.org/abs/2511.03113)
*Jiameng Chen,Yida Xiong,Kun Li,Hongzhi Zhang,Xiantao Cai,Wenbin Hu,Jia Wu*

Main category: cs.LG

TL;DR: FP-AbDiff是首个在整个生成轨迹中强制执行Fokker-Planck方程物理学的抗体生成器，通过FPE残差损失确保物理一致性，在抗体设计任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有抗体生成模型存在两个核心挑战：(i)缺乏动力学一致性，产生物理上不可行的结构；(ii)由于数据稀缺和结构偏差导致泛化能力差。

Method: 在SE(3)-等变扩散框架中引入FPE物理约束，在CDR几何混合流形上最小化FPE残差损失，将局部学习的去噪分数组装成全局一致的概率流。

Result: 在de novo CDR-H3设计中达到0.99Å RMSD，比之前最佳模型提升25%；在六CDR协同设计中，全链RMSD降低约15%，CDR-H3环的氨基酸恢复率达到45.67%。

Conclusion: 通过将生成动力学与物理定律对齐，FP-AbDiff增强了鲁棒性和泛化能力，为物理忠实且功能可行的抗体设计建立了原则性方法。

Abstract: Computational antibody design holds immense promise for therapeutic
discovery, yet existing generative models are fundamentally limited by two core
challenges: (i) a lack of dynamical consistency, which yields physically
implausible structures, and (ii) poor generalization due to data scarcity and
structural bias. We introduce FP-AbDiff, the first antibody generator to
enforce Fokker-Planck Equation (FPE) physics along the entire generative
trajectory. Our method minimizes a novel FPE residual loss over the mixed
manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising
scores to assemble into a globally coherent probability flow. This
physics-informed regularizer is synergistically integrated with deep biological
priors within a state-of-the-art SE(3)-equivariant diffusion framework.
Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a
new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean
Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25%
improvement over the previous state-of-the-art model, AbX, and the highest
reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored
in the more challenging six-CDR co-design task, where our model delivers
consistently superior geometric precision, cutting the average full-chain Root
Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain
Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By
aligning generative dynamics with physical laws, FP-AbDiff enhances robustness
and generalizability, establishing a principled approach for physically
faithful and functionally viable antibody design.

</details>


### [78] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 本文提出了基于增强重叠的对比学习理论，推导了对比学习性能的渐近紧界，并开发了一种无监督的表征评估指标。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习虽然取得了巨大成功，但其工作机制尚不清晰。本文旨在揭示对比学习的底层工作机制，特别是放松条件独立性假设，提出更实用的增强重叠假设。

Method: 首先基于条件独立性假设提供最紧的边界，然后放松该假设，提出增强重叠假设，并推导下游性能的渐近闭边界。通过理论分析发现，在激进的数据增强下，类内样本的支持集会更重叠，从而对齐正样本可使对比学习将类内样本聚类在一起。

Result: 从增强重叠的新视角开发了一种无监督的对比学习表征评估指标，该指标与下游性能高度一致，几乎不需要依赖额外模块。

Conclusion: 提出的增强重叠理论为理解对比学习机制提供了新视角，开发的无监督评估指标能有效评估表征质量，代码已开源。

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [79] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 提出了两种新颖的攻击框架StaDec和DyDec，通过利用对LLMs的理解来生成动态和自适应的对抗样本，评估LLMs在敏感任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在零样本任务中表现出色，但在处理敏感任务时需要评估其对对抗性输入的鲁棒性。

Method: 使用自动化的LLM驱动管道生成语义相似但能有效欺骗目标LLM的对抗样本，无需依赖外部启发式方法。

Result: 攻击方法随着LLMs的进步而演进，并在攻击者未知的模型间表现出强大的可迁移性。

Conclusion: 这项工作为LLMs的鲁棒性自我评估提供了系统性方法。

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


### [80] [Test Time Adaptation Using Adaptive Quantile Recalibration](https://arxiv.org/abs/2511.03148)
*Paria Mehrbod,Pedro Vianna,Geraldin Nanfack,Guy Wolf,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了自适应分位数重校准(AQR)方法，一种无需重新训练模型的无监督测试时自适应技术，通过通道级分位数对齐来适应预激活分布，在多种架构和数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统域自适应方法需要目标域先验知识或模型重训练的问题，以及现有测试时自适应方法无法捕捉复杂激活分布且局限于特定归一化层的局限性。

Method: AQR通过通道级分位数对齐修改预激活分布，采用鲁棒的尾部校准策略处理不同批次大小下的分布尾部估计问题，利用训练时计算的源域统计量实现无监督自适应。

Result: 在CIFAR-10-C、CIFAR-100-C和ImageNet-C数据集上的实验表明，AQR在多种架构下实现了鲁棒的自适应，优于现有的测试时自适应基线方法。

Conclusion: AQR具有在动态和不可预测数据分布的真实世界场景中部署的潜力，能够有效提升深度学习模型在分布偏移情况下的泛化能力。

Abstract: Domain adaptation is a key strategy for enhancing the generalizability of
deep learning models in real-world scenarios, where test distributions often
diverge significantly from the training domain. However, conventional
approaches typically rely on prior knowledge of the target domain or require
model retraining, limiting their practicality in dynamic or
resource-constrained environments. Recent test-time adaptation methods based on
batch normalization statistic updates allow for unsupervised adaptation, but
they often fail to capture complex activation distributions and are constrained
to specific normalization layers. We propose Adaptive Quantile Recalibration
(AQR), a test-time adaptation technique that modifies pre-activation
distributions by aligning quantiles on a channel-wise basis. AQR captures the
full shape of activation distributions and generalizes across architectures
employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of
estimating distribution tails under varying batch sizes, AQR incorporates a
robust tail calibration strategy that improves stability and precision. Our
method leverages source-domain statistics computed at training time, enabling
unsupervised adaptation without retraining models. Experiments on CIFAR-10-C,
CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR
achieves robust adaptation across diverse settings, outperforming existing
test-time adaptation baselines. These results highlight AQR's potential for
deployment in real-world scenarios with dynamic and unpredictable data
distributions.

</details>


### [81] [Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction](https://arxiv.org/abs/2511.03149)
*Atif Hassan,Tarun Kumar,Ashish Mishra,Sergey Serebryakov,Satish Kumar Mopur,Phanidhar Koganti,Murthy Chelankuri,Ramanagopal Vogety,Suparna Bhattacharya,Martin Foltin*

Main category: cs.LG

TL;DR: Forecast2Anomaly (F2A) 是一个新颖框架，通过联合预测-异常损失和检索增强生成模块，使时间序列基础模型具备零样本异常预测能力，在16个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常预测方法局限于特定系统，无法泛化到随时间演变的异常模式。时间序列基础模型虽然具有强大的泛化和零样本预测能力，但尚未应用于异常预测任务。

Method: 1. 联合预测-异常损失：微调TSFMs以准确预测异常时间点的未来信号
2. 检索增强生成模块：检索历史相关时间窗口并基于这些窗口进行预测，动态适应推理时的分布变化

Result: 在16个不同数据集和多个TSFM骨干网络上的广泛实验表明，F2A始终优于最先进的方法。

Conclusion: F2A通过目标微调和动态检索相结合，弥合了稳健TSFM零样本预测与零样本异常预测之间的差距，为实际应用提供了可扩展的零样本异常预测解决方案。

Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.

</details>


### [82] [UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems](https://arxiv.org/abs/2511.03168)
*Tingzhu Bi,Yicheng Pan,Xinrui Jiang,Huize Sun,Meng Ma,Ping Wang*

Main category: cs.LG

TL;DR: UnCLe是一种新颖的深度学习动态因果发现方法，通过解耦和重构网络将时间序列分解为语义表示，利用自回归依赖矩阵学习变量间依赖关系，通过分析时间扰动引起的预测误差来估计动态因果影响。


<details>
  <summary>Details</summary>
Motivation: 从观测时间序列中发现因果关系对于理解复杂系统至关重要。现有方法大多推断静态因果图，但现实系统通常表现出动态因果关系——关系随时间演变。准确捕捉这些时间动态需要时间分辨的因果图。

Method: 使用解耦器和重构器网络对输入时间序列进行解耦，通过自回归依赖矩阵学习变量间依赖关系，通过分析时间扰动引起的逐数据点预测误差来估计动态因果影响。

Result: 在静态因果发现基准测试中优于最先进基线方法，更重要的是在合成和真实世界动态系统（如人体运动）中能够准确捕捉和表示演化的时间因果关系。

Conclusion: UnCLe为揭示复杂现象中潜在的时间变化机制提供了一种有前景的方法。

Abstract: Uncovering cause-effect relationships from observational time series is
fundamental to understanding complex systems. While many methods infer static
causal graphs, real-world systems often exhibit dynamic causality-where
relationships evolve over time. Accurately capturing these temporal dynamics
requires time-resolved causal graphs. We propose UnCLe, a novel deep learning
method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler
and Recoupler networks to disentangle input time series into semantic
representations and learns inter-variable dependencies via auto-regressive
Dependency Matrices. It estimates dynamic causal influences by analyzing
datapoint-wise prediction errors induced by temporal perturbations. Extensive
experiments demonstrate that UnCLe not only outperforms state-of-the-art
baselines on static causal discovery benchmarks but, more importantly, exhibits
a unique capability to accurately capture and represent evolving temporal
causality in both synthetic and real-world dynamic systems (e.g., human
motion). UnCLe offers a promising approach for revealing the underlying,
time-varying mechanisms of complex phenomena.

</details>


### [83] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 提出了一种名为PSD的无监督周期性技能发现框架，通过将状态映射到圆形潜在空间来学习具有不同周期的周期性行为，在复杂机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前无监督技能发现方法忽视了技能的周期性特征，而许多机器人任务（特别是运动类任务）需要不同时间尺度的周期性行为，因此需要能够发现多样化周期性技能的方法。

Method: 训练编码器将状态映射到圆形潜在空间，在潜在表示中自然编码周期性，通过捕捉时间距离来学习具有不同周期的技能。

Result: PSD能够有效学习复杂机器人任务中具有不同周期的技能，即使在基于像素的观测下也能工作良好，并且在下游任务（如跨栏）中实现高性能。

Conclusion: PSD框架成功发现了多样化的周期性技能，与现有技能发现方法结合还能提供更丰富的行为库，扩展了智能体的技能储备。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [84] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: 提出了一种基于熵的线性注意力机制，通过理论证明和高效算法实现线性复杂度，在时空时间序列建模中达到竞争性性能并显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制因二次计算复杂度而难以扩展到长序列，需要开发更高效的线性注意力方法。

Method: 基于熵理论开发线性注意力机制，利用概率分布的结构相似性和熵值近似，实现线性复杂度的注意力计算。

Result: 在四个时空数据集上的实验表明，该方法在预测性能上具有竞争力或更优，同时大幅减少内存使用和计算时间。

Conclusion: 注意力机制的有效性可能主要源于适中的权重分布而非softmax的非线性，基于熵的线性注意力为长序列建模提供了高效解决方案。

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [85] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: 本文采用概率U-Net进行气候数据降尺度，结合确定性U-Net主干和变分潜在空间来捕捉不确定性，评估了四种训练目标在降尺度降水和温度数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 气候模型受限于计算成本，通常只能生成粗空间分辨率的输出，而许多气候变化影响研究需要更精细的尺度，统计降尺度方法可以弥补这一差距。

Method: 使用概率U-Net架构，结合确定性U-Net主干和变分潜在空间来捕捉随机不确定性，评估了afCRPS和WMSE-MS-SSIM两种训练目标在三种不同设置下的表现。

Result: WMSE-MS-SSIM在某些设置下对极端事件表现良好，而afCRPS在跨尺度的空间变异性捕捉方面表现更优。

Conclusion: 不同的训练目标在气候数据降尺度任务中各有优势，需要根据具体应用场景选择合适的目标函数。

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [86] [A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies](https://arxiv.org/abs/2511.03201)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: 提出基于VAE-MLP的轻量级物联网僵尸网络检测框架，系统评估两种量化策略（QAT和PTQ）对检测性能、存储效率和推理延迟的影响。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习检测方法计算量大，难以部署在资源受限的物联网设备上，需要轻量级检测模型。

Method: 使用预训练变分自编码器（VAE）的编码器将高维训练数据压缩为8维潜在向量，训练MLP分类器，然后系统评估QAT和PTQ两种量化策略。

Result: PTQ在检测精度上仅有轻微下降，实现6倍加速和21倍压缩；QAT精度下降更明显，实现3倍加速和24倍压缩。

Conclusion: 量化技术对设备级物联网僵尸网络检测具有实用性，其中PTQ在精度保持和性能提升方面表现更优。

Abstract: In an effort to counter the increasing IoT botnet-based attacks,
state-of-the-art deep learning methods have been proposed and have achieved
impressive detection accuracy. However, their computational intensity restricts
deployment on resource-constrained IoT devices, creating a critical need for
lightweight detection models. A common solution to this challenge is model
compression via quantization. This study proposes a VAE-MLP model framework
where an MLP-based classifier is trained on 8-dimensional latent vectors
derived from the high-dimensional train data using the encoder component of a
pretrained variational autoencoder (VAE). Two widely used quantization
strategies--Quantization-Aware Training (QAT) and Post-Training Quantization
(PTQ)--are then systematically evaluated in terms of their impact on detection
performance, storage efficiency, and inference latency using two benchmark IoT
botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with
respect to detection accuracy, the QAT strategy experienced a more noticeable
decline,whereas PTQ incurred only a marginal reduction compared to the original
unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in
size, while QAT achieved a 3x speedup and 24x compression, demonstrating the
practicality of quantization for device-level IoT botnet detection.

</details>


### [87] [Incorporating Quality of Life in Climate Adaptation Planning via Reinforcement Learning](https://arxiv.org/abs/2511.03238)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 使用强化学习在气候变化背景下优化城市洪水适应策略，通过集成评估模型寻找能提高长期生活质量的最佳适应路径。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致城市洪水频率和严重性增加，影响城市生活质量，而政策制定者需要应对气候变化的不确定性和城市洪水的复杂性。

Method: 采用强化学习结合集成评估模型，整合降雨预测、洪水模型、交通可达性模型和生活质量指数。

Result: 初步结果表明该方法能够学习最优适应措施，优于其他现实和真实世界的规划策略。

Conclusion: 强化学习框架能够有效识别提高长期生活质量的适应路径，为城市洪水管理提供新方法。

Abstract: Urban flooding is expected to increase in frequency and severity as a
consequence of climate change, causing wide-ranging impacts that include a
decrease in urban Quality of Life (QoL). Meanwhile, policymakers must devise
adaptation strategies that can cope with the uncertain nature of climate change
and the complex and dynamic nature of urban flooding. Reinforcement Learning
(RL) holds significant promise in tackling such complex, dynamic, and uncertain
problems. Because of this, we use RL to identify which climate adaptation
pathways lead to a higher QoL in the long term. We do this using an Integrated
Assessment Model (IAM) which combines a rainfall projection model, a flood
model, a transport accessibility model, and a quality of life index. Our
preliminary results suggest that this approach can be used to learn optimal
adaptation measures and it outperforms other realistic and real-world planning
strategies. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [88] [A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams](https://arxiv.org/abs/2511.03239)
*Philipp Reis,Philipp Rigoll,Christian Steinhauser,Jacob Langner,Eric Sax*

Main category: cs.LG

TL;DR: FCDC将数据收集建模为闭环控制问题，通过在线概率模型和反馈机制动态平衡探索与利用，减少数据冗余39.8%，提升数据集平衡性25.9%。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统受限于数据质量和多样性而非模型容量。传统开环数据收集方式积累冗余样本，导致存储效率低、标注成本高、泛化能力有限。

Method: FCDC使用在线概率模型近似已收集数据分布状态，基于似然和马氏距离等反馈信号自适应调节样本保留，形成闭环控制机制。

Result: 在真实数据流实验中，FCDC减少数据存储39.8%，提升数据集平衡性25.9%。合成数据集实验验证了该方法的可控性。

Conclusion: 数据收集本身可以主动控制，将收集从被动流水线阶段转变为数据驱动AI核心的自调节、反馈驱动过程。

Abstract: Modern AI systems are increasingly constrained not by model capacity but by
the quality and diversity of their data. Despite growing emphasis on
data-centric AI, most datasets are still gathered in an open-loop manner which
accumulates redundant samples without feedback from the current coverage. This
results in inefficient storage, costly labeling, and limited generalization. To
address this, this paper introduces \ac{FCDC}, a paradigm that formulates data
collection as a closed-loop control problem. \ac{FCDC} continuously
approximates the state of the collected data distribution using an online
probabilistic model and adaptively regulates sample retention using based on
feedback signals such as likelihood and Mahalanobis distance. Through this
feedback mechanism, the system dynamically balances exploration and
exploitation, maintains dataset diversity, and prevents redundancy from
accumulating over time. Besides showcasing the controllability of \ac{FCDC} on
a synthetic dataset, experiments on a real data stream show that \ac{FCDC}
produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data
storage by $\SI{39.8}{\percent}$. These results demonstrate that data
collection itself can be actively controlled, transforming collection from a
passive pipeline stage into a self-regulating, feedback-driven process at the
core of data-centric AI.

</details>


### [89] [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)
*Gang Bao,Yaohua Zang*

Main category: cs.LG

TL;DR: 提出IGNO框架，一种统一处理偏微分方程反问题的生成神经算子方法，无需标记训练数据，通过物理约束和潜在空间优化实现稳定、可扩展的反演。


<details>
  <summary>Details</summary>
Motivation: 解决偏微分方程反问题时，现有深度学习方法需要大量标记数据或局限于特定测量类型，在稀疏、噪声、高维或不连续系数情况下表现不佳。

Method: IGNO将高维系数场编码到低维潜在空间，通过神经算子解码器重建系数和PDE解，仅依赖物理约束训练，使用梯度优化和先验归一化流加速反演。

Result: 在包括不连续系数恢复和EIT问题在内的多种挑战性反问题中，IGNO在不同噪声水平下均优于现有方法，表现出准确、稳定和可扩展的性能。

Conclusion: IGNO为计算科学领域中的挑战性反问题提供了一个统一且强大的框架，具有优异的泛化能力和抗噪声性能。

Abstract: Solving inverse problems governed by partial differential equations (PDEs) is
central to science and engineering, yet remains challenging when measurements
are sparse, noisy, or when the underlying coefficients are high-dimensional or
discontinuous. Existing deep learning approaches either require extensive
labeled datasets or are limited to specific measurement types, often leading to
failure in such regimes and restricting their practical applicability. Here, a
novel generative neural operator framework, IGNO, is introduced to overcome
these limitations. IGNO unifies the solution of inverse problems from both
point measurements and operator-valued data without labeled training pairs.
This framework encodes high-dimensional, potentially discontinuous coefficient
fields into a low-dimensional latent space, which drives neural operator
decoders to reconstruct both coefficients and PDE solutions. Training relies
purely on physics constraints through PDE residuals, while inversion proceeds
via efficient gradient-based optimization in latent space, accelerated by an a
priori normalizing flow model. Across a diverse set of challenging inverse
problems, including recovery of discontinuous coefficients from solution-based
measurements and the EIT problem with operator-based measurements, IGNO
consistently achieves accurate, stable, and scalable inversion even under
severe noise. It consistently outperforms the state-of-the-art method under
varying noise levels and demonstrates strong generalization to
out-of-distribution targets. These results establish IGNO as a unified and
powerful framework for tackling challenging inverse problems across
computational science domains.

</details>


### [90] [Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways](https://arxiv.org/abs/2511.03243)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 使用强化学习在气候变化背景下制定洪水适应政策，考虑不确定性和不同优先级（经济vs生活质量）


<details>
  <summary>Details</summary>
Motivation: 气候变化将增加洪水事件的频率和严重性，需要有效的适应政策，但政策设计面临长期气候影响的不确定性和未明确说明的规范性选择

Method: 提出强化学习框架，结合综合评估模型、降雨和洪水模型，计算洪水对生活质量、交通和基础设施的影响

Result: 优先考虑生活质量的模型会导致更多适应支出，并在研究区域内更均匀地分配支出

Conclusion: 强化学习有助于在不确定条件下识别适应路径，并明确建模不同适应优先级，规范性假设会显著改变适应政策

Abstract: Climate change will cause an increase in the frequency and severity of flood
events, prompting the need for cohesive adaptation policymaking. Designing
effective adaptation policies, however, depends on managing the uncertainty of
long-term climate impacts. Meanwhile, such policies can feature important
normative choices that are not always made explicit. We propose that
Reinforcement Learning (RL) can be a useful tool to both identify adaptation
pathways under uncertain conditions while it also allows for the explicit
modelling (and consequent comparison) of different adaptation priorities (e.g.
economic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link
together a rainfall and flood model, and compute the impacts of flooding in
terms of quality of life (QoL), transportation, and infrastructure damage. Our
results show that models prioritising QoL over economic impacts results in more
adaptation spending as well as a more even distribution of spending over the
study area, highlighting the extent to which such normative assumptions can
alter adaptation policy. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [91] [GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models](https://arxiv.org/abs/2511.03251)
*Zhibin Wang,Zhixing Zhang,Shuqi Wang,Xuanting Xie,Zhao Kang*

Main category: cs.LG

TL;DR: GMoPE是一个将混合专家架构与基于提示的图学习相结合的新框架，通过专家特定提示向量和结构感知路由实现跨领域泛化，显著降低适应成本。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在跨领域泛化中的负迁移、可扩展性差和适应成本高的问题，推进通用图基础模型的发展。

Method: 结合混合专家架构与提示学习，使用专家特定提示向量和结构感知路由，引入软正交约束促进专家多样性，采用仅提示微调策略降低复杂度。

Result: 在各种预训练策略和下游任务中持续优于现有方法，性能接近全参数微调但仅需少量适应开销。

Conclusion: GMoPE为推进通用化和高效的图基础模型提供了一个原则性和可扩展的框架。

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on
task-specific benchmarks, yet their ability to generalize across diverse
domains and tasks remains limited. Existing approaches often struggle with
negative transfer, scalability issues, and high adaptation costs. To address
these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel
framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture
with prompt-based learning for graphs. GMoPE leverages expert-specific prompt
vectors and structure-aware MoE routing to enable each expert to specialize in
distinct subdomains and dynamically contribute to predictions. To promote
diversity and prevent expert collapse, we introduce a soft orthogonality
constraint across prompt vectors, encouraging expert specialization and
facilitating a more balanced expert utilization. Additionally, we adopt a
prompt-only fine-tuning strategy that significantly reduces spatiotemporal
complexity during transfer. We validate GMoPE through extensive experiments
under various pretraining strategies and multiple downstream tasks. Results
show that GMoPE consistently outperforms state-of-the-art baselines and
achieves performance comparable to full parameter fine-tuning-while requiring
only a fraction of the adaptation overhead. Our work provides a principled and
scalable framework for advancing generalizable and efficient graph foundation
models.

</details>


### [92] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 在严格控制的预训练设置下，当独特数据有限时，扩散语言模型通过更多轮次训练持续超越自回归模型，这种优势归因于任意顺序建模、迭代双向去噪的超密集计算和内置蒙特卡洛增强。


<details>
  <summary>Details</summary>
Motivation: 研究在数据受限情况下，扩散语言模型与自回归模型的性能比较，探索扩散模型在有限数据下的优势及其原因。

Method: 在严格控制的预训练设置下，比较扩散语言模型和自回归模型在不同数据量和训练轮次下的表现，分析影响性能差异的关键因素。

Result: 1.7B参数的DLM在约1.5T token计算预算下，使用10B独特Python token训练后超越了匹配设置下的AR编码器；1B参数的DLM仅用1B token就实现了HellaSwag >56%和MMLU >33%的准确率。

Conclusion: 扩散语言模型在数据受限情况下具有显著优势，验证交叉熵上升并不一定意味着下游性能下降，为有限数据下的语言模型训练提供了新思路。

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [93] [Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning](https://arxiv.org/abs/2511.03279)
*Ning Lyu,Yuxi Wang,Ziyu Cheng,Qingyuan Zhang,Feng Chen*

Main category: cs.LG

TL;DR: 提出基于深度强化学习的自适应API限流策略，相比传统固定阈值方法在高负载场景下提升23.7%吞吐量并降低31.4% P99延迟


<details>
  <summary>Details</summary>
Motivation: 传统限流算法难以适应动态流量模式和变化的系统负载，需要更智能的自适应限流机制来确保系统稳定性和服务质量

Method: 设计结合DQN和A3C算法的混合架构，将限流决策过程建模为马尔可夫决策过程，通过环境交互学习最优限流策略

Result: 在Kubernetes集群环境中实验显示，相比传统固定阈值策略，吞吐量提升23.7%，P99延迟降低31.4%；90天生产部署处理5亿日请求，服务降级事件减少82%，人工干预减少68%

Conclusion: 基于深度强化学习的自适应限流策略能有效平衡系统吞吐量和服务延迟，在实际生产环境中具有显著效果

Abstract: As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.

</details>


### [94] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: 提出了一种新的多参考对齐算法，通过概率建模和相对位姿作为冗余变量，消除全局对称性，实现更直接的解决方案和更好的收敛性。去中心化方法通过循环一致性避免立方级计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 从分子成像到无线通信，从多个未对齐观测中校准和重建信号对系统性能至关重要。多参考对齐问题在冷冻电镜、计算机视觉和无线通信等实际应用中普遍存在。

Method: 使用概率方法建模多参考对齐问题，将相对位姿作为冗余变量进行边缘化处理，消除问题的全局对称性。提出去中心化算法，通过循环一致性避免集中式方法的立方级计算复杂度。

Result: 两种提出的算法在各种实验设置下都实现了更低的重建误差。去中心化方法显著节省了计算资源。

Conclusion: 通过概率建模和相对位姿边缘化，成功解决了多参考对齐问题中的全局对称性挑战，提出的算法在精度和计算效率方面都有显著提升。

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [95] [Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices](https://arxiv.org/abs/2511.03285)
*Qingyuan Zhang,Ning Lyu,Le Liu,Yuxi Wang,Ziyu Cheng,Cancan Hua*

Main category: cs.LG

TL;DR: 提出了一个结合图神经网络和时间建模的统一框架，用于微服务架构中的异常检测和根因溯源。


<details>
  <summary>Details</summary>
Motivation: 解决微服务架构中异常检测和根因溯源的问题，通过统一建模从局部异常检测到全局调用链追踪。

Method: 将微服务调用链抽象为有向图，使用图卷积聚合节点特征并建模依赖关系，引入门控循环单元建模时间演化，定义节点级和路径级异常评分函数。

Result: 在AUC、ACC、Recall和F1-Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性。

Conclusion: 为微服务异常检测提供了新的技术路径，为分布式系统智能运维奠定了方法论基础。

Abstract: This study addresses the problem of anomaly detection and root cause tracing
in microservice architectures and proposes a unified framework that combines
graph neural networks with temporal modeling. The microservice call chain is
abstracted as a directed graph, where multidimensional features of nodes and
edges are used to construct a service topology representation, and graph
convolution is applied to aggregate features across nodes and model
dependencies, capturing complex structural relationships among services. On
this basis, gated recurrent units are introduced to model the temporal
evolution of call chains, and multi-layer stacking and concatenation operations
are used to jointly obtain structural and temporal representations, improving
the ability to identify anomaly patterns. Furthermore, anomaly scoring
functions at both the node and path levels are defined to achieve unified
modeling from local anomaly detection to global call chain tracing, which
enables the identification of abnormal service nodes and the reconstruction of
potential anomaly propagation paths. Sensitivity experiments are then designed
from multiple dimensions, including hyperparameters, environmental
disturbances, and data distribution, to evaluate the framework, and results
show that it outperforms baseline methods in key metrics such as AUC, ACC,
Recall, and F1-Score, maintaining high accuracy and stability under dynamic
topologies and complex environments. This research not only provides a new
technical path for anomaly detection in microservices but also lays a
methodological foundation for intelligent operations in distributed systems.

</details>


### [96] [Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods](https://arxiv.org/abs/2511.03304)
*Felix Störck,Fabian Hinder,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种用于核方法的连续公平性方法，通过核嵌入处理连续保护属性，在支持向量回归中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在日常社会生活中的广泛应用，公平性成为重要议题。现有研究主要关注离散属性，而连续属性（特别是回归问题中的连续公平性）研究较少。

Method: 将迭代零空间投影方法推广到核方法，提出模型和公平性分数无关的核嵌入方法，适用于连续保护属性。

Result: 与支持向量回归结合使用时，该方法在多个数据集上表现出竞争性或改进的性能。

Conclusion: 该方法显著扩展了连续公平性研究的范围，为处理连续保护属性提供了有效的核方法解决方案。

Abstract: With the on-going integration of machine learning systems into the everyday
social life of millions the notion of fairness becomes an ever increasing
priority in their development. Fairness notions commonly rely on protected
attributes to assess potential biases. Here, the majority of literature focuses
on discrete setups regarding both target and protected attributes. The
literature on continuous attributes especially in conjunction with regression
-- we refer to this as \emph{continuous fairness} -- is scarce. A common
strategy is iterative null-space projection which as of now has only been
explored for linear models or embeddings such as obtained by a non-linear
encoder. We improve on this by generalizing to kernel methods, significantly
extending the scope. This yields a model and fairness-score agnostic method for
kernel embeddings applicable to continuous protected attributes. We demonstrate
that our novel approach in conjunction with Support Vector Regression (SVR)
provides competitive or improved performance across multiple datasets in
comparisons to other contemporary methods.

</details>


### [97] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir Demirović*

Main category: cs.LG

TL;DR: SORTD是一个新颖框架，用于高效枚举Rashomon集合（性能相似但结构不同的决策树），相比现有技术将运行时间减少了两个数量级，支持可分离和全序目标函数。


<details>
  <summary>Details</summary>
Motivation: 稀疏决策树学习提供准确且可解释的预测模型，但单一"最佳"树可能无法满足所有需求。Rashomon集合包含性能相似但结构不同的树，可以增强变量重要性分析、丰富解释性，并让用户根据偏好选择更简单或满足特定标准（如公平性）的树。

Method: 提出SORTD框架，按目标值顺序枚举Rashomon集合中的树，提供随时可用的行为。支持任何可分离和全序的目标函数，并支持使用其他可分离（部分有序）目标函数进行后评估。

Result: 实验表明SORTD相比现有技术将运行时间减少了两个数量级，能够更高效地计算Rashomon集合。

Conclusion: SORTD使探索Rashomon集合在现实应用中更加实用，为高风险应用提供了更好的决策树选择灵活性。

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [98] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: 提出了一种无需数据的模块化多标签意图识别管道DMTC，通过提示工程生成合成查询、Sentence-T5编码和在线焦点对比损失训练，在交通领域应用中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别系统依赖大量标注数据且难以处理细粒度多标签分类，需要消除昂贵数据收集需求并提高多标签意图理解的准确性。

Method: 三步骤管道：1) 使用提示工程引导LLM生成多样化合成查询；2) 用Sentence-T5模型编码文本查询获得语义嵌入；3) 使用新颖的在线焦点对比损失训练轻量级分类器。

Result: 在海事交通场景中，DMTC实现5.35%的汉明损失和95.92%的AUC，优于最先进的多标签分类器和LLM基线。Sentence-T5嵌入比替代编码器提高至少3.29%的子集准确率，OFC损失相比标准对比目标带来额外0.98%增益。

Conclusion: 该系统能够无缝地将用户查询路由到特定任务模块，为无需昂贵人工标注的完全自主意图感知智能体奠定了基础。

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [99] [TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets](https://arxiv.org/abs/2511.03368)
*Hongrun Ren,Yun Xiong,Lei You,Yingying Wang,Haixu Xiong,Yangyong Zhu*

Main category: cs.LG

TL;DR: 提出了一个统一的数据-模型耦合市场，将数据集和模型交易作为一个单一系统处理，通过供需映射形成闭环，保证均衡价格的存在性、唯一性和全局收敛性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型经济的兴起使训练数据集和预训练模型市场相互交织，但现有定价方法仍将数据和模型交易分离，或依赖偏向一方的代理中心化流程，缺乏同时覆盖数据卖家、模型生产者和模型买家的对称机制。

Method: 通过供应端映射将数据集支付转化为买家可见的模型报价，需求端映射通过基于Shapley值的分配将买家价格传播回数据集，形成连接四个交互的闭环：双向供需传播以及买卖双方之间的相互耦合。

Result: 证明联合算子是一个标准干扰函数(SIF)，保证均衡价格的存在性、唯一性和全局收敛性。实验显示相比代理中心和单边基线，该方法能高效收敛并提高公平性。

Conclusion: 提出的统一数据-模型耦合市场机制能够有效解决数据与模型市场的分离问题，实现三方共赢的定价策略。

Abstract: The rise of the machine learning (ML) model economy has intertwined markets
for training datasets and pre-trained models. However, most pricing approaches
still separate data and model transactions or rely on broker-centric pipelines
that favor one side. Recent studies of data markets with externalities capture
buyer interactions but do not yield a simultaneous and symmetric mechanism
across data sellers, model producers, and model buyers. We propose a unified
data-model coupled market that treats dataset and model trading as a single
system. A supply-side mapping transforms dataset payments into buyer-visible
model quotations, while a demand-side mapping propagates buyer prices back to
datasets through Shapley-based allocation. Together, they form a closed loop
that links four interactions: supply-demand propagation in both directions and
mutual coupling among buyers and among sellers. We prove that the joint
operator is a standard interference function (SIF), guaranteeing existence,
uniqueness, and global convergence of equilibrium prices. Experiments
demonstrate efficient convergence and improved fairness compared with
broker-centric and one-sided baselines. The code is available on
https://github.com/HongrunRen1109/Triple-Win-Pricing.

</details>


### [100] [Adaptable Hindsight Experience Replay for Search-Based Learning](https://arxiv.org/abs/2511.03405)
*Alexandros Vazaios,Jannis Brugger,Cedric Derstroff,Kristian Kersting,Mira Mezini*

Main category: cs.LG

TL;DR: 提出了Adaptable HER框架，将Hindsight Experience Replay与AlphaZero结合，通过重新标记搜索树中的失败轨迹来改进稀疏奖励环境下的训练效果。


<details>
  <summary>Details</summary>
Motivation: AlphaZero在稀疏奖励环境下早期训练阶段网络无法提供有效指导，传统训练方法受限。

Method: 开发Adaptable HER框架，灵活调整HER的重标记目标、策略目标和轨迹选择等属性，并与AlphaZero集成。

Result: 实验（包括方程发现）表明该方法优于纯监督学习或强化学习。

Conclusion: 修改HER的可能性是有益的，Adaptable HER框架在稀疏奖励问题中表现出色。

Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for
two-player games, dynamically balance exploration and exploitation using neural
network guidance. This combination makes them also suitable for classical
search problems. However, the original method of training the network with
simulation results is limited in sparse reward settings, especially in the
early stages, where the network cannot yet give guidance. Hindsight Experience
Replay (HER) addresses this issue by relabeling unsuccessful trajectories from
the search tree as supervised learning signals. We introduce Adaptable HER
(\ours{}), a flexible framework that integrates HER with AlphaZero, allowing
easy adjustments to HER properties such as relabeled goals, policy targets, and
trajectory selection. Our experiments, including equation discovery, show that
the possibility of modifying HER is beneficial and surpasses the performance of
pure supervised or reinforcement learning.

</details>


### [101] [POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding](https://arxiv.org/abs/2511.03464)
*Mihriban Kocak Balik,Pekka Marttinen,Negar Safinianaini*

Main category: cs.LG

TL;DR: POEMS是一个可解释的多组学集成框架，通过稀疏解码和专家乘积模型保持预测性能的同时提供可解释性，解决了深度生成模型中预测性能与可解释性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型在预测性能与可解释性之间存在权衡：要么牺牲可解释性追求预测性能，要么通过线性化解码器来强制可解释性但削弱非线性表达能力。需要克服这一权衡。

Method: 1) 使用稀疏连接将特征映射到潜在因子，直接转化为生物标志物发现；2) 通过专家乘积模型在共享潜在空间中实现跨组学关联；3) 通过门控网络自适应计算每个组学在表示学习中的贡献；4) 提出高效的稀疏解码器。

Result: 在癌症亚型分型案例研究中，POEMS实现了具有竞争力的聚类和分类性能，同时提供了新颖的可解释性见解，证明生物标志物洞察和预测准确性可以在多组学表示学习中并存。

Conclusion: POEMS框架表明，在多组学表示学习中，基于生物标志物的洞察力和预测准确性可以共存，无需在网络中线性化任何部分即可实现可解释性。

Abstract: Integrating different molecular layers, i.e., multiomics data, is crucial for
unraveling the complexity of diseases; yet, most deep generative models either
prioritize predictive performance at the expense of interpretability or enforce
interpretability by linearizing the decoder, thereby weakening the network's
nonlinear expressiveness. To overcome this tradeoff, we introduce POEMS:
Product Of Experts for Interpretable Multiomics Integration using Sparse
Decoding, an unsupervised probabilistic framework that preserves predictive
performance while providing interpretability. POEMS provides interpretability
without linearizing any part of the network by 1) mapping features to latent
factors using sparse connections, which directly translates to biomarker
discovery, 2) allowing for cross-omic associations through a shared latent
space using product of experts model, and 3) reporting contributions of each
omic by a gating network that adaptively computes their influence in the
representation learning. Additionally, we present an efficient sparse decoder.
In a cancer subtyping case study, POEMS achieves competitive clustering and
classification performance while offering our novel set of interpretations,
demonstrating that biomarker based insight and predictive accuracy can coexist
in multiomics representation learning.

</details>


### [102] [Efficient Neural Networks with Discrete Cosine Transform Activations](https://arxiv.org/abs/2511.03531)
*Marc Martinez-Gost,Sara Pepe,Ana Pérez-Neira,Miguel Ángel Lagunas*

Main category: cs.LG

TL;DR: 本文扩展了表达性神经网络(ENN)的研究，这是一种使用离散余弦变换(DCT)参数化自适应激活函数的多层感知机。该框架强调效率、可解释性和剪枝能力，通过DCT参数化实现结构化表示，可识别冗余组件并进行高效剪枝。


<details>
  <summary>Details</summary>
Motivation: 基于先前证明ENN具有强大表达能力的成果，本研究进一步强调其效率、可解释性和剪枝能力，旨在将信号处理概念原则性地整合到神经网络设计中。

Method: 使用DCT参数化自适应激活函数，提供结构化和去相关的表示，揭示每个神经元的功能角色，并基于此提出高效的剪枝策略，移除不必要的DCT系数。

Result: 在分类和隐式神经表示任务上的实验结果表明，ENN在保持参数数量较低的同时达到最先进精度，且由于DCT基的正交性和有界性，可安全剪枝高达40%的激活系数。

Conclusion: ENN框架在信号处理概念与神经网络设计之间实现了原则性整合，在表达能力、紧凑性和可解释性之间达到了平衡的权衡。

Abstract: In this paper, we extend our previous work on the Expressive Neural Network
(ENN), a multilayer perceptron with adaptive activation functions parametrized
using the Discrete Cosine Transform (DCT). Building upon previous work that
demonstrated the strong expressiveness of ENNs with compact architectures, we
now emphasize their efficiency, interpretability and pruning capabilities. The
DCT-based parameterization provides a structured and decorrelated
representation that reveals the functional role of each neuron and allows
direct identification of redundant components. Leveraging this property, we
propose an efficient pruning strategy that removes unnecessary DCT coefficients
with negligible or no loss in performance. Experimental results across
classification and implicit neural representation tasks confirm that ENNs
achieve state-of-the-art accuracy while maintaining a low number of parameters.
Furthermore, up to 40% of the activation coefficients can be safely pruned,
thanks to the orthogonality and bounded nature of the DCT basis. Overall, these
findings demonstrate that the ENN framework offers a principled integration of
signal processing concepts into neural network design, achieving a balanced
trade-off between expressiveness, compactness, and interpretability.

</details>


### [103] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 本文提出了一个将已知群对称性融入基于核的强化学习的理论和算法框架，开发了对称感知的乐观最小二乘值迭代方法，通过不变核编码奖励和转移动态的不变性，显著提升了样本效率。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的强化学习环境中存在固有的对称性，可以利用这些对称性来提高学习效率。

Method: 提出了对称感知的乐观最小二乘值迭代（LSVI）方法，使用不变核来编码奖励和转移动态的不变性。

Result: 理论分析建立了不变RKHS的最大信息增益和覆盖数的新界限，实证结果在定制Frozen Lake环境和2D布局设计问题上证实了对称感知RL相比标准核方法有显著更好的性能。

Conclusion: 这些发现凸显了结构先验在设计更样本高效的强化学习算法中的价值。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [104] [RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse](https://arxiv.org/abs/2511.03475)
*Yinsicheng Jiang,Yeqi Huang,Liang Cheng,Cheng Deng,Xuan Sun,Luo Mai*

Main category: cs.LG

TL;DR: RAGBoost是一个高效的检索增强生成系统，通过准确保持的上下文重用实现高缓存复用率而不牺牲准确性，将现有LLM推理引擎的预填充性能提升1.5-3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成(RAG)系统在预填充性能上存在瓶颈，特别是当现代应用需要更长更复杂的输入时。现有缓存技术要么保持准确性但缓存复用率低，要么提高复用率但牺牲推理质量。

Method: RAGBoost通过检测并发会话和多轮交互中的重叠检索项，使用高效的上下文索引、排序和去重来最大化复用，同时通过轻量级上下文提示保持推理保真度。

Result: RAGBoost将现有LLM推理引擎的预填充性能提升1.5-3倍，同时在多样化的RAG和代理AI工作负载中保持甚至提高了推理准确性。

Conclusion: RAGBoost展示了在不牺牲准确性的情况下实现高效上下文重用的可行性，为RAG系统提供了显著的性能提升。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.

</details>


### [105] [NAP: Attention-Based Late Fusion for Automatic Sleep Staging](https://arxiv.org/abs/2511.03488)
*Alvise Dei Rossi,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Luigi Fiorillo,Francesca Faraci*

Main category: cs.LG

TL;DR: NAP是一个基于注意力的模型，通过三轴注意力机制聚合多通道预测，能够适应不同输入维度，在睡眠分期任务中实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 多导睡眠图信号具有高度异质性，现有模型依赖固定模态或通道子集，未能充分利用其多模态特性。

Method: 使用三轴注意力机制（时间、空间、预测器级）聚合预训练单通道模型的输出，适应不同输入维度。

Result: NAP持续优于单个预测器和简单集成方法，在多个数据集上实现最先进的零样本泛化性能。

Conclusion: 该方法可扩展到其他多模态生理应用，展示了在异质多模态数据上的有效泛化能力。

Abstract: Polysomnography signals are highly heterogeneous, varying in modality
composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal,
occipital EEG), and acquisition protocols across datasets and clinical sites.
Most existing models that process polysomnography data rely on a fixed subset
of modalities or channels and therefore neglect to fully exploit its inherently
multimodal nature. We address this limitation by introducing NAP (Neural
Aggregator of Predictions), an attention-based model which learns to combine
multiple prediction streams using a tri-axial attention mechanism that captures
temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to
different input dimensions. By aggregating outputs from frozen, pretrained
single-channel models, NAP consistently outperforms individual predictors and
simple ensembles, achieving state-of-the-art zero-shot generalization across
multiple datasets. While demonstrated in the context of automated sleep staging
from polysomnography, the proposed approach could be extended to other
multimodal physiological applications.

</details>


### [106] [Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances](https://arxiv.org/abs/2511.03565)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 这篇论文是关于模仿学习最新进展的综述，提出了新的分类法来反映当前研究现状和趋势，批判性地分析了代表性工作的优缺点和评估实践，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，模仿学习的能力和可扩展性显著提升，出现了许多新方法来解决长期存在的挑战如泛化、协变量漂移和演示质量等问题。

Method: 作者回顾了模仿学习的最新研究进展，提出了一个与现有分类不同的新分类法，以更好地反映当前IL研究层次和趋势。

Result: 论文系统地总结了模仿学习领域的最新趋势、方法创新和实际应用，并对代表性工作进行了批判性分析。

Conclusion: 论文概述了模仿学习的关键挑战和未来研究的开放方向，为该领域的发展提供了重要参考。

Abstract: Imitation learning (IL) enables agents to acquire skills by observing and
replicating the behavior of one or multiple experts. In recent years, advances
in deep learning have significantly expanded the capabilities and scalability
of imitation learning across a range of domains, where expert data can range
from full state-action trajectories to partial observations or unlabeled
sequences. Alongside this growth, novel approaches have emerged, with new
methodologies being developed to address longstanding challenges such as
generalization, covariate shift, and demonstration quality. In this survey, we
review the latest advances in imitation learning research, highlighting recent
trends, methodological innovations, and practical applications. We propose a
novel taxonomy that is distinct from existing categorizations to better reflect
the current state of the IL research stratum and its trends. Throughout the
survey, we critically examine the strengths, limitations, and evaluation
practices of representative works, and we outline key challenges and open
directions for future research.

</details>


### [107] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: GRPO作为PPO的可扩展替代方案，通过轨迹组间比较估计优势值而无需学习critic。研究发现：学习critic在长时域任务中仍不可或缺；GRPO受益于高折扣因子；小批量分组策略优于大批量。


<details>
  <summary>Details</summary>
Motivation: 探究在策略梯度方法中学习baseline的必要性，系统研究GRPO在经典强化学习环境中的表现。

Method: 在离散和连续控制任务中进行受控消融实验，隔离baseline、折扣因子和分组采样的影响。

Result: 学习critic在长时域任务中表现更佳；GRPO在高折扣因子下表现更好；小批量分组策略效果更优。

Conclusion: 揭示了无critic方法在经典控制任务中的局限性，以及在特定条件下作为学习值函数替代方案的可行性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [108] [Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations](https://arxiv.org/abs/2511.03578)
*Mainak Singha*

Main category: cs.LG

TL;DR: CPL框架通过在物理约束集合上投影网络输出，确保神经网络求解偏微分方程时严格遵守物理定律，包括守恒律、熵条件等，消除了物理违规现象。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络求解偏微分方程时经常违反物理定律，如不守恒、熵增错误等，需要一种能保证物理合规性的训练方法。

Method: 使用约束投影学习(CPL)，将网络输出投影到由守恒、Rankine-Hugoniot平衡、熵和正定性定义的约束集合交集上；结合总变差阻尼(TVD)抑制振荡和滚动课程训练确保长期一致性。

Result: 在Burgers和Euler系统上，CPL产生稳定、物理合规的解，守恒律达到机器精度，总变差不增长，熵和误差有界，计算开销仅增加约10%。

Conclusion: CPL使物理合规性成为学习过程的内在属性，而非期望神经网络自发遵守物理定律。

Abstract: Neural networks can approximate solutions to partial differential equations,
but they often break the very laws they are meant to model-creating mass from
nowhere, drifting shocks, or violating conservation and entropy. We address
this by training within the laws of physics rather than beside them. Our
framework, called Constraint-Projected Learning (CPL), keeps every update
physically admissible by projecting network outputs onto the intersection of
constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and
positivity. The projection is differentiable and adds only about 10%
computational overhead, making it fully compatible with back-propagation. We
further stabilize training with total-variation damping (TVD) to suppress small
oscillations and a rollout curriculum that enforces consistency over long
prediction horizons. Together, these mechanisms eliminate both hard and soft
violations: conservation holds at machine precision, total-variation growth
vanishes, and entropy and error remain bounded. On Burgers and Euler systems,
CPL produces stable, physically lawful solutions without loss of accuracy.
Instead of hoping neural solvers will respect physics, CPL makes that behavior
an intrinsic property of the learning process.

</details>


### [109] [Byzantine-Robust Federated Learning with Learnable Aggregation Weights](https://arxiv.org/abs/2511.03529)
*Javad Parsa,Amir Hossein Daghestani,André M. H. Teixeira,Mikael Johansson*

Main category: cs.LG

TL;DR: 提出了一种新的拜占庭鲁棒联邦学习优化方法，通过将聚合权重作为可学习参数与全局模型参数联合优化，在异构数据和恶意客户端存在的情况下显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中恶意（拜占庭）客户端的存在对系统鲁棒性构成严重威胁，特别是在客户端数据分布异构的情况下，传统方法难以有效应对。

Method: 提出拜占庭鲁棒联邦学习优化问题，将聚合权重作为可学习参数；开发具有强收敛保证的交替最小化算法；分析目标函数的拜占庭弹性。

Result: 在各种数据集和攻击场景下与最先进方法对比，实验结果表明该方法始终优于现有方法，特别是在高度异构数据和大量恶意客户端的情况下表现更佳。

Conclusion: 所提出的自适应加权聚合方法能有效提升联邦学习在拜占庭攻击下的鲁棒性，为解决异构数据环境中的安全问题提供了有效解决方案。

Abstract: Federated Learning (FL) enables clients to collaboratively train a global
model without sharing their private data. However, the presence of malicious
(Byzantine) clients poses significant challenges to the robustness of FL,
particularly when data distributions across clients are heterogeneous. In this
paper, we propose a novel Byzantine-robust FL optimization problem that
incorporates adaptive weighting into the aggregation process. Unlike
conventional approaches, our formulation treats aggregation weights as
learnable parameters, jointly optimizing them alongside the global model
parameters. To solve this optimization problem, we develop an alternating
minimization algorithm with strong convergence guarantees under adversarial
attack. We analyze the Byzantine resilience of the proposed objective. We
evaluate the performance of our algorithm against state-of-the-art
Byzantine-robust FL approaches across various datasets and attack scenarios.
Experimental results demonstrate that our method consistently outperforms
existing approaches, particularly in settings with highly heterogeneous data
and a large proportion of malicious clients.

</details>


### [110] [Flat Minima and Generalization: Insights from Stochastic Convex Optimization](https://arxiv.org/abs/2511.03548)
*Matan Schliserman,Shira Vansover-Hager,Tomer Koren*

Main category: cs.LG

TL;DR: 本文研究发现，在随机凸优化设置中，平坦最小值可能导致较差的泛化性能，而尖锐最小值反而能实现最优泛化。两种基于平坦度感知的算法（SA-GD和SAM）虽然能找到平坦解，但泛化风险仍可能很高。


<details>
  <summary>Details</summary>
Motivation: 理解学习算法的泛化行为是学习理论的核心目标。近年来流行的一种解释是，学习算法在实践中成功是因为它们收敛到平坦最小值，这与改进的泛化性能相关。本文旨在研究平坦最小值与泛化之间的关联。

Method: 在非负β-光滑目标的随机凸优化经典设置中，分析平坦最小值与泛化的关系。研究两种平坦度感知算法：SA-GD（在预定义邻域内对最大损失执行梯度步骤）和SAM（基于归一化上升步骤的SA-GD计算高效近似）。

Result: 1. 平坦经验最小值可能产生Ω(1)的总体风险，而尖锐最小值能实现最优泛化；2. SA-GD能快速收敛到平坦最小值，但总体风险仍可能高达Ω(1)；3. SAM虽然最小化经验损失，但可能收敛到尖锐最小值，同样产生Ω(1)的总体风险。

Conclusion: 即使在基本的随机凸优化设置中，平坦最小值也不能保证良好的泛化性能。平坦度感知算法（SA-GD和SAM）找到的平坦解可能泛化较差，挑战了平坦最小值必然带来更好泛化的普遍观点。

Abstract: Understanding the generalization behavior of learning algorithms is a central
goal of learning theory. A recently emerging explanation is that learning
algorithms are successful in practice because they converge to flat minima,
which have been consistently associated with improved generalization
performance. In this work, we study the link between flat minima and
generalization in the canonical setting of stochastic convex optimization with
a non-negative, $\beta$-smooth objective. Our first finding is that, even in
this fundamental and well-studied setting, flat empirical minima may incur
trivial $\Omega(1)$ population risk while sharp minima generalizes optimally.
Then, we show that this poor generalization behavior extends to two natural
''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),
designed to bias optimization toward flat solutions: Sharpness-Aware Gradient
Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which
performs gradient steps on the maximal loss in a predefined neighborhood, we
prove that while it successfully converges to a flat minimum at a fast rate,
the population risk of the solution can still be as large as $\Omega(1)$,
indicating that even flat minima found algorithmically using a sharpness-aware
gradient method might generalize poorly. For SAM, a computationally efficient
approximation of SA-GD based on normalized ascent steps, we show that although
it minimizes the empirical loss, it may converge to a sharp minimum and also
incur population risk $\Omega(1)$. Finally, we establish population risk upper
bounds for both SA-GD and SAM using algorithmic stability techniques.

</details>


### [111] [DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay](https://arxiv.org/abs/2511.03670)
*Daniel Perkins,Oscar J. Escobar,Luke Green*

Main category: cs.LG

TL;DR: 对DQN在有限环境中的研究，重点分析ε-greedy探索策略和优先经验回放的影响，提供实用的强化学习建议。


<details>
  <summary>Details</summary>
Motivation: 研究DQN训练中探索策略与记忆管理之间的权衡和交互作用，为资源受限环境提供稳健的强化学习方案。

Method: 通过系统实验评估ε衰减调度对学习效率、收敛行为和奖励优化的影响，比较均匀、无回放和优先回放策略。

Result: 优先经验回放能带来更快的收敛速度和更高的回报，实验结果显示不同策略在多个模拟中的表现差异。

Conclusion: 揭示了探索策略与记忆管理之间的权衡关系，为资源受限环境下的DQN训练提供了实用建议。

Abstract: We present a detailed study of Deep Q-Networks in finite environments,
emphasizing the impact of epsilon-greedy exploration schedules and prioritized
experience replay. Through systematic experimentation, we evaluate how
variations in epsilon decay schedules affect learning efficiency, convergence
behavior, and reward optimization. We investigate how prioritized experience
replay leads to faster convergence and higher returns and show empirical
results comparing uniform, no replay, and prioritized strategies across
multiple simulations. Our findings illuminate the trade-offs and interactions
between exploration strategies and memory management in DQN training, offering
practical recommendations for robust reinforcement learning in
resource-constrained settings.

</details>


### [112] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*Günther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: TabGemma是一个用于表格预测的LLM模型，通过科学记数法处理数字和n-gram检索选择示例，在语义丰富的分类任务中达到SOTA，但在回归任务中数据量大时不如传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在表格预测中面临的两个实际问题：不稳定的数字标记化和有限的上下文大小，使LLM能够更好地处理包含文本、数值和分类字段的混合表格数据。

Method: 1. 使用带符号科学记数法规范化数字；2. 在12B Gemma 3模型上使用大规模真实数据集进行目标插补目标的继续预训练；3. 使用紧凑的n-gram检索选择信息丰富的示例以适应128k标记窗口。

Result: 在语义丰富的基准测试中，TabGemma在分类任务（低数据和高数据场景）中达到新的SOTA，且性能随上下文行数增加而单调提升；在回归任务中，小样本时具有竞争力，但数据量大时落后于传统方法。

Conclusion: LLM在配备专用数字处理和上下文检索的情况下，可以成为语义丰富表格任务的有效上下文学习器，但需要在数字建模和长上下文扩展方面进一步改进。

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [113] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*Eugène Berta,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: 本文提出了基于逻辑回归的参数化后验校准方法，通过结构化正则化、鲁棒预处理和高效优化来解决多类校准中的过拟合问题，显著提升了现有校准技术的性能。


<details>
  <summary>Details</summary>
Motivation: 参数化后验校准方法广泛应用于确保分类器提供可信的概率估计。本文旨在从理论角度为基于逻辑回归的校准函数提供动机，并解决多类校准中由于参数增加和校准数据有限导致的过拟合挑战。

Method: 使用基于逻辑回归的参数化校准函数，通过结构化正则化、鲁棒预处理和高效优化技术来管理偏差-方差权衡，避免过拟合问题。

Result: 通过大量实验证明，该方法在管理偏差-方差权衡方面表现有效，相比现有的基于逻辑回归的校准技术（如温度缩放、向量缩放和矩阵缩放）取得了显著提升。

Conclusion: 提出的方法为多类校准提供了有效的解决方案，通过结构化正则化和优化技术克服了过拟合问题，并提供了高效易用的开源实现，成为现有校准方法的有力替代方案。

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [114] [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)
*Mohsen Ahmadzadeh,Kaichang Chen,Georges Gielen*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体LLM的模拟电路尺寸设计框架AnaFlow，通过智能体协作实现样本高效且可解释的电路参数优化


<details>
  <summary>Details</summary>
Motivation: 传统模拟电路设计依赖手工过程，设计周期长且易出错；现有AI方法需要大量耗时仿真且缺乏可解释性

Method: 采用多智能体工作流，LLM智能体协作解释电路拓扑、理解设计目标，并通过可解释推理迭代优化电路参数，结合自适应仿真策略

Result: 在两种不同复杂度的电路上成功完成全自动尺寸设计，相比纯贝叶斯优化和强化学习方法更有效，能够从优化历史中学习避免错误并加速收敛

Conclusion: AnaFlow为模拟EDA提供了透明设计助手的新范式，其内在可解释性使其成为强大的模拟设计空间探索工具

Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the
physical world. Their design, however, remains a largely handcrafted process,
resulting in long and error-prone design cycles. While the recent rise of
AI-based reinforcement learning and generative AI has created new techniques to
automate this task, the need for many time-consuming simulations is a critical
bottleneck hindering the overall efficiency. Furthermore, the lack of
explainability of the resulting design solutions hampers widespread adoption of
the tools. To address these issues, a novel agentic AI framework for
sample-efficient and explainable analog circuit sizing is presented. It employs
a multi-agent workflow where specialized Large Language Model (LLM)-based
agents collaborate to interpret the circuit topology, to understand the design
goals, and to iteratively refine the circuit's design parameters towards the
target goals with human-interpretable reasoning. The adaptive simulation
strategy creates an intelligent control that yields a high sample efficiency.
The AnaFlow framework is demonstrated for two circuits of varying complexity
and is able to complete the sizing task fully automatically, differently from
pure Bayesian optimization and reinforcement learning approaches. The system
learns from its optimization history to avoid past mistakes and to accelerate
convergence. The inherent explainability makes this a powerful tool for analog
design space exploration and a new paradigm in analog EDA, where AI agents
serve as transparent design assistants.

</details>


### [115] [Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning](https://arxiv.org/abs/2511.03616)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 提出深度隐式模仿强化学习框架，结合深度强化学习和仅观察数据集的隐式模仿学习，解决传统模仿学习需要完整状态-动作演示和最优专家的限制。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要完整的状态-动作演示和最优专家，这严重限制了实际应用，因为许多现实场景只提供状态观察而没有对应动作，且专家表现往往次优。

Method: 主要算法DIIQN采用动作推断机制通过在线探索重建专家动作，并集成动态置信机制自适应平衡专家引导和自主学习。HA-DIIQN算法处理专家和智能体具有不同动作集的情况，引入不可行性检测机制和桥接过程。

Result: DIIQN相比标准DQN实现高达130%的更高回合回报，持续优于现有无法超越专家表现的隐式模仿方法。在异构动作设置中，HA-DIIQN比基线学习速度快64%，可利用传统方法无法使用的专家数据集。

Conclusion: 该框架通过结合隐式模仿学习和强化学习，能够利用专家指导加速训练，同时保持超越次优专家表现的能力，在异构动作场景中表现出色。

Abstract: Imitation learning traditionally requires complete state-action
demonstrations from optimal or near-optimal experts. These requirements
severely limit practical applicability, as many real-world scenarios provide
only state observations without corresponding actions and expert performance is
often suboptimal. In this paper we introduce a deep implicit imitation
reinforcement learning framework that addresses both limitations by combining
deep reinforcement learning with implicit imitation learning from
observation-only datasets. Our main algorithm, Deep Implicit Imitation
Q-Network (DIIQN), employs an action inference mechanism that reconstructs
expert actions through online exploration and integrates a dynamic confidence
mechanism that adaptively balances expert-guided and self-directed learning.
This enables the agent to leverage expert guidance for accelerated training
while maintaining capacity to surpass suboptimal expert performance. We further
extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to
tackle scenarios where expert and agent possess different action sets, a
challenge previously unaddressed in the implicit imitation learning literature.
HA-DIIQN introduces an infeasibility detection mechanism and a bridging
procedure identifying alternative pathways connecting agent capabilities to
expert guidance when direct action replication is impossible. Our experimental
results demonstrate that DIIQN achieves up to 130% higher episodic returns
compared to standard DQN, while consistently outperforming existing implicit
imitation methods that cannot exceed expert performance. In heterogeneous
action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging
expert datasets unusable by conventional approaches. Extensive parameter
sensitivity analysis reveals the framework's robustness across varying dataset
sizes and hyperparameter configurations.

</details>


### [116] [Financial Management System for SMEs: Real-World Deployment of Accounts Receivable and Cash Flow Prediction](https://arxiv.org/abs/2511.03631)
*Bartłomiej Małkus,Szymon Bobek,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 为中小企业开发了一个集成财务预测系统，包含应收账款预测和现金流预测功能，专门针对资源有限的小企业需求。


<details>
  <summary>Details</summary>
Motivation: 中小企业和自由职业者面临独特的财务管理挑战，包括资源有限、客户基础小和数据可用性受限，现有企业级财务工具无法满足他们的实际需求。

Method: 系统包含两个关键组件：用于预测发票付款延迟的二元分类模型，以及处理不完整和有限历史数据的多模块现金流预测模型。原型系统已作为Web应用程序部署。

Result: 系统已成功集成到Cluee平台（为自由职业者提供财务管理工具的初创公司），证明了在实际中小企业财务管理中的可行性。

Conclusion: 该系统填补了企业级财务工具与中小企业实际需求之间的空白，为资源受限的小企业提供了实用的财务预测解决方案。

Abstract: Small and Medium Enterprises (SMEs), particularly freelancers and early-stage
businesses, face unique financial management challenges due to limited
resources, small customer bases, and constrained data availability. This paper
presents the development and deployment of an integrated financial prediction
system that combines accounts receivable prediction and cash flow forecasting
specifically designed for SME operational constraints. Our system addresses the
gap between enterprise-focused financial tools and the practical needs of
freelancers and small businesses. The solution integrates two key components: a
binary classification model for predicting invoice payment delays, and a
multi-module cash flow forecasting model that handles incomplete and limited
historical data. A prototype system has been implemented and deployed as a web
application with integration into Cluee's platform, a startup providing
financial management tools for freelancers, demonstrating practical feasibility
for real-world SME financial management.

</details>


### [117] [nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN](https://arxiv.org/abs/2511.03634)
*Alexander Pfefferle,Johannes Hog,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: nanoTabPFN是一个简化的轻量级TabPFN v2实现，通过预生成训练数据，在单GPU上1分钟内完成预训练，性能与传统机器学习基线相当，使表格基础模型更易于学生和研究人员使用。


<details>
  <summary>Details</summary>
Motivation: 现有开源表格基础模型实现复杂（超过1万行代码），缺乏架构文档和代码质量，难以理解、不友好且难以适应新实验，阻碍了学生和研究人员的可访问性。

Method: 简化TabPFN v2架构和训练循环，使用预生成的训练数据，在单GPU上进行快速预训练。

Result: 在小数据设置下，1分钟内预训练性能与传统机器学习基线相当，比TabPFN v2预训练快160,000倍。

Conclusion: nanoTabPFN消除了对大型计算资源的需求，使表格基础模型的预训练在教育目的上变得可访问。

Abstract: Tabular foundation models such as TabPFN have revolutionized predictive
machine learning for tabular data. At the same time, the driving factors of
this revolution are hard to understand. Existing open-source tabular foundation
models are implemented in complicated pipelines boasting over 10,000 lines of
code, lack architecture documentation or code quality. In short, the
implementations are hard to understand, not beginner-friendly, and complicated
to adapt for new experiments. We introduce nanoTabPFN, a simplified and
lightweight implementation of the TabPFN v2 architecture and a corresponding
training loop that uses pre-generated training data. nanoTabPFN makes tabular
foundation models more accessible to students and researchers alike. For
example, restricted to a small data setting it achieves a performance
comparable to traditional machine learning baselines within one minute of
pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This
eliminated requirement of large computational resources makes pre-training
tabular foundation models accessible for educational purposes. Our code is
available at https://github.com/automl/nanoTabPFN.

</details>


### [118] [SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection](https://arxiv.org/abs/2511.03661)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.LG

TL;DR: 本研究提出了一个机器学习驱动的框架，用于检测医疗物联网中的恶意网络攻击和设备故障异常，评估了8种机器学习模型在三种学习范式下的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网设备的集成带来了严重的安全性和可靠性挑战，增加了遭受网络威胁和操作异常的风险，需要有效的异常检测策略来保障医疗设备的安全运行。

Method: 使用包含20万条记录的数据集，评估了8种机器学习模型：监督学习（XGBoost、KNN）、半监督学习（GAN、VAE）和无监督学习（One-Class SVM、Isolation Forest、GNN、LSTM Autoencoders），采用F1分数、精确率、召回率、准确率、ROC-AUC和计算效率等多指标进行综合评估。

Result: XGBoost在异常检测中达到99%准确率且计算开销最小（0.04秒），Isolation Forest在精确率和召回率间取得良好平衡。KNN在攻击检测中实现近乎完美的性能且计算成本最低（0.05秒），VAE达到97%准确率。GAN计算成本最高且性能最差。

Conclusion: 该框架通过有效的异常检测策略增强了医疗物联网安全性，能够早期发现网络威胁和设备故障，预防数据泄露，减少系统停机时间，确保医疗设备持续安全运行，保护患者健康和信任。

Abstract: The integration of IoT devices in healthcare introduces significant security
and reliability challenges, increasing susceptibility to cyber threats and
operational anomalies. This study proposes a machine learning-driven framework
for (1) detecting malicious cyberattacks and (2) identifying faulty device
anomalies, leveraging a dataset of 200,000 records. Eight machine learning
models are evaluated across three learning approaches: supervised learning
(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative
Adversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised
learning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph
Neural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The
comprehensive evaluation was conducted across multiple metrics like F1-score,
precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost
achieved 99\% accuracy with minimal computational overhead (0.04s) for anomaly
detection, while Isolation Forest balanced precision and recall effectively.
LSTM Autoencoders underperformed with lower accuracy and higher latency. For
attack detection, KNN achieved near-perfect precision, recall, and F1-score
with the lowest computational cost (0.05s), followed by VAE at 97% accuracy.
GAN showed the highest computational cost with lowest accuracy and ROC-AUC.
These findings enhance IoT-enabled healthcare security through effective
anomaly detection strategies. By improving early detection of cyber threats and
device failures, this framework has the potential to prevent data breaches,
minimize system downtime, and ensure the continuous and safe operation of
medical devices, ultimately safeguarding patient health and trust in IoT-driven
healthcare solutions.

</details>


### [119] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ框架通过行为一致性信号实现离线到在线强化学习的平滑过渡，利用离线数据中的隐式行为模型在在线微调期间提供指导，减少分布偏移影响。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习训练的策略在动态环境中部署时，由于分布偏移和未见状态-动作对上的不可靠值估计而表现不佳，需要可靠的离线到在线过渡方法。

Method: BAQ采用双目标损失函数：(i)在不确定性高时将在线策略对齐到离线行为；(ii)随着在线经验积累逐渐放松约束，通过自适应机制减少误差传播。

Result: 在标准基准测试中，BAQ始终优于先前的离线到在线RL方法，实现了更快的恢复速度、改进的鲁棒性和更高的整体性能。

Conclusion: 隐式行为适应是可靠现实世界策略部署的原则性和实用解决方案，能够稳定早期在线更新并加速对新场景的适应。

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


### [120] [Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2511.03710)
*Guanning Zeng,Zhaoyi Zhou,Daman Arora,Andrea Zanette*

Main category: cs.LG

TL;DR: 提出了一种基于收缩估计的基线方法，用于改进强化学习中的策略梯度估计，通过结合每个提示和跨提示的均值来降低方差，提高训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习验证奖励（RLVR）中，通常使用每个提示的经验均值作为基线来稳定训练，但在低生成数量情况下估计不准确。受Stein悖论启发，希望改进均值估计精度。

Method: 使用收缩估计器，将每个提示的均值与跨提示的均值相结合，构建一个无需额外超参数或计算的基线替代方案。

Result: 理论上证明收缩基线能产生更低方差的策略梯度估计器；实证显示收缩基线始终优于标准经验均值基线，降低梯度更新方差，提高训练稳定性。

Conclusion: 收缩基线作为现有每个提示均值基线的即插即用替代方案，能有效改进RLVR训练效果。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for post-training large reasoning models (LRMs) using
policy-gradient methods such as GRPO. To stabilize training, these methods
typically center trajectory rewards by subtracting the empirical mean for each
prompt. Statistically, this centering acts as a control variate (or baseline),
reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages
for each prompt in a batch. Drawing inspiration from Stein's paradox, we
propose using shrinkage estimators that combine per-prompt and across-prompt
means to improve the overall per-prompt mean estimation accuracy --
particularly in the low-generation regime typical of RLVR. Theoretically, we
construct a shrinkage-based baseline that provably yields lower-variance
policy-gradient estimators across algorithms. Our proposed baseline serves as a
drop-in replacement for existing per-prompt mean baselines, requiring no
additional hyper-parameters or computation. Empirically, shrinkage baselines
consistently outperform standard empirical-mean baselines, leading to
lower-variance gradient updates and improved training stability.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [121] [Inferential Theory for Pricing Errors with Latent Factors and Firm Characteristics](https://arxiv.org/abs/2511.03076)
*Jungjun Choi,Ming Yuan*

Main category: econ.EM

TL;DR: 提出了一个结合潜在因子和公司特征的新因子模型框架，将错误定价分解为内部alpha和外部alpha两个独立成分，解决了正交性、基依赖和单位敏感性等问题。


<details>
  <summary>Details</summary>
Motivation: 现有因子模型在错误定价分析中存在正交性、基依赖和单位敏感性等问题，需要更统一的统计和特征导向方法框架。

Method: 基于低秩方法开发估计器，采用显式去偏技术，提供闭式解和严格的推断理论，允许特征数量增长并放宽样本维度假设。

Result: 使用2000-2019年美国股票数据，发现显著的内部alpha和外部alpha，前者呈现行业层面协同变动，后者反映超出公司基本面的异质性冲击。

Conclusion: 该框架统一了统计和特征导向的因子建模方法，在理论发展和错误定价结构理解方面提供了重要进展。

Abstract: We study factor models that combine latent factors with firm characteristics
and propose a new framework for modeling, estimating, and inferring pricing
errors. Following Zhang (2024), our approach decomposes mispricing into two
distinct components: inside alpha, explained by firm characteristics but
orthogonal to factor exposures, and outside alpha, orthogonal to both factors
and characteristics. Our model generalizes those developed recently such as
Kelly et al. (2019) and Zhang (2024), resolving issues of orthogonality, basis
dependence, and unit sensitivity. Methodologically, we develop estimators
grounded in low-rank methods with explicit debiasing, providing closed-form
solutions and a rigorous inferential theory that accommodates a growing number
of characteristics and relaxes standard assumptions on sample dimensions.
Empirically, using U.S. stock returns from 2000-2019, we document strong
evidence of both inside and outside alphas, with the former showing
industry-level co-movements and the latter reflecting idiosyncratic shocks
beyond firm fundamentals. Our framework thus unifies statistical and
characteristic-based approaches to factor modeling, offering both theoretical
advances and new insights into the structure of pricing errors.

</details>


### [122] [The Economics of Spatial Coordination in Critical Infrastructure Investment](https://arxiv.org/abs/2511.03091)
*L Kaili Diamond,Benjamin Gilbert*

Main category: econ.EM

TL;DR: 提出了一种结合嵌套固定点动态规划和模拟矩方法的混合方法，用于估计结构动态离散选择模型中的空间协调机制，并在Titan超级计算机的GPU替换数据中识别出两种协调机制：顺序替换级联和同时故障批处理。


<details>
  <summary>Details</summary>
Motivation: 解决在空间环境中估计结构动态离散选择模型的计算难题，同时保持结构解释能力，以理解基础设施管理中的空间协调行为。

Method: 结合嵌套固定点(NFXP)动态规划和模拟矩方法(MSM)的混合方法，应用于12,915个设备位置的GPU替换数据。

Result: 识别出两种协调机制：顺序替换级联(γ_lag = -0.793)和同时故障批处理(γ_fail = -0.265)，顺序协调强度约为故障批处理的3倍，空间相互依赖性解释了5.3%的变异，在高温环境中协调效应比冷区强10倍以上。

Conclusion: 正式检验明确拒绝空间独立性，表明忽略空间协调的基础设施政策会系统性错误干预时机并失去可用的协调收益。

Abstract: We develop a hybrid approach to estimate spatial coordination mechanisms in
structural dynamic discrete choice models by combining nested fixed-point
(NFXP) dynamic programming with method of simulated moments (MSM), achieving
computational tractability in spatial settings while preserving structural
interpretation. Applying this framework to GPU replacement data from 12,915
equipment locations in Oak Ridge National Laboratory's Titan supercomputer, we
identify two distinct coordination mechanisms: sequential replacement cascades
(gamma_lag = -0.793) and contemporaneous failure batching (gamma_fail =
-0.265). Sequential coordination dominates - approximately three times stronger
than failure batching - indicating that operators engage in deliberate
strategic behavior rather than purely reactive responses. Spatial
interdependencies account for 5.3% of variation unexplained by
independent-decision models, with coordination concentrated in high-risk
thermal environments exhibiting effects more than 10 times stronger than cool
zones. Formal tests decisively reject spatial independence (chi-squared(2) =
685.38, p < 0.001), demonstrating that infrastructure policies ignoring spatial
coordination will systematically mistime interventions and forgo available
coordination gains.

</details>


### [123] [Large Bayesian Tensor Autoregressions](https://arxiv.org/abs/2511.03097)
*Yaling Qi*

Main category: econ.EM

TL;DR: 提出了一种贝叶斯张量自回归框架，用于分析多维时间序列，特别关注国际贸易数据，并引入随机波动率来处理时变波动性。


<details>
  <summary>Details</summary>
Motivation: 多维经济数据集（如双边贸易数据）日益增多，现有文献通常假设同方差性，无法捕捉COVID-19疫情和战争爆发等事件导致的时变波动性。

Method: 开发了基于低秩Tucker分解和分层收缩先验的高效采样方法，将张量自回归模型与随机波动率结合。

Result: 模型能够有效捕捉国际贸易中的时变波动性，并通过Tucker分解将高维贸易流投影到全球因子上。

Conclusion: 该框架为分析大型多维时间序列提供了灵活且计算高效的方法，特别适用于捕捉重大事件对经济数据的动态影响。

Abstract: The availability of multidimensional economic datasets has grown
significantly in recent years. An example is bilateral trade values across
goods among countries, comprising three dimensions -- importing countries,
exporting countries, and goods -- forming a third-order tensor time series.
This paper introduces a general Bayesian tensor autoregressive framework to
analyze the dynamics of large, multidimensional time series with a particular
focus on international trade across different countries and sectors. Departing
from the standard homoscedastic assumption in this literature, we incorporate
flexible stochastic volatility into the tensor autoregressive models. The
proposed models can capture time-varying volatility due to the COVID-19
pandemic and recent outbreaks of war. To address computational challenges and
mitigate overfitting, we develop an efficient sampling method based on low-rank
Tucker decomposition and hierarchical shrinkage priors. Additionally, we
provide a factor interpretation of the model showing how the Tucker
decomposition projects large-dimensional disaggregated trade flows onto global
factors.

</details>


### [124] [Unbiased Regression-Adjusted Estimation of Average Treatment Effects in Randomized Controlled Trials](https://arxiv.org/abs/2511.03236)
*Alberto Abadie,Mehrdad Ghadiri,Ali Jadbabaie,Mahyar JafariNodeh*

Main category: econ.EM

TL;DR: 本文提出了留一法回归调整估计量(LOORA)，用于随机对照试验中估计平均处理效应。该方法消除了传统回归调整的有限样本偏差，在简单和完全随机分配下提供了精确的方差表达式，并通过岭正则化提高小样本稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统回归调整方法在随机对照试验中存在有限样本偏差问题，需要开发既能消除偏差又能保持渐近效率的估计方法。

Method: 使用留一法回归调整，结合岭正则化来限制高杠杆观测值的影响，将估计量视为两步过程处理置信区间构造。

Result: LOORA消除了显著偏差，在有限样本中实现接近名义水平的置信区间覆盖率，同时在大样本中达到Lin(2013)描述的回归调整估计量的渐近效率。

Conclusion: LOORA方法在保持无偏性的同时实现了渐近效率，为随机对照试验中的平均处理效应估计提供了可靠的工具。

Abstract: This article introduces a leave-one-out regression adjustment estimator
(LOORA) for estimating average treatment effects in randomized controlled
trials. The method removes the finite-sample bias of conventional regression
adjustment and provides exact variance expressions for LOORA versions of the
Horvitz-Thompson and difference-in-means estimators under simple and complete
random assignment. Ridge regularization limits the influence of high-leverage
observations, improving stability and precision in small samples. In large
samples, LOORA attains the asymptotic efficiency of regression-adjusted
estimator as characterized by Lin (2013, Annals of Applied Statistics), while
remaining exactly unbiased. To construct confidence intervals, we rely on
asymptotic variance estimates that treat the estimator as a two-step procedure,
accounting for both the regression adjustment and the random assignment stages.
Two within-subject experimental applications that provide realistic joint
distributions of potential outcomes as ground truth show that LOORA eliminates
substantial biases and achieves close-to-nominal confidence interval coverage.

</details>


### [125] [Using spatial modeling to address covariate measurement error](https://arxiv.org/abs/2511.03306)
*Susanne M. Schennach,Vincent Starck*

Main category: econ.EM

TL;DR: 提出一种利用空间数据处理协变量测量误差的新估计方法，通过将邻近观测作为重复测量，结合算子对角化方法建立识别性，适用于非线性模型和非经典误差。


<details>
  <summary>Details</summary>
Motivation: 解决协变量测量误差问题，特别是在空间数据可用的情况下，传统方法往往依赖分布假设或无法处理非经典误差。

Method: 使用邻近观测作为重复测量，控制随机距离影响，结合算子对角化方法；实现采用筛半参数最大似然估计、第一步核估计和模拟方法。

Result: 通过控制模拟和应用研究（评估前殖民政治结构对非洲当前经济发展的影响）验证了方法的有效性。

Conclusion: 该方法为处理协变量测量误差提供了一种不依赖先验分布假设的通用解决方案，特别适用于空间数据场景。

Abstract: We propose a new estimation methodology to address the presence of covariate
measurement error by exploiting the availability of spatial data. The approach
uses neighboring observations as repeated measurements, after suitably
controlling for the random distance between the observations in a way that
allows the use of operator diagonalization methods to establish identification.
The method is applicable to general nonlinear models with potentially
nonclassical errors and does not rely on a priori distributional assumptions
regarding any of the variables. The method's implementation combines a sieve
semiparametric maximum likelihood with a first-step kernel estimator and
simulation methods. The method's effectiveness is illustrated through both
controlled simulations and an application to the assessment of the effect of
pre-colonial political structure on current economic development in Africa.

</details>


### [126] [Multivariate Ordered Discrete Response Models with Lattice Structures](https://arxiv.org/abs/2511.03418)
*Tatiana Komarova,William Matcham*

Main category: econ.EM

TL;DR: 分析具有格结构的多元有序离散响应模型，研究跨多个维度进行窄括号选择的决策者。在非参数框架下识别参数、阈值和误差分布，并在参数化双变量probit模型中分别识别回归参数、阈值和相关性参数。


<details>
  <summary>Details</summary>
Motivation: 研究决策者在多个维度上采用窄括号选择策略的离散响应模型，这类模型在经济学和决策理论中具有广泛应用，但现有研究对这类格结构模型的识别和估计问题关注不足。

Method: 采用半参数框架建模，将潜在连续过程建模为协变量指数和未观测误差之和，推导参数、阈值和误差联合累积分布函数的识别条件。对于参数化双变量probit模型，分别识别回归参数、阈值和相关性参数。

Result: 建立了格结构多元有序离散响应模型的识别条件，提出了半参数和参数模型的估计方法，并通过模拟研究验证了估计器的性能。

Conclusion: 为具有格结构的多元有序离散响应模型提供了完整的识别和估计框架，扩展了现有离散选择模型的理论基础，为实证研究提供了方法论支持。

Abstract: We analyze multivariate ordered discrete response models with a lattice
structure, modeling decision makers who narrowly bracket choices across
multiple dimensions. These models map latent continuous processes into discrete
responses using functionally independent decision thresholds. In a
semiparametric framework, we model latent processes as sums of covariate
indices and unobserved errors, deriving conditions for identifying parameters,
thresholds, and the joint cumulative distribution function of errors. For the
parametric bivariate probit case, we separately derive identification of
regression parameters and thresholds, and the correlation parameter, with the
latter requiring additional covariate conditions. We outline estimation
approaches for semiparametric and parametric models and present simulations
illustrating the performance of estimators for lattice models.

</details>


### [127] [The moment is here: a generalised class of estimators for fuzzy regression discontinuity designs](https://arxiv.org/abs/2511.03424)
*Stuart Lane*

Main category: econ.EM

TL;DR: 标准模糊断点回归(FRD)估计量存在无限矩问题，作者提出了一类广义FRD估计量，具有有限矩和更好的小样本性能。


<details>
  <summary>Details</summary>
Motivation: 标准FRD估计量在任何有限样本中都没有有限阶矩，导致估计不精确、重尾分布，在小样本或处理概率断点较小时推断不准确。

Method: 提出了一类广义FRD估计量，包含一个连续统的估计量，具有有限样本中的所有有限阶矩，并包含标准FRD和锐断点(SRD)估计量。该类通过单个调优参数索引，提供了简单取值。

Result: 新估计量在大量蒙特卡洛模拟中显示出中位数偏差、中位数绝对偏差和均方根误差的显著改善，在小样本或处理概率断点较小时保持稳定。

Conclusion: 新估计量和置信区间在稳定性和性能方面都有显著改进，在班级规模对教育成就影响的数据分析中也得到验证。

Abstract: The standard fuzzy regression discontinuity (FRD) estimator is a ratio of
differences of local polynomial estimators. I show that this estimator does not
have finite moments of any order in finite samples, regardless of the choice of
kernel function, bandwidth, or order of polynomial. This leads to an imprecise
estimator with a heavy-tailed sampling distribution, and inaccurate inference
with small sample sizes or when the discontinuity in the probability of
treatment assignment at the cutoff is small. I present a generalised class of
computationally simple FRD estimators, which contains a continuum of estimators
with finite moments of all orders in finite samples, and nests both the
standard FRD and sharp (SRD) estimators. The class is indexed by a single
tuning parameter, and I provide simple values that lead to substantial
improvements in median bias, median absolute deviation and root mean squared
error. These new estimators remain very stable in small samples, or when the
discontinuity in the probability of treatment assignment at the cutoff is
small. Simple confidence intervals that have strong coverage and length
properties in small samples are also developed. The improvements are seen
across a wide range of models and using common bandwidth selection algorithms
in extensive Monte Carlo simulations. The improved stability and performance of
the estimators and confidence intervals is also demonstrated using data on
class size effects on educational attainment.

</details>


### [128] [Leniency Designs: An Operator's Manual](https://arxiv.org/abs/2511.03572)
*Paul Goldsmith-Pinkham,Peter Hull,Michal Kolesár*

Main category: econ.EM

TL;DR: 本文提供了一个逐步指南，用于设计基于法官或审查员工具的宽松度研究，利用最新的计量经济学文献。介绍了无偏折刀工具变量估计器（UJIVE），专门用于利用外生宽松度变异，避免在存在多个决策者或控制变量时的微妙偏差。


<details>
  <summary>Details</summary>
Motivation: 开发一个系统的方法来设计和验证基于审查员或法官工具的研究，解决现有方法中可能存在的偏差问题，并提供统计推断的指导。

Method: 使用无偏折刀工具变量估计器（UJIVE），该方法专门设计用于利用外生宽松度变异。展示了如何用UJIVE评估关键假设，包括准随机分配和平均第一阶段单调性，并探讨处理效应估计的外部有效性。

Result: 通过重新分析Farre-Mensa等人（2020）的研究，利用准随机审查员分配来估计专利对初创企业的价值，验证了所提出的检查清单。

Conclusion: UJIVE是专门为利用外生宽松度变异而构建的有效工具，能够避免微妙偏差，并可用于评估研究设计的关键假设和外部有效性。非聚类标准误通常是合适的统计推断方法。

Abstract: We develop a step-by-step guide to leniency (a.k.a. judge or examiner
instrument) designs, drawing on recent econometric literatures. The unbiased
jackknife instrumental variables estimator (UJIVE) is purpose-built for
leveraging exogenous leniency variation, avoiding subtle biases even in the
presence of many decision-makers or controls. We show how UJIVE can also be
used to assess key assumptions underlying leniency designs, including
quasi-random assignment and average first-stage monotonicity, and to probe the
external validity of treatment effect estimates. We further discuss statistical
inference, arguing that non-clustered standard errors are often appropriate. A
reanalysis of Farre-Mensa et al. (2020), using quasi-random examiner assignment
to estimate the value of patents to startups, illustrates our checklist.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [129] [Scalable Single-Cell Gene Expression Generation with Latent Diffusion Models](https://arxiv.org/abs/2511.02986)
*Giovanni Palla,Sudarshan Babu,Payam Dibaeinia,James D. Pearce,Donghui Li,Aly A. Khan,Theofanis Karaletsos,Jakub M. Tomczak*

Main category: stat.ML

TL;DR: 提出了scLDM模型，一种用于单细胞基因表达数据的可扩展潜在扩散模型，该模型尊重数据的可交换性特性，在多种实验中都表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞基因表达的计算建模对于理解细胞过程至关重要，但生成真实的表达谱仍然是一个主要挑战，因为基因表达数据的计数性质和基因间复杂的潜在依赖性。现有生成模型通常施加人为的基因排序或依赖浅层神经网络架构。

Method: 使用具有固定大小潜在变量的VAE，采用统一的Multi-head Cross-Attention Block架构，在编码器中实现置换不变池化，在解码器中实现置换等变解池化。用基于Diffusion Transformers和线性插值的潜在扩散模型替换高斯先验，支持多条件无分类器引导的高质量生成。

Result: 在观察性和扰动性单细胞数据以及细胞级分类等下游任务的各种实验中表现出优越性能。

Conclusion: scLDM模型能够有效处理单细胞基因表达数据的生成任务，在多个应用场景中都取得了良好效果。

Abstract: Computational modeling of single-cell gene expression is crucial for
understanding cellular processes, but generating realistic expression profiles
remains a major challenge. This difficulty arises from the count nature of gene
expression data and complex latent dependencies among genes. Existing
generative models often impose artificial gene orderings or rely on shallow
neural network architectures. We introduce a scalable latent diffusion model
for single-cell gene expression data, which we refer to as scLDM, that respects
the fundamental exchangeability property of the data. Our VAE uses fixed-size
latent variables leveraging a unified Multi-head Cross-Attention Block (MCAB)
architecture, which serves dual roles: permutation-invariant pooling in the
encoder and permutation-equivariant unpooling in the decoder. We enhance this
framework by replacing the Gaussian prior with a latent diffusion model using
Diffusion Transformers and linear interpolants, enabling high-quality
generation with multi-conditional classifier-free guidance. We show its
superior performance in a variety of experiments for both observational and
perturbational single-cell data, as well as downstream tasks like cell-level
classification.

</details>


### [130] [Unifying Information-Theoretic and Pair-Counting Clustering Similarity](https://arxiv.org/abs/2511.03000)
*Alexander J. Gates*

Main category: stat.ML

TL;DR: 本文提出了一个统一的分析框架，将聚类相似性度量的两个主要家族（配对计数和信息论方法）联系起来，揭示了它们之间的深层数学关系。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类相似性度量方法存在分歧，有时甚至产生矛盾的结果。虽然前人工作发现了这两个家族之间的相似性并应用了经验归一化方法，但它们之间的深层分析联系仍未完全理解。

Method: 开发了一个分析框架，从两个互补的视角统一这两个家族：1）将它们表达为观测与期望共现的加权展开；2）将配对计数推广到k元组一致性，并展示信息论度量可以视为系统累积高阶共分配结构。

Result: 该框架清晰地解释了Rand指数和互信息等度量，展示了每个家族中其他指数如何作为自然扩展出现，阐明了两个体系何时以及为何会产生分歧。

Conclusion: 该统一框架为选择、解释和扩展聚类相似性度量提供了原则性基础，将它们的敏感性直接与加权和近似阶数联系起来。

Abstract: Comparing clusterings is central to evaluating unsupervised models, yet the
many existing similarity measures can produce widely divergent, sometimes
contradictory, evaluations. Clustering similarity measures are typically
organized into two principal families, pair-counting and information-theoretic,
reflecting whether they quantify agreement through element pairs or aggregate
information across full cluster contingency tables. Prior work has uncovered
parallels between these families and applied empirical normalization or
chance-correction schemes, but their deeper analytical connection remains only
partially understood. Here, we develop an analytical framework that unifies
these families through two complementary perspectives. First, both families are
expressed as weighted expansions of observed versus expected co-occurrences,
with pair-counting arising as a quadratic, low-order approximation and
information-theoretic measures as higher-order, frequency-weighted extensions.
Second, we generalize pair-counting to $k$-tuple agreement and show that
information-theoretic measures can be viewed as systematically accumulating
higher-order co-assignment structure beyond the pairwise level. We illustrate
the approaches analytically for the Rand index and Mutual Information, and show
how other indices in each family emerge as natural extensions. Together, these
views clarify when and why the two regimes diverge, relating their
sensitivities directly to weighting and approximation order, and provide a
principled basis for selecting, interpreting, and extending clustering
similarity measures across applications.

</details>


### [131] [Precise asymptotic analysis of Sobolev training for random feature models](https://arxiv.org/abs/2511.03050)
*Katharine E Fisher,Matthew TC Li,Youssef Marzouk,Timo Schorlepp*

Main category: stat.ML

TL;DR: 本文研究了Sobolev训练（同时使用函数和梯度数据进行回归）对高维过参数化随机特征模型泛化误差的影响，发现在某些情况下补充梯度数据并不能普遍提高预测性能，过参数化程度应指导训练方法的选择。


<details>
  <summary>Details</summary>
Motivation: 梯度信息在实际应用中广泛可用且有用，但理论上对于Sobolev训练如何影响高维过参数化模型的泛化误差知之甚少。

Method: 使用统计物理中的复制方法和算子值自由概率理论中的线性化方法，推导出训练后随机特征模型泛化误差的闭式描述，并通过将梯度数据投影到有限维子空间来模拟实际实现。

Result: 对于单索引模型描述的目标函数，补充函数数据与额外梯度数据并不能普遍提高预测性能，过参数化程度应指导训练方法的选择。

Conclusion: 研究结果确定了模型通过插值噪声函数和梯度数据实现最佳性能的设置，过参数化程度是选择训练方法的关键因素。

Abstract: Gradient information is widely useful and available in applications, and is
therefore natural to include in the training of neural networks. Yet little is
known theoretically about the impact of Sobolev training -- regression with
both function and gradient data -- on the generalization error of highly
overparameterized predictive models in high dimensions. In this paper, we
obtain a precise characterization of this training modality for random feature
(RF) models in the limit where the number of trainable parameters, input
dimensions, and training data tend proportionally to infinity. Our model for
Sobolev training reflects practical implementations by sketching gradient data
onto finite dimensional subspaces. By combining the replica method from
statistical physics with linearizations in operator-valued free probability
theory, we derive a closed-form description for the generalization errors of
the trained RF models. For target functions described by single-index models,
we demonstrate that supplementing function data with additional gradient data
does not universally improve predictive performance. Rather, the degree of
overparameterization should inform the choice of training method. More broadly,
our results identify settings where models perform optimally by interpolating
noisy function and gradient data.

</details>


### [132] [Provable Accelerated Bayesian Optimization with Knowledge Transfer](https://arxiv.org/abs/2511.03125)
*Haitao Lin,Boxin Zhao,Mladen Kolar,Chong Liu*

Main category: stat.ML

TL;DR: DeltaBO算法通过构建源函数和目标函数差异函数的不确定性量化方法，在贝叶斯优化中实现知识迁移，显著提升优化效率。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化知识迁移方法要么缺乏理论保证，要么与非迁移设置的遗憾界相同，无法充分利用历史知识来加速目标任务的优化过程。

Method: 提出DeltaBO算法，基于源函数和目标函数的差异函数构建不确定性量化方法，允许源函数和目标函数属于不同的再生核希尔伯特空间。

Result: 理论证明DeltaBO的遗憾界为$\tilde{\mathcal{O}}(\sqrt{T(T/N + \gamma_\delta)})$，其中$N$是源任务评估次数，通常$N \\gg T$。在相似任务场景下，$\gamma_\\delta$远小于$\gamma_f$。实证研究显示DeltaBO优于基线方法。

Conclusion: DeltaBO通过差异函数的不确定性量化实现了有效的知识迁移，在理论和实证上都优于现有方法，特别适用于源任务和目标任务相似的场景。

Abstract: We study how Bayesian optimization (BO) can be accelerated on a target task
with historical knowledge transferred from related source tasks. Existing works
on BO with knowledge transfer either do not have theoretical guarantees or
achieve the same regret as BO in the non-transfer setting,
$\tilde{\mathcal{O}}(\sqrt{T \gamma_f})$, where $T$ is the number of
evaluations of the target function and $\gamma_f$ denotes its information gain.
In this paper, we propose the DeltaBO algorithm, in which a novel
uncertainty-quantification approach is built on the difference function
$\delta$ between the source and target functions, which are allowed to belong
to different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions,
we prove that the regret of DeltaBO is of order $\tilde{\mathcal{O}}(\sqrt{T
(T/N + \gamma_\delta)})$, where $N$ denotes the number of evaluations from
source tasks and typically $N \gg T$. In many applications, source and target
tasks are similar, which implies that $\gamma_\delta$ can be much smaller than
$\gamma_f$. Empirical studies on both real-world hyperparameter tuning tasks
and synthetic functions show that DeltaBO outperforms other baseline methods
and support our theoretical claims.

</details>


### [133] [Provable Separations between Memorization and Generalization in Diffusion Models](https://arxiv.org/abs/2511.03202)
*Zeqi Ye,Qijie Zhu,Molei Tao,Minshuo Chen*

Main category: stat.ML

TL;DR: 本文通过统计估计和网络逼近两个互补视角，建立了双重分离理论来解释扩散模型中的记忆化问题，并基于此开发了一种剪枝方法来减少记忆化同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在各领域取得了显著成功，但容易产生记忆化问题——复制训练数据而非生成新颖输出，这不仅限制了其创造潜力，还引发了隐私和安全担忧。目前对记忆化的理论理解仍然有限。

Method: 从统计估计角度证明真实评分函数不会最小化经验去噪损失，从网络逼近角度证明实现经验评分函数需要网络规模随样本量增长。基于这些理论洞察，开发了一种基于剪枝的方法来减少扩散变换器中的记忆化。

Result: 建立了双重分离理论框架，揭示了记忆化的根本原因，并验证了剪枝方法在减少记忆化的同时能够保持生成质量。

Conclusion: 通过理论分析和实验验证，本文为理解扩散模型记忆化提供了新的理论框架，并提出了有效的缓解策略，有助于提升扩散模型的创造性和安全性。

Abstract: Diffusion models have achieved remarkable success across diverse domains, but
they remain vulnerable to memorization -- reproducing training data rather than
generating novel outputs. This not only limits their creative potential but
also raises concerns about privacy and safety. While empirical studies have
explored mitigation strategies, theoretical understanding of memorization
remains limited. We address this gap through developing a dual-separation
result via two complementary perspectives: statistical estimation and network
approximation. From the estimation side, we show that the ground-truth score
function does not minimize the empirical denoising loss, creating a separation
that drives memorization. From the approximation side, we prove that
implementing the empirical score function requires network size to scale with
sample size, spelling a separation compared to the more compact network
representation of the ground-truth score function. Guided by these insights, we
develop a pruning-based method that reduces memorization while maintaining
generation quality in diffusion transformers.

</details>


### [134] [RKUM: An R Package for Robust Kernel Unsupervised Methods](https://arxiv.org/abs/2511.03216)
*Md Ashad Alam*

Main category: stat.ML

TL;DR: RKUM是一个R包，用于实现稳健的基于核的无监督方法，包括稳健核协方差算子和交叉协方差算子的估计，以及稳健核典型相关分析和影响函数计算。


<details>
  <summary>Details</summary>
Motivation: 传统核方法使用二次损失函数，对污染或噪声数据敏感，需要开发稳健的核学习方法以在受污染数据条件下进行可靠分析。

Method: 使用广义损失函数替代传统二次损失来估计稳健核协方差算子和交叉协方差算子，实现稳健核典型相关分析和影响函数计算。

Result: 实验表明，标准核典型相关分析的影响函数能有效识别异常值，而RKUM实现的稳健核方法对数据污染的敏感性降低。

Conclusion: RKUM为高维数据应用中的稳健核分析提供了一个高效且可扩展的平台。

Abstract: RKUM is an R package developed for implementing robust kernel-based
unsupervised methods. It provides functions for estimating the robust kernel
covariance operator (CO) and the robust kernel cross-covariance operator (CCO)
using generalized loss functions instead of the conventional quadratic loss.
These operators form the foundation of robust kernel learning and enable
reliable analysis under contaminated or noisy data conditions. The package
includes implementations of robust kernel canonical correlation analysis
(Kernel CCA), as well as the influence function (IF) for both standard and
multiple kernel CCA frameworks. The influence function quantifies sensitivity
and helps detect influential or outlying observations across two-view and
multi-view datasets. Experiments using synthesized two-view and multi-view data
demonstrate that the IF of the standard kernel CCA effectively identifies
outliers, while the robust kernel methods implemented in RKUM exhibit reduced
sensitivity to contamination. Overall, RKUM provides an efficient and
extensible platform for robust kernel-based analysis in high-dimensional data
applications.

</details>


### [135] [Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity](https://arxiv.org/abs/2511.03606)
*Diego Martinez-Taboada,Tomas Gonzalez,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 该论文研究了向量值自归一化过程的浓度界限，扩展了标量情况下的结果，特别关注轻尾分布（如Bennett或Bernstein界限）而非仅限于亚高斯框架。


<details>
  <summary>Details</summary>
Motivation: 自归一化过程在从序列决策到计量经济学的广泛应用中至关重要，但向量值过程在亚高斯框架之外的研究相对不足。

Method: 提供了超越亚高斯性的轻尾自归一化过程的浓度界限，包括Bennett和Bernstein类型的界限。

Result: 获得了向量值自归一化过程的浓度界限，适用于更广泛的轻尾分布情况。

Conclusion: 研究结果在线性回归和（核化）线性赌博机等应用中具有相关性，扩展了自归一化过程的理论基础。

Abstract: The study of self-normalized processes plays a crucial role in a wide range
of applications, from sequential decision-making to econometrics. While the
behavior of self-normalized concentration has been widely investigated for
scalar-valued processes, vector-valued processes remain comparatively
underexplored, especially outside of the sub-Gaussian framework. In this
contribution, we provide concentration bounds for self-normalized processes
with light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds).
We illustrate the relevance of our results in the context of online linear
regression, with applications in (kernelized) linear bandits.

</details>


### [136] [Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning](https://arxiv.org/abs/2511.03693)
*Md Ahasanul Arafath,Abhijit Kumar Ghosh,Md Rony Ahmed,Sabrin Afroz,Minhazul Hosen,Md Hasan Moon,Md Tanzim Reza,Md Ashad Alam*

Main category: stat.ML

TL;DR: 提出了一种用于结直肠癌病理分级的隐私保护联邦学习框架，通过多尺度特征学习和分布式训练，在保护数据隐私的同时提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌分级存在观察者间差异性和多机构数据共享的隐私限制问题，传统集中式训练模型违反数据治理法规且忽略了多尺度分析的重要性。

Method: 采用双流ResNetRS50主干网络同时捕获细粒度核细节和更广泛的组织级上下文，集成到使用FedProx稳定的联邦学习系统中，以减轻异构数据分布下的客户端漂移。

Result: 在CRC-HGD数据集上达到83.5%的整体准确率，优于集中式模型(81.6%)，对最具侵袭性的III级肿瘤召回率达到87.5%，在40倍放大倍率下准确率提升至88.0%。

Conclusion: 该联邦多尺度方法不仅保护患者隐私，还增强了模型性能和泛化能力，为可部署的隐私感知临床AI建立了基础。

Abstract: Colorectal cancer (CRC) grading is a critical prognostic factor but remains
hampered by inter-observer variability and the privacy constraints of
multi-institutional data sharing. While deep learning offers a path to
automation, centralized training models conflict with data governance
regulations and neglect the diagnostic importance of multi-scale analysis. In
this work, we propose a scalable, privacy-preserving federated learning (FL)
framework for CRC histopathological grading that integrates multi-scale feature
learning within a distributed training paradigm. Our approach employs a
dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear
detail and broader tissue-level context. This architecture is integrated into a
robust FL system stabilized using FedProx to mitigate client drift across
heterogeneous data distributions from multiple hospitals. Extensive evaluation
on the CRC-HGD dataset demonstrates that our framework achieves an overall
accuracy of 83.5%, outperforming a comparable centralized model (81.6%).
Crucially, the system excels in identifying the most aggressive Grade III
tumors with a high recall of 87.5%, a key clinical priority to prevent
dangerous false negatives. Performance further improves with higher
magnification, reaching 88.0% accuracy at 40x. These results validate that our
federated multi-scale approach not only preserves patient privacy but also
enhances model performance and generalization. The proposed modular pipeline,
with built-in preprocessing, checkpointing, and error handling, establishes a
foundational step toward deployable, privacy-aware clinical AI for digital
pathology.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [137] [Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997)
*Jon Kutasov,Chloe Loughridge,Yuqi Sun,Henry Sleight,Buck Shlegeris,Tyler Tracy,Joe Benton*

Main category: cs.AI

TL;DR: 本文系统评估了AI控制协议在SHADE-Arena环境中的安全性，发现关键行动延迟协议对红队攻击具有高度鲁棒性，而重采样策略在红队了解协议内部机制时安全性显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统作为智能体广泛部署，确保其安全运行变得至关重要。AI控制通过监控AI代理行为并在必要时干预或审计来降低风险，需要评估这些协议的有效性和对适应性攻击的鲁棒性。

Method: 在SHADE-Arena多样化智能体环境中系统评估多种控制协议，包括信任模型延迟、重采样和关键行动延迟等蓝队协议，并迭代开发红队攻击策略来测试协议鲁棒性。

Result: 重采样和关键行动延迟协议将安全性从50%提升到96%；但当红队了解重采样机制时，攻击成功率大幅提升，安全性降至17%；关键行动延迟协议对最强红队策略仍保持高度鲁棒性。

Conclusion: 关键行动延迟协议对适应性攻击具有高度鲁棒性，而重采样策略在攻击者了解协议内部机制时易受攻击，强调了拒绝攻击策略访问协议内部信息的重要性。

Abstract: As AI systems become more capable and widely deployed as agents, ensuring
their safe operation becomes critical. AI control offers one approach to
mitigating the risk from untrusted AI agents by monitoring their actions and
intervening or auditing when necessary. Evaluating the safety of these
protocols requires understanding both their effectiveness against current
attacks and their robustness to adaptive adversaries. In this work, we
systematically evaluate a range of control protocols in SHADE-Arena, a dataset
of diverse agentic environments. First, we evaluate blue team protocols,
including deferral to trusted models, resampling, and deferring on critical
actions, against a default attack policy. We find that resampling for
incrimination and deferring on critical actions perform best, increasing safety
from 50% to 96%. We then iterate on red team strategies against these protocols
and find that attack policies with additional affordances, such as knowledge of
when resampling occurs or the ability to simulate monitors, can substantially
improve attack success rates against our resampling strategy, decreasing safety
to 17%. However, deferring on critical actions is highly robust to even our
strongest red team strategies, demonstrating the importance of denying attack
policies access to protocol internals.

</details>


### [138] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: 提出了PublicAgent多智能体框架，通过将端到端数据分析工作流分解为专门化的智能体（意图澄清、数据集发现、分析和报告），解决了LLM在复杂分析任务中的注意力稀释、推理模式冲突和错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 开放数据仓库具有支持循证决策的潜力，但缺乏专业知识（数据集发现、模式映射、统计分析）的非专家难以访问。虽然大语言模型在单个任务上表现良好，但在端到端分析工作流中暴露出根本性限制。

Method: 设计多智能体框架，将分析流程分解为四个专门化智能体：意图澄清、数据集发现、分析和报告。这种架构在智能体上下文中保持专注注意力，并在每个阶段实现验证。

Result: 评估了5个模型和50个查询，得出多智能体LLM系统的五个设计原则：1) 专门化独立于模型强度提供价值；2) 智能体分为通用型和条件型；3) 不同智能体缓解不同故障模式；4) 架构优势在任务复杂度中持续存在；5) 智能体效果在不同模型间差异显著。

Conclusion: 这些原则指导了在复杂分析工作流中何时以及为何需要专门化，同时通过自然语言接口实现了对公共数据的更广泛访问。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [139] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: ScalingEval是一个大规模基准测试研究，系统比较了36个LLM作为评估者的性能，发现Claude 3.5 Sonnet决策置信度最高，Gemini 1.5 Pro整体表现最佳，GPT-4o在延迟-精度-成本权衡最优，GPT-OSS 20B在开源模型中领先。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型作为评估者对于构建可扩展且可信赖的评估流程至关重要，需要系统性的基准测试来比较不同模型的性能。

Method: 采用多智能体框架，通过可扩展的多数投票将模式审计和问题代码聚合成真实标签，无需人工标注即可实现LLM评估者的可重复比较。

Result: 在互补商品推荐任务中，不同模型在不同维度表现各异：Claude 3.5 Sonnet决策置信度最高，Gemini 1.5 Pro跨类别整体性能最佳，GPT-4o在延迟-精度-成本权衡最优，GPT-OSS 20B在开源模型中领先。结构化领域（电子、体育）共识强，生活方式类别（服装、食品）存在持续分歧。

Conclusion: ScalingEval建立了可重复的基准测试和评估协议，为LLM作为评估者提供了关于可扩展性、可靠性和模型系列权衡的可操作指导。

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [140] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 构建首个基准测试评估LLMs对现实世界概率分布的掌握程度，发现LLMs整体表现不佳，未能自然内化现实世界统计数据。


<details>
  <summary>Details</summary>
Motivation: 区分事实性知识和概率性知识，测试LLMs是否能够内化描述现实世界人口的经验分布，挑战LLMs作为通用分布逼近器的观点。

Method: 开发首个基准测试，评估LLMs在经济学、健康、教育和社会行为等领域对现实世界经验分布的掌握情况。

Result: LLMs整体表现不佳，未能自然内化现实世界统计数据，缺乏对观测分布（PCH第一层）的知识。

Conclusion: LLMs在概率分布知识方面存在根本性限制，根据因果层次定理，这意味着它们在干预性和反事实性知识方面也受到限制。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [141] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: SnapStream是一种KV缓存压缩方法，可在保持模型精度的同时实现4倍片上内存使用改进，并在生产环境中成功部署。


<details>
  <summary>Details</summary>
Motivation: 随着100B+参数大语言模型和100k+上下文长度的普及，对片上内存的需求激增。现有KV缓存压缩技术难以在工业部署框架中应用，且对现代指令跟随和推理模型的精度影响不明确。

Method: 开发SnapStream KV缓存压缩方法，在静态图和连续批处理的工业框架中实现稀疏KV注意力技术，并在Llama-3.1-8B-Instruct和DeepSeek-R1上进行验证。

Result: 在16路张量并行部署的DeepSeek-671B上，128k上下文长度下达到1832 tokens/秒，在LongBench-v2、AIME24和LiveCodeBench上引入最小精度损失。

Conclusion: SnapStream是首个在生产推理系统中成功部署的稀疏KV注意力技术，实现了4倍内存使用改进和最小精度损失。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [142] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 提出了一种基于能力的大语言模型监控新方法，替代传统的基于任务的监控，通过共享模型能力进行跨任务监控，以检测系统性弱点、长尾错误和涌现行为。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习监控方法基于任务和数据集漂移假设，但大语言模型并非针对特定任务训练，其性能退化不能简单归因于数据集漂移，需要新的监控原则。

Method: 能力基础监控方法，围绕共享的模型能力（如摘要、推理、翻译、安全防护）组织监控，而非独立评估每个下游任务。

Result: 该方法能够实现跨任务检测系统性弱点、长尾错误和涌现行为，为医疗领域大语言模型提供可扩展的监控基础。

Conclusion: 能力基础监控为大语言模型和未来通用人工智能模型在医疗领域的安全、自适应和协作监控提供了可扩展的基础。

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [143] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 对miniF2F基准测试中形式化与非形式化陈述的全面分析，揭示了在数学奥林匹克竞赛AI系统中，由于陈述不一致导致的性能下降，并提出了改进版本miniF2F-v2。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在数学奥林匹克竞赛中的表现，特别是理解自然语言问题、形式化到Lean语言并证明的能力，发现现有基准测试存在陈述不一致问题。

Method: 分析miniF2F基准测试中的形式化与非形式化陈述差异，纠正所有错误和简化，创建miniF2F-v2版本，并在改进后的基准上评估完整的定理证明流程。

Result: 在原始miniF2F上最佳准确率为36%，远低于文献中报告的自动形式化(97%)和定理证明(69%)的单独SOTA准确率。在miniF2F-v2上准确率提升至70%，但仍显示自动形式化模型与定理证明器之间存在显著不对齐。

Conclusion: 高质量基准测试有助于更好地评估形式推理领域的进展，并更准确地诊断自动形式化和定理证明模型的失败与成功模式。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [144] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: 提出了一种基于多模态大语言模型辅助烟花算法设计的新框架，通过引入关键部分概念扩展烟花算法到复杂高维任务，在旅行商问题和电子设计自动化问题上取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在处理非凸、高维、黑箱等复杂优化问题时效率低下，而大语言模型在语言理解和代码生成方面的进步为优化算法设计提供了新思路。

Method: 以烟花算法为基础优化器，结合多模态大语言模型，提出关键部分概念来扩展烟花算法处理复杂高维任务，并利用大语言模型的多模态特性充分挖掘优化过程中的信息。

Result: 在新框架下生成的烟花算法在多个问题实例上达到或超越了当前最优结果。

Conclusion: 大语言模型辅助的优化算法设计框架能够有效提升传统优化方法在复杂问题上的性能，为优化算法创新提供了新范式。

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [145] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的LLM安全响应框架，通过输入级安全分类和输出级RAG增强，实现了99.3%的风险召回率和100%的安全评分。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，其安全问题日益突出，严重制约了在关键领域的可信部署。

Method: 输入级采用基于监督微调的四级安全分类模型（安全、不安全、条件安全、专注注意）；输出级结合RAG和专门微调的解释模型，确保响应基于可信知识库。

Result: 在公共安全评估基准上显著优于基线模型TinyR1-Safety-8B，在专有高风险测试集上获得100%安全评分。

Conclusion: 该研究为构建高安全性、高可信度的LLM应用提供了有效的工程路径。

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [146] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文提出了一种验证形式化可解释AI（XAI）的新方法，并评估了公开可用的形式化解释器PyXAI，发现其在大多数数据集上计算出的解释存在错误。


<details>
  <summary>Details</summary>
Motivation: 形式化XAI相比非正式方法具有理论严谨性保证，但实际实现的形式化解释器的验证研究较少，需要开发验证方法来确保其正确性。

Method: 开发了一种新颖的形式化解释器验证方法，并对公开的形式化解释器PyXAI进行了系统性评估。

Result: 实验发现PyXAI在大多数分析的数据集上计算出的解释存在错误，验证了所提验证方法的重要性。

Conclusion: 形式化解释器的实际实现需要严格的验证方法，本文提出的验证方法对于确保形式化XAI系统的可靠性至关重要。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [147] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: 提出一个多智能体AI框架来形式化工程设计过程，通过专业智能体协作生成和优化设计候选方案，并以NACA翼型气动优化为例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统工程设计方法需要多领域专业知识，导致复杂协作和迭代优化过程，资源密集且效率低下。

Method: 构建包含三个专业AI智能体的框架：图本体学家使用LLM构建领域知识图谱，系统工程师制定技术需求，设计工程师生成候选设计，形成迭代反馈循环。

Result: 成功应用于4位数NACA翼型的气动优化，通过智能体协作提高了设计过程的效率和一致性。

Conclusion: 配备结构化知识表示的协作AI智能体能够显著提升工程设计过程的效率、一致性和质量。

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [148] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 本文介绍了为Adobe Summit开发的领域特定AI助手Summit Concierge，采用人在回路开发工作流，结合提示工程、检索增强和轻量级人工验证，解决了数据稀疏、质量保证和快速部署等现实约束。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI助手提升企业环境中的生产力、简化信息访问并改善用户体验，特别是在Adobe Summit这样的企业活动中。

Method: 采用人在回路开发工作流，结合提示工程、检索增强和轻量级人工验证，构建能够处理各类活动相关查询的AI助手。

Result: 成功开发并部署了Summit Concierge系统，展示了即使在冷启动场景下，敏捷的反馈驱动开发也能实现可扩展且可靠的AI助手。

Conclusion: 敏捷的反馈驱动开发方法能够实现可扩展和可靠的AI助手，即使在数据稀疏的冷启动场景中也能有效运作。

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [149] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: 大型语言模型能够从少量人格量表数据中准确预测人类心理特征的相关结构，通过两阶段推理过程实现零样本性能，接近有监督机器学习算法的准确度。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能够从最小化的定量输入中建模人类心理特征的相关结构，探索其模拟人类心理的能力。

Method: 使用816名人类个体的五大性格量表数据提示各种LLM，让它们模拟在其他九个心理量表上的回答，分析其推理过程和信息选择策略。

Result: LLM生成回答的跨量表相关模式与人类数据高度一致（R² > 0.89），零样本性能显著优于基于语义相似度的预测，接近直接训练机器学习算法的准确度。

Conclusion: LLM能够通过抽象和推理过程从最小数据中精确预测个体心理特征，为心理模拟提供了强大工具，并揭示了其涌现的推理能力。

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [150] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: 提出了一个名为AAA的web可访问性审计框架，通过人机协作模型实现WCAG-EM标准，包含GRASP采样方法和MaC多模态助手，支持规模化端到端审计。


<details>
  <summary>Details</summary>
Motivation: 当前网站可访问性审计方法资源密集且难以规模化，大多数网站界面不符合标准，阻碍了数字空间的社会福利、公正和平等。

Method: AAA框架包含两个核心创新：GRASP基于图的多模态采样方法，通过学习视觉、文本和关系线索的嵌入确保代表性页面覆盖；MaC多模态大语言模型副驾驶，通过跨模态推理为审计员提供智能辅助。

Result: 实验证明该方法有效，并提供了四个新数据集用于基准测试，发现小规模语言模型经过微调后能成为有能力的专家。

Conclusion: 该框架实现了可扩展的端到端web可访问性审计，通过AI增强的辅助为现实世界带来影响。

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [151] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: 对多种机器学习模型（决策树、决策集、决策列表、布尔电路及其集成）中解释问题的参数化复杂度进行理论研究，涵盖溯因和对比两种解释类型及其局部和全局变体。


<details>
  <summary>Details</summary>
Motivation: 填补可解释AI领域的理论空白，为具有透明内部机制的机器学习模型提供解释复杂性的基础理解，促进AI系统的透明度和问责制。

Method: 采用参数化复杂度理论框架，系统分析不同ML模型中解释问题的计算复杂度，包括决策树、决策集、决策列表、布尔电路等模型。

Result: 为各种ML模型的解释问题建立了参数化复杂度理论框架，揭示了不同模型解释问题的计算复杂性特征。

Conclusion: 这项工作为XAI领域提供了重要的理论基础，对进一步研究AI系统的透明性和可解释性具有重要价值，推动了AI问责制的发展。

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>


### [152] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: Solly是首个在简化版Liar's Poker中达到精英人类水平的AI智能体，通过自对弈的深度强化学习训练，在单挑和多玩家游戏中都表现出色，超越了大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 虽然AI在德州扑克等游戏中取得了突破，但这些游戏的多玩家动态较弱。研究者希望开发能在多玩家参与度更高的Liar's Poker中达到精英水平的AI。

Method: 使用无模型的actor-critic深度强化学习算法，通过自对弈进行训练。

Result: Solly在胜率和收益方面都达到了精英人类水平，在单挑和多玩家游戏中都获胜超过50%的手牌，并且超越了具有推理能力的大型语言模型。

Conclusion: Solly成功开发了新颖的叫价策略，有效随机化游戏行为，并且不容易被世界级人类玩家利用，证明了深度强化学习在多玩家不完全信息游戏中的有效性。

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [153] [Multifractality and sample size influence on Bitcoin volatility patterns](https://arxiv.org/abs/2511.03314)
*Tetsuya Takaishi*

Main category: q-fin.ST

TL;DR: 研究比特币数据中已实现波动率时间序列的Hurst指数有限样本效应，发现Hurst指数随采样周期增加而减小，并提出了拟合数据的有限样本模型。


<details>
  <summary>Details</summary>
Motivation: 分析有限样本对已实现波动率时间序列Hurst指数的影响，特别是比特币数据中的粗糙波动现象。

Method: 使用比特币数据，研究不同采样周期下Hurst指数的变化，提出有限样本模型进行拟合，并进行多重分形分析。

Result: Hurst指数随采样周期增加而减小，当Δ→0时Hurst指数小于1/2，表明存在粗糙波动。5分钟已实现波动率的相对误差为1%。已实现波动率的多重分形性小于价格收益率时间序列。

Conclusion: 有限样本效应对Hurst指数有显著影响，比特币波动率表现出粗糙特性，且已实现波动率的多重分形性较弱。

Abstract: The finite sample effect on the Hurst exponent (HE) of realized volatility
time series is examined using Bitcoin data. This study finds that the HE
decreases as the sampling period $\Delta$ increases and a simple finite sample
ansatz closely fits the HE data. We obtain values of the HE as $\Delta
\rightarrow 0$, which are smaller than 1/2, indicating rough volatility. The
relative error is found to be $1\%$ for the widely used five-minute realized
volatility. Performing a multifractal analysis, we find the multifractality in
the realized volatility time series, smaller than that of the price-return time
series.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [154] [Robust optimal consumption, investment and reinsurance for recursive preferences](https://arxiv.org/abs/2511.03031)
*Elizabeth Dadzie,Wilfried Kuissi-Kamdem,Marcel Ndengo*

Main category: math.OC

TL;DR: 该论文研究了具有Epstein-Zin递归偏好的保险公司在模型不确定性下的稳健最优消费、投资和再保险问题，通过求解耦合的前向-后向随机微分方程得到闭式解。


<details>
  <summary>Details</summary>
Motivation: 保险公司在模型不确定性下需要制定稳健的最优策略，考虑模糊厌恶、风险厌恶和跨期替代弹性等因素对决策的影响。

Method: 使用扩散近似的Cramér-Lundberg模型描述保险公司盈余，通过求解耦合的前向-后向随机微分方程推导最优策略和价值函数的闭式解。

Result: 获得了最优消费、投资和再保险策略的闭式解，发现最优消费随风险厌恶和EIS增加而减少，投资和再保险策略与金融市场和保险市场参数相互依赖。

Conclusion: 该研究为保险公司在深度不确定性下管理资本配置和风险转移提供了综合框架，揭示了模糊厌恶等行为因素对最优决策的重要影响。

Abstract: This paper investigates a robust optimal consumption, investment, and
reinsurance problem for an insurer with Epstein-Zin recursive preferences
operating under model uncertainty. The insurer's surplus follows the diffusion
approximation of the Cram\'er-Lundberg model, and the insurer can purchase
proportional reinsurance. Model ambiguity is characterised by a class of
equivalent probability measures, and the insurer, being ambiguity-averse, aims
to maximise utility under the worst-case scenario. By solving the associated
coupled forward-backward stochastic differential equation (FBSDE), we derive
closed-form solutions for the optimal strategies and the value function. Our
analysis reveals how ambiguity aversion, risk aversion, and the elasticity of
intertemporal substitution (EIS) influence the optimal policies. Numerical
experiments illustrate the effects of key parameters, showing that optimal
consumption decreases with higher risk aversion and EIS, while investment and
reinsurance strategies are co-dependent on both financial and insurance market
parameters, even without correlation. This study provides a comprehensive
framework for insurers to manage capital allocation and risk transfer under
deep uncertainty.

</details>


### [155] [Towards a geometric characterization of unbounded integer cubic optimization problems via thin rays](https://arxiv.org/abs/2511.02983)
*Alberto Del Pia*

Main category: math.OC

TL;DR: 本文研究了无界整数多项式优化问题的几何特征，发现对于三次多项式，传统的沿射线无界性特征不再适用，因此引入了薄射线的概念来表征无界性。


<details>
  <summary>Details</summary>
Motivation: 传统上，无界整数线性和二次优化问题可以通过沿射线的无界性来完全表征，但对于三次多项式，这种特征不再成立，因此需要新的几何工具来理解更高次多项式优化问题的无界性。

Method: 引入薄射线的概念（即具有任意小邻域的射线），并证明对于三维及以下整数三次优化问题，薄射线可以表征无界性。同时提供了任意维度下整数二次优化问题的完整无界性特征。

Result: 证明了薄射线可以表征三维及以下整数三次优化问题的无界性，并推测在所有维度都成立。同时在不假设有理系数的条件下，完整表征了任意维度整数二次优化问题的无界性。

Conclusion: 薄射线是分析整数多项式优化问题的重要新工具，特别适用于超越二次情况的更高次多项式优化问题，为理解这类问题的无界性提供了新的几何视角。

Abstract: We study geometric characterizations of unbounded integer polynomial
optimization problems. While unboundedness along a ray fully characterizes
unbounded integer linear and quadratic optimization problems, we show that this
is not the case for cubic polynomials. To overcome this, we introduce thin
rays, which are rays with an arbitrarily small neighborhood, and prove that
they characterize unboundedness for integer cubic optimization problems in
dimension up to three, and we conjecture that the same holds in all dimensions.
Our techniques also provide a complete characterization of unbounded integer
quadratic optimization problems in arbitrary dimension, without assuming
rational coefficients. These results underscore the significance of thin rays
and offer new tools for analyzing integer polynomial optimization problems
beyond the quadratic case.

</details>


### [156] [Projection-width: a unifying structural parameter for separable discrete optimization](https://arxiv.org/abs/2511.02990)
*Alberto Del Pia*

Main category: math.OC

TL;DR: 本文引入了投影宽度的概念，用于可分离约束系统，通过变量和约束的分支分解定义。证明了当投影宽度多项式有界时，多个基本离散优化和计数问题可在多项式时间内求解。


<details>
  <summary>Details</summary>
Motivation: 统一和扩展多个研究领域（整数线性优化、二元多项式优化、布尔可满足性）中已知的最强可解性结果，为非线性离散优化和计数问题提供更广泛的易处理类别。

Method: 通过变量和约束的分支分解定义投影宽度，并基于此结构特性设计多项式时间算法。

Result: 当投影宽度多项式有界时，优化、计数、top-k和加权约束违反等基本问题都可在多项式时间内求解。

Conclusion: 投影宽度提供了一个统一的结构视角，能够统一和显著推广多个独立发展的重要可解性结果，识别出广泛的易处理非线性离散优化和计数问题类别。

Abstract: We introduce the notion of projection-width for systems of separable
constraints, defined via branch decompositions of variables and constraints. We
show that several fundamental discrete optimization and counting problems can
be solved in polynomial time when the projection-width is polynomially bounded.
These include optimization, counting, top-k, and weighted constraint violation.
Our results identify a broad class of tractable nonlinear discrete optimization
and counting problems. Even when restricted to the linear setting, they subsume
and substantially extend some of the strongest known tractability results
across multiple research areas: integer linear optimization, binary polynomial
optimization, and Boolean satisfiability. Although these results originated
independently within different communities and for seemingly distinct problem
classes, our framework unifies and significantly generalizes them under a
single structural perspective.

</details>


### [157] [Min-Max Optimization Is Strictly Easier Than Variational Inequalities](https://arxiv.org/abs/2511.03052)
*Henry Shugart,Jason M. Altschuler*

Main category: math.OC

TL;DR: 本文研究了绕过变分不等式直接解决凸凹min-max问题是否可能更快。在无约束二次目标的标准设置中，发现min-max问题的一阶算法最优收敛率确实优于对应的变分不等式。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过将min-max问题转化为变分不等式来解决，但这种方法可能丢失了min和max变量的不对称性。本文旨在探索是否可以通过绕过这种转换来获得更快的求解速度。

Method: 使用极值多项式的尖锐特征化来分析最优收敛率，通过Green函数和共形映射计算这些多项式。重点研究了无约束二次目标这一标准设置。

Result: 在无约束二次目标情况下，min-max问题的一阶算法最优收敛率严格优于对应的变分不等式。min-max算法能够利用min和max变量的不对称性，而这种特性在转换为变分不等式时会丢失。

Conclusion: 直接解决min-max问题可以比通过变分不等式方法获得更快的收敛速度，关键在于能够利用min和max变量的不对称性。这为min-max问题的求解提供了新的思路。

Abstract: Classically, a mainstream approach for solving a convex-concave min-max
problem is to instead solve the variational inequality problem arising from its
first-order optimality conditions. Is it possible to solve min-max problems
faster by bypassing this reduction? This paper initiates this investigation. We
show that the answer is yes in the textbook setting of unconstrained quadratic
objectives: the optimal convergence rate for first-order algorithms is strictly
better for min-max problems than for the corresponding variational
inequalities. The key reason that min-max algorithms can be faster is that they
can exploit the asymmetry of the min and max variables--a property that is lost
in the reduction to variational inequalities. Central to our analyses are sharp
characterizations of optimal convergence rates in terms of extremal polynomials
which we compute using Green's functions and conformal mappings.

</details>


### [158] [Optimal Boundary Control of Diffusion on Graphs via Linear Programming](https://arxiv.org/abs/2511.03129)
*Harbir Antil,Rainald Löhner,Felipe Pérez*

Main category: math.OC

TL;DR: 提出了一个用于几何网络上稳态扩散和通量优化的线性规划框架，通过离散扩散定律和边界电位控制来优化网络通量，确保物理约束条件。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理几何网络中扩散和通量优化的数学框架，结合物理约束条件，为大规模网络系统提供有效的优化方法。

Method: 使用线性规划方法，在加权有向图上建立离散扩散模型，边界电位作为控制变量，施加通量上限和符号约束，确保物理可行性。

Result: 证明了在无负衰退方向条件下LP存在全局最优解，识别了可行域有界性的充分条件，适用于全秩和秩亏缺的通量映射。

Conclusion: 该框架成功地将经典线性规划理论与现代网络扩散建模相结合，为大规模几何网络优化提供了有效的数学工具。

Abstract: We propose a linear programming (LP) framework for steady-state diffusion and
flux optimization on geometric networks. The state variable satisfies a
discrete diffusion law on a weighted, oriented graph, where conductances are
scaled by edge lengths to preserve geometric fidelity. Boundary potentials act
as controls that drive interior fluxes according to a linear network Laplacian.
The optimization problem enforces physically meaningful sign and flux-cap
constraints at all boundary edges, derived directly from a gradient bound. This
yields a finite-dimensional LP whose feasible set is polyhedral, and whose
boundedness and solvability follow from simple geometric or algebraic
conditions on the network data.
  We prove that under the absence of negative recession
directions--automatically satisfied in the presence of finite box bounds, flux
caps, or sign restrictions--the LP admits a global minimizer. Several
sufficient conditions guaranteeing boundedness of the feasible region are
identified, covering both full-rank and rank-deficient flux maps. The analysis
connects classical results such as the Minkowski--Weyl decomposition, Hoffman's
bound, and the fundamental theorem of linear programming with modern
network-based diffusion modeling.
  Two large-scale examples illustrate the framework: (i) A typical large
stadium in a major modern city, which forms a single connected component with
relatively uniform corridor widths, and a (ii) A complex street network
emanating from a large, historical city center, which forms a multi-component
system.

</details>


### [159] [Adaptive directional decomposition methods for nonconvex constrained optimization](https://arxiv.org/abs/2511.03210)
*Qiankun Shi,Xiao Wang*

Main category: math.OC

TL;DR: 提出了一种新的非凸约束优化算法框架，采用分解策略平衡目标函数减少和约束满足，在确定性和随机设置下分别达到O(ϵ⁻²)和Õ(ϵ⁻⁴)到Õ(ϵ⁻⁶)的迭代复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究非凸约束优化问题，同时处理等式和不等式约束，涵盖确定性和随机设置，旨在开发高效的算法框架。

Method: 采用分解策略平衡目标减少和约束满足，结合自适应步长和惩罚参数更新，在随机设置中利用扰动理论分析理论性质。

Result: 确定性设置下达到O(ϵ⁻²)迭代复杂度找到ϵ-KKT点；随机设置下梯度评估复杂度为Õ(ϵ⁻⁴,ϵ⁻⁶)，约束评估复杂度为Õ(ϵ⁻³,ϵ⁻⁵)。

Conclusion: 提出的自适应方向分解方法在复杂度上达到或优于现有最优结果，为非凸约束优化提供了有效的解决方案。

Abstract: In this paper, we study nonconvex constrained optimization problems with both
equality and inequality constraints, covering deterministic and stochastic
settings. We propose a novel first-order algorithm framework that employs a
decomposition strategy to balance objective reduction and constraint
satisfaction, together with adaptive update of stepsizes and merit parameters.
Under certain conditions, the proposed adaptive directional decomposition
methods attain an iteration complexity of order \(O(\epsilon^{-2})\) for
finding an \(\epsilon\)-KKT point in the deterministic setting. In the
stochastic setting, we further develop stochastic variants of approaches and
analyze their theoretical properties by leveraging the perturbation theory. We
establish the high-probability oracle complexity to find an $\epsilon$-KKT
point of order \( \tilde O(\epsilon^{-4}, \epsilon^{-6}) \) (resp. \(\tilde
O(\epsilon^{-3}, \epsilon^{-5}) \)) for gradient and constraint evaluations, in
the absence (resp. presence) of sample-wise smoothness. To the best of our
knowledge, the obtained complexity bounds are comparable to, or improve upon,
the state-of-the-art results in the literature.

</details>


### [160] [Technical results on the convergence of quasi-Newton methods for nonsmooth optimization](https://arxiv.org/abs/2511.03296)
*Bennet Gebken*

Main category: math.OC

TL;DR: 该论文分析了BFGS方法在非光滑函数优化中的收敛性，提出了特征值行为假设，并证明了在这些假设下能够达到临界点。


<details>
  <summary>Details</summary>
Motivation: 尽管BFGS方法在实践中对非光滑函数优化有效，但缺乏理论收敛性证明，特别是割线方程导致拟牛顿矩阵特征值消失的行为尚未被充分分析。

Method: 通过数值实验推导特征值行为的假设，然后在这些假设下证明极限点的临界性，并分析拟牛顿方法如何探索分段结构。

Result: 建立了特征值行为与收敛性之间的关系，证明了在特定假设下BFGS方法能够达到临界点。

Conclusion: 虽然未能证明观察到的特征值行为确实发生，但这些结果为非光滑函数优化的收敛性提供了新的见解和直觉。

Abstract: It is well-known by now that the BFGS method is an effective method for
minimizing nonsmooth functions. However, despite its popularity, theoretical
convergence results are almost non-existent. One of the difficulties when
analyzing the nonsmooth case is the fact that the secant equation forces
certain eigenvalues of the quasi-Newton matrix to vanish, which is a behavior
that has not yet been fully analyzed. In this article, we show what kind of
behavior of the eigenvalues would be sufficient to be able to prove the
convergence for piecewise differentiable functions. More precisely, we derive
assumptions on the behavior from numerical experiments and then prove
criticality of the limit under these assumptions. Furthermore, we show how
quasi-Newton methods are able to explore the piecewise structure. While we do
not prove that the observed behavior of the eigenvalues actually occurs, we
believe that these results still give insight, and a certain intuition, for the
convergence for nonsmooth functions.

</details>


### [161] [Solutions of Two-stage Stochastic Minimax Problems](https://arxiv.org/abs/2511.03339)
*Hailin Sun,Xiaojun Chen*

Main category: math.OC

TL;DR: 本文研究一类两阶段随机极小极大问题，其中第一阶段目标函数是非凸-凹的，第二阶段是强凸-凹的。建立了第二阶段值函数和解函数的性质，分析了鞍点、极小极大点和KKT点的存在性与关系。采用样本平均逼近方法，证明了KKT点的收敛性，并提出了一种非精确并行近端梯度下降上升算法。


<details>
  <summary>Details</summary>
Motivation: 解决两阶段随机极小极大问题，其中第一阶段非凸-凹、第二阶段强凸-凹的复杂结构，需要建立理论框架和有效算法。

Method: 使用样本平均逼近方法处理随机性，提出非精确并行近端梯度下降上升算法求解问题。

Result: 建立了第二阶段值函数和解函数的理论性质，证明了KKT点的收敛性，数值实验验证了算法的有效性。

Conclusion: 提出的理论框架和算法能够有效解决这类两阶段随机极小极大问题，样本平均逼近方法具有良好的收敛性质。

Abstract: This paper introduces a class of two-stage stochastic minimax problems where
the first-stage objective function is nonconvex-concave while the second-stage
objective function is strongly convex-concave. We establish properties of the
second-stage minimax value function and solution functions, and characterize
the existence and relationships among saddle points, minimax points, and KKT
points. We apply the sample average approximation (SAA) to the class of
two-stage stochastic minimax problems and prove the convergence of the KKT
points as the sample size tends to infinity. An inexact parallel proximal
gradient descent ascent algorithm is proposed to solve this class of problems
with the SAA. Numerical experiments demonstrate the effectiveness of the
proposed algorithm and validate the convergence properties of the SAA approach.

</details>


### [162] [Proximal gradient descent on the smoothed duality gap to solve saddle point problems](https://arxiv.org/abs/2511.03442)
*Olivier Fercoq*

Main category: math.OC

TL;DR: 提出一种算法来最小化自中心平滑间隙，将凸凹鞍点问题转化为最小化问题，在最坏情况下复杂度与重启平均原始对偶混合梯度方法相当，在有利情况下具有线性收敛性。


<details>
  <summary>Details</summary>
Motivation: 凸凹鞍点问题在优化中很常见，但传统方法存在局限性。自中心平滑间隙作为最近引入的最优性度量，能够有效解决这类问题，尽管它本身不是凸函数。

Method: 提出一种算法来最小化自中心平滑间隙，该间隙可表示为凸函数（可能非光滑）和平滑弱凸函数的和。算法将鞍点问题转化为最小化问题。

Result: 算法在最坏情况下的复杂度与重启平均原始对偶混合梯度方法相当，在有利条件下能够实现线性收敛。

Conclusion: 通过最小化自中心平滑间隙，成功将凸凹鞍点问题转化为最小化问题，并开发出具有良好收敛性能的算法。

Abstract: In this paper, we minimize the self-centered smoothed gap, a recently
introduced optimality measure, in order to solve convex-concave saddle point
problems. The self-centered smoothed gap can be computed as the sum of a
convex, possibly nonsmooth function and a smooth weakly convex function.
Although it is not convex, we propose an algorithm that minimizes this
quantity, effectively reducing convex-concave saddle point problems to a
minimization problem. Its worst case complexity is comparable to the one of the
restarted and averaged primal dual hybrid gradient method, and the algorithm
enjoys linear convergence in favorable cases.

</details>


### [163] [A Support-Set Algorithm for Optimization Problems with Nonnegative and Orthogonal Constraints](https://arxiv.org/abs/2511.03443)
*Lei Wang,Xin Liu,Xiaojun Chen*

Main category: math.OC

TL;DR: 提出了一种支持集算法，用于解决具有非负和正交约束的优化问题，该算法通过固定支持集来高效计算全局解，并保证迭代的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究具有非负和正交约束的优化问题，其中可行矩阵具有稀疏性模式，每行最多一个非零项。利用这一结构特性可以显著提高计算效率。

Method: 提出支持集算法，通过策略性地更新支持集来调整非零项的位置，保持迭代的可行性，并利用目标函数近端线性化的闭式解。

Result: 算法全局收敛到一阶驻点，达到ε-近似一阶驻点的迭代复杂度为O(ε^{-2})。数值实验在非负PCA、聚类和社区检测等应用中表现优异。

Conclusion: 支持集算法在保持可行性的同时，显著提高了计算效率，适用于具有非负和正交约束的稀疏优化问题。

Abstract: In this paper, we investigate optimization problems with nonnegative and
orthogonal constraints, where any feasible matrix of size $n \times p$ exhibits
a sparsity pattern such that each row accommodates at most one nonzero entry.
Our analysis demonstrates that, by fixing the support set, the global solution
of the minimization subproblem for the proximal linearization of the objective
function can be computed in closed form with at most $n$ nonzero entries.
Exploiting this structural property offers a powerful avenue for dramatically
enhancing computational efficiency. Guided by this insight, we propose a
support-set algorithm preserving strictly the feasibility of iterates. A
central ingredient is a strategically devised update scheme for support sets
that adjusts the placement of nonzero entries. We establish the global
convergence of the support-set algorithm to a first-order stationary point, and
show that its iteration complexity required to reach an $\epsilon$-approximate
first-order stationary point is $O (\epsilon^{-2})$. Numerical results are
strongly in favor of our algorithm in real-world applications, including
nonnegative PCA, clustering, and community detection.

</details>


### [164] [A Review of Bilevel Optimization: Methods, Emerging Applications, and Recent Advancements](https://arxiv.org/abs/2511.03448)
*Dhaval Pujara,Ankur Sinha*

Main category: math.OC

TL;DR: 本文对双层优化问题的求解技术进行了全面综述，涵盖了线性、混合整数、单目标和多目标等各种形式的双层问题，介绍了经典和进化求解方法，并讨论了在一般优化问题分解和神经网络架构搜索中的应用。


<details>
  <summary>Details</summary>
Motivation: 双层优化适用于分层决策场景，决策者需要考虑利益相关者对其行动的响应来实现自身目标，这种嵌套优化结构在现实应用中广泛存在。

Method: 综述了文献中提出的各种双层优化求解技术，包括经典方法和进化方法，涵盖线性、混合整数、单目标和多目标等不同问题形式。

Result: 提供了双层优化方法的全面概述，介绍了两个新兴应用：一般优化问题的双层分解方法和神经网络架构搜索中的双层优化应用。

Conclusion: 双层优化是处理分层决策问题的有效框架，在机器学习和一般优化等领域具有重要应用价值，需要继续研究更高效的求解算法。

Abstract: This paper presents a comprehensive review of techniques proposed in the
literature for solving bilevel optimization problems encountered in various
real-life applications. Bilevel optimization is an appropriate choice for
hierarchical decision-making situations, where a decision-maker needs to
consider a possible response from stakeholder(s) for each of its actions to
achieve his own goals. Mathematically, it leads to a nested optimization
structure, in which a primary (leader's) optimization problem contains a
secondary (follower's) optimization problem as a constraint. Various forms of
bilevel problems, including linear, mixed-integer, single-objective, and
multi-objective, are covered. For bilevel problem solving methods, various
classical and evolutionary approaches are explained. Along with an overview of
various areas of applications, two recent considerations of bilevel approach
are introduced. The first application involves a bilevel decomposition approach
for solving general optimization problems, and the second application involves
Neural Architecture Search (NAS), which is a prime example of a bilevel
optimization problem in the area of machine learning.

</details>


### [165] [Explicit Ensemble Learning Surrogate for Joint Chance-Constrained Optimal Power Flow](https://arxiv.org/abs/2511.03515)
*Amir Bahador Javadi,Amin Kargarian*

Main category: math.OC

TL;DR: 提出了一种基于集成支持向量机的联合机会约束最优潮流方法，通过多个线性分类器训练模拟数据，使用Big-M重构将其转化为可处理的超平面约束，实现了概率线路限制的多面体近似。


<details>
  <summary>Details</summary>
Motivation: 可再生能源渗透率增加给电力系统带来不确定性，传统确定性优化方法面临挑战。机会约束优化能平衡成本和风险，但联合机会约束存在计算困难。

Method: 使用集成支持向量机作为联合机会约束最优潮流的代理模型，在模拟最优潮流数据上训练多个线性分类器，通过Big-M重构将其转化为超平面约束。

Result: 在IEEE 118节点系统上的数值实验表明，该方法实现了接近最优的成本，平均误差仅为0.03%。

Conclusion: 集成代理模型有望成为电力系统风险感知优化的高效透明工具。

Abstract: The increasing penetration of renewable generation introduces uncertainty
into power systems, challenging traditional deterministic optimization methods.
Chance-constrained optimization offers an approach to balancing cost and risk;
however, incorporating joint chance constraints introduces computational
challenges. This paper presents an ensemble support vector machine surrogate
for joint chance constraint optimal power flow, where multiple linear
classifiers are trained on simulated optimal power flow data and embedded as
tractable hyperplane constraints via Big--M reformulations. The surrogate
yields a polyhedral approximation of probabilistic line flow limits that
preserves interpretability and scalability. Numerical experiments on the IEEE
118-bus system show that the proposed method achieves near-optimal costs with a
negligible average error of $0.03\%$. These results demonstrate the promise of
ensemble surrogates as efficient and transparent tools for risk-aware
optimization of power systems.

</details>


### [166] [HJB equations driven by the Dirichlet-Ferguson Laplacian in Wasserstein-Sobolev spaces](https://arxiv.org/abs/2511.03522)
*François Delarue,Mattia Martini,Giacomo Enrico Sodini*

Main category: math.OC

TL;DR: 本文研究了定义在平环面概率测度空间上的线性和非线性PDE，建立了基于Wasserstein-Sobolev空间的解析框架，证明了输运-扩散方程和Hamilton-Jacobi方程解的存在唯一性，并连接了PDE方法与相互作用粒子系统的概率表示。


<details>
  <summary>Details</summary>
Motivation: 研究概率测度空间上的PDE理论，建立无限维Laplacian诱导的Dirichlet形式与Wasserstein-Sobolev空间的联系，为在Wasserstein空间中分析输运-扩散方程和Hamilton-Jacobi方程提供理论基础。

Method: 开发基于Wasserstein-Sobolev空间H^{1,2}(P(T^d), W_2, D)的解析框架，利用Dirichlet-Ferguson测度，建立PDE方法与相互作用粒子系统的概率表示之间的联系。

Result: 证明了在Wasserstein空间中输运-扩散方程和Hamilton-Jacobi方程强解的存在唯一性，并给出了相应的Kolmogorov型概率表示。

Conclusion: 成功建立了概率测度空间上PDE的完整理论框架，将PDE方法与概率表示相结合，并扩展到半线性方程和平均场最优控制问题，提供了有限维近似方法。

Abstract: We study linear and nonlinear PDEs defined on the space of
$\mathcal{P}(\mathbb{T}^d)$ over the flat torus $\mathbb{T}^d$, equipped with
the Dirichlet-Ferguson measure $\mathcal{D}$. We first develop an analytic
framework based on the Wasserstein-Sobolev space
$H^{1,2}(\mathcal{P}(\mathbb{T}^d), W_2, \mathcal{D})$ associated with the
Dirichlet form induced by the infinite-dimensional Laplacian acting on
functions of measures. Within this setting, we establish existence and
uniqueness results for transport-diffusion and Hamilton-Jacobi equations in the
Wasserstein space. Our analysis connects the PDE approach with a corresponding
interacting particles system providing a probabilistic (Kolmogorov-type)
representation of strong solutions. Finally, we extend the theory to semilinear
equations and mean-field optimal control problems, together with consistent
finite-dimensional approximations.

</details>


### [167] [Improving Directions in Mixed Integer Bilevel Linear Optimization](https://arxiv.org/abs/2511.03566)
*Federico Battista,Ted K. Ralphs*

Main category: math.OC

TL;DR: 提出基于改进方向的混合整数双层线性规划求解框架，通过单一子问题同时处理可行性检查和有效不等式生成，提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 现有MIBLP求解方法分别管理可行性检查和不等式生成两种子问题，效率较低。本文探索两者的紧密联系，寻求统一处理方案。

Method: 构建基于追随者问题改进可行方向的单一子问题框架，将可行性检查和不等式生成统一处理，扩展最优性松弛层次到混合整数场景。

Result: 数值实验表明该方法能带来实际性能提升，改进方向相关的交割闭包与松弛可行域完全一致。

Conclusion: 改进方向在MIBLP求解中具有基础性作用，统一处理可行性检查和不等式生成能有效提升求解效率。

Abstract: We consider the central role of improving directions in solution methods for
mixed integer bilevel linear optimization problems (MIBLPs). Current
state-of-the-art methods for solving MIBLPs employ the branch-and-cut framework
originally developed for solving mixed integer linear optimization problems.
This approach relies on oracles for two kinds of subproblems: those for
checking whether a candidate pair of leader's and follower's decisions is
bilevel feasible, and those required for generating valid inequalities.
Typically, these two types of oracles are managed separately, but in this work,
we explore their close connection and propose a solution framework based on
solving a single type of subproblem: determining whether there exists a
so-called improving feasible direction for the follower's problem. Solution of
this subproblem yields information that can be used both to check feasibility
and to generate strong valid inequalities. Building on prior works, we expose
the foundational role of improving directions in enforcing the follower's
optimality condition and extend a previously known hierarchy of
optimality-based relaxations to the mixed-integer setting, showing that the
associated relaxed feasible regions coincide exactly with the closure
associated with intersection cuts derived from improving directions. Numerical
results with an implementation using a modified version of the open source
solver MibS show that this approach can yield practical improvements.

</details>


### [168] [Exploiting Over-Approximation Errors as Preview Information for Nonlinear Control](https://arxiv.org/abs/2511.03577)
*Antoine Aspeel,Antoine Girard,Thiago Alves Lima*

Main category: math.OC

TL;DR: 提出了一种利用过近似误差作为预览信息的非线性约束系统控制方法，通过固定点方程求解具体化问题


<details>
  <summary>Details</summary>
Motivation: 传统方法将过近似误差视为未知扰动，而本文发现这种误差可以作为输入相关的预览信息加以利用

Method: 引入知情策略概念，将具体化问题表述为固定点方程，使用Brouwer固定点定理保证解的存在性，针对不同系统类型采用闭式、线性或凸规划方法求解

Result: 为输入仿射系统提供了高效计算方案，为非线性系统设计了基于Banach固定点定理的迭代方法

Conclusion: 过近似误差可以作为有价值的预览信息，通过固定点理论框架能够有效解决非线性约束系统的控制问题

Abstract: We study the control of nonlinear constrained systems via
over-approximations. Our key observation is that the over-approximation error,
rather than being an unknown disturbance, can be exploited as input-dependent
preview information. This leads to the notion of informed policies, which
depend on both the state and the error. We formulate the concretization problem
-- recovering a valid input for the true system from a preview-based policy --
as a fixed-point equation. Existence of solutions follows from the Brouwer
fixed-point theorem, while efficient computation is enabled through
closed-form, linear, or convex programs for input-affine systems, and through
an iterative method based on the Banach fixed-point theorem for nonlinear
systems.

</details>


### [169] [Geometrically robust least squares through manifold optimization](https://arxiv.org/abs/2511.03644)
*Jeremy Coulson,Alberto Padoan,Cyrus Mostajeran*

Main category: math.OC

TL;DR: 提出了一种解决几何鲁棒最小二乘问题的方法，将问题表述为乘积流形上的极小极大优化问题，使用精确罚函数法处理约束，并提出一阶梯度下降上升算法。


<details>
  <summary>Details</summary>
Motivation: 解决在信号处理和数驱控制等领域中，模型受几何约束的鲁棒最小二乘问题。

Method: 将问题表述为乘积流形上的极小极大优化问题，对约束应用精确罚函数法，提出一阶梯度下降上升算法。

Result: 通过示例说明了算法的收敛特性，证明了方法的有效性。

Conclusion: 该方法为解决信号处理和数驱控制中的广泛问题提供了鲁棒方法。

Abstract: This paper presents a methodology for solving a geometrically robust least
squares problem, which arises in various applications where the model is
subject to geometric constraints. The problem is formulated as a minimax
optimization problem on a product manifold, where one variable is constrained
to a ball describing uncertainty. To handle the constraint, an exact penalty
method is applied. A first-order gradient descent ascent algorithm is proposed
to solve the problem, and its convergence properties are illustrated by an
example. The proposed method offers a robust approach to solving a wide range
of problems arising in signal processing and data-driven control.

</details>


### [170] [High-order Accumulative Regularization for Gradient Minimization in Convex Programming](https://arxiv.org/abs/2511.03723)
*Yao Ji,Guanghui Lan*

Main category: math.OC

TL;DR: 提出了高阶累积正则化(AR)框架，统一解决凸和一致凸梯度范数最小化问题，将快速函数值残差收敛率转化为匹配的梯度范数收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有高阶方法存在函数值残差快速下降但梯度范数收敛较慢的差距，需要系统性地将函数值收敛率转化为梯度范数收敛率。

Method: 引入高阶累积正则化框架，设计参数自由算法，无需输入问题参数，允许高阶步骤的不精确解。

Result: 在凸问题中，AR方法达到了与函数值残差收敛率相匹配的梯度范数收敛率；在一致凸设置下，在不同下曲率条件下建立了梯度范数的线性、超线性和次线性收敛。

Conclusion: 该框架显著推广了现有参数自由和不精确高阶方法，将一阶算法作为特例，为广泛光滑度和曲率机制下的快速梯度最小化提供了统一方法。

Abstract: This paper develops a unified framework of high-order accumulative
regularization (AR) framework for convex and uniformly convex gradient norm
minimization. Existing high-order methods often exhibit a gap: the function
value residual decreases fast, while the gradient norm converges much slower.
To close this gap, we introduce AR that systematically transforms fast function
value residual convergence rate into fast (matching) gradient norm convergence
rate.
  Specifically, for composite convex problems, for computing an approximate
solution such that the norm of its (sub)gradient does not exceed $\varepsilon,$
the proposed AR methods match the best corresponding convergence rate for the
function value residual. We further extend the framework to uniformly convex
settings, establishing linear, superlinear and sublinear convergence of the
gradient norm under different lower curvature conditions. Moreover, we design
parameter-free algorithms that require no input of problem parameters, e.g.,
Lipschitz constant of the $p$-th order gradient, the initial optimality gap and
the uniform convexity parameter, and allows inexact solution for each
high-order step. To our best knowledge, no parameter-free methods can attain
such a fast gradient norm convergence rate which matches that of the function
value residual in the convex case, and no such parameter-free methods for
uniformly convex problems exist. These results substantially generalize
existing parameter-free and inexact high-order methods and recover first-order
algorithms as special cases, providing a unified approach for fast gradient
minimization across a broad range of smoothness and curvature regimes.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [171] [Robust reduced-order model predictive control using peak-to-peak analysis of filtered signals](https://arxiv.org/abs/2511.03002)
*Johannes Köhler,Carlo Scholz,Melanie Zeilinger*

Main category: eess.SY

TL;DR: 提出一种基于降阶模型(ROM)的模型预测控制(MPC)方法，通过预测误差边界系统保证全阶系统的约束满足和鲁棒性能，在100维质量-弹簧-阻尼器系统上相比现有方法将保守性降低了四个数量级。


<details>
  <summary>Details</summary>
Motivation: 解决大规模线性系统的MPC设计问题，使用降阶模型实现计算可行性，同时保证鲁棒约束满足。

Method: 使用降阶模型结合鲁棒控制工具，通过预测标量误差边界系统来获得全阶系统输出的保证边界，然后基于此边界制定鲁棒ROM-MPC。方法包括误差分析、峰值增益边界和使用滤波信号三个步骤。

Result: 在100维质量-弹簧-阻尼器系统上验证，相比现有方法将保守性降低了四个数量级。

Conclusion: 该方法能够在大规模线性系统中实现计算可行的MPC设计，同时保证鲁棒约束满足和性能，显著减少了保守性。

Abstract: We address the design of a model predictive control (MPC) scheme for
large-scale linear systems using reduced-order models (ROMs). Our approach uses
a ROM, leverages tools from robust control, and integrates them into an MPC
framework to achieve computational tractability with robust constraint
satisfaction. Our key contribution is a method to obtain guaranteed bounds on
the predicted outputs of the full-order system by predicting a (scalar)
error-bounding system alongside the ROM. This bound is then used to formulate a
robust ROM-based MPC that guarantees constraint satisfaction and robust
performance. Our method is developed step-by-step by (i) analysing the error,
(ii) bounding the peak-to-peak gain, an (iii) using filtered signals. We
demonstrate our method on a 100-dimensional mass-spring-damper system,
achieving over four orders of magnitude reduction in conservatism relative to
existing approaches.

</details>


### [172] [Oscillation Analysis and Damping Control for a Proposed North American AC-DC Macrogrid](https://arxiv.org/abs/2511.03017)
*Kaustav Chatterjee,Sameer Nekkalapu,Antos Varghese,Marcelo Elizondo,Quan Nguyen,Xiaoyuan Fan*

Main category: eess.SY

TL;DR: 本文研究了北美东西部电网通过多端直流系统互联对小信号稳定性的影响，识别了阻尼不足的区间振荡模式，并设计了基于广域反馈的阻尼控制器来抑制不稳定振荡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注东西部电网互联的容量共享和频率支持优势，但尚未深入分析由此可能产生的小信号稳定性挑战，特别是区间振荡问题。

Method: 开发了定制化的多端直流系统动态模型，并与工业级的东西部电网模型集成；采用基于模型的振荡分析方法识别阻尼不足的区间模式；设计了基于广域反馈的阻尼控制器，通过频率扫描方法进行数据驱动的模型线性化和控制器综合。

Result: 识别了多端直流系统集成后东西部电网区间模式的变化，发现了阻尼不足的振荡模式；设计的阻尼控制器在正常运行工况和选定故障场景下均能有效改善阻尼性能。

Conclusion: 东西部电网通过多端直流系统互联会引入小信号稳定性风险，但通过精心设计的阻尼控制器可以有效缓解这些风险，确保系统的稳定运行。

Abstract: In recent years, several studies conducted by both industry and U.S.
Department of Energy (DOE)-funded initiatives have proposed linking North
America's Eastern and Western Interconnections (EI and WI) through a
multiterminal DC (MTDC) macrogrid. These studies have explored the advantages
and opportunities of the proposed configuration from the perspectives of
capacity sharing and frequency support. However, the potential challenges of
small-signal stability arising from this interconnection have not been
thoroughly examined. To address this gap, detailed model-based simulation
studies are performed in this paper to assess the risks of poorly damped
inter-area oscillations in the proposed macrogrid. A custom-built dynamic model
of the MTDC system is developed and integrated with industry-grade models of
the EI and WI, incorporating high levels of inverter-based energy resources.
Through model-based oscillation analysis, potential shifts in inter-area modes
for both EI and WI, resulting from the MTDC integration are characterized, and
modes with inadequate damping are identified. Furthermore, to mitigate the
risks of unstable oscillations, supplementary damping controllers are designed
for the MTDC system, leveraging wide-area feedback to modulate active power set
points at selected converter stations. A frequency scanning approach is
employed for data-driven model linearization and controller synthesis. The
damping performance is evaluated under the designed operating conditions and
selected contingency scenarios.

</details>


### [173] [Quantifying Power Systems Resilience Using Statistical Analysis and Bayesian Learning](https://arxiv.org/abs/2511.03043)
*Apsara Adhikari,Charlotte Wertz,Anamika Dubey,Arslan Ahmad,Ian Dobson*

Main category: eess.SY

TL;DR: 提出了一个使用统计和贝叶斯学习方法的框架，定量建模天气参数与电力系统韧性指标之间的关系，识别出风速、温度和降水是影响特定地区韧性指标的关键天气变量。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件日益频繁和强烈，显著影响电网，导致大规模停电并影响电力系统韧性，但目前缺乏系统建模天气参数影响以量化韧性的研究。

Method: 利用真实世界的公开停电和天气数据，采用统计和贝叶斯学习方法建立天气参数与电力系统韧性指标的定量关系模型。

Result: 通过对伊利诺伊州库克县和佛罗里达州迈阿密-戴德县的案例研究，发现风速、温度和降水是韧性分析和风险评估的关键因素，且这些天气变量在联合研究时具有组合效应。

Conclusion: 该框架为理解天气事件如何影响配电系统性能提供了宝贵见解，支持决策者制定更有效的风险缓解、资源分配和适应气候变化条件的策略。

Abstract: The increasing frequency and intensity of extreme weather events is
significantly affecting the power grid, causing large-scale outages and
impacting power system resilience. Yet limited work has been done on
systematically modeling the impacts of weather parameters to quantify
resilience. This study presents a framework using statistical and Bayesian
learning approaches to quantitatively model the relationship between weather
parameters and power system resilience metrics. By leveraging real-world
publicly available outage and weather data, we identify key weather variables
of wind speed, temperature, and precipitation influencing a particular region's
resilience metrics. A case study of Cook County, Illinois, and Miami-Dade
County, Florida, reveals that these weather parameters are critical factors in
resiliency analysis and risk assessment. Additionally, we find that these
weather variables have combined effects when studied jointly compared to their
effects in isolation. This framework provides valuable insights for
understanding how weather events affect power distribution system performance,
supporting decision-makers in developing more effective strategies for risk
mitigation, resource allocation, and adaptation to changing climatic
conditions.

</details>


### [174] [Microgrids optimal radial reconfiguration via FORWARD algorithm](https://arxiv.org/abs/2511.03059)
*Joan Vendrell Gallart,Russell Bent,Solmaz Kia*

Main category: eess.SY

TL;DR: 本文提出了一种基于排列的迭代搜索方法，结合FORWARD方法，用于解决微电网资源分配和辐射状配置设计问题，有效识别可行且接近最优的辐射状网络结构，并作为MINLP求解器的预热策略。


<details>
  <summary>Details</summary>
Motivation: 微电网在整合分布式能源资源、增强能源韧性和减少停电影响方面具有潜力，但其分散化和动态运行带来了复杂的能源管理问题，包括供需平衡、系统稳定性和运营成本最小化，这些问题通常需要解决计算上难以处理的NP-hard MINLP问题。

Method: 提出了一种基于排列的迭代搜索方法，结合FORWARD方法，用于高效识别可行且接近最优的辐射状网络结构，同时固有地尊重物理约束，并将该方法作为基准MINLP求解器的预热策略。

Result: 该方法能够有效解决微电网资源分配和辐射状配置设计问题，识别可行且接近最优的辐射状网络结构，为全面的微电网设计提供可扩展的解决方案。

Conclusion: 通过引入基于排列的迭代搜索方法结合FORWARD方法，本文提供了一种可扩展的解决方案，用于解决微电网资源分配和辐射状配置设计中的复杂MINLP问题，提高了求解效率和可行性保证。

Abstract: Microgrids offer a promising paradigm for integrating distributed energy
resources, bolstering energy resilience, and reducing the impact of blackouts.
However, their inherent decentralization and dynamic operation present
substantial energy management complexities. These complexities, including
balancing supply and demand, ensuring system stability, and minimizing
operational costs, often necessitate solving computationally intractable
NP-hard Mixed-Integer Non-Linear Programming (MINLP) problems. Traditional
MINLP solvers struggle with the scalability and feasibility guarantees required
for these challenges. To address this, this paper tackles the problem of
resource allocation and radial configuration design for microgrid power
distribution and proposes and abstracted problem which is solved by introducing
a permutation-based iterative search method over the recently introduced
FORWARD method to efficiently identify feasible, near-optimal radial network
structures while inherently respecting physical constraints. Furthermore, this
paper investigates the integration of the proposed method as a warm-start
strategy for benchmark MINLP solvers offering a scalable solution for
comprehensive microgrid design.

</details>


### [175] [Active Noise Control Method Using Time Domain Neural Networks for Path Decoupling](https://arxiv.org/abs/2511.03162)
*Yijing Chu,Qinxuan Xiang,Sipei Zhao,Ming Wu,Y. Zhao,Guangzheng Yu*

Main category: eess.SY

TL;DR: 提出了一种结合固定值神经网络和自适应策略的混合方法，用于高效的去中心化主动噪声控制，解决了多通道系统中的串扰和预滤波误差问题。


<details>
  <summary>Details</summary>
Motivation: 在去中心化主动噪声控制系统中，多通道次级源和误差麦克风之间的串扰显著降低控制精度，而滤波-x类算法中的参考信号预滤波可能进一步引入建模误差。

Method: 采用混合方法：自适应滤波器使用LMS算法在线建模自身通道的主路径，神经网络（DecNet）用于次级路径反演和解耦，在时域实现以保证因果性并避免延迟。

Result: 使用实测声学路径的仿真结果表明，该方法在不同声学条件下优于现有的基于传统自适应滤波器或神经网络固定系数方法的ANC算法。

Conclusion: 提出的DecNet-LMS混合算法有效解决了去中心化ANC系统中的串扰和预滤波问题，提高了控制精度和性能。

Abstract: In decentralized active noise control (ANC) systems, crosstalk between
multichannel secondary sources and error microphones significantly degrades
control accuracy. Moreover, prefiltering reference signals in filtered-x (Fx)
type algorithms may further introduce modeling errors. A theoretical analysis
of the Fx-based decentralized control algorithm was performed, which reveals
how prefiltering and crosstalk affect the control performance. Then, a hybrid
method combining fixed-value neural networks and adaptive strategies was
proposed for efficient decentralized ANC. The adaptive filter models the
primary path of its own channel online using the least mean square (LMS)
algorithm while the neural network (named DecNet) is used for secondary paths
inverting and decoupling. The hybrid DecNet-LMS algorithm was implemented in
the time domain to guarantee causality and avoid latency. Simulation results
with measured acoustic paths show that the proposed method outperforms the
existing ANC algorithms using either traditional adaptive filters or neural
network-based fixed-coefficient methods under different acoustic conditions.

</details>


### [176] [MHE in Output Feedback Control of Uncertain Nonlinear Systems via IQCs](https://arxiv.org/abs/2511.03221)
*Yang Guo,Stefan Streif*

Main category: eess.SY

TL;DR: 提出了一种针对非线性约束系统的移动水平估计方案，该方案能够处理参数或静态非线性不确定性，并与预定状态反馈控制器结合，在满足特定可检测性条件下实现闭环系统的输入到状态稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对具有参数或静态非线性不确定性的非线性约束系统，需要开发能够在存在估计误差的情况下保持系统稳定性的估计方案，特别是与预定控制器协同工作的情况。

Method: 利用积分二次约束引入了一种新的鲁棒可检测性概念，提出了移动水平估计方案，该方案与预定状态反馈控制器结合，确保闭环系统对外部干扰具有输入到状态稳定性。

Result: 当不确定系统在控制器驱动下满足所提出的可检测性概念时，所设计的移动水平估计方案能够保证闭环系统对外部干扰的输入到状态稳定性。

Conclusion: 所提出的移动水平估计方案为非线性约束系统提供了一种有效的状态估计方法，能够在存在不确定性的情况下与预定控制器协同工作，确保闭环系统的稳定性。

Abstract: We propose a moving horizon estimation (MHE) scheme for general nonlinear
constrained systems with parametric or static nonlinear uncertainties and a
predetermined state feedback controller that is assumed to robustly stabilize
the system in the absence of estimation errors. Leveraging integral quadratic
constraints (IQCs), we introduce a new notion of detectability that is robust
to possibly non-parametric uncertainties and verifiable in practice. Assuming
that the uncertain system driven by the controller satisfies this notion of
detectability, we provide an MHE formulation such that the closed-loop system
formed of the uncertain system, the controller and MHE is input-to-state stable
w.r.t. exogenous disturbances.

</details>


### [177] [Theoretical and Experimental Limitations of RoCoF Estimation](https://arxiv.org/abs/2511.03249)
*Gutierrez-Florensa,F. Sanniti,D. Tedeschi,L. Sigrist,A. Ortega,F. Milano*

Main category: eess.SY

TL;DR: 提出了一种基于微分几何和流体力学概念的数值鲁棒方法，用于精确估计频率变化率(RoCoF)，并在低惯量电力系统中测试了该方法，开发了更快的基于RoCoF的欠频减载控制方案。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统向低惯量和基于变流器的发电资产转变，增加了暂态严重性，使得频率和RoCoF估计对现有设备更加复杂和不精确，这影响了反孤岛、欠频减载等保护系统的可靠性。

Method: 采用从微分几何和流体力学继承的概念，开发了一种数值鲁棒方法，使用高采样率的真实实验测量数据进行测试，并基于此开发了更快的RoCoF基欠频减载控制逻辑。

Result: 该方法提供了关于故障性质的信息，可用于改进保护系统的响应，在低惯量电力系统中实现了更精确的RoCoF估计。

Conclusion: 所提出的数值鲁棒方法能够有效解决低惯量电力系统中RoCoF估计的挑战，为保护系统提供更可靠的故障信息，从而改善系统安全运行。

Abstract: A precise estimation of the Rate of Change of Frequency (RoCoF) is crucial
for secure power system operation. In fact, RoCoF is strictly related to the
amount of the available physical and/or virtual inertia of the system and the
severity of the active power unbalance following a disturbance. For this
reason, it is widely exploited in different protection systems, e.g.,
Anti-Islanding, Under Frequency Load Shedding (UFLS) and wide-area protection
systems. The new paradigm of modern power systems, with a low-inertia and
converter-based generation assets, is increasing the transient severity, making
the frequency and the RoCoF estimation more complex and less precise for the
actual devices. This work addresses this issue by proposing a numerically
robust approach based on concepts inherited from differential geometry and
fluid mechanics. The proposed approach is then tested with high-sampling real
experimental measurements and used to develop a faster control logic for a
RoCoF-based UFLS control scheme. The proposed approach provides information to
protections regarding the nature of the contingency which can be used to
improve its response.

</details>


### [178] [Evolutionary Dynamics in Continuous-time Finite-state Mean Field Games -- Part II: Stability](https://arxiv.org/abs/2511.03297)
*Leonardo Pedroso,Andrea Agazzi,W. P. M. H. Heemels,Mauro Salazar*

Main category: eess.SY

TL;DR: 本文研究了具有大群体玩家的动态博弈中混合静态纳什均衡(MSNE)的演化稳定性，给出了确保局部和全局稳定性的条件。


<details>
  <summary>Details</summary>
Motivation: 在Part I中引入了演化模型和MSNE解概念，现在需要研究MSNE在演化动态下的稳定性，以理解其在长期动态博弈中的稳健性。

Method: 通过分析MSNE的结构和博弈的收益映射，推导出在演化动态下确保局部和全局稳定性的条件。

Result: 得到了MSNE在演化动态下稳定性的充分条件，这些条件既涉及MSNE的结构特性，也涉及博弈收益映射的性质。

Conclusion: MSNE的演化稳定性条件揭示了其在面对策略偏离时能够稳健出现和持续的条件，为大群体动态博弈的长期可行性提供了理论依据。

Abstract: We study a dynamic game with a large population of players who choose actions
from a finite set in continuous time. Each player has a state in a finite state
space that evolves stochastically with their actions. A player's reward depends
not only on their own state and action but also on the distribution of states
and actions across the population, capturing effects such as congestion in
traffic networks. In Part I, we introduced an evolutionary model and a new
solution concept - the mixed stationary Nash Equilibrium (MSNE) - which
coincides with the rest points of the mean field evolutionary model under
meaningful families of revision protocols. In this second part, we investigate
the evolutionary stability of MSNE. We derive conditions on both the structure
of the MSNE and the game's payoff map that ensure local and global stability
under evolutionary dynamics. These results characterize when MSNE can robustly
emerge and persist against strategic deviations, thereby providing insight into
its long-term viability in large population dynamic games.

</details>


### [179] [Lightwave Power Transfer-Enabled Underwater Optical ISAC Systems under Ship Attitude Variation](https://arxiv.org/abs/2511.03366)
*Kapila W. S. Palitharathna,Constantinos Psomas,Ioannis Krikidis*

Main category: eess.SY

TL;DR: 提出了一种光波能量传输的水下光集成感知与通信系统，通过海面船只上的接入点向海底传感器和目标节点传输光信号，实现能量收集、上行通信和目标定位功能。


<details>
  <summary>Details</summary>
Motivation: 解决水下环境中集成感知与通信的挑战，特别是在考虑船只姿态变化等实际部署条件下的系统性能优化问题。

Method: 使用光波信号传输，建立包含船只滚转、俯仰和偏航角变化的系统模型，推导目标定位均方误差和上行数据速率的闭式近似表达式。

Result: 分析和仿真结果高度一致，验证了所提模型和推导表达式，揭示了O-ISAC系统中的基本通信-感知权衡，实现了10^{-2} m²的最小定位误差和0.55的最优能量收集使用比。

Conclusion: 该系统在考虑实际部署条件下能够有效实现水下光集成感知与通信，提供了包括最优相机布置和能量收集策略在内的有价值设计见解。

Abstract: In this paper, we propose a lightwave power transfer-enabled underwater
optical integrated sensing and communication (O-ISAC) system, where an access
point (AP) mounted on a seasurface ship transmits lightwave signals to two
nodes, namely ($i$) a seabed sensor that harvests energy and transmits uplink
information to the AP, and ($ii$) a sensing target whose position is estimated
by the AP using an array of pinhole cameras. To capture practical deployment
conditions, the ship attitude variation is modeled through its roll, pitch, and
yaw angles, each following a Gaussian distribution under low-to-moderate sea
states. Closed-form approximations are derived for the mean squared error (MSE)
of target localization and the achievable uplink data rate. Analytical and
simulation results demonstrate excellent agreement, validating the proposed
models and derived expressions, while revealing the fundamental
communication-sensing tradeoff in the O-ISAC system. The results further
provide valuable design insights, including the optimal camera placement on the
ship to minimize localization error, achieving a minimum MSE of $10^{-2}$
$\text{m}^2$ with multiple cameras under roll, pitch, and yaw angle variation
of $10^{\circ}$, and the optimal harvest-use ratio of $0.55$ for the considered
setup.

</details>


### [180] [A Digital Twin of Evaporative Thermo-Fluidic Process in Fixation Unit of DoD Inkjet Printers](https://arxiv.org/abs/2511.03379)
*Samarth Toolhally,Joeri Roelofs,Siep Weiland,Amritam Das*

Main category: eess.SY

TL;DR: 提出了喷墨打印机固定单元的模块化数字孪生模型，通过无限维状态估计器从有限传感器数据推断固定状态，并开发H∞最优Luenberger状态估计器实时监测纸张上的时空热效应。


<details>
  <summary>Details</summary>
Motivation: 喷墨打印中纸张的最佳湿度对打印质量至关重要，需要通过热风冲击在固定单元中实现。需要开发能够建模热流体干燥过程并监测其时空性能的数字孪生系统。

Method: 采用图论模型实现模块化，每个节点代表固定单元不同部分的热流体动力学。蒸发建模为非线性边界效应，通过线性分式表示与节点动力学耦合。使用偏积分方程框架进行稳定性、输入输出分析、仿真和快速原型设计。

Result: 开发了模块化数字孪生模型，能够从有限传感器数据推断固定状态，对干扰保持鲁棒性。通过商业打印机的操作数据验证了模型有效性。

Conclusion: 提出的数字孪生方法成功实现了喷墨打印机固定单元的实时监测和状态估计，为打印质量优化提供了有效工具。

Abstract: In inkjet printing, optimal paper moisture is crucial for print quality,
achieved through hot-air impingement in the fixation unit. This paper presents
a modular digital twin of the fixation unit, modeling the thermo-fluidic drying
process and monitoring its spatio-temporal performance. The novel approach
formulates the digital twin as an infinite-dimensional state estimator that
infers fixation states from limited sensor data, while remaining robust to
disturbances. Modularity is achieved through a graph-theoretic model, where
each node represents thermo-fluidic dynamics in different sections of the
fixation unit. Evaporation is modeled as a nonlinear boundary effect coupled
with node dynamics via Linear Fractional Representation. Using the Partial
Integral Equation (PIE) framework, we develop a unified approach for stability,
input-output analysis, simulation, and rapid prototyping, validated with
operational data from a commercial printer. An $\mathcal{H}_{\infty}$-optimal
Luenberger state estimator is then synthesized to estimate thermal states from
available sensor data, enabling real-time monitoring of spatio-temporal thermal
effects on paper sheets.

</details>


### [181] [Maximum Likelihood Estimation of Dynamic Sub-Networks with Missing Data](https://arxiv.org/abs/2511.03391)
*João Victor Galvão da Mata,Anders Hansson,Martin S. Andersen*

Main category: eess.SY

TL;DR: 提出一种最大似然估计方法，能够在复杂互联系统中识别子网络而无需估计整个网络，显著降低计算复杂度并增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然估计在大规模网络识别中计算成本过高，且需要共享敏感的内部数据，存在隐私和效率问题。

Method: 基于特定拓扑条件，仅使用目标子网络和直接连接的分离器子网络中的局部测量信号来估计子网络参数。

Result: 建立了网络可分离性的理论条件，推导了子网络的概率密度函数，并通过数值示例验证了方法的有效性。

Conclusion: 该方法能够显著降低计算复杂度，同时通过避免跨组织边界共享敏感数据来增强隐私保护，为大规模网络识别提供了实用解决方案。

Abstract: Maximum likelihood estimation is effective for identifying dynamical systems,
but applying it to large networks becomes computationally prohibitive. This
paper introduces a maximum likelihood estimation method that enables
identification of sub-networks within complex interconnected systems without
estimating the entire network. The key insight is that under specific
topological conditions, a sub-network's parameters can be estimated using only
local measurements: signals within the target sub-network and those in the
directly connected to the so-called separator sub-network. This approach
significantly reduces computational complexity while enhancing privacy by
eliminating the need to share sensitive internal data across organizational
boundaries. We establish theoretical conditions for network separability,
derive the probability density function for the sub-network, and demonstrate
the method's effectiveness through numerical examples.

</details>


### [182] [An Alternative Derivation and Optimal Design Method of the Generalized Bilinear Transformation for Discretizing Analog Systems](https://arxiv.org/abs/2511.03403)
*Shen Chen,Yanlong Li,Jiamin Cui,Wei Yao,Jisong Wang,Yixin Tian,Chaohou Liu,Yang Yang,Jiaxi Ying,Zeng Liu,Jinjun Liu*

Main category: eess.SY

TL;DR: 本文提出了广义双线性变换(GBT)的新推导方法，通过六边形形状近似误差函数面积来定义形状因子α，揭示了其物理意义为后向矩形比，并开发了基于归一化幅值或相位误差的形状因子优化设计方法。


<details>
  <summary>Details</summary>
Motivation: 传统的欧拉和Tustin变换是GBT的特例，但GBT的设计参数α的物理意义和最优设计方法研究不足，需要深入探讨。

Method: 采用新的六边形形状近似误差函数面积来推导GBT，定义形状因子α，通过域映射分析稳定性范围[0.5,1]，基于归一化幅值或相位误差构建目标函数进行优化设计。

Result: 揭示了形状因子α的物理意义为后向矩形比，稳定范围为[0.5,1]，观察到幅值和相位两种失真模式，通过低通滤波器设计验证了方法的有效性。

Conclusion: 提出的GBT推导方法和形状因子优化设计能够有效控制数字系统设计中的失真，为数字滤波器设计提供了新的理论依据和实用方法。

Abstract: A popular method for designing digital systems is transforming the transfer
function of the corresponding analog systems from the continuous-time domain
(s-domain) into the discrete-time domain (z-domain) using the Euler or Tustin
method. We demonstrate that these transformations are two specific forms of the
Generalized Bilinear Transformation (GBT) with a design parameter, $\alpha$.
However, the physical meaning and optimal design method for this parameter are
not sufficiently studied. In this paper, we propose an alternative derivation
of the GBT derived by employing a new hexagonal shape to approximate the
enclosed area of the error function, and we define the parameter $\alpha$ as
the shape factor. The physical meaning of the shape factor is firstly revealed,
which equals to the percentage of the backward rectangular ratio of the
proposed hexagonal shape. We demonstrate that the stable range of the shape
factor is [0.5, 1] through domain mapping. Depending on the operating
frequencies and the shape factor, we observe two distinct distortion modes,
i.e., the magnitude and phase distortion. We proceed to develop an optimal
design method for the shape factor based on an objective function in form of
the normalized magnitude or phase error. Finally, a low-pass filter (LPF) is
designed and tested to verify the effectiveness of the proposed method by
comparing the theoretical calculations with the experimental results.

</details>


### [183] [System Identification of a Moored ASV with Recessed Moon Pool via Deterministic and Bayesian Hankel-DMDc](https://arxiv.org/abs/2511.03482)
*Giorgio Palma,Ivan Santic,Andrea Serani,Lorenzo Minno,Matteo Diez*

Main category: eess.SY

TL;DR: 本研究使用HDMDc和BHDMDc方法对系泊条件下的自主水面艇进行系统辨识，在规则和不规则波浪条件下验证了模型预测能力，首次展示了在不同海况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决带有月池的自主水面艇在系泊条件下的非线性响应建模挑战，月池内的晃荡效应增加了建模难度。

Method: 采用Hankel动态模态分解控制(HDMDc)及其贝叶斯扩展(BHDMDc)构建数据驱动的降阶模型，基于船体运动和系泊载荷测量数据。

Result: HDMDc提供了准确的确定性预测，BHDMDc通过考虑超参数选择的变异性实现了不确定性感知的模型响应表征，两种方法都能预测未见过的规则和不规则波浪激励下的船体响应。

Conclusion: HDMDc基降阶模型是系统辨识的可行数据驱动替代方案，首次展示了在不同训练集海况下的泛化能力，在重现船体动力学方面达到高精度。

Abstract: This study addresses the system identification of a small autonomous surface
vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition
with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were
carried out on a Codevintec CK-14e ASV in the towing tank of CNR-INM, under
both irregular and regular head-sea wave conditions. The ASV under
investigation features a recessed moon pool, which induces nonlinear responses
due to sloshing, thereby increasing the modelling challenge. Data-driven
reduced-order models were built from measurements of vessel motions and mooring
loads. The HDMDc framework provided accurate deterministic predictions of
vessel dynamics, while the Bayesian formulation enabled uncertainty-aware
characterization of the model response by accounting for variability in
hyperparameter selection. Validation against experimental data demonstrated
that both HDMDc and BHDMDc can predict the vessel's response to unseen regular
and irregular wave excitations. In conclusion, the study shows that HDMDc-based
ROMs are a viable data-driven alternative for system identification,
demonstrating for the first time their generalization capability for a sea
condition different from the training set, achieving high accuracy in
reproducing vessel dynamics.

</details>


### [184] [Data-driven Modeling of Grid-following Control in Grid-connected Converters](https://arxiv.org/abs/2511.03494)
*Amir Bahador Javadi,Philip Pong*

Main category: eess.SY

TL;DR: 评估稀疏识别非线性动力学和深度符号回归方法在捕捉现代电网复杂动态方面的有效性，使用基于转换器的资源替代传统发电机的系统设置生成合成数据。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源集成和智能电网技术发展，需要灵活可扩展的建模方法来准确捕捉现代电网的复杂动态。

Method: 使用基于转换器的资源替代传统发电机，在连接到无限母线的无损传输线系统中生成合成数据，评估稀疏识别非线性动力学和深度符号回归方法。

Result: 通过系统设置生成合成数据，用于评估这些方法在捕捉系统动态方面的有效性。

Conclusion: 研究为评估数据驱动方法在电力系统动态建模中的应用提供了框架和验证平台。

Abstract: As power systems evolve with the integration of renewable energy sources and
the implementation of smart grid technologies, there is an increasing need for
flexible and scalable modeling approaches capable of accurately capturing the
complex dynamics of modern grids. To meet this need, various methods, such as
the sparse identification of nonlinear dynamics and deep symbolic regression,
have been developed to identify dynamical systems directly from data. In this
study, we examine the application of a converter-based resource as a
replacement for a traditional generator within a lossless transmission line
linked to an infinite bus system. This setup is used to generate synthetic data
in grid-following control mode, enabling the evaluation of these methods in
effectively capturing system dynamics.

</details>


### [185] [Powered Descent Trajectory Optimization of Chandrayaan-3 using Radau Collocation and Controllable Sets](https://arxiv.org/abs/2511.03594)
*Suraj Kumar,Aditya Rallapalli,Ashok Kumar Kakula,Bharat Kumar GVP*

Main category: eess.SY

TL;DR: 本文介绍了印度月船3号任务的动力下降轨迹设计，采用伪谱Radau配点优化框架和基于可控性的航点细化方法，提高了轨迹对状态和控制扰动的鲁棒性，并量化了燃料消耗与鲁棒性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 印度于2023年8月23日成功实现月球软着陆，成为第四个实现此目标的国家。本文旨在设计月船3号任务的动力下降轨迹，确保在状态和控制扰动下的鲁棒性。

Method: 基于伪谱Radau配点的优化框架，结合基于可控性的航点细化方法，增强轨迹对扰动的鲁棒性。

Result: 成功设计了月船3号任务的动力下降轨迹，量化了燃料消耗与鲁棒性之间的权衡，为任务规划提供了实际考虑。

Conclusion: 通过优化框架和航点细化方法，月船3号任务实现了鲁棒的动力下降轨迹设计，为未来类似任务提供了参考。

Abstract: India achieved a significant milestone on August $23^{\text{rd}}$ 2023,
becoming the fourth country to accomplish a soft landing on the Moon. This
paper presents the powered descent trajectory design for the Chandrayaan-3
mission. The optimization framework is based on pseudospectral Radau
collocation, and controllability-based waypoint refinement is employed to
further enhance the robustness of the trajectory against state and control
perturbations. Furthermore, the trade-off between fuel consumption and
robustness is explicitly quantified, providing insights into the practical
considerations of mission planning.

</details>


### [186] [Artificial-reference tracking MPC with probabilistically validated performance on industrial embedded systems](https://arxiv.org/abs/2511.03603)
*Victor Gracia,Pablo Krupa,Filiberto Fele,Teodoro Alamo*

Main category: eess.SY

TL;DR: 提出了一种在嵌入式系统中高效实现带有人工参考的模型预测控制(MPC)的方法，通过结构利用的一阶方法解决，并集成了偏移消除方案、约束收紧和软约束等实用特性，同时提供了闭环系统长期运行的性能概率验证框架。


<details>
  <summary>Details</summary>
Motivation: 工业嵌入式系统计算资源有限，通常只能执行简单控制算法。虽然已有研究尝试在嵌入式系统上实现MPC，但通常采用简化的线性或显式公式，缺乏实际应用所需的特性和属性。

Method: 使用结构利用的一阶方法实现带有人工参考的MPC跟踪控制，集成了偏移消除方案、约束收紧参数和软约束，并在可编程逻辑控制器(PLC)上通过硬件在环设置控制非线性连续搅拌釜反应器进行验证。

Result: 开发的方法在保持小计算成本的同时，能够处理实际应用中的关键特性，包括扰动和模型失配下的可行性保持。通过概率验证框架评估了闭环系统在约束违反和MPC优化算法每次迭代所需次数方面的性能。

Conclusion: 该方法为嵌入式系统提供了一种高效的MPC实现方案，能够满足实际工业应用的需求，并通过概率验证确保了长期运行的可靠性。

Abstract: Industrial embedded systems are typically used to execute simple control
algorithms due to their low computational resources. Despite these limitations,
the implementation of advanced control techniques such as Model Predictive
Control (MPC) has been explored by the control community in recent years,
typically considering simple linear formulations or explicit ones to facilitate
the online computation of the control input. These simplifications often lack
features and properties that are desirable in real-world environments. In this
article, we present an efficient implementation for embedded systems of MPC for
tracking with artificial reference, solved via a recently developed
structure-exploiting first-order method. This formulation is tailored to a wide
range of applications by incorporating essential practical features at a small
computational cost, including integration with an offset-free scheme, back-off
parameters that enable constraint tightening, and soft constraints that
preserve feasibility under disturbances or plant-model mismatch. We accompany
this with a framework for probabilistic performance validation of the
closed-loop system over long-term operation. We illustrate the applicability of
the approach on a Programmable Logic Controller (PLC), incorporated in a
hardware-in-the-loop setup to control a nonlinear continuous stirred-tank
reactor. The behavior of the closed-loop system is probabilistically validated
with respect to constraint violations and the number of iterations required at
each time step by the MPC optimization algorithm.

</details>


### [187] [A Constant-Gain Equation-Error Framework for Airliner Aerodynamic Monitoring Using QAR Data](https://arxiv.org/abs/2511.03678)
*Ruiying Wen,Yuntao Dai,Hongyong Wang*

Main category: eess.SY

TL;DR: 提出了一种恒定增益方程误差方法（CG-EEM），用于解决使用QAR数据监测飞机气动性能时传统方法失效的问题，该方法在低激励巡航数据中表现稳定且准确。


<details>
  <summary>Details</summary>
Motivation: 传统状态传播滤波器因缺乏关键参数（如飞机转动惯量）而无法应用于QAR数据的气动性能监测，而标准递归估计器在低激励巡航数据中会出现过早收敛或不稳定的问题。

Method: 采用恒定增益方程误差方法（CG-EEM），使用类似卡尔曼滤波的恒定增益估计器，专门针对巡航飞行的平稳、低信噪比特性设计。

Result: 在包含200多个航班的多机队数据集上验证，CG-EEM产生了高度一致、物理上合理的气动参数，并能正确识别不同机型间的性能差异。

Conclusion: CG-EEM提供了一个稳健、可扩展且计算效率高的工具，可用于全机队性能监测和性能退化的早期检测。

Abstract: Monitoring the in-service aerodynamic performance of airliners is critical
for operational efficiency and safety, but using operational Quick Access
Recorder (QAR) data for this purpose presents significant challenges. This
paper first establishes that the absence of key parameters, particularly
aircraft moments of inertia, makes conventional state-propagation filters
fundamentally unsuitable for this application. This limitation necessitates a
decoupled, Equation-Error Method (EEM). However, we then demonstrate through a
comparative analysis that standard recursive estimators with time-varying
gains, such as Recursive Least Squares (RLS), also fail within an EEM
framework, exhibiting premature convergence or instability when applied to
low-excitation cruise data. To overcome these dual challenges, we propose and
validate the Constant-Gain Equation-Error Method (CG-EEM). This framework
employs a custom estimator with a constant, Kalman-like gain, which is
perfectly suited to the stationary, low-signal-to-noise characteristics of
cruise flight. The CG-EEM is extensively validated on a large, multi-fleet
dataset of over 200 flights, where it produces highly consistent, physically
plausible aerodynamic parameters and correctly identifies known performance
differences between aircraft types. The result is a robust, scalable, and
computationally efficient tool for fleet-wide performance monitoring and the
early detection of performance degradation.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [188] [PELVE from a regulatory perspective](https://arxiv.org/abs/2511.03551)
*Christian Laudagé,Jörn Sass*

Main category: q-fin.MF

TL;DR: 该论文提出了基于PELVE概念的方法来确定多个保险公司的预期短缺(ES)水平，从监管机构角度出发，分析了ES水平的存在性和唯一性，并针对椭圆分布和多元正则分布推导了表达式。


<details>
  <summary>Details</summary>
Motivation: 在Solvency II框架下使用VaR存在共识认为ES是更合适的风险度量，但转向ES需要确定相应的ES水平。现有PELVE方法只考虑单个保险公司，需要从监管机构角度考虑多个保险公司的情况。

Method: 提出了基于PELVE概念的多保险公司方法，分析ES水平的存在性和唯一性，推导椭圆分布和多元正则分布下的表达式，并进行案例研究比较不同方法。

Result: 研究发现方法选择对来自不同分布族的赔付至关重要，为椭圆分布和多元正则分布推导了表达式，并建立了极限结果。

Conclusion: 推荐特定的方法，强调在多个保险公司情况下选择合适的ES水平确定方法的重要性。

Abstract: Under Solvency II, the Value-at-Risk (VaR) is applied, although there is
broad consensus that the Expected Shortfall (ES) constitutes a more appropriate
measure. Moving towards ES would necessitate specifying the corresponding ES
level. The recently introduced Probability Equivalent Level of VaR and ES
(PELVE) determines this by requiring that ES equals the prescribed VaR for a
given future payoff, reflecting the situation of an individual insurer. We
incorporate the regulator's perspective by proposing PELVE-inspired methods for
multiple insurers. We analyze existence and uniqueness of the resulting ES
levels, derive expressions for elliptically distributed payoffs and establish
limit results for multivariate regularly distributed payoffs. A case study
highlights that the choice of method is crucial when payoffs arise from
different distribution families. Moreover, we recommend specific methods.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [189] [Google's Hidden Empire](https://arxiv.org/abs/2511.02931)
*Aline Blankertz,Brianna Rock,Nicholas Shaxson*

Main category: cs.CY

TL;DR: 本文揭示了谷歌在全球数字和企业格局中的惊人规模，远超其他大型科技公司。谷歌通过收购、支持和投资建立了6000多家公司的帝国，其市场力量比先前记录的要大得多。


<details>
  <summary>Details</summary>
Motivation: 研究谷歌通过并购活动积累的市场力量，以及反垄断监管失败如何导致这种状况，为当前谷歌320亿美元收购Wiz的交易提供经验教训。

Method: 通过分析谷歌的并购数据，追溯反垄断失败的原因，特别是监管机构和咨询公司采用的新古典经济学方法的局限性，并以谷歌/DoubleClick和谷歌/Fitbit合并案为例。

Result: 发现谷歌已建立包含6000多家公司的庞大网络，其数字市场基础设施和动态控制力远超预期，反垄断监管因方法过于狭隘而未能有效干预。

Conclusion: 过去反垄断监管的失败经验可为当前谷歌320亿美元收购Wiz的交易提供重要参考，需要更全面的监管方法来应对垂直和混合市场力量集中带来的损害。

Abstract: This paper presents striking new data about the scale of Google's involvement
in the global digital and corporate landscape, head and shoulders above the
other big tech firms. While public attention and some antitrust scrutiny has
focused on these firms' mergers and acquisitions (M&A) activities, Google has
also been amassing an empire of more than 6,000 companies which it has
acquired, supported or invested in, across the digital economy and beyond. The
power of Google over the digital markets infrastructure and dynamics is likely
greater than previously documented. We also trace the antitrust failures that
have led to this state of affairs. In particular, we explore the role of
neoclassical economics practiced both inside the regulatory authorities and by
consultants on the outside. Their unduly narrow approach has obscured harms
from vertical and conglomerate concentrations of market power and erected ever
higher hurdles for enforcement action, as we demonstrate using examples of the
failure to intervene in the Google/DoubleClick and Google/Fitbit mergers. Our
lessons from the past failures can inform the current approach towards one of
the biggest ever big tech M&A deals: Google's $32 billion acquisition of the
Israeli cloud cybersecurity firm Wiz.

</details>


### [190] [Teaching Quantum Computing through Lab-Integrated Learning: Bridging Conceptual and Computational Understanding](https://arxiv.org/abs/2511.02844)
*Umar Farooq,Krishna Upadhyay*

Main category: cs.CY

TL;DR: 该论文介绍了一种结合本科和研究生课程的量子计算教学方法，采用实验整合学习模式，通过编程实验帮助学生从经典编程思维转向量子计算思维。


<details>
  <summary>Details</summary>
Motivation: 量子计算教育需要学生超越经典编程的直觉，发展基于概率、测量和干涉的推理技能。传统教学方法难以帮助学生理解量子概念，因此需要创新的教学策略。

Method: 采用实验整合学习模型，将讲座与每周编程实验配对。课程从Quantum Without Linear Algebra (QWLA)开始，使用直观的字典表示介绍核心概念，然后过渡到IBM Qiskit进行电路设计、噪声模拟和算法实现。

Result: 学生工作和反馈分析表明，动手实验提高了学生的信心、概念清晰度和跨表示流利度，但也揭示了在调试、测量推理和理解概率结果方面的持续挑战。

Conclusion: 实验整合学习为计算机科学教育中的量子计算教学提供了一种有效且易于实施的方法，能够支持概念转变和渐进理解。

Abstract: Quantum computing education requires students to move beyond classical
programming intuitions related to state, determinism, and debugging, and to
develop reasoning skills grounded in probability, measurement, and
interference. This paper reports on the design and delivery of a combined
undergraduate and graduate course at Louisiana State University that employed a
lab-integrated learning model to support conceptual change and progressive
understanding. The course paired lectures with weekly programming labs that
served as environments for experimentation and reflection. These labs enabled
students to confront misconceptions and refine their mental models through
direct observation and evidence-based reasoning. Instruction began with Quantum
Without Linear Algebra (QWLA), which introduced core concepts such as
superposition and entanglement through intuitive, dictionary representations.
The course then transitioned to IBM Qiskit, which provided a professional
framework for circuit design, noise simulation, and algorithm implementation.
Analysis of student work and feedback indicated that hands-on experimentation
improved confidence, conceptual clarity, and fluency across representations. At
the same time, it revealed persistent challenges in debugging, reasoning about
measurement, and understanding probabilistic outcomes. This paper presents the
course structure, instructional strategies, and lessons learned, and argues
that lab-integrated learning offers an effective and accessible approach to
teaching quantum computing in computer science education.

</details>


### [191] [Academics and Generative AI: Empirical and Epistemic Indicators of Policy-Practice Voids](https://arxiv.org/abs/2511.02875)
*R. Yamamoto Ravenor*

Main category: cs.CY

TL;DR: 开发了一个十项间接引出工具，用于检测学术机构规则与AI实践之间的差距，通过三个过滤指标来评估AI整合评估能力、部门必要性和本体论立场。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在学术界的普及，政策与实践之间的分歧变得重要，需要可审计的对齐指标。

Method: 采用结构化解释框架，从学者中提取经验和认知信号，通过三个过滤指标来识别制度规则与AI使用之间的差距。

Result: 提出了三个过滤指标：AI整合评估能力、部门级必要性和本体论立场，用于映射采购声明与证据类别。

Conclusion: 该工具能够有效识别和评估学术机构AI政策与实践之间的对齐差距，为审计提供实证基础。

Abstract: As generative AI diffuses through academia, policy-practice divergence
becomes consequential, creating demand for auditable indicators of alignment.
This study prototypes a ten-item, indirect-elicitation instrument embedded in a
structured interpretive framework to surface voids between institutional rules
and practitioner AI use. The framework extracts empirical and epistemic signals
from academics, yielding three filtered indicators of such voids: (1)
AI-integrated assessment capacity (proxy) - within a three-signal screen (AI
skill, perceived teaching benefit, detection confidence), the share who would
fully allow AI in exams; (2) sector-level necessity (proxy) - among high output
control users who still credit AI with high contribution, the proportion who
judge AI capable of challenging established disciplines; and (3) ontological
stance - among respondents who judge AI different in kind from prior tools,
report practice change, and pass a metacognition gate, the split between
material and immaterial views as an ontological map aligning procurement claims
with evidence classes.

</details>


### [192] [A Criminology of Machines](https://arxiv.org/abs/2511.02895)
*Gian Maria Campedelli*

Main category: cs.CY

TL;DR: 该论文主张犯罪学需要应对自主AI代理兴起带来的犯罪和社会控制影响，提出将AI视为具有计算、社会和法律维度的实体，并分析了多代理AI系统的风险及四个关键研究问题。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理在各行业和数字环境中的部署日益增多，机器间互动将变得更加普遍，犯罪学必须开始应对这种转变对犯罪和社会控制的影响。

Method: 基于行动者网络理论和Woolgar的机器社会学框架，提出将AI视为具有能动性的实体，构建双重分类法来分析AI代理间互动产生越轨、违法或犯罪结果的渠道。

Result: 提出了四个关键研究问题：机器是否会简单模仿人类、现有犯罪理论是否适用于AI代理的越轨行为、哪些犯罪行为将首先受影响、以及这种社会转变如何影响警务。

Conclusion: 犯罪学家迫切需要从理论和实证层面参与多代理AI系统对犯罪研究的影响，并在AI安全和治理辩论中发挥更积极的作用。

Abstract: While the possibility of reaching human-like Artificial Intelligence (AI)
remains controversial, the likelihood that the future will be characterized by
a society with a growing presence of autonomous machines is high. Autonomous AI
agents are already deployed and active across several industries and digital
environments and alongside human-human and human-machine interactions,
machine-machine interactions are poised to become increasingly prevalent. Given
these developments, I argue that criminology must begin to address the
implications of this transition for crime and social control. Drawing on
Actor-Network Theory and Woolgar's decades-old call for a sociology of machines
-- frameworks that acquire renewed relevance with the rise of generative AI
agents -- I contend that criminologists should move beyond conceiving AI solely
as a tool. Instead, AI agents should be recognized as entities with agency
encompassing computational, social, and legal dimensions. Building on the
literature on AI safety, I thus examine the risks associated with the rise of
multi-agent AI systems, proposing a dual taxonomy to characterize the channels
through which interactions among AI agents may generate deviant, unlawful, or
criminal outcomes. I then advance and discuss four key questions that warrant
theoretical and empirical attention: (1) Can we assume that machines will
simply mimic humans? (2) Will crime theories developed for humans suffice to
explain deviant or criminal behaviors emerging from interactions between
autonomous AI agents? (3) What types of criminal behaviors will be affected
first? (4) How might this unprecedented societal shift impact policing? These
questions underscore the urgent need for criminologists to theoretically and
empirically engage with the implications of multi-agent AI systems for the
study of crime and play a more active role in debates on AI safety and
governance.

</details>


### [193] [Ownership and Flow Primitives for Scalable Consent Management in Digital Public Infrastructures](https://arxiv.org/abs/2511.02950)
*Rohith Vaidyanathan,Srinath Srinivasa,Praseeda,Dev Shinde*

Main category: cs.CY

TL;DR: 本文提出了数字公共基础设施（DPI）中数字资产所有权模式和相应同意数据流的基础抽象表示方法，旨在解决大规模人口中的复杂同意管理问题。


<details>
  <summary>Details</summary>
Motivation: DPI设计中的关键挑战是在大规模人群中解决复杂的同意问题，需要在个人自主权、公共福祉和国家主权之间取得平衡，并使同意管理符合数据共享法规要求。

Method: 提出了一套基础抽象来表示数字资产的所有权模式及其对同意数据流的影响，构建了形式化的数据所有权模型架构。

Result: 所提出的架构实现了同意端到端的可追溯性、对数据共享的细粒度控制，并与不断发展的法律和监管框架保持一致。

Conclusion: 该架构满足了数字公共基础设施对透明、安全和以用户为中心的同意管理日益增长的需求，通过形式化的数据所有权模型支持有效的同意管理。

Abstract: Digital public infrastructures (DPIs) represent networks of open technology
standards, applications, services, and digital assets made available for the
public good. One of the key challenges in DPI design is to resolve complex
issues of consent, scaled over large populations. While the primary objective
of consent management is to empower the data owner, ownership itself can come
with variegated morphological forms with different implications over consent.
Questions of ownership in a public space also have several nuances where
individual autonomy needs to be balanced with public well-being and national
sovereignty. This requires consent management to be compliant with applicable
regulations for data sharing. This paper addresses the question of representing
modes of ownership of digital assets and their corresponding implications for
consensual data flows in a DPI. It proposes a set of foundational abstractions
to represent them. Our proposed architecture responds to the growing need for
transparent, secure, and user-centric consent management within Digital Public
Infrastructure (DPI). Incorporating a formalised data ownership model enables
end-to-end traceability of consent, fine-grained control over data sharing, and
alignment with evolving legal and regulatory frameworks.

</details>


### [194] [Retrofitters, pragmatists and activists: Public interest litigation for accountable automated decision-making](https://arxiv.org/abs/2511.03211)
*Henry Fraser,Zahra Stardust*

Main category: cs.CY

TL;DR: 本文探讨了在澳大利亚通过公益诉讼促进AI和自动化决策问责的作用，分析了在监管面临挑战时如何利用现有法律进行有效治理。


<details>
  <summary>Details</summary>
Motivation: 由于自动化决策监管面临地缘政治阻力，需要依靠现有法律的执行来实现有效治理，公益诉讼成为促进透明度和问责的重要途径。

Method: 通过对澳大利亚公益诉讼律师、技术政策活动家和技术法学者的访谈，将公益诉讼定位为自动化决策问责生态系统的一部分。

Result: 汇集并组织了关于有效公益诉讼的实用策略和战术，同时认识到法律系统的局限性，并提出了克服这些限制所需的制度安排。

Conclusion: 公益诉讼是适应新技术的法律改造过程，需要相应的制度支持才能有效促进自动化决策的问责和正义。

Abstract: This paper examines the role of public interest litigation in promoting
accountability for AI and automated decision-making (ADM) in Australia. Since
ADM regulatio faces geopolitical headwinds, effective governance will have to
rely at least in part on the enforcement of existing laws. Drawing on
interviews with Australian public interest litigators, technology policy
activists, and technology law scholars, the paper positions public interest
litigation as part of a larger ecosystem for transparency, accountability and
justice with respect to ADM. It builds on one participants's characterisation
of litigation about ADM as an exercise in legal retrofitting: adapting old laws
to new circumstances. The paper's primary contribution is to aggregate,
organise and present original insights on pragmatic strategies and tactics for
effective public interest litigation about ADM. Naturally, it also contends
with the limits of these strategies, and of the legal system. Where limits are,
however, capable of being overcome, the paper presents findings on urgent
needs: the enabling institutional arrangements without which effective
litigation and accountability will falter. The paper is relevant to law and
technology scholars; individuals and groups harmed by ADM; public interest
litigators and technology lawyers; civil society and advocacy organisations;
and policymakers.

</details>
